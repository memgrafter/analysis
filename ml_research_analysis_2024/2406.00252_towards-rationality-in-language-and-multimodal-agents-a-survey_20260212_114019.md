---
ver: rpa2
title: 'Towards Rationality in Language and Multimodal Agents: A Survey'
arxiv_id: '2406.00252'
source_url: https://arxiv.org/abs/2406.00252
tags:
- arxiv
- preprint
- language
- wang
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically examines the concept of rationality
  in language and multimodal agents, identifying four necessary axioms: information
  grounding, logical consistency, invariance from irrelevant context, and orderability
  of preference. It reviews state-of-the-art approaches including multimodal foundation
  models, multi-agent systems, neuro-symbolic reasoning, and external tool integration
  that advance these axioms.'
---

# Towards Rationality in Language and Multimodal Agents: A Survey

## Quick Facts
- arXiv ID: 2406.00252
- Source URL: https://arxiv.org/abs/2406.00252
- Reference count: 40
- Key outcome: Survey identifies four necessary rationality axioms and reviews approaches advancing them in language and multimodal agents

## Executive Summary
This survey systematically examines rationality in language and multimodal agents by identifying four necessary axioms: information grounding, logical consistency, invariance from irrelevant context, and orderability of preference. The authors review state-of-the-art approaches including multimodal foundation models, multi-agent systems, neuro-symbolic reasoning, and external tool integration that advance these axioms. The paper highlights limitations in current evaluation methods for measuring agent rationality and outlines open challenges in inherently enhancing agent rationality beyond reliance on external tools. It emphasizes the growing importance of rational agents in critical domains like healthcare and finance where consistent, reliable decision-making is essential.

## Method Summary
The paper conducts a systematic literature review through the lens of rationality axioms, examining how different approaches contribute to each axiom. The authors analyze multimodal foundation models, multi-agent collaboration systems, neuro-symbolic reasoning approaches, and external tool utilization methods. They evaluate current evaluation methods and identify gaps in comprehensive rationality assessment. The survey synthesizes findings from 40 references to map the current landscape of rational agent development and identify open challenges.

## Key Results
- Four rationality axioms identified: information grounding, logical consistency, invariance from irrelevant context, and orderability of preference
- Multimodal foundation models provide richer grounding through cross-modal pretraining but still face hallucination challenges
- Multi-agent systems enable System 2-like reasoning through debate and consensus mechanisms
- External tools and neuro-symbolic reasoning provide deterministic execution but create dependency challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal grounding reduces hallucinations by providing richer, more accurate input representations.
- Mechanism: Large multimodal models (LMMs) tokenize both visual and language inputs into a joint embedding space through cross-modal pretraining. This unified representation allows the model to reconcile data from different modalities, reducing the reliance on purely language-based generation that can hallucinate. For example, VL-BERT and CLIP learn cross-modal correlations, making image tokens "a foreign language" that the model can understand alongside text.
- Core assumption: The joint embedding space captures sufficient semantic alignment between modalities to ground responses in factual reality.
- Evidence anchors:
  - [abstract] "Multimodal foundation models... offer a more comprehensive grounding of information, thereby enhancing the understanding of decision-making contexts."
  - [section 4.1.1] "Multimodal foundation models, including but not limited to CLIP (Radford et al., 2021), VL-BERT and ViLBERT (Su et al., 2019; Lu et al., 2019), BLIP-2 (Li et al., 2023d), UniAudio (Yang et al., 2023a)... serve as the cornerstones for downstream tasks in multimodal agent systems."
  - [corpus] Weak - corpus papers do not directly address multimodal grounding mechanisms.
- Break condition: If cross-modal pretraining fails to align semantic representations adequately, the model may still hallucinate based on one modality or produce misaligned outputs.

### Mechanism 2
- Claim: Multi-agent collaboration improves logical consistency by enabling deliberate, System 2-like reasoning through debate and consensus.
- Mechanism: Multiple agents engage in cross-examination and debate, allowing for error detection and correction that mimics the slow, deliberate thinking of human System 2 processes. This collective deliberation can capture initial errors and achieve consensus with fewer inconsistencies than a single LLM. For example, LM vs LM introduces cross-examination between two agents to detect errors and make factuality decisions.
- Core assumption: Agents can effectively critique each other's reasoning and converge on a logically consistent consensus.
- Evidence anchors:
  - [abstract] "Multi-agent systems that promote debate and consensus among AI agents can help align outputs more closely with the slow, deliberate decision-making typical of System 2 processes, thus enhancing logical consistency."
  - [section 4.2.1] "Multi-agent systems introduce collaboration among multiple agents (Zhang et al., 2024d), enabling collective deliberation through cross-examination and debate. For instance, LM vs LM (Cohen et al., 2023) introduces a cross-examination between two agents to detect errors and make factuality decisions."
  - [corpus] Weak - corpus papers focus on economic rationality and game-theoretic reasoning rather than multi-agent debate mechanisms.
- Break condition: If agents cannot effectively critique each other or if debate leads to polarization rather than consensus, logical consistency may not improve.

### Mechanism 3
- Claim: External tools and neuro-symbolic reasoning modules provide deterministic execution that ensures consistent outcomes and reduces reliance on probabilistic generation.
- Mechanism: By translating natural language queries into function calls with predefined syntax, tools and symbolic reasoners narrow their focus to logical essence, ignoring irrelevant context. This abstraction enables more focused and efficient reasoning processes. For example, Parsel combines a code-LLM with a constraint solver to deterministically handle decomposed tasks.
- Core assumption: The translation from natural language to formal representations preserves logical equivalence and the tools can execute reliably.
- Evidence anchors:
  - [abstract] "Tool utilization abstracts problems into deterministic tool executions, such as calculators, calendars (Schick et al., 2024), and programming codes, and seamlessly integrate reliable outputs back to the responses."
  - [section 4.2.2] "ToolAlpaca (Tang et al., 2023) generates a tool-use corpus in a multi-agent simulation environment. Binder (Cheng et al., 2022) converts queries into Python or SQL codes to interact deterministically with structured knowledge bases."
  - [corpus] Weak - corpus papers do not directly address tool utilization or neuro-symbolic reasoning mechanisms.
- Break condition: If the translation process introduces errors or if tools fail to execute as expected, the system may produce inconsistent or incorrect outputs.

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: Understanding how LMMs create unified embeddings across modalities is crucial for grasping how they achieve better information grounding and reduce hallucinations.
  - Quick check question: How do models like CLIP and VL-BERT learn to align visual and language representations in a shared embedding space?

- Concept: System 1 vs System 2 thinking
  - Why needed here: Recognizing the difference between fast, intuitive (System 1) and slow, deliberate (System 2) thinking processes helps explain why multi-agent debate can improve logical consistency.
  - Quick check question: What are the key characteristics that distinguish System 1 thinking from System 2 thinking in human cognition?

- Concept: Neuro-symbolic integration
  - Why needed here: Understanding how neural networks can be combined with symbolic reasoning systems is essential for grasping how deterministic execution ensures consistent outcomes.
  - Quick check question: How do neuro-symbolic approaches like Parsel combine neural networks with symbolic solvers to achieve reliable reasoning?

## Architecture Onboarding

- Component map: Multimodal data (text, images, audio) → Large multimodal model (CLIP, VL-BERT) → Reasoning modules (multi-agent system, neuro-symbolic reasoner, or external tools) → Grounded, consistent responses → Feedback loop (conformal risk control or utility maximization)

- Critical path: Multimodal input → Joint embedding → Reasoning (multi-agent or symbolic) → Tool execution (if needed) → Output with risk control

- Design tradeoffs:
  - Multimodal vs unimodal: Multimodal provides richer grounding but increases complexity and computational cost.
  - Multi-agent vs single-agent: Multi-agent improves logical consistency through debate but adds coordination overhead.
  - Tool reliance vs model capability: Tools provide deterministic execution but create dependency and potential failure points.

- Failure signatures:
  - Hallucinations persist despite multimodal input: Cross-modal alignment may be insufficient.
  - Inconsistent outputs from multi-agent system: Agents may not effectively critique or consensus may be difficult to achieve.
  - Tool execution failures: Translation from natural language to formal representations may introduce errors.

- First 3 experiments:
  1. Implement a simple CLIP-based multimodal grounding system and measure hallucination reduction on a standard benchmark.
  2. Create a two-agent debate system for logical consistency checking and evaluate performance on a reasoning task.
  3. Integrate a Python code execution tool with an LLM and measure consistency improvements on arithmetic and symbolic reasoning tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a single foundation model be trained to inherently achieve rationality without relying on external tools and modules?
- Basis in paper: [explicit] The paper states that current approaches are "neither sufficient nor necessary to achieve human-level rationality" and remain dependent on external tools, raising the question of whether we can "leverage these more rational outputs... to inherently enhance a single foundation model's rationality in its initial responses without external assistance."
- Why unresolved: All current approaches discussed in the paper (neuro-symbolic reasoning, external tools, multi-agent collaboration) are external modules that augment LLM capabilities rather than being baked into the model itself. The paper explicitly questions whether we can "close the loop and bake these more rational outputs back into foundation models themselves."
- What evidence would resolve it: Demonstrating a foundation model that achieves similar rationality metrics as current tool-augmented systems but without requiring external modules, or showing that training with rationality-focused rewards improves inherent model rationality in initial responses.

### Open Question 2
- Question: What comprehensive evaluation metrics can accurately measure rationality across all four proposed axioms simultaneously?
- Basis in paper: [explicit] The paper highlights "the lack of sufficient evaluation metrics and benchmarks in the existing literature to adequately measure the rationality of agents" and notes that "existing evaluations predominantly focus on the final performance, neglecting the most interesting intermediate steps and different axioms of rationality."
- Why unresolved: Current benchmarks focus on specific aspects like hallucination detection or logical consistency, but no comprehensive framework exists to evaluate all four rationality axioms (information grounding, logical consistency, invariance from irrelevant context, orderability of preference) in a unified manner.
- What evidence would resolve it: Development and validation of a multi-dimensional evaluation framework that can simultaneously assess all four rationality axioms across diverse tasks and contexts, with demonstrated reliability and statistical significance.

### Open Question 3
- Question: How can multi-modality be more effectively integrated into multi-agent systems to enhance rationality?
- Basis in paper: [explicit] The paper notes that "Research into the integration of multi-modality within multi-agent systems would be promising" and observes that "Fields such as multi-agent collaboration and symbolic reasoning, as shown in Figure 2, currently under-utilize the potential of multimodal sensory inputs."
- Why unresolved: While the paper discusses multimodal foundation models and their benefits for information grounding, it doesn't explore how multi-modality could be leveraged within multi-agent systems beyond simple visual-language understanding tasks.
- What evidence would resolve it: Demonstrated improvements in multi-agent system rationality (across all four axioms) when incorporating diverse modalities like audio, sensor data, or structured information, with clear comparisons to unimodal approaches.

## Limitations

- Limited empirical validation: The survey primarily relies on qualitative assessment rather than quantitative benchmarks measuring progress toward each rationality axiom.
- No comprehensive evaluation framework: The paper identifies gaps in current assessment methods but doesn't provide systematic analysis or propose new evaluation frameworks.
- Unclear axiom conflicts: The survey doesn't address potential conflicts between axioms (e.g., when grounding might conflict with logical consistency).

## Confidence

- High Confidence: The identification of four rationality axioms (information grounding, logical consistency, invariance, orderability) - these are well-established concepts in decision theory and AI alignment literature.
- Medium Confidence: The characterization of current approaches (multimodal models, multi-agent systems, neuro-symbolic reasoning, tools) and their theoretical contributions to each axiom - supported by literature but lacking comprehensive empirical validation.
- Low Confidence: The assessment of evaluation methods and their adequacy for measuring rationality - the survey identifies gaps but doesn't provide systematic analysis of existing benchmarks or propose new evaluation frameworks.

## Next Checks

1. Conduct empirical studies comparing hallucination rates between unimodal and multimodal language models on standardized benchmarks with visual grounding requirements.

2. Implement and evaluate a multi-agent debate system on a suite of logical consistency tests, measuring both improvement in consistency and potential new failure modes introduced by debate.

3. Develop and validate a comprehensive evaluation framework that simultaneously measures all four rationality axioms, addressing the survey's identified gaps in current assessment methods.