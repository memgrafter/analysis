---
ver: rpa2
title: 'AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data for
  3D-Native Segmentation'
arxiv_id: '2408.00640'
source_url: https://arxiv.org/abs/2408.00640
tags:
- pretraining
- segmentation
- dataset
- amaes
- u-net
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AMAES, a framework for pretraining 3D segmentation
  models using Masked Autoencoder (MAE) with intensity-based augmentation reversal.
  The authors propose BRAINS-45K, the largest public brain MRI dataset (44,756 volumes)
  for pretraining.
---

# AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data for 3D-Native Segmentation

## Quick Facts
- **arXiv ID**: 2408.00640
- **Source URL**: https://arxiv.org/abs/2408.00640
- **Reference count**: 30
- **Key outcome**: Introduces AMAES, a framework for pretraining 3D segmentation models using Masked Autoencoder with intensity-based augmentation reversal on BRAINS-45K (44,756 volumes), showing significant performance improvements over training from scratch and SwinUNETR pretraining across three challenging segmentation tasks.

## Executive Summary
This paper presents AMAES, a self-supervised pretraining framework for 3D brain MRI segmentation that leverages Masked Autoencoder (MAE) with intensity-based augmentation reversal. The authors introduce BRAINS-45K, the largest public brain MRI dataset, assembled from multiple sources to create a heterogeneous training corpus. By pretraining U-Net and MedNeXt backbones on this dataset, they demonstrate substantial improvements in downstream segmentation tasks (tumor, stroke lesion, white matter hyperintensities) compared to training from scratch or using SwinUNETR pretraining, particularly in low-resource settings with limited labeled data.

## Method Summary
AMAES combines masked image modeling with augmentation reversal, where the model learns to reconstruct masked patches from unmasked versions with only spatial transformations applied. The framework uses a lightweight decoder (half the convolutional layers of standard U-Net decoder) during pretraining to force the encoder to learn more expressive representations while reducing computational cost. Training is performed on 3D patches (128³ voxels) from the BRAINS-45K dataset with 60% masking probability and 4³ voxel subpatches. Spatial augmentations are applied during finetuning, while intensity-based augmentations are used during pretraining to learn intensity-invariant representations.

## Key Results
- Pretraining on BRAINS-45K with AMAES significantly improves segmentation performance across all evaluated tasks
- U-Net XL achieves 58.98 Dice score on BraTS21 with 20 training samples using AMAES vs 56.00 without pretraining
- Lightweight decoder design reduces pretraining time by 20% while increasing downstream performance by 1.96 Dice points
- AMAES outperforms SwinUNETR pretraining on the same downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on a large-scale, heterogeneous brain MRI dataset improves downstream segmentation performance in low-resource settings.
- Mechanism: Exposure to diverse acquisition parameters, scanner types, and patient populations during pretraining allows the model to learn domain-invariant features that generalize better to new segmentation tasks with limited labeled data.
- Core assumption: The pretraining dataset is sufficiently diverse and representative of real-world clinical variability.
- Evidence anchors:
  - [abstract] "BRAINS-45K, the largest public dataset available... The results highlight that pretraining on the proposed dataset with AMAES significantly improves segmentation performance in the majority of evaluated cases."
  - [section 2] "The resulting dataset is highly heterogeneous and assembled to simulate the characteristics of clinical data."
- Break condition: If the pretraining dataset lacks sufficient diversity or if downstream tasks have very different characteristics than the pretraining data.

### Mechanism 2
- Claim: Masked Autoencoder pretraining with intensity-based augmentation reversal learns representations robust to intensity shifts between different domains.
- Mechanism: By masking patches and reconstructing them from unmasked versions with only spatial transformations, the model learns to predict intensity values invariant to intensity-based augmentations while being covariant to spatial transformations.
- Core assumption: Intensity-based augmentations during pretraining don't destroy semantic information that would be needed for reconstruction.
- Evidence anchors:
  - [abstract] "AMAES combines MIM with augmentation reversal to learn representations robust to intensity shifts between different domains."
  - [section 3.1] "We apply an extensive augmentation pipeline to the data during pretraining... we seek to learn representations that are invariant to all other augmentations, for instance additive noise."
- Break condition: If intensity-based augmentations remove critical intensity information needed for segmentation tasks.

### Mechanism 3
- Claim: Using a lightweight decoder during pretraining forces the encoder to learn more expressive representations while reducing computational cost.
- Mechanism: A smaller decoder provides less capacity for reconstruction, requiring the encoder to extract more informative features. This improves downstream performance while reducing pretraining time and memory usage.
- Core assumption: The encoder has sufficient capacity to learn the necessary representations even with a constrained decoder.
- Evidence anchors:
  - [section 3.1] "Since the decoder is discarded after pretraining, it is desirable to use as small a decoder as possible... the light weight decoder not only decreases pretraining time with 20%, but also further increases finetuning performance by 1.96 dice points."
  - [table 2] Direct comparison showing performance improvement with lightweight decoder.
- Break condition: If the encoder capacity is insufficient to learn necessary features with the constrained decoder.

## Foundational Learning

- Concept: Masked Autoencoder (MAE) architecture
  - Why needed here: Forms the basis of the pretraining framework, where patches are masked and the model learns to reconstruct them.
  - Quick check question: What is the difference between a standard autoencoder and a masked autoencoder?

- Concept: Domain adaptation and generalization
  - Why needed here: Understanding how pretraining on diverse data helps models generalize to new tasks and domains.
  - Quick check question: How does exposure to diverse training data during pretraining help with out-of-domain generalization?

- Concept: Augmentation strategies in self-supervised learning
  - Why needed here: The choice of augmentations and their targets (intensity vs spatial) is crucial for learning robust representations.
  - Quick check question: Why would you want the model to be invariant to intensity augmentations but covariant to spatial augmentations?

## Architecture Onboarding

- Component map:
  Backbone encoder (U-Net or MedNeXt) -> Masking module (60% patch masking probability, 4x4x4 voxel subpatches) -> Lightweight decoder (half the convolutional layers of standard U-Net decoder) -> Augmentation pipeline (spatial and intensity-based)

- Critical path:
  1. Apply spatial and intensity augmentations to input patch
  2. Mask patches with 60% probability
  3. Pass through encoder
  4. Decode to reconstruct unmasked image with only spatial transformations
  5. Compute MSE loss between reconstruction and target
  6. Transfer encoder weights to finetuning stage

- Design tradeoffs:
  - Lightweight decoder: Faster pretraining, better downstream performance vs. potential underfitting
  - Skip connections: Not used during pretraining to force encoder learning vs. possible performance benefit
  - Augmentation strategy: Intensity invariance for robustness vs. potential loss of intensity-specific information

- Failure signatures:
  - Catastrophic forgetting during finetuning (when using full augmentation package)
  - Underperformance on out-of-domain data (if pretraining dataset lacks diversity)
  - Overfitting on downstream tasks (if pretraining is insufficient)

- First 3 experiments:
  1. Pretrain U-Net XL with AMAES on BRAINS-45K, then finetune on BraTS21 with 20 samples
  2. Compare pretraining with and without intensity augmentations on downstream performance
  3. Test out-of-domain generalization by evaluating on WMH OOD dataset after pretraining on BRAINS-45K

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AMAES performance scale with increasing dataset size beyond BRAINS-45K?
- Basis in paper: [inferred] The paper states "We have not applied AMAES to the multi-sequence versions of the dataset and neither have we applied it to datasets with more datapoints than ntrain = 20."
- Why unresolved: The paper only evaluates on the 44,756-volume BRAINS-45K dataset and a low-resource setting with 20 training samples, not exploring scalability to larger datasets.
- What evidence would resolve it: Testing AMAES pretraining on progressively larger datasets and measuring downstream performance gains would show the scaling relationship.

### Open Question 2
- Question: What is the impact of skip connections during pretraining on downstream segmentation performance?
- Basis in paper: [explicit] "While skip connections are used during finetuning, our proposed encoder-decoder architecture does not include skip connections during pretraining. We found that using skip connections during pretraining tended to slightly adversely affect downstream performance..."
- Why unresolved: The paper notes skip connections slightly hurt pretraining performance but doesn't conduct a systematic ablation study or explore architectural modifications to resolve this trade-off.
- What evidence would resolve it: A controlled experiment varying skip connection usage during pretraining while keeping other factors constant would clarify their optimal role.

### Open Question 3
- Question: How does AMAES compare to SwinUNETR when using SwinUNETR as the backbone encoder?
- Basis in paper: [explicit] "This study neither tests the SwinUNETR framework using different backbones and nor tests AMAES using a SwinUNETR backbone, since it is not readily compatible with a U-Net decoder."
- Why unresolved: The paper uses U-Net and MedNeXt backbones for AMAES but doesn't explore using SwinUNETR's Swin-Transformer architecture, which was the focus of prior work.
- What evidence would resolve it: Implementing AMAES with a Swin-Transformer encoder and testing it on the same downstream tasks would enable a direct architectural comparison.

## Limitations

- The exact composition and preprocessing details of BRAINS-45K beyond the stated sources remain unclear, limiting reproducibility and understanding of the dataset's heterogeneity.
- The paper doesn't report comprehensive ablations on the importance of individual augmentation types in the reversal strategy, leaving uncertainty about which components drive performance improvements.
- The framework hasn't been tested on datasets with substantially different characteristics than BRAINS-45K, limiting understanding of the learned representations' robustness to domain shifts.

## Confidence

- **High Confidence**: The core mechanism that pretraining on large-scale, heterogeneous data improves downstream segmentation performance in low-resource settings.
- **Medium Confidence**: The specific claim that intensity-based augmentation reversal is superior to standard MIM approaches.
- **Medium Confidence**: The assertion that the lightweight decoder design significantly improves both pretraining efficiency and downstream performance.

## Next Checks

1. Conduct controlled ablation experiments to isolate the contribution of intensity-based augmentation reversal by comparing against standard MIM with only spatial augmentations.

2. Perform quantitative analysis of BRAINS-45K's composition across different modalities, scanner types, and demographic groups to verify the claimed heterogeneity.

3. Evaluate AMAES pretraining on datasets with substantially different characteristics than BRAINS-45K to test the robustness of the learned representations to domain shifts.