---
ver: rpa2
title: Taxonomy Tree Generation from Citation Graph
arxiv_id: '2410.03761'
source_url: https://arxiv.org/abs/2410.03761
tags:
- review
- literature
- generation
- papers
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HiReview, a novel framework for automatic literature
  review generation that combines hierarchical taxonomy generation with retrieval-augmented
  large language models. The approach first retrieves relevant papers from citation
  networks using a graph context-aware method that considers both textual similarity
  and citation relationships, then generates a hierarchical taxonomy tree by clustering
  papers and creating central topics at each level.
---

# Taxonomy Tree Generation from Citation Graph

## Quick Facts
- arXiv ID: 2410.03761
- Source URL: https://arxiv.org/abs/2410.03761
- Reference count: 40
- Key outcome: HiReview achieves LLMScore of 0.9163, BertScore of 0.9484, and high scores in coverage (0.9428), structure (0.9358), and relevance (0.8449) on literature review generation task

## Executive Summary
This paper introduces HiReview, a framework for automatic literature review generation that leverages citation network structures to create hierarchical taxonomies and guide content generation. The approach combines graph context-aware retrieval, hierarchical clustering, and retrieval-augmented LLMs to produce comprehensive literature reviews organized by topic. Experiments on 518 review papers demonstrate superior performance compared to state-of-the-art methods, with strong metrics across coverage, structure, and relevance dimensions.

## Method Summary
HiReview operates through a two-stage taxonomy-then-generation framework. First, it retrieves relevant papers from citation networks using a graph context-aware strategy that enhances BM25 relevance scores with neighbor information. Second, it clusters papers hierarchically based on both textual content and citation structure, then generates a taxonomy tree by creating central topics at each level. Finally, it uses GPT-4o to generate content for each topic, guided by the taxonomy structure. The framework employs GAT-based GNNs for clustering and fine-tunes LLaMA-2-7b for taxonomy generation through LoRA adaptation.

## Key Results
- Achieves LLMScore of 0.9163 and BertScore of 0.9484 on literature review generation
- Scores high on coverage (0.9428), structure (0.9358), and relevance (0.8449)
- Demonstrates superior stability and consistency compared to pure LLM and naive RAG approaches
- Outperforms state-of-the-art methods in comprehensive evaluation

## Why This Works (Mechanism)

### Mechanism 1: Graph Context-Aware Retrieval
- Claim: Aggregating neighbor relevance scores improves retrieval accuracy in citation networks
- Mechanism: For each paper, compute relevance score using BM25, then enhance it by adding weighted relevance scores of neighboring papers
- Core assumption: Papers in citation networks that are topically related tend to cite each other, so neighbor information provides valuable context
- Evidence anchors:
  - [abstract]: "we propose a graph retrieval strategy to address Challenge 1, which aggregates neighbor relevance scores during retrieval"
  - [section]: "Rather than relying solely on the individual attributes of each paper, we enhance relevance scoring by incorporating information from the paper's neighbors"
  - [corpus]: Weak evidence - paper doesn't provide quantitative retrieval performance data for this mechanism specifically
- Break condition: If citation networks don't reflect topical similarity, or if papers have many irrelevant citations, neighbor aggregation could introduce noise

### Mechanism 2: Hierarchical Graph Clustering with Density-Based Boundaries
- Claim: Using node density to determine cluster boundaries creates more semantically meaningful clusters than simple similarity thresholds
- Mechanism: Calculate node density as similarity-weighted proportion of same-cluster nodes in neighborhood, then use density comparisons to determine cluster membership
- Core assumption: High-density nodes are more likely to be cluster cores, while low-density nodes are in ambiguous boundary regions
- Evidence anchors:
  - [abstract]: "we propose a hierarchical citation graph clustering method that recursively groups related papers based on both textual content and citation structure"
  - [section]: "This density assesses how densely connected each node is within its local neighborhood. High-density nodes are more likely to be in the core of a cluster"
  - [corpus]: Weak evidence - no quantitative clustering evaluation results provided in abstract
- Break condition: In citation networks with complex overlapping relationships or non-uniform density distributions, this approach might create suboptimal clusters

### Mechanism 3: Bottom-Up Taxonomy Generation with Graph-Enhanced LLM Prompts
- Claim: Generating central topics bottom-up using graph embeddings and text contexts creates coherent hierarchical taxonomies
- Mechanism: Extract subgraph embeddings for each cluster, combine with text embeddings through MLP, generate topics iteratively from leaves to root
- Core assumption: The relationships between topics are hierarchical and can be captured by propagating information from specific (leaf) to general (root) levels
- Evidence anchors:
  - [abstract]: "we develop a novel taxonomy node verbalization strategy that iteratively generates central concepts for each cluster, leveraging a pre-trained large language model (LLM)"
  - [section]: "For the node u ∈ Vl+1 that corresponds to a cluster ci l, the topic that node u represents is generated as... leveraging both graph-based and text-based prompts"
  - [corpus]: Weak evidence - no specific examples of generated taxonomies or quantitative evaluation of taxonomy quality
- Break condition: If the hierarchical relationships between topics are not naturally tree-like, forcing a hierarchical structure might create artificial or incorrect relationships

## Foundational Learning

### Graph Neural Networks
- Why needed here: GNNs are essential for capturing the structural relationships in citation networks and aggregating information from neighboring nodes
- Quick check question: What is the key difference between how GNNs and traditional neural networks handle node relationships in graphs?

### Hierarchical Clustering
- Why needed here: Hierarchical clustering is necessary to create the multi-level taxonomy structure that organizes literature review topics from general to specific
- Quick check question: What is the fundamental difference between hard clustering and soft clustering, and why does this work use both at different levels?

### Retrieval-Augmented Generation
- Why needed here: RAG techniques help ground LLM-generated content in actual literature rather than relying solely on the model's parametric knowledge
- Quick check question: How does retrieval-augmented generation help mitigate the hallucination problem in LLM outputs?

## Architecture Onboarding

- Component map: Graph Retrieval Module -> Hierarchical Clustering Module -> Taxonomy Generation Module -> Content Generation Module
- Critical path: Retrieval → Clustering → Taxonomy Generation → Content Generation
- Design tradeoffs:
  - Using BM25 instead of dense retrieval for better performance with neighbor information
  - Pre-training clustering module separately to simplify joint optimization
  - Using 2-hop citation networks for balanced scale vs. relevance
  - Generating taxonomies bottom-up for better coherence
- Failure signatures:
  - Poor retrieval → irrelevant papers in clusters → noisy taxonomy → irrelevant content
  - Clustering failures → incorrect topic groupings → incoherent taxonomy structure
  - Taxonomy generation failures → misaligned hierarchical relationships → confused content organization
  - Content generation failures → factual errors or poor coverage despite good taxonomy
- First 3 experiments:
  1. Test graph context-aware retrieval vs. standard BM25 on a small citation network with known ground truth
  2. Evaluate hierarchical clustering performance on a synthetic citation network with clear hierarchical structure
  3. Compare taxonomy-then-generation vs. outline-then-generation approaches on a simple literature review task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HiReview scale when applied to citation networks significantly larger than the 2-hop networks used in the experiments?
- Basis in paper: [inferred] The paper mentions that graph-aware retrieval significantly alleviates the issue of retrieving a large number of papers, and that HiReview maintains stable retrieval accuracy even when applied to large citation networks. However, the experiments only tested 2-hop citation networks.
- Why unresolved: The paper does not provide experimental results or analysis of HiReview's performance on citation networks larger than 2-hop.
- What evidence would resolve it: Conducting experiments with citation networks of increasing size (e.g., 3-hop, 4-hop, or even larger) and evaluating HiReview's performance in terms of retrieval accuracy, clustering quality, and content generation quality.

### Open Question 2
- Question: How does the choice of GNN architecture impact the performance of HiReview, and are there more suitable GNN architectures for this task?
- Basis in paper: [explicit] The paper mentions that GAT achieved the highest performance among the tested GNN architectures (GAT, GCN, and Graph Transformer). However, it does not explore other GNN architectures or provide a detailed analysis of the impact of GNN choice on HiReview's performance.
- Why unresolved: The paper only provides a comparison of a few GNN architectures and does not explore the broader landscape of GNN options or provide insights into the specific requirements of HiReview that make GAT the most suitable choice.
- What evidence would resolve it: Conducting experiments with a wider range of GNN architectures, including newer or more specialized GNNs for citation networks or text data, and evaluating their impact on HiReview's performance in terms of clustering quality, taxonomy generation, and content generation quality.

### Open Question 3
- Question: How does the performance of HiReview compare to human-written literature reviews, and what are the key areas where HiReview can be further improved to match human-level quality?
- Basis in paper: [inferred] The paper evaluates HiReview's performance using LLMScore, BertScore, and other metrics, comparing it to human-written literature reviews. However, it does not provide a detailed qualitative analysis of the differences between HiReview-generated reviews and human-written reviews.
- Why unresolved: The paper focuses on quantitative evaluation metrics but does not provide insights into the specific strengths and weaknesses of HiReview-generated reviews compared to human-written reviews, or identify areas where HiReview can be further improved to match human-level quality.
- What evidence would resolve it: Conducting a qualitative analysis of HiReview-generated reviews and human-written reviews, identifying key differences in terms of content coverage, structure, relevance, coherence, and writing style. Based on this analysis, propose specific improvements to HiReview, such as incorporating more advanced natural language generation techniques, fine-tuning the model on a larger corpus of human-written literature reviews, or developing more sophisticated methods for evaluating the quality of generated reviews.

## Limitations

- Dataset Specificity: The evaluation relies on 518 review papers with hierarchical taxonomy annotations that are not publicly available
- Quantitative Validation Gaps: Lacks quantitative ablation studies for individual components and specific retrieval performance metrics
- Computational Complexity: Multiple complex components likely require substantial computational resources, but resource requirements are not discussed

## Confidence

**High Confidence** (supported by strong evidence and clear mechanisms):
- The overall framework architecture is well-defined and logically structured
- The core idea of using citation network structure to enhance literature review generation is sound
- The two-stage approach (taxonomy-then-generation) has theoretical merit

**Medium Confidence** (supported by reasonable evidence but with some gaps):
- The graph context-aware retrieval mechanism's effectiveness (no specific quantitative validation provided)
- The hierarchical clustering approach's ability to create meaningful topic boundaries (no clustering quality metrics reported)
- The bottom-up taxonomy generation strategy's coherence (no qualitative examples of generated taxonomies)

**Low Confidence** (weak evidence or significant implementation unknowns):
- The actual quality of generated literature reviews (no expert evaluation or human study results)
- The specific implementation details needed for reproduction (unknown hyperparameters, exact architectures)

## Next Checks

1. **Ablation Study on Retrieval Performance**: Implement a controlled experiment comparing graph context-aware retrieval (BM25 + neighbor aggregation) against standard BM25 and dense retrieval methods on a small citation network with ground truth relevance judgments. Measure precision@k, recall@k, and mean average precision to quantify the retrieval improvement.

2. **Clustering Quality Evaluation**: Apply the hierarchical clustering algorithm to a synthetic citation network with known hierarchical structure (e.g., papers organized in clear topic-subtopic relationships). Compute standard clustering metrics (purity, normalized mutual information, adjusted Rand index) at each hierarchy level to validate that the density-based clustering creates semantically meaningful clusters.

3. **Taxonomy Coherence Analysis**: Generate taxonomies using the proposed bottom-up approach on a simple literature review task with 50-100 papers. Have domain experts evaluate the generated taxonomies for coherence, hierarchical correctness, and coverage. Compare against alternative approaches (top-down generation, outline-then-generation) using pairwise comparisons or Likert-scale ratings.