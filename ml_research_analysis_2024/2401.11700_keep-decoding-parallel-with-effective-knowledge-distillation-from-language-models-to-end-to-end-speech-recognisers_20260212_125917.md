---
ver: rpa2
title: Keep Decoding Parallel with Effective Knowledge Distillation from Language
  Models to End-to-end Speech Recognisers
arxiv_id: '2401.11700'
source_url: https://arxiv.org/abs/2401.11700
tags:
- bert
- decoding
- intermediate
- speech
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a knowledge distillation method that uses intermediate
  layer losses from an attention decoder to improve the language modeling capability
  of non-autoregressive CTC speech recognition models. The method distills BERT's
  token probabilities into intermediate encoder layers via an auxiliary attention
  decoder, in addition to the final layer.
---

# Keep Decoding Parallel with Effective Knowledge Distillation from Language Models to End-to-end Speech Recognisers

## Quick Facts
- arXiv ID: 2401.11700
- Source URL: https://arxiv.org/abs/2401.11700
- Authors: Michael Hentschel; Yuta Nishikawa; Tatsuya Komatsu; Yusuke Fujita
- Reference count: 0
- Primary result: 7% relative WER reduction compared to conventional KD on LibriSpeech

## Executive Summary
This paper addresses the challenge of improving language modeling capabilities in non-autoregressive CTC speech recognition models while maintaining their parallel decoding efficiency. The authors propose an intermediate attention decoder knowledge distillation (interAED-KD) method that distills token probabilities from BERT into intermediate encoder layers via an auxiliary attention decoder. The approach aims to bridge the gap between autoregressive models' superior language modeling and non-autoregressive models' computational efficiency. Experiments on LibriSpeech demonstrate that the method achieves WER improvements comparable to beam search decoding with external language models while maintaining a six times lower real-time factor.

## Method Summary
The proposed interAED-KD method uses BERT to provide token probabilities that are distilled into intermediate layers of a non-autoregressive CTC speech recognition model through an auxiliary attention decoder. Unlike conventional knowledge distillation that only targets the final layer, this approach applies distillation losses at multiple intermediate encoder layers. The auxiliary attention decoder is specifically designed to facilitate the transfer of BERT's language modeling knowledge to the encoder representations. During training, the model optimizes both the CTC objective and the auxiliary decoder's reconstruction loss, with the distillation signal coming from BERT's token probabilities rather than the ground truth labels.

## Key Results
- 7% relative WER reduction compared to conventional knowledge distillation on LibriSpeech
- Achieved similar or better WER than beam search decoding with external LM
- Maintained six times lower real-time factor compared to autoregressive decoding
- Demonstrated effectiveness of intermediate layer distillation in non-autoregressive models

## Why This Works (Mechanism)
The method works by leveraging BERT's strong language modeling capabilities to enhance the encoder representations in a non-autoregressive CTC model. By distilling knowledge at intermediate layers rather than just the final layer, the approach ensures that language modeling information is integrated throughout the encoder hierarchy. The auxiliary attention decoder serves as a bridge between BERT's token-level predictions and the encoder's intermediate representations, allowing for more effective knowledge transfer. This multi-layer distillation approach addresses the inherent weakness of CTC models in capturing long-range dependencies and language context, which are critical for accurate speech recognition.

## Foundational Learning

**Connectionist Temporal Classification (CTC)**: A loss function for sequence-to-sequence problems where input and output lengths differ, commonly used in speech recognition to handle variable-length alignments without explicit frame-level supervision.

Why needed: CTC provides the non-autoregressive decoding framework that enables parallel inference, which is essential for the model's efficiency benefits.

Quick check: Verify that the model maintains monotonic alignment between speech frames and output tokens without explicit attention mechanisms.

**Knowledge Distillation**: A training technique where a "teacher" model's predictions are used to guide the training of a "student" model, typically by minimizing the difference between their output distributions.

Why needed: Enables transfer of language modeling capabilities from powerful pre-trained models (BERT) to the speech recognition model without requiring labeled language data.

Quick check: Confirm that the distillation loss is applied at both intermediate and final layers as claimed.

**Attention Mechanisms**: Neural network components that allow models to dynamically weight different parts of the input when making predictions, typically used in sequence-to-sequence models for alignment.

Why needed: The auxiliary attention decoder is crucial for bridging BERT's token predictions with the encoder's intermediate representations during distillation.

Quick check: Verify that the attention decoder operates independently of the main CTC decoding path.

## Architecture Onboarding

Component map: Speech input -> CNN feature extractor -> Transformer encoder -> CTC decoder + Auxiliary attention decoder -> Output

Critical path: The forward pass through the CNN feature extractor and Transformer encoder, followed by both CTC and attention decoder outputs during training. During inference, only the CTC path is used for parallel decoding.

Design tradeoffs: The method trades increased training complexity and potential overfitting risk for improved language modeling without sacrificing inference speed. The auxiliary attention decoder adds parameters and computational overhead during training but is discarded during inference.

Failure signatures: Poor distillation performance when BERT and speech domain vocabularies have low overlap; degraded WER when intermediate layer losses are weighted incorrectly; potential training instability from competing CTC and attention decoder objectives.

First experiments:
1. Verify baseline CTC model performance on LibriSpeech without any distillation
2. Test conventional knowledge distillation (only final layer) for ablation comparison
3. Validate that intermediate layer distillation improves over single-layer distillation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to LibriSpeech dataset, limiting generalization claims to other speech domains
- Dependence on BERT for token probability distillation may introduce computational overhead and domain mismatch
- Method requires careful hyperparameter tuning and may complicate training dynamics

## Confidence

| Claim | Confidence |
|-------|------------|
| 7% relative WER reduction on LibriSpeech | High |
| Six times lower real-time factor vs beam search | High |
| Matching beam search with external LM performance | High |
| Intermediate layer distillation is critical for improvement | Medium |
| Method "effectively" improves language modeling | Low |

## Next Checks

1. Evaluate the method on additional speech recognition datasets with varying acoustic conditions and vocabulary complexity to assess generalization.

2. Conduct ablation studies to quantify the individual contributions of intermediate layer distillation versus final layer distillation.

3. Measure the computational overhead introduced by BERT-based distillation during both training and inference to verify the claimed efficiency benefits.