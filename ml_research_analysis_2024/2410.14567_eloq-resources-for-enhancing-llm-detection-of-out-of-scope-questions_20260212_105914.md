---
ver: rpa2
title: 'ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions'
arxiv_id: '2410.14567'
source_url: https://arxiv.org/abs/2410.14567
tags:
- document
- questions
- question
- out-of-scope
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of large language models (LLMs)
  generating hallucinated answers to out-of-scope questions in retrieval-augmented
  generation (RAG) systems. The authors propose ELOQ, a framework that automatically
  generates diverse out-of-scope questions from post-training cutoff documents using
  a guided hallucination method, followed by human verification.
---

# ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions

## Quick Facts
- **arXiv ID:** 2410.14567
- **Source URL:** https://arxiv.org/abs/2410.14567
- **Reference count:** 40
- **Primary result:** ELOQ achieves 94.91% accuracy in generating out-of-scope questions and enables smaller models to match larger ones in out-of-scope detection accuracy

## Executive Summary
This paper introduces ELOQ, a framework that automatically generates diverse out-of-scope questions from post-training cutoff documents using guided hallucination, followed by human verification. The method addresses the problem of large language models hallucinating answers to unanswerable questions in retrieval-augmented generation systems. ELOQ achieves 94.91% accuracy in generating out-of-scope and in-scope questions, with human-verified datasets showing that LLMs can successfully defuse 87.61-98.23% of out-of-scope questions using prompting techniques. The framework also enables smaller models to match larger ones in out-of-scope detection accuracy when trained on ELOQ data.

## Method Summary
ELOQ uses a guided hallucination-based approach to generate out-of-scope questions from news articles published after LLM knowledge cutoffs. The method involves claim extraction from documents, iterative masking and recovery with hallucination injection, question generation, and human verification. For detection, ELOQ employs binary classification using unused-token embeddings from LLMs. The framework is evaluated on both question generation quality and LLM defusion success rates using various prompting strategies including basic, two-shot, and zero-shot chain-of-thought approaches.

## Key Results
- 94.91% accuracy in generating out-of-scope and in-scope questions from news articles
- LLMs achieve 87.61-98.23% success in defusing out-of-scope questions using prompting techniques
- Smaller models (3B-8B) trained on ELOQ data match larger models (70B) in out-of-scope detection accuracy
- Classifier using unused-token embeddings outperforms direct generation prompts for detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Guided hallucination injection produces out-of-scope questions that are semantically similar to source documents yet unanswerable from them.
- Mechanism: Iterative masking and recovery of claims generates hallucinations; claims fully supported by source are removed, leaving only out-of-scope claims which are then converted to questions.
- Core assumption: LLM can generate semantically similar but false claims when provided masked originals and iteratively recover them with noise injected.
- Evidence anchors: [abstract] "We propose a guided hallucination-based approach ELOQ to automatically generate a diverse set of out-of-scope questions from post-cutoff documents, followed by human verification to ensure quality."

### Mechanism 2
- Claim: Larger LLMs benefit more from zero-shot chain-of-thought prompting for defusion than smaller models.
- Mechanism: Bigger models better utilize internal reasoning capabilities and extensive knowledge to reason through why a question cannot be answered and generate appropriate defusion responses.
- Core assumption: Model size correlates with reasoning depth and knowledge breadth, enabling better defusion via CoT.
- Evidence anchors: [section 4.1] "Larger models (70B) benefit more from Zero-shot-CoT than from Two-shot prompting, achieving accuracy gains of 5.07% to 9.98%."

### Mechanism 3
- Claim: Unused-token classification detects out-of-scope questions better than direct generation prompts.
- Mechanism: Classifier trained on LLM's unused-token representations extracts internal sequence features indicating question scope, outperforming LLM's direct attempts to detect out-of-scope.
- Core assumption: LLM's internal representations contain richer information about scope than its prompt-following behavior reveals.
- Evidence anchors: [section 4.2.2] "Notably, the classifiers consistently outperform the 'Direct Generation' method."

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG) and hallucination
  - Why needed here: ELOQ specifically addresses RAG systems' hallucination when encountering out-of-scope questions, requiring understanding of how RAG works and what hallucination means.
  - Quick check question: What is the difference between a document being retrieved but not containing an answer vs. the document being irrelevant?

- Concept: Semantic similarity vs. semantic relevance
  - Why needed here: Out-of-scope questions must appear semantically similar to source documents to fool LLMs but lack actual answer content, requiring distinction between surface similarity and content relevance.
  - Quick check question: Can a question be semantically similar to a document but not semantically relevant to answering it?

- Concept: Classification vs. generation in LLM outputs
  - Why needed here: ELOQ uses both generation (for questions) and classification (for detection), requiring understanding when to use each approach and their tradeoffs.
  - Quick check question: When would you prefer a classification approach over direct prompting for a detection task?

## Architecture Onboarding

- Component map: Data generation pipeline (claim extraction → hallucination injection → question generation → human verification) → evaluation pipeline (defusion detection → out-of-scope detection) → classifier training module → retrieval evaluation module
- Critical path: Generate ELOQ-Silver → train classifier on Silver → evaluate on ELOQ-Gold → use classifier for out-of-scope detection in production RAG systems
- Design tradeoffs: Manual verification ensures quality but limits scale; automated generation scales but risks lower quality; larger models perform better but cost more; direct prompting is simpler but less accurate than classifier approach
- Failure signatures: Low retrieval relevance scores (indicating questions aren't semantically similar), classifier accuracy below direct prompting baseline, defusion responses still attempting to answer out-of-scope questions, human verification rejecting high percentage of generated questions
- First 3 experiments:
  1. Generate 100 out-of-scope questions from 10 documents and measure retrieval Recall@10 to confirm semantic similarity.
  2. Train classifier on 80% of generated data and evaluate on 20% to establish baseline accuracy.
  3. Compare defusion success rates of GPT-3.5 vs. Llama 3.1 70B on same out-of-scope questions using zero-shot-CoT prompting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of ELOQ-generated out-of-scope questions compare to human-annotated unanswerable questions from existing datasets like SQuAD 2.0 when used for training detection models?
- Basis in paper: [explicit] The paper compares ELOQ to existing datasets in Table 1 but doesn't directly compare their effectiveness for training detection models.
- Why unresolved: The paper focuses on ELOQ's internal validation but doesn't benchmark against human-annotated datasets for training effectiveness.
- What evidence would resolve it: A head-to-head comparison of detection model performance when trained on ELOQ vs. human-annotated datasets like SQuAD 2.0.

### Open Question 2
- Question: What is the optimal number of hallucination injection rounds in the guided hallucination method for balancing question quality and generation efficiency?
- Basis in paper: [inferred] The paper mentions using 3 rounds as an example but doesn't systematically explore the impact of different numbers of rounds.
- Why unresolved: The paper doesn't investigate how varying the number of hallucination rounds affects the quality or efficiency of question generation.
- What evidence would resolve it: Experiments varying the number of hallucination rounds (e.g., 1, 3, 5, 7) and measuring their impact on question quality metrics and generation time.

### Open Question 3
- Question: How does the performance of smaller models trained on ELOQ data compare to larger models when both are evaluated on out-of-scope questions from different domains or knowledge domains?
- Basis in paper: [explicit] The paper shows smaller models can match larger ones on ELOQ data but doesn't test cross-domain generalization.
- Why unresolved: The paper only evaluates on the ELOQ dataset and doesn't test whether smaller models maintain their advantage on out-of-scope questions from different domains.
- What evidence would resolve it: Testing trained models on out-of-scope questions from domains not represented in the ELOQ training data (e.g., medical, legal, technical domains).

## Limitations
- The framework's effectiveness may be constrained by the specific domain (news articles) and time period (2024) used for generation, potentially limiting generalizability
- Performance claims rely heavily on human verification quality for the ELOQ-Gold dataset, which limits scalability and introduces potential subjectivity
- The evaluation focuses primarily on binary classification accuracy without extensive analysis of false positive/negative rates in real-world deployment scenarios

## Confidence
- **High Confidence:** The core mechanism of using guided hallucination for generating out-of-scope questions and the general superiority of classifier approaches over direct prompting for detection
- **Medium Confidence:** The specific performance metrics (94.91% generation accuracy, 98.23% defusion success) and the relative performance differences between model sizes for CoT prompting
- **Medium Confidence:** The claim that unused-token embeddings provide superior detection capabilities, as this relies on specific implementation details

## Next Checks
1. **Domain Generalization Test:** Apply ELOQ to generate out-of-scope questions from a different domain (e.g., scientific literature or legal documents) and evaluate classifier performance transfer to ensure the approach isn't overly specialized to news articles.

2. **Temporal Robustness Validation:** Generate out-of-scope questions from documents published before and after the training period (2024) to test whether the classifier maintains performance on temporally diverse data, addressing potential temporal bias in the generated dataset.

3. **False Positive/Negative Analysis:** Conduct a detailed error analysis on the classifier's predictions, categorizing false positives and false negatives to understand whether the approach has systematic biases (e.g., certain types of semantic similarity patterns being consistently misclassified) that could impact real-world deployment reliability.