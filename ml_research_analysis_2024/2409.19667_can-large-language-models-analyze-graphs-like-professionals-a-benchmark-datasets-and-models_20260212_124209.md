---
ver: rpa2
title: Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets
  and Models
arxiv_id: '2409.19667'
source_url: https://arxiv.org/abs/2409.19667
tags:
- code
- graph
- llms
- accuracy
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ProGraph, a benchmark for evaluating large
  language models (LLMs) on graph analysis using external APIs, addressing the limitation
  of existing benchmarks that only handle small graphs. ProGraph contains 512 manually
  crafted problems across three categories: basic graph theory, graph statistical
  learning, and graph embedding, with problems scalable to millions of nodes.'
---

# Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models

## Quick Facts
- arXiv ID: 2409.19667
- Source URL: https://arxiv.org/abs/2409.19667
- Reference count: 40
- Current LLMs achieve only 36% accuracy on ProGraph benchmark for graph analysis tasks

## Executive Summary
This paper addresses the gap in evaluating large language models (LLMs) on professional-level graph analysis tasks by introducing ProGraph, a comprehensive benchmark with 512 manually crafted problems. Unlike existing benchmarks that focus on small graphs, ProGraph enables evaluation on graphs scalable to millions of nodes through API-based code generation approaches. The authors also develop LLM4Graph, a collection of datasets and methods to improve LLM performance, demonstrating that retrieval-augmented generation and fine-tuning can significantly enhance model capabilities.

## Method Summary
The researchers created ProGraph benchmark containing 512 hand-crafted problems across three categories: basic graph theory, graph statistical learning, and graph embedding. To improve LLM performance, they constructed LLM4Graph datasets including crawled API documentation and auto-generated code datasets for six Python graph libraries. They enhanced closed-source LLMs using retrieval-augmented generation (RAG) with document datasets, and fine-tuned open-source models (Llama3, Deepseek Coder) on code datasets. The approach shifts graph reasoning from direct LLM processing to generating executable Python code using established graph libraries.

## Key Results
- Current LLMs achieve only 36% accuracy on ProGraph benchmark
- RAG-enhanced closed-source models show 11-32% absolute accuracy improvements
- Fine-tuned open-source models demonstrate enhanced graph analysis capabilities
- ProGraph problems can scale to graphs with millions of nodes while maintaining evaluation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can analyze large graphs by generating API-based code rather than direct reasoning over graph topology.
- Mechanism: Instead of requiring LLMs to process entire adjacency lists directly, the system prompts them to write Python code using established graph libraries (NetworkX, igraph, etc.). This shifts the reasoning burden from the LLM to optimized library functions, enabling handling of graphs with millions of nodes.
- Core assumption: The LLM has sufficient code generation capability and API knowledge to produce correct and executable code.
- Evidence anchors:
  - [abstract] "human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales."
  - [section 1] "a question naturally arises: can LLMs analyze graphs like professionals?"
- Break condition: LLM fails to generate syntactically correct or logically appropriate code for the API calls.

### Mechanism 2
- Claim: Retrieval-Augmented Generation (RAG) improves LLM performance by providing relevant API documentation.
- Mechanism: Before answering questions, the system retrieves relevant API information from the LLM4Graph document dataset. This context helps the LLM select appropriate functions and parameters, improving accuracy.
- Core assumption: The retrieved documentation is relevant and concise enough to enhance understanding without overwhelming the LLM.
- Evidence anchors:
  - [section 4.2] "By augmenting closed-source LLMs with document retrieval... we show 11-32% absolute improvements in their accuracies."
  - [section 5.3] "By incorporating a RAG mechanism with the LLM4Graph, we demonstrate that it is possible to substantially improve the performance of closed-source LLMs."
- Break condition: Retrieved documents are too lengthy or irrelevant, causing confusion rather than clarity.

### Mechanism 3
- Claim: Fine-tuning open-source LLMs on the code dataset improves their graph analysis capabilities.
- Mechanism: The system fine-tunes open-source models (Llama3, Deepseek Coder) on auto-generated QA pairs that include Python code for graph operations. This specialized training improves their ability to generate correct code for graph analysis tasks.
- Core assumption: The auto-generated code dataset contains sufficient variety and quality to improve model performance.
- Evidence anchors:
  - [section 5.4] "We use the above datasets to improve various LLMs in handling graph analysis tasks."
  - [section 4.2] "We use the two code datasets... to fine-tune Llama-3-8B-Instruct and Deepseek-Coder-7B-Instruct."
- Break condition: Fine-tuning data is too homogeneous or contains systematic errors that propagate to the model.

## Foundational Learning

- Concept: Graph theory fundamentals (nodes, edges, connectivity, centrality, clustering)
  - Why needed here: Understanding these concepts is essential for both formulating questions and interpreting API documentation.
  - Quick check question: What is the difference between node connectivity and edge connectivity in a graph?

- Concept: Python programming and API usage
  - Why needed here: The entire system relies on LLMs generating Python code that calls specific graph library functions correctly.
  - Quick check question: How would you import and use NetworkX to create a simple graph and find the shortest path between two nodes?

- Concept: Retrieval-Augmented Generation (RAG) principles
  - Why needed here: RAG is a core mechanism for enhancing LLM performance by providing relevant context from external documents.
  - Quick check question: What are the key components of a RAG system, and how does it differ from standard prompt engineering?

## Architecture Onboarding

- Component map:
  ProGraph Benchmark -> LLM4Graph Datasets -> LLM Models -> RAG System/Fine-tuning Pipeline -> Code Execution and Evaluation

- Critical path: 
  1. Generate or select a graph analysis problem
  2. Retrieve relevant API documentation (RAG)
  3. Generate code solution using LLM
  4. Execute code and evaluate results
  5. Fine-tune models using code dataset for improved performance

- Design tradeoffs:
  - Hand-crafted vs. auto-generated problems: ProGraph prioritizes quality and realism over quantity
  - Document vs. code datasets: Different approaches for closed-source (RAG) vs. open-source (fine-tuning) models
  - API complexity: Balancing between simple single-API problems and complex multi-API problems

- Failure signatures:
  - Low pass rate: LLM fails to generate executable code
  - Low accuracy: Generated code runs but produces incorrect results
  - RAG failure: Retrieved documents are irrelevant or too lengthy
  - Fine-tuning divergence: Model performance degrades after training

- First 3 experiments:
  1. Test LLM code generation on simple single-API problems without RAG
  2. Add RAG to closed-source models and measure accuracy improvement
  3. Fine-tune open-source models on code dataset and compare with RAG-enhanced closed-source models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM performance trends change as graph size scales beyond the 10^6 node limit of ProGraph, and what architectural modifications might enable effective processing of truly large-scale graphs?
- Basis in paper: [explicit] The paper states ProGraph can scale to 10^6 nodes but doesn't explore beyond this limit
- Why unresolved: The benchmark was capped at 10^6 nodes for practical evaluation purposes, leaving scalability beyond this unexplored
- What evidence would resolve it: Empirical testing of LLMs on graphs with 10^7-10^8 nodes using streaming or distributed processing approaches

### Open Question 2
- Question: How do different graph embedding techniques compare in terms of LLM interpretability when used as features for downstream tasks?
- Basis in paper: [inferred] The paper evaluates graph embedding tasks but doesn't analyze how different embedding methods affect LLM reasoning
- Why unresolved: The benchmark treats graph embedding as a black box task without analyzing embedding quality or interpretability
- What evidence would resolve it: Comparative analysis of LLM performance across multiple embedding algorithms (node2vec, graph attention, etc.) with visualization of learned representations

### Open Question 3
- Question: What is the relationship between the number of required APIs in a problem and the effectiveness of retrieval-augmented generation techniques?
- Basis in paper: [explicit] The paper notes that closed-source models perform significantly worse on multi-API problems (hard level) compared to single-API problems
- Why unresolved: The analysis shows performance degradation with multiple APIs but doesn't quantify the relationship or identify optimal RAG strategies for complex problems
- What evidence would resolve it: Systematic experiments varying the number of required APIs and measuring RAG effectiveness at each level, potentially revealing thresholds where RAG becomes counterproductive

## Limitations
- The benchmark contains only 512 problems despite claiming scalability to millions of nodes
- Automated evaluation using GPT-4o introduces potential circularity in the evaluation framework
- Focus on Python graph libraries may limit generalizability to other programming environments
- Limited analysis of how different graph embedding techniques affect LLM reasoning

## Confidence
**High Confidence**:
- Current LLMs achieve low accuracy (36%) on graph analysis tasks
- RAG and fine-tuning methods improve LLM performance on graph tasks
- API-based code generation is a viable approach for graph analysis

**Medium Confidence**:
- The 11-32% accuracy improvements are directly attributable to the proposed methods
- The ProGraph benchmark adequately represents professional graph analysis tasks
- The LLM4Graph datasets are sufficiently comprehensive for model improvement

**Low Confidence**:
- The scalability of the approach to truly massive graphs (millions of nodes)
- The long-term effectiveness of the fine-tuning approach without domain adaptation
- The robustness of the evaluation framework against model-specific biases

## Next Checks
1. Reproduce the core results: Implement the baseline evaluation and RAG-enhanced inference using the publicly available datasets to verify the reported accuracy improvements.

2. Stress test the fine-tuning pipeline: Generate additional code datasets with varying complexity and test whether the fine-tuned models maintain performance on increasingly complex multi-API problems.

3. Evaluate on alternative graph libraries: Test the best-performing models on a different set of graph libraries (e.g., graph-tool, PyGraphistry) to assess generalizability beyond the training domain.