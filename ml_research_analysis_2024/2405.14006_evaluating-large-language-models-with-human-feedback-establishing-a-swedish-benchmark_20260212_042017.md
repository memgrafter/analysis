---
ver: rpa2
title: 'Evaluating Large Language Models with Human Feedback: Establishing a Swedish
  Benchmark'
arxiv_id: '2405.14006'
source_url: https://arxiv.org/abs/2405.14006
tags:
- language
- benchmark
- swedish
- human
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the lack of comprehensive evaluation of large
  language models (LLMs) for Swedish, a low-resource language. It introduces a human
  benchmark based on a modified ChatbotArena platform, where users rank model responses
  via forced choice comparison.
---

# Evaluating Large Language Models with Human Feedback: Establishing a Swedish Benchmark

## Quick Facts
- **arXiv ID:** 2405.14006
- **Source URL:** https://arxiv.org/abs/2405.14006
- **Reference count:** 6
- **Primary result:** Introduced a human benchmark for Swedish LLMs using modified ChatbotArena platform with ELO ratings

## Executive Summary
This study addresses the lack of comprehensive evaluation of large language models for Swedish, a low-resource language. The research introduces a human benchmark based on a modified ChatbotArena platform, where users rank model responses via forced choice comparison. Twelve models were evaluated, including GPT-4, GPT-3.5, Claude variants, Llama Instruct models, and Swedish-tuned models like Dolphin-2.9-llama3b-8b-flashback and BeagleCatMunin. The benchmark combines human feedback with objective metrics from ScandEval to assess model performance, with the aim of creating a leaderboard and improving Swedish language model development through broad public participation in evaluations.

## Method Summary
The study implements a human evaluation benchmark using a modified ChatbotArena platform where users compare responses from two randomly selected models and choose their preferred response. The system uses ELO ratings to aggregate pairwise comparisons into an overall model ranking. Twelve models were included in the evaluation, with ten accessed via API and two hosted locally on RTX-4090 GPU. The benchmark combines this subjective human feedback with objective metrics from ScandEval to provide a comprehensive evaluation framework for Swedish language models.

## Key Results
- Twelve models evaluated including GPT-4, Claude variants, Llama Instruct models, and Swedish-tuned models
- Platform uses forced binary ranking via ELO system to create reliable preference signals between models
- Benchmark combines human feedback with objective metrics from ScandEval for comprehensive evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forced binary ranking via ELO system creates reliable preference signals between models
- Mechanism: By forcing users to choose between two models on the same prompt, subjective biases are reduced compared to rating individual models in isolation. The ELO rating system then aggregates these pairwise comparisons into a stable model ranking.
- Core assumption: Users can make consistent binary choices when comparing two responses side-by-side, and these choices reflect meaningful quality differences
- Evidence anchors: [abstract] "uses forced choice ranking" and "create a benchmark of model performance using ELO ratings" [section] "Our benchmark draws heavily on the LMSYS benchmark and we have adopted the source code to work with our Swedish model" [corpus] Weak evidence - no direct citations about ELO effectiveness in LLM evaluation found in neighboring papers
- Break condition: If user preferences become inconsistent (choosing A over B, B over C, but C over A), the ELO system would produce unstable rankings or require additional normalization

### Mechanism 2
- Claim: Combining objective metrics with subjective human feedback provides a more complete evaluation than either alone
- Mechanism: Automatic tools like ScandEval provide consistent, scalable measurement across multiple benchmarks, while human feedback captures nuanced preferences that automated metrics miss. Together they create a multi-dimensional evaluation framework.
- Core assumption: Objective metrics and human preferences measure complementary aspects of model quality
- Evidence anchors: [section] "We believe that combining objective evaluation with a tool such as Scandeval with subjective evaluation with a tool such as Swedish Chatbot Arena is the most efficient way to get an accurate assessment of model performance" [section] "Human feedback plays a crucial role in the democratic evaluation of large language models" and "Automatic evaluation tools... play an indispensable role" [corpus] Moderate evidence - neighboring papers discuss complementary evaluation approaches but don't specifically validate this combination
- Break condition: If human preferences systematically diverge from objective metrics in predictable ways, the combined approach could introduce conflicting signals rather than complementary ones

### Mechanism 3
- Claim: Using Swedish-specific data in fine-tuning creates measurable performance improvements for Swedish language tasks
- Mechanism: Models fine-tuned on Swedish data (like Dolphin-2.9-llama3b-8b-flashback and BeagleCatMunin) learn language-specific patterns, vocabulary, and cultural context that generic models miss, resulting in better Swedish text generation and comprehension.
- Core assumption: Language-specific fine-tuning data contains patterns that generalize to Swedish language tasks
- Evidence anchors: [section] "The model is a merge of timpal0l/Llama-3-8B-flashback-v1 and cognitivecomputations/dolphin-2.9-llama3-8b which in turn are fine-tunes on Swedish language data (Flashback)" [section] "BeagleCatMunin is similar to Dolphin-2.9-llama3b-8b-flashback by being a merge of fine-tuned models... trained on flashback data" [corpus] Weak evidence - no direct citations about Swedish-specific fine-tuning effectiveness found in neighboring papers
- Break condition: If Swedish-specific fine-tuning overfits to the training data distribution or introduces bias, performance could degrade on out-of-distribution Swedish text

## Foundational Learning

- **Concept:** ELO rating system for aggregating pairwise comparisons
  - Why needed here: The benchmark relies on users choosing between two models, and ELO provides a mathematically sound way to convert these pairwise preferences into an overall ranking
  - Quick check question: If Model A beats Model B 60% of the time and Model B beats Model C 70% of the time, can ELO determine where Model A stands relative to Model C without them being directly compared?

- **Concept:** Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding how models were trained helps interpret their performance - models with RLHF have human preferences baked into their optimization, while others don't
  - Quick check question: How does RLHF differ from standard supervised fine-tuning, and what types of model behaviors does it specifically target?

- **Concept:** Model merging techniques
  - Why needed here: Two of the evaluated models (Dolphin-2.9 and BeagleCatMunin) were created through model merging, which is a non-standard training approach that affects their capabilities
  - Quick check question: What are the advantages and limitations of model merging compared to training a single model from scratch?

## Architecture Onboarding

- **Component map:** User visits site → prompt generated → two models respond → user selects preferred response → ELO score updated → data stored for leaderboard creation
- **Critical path:** User visits site → prompt generated → two models respond → user selects preferred response → ELO score updated → data stored for leaderboard creation
- **Design tradeoffs:** Local GPU hosting provides control but limits model selection; API hosting enables more models but introduces dependency on external services and potential latency; binary ranking simplifies user decisions but may miss nuanced quality differences
- **Failure signatures:** API timeouts causing incomplete evaluations; GPU memory exhaustion preventing model loading; inconsistent ELO rankings suggesting user preference noise; data collection lag preventing timely leaderboard updates
- **First 3 experiments:**
  1. Deploy with a minimal set of 2-3 models and monitor API response times and GPU utilization to identify bottlenecks
  2. Conduct internal testing with team members to validate prompt generation and response display functionality before public launch
  3. Run a small-scale user test (10-20 participants) to verify ELO system stability and identify any UI/UX issues in the ranking interface

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between human feedback and automatic evaluation metrics for assessing LLM performance in low-resource languages?
- Basis in paper: [explicit] The paper states "We believe that combining objective evaluation with a tool such as Scandeval with subjective evaluation with a tool such as Swedish Chatbot Arena is the most efficient way to get an accurate assessment of model performance."
- Why unresolved: The paper proposes combining both methods but does not empirically determine the optimal weighting or integration strategy between human feedback and automatic metrics.
- What evidence would resolve it: Empirical studies comparing different weighting schemes between human feedback and automatic metrics across multiple low-resource languages, measuring correlation with real-world task performance.

### Open Question 2
- Question: How does the performance of fine-tuned and merged models compare to larger proprietary models when evaluated by native speakers in practical applications?
- Basis in paper: [explicit] The paper includes both smaller open models (Dolphin-2.9-llama3b-8b-flashback, BeagleCatMunin) and larger proprietary models (GPT-4, Claude) but only mentions their rankings will be presented after sufficient data collection.
- Why unresolved: The paper lists the models and their initial rankings but does not provide final comparative results or analysis of practical utility differences.
- What evidence would resolve it: Detailed comparative analysis of task completion rates, user satisfaction scores, and error patterns across different model sizes and training approaches in real-world Swedish language tasks.

### Open Question 3
- Question: What specific linguistic phenomena or error patterns distinguish high-performing from low-performing models for Swedish language tasks?
- Basis in paper: [inferred] The paper evaluates models for Swedish but does not analyze specific linguistic strengths or weaknesses that differentiate model performance.
- Why unresolved: While the benchmark will rank models, it does not include systematic linguistic error analysis or identification of which language features (morphology, syntax, semantics, pragmatics) most strongly correlate with model quality.
- What evidence would resolve it: Linguistic error analysis categorizing model mistakes by linguistic feature type, identifying which features most strongly predict overall model quality in Swedish.

## Limitations

- The effectiveness of ELO-based pairwise ranking depends heavily on consistency of human preferences, which lacks direct validation
- Integration methodology between human feedback and ScandEval metrics is not specified, making it unclear how the two approaches are combined
- Evaluation is limited to 12 models, with only 2 specifically tuned for Swedish, potentially creating a narrow assessment scope

## Confidence

**High confidence:** The technical implementation of the ChatbotArena platform and ELO rating system is straightforward and well-established. The use of forced-choice comparisons between models is a standard approach in preference learning.

**Medium confidence:** The claim that combining human feedback with objective metrics provides a more complete evaluation is reasonable but lacks empirical validation specific to Swedish LLMs. The effectiveness of this combination approach needs more evidence.

**Low confidence:** The assertion that Swedish-specific fine-tuning creates measurable performance improvements is supported only by model descriptions rather than systematic comparison with non-Swedish-tuned models. The impact of Swedish fine-tuning on actual performance differences is not quantitatively demonstrated.

## Next Checks

1. **Inter-rater reliability analysis:** Compute Cohen's kappa or similar agreement metrics across multiple evaluators to quantify the consistency of human preferences. If agreement is low, the ELO rankings may not reflect true model quality differences.

2. **Statistical significance testing:** Perform bootstrap analysis or permutation tests on the ELO rankings to determine whether observed performance differences between models are statistically significant or could arise from random noise in human preferences.

3. **Correlation analysis between evaluation methods:** Calculate Pearson/Spearman correlation coefficients between human preference rankings and ScandEval objective metrics to empirically validate whether the two evaluation approaches measure complementary or redundant aspects of model performance.