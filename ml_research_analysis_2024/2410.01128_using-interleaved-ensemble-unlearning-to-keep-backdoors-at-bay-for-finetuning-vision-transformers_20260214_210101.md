---
ver: rpa2
title: Using Interleaved Ensemble Unlearning to Keep Backdoors at Bay for Finetuning
  Vision Transformers
arxiv_id: '2410.01128'
source_url: https://arxiv.org/abs/2410.01128
tags:
- data
- poisoned
- backdoor
- attacks
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a backdoor defense method for Vision Transformers
  (ViTs) during fine-tuning on potentially poisoned datasets. The core idea is to
  use an ensemble of two ViTs: a shallow "poisoned" module to identify likely backdoored
  samples, and a deeper "robust" module that learns only from clean data and unlearns
  potentially poisoned data asynchronously.'
---

# Using Interleaved Ensemble Unlearning to Keep Backdoors at Bay for Finetuning Vision Transformers

## Quick Facts
- arXiv ID: 2410.01128
- Source URL: https://arxiv.org/abs/2410.01128
- Authors: Zeyu Michael Li
- Reference count: 40
- Primary result: Interleaved ensemble unlearning significantly improves backdoor defense for Vision Transformers during fine-tuning

## Executive Summary
This paper addresses the critical challenge of backdoor defense in Vision Transformers during fine-tuning on potentially poisoned datasets. The proposed method, interleaved ensemble unlearning, uses two ViT modules working in tandem - a shallow "poisoned" module that identifies likely backdoored samples, and a deeper "robust" module that learns only from clean data. The approach asynchronously unlearns backdoor patterns while maintaining high clean accuracy. Experiments demonstrate significant improvements over state-of-the-art defenses across multiple attacks and datasets, though performance varies with dataset complexity and attack strength.

## Method Summary
The method employs an ensemble of two Vision Transformer modules with complementary roles. The shallow "poisoned" module is pre-trained to be highly confident on poisoned data while showing lower confidence on clean samples. During fine-tuning, the deeper "robust" module learns exclusively from data that the poisoned module is not confident about. Samples with high confidence from the poisoned module are added to an unlearn set, which is then used to asynchronously unlearn backdoor patterns from the robust module using a dynamic unlearning rate. This interleaved approach allows the robust module to maintain clean accuracy while systematically removing backdoor influences.

## Key Results
- Significant improvements in both Attack Success Rate (ASR) and Clean Accuracy (CA) compared to state-of-the-art defenses
- Effective across 11 different backdoor attacks and three datasets
- Demonstrates robustness across different ViT architectures
- Shows effectiveness against clean-label attacks

## Why This Works (Mechanism)
The method works by creating a dual-module system where one module specializes in detecting poisoned samples while the other focuses on learning clean representations. The shallow module acts as a filter, identifying suspicious samples based on confidence scores. By training the robust module only on uncertain samples, it avoids learning backdoor patterns embedded in high-confidence poisoned data. The asynchronous unlearning process ensures that any backdoor influence that does affect the robust module is systematically removed over time, creating a defense-in-depth approach that maintains model utility while eliminating malicious triggers.

## Foundational Learning

**Backdoor Attacks in Vision Models**
*Why needed:* Understanding how adversaries inject hidden triggers that activate malicious behaviors during inference
*Quick check:* Can you explain the difference between clean-label and dirty-label backdoor attacks?

**Vision Transformer Architecture**
*Why needed:* ViTs process images differently than CNNs, requiring specialized defense approaches
*Quick check:* How do self-attention mechanisms in ViTs affect vulnerability to backdoor attacks?

**Ensemble Learning for Security**
*Why needed:* Multiple models can provide complementary security through diverse decision boundaries
*Quick check:* What are the benefits of using different model depths in an ensemble for backdoor detection?

**Unlearning Techniques**
*Why needed:* Removing learned patterns without full retraining is crucial for practical deployment
*Quick check:* How does asynchronous unlearning differ from traditional fine-tuning approaches?

## Architecture Onboarding

**Component Map:**
Pre-trained Shallow ViT -> Confidence Scoring -> Data Filtering -> Robust ViT Training
Robust ViT -> Unlearning Module -> Dynamic Unlearning Rate -> Updated Robust ViT
Unlearn Set Accumulator -> Sample Selection -> Unlearning Trigger

**Critical Path:**
Poisoned Module Confidence Assessment → Data Selection for Robust Module → Robust Module Training → Unlearn Set Population → Asynchronous Unlearning → Updated Robust Model

**Design Tradeoffs:**
The method balances defense effectiveness against computational overhead through the shallow poisoned module design. While deeper poisoned modules might provide better detection, they would increase computational cost and potentially introduce their own vulnerabilities. The asynchronous unlearning approach trades immediate convergence for sustained defense effectiveness throughout training.

**Failure Signatures:**
- Poor confidence discrimination between clean and poisoned samples
- Rapid accumulation of unlearn samples leading to catastrophic forgetting
- Inconsistent unlearning rates causing instability in robust module learning
- Over-reliance on poisoned module leading to missed backdoor patterns

**First 3 Experiments to Run:**
1. Test confidence score distribution differences between clean and poisoned samples on a validation set
2. Measure unlearning effectiveness by tracking backdoor activation strength over training epochs
3. Evaluate clean accuracy degradation when varying the dynamic unlearning rate schedule

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Performance degradation on simpler datasets (CIFAR-10) and against weaker attacks suggests the approach may be overly complex for certain scenarios
- Pre-training requirement for the shallow poisoned module adds computational overhead and deployment complexity
- Asynchronous unlearning process introduces timing considerations that could affect convergence in different training environments

## Confidence

**High Confidence:**
- Experimental results demonstrating improved ASR and CA compared to baseline defenses across multiple attacks and datasets

**Medium Confidence:**
- Generalizability claims across different ViT architectures would benefit from testing on broader range of model sizes and configurations
- Effectiveness against clean-label attacks is demonstrated but robustness to more sophisticated variations remains unclear

## Next Checks

1. Test performance on larger-scale datasets (e.g., ImageNet) to validate scalability and assess computational overhead in resource-constrained environments

2. Evaluate against adaptive attackers who know the defense mechanism and can modify their poisoning strategies accordingly

3. Conduct ablation studies to determine optimal depth for the shallow poisoned module and impact of different unlearning rate schedules on final performance