---
ver: rpa2
title: 'Climate Adaptation with Reinforcement Learning: Experiments with Flooding
  and Transportation in Copenhagen'
arxiv_id: '2409.18574'
source_url: https://arxiv.org/abs/2409.18574
tags:
- impacts
- adaptation
- transportation
- measures
- climate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a novel reinforcement learning (RL)-based framework
  for identifying optimal climate adaptation strategies to minimize urban flooding
  impacts on transportation infrastructure. The approach integrates climate projections,
  flood modeling, transportation simulation, and mobility impact assessment within
  a unified environment.
---

# Climate Adaptation with Reinforcement Learning: Experiments with Flooding and Transportation in Copenhagen

## Quick Facts
- **arXiv ID**: 2409.18574
- **Source URL**: https://arxiv.org/abs/2409.18574
- **Reference count**: 40
- **Key outcome**: RL-based policy reduces flood impacts on transportation by 47% by 2035 and 45% by 2100 versus random adaptation.

## Executive Summary
This study presents a reinforcement learning (RL) framework for identifying optimal climate adaptation strategies to minimize urban flooding impacts on transportation infrastructure. Using Copenhagen as a case study, the approach integrates climate projections, flood modeling, transportation simulation, and mobility impact assessment. The RL agent learns to determine the best timing and locations for implementing adaptation measures, such as elevating roads, to reduce both direct infrastructure damage and indirect travel delays. Experiments demonstrate that the RL-based policy outperforms random adaptation strategies while maintaining comparable adaptation costs.

## Method Summary
The framework integrates rainfall projections, flood modeling via SCALGO Live, a simplified four-step transportation model, and impact assessment using depth-damage and depth-disruption functions. The RL agent (PPO algorithm) selects adaptation actions (1m road elevation) across 29 TAZs to minimize combined economic losses. The environment is implemented with Gymnasium interface, and the agent is trained and evaluated over 80 years (2023-2100) using RCP8.5 climate scenarios.

## Key Results
- RL policy reduces cumulative flood impacts by 47% by 2035 and 45% by 2100 compared to random adaptation
- Adaptation costs remain comparable between RL and random policies
- Performance improvement is consistent across different reward weighting schemes
- Policy effectiveness persists across the full simulation period (2023-2100)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning optimizes the timing and location of road elevation to minimize flood impacts.
- Mechanism: The RL agent interacts with an integrated assessment model (IAM) that simulates rainfall, flood propagation, and transportation disruptions. By receiving rewards based on economic losses (road damage + travel delays), the agent learns to prioritize adaptation actions in zones with highest expected future impact.
- Core assumption: The Markov Decision Process (MDP) assumption holds—each state is independent and depends only on the current action and environment state.
- Evidence anchors:
  - [abstract]: "leverage RL to identify the most effective timing and locations for implementing measures, aiming to reduce both direct and indirect impacts of flooding."
  - [section]: "At each time step, our RL agent takes an action on a TAZ and collects information about the state of Copenhagen..."
- Break condition: If the flood dynamics or transportation demand patterns are highly non-stationary beyond the model's horizon, the learned policy may degrade.

### Mechanism 2
- Claim: Flood risk reduction is achieved through selective infrastructure upgrades informed by simulation-based scenario analysis.
- Mechanism: The framework uses SCALGO Live to simulate flood depths from projected rainfall events, then applies depth-damage and depth-disruption functions to quantify impacts. The RL policy chooses where to elevate roads (by 1m) to increase flood resilience.
- Core assumption: Road elevation by a fixed amount is a sufficient proxy for adaptation and that flood impacts are primarily a function of road depth.
- Evidence anchors:
  - [section]: "We defined one possible measure: elevate roads by 1 meter (i.e., increase the minimum water depth needed to affect roads)."
  - [section]: "Damage was then computed using depth-damage functions [29], effectively mapping the percentage of damage on a road according to the water depth at its location."
- Break condition: If other adaptation measures (e.g., drainage upgrades, permeable surfaces) are more cost-effective, the fixed elevation assumption may be suboptimal.

### Mechanism 3
- Claim: The RL-based policy outperforms random adaptation by strategically targeting high-impact zones.
- Mechanism: Through repeated simulation of rainfall and flood events across TAZs, the RL agent accumulates reward signals that reflect avoided losses. It learns to concentrate adaptation investments in areas where both road damage and travel delays are high.
- Core assumption: Cumulative reward is a valid proxy for adaptation effectiveness, and the reward weights (βR, βD, βA) appropriately balance infrastructure and mobility impacts.
- Evidence anchors:
  - [section]: "Table 1 compares the performance of the trained RL agent against a random agent... achieving a substantial reduction in direct and indirect impacts, lowering them by 47%."
  - [section]: "Our agent consistently outperforms the random policy, achieving significantly better outcomes overall."
- Break condition: If reward weights are mis-specified, the agent may over-prioritize one impact type at the expense of another.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The RL agent assumes the environment follows MDP dynamics—each decision depends only on the current state, not history.
  - Quick check question: In this framework, what makes a state "Markovian"? (Answer: It captures all necessary information from past to predict future impacts given current action.)

- Concept: Depth-damage and depth-disruption functions
  - Why needed here: These functions map flood water depth to economic losses for roads and travel delays, enabling the RL agent to quantify impact.
  - Quick check question: Why are these functions critical for the reward calculation? (Answer: They translate physical flood depth into monetary terms used in the reward function.)

- Concept: Four-Step Model (simplified)
  - Why needed here: It generates origin-destination trip matrices and routes to simulate how floods disrupt mobility across the city.
  - Quick check question: What part of the 4SM is omitted here, and why? (Answer: Mode choice and frequency modeling are omitted; only road network and motorized trips are modeled for simplicity.)

## Architecture Onboarding

- Component map: Rainfall projection -> Flood model (SCALGO Live) -> Transportation model (4SM simplified) -> Impact model (depth-damage + depth-disruption) -> RL agent -> Adaptation action (road elevation)
- Critical path: Rainfall event -> Flood depth simulation -> Impact calculation -> Reward aggregation -> RL action selection -> Adaptation implementation
- Design tradeoffs:
  - Fixed 1m elevation vs. variable measures: simplicity vs. realism
  - Stationarity assumption for rainfall vs. dynamic climate trends: tractability vs. accuracy
  - Full 4SM vs. simplified: computational efficiency vs. realism
- Failure signatures:
  - Over-concentration of adaptation in few TAZs while others remain vulnerable
  - Reward signal dominated by one impact type (e.g., travel delays) leading to skewed policies
  - Degradation of policy performance when climate conditions shift beyond training distribution
- First 3 experiments:
  1. Run baseline: random policy over 2023–2035, record cumulative reward and adaptation cost.
  2. Train RL agent (PPO) with same climate scenarios, compare cumulative reward and impact reduction.
  3. Vary reward weights (βR, βD, βA) and observe policy shift between road protection vs. mobility preservation.

## Open Questions the Paper Calls Out

- **Question**: How does the inclusion of additional adaptation measures (e.g., permeable roads, road durability enhancements) impact the effectiveness of the RL-based policy compared to the single measure (road elevation) used in the current study?
  - Basis in paper: [explicit] The paper mentions that future work includes expanding the framework to incorporate additional adaptation measures.
  - Why unresolved: The current study only tests one adaptation measure (road elevation), so the relative effectiveness of multiple measures remains unknown.
  - What evidence would resolve it: Experimental results comparing RL-based policies using different combinations of adaptation measures would demonstrate their relative effectiveness and cost-efficiency.

- **Question**: What is the impact of incorporating additional transport modes (e.g., cycling, walking) into the framework on the overall effectiveness of the RL-based adaptation strategy?
  - Basis in paper: [explicit] The paper suggests expanding the case study to include other modes of transport in future work.
  - Why unresolved: The current study focuses exclusively on motorized road transport, so the benefits of a multimodal approach are untested.
  - What evidence would resolve it: Simulation results showing the performance of the RL policy when applied to a multimodal transportation network would clarify the benefits of including non-motorized transport modes.

- **Question**: How does the RL-based policy perform when applied to a larger geographical area, such as the entire city of Copenhagen and its metropolitan region?
  - Basis in paper: [explicit] The paper states that future work includes expanding the case study to encompass the entire city of Copenhagen and its metropolitan area.
  - Why unresolved: The current study is limited to the city center (29 TAZ), so scalability and performance in a larger area remain uncertain.
  - What evidence would resolve it: Results from experiments conducted on the full Copenhagen metropolitan area would demonstrate whether the RL policy scales effectively and maintains performance with increased complexity.

## Limitations

- Stationarity assumption for rainfall and travel demand may not hold under rapidly changing climate conditions
- Fixed 1m road elevation oversimplifies the range of possible adaptation measures
- Simplified transportation model omits mode choice and frequency, limiting realism

## Confidence

- **High**: RL outperforms random adaptation in controlled Copenhagen simulation
- **Medium**: Results generalize to RCP8.5 scenario within training distribution
- **Low**: Policy effectiveness in other cities or climate pathways without additional validation

## Next Checks

1. **Reward sensitivity analysis**: Systematically vary βR, βD, and βA to identify conditions under which the RL policy shifts between road protection and mobility preservation priorities.

2. **Temporal robustness test**: Evaluate policy performance under non-stationary rainfall patterns by introducing climate trend deviations beyond the training distribution.

3. **Alternative adaptation measures**: Replace fixed 1m elevation with a cost-benefit optimized portfolio including drainage upgrades and permeable surfaces to assess policy robustness.