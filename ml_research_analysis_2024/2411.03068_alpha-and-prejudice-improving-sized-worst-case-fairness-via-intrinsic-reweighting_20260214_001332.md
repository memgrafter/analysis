---
ver: rpa2
title: "Alpha and Prejudice: Improving $\u03B1$-sized Worst-case Fairness via Intrinsic\
  \ Reweighting"
arxiv_id: '2411.03068'
source_url: https://arxiv.org/abs/2411.03068
tags:
- fairness
- training
- group
- which
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles \u03B1-sized worst-case fairness, a setting\
  \ where sensitive demographic information is unavailable but a lower bound on the\
  \ minimal group proportion (\u03B1) is known. The authors propose Intrinsic ReWeighting\
  \ (IRW), which assigns sample weights based on individual gradients' contributions\
  \ to fairness, rather than solely on loss values."
---

# Alpha and Prejudice: Improving $α$-sized Worst-case Fairness via Intrinsic Reweighting

## Quick Facts
- arXiv ID: 2411.03068
- Source URL: https://arxiv.org/abs/2411.03068
- Reference count: 40
- One-line primary result: IRW improves worst-off group accuracy by assigning sample weights based on gradient similarity to the worst-off group, achieving better fairness metrics than existing approaches without requiring sensitive attribute labels.

## Executive Summary
This paper addresses α-sized worst-case fairness in machine learning where sensitive demographic information is unavailable but a lower bound on the minimal group proportion (α) is known. The authors propose Intrinsic ReWeighting (IRW), which assigns sample weights based on individual gradients' contributions to fairness rather than solely on loss values. They develop a stochastic learning scheme for efficiency and a robust variant (IRWO) using gradient-based clustering to handle outliers. Experiments on four benchmark datasets demonstrate that IRW outperforms existing approaches, achieving better worst-off group accuracy and fairness metrics even compared to methods that explicitly use sensitive attributes.

## Method Summary
IRW tackles α-sized worst-case fairness by reweighting training samples based on their intrinsic importance to fairness, measured through gradient similarity rather than loss values. For each sample, IRW computes a weight proportional to the positive dot product between its gradient and the combined gradient of the worst-off group samples. This captures how much the sample contributes to improving the worst-off group's loss. The method includes stochastic update schemes (global and local) to scale to large datasets by computing weights either once per epoch or per mini-batch. A robust variant, IRWO, uses DBSCAN clustering on pairwise gradient distances to identify and remove outliers before computing weights.

## Key Results
- IRW achieves better worst-off group accuracy and fairness metrics compared to existing approaches on four benchmark datasets
- Gradient-based reweighting outperforms loss-based approaches, particularly for samples with large losses but zero gradients
- IRWO variant effectively handles outliers through gradient-based clustering, achieving the best performance among all compared methods
- Local and global stochastic update schemes maintain effectiveness while improving computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** IRW improves worst-off group accuracy by assigning sample weights based on gradient similarity to the combined gradient of the worst-off group, rather than solely on loss values.
- **Mechanism:** For each sample, IRW computes a weight proportional to the positive part of the dot product between its gradient and the combined gradient of the worst-off group samples. This captures how much the sample contributes to improving the worst-off group's loss.
- **Core assumption:** The direction of a sample's gradient relative to the worst-off group's combined gradient is a better indicator of its importance for fairness than its absolute loss value.
- **Evidence anchors:**
  - [abstract] "We propose reweighting the training samples based on their intrinsic importance to fairness... based on individual gradients' contributions to fairness, rather than solely on loss values."
  - [section 4.2] "We challenge the strategy that samples with larger losses should always be assigned larger weights. Because a sample with a large loss but a zero gradient cannot be considered contributive."
  - [corpus] Weak: corpus neighbors discuss reweighting but not specifically gradient-based approaches for fairness.
- **Break condition:** If gradients become unreliable (e.g., due to vanishing gradients or in very deep networks), the gradient similarity measure may fail to identify truly contributive samples.

### Mechanism 2
- **Claim:** The stochastic update schemes (global and local) maintain IRW's effectiveness while scaling to large datasets.
- **Mechanism:** Instead of computing weights globally over the entire dataset, IRW can compute weights either once per epoch using the global top-Nα samples, or per mini-batch using the local top-Bα samples, reducing computational cost.
- **Core assumption:** The sample weights do not change significantly within an epoch, so using a fixed weight assignment for that epoch is acceptable.
- **Evidence anchors:**
  - [section 4.3] "We adopt stochastic update by proposing two schemes... we do not re-compute w∗ for them to save computation costs."
  - [section 6.5] "The results show that local worst-case training which selects α-sized worst samples among each mini-batch are aligned with the global counterpart; their training loss and test accuracy curves are close to each other."
  - [corpus] Weak: corpus neighbors discuss scalability but not specifically for gradient-based reweighting in fairness contexts.
- **Break condition:** If the data distribution shifts rapidly within an epoch, the fixed weights could become stale and degrade performance.

### Mechanism 3
- **Claim:** IRWO handles outliers by using gradient-based clustering to identify and remove samples with low local density in gradient space before computing weights.
- **Mechanism:** IRWO computes pairwise distances between per-sample gradients, uses DBSCAN clustering on these distances to label outliers (samples with label -1), and removes them before applying IRW's weighting scheme.
- **Core assumption:** Outliers in the context of worst-case fairness have gradients that are dissimilar from the majority, making them identifiable through density-based clustering in gradient space.
- **Evidence anchors:**
  - [section 5] "We develop a variant of IRW, named by IRWO, to handle potential outliers during training... We first compute pairwise distance between every two samples' gradient within a batch."
  - [section 6.6] "IRWO achieves the best performance among all the methods compared, showing the superiority of the proposed method."
  - [corpus] Weak: corpus neighbors discuss outlier handling but not specifically using gradient clustering for fairness.
- **Break condition:** If outliers are not distinguishable by gradient density (e.g., they form their own cluster), DBSCAN may fail to remove them.

## Foundational Learning

- **Concept:** Gradient-based sample importance (influence functions)
  - **Why needed here:** IRW relies on computing per-sample gradients and their similarity to identify contributive samples for fairness. Understanding influence functions provides the theoretical foundation.
  - **Quick check question:** What does the influence function tell us about a training sample's importance to a model's prediction?

- **Concept:** Distributionally robust optimization (DRO)
  - **Why needed here:** The α-sized worst-case fairness framework builds on DRO concepts, using a bound on group proportions to formulate the problem without needing explicit group labels.
  - **Quick check question:** How does DRO provide a worst-case guarantee when the true data distribution is unknown?

- **Concept:** Local differential privacy and randomized response
  - **Why needed here:** The paper justifies the α-sized setting by showing how α can be estimated from randomized responses while preserving privacy, making the setting practical.
  - **Quick check question:** How does the randomized response mechanism ensure (ln 3, 0)-differential privacy while allowing estimation of group proportions?

## Architecture Onboarding

- **Component map:** Data loader → Forward pass → Instance-wise backward pass → Per-sample gradient computation → Distance matrix computation (IRWO) → DBSCAN clustering (IRWO) → Weight computation (IRW) → Weighted gradient aggregation → Parameter update
- **Critical path:** The instance-wise backward pass and weight computation are the most computationally intensive steps, especially for large models.
- **Design tradeoffs:** Using global weights (computed once per epoch) vs. local weights (computed per mini-batch) involves a tradeoff between computational efficiency and weight accuracy.
- **Failure signatures:** If the worst-off group accuracy plateaus or decreases, it could indicate issues with weight computation, outlier removal, or learning rate scheduling.
- **First 3 experiments:**
  1. Verify that IRW assigns higher weights to samples with gradients more aligned with the worst-off group's combined gradient (compare to loss-based weighting).
  2. Test the impact of outlier removal by comparing IRW vs. IRWO on a dataset known to contain outliers.
  3. Evaluate the scalability of global vs. local weight computation schemes on a large dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of intrinsic reweighting (IRW) compare to other reweighting strategies when applied to datasets with more than four groups or continuous sensitive attributes?
- Basis in paper: [inferred] The paper only evaluates on datasets with binary and multi-class classification tasks, but does not explore scenarios with more complex sensitive attributes.
- Why unresolved: The paper focuses on specific datasets with binary and multi-class classification, leaving the performance on datasets with more groups or continuous attributes unexplored.
- What evidence would resolve it: Conducting experiments on datasets with more groups or continuous sensitive attributes to compare the performance of IRW against other reweighting strategies.

### Open Question 2
- Question: What is the impact of using different gradient layers (e.g., earlier vs. later layers) on the effectiveness of intrinsic reweighting in large neural networks?
- Basis in paper: [explicit] The paper mentions that gradients are sometimes too complex to work with in larger neural networks and suggests selecting key layers for specific tasks, but does not provide a detailed analysis of this impact.
- Why unresolved: The paper acknowledges the complexity of gradients in large networks but does not explore the impact of using different layers on IRW's effectiveness.
- What evidence would resolve it: Conducting experiments that analyze the impact of using different gradient layers on IRW's performance in large neural networks.

### Open Question 3
- Question: How does the variance of resampling compare to reweighting in achieving fairness when using optimizers other than stochastic gradient descent (SGD)?
- Basis in paper: [explicit] The paper provides a theoretical corollary suggesting that resampling has lower variance than reweighting under SGD, but does not explore this under other optimizers.
- Why unresolved: The paper only provides theoretical insights for SGD and does not empirically test the variance comparison under other optimizers.
- What evidence would resolve it: Conducting empirical studies to compare the variance of resampling and reweighting under different optimizers to validate the theoretical findings.

## Limitations
- Scalability concerns: The method requires computing per-sample gradients and a distance matrix for IRWO, which may not scale well to very large datasets or complex models.
- Theoretical guarantees: The paper does not provide formal convergence guarantees for IRW or IRWO, particularly under the stochastic update schemes.
- Outlier detection limitations: The DBSCAN-based outlier removal depends on choosing appropriate hyperparameters and assumes outliers have low density in gradient space.

## Confidence
- **High confidence:** The experimental results showing IRW outperforms baselines on benchmark datasets are well-supported by the data.
- **Medium confidence:** The theoretical justification for why gradient similarity captures importance for fairness is sound, but the practical implications may vary depending on the specific model architecture and data characteristics.
- **Low confidence:** The claim that the stochastic update schemes maintain effectiveness while significantly improving efficiency is based on limited experiments and may not generalize to all settings.

## Next Checks
1. **Ablation on outlier detection:** Run IRW vs. IRWO on datasets with varying degrees of outliers to quantify the impact of outlier removal. Measure how many samples are removed and how this affects performance on minority groups.

2. **Scalability benchmarking:** Compare the runtime of IRW (with both global and local updates) against baseline methods on increasingly large datasets. Profile the computational cost of per-sample gradient computation and distance matrix calculation.

3. **Theoretical analysis:** Derive convergence rates for IRW under the stochastic update schemes. Analyze the variance introduced by the weight computation process and how it affects the overall optimization dynamics.