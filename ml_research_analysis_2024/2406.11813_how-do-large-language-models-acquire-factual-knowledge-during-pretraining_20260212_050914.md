---
ver: rpa2
title: How Do Large Language Models Acquire Factual Knowledge During Pretraining?
arxiv_id: '2406.11813'
source_url: https://arxiv.org/abs/2406.11813
tags:
- knowledge
- pretraining
- factual
- training
- memorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how large language models (LLMs) acquire
  factual knowledge during pretraining by injecting fictional knowledge into intermediate
  checkpoints and monitoring memorization and generalization. Key findings include:
  (1) factual knowledge acquisition occurs by accumulating small probability increases
  upon each encounter, diluted by subsequent forgetting; (2) scaling model size improves
  acquisition effectiveness, but training on more data does not; (3) a power-law relationship
  exists between training steps and forgetting of both memorization and generalization;
  (4) deduplicating training data and using larger batch sizes enhance knowledge retention
  by making models more robust to forgetting.'
---

# How Do Large Language Models Acquire Factual Knowledge During Pretraining?

## Quick Facts
- arXiv ID: 2406.11813
- Source URL: https://arxiv.org/abs/2406.11813
- Authors: Hoyeon Chang; Jinho Park; Seonghyeon Ye; Sohee Yang; Youngkyung Seo; Du-Seong Chang; Minjoon Seo
- Reference count: 40
- Key outcome: This study investigates how large language models (LLMs) acquire factual knowledge during pretraining by injecting fictional knowledge into intermediate checkpoints and monitoring memorization and generalization.

## Executive Summary
This study investigates how LLMs acquire factual knowledge during pretraining by injecting fictional knowledge into intermediate checkpoints and monitoring memorization and generalization. The researchers found that factual knowledge acquisition occurs through progressive accumulation of small probability increases at each training step, diluted by subsequent forgetting. Scaling model size improves acquisition effectiveness, but training on more data does not. A power-law relationship exists between training steps and forgetting of both memorization and generalization. Deduplicating training data and using larger batch sizes enhance knowledge retention by making models more robust to forgetting.

## Method Summary
The researchers injected fictional knowledge into intermediate OLMo checkpoints (1B and 7B sizes at early/mid/late stages) and resumed pretraining with modified batches containing this knowledge. They measured log probabilities of target spans to quantify knowledge acquisition and forgetting, examining three scenarios: knowledge presented multiple times (duplication), knowledge presented in paraphrased forms, and knowledge presented once. The study varied model sizes, batch sizes, and pretraining stages while tracking effectivity (immediate improvement in log probability) and retainability (forgetting over time).

## Key Results
- Factual knowledge acquisition occurs through progressive accumulation of small probability increases with subsequent forgetting
- Scaling model size improves acquisition effectiveness, but training on more data does not
- Power-law relationship exists between training steps and forgetting of both memorization and generalization
- Deduplicating training data and using larger batch sizes enhance knowledge retention by making models more robust to forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factual knowledge acquisition occurs through progressive accumulation of small probability increases at each training step, diluted by subsequent forgetting.
- Mechanism: Each time a model encounters a piece of factual knowledge during training, the probability of that knowledge increases slightly. However, when the knowledge is not presented again, the model gradually forgets it. Over multiple encounters, these small increases accumulate to form retained knowledge.
- Core assumption: The model's learning dynamics allow for gradual accumulation of knowledge through repeated exposure, even with forgetting between exposures.
- Evidence anchors:
  - [abstract] "factual knowledge acquisition occurs by progressively increasing the probability of factual knowledge presented in the pretraining data at each step. However, this increase is diluted by subsequent forgetting."
  - [section 4.1] "LLMs acquire factual knowledge by accumulating micro-acquisitions with subsequent forgetting each time the model encounters the knowledge during pretraining."
  - [corpus] Weak evidence - the corpus neighbors discuss incremental learning and forgetting but don't directly address the progressive accumulation mechanism described here.
- Break condition: If the forgetting rate exceeds the accumulation rate, the model will never retain the knowledge regardless of how many times it's encountered.

### Mechanism 2
- Claim: Scaling model size improves factual knowledge acquisition effectiveness, but training on more data does not.
- Mechanism: Larger models have greater capacity to encode and retain factual knowledge, leading to higher probability increases when encountering new information. However, simply seeing more tokens doesn't improve the model's ability to acquire knowledge because the acquisition mechanism (probability accumulation) remains the same regardless of pretraining stage.
- Core assumption: Model capacity, not exposure quantity, determines acquisition effectiveness.
- Evidence anchors:
  - [abstract] "scaling model size improves acquisition effectiveness, but training on more data does not"
  - [section 4.2] "the effectivity of fact acquisition does not improve with checkpoints trained with more tokens"
  - [corpus] Moderate evidence - the corpus mentions knowledge acquisition during pretraining but doesn't specifically address why scaling model size helps while more data doesn't.
- Break condition: If the relationship between model capacity and knowledge encoding changes at extreme scales, or if the acquisition mechanism fundamentally changes with more diverse data.

### Mechanism 3
- Claim: There is a learnability threshold - knowledge presented with intervals longer than this threshold cannot be acquired regardless of pretraining duration.
- Mechanism: Popular knowledge is presented frequently enough (intervals shorter than the threshold) to accumulate sufficient probability to be generated as output. Unpopular/long-tail knowledge has intervals exceeding the threshold, making acquisition impossible even with unlimited training.
- Core assumption: Knowledge acquisition requires multiple encounters within a specific timeframe to overcome forgetting.
- Evidence anchors:
  - [abstract] "we hypothesize that the popularity of the knowledge in the pretraining data influences how quickly this knowledge begins to be 'revealed' in the generated sequences"
  - [section 4.4] "if a given factual knowledge in the pretraining dataset is in the long-tail and the knowledge is presented to the model with an interval longer than a certain threshold, such knowledge will be impossible to be decoded as the top-k generation of the model"
  - [corpus] Moderate evidence - corpus neighbors discuss forgetting and knowledge retention but don't explicitly mention learnability thresholds.
- Break condition: If the forgetting rate changes significantly with model size or training conditions, the threshold would shift and invalidate this mechanism.

## Foundational Learning

- Concept: Probability accumulation through repeated exposure
  - Why needed here: This is the core mechanism by which LLMs acquire factual knowledge - understanding how small probability increases accumulate over time is essential to grasping the paper's findings.
  - Quick check question: If a model encounters a fact once and forgets it completely, how many times must it encounter the same fact (assuming consistent probability increases) to retain it?

- Concept: Power-law relationships in learning dynamics
  - Why needed here: The paper shows that forgetting follows a power-law relationship with training steps, which is crucial for understanding how knowledge retention changes over time.
  - Quick check question: If retainability follows R(t) ∝ t^(-a), what happens to the fraction of retained knowledge as training steps increase exponentially?

- Concept: Batch size effects on learning dynamics
  - Why needed here: The paper demonstrates that larger batch sizes affect forgetting rates and learnability thresholds, which is important for understanding practical training implications.
  - Quick check question: How would you expect changing from batch size 2048 to 128 to affect the frequency of knowledge encounters and the resulting forgetting dynamics?

## Architecture Onboarding

- Component map: OLMo checkpoints (1B, 7B) → FICTIONAL KNOWLEDGE injection → Modified pretraining batches → Log probability measurement → Effectivity/Retainability calculation
- Critical path: Inject fictional knowledge → Continue pretraining with modified batches → Measure log probabilities at each step → Calculate effectivity and retainability metrics → Analyze forgetting dynamics and learning thresholds
- Design tradeoffs: Using fictional knowledge ensures no prior exposure but may not perfectly represent real knowledge acquisition. Measuring log probabilities provides fine-grained analysis but doesn't directly measure generation quality.
- Failure signatures: If the forgetting rate is too high relative to accumulation, knowledge won't be retained. If batch size is too small, the learnability threshold becomes too restrictive. If data isn't properly deduplicated, generalization suffers.
- First 3 experiments:
  1. Reproduce the basic effectivity measurement by injecting a single piece of knowledge and tracking log probability changes over training steps.
  2. Compare effectivity between 1B and 7B models using identical knowledge injection to verify the scaling relationship.
  3. Test the learnability threshold hypothesis by injecting knowledge at different frequencies and measuring retention rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between the accumulation of probability of factual knowledge and the model's generation output?
- Basis in paper: [explicit] The paper mentions that this relationship is not analyzed: "Although they do not affect the findings and implications of our work, there are several limitations. First, we do not perform evaluations based on the generation output of the model, and we do not investigate the exact relationship between the model's accumulation of probability of factual knowledge and the model's generation output."
- Why unresolved: The study focuses on analyzing log probabilities and training dynamics rather than actual generation outputs, leaving the connection between probability accumulation and generation behavior unexplored.
- What evidence would resolve it: Experiments measuring how the accumulated probability of factual knowledge translates to its appearance in generated text, possibly through controlled generation studies or probability-threshold analysis.

### Open Question 2
- Question: How do very early stages of pretraining differ in their factual knowledge acquisition dynamics?
- Basis in paper: [explicit] The paper acknowledges this limitation: "Second, we do not analyze the pretraining dynamics at very early stages, which can exhibit significantly different behaviors [24]."
- Why unresolved: The study uses intermediate checkpoints and does not examine the earliest pretraining stages, missing potentially unique early-stage dynamics.
- What evidence would resolve it: Analysis of the earliest checkpoints and training steps, possibly using more frequent checkpointing or specialized analysis techniques for early-stage training.

### Open Question 3
- Question: How does varying batch size and learning rate affect factual knowledge acquisition across a wider range of values?
- Basis in paper: [explicit] The paper states: "Third, we do not study the effect of training batch size and learning rate on the dynamics of factual knowledge acquisition across multiple values."
- Why unresolved: The study only examines two specific batch sizes (2048 and 128) and doesn't systematically vary learning rates across multiple values.
- What evidence would resolve it: Comprehensive experiments varying both batch sizes and learning rates across multiple values to identify optimal ranges for factual knowledge acquisition.

## Limitations
- Dataset Representativeness: The use of fictional knowledge may not fully capture how real-world factual knowledge is acquired.
- Forgetting Dynamics Generalizability: The observed power-law forgetting relationship may not generalize across all model architectures or knowledge types.
- Learnability Threshold Sensitivity: The identified threshold appears sensitive to hyperparameters like batch size and model capacity.

## Confidence
- High Confidence: Factual knowledge acquisition occurs through progressive accumulation with forgetting; scaling model size improves effectiveness; power-law relationship exists between training steps and forgetting.
- Medium Confidence: Deduplicating training data enhances knowledge retention; larger batch sizes make models more robust to forgetting; knowledge popularity influences acquisition learnability.
- Low Confidence: Specific learnability threshold values across different training configurations; exact relationship between batch size and memorization efficiency per token; generalization to other model architectures.

## Next Checks
1. **Cross-Architecture Validation**: Test the same knowledge injection methodology on different LLM architectures (e.g., LLaMA, GPT-style models) to verify whether the progressive accumulation and forgetting dynamics generalize beyond the OLMo family.

2. **Real Knowledge Replication**: Replace fictional knowledge with carefully selected real-world facts that are guaranteed to be absent from pretraining data, then compare acquisition patterns to validate whether synthetic knowledge acquisition mirrors real knowledge acquisition.

3. **Long-Tail Knowledge Stress Test**: Systematically vary the frequency of knowledge presentation (from very frequent to extremely rare) across multiple orders of magnitude to more precisely map out the learnability threshold and test whether the proposed power-law relationship holds at the extremes.