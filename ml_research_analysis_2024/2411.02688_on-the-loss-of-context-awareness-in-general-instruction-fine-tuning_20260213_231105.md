---
ver: rpa2
title: On the Loss of Context-awareness in General Instruction Fine-tuning
arxiv_id: '2411.02688'
source_url: https://arxiv.org/abs/2411.02688
tags:
- context
- attention
- instruction
- layer
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Instruction fine-tuning (SFT) with chat templates impairs context
  awareness in open-source LLMs, as models shift attention away from user tokens toward
  assistant tokens. This leads to performance degradation on tasks requiring precise
  context retrieval.
---

# On the Loss of Context-awareness in General Instruction Fine-tuning

## Quick Facts
- **arXiv ID**: 2411.02688
- **Source URL**: https://arxiv.org/abs/2411.02688
- **Reference count**: 40
- **Primary result**: Chat templates in instruction fine-tuning shift model attention away from user context, degrading context awareness.

## Executive Summary
Instruction fine-tuning (SFT) with chat templates improves instruction-following capabilities but impairs context awareness in open-source LLMs. The paper identifies that chat templates cause models to shift attention away from user-provided context tokens toward assistant-generated tokens, leading to performance degradation on context-dependent tasks. The authors introduce a context-dependency metric to identify examples requiring context and demonstrate that adding an indicator token during fine-tuning enables models to selectively increase attention to user tokens when the indicator is present, restoring context awareness without sacrificing general instruction-following performance.

## Method Summary
The method involves fine-tuning pretrained models on instruction datasets with chat templates, then computing context-dependency scores for each example based on attention allocation to user tokens. Examples identified as context-dependent are augmented with an indicator token during fine-tuning. The model learns to associate the indicator with increased attention to user context. During inference, the indicator is added to context-dependent queries to recover context awareness. The approach is evaluated across three LLMs (TinyLlama-1.1B, Llama-2-7B, Llama-3-8B) and three datasets (ShareGPT, UltraChat-200K, WizardLM-70K) on context-dependent tasks including NIH, QuAC, SQuAD, and DROP.

## Key Results
- Chat templates in SFT shift attention from user tokens to assistant tokens, degrading context awareness
- Performance degradation correlates with bias learned from context-independent examples in training data
- Context-dependency indicator token during fine-tuning restores context awareness without harming general instruction-following
- Method works across three different model sizes and instruction datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chat templates cause attention to shift from user to assistant tokens during SFT
- Mechanism: Role markers in chat templates partition input tokens, leading the model to allocate lower attention to user tokens and higher attention to assistant tokens, especially for the first response token
- Core assumption: Attention allocation patterns learned during fine-tuning directly influence context retrieval behavior during inference
- Evidence anchors: [abstract] "Models shift attention away from user tokens toward assistant tokens"; [section 2.2] "attention allocated to user tokens decreases while attention to assistant tokens increases"
- Break condition: Different attention architectures or pruned attention heads may not exhibit this shift

### Mechanism 2
- Claim: Performance decline correlates with bias from context-independent training examples
- Mechanism: Context-independent examples (e.g., common knowledge questions) teach the model to deprioritize context, generalizing to all tasks when chat templates are present
- Core assumption: Training dataset composition creates systematic attention allocation bias
- Evidence anchors: [abstract] "bias can be learned from training examples that align with the model's internal knowledge"; [section 2.1] "NIH error increases for most models after instruction fine-tuning"
- Break condition: Carefully curated training datasets excluding context-independent examples

### Mechanism 3
- Claim: Context-dependency indicator token teaches selective attention to user tokens
- Mechanism: Indicator token acts as conditional signal, teaching model to route more attention to user tokens when present
- Core assumption: Model can learn association between indicator and context focus
- Evidence anchors: [abstract] "conditional instruction fine-tuning with a context-dependency indicator"; [section 3.2] "learns to allocate increased attention to user tokens when the indicator is present"
- Break condition: Insufficient training data prevents learning the indicator-context association

## Foundational Learning

- **Concept**: Attention mechanism in transformer models
  - Why needed here: Core argument relies on analyzing and manipulating attention weights
  - Quick check question: How does self-attention compute relevance between two tokens?

- **Concept**: Supervised fine-tuning (SFT) for instruction following
  - Why needed here: Paper investigates how SFT with chat templates degrades context awareness
  - Quick check question: What's the difference between SFT and RLHF?

- **Concept**: Context awareness and hallucination in LLMs
  - Why needed here: Defines context awareness as ability to retrieve and use user-provided context
  - Quick check question: How does poor context awareness lead to hallucinations?

## Architecture Onboarding

- **Component map**: Input processor -> Attention module -> Context-dependency scorer -> Fine-tuning pipeline -> Evaluation harness
- **Critical path**: 1) Identify context-dependent examples using attention metric 2) Append indicator token to selected examples 3) Fine-tune model on modified dataset 4) Add indicator during inference 5) Verify improved context awareness
- **Design tradeoffs**: Indicator token vs. architectural changes (simpler but requires retraining); threshold selection (precision vs. recall); layer selection (efficiency vs. accuracy)
- **Failure signatures**: Indicator conflicts with vocabulary; threshold too high/low (misses examples or adds false positives); attention heads not representative
- **First 3 experiments**: 1) Replicate NIH performance drop with/without chat templates 2) Apply attention steering and measure NIH improvement 3) Compute context-dependency scores and analyze sparsity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does attention bias exist in larger or smaller models?
- Basis: Authors couldn't test extremely large models; bias may persist but untested
- Why unresolved: Computational resource limitations
- What evidence would resolve it: Systematic experiments across model sizes (1B to 70B+)

### Open Question 2
- Question: Can context-dependency metric be model-agnostic?
- Basis: Authors note metric is "model-dependent" because models with prior knowledge may not need context
- Why unresolved: Single seed model used; no exploration of different seed models
- What evidence would resolve it: Compare subsets and performance using multiple seed models

### Open Question 3
- Question: What's the long-term effect of indicator on instruction-following versatility?
- Basis: Method maintains performance on general tasks initially, but prolonged use might narrow adaptability
- Why unresolved: Only short-term performance evaluated
- What evidence would resolve it: Longitudinal studies tracking performance across diverse instruction types

## Limitations

- Architecture specificity: Results focus on decoder-only transformers with chat templates; may not generalize to other architectures
- Domain generalization: Method validated on QA tasks but not on domains like code generation or mathematical reasoning
- Inference dependency: Requires adding indicator token during inference, creating practical deployment challenges

## Confidence

- **High confidence**: Attention shift from user to assistant tokens with chat templates is well-supported by attention visualization across three models
- **Medium confidence**: Performance degradation correlates with bias from context-independent examples (indirect evidence)
- **Medium confidence**: Indicator token effectiveness demonstrated empirically but underlying mechanism is inferred

## Next Checks

1. **Ablation study on threshold sensitivity**: Systematically vary context-dependency threshold Î² (0.4, 0.5, 0.6, 0.7) to reveal optimal threshold and dataset specificity

2. **Cross-dataset generalization test**: Apply method to code generation or mathematical reasoning tasks to test generalization beyond QA benchmarks

3. **Attention visualization during inference**: Compare attention allocation with and without indicator token on same queries to directly validate hypothesized attention increase