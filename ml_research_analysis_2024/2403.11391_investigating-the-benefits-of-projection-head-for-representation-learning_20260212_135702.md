---
ver: rpa2
title: Investigating the Benefits of Projection Head for Representation Learning
arxiv_id: '2403.11391'
source_url: https://arxiv.org/abs/2403.11391
tags:
- learning
- representations
- features
- head
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical and empirical analysis of the projection
  head technique in contrastive learning. The authors show that using a projection
  head leads to layer-wise progressive feature weighting, where deeper layers assign
  increasingly unequal weights to features.
---

# Investigating the Benefits of Projection Head for Representation Learning

## Quick Facts
- arXiv ID: 2403.11391
- Source URL: https://arxiv.org/abs/2403.11391
- Reference count: 40
- Primary result: Projection heads create layer-wise progressive feature weighting, making lower layers' representations more transferable, especially under distribution shift.

## Executive Summary
This paper provides the first theoretical understanding of why projection heads are effective in contrastive learning. The authors demonstrate that projection heads induce layer-wise progressive feature weighting, where deeper layers assign increasingly unequal weights to features. This makes lower layers' representations more normalized and less specialized to the pretraining task, enhancing their transferability to downstream tasks. The work combines theoretical analysis with empirical validation on synthetic and real datasets, showing that pre-projection representations often outperform post-projection ones, particularly when pretraining data has a mismatch with downstream tasks.

## Method Summary
The paper analyzes projection heads through both theoretical and empirical approaches. Theoretically, it examines linear and non-linear two-layer models trained with contrastive loss, deriving conditions under which layer-wise progressive feature weighting occurs. Empirically, the authors validate their findings on synthetic data, MNIST-on-CIFAR10, CIFAR100 for coarse-to-fine transfer, UrbanCars for few-shot adaptation, and shifted versions of ImageNet. The method involves training models with projection heads, then evaluating both pre-projection and post-projection representations on downstream tasks through linear evaluation or fine-tuning.

## Key Results
- Linear models progressively weight features more unequally at deeper layers during contrastive learning
- Lower layers' representations are more transferable to downstream tasks, especially under data distribution mismatch
- Non-linear models allow lower layers to learn features absent in higher layers
- Using pre-projection representations can mitigate class collapse in supervised learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projection heads create layer-wise progressive feature weighting, making deeper layers' representations more specialized and lower layers' representations more transferable.
- Mechanism: Linear models trained with contrastive loss develop progressively unequal feature weights across layers. Lower layers maintain more normalized and less specialized representations due to implicit bias in training algorithms.
- Core assumption: Training algorithms implicitly bias toward solutions where feature weights become increasingly unequal with depth.
- Evidence anchors:
  - [abstract] "linear models progressively weight features more unequally at deeper layers"
  - [section] "The implicit bias of training algorithms leads to layer-wise progressive feature weighting, where features become increasingly unequal as we go deeper into the layers"
  - [corpus] Weak evidence; no corpus papers directly address progressive weighting in contrastive learning.

### Mechanism 2
- Claim: Non-linear models allow lower layers to learn features absent in higher layers, improving transferability.
- Mechanism: Non-linear activations enable lower layers to capture features that are suppressed or eliminated in post-projection representations. This allows pre-projection representations to contain useful information discarded by the projection head.
- Core assumption: Non-linearities in the network can completely eliminate certain features in higher layers while preserving them in lower layers.
- Evidence anchors:
  - [abstract] "Non-linear models allow lower layers to learn features absent in higher layers"
  - [section] "We demonstrate that introducing non-linearity into the network allows lower layers to learn features that are completely absent in higher layers"
  - [corpus] No direct corpus evidence for feature suppression by non-linearities in contrastive learning context.

### Mechanism 3
- Claim: Pre-projection representations mitigate class/neural collapse in supervised learning.
- Mechanism: Lower layers can learn subclass-level features that are not represented in the final layer, preventing representations within each class from becoming indistinguishable at a finer-grained level.
- Core assumption: Class collapse occurs when representations within each class become indistinguishable at subclass level, and lower layers retain subclass-level information.
- Evidence anchors:
  - [abstract] "lower layers can learn subclass-level features that are not represented in the final layer"
  - [section] "lower layers can learn subclass-level features not represented in the final layer in supervised learning, suggesting that using and then discarding the projection head can mitigate the issue of class/neural collapse"
  - [corpus] Weak evidence; no corpus papers directly address subclass-level feature learning in projection head context.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The paper's theoretical analysis and empirical validation center on contrastive learning and its use of projection heads.
  - Quick check question: What is the key difference between contrastive learning and non-contrastive self-supervised learning in terms of their loss functions and objectives?

- Concept: Implicit bias in optimization
  - Why needed here: The paper's main theoretical finding relies on understanding how training algorithms implicitly bias toward certain solutions (specifically, progressive feature weighting).
  - Quick check question: How does implicit bias in gradient-based optimization differ from explicit regularization, and why is it particularly relevant to understanding projection head benefits?

- Concept: Class collapse/neural collapse
  - Why needed here: The paper extends its analysis to supervised learning, where it demonstrates how projection heads can mitigate class/neural collapse by preserving subclass-level features.
  - Quick check question: What are the key characteristics of class collapse in supervised contrastive learning, and how does it differ from neural collapse in standard supervised learning?

## Architecture Onboarding

- Component map: Encoder -> Projection Head -> Pre-projection Representations -> Post-projection Representations
- Critical path:
  1. Pretrain encoder + projection head using contrastive loss
  2. Discard projection head after pretraining
  3. Use pre-projection representations for downstream tasks (linear evaluation or fine-tuning)
- Design tradeoffs:
  - Shallow vs. deep projection heads: Shallower heads preserve more information from lower layers but may provide less task-specific optimization
  - Fixed vs. trainable reweighting heads: Fixed heads offer interpretability and control but may not optimize as well for specific data distributions
  - Number of layers: Deeper encoders with projection heads show more pronounced progressive weighting effects
- Failure signatures:
  - Post-projection representations outperform pre-projection: May indicate that downstream task aligns well with pretraining objective, or that projection head is too shallow
  - Both pre- and post-projection perform poorly: Could indicate issues with pretraining data, augmentation strategy, or model architecture
  - Performance degrades with additional projection head layers: May suggest over-specialization or excessive information loss
- First 3 experiments:
  1. Train a two-layer linear model on synthetic data with varying feature strengths and augmentation levels; compare pre- and post-projection representations on downstream tasks
  2. Implement the MNIST-on-CIFAR10 experiment from the paper to validate the effect of data augmentation on feature weighting
  3. Replace the trainable projection head with a fixed reweighting head (e.g., [r₁, r₂/κ, ..., rₚ/κ]) and compare performance on a simple downstream classification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal depth for layer truncation when there is a mismatch between pretraining and downstream tasks?
- Basis in paper: [explicit] The paper discusses how lower layers provide more transferable representations when there is a mismatch between pretraining and downstream tasks, but determining the exact optimal layer is challenging.
- Why unresolved: The paper acknowledges that the optimal layer depends on various factors including the position of the downstream-relevant feature, feature strengths, and weights assigned during pretraining. These factors interact in complex ways that make analytical determination difficult.
- What evidence would resolve it: Empirical studies comparing performance across different truncation depths on diverse downstream tasks, along with theoretical analysis connecting these factors to optimal layer selection.

### Open Question 2
- Question: How does weight decay affect the benefits of using pre-projection representations in non-linear models?
- Basis in paper: [explicit] The paper shows that larger weight decay diminishes the benefits of pre-projection representations and can even turn them into a detriment, but notes that with reasonable weight decay the benefit remains evident.
- Why unresolved: The paper only provides a theoretical discussion and limited empirical validation (Figure 3d) showing the effect at one weight decay value. The precise relationship between weight decay magnitude and representation quality is not characterized.
- What evidence would resolve it: Systematic experiments varying weight decay across multiple orders of magnitude, measuring the gap between pre- and post-projection performance, and theoretical analysis of how weight decay affects feature learning in lower layers.

### Open Question 3
- Question: Would making the reweighting head's hyperparameter κ trainable improve performance compared to a fixed value?
- Basis in paper: [explicit] The paper introduces a fixed reweighting head as an alternative to the projection head and shows it achieves comparable performance, suggesting this as a direction for future work.
- Why unresolved: The paper only evaluates fixed reweighting heads and does not explore whether learning κ during training could yield better results by adapting to the specific data and task.
- What evidence would resolve it: Empirical comparison of fixed versus trainable κ across multiple datasets and tasks, measuring both final performance and the learned values of κ in different scenarios.

## Limitations
- Theoretical analysis focuses on linear models with two layers, which may not capture behavior of deeper, non-linear networks
- Synthetic data experiments use simplified feature distributions that may not represent real-world data complexity
- Limited exploration of how different projection head architectures affect progressive weighting

## Confidence
- High Confidence: Empirical observations showing pre-projection representations transfer better than post-projection representations, particularly under distribution shift
- Medium Confidence: Theoretical explanation of progressive feature weighting in linear models, though limited to two-layer architectures
- Medium Confidence: Extension to supervised learning and class collapse mitigation, supported by experiments but lacking extensive ablation studies

## Next Checks
1. Replicate the progressive weighting phenomenon in deeper networks (3+ layers) with varying non-linear activation functions to test the generality of the theoretical findings
2. Conduct controlled experiments varying projection head architecture (depth, width, activation functions) to quantify their impact on feature weighting and transferability
3. Test the pre-projection representation benefits on additional downstream tasks and datasets, particularly in cross-domain transfer scenarios, to validate the robustness of the findings