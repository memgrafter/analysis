---
ver: rpa2
title: Analysing the Residual Stream of Language Models Under Knowledge Conflicts
arxiv_id: '2410.16090'
source_url: https://arxiv.org/abs/2410.16090
tags:
- knowledge
- layer
- residual
- stream
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how large language models (LLMs) detect\
  \ and resolve conflicts between their parametric knowledge and contextual information.\
  \ The authors analyze the residual stream\u2014the intermediate activations in transformer\
  \ models\u2014to determine whether LLMs can internally register knowledge conflicts\
  \ and which knowledge source they will rely on."
---

# Analysing the Residual Stream of Language Models Under Knowledge Conflicts

## Quick Facts
- arXiv ID: 2410.16090
- Source URL: https://arxiv.org/abs/2410.16090
- Reference count: 40
- Primary result: LLMs register knowledge conflict signals in residual streams at layers 13-14, with distinct skewness patterns indicating which knowledge source (contextual vs parametric) will be used

## Executive Summary
This paper investigates how large language models resolve conflicts between their stored parametric knowledge and contextual information by analyzing the residual stream—the intermediate activations in transformer models. The authors develop a probing methodology to detect conflict signals and predict which knowledge source the model will ultimately use. They find that conflict detection occurs around layers 13-14 with 90% accuracy, and knowledge source prediction peaks at layer 17, after conflict detection but before answer generation. The study reveals that residual streams exhibit significantly different skewness patterns depending on whether the model uses contextual or parametric knowledge, particularly from layers 17-30.

## Method Summary
The methodology involves probing residual stream activations at different transformer layers using logistic regression classifiers to detect knowledge conflict signals and predict knowledge source selection. The authors examine hidden states, MLP activations, and attention activations across Llama3-8B and Llama2-7B models on three conflict datasets (NQSwap, Macnoise, ConflictQA). For conflict detection, they train probes on activations from conflicting instances versus non-conflicting instances. For knowledge source prediction, they compare residual stream patterns between instances where the model ultimately uses contextual knowledge versus parametric knowledge. Skewness analysis using Kurtosis, Hoyer, and Gini indices quantifies distributional differences in the residual stream across layers.

## Key Results
- LLMs register knowledge conflict signals in residual streams with 90% accuracy, peaking at layers 13-14
- Distinct skewness patterns emerge in residual streams when models use contextual versus parametric knowledge, with contextual knowledge producing significantly more skewed distributions from layers 17-30
- Knowledge source prediction accuracy reaches peak performance around layer 17, occurring after conflict detection but before answer generation
- The probing methodology successfully generalizes across three different conflict datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs internally register knowledge conflict signals in the residual stream that can be detected through probing
- Mechanism: The model processes conflicting information between parametric knowledge and contextual evidence, creating distinguishable activation patterns in intermediate layers that encode the conflict state
- Core assumption: The residual stream contains sufficient information about internal processing states to be probed for specific knowledge conflict signals
- Evidence anchors:
  - [abstract] "LLMs can internally register the signal of knowledge conflict in the residual stream, which can be accurately detected by probing the intermediate model activations"
  - [section 4] "the signal of knowledge conflict rises from the intermediate layers (e.g., the 13th layer of Llama3-8B)"
  - [corpus] Weak - neighbors discuss conflict detection but don't specifically address residual stream probing methodology

### Mechanism 2
- Claim: Different skewness patterns in the residual stream indicate which knowledge source (contextual vs parametric) the model will use to resolve conflicts
- Mechanism: When processing conflicting information, the model's residual stream exhibits distinct distributional characteristics depending on whether it ultimately relies on contextual or parametric knowledge, with contextual knowledge producing more skewed distributions
- Core assumption: The choice between knowledge sources creates measurable distributional differences in the residual stream that persist through intermediate layers
- Evidence anchors:
  - [abstract] "the residual stream shows significantly different patterns when the model relies on contextual knowledge versus parametric knowledge to resolve conflicts"
  - [section 4] "when the model uses contextual knowledge, the residual stream exhibits a significantly skewed distribution compared with when it uses its parametric knowledge"
  - [corpus] Weak - neighbors discuss knowledge selection but don't specifically address skewness-based detection

### Mechanism 3
- Claim: Conflict detection precedes knowledge source selection in the model's processing hierarchy
- Mechanism: The model first identifies that a conflict exists in early-to-mid layers (around layer 13-14), then makes a decision about which knowledge source to use in later layers (around layer 17)
- Core assumption: The model processes conflict detection as a separate stage from the actual decision of which knowledge to use, with temporal ordering reflected in layer progression
- Evidence anchors:
  - [section 4] "the probing model's performance gradually improves from the first layer to the 16th layer, which occurs after the signal of knowledge conflict has already reached its peak at the 13th and 14th layers"
  - [section 4] "This observation suggests that the decision of which knowledge to use occurs after the detection of the knowledge conflict signal"
  - [corpus] Weak - neighbors don't discuss temporal ordering of conflict processing stages

## Foundational Learning

- Concept: Residual stream analysis in transformers
  - Why needed here: The entire methodology relies on understanding how information flows through transformer layers via residual connections
  - Quick check question: What is the difference between hidden states, MLP activations, and attention activations in the residual stream framework?

- Concept: Linear probing for information detection
  - Why needed here: The conflict detection and knowledge source prediction both use logistic regression probes on residual stream activations
  - Quick check question: How does logistic regression probing differ from fine-tuning when analyzing model internals?

- Concept: Knowledge conflict benchmarking
  - Why needed here: Understanding the NQSwap, Macnoise, and ConflictQA datasets is crucial for interpreting results and designing new experiments
  - Quick check question: What distinguishes parametric knowledge from contextual knowledge in the conflict detection setup?

## Architecture Onboarding

- Component map:
  - Input processing: Questions and evidence pairs create conflict scenarios
  - Residual stream layers: Hidden states, MLP activations, and attention activations at each layer
  - Probing layer: Logistic regression classifiers applied to specific positions in the residual stream
  - Output analysis: Accuracy, AUROC, and AUPRC metrics for conflict detection and knowledge source prediction

- Critical path: Conflict signal detection (layers 8-14) → Knowledge source prediction (layers 14-17) → Answer generation
- Design tradeoffs: Probing accuracy vs. computational overhead; specificity of conflict detection vs. generalizability across datasets
- Failure signatures: Random probing accuracy suggests missing conflict signal; low AUROC indicates poor discriminative power; similar skewness patterns suggest indistinguishable knowledge source selection
- First 3 experiments:
  1. Replicate conflict detection probing on a new layer range to verify the 13-14 layer peak
  2. Test skewness pattern detection using different statistical measures (entropy, KL divergence)
  3. Apply the probing methodology to a different model architecture to test generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanism allows the residual stream to exhibit distinct skewness patterns when using contextual versus parametric knowledge?
- Basis in paper: [inferred] The paper observes that residual streams show significantly different skewness patterns when models use contextual knowledge versus parametric knowledge, particularly from layers 17-30, but does not explain the underlying mechanism.
- Why unresolved: The paper identifies the phenomenon but doesn't investigate what causes the skewness differences at a mechanistic level or what circuit-level operations create these distinct patterns.
- What evidence would resolve it: Detailed mechanistic analysis of attention and MLP operations that create these skewness patterns, possibly through circuit analysis or feature attribution methods.

### Open Question 2
- Question: Can the skewness pattern differences be leveraged to actively steer knowledge selection rather than just predict it?
- Basis in paper: [explicit] The authors mention this work is preliminary to "Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering" suggesting this is an open direction.
- Why unresolved: The paper only demonstrates detection and prediction of knowledge source selection, not active intervention or control over the selection process.
- What evidence would resolve it: Experimental demonstration of successfully steering knowledge selection by manipulating the residual stream skewness patterns, with quantitative evaluation of intervention effectiveness.

### Open Question 3
- Question: Do the observed patterns generalize across different model architectures and scales beyond Llama2-7B and Llama3-8B?
- Basis in paper: [inferred] The paper tests on multiple datasets but only two model architectures, leaving open whether these patterns are universal or architecture-specific.
- Why unresolved: The analysis is limited to specific Llama model variants, and the paper doesn't explore whether similar patterns exist in other transformer architectures (GPT, Mistral, etc.) or different model scales.
- What evidence would resolve it: Systematic analysis of residual stream patterns across diverse model architectures, sizes, and training methodologies to establish generalizability.

## Limitations

- Dataset Specificity: The conflict detection and knowledge selection patterns may be specific to the NQSwap, Macnoise, and ConflictQA datasets, which are synthetically constructed conflict scenarios that may not generalize to real-world applications.
- Model Architecture Dependence: The residual stream patterns and layer-specific behaviors could vary significantly across different model families, with the 13-14 layer conflict detection peak and 17-layer knowledge selection peak potentially not translating to other architectures.
- Statistical Method Sensitivity: The skewness-based analysis relies on specific statistical measures that may be sensitive to activation normalization, token position selection, and implementation details.

## Confidence

**High Confidence**: The conflict detection mechanism through residual stream probing is well-supported, with consistent 90% accuracy across multiple runs and datasets. The temporal ordering finding (conflict detection before knowledge selection) is robust given the layer progression analysis.

**Medium Confidence**: The skewness pattern differences between contextual and parametric knowledge selection show statistical significance but require careful interpretation. The magnitude of differences and their practical implications for controlling knowledge selection remain somewhat uncertain.

**Low Confidence**: Generalizability of the specific layer numbers (13-14 for detection, 17 for selection) across different model architectures and the causal relationship between observed patterns and actual decision-making processes.

## Next Checks

1. **Cross-Dataset Validation**: Test the conflict detection and knowledge selection probing methodology on naturally occurring knowledge conflicts from real-world question-answering datasets (e.g., open-domain QA with retrieved documents) to verify that the residual stream patterns are not dataset artifacts.

2. **Architecture Transferability Test**: Apply the same probing methodology to different transformer architectures (e.g., GPT-3, Mistral, or open-source alternatives) to determine whether the layer-specific patterns (detection at 13-14, selection at 17) are architecture-dependent or universal features of transformer-based conflict resolution.

3. **Intervention Experiment**: Design experiments that manipulate the residual stream at key layers (13-14 for conflict detection, 17 for knowledge selection) to test whether the observed patterns are causally related to conflict resolution decisions, rather than just correlational signals.