---
ver: rpa2
title: Asymptotics of Learning with Deep Structured (Random) Features
arxiv_id: '2402.13999'
source_url: https://arxiv.org/abs/2402.13999
tags:
- random
- feature
- learning
- networks
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a rigorous asymptotic characterization of the
  test error for ridge regression using Lipschitz-continuous feature maps, in the
  high-dimensional regime where input dimension, hidden layer widths, and training
  samples grow proportionally. The characterization is expressed in terms of the population
  covariance of the features.
---

# Asymptotics of Learning with Deep Structured (Random) Features
## Quick Facts
- arXiv ID: 2402.13999
- Source URL: https://arxiv.org/abs/2402.13999
- Authors: Dominik Schröder; Daniil Dmitriev; Hugo Cui; Bruno Loureiro
- Reference count: 40
- Primary result: Tight asymptotic characterization of test error for ridge regression with Lipschitz-continuous feature maps in high-dimensional regime

## Executive Summary
This paper provides a rigorous asymptotic characterization of the test error for ridge regression using Lipschitz-continuous feature maps in the high-dimensional regime where input dimension, hidden layer widths, and training samples grow proportionally. The authors focus on Gaussian rainbow neural networks—deep, non-linear networks with structured random weights—and derive a closed-form formula for the feature covariance in terms of the weight matrices. The results extend previous work on unstructured random weights to the case of structured (and potentially correlated) weights, providing a theoretical foundation for understanding the generalization behavior of these models.

## Method Summary
The authors study ridge regression with structured random features in the proportional high-dimensional regime. They derive deterministic equivalents for resolvent matrices using anisotropic Marchenko-Pastur laws and multi-resolvent techniques, replacing sample-dependent terms with population-level quantities. For Gaussian rainbow networks, they propose a linearization approach to approximate population covariances of deep structured feature maps, expressing them as functions of weight matrices and activation statistics. The theory is validated on synthetic and real data, comparing theoretical predictions with empirical test errors from trained networks in the lazy regime.

## Key Results
- Tight asymptotic characterization of test error for ridge regression with Lipschitz-continuous feature maps in high-dimensional regime
- Closed-form formula for feature covariance of Gaussian rainbow neural networks expressed in terms of weight matrices
- Extension of unstructured random feature asymptotics to structured (correlated) weights
- Theoretical predictions capture learning curves of some networks trained by gradient descent in the lazy regime

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In the high-dimensional proportional regime, the generalization error of ridge regression with structured random features can be tightly characterized by deterministic equivalents involving population covariances.
- Mechanism: The authors derive anisotropic Marchenko-Pastur laws and multi-resolvent deterministic equivalents to approximate complex expressions in the generalization error formula, replacing sample-dependent terms with population-level quantities.
- Core assumption: The feature maps are Lipschitz-continuous and the data-feature and label-feature covariances have bounded spectral norms.
- Evidence anchors:
  - [abstract]: "For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer..."
  - [section 3]: Derivation of anisotropic Marchenko-Pastur law and multi-resolvent equivalents in Theorem 3.3 and Proposition A.4
  - [corpus]: Weak related work on unstructured RF asymptotics (e.g., [10, 12, 13]) suggests this mechanism extends prior methods
- Break Condition: If the feature maps are not Lipschitz or the covariance bounds fail, the concentration estimates break down and the deterministic equivalents no longer hold.

### Mechanism 2
- Claim: Gaussian rainbow networks with structured but correlated weights can be linearized into equivalent stochastic linear models with closed-form covariance expressions.
- Mechanism: The population covariances of deep structured feature maps are recursively approximated by linearized covariances that depend only on products of weight matrices and activation statistics, capturing inter-layer correlations.
- Core assumption: Rows of weight matrices are sub-Gaussian with bounded covariances and asymptotic orthogonality.
- Evidence anchors:
  - [section 4]: Definition 4.2 and Conjecture 4.3 state the linearization and its validity
  - [section 4.1]: Proof of Theorem 4.4 for two-layer case using Wiener chaos and Stein's method
  - [corpus]: Related linearization work for unstructured RFs ([12, 13, 19]) supports this approach but assumes independence
- Break Condition: If weight correlations violate the sub-Gaussianity or orthogonality assumptions, the linearization error grows and the closed-form approximation fails.

### Mechanism 3
- Claim: Trained finite-width neural networks in the lazy regime behave like linearized stochastic models, allowing the theory to predict their test error when retraining the readout.
- Mechanism: The trained network's effective feature map is approximated as a stochastic linear map with effective weights and noise covariance derived from the product of trained weight matrices and layer-wise statistics.
- Core assumption: The network is in the lazy regime where the feature map is effectively linear and the linearization holds.
- Evidence anchors:
  - [section 5]: Discussion of effective linear model and its comparison to trained networks in Figures 2 and 3
  - [section 5]: Mention of lazy training regime and inductive bias encoding
  - [corpus]: Related work on lazy training and linearization ([26–29, 50]) supports this regime interpretation
- Break Condition: If the network leaves the lazy regime or has strong non-linear feature evolution, the linearization approximation fails and the test error prediction diverges.

## Foundational Learning

- Concept: High-dimensional random matrix theory (e.g., Marchenko-Pastur law)
  - Why needed here: The core asymptotic analysis relies on deterministic equivalents of resolvents and sample covariance matrices in proportional high-dimensional limits.
  - Quick check question: What is the Stieltjes transform of the Marchenko-Pastur distribution and how does it relate to the self-consistent equation for the resolvent?

- Concept: Concentration inequalities for Lipschitz functions of Gaussian vectors
  - Why needed here: To control fluctuations of quadratic forms and resolvent traces, ensuring the deterministic equivalents hold with high probability.
  - Quick check question: How does the Hanson-Wright inequality bound the deviation of quadratic forms for sub-Gaussian vectors?

- Concept: Wiener chaos expansion and Malliavin calculus
  - Why needed here: Used to linearize the population covariances of deep structured feature maps by decomposing nonlinear functions into uncorrelated chaos components.
  - Quick check question: How does the p-th Wiener chaos represent nonlinear functions of Gaussian vectors and why does truncation yield a linearization?

## Architecture Onboarding

- Component map:
  - Feature map φ(x): Deep structured (rainbow) network with random weights and inter-layer correlations
  - Ridge regression on readout weights: Closed-form solution using resolvent G
  - Asymptotic analysis: Deterministic equivalents for G and multi-resolvent expressions
  - Linearization: Recursive approximation of population covariances for random features
  - Empirical validation: Synthetic and real data experiments comparing theory to test error

- Critical path:
  1. Define feature map and assumptions (Lipschitz, covariance bounds)
  2. Derive anisotropic Marchenko-Pastur law for G
  3. Extend to multi-resolvent expressions (GΩG, etc.)
  4. Linearize population covariances for structured random features
  5. Combine into test error formula
  6. Validate on synthetic/real data

- Design tradeoffs:
  - Structured vs unstructured weights: Structured allows richer models but requires more complex linearization
  - Depth of linearization: Full L-layer linearization is tedious; two-layer case is rigorous
  - Lazy regime assumption: Enables linearization but limits applicability to non-lazy training

- Failure signatures:
  - Generalization error deviates from theory when feature maps are not Lipschitz or covariances unbounded
  - Linearized covariance approximation fails when weight correlations violate sub-Gaussianity or orthogonality
  - Test error prediction diverges when network leaves lazy regime or has strong non-linear evolution

- First 3 experiments:
  1. Synthetic data with known covariance structure: Compare empirical test error to theoretical prediction across sample sizes
  2. Two-layer rainbow network: Validate linearization accuracy by comparing empirical vs linearized population covariances
  3. Lazy regime training: Train shallow network, retrain readout, compare test error to effective linear model prediction

## Open Questions the Paper Calls Out
None

## Limitations
- The linearization of population covariances for deep structured feature maps remains a conjecture (Conjecture 4.3) for general rainbow networks, with only the two-layer case rigorously proven
- The theory relies on bounded spectral norms of covariances and Lipschitz continuity of feature maps, which may not be satisfied in all practical scenarios
- The numerical experiments are limited to specific architectures and datasets, and theoretical predictions are computationally intensive for large feature dimensions

## Confidence
- **High Confidence**: The derivation of anisotropic Marchenko-Pastur laws and multi-resolvent deterministic equivalents (Mechanism 1) - this follows standard random matrix theory techniques and is rigorously proven
- **Medium Confidence**: The linearization of population covariances for structured random features (Mechanism 2) - the two-layer case is proven, but the general deep case remains a conjecture
- **Medium Confidence**: The application to trained networks in the lazy regime (Mechanism 3) - this is supported by empirical results but relies on the assumption that the network remains in the lazy regime

## Next Checks
1. **Numerical stability check**: Test the theoretical prediction formula on synthetic data with varying dimensions and sample sizes to identify regimes where numerical instability occurs and assess the need for approximation methods
2. **Lazy regime boundary**: Systematically vary network width and training duration to map out when trained networks transition out of the lazy regime and observe how this affects the agreement between empirical test error and theoretical predictions
3. **Covariance linearization accuracy**: For rainbow networks of varying depths, compare the empirical population covariances with the linearized approximations to quantify the error introduced by the linearization and validate Conjecture 4.3