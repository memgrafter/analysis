---
ver: rpa2
title: Lookahead Counterfactual Fairness
arxiv_id: '2412.01065'
source_url: https://arxiv.org/abs/2412.01065
tags:
- counterfactual
- fairness
- learning
- have
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of ensuring fairness in machine
  learning predictions while accounting for downstream effects on individuals' future
  outcomes. The authors introduce "lookahead counterfactual fairness" (LCF), a novel
  fairness notion that evaluates counterfactual fairness over individuals' future
  statuses rather than just current predictions.
---

# Lookahead Counterfactual Fairness

## Quick Facts
- arXiv ID: 2412.01065
- Source URL: https://arxiv.org/abs/2412.01065
- Reference count: 40
- One-line primary result: Lookahead counterfactual fairness (LCF) improves fairness in future outcomes while maintaining prediction accuracy

## Executive Summary
This paper introduces lookahead counterfactual fairness (LCF), a novel fairness notion that evaluates counterfactual fairness over individuals' future statuses rather than just current predictions. The method addresses the problem of ensuring fairness in machine learning predictions while accounting for downstream effects on individuals' future outcomes. The authors theoretically identify conditions under which LCF can be satisfied and propose an algorithm for training ML models under LCF. Results show that their approach can significantly improve fairness in terms of future outcomes (with AFCE improvements up to 100% and UIR improvements up to 88.6%) while maintaining comparable prediction accuracy to baseline methods.

## Method Summary
The LCF approach modifies predictions based on counterfactual variables and individual responses, adjusting the prediction function to minimize disparity between factual and counterfactual future statuses. The method assumes individuals strategically adapt their features toward the gradient of the decision function in response to ML predictions. The predictor takes the form g(ˇY,U) = p1ˇY² + p2ˇY + p3 + h(U), where parameters are set to satisfy theoretical conditions for LCF. The algorithm involves estimating structural equations from training data, generating counterfactual variables through sampling, constructing predictors that satisfy LCF conditions, and optimizing parameters through training.

## Key Results
- AFCE improvements up to 100% on synthetic datasets compared to baseline methods
- UIR improvements up to 88.6% on real-world datasets (Law School Admissions)
- Maintained comparable prediction accuracy (MSE) to baseline methods while achieving fairness gains
- Path-dependent LCF extension demonstrated effectiveness across different scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LCF enforces counterfactual fairness over individuals' future statuses, ensuring fair downstream outcomes
- Mechanism: The algorithm modifies predictions based on counterfactual variables and individual responses, adjusting the prediction function to minimize disparity between factual and counterfactual future statuses
- Core assumption: Individuals strategically adapt their features toward the gradient of the decision function in response to ML predictions
- Evidence anchors:
  - [abstract]: "Lookahead counterfactual fairness (LCF), a fairness notion accounting for the downstream effects of ML models which requires the individual future status to be counterfactually fair."
  - [section]: "LCF takes one step further by enforcing the individual future status (after responding to ML predictions) to be the same."
- Break condition: If individual responses do not follow the assumed gradient-based strategy, or if the structural causal model is non-linear and cannot be captured by the proposed method

### Mechanism 2
- Claim: The predictor form g(ˇY,U) = p1ˇY² + p2ˇY + p3 + h(U) ensures LCF by balancing prediction accuracy and fairness through the parameter p1
- Mechanism: The predictor uses counterfactual random variables and unobservable variables to construct a function that minimizes the disparity between factual and counterfactual future outcomes when p1 is set appropriately
- Core assumption: The causal model is linear or has specific properties that allow construction of a predictor satisfying LCF
- Evidence anchors:
  - [section]: "The following predictor satisfies LCF, g(ˇY,U) = p1ˇY² + p2ˇY + p3 + h(U), where p1 = T/2 with T := 1/(η(||w⊙α||² +γ²))."
- Break condition: If the causal model is non-linear and cannot satisfy the conditions in Theorems 5.2 or 5.3, or if p1 is not properly tuned

### Mechanism 3
- Claim: Path-dependent LCF extends the fairness notion to only consider unfairness along certain causal paths, providing more granular control over fairness constraints
- Mechanism: The predictor is modified to use path-dependent counterfactual random variables, focusing on specific feature sets that contribute to unfairness along identified paths
- Core assumption: Unfair paths can be identified and separated from fair paths in the causal graph
- Evidence anchors:
  - [section]: "We say an ML model satisfies path-dependent lookahead counterfactual fairness w.r.t. the unfair path setPGA if the following holds∀a, ˇa∈A,X ∈X,y∈Y."
- Break condition: If unfair paths cannot be accurately identified or if the separation of features along fair/unfair paths is not possible

## Foundational Learning

- Concept: Counterfactual fairness
  - Why needed here: LCF builds upon counterfactual fairness by extending it to future outcomes, so understanding the original concept is essential
  - Quick check question: What is the key difference between observational and counterfactual fairness?

- Concept: Structural causal models (SCM)
  - Why needed here: The method relies on SCM to model the causal relationships between variables and to perform counterfactual inference
  - Quick check question: How do structural equations in an SCM differ from standard regression equations?

- Concept: Individual response models
  - Why needed here: The algorithm assumes individuals respond to ML predictions by strategically changing their features, which is central to the LCF framework
  - Quick check question: What are the implications of assuming individuals move features toward the gradient of the decision function?

## Architecture Onboarding

- Component map: Data preprocessing -> Structural Equation Estimation -> Counterfactual Generation -> Predictor Construction -> Training -> Evaluation
- Critical path: Data → Structural Equation Estimation → Counterfactual Generation → Predictor Construction → Training → Evaluation
- Design tradeoffs:
  - Accuracy vs. fairness: Balancing prediction performance with LCF satisfaction through parameter p1
  - Computational complexity: Sampling from conditional distributions and counterfactual generation can be expensive
  - Model flexibility: Linear causal models are easier to handle but may not capture complex relationships
- Failure signatures:
  - High MSE with low AFCE improvement: Predictor may be too constrained by LCF
  - Low MSE with high AFCE: Predictor may not be satisfying LCF conditions
  - Unstable training: Issues with sampling or optimization
- First 3 experiments:
  1. Validate counterfactual generation: Check that counterfactual variables are correctly generated from the estimated structural equations
  2. Test predictor construction: Verify that the predictor satisfies the theoretical conditions for LCF
  3. Evaluate trade-off: Measure the accuracy-fairness trade-off by varying parameter p1 and observing changes in MSE and AFCE

## Open Questions the Paper Calls Out
- How do LCF methods perform in real-world deployment with continuously evolving human behaviors and feedback loops?
- What are the computational trade-offs between perfect LCF (p1 = T/2) and relaxed LCF (0 < p1 < T/2) in high-dimensional feature spaces?
- How robust is LCF to misspecification of the causal model structure and response functions?

## Limitations
- The approach assumes individuals respond strategically to predictions by following the gradient of the decision function, which may not hold in real-world scenarios
- Theoretical guarantees rely on linear causal models, and extension to non-linear cases remains an open question
- Computational complexity of counterfactual generation and sampling could limit scalability to large datasets or high-dimensional feature spaces

## Confidence
- **High**: The theoretical framework for LCF and its mathematical conditions (Theorems 5.1-5.3) appear sound and well-constructed
- **Medium**: The experimental results on synthetic data demonstrating the accuracy-fairness trade-off
- **Medium**: The extension to path-dependent fairness and its theoretical properties
- **Low**: The real-world dataset results, particularly for the loan prediction task where the structural equations are less specified

## Next Checks
1. Sensitivity Analysis: Test the robustness of LCF predictions to different individual response models beyond the assumed gradient-based strategy
2. Non-linear Extension: Implement a proof-of-concept extension to handle non-linear causal relationships and evaluate performance degradation
3. Scalability Test: Benchmark the computational requirements and training time on progressively larger datasets to identify practical scaling limits