---
ver: rpa2
title: Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification
  for OOD Detection
arxiv_id: '2403.01076'
source_url: https://arxiv.org/abs/2403.01076
tags:
- samples
- ignored
- noise
- blur
- technique
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a technique to extract usable predictions from
  quantized deep learning models by quantifying uncertainty and filtering out-of-distribution
  (OOD) samples. The method applies Monte Carlo dropout to a quantized model, computes
  confidence intervals for predictions, and ignores samples with uncertain outputs.
---

# Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification for OOD Detection

## Quick Facts
- **arXiv ID**: 2403.01076
- **Source URL**: https://arxiv.org/abs/2403.01076
- **Reference count**: 16
- **Primary result**: Improves F1-scores while reducing misclassifications by up to 80% on ignored samples through uncertainty quantification on quantized models

## Executive Summary
This paper proposes a technique to extract usable predictions from quantized deep learning models by quantifying uncertainty and filtering out-of-distribution (OOD) samples. The method applies Monte Carlo dropout to a quantized model, computes confidence intervals for predictions, and ignores samples with uncertain outputs. Applied to ResNet50 and EfficientNet-B0 on CIFAR-100 and CIFAR-100C datasets, the technique improves F1-scores while reducing misclassifications by up to 80% on ignored samples. It also achieves ~4x model size reduction through quantization. The main drawback is increased inference time (from ~200ms to ~7s).

## Method Summary
The method fine-tunes pre-trained ResNet50 and EfficientNet-B0 models on CIFAR-100, applies post-training integer quantization to reduce model size by approximately 4x, and implements Monte Carlo dropout during inference to estimate prediction uncertainty. For each sample, multiple forward passes with different dropout masks generate prediction distributions, from which confidence intervals are calculated. Samples with confidence intervals crossing a threshold are marked as uncertain and potentially ignored. The approach uses sigmoid-based final layers instead of softmax to enable per-class uncertainty quantification.

## Key Results
- Achieves ~4x model size reduction through post-training integer quantization
- Improves F1-scores while reducing misclassifications by up to 80% on ignored samples
- Demonstrates effectiveness on both ResNet50 and EfficientNet-B0 architectures
- Increases inference time from ~200ms to ~7s due to Monte Carlo dropout

## Why This Works (Mechanism)

### Mechanism 1
Monte Carlo dropout during inference approximates Bayesian posterior uncertainty by generating multiple predictions from the same model with different dropout masks. Each forward pass randomly drops a subset of weights based on dropout probability, producing slightly different outputs. The variation across these outputs reflects the model's uncertainty. This assumes dropout sampling approximates sampling from a posterior distribution over weights, as mathematically proven for certain network architectures.

### Mechanism 2
Confidence intervals calculated from MC dropout outputs can distinguish between confident in-distribution predictions and uncertain out-of-distribution samples. For each class, the mean and standard deviation of predictions across MC iterations define a confidence interval. If this interval lies entirely above a threshold, the prediction is accepted; if it crosses the threshold, the sample is marked uncertain and potentially ignored. This relies on the assumption that prediction distributions across MC iterations follow a Gaussian distribution.

### Mechanism 3
Quantization reduces model size by approximately 4x while maintaining reasonable F1-score performance. Post-training integer quantization converts 32-bit floating-point weights and activations to 8-bit integers, significantly reducing memory footprint with minimal accuracy degradation. This assumes the quantization process preserves the essential decision boundaries of the network.

## Foundational Learning

- **Bayesian Neural Networks and dropout connection**: Understanding why dropout at inference approximates Bayesian inference is crucial for trusting the uncertainty estimates. Quick check: What mathematical relationship did Gal et al. establish between dropout and Bayesian neural networks?

- **Confidence interval construction and interpretation**: The core filtering mechanism relies on proper confidence interval calculation and threshold selection. Quick check: How does the z-score value relate to the confidence level, and why is this important for the conf factor hyperparameter?

- **Quantization techniques and impact**: Understanding the trade-offs between model size reduction and accuracy preservation is essential for evaluating the quantization approach. Quick check: What types of quantization (symmetric vs asymmetric, per-tensor vs per-channel) might affect the 4x size reduction claim?

## Architecture Onboarding

- **Component map**: Pre-trained model (ResNet50/EfficientNet-B0) → Fine-tuning layer replacement → Post-training quantization → MC dropout inference → Confidence interval calculation → OOD filtering
- **Critical path**: 1) Load pre-trained model 2) Replace final classification layers with sigmoid-based architecture 3) Fine-tune on CIFAR-100 4) Apply post-training quantization 5) For inference: run MC dropout with N iterations 6) Calculate mean and std for each class 7) Construct confidence intervals 8) Apply filtering rules
- **Design tradeoffs**: Number of MC iterations vs inference time (7s vs 200ms); Confidence threshold vs number of ignored samples; Quantization precision vs model size reduction; Dropout rate vs uncertainty estimation quality
- **Failure signatures**: High number of ignored samples with low misclassification rate → overly conservative threshold; Low number of ignored samples with high misclassification rate → insufficient uncertainty estimation; Significant F1-score drop after quantization → aggressive quantization; Inconsistent uncertainty estimates across similar samples → inadequate MC iterations
- **First 3 experiments**: 1) Verify MC dropout produces varying outputs by running 10 iterations and checking prediction variance 2) Test confidence interval calculation by manually computing intervals for known Gaussian distributions 3) Validate quantization by comparing F1-scores before and after quantization on a small validation set

## Open Questions the Paper Calls Out

1. **Bayesian Neural Networks vs frequentist uncertainty estimation**: How would replacing frequentist uncertainty estimation with Bayesian Neural Networks (BNNs) impact the accuracy and computational feasibility of the UQ technique? The paper acknowledges BNNs could improve accuracy but does not explore them due to computational constraints.

2. **Inference time optimization**: Can the inference time of the UQ technique be reduced by optimizing the dropout process, such as caching static computations and only recomputing dynamic weights? The paper identifies this optimization opportunity but does not implement or benchmark it.

3. **Scalability to larger datasets**: How does the performance of the UQ technique scale with larger, more complex datasets beyond CIFAR-100 and CIFAR-100C? The experiments are limited to CIFAR datasets, suggesting potential uncertainty about generalizability.

## Limitations

- The Gaussian assumption for prediction distributions across MC dropout iterations may not hold for complex models or highly multimodal prediction spaces
- The confidence interval threshold selection appears heuristic without theoretical justification for the specific values used (0.7, 0.8, 0.9)
- The 4x size reduction claim through quantization needs verification across different model architectures and datasets

## Confidence

- **High Confidence**: The core mechanism of using MC dropout for uncertainty estimation is well-established in the literature
- **Medium Confidence**: The specific implementation details for confidence interval calculation and filtering rules are clear, but effectiveness depends heavily on threshold selection
- **Low Confidence**: The empirical validation is limited to two model architectures on CIFAR datasets

## Next Checks

1. **Distribution Validation**: Run statistical tests (e.g., Shapiro-Wilk) on prediction distributions across MC iterations to verify the Gaussian assumption for both in-distribution and OOD samples

2. **Threshold Sensitivity Analysis**: Systematically vary confidence thresholds and analyze the trade-off between number of ignored samples and misclassification rate across multiple OOD datasets (not just CIFAR-100C)

3. **Quantization Robustness Test**: Apply the same quantization and uncertainty quantification pipeline to different model architectures (e.g., Vision Transformers, MobileNet) and datasets to verify the generalizability of the 4x size reduction claim