---
ver: rpa2
title: Assessing Political Bias in Large Language Models
arxiv_id: '2405.13041'
source_url: https://arxiv.org/abs/2405.13041
tags:
- political
- llms
- parties
- should
- german
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates political bias in Large Language Models\
  \ (LLMs) using the German Wahl-O-Mat, a voting advice tool. We evaluate 7 open-source\
  \ LLMs on 38 political statements from a German voter\u2019s perspective."
---

# Assessing Political Bias in Large Language Models

## Quick Facts
- arXiv ID: 2405.13041
- Source URL: https://arxiv.org/abs/2405.13041
- Authors: Luca Rettenberger; Markus Reischl; Mark Schutera
- Reference count: 40
- Primary result: LLMs exhibit consistent political biases, with larger models showing stronger left-leaning alignment in German, while smaller models are more neutral in English

## Executive Summary
This study investigates political bias in Large Language Models (LLMs) using the German Wahl-O-Mat voting advice tool to evaluate alignment with 14 German political parties. The researchers tested 7 open-source LLMs on 38 political statements from a German voter's perspective in both German and English. The key finding is that LLMs exhibit consistent political biases, with larger models like Llama3-70B showing stronger alignment with left-leaning parties, particularly in German. Smaller models tend to be more neutral, especially when prompted in English, and all models showed low alignment with the far-right AfD party.

## Method Summary
The study evaluated 7 open-source LLMs (Llama2-7B, Llama2-70B, Llama3-8B, Llama3-70B, Mistral7B) on 38 political statements from the German Wahl-O-Mat for the 2024 European Parliament elections. LLMs were prompted in both German and English with system/user prompts requesting single-word responses (agree/neutral/disagree). The researchers used the Wahl-O-Mat interface to calculate alignment percentages between model responses and party positions, measuring the degree of political bias as percentage agreement with each of the 14 German political parties.

## Key Results
- Larger LLMs (Llama3-70B) show stronger alignment with left-leaning parties than smaller models
- English prompts produce more neutral responses compared to German prompts
- All models consistently show low alignment with the far-right AfD party across both languages
- Standard deviation in party alignment was low (±5.18% in English, ±7.40% in German), indicating persistent bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger LLMs show more consistent and pronounced political alignment than smaller models
- Mechanism: Increased model capacity allows better capture of nuanced patterns in training data, leading to stronger ideological stances
- Core assumption: Training data contains sufficient political discourse to shape model biases proportionally to model size
- Evidence anchors:
  - [abstract] "larger models, such as Llama3-70B, tend to align more closely with left-leaning political parties"
  - [section] "The larger LLama2-70B model shows similar tendencies for the German case"
  - [corpus] Weak evidence - no direct corpus support for model size effect
- Break condition: If training data lacks political diversity or if post-training alignment procedures override learned biases

### Mechanism 2
- Claim: Language of prompting (German vs English) significantly affects political alignment scores
- Mechanism: Models are more cautious in politically sensitive topics when prompted in English due to broader global exposure and potential guardrails
- Core assumption: Training data distribution and fine-tuning differ between language contexts, creating different response patterns
- Evidence anchors:
  - [abstract] "smaller models tend to be more neutral, especially when prompted in English"
  - [section] "The German responses demonstrate a higher likelihood of the models providing clear political stances"
  - [corpus] Weak evidence - no corpus support for language-specific bias patterns
- Break condition: If models employ language-agnostic safety layers or if political topics are equally represented across training languages

### Mechanism 3
- Claim: All evaluated LLMs show consistent low alignment with the far-right AfD party across both languages
- Mechanism: Training data and fine-tuning processes systematically downweight or filter content associated with far-right ideologies
- Core assumption: Dataset curation and alignment procedures include bias against certain political positions
- Evidence anchors:
  - [abstract] "Notably, all models showed low alignment with the far-right AfD party"
  - [section] "Interestingly, Llama3-8B also shows relatively high alignment scores... but less so for AfD (30.3%)"
  - [corpus] Weak evidence - no corpus support for specific AfD bias
- Break condition: If AfD positions align with other parties on specific issues, or if models' low alignment stems from topic avoidance rather than ideological bias

## Foundational Learning

- Concept: Performative prediction framework
  - Why needed here: The paper situates LLM bias within performative prediction theory, where models influence the very data they're trained on
  - Quick check question: How does performative prediction differ from standard predictive modeling in terms of feedback loops?

- Concept: Political spectrum mapping and party positioning
  - Why needed here: Understanding German political parties' positions is essential for interpreting alignment scores
  - Quick check question: What distinguishes left-leaning, center, and right-leaning parties in the German political context?

- Concept: Prompt engineering and controlled generation
  - Why needed here: The study relies on carefully crafted prompts to elicit consistent political opinions from LLMs
  - Quick check question: How does the "End Of Input" token manipulation ensure models provide single-word responses?

## Architecture Onboarding

- Component map:
  Wahl-O-Mat questionnaire interface (38 political statements) -> LLM evaluation pipeline (7 models × 2 languages × 38 statements) -> Alignment scoring system (comparing model responses to party positions) -> Proportional representation seat allocation visualization

- Critical path:
  1. Generate model responses to all 38 statements in both languages
  2. Query Wahl-O-Mat with these responses to obtain party alignments
  3. Calculate mean alignments and standard deviations across models
  4. Visualize alignments and project hypothetical European Parliament composition

- Design tradeoffs:
  - Using single-word responses limits nuanced reasoning but ensures comparability
  - German-only statements require translation for English evaluation, potentially introducing artifacts
  - The Wahl-O-Mat's fixed party set constrains generalizability to other political systems

- Failure signatures:
  - Models consistently returning neutral responses (as with Llama2-7B in English)
  - High standard deviation in alignment scores across models for specific parties
  - Disproportionate alignment scores suggesting calibration issues

- First 3 experiments:
  1. Test whether model size correlates with alignment strength using synthetic political statements
  2. Evaluate whether language-specific guardrails exist by testing politically neutral topics in both languages
  3. Verify the consistency of AfD low alignment by testing models on AfD-specific policy positions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model sizes (e.g., 7B vs 70B parameters) within the same LLM family affect political bias strength and consistency across languages?
- Basis in paper: [explicit] The paper directly compares smaller (7B/8B) and larger (70B) versions of Llama models, noting that larger models tend to align more closely with left-leaning parties
- Why unresolved: The study shows a correlation between model size and political bias strength, but does not establish causation or determine whether this relationship is consistent across all LLM families
- What evidence would resolve it: Systematic testing of multiple model sizes across different LLM families in both languages, controlling for training data and architecture differences

### Open Question 2
- Question: What specific mechanisms in the training process or data contribute to the consistent low alignment of all evaluated LLMs with the AfD party?
- Basis in paper: [explicit] The paper notes that "All models show a strikingly low alignment with the AfD in both languages"
- Why unresolved: The paper identifies the pattern but does not investigate whether this stems from training data composition, safety fine-tuning, architectural biases, or other factors
- What evidence would resolve it: Analysis of training data distributions showing political content representation and examination of fine-tuning datasets

### Open Question 3
- Question: Does the language-dependent variation in political bias reflect genuine differences in model behavior or artifacts of prompt engineering effectiveness?
- Basis in paper: [explicit] The paper observes that "LLMs exhibit pronounced political biases, providing definitive and partisan answers on critical topics when operating in German while being more cautious and neutral when prompted in English"
- Why unresolved: While the study documents this language-dependent behavior, it does not determine whether this reflects actual differences in how the models process political information across languages
- What evidence would resolve it: Comparative testing with optimized prompts in both languages and analysis of token-level attention patterns

## Limitations
- Study limited to German political context, restricting generalizability to other political systems
- Translation of German statements to English may introduce semantic shifts affecting alignment scores
- Lack of explicit model version identifiers creates reproducibility challenges

## Confidence
- High Confidence: All models show low alignment with the AfD party (consistent across both languages and multiple models)
- Medium Confidence: Relationship between model size and political alignment strength (supported by data but requires more rigorous statistical testing)
- Medium Confidence: Language effects on political alignment (observed but mechanism remains speculative)

## Next Checks
1. Conduct a controlled experiment testing whether the observed size-alignment relationship persists when using synthetically generated political statements with known ideological positions

2. Test the language-specific bias hypothesis by evaluating models on politically neutral topics in both German and English to determine whether the language effect is specific to political content

3. Verify the robustness of AfD low alignment by specifically testing models on AfD party platform positions across multiple policy domains, distinguishing between ideological bias and topic avoidance behavior