---
ver: rpa2
title: Dual Contrastive Transformer for Hierarchical Preference Modeling in Sequential
  Recommendation
arxiv_id: '2410.22790'
source_url: https://arxiv.org/abs/2410.22790
tags:
- item
- preference
- recommendation
- sequential
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of hierarchical preference modeling
  in sequential recommendation. Existing methods focus only on low-level preferences
  based on item IDs, neglecting high-level preferences revealed by item categories.
---

# Dual Contrastive Transformer for Hierarchical Preference Modeling in Sequential Recommendation

## Quick Facts
- arXiv ID: 2410.22790
- Source URL: https://arxiv.org/abs/2410.22790
- Reference count: 40
- Outperforms state-of-the-art methods by 1.53%-17.61% in HR and NDCG metrics

## Executive Summary
This paper addresses the challenge of hierarchical preference modeling in sequential recommendation, where existing methods focus only on low-level preferences based on item IDs while neglecting high-level preferences revealed by item categories. The authors propose a Hierarchical Preference Modeling (HPM) framework that explicitly models both levels of user preferences through a novel Dual Transformer architecture, Dual Contrastive Learning scheme, and Semantics-enhanced Context Embedding module. Experimental results on six Amazon datasets demonstrate significant improvements over state-of-the-art methods, particularly on sparse datasets like Clothing where the framework shows up to 17.61% improvement in NDCG@50.

## Method Summary
The HPM framework employs a Dual Transformer module that processes item IDs and categories through separate self-attention pathways to capture both fine-grained item-level and coarse-grained category-level preferences. The framework incorporates Dual Contrastive Learning that applies contrastive loss at both item and category levels to enhance preference discrimination. A Semantics-enhanced Context Embedding module uses temporal kernel functions to weight relation embeddings based on time intervals, capturing the strength of item relationships that decay or grow predictably over time depending on relation type. The model is trained with a joint loss combining pairwise ranking loss (BPR) and contrastive loss, optimized with Adam.

## Key Results
- HPM outperforms state-of-the-art methods by 1.53%-17.61% in HR and NDCG metrics across six Amazon datasets
- Largest improvements observed on sparse datasets like Clothing (17.61% NDCG@50 improvement)
- Dual Transformer and Dual Contrastive Learning components provide complementary benefits to the overall framework

## Why This Works (Mechanism)

### Mechanism 1
Dual Transformer learns both high- and low-level preference dynamics by processing item IDs and categories separately through parallel self-attention pathways, then fusing via averaging. This assumes high-level preferences change slowly while low-level preferences shift rapidly. Break condition: If item and category embeddings are poorly aligned, dual pathways may learn conflicting rather than complementary representations.

### Mechanism 2
Dual Contrastive Learning provides stronger supervision by contrasting both item and category representations at separate granularities. Item-level contrastive loss compares semantics-enhanced context embeddings with item representations, while category-level does the same at category level. This assumes contrasting at both granularities captures richer preference dynamics than single-level contrast. Break condition: If contrastive losses are imbalanced (λ not properly tuned), one level may dominate and undermine the other.

### Mechanism 3
Semantics-enhanced Context Embedding captures temporal dynamics of item relationships using temporal kernel functions (normal distribution for complementary, negative-positive for substitute relations) to weight relation embeddings based on time intervals. This assumes relationship strength decays or grows predictably over time depending on relation type. Break condition: If temporal decay parameters are poorly estimated, semantic enhancement may introduce noise rather than useful context.

## Foundational Learning

- **Transformer self-attention mechanism**: Core building block for learning sequential dependencies at both item and category levels. Quick check: How does multi-head self-attention differ from single-head in capturing item relationships?

- **Contrastive learning principles**: Provides additional supervision signals beyond traditional ranking loss. Quick check: What's the difference between instance-level and feature-level contrastive learning in sequential recommendation?

- **Knowledge graph embeddings (TransE)**: Provides pre-trained item relation embeddings that SCEL module builds upon. Quick check: How does TransE model the head-tail-relation relationship mathematically?

## Architecture Onboarding

- **Component map**: Item/Category embeddings → Dual Transformer → SCEL enhancement → Dual Contrastive Learning → Combined prediction
- **Critical path**: The flow from input embeddings through separate item/category processing to final combined prediction
- **Design tradeoffs**: Separate vs. joint processing of item and category information; number of transformer layers vs. computational efficiency; balance between ranking loss and contrastive loss (λ parameter); temporal kernel function complexity vs. overfitting risk
- **Failure signatures**: If dual transformer outputs are too dissimilar, may indicate poor embedding alignment; if contrastive loss dominates ranking loss, may cause preference drift; if SCEL enhancement provides negligible improvement, may indicate weak relation signals in data
- **First 3 experiments**: 1) Ablation test: Remove SCEL module and measure performance drop, particularly on sparse datasets; 2) Hyperparameter sweep: Test λ values [0.1, 0.5, 1.0, 2.0] to find optimal contrastive loss balance; 3) Embedding analysis: Visualize item and category embeddings to verify they capture meaningful hierarchical structure

## Open Questions the Paper Calls Out

- How does HPM performance change when incorporating more complex temporal kernels beyond normal distribution and combined negative/positive kernels used in SCEL module? The current temporal kernel functions are relatively simple and may not capture all types of preference dynamics.

- What is the impact of varying the balance between item-level and category-level contrastive learning in DCL module on recommendation performance? The paper mentions λ controls contrastive loss strength but only reports results with λ=1.

- How does HPM perform on datasets with different levels of sparsity and varying numbers of item categories? The paper mentions HPM shows larger improvements on sparse datasets but lacks detailed analysis across different sparsity levels.

## Limitations

- The Dual Transformer architecture assumes item and category representations can be meaningfully separated and processed in parallel, which may not hold when items belong to multiple categories or when category hierarchies are complex.

- The temporal kernel functions for relation modeling rely on fixed parametric forms that may not capture all temporal dynamics in real-world data across different domains.

- The framework's performance heavily depends on proper hyperparameter tuning, particularly the balance between item-level and category-level contrastive learning.

## Confidence

- **High**: The overall framework design and experimental methodology are robust
- **Medium**: The effectiveness of dual contrastive learning depends on proper hyperparameter tuning
- **Low**: The generalizability of temporal kernel functions across different domains and relation types

## Next Checks

1. **Ablation Study**: Systematically remove each component (Dual Transformer, SCEL, Dual Contrastive Learning) to quantify their individual contributions and verify they provide complementary benefits rather than redundancy.

2. **Hyperparameter Sensitivity**: Conduct a comprehensive grid search over λ values [0.1, 0.5, 1.0, 2.0] and temporal kernel parameters to identify the most robust configuration and assess sensitivity to hyperparameter choices.

3. **Cross-Domain Evaluation**: Test the framework on non-Amazon datasets (e.g., MovieLens, LastFM) to evaluate whether the hierarchical preference modeling approach generalizes beyond product recommendation scenarios.