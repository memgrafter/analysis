---
ver: rpa2
title: 'Altogether: Image Captioning via Re-aligning Alt-text'
arxiv_id: '2410.17251'
source_url: https://arxiv.org/abs/2410.17251
tags:
- image
- captions
- alt-texts
- alt-text
- caption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Altogether, a method to improve image captioning
  by re-aligning existing alt-texts with the images they describe. The approach involves
  multiple rounds of human annotation where annotators start with the original alt-text
  and iteratively refine it to better match the image content, preserving concrete
  visual concepts while removing hallucinations.
---

# Altogether: Image Captioning via Re-aligning Alt-text

## Quick Facts
- arXiv ID: 2410.17251
- Source URL: https://arxiv.org/abs/2410.17251
- Reference count: 25
- Key outcome: Improves image captioning by re-aligning alt-texts with images through iterative human annotation, achieving 4% higher CLIP scores and better downstream task performance

## Executive Summary
Altogether presents a method to improve image captioning by re-aligning existing alt-text metadata with the images they describe. The approach involves multiple rounds of human annotation where annotators start with original alt-texts and iteratively refine them to better match image content, preserving concrete visual concepts while removing hallucinations. A parameter-efficient captioner is then trained to generalize this re-alignment process at scale. The resulting captions show significant improvements over state-of-the-art captioners, with 4% higher CLIP scores, better alignment with ground truth, and improved performance in downstream tasks including text-to-image generation and zero-shot classification.

## Method Summary
The method involves a multi-stage pipeline: (1) collecting images with alt-text metadata, (2) performing 3 rounds of human annotation where annotators iteratively refine alt-texts to better match image content, (3) training a parameter-efficient captioner that combines a frozen CLIP image encoder with a trainable OPT 1.3B text decoder, and (4) applying the captioner to generate re-aligned synthetic captions at scale. The captioner takes both image embeddings and alt-text tokens as input, learning to ground alt-text information and transform it into dense, image-aligned captions that preserve concrete visual concepts while being properly aligned with image content.

## Key Results
- Achieves 4% higher CLIP score (33.1 vs 29.3) compared to original alt-texts
- Outperforms off-the-shelf captioners like GPT-4V and GPT-4o on captioning metrics
- Improves downstream text-to-image generation CLIP scores by 1.3-1.8%
- Increases zero-shot classification accuracy by 1.1% absolute

## Why This Works (Mechanism)

### Mechanism 1
Re-aligning alt-texts with images through iterative human annotation improves caption quality by preserving concrete visual concepts while removing hallucinations. Human annotators start with existing alt-text metadata and iteratively refine it to better match image content. Each round preserves the creator's specialized knowledge (e.g., specific object names like "iguana" vs generic "lizard") while adding or removing information based on visual evidence.

### Mechanism 2
Training a parameter-efficient captioner on re-aligned alt-text data enables generalization of the re-alignment process at scale. A lightweight text decoder (OPT 1.3B) is trained to take both image embeddings and alt-text tokens as input, learning to ground alt-text information and transform it into dense, image-aligned captions. The captioner can then process billions of images efficiently.

### Mechanism 3
Re-aligned synthetic captions improve downstream tasks differently than traditional captioning approaches, with text-to-image generation benefiting most from full synthetic data while classification benefits from selective mixing. Synthetic captions trained with 100% re-aligned data maximize control and grounding for T2I generation, while classification tasks benefit from a 15% mixing ratio that provides complementary information without overwhelming the model with potentially noisy synthetic data.

## Foundational Learning

- Concept: CLIP score as an image-text alignment metric
  - Why needed here: The paper uses CLIP score extensively to evaluate how well captions align with images, both for human-annotated data and generated captions
  - Quick check question: What does a 4% improvement in CLIP score (from 29.3 to 33.1) indicate about the quality of re-aligned captions compared to original alt-text?

- Concept: Parameter-efficient fine-tuning with prefix language models
  - Why needed here: The captioner architecture freezes the image encoder and only trains a lightweight text decoder, allowing efficient scaling to billions of images
  - Quick check question: How does the mapping network transform CLIP embeddings into visual tokens that can be fed into a text decoder as "image prompt"?

- Concept: Text-to-image generation training dynamics
  - Why needed here: The paper trains DiT-XL models with different mixing ratios of synthetic vs original captions, showing that T2I generation requires very different data composition than classification tasks
  - Quick check question: Why does training with 100% synthetic captions improve text-to-image generation CLIP scores by 1.8% compared to using only original alt-text captions?

## Architecture Onboarding

- Component map:
  - Image Encoder: Meta CLIP 1.0 ViT-H/14 (frozen)
  - Mapping Network: Transformer that converts 1024-dim CLIP embeddings to 40 visual tokens
  - Text Decoder: OPT 1.3B that takes visual tokens + alt-text tokens as input
  - Training Loop: Pre-training on 22M image-alt-text pairs, fine-tuning on 23k human-annotated images with 3 rounds of re-alignment

- Critical path:
  Image → CLIP encoder → 1024-dim embedding → Mapping network → 40 visual tokens → Visual tokens + alt-text tokens → Text decoder → Caption generation

- Design tradeoffs:
  - Frozen image encoder vs trainable: Sacrifices some adaptation capability for efficiency and consistency
  - Small text decoder (1.3B) vs larger models: Enables faster inference and training at scale, but may limit complex reasoning
  - 3 rounds of human annotation vs more/less: Balances quality improvement with annotation cost

- Failure signatures:
  - CLIP score improvement without corresponding BLEU/METEOR improvement: Model may be generating more aligned but less grammatically correct captions
  - Degradation when removing alt-text tokens: Model relies too heavily on alt-text rather than image understanding
  - Poor performance on out-of-domain images: Training data distribution mismatch

- First 3 experiments:
  1. Ablation study: Compare caption quality with vs without alt-text tokens as input (w/ alt vs w/o alt in Table 1)
  2. Mixing ratio study: Train T2I models with different p ratios of synthetic vs original captions (Table 5)
  3. Round comparison: Evaluate caption quality after different numbers of human annotation rounds (Altogether(2) vs Altogether(3) in Table 1)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Altogether scale with the size of the captioner model, and what is the optimal model size for balancing quality improvements against computational cost?
- Basis in paper: [inferred] The paper mentions using a lightweight text decoder (OPT 1.3B) for efficient training and inference, but does not explore performance across different model sizes
- Why unresolved: The paper only evaluates one specific model architecture and size, without comparing performance to larger models like GPT-4 or smaller models
- What evidence would resolve it: Systematic evaluation of Altogether across multiple model sizes (e.g., OPT 125M, 350M, 1.3B, 2.7B) comparing CLIP scores, captioning metrics, and downstream task performance while tracking computational requirements

### Open Question 2
- Question: What is the long-term stability of captions generated by Altogether, and do repeated re-alignment rounds lead to diminishing returns or potential degradation in quality?
- Basis in paper: [explicit] The paper mentions performing 3 rounds of annotation and training captioners after each round, but does not investigate whether additional rounds would continue improving quality or potentially degrade it
- Why unresolved: The paper only explores up to 3 rounds of annotation and does not test whether performance plateaus, improves further, or potentially degrades with more rounds
- What evidence would resolve it: Extended human annotation studies with 5+ rounds, measuring changes in caption quality metrics (CLIP score, BLEU, METEOR, etc.) across rounds to identify convergence points or degradation patterns

### Open Question 3
- Question: How does Altogether perform on out-of-domain images that differ significantly from the training data distribution, and what are the failure modes when encountering novel visual concepts?
- Basis in paper: [explicit] The paper mentions using WIT and MetaCLIP datasets with the goal of "good coverage on web images in order to mitigate the risk of inference on out-of-domain images" but does not systematically evaluate out-of-domain performance
- Why unresolved: While the paper mentions using diverse data sources, it does not explicitly test the model on truly out-of-domain images or analyze failure modes for novel visual concepts
- What evidence would resolve it: Comprehensive evaluation on datasets with different distributions (medical images, satellite imagery, artistic renderings, etc.) with detailed error analysis categorizing types of failures (hallucinations, missing concepts, incorrect attributes)

## Limitations
- Annotation cost and scalability: The 3-round human annotation process requires significant human resources and may not scale efficiently to larger datasets or more diverse image domains
- Synthetic data quality variance: Effectiveness depends heavily on the quality of original alt-texts, and if source alt-texts contain significant hallucinations, the iterative refinement process may not fully correct these issues
- Downstream task specificity: While showing strong improvements for text-to-image generation and classification, it's unclear how well these benefits transfer to other vision-language tasks like visual question answering or image retrieval

## Confidence
- **High Confidence**: The captioner architecture combining frozen CLIP encoder with trainable OPT 1.3B decoder can effectively learn to re-align alt-texts with images, as evidenced by consistent CLIP score improvements (4% gain) and superior performance over baseline captioners
- **Medium Confidence**: The specific 3-round human annotation protocol is optimal for capturing the re-alignment process, though the paper doesn't provide systematic comparison with other annotation strategies or numbers of rounds
- **Low Confidence**: The approach will scale effectively to billions of images while maintaining caption quality, as the paper only demonstrates results on datasets of 22M images for pre-training and 23k for fine-tuning

## Next Checks
1. **Ablation study on annotation rounds**: Systematically compare caption quality and downstream task performance across different numbers of human annotation rounds (1-5 rounds) to determine if 3 rounds is truly optimal or if more rounds provide additional benefits at reasonable annotation costs

2. **Cross-domain robustness testing**: Evaluate the captioner and downstream task performance on out-of-distribution image domains (medical imaging, satellite imagery, artistic photography) to assess how well the re-alignment process generalizes beyond the web-based image-alt-text pairs used in training

3. **Alternative architecture comparison**: Implement and compare the re-alignment approach using different captioner architectures (e.g., frozen CLIP encoder with different decoder sizes, fully trainable models, or alternative VLMs) to determine if the specific OPT 1.3B architecture is essential or if other architectures could achieve similar or better results