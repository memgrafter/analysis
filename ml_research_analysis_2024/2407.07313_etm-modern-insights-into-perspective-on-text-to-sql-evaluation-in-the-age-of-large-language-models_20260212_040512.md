---
ver: rpa2
title: 'ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age
  of Large Language Models'
arxiv_id: '2407.07313'
source_url: https://arxiv.org/abs/2407.07313
tags:
- select
- queries
- 'false'
- where
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ETM (Enhanced Tree Matching), a new evaluation
  metric for Text-to-SQL models that addresses limitations in existing metrics EXE
  (Execution Accuracy) and ESM (Exact Set Matching). ETM uses abstract syntax tree
  (AST) comparison with verifiable equivalence rules to better capture semantic correctness
  while allowing for syntactic variation.
---

# ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age of Large Language Models

## Quick Facts
- arXiv ID: 2407.07313
- Source URL: https://arxiv.org/abs/2407.07313
- Reference count: 25
- Primary result: New evaluation metric ETM reduces false positive rates from 23.0% to 0.3% and false negative rates from 28.9% to 2.7% compared to EXE and ESM

## Executive Summary
This paper introduces ETM (Enhanced Tree Matching), a new evaluation metric for Text-to-SQL models that addresses limitations in existing metrics EXE (Execution Accuracy) and ESM (Exact Set Matching). ETM uses abstract syntax tree (AST) comparison with verifiable equivalence rules to better capture semantic correctness while allowing for syntactic variation. Tested against nine state-of-the-art models on Spider and BIRD datasets, ETM demonstrates significant improvements in evaluation accuracy, particularly for models using pretrained large language models (PLMs).

## Method Summary
ETM is an evaluation metric that compares SQL queries using abstract syntax tree (AST) representations rather than execution results or exact string matching. The method parses both predicted and gold queries into ASTs, applies normalization rules to account for syntactic variations, and leverages database schema constraints to verify semantic equivalence. The metric is implemented as an open-source evaluation script that takes model outputs and database schemas as inputs to produce evaluation results.

## Key Results
- ETM reduces false positive rates from 23.0% to 0.3% compared to EXE and ESM
- ETM reduces false negative rates from 28.9% to 2.7% compared to EXE and ESM
- ETM demonstrates superior performance for evaluating PLM-generated queries due to its semantic focus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ETM reduces false positives by comparing SQL queries at the AST level instead of just execution results.
- Mechanism: The metric parses both predicted and gold queries into Abstract Syntax Trees (ASTs), then applies normalization rules to account for syntactic variations (like aliases, ordering of joins, etc.). This semantic comparison catches cases where different SQL syntax produces the same output but is not semantically equivalent.
- Core assumption: AST structure reliably captures semantic meaning of SQL queries across a broad range of syntactic forms.
- Evidence anchors:
  - [abstract]: "ETM uses abstract syntax tree (AST) comparison with verifiable equivalence rules to better capture semantic correctness while allowing for syntactic variation."
  - [section]: "ETM applies a set of verifiable equivalence rules to transform queries into normalized forms before comparison, reducing false negatives present in ESM."
- Break condition: If AST normalization rules miss a semantically significant structural difference, or if two different ASTs represent the same semantics but the rules don't recognize this.

### Mechanism 2
- Claim: ETM reduces false negatives by using verifiable database schema constraints to identify semantically equivalent queries.
- Mechanism: ETM includes equivalence rules that leverage database schema information (primary key-foreign key relationships, column uniqueness, NULL constraints) to determine when structurally different queries are semantically equivalent. This allows it to accept valid variations that ESM would reject.
- Core assumption: Database schemas contain sufficient information to verify semantic equivalence of SQL queries in most practical cases.
- Evidence anchors:
  - [section]: "ETM applies a set of verifiable equivalence rules to transform queries into normalized forms before comparison, reducing false negatives present in ESM."
  - [section]: "We carefully examine every false negative case and compile verifiable assumptions that are sufficiently general for any database schema to alleviate this challenge."
- Break condition: If schema constraints are incomplete or the equivalence rules miss edge cases where queries are semantically equivalent but not covered by the current rules.

### Mechanism 3
- Claim: ETM is more robust than EXE and ESM for evaluating PLM-generated queries due to its semantic focus.
- Mechanism: PLMs tend to generate more stylistically diverse SQL than fine-tuned models. ETM's semantic evaluation handles this variation better than EXE (which can be fooled by different syntax producing same output) or ESM (which is too rigid in matching). The AST-based comparison and normalization rules capture semantic correctness regardless of syntactic style.
- Core assumption: The diversity in PLM-generated SQL is primarily syntactic rather than semantic, and ETM's semantic focus can distinguish between valid variations and actual errors.
- Evidence anchors:
  - [abstract]: "ETM is tested against nine state-of-the-art models on Spider and BIRD datasets. ETM reduces false positive rates from 23.0% to 0.3% and false negative rates from 28.9% to 2.7% compared to EXE and ESM."
  - [section]: "Models using pretrained LLMs without fine-tuning...perform particularly well on EXE...When using ESM as the primary metric, no PLM-based models rank highly...it is critical to examine these metrics and determine the most suitable approach for an accurate evaluation of model performance, as the disparity between them disproportionately impacts PLM-based models compared to those using fine-tuned LLMs (henceforth, FLM)."
- Break condition: If PLM-generated SQL varies semantically in ways that ETM's rules don't capture, or if the semantic variations are too diverse for the current rule set to handle.

## Foundational Learning

- Concept: Abstract Syntax Trees (ASTs) in programming languages
  - Why needed here: ETM relies on parsing SQL queries into ASTs to compare their structure rather than just their textual form or execution results.
  - Quick check question: What is the difference between an AST and the original source code, and why is AST useful for comparing program equivalence?

- Concept: SQL equivalence and undecidability
  - Why needed here: The paper mentions that "SQL equivalence has been shown to be undecidable," which is why ETM uses heuristics and rules rather than perfect equivalence checking.
  - Quick check question: What does it mean for SQL equivalence to be undecidable, and how does this limitation affect evaluation metrics?

- Concept: Database schema constraints (primary keys, foreign keys, unique constraints, NULL constraints)
  - Why needed here: ETM uses these constraints as verifiable assumptions to determine when structurally different queries are semantically equivalent.
  - Quick check question: How can knowing a column is UNIQUE or NON_NULL help determine whether two SQL queries are semantically equivalent?

## Architecture Onboarding

- Component map: SQL Parser -> AST Normalizer -> Schema Analyzer -> Equivalence Checker -> Rule Engine
- Critical path: Input query → AST parsing → Schema analysis → Normalization → Equivalence checking → Output match/mismatch
- Design tradeoffs:
  - Precision vs. recall: Stricter rules reduce false positives but may increase false negatives
  - Rule coverage vs. complexity: More rules handle more cases but increase maintenance burden
  - Schema dependency vs. generality: Using schema constraints improves accuracy but requires schema availability
  - Performance vs. thoroughness: More comprehensive checking is slower but more accurate
- Failure signatures:
  - High false positive rate: AST normalization rules may be too permissive
  - High false negative rate: Schema analysis may be incomplete or rules may miss equivalence cases
  - Performance issues: Complex AST transformations or excessive rule checking
  - Inconsistent results: Non-deterministic rule application or parsing issues
- First 3 experiments:
  1. Test ETM on a small dataset with known semantic equivalences to verify the normalization rules correctly identify equivalent queries
  2. Compare ETM's false positive/negative rates against EXE and ESM on a diverse set of PLM-generated queries to validate the claimed improvements
  3. Measure ETM's performance (runtime) on queries of varying complexity to establish scalability and identify potential bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of verifiable equivalence rules impact the false negative rate across different model architectures (PLM vs FLM)?
- Basis in paper: [explicit] The paper analyzes the impact of individual equivalence rules on ETM's performance, showing that some rules benefit certain models more than others (e.g., Rule 14 was most important for models using the training set, while Rules 6 was most useful for GPT-based models)
- Why unresolved: While the paper shows rule-by-rule impact, it doesn't systematically analyze how rule effectiveness varies across model types or what this reveals about different model architectures' query generation tendencies
- What evidence would resolve it: A comprehensive analysis comparing rule effectiveness across model categories, potentially revealing architectural biases in how different models approach SQL generation

### Open Question 2
- Question: What is the relationship between natural language question ambiguity and false positive rates in ETM compared to EXE and ESM?
- Basis in paper: [inferred] The paper notes that ETM aims to reduce variance by being more stringent, but doesn't explicitly examine how question ambiguity affects false positives across metrics
- Why unresolved: The paper focuses on syntactic and semantic query differences but doesn't analyze how ambiguous natural language questions contribute to evaluation metric failures
- What evidence would resolve it: A study categorizing questions by ambiguity level and measuring false positive rates across metrics for each category

### Open Question 3
- Question: Can ETM be extended to handle temporal databases and time-series queries effectively?
- Basis in paper: [inferred] ETM handles complex queries but the paper doesn't address temporal aspects like TIMESTAMP operations, time-based aggregations, or temporal JOINs
- Why unresolved: The evaluation datasets (Spider and BIRD) don't include temporal queries, and the equivalence rules don't address time-based semantic equivalence
- What evidence would resolve it: Testing ETM on a temporal database benchmark and developing additional equivalence rules for temporal query patterns

## Limitations

- Rule Coverage and Generalization: While the paper claims ETM's equivalence rules are "sufficiently general for any database schema," the evaluation only demonstrates performance on Spider and BIRD datasets. The generalizability to other database schemas with different structures, constraints, and complexity levels remains uncertain.

- Performance Trade-offs: The paper does not provide runtime performance metrics for ETM compared to EXE and ESM. Given that ETM involves AST parsing, schema analysis, and multiple transformation rules, there may be significant computational overhead that could limit its practical applicability for large-scale evaluations.

- Edge Case Handling: The paper acknowledges SQL equivalence is undecidable and relies on heuristics. The specific handling of edge cases (queries involving window functions, recursive queries, or complex subqueries) is not detailed, raising questions about robustness across all SQL constructs.

## Confidence

**False Positive Reduction (High Confidence)**: The dramatic reduction from 23.0% to 0.3% in false positive rates is well-supported by the mechanistic explanation of AST-based semantic comparison versus execution-based comparison. The difference between structural and behavioral evaluation is clearly articulated.

**False Negative Reduction (Medium Confidence)**: While the reduction from 28.9% to 2.7% is significant, the reliance on schema constraints for verifying equivalence introduces uncertainty about performance across different database schemas not represented in Spider and BIRD.

**PLM-Specific Benefits (Medium Confidence)**: The claim that ETM better handles PLM-generated SQL diversity is supported by comparative analysis but lacks systematic ablation studies showing how different types of syntactic variation affect each metric differently.

## Next Checks

1. **Cross-Dataset Generalization Test**: Evaluate ETM on additional Text-to-SQL datasets with different database schemas (e.g., WikiSQL, GeoQuery, or domain-specific datasets) to verify rule generalizability beyond Spider and BIRD.

2. **Performance Benchmarking**: Measure and compare runtime performance of ETM versus EXE and ESM on queries of increasing complexity to establish practical scalability and identify performance bottlenecks.

3. **Edge Case Analysis**: Systematically test ETM on SQL queries containing advanced features (window functions, recursive queries, complex subqueries) and edge cases (queries with ambiguous schema references, queries involving circular foreign key relationships) to identify potential failure modes not captured in the main evaluation.