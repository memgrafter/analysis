---
ver: rpa2
title: Mind the Graph When Balancing Data for Fairness or Robustness
arxiv_id: '2406.17433'
source_url: https://arxiv.org/abs/2406.17433
tags:
- data
- balancing
- distribution
- causal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of data balancing as
  a mitigation strategy for fairness and robustness failures in machine learning models.
  The authors analyze how balancing the training data distribution impacts the model's
  predictions and identify conditions under which balancing leads to invariant and
  optimal models.
---

# Mind the Graph When Balancing Data for Fairness or Robustness

## Quick Facts
- **arXiv ID:** 2406.17433
- **Source URL:** https://arxiv.org/abs/2406.17433
- **Reference count:** 40
- **Key outcome:** Data balancing is not universally effective for fairness/robustness and can interfere with other mitigation strategies

## Executive Summary
This paper provides a theoretical and empirical analysis of data balancing as a mitigation strategy for fairness and robustness failures in machine learning. The authors demonstrate that data balancing does not always correspond to selectively removing undesired dependencies in the causal graph of the task, and can interfere with other mitigation techniques like regularization. Through semi-synthetic experiments on MNIST, Amazon reviews, and CelebA datasets, they showcase failure modes of data balancing and provide insights into when balancing might be insufficient or even detrimental.

## Method Summary
The paper investigates data balancing through joint resampling to create uniform distributions over protected attributes and outcomes. The method involves generating semi-synthetic datasets where the ground truth causal structure is known, then applying joint balancing to create a balanced distribution Q. Models are trained on both original and balanced distributions and evaluated on multiple target distributions including the original, balanced, and ground truth distributions. The analysis combines theoretical work using causal Bayesian networks with empirical validation across different model architectures and datasets.

## Key Results
- Data balancing only leads to invariant and optimal models under specific causal graph structures where X⊥Z is a sufficient statistic for Y
- Balancing can fail when there are entangled signals between Y and Z or additional confounders in the graph
- Regularization techniques like conditional MMD can improve fairness/robustness but may interfere with balancing effectiveness
- Pre-training on large diverse datasets provides some robustness but doesn't eliminate the need for careful analysis of the causal structure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint data balancing can remove statistical dependence between Y and Z under certain causal graph structures, enabling risk-invariant and optimal models.
- **Mechanism:** By resampling the training distribution to match P(Z)P(Y)/P(Z,Y), the method adjusts the data distribution to approximate a scenario where Y and Z are independent. This aligns with the assumption in the causal graph that removing the red edges (undesired paths) leads to a distribution where X⊥Z is a sufficient statistic for Y and independent of Z given Y.
- **Core assumption:** The causal graph structure allows for a factorization where the undesirable paths between Y and Z can be "dropped" without affecting the remaining dependencies.
- **Evidence anchors:**
  - [abstract] "Our results display that, in many cases, the balanced distribution does not correspond to selectively removing the undesired dependencies in a causal graph of the task"
  - [section 4] "If X⊥Z ⊥⊥Q Z | Y and X⊥Z is a sufficient statistic for Y in Q, then the risk-minimizer f(X) := EQ[Y | X] is risk-invariant and optimal w.r.t. P"
  - [corpus] Weak evidence; related works focus on balancing but not on the causal graph factorization requirement.
- **Break condition:** If the causal graph contains entangled signals (XY∧Z) or additional confounders (V), the balanced distribution Q does not factorize according to the desired graph G0, invalidating the mechanism.

### Mechanism 2
- **Claim:** Regularization can enforce independence between model representations and Z, improving fairness or robustness when combined with data balancing.
- **Mechanism:** Adding a conditional independence constraint (e.g., f(X) ⊥⊥Q Z | Y) during training encourages the model to rely only on X⊥Z and not on X⊥Y or XY∧Z. This can align with the causal assumption that X⊥Z is a sufficient statistic for Y.
- **Core assumption:** The regularization term does not interfere with the information in X⊥Z that is necessary for predicting Y.
- **Evidence anchors:**
  - [section 5] "Veitch et al. [73] recommend to impose an independence between f(X) and Z conditioned on Y"
  - [section 5.1] "There is little variation between the metrics across MMD strengths, and the model is fair and robust"
  - [corpus] Limited direct evidence; most related work focuses on balancing alone.
- **Break condition:** If the data balancing already induces dependence between X⊥Z and Z (e.g., in causal tasks), the regularization may destroy necessary information in X⊥Z, leading to poor performance.

### Mechanism 3
- **Claim:** Pre-training on large, diverse datasets can provide a disentangled representation that is more robust to spurious correlations.
- **Mechanism:** Models pre-trained on large datasets (e.g., ImageNet) learn representations that capture more general features, reducing reliance on dataset-specific spurious patterns. When fine-tuned on balanced data, these representations are less likely to encode Z.
- **Core assumption:** The pre-training dataset is sufficiently diverse to cover the variations in Z without overfitting to spurious correlations.
- **Evidence anchors:**
  - [section 6] "We first attempt to improve our representation by pre-training the VGG with ImageNet... While we observe an increase in performance with pre-training, there is no clear decrease in equalized odds"
  - [section 3] "Kirichenko et al. [40], as the success of their data balancing mitigation only holds when using models pre-trained on large datasets"
  - [corpus] Weak evidence; related works discuss pre-training benefits but not in the context of disentangled representations for fairness.
- **Break condition:** If the pre-training dataset itself contains similar spurious correlations, the representation may not be disentangled, and balancing may still fail.

## Foundational Learning

- **Concept:** Causal Bayesian Networks (CBNs) and d-separation
  - **Why needed here:** The paper uses CBNs to model the data generating process and analyze how data balancing affects dependencies. Understanding d-separation is crucial for interpreting when variables are independent given others.
  - **Quick check question:** In a graph Z → X → Y, is X independent of Z given Y? (Answer: No, because Y is a collider.)

- **Concept:** Sufficient statistic in the context of risk minimization
  - **Why needed here:** The paper defines X⊥Z as a sufficient statistic for Y in Q if EQ[Y | X] = EQ[Y | X⊥Z]. This concept is central to understanding when data balancing leads to risk-invariant models.
  - **Quick check question:** If X⊥Z is a sufficient statistic for Y in Q, does f(X) vary with X⊥Y or XY∧Z? (Answer: No, because the risk-minimizing function only depends on X⊥Z.)

- **Concept:** Regularization for conditional independence
  - **Why needed here:** The paper discusses adding regularization terms to enforce f(X) ⊥⊥Q Z | Y. Understanding how to operationalize this in practice is key for implementing the proposed solutions.
  - **Quick check question:** What is the effect of adding a conditional MMD regularizer f(X) ⊥⊥Q Z | Y during training? (Answer: It encourages the model to not depend on Z given Y, potentially improving fairness.)

## Architecture Onboarding

- **Component map:**
  Data → Causal Graph Analyzer → Balancing Module → Model Trainer → Regularization Module → Evaluation Module → Fairness/Robustness Metrics

- **Critical path:**
  1. Analyze the causal graph of the task.
  2. Determine if X⊥Z is a sufficient statistic for Y in the balanced distribution Q.
  3. If not, consider adding regularization or disentangling the representation.
  4. Train the model on Q and evaluate fairness/robustness metrics.

- **Design tradeoffs:**
  - Balancing vs. regularization: Balancing is simpler but may not remove all dependencies; regularization is more targeted but can interfere with necessary information.
  - Subsampling vs. reweighting: Subsampling is easier to implement but may lead to higher variance; reweighting preserves more data but may be noisier.
  - Pre-training vs. from-scratch: Pre-training can provide better representations but may not always be available or relevant.

- **Failure signatures:**
  - High equalized odds or low worst-group accuracy despite balancing: Indicates the balanced distribution does not remove all dependencies.
  - Performance drop on P0 compared to Q: Suggests the model is not risk-invariant.
  - Encoding of Z in the model's representation: Implies the representation is not disentangled.

- **First 3 experiments:**
  1. **MNIST with purely spurious correlation:** Train on balanced vs. unbalanced data, evaluate on P0. Expect balanced data to succeed.
  2. **MNIST with added confounder V:** Train on balanced data, evaluate on P0. Expect failure due to V.
  3. **CelebA with VGG:** Train on balanced data, add MMD regularization, evaluate encoding of Z. Expect improvement with regularization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does data balancing remain effective when applied to multi-class classification tasks with more than two classes and sensitive attributes with more than two values?
- Basis in paper: [inferred] The paper only investigates binary classification tasks and binary sensitive attributes, but the principles of data balancing and its failure modes may extend to more complex settings.
- Why unresolved: The paper does not provide any theoretical analysis or experimental results for multi-class or multi-valued sensitive attributes, leaving the effectiveness of data balancing in these scenarios unknown.
- What evidence would resolve it: Experiments on multi-class classification tasks with multi-valued sensitive attributes, comparing the performance of data balancing to other mitigation strategies, and analyzing the conditions under which data balancing is effective or ineffective.

### Open Question 2
- Question: How does the choice of data balancing method (e.g., oversampling, undersampling, reweighting) impact the model's fairness and robustness properties, and are there specific methods that are more effective than others?
- Basis in paper: [explicit] The paper mentions different methods for joint balancing (e.g., subsampling, upsampling, reweighting) but does not compare their effectiveness or analyze their impact on fairness and robustness.
- Why unresolved: The paper focuses on joint balancing and does not provide a comprehensive comparison of different data balancing methods or their impact on model performance.
- What evidence would resolve it: Experiments comparing different data balancing methods (e.g., oversampling, undersampling, reweighting) on various tasks and analyzing their impact on fairness, robustness, and model performance.

### Open Question 3
- Question: Can data balancing be effectively combined with other mitigation strategies (e.g., adversarial training, regularization) to improve fairness and robustness, or does it hinder their effectiveness?
- Basis in paper: [explicit] The paper discusses the potential interference between data balancing and regularization, but does not provide a comprehensive analysis of how data balancing interacts with other mitigation strategies.
- Why unresolved: The paper focuses on the interaction between data balancing and regularization but does not explore other mitigation strategies or their potential synergies or conflicts with data balancing.
- What evidence would resolve it: Experiments combining data balancing with other mitigation strategies (e.g., adversarial training, regularization) and analyzing their combined impact on fairness, robustness, and model performance.

## Limitations

- The theoretical results assume specific causal graph structures that may not hold in many real-world scenarios
- The semi-synthetic experiments use controlled datasets where ground truth distributions are known, limiting generalizability
- The interaction between data balancing and regularization is not fully understood and may vary across different model architectures

## Confidence

- **High confidence:** The theoretical framework using causal Bayesian networks and the conditions for successful data balancing are well-established and mathematically sound
- **Medium confidence:** The failure modes demonstrated in semi-synthetic experiments are convincing but their applicability to real-world scenarios needs further validation
- **Low confidence:** The interaction between data balancing and regularization is not fully understood, and recommendations may not generalize across different model architectures and datasets

## Next Checks

1. **Real-world causal structure validation:** Apply the framework to real datasets where the causal graph can be partially validated through domain knowledge or causal discovery methods to test whether theoretical assumptions hold outside controlled environments.

2. **Ablation study on regularization:** Systematically vary the strength of MMD regularization while keeping balancing fixed to precisely characterize when and how regularization interferes with balancing effectiveness.

3. **Alternative balancing methods comparison:** Compare joint balancing with other data balancing approaches (reweighting, conditional balancing) across the same semi-synthetic tasks to determine if failure modes are specific to the chosen balancing method or more general.