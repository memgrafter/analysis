---
ver: rpa2
title: Anomaly Detection in OKTA Logs using Autoencoders
arxiv_id: '2411.07314'
source_url: https://arxiv.org/abs/2411.07314
tags:
- user
- data
- actor
- okta
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work applies autoencoders to detect anomalies in Okta user
  login behavior using unsupervised learning. The method transforms raw Okta logs
  into encoded features (geohashed locations, application usage patterns, categorical
  variables) and trains actor-specific autoencoders to model normal behavior.
---

# Anomaly Detection in OKTA Logs using Autoencoders

## Quick Facts
- arXiv ID: 2411.07314
- Source URL: https://arxiv.org/abs/2411.07314
- Authors: Jericho Cain; Hayden Beadles; Karthik Venkatesan
- Reference count: 40
- Primary result: Achieved F1 scores ≥0.97 for 13/18 users in detecting geographically distant Okta logins

## Executive Summary
This work applies autoencoders to detect anomalies in Okta user login behavior using unsupervised learning. The method transforms raw Okta logs into encoded features (geohashed locations, application usage patterns, categorical variables) and trains actor-specific autoencoders to model normal behavior. Anomalies are identified via reconstruction error exceeding actor-specific thresholds. Testing on 18 users with injected location anomalies achieved F1 scores ≥0.97 for 13 users, with strong performance for detecting geographically distant logins (near-perfect detection in some cases).

## Method Summary
The approach processes Okta System Log data through an ETL pipeline that flattens JSON structure and extracts key features including event hour, day of week, event outcome, location, and application. Location data is encoded using geohashing with 3-bit precision to reduce network noise while preserving geographic patterns. Application usage frequencies are analyzed with Wilson Score Intervals to account for population behavior. Categorical variables are encoded using entity embeddings to capture semantic relationships. Actor-specific autoencoders are trained on bootstrap-sampled data from each user's historical logins, using Dice Loss as the reconstruction error metric. Anomalies are flagged when reconstruction error exceeds actor-specific thresholds determined through standard deviation analysis.

## Key Results
- F1 scores ≥0.97 achieved for 13 out of 18 tested users
- Near-perfect detection of geographically distant logins (1000-mile distance delta)
- Event hour and day-of-week features showed weaker results, likely due to encoding granularity issues
- Geohashing effectively reduced noise while maintaining anomaly detection capability

## Why This Works (Mechanism)

### Mechanism 1
Autoencoders can effectively model normal user behavior and detect anomalies by comparing reconstruction error against personalized thresholds. By training an autoencoder on each user's historical Okta login data, the model learns the typical patterns of that user's behavior. When new data is presented, the reconstruction error between the input and the model's output indicates how much the new behavior deviates from the learned norm. Personalized thresholds based on each user's training data ensure appropriate sensitivity.

### Mechanism 2
Geohashing effectively reduces noise in location data while preserving meaningful geographic patterns. Raw latitude/longitude data from Okta logs contains high variability due to network factors. Geohashing converts these coordinates into discrete regions (bounded boxes), reducing this noise while maintaining the essential geographic information needed to identify unusual login locations.

### Mechanism 3
Entity embedding captures semantic relationships between categorical variables better than one-hot encoding. Traditional one-hot encoding treats all categories as equidistant, losing valuable relationship information. Entity embedding maps categorical variables (like days of the week or applications) into dense vectors where similar categories are closer in the embedding space, allowing the model to learn these relationships.

## Foundational Learning

- **Autoencoder architecture and training**: Understanding how autoencoders learn to reconstruct inputs and how reconstruction error indicates anomalies is fundamental to this approach. Quick check: What is the difference between an undercomplete and overcomplete autoencoder, and why is undercompleteness important for anomaly detection?

- **Geohashing and spatial data representation**: The method relies on converting geographic coordinates into discrete regions to reduce noise while preserving meaningful patterns. Quick check: How does changing the precision of a geohash affect the size of the resulting bounding box and the granularity of location data?

- **Statistical confidence intervals and frequency estimation**: The method uses Wilson Score Intervals to adjust application frequency estimates for population behavior, which is crucial for identifying unusual application usage patterns. Quick check: Why is the Wilson Score Interval preferred over simple frequency counts when estimating application usage from sample data?

## Architecture Onboarding

- **Component map**: Data Ingestion -> ETL Pipeline -> Feature Engineering -> Training Pipeline -> Scoring Pipeline -> Evaluation
- **Critical path**: Data extraction from Okta logs → Feature engineering (geohashing, application analysis, encoding) → Bootstrap sampling to create training datasets per user → Autoencoder training with Dice loss → Threshold determination via standard deviation analysis → Real-time scoring of new events
- **Design tradeoffs**: Actor-specific vs. global models (personalization vs. computational resources); Geohash precision (detail vs. sensitivity to network variability); Embedding dimensions (nuance vs. model complexity)
- **Failure signatures**: High false positive rate (geohash precision too fine or thresholds too sensitive); High false negative rate (geohash precision too coarse or thresholds not sensitive enough); Poor F1 scores for specific actors (erratic behavior patterns)
- **First 3 experiments**: Test different geohash precisions (3-bit, 4-bit, 5-bit) on a sample user to find optimal balance; Compare one-hot encoding vs. entity embedding for categorical features; Inject location anomalies at different distances (100 miles, 500 miles, 1000 miles) to determine detection thresholds

## Open Questions the Paper Calls Out

- How does the autoencoder model handle cases where user behavior is inherently unpredictable, such as frequent travel or irregular work schedules, which may lead to false positives in anomaly detection?

- What is the impact of reducing granularity for event hour and day-of-week features on the detection of anomalies, and how does this compare to the current approach?

- How does the model perform in detecting anomalies in features other than location, such as device type or operating system, and what is the significance of these anomalies in cybersecurity?

## Limitations

- Data representation limitations due to geohash precision tradeoffs between sensitivity and specificity
- Model generalization concerns when users have sparse historical login data
- Temporal dynamics unaddressed, treating user behavior as relatively static without modeling evolution

## Confidence

**High Confidence**: Actor-specific autoencoders can effectively detect location-based anomalies with sufficient training data; Geohashing reduces location noise while preserving geographic patterns; Entity embeddings capture categorical relationships better than one-hot encoding.

**Medium Confidence**: Dice Loss is superior to traditional reconstruction metrics for this use case; 3-bit geohash precision represents optimal balance for tested scenarios; Wilson Score Intervals meaningfully improve application frequency estimation.

**Low Confidence**: Method generalizes well to users with sparse login data; Performance remains consistent across different geographic regions or cultural contexts; Approach handles rapid behavioral changes without significant performance degradation.

## Next Checks

1. **Geohash Precision Sensitivity Analysis**: Systematically test different geohash precisions (2-bit through 5-bit) across multiple users with varying geographic mobility patterns to identify optimal precision levels and measure impact on false positive/negative rates.

2. **Temporal Drift Evaluation**: Implement time-based cross-validation where models are trained on older data and tested on progressively newer data to quantify performance degradation over time and determine optimal retraining intervals.

3. **Sparse Data Performance Assessment**: Evaluate model performance on users with minimal historical data (fewer than 50 login events) by comparing actor-specific models against hybrid approaches that combine user-specific and global behavior patterns, establishing minimum data requirements for reliable anomaly detection.