---
ver: rpa2
title: Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language
  Models
arxiv_id: '2408.08210'
source_url: https://arxiv.org/abs/2408.08210
tags:
- reasoning
- counterfactual
- problem
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether large language models (LLMs) exhibit
  reasoning abilities using probabilistic concepts of necessity (PN) and sufficiency
  (PS). The authors propose a framework to evaluate LLMs by comparing true PN and
  PS values computed from causal reasoning graphs with those inferred from factual
  and counterfactual datasets generated by LLMs.
---

# Does Reasoning Emerge? Examining the Probabilities of Causation in Large Language Models

## Quick Facts
- arXiv ID: 2408.08210
- Source URL: https://arxiv.org/abs/2408.08210
- Authors: Javier González; Aditya V. Nori
- Reference count: 40
- Key outcome: Evaluates LLM reasoning using probabilistic necessity/sufficiency measures, finding GPT-4 shows limited improvement on counterfactual tasks

## Executive Summary
This paper examines whether large language models exhibit genuine reasoning capabilities by analyzing their performance on counterfactual reasoning tasks using probabilistic measures of necessity (PN) and sufficiency (PS). The authors develop a framework that evaluates LLMs by comparing their inferred PN/PS values from factual and counterfactual datasets against ground truth values computed from causal reasoning graphs. Testing GPT-2, GPT-3.5-turbo, and GPT-4 on three mathematical problems reveals that while GPT-4 shows some improvement on simpler problems, all models struggle significantly with counterfactual reasoning, suggesting that current LLMs lack true causal understanding.

## Method Summary
The paper introduces a framework to assess LLM reasoning by mapping problems to causal reasoning graphs with boolean conditions, then computing ground truth PN and PS values. Factual and counterfactual datasets are generated for each problem, and LLMs are queried to produce their own datasets. The estimated PN/PS values from LLM responses are compared to ground truth using inconsistency rates (FIR and CIR) and overlap metrics (γ-PN-overlap and γ-PS-overlap). The evaluation is applied to three mathematical problems—divisibility by 6, even sum of integers, and candy party distribution—using GPT-2, GPT-3.5-turbo, and GPT-4.

## Key Results
- GPT-4 demonstrates some reasoning capabilities for the simpler divisibility problem but struggles with counterfactual reasoning
- Factual and counterfactual consistency rates show significant gaps, indicating models rely on pattern matching rather than true causal understanding
- Overall, current LLMs fail to achieve ground truth PN/PS values, with performance degrading substantially on counterfactual tasks
- The evaluation reveals that relying solely on factual error rates can overestimate reasoning abilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Probabilistic necessity (PN) and sufficiency (PS) measures capture causal reasoning in LLMs when the problem can be modeled as a causal reasoning graph.
- **Mechanism**: The paper maps LLM responses to PN/PS values computed from counterfactual reasoning tasks. If LLM outputs match the true PN/PS from the reasoning graph, the model demonstrates causal reasoning.
- **Core assumption**: PN and PS are well-defined and identifiable for the causal models underlying the reasoning problems.
- **Evidence anchors**:
  - [abstract]: "This paper introduces a framework that is both theoretical and practical, aimed at assessing how effectively LLMs are able to replicate real-world reasoning mechanisms using these probabilistic measures."
  - [section]: "We show that when a problem can be solved via a reasoning graph of boolean conditions, denoted by G, the PN and PS can be computed using a causal model underlying G."
  - [corpus]: Weak - corpus papers focus on machine learning approaches to estimate PN/PS, not LLM evaluation.
- **Break condition**: If the causal model assumptions (monotonicity, identifiability) fail, PN/PS computation is invalid and the evaluation breaks down.

### Mechanism 2
- **Claim**: LLM reasoning abilities are revealed by comparing factual vs counterfactual consistency rates.
- **Mechanism**: Factual consistency measures how well LLMs answer direct questions; counterfactual consistency measures how well they handle hypothetical interventions. Large gaps indicate limited reasoning.
- **Core assumption**: Counterfactual reasoning requires understanding the underlying causal structure, not just memorizing patterns.
- **Evidence anchors**:
  - [abstract]: "Central to this debate are two key probabilistic concepts that are essential for connecting causes to their effects: the probability of necessity (PN) and the probability of sufficiency (PS)."
  - [section]: "We assess the LLM's reasoning capability by comparing the estimated (distribution of) PN and PS from the DLLM F and DLLM CF datasets with the actual values derived from DF and DCF datasets."
  - [corpus]: Weak - corpus focuses on algorithmic estimation of PN/PS, not LLM counterfactual evaluation.
- **Break condition**: If LLMs can mimic counterfactual reasoning through pattern matching without true causal understanding, the method overestimates reasoning ability.

### Mechanism 3
- **Claim**: LLM reasoning improves with model size/complexity, as evidenced by GPT-4 outperforming GPT-2 on counterfactual tasks.
- **Mechanism**: The paper tests multiple GPT models on the same reasoning problems, showing that larger models (GPT-4) achieve closer PN/PS estimates to ground truth.
- **Core assumption**: Model scale correlates with emergent reasoning capabilities beyond statistical pattern matching.
- **Evidence anchors**:
  - [abstract]: "Our research marks an important step towards gaining a deeper understanding of when LLMs are capable of reasoning, as illustrated by a series of math examples."
  - [section]: "An emerging trend towards reasoning is observed in the GPT family of models, particularly seen with GPT-4 for the Div6 problem."
  - [corpus]: Weak - corpus papers discuss algorithmic approaches to PN/PS, not scaling laws for LLM reasoning.
- **Break condition**: If improvements are due to better training data or prompt engineering rather than true reasoning emergence, the mechanism fails.

## Foundational Learning

- **Concept**: Causal reasoning graphs as representations of logical steps in problem-solving
  - **Why needed here**: The paper evaluates LLM reasoning by comparing outputs against ground truth PN/PS computed from causal models
  - **Quick check question**: Given a problem like "if N is divisible by 2 and 3, is it divisible by 6?", can you draw the causal reasoning graph with boolean conditions as nodes?

- **Concept**: Counterfactual queries and interventions (do-calculus)
  - **Why needed here**: The evaluation framework requires counterfactual datasets where variables are intervened upon to compute PN/PS
  - **Quick check question**: For the divisibility problem, what would the counterfactual dataset look like if we set "divisible by 3" to true for all numbers?

- **Concept**: Probabilistic causation (necessity and sufficiency)
  - **Why needed here**: These measures quantify whether LLM outputs reflect true causal understanding vs. pattern matching
  - **Quick check question**: If an LLM always says "yes" to divisibility by 6 when a number is divisible by both 2 and 3, what would PN and PS be?

## Architecture Onboarding

- **Component map**: Reasoning Graph -> Ground Truth Datasets -> Factual/Counterfactual Prompts -> LLM Responses -> Estimated PN/PS -> Comparison with Ground Truth
- **Critical path**: 
  1. Construct causal reasoning graph for problem
  2. Generate ground truth factual and counterfactual datasets
  3. Create factual and counterfactual prompts for LLM
  4. Collect LLM responses (multiple samples for uncertainty)
  5. Compute estimated PN/PS from LLM datasets
  6. Compare with ground truth PN/PS
- **Design tradeoffs**:
  - Using boolean variables simplifies computation but limits expressiveness
  - Multiple LLM samples needed for uncertainty estimation increases computational cost
  - Manual prompt design required for each problem type
- **Failure signatures**:
  - Negative PN/PS values indicate inconsistencies in LLM responses
  - Large gaps between factual and counterfactual consistency rates suggest pattern matching
  - Estimated PN/PS distributions far from ground truth indicate lack of causal reasoning
- **First 3 experiments**:
  1. Implement the divisibility by 6 problem: create reasoning graph, generate datasets, test with a small LLM
  2. Add the even sum of integers problem: verify the framework works for different causal structures
  3. Test GPT-2 vs GPT-4 on the same problems: observe improvement in counterfactual consistency

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The evaluation framework relies heavily on the validity of causal reasoning graphs, which may not capture all forms of reasoning
- Results are based on three specific mathematical problems, limiting generalizability to other reasoning domains
- The method requires manual construction of reasoning graphs and prompts for each problem, making it labor-intensive
- The study doesn't account for potential prompt engineering effects on model performance

## Confidence
- **High confidence**: The mechanism showing that factual vs counterfactual consistency gaps indicate limited reasoning (Mechanism 2)
- **Medium confidence**: The overall framework for evaluating LLM reasoning through PN/PS measures (Mechanism 1)
- **Low confidence**: Claims about reasoning emergence purely from model scale improvements (Mechanism 3)

## Next Checks
1. Test the framework on non-mathematical reasoning problems (e.g., common sense reasoning tasks) to assess generalizability
2. Implement automated reasoning graph generation to reduce manual effort and enable larger-scale evaluation
3. Conduct ablation studies on prompt engineering to isolate true reasoning improvements from presentation effects