---
ver: rpa2
title: 'Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation
  with Key Point Recall'
arxiv_id: '2410.23000'
source_url: https://arxiv.org/abs/2410.23000
tags:
- question
- questions
- point
- points
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LONG 2RAG is a benchmark for evaluating retrieval-augmented generation
  (RAG) systems on long-context and long-form tasks. It includes 280 complex questions
  across 10 domains, each with 5 real-world retrieved documents averaging 2,444 words.
---

# Long$^2$RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall

## Quick Facts
- **arXiv ID:** 2410.23000
- **Source URL:** https://arxiv.org/abs/2410.23000
- **Reference count:** 40
- **Key outcome:** LONG 2RAG is a benchmark for evaluating retrieval-augmented generation (RAG) systems on long-context and long-form tasks.

## Executive Summary
LONG 2RAG is a comprehensive benchmark designed to evaluate retrieval-augmented generation systems on long-context and long-form tasks. The benchmark includes 280 complex questions across 10 domains, each accompanied by 5 real-world retrieved documents averaging 2,444 words. It introduces Key Point Recall (KPR), a novel metric that measures how well models incorporate key points from retrieved documents into their responses. Through experiments with 9 large language models, the study demonstrates that closed-source models like GPT-4o generally outperform open-source alternatives, and that performance consistently degrades as document length increases.

## Method Summary
The LONG 2RAG benchmark evaluates RAG systems through a three-stage pipeline: document retrieval, generation, and evaluation. For each of the 280 questions spanning 10 domains, the system retrieves 5 documents with an average length of 2,444 words. The Key Point Recall (KPR) metric is introduced to measure generation quality by comparing model responses against human-labeled key points from retrieved documents. Three annotators evaluate each response for both correctness and key point coverage, with majority voting resolving disagreements. The evaluation considers both open-source and closed-source LLMs across various sizes, measuring performance as document length varies.

## Key Results
- Closed-source models like GPT-4o significantly outperform open-source models in long-context RAG tasks
- Performance decreases consistently as document length increases across all tested models
- The Key Point Recall metric effectively captures the quality of document incorporation in generated responses

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its realistic task design and comprehensive evaluation framework. By using real-world retrieved documents averaging 2,444 words and complex multi-domain questions, it creates challenging scenarios that mirror practical RAG applications. The KPR metric provides granular measurement of how well models extract and utilize information from retrieved documents, going beyond simple accuracy metrics. The three-annotator evaluation system with majority voting ensures robust assessment of both correctness and key point coverage, reducing individual bias in the evaluation process.

## Foundational Learning

**Retrieval-Augmented Generation (RAG)** - Combines information retrieval with text generation by first retrieving relevant documents then generating responses based on them. Needed because it enables models to access external knowledge without retraining. Quick check: Does the system first retrieve documents before generating responses?

**Long-context Processing** - Ability to handle extended input sequences beyond standard context window limits. Critical for processing lengthy documents in RAG systems. Quick check: What is the maximum context length the model can handle?

**Key Point Extraction** - Identifying and extracting the most salient information from documents. Essential for measuring whether models capture important content. Quick check: Are key points manually labeled by human annotators?

**Multi-domain Evaluation** - Testing across diverse subject areas to ensure generalizability. Important because performance can vary significantly across domains. Quick check: How many distinct domains are represented in the benchmark?

## Architecture Onboarding

**Component Map:** Query -> Document Retriever -> Document Concatenator -> LLM Generator -> Response Evaluator

**Critical Path:** Query → Document Retriever → LLM Generator → Key Point Recall Evaluator

**Design Tradeoffs:** The benchmark balances document length (challenging for models) against retrieval quality (affecting downstream performance). Using 5 documents per query provides sufficient context while remaining computationally feasible. The KPR metric prioritizes information coverage over fluency, potentially missing nuances in response quality.

**Failure Signatures:** Performance degradation with longer documents suggests context window limitations. Lower KPR scores indicate poor key point extraction or incorporation. Domain-specific performance drops reveal knowledge gaps in certain subject areas.

**3 First Experiments:**
1. Test baseline performance using only the top-1 retrieved document versus all 5 documents
2. Compare KPR scores when using different retrieval methods (BM25 vs dense retrieval)
3. Evaluate performance degradation by progressively increasing the number of retrieved documents

## Open Questions the Paper Calls Out
None

## Limitations
- Human-labeled key points introduce subjectivity despite using three annotators and majority voting
- The choice of 5 retrieved documents may not represent all real-world retrieval scenarios
- Limited examination of how different retrieval strategies impact final performance
- Analysis focused on specific models, potentially missing broader trends across available LLMs

## Confidence

**High confidence:** Benchmark construction methodology and finding that performance degrades with longer documents

**Medium confidence:** Comparative performance analysis between open-source and closed-source models, domain-specific findings

## Next Checks

1. Replicate main experiments using different retrieval methods (e.g., hybrid BM25-dense retrieval) to assess how retrieval strategy impacts KPR scores across model sizes

2. Conduct sensitivity analysis on KPR metric by varying number of key points annotated per document to determine metric stability

3. Test benchmark with additional model families not included in original study, particularly newer long-context specialized models, to validate generalizability of document length effects