---
ver: rpa2
title: Mitigating Multilingual Hallucination in Large Vision-Language Models
arxiv_id: '2408.00550'
source_url: https://arxiv.org/abs/2408.00550
tags:
- languages
- multilingual
- hallucination
- lvlms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-stage framework to address multilingual
  hallucinations in Large Vision-Language Models (LVLMs). The method improves multilingual
  instruction-following with supervised fine-tuning, then enhances hallucination resistance
  using a cross-lingual alignment method that automatically constructs hallucination-aware
  training data without manual annotation.
---

# Mitigating Multilingual Hallucination in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2408.00550
- Source URL: https://arxiv.org/abs/2408.00550
- Reference count: 40
- Key outcome: Introduces a two-stage framework that improves multilingual instruction-following with supervised fine-tuning and enhances hallucination resistance through cross-lingual alignment, achieving an average accuracy improvement of 19.0% on POPE across 13 languages.

## Executive Summary
This paper addresses the critical problem of multilingual hallucination in Large Vision-Language Models (LVLMs), where models generate incorrect answers in non-English languages. The authors propose a two-stage Multilingual Hallucination Removal (MHR) framework that first improves multilingual instruction-following through supervised fine-tuning, then constructs hallucination-aware training data automatically using cross-lingual alignment. The method demonstrates significant performance improvements across multiple multilingual benchmarks without requiring manual annotation of hallucination data in each language.

## Method Summary
The MHR framework operates in two stages. First, it performs multilingual supervised fine-tuning using the PALO dataset (2.08M instruction-answer pairs) to improve instruction-following ability across 13 languages. Second, it employs a cross-lingual alignment method that generates multiple responses per query and identifies hallucination-aware pairs by comparing semantic distances to English hallucination references. These pairs are then used for Direct Preference Optimization (DPO) training to teach the model to prefer non-hallucinating responses. The approach eliminates the need for manual hallucination annotation in each target language.

## Key Results
- Achieves an average accuracy improvement of 19.0% on the POPE MUL benchmark across 13 languages
- Demonstrates substantial reductions in hallucination rates while increasing qualified content generation
- Shows strong performance across both high-resource and low-resource languages, validating the framework's effectiveness
- Reduces annotation costs by automatically constructing hallucination-aware training data without manual labeling

## Why This Works (Mechanism)

### Mechanism 1
Poor instruction-following ability in non-English languages causes multilingual hallucinations. Multilingual supervised fine-tuning aligns the model's response format with instruction expectations across languages, reducing irrelevant or nonsensical outputs. The core assumption is that base LVLMs understand English instructions but fail to follow them in other languages due to insufficient multilingual training data.

### Mechanism 2
Multilingual hallucination is mitigated by automatically constructing hallucination-aware training data without manual annotation. The cross-lingual alignment method generates multiple responses per language, then selects hallucination-aware pairs by comparing semantic distance to English hallucination and non-hallucination answers. The core assumption is that English hallucination datasets can serve as references to identify hallucinations in other languages through semantic alignment.

### Mechanism 3
Direct Preference Optimization (DPO) with the constructed hallucination-aware data reduces factually inaccurate outputs. The DPO loss function trains the model to prefer non-hallucinating responses over hallucinating ones using pairs from cross-lingual alignment. The core assumption is that DPO can effectively learn preferences from automatically constructed hallucination-aware pairs.

## Foundational Learning

- **Multilingual instruction-following ability**: Why needed - Non-English languages often produce irrelevant or nonsensical answers due to poor instruction comprehension. Quick check - What happens if a model receives a query in a language it wasn't fine-tuned on? (Expected: it may produce irrelevant or malformed responses.)

- **Cross-lingual semantic alignment**: Why needed - To automatically construct hallucination-aware training data without manual annotation for each language. Quick check - How does a translation model help in identifying hallucinations across languages? (Expected: by translating responses to a common language and comparing semantic similarity.)

- **Direct Preference Optimization (DPO)**: Why needed - To train the model to prefer non-hallucinating responses over hallucinating ones using the constructed data pairs. Quick check - What is the key difference between DPO and traditional RLHF in terms of training complexity? (Expected: DPO is simpler and more efficient as it doesn't require a separate reward model.)

## Architecture Onboarding

- **Component map**: Base LVLM → Multilingual SFT → Cross-lingual Alignment → DPO Training → Final Multilingual LVLM
- **Critical path**: Multilingual SFT → Cross-lingual Alignment → DPO Training
- **Design tradeoffs**: Manual data annotation vs. automatic data construction (tradeoff between data quality and scalability)
- **Failure signatures**: Poor instruction-following in non-English languages → High hallucination rates across languages → Ineffective preference learning
- **First 3 experiments**:
  1. Evaluate instruction-following ability in non-English languages before and after multilingual SFT
  2. Test the accuracy of cross-lingual alignment in identifying hallucination-aware pairs
  3. Measure hallucination reduction in the final model across different languages

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the cross-lingual alignment method perform when applied to languages with vastly different grammatical structures, such as Korean or Arabic, compared to more structurally similar languages like Spanish or French?
- **Open Question 2**: What is the optimal balance between the quantity and quality of generated hallucination-aware data pairs when varying the hyperparameter K in the cross-lingual alignment process?
- **Open Question 3**: How does the MHR framework's performance scale with the size of the LVLM, and what is the relationship between model size and hallucination mitigation effectiveness?

## Limitations

- Data quality uncertainty in automatically constructed hallucination-aware pairs may limit DPO training effectiveness
- Generalization to truly unseen languages remains untested, particularly low-resource languages with different linguistic structures
- Benchmark representativeness may affect generalizability as evaluations use extended versions of existing benchmarks

## Confidence

- **High Confidence**: The 19.0% average accuracy improvement on POPE MUL is well-supported by benchmark results
- **Medium Confidence**: The claim about poor instruction-following causing multilingual hallucinations is supported qualitatively but lacks quantitative causation evidence
- **Low Confidence**: The assertion that cross-lingual alignment can automatically construct hallucination-aware data without manual annotation lacks detailed validation of alignment quality

## Next Checks

1. Conduct a human evaluation study to assess the accuracy of automatically identified hallucination-aware pairs across multiple languages, comparing them against manually annotated pairs to quantify potential noise in the training data.

2. Evaluate the framework's performance on a set of languages not included in the original 13-language evaluation to test the method's generalization capabilities to truly unseen linguistic structures and resource levels.

3. Perform controlled experiments removing either the multilingual SFT stage or the cross-lingual alignment method to quantify the individual contributions of each component to the overall hallucination mitigation performance.