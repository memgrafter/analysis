---
ver: rpa2
title: Scaling Optimal LR Across Token Horizons
arxiv_id: '2409.19913'
source_url: https://arxiv.org/abs/2409.19913
tags:
- optimal
- token
- scaling
- size
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper empirically demonstrates that the optimal learning\
  \ rate for large language models decreases with longer training token horizons,\
  \ following a scaling law that enables zero-overhead hyperparameter transfer across\
  \ data sizes. The authors show that for a fixed model architecture, the optimal\
  \ learning rate scales as LR = BD^(-\u03B2) where \u03B2 is approximately 0.32 for\
  \ models \u2265 760M parameters."
---

# Scaling Optimal LR Across Token Horizons

## Quick Facts
- arXiv ID: 2409.19913
- Source URL: https://arxiv.org/abs/2409.19913
- Reference count: 34
- Optimal learning rate decreases with longer training token horizons following LR* = BD^(-β) with β ≈ 0.32 for models ≥ 760M parameters

## Executive Summary
This paper demonstrates that the optimal learning rate for large language models decreases systematically with longer training token horizons, following a power-law scaling relationship. Through extensive ablation studies across multiple model sizes (50M-2.7B parameters) and token horizons (25B-800B tokens), the authors show that practitioners can predict optimal learning rates for long training runs from shorter experiments. The key insight is that LR* = BD^(-β) enables zero-overhead hyperparameter transfer, where β is approximately 0.32 for models above 760M parameters. This finding challenges the conventional wisdom that learning rates can be transferred across token horizons using muP parametrization.

## Method Summary
The authors conducted ablation studies using Megatron codebase with GPT-3 style decoder-only transformers, training on the RefinedWeb dataset across multiple model sizes and token horizons. They systematically varied learning rates and token horizons, recording validation loss to fit quadratic curves and extract optimal learning rates. Scaling laws were then fitted to these optimal learning rates using least squares regression. Bootstrapping was employed to estimate variance in the optimal learning rate estimates. The methodology involved training models with weight decay 0.1, gradient clipping 1.0, cosine LR schedule, and qk-norm enabled, with batch sizes of 0.5M tokens.

## Key Results
- Optimal learning rate decreases with longer token horizons across all model sizes tested
- Scaling law LR* = BD^(-β) enables accurate prediction of optimal LR for long horizons from short ones
- Llama-1 likely used LR 2.5× higher than optimal for its 1T token training regime
- R² values for scaling law fits range from 0.96-0.99, indicating strong predictive power

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal learning rate decreases with longer token horizons due to diminishing returns from gradient signal accumulation
- Mechanism: As training progresses over more tokens, the effective batch size increases in terms of unique gradient updates, requiring smaller per-update learning rates to maintain stable convergence
- Core assumption: The relationship between token horizon and gradient signal accumulation is consistent across different model architectures
- Evidence anchors:
  - [abstract] "the optimal LR changes significantly with token horizon -- longer training necessitates smaller LR"
  - [section] "the optimal LR decreases with longer token horizons across model sizes"
  - [corpus] Weak evidence - no direct mention of gradient signal accumulation
- Break condition: If the dataset contains highly redundant tokens or the model reaches a plateau where additional tokens provide minimal gradient signal

### Mechanism 2
- Claim: Power-law scaling relationship enables zero-overhead hyperparameter transfer
- Mechanism: The optimal learning rate follows LR* = BD^(-β), allowing practitioners to predict optimal LR for long horizons from shorter ones by fitting scaling constants
- Core assumption: The scaling law parameters (B and β) are stable across different model architectures when model size exceeds certain threshold
- Evidence anchors:
  - [abstract] "the optimal LR for longer horizons can be accurately estimated from shorter horizons via such scaling laws"
  - [section] "R2 of these fits are in the range 0.99-0.96 (see Table 5 in Appendix A for exact values)"
  - [corpus] Moderate evidence - related work on hyperparameter transfer but not specifically for token horizons
- Break condition: If the scaling law breaks down for extremely long horizons (>800B tokens) or if architectural changes significantly alter the gradient dynamics

### Mechanism 3
- Claim: MuP parametrization does not enable LR transfer across token horizons
- Mechanism: While MuP allows LR transfer across model sizes, it doesn't address the fundamental relationship between LR and token horizon exposure
- Core assumption: The mechanisms governing optimal LR for model size scaling are distinct from those for token horizon scaling
- Evidence anchors:
  - [section] "we see that the optimal LR decreases with longer token horizons, demonstrating that LR does not transfer across horizons even with muP"
  - [corpus] Weak evidence - limited discussion of MuP limitations in token horizon context
- Break condition: If future research demonstrates that modified MuP-like approaches can capture both model size and token horizon scaling simultaneously

## Foundational Learning

- Concept: Power-law scaling relationships
  - Why needed here: The core mechanism relies on fitting LR* = BD^(-β) to predict optimal learning rates
  - Quick check question: Given LR* = 1e-3 at 100B tokens and β = 0.32, what would be the predicted LR* at 400B tokens?

- Concept: Quadratic fitting for finding optimal points
  - Why needed here: The optimal learning rate is estimated by fitting quadratic curves to validation loss vs log(LR) data
  - Quick check question: If a quadratic fit gives L(LR) = a*LR^2 + b*LR + c, how do you find the LR that minimizes this loss?

- Concept: Bootstrapping for variance estimation
  - Why needed here: Used to quantify uncertainty in optimal LR estimates by resampling experimental data
  - Quick check question: If you bootstrap 1000 samples from your data, what two statistics should you calculate to understand the uncertainty in your optimal LR estimate?

## Architecture Onboarding

- Component map:
  - RefinedWeb dataset loading and tokenization -> GPT-3 style decoder-only transformers -> Megatron training framework with Adam optimizer -> Validation loss tracking with quadratic fitting

- Critical path:
  1. Load dataset and create token batches
  2. Initialize model with specified architecture
  3. Train with varying LR and token horizons
  4. Record validation loss and fit quadratic curves
  5. Extract optimal LR and fit scaling laws

- Design tradeoffs:
  - Fixed batch size vs. variable batch size across model sizes
  - Quadratic vs. higher-order polynomial fitting for optimal LR estimation
  - Number of seeds vs. computational cost for variance estimation

- Failure signatures:
  - Diverging training runs (especially at high LR values)
  - Poor R² values for scaling law fits (<0.9)
  - Large variance in optimal LR estimates across seeds

- First 3 experiments:
  1. Run ablation study on 350M parameter model with 25B, 50B, 100B tokens at base LR × {0.25, 0.5, 1, 2, 4}
  2. Fit quadratic curves to validation loss vs log(LR) and extract optimal LR for each token horizon
  3. Plot optimal LR vs token horizon to verify decreasing trend before attempting scaling law fitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the β scaling exponent remain constant across all model sizes, or does it vary for very small models (below 760M parameters)?
- Basis in paper: [explicit] The authors note that "For smaller models (mainly 50m and 125m) a larger β is observed from the experiments" and consider Equation (3) valid only for models ≥ 760M parameters.
- Why unresolved: The paper explicitly states this limitation but does not investigate why smaller models show different scaling behavior or determine the exact threshold where β stabilizes.
- What evidence would resolve it: Experiments systematically testing β across a wide range of model sizes (e.g., 10M, 50M, 100M, 200M, 350M) with sufficient statistical power to determine the exact size range where β converges to the 0.32 value observed for larger models.

### Open Question 2
- Question: How does the optimal learning rate scaling behavior change when training with repeated tokens (multiple epochs over the same data) versus unique tokens?
- Basis in paper: [explicit] The authors conduct experiments showing "it is the token horizon rather than the number of unique tokens which determines the scaling" but only test this with a fixed dataset of 25B unique tokens.
- Why unresolved: The experiments are limited to one specific dataset size and do not explore whether the scaling relationship holds when the number of unique tokens is much smaller than the total token horizon.
- What evidence would resolve it: Experiments varying both the number of unique tokens and the total token horizon independently, testing scenarios where the same tokens are seen multiple times versus when the dataset grows proportionally with the token horizon.

### Open Question 3
- Question: How do architectural modifications beyond model size (such as different attention mechanisms, mixture-of-experts, or model width/depth ratios) affect the learning rate scaling law?
- Basis in paper: [explicit] The authors acknowledge that "model architectural modifications like mixture of experts, model width and depth, different attention types could plausibly interact with both the LR and token horizon" but defer this investigation to future work.
- Why unresolved: The study focuses on standard transformer architectures with fixed architectural ratios, leaving the interaction between architecture-specific design choices and learning rate scaling unexplored.
- What evidence would resolve it: Experiments systematically varying architectural parameters (e.g., attention mechanisms, MoE routing, width/depth ratios) while measuring the resulting optimal learning rate scaling across token horizons, potentially leading to architecture-specific scaling laws.

## Limitations

- Generalization uncertainty to architectures beyond GPT-3-style transformers
- Dataset-specific effects not quantified across different token distributions
- Single training run per configuration limits statistical confidence
- Implementation-dependent results may not transfer to other frameworks

## Confidence

**High Confidence**: The empirical observation that optimal learning rate decreases with longer training token horizons is robust and well-supported by the ablation studies across multiple model sizes. The quadratic fitting methodology for finding optimal points is standard and reliable.

**Medium Confidence**: The specific scaling law parameters (B and β) and their claimed stability across model sizes. While the fits show good R² values, the statistical significance of β ≈ 0.32 across architectures is not rigorously established. The prediction accuracy of 10-15% relative error is reasonable but could vary significantly for different model families.

**Low Confidence**: The application of these findings to extremely long training horizons (>800B tokens) and the extrapolation that Llama-1's LR was suboptimal by a factor of 2.5. These claims extend beyond the experimental validation range and make assumptions about the continuity of the scaling law.

## Next Checks

**Check 1: Cross-Dataset Validation** - Train the same model architectures on at least two additional datasets with different statistical properties (e.g., one with high token redundancy and one with diverse vocabulary). Verify whether the scaling law parameters change significantly and quantify the dataset dependency of the optimal learning rate.

**Check 2: Statistical Significance Analysis** - Run multiple independent training seeds for a subset of the configurations (at least 5 seeds for 2-3 model sizes and horizons). Calculate confidence intervals for the optimal learning rate estimates and perform hypothesis tests to determine whether β ≈ 0.32 is statistically different from alternative values.

**Check 3: Architecture Transfer Test** - Apply the scaling law predictions to a fundamentally different architecture (e.g., BERT-style encoder or recurrent model) within the same parameter range. Train models using predicted optimal LRs for various horizons and measure the prediction error compared to empirically finding the optimal LR for each configuration.