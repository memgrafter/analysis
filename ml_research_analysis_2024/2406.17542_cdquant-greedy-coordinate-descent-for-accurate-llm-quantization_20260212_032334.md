---
ver: rpa2
title: 'CDQuant: Greedy Coordinate Descent for Accurate LLM Quantization'
arxiv_id: '2406.17542'
source_url: https://arxiv.org/abs/2406.17542
tags:
- gptq
- quantization
- table
- owc-cd
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CDQuant introduces a greedy coordinate descent algorithm to improve
  the accuracy of LLM post-training quantization, addressing the limitations of GPTQ's
  single-cycle coordinate updates. The method optimizes the layer-wise reconstruction
  loss by iteratively selecting the coordinate that yields the largest loss reduction,
  enabling higher-quality quantization for 2-4 bit weights.
---

# CDQuant: Greedy Coordinate Descent for Accurate LLM Quantization

## Quick Facts
- arXiv ID: 2406.17542
- Source URL: https://arxiv.org/abs/2406.17542
- Authors: Pranav Ajit Nair; Arun Sai Suggala
- Reference count: 40
- Key outcome: Greedy coordinate descent algorithm improves LLM post-training quantization accuracy, achieving up to 10% perplexity gains over GPTQ in INT2 quantization

## Executive Summary
CDQuant introduces a greedy coordinate descent algorithm to address the limitations of GPTQ's single-cycle coordinate updates in post-training quantization. The method optimizes layer-wise reconstruction loss by iteratively selecting coordinates that yield the largest loss reduction, enabling higher-quality quantization for 2-4 bit weights. Experiments on Gemma and PaLM2 models demonstrate consistent perplexity improvements over GPTQ, with up to 10% gains in INT2 quantization of PaLM2-Otter and 5% improvements when integrated with state-of-the-art PTQ methods like QuIP and FrameQuant.

## Method Summary
CDQuant improves upon GPTQ by using a greedy coordinate descent approach that iteratively selects the coordinate update yielding the largest reduction in layer-wise reconstruction loss. Instead of updating all coordinates in a single cycle, CDQuant evaluates each coordinate's impact on the loss function and prioritizes those that provide the greatest improvement. The algorithm maintains the layer-wise reconstruction loss minimization framework while adding computational overhead through coordinate evaluation. This approach allows for higher-quality quantization, particularly for low-bit configurations (2-4 bits), and integrates seamlessly with existing quantization pipelines as a drop-in replacement for GPTQ.

## Key Results
- Achieves up to 10% perplexity improvement over GPTQ in INT2 quantization of PaLM2-Otter models
- Maintains near-optimal performance with reduced iterations (din/8) compared to GPTQ's din
- Improves performance when combined with state-of-the-art PTQ methods like QuIP and FrameQuant

## Why This Works (Mechanism)
CDQuant's greedy coordinate selection strategy addresses GPTQ's limitation of updating all coordinates in a single pass without considering their relative importance. By iteratively evaluating each coordinate's impact on the reconstruction loss and prioritizing updates that yield the largest reduction, CDQuant achieves more efficient optimization of the quantization process. The method leverages the layer-wise reconstruction loss framework but adds computational overhead through coordinate evaluation, which is justified by the significant improvements in quantization quality, particularly for low-bit configurations where accuracy degradation is most pronounced.

## Foundational Learning
- **Layer-wise reconstruction loss**: Loss function minimized during quantization to preserve activation similarity between full-precision and quantized weights. Needed to understand the optimization objective that CDQuant improves upon.
- **Greedy coordinate descent**: Optimization algorithm that iteratively selects the coordinate update yielding the largest improvement in the objective function. Required to grasp how CDQuant differs from GPTQ's single-pass approach.
- **INT quantization (INT2-4)**: Integer quantization with 2-4 bits per weight, representing a trade-off between model size and accuracy. Essential for understanding the target application space and performance gains.
- **Coordinate selection overhead**: Computational cost of evaluating each coordinate's impact on the loss function before updating. Important for assessing the trade-off between improved quality and increased runtime.

## Architecture Onboarding
- **Component map**: Input weights -> Layer-wise reconstruction loss computation -> Coordinate evaluation and selection -> Weight update -> Output quantized weights
- **Critical path**: The bottleneck is coordinate evaluation, which requires computing the loss reduction for each coordinate before selection. This adds O(n) overhead per iteration compared to GPTQ's O(1) per-coordinate update.
- **Design tradeoffs**: Quality vs. runtime - CDQuant improves quantization accuracy at the cost of increased computational overhead. The greedy approach provides better optimization but requires evaluating all coordinates per iteration.
- **Failure signatures**: Poor performance when coordinate evaluation becomes prohibitively expensive for extremely large models, or when the greedy selection strategy gets stuck in local minima that could be avoided by randomization.
- **First experiments**: 1) Compare perplexity of CDQuant vs GPTQ on INT4 quantization of Gemma-7B, 2) Measure runtime overhead of CDQuant on 7B parameter model, 3) Test CDQuant integration with QuIP on PaLM2-M model

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of CDQuant compare to QuantEase when both are optimized for speed rather than quality?
- Basis in paper: The paper compares CDQuant with QuantEase and notes that both achieve similar performance, but QuantEase uses a cyclic approach while CDQuant uses a greedy strategy.
- Why unresolved: The paper does not provide a direct comparison of runtime efficiency between CDQuant and QuantEase under equivalent quality constraints.
- What evidence would resolve it: A controlled experiment comparing the runtime of CDQuant and QuantEase on the same model and dataset, ensuring both achieve similar perplexity scores.

### Open Question 2
- Question: Can the Hessian clipping technique used in attention quantization be effectively applied to other layers or models to improve quantization quality?
- Basis in paper: The paper mentions that Hessian clipping improves performance for both GPTQ and CDQuant in attention layers.
- Why unresolved: The paper does not explore the application of Hessian clipping beyond attention layers or different model architectures.
- What evidence would resolve it: Experiments applying Hessian clipping to FFN layers or different model types, comparing perplexity and runtime with and without clipping.

### Open Question 3
- Question: What is the impact of reducing the number of iterations in CDQuant on models with different sizes and architectures?
- Basis in paper: The paper shows that CDQuant maintains near-optimal performance with significantly fewer iterations (e.g., din/8) on Gemma and PaLM2 models.
- Why unresolved: The study focuses on specific model families, and it is unclear if the same reduction in iterations would be effective for other model sizes or architectures.
- What evidence would resolve it: Testing CDQuant with reduced iterations on a diverse set of models, including those larger or smaller than those studied, to assess performance consistency.

### Open Question 4
- Question: How does the performance of CDQuant scale with increasingly large models, such as those with hundreds of billions of parameters?
- Basis in paper: The paper claims that CDQuant scales efficiently to models with hundreds of billions of parameters.
- Why unresolved: The experiments conducted are on models with up to tens of billions of parameters, leaving the scalability claim unverified for the largest models.
- What evidence would resolve it: Applying CDQuant to models with hundreds of billions of parameters and measuring performance, runtime, and memory usage compared to GPTQ.

## Limitations
- Computational overhead from coordinate evaluation may become prohibitive for extremely large models
- Limited experimental validation on models beyond tens of billions of parameters
- Performance improvements measured primarily through perplexity rather than downstream task accuracy

## Confidence
- Scalability to massive models: Medium confidence (tested up to tens of billions of parameters)
- Computational efficiency: Medium confidence (overhead not fully characterized across model sizes)
- Generalizability across architectures: Low confidence (primarily tested on Gemma and PaLM2)

## Next Checks
1. Benchmark CDQuant on models exceeding 10 billion parameters to verify scalability claims and measure wall-clock time overhead relative to GPTQ
2. Evaluate downstream task performance (GLUE, SuperGLUE, MMLU) rather than relying solely on perplexity metrics to assess practical impact
3. Conduct ablation studies testing different coordinate selection strategies and convergence thresholds to quantify sensitivity to algorithmic hyperparameters