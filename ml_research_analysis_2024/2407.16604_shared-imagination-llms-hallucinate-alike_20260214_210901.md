---
ver: rpa2
title: 'Shared Imagination: LLMs Hallucinate Alike'
arxiv_id: '2407.16604'
source_url: https://arxiv.org/abs/2407.16604
tags:
- questions
- question
- choice
- answer
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the surprising phenomenon that large language
  models (LLMs) can successfully answer each other's purely imaginary questions, even
  though these questions are based on fictional concepts. The authors introduce the
  Imaginary Question Answering (IQA) task, where one model generates multiple-choice
  questions about made-up concepts, and another model attempts to answer them.
---

# Shared Imagination: LLMs Hallucinate Alike

## Quick Facts
- arXiv ID: 2407.16604
- Source URL: https://arxiv.org/abs/2407.16604
- Reference count: 40
- Key finding: Large language models achieve high agreement rates on each other's purely imaginary questions, suggesting shared "imagination spaces"

## Executive Summary
This paper explores a surprising phenomenon where large language models (LLMs) can successfully answer each other's questions about purely fictional concepts. The authors introduce the Imaginary Question Answering (IQA) task, where one model generates multiple-choice questions about made-up concepts, and another model attempts to answer them. Despite the fictional nature of these questions, models achieve remarkably high correctness rates - 54% for direct questions and 86% for context-based questions - far exceeding random chance. This suggests that LLMs share a common "imagination space" where they agree on how to generate and interpret hallucinated content.

The study investigates this phenomenon through six research questions examining data characteristics, heuristics for correct choices, fictionality awareness, and universality across different model types. The findings reveal that model agreement on imaginary questions is not random but reflects systematic patterns in how models generate and interpret fictional content. This shared imagination space has important implications for understanding model homogeneity, hallucination detection, and computational creativity.

## Method Summary
The authors introduce the Imaginary Question Answering (IQA) task, where a source model generates multiple-choice questions about fictional concepts, and a target model attempts to answer them. The source model creates questions with four options (one correct, three distractors) about invented concepts like "flamango" (a fictional fruit). The target model then selects an answer based solely on the question stem and options. The study uses various LLM pairs including GPT-3.5/4, Llama, Vicuna, and others. Researchers analyze agreement patterns, investigate whether models recognize fictionality, examine universal agreement across model families, and test whether models are aware they're answering imaginary questions. The experiments compare performance on imaginary versus real questions and analyze response patterns to understand the underlying mechanisms of shared imagination.

## Key Results
- Models achieve 54% correctness on direct imaginary questions and 86% on context-based imaginary questions, significantly above random chance
- Model pairs show high agreement rates on imaginary questions, suggesting systematic patterns in how models generate and interpret fictional content
- Models demonstrate varying levels of awareness about fictionality, with some recognizing imaginary concepts while still providing answers
- The phenomenon persists across different model families, indicating a potentially universal aspect of LLM behavior

## Why This Works (Mechanism)
The shared imagination phenomenon emerges from the fundamental way LLMs generate text through next-token prediction. When models hallucinate fictional concepts, they do so following learned statistical patterns from their training data. These patterns create a shared space of plausible fictional content that different models independently discover and populate. The multiple-choice format provides structural constraints that guide models toward similar reasoning paths when evaluating options. Models appear to generate fictional content that fits within recognizable conceptual frameworks (fruits, animals, objects) with consistent attribute patterns, making it easier for other models to navigate this shared imagination space.

## Foundational Learning
- **LLM hallucination mechanics**: Understanding how models generate plausible but fictional content is crucial for interpreting the IQA results and their implications for model behavior
- **Cross-model agreement patterns**: Recognizing that agreement on imaginary questions reflects systematic rather than random behavior helps validate the shared imagination hypothesis
- **Fictionality awareness**: Understanding whether models recognize fictional content versus treating it as real information is key to interpreting their response strategies
- **Multiple-choice reasoning**: The structured format of multiple-choice questions provides constraints that may amplify agreement patterns compared to open-ended responses
- **Model homogeneity effects**: Recognizing how training data overlap and architectural similarities contribute to shared behavioral patterns across different models
- **Computational creativity bounds**: Understanding the limits of LLM creativity helps contextualize whether shared imagination represents genuine conceptual understanding or learned statistical patterns

## Architecture Onboarding

**Component map**: Source LLM (imagination generator) -> Question formatter -> Target LLM (answer selector) -> Agreement evaluator -> Analysis module

**Critical path**: The core experimental pipeline flows from source model generating fictional questions, through target model selecting answers, to agreement analysis. This path determines whether shared imagination exists by measuring cross-model agreement rates on purely imaginary content.

**Design tradeoffs**: The study prioritizes controlled comparison between imaginary and real questions while accepting that multiple-choice format may not fully capture open-ended reasoning capabilities. The choice to use various model pairs balances generality against potential confounding from training data overlap.

**Failure signatures**: Low agreement rates would suggest random hallucination patterns rather than shared imagination spaces. Poor performance on real questions would indicate general reasoning deficits rather than specific imaginary question challenges. Failure to recognize fictionality would suggest models treat all content uniformly regardless of plausibility.

**First experiments**:
1. Establish baseline agreement rates on real questions to demonstrate that imaginary question agreement exceeds what's expected from general model similarity
2. Test same-model pairs (source and target are identical) to establish upper bounds for agreement rates
3. Compare agreement rates across different model families to assess universality of the shared imagination phenomenon

## Open Questions the Paper Calls Out
The paper acknowledges that their analysis focuses on model agreement rather than absolute correctness, raising questions about whether high agreement rates truly indicate shared understanding or simply reveal systematic biases in model behavior. The authors note that their findings are based on a specific experimental setup with multiple-choice questions, which may not generalize to other question formats or real-world applications.

## Limitations
- Multiple-choice format may artificially constrain responses and inflate agreement rates compared to open-ended questions
- Study does not extensively explore the role of training data overlap between models, which could explain some observed agreement patterns
- Findings may not generalize to practical applications where models encounter mixed real and imaginary content in unstructured formats

## Confidence
- Model agreement on imaginary questions is a genuine phenomenon: High
- Shared imagination space exists across models: Medium (could reflect shared training data biases rather than genuine conceptual understanding)
- Implications for hallucination detection are valid: Medium (needs more empirical validation in practical scenarios)

## Next Checks
1. Conduct experiments with models trained on completely disjoint datasets to isolate whether agreement stems from shared imagination or shared training data
2. Test the IQA task with open-ended questions rather than multiple-choice to assess if agreement patterns hold
3. Implement a controlled study measuring model performance on imaginary versus real questions with identical structural properties to quantify the specific contribution of "shared imagination" versus general reasoning capabilities