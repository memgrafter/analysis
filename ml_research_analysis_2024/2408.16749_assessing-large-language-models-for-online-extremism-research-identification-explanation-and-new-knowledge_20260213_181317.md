---
ver: rpa2
title: 'Assessing Large Language Models for Online Extremism Research: Identification,
  Explanation, and New Knowledge'
arxiv_id: '2408.16749'
source_url: https://arxiv.org/abs/2408.16749
tags:
- extremist
- online
- post
- extremism
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated Bidirectional Encoder Representations from
  Transformers (BERT) and Generative Pre-Trained Transformers (GPT) for detecting
  and classifying online domestic extremist posts. Social media posts containing "far-right"
  and "far-left" ideological keywords were collected and manually labeled as extremist
  or non-extremist.
---

# Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge

## Quick Facts
- arXiv ID: 2408.16749
- Source URL: https://arxiv.org/abs/2408.16749
- Reference count: 25
- GPT models outperformed BERT models in detecting and classifying online extremist content

## Executive Summary
This study evaluates the effectiveness of BERT and GPT models for identifying and classifying online extremist content from Twitter. The researchers collected tweets containing far-right and far-left ideological keywords, manually labeled them as extremist or non-extremist, and further classified extremist posts into five contributing elements. Both binary classification (extremist vs. non-extremist) and multi-class element classification were performed. The study found that GPT models, particularly when using more detailed prompts, consistently outperformed BERT models in both tasks, with different GPT versions showing varying sensitivities to different types of extremism.

## Method Summary
The research collected Twitter posts containing extremist-related keywords on May 28th, 2023, and manually labeled them for supervised learning. For BERT evaluation, models were trained using varying training data sizes and tested for knowledge transfer between categories. GPT 3.5 and GPT 4 were evaluated using zero-shot learning with four distinct prompt styles: naïve, layperson-definition, role-playing, and professional-definition. Performance was measured using precision, recall, and F1-scores for both binary classification and multi-class element classification tasks.

## Key Results
- GPT models consistently outperformed BERT models in both binary and multi-class classification tasks
- More detailed prompts generally yielded better results, though not uniformly (Prompt 3 outperformed Prompt 4 despite containing less detailed information)
- GPT 3.5 showed better performance at classifying far-left extremist posts, while GPT 4 performed better at classifying far-right extremist posts

## Why This Works (Mechanism)
The superior performance of GPT models stems from their ability to understand context and nuance in language without requiring extensive training data, leveraging their pre-trained knowledge to identify extremist content through zero-shot learning. The detailed prompts help guide the models' reasoning processes, enabling them to better distinguish between extremist and non-extremist content based on linguistic patterns, rhetorical devices, and contextual cues that may not be captured by traditional supervised learning approaches.

## Foundational Learning
- Supervised vs. Zero-shot Learning: Understanding the difference between models trained on labeled data versus those leveraging pre-existing knowledge without specific training. Why needed: The study compares these two fundamental approaches to extremism detection. Quick check: Verify whether your task has sufficient labeled data or requires zero-shot approaches.
- Prompt Engineering: The art of crafting effective prompts to guide model behavior and outputs. Why needed: Different prompt styles significantly impacted GPT performance in this study. Quick check: Test multiple prompt variations to identify optimal formulations for your specific task.
- Binary vs. Multi-class Classification: Distinguishing between simple categorization and complex classification with multiple categories. Why needed: The study evaluated both approaches for extremism detection. Quick check: Determine whether your use case requires simple categorization or detailed classification.
- Model Version Sensitivity: Recognition that different model versions may exhibit varying performance characteristics. Why needed: GPT 3.5 and 4 showed different sensitivities to left vs. right extremism. Quick check: Test multiple model versions when accuracy is critical.

## Architecture Onboarding
Component Map: Twitter Data Collection -> Manual Labeling -> BERT Training/Evaluation -> GPT Prompt Testing -> Performance Analysis
Critical Path: Data Collection → Labeling → Model Evaluation → Comparison → Analysis
Design Tradeoffs: BERT offers controlled training but requires labeled data and may miss nuanced context; GPT provides flexibility through zero-shot learning but shows version-dependent biases and inconsistent performance across extremism types.
Failure Signatures: BERT failures typically manifest as low F1-scores due to insufficient or irrelevant training data; GPT failures appear as inconsistent sensitivities between model versions and prompt-dependent performance variations.
First Experiments: 1) Test BERT with minimal training data to establish baseline performance; 2) Compare GPT 3.5 and 4 performance on a small sample with all four prompts; 3) Evaluate BERT vs GPT performance on far-left vs far-right content separately.

## Open Questions the Paper Calls Out
- Does GPT model performance on extremism classification tasks generalize across different languages and social media platforms beyond English Twitter?
- What is the optimal balance between prompt detail and complexity for maximizing GPT performance on extremism classification tasks?
- How do different GPT model versions' sensitivities to extremism evolve over time with model updates?

## Limitations
- Study limited to English Twitter data collected on a single date, potentially not representative of broader online extremism landscape
- Manual labeling process introduces potential subjectivity and biases that could impact results
- Models not evaluated under adversarial conditions or against attempts to evade detection

## Confidence
High confidence in main conclusions regarding GPT vs BERT performance comparison, with recognition of study limitations including data representativeness and lack of adversarial testing.

## Next Checks
1. Replicate study using larger and more diverse corpus from multiple platforms and time periods to assess generalizability
2. Analyze manual labeling process to identify potential biases and develop more consistent labeling guidelines
3. Evaluate model performance under adversarial conditions including coded language and deliberate evasion attempts