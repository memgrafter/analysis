---
ver: rpa2
title: 'Tx-LLM: A Large Language Model for Therapeutics'
arxiv_id: '2406.06316'
source_url: https://arxiv.org/abs/2406.06316
tags:
- smiles
- datasets
- auroc
- drug
- tx-llm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tx-LLM is a generalist large language model fine-tuned from PaLM-2
  for therapeutic applications. It encodes knowledge about diverse drug modalities
  by training on 709 datasets spanning 66 tasks across the drug discovery pipeline,
  including small molecules, proteins, nucleic acids, cell lines, and diseases.
---

# Tx-LLM: A Large Language Model for Therapeutics

## Quick Facts
- arXiv ID: 2406.06316
- Source URL: https://arxiv.org/abs/2406.06316
- Reference count: 40
- Primary result: Achieves SOTA on 22/66 therapeutic tasks by fine-tuning PaLM-2 on diverse drug discovery datasets

## Executive Summary
Tx-LLM is a generalist large language model fine-tuned from PaLM-2 for therapeutic applications. It encodes knowledge about diverse drug modalities by training on 709 datasets spanning 66 tasks across the drug discovery pipeline, including small molecules, proteins, nucleic acids, cell lines, and diseases. Tx-LLM achieves state-of-the-art performance on 22 out of 66 tasks and competitive performance on another 21, particularly excelling at tasks combining molecular SMILES representations with text (e.g., cell line names or disease names). The model demonstrates positive transfer between tasks with diverse drug types and shows promise as an end-to-end tool for therapeutic development.

## Method Summary
Tx-LLM is developed by fine-tuning PaLM-2 on the Therapeutics instruction Tuning (TxT) dataset, which consists of 709 therapeutic datasets from the Therapeutics Data Commons (TDC) formatted as instruction-tuning prompts. The training uses a mixture of 70% 0-shot and 30% few-shot prompts, with dataset mixture ratios proportional to the number of datapoints in each dataset. The model leverages SMILES strings for small molecules and amino acid sequences for proteins, combined with textual context about diseases and cell lines.

## Key Results
- Achieves SOTA performance on 22 out of 66 therapeutic tasks
- Shows positive transfer from training on diverse drug types to small-molecule-only tasks
- Particularly excels at tasks combining molecular SMILES representations with text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tx-LLM benefits from contextual knowledge in PaLM-2's pretraining when handling SMILES + text tasks.
- Mechanism: The base LLM was pretrained on diverse natural language text, so it already encodes general domain knowledge (e.g., disease names, cell line descriptions). When fine-tuned on SMILES + text datasets, this prior context can be directly leveraged, boosting performance above or near SOTA.
- Core assumption: Text features in therapeutic datasets are represented in forms that align with PaLM-2's pretraining distribution.
- Evidence anchors:
  - [abstract]: "Tx-LLM is particularly powerful and exceeds best-in-class performance on average for tasks combining molecular SMILES representations with text such as cell line names or disease names, likely due to context learned during pretraining."
  - [section]: "Tx-LLM is particularly effective at combining SMILES and text... the text representations for diseases and cell lines... may be due to the text representations for diseases and cell lines, both because text is a natural representation for a LLM and because the base LLM may have learned context about these in its pretraining."
  - [corpus]: Weak - no explicit neighbor paper discusses text context benefits for LLMs in therapeutics.

### Mechanism 2
- Claim: Tx-LLM shows positive transfer from training on diverse drug types to small-molecule-only tasks.
- Mechanism: By fine-tuning on datasets that include proteins, nucleic acids, cells, and diseases alongside small molecules, the model learns shared representation patterns that generalize across modalities. This cross-modal training improves small-molecule performance beyond what molecule-only training achieves.
- Core assumption: Different drug types share enough underlying structure (string-based, biochemical context) that learning from one type benefits another.
- Evidence anchors:
  - [section]: "we find evidence of positive transfer between datasets with diverse drug types... training on datasets including biological sequences improves performances on molecular datasets."
  - [section]: "The model trained on all datasets performs better than the model trained on small molecule datasets when evaluated on 43 out of 56 small molecule datasets."
  - [corpus]: Weak - no neighbor paper explicitly discusses transfer between diverse drug modalities in a generalist LLM.

### Mechanism 3
- Claim: Providing context in prompts significantly improves Tx-LLM performance on complex, multi-task datasets.
- Mechanism: Context gives the model background grounding (e.g., assay descriptions, disease background) that disambiguates subtasks and aligns the model's generation with the task intent. Without it, the model lacks necessary situational cues.
- Core assumption: The LLM can use free-text context to disambiguate and focus its predictions on the correct subtask.
- Evidence anchors:
  - [section]: "we studied the effect of removing this context... we observed that removing the context reduced performance in 49 out of 66 datasets."
  - [section]: "this was especially true for the ToxCast dataset... Without providing assay-specific information in the context, the model would have no way of differentiating subtasks with different labels."
  - [corpus]: Weak - no neighbor paper explicitly tests context removal in biomedical LLM prompts.

## Foundational Learning

- Concept: Instruction tuning vs. domain fine-tuning
  - Why needed here: Tx-LLM uses TxT (Therapeutics instruction Tuning) to shape model behavior on therapeutic tasks. Understanding the difference between generic domain fine-tuning and instruction tuning is key to reproducing or extending the approach.
  - Quick check question: What distinguishes "instruction tuning" from standard "domain fine-tuning" in LLM adaptation?

- Concept: SMILES and sequence string representations
  - Why needed here: Tx-LLM represents small molecules as SMILES strings, proteins as amino acid sequences, etc. Understanding these string formats is necessary to prepare datasets and debug tokenization issues.
  - Quick check question: How do SMILES strings encode molecular topology, and what are common parsing pitfalls?

- Concept: Positive transfer in multi-task learning
  - Why needed here: The model gains performance on small-molecule tasks by also training on protein/nucleic-acid tasks. Recognizing when and why transfer occurs helps in dataset curation and architecture design.
  - Quick check question: What are the key conditions under which multi-task training leads to positive transfer?

## Architecture Onboarding

- Component map: TxT dataset builder -> PaLM-2 base LLM -> Tx-LLM fine-tuned model
- Critical path: Create TxT prompts -> Fine-tune PaLM-2 on TxT -> Evaluate on TDC datasets
- Design tradeoffs:
  - Using SMILES vs. graph representations: Tx-LLM trades off accuracy on molecule-only tasks for generalist capability.
  - Context inclusion: Adding context boosts performance but increases prompt length and inference cost.
  - Shot strategy: Random vs. KNN shots showed no clear winner, but KNN may help when dataset sizes are small.
- Failure signatures:
  - Invalid SMILES outputs (hallucinations)
  - Poor performance on molecule-only tasks vs. SOTA GNNs
  - Context removal leads to subtask confusion (e.g., ToxCast assays)
- First 3 experiments:
  1. Fine-tune a small Tx-LLM on molecule-only datasets vs. full TxT; compare small-molecule task performance.
  2. Remove context from prompts for one dataset type (e.g., ToxCast); measure performance drop.
  3. Swap shot selection method (random â†” KNN) for few-shot tasks; evaluate consistency across tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Tx-LLM performance scale with model size beyond the tested S and M variants?
- Basis in paper: [explicit] The authors observed performance improvements from Tx-LLM (S) to Tx-LLM (M) but did not test larger variants.
- Why unresolved: The paper only evaluated two model sizes (S and M), leaving open whether further scaling would continue to improve performance.
- What evidence would resolve it: Training and evaluating Tx-LLM with larger variants (L, XL) on the same TDC benchmarks would determine if performance gains continue with scale.

### Open Question 2
- Question: Can Tx-LLM's context generation process be automated rather than manually curated?
- Basis in paper: [inferred] The authors manually created contexts for each dataset based on literature search, which is time-consuming and may limit scalability.
- Why unresolved: Manual context creation is labor-intensive and may not be feasible for continuously expanding therapeutic datasets.
- What evidence would resolve it: Developing and evaluating automated context generation methods (e.g., using LLM-generated descriptions) while maintaining or improving performance would address this limitation.

### Open Question 3
- Question: How does Tx-LLM performance change when evaluated on prospective (time-separated) data versus retrospective data splits?
- Basis in paper: [explicit] The authors used TDC's recommended split methods but did not specifically test temporal validation, which better simulates real-world deployment.
- Why unresolved: Most TDC datasets use random or scaffold splits, which may overestimate real-world generalization.
- What evidence would resolve it: Re-evaluating Tx-LLM using temporal splits where test data is strictly from later time periods than training data would reveal performance in deployment-like conditions.

## Limitations

- Model's reliance on SMILES string representations limits performance on molecule-only tasks compared to specialized graph neural networks
- Inability to guarantee chemically valid SMILES outputs represents a fundamental limitation of autoregressive generation
- Data contamination concerns from PaLM-2 pretraining overlapping with TDC dataset features

## Confidence

- **High confidence**: Tx-LLM's superior performance on SMILES + text tasks
- **Medium confidence**: Positive transfer mechanism between diverse drug types
- **Medium confidence**: Context importance finding
- **Low confidence**: Absence of data contamination effects

## Next Checks

1. **Cross-modal transfer validation**: Fine-tune two versions of Tx-LLM - one on molecule-only datasets and one on the full TxT dataset - then compare performance on held-out small-molecule tasks to quantify the transfer benefit across drug modalities.

2. **Context component ablation**: Systematically remove individual context components (assay descriptions, disease background, etc.) from prompts across multiple dataset types to identify which context elements drive performance improvements versus which are redundant.

3. **SMILES validity verification**: Implement post-generation validation to measure the percentage of chemically valid SMILES strings produced by Tx-LLM across different task types, and compare with alternative molecular representation approaches like graph-based methods.