---
ver: rpa2
title: 'MNIST-Nd: a set of naturalistic datasets to benchmark clustering across dimensions'
arxiv_id: '2410.16124'
source_url: https://arxiv.org/abs/2410.16124
tags:
- clustering
- datasets
- dimensions
- across
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MNIST-Nd, a synthetic benchmark dataset for
  evaluating clustering algorithms in high-dimensional spaces. It addresses the lack
  of realistic high-dimensional clustering benchmarks by creating datasets with varying
  dimensions (2 to 64) using mixture variational autoencoders trained on MNIST.
---

# MNIST-Nd: a set of naturalistic datasets to benchmark clustering across dimensions

## Quick Facts
- arXiv ID: 2410.16124
- Source URL: https://arxiv.org/abs/2410.16124
- Authors: Polina Turishcheva; Laura Hansel; Martin Ritzert; Marissa A. Weis; Alexander S. Ecker
- Reference count: 27
- Primary result: Introduces synthetic benchmark datasets for evaluating clustering algorithms in high-dimensional spaces (2-64 dimensions) using mixture VAEs trained on MNIST

## Executive Summary
This paper addresses the critical gap in clustering evaluation by introducing MNIST-Nd, a set of synthetic benchmark datasets spanning dimensions from 2 to 64. The datasets are generated using mixture variational autoencoders trained on MNIST, maintaining consistent signal-to-noise ratios across dimensions while introducing realistic cluster overlap. The authors benchmark four common clustering algorithms (k-means, GMM, TMM, Leiden) and demonstrate that Leiden clustering maintains superior performance and stability in higher dimensions compared to other methods, with ARI scores remaining relatively stable while other methods show significant degradation.

## Method Summary
The method involves training mixture variational autoencoders (m-VAEs) with β-VAE framework on MNIST, scaling β inversely with latent dimension to maintain consistent regularization across datasets. The resulting embeddings are extracted in dimensions of 2, 4, 8, 16, 32, and 64, creating synthetic datasets that preserve the MNIST digit classes. The authors evaluate clustering performance using Adjusted Rand Index (ARI), assess dataset quality through classification accuracy (~90% target) and DISCO scores for cluster overlap, and test robustness through stability analysis and bootstrapping experiments.

## Key Results
- Leiden clustering outperforms distance-based (k-means) and density-based (GMM, TMM) methods in higher dimensions for both performance and stability
- ARI scores remain relatively stable for Leiden clustering across dimensions while other methods show significant degradation
- The datasets maintain consistent signal-to-noise ratios across dimensions with realistic cluster overlap similar to real-world data
- Classification accuracy remains approximately 90% across all dimensionalities, confirming consistent embedding quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mixture VAE generates datasets with consistent signal-to-noise ratio across dimensions
- Mechanism: By scaling β inversely with latent dimension, the KL divergence loss grows proportionally with dimensionality, maintaining comparable regularization across models
- Core assumption: The KL loss growth rate is approximately linear with dimensionality
- Evidence anchors:
  - [abstract] "The datasets maintain consistent signal-to-noise ratios across dimensions"
  - [section] "We use the β-VAE framework (Higgins et al., 2017) to scale the importance of the Kullback-Leibler (KL) loss. As the KL loss is unbounded and grows with the number of dimensions, we scale β inversely proportional to the dimensionality such that all datasets are similarly regularized to match the prior shape."
- Break condition: If KL loss growth deviates significantly from linearity with dimensions, or if the β scaling doesn't properly compensate for this growth

### Mechanism 2
- Claim: Leiden clustering maintains performance in high dimensions while other methods degrade
- Mechanism: Leiden operates on graph structures that preserve local density relationships, making it less sensitive to distance concentration in high dimensions
- Core assumption: Graph-based methods are inherently more robust to the curse of dimensionality than distance/density-based methods
- Evidence anchors:
  - [abstract] "Leiden clustering outperforms others in higher dimensions for both performance and stability, with ARI scores remaining relatively stable while other methods show significant degradation"
  - [section] "We choose k-means as an example for distance-based clustering, Gaussian and t-distribution mixture modelling (GMM and TMM) as density-based, and Leiden clustering as a graph-based clustering method."
- Break condition: If graph structure itself becomes distorted in high dimensions, or if local density estimation fails to capture meaningful structure

### Mechanism 3
- Claim: The datasets exhibit realistic cluster overlap similar to real-world data
- Mechanism: The mixture VAE's prior with ten components creates overlapping density modes, and DISCO scores confirm this overlap
- Core assumption: The learned embeddings naturally capture overlapping structure from the training data
- Evidence anchors:
  - [section] "As many real-world datasets have overlapping density between clusters, we want to ensure our toy datasets has it as well. To check this, we estimated DISCO scores (Anonymous, 2024) and search for density peaks"
  - [section] "The majority of points of our embeddings are scored around zero or below, suggesting a noticeable density overlap"
- Break condition: If the VAE learns to perfectly separate clusters or if the overlap is artificially inflated

## Foundational Learning

- Concept: Mixture Variational Autoencoders (m-VAEs)
  - Why needed here: The paper relies on m-VAEs to generate synthetic datasets with controllable dimensionality while maintaining realistic noise characteristics
  - Quick check question: What distinguishes a mixture VAE from a standard VAE, and why is this important for creating clustered datasets?

- Concept: Cluster validation metrics (ARI, DISCO, DBCV)
  - Why needed here: Multiple validation approaches are used to assess both clustering performance and the quality of the generated datasets
  - Quick check question: How does ARI differ from metrics like homogeneity or completeness, and when would each be most appropriate?

- Concept: The curse of dimensionality in clustering
- Why needed here: Understanding why traditional clustering methods fail in high dimensions is central to the paper's contribution
  - Quick check question: Why do pairwise distances become less informative in high-dimensional spaces, and how does this affect clustering algorithms?

## Architecture Onboarding

- Component map: MNIST dataset → m-VAE training with varying latent dimensions → latent space extraction → clustering algorithm benchmarking → performance evaluation
- Critical path: m-VAE training → latent space extraction → clustering → ARI computation → stability/robustness analysis
- Design tradeoffs: Realistic noise vs. perfect separability; computational cost of high-dimensional training vs. benchmarking utility; dataset complexity vs. interpretability
- Failure signatures: If classification accuracy varies significantly across dimensions; if Leiden doesn't outperform other methods; if DISCO scores indicate no cluster overlap
- First 3 experiments:
  1. Train m-VAEs with 2, 4, and 8 latent dimensions and verify that classification accuracy remains consistent
  2. Compute DISCO scores and density peaks for the 2D and 8D embeddings to verify cluster overlap
  3. Run k-means clustering on the 2D and 8D datasets and compare ARI scores to establish baseline degradation

## Open Questions the Paper Calls Out

- How do Leiden clustering results on MNIST-Nd compare to other high-dimensional synthetic datasets with similar noise characteristics?
  - Basis in paper: [explicit] The authors note that validation on additional datasets is needed, suggesting uncertainty about whether Leiden's performance is specific to MNIST-Nd or generalizes to other high-dimensional clustering benchmarks.
  - Why unresolved: The study only benchmarks on MNIST-Nd, which was specifically designed to have consistent signal-to-noise ratios across dimensions. Other synthetic datasets like DENSIRED or Gaussian mixtures may have different noise structures that could affect clustering algorithm performance differently.
  - What evidence would resolve it: Benchmarking Leiden clustering on multiple synthetic high-dimensional datasets with varying noise structures (e.g., DENSIRED, Gaussian mixtures with dimension-dependent variance) while maintaining the same experimental protocol would reveal whether Leiden's robustness is dataset-specific or generalizable.

- What is the relationship between the number of samples required for stable clustering and dimensionality in the MNIST-Nd datasets?
  - Basis in paper: [inferred] The authors observe that "ARI values decline with higher dimensions because more points are needed to confidently estimate distances and densities in high-dimensional space," but do not quantify this relationship.
  - Why unresolved: The paper mentions the decline in ARI with increasing dimensions but does not provide a formal analysis of how sample size requirements scale with dimensionality or at what point the sample size becomes insufficient for reliable clustering.
  - What evidence would resolve it: Conducting a systematic study varying the number of samples across different dimensionalities while measuring clustering stability would establish the scaling relationship between required sample size and dimensionality, potentially informing guidelines for clustering high-dimensional data.

- How do different dimensionality reduction techniques (t-SNE, UMAP, PCA) affect clustering performance on MNIST-Nd compared to clustering in the original high-dimensional space?
  - Basis in paper: [explicit] The authors note that "t-SNE embeddings in higher dimensional datasets look somewhat more condensed" and that "dimensionality reduction or clustering is commonly used" to uncover structure in high-dimensional data, suggesting this comparison is relevant.
  - Why unresolved: The study focuses on clustering in the original high-dimensional space and only uses t-SNE for visualization, leaving open the question of whether dimensionality reduction could mitigate the challenges of clustering in high dimensions.
  - What evidence would resolve it: Comparing clustering performance on MNIST-Nd using the original high-dimensional embeddings versus various dimensionality reduction techniques would reveal whether reducing dimensionality improves clustering outcomes and which methods are most effective for this task.

## Limitations
- The claims about consistent signal-to-noise ratios across dimensions rely on the assumption that KL divergence scales linearly with latent dimensionality, which may not hold precisely in practice
- The limited experimental scope (testing only four clustering algorithms) may not fully represent the space of possible clustering approaches
- While the synthetic nature of MNIST-Nd allows controlled experimentation, it remains unclear how well results generalize to truly naturalistic high-dimensional datasets with different characteristics

## Confidence
- **High confidence**: The basic methodology of using m-VAE for dataset generation and the overall framework for benchmarking clustering algorithms
- **Medium confidence**: Claims about Leiden's superior performance in high dimensions, pending broader algorithmic validation
- **Medium confidence**: The consistency of signal-to-noise ratios across dimensions, pending verification of KL scaling assumptions

## Next Checks
1. **KL Scaling Verification**: Empirically measure the KL divergence values across different latent dimensions during m-VAE training to verify that the β scaling maintains approximately constant regularization strength.

2. **Broader Algorithm Testing**: Extend the benchmarking to include additional clustering approaches (e.g., DBSCAN, HDBSCAN, spectral clustering) to determine if Leiden's apparent superiority is robust across the broader clustering algorithm landscape.

3. **Cross-Dataset Generalization**: Test the same clustering algorithms on MNIST-Nd and at least one real-world high-dimensional dataset (e.g., single-cell RNA sequencing data) to assess whether performance patterns observed in MNIST-Nd translate to practical applications.