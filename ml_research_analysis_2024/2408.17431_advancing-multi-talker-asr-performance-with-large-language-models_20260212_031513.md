---
ver: rpa2
title: Advancing Multi-talker ASR Performance with Large Language Models
arxiv_id: '2408.17431'
source_url: https://arxiv.org/abs/2408.17431
tags:
- speech
- training
- encoder
- multi-talker
- llm-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an LLM-based approach for multi-talker automatic
  speech recognition (ASR), addressing the challenge of recognizing overlapping speech
  from multiple speakers in conversational scenarios. The proposed method leverages
  serialized output training (SOT) with large language models (LLMs), using pre-trained
  speech encoders and LLMs fine-tuned on multi-talker datasets.
---

# Advancing Multi-talker ASR Performance with Large Language Models

## Quick Facts
- arXiv ID: 2408.17431
- Source URL: https://arxiv.org/abs/2408.17431
- Authors: Mohan Shi; Zengrui Jin; Yaoxun Xu; Yong Xu; Shi-Xiong Zhang; Kun Wei; Yiwen Shao; Chunlei Zhang; Dong Yu
- Reference count: 0
- One-line primary result: LLM-based approach surpasses traditional AED methods on LibriMix and achieves state-of-the-art performance on AMI meeting dataset

## Executive Summary
This paper introduces an LLM-based approach for multi-talker automatic speech recognition (ASR), addressing the challenge of recognizing overlapping speech from multiple speakers in conversational scenarios. The proposed method leverages serialized output training (SOT) with large language models (LLMs), using pre-trained speech encoders and LLMs fine-tuned on multi-talker datasets. The architecture concatenates speech embeddings with text embeddings as input to the LLM, which generates transcriptions for overlapping speech. Experimental results show that the proposed LLM-based approach surpasses traditional attention-based encoder-decoder (AED) methods on the simulated dataset LibriMix and achieves state-of-the-art performance on the real-world AMI meeting dataset, outperforming AED models trained with 1000 times more supervised data.

## Method Summary
The proposed LLM-based multi-talker ASR approach uses serialized output training (SOT) with a WavLM speech encoder and Vicuna-7B LLM. The system concatenates speech embeddings with text embeddings through a projector module, then feeds this combined representation into the LLM. The architecture employs LoRA fine-tuning for efficient adaptation to SOT-style transcriptions. Training involves either single-stage joint training or multi-stage sequential training where modules are unfrozen in order (projector → speech encoder → LoRA). The approach can freeze or fine-tune the speech encoder depending on whether it was pre-trained on LibriMix using AED.

## Key Results
- LLM-based approach outperforms AED methods on LibriMix two-speaker dataset
- Achieves state-of-the-art performance on AMI meeting dataset with up to 5 speakers
- Outperforms AED models trained with 1000× more supervised data on AMI
- Demonstrates superior long-context awareness and cross-utterance modeling capabilities

## Why This Works (Mechanism)

### Mechanism 1
LLM-based decoders outperform AED decoders in SOT-style multi-talker ASR due to superior long-context awareness and cross-utterance modeling capabilities. The LLM decoder, pre-trained on vast text data, inherently understands long-range dependencies and complex conversational contexts, which are critical for accurately transcribing concatenated SOT-style outputs from multiple speakers. The LLM's decoder architecture is better suited than AED's cross-attention for modeling long and context-rich SOT-style transcriptions.

### Mechanism 2
Using WavLM encoders fine-tuned on LibriMix with AED improves LLM-based model performance by providing high-quality speech representations. The WavLM encoder, when pre-trained and fine-tuned on multi-talker data, effectively captures overlapping speech features, which are then aligned with the LLM's text embedding space through the projector module. The WavLM encoder's ability to handle overlapped speech is transferable to the LLM-based architecture.

### Mechanism 3
LoRA fine-tuning of the LLM adapts its output style to the SOT-based multi-talker transcription format without requiring full model retraining. LoRA introduces low-rank adaptations to the LLM's weights, allowing it to learn the specific style of SOT-style transcriptions (including speaker change symbols and concatenated utterances) efficiently. LoRA can effectively adapt the LLM's output style without compromising its pre-trained language understanding capabilities.

## Foundational Learning

- Concept: Serialized Output Training (SOT)
  - Why needed here: SOT is the core methodology for generating training targets for multi-talker ASR by concatenating transcriptions in speaker emission order.
  - Quick check question: What is the role of the speaker change symbol in SOT transcriptions?

- Concept: Large Language Model (LLM) pre-training and fine-tuning
  - Why needed here: Understanding how LLMs learn language patterns and how fine-tuning adapts them to specific tasks is crucial for leveraging their capabilities in ASR.
  - Quick check question: What is the difference between LoRA fine-tuning and full fine-tuning of an LLM?

- Concept: Speech representation learning and alignment with text embeddings
  - Why needed here: The projector module bridges the gap between speech and text modalities, requiring knowledge of how speech features can be mapped to LLM-compatible embeddings.
  - Quick check question: Why is downsampling the speech representation beneficial for LLM processing?

## Architecture Onboarding

- Component map: WavLM encoder → downsampler → projector → LLM → output
- Critical path: Speech → WavLM → downsample → project → LLM → transcription
- Design tradeoffs: Encoder freezing vs. joint training, LoRA vs. full fine-tuning, downsampling rate selection
- Failure signatures: Poor alignment between speech embeddings and text embeddings, LLM output not matching SOT style, speaker counting errors
- First 3 experiments:
  1. Compare WavLM Base+ vs. Large as encoders with frozen vs. fine-tuned configurations.
  2. Evaluate single-stage vs. multi-stage training strategies with and without LoRA.
  3. Test different downsampling rates (n=5, 10, 15) on LibriMix validation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed LLM-based approach for multi-talker ASR perform on real-world datasets beyond AMI, such as those with more than 4 speakers or different conversational contexts?
- Basis in paper: [inferred] The paper mentions the potential of LLM-based models in handling speech processing tasks in complex and challenging scenarios, and demonstrates state-of-the-art performance on AMI. However, it does not explore performance on other real-world datasets or scenarios with more speakers.
- Why unresolved: The study focused on AMI and LibriMix datasets, leaving open the question of generalizability to other real-world scenarios with varying speaker counts and conversational contexts.
- What evidence would resolve it: Experiments on diverse real-world datasets with varying speaker counts and conversational contexts, demonstrating consistent performance improvements with the LLM-based approach.

### Open Question 2
- Question: What is the impact of different pre-trained speech encoders on the performance of the LLM-based multi-talker ASR approach, and how does it compare to the WavLM model used in the study?
- Basis in paper: [explicit] The paper uses WavLM as the speech encoder due to its suitability for multi-talker scenarios, but mentions that the performance advantage of the LLM-based system over the AED-based system is not very pronounced on LibriMix.
- Why unresolved: The study focuses on WavLM, leaving open the question of how other pre-trained speech encoders might impact performance.
- What evidence would resolve it: Comparative experiments using different pre-trained speech encoders (e.g., Hubert, Wav2Vec2.0) to evaluate their impact on the LLM-based multi-talker ASR approach's performance.

### Open Question 3
- Question: How does the LLM-based multi-talker ASR approach handle speaker diarization and counting in more complex scenarios with overlapping speech and noise, and what are the potential limitations?
- Basis in paper: [inferred] The paper demonstrates the LLM-based approach's performance on AMI-SDM, but mentions that it is less accurate in estimating the number of speakers compared to the AED model trained with large-scale supervised data.
- Why unresolved: The study does not explore the approach's limitations in handling speaker diarization and counting in more complex scenarios with overlapping speech and noise.
- What evidence would resolve it: Experiments on datasets with complex overlapping speech and noise, evaluating the LLM-based approach's accuracy in speaker diarization and counting, and identifying potential limitations.

## Limitations

- The paper lacks direct comparisons with other encoder types on the same task, making it difficult to assess whether WavLM is the optimal choice for the LLM-based architecture.
- The specific implementation details of the SOT-style transcription generation for AMI utterance groups, including the exact timestamps and speaker overlap handling, are not fully detailed.
- The paper does not provide direct comparisons of LoRA vs. full fine-tuning on the same task, leaving uncertainty about the optimal fine-tuning strategy.

## Confidence

- High confidence in the experimental setup and methodology.
- Medium confidence in the effectiveness of the WavLM encoder for the LLM-based architecture.
- Low confidence in the optimal fine-tuning strategy (LoRA vs. full fine-tuning) and the generalizability of the results to other encoder types.

## Next Checks

1. Conduct a direct comparison of the LLM-based approach with other encoder types (e.g., HuBERT, Wav2Vec2) on the same task to assess the optimality of WavLM.
2. Perform a detailed analysis of the SOT-style transcription generation process for AMI utterance groups, including the exact timestamps and speaker overlap handling, to ensure reproducibility.
3. Compare the performance of LoRA fine-tuning with full fine-tuning on the same task to determine the optimal fine-tuning strategy for the LLM-based architecture.