---
ver: rpa2
title: Extraction Propagation
arxiv_id: '2402.15883'
source_url: https://arxiv.org/abs/2402.15883
tags:
- have
- neural
- each
- networks
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Extraction Propagation (Xprop), a novel neural
  network architecture designed to address limitations of traditional backpropagation
  such as vanishing gradients and degradation. The core idea is to replace gradient
  propagation with vector-valued messages called extractions, computed via forward
  passes, which are then used to update parameters.
---

# Extraction Propagation

## Quick Facts
- arXiv ID: 2402.15883
- Source URL: https://arxiv.org/abs/2402.15883
- Reference count: 12
- Primary result: Introduces Extraction Propagation (Xprop), a neural network architecture that replaces gradient backpropagation with forward-pass computed vector-valued messages called extractions

## Executive Summary
This paper introduces Extraction Propagation (Xprop), a novel neural network architecture designed to address limitations of traditional backpropagation such as vanishing gradients and degradation. The core idea is to replace gradient propagation with vector-valued messages called extractions, computed via forward passes, which are then used to update parameters. The architecture consists of many small neural networks interacting in a directed acyclic graph structure, with primary and complementary propagators handling message passing and trainers computing predictions.

## Method Summary
Extraction Propagation replaces traditional backpropagation with a forward-pass-based approach where vector-valued messages called extractions are computed recursively through a directed acyclic graph. The architecture uses primary propagators to combine extractions from child nodes, complementary propagators to incorporate information from parent nodes, and trainers to generate predictions. Parameters are updated using gradient descent on computed extractions. The method employs two modes (stochastic and deterministic) and includes an alternative algorithm Xprop* to address potential synchronization issues.

## Key Results
- Introduces a theoretical framework for neural networks based on forward-pass message passing rather than gradient backpropagation
- Provides convergence analysis under optimal component neural network assumptions
- Presents several example architectures including tree-structured networks, multi-layer networks, and attention-based networks inspired by transformers
- Proposes Xprop* as an alternative algorithm to address synchronization issues in parameter updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The architecture learns to propagate information effectively throughout the network, ensuring that local predictions converge to global optima regardless of the random choices made during training.
- Mechanism: By using forward-pass computed vector-valued messages (extractions) instead of gradient backpropagation, the architecture creates a self-consistent learning process where each vertex learns to optimize its local prediction given the extracted information from its children and the complementary information from its parent.
- Core assumption: The component neural networks converge optimally to their respective functions f, f†, and g as described in the analysis section.
- Evidence anchors:
  - [abstract]: "we do back it up with some theory (based on the assumption that the component neural networks converge optimally)"
  - [section 5]: "Assuming convergence, we define the functions f, f†, g" and "Assuming that the aggregated neural networks at each vertex/arc converge to their optima"
  - [corpus]: Weak - No direct empirical evidence in corpus; only theoretical claims
- Break condition: If the component neural networks fail to converge optimally, or if the extraction spaces S and S† are too small to capture the necessary information for accurate predictions.

### Mechanism 2
- Claim: The architecture can learn complex tasks through an attention-based mechanism inspired by transformers.
- Mechanism: By creating multiple layers where each vertex aggregates information from all previous vertices through tree structures, the architecture can learn to attend to relevant parts of the input sequence, similar to how transformers work but using the extraction propagation framework.
- Core assumption: The parameter sharing scheme across layers and vertices allows the network to learn effective attention patterns.
- Evidence anchors:
  - [section 6.3]: "We now introduce a family of exnets inspired by the transformer [10] architecture" and detailed description of attention-based exnets
  - [corpus]: Weak - No empirical validation of attention mechanism performance
- Break condition: If the parameter sharing is too restrictive, preventing the network from learning diverse attention patterns, or if the tree structure cannot effectively capture long-range dependencies.

### Mechanism 3
- Claim: The alternative algorithm Xprop* addresses potential synchronization issues between parameter updates in different parts of the network.
- Mechanism: By running epochs on individual vertices and updating functions ξ and ξ' only at the end of each epoch, Xprop* ensures that parameter updates are consistent across the network and prevents rapid changes in one part from destabilizing other parts.
- Core assumption: The epoch-based approach with vertex ordering ensures consistency in the primary architecture.
- Evidence anchors:
  - [section 7]: "One potential issue with Xprop is that, given some v ∈ V \ I with v◁ /∈ I, the parameters of the primary propagator on v◁ may change too fast for the parameters of the primary propagator on v to keep up" and description of Xprop* solution
  - [corpus]: Weak - No empirical validation of Xprop* effectiveness
- Break condition: If the epoch-based approach is too slow for practical applications, or if the vertex ordering constraint becomes too restrictive for complex network topologies.

## Foundational Learning

- Concept: Directed Acyclic Graphs (DAGs)
  - Why needed here: The extraction network architecture is fundamentally based on DAG structures where information flows from leaves to root (up pass) and complementary information flows from root to leaves (down pass)
  - Quick check question: Can you explain why cycles would be problematic for the extraction propagation algorithm?

- Concept: Forward vs Backward Propagation
  - Why needed here: The key innovation is replacing backward gradient propagation with forward-pass computed extractions, which requires understanding the difference between these two approaches and their implications
  - Quick check question: What are the main advantages and disadvantages of computing messages via forward passes versus gradients via backward passes?

- Concept: Parameter Sharing in Neural Networks
  - Why needed here: The architecture allows for parameter sharing across different vertices and arcs, similar to how convolutional neural networks share parameters across spatial locations
  - Quick check question: How does parameter sharing affect the number of parameters that need to be learned and the generalization capabilities of the network?

## Architecture Onboarding

- Component map:
  - Primary propagator (ϕ): Neural network at each internal vertex that combines extractions from children
  - Complementary propagator (ϕ†): Neural network on each arc that combines complementary extraction from parent with primary extraction from sibling
  - Trainer (ψ): Neural network at each internal vertex that produces predictions from primary and complementary extractions
  - Tokeniser (τ): Function that converts raw input instances into initial primary extractions at leaves
  - Extraction spaces (S, S†): Euclidean spaces containing primary and complementary extractions respectively

- Critical path:
  1. Tokenisation of input at leaves
  2. Up pass: Recursive computation of primary extractions from leaves to root
  3. Prediction generation at root using trainer
  4. Down pass: Recursive computation of complementary extractions from root to leaves
  5. Parameter updates using computed extractions

- Design tradeoffs:
  - Tree-structured vs generic DAG: Tree structures simplify implementation but may limit expressiveness
  - Stochastic vs deterministic mode: Stochastic mode introduces randomness that may help exploration but makes behavior less predictable
  - Extraction space dimensionality: Larger spaces can capture more information but increase computational cost and risk overfitting
  - Parameter sharing: Can reduce parameters and improve generalization but may limit model capacity

- Failure signatures:
  - Vanishing information: If extractions become too small or uninformative during propagation
  - Mode collapse: If stochastic mode always selects the same parent, reducing exploration
  - Synchronization issues: If parameter updates in one part of the network destabilize other parts (addressed by Xprop*)
  - Computational inefficiency: If the recursive nature leads to excessive computation compared to standard backpropagation

- First 3 experiments:
  1. Implement a simple tree-structured exnet for binary classification on MNIST digits, comparing performance with standard feedforward networks
  2. Test the stochastic vs deterministic modes on a small dataset to observe differences in convergence behavior and final performance
  3. Implement Xprop* on a slightly more complex task to verify whether the epoch-based approach addresses potential synchronization issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Extraction Propagation (Xprop) architecture actually resolve the vanishing gradient and degradation problems in practice?
- Basis in paper: [explicit] The paper states "Currently the performance is conjectured as we are yet to implement the architecture" and "we do back it up with some theory"
- Why unresolved: The paper only provides theoretical analysis under convergence assumptions but no empirical validation has been conducted
- What evidence would resolve it: Empirical experiments comparing Xprop to standard backpropagation on benchmark tasks, measuring convergence speed and performance on deep networks

### Open Question 2
- Question: Can the Extraction Propagation architecture scale to very large networks with millions of parameters?
- Basis in paper: [inferred] The paper introduces an alternative algorithm Xprop* to address potential synchronization issues, suggesting scalability concerns
- Why unresolved: The paper notes "One potential issue with Xprop is that, given some v ∈ V \ I with v◁ /∈ I, the parameters of the primary propagator on v◁ may change too fast for the parameters of the primary propagator on v to keep up"
- What evidence would resolve it: Implementation and testing of Xprop on large-scale networks, measuring computational efficiency and parameter update stability

### Open Question 3
- Question: How does the choice between stochastic and deterministic modes affect the performance and convergence of Xprop?
- Basis in paper: [explicit] The paper describes both stochastic and deterministic modes and notes they differ in how complementary extractions are handled
- Why unresolved: The paper provides theoretical analysis but no experimental comparison between the two modes
- What evidence would resolve it: Empirical studies comparing stochastic and deterministic modes on various tasks, measuring convergence speed and final performance

## Limitations
- Theoretical framework lacks empirical validation with no implemented experiments
- Performance claims are conjectured rather than demonstrated through experiments
- Neural network architectures for propagators and trainers are not specified
- Tokenization schemes for mapping instances to extractions are undefined

## Confidence

- Mechanism 1 (Information propagation through extractions): Medium - Theoretical analysis provided but no empirical validation
- Mechanism 2 (Attention-based architecture): Low - Conceptual framework described but untested
- Mechanism 3 (Xprop* synchronization solution): Medium - Addresses a plausible issue but unproven effectiveness

## Next Checks

1. Implement a minimal tree-structured exnet on MNIST and compare convergence speed and accuracy against standard feedforward networks
2. Test stochastic vs deterministic modes on a small regression task to observe exploration-exploitation tradeoffs
3. Benchmark Xprop* on a sequential task where synchronization issues are likely to emerge, measuring stability compared to basic Xprop