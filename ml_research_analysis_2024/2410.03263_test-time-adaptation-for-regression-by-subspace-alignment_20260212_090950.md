---
ver: rpa2
title: Test-time Adaptation for Regression by Subspace Alignment
arxiv_id: '2410.03263'
source_url: https://arxiv.org/abs/2410.03263
tags:
- feature
- source
- subspace
- regression
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses test-time adaptation (TTA) for regression
  tasks, where models must adapt to target domain data without labels. Most existing
  TTA methods are designed for classification, which outputs class probabilities,
  making them unsuitable for regression models that output scalar values.
---

# Test-time Adaptation for Regression by Subspace Alignment
## Quick Facts
- arXiv ID: 2410.03263
- Source URL: https://arxiv.org/abs/2410.03263
- Reference count: 40
- Primary result: Proposes SSA for regression TTA, achieving R²=0.511 on SVHN→MNIST vs 0.406 for source model

## Executive Summary
This paper addresses test-time adaptation (TTA) for regression tasks, where models must adapt to target domain data without labels. Most existing TTA methods are designed for classification, making them unsuitable for regression models that output scalar values. The authors propose Significant-subspace Alignment (SSA), which detects the feature subspace in which regression model features are concentrated and aligns the target feature distribution to the source distribution within this subspace. Experiments on real-world regression tasks including digit recognition, face age prediction, head pose estimation, and housing price prediction show that SSA outperforms various classification-based TTA baselines, achieving higher R² scores.

## Method Summary
SSA detects the feature subspace where regression features concentrate using PCA on source features, then aligns target feature distributions to source distributions within this subspace. The method includes dimension weighting that prioritizes feature dimensions more influential on the output. For each target batch, SSA projects features onto the detected subspace, computes KL divergence between source and target projected feature distributions, and updates the feature extractor to minimize this divergence. The approach addresses the fundamental mismatch between classification-focused TTA methods and regression tasks by recognizing that regression features typically occupy low-dimensional subspaces.

## Key Results
- SSA achieves R²=0.511 on SVHN→MNIST digit regression compared to 0.406 for source model
- Outperforms classification-based TTA baselines across digit recognition, face age prediction, head pose estimation, and housing price prediction tasks
- Demonstrates effectiveness by aligning features only within the detected subspace rather than full-dimensional alignment

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Feature alignment in regression fails without subspace detection because regression features concentrate in low-variance subspaces
- Mechanism: Regression models trained with MSE loss produce features that lie in small subspaces with most dimensions having near-zero variance. Aligning all dimensions equally dilutes the adaptation signal, making naive alignment ineffective or harmful
- Core assumption: Regression models produce features distributed in low-dimensional subspaces compared to classification models
- Evidence anchors:
  - [abstract] "we found that naive feature alignment employed in existing TTA methods for classification is ineffective or even worse for regression because the features are distributed in a small subspace"
  - [section] "Table 1 shows the numbers of valid (having non-zero variance) feature dimensions and feature subspace dimensions"
  - [corpus] Weak - No direct corpus evidence for regression-specific feature subspace concentration

### Mechanism 2
- Claim: Dimension weighting prioritizes feature dimensions that significantly affect model output
- Mechanism: In regression, features are projected onto a single scalar output. Dimensions with higher projection weights onto this output line have greater impact. Weighting these dimensions during alignment improves adaptation effectiveness
- Core assumption: The regression output is computed as a weighted sum of features, making some feature dimensions more important than others
- Evidence anchors:
  - [abstract] "dimension weighting raises the importance of the dimensions of the feature subspace that have greater significance to the output"
  - [section] "dimension weighting raises the importance of the subspace dimensions with respect to their effect on the output"
  - [corpus] Missing - No direct corpus evidence for dimension weighting effectiveness

### Mechanism 3
- Claim: Subspace projection makes feature distributions approximately Gaussian, enabling accurate KL divergence computation
- Mechanism: Projecting high-dimensional features onto PCA subspace decorrelates them. With many original dimensions, the central limit theorem ensures projected features follow approximately Gaussian distributions, making KL divergence a valid alignment metric
- Core assumption: PCA projection decorrelates features and with sufficient original dimensions, the sum of uncorrelated terms approaches Gaussian distribution
- Evidence anchors:
  - [section] "features are likely to follow a Gaussian distribution when projected onto the feature subspace detected by subspace detection as the number of original feature dimensions increases by the central limit theorem"
  - [section] "subspace detection uses the PCA, the features projected onto the subspace are decorrelated"
  - [corpus] Weak - No direct corpus evidence for central limit theorem application in TTA

## Foundational Learning
- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA identifies the subspace where features concentrate, enabling targeted alignment instead of noisy full-dimensional alignment
  - Quick check question: What does PCA compute to identify the principal subspace?

- Concept: KL divergence between Gaussian distributions
  - Why needed here: KL divergence provides a principled metric for measuring and minimizing the distribution gap between source and target features
  - Quick check question: What are the two parameters needed to compute KL divergence between univariate Gaussians?

- Concept: Covariance matrix and eigenvalue decomposition
  - Why needed here: Computing the covariance matrix and its eigenvalues/eigenvectors is essential for PCA-based subspace detection
  - Quick check question: What does the magnitude of an eigenvalue represent in PCA?

## Architecture Onboarding
- Component map:
  - Feature extractor gϕ -> Linear regressor hψ -> Output
  - Subspace detector -> PCA computation -> Basis Vs and eigenvalues λs
  - Dimension weighting -> Weight computation αd = 1 + |w⊤vsd| -> Weighted alignment
  - Feature aligner -> Projection and KL divergence computation -> Feature extractor update

- Critical path:
  1. Pre-train source model on source data
  2. Compute source feature statistics (mean, covariance)
  3. Perform PCA to get Vs, λs
  4. For each target batch: project features, compute alignment loss, update feature extractor

- Design tradeoffs:
  - Number of subspace dimensions K: More dimensions capture more information but risk including noise; fewer dimensions are stable but may miss important features
  - Learning rate: Too high causes instability; too low results in slow/no adaptation
  - Batch size: Larger batches provide better statistics but require more memory

- Failure signatures:
  - R² scores worse than source model: Indicates over-alignment or incorrect subspace detection
  - Loss divergence: Suggests too many subspace dimensions or inappropriate learning rate
  - Feature reconstruction error high: Means target features don't fit in source subspace

- First 3 experiments:
  1. Run SSA with K=100 on SVHN→MNIST and verify R² improvement over source model
  2. Compare SSA with naive feature alignment (no subspace detection) on the same task
  3. Test sensitivity to K by varying it from 10 to 400 and observing R² changes

## Open Questions the Paper Calls Out
None

## Limitations
- Limited theoretical justification for why regression features concentrate in low-dimensional subspaces
- Performance sensitivity to selecting appropriate number of subspace dimensions K
- Dataset-specific effectiveness that may not generalize to all regression tasks

## Confidence
**High confidence**: The empirical results showing SSA outperforming baseline TTA methods on tested regression tasks. The mathematical formulation of PCA-based subspace detection and KL divergence-based alignment is sound.

**Medium confidence**: The claim that regression features concentrate in low-dimensional subspaces. While supported by variance analysis in Table 1, this observation may be dataset-specific rather than a universal property of regression models.

**Low confidence**: The theoretical justification for dimension weighting effectiveness and the Gaussianity assumption for projected features. These claims rely on intuitive reasoning rather than rigorous mathematical proofs.

## Next Checks
1. **Subspace dimensionality analysis**: Systematically vary K across orders of magnitude (10, 50, 100, 200, 400, 800) on each dataset and plot R² scores to identify optimal ranges and saturation points

2. **Cross-dataset generalization test**: Apply SSA trained on one regression task (e.g., digit recognition) to a different regression task (e.g., housing prices) without task-specific fine-tuning

3. **Ablation study on dimension weighting**: Implement SSA variants with dimension weighting disabled and compare performance. Additionally, test alternative weighting schemes (e.g., based on feature variance rather than output correlation)