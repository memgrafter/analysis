---
ver: rpa2
title: Uncovering Limitations of Large Language Models in Information Seeking from
  Tables
arxiv_id: '2406.04113'
source_url: https://arxiv.org/abs/2406.04113
tags:
- table
- llms
- tables
- information
- cell
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating large language
  models' (LLMs) capability in seeking information from tables. To overcome the unreliable
  evaluation caused by text similarity-based metrics, the authors introduce a new
  benchmark called TabIS, which adopts a single-choice question format with two options
  per question.
---

# Uncovering Limitations of Large Language Models in Information Seeking from Tables

## Quick Facts
- arXiv ID: 2406.04113
- Source URL: https://arxiv.org/abs/2406.04113
- Reference count: 40
- Primary result: LLMs exhibit poor table structure understanding and struggle with robustness against pseudo-relevant tables

## Executive Summary
This paper introduces TabIS, a new benchmark for evaluating large language models' (LLMs) capability in seeking information from tables. To address the unreliability of text similarity-based metrics in evaluating free-form generation, TabIS adopts a single-choice format with two options per question. Experiments on 12 representative LLMs reveal that while GPT-4-turbo performs marginally satisfactory, both proprietary and open-source models struggle significantly with table information seeking tasks, particularly in understanding table structures and maintaining robustness against irrelevant contextual tables.

## Method Summary
The authors construct the TabIS benchmark by transforming two high-quality table-to-text generation datasets (ToTTo and HiTab) into a single-choice question format. They develop a pipeline for generating options using four strategies: manual annotation, model-instruction, exam-judge, and human-assisted. The exam-judge method uses a weak LLM to generate unfaithful statements and a strong LLM to select the most deceptive option. The benchmark includes three subsets: basic TIS (B-TIS), TIS requiring structure understanding (SU-TIS), and TIS from multiple tables (M-TIS). Evaluation is conducted on 12 representative LLMs using accuracy as the primary metric, with a random guess baseline of 50%.

## Key Results
- GPT-4-turbo achieves marginally satisfactory performance, while other models perform inadequately on TabIS
- LLMs exhibit poor understanding of table structures, particularly in SU-TIS tasks
- Models struggle to balance TIS performance and robustness against pseudo-relevant tables
- Fine-tuned Llama2-13b-chat shows limited improvement over baseline, highlighting the challenge of the task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using single-choice format instead of text generation eliminates unreliable evaluation caused by surface-level metrics like BLEU and ROUGE
- Mechanism: Text generation outputs can vary greatly in style from reference answers, causing inconsistent metric scores even for correct responses. Single-choice forces models to select from two options, making evaluation binary and reliable
- Core assumption: LLMs can be evaluated more accurately when forced to choose between two answers rather than generate free-form text
- Evidence anchors:
  - [abstract]: "To avoid the unreliable evaluation caused by text similarity-based metrics, TabIS adopts a single-choice question format (with two options per question) instead of a text generation format"
  - [section]: "An example of this issue is illustrated in Figure 1 where a fine-tuned model's incorrect description receives higher BLEU/ROUGE scores than the correct output from GPT-3.5"

### Mechanism 2
- Claim: Exam-Judge method generates the most challenging options by using a weak LLM to generate unfaithful statements and a strong LLM to select the most deceptive one
- Mechanism: Weak LLM generates multiple candidate responses that are likely unfaithful to the table fact. Strong LLM filters and selects the option most similar to the reference answer but unfaithful, creating high-difficulty distractors
- Core assumption: The combination of weak and strong LLM agents can produce more deceptive options than manual annotation alone
- Evidence anchors:
  - [section]: "Exam-Judge (EJ). Given the table T and a set of cells C, we first instruct a weak LLM agent to describe the cells in natural language, yielding multiple candidate responses... Subsequently, a more advanced LLM agent is employed to identify responses that are unfaithful to the table"
  - [section]: "We find this method is good at generating difficult instances"

### Mechanism 3
- Claim: Adding pseudo-relevant tables creates a realistic challenge for LLMs by requiring them to distinguish between relevant and irrelevant information in retrieval-augmented settings
- Mechanism: Pseudo-relevant tables are generated to have similar structure and headers to the golden table but different data entries. This forces models to focus on content rather than superficial similarity when answering questions
- Core assumption: LLMs struggle to filter out irrelevant context when additional tables are provided, mimicking real-world retrieval-augmented scenarios
- Evidence anchors:
  - [section]: "To mimic this scenario, we investigate the effects of adding one pseudo-relevant table, which appears relevant to the main table but does not provide useful information to answer the question"
  - [section]: "LLMs struggle to balance between TIS performance and robustness against pseudo-relevant tables, especially for open-source models"

## Foundational Learning

- Concept: Table-to-text generation (TTG)
  - Why needed here: TabIS uses TTG datasets as the source for constructing questions, so understanding TTG is essential for grasping the benchmark's foundation
  - Quick check question: What is the difference between the input and output in a TTG task?
- Concept: Adversarial filtering
  - Why needed here: Used to divide instances into easy and hard categories based on model predictions, ensuring the benchmark includes challenging questions
  - Quick check question: How does adversarial filtering help in creating a high-quality benchmark?
- Concept: Table structure understanding
  - Why needed here: SU-TIS subset requires models to locate information based on positional cues, testing their ability to comprehend table layouts
  - Quick check question: Why is table structure understanding crucial for information seeking from tables?

## Architecture Onboarding

- Component map: TTG datasets → option generation (MI, MO, EJ, HA) → adversarial filtering → manual checking → benchmark construction (B-TIS, SU-TIS, M-TIS) → LLM evaluation
- Critical path: Option generation → adversarial filtering → manual checking → benchmark construction → LLM evaluation
- Design tradeoffs:
  - Single-choice format ensures reliable evaluation but may not reflect real-world free-form question answering
  - Using GPT-4 for option generation ensures high quality but introduces potential bias
  - Including pseudo-relevant tables increases difficulty but may not represent all real-world scenarios
- Failure signatures:
  - If model performance remains near-random across all subsets, the benchmark may be too difficult
  - If certain option generation strategies consistently produce easier questions, the balance needs adjustment
  - If pseudo-relevant tables are too obvious or too subtle, they won't effectively test robustness
- First 3 experiments:
  1. Test a simple LLM on B-TIS without pseudo-relevant tables to establish baseline performance
  2. Compare performance on easy vs. hard subsets to validate adversarial filtering effectiveness
  3. Test model robustness by evaluating performance with and without pseudo-relevant tables on M-TIS subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively improve LLMs' performance on table structure understanding (TSU) tasks?
- Basis in paper: [explicit] The paper identifies that LLMs exhibit poor performance on TSU tasks and the accuracy varies greatly across different positions and cell contents
- Why unresolved: The paper highlights the challenge but does not provide specific solutions or methods to improve TSU performance
- What evidence would resolve it: Experimental results showing improved TSU performance on the proposed TSU dataset after applying new training methods or model architectures

### Open Question 2
- Question: What are the most effective strategies for balancing TIS performance and robustness against pseudo-relevant tables?
- Basis in paper: [explicit] The paper finds that LLMs struggle to balance between TIS performance and robustness against pseudo-relevant tables, especially for open-source models
- Why unresolved: The paper identifies the problem but does not explore potential solutions or strategies to address this issue
- What evidence would resolve it: Comparative analysis of different approaches (e.g., data augmentation, model fine-tuning) showing improved performance and robustness on the M-TIS subset

### Open Question 3
- Question: How can we generate more diverse and challenging questions for the TabIS benchmark?
- Basis in paper: [inferred] The paper mentions that the templates used for generating TIS questions are relatively simplistic and suggests that richer and more diverse questions would enhance the quality of the benchmark
- Why unresolved: The paper does not explore methods for generating more diverse and challenging questions beyond the current approach
- What evidence would resolve it: Evaluation of the TabIS benchmark with a wider range of question types and difficulty levels, demonstrating improved differentiation of LLM capabilities

## Limitations
- The benchmark's reliance on GPT-4 for option generation may introduce evaluation bias, though manual verification mitigates this partially
- Single-choice format, while reliable for evaluation, doesn't reflect real-world free-form question answering scenarios
- Pseudo-relevant tables may not fully capture the complexity of real-world retrieval-augmented scenarios where multiple relevant tables coexist

## Confidence
- High: LLMs struggle with table structure understanding (SU-TIS results) and exhibit poor robustness against pseudo-relevant tables
- Medium: Exam-Judge method effectively generates challenging options, though direct comparison with alternative methods is limited
- Medium: TabIS provides more reliable evaluation than text similarity metrics, based on qualitative examples but limited quantitative validation

## Next Checks
1. Test whether Exam-Judge-generated options remain consistently more challenging than other generation strategies across multiple model families
2. Evaluate whether GPT-4 option generation bias systematically affects results by comparing with human-generated options on a subset
3. Assess whether fine-tuned models show improved robustness against pseudo-relevant tables through additional training on multi-table contexts