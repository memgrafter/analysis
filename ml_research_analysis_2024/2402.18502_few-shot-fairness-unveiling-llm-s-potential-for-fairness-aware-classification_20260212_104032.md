---
ver: rpa2
title: 'Few-Shot Fairness: Unveiling LLM''s Potential for Fairness-Aware Classification'
arxiv_id: '2402.18502'
source_url: https://arxiv.org/abs/2402.18502
tags:
- fairness
- llms
- gemini
- gpt4
- income
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether Large Language Models (LLMs) can
  achieve fair outcomes in classification tasks using in-context learning. We introduce
  a fairness framework with eight fairness definitions, modulated by varying levels
  of abstraction, and incorporate these rules into in-context learning prompts.
---

# Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification

## Quick Facts
- arXiv ID: 2402.18502
- Source URL: https://arxiv.org/abs/2402.18502
- Reference count: 40
- Key outcome: GPT-4 outperforms LLaMA-2 and Gemini in achieving fair outcomes in classification tasks through in-context learning, improving fairness metrics without sacrificing accuracy.

## Executive Summary
This study investigates whether Large Language Models can achieve fair outcomes in classification tasks using in-context learning. We introduce a fairness framework with eight fairness definitions modulated by varying levels of abstraction, and incorporate these rules into in-context learning prompts. Using the UCI Adult Income Dataset, we compare three LLMs—GPT-4, LLaMA-2, and Gemini—across zero-shot and few-shot setups, employing Retrieval Augmented Generation for example selection. Results show that GPT-4 outperforms other models in both accuracy and fairness metrics, while LLaMA-2 struggles in zero-shot scenarios and Gemini underperforms in few-shot setups. GPT-4 demonstrates improvements in fairness metrics such as Disparate Impact and True Positive Rate without sacrificing accuracy.

## Method Summary
The study employs in-context learning with three LLMs (GPT-4, LLaMA-2, Gemini) on the UCI Adult Income Dataset, using a fairness framework with eight definitions at two abstraction levels. For few-shot scenarios, Retrieval Augmented Generation selects 20 relevant demonstrations per test instance. Fairness prompts are constructed for each definition and abstraction level, and model outputs are evaluated using accuracy and multiple fairness metrics. The experimental design compares zero-shot versus few-shot performance and assesses the impact of abstract versus detailed fairness prompts.

## Key Results
- GPT-4 outperforms LLaMA-2 and Gemini in both zero-shot and few-shot scenarios for accuracy and fairness metrics
- GPT-4 achieves improvements in fairness metrics (Disparate Impact, True Positive Rate) without accuracy loss
- Generic fairness prompts perform comparably to specific fairness definitions
- LLaMA-2 struggles in zero-shot scenarios while Gemini underperforms in few-shot setups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 achieves higher fairness metrics without accuracy loss due to in-context fairness rules modulating prediction outcomes.
- Mechanism: When fairness definitions are included in the prompt (e.g., "Ensure Equal Opportunity on the basis of 'gender'"), GPT-4 adjusts its classification to equalize outcomes across demographic groups while maintaining overall prediction quality.
- Core assumption: GPT-4 can interpret fairness definitions as operational constraints and apply them during inference.
- Evidence anchors:
  - [abstract]: "GPT-4 demonstrates improvements in fairness metrics such as Disparate Impact and True Positive Rate without sacrificing accuracy."
  - [section]: "GPT-4 demonstrates superior results in both the zero-shot and few-shot scenarios."
  - [corpus]: Weak evidence; no directly related work explicitly shows GPT-4 internalizing fairness rules via in-context learning.
- Break condition: If fairness rules are too vague or contradictory, GPT-4 may fail to produce consistent adjustments, leading to degraded accuracy or fairness.

### Mechanism 2
- Claim: Retrieval Augmented Generation (RAG) improves fairness in few-shot learning by selecting contextually relevant demonstrations.
- Mechanism: RAG retrieves 20 most similar examples from the training set for each test instance, providing GPT-4 with examples that implicitly encode fair decision boundaries, thereby improving its predictions.
- Core assumption: Similar test instances have similar optimal fairness-aware decisions, so relevant demonstrations guide the model.
- Evidence anchors:
  - [section]: "We leverage the use of Retrieval Augmented Generation (RAG) for selecting in-context examples from 46,621 instances, for a given test instance."
  - [corpus]: No direct evidence in related work; RAG is mentioned for general few-shot learning, not fairness.
- Break condition: If retrieved examples are not representative of the fairness criteria, the model may inherit biased patterns.

### Mechanism 3
- Claim: Abstract fairness prompts (Rule A) perform comparably to detailed prompts (Rule D) because LLMs generalize fairness concepts.
- Mechanism: GPT-4 interprets high-level fairness instructions and applies them consistently without needing explicit contextual definitions, suggesting robust internal fairness understanding.
- Core assumption: LLMs have learned general fairness representations during pretraining that transfer across specific fairness definitions.
- Evidence anchors:
  - [section]: "We explore the impact of incorporating a generic notion of fairness into prompts... Utilizing a specific abstract prompt tailored for a particular fairness definition yields results comparable to those obtained through a generic fairness prompt."
  - [corpus]: Weak; no related work explicitly compares abstract vs. detailed fairness prompts in LLMs.
- Break condition: If the task is too specific or context-dependent, abstract prompts may fail to guide the model accurately.

## Foundational Learning

- Concept: Fairness definitions (e.g., Demographic Parity, Equal Opportunity)
  - Why needed here: These metrics define what "fair" means for classification and guide how prompts are constructed.
  - Quick check question: What is the difference between Equal Opportunity and Equalized Odds?
- Concept: In-context learning
  - Why needed here: The study relies on prompting rather than fine-tuning, so understanding how LLMs learn from demonstrations is critical.
  - Quick check question: How does the order of demonstrations affect few-shot learning outcomes?
- Concept: Retrieval Augmented Generation (RAG)
  - Why needed here: RAG is used to select demonstrations for few-shot learning; knowing its mechanics is key to reproducing results.
  - Quick check question: What embedding model is used to retrieve similar examples in this study?

## Architecture Onboarding

- Component map: Dataset loader -> Fairness prompt generator -> RAG retriever -> LLM inference -> Metric calculator
- Critical path:
  1. Load dataset and split test instances.
  2. For each test instance, generate fairness prompt.
  3. If few-shot, use RAG to retrieve 20 demonstrations.
  4. Send prompt to LLM (GPT-4, LLaMA-2, or Gemini).
  5. Parse prediction and compute fairness metrics.
- Design tradeoffs:
  - Zero-shot vs. few-shot: Simpler but less accurate vs. more accurate but requires retrieval overhead.
  - Abstract vs. detailed prompts: Less effort to construct vs. more explicit guidance.
- Failure signatures:
  - Low accuracy across all models → Prompt construction or dataset issue.
  - Fairness metrics not improving → Model does not understand fairness rules or retrieval is ineffective.
- First 3 experiments:
  1. Run zero-shot with no fairness prompt on GPT-4; verify baseline accuracy.
  2. Run zero-shot with Demographic Parity prompt; compare fairness metric improvement.
  3. Run few-shot with RAG retrieval; check if fairness metrics improve over zero-shot.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLMs perform on fairness metrics when applied to datasets from diverse countries and cultures?
- Basis in paper: [inferred] The authors acknowledge that their analysis may be impacted by selection bias due to using a dataset specific to the United States and note that LLMs exhibit bias towards English-speaking countries.
- Why unresolved: The paper focuses solely on the UCI Adult Income Dataset, which is specific to the United States, and does not explore how LLMs perform on datasets from other countries or cultures.
- What evidence would resolve it: Conducting experiments with LLMs on datasets from diverse countries and cultures, and comparing their performance on fairness metrics across these datasets.

### Open Question 2
- Question: How do different LLMs perform on fairness metrics when considering multiple demographic factors beyond gender?
- Basis in paper: [explicit] The authors state that their study focuses solely on one demographic, namely gender, and suggest that a more comprehensive study incorporating additional demographics could offer deeper insights.
- Why unresolved: The paper only considers gender as the protected attribute and does not explore how LLMs perform on fairness metrics when considering other demographic factors such as race, age, or religion.
- What evidence would resolve it: Conducting experiments with LLMs on datasets that include multiple demographic factors and comparing their performance on fairness metrics across these factors.

### Open Question 3
- Question: How do different LLMs perform on fairness metrics when using larger datasets and more complex tasks?
- Basis in paper: [inferred] The authors suggest that their study could be extended to a larger dataset and more complex tasks to gain a deeper understanding of how LLMs respond to fairness considerations.
- Why unresolved: The paper uses a relatively small dataset (47,621 instances) and focuses on a simple classification task. It does not explore how LLMs perform on fairness metrics when using larger datasets or more complex tasks such as natural language understanding or generation.
- What evidence would resolve it: Conducting experiments with LLMs on larger datasets and more complex tasks, and comparing their performance on fairness metrics across these settings.

## Limitations
- Evaluation restricted to single dataset (UCI Adult Income), limiting external validity
- Abstract vs. detailed fairness prompt comparison lacks statistical significance testing
- Does not explore interactions between different fairness definitions or scalability with dataset size

## Confidence

**High confidence**: GPT-4 outperforms LLaMA-2 and Gemini in both accuracy and fairness metrics across zero-shot and few-shot scenarios. The comparative performance differences between models are consistent and well-supported by results.

**Medium confidence**: The claim that abstract fairness prompts perform comparably to detailed prompts requires further validation, as statistical testing is absent and the effect may not generalize across different fairness definitions or datasets.

**Medium confidence**: The assertion that GPT-4 improves fairness metrics without accuracy loss is supported, but the mechanism by which in-context learning achieves this balance needs more detailed analysis.

## Next Checks
1. Replicate experiments across multiple fairness-relevant datasets (e.g., COMPAS, German Credit) to assess external validity and identify potential dataset-specific effects.
2. Conduct statistical significance testing on fairness metric comparisons, particularly between abstract and detailed prompt conditions, to determine if performance differences are meaningful.
3. Test the robustness of RAG-selected demonstrations by varying the number of retrieved examples (5, 10, 20, 50) and measuring the impact on both fairness metrics and accuracy to optimize the retrieval process.