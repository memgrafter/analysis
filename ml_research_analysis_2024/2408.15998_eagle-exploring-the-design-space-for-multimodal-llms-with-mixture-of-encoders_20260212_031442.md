---
ver: rpa2
title: 'Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders'
arxiv_id: '2408.15998'
source_url: https://arxiv.org/abs/2408.15998
tags:
- vision
- encoders
- visual
- language
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the design space for multimodal large language
  models (MLLMs) using a mixture of vision encoders. It systematically compares various
  fusion strategies, finding that simple channel concatenation is as effective as
  complex methods.
---

# Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders

## Quick Facts
- arXiv ID: 2408.15998
- Source URL: https://arxiv.org/abs/2408.15998
- Reference count: 34
- Key outcome: Mixture of vision encoders approach achieves state-of-the-art results on major MLLM benchmarks

## Executive Summary
Eagle is a systematic exploration of the design space for multimodal large language models (MLLMs) using a mixture of vision encoders. The study challenges conventional practices by demonstrating that unlocking vision encoders during training and using simple channel concatenation for fusion are highly effective strategies. The resulting Eagle model family achieves state-of-the-art performance across multiple benchmarks, particularly excelling in OCR and document understanding tasks.

## Method Summary
The Eagle framework explores vision encoder selection, fusion strategies, training recipes, and vision-language pre-alignment. It uses various vision experts pre-trained on different tasks, adapts them to handle high-resolution inputs, and combines them through channel concatenation. A pre-alignment stage is introduced to bridge the gap between vision-focused encoders and language tokens before joint training. The approach uses LLaVA-1.5 architecture with Vicuna-v1.5 as the language model, trained on LLaVA-595k for pre-training and Eagle1.8M for supervised fine-tuning.

## Key Results
- State-of-the-art performance on major MLLM benchmarks (MMMU, POPE, ScienceQA, etc.)
- Particularly excels in OCR and document understanding tasks
- Simple channel concatenation fusion outperforms complex mixing architectures
- Unlocking vision encoders during training significantly improves performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unlocking vision encoders during training is crucial for optimal MLLM performance
- Mechanism: Allows vision encoders to adapt their representations to better align with the language model's processing patterns
- Core assumption: Vision encoders have different representational biases that need adaptation when integrated with language models
- Evidence anchors:
  - Abstract: "Updating the CLIP encoder during SFT significantly improves performance at higher resolutions"
  - Section: "Updating the CLIP encoder during SFT significantly improves performance at higher resolutions but slightly reduces it when using the pre-training resolution"
- Break condition: If vision encoders are already perfectly aligned with language model requirements

### Mechanism 2
- Claim: Simple channel concatenation is as effective as complex fusion strategies
- Mechanism: Preserves complementary strengths of different vision experts without introducing additional complexity
- Core assumption: Distinct advantages of different vision encoders can be preserved through simple fusion methods
- Evidence anchors:
  - Abstract: "Simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures"
  - Section: "Channel Concatenation stands out with the best performance, expandability, and efficiency"
- Break condition: If specific fusion strategies provide domain-specific advantages that channel concatenation cannot capture

### Mechanism 3
- Claim: Pre-alignment stage significantly improves performance when integrating multiple task-specific vision encoders
- Mechanism: Individual vision experts are first aligned with the language model separately before joint training
- Core assumption: Vision encoders pre-trained on different tasks have representational inconsistencies that create difficulties when combined
- Evidence anchors:
  - Abstract: "We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens"
  - Section: "Table 5: The effectiveness of Pre-alignment... shows that the Pre-Align strategy more effectively mitigates the inherent biases of each vision expert"
- Break condition: If vision encoders are already sufficiently aligned or if pre-alignment introduces additional training complexity without performance gains

## Foundational Learning

- Concept: Vision encoder adaptation for high-resolution inputs
  - Why needed here: Standard pre-trained vision encoders are often trained on low-resolution images and cannot capture fine-grained details important for tasks like OCR
  - Quick check question: What are the two main approaches for adapting vision encoders to handle higher resolution inputs?

- Concept: Multi-expert fusion strategies
  - Why needed here: Combining different vision encoders can leverage their complementary strengths but requires careful fusion design
  - Quick check question: What are the three main categories of fusion strategies explored in this work?

- Concept: Pre-training vs. supervised fine-tuning in MLLMs
  - Why needed here: Understanding the different training stages and their purposes is crucial for implementing the proposed methodology
  - Quick check question: What is the key difference between the pre-training and supervised fine-tuning stages in the Eagle training strategy?

## Architecture Onboarding

- Component map: Vision encoders → Pre-alignment stage → Joint projector training → Supervised fine-tuning; Language model (Vicuna/LLaMA) integrated throughout
- Critical path: Vision encoder selection → High-resolution adaptation → Fusion strategy selection → Pre-alignment implementation → Joint training → Fine-tuning
- Design tradeoffs: Simple channel concatenation vs. complex fusion methods (performance vs. efficiency); Unfreezing encoders (better adaptation vs. risk of forgetting); Pre-alignment (improved synergy vs. additional training complexity)
- Failure signatures: Performance degradation with unfrozen encoders (catastrophic forgetting); No improvement with pre-alignment (encoders already aligned); Channel concatenation failures (conflicting representations)
- First 3 experiments:
  1. Test high-resolution adaptation methods (interpolation vs. tiling) with different vision encoders
  2. Compare fusion strategies (channel concatenation vs. sequence append vs. injection methods) on a subset of vision encoders
  3. Implement and evaluate pre-alignment strategy with one pair of vision encoders before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Eagle models change when incorporating vision encoders pre-trained on specialized tasks like 3D understanding or medical imaging, beyond the tasks already explored?
- Basis in paper: Inferred from the exploration of vision experts pre-trained on different tasks and the finding that incorporating additional vision experts leads to consistent gains
- Why unresolved: The paper focuses on a specific set of vision experts and does not explore the potential benefits of incorporating encoders trained on other specialized domains
- What evidence would resolve it: Comparative experiments evaluating Eagle models with and without vision encoders pre-trained on 3D understanding or medical imaging tasks

### Open Question 2
- Question: What is the optimal number of vision experts to incorporate in Eagle models, considering the trade-off between performance gains and computational cost?
- Basis in paper: Explicit discussion on the extension to multi-experts and the use of a round-robin scheme to select the optimal combination
- Why unresolved: While the paper identifies a specific combination as optimal, it does not provide a systematic analysis of how performance scales with the number of experts
- What evidence would resolve it: Ablation studies varying the number of vision experts in Eagle models, measuring performance and computational efficiency

### Open Question 3
- Question: How does the pre-alignment strategy affect the fine-tuning process and final performance of Eagle models compared to alternative approaches like joint training from scratch?
- Basis in paper: Explicit introduction of the pre-alignment stage and the finding that it enhances MLLM performance significantly
- Why unresolved: The paper presents pre-alignment as effective but does not compare it directly to other approaches like joint training from scratch
- What evidence would resolve it: Comparative experiments training Eagle models using different strategies: pre-alignment, joint training from scratch, and distillation

## Limitations
- The generalizability of findings to other MLLM architectures beyond the Vicuna/LLaMA family
- The scalability of the pre-alignment concept to larger vision expert pools
- The real-world deployment efficiency and cost-effectiveness of Eagle models

## Confidence
- **High Confidence**: The empirical findings regarding unlocking vision encoders during training and channel concatenation as fusion strategy
- **Medium Confidence**: The proposed pre-alignment stage mechanism and its impact on model performance
- **Low Confidence**: The scalability and efficiency claims for real-world applications

## Next Checks
1. Test vision encoder unlocking and channel concatenation strategies with different base MLLM architectures to assess generalizability
2. Systematically evaluate the pre-alignment stage with increasing numbers of vision experts to identify performance bottlenecks
3. Conduct comprehensive evaluation of Eagle models' performance, efficiency, and cost-effectiveness in practical applications beyond benchmarks