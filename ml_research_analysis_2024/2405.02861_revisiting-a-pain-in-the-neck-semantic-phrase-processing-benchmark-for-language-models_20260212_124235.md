---
ver: rpa2
title: 'Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language
  Models'
arxiv_id: '2405.02861'
source_url: https://arxiv.org/abs/2405.02861
tags:
- semantic
- tasks
- phrase
- linguistics
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LEXBENCH, a benchmark suite for evaluating
  language models on semantic phrase processing tasks. It includes ten tasks across
  four types of semantic phrases: idiomatic expressions, noun compounds, verbal constructions,
  and lexical collocations.'
---

# Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models

## Quick Facts
- **arXiv ID:** 2405.02861
- **Source URL:** https://arxiv.org/abs/2405.02861
- **Reference count:** 40
- **Primary result:** Introduces LEXBENCH, a benchmark evaluating 15 language models on semantic phrase processing tasks, showing large models outperform smaller ones but still lag behind fine-tuned models in semantic relation categorization.

## Executive Summary
This paper introduces LEXBENCH, a comprehensive benchmark suite for evaluating language models on semantic phrase processing tasks. The benchmark includes ten tasks across four types of semantic phrases: idiomatic expressions, noun compounds, verbal constructions, and lexical collocations. The authors test 15 language models with varying architectures and parameter scales using classification, extraction, and interpretation tasks. Results demonstrate that large language models like GPT-4 perform better than smaller models in most tasks, validating the scaling law. However, few-shot language models still lag behind fine-tuned models in semantic relation categorization, and human evaluation indicates that even strong models' performance is comparable to human level in semantic phrase processing.

## Method Summary
The authors compile ten datasets from existing sources covering four semantic phrase types and evaluate 15 language models (ranging from BERT and T5 to GPT-4 and Claude-3) using few-shot and zero-shot in-context learning, plus supervised fine-tuning for smaller models. Evaluation uses exact match, sequence accuracy, ROUGE-L, BERT Score, and perplexity depending on task type. Human evaluation is conducted with three graduate students annotating test examples. The benchmark is available at https://github.com/jacklanda/LexBench with datasets and evaluation scripts.

## Key Results
- Large language models like GPT-4 significantly outperform smaller models on semantic phrase processing tasks, validating the scaling law
- Few-shot language models still lag behind fine-tuned models in semantic relation categorization tasks
- Human evaluation shows strong models achieve performance comparable to human level in semantic phrase processing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large language models exhibit superior performance in semantic phrase processing tasks compared to smaller models, validating the scaling law.
- **Mechanism:** The scaling law posits that as model size increases, the model's performance on downstream tasks improves. In the context of semantic phrase processing, larger models have more parameters and can capture more complex patterns and relationships in the data.
- **Core assumption:** The data distribution for semantic phrase processing tasks is such that larger models can learn more effectively from it.
- **Evidence anchors:**
  - [abstract] "Results show that large language models like GPT-4 perform better than smaller models in most tasks, validating the scaling law."
  - [section] "As shown in Table 3 and Figure 4, the ability of Llama models increases steadily as the number of model parameters scales up. This finding strongly implies that the overall capacity of LLMs in semantic phrase processing is determined by the Scaling Law (Kaplan et al., 2020)."
  - [corpus] The corpus contains a diverse set of semantic phrase processing tasks, providing a comprehensive testbed for evaluating the scaling law.
- **Break condition:** If the data distribution for semantic phrase processing tasks is not well-suited for large models, or if the model architecture has inherent limitations, the scaling law may not hold.

### Mechanism 2
- **Claim:** In-context learning with few-shot examples can improve the performance of large language models on semantic phrase processing tasks.
- **Mechanism:** In-context learning allows models to adapt to a task by providing a few input-output pairs in the prompt, without any parameter updates. This enables models to learn from the provided examples and apply that knowledge to the task at hand.
- **Core assumption:** The few-shot examples provided in the prompt are representative of the task and can guide the model's understanding.
- **Evidence anchors:**
  - [abstract] "Through the experiments, we first validate the scaling law and find that, as expected, large models excel better than the smaller ones in most tasks. Second, we investigate further through the scaling semantic relation categorization and find that few-shot LMs still lag behind vanilla fine-tuned models in the task."
  - [section] "The results of experiments (cf. Table 3 and Table 5) highlight the effectiveness of ICL. We observed that many tasks had benefits, but some of the others had decayed in performance."
  - [corpus] The corpus includes tasks that can benefit from in-context learning, such as semantic relation categorization and extraction.
- **Break condition:** If the few-shot examples are not representative or if the model fails to learn from them effectively, in-context learning may not improve performance.

### Mechanism 3
- **Claim:** The performance of large language models on semantic phrase processing tasks is comparable to human-level performance in some cases.
- **Mechanism:** Large language models have been trained on vast amounts of data and can capture complex patterns and relationships in language. In certain tasks, they may have learned enough to match or even exceed human performance.
- **Core assumption:** The tasks in the benchmark are well-designed and representative of real-world semantic phrase processing challenges.
- **Evidence anchors:**
  - [abstract] "Third, through human evaluation, we find that the performance of strong models is comparable to the human level regarding semantic phrase processing."
  - [section] "Our results (cf. Table 3,5) show that humans only dominate in three out of ten tasks, with models exhibiting superior performance in the rest."
  - [corpus] The corpus includes tasks that require understanding and processing of semantic phrases, which are challenging for both humans and machines.
- **Break condition:** If the benchmark tasks are not representative of real-world challenges or if the human evaluation is not rigorous, the claim of human-level performance may not hold.

## Foundational Learning

- **Concept:** Semantic phrase processing
  - **Why needed here:** The benchmark is designed to evaluate models' ability to understand and process semantic phrases, which are crucial for many natural language processing tasks.
  - **Quick check question:** What are the different types of semantic phrases and how do they differ in terms of compositionality and idiomaticity?

- **Concept:** Scaling law
  - **Why needed here:** The scaling law is a key factor in understanding why larger models perform better on semantic phrase processing tasks.
  - **Quick check question:** How does the scaling law apply to semantic phrase processing tasks, and what are the implications for model design?

- **Concept:** In-context learning
  - **Why needed here:** In-context learning is a technique used to improve the performance of large language models on downstream tasks without parameter updates.
  - **Quick check question:** How does in-context learning work, and what are the benefits and limitations of this approach?

## Architecture Onboarding

- **Component map:** Benchmark -> Ten tasks (idiomatic expressions, noun compounds, verbal constructions, lexical collocations) -> Classification, extraction, interpretation -> 15 models (BERT, T5, GPT-4, Claude-3, etc.) -> Few-shot/zero-shot in-context learning or fine-tuning
- **Critical path:** Evaluate each model on each task using specified prompts and metrics, compare performance to human-level performance, analyze results to identify strengths and weaknesses
- **Design tradeoffs:** The benchmark includes a mix of tasks requiring different skills, allowing comprehensive evaluation but potentially revealing model-specific weaknesses
- **Failure signatures:** Consistent poor performance on certain phrase types may indicate lack of understanding or architectural limitations
- **First 3 experiments:**
  1. Evaluate a large language model on the benchmark and compare its performance to human-level performance
  2. Analyze the model's performance on each task to identify strengths and weaknesses
  3. Experiment with different in-context learning strategies to improve the model's performance on specific tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do language models perform on semantic phrase processing tasks involving discontinuous or non-adjacent multiword expressions?
- **Basis in paper:** [inferred] The paper acknowledges that the current dataset focuses on four types of common phrase phenomena and mentions that other semantic phrases may be distributed as a long-tailed distribution. It also suggests that future extensions could include complex function words, multiword named entities, and multiword terms.
- **Why unresolved:** The paper does not provide any empirical results or analysis on how language models handle discontinuous or non-adjacent multiword expressions, which are common in many languages and can be challenging for NLP systems.
- **What evidence would resolve it:** Conducting experiments with datasets containing discontinuous or non-adjacent multiword expressions and comparing the performance of language models on these tasks to their performance on the current benchmark would provide insights into their capabilities in this area.

### Open Question 2
- **Question:** To what extent do language models' performance on semantic phrase processing tasks improve with increased model size and in-context learning, and are there diminishing returns?
- **Basis in paper:** [explicit] The paper investigates the scaling law and finds that larger models generally perform better than smaller ones in most tasks. It also explores the effect of in-context learning with different numbers of demonstration examples.
- **Why unresolved:** While the paper shows a general trend of improved performance with larger models and in-context learning, it does not provide a detailed analysis of the relationship between model size, in-context learning, and performance. It also does not address whether there are diminishing returns or an optimal point beyond which further increases in model size or in-context examples do not significantly improve performance.
- **What evidence would resolve it:** Conducting experiments with a wider range of model sizes and varying the number of in-context examples could help identify the relationship between these factors and performance. Analyzing the marginal improvements gained from each increase in model size or in-context examples would help determine if there are diminishing returns.

### Open Question 3
- **Question:** How do language models compare to humans in terms of their ability to process and understand semantic phrases, and in which specific aspects do they excel or fall short?
- **Basis in paper:** [explicit] The paper includes a human evaluation component where three graduate students annotated examples from the test set of each task. It reports that humans only dominate in three out of ten tasks, with models exhibiting superior performance in the rest.
- **Why unresolved:** While the paper provides a comparison of overall performance between language models and humans, it does not delve into the specific aspects of semantic phrase processing where language models excel or fall short compared to humans. It also does not provide insights into the strategies or approaches used by humans that contribute to their performance.
- **What evidence would resolve it:** Conducting a more detailed analysis of the human evaluation results, including qualitative assessments of the errors made by language models and humans, could help identify the specific areas where language models outperform or underperform humans. Additionally, investigating the strategies and approaches used by humans during the annotation process could provide insights into how language models could be improved to better mimic human performance.

## Limitations

- Dataset sizes are relatively small for several tasks, particularly in the idiomatic expression and verbal construction categories, limiting statistical power for detecting performance differences
- Few-shot performance can be sensitive to prompt engineering, but the paper lacks detailed analysis of prompt sensitivity or ablation studies to quantify this effect
- The comparison between few-shot learning and fine-tuning is limited, as fine-tuned models were only evaluated on smaller architectures rather than larger models that could benefit from scaling effects

## Confidence

- **Scaling Law Validation:** High Confidence - Strongly supported by systematic evaluation across multiple model sizes and consistent performance patterns
- **Human-Level Performance Comparison:** Medium Confidence - Based on limited evaluation with 10 human raters and no reported inter-rater reliability measures
- **Few-Shot Learning Effectiveness:** Medium Confidence - Shows mixed results with some tasks improving and others declining, lacking systematic analysis of success factors

## Next Checks

1. **Statistical Power Analysis:** Conduct formal power analysis to determine minimum dataset size needed to reliably detect performance differences, particularly for smaller datasets (PIECES-IC and VOCAIC)

2. **Prompt Sensitivity Study:** Systematically vary prompt structure, few-shot example selection, and template wording across tasks to quantify variance in model performance and identify which aspects of prompt engineering have largest impact

3. **Fine-tuning Scaling Experiment:** Evaluate fine-tuned models across a broader range of model sizes (including larger architectures) to determine whether the gap between few-shot and fine-tuned performance persists at scale