---
ver: rpa2
title: 'Comparison of Open-Source and Proprietary LLMs for Machine Reading Comprehension:
  A Practical Analysis for Industrial Applications'
arxiv_id: '2406.13713'
source_url: https://arxiv.org/abs/2406.13713
tags:
- language
- performance
- arxiv
- information
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks open-source Large Language Models (LLMs)
  for efficient Question Answering (QA) in industrial applications, comparing their
  performance against proprietary models like ChatGPT-4 and ChatGPT-3.5. The study
  evaluates various open-source models, including LLaMA-2, Dolphin-2, and Mistral-7B,
  across multiple quantization strategies to assess trade-offs between performance
  and resource efficiency.
---

# Comparison of Open-Source and Proprietary LLMs for Machine Reading Comprehension: A Practical Analysis for Industrial Applications

## Quick Facts
- arXiv ID: 2406.13713
- Source URL: https://arxiv.org/abs/2406.13713
- Authors: Mahaman Sanoussi Yahaya Alassan; Jessica LÃ³pez Espejel; Merieme Bouhandi; Walid Dahhane; El Hassane Ettifouri
- Reference count: 19
- Open-source models can achieve competitive QA performance with 7-12% lower EM scores than ChatGPT-4 while requiring significantly less RAM

## Executive Summary
This paper benchmarks open-source Large Language Models (LLMs) for efficient Question Answering (QA) in industrial applications, comparing their performance against proprietary models like ChatGPT-4 and ChatGPT-3.5. The study evaluates various open-source models, including LLaMA-2, Dolphin-2, and Mistral-7B, across multiple quantization strategies to assess trade-offs between performance and resource efficiency. Results show that while proprietary models achieve higher Exact Match (EM) scores (87% for ChatGPT-4, 83% for ChatGPT-3.5), open-source alternatives like Mistral-7B-OpenOrca and Dolphin-2.6-Mistral-7B offer competitive performance with significantly lower resource requirements. Specifically, Mistral-7B-OpenOrca achieves an EM score of 83% with efficient RAM usage (7.63 GB), demonstrating that open-source models can provide viable, cost-effective solutions for industrial QA applications where computational resources are limited.

## Method Summary
The study evaluates LLMs on a custom dataset of 40 samples across banking, healthcare, and e-commerce domains, using OpenAI's ChatGPT-3.5 for dataset generation. Models are tested using prompting techniques with HTML preprocessing via BeautifulSoup and JSON postprocessing. Performance is measured using Exact Match (EM) and ROUGE metrics across various quantization strategies (Q2K, Q3KM, Q5KM). The evaluation includes both proprietary models (ChatGPT-4, ChatGPT-3.5) and open-source variants of LLaMA-2, Dolphin-2, and Mistral-7B, with resource usage tracked for each configuration.

## Key Results
- ChatGPT-4 achieves 87% EM score, ChatGPT-3.5 achieves 83% EM score on the evaluation dataset
- Mistral-7B-OpenOrca achieves 83% EM score with 7.63 GB RAM usage, matching ChatGPT-3.5 performance
- Quantization strategies show trade-offs between model size reduction and precision loss, with Q5KM offering better accuracy than Q2K at the cost of higher resource requirements
- Open-source models demonstrate competitive performance for industrial QA applications while requiring significantly fewer computational resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization enables open-source LLMs to run on CPUs with acceptable accuracy loss
- Mechanism: Weight quantization (e.g., Q3K, Q5KM) reduces model precision from FP32 to lower bit widths, shrinking memory footprint and speeding inference while preserving core reasoning patterns
- Core assumption: Quantization-induced precision loss affects only less critical weights, preserving task-relevant representations
- Evidence anchors:
  - [abstract] "Results show that while proprietary models achieve higher Exact Match (EM) scores... open-source alternatives like Mistral-7B-OpenOrca and Dolphin-2.6-Mistral-7B offer competitive performance with significantly lower resource requirements"
  - [section] "LLaMA-2 models showed varied performance based on quantization strategies, indicating a trade-off between model size reduction and precision loss"
  - [corpus] Weak - no direct quantization studies in neighbors, but similar MRC benchmarks exist
- Break condition: Excessive quantization causes collapse of attention patterns or context window handling

### Mechanism 2
- Claim: Open-source models achieve competitive QA performance through specialized fine-tuning datasets
- Mechanism: Training on domain-specific corpora (e.g., OpenOrca) enhances model ability to extract structured information from unstructured text
- Core assumption: Task-specific fine-tuning data captures the distribution of industrial QA scenarios better than general web data
- Evidence anchors:
  - [abstract] "Mistral-7B-OpenOrca achieves an EM score of 83% with efficient RAM usage (7.63 GB)"
  - [section] "Mistral-7B-OpenOrca leverages the advanced architecture of the Mistral-7B model, augmented through fine-tuning with the OpenOrca dataset"
  - [corpus] Weak - neighbors focus on MRC benchmarks but not specialized fine-tuning strategies
- Break condition: Fine-tuning data doesn't represent target domain, causing overfitting or poor generalization

### Mechanism 3
- Claim: Proprietary models maintain superiority through larger model capacity and better context handling
- Mechanism: Increased parameter count (175B for GPT-3) and sophisticated architecture enable superior long-range dependency modeling and nuanced context understanding
- Core assumption: Additional parameters provide meaningful representational capacity beyond diminishing returns
- Evidence anchors:
  - [abstract] "ChatGPT-4 achieves higher Exact Match (EM) scores (87% for ChatGPT-4, 83% for ChatGPT-3.5)"
  - [section] "Quantized models often struggled with maintaining contextual accuracy in longer sequences, whereas proprietary models like ChatGPT showed superior handling of complex contexts"
  - [corpus] Weak - neighbors mention MRC evaluation but don't compare parameter scales directly
- Break condition: Architectural innovations plateau, making parameter scaling less effective

## Foundational Learning

- Concept: Quantization strategies and their impact on model performance
  - Why needed here: Understanding Q2K, Q3KM, Q5KM tradeoffs is critical for selecting appropriate models
  - Quick check question: What's the typical accuracy drop when moving from Q3K to Q5KM for 7B parameter models?

- Concept: Exact Match (EM) and ROUGE metrics for QA evaluation
  - Why needed here: These metrics determine how we compare model performance against proprietary baselines
  - Quick check question: How does EM differ from F1-score in QA evaluation, and why might one be preferred over the other?

- Concept: CPU deployment constraints for LLMs
  - Why needed here: Industrial applications often require CPU inference due to cost or infrastructure limitations
  - Quick check question: What's the typical RAM requirement for running a 7B parameter model on CPU with Q5KM quantization?

## Architecture Onboarding

- Component map: Model selection -> Quantization choice -> Prompt engineering -> Evaluation pipeline -> Resource allocation
- Critical path: Quantization -> CPU inference -> RAM usage -> EM score -> Resource efficiency
- Design tradeoffs: Higher quantization bits = better accuracy but higher resource usage; specialized fine-tuning = better performance but less general applicability
- Failure signatures: Context window overflow, memory allocation errors, degraded EM scores with increased quantization
- First 3 experiments:
  1. Compare Q3K vs Q5KM quantization for Mistral-7B-OpenOrca on CPU to measure accuracy-resource tradeoff
  2. Test prompt engineering variations to improve EM scores for quantized models
  3. Profile RAM usage across different quantization levels to establish resource constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do quantized models like LLaMA-2 and Dolphin-2 handle long-context QA tasks compared to their non-quantized counterparts, and what architectural modifications could improve their performance in such scenarios?
- Basis in paper: [inferred] The paper notes that quantized models often struggle with maintaining contextual accuracy in longer sequences, which affects their EM scores. It also mentions that models like ChatGPT, with their sophisticated architectures, handle context better, resulting in higher EM and ROUGE scores.
- Why unresolved: The paper does not provide a detailed analysis of how specific quantized models perform on long-context tasks or what architectural changes could mitigate these issues.
- What evidence would resolve it: A comparative study evaluating quantized models on long-context QA tasks, along with experiments testing architectural modifications (e.g., improved attention mechanisms or context compression techniques) to enhance their performance.

### Open Question 2
- Question: What is the optimal quantization strategy for open-source LLMs to balance performance and resource efficiency in industrial QA applications, and how does this vary across different model architectures?
- Basis in paper: [explicit] The paper discusses various quantization strategies (e.g., Q2K, Q3KM, Q5KM) and their impact on performance and resource usage but does not identify a universally optimal strategy. It highlights trade-offs between model size reduction and precision loss.
- Why unresolved: The study evaluates multiple quantization strategies but does not determine which strategy is best for specific use cases or model architectures.
- What evidence would resolve it: Systematic experiments comparing the performance of different quantization strategies across various open-source models and use cases, identifying the most effective approach for each scenario.

### Open Question 3
- Question: How does the performance of open-source LLMs in QA tasks vary across different domains (e.g., healthcare, e-commerce, banking), and what domain-specific fine-tuning techniques could enhance their accuracy?
- Basis in paper: [explicit] The paper mentions that the dataset includes samples from diverse domains (banking, healthcare, e-commerce) but does not analyze domain-specific performance differences or fine-tuning strategies.
- Why unresolved: The study evaluates models on a mixed-domain dataset without isolating or comparing performance across individual domains.
- What evidence would resolve it: Domain-specific benchmarking of open-source LLMs, along with experiments testing fine-tuning techniques tailored to each domain to improve accuracy and relevance.

### Open Question 4
- Question: What are the long-term resource implications of deploying open-source LLMs in industrial settings, including maintenance, updates, and scalability, compared to proprietary models?
- Basis in paper: [inferred] The paper focuses on initial performance and resource usage but does not address long-term operational considerations such as maintenance costs, update frequency, or scalability challenges.
- Why unresolved: The study provides a snapshot of model performance and resource efficiency but lacks a longitudinal analysis of deployment sustainability.
- What evidence would resolve it: A longitudinal study tracking the performance, resource usage, and operational costs of open-source LLMs in industrial applications over time, compared to proprietary alternatives.

## Limitations

- Limited dataset scope with only 40 samples across three domains may not generalize to broader industrial applications
- Exclusive focus on Exact Match metric doesn't capture nuanced aspects of answer quality like partial correctness or reasoning coherence
- No testing on dynamic, real-world industrial data streams to assess model robustness in production environments

## Confidence

**High Confidence**: The performance rankings of proprietary vs. open-source models (ChatGPT-4 at 87% EM vs. Mistral-7B-OpenOrca at 83% EM) are well-supported by the direct comparison methodology and clear metric definitions. The resource efficiency findings (Mistral-7B-OpenOrca requiring 7.63 GB RAM) are concrete and verifiable through model profiling.

**Medium Confidence**: The claims about quantization strategies affecting performance have moderate support but lack detailed analysis of which quantization levels work best for specific model architectures. The assertion that specialized fine-tuning datasets improve industrial QA performance is plausible but not extensively validated across multiple domain-specific datasets.

**Low Confidence**: The study's generalization of findings to all industrial applications is weakly supported given the narrow dataset scope. The performance claims for open-source models as "viable, cost-effective solutions" lack long-term operational cost analysis including maintenance, retraining, and potential accuracy degradation over time.

## Next Checks

1. **Dataset Diversity Validation**: Test the top-performing open-source models (Mistral-7B-OpenOrca, Dolphin-2.6-Mistral-7B) on an expanded dataset with 200+ samples across 10+ industrial domains to verify if the 4% EM gap with ChatGPT-4 persists across broader use cases.

2. **Quantization-Aware Evaluation**: Conduct systematic ablation studies comparing different quantization levels (Q2K, Q3KM, Q5KM) across all model variants to establish precise accuracy-resource tradeoff curves, particularly focusing on how quantization affects performance on longer context sequences typical in industrial documents.

3. **Long-term Operational Assessment**: Deploy the top three models in a simulated industrial environment for one month, tracking not just inference accuracy but also API response consistency, memory leak patterns, and accuracy degradation when processing continuous data streams versus static datasets.