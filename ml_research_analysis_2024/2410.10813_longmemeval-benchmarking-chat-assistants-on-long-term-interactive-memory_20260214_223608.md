---
ver: rpa2
title: 'LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory'
arxiv_id: '2410.10813'
source_url: https://arxiv.org/abs/2410.10813
tags:
- memory
- question
- information
- user
- chat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LONG MEMEVAL, a comprehensive benchmark
  designed to evaluate the long-term memory capabilities of chat assistants across
  five core tasks: information extraction, multi-session reasoning, temporal reasoning,
  knowledge updates, and abstention. The benchmark features 500 human-curated questions
  embedded within scalable user-assistant chat histories, presenting significant challenges
  to existing memory systems.'
---

# LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory

## Quick Facts
- **arXiv ID:** 2410.10813
- **Source URL:** https://arxiv.org/abs/2410.10813
- **Authors:** Di Wu; Hongwei Wang; Wenhao Yu; Yuwei Zhang; Kai-Wei Chang; Dong Yu
- **Reference count:** 40
- **Primary result:** 30% accuracy drop on memorizing information across sustained interactions

## Executive Summary
This paper introduces LONG MEMEVAL, a comprehensive benchmark designed to evaluate the long-term memory capabilities of chat assistants across five core tasks: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. The benchmark features 500 human-curated questions embedded within scalable user-assistant chat histories, presenting significant challenges to existing memory systems. Evaluations show commercial chat assistants and long-context LLMs exhibit a 30% accuracy drop on memorizing information across sustained interactions. The authors propose a unified memory framework consisting of three stages: indexing, retrieval, and reading. Through extensive experiments, they identify effective memory design optimizations including session decomposition for value granularity, fact-augmented key expansion for indexing, and time-aware query expansion for temporal reasoning. These optimizations significantly improve both memory recall and downstream question answering performance on LONG MEMEVAL.

## Method Summary
LONG MEMEVAL evaluates long-term memory through a three-stage memory framework: indexing, retrieval, and reading. The benchmark uses 500 human-curated questions embedded in scalable chat histories with two standard settings (LONG MEMEVALS ~115k tokens, LONG MEMEVALM ~1.5M tokens). Evaluation employs GPT-4o as an automated judge using expert-written prompts with task-specific variations. The memory system operates through four control points: value granularity (session vs round decomposition), key expansion (plain vs fact-augmented), query formulation (direct vs time-aware expansion), and reading strategy (direct vs Chain-of-Note). Performance metrics include question answering accuracy, memory recall (Recall@k, NDCG@k), and analysis of accuracy drops across sustained interactions.

## Key Results
- Commercial chat assistants and long-context LLMs show a 30% accuracy drop on memorizing information across sustained interactions
- Session decomposition for value granularity significantly improves memory recall compared to session-level storage
- Fact-augmented key expansion and time-aware query expansion provide substantial performance gains
- Chain-of-Note reading strategy with structured JSON format improves both retrieval and answer quality

## Why This Works (Mechanism)
The three-stage memory framework addresses the fundamental challenge of maintaining context over extended conversations by systematically organizing information storage, retrieval, and usage. The key innovations work synergistically: session decomposition provides appropriate value granularity for different memory needs, fact-augmented key expansion ensures comprehensive indexing coverage, and time-aware query expansion enables effective temporal reasoning. The Chain-of-Note reading strategy leverages structured storage formats to enable more accurate information extraction from retrieved memories.

## Foundational Learning
- **Session decomposition vs round decomposition:** Session-level storage is simpler but less precise for tracking individual interactions. Round decomposition provides finer granularity for tracking specific exchanges but increases storage complexity. *Why needed:* Different memory tasks require different levels of precision in tracking conversational context. *Quick check:* Test whether round decomposition improves recall for questions about specific conversational turns.
- **Fact-augmented key expansion:** Plain keys capture basic entities while fact-augmented keys include additional contextual information. *Why needed:* Richer indexing improves retrieval accuracy for complex queries requiring multiple contextual elements. *Quick check:* Compare retrieval performance between plain and fact-augmented keys on multi-fact questions.
- **Time-aware query expansion:** Direct queries use original question formulation while time-aware expansion incorporates temporal reasoning elements. *Why needed:* Temporal reasoning requires explicit time-related information that may not be present in the original question. *Quick check:* Evaluate performance on temporal reasoning questions with and without time-aware expansion.

## Architecture Onboarding

**Component map:** Indexing -> Retrieval -> Reading -> Question Answering

**Critical path:** Chat History → Indexing (Value Granularity + Key Expansion) → Storage → Retrieval (Query Formulation) → Reading Strategy → Final Answer

**Design tradeoffs:** The framework balances between storage efficiency (session vs round decomposition) and retrieval precision, between indexing comprehensiveness (plain vs fact-augmented keys) and storage overhead, and between reading simplicity (direct) and accuracy (Chain-of-Note).

**Failure signatures:** Poor memory recall indicates inadequate value granularity selection, low question answering accuracy despite high retrieval metrics suggests ineffective reading strategy implementation, and systematic temporal reasoning failures point to insufficient query expansion.

**First experiments:**
1. Establish baseline performance using session decomposition, plain key expansion, direct query formulation, and direct reading strategy
2. Test individual optimization components by comparing session vs round decomposition on recall metrics
3. Evaluate Chain-of-Note reading strategy with structured JSON format against direct reading approach

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on GPT-4o as automated judge for answer quality assessment, introducing evaluation opacity
- Does not provide ablation analyses to quantify individual contribution of each optimization component
- Limited testing scope to GPT-4o and Claude 3 Opus, lacking generalizability assessment across diverse model architectures

## Confidence
- **30% accuracy drop claim:** Medium - supported by empirical evaluation but exact magnitude may vary with evaluation prompts
- **Effectiveness of proposed optimizations:** Medium - demonstrated improvements but lacking individual component contribution analysis
- **Generalizability of findings:** Low - limited to specific model architectures without broader validation

## Next Checks
1. Conduct human evaluation of a random sample of 50 questions to verify GPT-4o judge consistency and assess potential systematic biases in automated evaluation
2. Implement ablation study testing each memory optimization component independently to determine their individual contribution to performance improvements
3. Test the benchmark on additional long-context models beyond GPT-4o and Claude 3 Opus to evaluate generalizability of findings across different model architectures