---
ver: rpa2
title: 'Show, Don''t Tell: Learning Reward Machines from Demonstrations for Reinforcement
  Learning-Based Cardiac Pacemaker Synthesis'
arxiv_id: '2411.01750'
source_url: https://arxiv.org/abs/2411.01750
tags:
- pacemaker
- learning
- reward
- heart
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to learn reward machines from expert
  demonstrations for reinforcement learning (RL)-based cardiac pacemaker design. The
  key challenge addressed is the manual and error-prone process of designing reward
  functions for RL agents, especially in safety-critical systems like pacemakers.
---

# Show, Don't Tell: Learning Reward Machines from Demonstrations for Reinforcement Learning-Based Cardiac Pacemaker Synthesis

## Quick Facts
- arXiv ID: 2411.01750
- Source URL: https://arxiv.org/abs/2411.01750
- Reference count: 29
- Key outcome: Demonstrates that RL agents can learn to operate cardiac pacemakers from demonstrations, achieving zero errors over 350,000 steps across multiple arrhythmia types

## Executive Summary
This paper presents a novel approach to synthesizing cardiac pacemaker controllers using reinforcement learning guided by reward machines learned from expert demonstrations. The key innovation is replacing the traditional manual reward function design with a deep learning model that classifies pacemaker execution traces as correct or incorrect. Using an LSTM-based reward machine trained on labeled traces, an RL agent successfully learns to operate a cardiac pacemaker, achieving perfect performance across multiple arrhythmia types. The approach addresses the challenge of designing reward functions for safety-critical systems where traditional methods are error-prone and time-consuming.

## Method Summary
The method involves three main stages: First, labeled execution traces are generated by simulating a cardiac pacemaker with a heart model. Second, a deep learning model (LSTM or transformer) is trained to classify these traces, creating a learned reward machine that can distinguish correct from incorrect pacemaker actions. Third, a reinforcement learning agent is trained using policy gradient methods with the reward machine providing feedback. The system is validated across multiple arrhythmia types, demonstrating the learned pacemaker's ability to generate appropriate pacing responses without errors.

## Key Results
- The LSTM-based reward machine achieves 0.96 precision and 0.96 recall in classifying pacemaker actions
- The RL agent learns to operate the cardiac pacemaker with zero erroneous paces over 350,000 steps
- The approach successfully handles five different arrhythmia types plus healthy heart behavior
- LSTM architectures outperform transformers in this application

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reward machine can learn to distinguish correct from incorrect pacemaker actions from labeled execution traces.
- Mechanism: A deep learning model (LSTM or transformer) is trained to classify the next action in a sequence of pacemaker traces as correct or incorrect, using context windows of varying lengths.
- Core assumption: The sequence of actions in a trace contains sufficient information to determine whether the next action is correct, even without explicit knowledge of the underlying physiological state.
- Evidence anchors:
  - [abstract] "The proposed solution uses deep learning (LSTM and transformer architectures) to classify pacemaker execution traces as correct or incorrect, creating a learned reward machine."
  - [section] "We propose a rewards machine that learns to identify whether a pacemaker run was successful or not from its execution trace."
  - [corpus] Weak evidence - no direct mention of reward machines in related papers, but "Learning Reward Machines from Partially Observed Policies" and "Reward Machine Inference for Robotic Manipulation" suggest broader applicability of the concept.
- Break condition: If the underlying physiological state or timing constraints are not adequately captured in the action sequences, the model may fail to accurately classify correct and incorrect actions.

### Mechanism 2
- Claim: The learned reward machine can guide a reinforcement learning agent to learn to operate a cardiac pacemaker correctly.
- Mechanism: The RL agent uses the reward machine to evaluate its actions during training, receiving rewards based on the correctness of its pacing decisions as determined by the reward machine.
- Core assumption: The reward machine provides a reliable signal for the RL agent to learn the correct pacing strategy, even though the reward machine itself is learned from examples rather than explicitly programmed.
- Evidence anchors:
  - [abstract] "Using this reward machine, an RL agent successfully learns to operate a cardiac pacemaker, generating no erroneous paces over 350,000 steps across multiple arrhythmia types."
  - [section] "We deploy a stochastic policy gradient algorithm with reward machines for learning pacemaker functionality."
  - [corpus] Weak evidence - no direct mention of RL with learned reward machines, but "Reinforcement Learning with Stochastic Reward Machines" suggests the feasibility of this approach.
- Break condition: If the reward machine's classifications are not sufficiently accurate or do not capture the full complexity of the pacing requirements, the RL agent may learn an incorrect or suboptimal pacing strategy.

### Mechanism 3
- Claim: The approach can be applied to other complex, safety-critical, real-time control problems beyond cardiac pacemakers.
- Mechanism: The framework of learning a reward machine from expert demonstrations and using it to guide RL-based controller synthesis is generalizable to other domains where explicit reward function design is challenging.
- Core assumption: The core challenge of designing reward functions for RL is similar across different domains, and the proposed solution of learning reward machines from demonstrations is broadly applicable.
- Evidence anchors:
  - [abstract] "While our focus is on the cardiac pacemaker case study, the techniques presented may be applicable to other settings."
  - [section] "While the research we presented here focused on a pacemaker as a real world example of complex design, the field of medical device, both implanted and external, is vast."
  - [corpus] Weak evidence - no direct mention of applicability to other domains, but the general concept of reward machines and their inference is discussed in related papers.
- Break condition: If the specific characteristics of the domain (e.g., the nature of the demonstrations, the complexity of the control task) differ significantly from the pacemaker case study, the approach may not generalize effectively.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The pacemaker control problem is formalized as an MDP, where the RL agent learns a policy to maximize expected cumulative reward.
  - Quick check question: What are the components of an MDP, and how do they relate to the pacemaker control problem?

- Concept: Policy Gradient Methods
  - Why needed here: The RL agent is trained using a policy gradient algorithm, which directly optimizes the parameterized policy based on the expected return.
  - Quick check question: How does the policy gradient theorem allow for the computation of the gradient of the expected return with respect to the policy parameters?

- Concept: Sequence Classification
  - Why needed here: The reward machine is implemented as a sequence classifier, which learns to predict whether the next action in a pacemaker trace is correct based on the preceding context.
  - Quick check question: What are the key components of a sequence classification model, and how do they apply to the pacemaker trace classification task?

## Architecture Onboarding

- Component map:
  - Heart model -> Pacemaker automaton -> Reward machine (deep learning model) -> RL agent -> Validation framework

- Critical path:
  1. Generate labeled traces using the pacemaker automaton and heart model
  2. Train the reward machine (LSTM or transformer) on the labeled traces
  3. Train the RL agent using the reward machine to provide feedback
  4. Validate the learned pacemaker agent's performance

- Design tradeoffs:
  - Context window size: Longer context windows may provide more information but also increase the complexity of the reward machine
  - Deep learning architecture: LSTMs may be more suitable for capturing sequential dependencies, while transformers may be more effective with longer context windows
  - Exploration vs. exploitation: The RL agent needs to balance exploring different pacing strategies with exploiting the current best strategy based on the reward signal

- Failure signatures:
  - High error rate in the reward machine's classifications: Indicates that the reward machine may not have learned to accurately distinguish correct and incorrect actions
  - Poor performance of the RL agent: Suggests that the reward machine's feedback is not sufficient for the RL agent to learn an effective pacing strategy
  - Inability to generalize to unseen arrhythmia types: Implies that the learned pacemaker agent may not have captured the full complexity of the pacing requirements

- First 3 experiments:
  1. Train and evaluate the reward machine (LSTM and transformer) on the labeled traces, comparing their performance across different context window sizes
  2. Train the RL agent using the best-performing reward machine and assess its ability to learn a correct pacing strategy
  3. Validate the learned pacemaker agent's performance on a held-out set of arrhythmia types and compare it to a baseline pacemaker implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the reward machine learning approach generalize to more complex cardiac arrhythmias beyond those tested?
- Basis in paper: [inferred] The paper tests on five arrhythmia types and healthy heart behavior, but does not explore more complex or rare arrhythmias.
- Why unresolved: The paper focuses on a limited set of common arrhythmias, leaving open the question of scalability to more diverse cardiac conditions.
- What evidence would resolve it: Testing the approach on a broader range of arrhythmia types and comparing performance to traditional methods.

### Open Question 2
- Question: How would the performance change if using real patient data instead of simulated traces?
- Basis in paper: [explicit] The authors note they had to build their own dataset from scratch as real cardiac waveform databases contain ECG rather than intracardiac EGM signals.
- Why unresolved: The study uses simulated data, and real-world noise, variability, and clinical complexity may affect the learned reward machine's performance.
- What evidence would resolve it: Conducting experiments using actual patient EGM traces annotated by electrophysiologists.

### Open Question 3
- Question: Can the learned reward machine be verified for safety properties beyond empirical testing?
- Basis in paper: [explicit] The authors acknowledge they did not address safety guarantees directly and point to formal verification methods.
- Why unresolved: While the RL agent performs well empirically, formal guarantees of safety-critical properties remain unexplored.
- What evidence would resolve it: Applying formal verification techniques to the learned reward machine to prove safety properties mathematically.

### Open Question 4
- Question: How does the context window size affect the trade-off between computational efficiency and learning accuracy?
- Basis in paper: [explicit] The paper experiments with context sizes of 20, 30, 50, and 100 steps, observing minimal performance differences for LSTMs.
- Why unresolved: The study does not explore whether larger context windows might capture longer-term dependencies or if computational costs increase disproportionately.
- What evidence would resolve it: Systematic evaluation of context window sizes across a wider range, measuring both accuracy and computational requirements.

## Limitations

- The approach has only been validated on simulated cardiac pacemaker models, not actual medical devices
- The generalizability to other complex control problems beyond pacemakers remains theoretical with limited empirical validation
- The performance metrics, while impressive, are based on a limited set of arrhythmia types and may not capture the full complexity of real-world cardiac conditions

## Confidence

- **High Confidence**: The core methodology of using deep learning models to classify pacemaker execution traces as correct or incorrect is well-established and the reported performance metrics (0.96 precision/recall) are specific and verifiable.
- **Medium Confidence**: The claim that RL agents can successfully learn to operate cardiac pacemakers using the learned reward machine is supported by experimental results, but the simulation-based nature of validation reduces confidence in real-world applicability.
- **Medium Confidence**: The assertion that this approach can generalize to other complex, safety-critical control problems is plausible given related work on reward machines, but lacks empirical validation beyond pacemakers.

## Next Checks

1. Conduct ablation studies varying context window sizes and model architectures to identify the optimal configuration for different arrhythmia types and complexity levels.
2. Perform cross-validation on held-out arrhythmia types not seen during training to assess the generalization capability of the learned pacemaker agent.
3. Test the learned pacemaker agent against a wider range of baseline implementations, including clinically approved pacemaker algorithms, to establish performance benchmarks.