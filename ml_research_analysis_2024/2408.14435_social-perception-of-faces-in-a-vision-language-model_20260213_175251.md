---
ver: rpa2
title: Social Perception of Faces in a Vision-Language Model
arxiv_id: '2408.14435'
source_url: https://arxiv.org/abs/2408.14435
tags:
- social
- cosine
- images
- perception
- black
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates social perception in CLIP, a widely used
  vision-language model, by comparing text-image embeddings using socially validated
  prompts and synthetic face images with systematically varied attributes. The research
  finds that CLIP exhibits human-like social judgments, with facial expression having
  a stronger impact on perception than age and lighting.
---

# Social Perception of Faces in a Vision-Language Model

## Quick Facts
- arXiv ID: 2408.14435
- Source URL: https://arxiv.org/abs/2408.14435
- Authors: Carina I. Hausladen; Manuel Knott; Colin F. Camerer; Pietro Perona
- Reference count: 40
- Primary result: CLIP exhibits human-like social judgments of faces, with expression having stronger impact than age/lighting, and reveals intersectional biases particularly affecting Black women

## Executive Summary
This study investigates social perception in CLIP, a widely used vision-language model, by comparing text-image embeddings using socially validated prompts and synthetic face images with systematically varied attributes. The research finds that CLIP exhibits human-like social judgments, with facial expression having a stronger impact on perception than age and lighting. Notably, age, gender, and race significantly influence CLIP's social perception, revealing biases particularly affecting Black women across different ages and expressions. The experimental approach using controlled synthetic data yields sharper, more reliable observations than observational methods and demonstrates that uncontrolled visual attributes in wild-collected datasets can obscure important bias patterns. This method provides a robust framework for studying algorithmic bias and generating testable hypotheses for human studies.

## Method Summary
The study uses CLIP ViT-B/32 to compute cosine similarities between text and image embeddings, measuring social perception through difference in cosine similarity compared to neutral prompts. Synthetic face dataset (CausalFace) with systematically varied attributes (age, gender, race, facial expression, lighting, pose) is generated using StyleGAN2. Text prompts based on social psychology literature (SCM and ABC models) evaluate traits like warmth, competence, agency, belief, and communion. The âˆ† cosine similarity metric normalizes baseline biases. Two real-world datasets (FairFace, UTKFace) are used for comparison. Statistical analysis identifies patterns in social perception and potential biases across demographic groups.

## Key Results
- CLIP exhibits human-like social perception, making fine-grained associations between facial appearance and social traits
- Facial expression impacts social perception more than age, with lighting having comparable impact to age
- Age, gender, and race significantly influence CLIP's social perception, with distinctive U-shaped warmth trajectory and declining competence for Black women with age
- Non-protected attributes (expression, lighting, pose) can obscure bias patterns in wild-collected datasets, making experimental approaches more reliable

## Why This Works (Mechanism)

### Mechanism 1
Synthetic face generation allows systematic isolation of individual facial attributes without confounds from uncontrolled visual factors. By generating synthetic faces where each attribute (age, gender, race, expression, lighting, pose) is varied independently, the study eliminates spurious correlations between attributes that occur in wild-collected datasets. The synthetic generation process can produce realistic faces where attribute manipulations are perceptually effective and do not introduce unintended visual confounds.

### Mechanism 2
CLIP exhibits human-like social perception of faces by making fine-grained associations between facial appearance and social traits. CLIP's embedding space captures semantic dimensions of social perception (warmth, competence, agency, belief, communion) such that cosine similarity between face embeddings and trait-associated text embeddings reflects social judgments similar to human observers. CLIP's training on diverse vision-language pairs enables it to form meaningful associations between visual features and abstract social concepts, even though social perception was not an explicit training objective.

### Mechanism 3
Non-protected visual attributes (expression, lighting, pose) have comparable impact on social perception to protected attributes, potentially obscuring bias patterns in observational studies. When facial expression, lighting, and pose vary in uncontrolled ways, they create noise that can mask or confound the effects of protected attributes on social perception, leading to incorrect conclusions about algorithmic bias. The variation introduced by non-protected attributes is substantial enough to affect social perception measures and is not uniformly distributed across demographic groups.

## Foundational Learning

- **Concept**: Cosine similarity in embedding space
  - Why needed here: The study uses cosine similarity between CLIP image and text embeddings to quantify social perception associations
  - Quick check question: What does a high cosine similarity between a face image embedding and a "friendly person" text embedding indicate about CLIP's social perception?

- **Concept**: Controlled experimental manipulation vs. observational studies
  - Why needed here: The study contrasts its experimental approach (synthetically varying individual attributes) with observational approaches (using wild-collected data with uncontrolled confounds)
  - Quick check question: Why might age-related bias patterns appear different when comparing synthetic data with controlled expression to wild-collected data?

- **Concept**: Intersectionality in algorithmic bias
  - Why needed here: The study examines how combinations of attributes (e.g., Black women across different ages and expressions) produce unique bias patterns not predictable from single-attribute analysis
  - Quick check question: What unique pattern did the study find for Black women that was not apparent when examining race or gender separately?

## Architecture Onboarding

- **Component map**: Synthetic face generation pipeline -> Attribute variation control -> CLIP embedding computation -> Cosine similarity analysis -> Statistical comparison across demographic groups
- **Critical path**: Synthetic face generation -> Attribute validation -> CLIP embedding computation -> Social perception metric calculation -> Bias pattern identification
- **Design tradeoffs**: Using synthetic faces provides control but may lack ecological validity; using CLIP enables fine-grained analysis but may reflect training data biases rather than human perception
- **Failure signatures**: Unexpected attribute correlations in synthetic data; CLIP embeddings not capturing intended social dimensions; statistical significance without practical significance
- **First 3 experiments**:
  1. Validate that synthetic face attribute manipulations produce expected perceptual changes across all demographic groups
  2. Confirm that neutral text prompts show systematic bias across demographic groups before calculating delta similarities
  3. Test whether non-protected attributes (expression, lighting, pose) produce consistent effects across all social perception dimensions

## Open Questions the Paper Calls Out

### Open Question 1
Do uncontrolled visual attributes in wild-collected datasets systematically obscure bias patterns, and if so, which attributes are most problematic? The paper identifies these confounds but does not quantify their relative contribution to observed bias patterns across different datasets or test specific methods to control for them.

### Open Question 2
Does the experimental approach using synthetic faces generate hypotheses about human social perception that can be validated through human subject studies? The authors note that their findings reveal gaps in human social psychology literature, such as the unexpected relationship between smiling and negative communion perception in women, and suggest these could be tested with human subjects.

### Open Question 3
Are the unique age-related perception patterns for Black women specific to CLIP or present across different vision-language models and/or human perception? The paper identifies a distinctive U-shaped warmth trajectory and declining competence for Black women with age in CLIP, but notes these patterns differ from some human studies and are not evident in wild-collected datasets.

## Limitations
- The study's findings rely on synthetic faces that may not fully capture the complexity and nuance of real human faces, potentially limiting ecological validity
- The CLIP model used (ViT-B/32) may have different social perception patterns than larger CLIP variants, and results may not generalize across model sizes
- The study focuses on six attributes, but real-world face perception involves many more factors that were not controlled or measured

## Confidence

- **High Confidence**: CLIP exhibits systematic social perception patterns that correlate with human judgments
- **Medium Confidence**: Non-protected attributes can obscure bias patterns in observational studies
- **Medium Confidence**: Age, gender, and race significantly influence CLIP's social perception with particular patterns for Black women

## Next Checks

1. Replicate the study using larger CLIP models (ViT-L/14) to determine if social perception patterns scale with model capacity and if findings are model-specific
2. Conduct human validation studies comparing human social perception ratings of the same synthetic faces to CLIP's judgments to verify the human-like perception claim
3. Test the experimental approach on additional real-world face datasets with uncontrolled visual attributes to quantify how much non-protected attributes obscure bias patterns compared to the synthetic control condition