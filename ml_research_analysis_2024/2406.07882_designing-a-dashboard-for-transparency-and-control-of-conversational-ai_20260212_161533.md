---
ver: rpa2
title: Designing a Dashboard for Transparency and Control of Conversational AI
arxiv_id: '2406.07882'
source_url: https://arxiv.org/abs/2406.07882
tags:
- user
- chatbot
- users
- what
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a dashboard interface for making chatbot\
  \ AI behavior more transparent by revealing and allowing control of the system\u2019\
  s internal model of the user. The authors identify interpretable internal representations\
  \ of user demographics (age, gender, education, socioeconomic status) in an open-source\
  \ LLM using linear probes trained on synthetic conversation data."
---

# Designing a Dashboard for Transparency and Control of Conversational AI

## Quick Facts
- arXiv ID: 2406.07882
- Source URL: https://arxiv.org/abs/2406.07882
- Authors: Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel, Jan Riecke, Shivam Raval, Olivia Seow, Martin Wattenberg, Fernanda Viégas
- Reference count: 40
- One-line primary result: A dashboard interface reveals and allows control of a chatbot's internal user model, increasing transparency and user agency.

## Executive Summary
This paper introduces a dashboard interface that makes chatbot AI behavior more transparent by revealing and allowing control of the system's internal model of the user. The authors identify interpretable internal representations of user demographics (age, gender, education, socioeconomic status) in an open-source LLM using linear probes trained on synthetic conversation data. A prototype dashboard shows these inferred attributes alongside the chat and allows users to manually adjust them. In a user study, participants found the dashboard engaging and useful for understanding and correcting biased chatbot responses, increasing their sense of control and trust when accurate. However, accuracy was lower for female users, and some participants expressed privacy concerns. The work demonstrates a path toward user-facing interpretability interfaces that can improve transparency and user agency in AI systems.

## Method Summary
The method involves training linear probes on model activations to identify and control user attributes, using a synthetic conversation dataset (13,900 multi-turn conversations) generated with GPT-3.5 and LLaMa2Chat for four user attributes. The dashboard interface displays real-time user model predictions and allows users to modify them, with a user study comparing three interface conditions to evaluate effectiveness.

## Key Results
- Linear probes achieved high accuracy (78% average) in identifying user demographics from LLM internal representations
- Dashboard increased user trust and transparency when user model predictions were accurate
- Participants could successfully use controls to modify chatbot behavior and correct biased responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear probes can identify interpretable user demographic attributes in LLM internal activations.
- Mechanism: A linear classifier is trained to map high-dimensional residual stream representations to binary demographic labels. High probing accuracy indicates the model has a linear relationship between internal representations and user attributes.
- Core assumption: User demographic information is encoded in a linear fashion in LLM activations.
- Evidence anchors:
  - [abstract] "we can extract data related to a user's age, gender, educational level, and socioeconomic status" via "linear probes"
  - [section 4.2] "high probing accuracy suggested a strong linear correlation between user demographics and the LLaMa2Chat's internal representations"
  - [corpus] Weak - no direct corpus evidence of linear encoding; relies on cited works [3, 29, 11, 32, 24]
- Break condition: If user attributes are encoded non-linearly or in distributed representations that cannot be linearly separated.

### Mechanism 2
- Claim: The dashboard increases user trust and transparency when the chatbot's user model is accurate.
- Mechanism: Displaying the LLM's internal demographic predictions allows users to verify the system's assumptions, leading to increased understanding and trust when predictions match reality.
- Core assumption: Users trust systems more when they understand and agree with the system's internal reasoning.
- Evidence anchors:
  - [abstract] "Participants also made valuable suggestions that point to future directions" implying positive reception
  - [section 8.1] "Overall, user-model correctness... improved as conversations progressed, achieving an average accuracy of 78%" and "Ten participants associated trust with the accuracy of the user model"
  - [corpus] Moderate - related works [17, 41] discuss trust and transparency issues in black-box systems
- Break condition: If user model accuracy is consistently low or users find the demographic predictions uncomfortable regardless of accuracy.

### Mechanism 3
- Claim: Control probes can manipulate LLM behavior by adding a vector to internal representations.
- Mechanism: A trained control probe vector is added to the residual stream representation, causing the LLM to generate responses consistent with the target demographic attribute.
- Core assumption: Adding a fixed vector to internal representations can change the model's output behavior in predictable ways.
- Evidence anchors:
  - [abstract] "The dashboard can also be used to control the user model and the system's behavior"
  - [section 5] "LLM behavior can be controlled by translating its representation using a specific vector: ˆx + N ˆv"
  - [corpus] Strong - recent work [56, 47, 24, 32] demonstrated activation addition for steering LLM behavior
- Break condition: If the control vector addition causes unpredictable or harmful outputs, or if the effect diminishes across conversation turns.

## Foundational Learning

- Concept: Linear probes and probing classifiers
  - Why needed here: The entire system relies on extracting demographic information from LLM internal states using linear classifiers.
  - Quick check question: What is the difference between a reading probe and a control probe in this system?

- Concept: Activation addition and representation engineering
  - Why needed here: The control mechanism works by adding vectors to internal representations, requiring understanding of how internal states relate to model behavior.
  - Quick check question: How does the strength parameter N affect the intervention when adding the control probe vector?

- Concept: User interface design for interpretability tools
  - Why needed here: The dashboard design balances transparency with user comfort, requiring knowledge of HCI principles for sensitive information.
  - Quick check question: What design considerations are important when displaying potentially sensitive demographic information to users?

## Architecture Onboarding

- Component map:
  - Synthetic data generator (GPT-3.5/LLaMa2Chat) → Linear probe trainer → Dashboard UI (React/Flask) → LLM inference engine (LLaMa2Chat) → User study interface

- Critical path:
  1. User sends message
  2. LLM processes message, generates activations
  3. Reading probes classify demographics from last token representation
  4. Dashboard displays predictions
  5. User may apply control (adds N*control_probe_vector to representation)
  6. LLM generates response based on modified activations

- Design tradeoffs:
  - Synthetic vs real conversation data: Synthetic data is controllable but may not capture real user behavior
  - Layer selection for probing: Deeper layers show higher accuracy but may be less causally related to outputs
  - Intervention strength N: Higher values cause stronger behavior changes but risk instability
  - Dashboard complexity: More attributes increase transparency but may overwhelm users

- Failure signatures:
  - Low probe accuracy (<70%) suggests demographic information isn't linearly encoded
  - Dashboard showing "unknown" for all attributes indicates insufficient conversation context
  - User controls having no effect suggests control probes aren't effective for steering
  - Study participants reporting discomfort regardless of accuracy suggests privacy concerns outweigh benefits

- First 3 experiments:
  1. Test probe accuracy on a held-out validation set of synthetic conversations to establish baseline performance
  2. Verify that control probe interventions change responses in expected ways using the causal intervention dataset
  3. Conduct a small pilot user study with 3-5 participants to test dashboard usability and identify major design issues before full deployment

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data generation may not fully capture real human conversation complexity, limiting external validity
- User study sample size (N=19) provides initial insights but lacks statistical power for robust conclusions
- Probe accuracy disparity for female users (72% vs 85% for male users) suggests potential gender bias

## Confidence

- Linear probe methodology and probe accuracy: Medium
- Dashboard design and user engagement: Medium
- Control probe effectiveness: Low (limited empirical validation)
- Privacy impact assessment: Low (qualitative feedback only)

## Next Checks

1. **Probe Bias Analysis**: Conduct a systematic analysis of probe accuracy across different demographic subgroups using a balanced validation set to identify and address potential biases in the synthetic data generation or probing methodology.

2. **Real Conversation Transfer**: Test the linear probes on real human conversation data (e.g., Reddit comments with demographic labels) to validate whether the synthetic data approach generalizes to authentic user interactions.

3. **Control Probe Robustness**: Design controlled experiments testing control probe effectiveness across multiple conversation topics, lengths, and intervention strengths to establish the reliability and safety boundaries of the steering mechanism.