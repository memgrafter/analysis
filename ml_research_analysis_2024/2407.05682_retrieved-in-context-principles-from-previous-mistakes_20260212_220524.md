---
ver: rpa2
title: Retrieved In-Context Principles from Previous Mistakes
arxiv_id: '2407.05682'
source_url: https://arxiv.org/abs/2407.05682
tags:
- principles
- reasoning
- mistakes
- principle
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel teacher-student framework for large
  language models (LLMs) to learn from previous mistakes. The method, called Retrieved
  In-Context Principles (RICP), involves three main stages: insight generation, principle
  formulation, and principle utilization.'
---

# Retrieved In-Context Principles from Previous Mistakes

## Quick Facts
- arXiv ID: 2407.05682
- Source URL: https://arxiv.org/abs/2407.05682
- Reference count: 18
- Key outcome: RICP consistently improves model performance, with up to 22.6% relative improvement on AQuA

## Executive Summary
This paper introduces Retrieved In-Context Principles (RICP), a teacher-student framework that enables large language models to learn from previous mistakes and improve reasoning performance. The method analyzes student model errors to generate high-level reasons and specific insights, which are then clustered and retrieved to create customized principles integrated into the student's prompt during inference. Experiments across seven benchmarks in mathematical, commonsense, and logical reasoning demonstrate consistent performance improvements compared to baseline few-shot and zero-shot methods.

## Method Summary
RICP operates through a three-stage teacher-student framework: first, the teacher model analyzes mistakes made by the student on training data to generate high-level reasons and specific insights; second, these mistakes are clustered based on their underlying reasons to create task-level principles and question-level principles are generated by retrieving the most relevant mistakes for each new question; third, during inference, both principle sets are integrated into the student model's existing prompt to enhance its question-answering capabilities. The approach leverages hierarchical clustering and semantic similarity via embeddings to organize and retrieve relevant principles, improving error coverage and customization without requiring teacher intervention during inference.

## Key Results
- RICP consistently improves model performance across mathematical, commonsense, and logical reasoning tasks
- Achieves up to 22.6% relative improvement on AQuA dataset
- Outperforms both few-shot and zero-shot baseline methods across all seven benchmarks tested
- Task-level and question-level principles complement each other to enhance error coverage and customization

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical clustering of mistakes by underlying reason creates task-level principles that broaden error coverage. Clustering mistakes based on high-level reasons (e.g., "Misapplication of Proportion") groups similar error patterns, allowing the teacher model to generate principles that address multiple related mistakes at once. This avoids redundancy and ensures diverse coverage.

### Mechanism 2
Retrieval of most relevant mistakes per question creates question-level principles that improve customization. For each new question, the system retrieves the top-m most similar mistakes from each reason cluster, then clusters the insights from these retrieved mistakes to sample n insights as question-level principles. This tailors guidance to the specific question's context.

### Mechanism 3
Integration of both task-level and question-level principles into the existing prompt boosts performance without requiring teacher intervention during inference. The enhanced prompt concatenates task-level principles (shared across all questions) and question-level principles (specific to the current question) with the original prompt, guiding the student model without additional teacher calls.

## Foundational Learning

- Concept: K-means clustering and semantic similarity via embeddings
  - Why needed here: Clustering mistakes and insights requires a method to group semantically similar items, and embedding similarity is used to retrieve the most relevant mistakes for each question
  - Quick check question: How does K-means clustering decide cluster boundaries, and what embedding method is used for semantic similarity?

- Concept: Chain-of-thought (CoT) prompting
  - Why needed here: The student model uses CoT or related prompting strategies; understanding CoT helps reason why integrating principles into prompts can improve reasoning
  - Quick check question: What is the difference between zero-shot CoT and few-shot CoT, and how might principles enhance each?

- Concept: Teacher-student framework in machine learning
  - Why needed here: The overall approach relies on a teacher model analyzing student mistakes and generating guidance; knowing this paradigm clarifies the role of each component
  - Quick check question: In a teacher-student setup, what are the typical responsibilities of the teacher model, and how is knowledge transferred?

## Architecture Onboarding

- Component map: Student model -> Mistake collection pipeline -> Teacher model -> Hierarchical clustering module -> Retrieval module -> Prompt integration -> Student model inference
- Critical path: 1) Student model evaluated on training set → mistakes collected 2) Teacher model generates high-level reasons and insights for each mistake 3) Mistakes clustered by reason → task-level principles generated 4) For each new question, retrieve similar mistakes per cluster → cluster insights → question-level principles 5) Combine task-level and question-level principles with original prompt → student model inference
- Design tradeoffs: Clustering granularity vs. coverage; Number of retrieved mistakes (m) vs. noise; Prompt length limits vs. principle comprehensiveness
- Failure signatures: Student performance does not improve or degrades; Retrieval returns irrelevant mistakes; Clustering fails to group semantically similar mistakes
- First 3 experiments: 1) Run student model on training set, collect mistakes, and verify teacher can generate at least one high-level reason and multiple insights per mistake 2) Apply hierarchical clustering to mistakes and manually inspect clusters to confirm semantic coherence 3) For a held-out question, retrieve similar mistakes, cluster their insights, and manually verify that question-level principles are relevant and non-redundant

## Open Questions the Paper Calls Out

- Question: How does the size of the insight corpus affect the performance of RICP?
  - Basis in paper: Explicit
  - Why unresolved: The paper shows that both excessively large and small corpora result in performance decline, but does not specify the optimal size range for different reasoning tasks or model types
  - What evidence would resolve it: Experiments varying the insight corpus size across different reasoning tasks and model types to determine the optimal size range

- Question: How does the choice of clustering algorithm affect the performance of RICP?
  - Basis in paper: Inferred
  - Why unresolved: The paper uses K-means clustering for both reason and insight clustering, but does not explore the impact of different clustering algorithms on performance
  - What evidence would resolve it: Experiments comparing the performance of RICP using different clustering algorithms (e.g., hierarchical clustering, DBSCAN) on various benchmarks

- Question: How does the number of task-level and question-level principles affect the performance of RICP?
  - Basis in paper: Explicit
  - Why unresolved: The paper shows that both excessively high and low values for the number of retrieved questions and insights adversely affect performance, but does not provide a clear optimal range
  - What evidence would resolve it: Experiments varying the number of task-level and question-level principles across different reasoning tasks and model types to determine the optimal range

- Question: How does RICP perform when integrated with external tools like calculators or retrieval-augmented generation?
  - Basis in paper: Explicit
  - Why unresolved: The paper mentions that RICP can be seamlessly integrated with RAG and external tools to further enhance model performance, but does not provide experimental results
  - What evidence would resolve it: Experiments comparing the performance of RICP with and without integration of external tools on various reasoning tasks

## Limitations

- The teacher model (GPT-4-Turbo) generates both reasons and insights, raising concerns about whether the same model is evaluating its own outputs
- The approach requires running the student model on training data to collect mistakes, which may not be feasible for all applications and introduces potential train-test contamination risk
- Evaluation covers seven benchmarks but improvements are measured relative to baseline few-shot or zero-shot performance without comparisons to other advanced prompting methods

## Confidence

- Task-level principles improve error coverage: Medium
- Question-level principles improve customization: Medium
- Integration of both principle types boosts performance: High
- Up to 22.6% relative improvement on AQuA: Medium

## Next Checks

1. Implement ablation studies removing task-level principles, question-level principles, or both to quantify their individual contributions to performance gains
2. Conduct cross-dataset validation where principles generated from one reasoning task are applied to another task to test generalizability
3. Perform manual inspection of 50+ randomly selected mistakes and their corresponding principles to verify semantic coherence and relevance of the clustering and retrieval processes