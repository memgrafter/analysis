---
ver: rpa2
title: 'Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance
  and Generalization'
arxiv_id: '2409.18433'
source_url: https://arxiv.org/abs/2409.18433
tags:
- difficulty
- problems
- problem
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Easy2Hard-Bench, a benchmark suite that provides
  continuous, fine-grained difficulty ratings for problems across six domains including
  math, coding, chess puzzles, and reasoning tasks. Difficulty scores are derived
  using statistical models (IRT and Glicko-2) applied to real-world human or LLM performance
  data, enabling a more precise profiling of LLM capabilities across difficulty levels.
---

# Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization

## Quick Facts
- arXiv ID: 2409.18433
- Source URL: https://arxiv.org/abs/2409.18433
- Reference count: 40
- Key outcome: Introduces Easy2Hard-Bench with continuous difficulty ratings for LLM performance profiling across six domains

## Executive Summary
Easy2Hard-Bench is a benchmark suite that provides continuous, fine-grained difficulty ratings for problems across six domains including math, coding, chess puzzles, and reasoning tasks. The benchmark uses statistical models (IRT and Glicko-2) applied to real-world human or LLM performance data to derive standardized difficulty scores. Extensive experiments with six state-of-the-art LLMs demonstrate that performance generally declines with increasing difficulty, and easy-to-hard generalization experiments reveal that models benefit most when training and evaluation difficulties align.

## Method Summary
Easy2Hard-Bench uses IRT and Glicko-2 statistical models to estimate problem difficulty from performance data, providing continuous difficulty ratings across six benchmark datasets. The benchmark evaluates six state-of-the-art LLMs on these datasets using zero-shot or standard evaluation protocols. Easy-to-hard generalization experiments are conducted by training models on difficulty-specific subsets and evaluating across the full difficulty range, with performance visualized through heatmaps showing generalization patterns.

## Key Results
- Performance generally declines with increasing difficulty across all six domains
- Easy-to-hard generalization benefits most when training and evaluation difficulties align
- Training on harder samples often degrades performance compared to mixed-difficulty training
- Continuous difficulty ratings enable smooth performance curves and reveal model-specific generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** IRT and Glicko-2 statistical models can accurately estimate problem difficulty from performance data without requiring manual labeling
- **Mechanism:** These models use the probability of correct responses as a function of examinee ability and problem difficulty parameters. For IRT, the logistic function models P(correct) = f(ability - difficulty), while Glicko-2 updates ratings based on match outcomes and time-based reliability
- **Core assumption:** The performance data (solve rates, submission outcomes, ratings) accurately reflects the underlying difficulty structure of problems
- **Evidence anchors:** [abstract] "To systematically estimate problem difficulties, we collect abundant performance data on attempts to each problem by humans in the real world or LLMs on the prominent leaderboard"; [section] "Both IRT and Glicko-2 ensure a rigorous and comprehensive approach to difficulty estimation in Easy2Hard-Bench, providing a standardized complexity measure across varied domains"
- **Break condition:** When performance data is sparse, biased, or when problems have multiple correct solutions making binary correctness ambiguous

### Mechanism 2
- **Claim:** Continuous difficulty ratings enable fine-grained profiling of LLM performance and generalization across the difficulty spectrum
- **Mechanism:** By mapping problems to numerical difficulty values, we can plot smooth performance curves and analyze how model accuracy changes with increasing difficulty. This reveals patterns like performance plateaus, sharp drops, or model-specific generalization capabilities
- **Core assumption:** Performance correlates monotonically with difficulty across the full range, allowing meaningful comparisons
- **Evidence anchors:** [abstract] "Easy2Hard-Bench, a benchmark suite that provides continuous, fine-grained difficulty ratings for problems across six domains"; [section] "Thanks to the continuous difficulty rating and accompanying uncertainty for each problem, we can plot smooth average performance curves"
- **Break condition:** When difficulty ratings don't capture the true cognitive complexity of problems, or when models use different strategies for different difficulty ranges that aren't captured by accuracy metrics

### Mechanism 3
- **Claim:** Easy-to-hard generalization patterns emerge from training on difficulty-specific subsets and evaluating across the full difficulty range
- **Mechanism:** By training models on subsets of data with specific difficulty ranges and evaluating them across all difficulties, we can map generalization performance. The "generalization ridge" where training and evaluation difficulties align reveals optimal difficulty progression
- **Core assumption:** Difficulty is a meaningful axis for curriculum design and that models benefit from aligned training and evaluation difficulty levels
- **Evidence anchors:** [abstract] "Easy-to-hard generalization experiments reveal that models benefit most when training and evaluation difficulties align"; [section] "To capture the 'two-dimensional' generalization behavior, we divide the training data into bins based on difficulty ratings and undertake training a + b times"
- **Break condition:** When other factors (domain expertise, reasoning strategies) dominate performance more than raw difficulty, or when models plateau at certain difficulty levels regardless of training data

## Foundational Learning

- **IRT (Item Response Theory):**
  - Why needed here: IRT provides the statistical foundation for converting binary performance data into continuous difficulty estimates, which is essential for creating the benchmark's difficulty ratings
  - Quick check question: In IRT, if a problem has difficulty b and an examinee has ability θ, what is the probability they answer correctly when b = θ?

- **Glicko-2 Rating System:**
  - Why needed here: Glicko-2 handles dynamic environments where both problem difficulty and examinee ability change over time, making it suitable for coding and chess puzzle datasets
  - Quick check question: How does Glicko-2's rating deviation (RD) parameter change after a player competes frequently versus infrequently?

- **Curriculum Learning Principles:**
  - Why needed here: Understanding how models learn from easy to hard examples is crucial for interpreting the generalization experiments and designing better training paradigms
  - Quick check question: Why might training on harder samples sometimes degrade performance compared to training on mixed difficulties?

## Architecture Onboarding

- **Component map:** Dataset preprocessing pipeline → Difficulty estimation models (IRT/Glicko-2) → Standardized difficulty normalization → LLM evaluation framework → Generalization profiling experiments
- **Critical path:** Data collection → Difficulty estimation → Benchmark creation → Model evaluation → Generalization analysis
- **Design tradeoffs:** 
  - IRT vs Glicko-2: IRT assumes static abilities while Glicko-2 handles dynamic environments; choose based on data characteristics
  - Continuous vs categorical difficulty: Continuous enables fine-grained analysis but requires more sophisticated statistical modeling
  - Human vs model-based verification: Human verification is more accurate but expensive; model-based scales better but may have alignment issues
- **Failure signatures:**
  - Difficulty estimates don't correlate with human perception → Check IRT model fit and data quality
  - Performance doesn't decline with difficulty → Verify difficulty ratings are meaningful and problems are actually getting harder
  - Generalization experiments show no patterns → Check training/evaluation split methodology and ensure sufficient sample sizes
- **First 3 experiments:**
  1. Run IRT estimation on E2H-AMC data and compare with official item difficulties to validate the approach
  2. Plot model performance vs difficulty for one dataset to verify the correlation between ratings and actual performance
  3. Train a simple model on easy-only vs mixed-difficulty data from one dataset to observe basic generalization patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-as-a-judge reliably replace human evaluation for difficulty estimation in reasoning tasks like ARC and Winogrande?
- Basis in paper: [inferred] The paper compares IRT difficulty rankings with both human evaluations and GPT-4 rankings, finding that GPT-4's rankings do not align well with human preferences on E2H-ARC and E2H-Winogrande, while IRT aligns better with human judgments.
- Why unresolved: The study only tested GPT-4; other LLMs with potentially better reasoning capabilities might serve as more reliable judges. Additionally, the evaluation used only pairwise comparisons without considering multi-option or continuous difficulty scales.
- What evidence would resolve it: Systematic comparison of multiple LLMs (including newer models) as judges against human evaluations across diverse reasoning benchmarks, testing different evaluation formats and scales.

### Open Question 2
- Question: What training paradigms can improve LLM generalization from easy to hard problems?
- Basis in paper: [explicit] The paper observes that training on harder samples often degrades performance and that generalization benefits most when training and evaluation difficulties align, suggesting current paradigms are insufficient for scaling to harder problems.
- Why unresolved: The paper only explored supervised fine-tuning (SFT) with relatively small models. More advanced training methods like curriculum learning, meta-learning, or specialized architectures might yield better easy-to-hard generalization.
- What evidence would resolve it: Comparative studies of various training approaches (curriculum learning, meta-learning, adaptive training) across multiple difficulty ranges, measuring their impact on generalization performance.

### Open Question 3
- Question: How do different statistical models for difficulty estimation affect benchmark validity?
- Basis in paper: [explicit] The paper employs both IRT and Glicko-2 models for difficulty estimation but does not systematically compare their effectiveness or explore alternative models that might better capture difficulty nuances.
- Why unresolved: While both models are well-established, they make different assumptions about participant behavior and problem characteristics. The choice of model could significantly impact difficulty rankings and consequently benchmark validity.
- What evidence would resolve it: Systematic evaluation comparing IRT, Glicko-2, and alternative statistical models against human difficulty judgments across diverse benchmark datasets, measuring correlation and predictive accuracy.

## Limitations

- Difficulty estimation accuracy depends heavily on the quality and representativeness of performance data, with potential issues from sparse data or biased performance
- The benchmark focuses primarily on accuracy as a performance metric, potentially missing other important dimensions like reasoning quality or efficiency
- The assumption of unidimensional difficulty may not capture all aspects of problem complexity, particularly for tasks requiring multiple cognitive skills

## Confidence

- **High Confidence**: The mechanism by which continuous difficulty ratings enable fine-grained performance profiling (Mechanism 2). The empirical evidence from plotting performance curves across difficulty levels is directly observable and the methodology is straightforward.
- **Medium Confidence**: The statistical models (IRT and Glicko-2) accurately estimating problem difficulty from performance data (Mechanism 1). While the theoretical foundations are well-established, the specific application to LLM benchmarks and the handling of potential data quality issues requires careful validation.
- **Medium Confidence**: Easy-to-hard generalization patterns revealing optimal training difficulty alignment (Mechanism 3). The experimental results show clear patterns, but the causal interpretation and generalizability to different model architectures and training paradigms needs further investigation.

## Next Checks

1. **Data Quality Validation**: Conduct a human verification study where multiple annotators independently rate a subset of problems from different difficulty levels. Compare human difficulty perceptions with the IRT/Glicko-2 derived scores to quantify alignment and identify systematic biases.

2. **Statistical Model Fit Assessment**: For each dataset, calculate standard IRT model fit statistics (like item characteristic curve plots, discrimination parameter distributions) and compare with baseline expectations. Identify datasets where the models show poor fit and investigate potential causes.

3. **Cross-Domain Difficulty Transferability**: Select problems from different domains with similar difficulty ratings and conduct a controlled experiment to test whether these problems present comparable challenges to models. This would validate whether difficulty ratings are truly comparable across domains.