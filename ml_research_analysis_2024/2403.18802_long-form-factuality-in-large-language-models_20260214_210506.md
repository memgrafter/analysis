---
ver: rpa2
title: Long-form factuality in large language models
arxiv_id: '2403.18802'
source_url: https://arxiv.org/abs/2403.18802
tags:
- response
- facts
- what
- factuality
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models often generate factually incorrect content
  when responding to fact-seeking prompts. To address this, the paper introduces LongFact,
  a comprehensive prompt set of 2,280 questions across 38 topics designed to test
  long-form factuality.
---

# Long-form factuality in large language models

## Quick Facts
- arXiv ID: 2403.18802
- Source URL: https://arxiv.org/abs/2403.18802
- Reference count: 40
- Primary result: Introduces LongFact benchmark and SAFE evaluation method for long-form factuality

## Executive Summary
Large language models often generate factually incorrect content when responding to fact-seeking prompts. To address this, the paper introduces LongFact, a comprehensive prompt set of 2,280 questions across 38 topics designed to test long-form factuality. It also proposes SAFE (Search-Augmented Factuality Evaluator), an automated evaluation method that uses an LLM to break down responses into individual facts and verify each using Google Search. The paper introduces F1@K as an aggregated metric balancing factual precision and recall based on a user's preferred response length. Empirically, SAFE outperforms human annotators, achieving 72% agreement and winning 76% of 100 sampled disagreement cases, while being 20× cheaper.

## Method Summary
The paper proposes LongFact, a comprehensive prompt set containing 2,280 questions spanning 38 topics to evaluate long-form factuality in LLMs. For evaluation, they introduce SAFE (Search-Augmented Factuality Evaluator), an automated method that first uses an LLM to extract and break down long-form responses into individual claims, then verifies each claim by searching the web using Google Search and comparing results against the claim. The method uses F1@K as an aggregated metric to balance factual precision and recall based on desired response length. SAFE is benchmarked against human annotators, showing 72% agreement and winning 76% of sampled disagreements while being approximately 20× cheaper than human evaluation.

## Key Results
- SAFE outperforms human annotators with 72% agreement rate
- SAFE wins 76% of 100 sampled disagreement cases
- Larger models generally achieve better long-form factuality
- SAFE is approximately 20× cheaper than human evaluation

## Why This Works (Mechanism)
The paper doesn't explicitly detail a specific mechanism for why this works, focusing instead on the methodology and empirical results.

## Foundational Learning
- Long-form factuality: Understanding factual accuracy in extended responses
  - Why needed: LLMs often produce errors in longer, more complex outputs
  - Quick check: Can the model maintain accuracy across multi-paragraph responses?
- Automated fact verification: Using search engines to validate claims
  - Why needed: Manual verification is time-consuming and expensive
  - Quick check: Does search-augmented verification catch errors missed by humans?
- Aggregated evaluation metrics: F1@K for balancing precision and recall
  - Why needed: Single metrics may not capture the complexity of factuality
  - Quick check: Does F1@K provide a more comprehensive evaluation than accuracy alone?

## Architecture Onboarding
Component map: LongFact prompts -> LLM responses -> Claim extraction -> Google Search verification -> F1@K scoring

Critical path: The core evaluation pipeline follows LongFact prompts being processed by target LLMs, then SAFE extracts claims from responses, verifies them via Google Search, and calculates F1@K scores.

Design tradeoffs: The paper trades computational cost for evaluation comprehensiveness by using automated search-augmented verification instead of purely human evaluation, accepting potential search result noise for scalability.

Failure signatures: Model responses may fail due to hallucinated facts, outdated information, or over-generalization. SAFE may fail when search results are ambiguous, conflicting, or when claims are too nuanced for automated verification.

First experiments: 1) Run LongFact on a small model to establish baseline factuality, 2) Test SAFE's claim extraction accuracy on known factual and non-factual responses, 3) Validate search result matching by comparing SAFE outputs against human-verified claims.

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions in the provided content.

## Limitations
- Reliance on Google Search introduces uncertainty as search results may contain errors or be outdated
- The study focuses exclusively on English-language facts, limiting generalizability
- Automated approach may miss nuanced context or struggle with contested factual topics

## Confidence
- High Confidence: Comparative benchmarking showing larger models achieve better factuality
- Medium Confidence: SAFE's 72% agreement with human annotators and 76% win rate in disagreements
- Medium Confidence: Claim that SAFE is 20× cheaper than human evaluation

## Next Checks
1. Conduct cross-linguistic validation by adapting LongFact and SAFE to test non-English languages
2. Perform ablation studies on SAFE's components to quantify individual contributions to accuracy
3. Implement a blind validation study where human annotators evaluate responses without knowledge of SAFE-generated labels