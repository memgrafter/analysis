---
ver: rpa2
title: A New Type of Foundation Model Based on Recordings of People's Emotions and
  Physiology
arxiv_id: '2408.00030'
source_url: https://arxiv.org/abs/2408.00030
tags:
- data
- could
- foundation
- emotional
- physiological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces first-person foundation models (FPFMs), a
  novel approach to modeling human behavior by training AI systems on recordings of
  what people see, hear, and their emotional and physiological reactions. Unlike current
  foundation models trained on internet data, FPFMs aim to capture the relationship
  between environmental stimuli and human emotional states, and between those states
  and behavior.
---

# A New Type of Foundation Model Based on Recordings of People's Emotions and Physiology

## Quick Facts
- arXiv ID: 2408.00030
- Source URL: https://arxiv.org/abs/2408.00030
- Authors: David Gamez; Dionis Barcari; Aliya Grig
- Reference count: 15
- Primary result: Introduces first-person foundation models trained on recordings of what people see, hear, and their emotional/physiological reactions to create more accurate behavioral predictions than LLM-based approximations.

## Executive Summary
This paper introduces first-person foundation models (FPFMs), a novel approach to modeling human behavior by training AI systems on recordings of what people see, hear, and their emotional and physiological reactions. Unlike current foundation models trained on internet data, FPFMs aim to capture the relationship between environmental stimuli and human emotional states, and between those states and behavior. The authors developed a recording rig that captures visual and auditory input along with skin conductance, facial expressions, and brain activity (14-channel EEG). Their analysis suggests that a text-only FPFM on the scale of GPT-2 could be trained from approximately 50 days of data from a single individual.

## Method Summary
The method involves building a recording rig with sensors (camera, microphone, GSR, EEG) to capture multi-modal data from subjects. This raw data is processed using AI services (AWS Rekognition, Emotiv Cortex API) to extract features like sentiment, object labels, and cognition scores. A transformer-based foundation model is then trained on this processed data. For personalization, the base FPFM is fine-tuned using QLoRA on individual data, combined with RAG on a personal vector database containing contextual information about the individual.

## Key Results
- FPFMs could be trained from approximately 50 days of data from a single individual for a GPT-2 scale text-only model
- The recording rig captures visual, auditory, GSR, facial expression, and 14-channel EEG data simultaneously
- Potential applications include recommendation systems, personal assistants, dating platforms, and recruitment tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: First-person foundation models (FPFMs) can generate more accurate behavioral predictions than LLM-based approximations because they are trained on the actual emotional and physiological responses of the subject rather than inferred surface-level patterns.
- Mechanism: By mapping environmental stimuli → emotional/physiological state → external behavior, the model learns the internal motivations behind actions. This enables fine-grained predictions of behavior under varying emotional contexts.
- Core assumption: Emotional and physiological states are central drivers of human behavior and can be reliably captured and mapped to actions.
- Evidence anchors:
  - [abstract] "a new type of foundation model - a first-person foundation model - could be created from recordings of what a person sees and hears as well as their emotional and physiological reactions to these stimuli."
  - [section 3.1] "A FPFM would map environmental stimuli to a person's emotional and physiological states, and map a person's emotional and physiological states to their behavior."
  - [corpus] Weak: no direct empirical studies of FPFM performance in the provided corpus.

### Mechanism 2
- Claim: The recording rig enables capture of multi-modal data (visual, auditory, GSR, EEG, facial expression) that, when processed through AI algorithms, creates a rich representation of the subject's internal state and environment.
- Mechanism: Raw sensor streams are transformed into higher-level features (sentiment, object labels, cognition scores) that serve as training inputs for the FPFM. This reduces noise and focuses learning on behaviorally relevant signals.
- Core assumption: AI preprocessing can reliably extract meaningful emotional and cognitive features from raw physiological signals.
- Evidence anchors:
  - [section 4.1] "Cloud services, such as AWS Rekognition, and the Emotiv Cortex API, are used to analyze the raw data for higher level properties, such as text contents, sentiment, cognition, facial expression, and object labels."
  - [section 4.2] Data rates table shows integration of EEG, GSR, audio, and image processing.
  - [corpus] Missing: no comparative analysis of preprocessing accuracy in the corpus.

### Mechanism 3
- Claim: Fine-tuning a general FPFM on a specific individual's recorded data, combined with RAG on their personal vector database, produces a personalized digital twin that can simulate that person's reactions and behaviors accurately.
- Mechanism: Initial model is trained on multi-person data; then QLoRA-based fine-tuning adapts weights to individual patterns. RAG injects personal context (memories, preferences) to constrain responses to the individual's style.
- Core assumption: Transfer learning from a multi-person FPFM to an individual is feasible and preserves generalization while enabling personalization.
- Evidence anchors:
  - [section 3.3] "A more practical approach would be to train a FPFM on data from multiple people and then use fine tuning and RAG to customize the model for a specific individual."
  - [section 3.4] "A local copy of a FPFM would be fine-tuned, using QLoRA or a similar approach, on the user's local machine."
  - [corpus] Weak: no empirical results showing QLoRA fine-tuning success on FPFM data in the corpus.

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: FPFMs are described as using a transformer-based model; understanding self-attention, positional encoding, and scaling laws is essential to reason about training and inference costs.
  - Quick check question: What is the role of self-attention in handling variable-length sequences in transformer models?

- Concept: Physiological signal processing and feature extraction
  - Why needed here: The recording rig outputs raw EEG, GSR, and facial expression data; converting these to usable features (band powers, arousal scores) is critical for model training.
  - Quick check question: How does galvanic skin response correlate with emotional arousal, and what preprocessing steps improve signal quality?

- Concept: Fine-tuning and RAG integration in personalized models
  - Why needed here: Creating a digital twin requires combining base model adaptation (fine-tuning) with retrieval of personal context (RAG) to generate individualized responses.
  - Quick check question: In a fine-tuning + RAG pipeline, which component is responsible for updating model parameters and which is responsible for injecting external knowledge?

## Architecture Onboarding

- Component map: Recording rig (Raspberry Pi + sensors) → Data preprocessing service (AWS Rekognition, Emotiv Cortex) → Vector database (personal context) → FPFM inference service → Fine-tuning pipeline (QLoRA) → API endpoints for third-party access
- Critical path:
  1. Capture raw multi-modal data
  2. Preprocess into structured features
  3. Store in vector DB with embeddings
  4. Query FPFM with stimuli + state features
  5. Return predicted emotional/behavioral output
- Design tradeoffs:
  - Real-time vs. batch processing: real-time requires low-latency preprocessing; batch allows heavier feature extraction
  - On-device vs. cloud fine-tuning: on-device preserves privacy but limits compute; cloud offers speed but raises privacy concerns
  - Blurring faces vs. capturing familiar faces: blurring protects privacy but reduces emotional fidelity; unblurred requires consent
- Failure signatures:
  - High prediction variance across similar stimuli → preprocessing or model underfitting
  - Consistent misprediction of emotional valence → poor calibration of feature extraction
  - Slow inference times → inefficient model or suboptimal hardware choice
- First 3 experiments:
  1. Validate preprocessing pipeline: Feed raw EEG/GSR/audio to feature extractors and compare against ground-truth emotion labels from a small labeled dataset
  2. Test FPFM fine-tuning: Fine-tune a small transformer on multi-person data, then evaluate personalization accuracy on held-out individual sessions
  3. Measure RAG relevance: Query vector DB with stimuli embeddings and compute recall of relevant personal memories; tune embedding model if recall is low

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How accurately can FPFMs predict human behavior compared to current foundation models that lack emotional and physiological data?
- Basis in paper: [explicit] The paper states FPFMs would "replicate human behavior more effectively than surface-level approximations built with LLMs, RAG, cognitive models and prompt engineering" and notes current chatbots are "at best, a surface level approximation to the characters they are imitating."
- Why unresolved: The paper proposes FPFMs but doesn't provide empirical validation comparing their predictive accuracy to existing models.
- What evidence would resolve it: Comparative studies measuring prediction accuracy of FPFMs versus standard LLMs on behavioral prediction tasks using identical datasets.

### Open Question 2
- Question: What is the minimum amount of first-person data needed to train a functional FPFM that captures meaningful emotional and physiological responses?
- Basis in paper: [explicit] Table 3 provides rough estimates for GPT-1, GPT-2, and GPT-3 scale models, suggesting ~50 days of data for a GPT-2 scale text-only FPFM from a single individual.
- Why unresolved: These are theoretical estimates based on data volume, not validated through actual model training experiments.
- What evidence would resolve it: Empirical studies determining the relationship between training data duration/quality and model performance for FPFMs.

### Open Question 3
- Question: How can the privacy concerns of recording others' faces be adequately addressed while maintaining the model's ability to learn emotional responses to familiar people?
- Basis in paper: [explicit] The paper discusses automatic face blurring to protect privacy but notes this "would prevent a FPFM trained on the data from learning the strong emotional reactions that we have to familiar faces."
- Why unresolved: The proposed solution of individual consent for face recording is acknowledged but not evaluated for practicality or effectiveness.
- What evidence would resolve it: Field studies testing the feasibility and user acceptance of face recognition consent systems in real-world recording scenarios.

## Limitations

- No empirical validation of FPFM performance or comparison with existing models
- Assumptions about preprocessing accuracy for emotion detection from physiological signals are untested
- Privacy and legal considerations are acknowledged but not thoroughly explored or resolved

## Confidence

**High Confidence**: The technical feasibility of building a recording rig with the described sensors (Raspberry Pi, camera, microphone, GSR, EEG headset) is well-established. The general concept of training foundation models on large datasets is proven.

**Medium Confidence**: The theoretical framework for how FPFMs would map stimuli to emotional states and behavior follows logical reasoning. The applications proposed (recommendation systems, personal assistants, etc.) are reasonable given the capabilities described.

**Low Confidence**: The specific claims about training requirements (50 days for GPT-2 scale), the effectiveness of the preprocessing pipeline, and the success of fine-tuning and RAG for personalization all lack empirical support. The privacy and legal risk assessments are superficial.

## Next Checks

1. **Preprocessing Validation**: Conduct a controlled experiment where the recording rig captures data from subjects experiencing known emotional states (elicited through standardized stimuli). Compare the preprocessing output (emotion classification, object detection, sentiment analysis) against ground truth labels to measure accuracy.

2. **Transfer Learning Feasibility**: Train a small transformer model on multi-person FPFM data, then attempt fine-tuning on individual subjects. Measure personalization accuracy and compare against baseline models trained only on individual data to assess whether transfer learning provides benefits.

3. **Privacy Impact Assessment**: Conduct a detailed analysis of data collection practices, including what personally identifiable information is captured, how it could be deanonymized, and what legal frameworks (GDPR, CCPA) would apply. Develop and test privacy-preserving alternatives such as on-device processing and differential privacy techniques.