---
ver: rpa2
title: 'Case-Based or Rule-Based: How Do Transformers Do the Math?'
arxiv_id: '2402.17709'
source_url: https://arxiv.org/abs/2402.17709
tags:
- reasoning
- addition
- test
- training
- digit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models struggle with simple math tasks despite excelling
  at complex reasoning, suggesting they rely on "case-based reasoning" - memorizing
  similar examples rather than learning underlying rules. This work investigates this
  hypothesis through intervention experiments on five math tasks, showing transformers
  perform poorly when surrounding training cases are removed, even when sufficient
  data remains.
---

# Case-Based or Rule-Based: How Do Transformers Do the Math?

## Quick Facts
- arXiv ID: 2402.17709
- Source URL: https://arxiv.org/abs/2402.17709
- Authors: Yi Hu; Xiaojuan Tang; Haotong Yang; Muhan Zhang
- Reference count: 40
- Key outcome: Transformers rely on case-based reasoning for math tasks, struggling when training data is incomplete, but can learn rule-based reasoning through explicit supervision (RFFT).

## Executive Summary
This paper investigates whether transformers perform math tasks through rule-based reasoning or case-based memorization. Through systematic experiments on five mathematical tasks, the authors demonstrate that transformers exhibit "holes" in their accuracy distribution - failing on specific test cases even when trained on similar examples. This suggests they memorize patterns rather than learning underlying rules. The authors propose Rule-Following Fine-Tuning (RFFT), which teaches models to explicitly follow step-by-step rules, enabling significant improvements in generalization to longer problems. RFFT allows models fine-tuned on 1-5 digit addition to achieve over 95% accuracy on 12-digit addition problems - a 40% improvement over baseline methods.

## Method Summary
The study examines transformer reasoning mechanisms through intervention experiments on five math tasks: addition, modular addition, base addition, linear regression, and chicken & rabbit problems. The key experimental approach involves fine-tuning GPT-2 and Llama-2-7B on various training-test splits, including a "leave-square-out" method where 441 samples (forming a 20×20 square in the data space) are removed from training while maintaining >95% test accuracy. The authors compare performance between random splits and leave-square-out conditions to observe accuracy "holes." To address case-based reasoning limitations, they propose RFFT, which provides explicit rule supervision during fine-tuning. For reproduction, the minimum viable plan involves fine-tuning on addition tasks with 70/30 train-test splits, implementing leave-square-out experiments, and comparing performance across conditions.

## Key Results
- Transformers show accuracy "holes" when specific training data is removed, even with 95% overall test accuracy
- RFFT enables 1-5 digit fine-tuned models to generalize to 12-digit addition with >95% accuracy
- RFFT achieves 40% improvement over scratchpad methods for long-digit generalization
- Performance degradation in leave-square-out experiments suggests case-based rather than rule-based reasoning

## Why This Works (Mechanism)
Transformers appear to rely on pattern matching to similar training examples rather than extracting generalizable rules. When training data lacks coverage for certain regions of the problem space (the "holes"), models fail to generalize even when overall accuracy is high. RFFT works by explicitly providing rule-based supervision during training, teaching models to follow step-by-step procedures rather than memorizing solution patterns. This enables better generalization to longer or more complex problems that weren't seen during training.

## Foundational Learning
- **Case-based reasoning vs rule-based reasoning**: Understanding the distinction between memorizing examples versus learning abstract rules - needed to interpret the core hypothesis; quick check: can you explain why removing specific training samples causes failures?
- **Transformer fine-tuning mechanics**: How models adapt pre-trained weights to new tasks - needed to understand the experimental setup; quick check: what hyperparameters affect fine-tuning convergence?
- **Generalization bounds in machine learning**: How model performance transfers from training to unseen examples - needed to evaluate the claims about reasoning capabilities; quick check: can you distinguish between interpolation and extrapolation failures?

## Architecture Onboarding
- **Component map**: Pre-trained transformer (GPT-2/Llama-2) -> Fine-tuning pipeline -> Rule-Following Fine-Tuning (RFFT) or standard fine-tuning -> Evaluation on test tasks
- **Critical path**: Data preparation → Model fine-tuning → Intervention experiments (leave-square-out) → RFFT implementation → Performance evaluation
- **Design tradeoffs**: Case-based reasoning offers faster learning on seen patterns but poor generalization vs. rule-based reasoning requiring more supervision but better transfer
- **Failure signatures**: Accuracy "holes" in specific test regions, poor performance on longer problems despite good short-problem accuracy
- **First experiments**: 1) Fine-tune on addition with random split and observe baseline performance, 2) Implement leave-square-out and measure accuracy degradation, 3) Apply RFFT and test generalization to longer problems

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions in the provided content. The open questions section appears to be based on inferences from the paper's findings rather than direct statements from the authors.

## Limitations
- The study focuses exclusively on five synthetic math tasks, limiting generalizability to other reasoning domains
- Evidence shows correlation between data coverage and performance but doesn't definitively prove transformers cannot learn rule-based reasoning
- RFFT evaluation is limited to synthetic math problems; real-world applicability remains untested
- The 40% improvement metric needs clarification regarding baseline performance

## Confidence
- **High confidence**: Empirical observations of performance degradation when training data is removed
- **Medium confidence**: The characterization of transformer behavior as "case-based reasoning" based on current evidence
- **Low confidence**: Claims about the generality of case-based versus rule-based reasoning across all LLM tasks

## Next Checks
1. Test whether models can learn explicit rules through alternative training methods beyond RFFT to determine if the limitation is architectural or training-related
2. Evaluate model behavior on non-synthetic reasoning tasks (e.g., logical deduction or real-world problem solving) to assess if case-based reasoning extends beyond math
3. Conduct ablation studies removing different types of training data (not just "squares") to determine if performance degradation is specific to spatial patterns or general data coverage