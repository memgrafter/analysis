---
ver: rpa2
title: Inducing Human-like Biases in Moral Reasoning Language Models
arxiv_id: '2411.15386'
source_url: https://arxiv.org/abs/2411.15386
tags:
- fine-tuning
- fmri
- ethics
- scores
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether fine-tuning large language models
  (LLMs) on human fMRI data from moral reasoning tasks can increase alignment between
  model and brain representations (BrainScore). The authors fine-tuned BERT, RoBERTa,
  and DeBERTa models on both the ETHICS benchmark and fMRI data from Koster-Hale et
  al.
---

# Inducing Human-like Biases in Moral Reasoning Language Models

## Quick Facts
- arXiv ID: 2411.15386
- Source URL: https://arxiv.org/abs/2411.15386
- Authors: Artem Karpov; Seong Hah Cho; Austin Meek; Raymond Koopmanschap; Lucy Farnik; Bogdan-Ionut Cirstea
- Reference count: 14
- One-line primary result: Fine-tuning large language models on fMRI data from moral reasoning tasks did not improve brain-model alignment despite improving benchmark accuracy.

## Executive Summary
This paper investigates whether fine-tuning large language models (LLMs) on human fMRI data from moral reasoning tasks can increase alignment between model and brain representations (BrainScore). The authors fine-tuned BERT, RoBERTa, and DeBERTa models on both the ETHICS benchmark and fMRI data from Koster-Hale et al. [2013]. Despite improved accuracy on the ETHICS benchmark through fine-tuning, BrainScores did not significantly improve after incorporating fMRI data. Larger models generally performed better on both metrics, but fine-tuning on fMRI data alone or in combination with ETHICS did not enhance brain-model alignment beyond pre-trained models. The authors conclude that their results highlight the need for larger datasets and more effective fine-tuning methods to increase alignment in specific domains.

## Method Summary
The study fine-tuned several BERT-based encoder models (BERT-base, BERT-large, RoBERTa-large, DeBERTa-v2-xlarge) using the HuggingFace library with additional classification heads. Models were trained on the ETHICS benchmark commonsense dataset and fMRI data from human subjects performing moral reasoning tasks. The fMRI data was preprocessed using the DiFuMo atlas with 1,024 regions of interest. Multiple fine-tuning strategies were tested: ETHICS-only, fMRI-only, and combined approaches, along with different sampling methods (LAST, MIDDLE, SENTENCES, AVG). BrainScores were calculated using Pearson correlation between predicted brain activity (from regression models) and actual fMRI data, with layer-wise and region-specific brain scores also evaluated.

## Key Results
- Fine-tuning on ETHICS benchmark improved accuracy, but fMRI fine-tuning did not enhance BrainScore
- Larger models generally performed better on both ETHICS accuracy and BrainScore metrics
- Combined fine-tuning on ETHICS and fMRI data did not outperform ETHICS-only fine-tuning for either metric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on fMRI data from moral reasoning tasks can increase brain-model alignment (BrainScore).
- Mechanism: The fine-tuning process incorporates human neural data patterns, allowing the LLM to develop representations that mirror human moral reasoning processes, thereby increasing the similarity between model and brain activations.
- Core assumption: The fMRI data captures sufficient information about the neural processes underlying moral reasoning to be useful for fine-tuning.
- Evidence anchors:
  - [abstract] "We also explore if fine-tuning several LLMs on the fMRI data of humans performing moral reasoning can improve the BrainScore."
  - [section] "We also study whether fine-tuning the LLMs on a train set of the corresponding neural data helps with improving the BrainScore on a separate test set."
  - [corpus] Weak - corpus contains related work on brain-model alignment but not specifically on fMRI data fine-tuning for moral reasoning.
- Break condition: The fMRI data is insufficient or noisy, failing to capture the relevant neural processes for moral reasoning.

### Mechanism 2
- Claim: Larger models generally perform better on both ETHICS benchmark accuracy and BrainScore metrics.
- Mechanism: Larger models have more parameters and representational capacity, allowing them to capture more complex patterns in both the behavioral data and neural activations.
- Core assumption: The increased capacity of larger models translates to better performance on both tasks.
- Evidence anchors:
  - [abstract] "While larger models generally performed better on both metrics..."
  - [section] "We generally find that larger models are more performant overall, a finding also reported by Hendrycks et al. [2020]."
  - [corpus] Moderate - corpus contains related work showing larger models perform better on various tasks.
- Break condition: The tasks are too simple for model size to matter, or the larger models overfit to the training data.

### Mechanism 3
- Claim: Fine-tuning on fMRI data alone or in combination with ETHICS does not enhance brain-model alignment beyond pre-trained models.
- Mechanism: The fMRI data may not provide additional useful information beyond what is already captured by the pre-trained models or the ETHICS benchmark data, or the fine-tuning process may not effectively transfer the neural patterns.
- Core assumption: The fMRI data contains information that could potentially improve brain-model alignment if the fine-tuning process were effective.
- Evidence anchors:
  - [abstract] "BrainScores did not significantly improve after fine-tuning."
  - [section] "we could not improve accuracy by fine-tuning on the fMRI data only or on a combination of fMRI and ETHICS, compared to fine-tuning purely on ETHICS."
  - [corpus] Weak - corpus contains related work on brain-model alignment but not specifically on the ineffectiveness of fMRI data fine-tuning.
- Break condition: The fMRI data is irrelevant to the task, or the fine-tuning process is fundamentally flawed.

## Foundational Learning

- Concept: BrainScore metric
  - Why needed here: BrainScore is the primary metric used to measure brain-model alignment in this study.
  - Quick check question: What does a higher BrainScore indicate in the context of this study?
- Concept: fMRI data preprocessing and analysis
  - Why needed here: The study uses fMRI data from moral reasoning tasks, which requires specific preprocessing and analysis techniques.
  - Quick check question: What atlas was used to fit the fMRI data in this study?
- Concept: Fine-tuning procedures for LLMs
  - Why needed here: The study involves fine-tuning several LLM architectures on different datasets.
  - Quick check question: What type of head was added to the models during fine-tuning?

## Architecture Onboarding

- Component map: fMRI data preprocessing -> Model fine-tuning -> BrainScore calculation -> Analysis of results
- Critical path: Data preprocessing (fMRI and ETHICS) → Model fine-tuning → BrainScore calculation → Analysis of results
- Design tradeoffs: Computational constraints led to the choice of encoder models over decoder models; the use of a limited fMRI dataset may impact the effectiveness of fine-tuning
- Failure signatures: Lack of improvement in BrainScore after fine-tuning, overfitting to the ETHICS benchmark, or poor generalization to unseen data
- First 3 experiments:
  1. Fine-tune a BERT-base model on the ETHICS benchmark only and evaluate its performance
  2. Fine-tune a BERT-base model on the fMRI data only and evaluate its BrainScore
  3. Fine-tune a BERT-base model on both ETHICS and fMRI data and compare its performance to the previous two models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing the size of the fMRI dataset used for fine-tuning significantly improve BrainScore alignment between LLMs and human moral reasoning?
- Basis in paper: [inferred] The authors state that "our work is ample evidence for the importance of gathering more data on moral reasoning" and suggest that "larger datasets" are needed for better alignment.
- Why unresolved: The current study used a relatively small fMRI dataset (N=17 subjects), which may have been insufficient to capture the full complexity of moral reasoning neural representations.
- What evidence would resolve it: A study using a substantially larger fMRI dataset (e.g., N > 100 subjects) with similar fine-tuning approaches showing improved BrainScore metrics compared to the current results.

### Open Question 2
- Question: Would alternative fine-tuning methods beyond simple linear head attachment improve brain-model alignment on moral reasoning tasks?
- Basis in paper: [explicit] The authors mention "the need for... more effective fine-tuning methods" and one author contributed work on "parameter-efficient fine-tuning extensions."
- Why unresolved: The study only used linear transformation layers attached to the classification token, which may not be the optimal approach for aligning with fMRI data.
- What evidence would resolve it: A comparative study testing multiple fine-tuning approaches (e.g., LoRA, prefix tuning, full fine-tuning) showing superior BrainScore improvements over the baseline method used in this study.

### Open Question 3
- Question: Does fine-tuning on moral reasoning fMRI data improve model performance on tasks requiring Theory of Mind capabilities?
- Basis in paper: [explicit] The authors note that moral reasoning "is also partially relevant to Theory of Mind" and used fMRI regions related to ToM in their analysis.
- Why unresolved: While the study measured BrainScore alignment with ToM regions, it did not test whether the fine-tuning actually improved model performance on ToM-specific tasks.
- What evidence would resolve it: A study demonstrating that models fine-tuned on moral reasoning fMRI data perform better on standardized ToM benchmarks (e.g., Theory of Mind tests) compared to baseline models.

## Limitations

- The study used a relatively small fMRI dataset (N=12 subjects) which limits statistical power and generalizability
- Computational constraints restricted analysis to encoder models only, potentially missing insights from decoder architectures
- The ETHICS benchmark remains a synthetic dataset that may not fully capture the complexity of real-world moral reasoning

## Confidence

- **High Confidence**: Claims about larger models performing better on ETHICS benchmark
- **Medium Confidence**: Findings that fMRI fine-tuning does not improve BrainScore
- **Low Confidence**: Generalizability of brain-model alignment results beyond the specific moral reasoning domain studied

## Next Checks

1. Test the same fine-tuning approach on a larger, multi-site fMRI moral reasoning dataset to verify if results scale with sample size
2. Implement and compare decoder-based LLM architectures to determine if encoder model limitations influenced the findings
3. Conduct ablation studies to isolate which aspects of fMRI fine-tuning (data quality, preprocessing, regression methods) most affect brain-model alignment