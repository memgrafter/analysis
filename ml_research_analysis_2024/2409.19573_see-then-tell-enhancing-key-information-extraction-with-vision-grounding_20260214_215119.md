---
ver: rpa2
title: 'See then Tell: Enhancing Key Information Extraction with Vision Grounding'
arxiv_id: '2409.19573'
source_url: https://arxiv.org/abs/2409.19573
tags:
- text
- dataset
- document
- vision
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STNet, a novel end-to-end model that integrates
  key information extraction with vision grounding. The method leverages a unique
  <see token to identify relevant image regions, followed by a physical decoder that
  extracts bounding box coordinates for each answer.
---

# See then Tell: Enhancing Key Information Extraction with Vision Grounding

## Quick Facts
- arXiv ID: 2409.19573
- Source URL: https://arxiv.org/abs/2409.19573
- Authors: Shuhang Liu, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Jun Du, Qing Wang, Jianshu Zhang, Chenyu Liu
- Reference count: 8
- Primary result: Introduces STNet, achieving F1 scores of 88.1 on CORD, 87.8 on SROIE, and ANLS of 63.7 on DocVQA

## Executive Summary
This paper presents STNet, a novel end-to-end model that integrates key information extraction with vision grounding for document understanding. The model uses a unique `<see>` token to identify relevant image regions and a physical decoder to extract bounding box coordinates for each answer. By leveraging structured table recognition datasets and GPT-4 to generate the TVG dataset with question-answer pairs and vision grounding, STNet achieves state-of-the-art performance on multiple benchmarks while providing interpretable vision grounding for extracted answers.

## Method Summary
STNet combines vision encoding with text decoding to perform key information extraction with vision grounding. The model uses a Swin Transformer as the vision encoder and a BART-like Transformer as the text decoder. A special `<see>` token identifies relevant image regions, while a physical decoder extracts bounding box coordinates. The model is pre-trained on OCR, Document Read, and VQA tasks using the TVG dataset (958K questions from 65K table images), then fine-tuned on specialized datasets like CORD, SROIE, and DocVQA. The training uses 4 Tesla V100 48GB GPUs with a batch size of 28 and Adam optimizer.

## Key Results
- Achieves F1 score of 88.1 on CORD benchmark, outperforming previous state-of-the-art
- Reaches F1 score of 87.8 on SROIE benchmark with improved precision and recall
- Attains ANLS of 63.7 on DocVQA, demonstrating strong performance on document visual question answering

## Why This Works (Mechanism)
The integration of vision grounding with key information extraction allows STNet to not only identify relevant text spans but also precisely locate them in the document image. The `<see>` token acts as a visual attention mechanism, enabling the model to focus on specific regions before extracting answers. The physical decoder translates hidden states into bounding box coordinates, providing interpretable vision grounding. By pre-training on diverse tasks and fine-tuning on specialized datasets, the model learns robust document understanding capabilities.

## Foundational Learning
- **Vision Transformers (Swin Transformer)**: Used for encoding document images into visual features. Needed for capturing spatial relationships in document layouts. Quick check: Verify image resolution is 1280 × 960 and feature dimension D is 1024.
- **Sequence-to-Sequence Models (BART-like Transformer)**: Used for decoding text answers from visual features. Needed for handling the text generation task. Quick check: Confirm the model has 4 identical layers with 16 multi-heads.
- **Vision Grounding**: The process of mapping extracted answers to specific image regions. Needed for providing interpretable results. Quick check: Validate the physical decoder correctly outputs bounding box coordinates.
- **Dataset Generation with GPT-4**: Used to create the TVG dataset with question-answer pairs and vision grounding. Needed for training data with precise visual annotations. Quick check: Review a sample of generated QA pairs for quality and consistency.
- **Multi-task Pre-training**: Pre-training on OCR, Document Read, and VQA tasks. Needed for learning general document understanding capabilities. Quick check: Verify the pre-training ratio and tasks match the paper specifications.
- **Fine-tuning Strategy**: Fine-tuning on specialized datasets with a 1:1 ratio with TVG. Needed for adapting to specific benchmark tasks. Quick check: Confirm the learning rate is 5 × 10^-5 and optimizer is Adam.

## Architecture Onboarding

**Component Map**: Document Image -> Vision Encoder (Swin Transformer) -> `<see>` Token + Text Decoder (BART-like) -> Physical Decoder -> Answer + Bounding Box

**Critical Path**: Document Image → Vision Encoder → `<see>` Token → Text Decoder → Physical Decoder → Final Answer with Vision Grounding

**Design Tradeoffs**: 
- Uses a specialized `<see>` token instead of attention mechanisms for region identification, trading complexity for interpretability
- Employs a physical decoder for bounding box extraction rather than end-to-end coordinate prediction, prioritizing accuracy over speed
- Generates synthetic data with GPT-4 rather than manual annotation, trading data quality for scalability

**Failure Signatures**: 
- Poor region identification leading to irrelevant bounding boxes
- Inaccurate bounding box coordinates due to quantization errors
- Suboptimal performance on unstructured documents due to focus on structured layouts

**3 First Experiments**:
1. Implement and test the physical decoder with the specified quantization strategy to verify bounding box accuracy on the CORD dataset
2. Evaluate the model's performance on a sample of the TVG dataset to assess data quality and consistency
3. Conduct an ablation study by removing the `<see>` token to measure its impact on vision grounding accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on GPT-4 for dataset generation introduces potential variability in data quality and consistency
- Physical decoder implementation details, particularly quantization strategy, are not fully specified
- Focus on structured document understanding tasks may limit generalizability to more complex or unstructured documents

## Confidence

| Claim | Confidence |
|-------|------------|
| STNet architecture and integration of vision grounding | High |
| Benchmark results on CORD, SROIE, DocVQA | High |
| Quality and effectiveness of TVG dataset | Medium |
| Ablation studies and baseline comparisons | Medium |

## Next Checks

1. Implement and test the physical decoder with the specified quantization strategy and linear transformation to verify the accuracy of vision grounding on the CORD dataset
2. Conduct a thorough analysis of the TVG dataset by sampling and manually inspecting generated QA pairs to assess data quality and consistency
3. Reproduce the model's performance on the DocVQA benchmark, focusing on the ANLS metric and evaluating the interpretability of the vision grounding for extracted answers