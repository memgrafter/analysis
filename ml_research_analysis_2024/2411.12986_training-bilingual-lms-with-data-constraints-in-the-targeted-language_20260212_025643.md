---
ver: rpa2
title: Training Bilingual LMs with Data Constraints in the Targeted Language
arxiv_id: '2411.12986'
source_url: https://arxiv.org/abs/2411.12986
tags:
- data
- language
- english
- arxiv
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to improve pretrained language models
  in a target language with limited data by leveraging high-quality auxiliary language
  data, focusing on English as the auxiliary language. The authors systematically
  study the performance gap between training with data in a data-rich auxiliary language
  versus the target language, explore translation systems, analyze model scaling limitations,
  and propose new methods for upsampling auxiliary data.
---

# Training Bilingual LMs with Data Constraints in the Targeted Language

## Quick Facts
- arXiv ID: 2411.12986
- Source URL: https://arxiv.org/abs/2411.12986
- Reference count: 40
- Key outcome: High-quality English auxiliary data significantly improves target language performance for closely related languages, but model scaling is limited by target language data constraints

## Executive Summary
This paper investigates how to improve pretrained language models in a target language with limited data by leveraging high-quality auxiliary language data, focusing on English as the auxiliary language. The authors systematically study the performance gap between training with data in a data-rich auxiliary language versus the target language, explore translation systems, analyze model scaling limitations, and propose new methods for upsampling auxiliary data. Key findings include: (1) higher-quality English auxiliary datasets lead to significant performance gains in English and smaller gains in closely related languages like German; (2) most gains stem from relevant information sharing between auxiliary and target languages; (3) model scaling is limited when target language data is constrained, requiring target data to scale linearly with model size; and (4) upsampling important topics from the auxiliary language improves target language performance more than generic quality filtering. The results demonstrate that well-filtered English pretraining datasets can extend benefits to languages with limited data, especially for linguistically close languages, but performance gains diminish for more distant languages.

## Method Summary
The authors pretrain decoder-only transformer models (1.3B and 300M parameters) using a 5% target language and 95% auxiliary language (English) split from various English datasets. They experiment with data transformations including model-based quality filtering (DCLM), clustered dataset importance sampling, synthetic data upsampling, and translation systems. Training runs for 100K steps with batch size 1024, and models are evaluated on 6 knowledge-based QA tasks using lm-evaluation-harness. The study systematically varies data quantity (0.1B to 100B tokens) and model size (1.3B to 2.7B parameters) to understand scaling behavior under target language constraints.

## Key Results
- Higher-quality English auxiliary datasets lead to significant performance gains in English and smaller gains in closely related languages like German
- Model scaling is limited when target language data is constrained, requiring target data to scale linearly with model size
- Upsampling important topics from the auxiliary language improves target language performance more than generic quality filtering
- Performance gains from auxiliary data are most pronounced for linguistically close languages and diminish for more distant languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Better quality English auxiliary data improves target language performance by transferring relevant information across languages
- Mechanism: High-quality auxiliary data contains more diverse and representative samples of the target language's conceptual space, allowing the model to learn shared representations during pretraining
- Core assumption: There exists sufficient conceptual overlap between the auxiliary and target languages to enable knowledge transfer
- Evidence anchors:
  - [abstract] "higher-quality English auxiliary datasets lead to significant performance gains in English and smaller gains in closely related languages like German"
  - [section] "most gains stem from relevant information sharing between auxiliary and target languages"
  - [corpus] Corpus analysis shows 44% neighbor FMR, indicating moderate but present thematic overlap between papers on bilingual LM training and this work
- Break condition: Languages are too linguistically distant for meaningful information transfer to occur

### Mechanism 2
- Claim: Model scaling is limited when target language data is constrained, requiring target data to scale linearly with model size
- Mechanism: As model size increases, the model requires proportionally more target language data to avoid overfitting and performance saturation
- Core assumption: Chinchilla scaling laws apply to bilingual pretraining scenarios
- Evidence anchors:
  - [abstract] "model scaling is limited when target language data is constrained, requiring target data to scale linearly with model size"
  - [section] "performance in the target language saturates without increasing target data, regardless of increasing auxiliary data"
  - [corpus] Neighbor papers show limited citations, suggesting this specific scaling relationship in constrained data settings is underexplored
- Break condition: Target language data becomes abundant enough that scaling laws no longer limit performance

### Mechanism 3
- Claim: Upsampling important topics from the auxiliary language improves target language performance more than generic quality filtering
- Mechanism: Targeted data selection based on downstream task relevance ensures the model encounters critical concepts during pretraining, improving transfer
- Core assumption: Downstream task performance can be predicted from pretraining data characteristics
- Evidence anchors:
  - [abstract] "upsampling important topics from the auxiliary language improves target language performance more than generic quality filtering"
  - [section] "we upweight a subset of roughly 300B tokens from the English dataset... We see 4% improvement in English evaluations, and 2% improvement in the target language"
  - [corpus] Limited corpus evidence, but neighbor papers suggest data quality and selection are active research areas
- Break condition: Task-relevant topics cannot be identified in the auxiliary language or are not transferable to the target language

## Foundational Learning

- Concept: Language similarity and transfer
  - Why needed here: Understanding which auxiliary languages will be most effective for a given target language
  - Quick check question: If you have 250M tokens of German and unlimited English, will using French auxiliary data help more than English? Why or why not?

- Concept: Data scaling laws
  - Why needed here: Determining appropriate model sizes and training durations for constrained data scenarios
  - Quick check question: If you double your model size but keep target language data constant, what happens to perplexity on target language validation?

- Concept: Clustering and importance sampling
  - Why needed here: Implementing effective data selection strategies for auxiliary language upsampling
  - Quick check question: How would you modify the clustering approach if your target language evaluation data changes over time?

## Architecture Onboarding

- Component map: Data pipeline: Source filtering → Clustering → Importance weighting → Tokenization → Training
- Critical path: Data preparation and filtering (most time-consuming) → Clustering and importance weight computation → Model initialization and pretraining → Evaluation and analysis
- Design tradeoffs: Translation quality vs. training efficiency, Clustering granularity vs. computational cost, Model size vs. available target language data
- Failure signatures: Target language perplexity plateaus while English improves, Translation system produces low BLEU scores, Clustering produces imbalanced cluster sizes
- First 3 experiments: Train with mC4 English vs. FineWebEDU English as auxiliary data, Apply DCLM filter to auxiliary data and measure impact, Upsample science-related topics and compare with generic filtering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do performance gains from auxiliary data scale with linguistic distance between languages?
- Basis in paper: [explicit] The paper explicitly states that findings are inconsistent across target languages, hypothesizing that for languages "far" from English, better English datasets do not help as information is not shared between them. They observe improvements for Indo-European languages but not for Chinese, Japanese, or Korean
- Why unresolved: While the paper provides preliminary evidence of differential performance based on language families, it doesn't systematically quantify the relationship between linguistic distance and performance gains. The study is limited to a small set of languages across different families
- What evidence would resolve it: A systematic study measuring performance gains across many language pairs with varying degrees of linguistic similarity (using metrics like syntactic distance, vocabulary overlap, or typological features) would clarify this relationship

### Open Question 2
- Question: What is the optimal data scaling strategy when both model size and target language data are constrained?
- Basis in paper: [inferred] The paper finds that model scaling is limited when target language data is constrained, requiring target data to scale linearly with model size. They observe that performance saturates without increasing target data, regardless of increasing auxiliary data. However, they don't explore optimal strategies for the constrained regime
- Why unresolved: The paper identifies the limitation but doesn't provide prescriptive guidance on how to optimally allocate limited resources between model size and data acquisition, or how to balance target vs auxiliary data ratios at different scales
- What evidence would resolve it: Systematic experiments varying model size, target data amount, and auxiliary data ratios across a wider range of model sizes would establish optimal scaling strategies for data-constrained scenarios

### Open Question 3
- Question: How effective are synthetic data generation methods for data selection compared to real data?
- Basis in paper: [explicit] The paper experiments with generating synthetic examples for data selection and finds that synthetic data upsampling achieves performance within 1% of upsampling from downstream tasks for English evaluations and within 0.5% for German. However, this is based on a single experimental setup
- Why unresolved: While the paper shows synthetic data can be sufficient for computing sampling weights, it doesn't explore the broader applicability of this approach across different domains, tasks, or quality levels of synthetic generation
- What evidence would resolve it: Extensive experiments comparing synthetic vs real data selection across multiple domains, task types, and synthetic generation quality levels would establish the effectiveness and limitations of synthetic approaches

## Limitations
- Focus on English as auxiliary language limits generalizability to other auxiliary languages
- Translation system used is proprietary, making it difficult to assess translation quality and consistency
- Analysis focuses on zero-shot QA tasks, which may not capture all aspects of language model performance in target languages
- Assumes high-quality auxiliary data can effectively transfer to target languages, which may not hold for linguistically distant languages or domains with low conceptual overlap

## Confidence
- **High confidence:** The finding that model scaling is limited when target language data is constrained, requiring linear scaling of target data with model size
- **Medium confidence:** The claim that higher-quality auxiliary data leads to better target language performance
- **Low confidence:** The assertion that upsampling important topics from auxiliary language is more effective than generic quality filtering

## Next Checks
1. **Cross-linguistic validation:** Replicate the experiments with a different auxiliary language (e.g., French or Spanish) to test whether the observed benefits extend beyond English-based transfer, particularly for languages with different linguistic families

2. **Scaling law verification:** Conduct additional scaling experiments with target language data ranging from 100M to 1B tokens while varying model sizes to confirm the linear relationship between target data and model size, and identify the inflection point where target data becomes the bottleneck

3. **Topic sampling ablation:** Perform an ablation study comparing different topic selection methods (e.g., clustering vs. keyword filtering vs. random sampling) to determine whether the observed improvements are due to the specific topics selected or simply the act of upsampling any content from the auxiliary language