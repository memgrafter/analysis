---
ver: rpa2
title: Contrastive Factor Analysis
arxiv_id: '2407.21740'
source_url: https://arxiv.org/abs/2407.21740
tags:
- learning
- contrastive
- factor
- uncertainty
- non-negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Contrastive Factor Analysis (CFA), a novel
  framework that bridges traditional Factor Analysis (FA) with contrastive learning.
  CFA models the co-occurrence matrix in contrastive learning as a product of Gaussian
  latent variables, while its non-negative extension (CNFA) uses gamma latent variables.
---

# Contrastive Factor Analysis

## Quick Facts
- arXiv ID: 2407.21740
- Source URL: https://arxiv.org/abs/2407.21740
- Reference count: 10
- CIFAR-10 accuracy: 88.2% (vs 87.6% for SimCLR)

## Executive Summary
Contrastive Factor Analysis (CFA) bridges traditional Factor Analysis with contrastive learning by modeling the co-occurrence matrix as a product of Gaussian latent variables. The framework introduces a probabilistic formulation that enables uncertainty quantification and generalization to out-of-distribution data. A non-negative extension (CNFA) uses gamma latent variables to learn more disentangled representations. Experiments show CFA achieves 88.2% accuracy on CIFAR-10 (vs 87.6% for SimCLR), with CNFA improving to 88.2% and demonstrating superior disentanglement (SEPIN@10: 6.06 vs 0.79 for CL) and uncertainty estimation.

## Method Summary
CFA reframes contrastive learning's co-occurrence matrix factorization into a probabilistic factor analysis framework using Gaussian latent variables with variational posteriors. CNFA extends this by using non-negative gamma latent variables with Weibull posterior approximations for reparameterization. Both methods employ variational inference to learn expressive representations while capturing uncertainty. The framework uses a Resnet-18 backbone and trains for 200 epochs on CIFAR datasets and 100 epochs on ImageNet-100, with evaluation through linear probing, fine-tuning accuracy, PAvPU for uncertainty, and SEPIN@k for disentanglement.

## Key Results
- CIFAR-10 accuracy: CFA achieves 88.2% vs 87.6% for SimCLR baseline
- ImageNet-100: CNFA achieves 80.6% PAvPU@400 accuracy on fine-tuning
- Disentanglement: CNFA shows SEPIN@10 of 6.06 vs 0.79 for contrastive learning
- Uncertainty: CNFA demonstrates superior out-of-distribution generalization and uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mathematical equivalence between contrastive learning and matrix factorization enables CFA to model co-occurrence matrix with latent Gaussian variables
- Mechanism: CFA reframes contrastive learning's co-occurrence matrix factorization into probabilistic factor analysis framework using stochastic latent variables
- Core assumption: Co-occurrence matrix can be effectively factorized using probabilistic latent variables without losing discriminative power
- Evidence: Theoretical equivalence between spectral contrastive loss and matrix factorization objective
- Break condition: If equivalence assumption fails, CFA loses theoretical foundation

### Mechanism 2
- Claim: CNFA with gamma latent variables learns more disentangled representations than standard CFA
- Mechanism: Non-negative gamma latent variables enforce sparsity and non-negativity constraints that promote disentanglement
- Core assumption: Non-negative constraints on latent variables lead to more interpretable feature representations
- Evidence: Empirical results showing improved SEPIN@k metric for CNFA
- Break condition: If non-negativity constraint is too restrictive for data distribution

### Mechanism 3
- Claim: Variational inference with appropriate posterior approximations enables effective training while capturing uncertainty
- Mechanism: Gaussian variational posteriors for CFA and Weibull approximations for CNFA's gamma posteriors allow gradient-based optimization
- Core assumption: Chosen variational distributions adequately approximate true posteriors while remaining reparameterizable
- Evidence: Successful training of CFA and CNFA models with uncertainty estimates
- Break condition: If posterior approximations are poor, uncertainty estimates become unreliable

## Foundational Learning

- Concept: Contrastive learning fundamentals (InfoNCE loss, positive/negative sampling)
  - Why needed here: CFA builds directly on contrastive learning framework
  - Quick check question: What is the difference between InfoNCE loss and spectral contrastive loss?

- Concept: Variational inference and reparameterization tricks
  - Why needed here: Both CFA and CNFA rely on variational inference for posterior approximation
  - Quick check question: Why can't we directly use gamma distribution for CNFA's posterior?

- Concept: Non-negative matrix factorization and disentanglement
  - Why needed here: CNFA's non-negativity constraint is crucial for learning disentangled representations
  - Quick check question: How do non-negativity constraints in matrix factorization promote disentanglement?

## Architecture Onboarding

- Component map: Encoder backbone -> Feature extractor -> Latent variable inference network -> Generative model -> Training loop

- Critical path: 1) Sample positive/negative pairs from data 2) Extract features using backbone 3) Compute posterior parameters for latent variables 4) Sample latent variables using reparameterization 5) Compute likelihood and KL divergence 6) Backpropagate through ELBO

- Design tradeoffs:
  - Gaussian vs Weibull posteriors: Simpler implementation vs better approximation of gamma distribution
  - Dimension of latent space: Higher dimensions capture more information but increase computational cost
  - Number of negative samples: More negatives improve contrastive learning but increase training time

- Failure signatures:
  - Training instability: Poor posterior approximations or inappropriate latent space dimensions
  - Uncertainty estimates unreliable: Mismatch between assumed and true posterior distributions
  - No improvement over standard CL: Equivalence assumption fails or variational inference ineffective

- First 3 experiments:
  1. Verify co-occurrence matrix factorization on synthetic data with known latent structure
  2. Compare Weibull posterior approximation quality against other distributions
  3. Check if higher entropy correlates with prediction difficulty on CIFAR-10 validation set

## Open Questions the Paper Calls Out

- How does performance scale with increasing dimensionality of latent variables compared to traditional contrastive learning methods?
- What is the theoretical relationship between disentanglement properties of CNFA and underlying data distribution?
- How do CFA and CNFA perform in continual learning scenarios with evolving data distributions?

## Limitations

- Theoretical equivalence between contrastive learning and matrix factorization primarily demonstrated for spectral contrastive loss case
- Weibull approximation for gamma posteriors is heuristic rather than rigorously justified
- Lacks ablation studies on key hyperparameters like latent dimension size and posterior approximation choices

## Confidence

- Mechanism 1 (CFA equivalence): Medium - theoretical foundation established but empirical validation limited
- Mechanism 2 (CNFA disentanglement): Low-Medium - non-negativity benefits theoretically sound but empirical evidence limited
- Mechanism 3 (variational inference): Medium - standard technique but specific implementation details matter significantly

## Next Checks

1. Test CFA vs standard matrix factorization on synthetic data with known ground truth latent structure
2. Compare Weibull posterior approximation quality against other distributions for gamma latent variables
3. Conduct sensitivity analysis on latent dimension size and its impact on downstream task performance