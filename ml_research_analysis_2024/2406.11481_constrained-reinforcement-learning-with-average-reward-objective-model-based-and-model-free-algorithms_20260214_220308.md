---
ver: rpa2
title: 'Constrained Reinforcement Learning with Average Reward Objective: Model-Based
  and Model-Free Algorithms'
arxiv_id: '2406.11481'
source_url: https://arxiv.org/abs/2406.11481
tags:
- algorithm
- policy
- lemma
- where
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This monograph explores various model-based and model-free approaches
  for Constrained Reinforcement Learning (CRL) within the context of average reward
  Markov Decision Processes (MDPs). The primary objective is to maximize the average
  reward while adhering to specific constraints.
---

# Constrained Reinforcement Learning with Average Reward Objective: Model-Based and Model-Free Algorithms

## Quick Facts
- arXiv ID: 2406.11481
- Source URL: https://arxiv.org/abs/2406.11481
- Reference count: 15
- Primary result: Model-based and model-free algorithms for constrained RL with average reward objective, achieving sublinear regret and constraint violation bounds.

## Executive Summary
This monograph presents model-based and model-free approaches for Constrained Reinforcement Learning (CRL) in average reward Markov Decision Processes (MDPs). The work focuses on maximizing average reward while satisfying specific constraints. For model-based approaches, it develops optimism-based and posterior sampling-based algorithms with regret guarantees and constraint violation analysis. For model-free approaches, it explores a primal-dual policy gradient-based algorithm that achieves sublinear regret and constraint violation bounds. The monograph extends its analysis to weakly communicating MDPs, providing a comprehensive framework for CRL with theoretical guarantees.

## Method Summary
The paper develops both model-based and model-free algorithms for constrained reinforcement learning with average reward objectives. The model-based approaches use optimistic UCRL and posterior sampling methods, constructing confidence sets over transition kernels and solving conservative MDP optimizations. The model-free approach employs a primal-dual policy gradient algorithm that estimates advantage functions from long subtrajectories and updates parameters via gradient ascent/descent. The work extends to weakly communicating MDPs using finite-horizon approximations with occupancy measures, solving linear programs over convex polytopes of feasible occupancy measures.

## Key Results
- Model-based algorithms achieve regret bound of $\tilde{O}(\frac{1}{\delta}\sqrt{TMS/AT})$ and zero constraint violation
- Model-free algorithm achieves regret bound of $\tilde{O}(T^{4/5})$ and constraint violation of $\tilde{O}(T^{4/5})$
- Weakly communicating MDPs approach achieves O(T^(2/3)) regret and constraint violation bounds
- All algorithms maintain constraint satisfaction while optimizing average reward

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimistic and posterior sampling approaches achieve zero constraint violation and near-optimal regret by constructing confidence sets over transition kernels and solving conservative MDP optimization.
- Mechanism: Builds optimistic MDP or samples from Dirichlet posterior within confidence set around empirical transitions. Uses conservative constraint bound to ensure feasibility while allowing exploration. Recomputes policy only when sufficient samples are gathered.
- Core assumption: Underlying MDP is ergodic with Slater constant δ > 0 for constraints.
- Evidence anchors: Abstract mentions regret guarantees and constraint violation analysis; section states constraints can be satisfied with judicious choice of ϵe.

### Mechanism 2
- Claim: Primal-dual policy gradient algorithm achieves sublinear regret and constraint violation by estimating advantage functions from long subtrajectories and updating parameters via gradient ascent/descent.
- Mechanism: Samples multiple disjoint subtrajectories of length N = 4tmix log²T to estimate advantage functions. Gradient estimator averages over these to decorrelate variance. Updates parameters θ and λ using these estimates.
- Core assumption: Policy class is expressive enough with small transferred compatible function approximation error ϵbias.
- Evidence anchors: Abstract mentions achieving sublinear regret and constraint violation bounds; section provides gradient estimation error bound.

### Mechanism 3
- Claim: Finite-horizon approximation with occupancy measures achieves O(T^(2/3)) regret and constraint violation for weakly communicating MDPs.
- Mechanism: Divides time into episodes of length H ≈ (T/(S²A))^(1/3). Solves linear program over occupancy measures subject to constraint ⟨ν,c⟩ ≤ sp⋆c and transition confidence sets.
- Core assumption: Span of optimal bias function sp⋆c is known and true transition lies in Bernstein-type confidence set.
- Evidence anchors: Abstract mentions O(T^(2/3)) regret and constraint violation; section describes confidence set construction.

## Foundational Learning

- Concept: Ergodic vs. weakly communicating MDPs
  - Why needed here: Regret and constraint violation bounds depend on whether MDP is ergodic (uniform mixing time, stationary distribution) or weakly communicating (transient/recurrent states, span bounds).
  - Quick check question: Can you explain the difference between an ergodic MDP and a weakly communicating MDP in terms of state communication and mixing time?

- Concept: Policy gradient theorem and advantage estimation
  - Why needed here: Model-free algorithm relies on estimating ∇θJ via policy gradients, requiring accurate advantage function estimates from sampled trajectories.
  - Quick check question: How does the advantage function Aπθ(s,a) = Qπθ(s,a) - Vπθ(s) relate to the policy gradient, and why is it important to decorrelate the estimator?

- Concept: Confidence set construction for transition kernels
  - Why needed here: Both model-based approaches require constructing high-probability confidence sets around empirical transition estimates to ensure true MDP is contained.
  - Quick check question: What is the difference between L1 confidence ball in optimistic UCRL and Dirichlet posterior sampling, and how do they affect optimization?

## Architecture Onboarding

- Component map: Model-based module (optimistic UCRL or posterior sampling) -> Convex programs over occupancy measures with confidence constraints; Model-free module (primal-dual policy gradient) -> Trajectory sampling and parameter updates via gradient ascent/descent
- Critical path: Model-based: observe state → update empirical counts → check epoch condition → solve optimistic MDP → execute policy. Model-free: observe state → sample action → store trajectory → at epoch end, extract subtrajectories → estimate advantage → update parameters.
- Design tradeoffs: Model-based offers better sample efficiency and zero constraint violation but requires more memory and solving convex programs each epoch. Model-free scales to large state spaces with general parameterization but has higher regret and requires careful tuning of learning rates and epoch lengths.
- Failure signatures: Linear regret growth suggests epoch length too short or confidence sets too loose. High constraint violation indicates conservative parameter K too small. Noisy gradient estimates suggest advantage estimation window N or epoch length H insufficient.
- First 3 experiments:
  1. Implement ergodic CMDP with known transitions and verify convex program finds optimal policy with zero constraint violation.
  2. Run model-free algorithm on small ergodic CMDP and measure regret/constraint violation as function of epoch length H and learning rates α, β.
  3. Compare optimistic and posterior sampling approaches on weakly communicating CMDP and measure scaling of regret with T and span sp⋆c.

## Open Questions the Paper Calls Out

- Open Question 1: Can regret guarantees be improved and matched with the lower bound in terms of S for model-based RL algorithms?
  - Basis: Paper states there's a gap between current regret bounds and lower bound in terms of S.
  - Why unresolved: Current bounds have dependence on mixing time TM while lower bound depends on square root of diameter.
  - What evidence would resolve it: New algorithm or analysis technique achieving regret bounds matching lower bound in terms of S without dependence on mixing time.

- Open Question 2: Can model-free algorithm with sublinear regret and constraint violation guarantees be developed for weakly communicating MDPs?
  - Basis: Paper states no known model-free algorithm currently exists that guarantees sublinear regret and constraint violation for weakly communicating MDPs.
  - Why unresolved: Current model-free algorithms not designed to handle challenges of weakly communicating MDPs like lack of uniform bound for span of bias function.
  - What evidence would resolve it: New model-free algorithm specifically designed for weakly communicating MDPs achieving sublinear regret and constraint violation guarantees.

- Open Question 3: Can regret guarantees for parameterized model-free RL algorithm be improved to match Ω(√T) lower bound?
  - Basis: Paper states current O(T^(4/5)) bound is better than best-known result for tabular setup but far from Ω(√T) lower bound.
  - Why unresolved: Current regret bound still far from theoretical lower bound.
  - What evidence would resolve it: New algorithm or analysis technique achieving regret bounds matching Ω(√T) lower bound for parameterized model-free RL setting.

## Limitations
- Limited empirical validation primarily on single-queue flow and service control problem with known transitions
- Computational complexity of model-based approaches not fully characterized
- Extension to weakly communicating MDPs relies on impractical assumption of knowing span of optimal bias function

## Confidence
- Model-based regret and constraint violation bounds (ergodic): High
- Model-based regret and constraint violation bounds (weakly communicating): Medium
- Model-free regret and constraint violation bounds: Medium
- Empirical performance claims: Low

## Next Checks
1. **Scalability test**: Implement model-based algorithms on larger CMDP with hundreds of states and measure runtime of convex optimization solver as function of state and action space sizes.
2. **Unknown transition simulation**: Replace known transition model in single-queue example with unknown one and run model-based algorithms with confidence bounds. Compare achieved regret and constraint violation to theoretical predictions.
3. **Policy class expressiveness**: For model-free algorithm, systematically vary expressiveness of policy class (e.g., number of hidden layers in neural network) and measure impact on regret and constraint violation. Verify transferred compatible function approximation error decreases with increased expressiveness.