---
ver: rpa2
title: Predicting Word Similarity in Context with Referential Translation Machines
arxiv_id: '2407.06230'
source_url: https://arxiv.org/abs/2407.06230
tags:
- task
- similarity
- word
- features
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses predicting word similarity in context (GWSC)
  by casting it as a machine translation performance prediction (MTPP) task. The core
  method uses referential translation machines (RTMs) to build stacked machine learning
  models that predict artificial word pair similarity scores (awpss) by translating
  words into their contexts and computing similarity based on semantic databases and
  contextual features.
---

# Predicting Word Similarity in Context with Referential Translation Machines

## Quick Facts
- arXiv ID: 2407.06230
- Source URL: https://arxiv.org/abs/2407.06230
- Reference count: 7
- Key outcome: RTMs achieve top results in GWSC, ranking 1st, 3rd, 4th, and 5th across different evaluation metrics

## Executive Summary
This paper addresses predicting word similarity in context (GWSC) by casting it as a machine translation performance prediction (MTPP) task. The core method uses referential translation machines (RTMs) to build stacked machine learning models that predict artificial word pair similarity scores (awpss) by translating words into their contexts and computing similarity based on semantic databases and contextual features. RTMs achieve top results in GWSC, ranking 1st, 3rd, 4th, and 5th across different evaluation metrics. The method also extends to predicting emotion intensity in tweets and discriminative power of attributes with competitive performance.

## Method Summary
The method uses referential translation machines (RTMs) with stacked machine learning models to predict word similarity in context. Features are derived from semantic similarity databases (WordNet) and contextual features computed from dividing contexts into regions. RTMs use parfda for instance selection, MTPPS for feature derivation, and models like ridge regression, SVR, AdaBoost, and extremely randomized trees with feature selection and partial least squares. The approach achieved top results in GWSC, ranking 1st, 3rd, 4th, and 5th across different metrics.

## Key Results
- RTMs achieve top results in GWSC, ranking 1st, 3rd, 4th, and 5th across different evaluation metrics
- The method extends to predicting emotion intensity in tweets and discriminative power of attributes with competitive performance
- Evaluation uses Pearson's correlation, MAE, RAE, MAER, and MRAER metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Casting GWSC as MTPP allows the use of referential translation machines (RTMs) to generate contextual features that approximate human-graded similarity scores.
- Mechanism: RTMs translate words into their contexts and compute similarity using semantic databases (WordNet) and contextual features (intra-cwps, inter-cwps). The stacked machine learning models then learn to map these features to SimLex999-style similarity scores.
- Core assumption: Similarity in context can be modeled as a translation task where the "source" is the target word and the "target" is its surrounding context.
- Evidence anchors:
  - [abstract] "We identify the similarity between two words in English by casting the task as machine translation performance prediction (MTPP) between the words given the context and the distance between their similarities."
  - [section] "We use referential translation machines (RTMs), which allows a common representation for training and test sets and stacked machine learning models."
  - [corpus] Weak: No direct corpus evidence provided for RTM performance on GWSC; relies on claimed top rankings.
- Break condition: If contextual features fail to capture semantic similarity or if translation models do not generalize well to unseen contexts.

### Mechanism 2
- Claim: Intra-context word pair similarity (intra-cwps) features measure semantic attraction between regions surrounding the word pair, approximating SimLex999 similarity.
- Mechanism: The context is divided into 4 regions; averaged similarity matrices are built using semantic similarity databases to obtain similarity scores for word pairs among words appearing in different regions.
- Core assumption: Semantic similarity can be captured by averaging pairwise similarities across structured regions of context.
- Evidence anchors:
  - [section] "Intra context wps (intra-cwps) features measure the positive or negative semantic attraction between the regions surrounding the word pair."
  - [section] "The averaged intra-cwps scores approach the SimLex999 type of similarity."
  - [corpus] Weak: No explicit corpus evidence that this regional averaging improves similarity prediction; stated as an assumption.
- Break condition: If the regional division does not align with natural semantic boundaries or if averaging dilutes discriminative signals.

### Mechanism 3
- Claim: Inter-context word pair similarity (inter-cwps) features model the change in similarity when context changes, enabling prediction of context-dependent shifts.
- Mechanism: Each context is divided into 3 regions; inter-cwps scores are computed for word pairs appearing in paired regions of the two contexts, capturing contextual semantic shifts.
- Core assumption: Change in similarity across contexts can be modeled by comparing paired regional similarities between the two contexts.
- Evidence anchors:
  - [section] "Inter context wps (inter-cwps) features measure the contextual changes in the semantics surrounding the target word pair in the two contexts and they are used to model the change in wps even if both contexts have the same score."
  - [section] "The third feature set is the inter wps change features and they are used to obtain a score for the change with the difference between the averaged wps difference of intra and inter regions."
  - [corpus] Weak: No direct corpus evidence that inter-cwps captures context change; relies on method description.
- Break condition: If context change does not correlate with similarity change or if regional pairing is arbitrary.

## Foundational Learning

- Concept: Semantic similarity vs. relatedness
  - Why needed here: GWSC requires modeling graded similarity (not just relatedness), distinguishing between words that are semantically similar (e.g., "car" and "truck") versus merely related (e.g., "car" and "road").
  - Quick check question: What is the key difference between similarity and relatedness in semantic modeling, and why does it matter for GWSC?
- Concept: Contextual embeddings and feature engineering
  - Why needed here: The method relies on engineered features (intra-cwps, inter-cwps) rather than raw embeddings; understanding how to extract and combine such features is critical.
  - Quick check question: How do intra-cwps and inter-cwps features differ in what they capture about word pairs in context?
- Concept: Stacked machine learning and RTM feature generation
  - Why needed here: RTMs generate features for both training and test sets in the same space, enabling robust stacked models; understanding this process is key to replicating or extending the approach.
  - Quick check question: Why does RTM require a common feature space for training and test sets, and how does this enable prediction in unsupervised tasks?

## Architecture Onboarding

- Component map: Input (word pairs, contexts) -> Feature extraction (intra-cwps, inter-cwps, WordNet) -> RTM model (parfda, MTPPS) -> Stacked ML models -> Output (awpss, change scores)
- Critical path:
  1. Parse contexts and divide into regions
  2. Compute intra- and inter-cwps features using semantic databases
  3. Feed features into RTM-based stacked models
  4. Generate awpss and context change scores
- Design tradeoffs:
  - Regional division granularity vs. feature interpretability and noise
  - RTM vs. direct contextual embeddings (e.g., BERT) for feature generation
  - Monotonicity assumption for transitivity vs. more nuanced similarity propagation
- Failure signatures:
  - Poor correlation between awpss and human scores (SimLex999)
  - High variance in predictions for similar contexts
  - RTM feature space fails to capture context-specific nuances
- First 3 experiments:
  1. Verify that intra-cwps features correlate with SimLex999 scores on a held-out set
  2. Test whether inter-cwps captures context change by comparing predicted vs. actual shifts
  3. Compare RTM-based predictions with a baseline using only WordNet similarity scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the monotonicity assumption used to expand WPSIMDAT to 110K pairs affect the accuracy of word similarity predictions?
- Basis in paper: [explicit] The paper mentions using a monotonicity assumption to transitively compute word pair similarities, expanding the dataset size.
- Why unresolved: The paper doesn't provide empirical evidence on how this expansion affects prediction accuracy or whether the assumption introduces significant errors.
- What evidence would resolve it: Comparative experiments showing prediction accuracy with and without the expanded dataset, or analysis of how often the monotonicity assumption leads to incorrect similarity scores.

### Open Question 2
- Question: How do the inter-cwps and intra-cwps features contribute differently to the final word pair similarity predictions?
- Basis in paper: [explicit] The paper describes both feature types but doesn't analyze their individual contributions to prediction performance.
- Why unresolved: While both feature types are used in the model, their relative importance and individual impact on prediction accuracy are not quantified.
- What evidence would resolve it: Ablation studies showing prediction performance when using only inter-cwps, only intra-cwps, or different combinations of these features.

### Open Question 3
- Question: How does the choice of translation models and corpora for RTM feature generation impact prediction performance across different languages and domains?
- Basis in paper: [explicit] The paper mentions using specific translation models and WMT corpora but doesn't explore how different choices might affect results.
- Why unresolved: The paper doesn't investigate the sensitivity of RTM performance to the choice of translation models, corpora, or their size, which could significantly impact cross-lingual and domain-specific predictions.
- What evidence would resolve it: Systematic experiments varying translation models, corpora, and their sizes to measure impact on prediction accuracy across different language pairs and domains.

## Limitations
- The core mechanisms rely on engineered features (intra-cwps, inter-cwps) computed from semantic databases rather than learned representations, which may limit generalization to contexts with novel semantic relationships.
- The paper provides claimed top rankings but lacks direct corpus evidence demonstrating that the regional averaging and pairing approaches genuinely capture semantic similarity and context change.
- The RTM framework's reliance on parfda and MTPPS for instance selection and feature derivation is mentioned but not fully specified, making faithful reproduction challenging.

## Confidence
- **High Confidence**: The general framework of casting GWSC as MTPP using RTMs is well-defined and the evaluation metrics (Pearson's correlation, MAE, RAE, MAER, MRAER) are standard.
- **Medium Confidence**: The claimed top rankings across multiple metrics suggest strong performance, but without detailed corpus evidence or ablation studies, the specific contributions of intra-cwps and inter-cwps features are uncertain.
- **Low Confidence**: The exact implementation details of parfda, MTPPS, and the grid search ranges for model hyperparameters are not provided, limiting reproducibility.

## Next Checks
1. **Feature Correlation Validation**: Verify that intra-cwps features correlate with SimLex999 scores on a held-out test set, and compare this correlation to a baseline using only WordNet similarity scores.
2. **Context Change Capture**: Test whether inter-cwps features actually capture context-dependent similarity shifts by measuring the correlation between predicted and actual changes in similarity when contexts are altered.
3. **Ablation Study**: Perform an ablation study to determine the relative importance of intra-cwps, inter-cwps, and WordNet features in driving model performance, and assess whether removing any component significantly degrades results.