---
ver: rpa2
title: Just Label the Repeats for In-The-Wild Audio-to-Score Alignment
arxiv_id: '2411.07428'
source_url: https://arxiv.org/abs/2411.07428
tags:
- music
- alignment
- score
- audio
- sheet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of aligning scanned sheet music
  images with corresponding performance audio, particularly when the sheet music contains
  repeat signs and jumps. The authors propose a pragmatic workflow that combines automated
  alignment with quick human annotation of jumps, improving alignment quality significantly.
---

# Just Label the Repeats for In-The-Wild Audio-to-Score Alignment

## Quick Facts
- arXiv ID: 2411.07428
- Source URL: https://arxiv.org/abs/2411.07428
- Authors: Irmak Bukey; Michael Feffer; Chris Donahue
- Reference count: 0
- Primary result: Human-annotated repeat labels improve sheet music-audio alignment accuracy by 150% relative, from 33% to 82%

## Executive Summary
This paper addresses the challenge of aligning scanned sheet music with performance audio when the sheet music contains repeat signs and jumps. The authors propose a pragmatic solution that combines automated alignment with quick human annotation of repeats. By using dynamic time warping to align enhanced feature representations from both score and audio, and introducing a measure-aware evaluation protocol, they achieve significantly better alignment quality than previous approaches. The method shows that with minimal human supervision (less than 6 seconds per page for annotating repeats), alignment accuracy improves dramatically on a dataset of 13 piano pieces.

## Method Summary
The method uses measure detection as preprocessing to improve notehead detection consistency, then creates bootleg score representations from sheet music images. For audio features, it uses raw onset prediction probabilities from the Onsets and Frames transcription model rather than post-processed MIDI representations. Dynamic time warping aligns these feature representations, and a measure-aware evaluation protocol assesses alignment quality. The key innovation is a user interface that allows rapid annotation of repeat signs by clicking on measures, which provides DTW with exact jump locations and dramatically improves alignment accuracy.

## Key Results
- Alignment accuracy improves by 150% relative to prior work, from 33% to 82% MAcc on MeSA-13 dataset
- Human annotation of repeat signs takes less than 6 seconds per page on average
- Raw onset prediction probabilities from transcription models outperform MIDI representations for alignment
- Measure segmentation preprocessing improves detection consistency for noteheads and staff lines

## Why This Works (Mechanism)

### Mechanism 1
Human-annotated repeat labels dramatically improve alignment accuracy compared to fully automated jump handling. Humans can quickly identify and label repeat signs by clicking on measures, providing exact jump locations that DTW can use directly. This bypasses the need for complex automatic jump detection algorithms that struggle with ambiguous repeat notation. The core assumption is that annotating repeats takes only seconds per page and provides sufficient information for high-quality alignment.

### Mechanism 2
Using raw onset prediction probabilities from transcription models outperforms using post-processed MIDI transcriptions for alignment. Raw probabilities preserve more temporal detail and uncertainty information that gets smoothed out in quantized MIDI representations. DTW can better match the continuous feature spaces. The core assumption is that the transcription model's raw output contains more useful alignment information than its processed output.

### Mechanism 3
Measure detection preprocessing improves the quality of notehead and staff line detection for bootleg score creation. By segmenting full pages into individual measure images and resizing them uniformly, detection algorithms operate on more consistent input scales, reducing variance in detected feature sizes. The core assumption is that detection algorithms are sensitive to absolute pixel sizes of musical symbols.

## Foundational Learning

- Concept: Dynamic Time Warping (DTW) for sequence alignment
  - Why needed here: The core alignment algorithm that matches audio and score feature sequences while allowing for temporal warping
  - Quick check question: How does DTW handle sequences of different lengths, and what is the computational complexity?

- Concept: Music transcription and onset detection
  - Why needed here: Understanding how transcription models generate onset probabilities and why raw probabilities are more useful than quantized outputs
  - Quick check question: What is the difference between frame-level probabilities and onset predictions in music transcription models?

- Concept: Measure-aware evaluation metrics
  - Why needed here: The proposed evaluation framework measures alignment quality in musical measures rather than absolute time, which better reflects perceptual alignment quality
  - Quick check question: How does the MAcc metric differ from traditional alignment evaluation metrics like precision/recall?

## Architecture Onboarding

- Component map: Score feature extraction (measure detection → notehead detection → piano roll creation) → Audio feature extraction (transcription model → onset probabilities) → DTW alignment → Evaluation (measure-aware metrics)
- Critical path: Score feature extraction → Audio feature extraction → DTW alignment
- Design tradeoffs: Human annotation vs automation (accuracy vs scalability), raw probabilities vs quantized MIDI (temporal detail vs interpretability), measure segmentation vs full-page processing (detection consistency vs alignment complexity)
- Failure signatures: Poor alignment quality suggests issues with feature extraction, incorrect repeat labels, or inappropriate DTW parameters; evaluation metrics reveal whether failures are systematic or random
- First 3 experiments:
  1. Compare alignment quality with and without repeat labels on pieces containing repeats
  2. Test different audio feature representations (raw probabilities vs MIDI vs frame probabilities) on the same dataset
  3. Evaluate the impact of measure segmentation preprocessing on notehead detection quality across different sheet music styles

## Open Questions the Paper Calls Out
None

## Limitations
- Small dataset size (MeSA-13 with only 13 pieces) limits generalizability
- Human annotation workflow not tested for scalability or consistency across different annotators
- Measure detection benefits may be specific to the font and notation styles in the dataset

## Confidence
- **High confidence**: The overall improvement in alignment accuracy (150% relative gain from 33% to 82% MAcc) and the effectiveness of human-annotated repeat labels
- **Medium confidence**: The superiority of raw onset probabilities over MIDI representations
- **Low confidence**: The claimed benefits of measure detection preprocessing for notehead detection quality

## Next Checks
1. Test the human annotation workflow and alignment system on a larger, more diverse dataset to verify scalability and generalizability
2. Systematically compare raw onset probabilities, frame probabilities, and MIDI representations across different music genres and difficulty levels
3. Conduct a multi-annotator study where different users annotate repeat signs on the same sheet music to measure inter-annotator agreement