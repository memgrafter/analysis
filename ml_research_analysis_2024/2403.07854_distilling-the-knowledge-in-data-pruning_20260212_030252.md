---
ver: rpa2
title: Distilling the Knowledge in Data Pruning
arxiv_id: '2403.07854'
source_url: https://arxiv.org/abs/2403.07854
tags:
- pruning
- data
- student
- training
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of knowledge distillation (KD)
  to improve model accuracy when training on pruned datasets. By leveraging a teacher
  model trained on the full dataset to guide the student model during training on
  a pruned subset, the authors demonstrate significant improvements across datasets,
  pruning methods, and pruning fractions.
---

# Distilling the Knowledge in Data Pruning

## Quick Facts
- arXiv ID: 2403.07854
- Source URL: https://arxiv.org/abs/2403.07854
- Authors: Emanuel Ben-Baruch; Adam Botach; Igor Kviatkovsky; Manoj Aggarwal; Gérard Medioni
- Reference count: 40
- Key outcome: Knowledge distillation (KD) significantly improves model accuracy when training on pruned datasets, with random pruning and KD outperforming sophisticated pruning methods in high compression regimes.

## Executive Summary
This paper explores how knowledge distillation can mitigate the accuracy loss typically seen when training on pruned datasets. By using a teacher model trained on the full dataset to guide a student model during training on a pruned subset, the authors demonstrate consistent improvements across multiple datasets, pruning methods, and pruning fractions. A key finding is that random pruning with KD outperforms more sophisticated pruning algorithms, especially when significant compression is required. The paper also identifies an inverse relationship between the pruning factor and the optimal KD weight, suggesting that as the dataset becomes smaller, the student should rely more heavily on the teacher's soft predictions.

## Method Summary
The method involves training a teacher model on the full dataset, then pruning the dataset using various score-based or random methods to create a smaller subset. A student model with identical architecture is then trained on this pruned subset using a combined loss function that incorporates both classification loss and knowledge distillation loss. The KD weight α is adapted based on the pruning factor f, with lower pruning factors requiring higher α values to leverage the teacher's soft predictions more heavily. The approach is evaluated across CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets using various pruning methods including forgetting, GraNd, EL2N, memorization, and random pruning.

## Key Results
- KD consistently improves accuracy across all pruning methods and fractions compared to standard training without KD
- Random pruning with KD outperforms sophisticated pruning methods in high compression regimes
- There is a crucial inverse relationship between pruning factor f and optimal KD weight α
- For small pruning fractions, teachers with smaller capacities than the student can improve results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation reduces the bias of the student's estimation error when training on pruned data
- Mechanism: The teacher model, trained on the full dataset, provides soft predictions that encapsulate richer information about class relationships and data distribution than one-hot labels, guiding the student to learn more robust representations
- Core assumption: The teacher model's soft predictions contain valuable information about class hierarchies and data distribution not present in the pruned subset
- Evidence anchors: Abstract states significant improvement across datasets and pruning methods; section provides theoretical justification for linear regression case; corpus papers offer weak evidence focused on general KD contexts
- Break condition: If the teacher model is not well-trained or fails to capture essential information about the full dataset, soft predictions may not provide necessary guidance

### Mechanism 2
- Claim: KD helps mitigate the impact of noisy labels and low-quality images retained by pruning algorithms
- Mechanism: By relying more on the teacher's soft predictions, the student model learns from the teacher's understanding of label ambiguity and class relationships, reducing influence of potentially noisy or low-quality samples
- Core assumption: Pruning algorithms tend to retain harder samples that may have noisy labels or be of lower quality, and KD can help the student learn from the teacher's understanding of these ambiguities
- Evidence anchors: Abstract mentions mitigating impact of noisy labels and low-quality images; section explains how student learns label ambiguity through KD; corpus papers don't specifically address noisy labels in pruned datasets
- Break condition: If pruning algorithm doesn't retain harder samples or doesn't significantly increase noisy samples, KD benefit may be reduced

### Mechanism 3
- Claim: The optimal KD weight α is inversely related to the pruning factor f
- Mechanism: As pruning factor decreases and pruned subset becomes smaller and potentially noisier, increasing KD weight allows student to rely more on teacher's soft predictions that encapsulate information about full dataset distribution
- Core assumption: Teacher's soft predictions become more valuable as pruned subset size decreases and potential for noise increases
- Evidence anchors: Abstract demonstrates crucial connection between pruning factor and optimal KD weight; section empirically shows reliance on teacher increases as f decreases; corpus papers don't address relationship between KD weight and pruning factor
- Break condition: If pruned subset isn't significantly noisier than full dataset or teacher isn't well-trained, benefit of increasing KD weight may not be pronounced

## Foundational Learning

- Concept: Knowledge Distillation (KD)
  - Why needed here: KD is the core technique used to transfer knowledge from teacher model to student model when training on pruned data
  - Quick check question: What is the purpose of using the Kullback-Leibler (KL) divergence in the KD loss function?

- Concept: Self-Distillation
  - Why needed here: The paper focuses on self-distillation where teacher and student have identical architectures to demonstrate KD effectiveness in data pruning context
  - Quick check question: How does self-distillation differ from traditional KD in terms of teacher and student architectures?

- Concept: Data Pruning
  - Why needed here: Data pruning is the process of selecting representative samples from dataset to reduce computational costs while maintaining model accuracy
  - Quick check question: What are potential drawbacks of using score-based pruning algorithms, and how does KD help mitigate these drawbacks?

## Architecture Onboarding

- Component map: Full dataset -> Teacher model (pre-trained) -> Pruned subset (via pruning algorithm) -> Student model (trained with KD) -> Test evaluation

- Critical path:
  1. Pre-train teacher model on full dataset
  2. Apply pruning algorithm to select subset of data
  3. Train student model on pruned subset using combined loss function
  4. Evaluate student model's accuracy on test set

- Design tradeoffs:
  - Teacher model capacity: Larger teachers provide more accurate soft predictions but may increase capacity gap with student, potentially degrading accuracy in high pruning regimes
  - KD weight α: Higher values increase reliance on teacher's soft predictions but may reduce impact of ground-truth labels
  - Pruning factor f: Lower values result in smaller pruned subsets but may increase proportion of noisy or low-quality samples

- Failure signatures:
  - If student accuracy is lower than expected, teacher model may not be well-trained or pruning algorithm may not effectively select representative samples
  - If increasing KD weight α doesn't improve accuracy, teacher's soft predictions may not provide valuable information or pruned subset may not be significantly noisier than full dataset

- First 3 experiments:
  1. Train teacher model on full dataset and evaluate its accuracy
  2. Apply pruning algorithm to select subset of data and compare distribution of retained samples to full dataset
  3. Train student model on pruned subset using KD with varying values of α and evaluate accuracy on test set

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical justification for KD reducing estimation bias is provided only for linear regression, not deep learning contexts
- Findings focus primarily on classification tasks with CNNs, limiting generalizability to other architectures or problem domains
- Effectiveness depends heavily on quality of teacher model and characteristics of pruning algorithm
- Counterintuitive claim that smaller-capacity teachers can outperform larger ones in small pruning fractions needs more rigorous validation

## Confidence
- High Confidence: KD consistently improves accuracy when training on pruned datasets compared to standard training without KD
- Medium Confidence: The relationship between pruning factor f and optimal KD weight α is inversely proportional
- Medium Confidence: Random pruning with KD outperforms sophisticated pruning methods in high compression regimes
- Low Confidence: Smaller-capacity teachers can outperform larger ones in small pruning fractions due to reduced capacity gap

## Next Checks
1. Cross-architecture validation: Test whether observed KD benefits extend to transformer-based architectures and regression tasks beyond classification
2. Teacher quality sensitivity: Systematically evaluate how teacher model quality affects KD effectiveness on pruned data
3. Noise injection study: Create controlled experiments with varying levels of label noise in pruned subset to quantify KD's ability to mitigate noisy labels as claimed