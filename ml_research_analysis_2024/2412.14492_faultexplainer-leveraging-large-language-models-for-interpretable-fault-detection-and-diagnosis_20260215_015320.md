---
ver: rpa2
title: 'FaultExplainer: Leveraging Large Language Models for Interpretable Fault Detection
  and Diagnosis'
arxiv_id: '2412.14492'
source_url: https://arxiv.org/abs/2412.14492
tags:
- fault
- feed
- process
- root
- faults
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FaultExplainer, a novel tool for fault detection,
  diagnosis, and explanation in chemical processes. The system combines real-time
  sensor data visualization, PCA-based fault detection, and large language models
  (LLMs) to generate interpretable explanations for process faults.
---

# FaultExplainer: Leveraging Large Language Models for Interpretable Fault Detection and Diagnosis

## Quick Facts
- arXiv ID: 2412.14492
- Source URL: https://arxiv.org/abs/2412.14492
- Reference count: 13
- Key outcome: FaultExplainer combines PCA-based fault detection with LLM explanations to achieve 7-9/11 fault classification accuracy on TEP dataset

## Executive Summary
FaultExplainer introduces a novel tool for fault detection, diagnosis, and explanation in chemical processes by combining real-time sensor data visualization, PCA-based fault detection, and large language models (LLMs). The system uses PCA and T² statistics to detect faults and identify top contributing variables, which are then used to ground LLM explanations in domain-specific context. Evaluated on the Tennessee Eastman Process with two LLM models (GPT-4o and o1-preview) and two prompting strategies, the approach achieves 7-9 correct root cause identifications out of 11 detectable faults when provided with predefined causes, and 8 out of 11 with general reasoning.

## Method Summary
The approach uses PCA to reduce dimensionality of TEP data while retaining 90% variance, then applies T² statistics to detect faults when measurements deviate from normal operating conditions (threshold α=0.01). Top 6 contributing variables are identified using feature importance analysis and used to construct LLM prompts. Two prompting strategies are employed: one with predefined root causes (15 known fault types) and one allowing general reasoning. The system queries GPT-4o and o1-preview models to generate fault explanations, which are evaluated for accuracy and plausibility against ground truth.

## Key Results
- GPT-4o correctly classified 7 out of 11 detectable faults with predefined root causes, 8 out of 11 with general reasoning
- o1-preview demonstrated superior reasoning with 9 out of 11 correct classifications with predefined causes, 8 out of 11 with general reasoning
- Both models provided consistent and plausible interpretations of observed feature changes, though limitations occurred when feature contributions weren't directly linked to root causes
- PCA-based feature selection effectively identified contributing variables for most faults but struggled with subtle faults involving gradual changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCA-based fault detection with T² statistics effectively identifies significant deviations in multivariate process data.
- Mechanism: PCA reduces dimensionality of the Tennessee Eastman Process data while retaining 90% variance. T² statistics compare real-time measurements against normal operating conditions, flagging faults when T² exceeds a threshold (α=0.01).
- Core assumption: Process data follows approximately Gaussian distribution and linear relationships are sufficient for fault detection.
- Evidence anchors: [abstract] "PCA and T² statistics to detect faults and identify top contributing variables"; [section 4.1] "PCA-based approach. PCA is a classical statistical learning technique used to reduce the dimensionality of a dataset"; [corpus] Weak evidence - neighboring papers focus on transformers and deep learning, not PCA-based methods
- Break condition: When process data exhibits strong non-linearities or non-Gaussian distributions that PCA cannot adequately capture

### Mechanism 2
- Claim: LLM explanations grounded in process descriptions and top contributing features produce more reliable fault explanations than ungrounded LLMs.
- Mechanism: The system provides LLMs with detailed TEP process descriptions, manipulated vs measured variable distinctions, and top 6 PCA-contributing features with deviations from normal operation.
- Core assumption: LLMs can effectively reason about process dynamics when provided with structured, domain-specific context and quantitative feature information.
- Evidence anchors: [abstract] "ground LLM explanations in a detailed TEP process description, PCA, and feature importance analysis"; [section 4.2] "we provide a detailed description of the process, including its unit operations, streams, and variables"; [section 5.2.2] "Both models succeed in providing consistent and plausible interpretations of the observed feature changes"
- Break condition: When PCA-selected features do not directly relate to the root cause, leading to hallucinations even with grounding

### Mechanism 3
- Claim: Dual prompting strategy (predefined root causes vs general reasoning) enables testing of LLM reasoning capabilities in both familiar and novel fault scenarios.
- Mechanism: The Root Causes-Included Prompt provides 15 known fault types as constrained search space, while General Reasoning Prompt allows LLMs to independently infer root causes.
- Core assumption: LLMs can leverage both constrained and open-ended reasoning approaches effectively for fault diagnosis.
- Evidence anchors: [abstract] "We evaluate the LLMs' reasoning capabilities in two scenarios: one where historical root causes are provided, and one where they are not"; [section 4.2] "Two different prompts are developed to support fault diagnosis"; [section 5.2.1] "Among the 11 faults that can be identified using PCA, GPT-4o correctly classifies 7 faults, while o1-preview correctly classifies 9 faults"
- Break condition: When LLMs fail to correlate indirect feature changes with actual root causes, regardless of prompting strategy

## Foundational Learning

- Concept: Principal Component Analysis (PCA) and T² statistics
  - Why needed here: PCA reduces 41-dimensional TEP data to manageable components while T² statistics provide quantitative fault detection threshold
  - Quick check question: If a fault causes T²=3.5 and the threshold is T²₀.₀₁=2.8, what should the system do?

- Concept: Feature contribution analysis and top-k selection
  - Why needed here: Identifies which variables contribute most to detected faults, providing LLMs with focused diagnostic information
  - Quick check question: If variable X has contribution 0.45, variable Y has 0.30, and variable Z has 0.25, which three variables should be included in the LLM prompt?

- Concept: Prompt engineering for domain-specific reasoning
  - Why needed here: Structured prompts with process descriptions and feature deviations guide LLMs to generate grounded, rather than hallucinated, explanations
  - Quick check question: What key information must be included in prompts to prevent LLM hallucinations in chemical process fault diagnosis?

## Architecture Onboarding

- Component map: TEP Simulator → PCA Feature Extraction → T² Fault Detection → Top-k Feature Selection → LLM Prompt Construction → Explanation Generation → Interactive Web Interface
- Critical path: Sensor data acquisition → PCA model (trained on normal conditions) → T² computation → fault trigger detection → feature contribution analysis → LLM prompt generation → explanation output
- Design tradeoffs: PCA-based feature selection (fast, interpretable) vs deep learning feature extraction (more complex patterns, less interpretable); predefined root causes (higher accuracy for known faults) vs general reasoning (better for novel faults)
- Failure signatures: False negatives (faults missed due to PCA limitations), false positives (normal variations flagged as faults), LLM hallucinations (ungrounded explanations), feature irrelevance (selected features don't relate to root cause)
- First 3 experiments:
  1. Test PCA model on normal operating data to verify T² threshold sensitivity and false alarm rate
  2. Evaluate LLM explanations on known fault cases with both prompting strategies to compare accuracy
  3. Test system response to novel fault patterns not in the 15 predefined root causes to assess generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the system's feature selection methodology be improved to better capture subtle or indirect relationships between process variables and root causes, particularly for faults involving gradual changes like reaction kinetics drift?
- Basis in paper: [inferred] from limitations section discussing PCA's failure to identify features strongly tied to the true root cause in Faults 10 and 13
- Why unresolved: Current PCA-based approach relies on linear relationships and may miss complex, non-linear interactions between variables
- What evidence would resolve it: Comparative study showing improved fault detection accuracy using alternative feature selection methods (e.g., causal inference, domain-informed feature engineering) against PCA

### Open Question 2
- Question: What is the optimal balance between providing predefined root causes and allowing general reasoning for the LLM to achieve the best diagnostic accuracy across different types of faults?
- Basis in paper: [explicit] from the comparison between Root Causes-Included Prompt and General Reasoning Prompt settings
- Why unresolved: The paper shows both approaches have merits but doesn't determine which is superior or if a hybrid approach would work best
- What evidence would resolve it: Systematic evaluation of fault classification accuracy across various fault types using different ratios of predefined vs. general reasoning prompts

### Open Question 3
- Question: How can fine-tuned LLMs trained on chemical process data improve the specificity and reliability of fault explanations compared to general-purpose models like GPT-4o and o1-preview?
- Basis in paper: [explicit] from the discussion section suggesting this as a future improvement
- Why unresolved: The paper only tested general-purpose LLMs without domain-specific fine-tuning
- What evidence would resolve it: Comparative analysis of fault explanation accuracy and hallucination rates between fine-tuned chemical engineering LLMs and general-purpose models

## Limitations
- PCA-based approach assumes linear relationships in process data, potentially missing complex non-linear fault patterns
- Feature contribution analysis can fail when root causes don't directly correlate with top contributing variables, leading to LLM hallucinations
- System performance depends heavily on quality of TEP process description, limiting generalization to all industrial processes

## Confidence
- **High confidence**: PCA-based fault detection mechanism and T² statistics threshold calculation, as these are well-established statistical methods with clear mathematical foundations
- **Medium confidence**: LLM explanation quality and reasoning capabilities, since results show good performance but with some hallucinations and limitations in detecting subtle faults
- **Medium confidence**: Generalization to real-world industrial processes, as evaluation is limited to the Tennessee Eastman Process dataset which may not represent all chemical process complexities

## Next Checks
1. Test the system on synthetic non-linear fault patterns to evaluate PCA limitations and compare against deep learning-based feature extraction methods
2. Conduct ablation studies by varying the number of contributing features (currently 6) to determine optimal selection for LLM grounding
3. Evaluate the system on real-world industrial process data from multiple chemical plants to assess generalizability beyond the TEP benchmark