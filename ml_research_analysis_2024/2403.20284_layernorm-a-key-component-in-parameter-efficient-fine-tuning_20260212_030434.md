---
ver: rpa2
title: 'LayerNorm: A key component in parameter-efficient fine-tuning'
arxiv_id: '2403.20284'
source_url: https://arxiv.org/abs/2403.20284
tags:
- layernorm
- fine-tuning
- information
- parameters
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes which components of the BERT model are most
  important for fine-tuning. The key finding is that LayerNorm undergoes the most
  significant changes during fine-tuning across different GLUE tasks.
---

# LayerNorm: A key component in parameter-efficient fine-tuning

## Quick Facts
- arXiv ID: 2403.20284
- Source URL: https://arxiv.org/abs/2403.20284
- Authors: Taha ValizadehAslani; Hualou Liang
- Reference count: 27
- Key outcome: Fine-tuning only LayerNorm parameters achieves comparable performance to full fine-tuning with 0.015% of parameters

## Executive Summary
This paper investigates which components of BERT undergo the most significant changes during fine-tuning across different GLUE tasks. The analysis reveals that LayerNorm parameters experience the largest changes compared to other model components. Building on this finding, the authors demonstrate that fine-tuning only LayerNorm parameters can achieve performance comparable to full fine-tuning while using only 0.015% of the parameters. Furthermore, they show that a small subset of LayerNorm parameters selected using Fisher information can achieve even better performance than training the entire LayerNorm, making this approach more parameter-efficient than existing methods like BitFit.

## Method Summary
The method involves computing Fisher information for all parameters in BERT to identify the most important components for fine-tuning. The authors find that LayerNorm has the maximum Fisher information across all components. They then fine-tune only LayerNorm parameters (and subsets thereof selected by Fisher information) on GLUE tasks and compare performance against full fine-tuning and other parameter-efficient methods. The training procedure uses BERT-large-cased on GLUE benchmarks (excluding WNLI) with learning rates ranging from 1e-5 to 5e-5 for full fine-tuning and 1e-4 to 1e-3 for parameter-efficient methods.

## Key Results
- LayerNorm parameters undergo the largest changes during fine-tuning across GLUE tasks
- Fine-tuning only LayerNorm achieves comparable performance to full fine-tuning with 0.015% of parameters
- Fisher-selected subsets of LayerNorm parameters sometimes outperform training the entire LayerNorm
- The proposed method is more parameter-efficient than BitFit, using one-fifth of the parameters for similar performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LayerNorm parameters undergo the largest changes during fine-tuning compared to other model components.
- Mechanism: LayerNorm acts as a critical normalization layer whose learned parameters directly control the distribution of activations across layers. During fine-tuning, the model adapts these parameters to better align with task-specific data distributions, causing larger parameter shifts.
- Core assumption: The magnitude of parameter change during fine-tuning correlates with the importance of the component for task adaptation.
- Evidence anchors:
  - [abstract] The paper finds that output LayerNorm changes more than any other components when fine-tuned for different GLUE tasks.
  - [section 2.2] Heat maps show that the most significant change happens in the output LayerNorm across most GLUE tasks.
  - [corpus] The corpus neighbors include studies like "Geometric Interpretation of Layer Normalization and a Comparative Analysis with RMSNorm," supporting the significance of LayerNorm analysis.
- Break condition: If LayerNorm is removed or its parameters are frozen, performance degrades significantly (as shown by Kovaleva et al., 2021).

### Mechanism 2
- Claim: Fisher information is highest for LayerNorm parameters, indicating their importance for fine-tuning.
- Mechanism: Fisher information measures how much information a parameter carries about the model's output distribution. High Fisher information means the parameter is crucial for model predictions. The paper computes Fisher information for each parameter and finds LayerNorm has the maximum value.
- Core assumption: Fisher information is a reliable metric for identifying important parameters in a neural network.
- Evidence anchors:
  - [section 2.4] The Fisher information of each parameter is calculated, and LayerNorm is shown to have the maximum Fisher information among all BERT components.
  - [abstract] The paper demonstrates that LayerNorm possesses the maximum Fisher information among all the components of BERT.
  - [corpus] The corpus includes "PETapter: Leveraging PET-style classification heads for modular few-shot parameter-efficient fine-tuning," which may relate to parameter importance in fine-tuning.
- Break condition: If Fisher information is calculated differently or if the model architecture changes, the ranking of component importance might change.

### Mechanism 3
- Claim: Fine-tuning only LayerNorm achieves comparable performance to full fine-tuning with significantly fewer parameters.
- Mechanism: By focusing on the most important component (LayerNorm), the model can adapt to the task without needing to update all parameters. This reduces computational cost while maintaining performance.
- Core assumption: The changes in LayerNorm parameters are sufficient to capture the necessary task-specific adaptations.
- Evidence anchors:
  - [abstract] The paper shows that only fine-tuning the LayerNorm can reach comparable, or in some cases better, performance to full fine-tuning and other parameter-efficient fine-tuning methods.
  - [section 3.1] Results show that fine-tuning only LayerNorm has almost the same performance as BitFit, yet with one-fifth of the parameters.
  - [corpus] The corpus includes "Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection," which discusses trade-offs in fine-tuning strategies.
- Break condition: If the task requires significant changes in other components (e.g., attention mechanisms), fine-tuning only LayerNorm might not be sufficient.

## Foundational Learning

- Concept: Fisher Information
  - Why needed here: Fisher information is used to identify the most important parameters for fine-tuning, allowing for efficient parameter selection.
  - Quick check question: What does a high Fisher information value indicate about a parameter's importance in a neural network?
- Concept: Layer Normalization
  - Why needed here: LayerNorm is the key component identified for fine-tuning; understanding its function is crucial for implementing the method.
  - Quick check question: How does LayerNorm differ from Batch Normalization in terms of normalization scope?
- Concept: Parameter-Efficient Fine-Tuning
  - Why needed here: The paper's contribution is in demonstrating a more efficient fine-tuning method by focusing on a subset of parameters.
  - Quick check question: What is the main advantage of parameter-efficient fine-tuning over full fine-tuning?

## Architecture Onboarding

- Component map: BERT model with attention layers, feed-forward networks, and LayerNorm layers. Focus on identifying the output LayerNorm and attention LayerNorm components.
- Critical path: Compute Fisher information for all parameters → Identify LayerNorm parameters with highest Fisher information → Fine-tune only those parameters → Evaluate performance on GLUE tasks.
- Design tradeoffs: Using fewer parameters reduces computational cost but may limit the model's ability to adapt to complex tasks. The choice of which parameters to fine-tune involves balancing efficiency and performance.
- Failure signatures: If performance degrades significantly after fine-tuning only LayerNorm, it may indicate that other components are also important for the task. If Fisher information rankings change, the parameter selection method may need adjustment.
- First 3 experiments:
  1. Fine-tune the full BERT model on a GLUE task and record performance.
  2. Fine-tune only the LayerNorm parameters and compare performance to full fine-tuning.
  3. Use Fisher information to select a subset of LayerNorm parameters and fine-tune only those, evaluating performance degradation.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense, but the following questions emerge from the analysis and are supported by the evidence presented:

1. What is the exact mechanism by which LayerNorm maintains criticality in transformer architectures, and how does this differ from other normalization techniques like batch normalization?
   - Basis in paper: [explicit] The paper mentions that LayerNorm maintains criticality, unlike batch normalization which can cause gradient explosion. It references studies showing proper stacking of LayerNorm leads to critical architectures.
   - Why unresolved: The paper does not provide a detailed explanation of the mathematical or theoretical basis for LayerNorm's ability to maintain criticality.
   - What evidence would resolve it: A detailed mathematical analysis comparing the gradient flow and stability properties of LayerNorm versus batch normalization in transformer architectures, including empirical evidence of criticality maintenance.

2. How does the performance of LayerNorm-based fine-tuning compare to other parameter-efficient methods like LoRA or adapter-based approaches across a wider range of NLP tasks beyond GLUE?
   - Basis in paper: [explicit] The paper focuses on LayerNorm fine-tuning and compares it to BitFit, but does not extensively compare to other methods like LoRA or adapters.
   - Why unresolved: The paper's scope is limited to GLUE tasks and direct comparison with BitFit, leaving the broader performance landscape unexplored.
   - What evidence would resolve it: Comprehensive experiments comparing LayerNorm fine-tuning to LoRA, adapters, and other parameter-efficient methods across diverse NLP benchmarks, including domain-specific tasks and larger language models.

3. What are the theoretical implications of the observation that LayerNorm in later layers of BERT has higher Fisher information than earlier layers, and how does this relate to the overall learning dynamics of transformers?
   - Basis in paper: [explicit] The paper observes that LayerNorm in final layers has more Fisher information than initial layers, consistent with other studies showing larger gradients in later layers during fine-tuning.
   - Why unresolved: The paper does not explore the theoretical significance of this observation or its implications for understanding transformer learning dynamics.
   - What evidence would resolve it: Theoretical analysis connecting Fisher information distribution in LayerNorm to gradient flow, representation learning, and task-specific information processing in transformers, potentially supported by ablation studies and visualization of information flow.

## Limitations

- The findings are limited to BERT-large-cased on the GLUE benchmark and may not generalize to other transformer architectures or tasks.
- The analysis focuses on magnitude of change during fine-tuning as a proxy for importance, but this correlation may not hold for all tasks or model architectures.
- The selection of Fisher information as the metric for parameter importance may not capture all aspects of parameter relevance, particularly for tasks requiring significant changes in attention mechanisms or other components beyond LayerNorm.

## Confidence

**High Confidence:** The experimental results showing LayerNorm undergoes the largest changes during fine-tuning across GLUE tasks are well-supported by the heat maps and quantitative comparisons presented. The conclusion that fine-tuning only LayerNorm can achieve comparable performance to full fine-tuning with 0.015% of parameters is demonstrated convincingly with statistical significance.

**Medium Confidence:** The Fisher information analysis as a method for parameter selection shows promise, but the claim that it consistently identifies the most important parameters across different tasks has some variability. The paper shows that Fisher-selected subsets sometimes outperform training the entire LayerNorm, but this result is task-dependent and not universal across all GLUE benchmarks.

**Low Confidence:** The mechanism explaining why LayerNorm changes are most significant during fine-tuning is primarily correlational rather than causal. The paper does not provide ablation studies that would definitively prove LayerNorm is the causal driver of performance improvements versus other correlated factors.

## Next Checks

1. **Cross-Architecture Validation:** Test whether LayerNorm parameter importance holds for other transformer architectures (RoBERTa, DeBERTa, GPT-2) and whether the same fine-tuning strategy achieves similar parameter efficiency gains.

2. **Fisher Information Robustness:** Vary the computation of Fisher information (different batch sizes, different computation points in training) to assess stability of parameter rankings and determine if the same parameters are consistently identified as important across different calculation methods.

3. **Task Complexity Analysis:** Systematically evaluate the performance of LayerNorm-only fine-tuning across tasks of varying complexity and domain specificity to identify the threshold where additional parameter tuning becomes necessary for maintaining performance.