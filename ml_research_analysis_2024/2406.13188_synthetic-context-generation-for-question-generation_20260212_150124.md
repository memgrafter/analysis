---
ver: rpa2
title: Synthetic Context Generation for Question Generation
arxiv_id: '2406.13188'
source_url: https://arxiv.org/abs/2406.13188
tags:
- context
- synthetic
- question
- generation
- contexts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates training question generation (QG) models
  using synthetic contexts generated by large language models (LLMs) from question-answer
  pairs. The motivation is that obtaining domain-specific datasets with appropriate
  context is often more difficult than acquiring question-answer pairs.
---

# Synthetic Context Generation for Question Generation

## Quick Facts
- arXiv ID: 2406.13188
- Source URL: https://arxiv.org/abs/2406.13188
- Authors: Naiming Liu; Zichao Wang; Richard Baraniuk
- Reference count: 40
- Key outcome: Synthetic contexts generated by LLMs from question-answer pairs can effectively train question generation models, achieving comparable performance to real contexts.

## Executive Summary
This paper addresses the challenge of training question generation (QG) models when domain-specific context data is scarce. The authors propose using large language models (LLMs) to generate synthetic contexts from question-answer pairs, then fine-tuning smaller language models on these synthetic context-question-answer triplets. Experiments on OpenStax Biology and SQuAD datasets demonstrate that synthetic contexts are essential for QG tasks, fine-tuning smaller models outperforms prompting larger LLMs, and synthetic contexts can achieve comparable performance to real contexts. This approach enables effective QG model training even when real context data is unavailable.

## Method Summary
The method involves prompting an LLM (GPT-3.5) to generate synthetic contexts from question-answer pairs using zero-shot and few-shot prompting strategies. These synthetic context, question, and answer triplets are then used to fine-tune a smaller language model (Flan-T5-large) for the QG task. The fine-tuned model is evaluated on real contexts using standard QG metrics (BLEU-4, METEOR, ROUGE-L, BLEURT). The approach is tested on two datasets: OpenStax Biology textbook and SQuAD, with training conducted for 10 epochs using early stopping.

## Key Results
- Contexts are essential for QG tasks, even when synthetic contexts are used
- Fine-tuning smaller language models achieves better QG performance compared to prompting larger LLMs
- Synthetic contexts and real contexts achieve comparable QG performances
- Models trained on just 1000 synthetic contexts can yield performance comparable to models trained on all real contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contexts are essential for question generation, even if synthetic
- Mechanism: The question generation model uses the context as an anchor to ground the answer into a meaningful question. Without context, the model lacks the necessary information to generate coherent and contextually appropriate questions.
- Core assumption: The synthetic context generated by LLMs contains sufficient relevant information to support question generation.
- Evidence anchors:
  - [abstract] "contexts are essential for QG tasks, even if they are synthetic"
  - [section] "As demonstrated in Table 1, the performance of QG without context is significantly worse than QG with synthetic context"
  - [corpus] Weak - corpus evidence doesn't directly address this specific mechanism
- Break condition: If synthetic context fails to capture the essential information needed to generate questions from the answer, or if the context becomes too noisy or irrelevant.

### Mechanism 2
- Claim: Fine-tuning smaller language models outperforms prompting larger LLMs for QG
- Mechanism: Fine-tuning adapts the smaller model's parameters specifically to the QG task using the synthetic context-answer pairs, while prompting relies on in-context learning which is less effective for this task.
- Core assumption: The smaller model has sufficient capacity to learn the QG task when fine-tuned on domain-specific synthetic data.
- Evidence anchors:
  - [abstract] "fine-tuning smaller language models has the capability of achieving better performances as compared to prompting larger language models"
  - [section] "when synthetic context is available, fine-tuning smaller language models could achieve better performance compared to prompting larger language models"
  - [corpus] Weak - corpus evidence doesn't directly compare fine-tuning vs prompting
- Break condition: If the synthetic data quality degrades significantly, or if the smaller model lacks sufficient capacity to learn the QG task.

### Mechanism 3
- Claim: Synthetic contexts can achieve comparable performance to real contexts
- Mechanism: LLMs can generate synthetic contexts that capture the essential information needed for question generation, making them effective substitutes for real contexts.
- Core assumption: LLMs can generate diverse and relevant contexts that approximate the distribution of real contexts.
- Evidence anchors:
  - [abstract] "synthetic context and real context could achieve comparable performances"
  - [section] "models trained on merely 1000 synthetic contexts can yield strikingly comparable QG performances compared to models trained on all real context"
  - [corpus] Weak - corpus evidence doesn't directly compare synthetic vs real contexts
- Break condition: If the synthetic contexts fail to capture the diversity or relevance of real contexts, or if the synthetic generation introduces systematic biases.

## Foundational Learning

- Concept: Prompt engineering for context generation
  - Why needed here: Effective prompts are crucial for LLMs to generate high-quality synthetic contexts that support question generation
  - Quick check question: How would you design a prompt to generate a wikipedia-style paragraph that helps answer a specific question?

- Concept: Fine-tuning vs in-context learning
  - Why needed here: Understanding the tradeoffs between fine-tuning smaller models and prompting larger LLMs is essential for choosing the right approach
  - Quick check question: What are the key differences between fine-tuning and in-context learning in terms of model adaptation and performance?

- Concept: Evaluation metrics for QG
  - Why needed here: Proper evaluation of QG performance requires understanding metrics like BLEU, METEOR, ROUGE-L, and BLEURT
  - Quick check question: How do BLEU-4 and METEOR differ in evaluating the quality of generated questions?

## Architecture Onboarding

- Component map: LLM (GPT-3.5) -> Synthetic context generation -> Fine-tuning framework (Flan-T5-large) -> QG model
- Critical path: 1. Generate synthetic contexts from Q-A pairs using LLM 2. Create training dataset of C-Q-A triplets 3. Fine-tune smaller language model on synthetic data 4. Evaluate QG performance on real contexts
- Design tradeoffs:
  - LLM choice vs cost: More capable LLMs generate better contexts but are more expensive
  - Model size vs performance: Larger fine-tuning models perform better but require more resources
  - Few-shot vs zero-shot prompting: Few-shot generally produces better contexts but requires example selection
- Failure signatures:
  - Degraded QG performance despite high-quality synthetic contexts
  - Synthetic contexts that don't contain answer information
  - Fine-tuned model overfitting to synthetic data patterns
- First 3 experiments:
  1. Compare QG performance with and without context (baseline)
  2. Test different LLM prompts for synthetic context generation
  3. Evaluate synthetic vs real contexts with varying amounts of training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of synthetic to real context for training QG models?
- Basis in paper: Inferred from RQ6, where the paper explores mixing real and synthetic context and finds that including a small portion of real context can improve QG performance.
- Why unresolved: The paper only tests a few ratios (0%, 10%, 50%, 100% synthetic) and doesn't determine the optimal balance between synthetic and real context.
- What evidence would resolve it: Conducting experiments with various ratios of synthetic to real context (e.g., 20%, 30%, 40%, etc.) and comparing the QG performance would help determine the optimal ratio.

### Open Question 2
- Question: How does the quality of synthetic context generated by different LLMs compare for QG tasks?
- Basis in paper: Inferred from RQ4, where the paper assesses the quality of synthetic context generated by GPT-3.5 but doesn't compare it with other LLMs.
- Why unresolved: The paper only uses GPT-3.5 for generating synthetic context and doesn't explore the performance of other LLMs or their generated context.
- What evidence would resolve it: Generating synthetic context using various LLMs (e.g., GPT-4, Claude, etc.) and comparing the QG performance when using the generated context as training data would provide insights into the quality differences between LLMs for QG tasks.

### Open Question 3
- Question: Can fine-tuning larger language models on synthetic context improve their QG performance compared to prompting?
- Basis in paper: Inferred from RQ2, where the paper compares fine-tuning smaller models to prompting larger models but doesn't explore fine-tuning larger models.
- Why unresolved: The paper focuses on fine-tuning smaller models and prompting larger models, but doesn't investigate the potential benefits of fine-tuning larger models on synthetic context.
- What evidence would resolve it: Fine-tuning larger language models (e.g., GPT-3.5, GPT-4) on synthetic context and comparing their QG performance to the prompted versions would help determine if fine-tuning larger models can lead to better results.

## Limitations
- The quality of synthetic context generation fundamentally depends on the prompting strategy and base LLM's capabilities
- The approach's sensitivity to prompt engineering variations is not thoroughly characterized
- The evaluation focuses primarily on automatic metrics without extensive human evaluation

## Confidence
- Contexts are essential for QG (even synthetic): High
- Fine-tuning smaller models outperforms prompting: Medium
- Synthetic contexts can match real contexts: Medium

## Next Checks
1. Conduct ablation studies varying the number of few-shot examples in the context generation prompts to quantify their impact on synthetic context quality and downstream QG performance.

2. Perform cross-domain evaluation by training on synthetic contexts from one domain (e.g., SQuAD) and testing on real contexts from a different domain (e.g., OpenStax Biology) to assess generalization capabilities.

3. Implement human evaluation studies comparing synthetic contexts against real contexts for relevance, coherence, and information density to validate the automatic metric findings.