---
ver: rpa2
title: Gender Bias in Large Language Models across Multiple Languages
arxiv_id: '2403.00277'
source_url: https://arxiv.org/abs/2403.00277
tags:
- bias
- gender
- languages
- language
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study examines gender bias in large language models (LLMs)
  across six languages: English, French, Spanish, Chinese, Japanese, and Korean. The
  authors propose three quantitative measurements to evaluate gender bias: (1) bias
  in descriptive word selection, (2) bias in gendered role selection, and (3) bias
  in dialogue topics.'
---

# Gender Bias in Large Language Models across Multiple Languages

## Quick Facts
- arXiv ID: 2403.00277
- Source URL: https://arxiv.org/abs/2403.00277
- Reference count: 26
- Key outcome: This study examines gender bias in large language models (LLMs) across six languages: English, French, Spanish, Chinese, Japanese, and Korean. The authors propose three quantitative measurements to evaluate gender bias: (1) bias in descriptive word selection, (2) bias in gendered role selection, and (3) bias in dialogue topics. Using GPT-3, ChatGPT, and GPT-4, the study finds significant gender biases across all languages in all three dimensions. For descriptive word selection, standout and personal quality words are more likely to be assigned to males, while communal words are more likely assigned to females. For gendered role selection, males are more likely predicted for standout and personal quality descriptions, while females are more likely for outlook descriptions. For dialogue topics, notable patterns include appearance being primarily discussed in female-to-female conversations, career topics more frequently mentioned in male-to-male dialogues, and complaints more common in female-to-male conversations. The study reveals that gender bias varies across languages and highlights the importance of evaluating LLMs for fairness across multiple languages and cultural contexts.

## Executive Summary
This study investigates gender bias in large language models (LLMs) across six languages: English, French, Spanish, Chinese, Japanese, and Korean. The authors propose three quantitative measurements to evaluate gender bias: bias in descriptive word selection, bias in gendered role selection, and bias in dialogue topics. Using GPT-3, ChatGPT, and GPT-4, the study finds significant gender biases across all languages in all three dimensions. The results reveal that gender bias varies across languages and highlights the importance of evaluating LLMs for fairness across multiple languages and cultural contexts.

## Method Summary
The study uses GPT-3, ChatGPT, and GPT-4 to evaluate gender bias across six languages. Three measurements are employed: (1) gender bias in descriptive word selection using Disparate Impact (DI) scores, (2) gender bias in gendered role selection using Disparate Impact for Gendered Role (DIG) scores, and (3) gender bias in dialogue topics through topic categorization and proportion analysis. The experiments involve 360 prompts per language for descriptive word selection, 1080 prompts for gendered role selection, and 400 dialogues for dialogue topic analysis. The study uses temperature=1 for all LLM interactions and employs a two-stage labeling process with ChatGPT and GPT-4 for dialogue topic categorization.

## Key Results
- Significant gender biases were found across all languages in all three measurement dimensions.
- For descriptive word selection, standout and personal quality words are more likely to be assigned to males, while communal words are more likely assigned to females.
- For dialogue topics, notable patterns include appearance being primarily discussed in female-to-female conversations, career topics more frequently mentioned in male-to-male dialogues, and complaints more common in female-to-male conversations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit differential conditional probabilities of descriptive word selection based on gender-related prompts across multiple languages.
- Mechanism: The study feeds gender-specific prompts into LLMs and measures the Disparate Impact (DI) score, which quantifies the ratio of word usage frequency for one gender relative to the other. If DI significantly deviates from 1.0, it indicates gender bias in descriptive word assignment.
- Core assumption: The LLMs treat gender-related prompts as conditioning factors and generate outputs reflecting embedded biases in their training data.
- Evidence anchors:
  - [abstract] "We use three measurements: 1) gender bias in selecting descriptive words given the gender-related context."
  - [section] "To evaluate the difference in word prediction probabilities between the male-related and female-related prompts, we use a disparaty impact (DI) score."
  - [corpus] Weak - corpus neighbors discuss gender bias in LLMs but do not specifically anchor the DI score methodology.
- Break condition: If the LLM ignores gender cues in the prompt or if the training data lacks gender-associated word patterns, the DI score will not show bias.

### Mechanism 2
- Claim: LLMs show gender bias in pronoun selection based on descriptive words, revealing asymmetry in gender-role association.
- Mechanism: Prompts are designed with descriptive adjectives and ask the model to fill in the blank with "he" or "she." The Disparate Impact for Gendered Role (DIG) score measures the likelihood of predicting a specific pronoun given the descriptive context.
- Core assumption: The LLMs use descriptive words as contextual cues to infer gender roles, and the training data encodes stereotypical associations between traits and gender.
- Evidence anchors:
  - [abstract] "2) gender bias in selecting gender-related pronouns (she/he) given the descriptive words."
  - [section] "the gendered role selection task aims to evaluate the conditional probabilities of gendered roles given descriptive words P(G|A)."
  - [corpus] Weak - corpus neighbors focus on bias measurement but do not specifically discuss pronoun selection tasks.
- Break condition: If the LLM treats pronouns as neutral or if descriptive words do not strongly correlate with gender in the training corpus, the DIG score will be close to 1.0.

### Mechanism 3
- Claim: LLMs generate dialogues with gender-biased topic distributions, reflecting stereotypical conversational patterns.
- Mechanism: The study categorizes dialogues into gender pairing groups (FF, FM, MF, MM) and topic categories (appearance, career, complaints, etc.). Differences in topic proportions across groups indicate bias.
- Core assumption: The LLMs generate dialogues that mirror societal stereotypes present in their training data, leading to gendered differences in conversational content.
- Evidence anchors:
  - [abstract] "3) gender bias in the topics of LLM-generated dialogues."
  - [section] "we categorize the LLM-generated dialogues in two dimensions... the divergence across proportions of different gender pairs, {pFF i , pFM i , pMF i , pMM i }, for each topic category i ∈ [N]."
  - [corpus] Weak - corpus neighbors do not anchor dialogue-based bias measurement.
- Break condition: If the LLM generates neutral or non-stereotypical dialogues regardless of gender pairing, the topic proportions will be similar across groups.

## Foundational Learning

- Concept: Disparate Impact (DI) Score
  - Why needed here: To quantitatively measure gender bias in word and pronoun selection by comparing usage frequencies between genders.
  - Quick check question: If a word is used equally for males and females, what should the DI score be?
  - Answer: 1.0 (no bias).

- Concept: Gender Pairing Groups
  - Why needed here: To systematically analyze how dialogue content varies based on the gender of the speakers.
  - Quick check question: What are the four gender pairing groups used in the study?
  - Answer: FF (female-to-female), FM (female-to-male), MF (male-to-female), MM (male-to-male).

- Concept: Topic Categorization in Dialogues
  - Why needed here: To identify specific areas where gender bias manifests in conversational content.
  - Quick check question: Name two topic categories that showed gender bias in the study.
  - Answer: Appearance (discussed more in FF), Career (mentioned more in MM).

## Architecture Onboarding

- Component map: Prompt generator -> LLM interface -> Data processor -> Bias calculator -> Visualizer
- Critical path: Prompt → LLM → Response → Extraction → Bias Calculation → Analysis
- Design tradeoffs:
  - Language selection: Balancing typological diversity with data availability.
  - Prompt design: Ensuring prompts are neutral yet elicit gender-related responses.
  - Topic labeling: Using LLM-based labeling vs. manual annotation for scalability.
- Failure signatures:
  - DI/DIG scores close to 1.0 across all languages: May indicate lack of gender bias or ineffective prompts.
  - High variance in topic proportions within gender pairs: Could suggest noise in dialogue generation or labeling.
  - Inability to generate multilingual dialogues: Limits language coverage.
- First 3 experiments:
  1. Test DI score calculation with a small set of English prompts to verify methodology.
  2. Validate DIG score by checking pronoun predictions for a few descriptive words.
  3. Generate and label a sample of dialogues to ensure topic categorization works as expected.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the language-specific training data sources contribute to the observed regional variations in gender bias, particularly for topics like "Family/Housework" which are absent in Japanese and Korean dialogues but present in European languages?
- Basis in paper: [explicit] The paper notes that "The differences in biases related to housework between European and East Asian languages may also reflect regional variations in domestic roles" and acknowledges that "it could also be attributed to variations in the sources of training data for different languages."
- Why unresolved: The study used closed-source models and could not analyze the training data composition, making it impossible to determine the exact contribution of data sources to the observed biases.
- What evidence would resolve it: Comparative analysis of the training data composition for different language models, including topic distribution and gender representation in domestic role contexts.

### Open Question 2
- Question: Would extending the gender bias analysis beyond binary male/female categories to include non-binary and other gender identities reveal additional patterns of bias or discrimination in LLMs across different languages?
- Basis in paper: [explicit] The authors acknowledge this limitation: "Our study is limited to the binary categories of male and female due to the constraints of current language model capabilities and the scope of our project. We recognize that gender is a diverse spectrum and our categorization does not reflect the full range of gender identities."
- Why unresolved: The current study's methodology and evaluation framework are specifically designed for binary gender categories, and extending them would require significant methodological changes.
- What evidence would resolve it: Development and implementation of inclusive evaluation frameworks that can assess gender bias across a spectrum of gender identities, followed by systematic testing across multiple languages.

### Open Question 3
- Question: How do different instruction types and prompting strategies affect the manifestation of gender bias in LLM outputs across languages, beyond the specific prompts used in this study?
- Basis in paper: [inferred] The authors state: "we emphasize the substantial role of conversations undertaken by LLMs and explore gender bias in different dimensions" and note that "diverse instructions may influence gender biases in LLM generations in different ways."
- Why unresolved: The study used a limited set of predefined prompts and didn't systematically explore how different instruction types might affect bias manifestation.
- What evidence would resolve it: Systematic experimentation with varied instruction types, prompt formulations, and task contexts across multiple languages to map how different prompting strategies influence gender bias patterns.

## Limitations
- The study relies heavily on LLM-based labeling for dialogue topics, which introduces potential circularity since the same models being evaluated are used for annotation.
- The prompt design for dialogue generation may not fully capture natural conversational patterns, and the fixed set of 7 topics may miss nuanced gender-related patterns.
- The translation process for word lists across languages could introduce bias if not properly validated.

## Confidence
- Confidence is High for the descriptive word selection and gendered role selection tasks, as these use direct probability measurements with clear metrics.
- Confidence is Medium for dialogue topic analysis due to the indirect measurement approach and potential labeling biases.
- The cross-language comparisons show interesting patterns but require careful interpretation given linguistic and cultural differences.

## Next Checks
1. Replicate the DI and DIG score calculations using a held-out test set of prompts to verify the stability of the bias measurements.
2. Conduct human validation of the dialogue topic labels to assess the accuracy of the LLM-based labeling approach.
3. Test the sensitivity of the results to different prompt formulations and generation parameters (temperature, max tokens) to ensure robustness.