---
ver: rpa2
title: Localizing Memorization in SSL Vision Encoders
arxiv_id: '2409.19069'
source_url: https://arxiv.org/abs/2409.19069
tags:
- memorization
- data
- layers
- layermem
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two novel metrics, LayerMem and UnitMem,
  to localize memorization in SSL vision encoders. LayerMem quantifies memorization
  on a per-layer basis by comparing the alignment of representations between two encoders,
  while UnitMem measures memorization at the individual unit level by assessing sensitivity
  to specific training data points.
---

# Localizing Memorization in SSL Vision Encoders

## Quick Facts
- **arXiv ID**: 2409.19069
- **Source URL**: https://arxiv.org/abs/2409.19069
- **Reference count**: 40
- **Key outcome**: Introduces LayerMem and UnitMem metrics to localize memorization in SSL vision encoders, showing memorization increases with layer depth but highly memorizing units are distributed throughout encoders

## Executive Summary
This paper addresses the critical challenge of localizing memorization in self-supervised learning (SSL) vision encoders, which has significant implications for privacy, generalization, and model interpretability. The authors introduce two novel metrics - LayerMem for quantifying memorization at the layer level and UnitMem for measuring it at the individual unit level. Through extensive experiments across multiple SSL frameworks and architectures, they demonstrate that memorization patterns in SSL models differ substantially from those in supervised learning, with memorization increasing with layer depth while highly memorizing units are distributed throughout the entire encoder rather than concentrated in specific regions.

## Method Summary
The authors propose two complementary metrics for quantifying memorization in SSL vision encoders. LayerMem measures memorization on a per-layer basis by comparing the alignment of representations between two encoders trained on different subsets of data. The metric computes the cosine similarity between the alignment matrices of representations from different layers, with lower similarity indicating higher memorization. UnitMem quantifies memorization at the individual unit level by measuring each unit's sensitivity to specific training data points, using a perturbation-based approach where the impact of adding or removing data points on unit activations is measured. Both metrics are evaluated through controlled experiments where encoders are trained on overlapping and non-overlapping subsets of data to establish baselines for memorization assessment.

## Key Results
- Memorization increases monotonically with layer depth across all tested SSL frameworks (SimCLR, BYOL, MoCo)
- Highly memorizing units are distributed throughout the entire encoder rather than concentrated in specific layers, contrasting with supervised learning patterns
- Atypical or rare data points cause significantly higher memorization compared to typical samples
- In vision transformers, most memorization occurs in fully connected layers rather than attention mechanisms

## Why This Works (Mechanism)
Memorization in SSL vision encoders occurs through the optimization process where the model learns to distinguish between similar and dissimilar pairs during training. As representations become more refined through deeper layers, they capture increasingly specific features that may be tied to particular training examples, leading to increased memorization. The distributed nature of memorizing units suggests that SSL frameworks learn representations through a more holistic process compared to supervised learning, where memorization tends to be more localized. The sensitivity of atypical data points to memorization likely stems from their rarity, causing the model to overfit to these distinctive features during training.

## Foundational Learning

**Self-Supervised Learning (SSL)**: Learning representations without explicit labels by creating pretext tasks. *Why needed*: Forms the foundation for understanding how models can learn useful features without supervision. *Quick check*: Verify understanding of contrastive learning, masked autoencoding, and other SSL paradigms.

**Representation Alignment**: Measuring similarity between feature representations from different models or layers. *Why needed*: Critical for quantifying how similarly different encoders represent the same data. *Quick check*: Understand cosine similarity and other alignment metrics.

**Model Memorization**: The tendency of models to encode specific training examples rather than learning general patterns. *Why needed*: Central concept being measured and localized. *Quick check*: Distinguish between memorization and generalization in model behavior.

## Architecture Onboarding

**Component Map**: Input data -> SSL encoder (multiple layers) -> representation space -> downstream tasks. The encoder consists of initial layers (CNN/transformer blocks), middle layers, and final layers leading to the projection head.

**Critical Path**: The forward pass through the encoder layers, where representations are progressively refined. Memorization is measured by comparing representations across different training runs or encoders.

**Design Tradeoffs**: LayerMem trades computational efficiency for layer-level insights, while UnitMem provides fine-grained unit-level analysis at higher computational cost. The choice between them depends on whether layer-level or unit-level memorization is the primary concern.

**Failure Signatures**: High LayerMem scores indicate low memorization but may miss localized memorization; UnitMem scores require careful interpretation as some units naturally have higher sensitivity to data changes. Both metrics assume the compared encoders are sufficiently similar for meaningful comparison.

**First Experiments**:
1. Run LayerMem on a simple SSL framework (e.g., SimCLR) with two differently initialized encoders to establish baseline memorization scores
2. Apply UnitMem to identify top-10 memorizing units in a pretrained vision transformer
3. Compare memorization patterns between vision transformers and convolutional architectures using both metrics

## Open Questions the Paper Calls Out
The paper identifies several open questions including how memorization patterns change with different SSL objectives, whether the proposed metrics generalize to non-vision modalities like language and speech, and how to develop regularization techniques specifically targeted at reducing memorization in SSL frameworks. The authors also question whether the distributed memorization pattern observed in SSL is beneficial or detrimental to downstream task performance.

## Limitations

- LayerMem effectiveness depends on encoder similarity, but systematic analysis of training condition effects is lacking
- UnitMem requires held-out data evaluation, limiting applicability in some SSL scenarios
- Experiments focus primarily on vision transformers and standard SSL frameworks, limiting generalizability

## Confidence

**High Confidence**: Memorization increases with layer depth; atypical data points cause higher memorization
**Medium Confidence**: Distributed memorizing units differ from supervised learning patterns
**Low Confidence**: Most memorization in vision transformers occurs in fully connected layers

## Next Checks

1. Conduct ablation studies varying training duration and dataset size to quantify effects on LayerMem and UnitMem scores across different SSL frameworks
2. Perform systematic comparisons of memorization patterns between SSL and supervised learning models using identical architectures
3. Extend experiments to non-vision modalities (speech, text) and alternative architectures (convolutional networks) to assess metric generalizability