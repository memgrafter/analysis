---
ver: rpa2
title: Resolving Lexical Bias in Model Editing
arxiv_id: '2408.10411'
source_url: https://arxiv.org/abs/2408.10411
tags:
- edit
- editing
- training
- edits
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of lexical bias in model editing,
  where adapter-based methods struggle to differentiate between paraphrases and irrelevant
  prompts with similar lexical content. The authors propose PENME, a principled approach
  that employs a projector network trained via contrastive learning to learn a disentangled
  representation space, enabling precise localization of edits while maintaining distance
  between irrelevant prompts and preserving proximity among paraphrases.
---

# Resolving Lexical Bias in Model Editing

## Quick Facts
- arXiv ID: 2408.10411
- Source URL: https://arxiv.org/abs/2408.10411
- Reference count: 40
- Primary result: PENME achieves state-of-the-art model editing results, demonstrating superior performance in edit success, locality, and generalization across various transformer-based language models.

## Executive Summary
This paper addresses the challenge of lexical bias in model editing, where adapter-based methods struggle to differentiate between paraphrases and irrelevant prompts with similar lexical content. The authors propose PENME, a principled approach that employs a projector network trained via contrastive learning to learn a disentangled representation space, enabling precise localization of edits while maintaining distance between irrelevant prompts and preserving proximity among paraphrases. PENME achieves state-of-the-art model editing results, demonstrating superior performance in edit success, locality, and generalization across various transformer-based language models. The method is computationally efficient during inference and adaptable to different architectures.

## Method Summary
PENME addresses lexical bias in model editing by training a projector network via contrastive learning to create a disentangled representation space where lexical overlap is no longer the dominant factor in similarity. The projector network is integrated into an adapter and memory-based retrieval scheme for model editing. During inference, input prompts are projected into this space, and the nearest edit is retrieved from memory using Euclidean distance. A dynamically set similarity threshold per edit, learned from training data, enables precise scoping and prevents both misfires and missed paraphrases. The method is evaluated across multiple transformer-based language models and datasets, showing superior performance in edit success, locality, and generalization.

## Key Results
- PENME outperforms existing model editing approaches in edit success, locality, and generalization metrics.
- The projector network trained via contrastive learning effectively reduces lexical bias, enabling better differentiation between paraphrases and irrelevant prompts.
- PENME demonstrates adaptability across different transformer-based language models, including T5, Llama-2, and GPT2-XL.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The projector network trained via contrastive learning creates a disentangled representation space where lexical overlap is no longer the dominant factor in similarity.
- Mechanism: By training with pairs of edits and paraphrases as positives and edits and lexically similar neighbors as negatives, the projector learns to project these groups into distinct regions of the representation space. This reduces the influence of shared tokens and increases the influence of semantic content.
- Core assumption: Lexical similarity is a strong confounding factor in the original representation space, causing misfires and generalization failures.
- Evidence anchors:
  - [abstract]: "We demonstrate that current adapter methods are critically vulnerable to strong lexical biases, leading to issues such as applying edits to irrelevant prompts with overlapping words."
  - [section 6.1]: "Our analysis indicates that lexical factors predominantly shape model representations... 58% of edits from the Counterfact were closer to unrelated neighbours than the edit paraphrases."
  - [corpus]: Weak - no direct citation on contrastive learning for lexical bias mitigation in model editing.

### Mechanism 2
- Claim: The similarity threshold per edit, dynamically set from training data, enables precise scoping and prevents both misfires and missed paraphrases.
- Mechanism: By storing a learned threshold with each edit in the key-value memory, the retrieval system can adapt the decision boundary to the specific distribution of distances observed for that edit. This accounts for variations in how separable an edit is from its paraphrases versus its neighbors.
- Core assumption: The distance distribution between an edit and its paraphrases is consistently smaller than the distance to its neighbors in the projected space.
- Evidence anchors:
  - [section 4.2]: "Initial experimental findings regarding the threshold reveal that unseen test paraphrases typically demonstrate greater distance than the most distant previously seen training paraphrases, while the inter-paraphrase distances within the training set exhibit variation across edits."
  - [section 6.3]: "PENME shows high efficacy on locality and generalization compared to other model editing approaches."
  - [corpus]: Weak - no direct citation on dynamic per-edit thresholds in model editing.

### Mechanism 3
- Claim: Integration of the projector network at a mid-layer (layer 2) balances representation robustness and lexical dominance mitigation.
- Mechanism: Early layers are too token-specific and dominated by lexical features; later layers are more semantically abstract but may have converged too much to be easily separable. Layer 2 provides a sweet spot where lexical dominance is reduced but semantic differences are still pronounced.
- Core assumption: There exists a layer in the transformer where lexical dominance is minimized but semantic distinctions are still preserved.
- Evidence anchors:
  - [section 6.1]: "Except for the first layer in most models, the early layers demonstrate a reduced percentage of samples where neighbours are closer to edits than paraphrases. However, the trend shifts as we progress through the model’s depth."
  - [section 4.2]: "We opt for Option 1, as it guarantees a full edit success rate."
  - [corpus]: Weak - no direct citation on optimal layer selection for model editing adapters.

## Foundational Learning

- Concept: Contrastive learning and the InfoNCE loss function
  - Why needed here: To train the projector network to create separable clusters for edits, paraphrases, and neighbors without relying on lexical similarity.
  - Quick check question: What is the role of the margin parameter in the contrastive loss, and how does it affect the separability of clusters?

- Concept: Euclidean distance as a similarity metric in representation space
  - Why needed here: Used to compare projected representations during retrieval and to compute thresholds for scoping.
  - Quick check question: How does the choice of distance metric (Euclidean vs. cosine) affect the performance of the retrieval system in high-dimensional spaces?

- Concept: Adapter modules in transformer architectures
  - Why needed here: PENME is implemented as an adapter integrated alongside pointwise feed-forward layers, allowing modification of outputs without altering original model weights.
  - Quick check question: What are the trade-offs between inserting an adapter at different positions within a transformer block?

## Architecture Onboarding

- Component map:
  Pre-trained LLM (T5, Llama-2, GPT2-XL) -> Projector network (2-layer feedforward, trained via contrastive learning) -> Key-value memory (stores projected representations as keys, edit info + threshold as values) -> Retrieval system (computes distances, applies threshold, retrieves output or passes through)

- Critical path:
  1. Input prompt → LLM forward pass → extract layer 2 activations
  2. Projector network transforms activations
  3. Retrieve nearest edit from memory using Euclidean distance
  4. Compare distance to stored threshold
  5. If below threshold, return stored output; else return original LLM output

- Design tradeoffs:
  - Layer selection: Earlier layers preserve more lexical detail but are more biased; later layers are more semantic but harder to separate.
  - Threshold strategy: Option 1 (max paraphrase + alpha) ensures all training paraphrases are covered but risks higher misfire rates; Option 2 (min neighbor - alpha) ensures locality but risks missing paraphrases.
  - Projector size: Larger projectors may learn better separations but increase memory and compute cost.

- Failure signatures:
  - High generalization failure, low locality: Threshold too high, projector not separating well.
  - Low generalization, high locality: Threshold too low, projector over-separating.
  - Poor performance across all metrics: Projector not trained properly, or layer selection suboptimal.

- First 3 experiments:
  1. Train projector network on layer 2 representations and visualize t-SNE of edit, paraphrase, and neighbor clusters to verify separability.
  2. Vary alpha threshold from 0.05 to 0.20 and plot tradeoff curve between generalization and locality.
  3. Swap in different pre-trained models (e.g., T5 → Llama-2) and measure impact on performance to test architecture flexibility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the projector network's performance scale with increasingly larger models (e.g., Llama-2-70B or beyond)?
- Basis in paper: [inferred] The paper discusses projector network performance on T5-small, GPT-2XL, and Llama-2-7B, but does not explore scaling to much larger models.
- Why unresolved: The authors only evaluated three models of varying sizes, leaving the behavior on much larger models unexplored.
- What evidence would resolve it: Testing PENME on significantly larger models (e.g., Llama-2-70B or GPT-3) to compare performance, training efficiency, and stability.

### Open Question 2
- Question: Can the projector network be effectively pre-trained on a large corpus to eliminate the need for contrastive learning fine-tuning per model?
- Basis in paper: [explicit] The authors suggest this as a future direction in the conclusion, indicating uncertainty about its feasibility.
- Why unresolved: The paper does not provide experimental results on pre-training the projector network, only proposing it as a future investigation.
- What evidence would resolve it: Pre-training a projector network on a large dataset and evaluating its performance across different models without further fine-tuning.

### Open Question 3
- Question: How does the projector network handle long-form generation tasks beyond the question-answering paradigm used in the experiments?
- Basis in paper: [explicit] The authors mention in the conclusion that they plan to assess the projector network's effectiveness in more complex scenarios like long-form generation.
- Why unresolved: The current evaluation is limited to factual editing tasks, and the projector network's behavior in more complex generation tasks is untested.
- What evidence would resolve it: Applying PENME to tasks requiring long-form generation (e.g., story continuation or summarization) and evaluating its performance.

## Limitations
- The projector network's performance on much larger models (e.g., Llama-2-70B or beyond) is untested, leaving scalability questions unanswered.
- The contrastive learning setup is not fully detailed, making it difficult to assess whether the reported results are reproducible with alternative training configurations.
- The method's effectiveness on long-form generation tasks beyond question-answering is unexplored, limiting its applicability to more complex scenarios.

## Confidence
- High: Identification of lexical bias as a fundamental problem in model editing
- Medium: Effectiveness of the projector network in mitigating lexical bias based on ablation results
- Low: Generalizability of the layer 2 integration across diverse model architectures without further validation

## Next Checks
1. Conduct cross-architecture tests by applying PENME to models beyond T5, Llama-2, and GPT2-XL (e.g., BERT, RoBERTa) and evaluate whether the same layer 2 projector integration yields consistent improvements.
2. Perform ablation studies varying the projector network depth and contrastive loss margin to quantify their impact on lexical bias mitigation and editing performance.
3. Test the method on out-of-distribution prompts (e.g., different domains or tasks) to assess robustness to domain shift and identify potential failure modes.