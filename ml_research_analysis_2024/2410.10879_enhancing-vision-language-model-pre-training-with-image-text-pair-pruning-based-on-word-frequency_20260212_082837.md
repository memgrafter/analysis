---
ver: rpa2
title: Enhancing Vision-Language Model Pre-training with Image-text Pair Pruning Based
  on Word Frequency
arxiv_id: '2410.10879'
source_url: https://arxiv.org/abs/2410.10879
tags:
- wfpp
- data
- words
- dataset
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Word-Frequency-based Image-Text Pair Pruning
  (WFPP), a method to improve vision-language model (VLM) pre-training efficiency
  by selectively removing image-text pairs containing high-frequency words. Unlike
  prior metadata-based pruning approaches, WFPP prunes based directly on word frequencies
  within text captions, using a joint probability formula to determine which pairs
  to remove.
---

# Enhancing Vision-Language Model Pre-training with Image-text Pair Pruning Based on Word Frequency

## Quick Facts
- arXiv ID: 2410.10879
- Source URL: https://arxiv.org/abs/2410.10879
- Reference count: 40
- Primary result: WFPP achieves comparable ImageNet-1K performance using only 77-83% of data

## Executive Summary
This paper introduces Word-Frequency-based Image-Text Pair Pruning (WFPP), a method to improve vision-language model (VLM) pre-training efficiency by selectively removing image-text pairs containing high-frequency words. Unlike prior metadata-based pruning approaches, WFPP prunes based directly on word frequencies within text captions, using a joint probability formula to determine which pairs to remove. This creates a more balanced word-frequency distribution in the training data, which benefits model training. Experiments on CLIP models show WFPP achieves comparable or better performance on ImageNet-1K and multiple other datasets while using only 77-83% of the original data, enabling 1.3× faster pre-training. The method also outperforms metadata-based pruning approaches and maintains vocabulary diversity while reducing dominance of frequent words.

## Method Summary
The WFPP method calculates word frequencies across the training corpus, then determines which image-text pairs to prune based on a joint probability formula that accounts for the frequency of each word in the text. For each text, the method computes a retain probability based on the product of (1 - discard probability) for all words in the text. Texts with the lowest retain probabilities are pruned. The pruned dataset is then used for pre-training the vision-language model for 30 epochs, followed by fine-tuning on the full dataset for 1 additional epoch. This approach differs from prior methods that prune based on metadata or use individual word discard probabilities without considering joint probabilities across entire texts.

## Key Results
- Achieves comparable ImageNet-1K zero-shot classification accuracy (65.8%) using only 77-83% of the original data
- Outperforms metadata-based pruning approaches like MetaCLIP
- Maintains vocabulary diversity while reducing computational requirements by 1.3×
- Shows consistent improvements across multiple evaluation datasets including COCO and Flickr30K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Word-frequency imbalance in training data causes vision-language models to learn biased representations that overfit frequent concepts
- Mechanism: When high-frequency words dominate the training corpus, the model allocates disproportionate capacity to learn associations for these words, leading to suboptimal representation learning for low-frequency concepts
- Core assumption: The quality of learned visual-semantic embeddings depends on balanced exposure to both frequent and infrequent concepts during pre-training
- Evidence anchors:
  - [abstract] "The effect of WFPP is to reduce the dominance of frequent words. The result a better balanced word-frequency distribution in the dataset, which is known to improve the training of word embedding models"
  - [section] "WFPP offers two advantages over the data pruning proposed by MetaCLIP. First, WFPP selects image-text pairs on the basis of an individual score"
  - [corpus] Weak - the corpus neighbors don't directly address word-frequency imbalance in VLMs
- Break condition: If the word-frequency distribution has minimal impact on representation quality, or if model capacity is so large that it can easily learn all concepts regardless of frequency distribution

### Mechanism 2
- Claim: Pruning entire image-text pairs based on word frequency maintains vocabulary diversity while improving training efficiency
- Mechanism: By removing pairs containing high-frequency words rather than individual words, the method preserves the overall vocabulary size and diversity while reducing computational load
- Core assumption: Vocabulary richness is more important for model performance than raw data volume
- Evidence anchors:
  - [abstract] "WFPP in contrast selects entire image-text pairs for removal. Note that the WFPP manner of removing texts does not impact the overall vocabulary richness, measured in vocabulary size, which is important to maintain"
  - [section] "The advantage of this equation is that it filters out text containing frequent words to create a subset of the dataset with a balanced word distribution"
  - [corpus] Weak - corpus neighbors focus on different pruning approaches without addressing vocabulary preservation
- Break condition: If removing entire pairs causes significant loss of important visual concepts, or if the frequency-based scoring fails to preserve rare but important vocabulary

### Mechanism 3
- Claim: Fine-tuning on the full dataset after pre-training on pruned data compensates for any concept distribution gaps
- Mechanism: The additional fine-tuning epoch bridges the distribution gap between the pruned pre-training data and the full inference data, incorporating any unknown concepts that were initially removed
- Core assumption: A short fine-tuning phase on the full dataset can effectively adapt the model to concepts that were underrepresented during pre-training
- Evidence anchors:
  - [section] "After pre-training on the pruned subset, we fine-tuned the model on the entire dataset for one additional epoch to achieve better performance"
  - [section] "This additional epoch of fine-tuning aims to bridge the distribution gap between the pre-training and inference stages and to account for any unknown concepts that may have been present in the initially removed data"
  - [corpus] Weak - corpus neighbors don't discuss the fine-tuning compensation strategy
- Break condition: If the fine-tuning phase is insufficient to recover from the concept distribution gaps, or if the pruned data creates persistent biases that cannot be corrected by fine-tuning

## Foundational Learning

- Concept: Word frequency distribution in natural language corpora
  - Why needed here: Understanding how word frequencies naturally occur in web-scale datasets is crucial for designing effective pruning strategies
  - Quick check question: What is the typical distribution pattern for word frequencies in large text corpora (e.g., power law, normal distribution)?

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The model uses contrastive learning to align visual and textual representations, so understanding this mechanism is essential
  - Quick check question: How does InfoNCE loss encourage alignment between positive pairs while separating negative pairs in the embedding space?

- Concept: Vision transformer architecture and image encoding
  - Why needed here: The model uses ViT as the image encoder, so understanding how vision transformers process images is important
  - Quick check question: What is the role of patch embedding in vision transformers, and how does it transform images into token sequences?

## Architecture Onboarding

- Component map: ViT-B-16 image encoder → CLIP-style projection head → Text transformer encoder → Cosine similarity computation → InfoNCE loss
- Critical path: Data preprocessing (word frequency calculation) → Pair pruning (WFPP scoring) → Model pre-training (30 epochs) → Fine-tuning (1 epoch) → Evaluation
- Design tradeoffs: Data efficiency vs. concept coverage, computational savings vs. potential performance loss, simplicity vs. sophistication in pruning strategy
- Failure signatures: Significant performance degradation on rare concept categories, vocabulary loss despite pruning claims, fine-tuning phase failing to recover performance
- First 3 experiments:
  1. Verify WFPP scoring correctly identifies and prunes high-frequency word pairs by analyzing word frequency distributions before/after pruning
  2. Test pre-training on varying pruning percentages (50%, 70%, 90%) to find the optimal balance point
  3. Compare performance with and without the fine-tuning phase to validate the compensation mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does WFPP's performance scale when applied to extremely large datasets like LAION-400M, and what is the optimal pruning ratio for maximizing performance?
- Basis in paper: [explicit] The authors discuss applying WFPP to LAION-400M and suggest that performance improvements could scale up, estimating zero-shot classification accuracy exceeding 40% with 280 million samples.
- Why unresolved: The paper only discusses theoretical implications and does not provide experimental results for LAION-400M or other extremely large datasets.
- What evidence would resolve it: Empirical experiments applying WFPP to LAION-400M or similar large-scale datasets with various pruning ratios, measuring zero-shot classification accuracy and computational efficiency.

### Open Question 2
- Question: What is the impact of WFPP on rare words (frequency < t) and how does this affect model performance, particularly for domain-specific applications?
- Basis in paper: [explicit] The authors mention that rare words do not affect the probability of text being discarded (P(wi) = 1 for f(wi) ≤ t), but acknowledge that the impact on model performance is slight and suggest future work should investigate this.
- Why unresolved: The paper does not provide detailed analysis of how rare words affect model performance, particularly in specialized domains where rare words might be important.
- What evidence would resolve it: Detailed experiments comparing model performance on domain-specific datasets with and without rare words, measuring accuracy and robustness.

### Open Question 3
- Question: How does WFPP compare to other data pruning methods that use different balancing criteria, such as TF-IDF or metadata-based approaches, in terms of both efficiency and final model performance?
- Basis in paper: [explicit] The authors compare WFPP to MetaCLIP (a metadata-based approach) and show superior performance, but do not compare to other pruning methods like TF-IDF-based approaches.
- Why unresolved: The paper only provides comparison with one specific pruning method and does not explore the broader landscape of data pruning techniques.
- What evidence would resolve it: Comprehensive benchmarking of WFPP against various data pruning methods (TF-IDF, random sampling, metadata-based) across multiple datasets and tasks, measuring both efficiency gains and performance metrics.

## Limitations
- The method's effectiveness depends heavily on the quality of word frequency calculation and the chosen threshold parameter, which may require dataset-specific tuning
- The compensation through fine-tuning assumes that 1 epoch on full data is sufficient to recover from any concept distribution gaps, which may not hold for all datasets
- The approach's generalizability to non-English datasets or domains with different word frequency distributions remains unexplored

## Confidence
- **High Confidence**: Claims about computational efficiency gains (1.3× faster pre-training) and zero-shot classification performance improvements on ImageNet-1K
- **Medium Confidence**: Claims about vocabulary preservation and the effectiveness of the fine-tuning compensation strategy  
- **Low Confidence**: Claims about superior performance on rare concept categories and the method's generalizability to different domains

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically test different threshold values (t = 10^-6, 10^-7, 10^-8) to determine how sensitive the pruning strategy is to this parameter and identify the optimal balance between efficiency gains and performance retention.

2. **Vocabulary Coverage Evaluation**: Conduct detailed analysis of vocabulary preservation by comparing word occurrence distributions, rare word retention rates, and concept coverage between original and pruned datasets across multiple frequency ranges.

3. **Cross-Domain Generalization Test**: Apply WFPP to a different domain (e.g., medical imaging or specialized technical datasets) to evaluate whether the word frequency-based pruning strategy generalizes beyond general web-scale image-text pairs or requires domain-specific adaptation.