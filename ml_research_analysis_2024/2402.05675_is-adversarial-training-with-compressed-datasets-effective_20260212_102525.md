---
ver: rpa2
title: Is Adversarial Training with Compressed Datasets Effective?
arxiv_id: '2402.05675'
source_url: https://arxiv.org/abs/2402.05675
tags:
- adversarial
- dataset
- training
- methods
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the trade-off between adversarial robustness
  and dataset compression, demonstrating that compressed datasets obtained via standard
  dataset condensation (DC) methods do not effectively transfer adversarial robustness
  to models. Empirically, models trained with compressed datasets from DC methods
  exhibit high test accuracy but poor robust accuracy compared to models trained with
  randomly selected coreset subsets.
---

# Is Adversarial Training with Compressed Datasets Effective?

## Quick Facts
- arXiv ID: 2402.05675
- Source URL: https://arxiv.org/abs/2402.05675
- Reference count: 40
- Key outcome: Compressed datasets from DC methods do not effectively transfer adversarial robustness to models; MCS improves robust accuracy while maintaining test accuracy.

## Executive Summary
This work investigates whether compressed datasets obtained via dataset condensation (DC) methods can effectively transfer adversarial robustness to models. The authors demonstrate that DC methods, designed to optimize test accuracy, fail to preserve the data distribution geometry needed for adversarial robustness. To address this, they introduce Minimal Finite Covering (MCS), a robustness-aware dataset compression method that provably guarantees adversarial robustness by minimizing a generalized adversarial loss. Empirical results on MNIST and CIFAR10 show that MCS outperforms DC methods in robust accuracy while maintaining competitive test accuracy.

## Method Summary
The paper compares standard DC methods (DM, GM, TM) with coreset methods (MCS, Rand) for dataset compression under a fixed budget of 50 samples. MCS constructs a minimal finite covering of the dataset with a fixed radius η, ensuring that adversarial perturbations remain within the covered region. The method is evaluated on MNIST, CIFAR10, CIFAR100, and SVHN using ResNet-18 models trained with standard and adversarial training. Robust accuracy is measured using AutoAttack under ℓ∞ perturbations. The key innovation is the use of a generalized adversarial loss that weights synthetic points by the number of original points they cover, providing provable robustness guarantees.

## Key Results
- DC methods (DM, GM, TM) achieve high test accuracy but poor robust accuracy compared to coreset methods.
- MCS provides provable robustness guarantees by minimizing the generalized adversarial loss.
- On MNIST with 50 samples, MCS achieves 92.83% robust accuracy, outperforming DC methods.
- MCS preserves the covering structure of the data space, ensuring adversarial perturbations remain within the covered region.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compressed datasets from DC methods overfit to test accuracy, sacrificing adversarial robustness.
- Mechanism: DC methods optimize synthetic data to minimize a downstream task loss (e.g., matching gradients, training trajectories), which implicitly favors accuracy over robustness. The synthetic data distribution diverges from the true data distribution, breaking the statistical properties needed for adversarial training.
- Core assumption: Adversarial robustness requires the compressed dataset to preserve the geometry of the original data distribution under perturbations.
- Evidence anchors:
  - [abstract] "compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models"
  - [section] "DC methods have poor adversarial robustness, as they are designed to optimize for test accuracy"
  - [corpus] Weak: no direct citations but related to "Model Compression vs. Adversarial Robustness" which explores the trade-off empirically.
- Break condition: If the DC method explicitly incorporates a robustness-aware loss during synthesis (e.g., adversarial loss matching), the overfitting pattern may be mitigated.

### Mechanism 2
- Claim: MCS preserves the covering structure of the data space, ensuring that adversarial perturbations remain within the covered region.
- Mechanism: MCS constructs a minimal finite covering of the dataset with a fixed radius η. This guarantees that for any original point, a nearby synthetic point exists within η distance, so adversarial balls of radius ε around synthetic points cover the adversarial balls around original points if ε+η is small enough.
- Core assumption: The dataset satisfies the Zero Intersection Property (ZIP) or can be approximated to satisfy it.
- Evidence anchors:
  - [abstract] "provably robust by minimizing the generalized adversarial loss"
  - [section] "MCS approximates the distribution of the entire dataset"
  - [corpus] Weak: no direct citations but related to "Compression Aware Certified Training" which integrates certified robustness into compression.
- Break condition: If the data distribution is too complex (e.g., high-dimensional, overlapping classes), ZIP may not hold and the covering may fail to preserve robustness.

### Mechanism 3
- Claim: Generalized adversarial loss over MCS provides an upper bound on the classical adversarial loss over the original dataset.
- Mechanism: By weighting each synthetic point by the number of original points it covers (q(x,y)), the generalized loss over MCS upper bounds the loss over the full dataset. This allows provable robustness guarantees without retraining on the full dataset.
- Core assumption: The covering radius η is small enough that ε+η adversarial balls do not overlap between classes.
- Evidence anchors:
  - [abstract] "provably robust by minimizing the generalized adversarial loss"
  - [section] "minimizing the generalized adversarial loss over MCS provides a valid upper bound"
  - [corpus] Weak: no direct citations but related to "certified training" works that bound adversarial loss.
- Break condition: If η is too large relative to ε, the bounding property fails and the robust guarantee is lost.

## Foundational Learning

- Concept: Minimal Finite Covering (MFC)
  - Why needed here: MFC formalizes dataset compression as a geometric covering problem, enabling provable robustness guarantees.
  - Quick check question: Given a dataset T and radius η, how do you construct the adjacency matrix A(η) used in MCS?

- Concept: Zero Intersection Property (ZIP)
  - Why needed here: ZIP ensures that adversarial balls around synthetic points do not overlap across classes, which is necessary for the robustness proof.
  - Quick check question: What condition on ε and η ensures ZIP holds for a dataset T with class-separated components?

- Concept: Generalized adversarial loss
  - Why needed here: It weights synthetic points by the number of original points they represent, allowing the loss over MCS to upper bound the loss over the original dataset.
  - Quick check question: How does the weight q(x,y) in the generalized loss relate to the covering radius η?

## Architecture Onboarding

- Component map: Data preprocessing -> MCS construction -> Training pipeline -> Evaluation
- Critical path:
  1. Compute distance matrix for dataset.
  2. Solve η-MCS optimization to obtain synthetic dataset.
  3. Train model (standard or adversarial) on synthetic dataset.
  4. Evaluate robust accuracy on original test set.
- Design tradeoffs:
  - η vs k: Smaller η yields better coverage but requires more synthetic points; larger η reduces coverage quality but fewer points.
  - Computational cost: MCS requires solving an MILP/MIQCP, which is NP-hard in general; heuristics or approximations may be needed for large datasets.
  - Robustness guarantee: Only holds if ZIP condition is satisfied; otherwise empirical robustness may degrade.
- Failure signatures:
  - Robust accuracy much lower than standard accuracy despite MCS covering.
  - MCS synthetic points clustered in low-density regions, missing decision boundaries.
  - Solver fails to find feasible MCS within time limit.
- First 3 experiments:
  1. Run MCS on MNIST with η=0.1, k=50; train a simple CNN and evaluate robust accuracy.
  2. Compare MCS robust accuracy to random coreset and DM baselines under same budget.
  3. Vary η in MCS and plot the trade-off between test accuracy and robust accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does MCS sometimes outperform random sampling in test accuracy despite the latter's better performance in robustness?
- Basis in paper: [explicit] The paper observes that "MCS performs even better than MCS in most cases, both for test and robust accuracy" in low budget regimes, contradicting the expectation that MCS would excel at preserving robustness while random sampling maintains test accuracy.
- Why unresolved: The paper notes this behavior but does not provide a theoretical explanation for why MCS would have superior test performance compared to random sampling, particularly in the low budget regime.
- What evidence would resolve it: A theoretical analysis explaining the relationship between MCS's finite covering properties and its ability to preserve both test accuracy and robustness, along with controlled experiments varying budget sizes and dataset characteristics.

### Open Question 2
- Question: How would the proposed method perform on non-ZIP satisfying datasets, and what modifications would be necessary?
- Basis in paper: [explicit] The paper acknowledges that "the assumptions to satisfy ZIP in Section 5 are not satisfied by complex datasets" and notes that MCS "can be regarded as the deterministic version of random sampling, since both of them converges to the full dataset as the budget increases."
- Why unresolved: The current theoretical guarantees and empirical results are limited to low-dimensional datasets (MNIST) where ZIP conditions hold, leaving open the question of scalability to real-world, high-dimensional datasets.
- What evidence would resolve it: Experiments demonstrating MCS performance on complex datasets (e.g., CIFAR-100, ImageNet) with modified distance metrics or loss functions that relax the ZIP constraint, along with theoretical analysis of convergence properties.

### Open Question 3
- Question: What is the impact of different distance metrics beyond Euclidean distance on downstream performance in dataset compression?
- Basis in paper: [inferred] The paper mentions that "the choice of distance metric, beyond Euclidean distance, might impact downstream performance, which is not sufficiently discussed in the current paper" and only uses Euclidean distance throughout their experiments.
- Why unresolved: The paper uses a single distance metric (Euclidean) without exploring how alternative metrics might affect the trade-off between test accuracy and robustness in compressed datasets.
- What evidence would resolve it: Comparative experiments using various distance metrics (e.g., Mahalanobis, cosine similarity, learned metrics) on the same datasets, along with analysis of how these choices affect the Hausdorff distance approximation and subsequent model performance.

## Limitations

- Empirical evaluation is limited to small-scale datasets (MNIST, CIFAR10) and a fixed compression budget (50 samples).
- Computational complexity of the MILP/MIQCP optimization for MCS may limit scalability to larger datasets.
- Theoretical guarantees (ZIP condition, generalized loss upper bound) depend on data distribution and may not hold universally.

## Confidence

- **High Confidence**: The claim that DC methods sacrifice robustness for accuracy is well-supported by the empirical results showing poor robust accuracy despite high test accuracy.
- **Medium Confidence**: The effectiveness of MCS in improving robust accuracy is demonstrated, but the scalability and generalization to other datasets require further validation.
- **Low Confidence**: The theoretical guarantees (ZIP condition, generalized loss upper bound) are sound in principle, but their practical applicability depends on the data distribution and may not hold universally.

## Next Checks

1. Evaluate MCS on larger datasets (e.g., CIFAR100) and with varying compression budgets to assess scalability and robustness trade-offs.
2. Investigate the impact of the covering radius η on the trade-off between test accuracy and robust accuracy, and determine the optimal η for different datasets.
3. Compare MCS with other coreset methods (e.g., K-center, Random) under the same computational budget to validate its efficiency and effectiveness.