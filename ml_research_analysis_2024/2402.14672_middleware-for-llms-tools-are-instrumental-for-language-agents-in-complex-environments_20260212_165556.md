---
ver: rpa2
title: 'Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex
  Environments'
arxiv_id: '2402.14672'
source_url: https://arxiv.org/abs/2402.14672
tags:
- tools
- llms
- complex
- environments
- variable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how effectively large language models (LLMs)
  can handle complex environments with the aid of specialized tools. It introduces
  a novel class of tools called "middleware" that act as an intermediary layer between
  LLMs and complex environments such as knowledge bases and databases.
---

# Middleware for LLMs: Tools Are Instrumental for Language Agents in Complex Environments

## Quick Facts
- **arXiv ID**: 2402.14672
- **Source URL**: https://arxiv.org/abs/2402.14672
- **Reference count**: 19
- **Primary result**: GPT-4 achieves 2.8x performance on database tasks and 2.2x on knowledge base tasks when augmented with middleware tools

## Executive Summary
This paper investigates how effectively large language models (LLMs) can handle complex environments with the aid of specialized tools. It introduces a novel class of tools called "middleware" that act as an intermediary layer between LLMs and complex environments such as knowledge bases and databases. These tools are designed to assist LLMs in actively exploring and navigating these environments by abstracting away the underlying complexity. The evaluation on two benchmarks demonstrates the significant potential of augmenting LLMs with middleware tools, with GPT-4 achieving 2.8x the performance of the best baseline in database tasks and 2.2x in knowledge base tasks.

## Method Summary
The method involves developing customized middleware tools for knowledge bases and databases, and evaluating their effectiveness using two novel schemes: error feedback and decoupled generation. The evaluation is conducted on the curated benchmarks using various LLMs, including GPT-3.5-turbo and GPT-4. The middleware tools provide abstractions that shield the LLM from direct interaction with intricate database schemas or massive KB structures, allowing the LLM to proactively invoke navigational tools to gather relevant information. Error feedback provides detailed error information and prompts the LLM to correct mistakes, while decoupled generation separates the LLM's reasoning steps from tool use, allowing for more controlled and precise action selection.

## Key Results
- GPT-4 equipped with middleware tools achieves 2.8x performance on database tasks compared to direct prompting baselines
- GPT-4 with middleware achieves 2.2x performance on knowledge base tasks
- Middleware approach significantly outperforms direct prompting baselines by allowing the LLM to flexibly gather necessary information on demand

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Middleware tools enable LLMs to handle complex environments by abstracting environmental complexity and providing on-demand navigation
- Mechanism: The tools act as a middle layer shielding the LLM from direct interaction with intricate database schemas or massive KB structures. Instead, the LLM can proactively invoke navigational tools (e.g., get_relations, get_neighbors) to gather relevant information, bypassing the limitations of its short-term memory
- Core assumption: The LLM's reasoning capabilities are sufficient to orchestrate tool usage when provided with appropriate abstractions
- Evidence anchors: Abstract states middleware "abstracting away the underlying complexity"; Section 3.1 describes navigational and functional tools for exploration and precise operations
- Break condition: If tool abstractions become too complex for LLM reasoning or if LLM reasoning is insufficient to orchestrate tool usage effectively

### Mechanism 2
- Claim: Error feedback and decoupled generation improve the accuracy of tool usage by the LLM
- Mechanism: Error feedback provides detailed error information and prompts the LLM to correct mistakes, while decoupled generation separates the LLM's reasoning steps from tool use, allowing for more controlled and precise action selection
- Core assumption: The LLM has capacity for self-correction through feedback and can effectively reason about permissible actions when decoupled from tool execution
- Evidence anchors: Section 3.2 describes both mechanisms; mentions potential unreliability of self-correction with weak LLMs
- Break condition: If LLM is too weak to benefit from error feedback or decoupled generation, or if prompts are not effective

### Mechanism 3
- Claim: Middleware approach significantly outperforms direct prompting baselines by allowing LLM to flexibly gather necessary information on demand
- Mechanism: Direct prompting baselines can only feed limited information into LLM, which is insufficient for complex environments. Middleware approach allows LLM to dynamically query environment for relevant information
- Core assumption: Middleware tools provide more efficient way for LLM to access information than direct prompting
- Evidence anchors: Section 5.4 shows that including more database rows initially boosts baseline performance but eventually decreases it; middleware without sampled rows significantly outperforms all baselines
- Break condition: If middleware tools become too slow or inefficient, or if LLM tool usage is not accurate enough to gather necessary information

## Foundational Learning

- **Tool learning and reasoning with tools**
  - Why needed here: Middleware approach relies on LLM's ability to learn and reason with specialized tools
  - Quick check question: What are the key challenges in enabling LLMs to effectively use tools, and how do error feedback and decoupled generation address these challenges?

- **Complex environment interaction**
  - Why needed here: Middleware approach designed to help LLMs interact with complex environments like databases and knowledge bases
  - Quick check question: What are the key differences between interacting with a database and a knowledge base, and how do middleware tools address these differences?

- **Benchmarking and evaluation**
  - Why needed here: Paper evaluates middleware approach using benchmarks like BIRD and KBQA-AGENTS
  - Quick check question: What are the key challenges in designing benchmarks for evaluating LLM-tool interaction, and how do BIRD and KBQA-AGENTS address these challenges?

## Architecture Onboarding

- **Component map**: LLM -> Middleware Tools -> Complex Environment
- **Critical path**: 1) LLM receives instruction and decides which tool to use 2) LLM invokes tool with appropriate arguments 3) Tool executes action and returns result 4) LLM processes result and decides next action 5) Steps 1-4 repeated until task completion
- **Design tradeoffs**: 
  - Granularity of tools: Fine-grained tools provide more control but require more complex LLM reasoning; coarse-grained tools are easier to use but may be less flexible
  - Error handling: Detailed error feedback can improve accuracy but may slow process; decoupled generation improves controllability but adds complexity
  - Tool availability: Wide range of tools improves task handling but increases system complexity
- **Failure signatures**: LLM produces invalid actions unfaithful to rationale; LLM gets stuck in repeated tool usage loop; LLM fails to gather necessary information; middleware tools become too slow or inefficient
- **First 3 experiments**:
  1. Evaluate middleware approach on simple database query task and compare with direct prompting baseline
  2. Implement error feedback and decoupled generation mechanisms and evaluate impact on tool usage accuracy
  3. Design benchmark for evaluating LLM-tool interaction and use it to compare middleware approach with other tool learning frameworks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do middleware tools perform on other complex environments beyond databases and knowledge bases, such as webpages or physical environments?
- Basis in paper: Paper acknowledges that implementing customized tools for KBs and databases is less challenging compared to environments without straightforward query interface, and mentions plans to extend middleware across broader range of environments
- Why unresolved: Paper focuses on evaluating middleware tools on databases and knowledge bases, leaving performance on other complex environments unexplored
- What evidence would resolve it: Experiments evaluating middleware tools on diverse complex environments including webpages, physical environments, and other data sources

### Open Question 2
- Question: What is the impact of tool design strategies on performance of LLMs in complex environments?
- Basis in paper: Paper mentions tools are solely grounded in authors' experience and acknowledges need for more principled tool design strategy in future work
- Why unresolved: Paper does not explore different tool design strategies or compare performance of tools designed using various approaches
- What evidence would resolve it: Comparative study evaluating performance of LLMs using tools designed with different strategies (domain expertise, automated methods, combination)

### Open Question 3
- Question: How do middleware tools affect interpretability and explainability of LLM decisions in complex environments?
- Basis in paper: Paper introduces middleware tools as layer shielding LLM from environmental complexity but does not discuss implications for interpretability and explainability
- Why unresolved: Paper focuses on performance benefits but does not address potential trade-offs in model interpretability and explainability
- What evidence would resolve it: Analysis of decision-making process of LLMs using middleware tools including reasoning steps, tool usage, and final decisions; comparison of interpretability with and without middleware tools

## Limitations

- Evaluation conducted primarily on synthetic benchmarks (BIRD and KBQA-AGENTS) rather than real-world production environments
- Middleware tools are custom-built for specific tasks, generalizability to other complex environments remains untested
- Error feedback mechanisms rely on LLM self-correction capabilities that may vary across model versions and architectures

## Confidence

- **High confidence**: Middleware architecture design and core observation that tool augmentation improves LLM performance in structured environments
- **Medium confidence**: Specific error feedback and decoupled generation mechanisms, as these rely on LLM self-correction capabilities that may vary across model versions
- **Medium confidence**: Quantitative performance improvements, given controlled benchmark conditions

## Next Checks

1. Test middleware approach on real-world production databases and knowledge bases to assess practical applicability beyond synthetic benchmarks
2. Evaluate robustness of error feedback mechanisms across different LLM model sizes and architectures, particularly weaker models
3. Assess latency and computational overhead introduced by middleware layer in time-sensitive applications