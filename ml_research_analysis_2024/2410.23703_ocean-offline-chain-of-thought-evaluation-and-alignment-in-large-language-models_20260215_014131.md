---
ver: rpa2
title: 'OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language
  Models'
arxiv_id: '2410.23703'
source_url: https://arxiv.org/abs/2410.23703
tags:
- knowledge
- reasoning
- arxiv
- policy
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OCEAN, a framework for offline evaluation and
  alignment of chain-of-thought reasoning in large language models using knowledge
  graphs. The core method involves modeling LLM reasoning as a Markov Decision Process
  and developing a KG-IPS estimator that leverages knowledge graph preference modeling
  to provide feedback on reasoning paths.
---

# OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models

## Quick Facts
- **arXiv ID**: 2410.23703
- **Source URL**: https://arxiv.org/abs/2410.23703
- **Reference count**: 37
- **Key outcome**: OCEAN improves chain-of-thought alignment in LLMs using KG-IPS evaluation and knowledge graph preference modeling, outperforming supervised fine-tuning on knowledge-intensive and multi-hop reasoning tasks.

## Executive Summary
This paper introduces OCEAN, a framework for offline evaluation and alignment of chain-of-thought reasoning in large language models using knowledge graphs. The approach models LLM reasoning as a Markov Decision Process and develops a KG-IPS estimator that leverages knowledge graph preference modeling to provide feedback on reasoning paths. By verbalizing knowledge graph trajectories and incorporating entity-based and non-entity-based token weighting, OCEAN bridges the gap between LLM and KG reasoning while maintaining generalizability and generation quality. Experiments on multiple backbone LLMs demonstrate effective improvement in chain-of-thought alignment across various tasks.

## Method Summary
OCEAN models chain-of-thought reasoning as an MDP and uses knowledge graph preference modeling to generate token-level likelihood distributions for LLM-generated reasoning paths. The method verbalizes KG trajectories into natural language using GPT-4, then fine-tunes a small language model as the behavior policy that aligns with generative LLMs' behaviors. The KG-IPS estimator provides unbiased evaluation by weighting entity tokens according to the knowledge graph policy while using the base LLM policy for non-entity tokens. Direct policy optimization via gradient descent on the estimated value function enables efficient alignment without requiring online interaction.

## Key Results
- OCEAN achieves superior performance on knowledge-intensive and multi-hop reasoning tasks compared to supervised fine-tuning baselines
- The KG-IPS estimator provides unbiased evaluation with a theoretical lower bound on variance
- Entity-based and non-entity-based token weighting effectively reduces variance while maintaining alignment accuracy
- The framework maintains generalizability across different LLM architectures and reasoning task types

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: KG-IPS estimator provides unbiased evaluation of chain-of-thought reasoning by bridging LLM and KG reasoning through verbalized knowledge graph trajectories.
- **Mechanism**: The method models chain-of-thought reasoning as an MDP and uses knowledge graph preference modeling to generate token-level likelihood distributions. Entity tokens are weighted by the knowledge graph policy while non-entity tokens use the base LLM policy, reducing variance while maintaining evaluation accuracy.
- **Core assumption**: The verbalized knowledge graph policy can accurately simulate KG reasoning preference and align with LLM-generated reasoning paths.
- **Evidence anchors**:
  - [abstract]: "To overcome the reasoning heterogeneity and grounding problems, we leverage on-policy KG exploration and RL to model a KG policy that generates token-level likelihood distributions for LLM-generated chain-of-thought reasoning paths, simulating KG reasoning preference."
  - [section 4.2]: "To overcome the reasoning heterogeneity and grounding problems, we leverage on-policy knowledge graph exploration and reinforcement learning to model a knowledge graph policy that generates token-level likelihood distributions for LLM-generated chain-of-thought reasoning paths, simulating knowledge graph reasoning preference."
  - [corpus]: Weak - corpus contains related papers on alignment and chain-of-thought but none specifically discuss verbalized KG trajectories as a bridging mechanism.

### Mechanism 2
- **Claim**: Direct policy optimization using KG-IPS enables efficient alignment of LLMs with knowledge graph reasoning without affecting general abilities.
- **Mechanism**: The framework estimates policy values through KG-IPS and directly optimizes the target policy via gradient descent, maximizing estimated policy values while maintaining non-entity token behavior through separate weighting.
- **Core assumption**: The KG-IPS estimator's lower bound on variance ensures stable optimization without model degeneration.
- **Evidence anchors**:
  - [abstract]: "With the off-policy evaluated value function, we can directly enable off-policy optimization to further enhance chain-of-thought alignment."
  - [section 4.1]: "To further support our findings, we demonstrate that the optimal policy for the final reward is consistent with the optimal policy for the entity-based knowledge graph reward, which means the non-entity-based LLM reward can be considered as a regularization term that does not affect the optimal policy."
  - [corpus]: Weak - related papers discuss preference alignment but don't specifically address variance-controlled direct optimization in the KG-IPS context.

### Mechanism 3
- **Claim**: The verbalized knowledge graph policy can generalize across different LLMs by converting structured KG reasoning into natural language.
- **Mechanism**: KG trajectories are verbalized into natural language using GPT-4, then used to fine-tune a small language model as the behavior policy that aligns with generative LLMs' behaviors.
- **Core assumption**: Verbalized KG trajectories maintain sufficient semantic fidelity to represent the underlying reasoning patterns while being compatible with LLM generation.
- **Evidence anchors**:
  - [section 3.2]: "To align the action space between the knowledge graph preference policy µϕ and the target policy πθ, we leverage a small language model as the backbone of µϕ and fine-tune the model on verbalized trajectories as natural language contexts."
  - [section 4.2]: "Inspired by existing efforts in verbalizing structured knowledge graphs into natural language query (Seyler et al., 2017) and context (Agarwal et al., 2020; Wang et al., 2022a), we leverage the GPT-4 (Achiam et al., 2023) model f to verbalize each chain of triplets h into a chain-of-thoughts c = f (h)."
  - [corpus]: Moderate - several papers discuss cross-lingual alignment and verbalization of structured knowledge, suggesting this approach has precedent.

## Foundational Learning

- **Concept**: Markov Decision Process formulation of chain-of-thought reasoning
  - Why needed here: The MDP framework allows modeling the sequential decision-making process of LLM reasoning as states, actions, and rewards, enabling the application of reinforcement learning and offline evaluation techniques.
  - Quick check question: What are the state, action, and reward components in the LLM chain-of-thought MDP formulation?

- **Concept**: Inverse Propensity Scoring (IPS) for offline policy evaluation
  - Why needed here: IPS enables evaluation of target policies using only logged data without requiring online interaction, crucial for expensive-to-deploy LLMs.
  - Quick check question: How does the KG-IPS estimator differ from standard IPS in handling entity vs non-entity tokens?

- **Concept**: Knowledge graph reasoning and entity linking
  - Why needed here: Understanding how KGs represent knowledge as entity-relation triples and how to link natural language to these entities is essential for the verbalization and alignment process.
  - Quick check question: What challenges arise when grounding LLM-generated reasoning paths in knowledge graph structures?

## Architecture Onboarding

- **Component map**:
  - Target LLM (πθ) -> Base LLM (π0) -> Knowledge Graph (G) -> KG Policy (µϕ) -> Verbalization Module -> KG-IPS Estimator -> Optimization Module

- **Critical path**: Prompt → LLM reasoning generation → KG policy evaluation → KG-IPS scoring → Policy optimization → Improved reasoning

- **Design tradeoffs**:
  - Entity vs non-entity token weighting: Balancing accurate KG alignment with maintaining general LLM behavior
  - Verbalization quality vs computational cost: Higher quality verbalization improves alignment but increases overhead
  - KG policy complexity vs generalization: More complex KG policies may overfit to specific reasoning patterns

- **Failure signatures**:
  - High variance in KG-IPS estimates despite theoretical bounds
  - Degradation in non-reasoning tasks after optimization
  - Poor correlation between estimated values and actual reasoning quality
  - Entity linking failures causing KG policy to misalign with LLM reasoning

- **First 3 experiments**:
  1. Verify unbiasedness: Run KG-IPS on base LLM with known ground truth to confirm estimates match expected rewards
  2. Test variance bounds: Measure empirical variance across multiple reasoning paths and compare to theoretical lower bound
  3. Ablation study: Remove entity weighting or non-entity regularization to observe impact on alignment and general performance

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the variance of the KG-IPS estimator scale with the size and complexity of the knowledge graph used for feedback?
- **Open Question 2**: Can the KG-IPS estimator be extended to handle continuous action spaces in chain-of-thought reasoning?
- **Open Question 3**: How does the choice of the base LLM policy (π0) affect the performance and alignment of the target policy (πθ) in OCEAN?
- **Open Question 4**: Can the knowledge graph preference policy (µϕ) be dynamically updated during the alignment process to improve the quality of feedback?

## Limitations

- **Verbalization Fidelity**: The framework relies heavily on GPT-4 to verbalize knowledge graph trajectories into natural language that aligns with LLM reasoning, but minimal empirical validation exists for whether this process preserves critical reasoning structure.
- **Variance Control in Practice**: While theoretical analysis proves a lower bound on KG-IPS variance, the paper doesn't adequately demonstrate that this bound is tight enough in practice for stable optimization.
- **Generalization Across Reasoning Types**: The framework's performance on diverse reasoning types beyond knowledge-intensive and multi-hop reasoning remains untested, potentially limiting its applicability.

## Confidence

- **High Confidence**: The core theoretical framework for KG-IPS estimation and its application to offline policy evaluation is well-established with sound mathematical proofs.
- **Medium Confidence**: Experimental results showing improved performance are convincing but lack detailed variance measurements, ablation studies, and testing across diverse reasoning types.
- **Low Confidence**: The verbalization mechanism's effectiveness and the framework's ability to maintain performance across diverse reasoning types and LLM architectures without significant tuning are not well-established.

## Next Checks

1. **Variance Validation**: Conduct extensive empirical measurements of KG-IPS variance across different reasoning paths, task types, and LLM sizes to assess whether variance control mechanism is effective in practice.

2. **Verbalization Fidelity Analysis**: Perform detailed qualitative and quantitative analysis of verbalized knowledge graph trajectories, comparing semantic content and reasoning structure between original KG triples and their verbalized forms.

3. **Cross-Reasoning Generalization Test**: Evaluate OCEAN's performance on diverse reasoning tasks beyond knowledge-intensive and multi-hop reasoning, including causal reasoning, commonsense reasoning, and analogical reasoning tasks.