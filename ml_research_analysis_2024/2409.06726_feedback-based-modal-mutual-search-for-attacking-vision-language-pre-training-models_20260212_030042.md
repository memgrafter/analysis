---
ver: rpa2
title: Feedback-based Modal Mutual Search for Attacking Vision-Language Pre-training
  Models
arxiv_id: '2409.06726'
source_url: https://arxiv.org/abs/2409.06726
tags:
- adversarial
- attack
- fmms
- examples
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of generating effective adversarial
  examples for vision-language pre-training (VLP) models, where differences in feature
  representation spaces across models hinder transferability. To solve this, the authors
  propose Feedback-based Modal Mutual Search (FMMS), a new attack paradigm that introduces
  a modal mutual loss (MML) to push apart matched image-text pairs while pulling closer
  mismatched pairs, thereby exploring multiple update directions for adversarial examples.
---

# Feedback-based Modal Mutual Search for Attacking Vision-Language Pre-training Models

## Quick Facts
- arXiv ID: 2409.06726
- Source URL: https://arxiv.org/abs/2409.06726
- Authors: Renhua Ding; Xinze Zhang; Xiao Yang; Kun He
- Reference count: 7
- Key outcome: FMMS achieves up to 50% improvement in black-box attack success rates by bridging feature representation gaps through modal mutual loss and target model feedback

## Executive Summary
This paper addresses the critical challenge of adversarial example transferability in vision-language pre-training (VLP) models, where differences in feature representation spaces across models hinder attack effectiveness. The authors propose Feedback-based Modal Mutual Search (FMMS), a novel attack paradigm that introduces modal mutual loss to push apart matched image-text pairs while pulling closer mismatched pairs, thereby exploring multiple update directions for adversarial examples. Additionally, FMMS leverages target model feedback to iteratively refine adversarial examples, effectively bridging the feature representation gap between surrogate and target models. The method is evaluated on Flickr30K and MSCOCO datasets, achieving significantly higher attack success rates compared to state-of-the-art baselines.

## Method Summary
FMMS introduces a modal mutual loss (MML) that simultaneously increases distance between matched image-text pairs and decreases distance between mismatched pairs, creating multiple gradient directions for adversarial example generation. The method performs iterative refinement using target model feedback, constructing focused search spaces based on retrieval rankings (Top-N) to drive adversarial examples into the adversarial region. FMMS operates through multiple rounds of cross-modal interactions, exploring adversarial boundaries more effectively than single-round methods. The approach is evaluated against both fused VLP models (ALBEF, CLIP) and aligned models (VLMo, TCL), demonstrating superior performance in both black-box and white-box settings.

## Key Results
- FMMS achieves up to 50% improvement in attack success rates compared to state-of-the-art baselines in black-box settings
- The method demonstrates superior performance across both fused and aligned VLP model architectures
- FMMS shows consistent improvements on both Flickr30K and MSCOCO datasets, validating its effectiveness across different data distributions
- The approach effectively bridges feature representation gaps that typically hinder adversarial transferability between different VLP models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MML loss creates adversarial examples by pushing matched pairs apart while pulling mismatched pairs together, generating multiple gradient directions for exploration.
- Mechanism: The modal mutual loss (MML) simultaneously increases distance between matched image-text pairs and decreases distance between mismatched pairs, effectively exploring adversarial boundaries in both directions.
- Core assumption: The adversarial region can be more effectively explored by having multiple update directions rather than a single direction of pushing matched pairs apart.
- Evidence anchors:
  - [abstract]: "FMMS introduces a novel modal mutual loss (MML), aiming to push away the matched image-text pairs while randomly drawing mismatched pairs closer in feature space, guiding the update directions of the adversarial examples."
  - [section]: "MML = ( Lt′m, Lvadv , Ltadv ), to explore various update directions through multiple rounds of cross-modal interactions"
  - [corpus]: Weak - no direct corpus evidence for this specific mechanism

### Mechanism 2
- Claim: Target model feedback iteratively refines adversarial examples by narrowing the search space based on retrieval rankings.
- Mechanism: FMMS uses target model output (Top-N rankings) to construct a focused search space for the next iteration, leveraging more precise feedback to drive adversarial examples into the adversarial region.
- Core assumption: The target model's retrieval rankings provide meaningful information about the adversarial region that can be exploited for refinement.
- Evidence anchors:
  - [abstract]: "Additionally, FMMS leverages the target model feedback to iteratively refine adversarial examples, driving them into the adversarial region."
  - [section]: "Top-N search attack selects entries with match rankings from 1 to N to form search space Btr and Bir"
  - [corpus]: Weak - no direct corpus evidence for this specific feedback mechanism

### Mechanism 3
- Claim: Multi-round cross-modal interaction explores adversarial boundaries more effectively than single-round methods.
- Mechanism: By performing multiple rounds of cross-modal interactions guided by target model feedback, FMMS can explore a wider range of potential adversarial examples compared to methods that only perform single-round interactions.
- Core assumption: Multiple rounds of refinement based on target model feedback will converge on more effective adversarial examples than single-round approaches.
- Evidence anchors:
  - [abstract]: "FMMS exploits the target model feedback to perform multiple rounds of cross-modal interactions"
  - [section]: "In this way, we explore potential adversarial examples more effectively compared to the single-round multi-step cross-modal interaction as in SGA"
  - [corpus]: Weak - no direct corpus evidence for this specific multi-round mechanism

## Foundational Learning

- Concept: Vision-Language Pre-training (VLP) Models
  - Why needed here: Understanding the difference between fused and aligned VLP models is crucial for grasping why adversarial examples don't transfer well between different architectures.
  - Quick check question: What is the key architectural difference between fused VLP models like ALBEF and aligned VLP models like CLIP?

- Concept: Adversarial Transferability
  - Why needed here: The paper's core motivation is that adversarial examples don't transfer well between different VLP models due to feature representation differences.
  - Quick check question: Why do adversarial examples generated on fused models typically perform better when attacking other fused models rather than aligned models?

- Concept: Cross-modal Interactions
  - Why needed here: The MML loss and FMMS methodology rely on interactions between image and text modalities to generate effective adversarial examples.
  - Quick check question: How does the MML loss manipulate distances between matched and mismatched image-text pairs to guide adversarial example generation?

## Architecture Onboarding

- Component map:
  Image Encoder (FI) and Text Encoder (FT) -> Modal Mutual Loss (MML) -> Target Model Feedback Processor -> Search Space Constructor -> Adversarial Example Generator

- Critical path:
  1. Generate initial adversarial examples using MML loss
  2. Query target model for feedback (Top-N rankings or hard labels)
  3. Construct focused search space based on feedback
  4. Perform modal mutual search within this space
  5. Iterate until attack success or maximum iterations reached

- Design tradeoffs:
  - Number of iterations vs. computational cost (T=10 chosen as balance)
  - Size of Top-N search space vs. exploitation of target model feedback
  - Perturbation bounds for images (ϵv=2/255) and text (ϵt=1) vs. attack effectiveness
  - Full search vs. Top-N search strategy based on available feedback information

- Failure signatures:
  - ASR plateaus below desired threshold despite increased iterations
  - Attack success rate drops significantly when target model architecture differs from surrogate
  - Search space becomes too small, leading to convergence on ineffective examples
  - Mismatched pairs don't provide useful gradient information

- First 3 experiments:
  1. Compare FMMS (Top-N) against SGA baseline on Flickr30K dataset with ALBEF as surrogate and TCL as target
  2. Test different Top-N values (N=5, 10, 20) to find optimal search space size
  3. Evaluate Full FMMS vs. Top-N FMMS on the same dataset to quantify benefit of focused search space

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness of mismatched pair manipulation relies on untested assumptions about gradient information utility
- Iterative refinement depends on correlation between Top-N rankings and adversarial vulnerability without rigorous validation
- Multi-round cross-modal interaction shows improved performance but lacks isolation of contribution from increased computational budget
- Evaluation primarily limited to two datasets and a small set of VLP models, constraining generalizability claims

## Confidence
**High Confidence**: The empirical results showing FMMS outperforming baselines on the tested datasets and models. The reported attack success rates and quantitative improvements are well-documented and reproducible based on the provided methodology.

**Medium Confidence**: The core claim that MML loss improves adversarial transferability by exploring multiple update directions. While the mechanism is plausible and results support it, the paper lacks ablation studies isolating MML's contribution from other components.

**Low Confidence**: The theoretical justification for why mismatched pair manipulation provides useful gradient information, and the assumption that Top-N rankings from target models reliably indicate adversarial vulnerability. These mechanisms are described but not rigorously validated.

## Next Checks
1. **Ablation Study on MML Components**: Create variants of FMMS that only push matched pairs apart (traditional approach) versus only pull mismatched pairs closer versus the combined MML approach. This would isolate whether the mismatched pair manipulation genuinely contributes to improved transferability.

2. **Target Model Feedback Correlation Analysis**: Systematically vary the Top-N parameter and measure how ranking changes correlate with actual adversarial vulnerability. Additionally, test whether using random Top-N selections versus ranking-based selections shows different performance to validate the feedback mechanism's importance.

3. **Cross-Distribution Transferability Test**: Evaluate FMMS-generated adversarial examples on VLP models trained on different datasets or with different data distributions than the training data. This would test whether the method's effectiveness generalizes beyond the specific Flickr30K and MSCOCO domains used in the evaluation.