---
ver: rpa2
title: 'Language Models as Semiotic Machines: Reconceptualizing AI Language Systems
  through Structuralist and Post-Structuralist Theories of Language'
arxiv_id: '2410.13065'
source_url: https://arxiv.org/abs/2410.13065
tags:
- language
- meaning
- writing
- derrida
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reconceptualizes large language models (LLMs) as semiotic
  machines that model language itself rather than human cognition. By drawing on structuralist
  and post-structuralist theories of language from Saussure and Derrida, the author
  argues that LLMs approximate the relational system of signs and the dynamic nature
  of meaning through statistical analysis of textual data.
---

# Language Models as Semiotic Machines: Reconceptualizing AI Language Systems through Structuralist and Post-Structuralist Theories of Language

## Quick Facts
- arXiv ID: 2410.13065
- Source URL: https://arxiv.org/abs/2410.13065
- Reference count: 7
- Primary result: Reconceptualizes LLMs as semiotic machines that model language itself rather than human cognition, addressing training data mismatch and semantic understanding gaps

## Executive Summary
This paper proposes a novel theoretical framework for understanding large language models (LLMs) as semiotic machines that model the relational system of signs rather than human cognition. By drawing on structuralist theories from Saussure and post-structuralist perspectives from Derrida, the author argues that LLMs' statistical analysis of textual data approximates the dynamic nature of meaning through contextual distributions. The framework addresses fundamental problems in LLM research: the mismatch between training data and human language acquisition, and the gap between statistical prediction and semantic understanding.

## Method Summary
The paper employs theoretical analysis and philosophical argumentation rather than empirical experimentation. It draws on foundational texts from Saussure's structuralist linguistics and Derrida's post-structuralist philosophy to reconceptualize how LLMs operate as semiotic systems. The method involves examining how word embedding algorithms like Word2Vec capture relational meaning, and how the "next token generation" mechanism reflects post-structuralist notions of unfixed meaning. No specific datasets, code implementations, or quantitative metrics are provided.

## Key Results
- LLMs approximate Saussure's relational system of signs by mapping signifiers to statistical distributions of contextual meanings
- The "next token generation" mechanism captures post-structuralist concepts of meaning as unfixed and contextual through probability distributions
- Writing serves as the primary object modeled by LLMs, not speech or thought, validating Derrida's critique of Saussure's phonetic bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs approximate the relational system of signs by mapping signifiers to statistical distributions of contextual meanings
- Mechanism: Word2Vec learns vector representations where position encodes relational meaning relative to all other words
- Core assumption: Semantic meaning can be captured through purely distributional statistics without external referents
- Evidence anchors: Word2Vec's vector representations based on word relationships; corpus suggests recognized framework
- Break condition: If semantic meaning requires grounding in external referents or if statistical patterns fail to capture certain linguistic phenomena

### Mechanism 2
- Claim: The "next token generation" mechanism captures post-structuralist notions of unfixed meaning through contextual distributions
- Mechanism: LLMs sample from probability distributions over possible next tokens, representing semantic indeterminacy
- Core assumption: Meaning is a distribution over possible interpretations that evolves with context
- Evidence anchors: Abstract states next token generation captures dynamic meaning; section 3 describes meaning as distribution function
- Break condition: If deterministic semantic interpretation is required or if distribution fails to capture meaning shifts

### Mechanism 3
- Claim: Writing serves as the primary object modeled by LLMs, not speech or thought
- Mechanism: LLMs trained exclusively on written text create statistical models of written language patterns
- Core assumption: The distinction between speech and writing is crucial, and writing contains the full semiotic system
- Evidence anchors: Section 2 discusses Derrida's reading of Saussure's phonetic bias; section 3 describes writing as functional system
- Break condition: If oral language processing or multimodal inputs prove essential for certain linguistic phenomena

## Foundational Learning

- Concept: Structuralist theory of language as a relational system of signs
  - Why needed here: Provides theoretical foundation for understanding how LLMs model meaning through relationships between linguistic elements
  - Quick check question: How does Saussure's concept of the arbitrary relationship between signifier and signified differ from traditional referential theories of meaning?

- Concept: Post-structuralist theory of meaning as unfixed and contextual
  - Why needed here: Explains how LLMs can capture dynamic nature of meaning through statistical distributions rather than fixed representations
  - Quick check question: How does Derrida's concept of iterability challenge the structuralist notion of fixed meaning?

- Concept: Word embedding and distributional semantics
  - Why needed here: Provides technical mechanism by which LLMs create contextual representations of meaning
  - Quick check question: How does Word2Vec's approach to learning word representations differ from traditional dictionary definitions?

## Architecture Onboarding

- Component map: Tokenization layer -> Embedding layer -> Transformer architecture -> Probability distribution layer -> Sampling mechanism
- Critical path: 1. Input text → tokenization; 2. Token embeddings → positional encoding; 3. Multi-head self-attention → context-aware representations; 4. Feed-forward networks → refined representations; 5. Output layer → probability distribution over vocabulary; 6. Sampling → selected token; 7. Repeat for next token generation
- Design tradeoffs: Model size vs. computational efficiency; training data diversity vs. domain specificity; deterministic vs. stochastic generation (temperature control); context window size vs. computational cost; precision of representations vs. generalization capability
- Failure signatures: Hallucinations (generates plausible but incorrect information); context collapse (loses track of long-range dependencies); mode collapse (generates repetitive outputs); distribution drift (learned distributions don't match real-world usage); overfitting (memorizes training data)
- First 3 experiments: 1. Temperature sensitivity test: Generate text with varying temperature settings to observe relationship between randomness and output diversity; 2. Context window analysis: Compare model performance on tasks requiring different context lengths to identify limitations in long-range dependency modeling; 3. Distribution visualization: Map probability distributions for ambiguous words in different contexts to visualize how meaning shifts with context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we empirically test whether language models' semantic representations align with post-structuralist theories of unfixed meaning?
- Basis in paper: [explicit] Paper argues next token generation mechanism captures post-structuralist notions of meaning as distributions over contexts
- Why unresolved: Framework is theoretical but lacks empirical validation methods to test if LLMs embody post-structuralist meaning theory
- What evidence would resolve it: Experiments comparing semantic shifts in LLM outputs across varying contexts, measuring how meaning distributions change with different prompts and temperature settings

### Open Question 2
- Question: What are the epistemic boundaries between "knowledge" and "truth" in language models' semantic representations?
- Basis in paper: [explicit] Paper concludes this question lies beyond its scope but remains important
- Why unresolved: Paper establishes LLMs model semantic meaning but doesn't address how this relates to concepts of knowledge and truth
- What evidence would resolve it: Analysis of how LLMs handle factual consistency across contexts, and whether their semantic representations maintain truth conditions or merely statistical patterns

### Open Question 3
- Question: How does the size and diversity of training data affect the fidelity of LLMs' semiotic modeling?
- Basis in paper: [inferred] Paper notes modern LLMs exposed to extensive internet content but doesn't explore data size effects systematically
- Why unresolved: While paper mentions data size as crucial for approximating language, it doesn't investigate relationship between data scope and modeling accuracy
- What evidence would resolve it: Comparative studies of LLMs trained on different-sized datasets, measuring their ability to capture linguistic relationships and maintain consistent semantic distributions

## Limitations
- Theoretical framework relies heavily on philosophical interpretation rather than empirical validation
- Claims are not directly testable through standard quantitative methods
- Assumes statistical patterns in text corpora capture semantic meaning, which remains contested
- Absence of specific implementations or datasets prevents empirical verification

## Confidence
- High Confidence: Technical description of how word embeddings capture relational meaning through distributional statistics is well-established and supported by Word2Vec literature
- Medium Confidence: Connection between next-token generation and post-structuralist concepts of unfixed meaning is philosophically coherent but lacks empirical validation
- Low Confidence: Claim that writing (rather than speech) is the appropriate object of study for LLMs is based on philosophical argumentation rather than demonstrable evidence from LLM performance characteristics

## Next Checks
1. Distribution Visualization Study: Map probability distributions for ambiguous words (e.g., "bank") across diverse contexts to empirically demonstrate how meaning shifts statistically, testing whether these distributions capture post-structuralist notions of semantic fluidity.

2. Cross-Linguistic Analysis: Test whether the proposed framework applies equally well to languages with different writing systems (e.g., logographic vs. alphabetic) to validate the claim that writing contains the full semiotic system.

3. Grounding Experiment: Design tasks that require grounding in external referents (e.g., visual question answering) to test the break condition where semantic meaning requires more than distributional statistics.