---
ver: rpa2
title: 'PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual
  Learning'
arxiv_id: '2407.21571'
source_url: https://arxiv.org/abs/2407.21571
tags:
- learning
- pmoe
- arxiv
- experts
- router
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in large language models
  (LLMs) during continual learning, where new information overwrites previously acquired
  knowledge. The proposed solution, PMoE (Progressive Mixture of Experts with Asymmetric
  Transformer), introduces an asymmetric architecture with shallow layers for general
  knowledge and deep layers for new knowledge, progressively adding experts in deep
  layers and using a router for efficient allocation of new knowledge.
---

# PMoE: Progressive Mixture of Experts with Asymmetric Transformer for Continual Learning

## Quick Facts
- arXiv ID: 2407.21571
- Source URL: https://arxiv.org/abs/2407.21571
- Authors: Min Jae Jung; JooHee Kim
- Reference count: 13
- Primary result: PMoE achieves 12.2% backward transfer improvement and outperforms LoRA with replay by 2.2% in general ability and 1.8% in specialization performance

## Executive Summary
PMoE addresses catastrophic forgetting in large language models during continual learning by introducing an asymmetric transformer architecture. The key innovation is separating shallow layers for general knowledge preservation and deep layers for task-specific learning, with progressive expert addition and deep-layer routing. PMoE outperforms state-of-the-art methods on TRACE benchmarks while maintaining parameter efficiency through LoRA adapters.

## Method Summary
PMoE uses an asymmetric transformer where shallow layers (up to threshold τ) maintain general knowledge with a single expert, while deeper layers progressively add task-specific LoRA experts. A router positioned between shallow and deep layers uses deep features for efficient expert allocation. The method incorporates replay memory (1% of historical data) and is trained sequentially on tasks using AdamW optimizer with learning rate 3e-4 and cosine annealing. The architecture is built on Llama2-7b base model.

## Key Results
- Achieves backward transfer score of +12.2% compared to baselines
- Outperforms LoRA with replay by 2.2% in general ability and 1.8% in specialization performance
- Demonstrates effectiveness across 8 distinct tasks in TRACE benchmark
- Maintains parameter efficiency through progressive LoRA expert addition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric depth design preserves general knowledge while enabling efficient specialization for new tasks.
- Mechanism: Shallow layers retain general knowledge with single expert, deep layers add task-specific experts progressively. Separation prevents new task interference with consolidated general knowledge.
- Core assumption: General knowledge adequately represented in shallow layers, new tasks learnable in deep layers without interference.
- Evidence anchors: Abstract states "shallow layers retain general knowledge, while deeper layers acquire new task-specific knowledge"; section describes "asymmetric design, where shallow layers are dedicated to general abilities and deeper layers are tailored for newly acquired abilities."
- Break condition: If general knowledge requires deeper representation or new tasks interfere with general knowledge consolidation.

### Mechanism 2
- Claim: Deep-layer routing outperforms shallow-layer routing for expert allocation.
- Mechanism: Router uses deep features aggregating consolidated information for efficient allocation of new knowledge to appropriate experts.
- Core assumption: Deep features contain better aggregated representations of task-specific information than shallow features.
- Evidence anchors: Abstract mentions "router utilizes deep features aggregating consolidated information"; section hypothesizes "performance of the router is improved when using features from deep layers rather than shallow layers."
- Break condition: If deep features become too specialized or shallow features contain sufficient routing information.

### Mechanism 3
- Claim: Progressive expert addition enables parameter-efficient continual learning without catastrophic forgetting.
- Mechanism: PMoE progressively adds LoRA experts along with incremental tasks, preserving previous knowledge while efficiently allocating new parameters.
- Core assumption: LoRA parameter-efficient fine-tuning can effectively capture new task knowledge without full model retraining.
- Evidence anchors: Abstract states "PMoE incorporates progressively added experts in deep layers"; section explains "progressively add the LoRA along with incremental tasks."
- Break condition: If progressive addition leads to parameter explosion or existing experts cannot preserve previous knowledge adequately.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: PMoE is specifically designed to address catastrophic forgetting in continual learning scenarios.
  - Quick check question: What happens to previously learned knowledge when a neural network is trained on new tasks sequentially?

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: PMoE uses MoE principles with progressive expert addition and routing mechanisms.
  - Quick check question: How does a router determine which experts to activate for a given input in MoE systems?

- Concept: Parameter-efficient fine-tuning techniques (LoRA)
  - Why needed here: PMoE uses LoRA adapters as experts in its architecture.
  - Quick check question: What is the key hypothesis behind LoRA that makes it parameter-efficient for fine-tuning large models?

## Architecture Onboarding

- Component map:
  Pre-trained transformer base (Llama2-7b) -> Shallow layers (τ layers, single LoRA expert) -> Router (linear layer using deep features) -> Deep layers (N-τ layers, progressive multi-expert LoRA) -> Output

- Critical path:
  1. Input passes through shallow layers with general