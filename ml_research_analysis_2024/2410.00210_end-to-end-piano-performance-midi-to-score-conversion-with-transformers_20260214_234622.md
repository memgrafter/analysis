---
ver: rpa2
title: End-to-end Piano Performance-MIDI to Score Conversion with Transformers
arxiv_id: '2410.00210'
source_url: https://arxiv.org/abs/2410.00210
tags:
- data
- music
- score
- notes
- note
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of converting piano performance-MIDI
  files into detailed musical scores. The authors present an end-to-end deep learning
  approach using a transformer-based architecture with a novel tokenized representation
  for symbolic music data.
---

# End-to-end Piano Performance-MIDI to Score Conversion with Transformers

## Quick Facts
- arXiv ID: 2410.00210
- Source URL: https://arxiv.org/abs/2410.00210
- Reference count: 0
- This paper presents the first end-to-end transformer-based approach for converting piano performance-MIDI to detailed musical scores, achieving significant improvements over both deep learning and state-of-the-art HMM-based methods.

## Executive Summary
This paper addresses the challenging task of converting piano performance-MIDI (P-MIDI) files into detailed musical scores with proper notation, including note values, rhythmic structure, staff assignment, and ornaments. The authors propose an end-to-end transformer-based architecture that frames the task as sequence-to-sequence translation rather than note-wise classification, reducing alignment requirements and annotation costs. Their custom tokenization scheme preserves more score information while reducing sequence lengths by 3.5× compared to prior approaches. The method demonstrates superior performance on transcription metrics like MUSTER and is the first to directly predict notational details like trill marks and stem direction from performance data.

## Method Summary
The approach uses a transformer encoder-decoder architecture with a novel custom tokenization scheme for symbolic music data. P-MIDI input is represented as parallel token streams for pitch, onset, duration, and velocity, which are encoded using a Roformer model with rotary positional encodings and SwiGLU activations. The decoder generates parallel token streams for score attributes including pitch, musical onset, duration, measure length, hand, voice, stem, accidental, staccato, trill, and grace notes. A hybrid moj/mlj tokenization scheme represents absolute musical positions to avoid timing drift issues. The model is trained on paired P-MIDI and MusicXML data with beat-level alignment, as well as unpaired MusicXML data using a surrogate pitch conditioning strategy. Data augmentation includes transposition, tempo changes, and jitter.

## Key Results
- Achieves 4.2% improvement in MUSTER score over prior deep learning approaches and 2.3% improvement over complex HMM-based state-of-the-art pipelines
- Reduces sequence lengths by 3.5× compared to prior tokenization approaches while preserving more score information
- First method to directly predict notational details like trill marks, stem direction, and grace notes from performance data
- Demonstrates generalization to modern piano pop music outside the classical training domain

## Why This Works (Mechanism)

### Mechanism 1
Framing PM2S as sequence-to-sequence translation rather than note-wise classification enables the model to predict concise, accurate notation with proper rhythmic structure and barline placement. By modeling the full output score as a sequence to be generated from scratch, the transformer can handle complex musical structures (e.g., trills, tied notes, rests) that don't have a simple one-to-one mapping from performance notes. The input P-MIDI can be adequately represented as a sequence of token streams without requiring explicit alignment to ground truth score notes.

### Mechanism 2
The hybrid tokenization scheme (moj/mlj for musical onsets) enables accurate reconstruction of absolute musical positions while avoiding drift issues common to delta encoding. Using a measure-relative onset token (moj) combined with a measure-length token (mlj) allows the model to correctly place notes relative to barlines without accumulating timing errors over long sequences. The beat-level alignment provided in the training data is sufficiently accurate to enable the model to learn the relationship between moj/mlj tokens and correct musical timing.

### Mechanism 3
Training on unpaired MusicXML data with a surrogate pitch conditioning strategy significantly improves model performance by exposing it to more diverse notation patterns. By feeding the decoder with pitch tokens as surrogate input while masking other attributes, the model learns to predict complete score attributes from pitch information alone, leveraging the vast amount of available unpaired score data. Pitch information contains sufficient signal to predict other score attributes like rhythm, voice assignment, and ornamentation even without performance data.

## Foundational Learning

- Concept: Sequence-to-sequence modeling with transformers
  - Why needed here: The PM2S task requires generating variable-length output sequences (scores) from input sequences (performance MIDI) with complex dependencies between elements.
  - Quick check question: How does the transformer's self-attention mechanism help capture long-range dependencies in musical structure compared to recurrent models?

- Concept: Tokenization and quantization of continuous musical data
  - Why needed here: Musical timing and pitch must be represented as discrete tokens for the transformer while preserving enough precision for accurate notation.
  - Quick check question: Why does the paper use a 24-division quantization for musical durations instead of powers of 2, and what accuracy trade-off does this represent?

- Concept: Handling polyphony and voice separation in symbolic music
  - Why needed here: Piano scores require proper voice assignment and staff placement, which cannot be handled by monophonic transcription approaches.
  - Quick check question: How does the model distinguish between different voices in polyphonic passages, and what score attributes are used to represent this?

## Architecture Onboarding

- Component map: Input tokenization → Encoder processing → Decoder generation → Output tokenization → Score reconstruction
- Critical path: Input tokenization → Encoder processing → Decoder generation → Output tokenization → Score reconstruction
- Design tradeoffs:
  - Sequence length vs. information density: Compound tokens reduce length by 3.5× but require more complex decoding
  - Paired vs. unpaired training: Unpaired data improves performance but requires surrogate conditioning strategy
  - Fixed quantization vs. adaptive: 24-division grid balances accuracy and vocabulary size
- Failure signatures:
  - Timing drift: Incorrect barline placement or measure length tokens
  - Voice confusion: Notes assigned to wrong staff or voice number
  - Missing ornaments: Trills, grace notes, or staccato marks not predicted
  - Rhythm errors: Note durations quantized incorrectly or tied notes not properly handled
- First 3 experiments:
  1. Verify tokenization correctness: Convert a simple P-MIDI file to tokens and back, checking that timing and pitch are preserved within quantization tolerance
  2. Test encoder-decoder basic functionality: Train on a tiny dataset (e.g., 10 examples) with simplified output (just pitch and duration) to verify the model can learn basic mappings
  3. Validate beat alignment preprocessing: Run the greedy beat alignment algorithm on sample data and manually verify that notes are correctly assigned to inter-beat intervals, especially for cases with trills or grace notes

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several remain unresolved based on the content and limitations discussed:

### Open Question 1
How would the proposed transformer-based PM2S system perform on non-piano instruments or polyphonic music with multiple instruments? The paper focuses specifically on piano performance-MIDI to score conversion, noting that "the method generalizes out-of-genre to modern piano pop music" but not addressing other instruments or ensemble music. Experiments applying the same transformer architecture and tokenization scheme to other solo instruments (violin, guitar, etc.) and ensemble music would demonstrate the method's generalizability.

### Open Question 2
What is the upper limit of sequence length the current architecture can handle, and how does performance degrade beyond 512 notes? The paper mentions partitioning inputs into 512-note chunks with 64-note overlap for songs exceeding this length, but doesn't analyze performance degradation at scale. Systematic experiments measuring MUSTER and ScoreSimilarity metrics across different sequence lengths (256, 512, 1024, 2048 notes) would reveal performance trends and practical limits.

### Open Question 3
How would the model's performance change if trained with different proportions of paired vs. unpaired data, particularly when unpaired data significantly exceeds paired data? While the paper demonstrates that unpaired data helps, it only tests relatively modest increases (10,000 unpaired vs. 58,686 available) and doesn't examine scenarios where unpaired data vastly outnumbers paired data, which would be more realistic for many musical genres. Experiments with increasingly imbalanced datasets would show whether performance plateaus, declines, or continues improving with more unpaired data relative to paired data.

## Limitations
- Reliance on beat-level alignment rather than more precise onset alignment may limit handling of highly expressive performances with significant rubato or microtiming variations
- 24-division quantization for durations represents a trade-off between accuracy and vocabulary size, potentially losing some subtle rhythmic nuances
- Surrogate pitch conditioning strategy for unpaired MusicXML data may introduce biases if the model overfits to pitch-only patterns

## Confidence
- **High Confidence**: The core sequence-to-sequence architecture using transformers is well-established and the custom tokenization scheme that reduces sequence lengths by 3.5× while preserving score information is clearly demonstrated through quantitative metrics
- **Medium Confidence**: The claim of being the first to directly predict notational details like trill marks and stem direction from performance data is supported by the absence of such capabilities in prior work, though the evaluation focuses on transcription metrics rather than detailed notation accuracy
- **Low Confidence**: The effectiveness of the hybrid moj/mlj tokenization for preventing timing drift is theoretically sound but lacks extensive empirical validation across diverse performance styles and challenging cases like extreme tempo changes

## Next Checks
1. **Timing drift evaluation**: Create a test set with performances containing extreme rubato and tempo changes, then measure barline placement accuracy and timing drift over long sequences to validate the moj/mlj tokenization's effectiveness
2. **Notation detail accuracy**: Manually annotate a subset of predictions for detailed notation elements (trills, grace notes, stem directions) and compare against ground truth to verify the claim of direct prediction of these attributes
3. **Quantization sensitivity analysis**: Systematically vary the duration quantization divisions (8, 12, 24, 48) and measure the impact on transcription quality to determine the optimal trade-off between accuracy and vocabulary size