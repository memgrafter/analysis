---
ver: rpa2
title: 'The Unreasonable Effectiveness of Open Science in AI: A Replication Study'
arxiv_id: '2412.17859'
source_url: https://arxiv.org/abs/2412.17859
tags:
- data
- code
- reproducibility
- were
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the reproducibility of 30 highly cited
  AI studies by systematically attempting to replicate them. The researchers found
  that 50% of the studies were successfully reproduced (fully or partially), with
  code and data availability strongly correlating with reproducibility.
---

# The Unreasonable Effectiveness of Open Science in AI: A Replication Study

## Quick Facts
- arXiv ID: 2412.17859
- Source URL: https://arxiv.org/abs/2412.17859
- Reference count: 13
- Primary result: 50% of highly cited AI studies successfully reproduced; code+data sharing achieved 86% success rate

## Executive Summary
This replication study investigated the reproducibility of 30 highly cited AI studies from 2012, 2014, and 2016. Researchers systematically attempted to reproduce each study within 40-hour time limits, finding that 50% were successfully reproduced (fully or partially). The study demonstrated that sharing both code and data drastically improves reproducibility, with 86% success for code+data sharing versus 33% for data-only sharing. The quality of data documentation emerged as more critical than code documentation quality, though code sharing alone was sufficient for successful replication. The findings strongly support open science practices in AI research and highlight the importance of proper data documentation.

## Method Summary
The study systematically selected 30 highly cited AI papers (10 from each of 2012, 2014, and 2016) and attempted to reproduce their results within 40 hours per paper. Researchers retrieved code and data when available, classified studies by documentation type (R4 for code+data, R3 for data only), executed experiments using original or reimplemented code, and compared results to original outcomes. All encountered problems were documented and analyzed to identify patterns affecting reproducibility. The study used logistic regression to determine which problem types most strongly predicted reproducibility success or failure.

## Key Results
- 50% of studies successfully reproduced (fully or partially)
- Studies sharing both code and data had 86% success rate, while those sharing only data had 33% success rate
- Quality of data documentation correlated more strongly with reproducibility than code documentation quality
- Code sharing, even when poorly documented, was sufficient for successful replication

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharing both code and data drastically increases reproducibility success rates.
- Mechanism: Code and data availability remove ambiguity in method implementation and experiment setup, allowing independent replication with minimal assumptions.
- Core assumption: Code sharing, even if poorly documented, provides sufficient implementation details for successful replication.
- Evidence anchors:
  - Studies sharing both code and data had an 86% success rate, while those sharing only data had a 33% success rate.
  - Surprisingly, the quality of the code documentation does not correlate with successful replication. Whether the code is poorly documented, partially missing, or not versioned is not important for successful replication, as long as the code is shared.

### Mechanism 2
- Claim: High-quality data documentation is more critical for reproducibility than code documentation quality.
- Mechanism: Clear specification of data preprocessing, partitioning, and versioning ensures that third parties can recreate identical or equivalent datasets, which is essential for matching original results.
- Core assumption: Minor discrepancies in code can be compensated for, but mismatched or undocumented data preprocessing leads to irreproducible results.
- Evidence anchors:
  - The quality of the data documentation correlates with successful replication. Poorly documented or miss-specified data will probably result in unsuccessful replication.
  - Problem types with a TPR of 1.0 include P15 which is a mismatch between the dataset specified and the one found online, P18 is when the dataset description lacks information how it is divided into training, validation and test sets.

### Mechanism 3
- Claim: Sharing code in uninspectable forms (e.g., compiled binaries) correlates with irreproducibility.
- Mechanism: Uninspectable code prevents verification of the exact algorithm and experiment implementation, masking potential errors or deviations from the reported method.
- Core assumption: The availability of inspectable source code is a necessary condition for verifying that the method was implemented as described.
- Evidence anchors:
  - Only when code is shared in an uninspectable manner is it a source of irreproducibility.
  - Problems related to the quality of the documentation of code are not important for successful replication as long as code is shared. Only when code is shared in an uninspectable manner is it a source of irreproducibility.

## Foundational Learning

- Concept: Reproducibility types (R1-R4)
  - Why needed here: Understanding the classification of studies by available documentation (code, data, both) is essential for interpreting the study's findings on reproducibility.
  - Quick check question: What distinguishes an R4 study from an R3 study in terms of reproducibility potential?

- Concept: True Positive Rate (TPR) in reproducibility analysis
  - Why needed here: TPR is used to identify problem types that are most indicative of irreproducibility, guiding where to focus documentation efforts.
  - Quick check question: How does a problem type with a TPR of 1.0 differ in impact from one with a TPR of 0.0 on reproducibility?

- Concept: Logistic regression for feature importance
  - Why needed here: The study uses logistic regression to quantify which problem types most strongly predict reproducibility success or failure, informing best practices.
  - Quick check question: What does a high absolute weight (wi) in the logistic regression indicate about a problem type's impact on reproducibility?

## Architecture Onboarding

- Component map:
  - Data retrieval: Searching for datasets, validating matches, handling preprocessing
  - Code retrieval: Searching for implementations, distinguishing original from third-party, checking for inspectability
  - Experiment execution: Running experiments with provided or reimplemented code, setting seeds, logging all outputs
  - Result classification: Comparing outcomes (identical, consistent, failed) and aggregating to study-level results
  - Problem logging: Documenting all encountered issues and their sources (code, article, data, results, resources)

- Critical path:
  1. Retrieve and validate code and data for each study
  2. Execute experiments (reusing code where available, reimplementing otherwise)
  3. Compare results to original study outcomes
  4. Log all problems and classify reproducibility result
  5. Aggregate results and analyze problem type correlations

- Design tradeoffs:
  - Time vs. completeness: Limiting to 40 hours per study may leave experiments incomplete but allows broader coverage
  - Strict vs. lenient matching: Defining when reproduced results are "identical" vs. "consistent" affects success rates
  - Open vs. closed data: Excluding studies requiring new data collection limits scope but ensures reproducibility via available resources

- Failure signatures:
  - Data mismatch or missing preprocessing steps
  - Code shared only in compiled/uninspectable form
  - Ambiguous or missing experiment parameters
  - Hardware/software requirements beyond available resources

- First 3 experiments:
  1. Attempt to reproduce a study with both code and data shared, focusing on matching all reported results
  2. Reproduce a study with only data shared, reimplementing the code and documenting all encountered issues
  3. Try to reproduce a study with code shared in compiled form, documenting the inability to verify correctness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does mandating code and data sharing for AI research publications significantly improve reproducibility rates across different subfields of AI?
- Basis in paper: The paper concludes that sharing code and data is extremely important for reproducibility and suggests requiring researchers to argue why they cannot share, with poor reasons leading to summary rejection
- Why unresolved: While the paper shows strong correlation between sharing practices and reproducibility, it hasn't tested whether mandatory policies would actually change behavior or improve outcomes across the broader AI community
- What evidence would resolve it: A large-scale study comparing reproducibility rates before and after implementing mandatory code/data sharing policies at major AI conferences, across multiple subfields and venues

### Open Question 2
- Question: How much does the quality of data documentation specifically impact reproducibility compared to code documentation, and what are the most critical elements of data documentation?
- Basis in paper: The study found that data documentation quality correlates more strongly with reproducibility than code documentation quality, but code sharing alone was sufficient for success
- Why unresolved: The study identified this correlation but didn't deeply analyze which specific aspects of data documentation (e.g., preprocessing steps, partitioning methods, metadata) are most critical for reproducibility
- What evidence would resolve it: Controlled experiments varying specific elements of data documentation while keeping other factors constant, measuring their individual impact on reproducibility success rates

### Open Question 3
- Question: What is the reproducibility rate for AI research that relies on large language models or other proprietary systems where neither code nor training data are available?
- Basis in paper: The paper discusses this as a worrisome trend and notes that such research "could be (almost) impossible to independently reproduce," but doesn't provide empirical data on this specific scenario
- Why unresolved: The study focused on 30 highly cited papers from 2012-2016, before the widespread adoption of large proprietary AI systems, so it lacks data on this emerging category of research
- What evidence would resolve it: Systematic attempts to reproduce recent studies using proprietary systems like GPT-4 or Claude, documenting success rates and identifying specific barriers to reproducibility

## Limitations

- The study examined only 30 highly cited AI papers, which may not represent the broader research landscape
- The focus on papers from 2012, 2014, and 2016 limits temporal generalizability to current practices
- The 40-hour time limit per study may have prevented complete reproduction of more complex experiments

## Confidence

- High confidence: The finding that code+data sharing (86% success rate) significantly outperforms data-only sharing (33% success rate)
- Medium confidence: The claim that data documentation quality is more critical than code documentation quality
- Medium confidence: The generalizability of findings to current AI research practices, given the study's focus on papers from 2012-2016

## Next Checks

1. Replicate the study with a larger sample size (e.g., 100+ papers) covering more recent publications to assess temporal trends
2. Conduct a meta-analysis of similar reproducibility studies across different AI subfields to validate the relative importance of code vs. data documentation
3. Implement a controlled experiment comparing reproducibility outcomes when systematically varying levels of code and data documentation quality