---
ver: rpa2
title: Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination
arxiv_id: '2405.18556'
source_url: https://arxiv.org/abs/2405.18556
tags:
- group
- learning
- reward
- policy
- treatment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically reassesses the use of offline reinforcement
  learning (RL) in dynamic treatment regimes (DTRs), particularly in healthcare. Through
  over 17,000 experiments on a sepsis dataset, it demonstrates that RL algorithm performance
  significantly varies with evaluation metrics and reward designs.
---

# Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination

## Quick Facts
- arXiv ID: 2405.18556
- Source URL: https://arxiv.org/abs/2405.18556
- Reference count: 40
- Key outcome: RL performance in DTRs varies significantly with evaluation metrics and reward designs, with simple baselines sometimes outperforming complex algorithms

## Executive Summary
This paper critically examines the use of offline reinforcement learning (RL) in dynamic treatment regimes (DTRs) for healthcare, specifically analyzing sepsis treatment using the MIMIC-III dataset. Through over 17,000 experiments, the authors demonstrate that RL algorithm performance is highly sensitive to evaluation metrics and reward design choices. They show that simple baselines like random policies can outperform RL algorithms under certain policy evaluation methods, raising fundamental questions about current evaluation practices in DTR research.

## Method Summary
The authors conducted extensive experiments on a sepsis dataset, implementing 10 baseline policies including random, zero-drug, max-drug, alternating, weighted, supervised learning, and various RL algorithms (DQN, CQL, IQL, BCQ). They evaluated these policies using multiple off-policy evaluation (OPE) methods (WIS, DR, DM) across three different reward designs, with patient stratification based on NEWS2 risk change rate and variance. The study employed simplified single linear layer models and hyperparameter searches across seeds, learning rates, and batch sizes, totaling 17,550 evaluation experiments.

## Key Results
- RL algorithm performance significantly varies with changes in evaluation metrics and reward designs
- Simple baselines like random policies can outperform RL algorithms under certain policy evaluation methods
- Absence of standardized baselines and evaluation frameworks creates unreliable assessment of RL in DTRs

## Why This Works (Mechanism)

### Mechanism 1
Policy evaluation metrics vary significantly in their assessment of RL algorithms in DTRs, leading to inconsistent conclusions about algorithm effectiveness. Different OPE methods (WIS, DR, DM) weight trajectories and estimate values differently, causing algorithm rankings to shift based on the chosen metric. This creates apparent superiority or inferiority that is metric-dependent rather than algorithm-dependent.

### Mechanism 2
Reward design critically influences RL algorithm performance and evaluation reliability in DTR settings. Sparse or poorly structured rewards (like binary survival/death) make credit assignment difficult, while intermediate rewards (like risk scores) provide more frequent feedback that stabilizes learning and improves evaluation consistency.

### Mechanism 3
Absence of naive baselines and standardized evaluation frameworks leads to unreliable assessment of RL in DTRs. Without comparing against simple baselines (random, supervised learning) and lacking standardized metrics, it's impossible to determine whether complex RL methods provide meaningful improvements over simpler approaches.

## Foundational Learning

- Concept: Off-policy evaluation (OPE) methods and their variance properties
  - Why needed here: Understanding why different OPE metrics produce conflicting results is central to the paper's critique of DTR evaluation
  - Quick check question: What are the key differences between importance sampling, direct method, and doubly robust estimators in how they handle variance and bias?

- Concept: Markov Decision Process (MDP) formulation for medical decision-making
  - Why needed here: The paper shows how different MDP formulations affect RL algorithm performance, so understanding state/action/reward design is crucial
  - Quick check question: How do continuous vs discrete state representations and different reward structures impact RL convergence in clinical settings?

- Concept: Behavioral cloning and supervised learning as baselines
  - Why needed here: The paper demonstrates that SL can outperform RL in some cases, making it essential to understand when and why this occurs
  - Quick check question: Under what conditions would a supervised learning approach to treatment recommendation outperform reinforcement learning?

## Architecture Onboarding

- Component map: Data preprocessing → Patient stratification → MDP formulation → Baseline policies → RL algorithms → OPE methods → Evaluation metrics
- Critical path: 1) Load and preprocess MIMIC-III sepsis data, 2) Stratify patients by NEWS2 risk change rate and variance, 3) Implement MDP with continuous states and discrete actions, 4) Train behavioral policy via supervised learning, 5) Implement multiple RL algorithms (DQN, CQL, BCQ, IQL), 6) Run OPE using WIS, DR, DM methods across different rewards, 7) Compare against naive baselines and supervised learning
- Design tradeoffs: Continuous vs discrete states (preserves information vs simplifies), reward sparsity (clinically intuitive vs easier to learn), OPE variance (unbiased vs low variance)
- Failure signatures: OPE estimates inconsistent across metrics (behavioral policy or value function errors), RL underperforms naive baselines (reward design issues or insufficient exploration), stratification doesn't balance outcomes (data selection bias)
- First 3 experiments: 1) Compare WIS, DR, and DM estimates on the same RL policy using outcome reward to observe metric variance, 2) Run random policy through all OPE methods to establish baseline sanity checks, 3) Train supervised learning policy and compare against RL using multiple evaluation metrics

## Open Questions the Paper Calls Out

### Open Question 1
What are the optimal policy evaluation methods for DTRs that balance accuracy and robustness across diverse patient populations and reward structures? The paper demonstrates that different policy evaluation methods (WIS, DR, DM) perform inconsistently across various reward designs and patient subgroups, with some naive baselines outperforming complex RL algorithms under certain metrics.

### Open Question 2
How can reward function design in DTRs be optimized to improve both clinical meaningfulness and reinforcement learning algorithm performance? The paper shows that RL algorithms' performance varies dramatically with different reward structures (outcome-based, SOFA-based, NEWS2-based), with some rewards leading to better algorithm performance while others create learning challenges.

### Open Question 3
What are the most effective baseline policies for DTR evaluation that provide meaningful benchmarks while accounting for clinical complexity? The paper finds that naive baselines (random, zero-drug, max-drug, alternating) can outperform RL algorithms under certain evaluation metrics, raising concerns about current benchmarking practices.

## Limitations

- Findings primarily based on a single sepsis dataset, limiting generalizability to other clinical domains
- Analysis doesn't fully explore how dataset size, patient heterogeneity, or treatment complexity might affect RL performance
- Focuses on specific OPE methods without examining alternative evaluation frameworks that might be more robust

## Confidence

- High confidence: Core finding that evaluation metric choice significantly affects RL algorithm rankings
- Medium confidence: Claim that simple baselines can outperform complex RL methods
- Medium confidence: Critique of reward design impact
- Low confidence: Broader generalizability claims without testing across multiple disease contexts

## Next Checks

1. Cross-domain validation: Replicate the core experiments on at least two additional clinical datasets (e.g., diabetes management, ventilator weaning) to test generalizability of the evaluation challenges
2. OPE method comparison: Systematically compare the identified issues across additional OPE methods including marginalized importance sampling and sequential importance sampling to determine if the problems are method-specific
3. Baseline sensitivity analysis: Conduct controlled experiments varying baseline complexity (e.g., k-nearest neighbors, decision trees) to establish a spectrum of performance expectations and identify where RL methods become necessary