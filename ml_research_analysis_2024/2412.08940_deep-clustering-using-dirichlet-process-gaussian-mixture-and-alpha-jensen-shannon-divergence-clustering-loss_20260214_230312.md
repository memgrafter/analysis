---
ver: rpa2
title: Deep Clustering using Dirichlet Process Gaussian Mixture and Alpha Jensen-Shannon
  Divergence Clustering Loss
arxiv_id: '2412.08940'
source_url: https://arxiv.org/abs/2412.08940
tags:
- clustering
- deep
- space
- cluster
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two fundamental limitations in existing deep
  clustering methods: the use of asymmetric Kullback-Leibler divergence as a clustering
  loss function, and the requirement for prior knowledge of the number of clusters.
  The authors propose two main improvements: (1) replacing KL divergence with Jensen-Shannon
  divergence, specifically using a closed-form variant called alpha-JS divergence
  to overcome the asymmetry problem, and (2) introducing Dirichlet Process Gaussian
  Mixture Models to enable infinite cluster representation for joint clustering and
  model selection in the latent space.'
---

# Deep Clustering using Dirichlet Process Gaussian Mixture and Alpha Jensen-Shannon Divergence Clustering Loss

## Quick Facts
- arXiv ID: 2412.08940
- Source URL: https://arxiv.org/abs/2412.08940
- Authors: Kart-Leong Lim
- Reference count: 28
- Primary result: Proposed method achieves state-of-the-art accuracy on MIT67 (67 classes) and CIFAR100 (100 classes) unsupervised image clustering

## Executive Summary
This paper addresses two fundamental limitations in deep clustering: asymmetric KL divergence and the need for pre-specified cluster numbers. The authors propose using α-Jensen-Shannon divergence (α-JS) as a symmetric alternative to KL divergence and introduce Dirichlet Process Gaussian Mixture Models (DPGMM) for automatic model selection. The method enables deep clustering without prior knowledge of the number of clusters, demonstrating superior performance on large-scale image datasets compared to existing approaches.

## Method Summary
The method combines a variational autoencoder with DPGMM and α-JS divergence loss. During training, the encoder maps inputs to a latent space, where α-JS divergence measures similarity between the latent distribution and the DPGMM. Variational inference updates DPGMM parameters, allowing the model to automatically determine the optimal number of clusters. The two-stage training process first optimizes reconstruction, then jointly optimizes clustering and reconstruction using the combined loss.

## Key Results
- Outperforms traditional variational Bayes models and existing deep clustering approaches
- Achieves state-of-the-art accuracy on MIT67 (67 classes) and CIFAR100 (100 classes)
- Automatically determines optimal number of clusters during training without requiring prior knowledge
- Demonstrated effectiveness on large-scale datasets with 67 and 100 classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Alpha-JS divergence provides a closed-form solution for comparing Gaussian distributions while avoiding KL divergence's asymmetry.
- Mechanism: The skew parameter α allows Jensen-Shannon divergence to maintain symmetry between two Gaussian distributions by weighting their means and variances appropriately.
- Core assumption: The skew parameter α can be tuned to balance between the two distributions without introducing bias.
- Evidence anchors:
  - [abstract] "specifically using a closed form variant"
  - [section] "we consider another form of JSD which we called the αJSD: In Nielsen [17], the use of a skew parameter α allows the JSD to possess a closed form solution between two Gaussians"
  - [corpus] Weak - no direct mention of alpha-JS divergence in related papers
- Break condition: When α is poorly tuned, leading to one distribution dominating the divergence calculation.

### Mechanism 2
- Claim: Dirichlet Process Gaussian Mixture Models enable automatic determination of optimal cluster number during training.
- Mechanism: DPM uses an infinite cluster representation where cluster weights follow a Dirichlet distribution, allowing the model to prune redundant clusters and converge to the true number of clusters.
- Core assumption: The Dirichlet process prior can effectively model the decay distribution of cluster weights.
- Evidence anchors:
  - [abstract] "introducing Dirichlet Process Gaussian Mixture Models to enable infinite cluster representation"
  - [section] "We refer to the cluster weightage, v = {vk}T k=1 ∈ RZ in DPM, which is responsible for model selection"
  - [corpus] Weak - related papers mention Dirichlet processes but not in the context of deep clustering
- Break condition: When the truncation level T is set too low, preventing the model from discovering the true number of clusters.

### Mechanism 3
- Claim: Joint optimization of deep clustering and model selection through Type II regularizer improves clustering accuracy.
- Mechanism: The αJSD loss function regularizes the autoencoder's latent space to mimic GMM-like partitioning while simultaneously determining the optimal number of clusters through DPM.
- Core assumption: The autoencoder's latent space can be effectively regularized to exhibit clustering structure.
- Evidence anchors:
  - [abstract] "joint clustering and model selection in the latent space"
  - [section] "The second term or the Type II regularizer tries to enforce the latent space to exhibit a Kmeans like partitioning"
  - [corpus] Weak - related papers discuss clustering but not joint model selection with deep learning
- Break condition: When the reconstruction loss dominates the clustering loss, preventing the latent space from developing clear cluster boundaries.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) architecture
  - Why needed here: The paper builds upon VAE framework to create a clustering-friendly latent space
  - Quick check question: What is the key difference between a standard autoencoder and a variational autoencoder?

- Concept: Dirichlet Process mixture models
  - Why needed here: DPM provides the infinite cluster representation that enables automatic model selection
  - Quick check question: How does a Dirichlet Process differ from a standard Gaussian Mixture Model in terms of cluster representation?

- Concept: Jensen-Shannon divergence and its properties
  - Why needed here: JS divergence provides a symmetric alternative to KL divergence for comparing probability distributions
  - Quick check question: Why is symmetry important when comparing two probability distributions in clustering?

## Architecture Onboarding

- Component map:
  Encoder network (ResNet18 feature extractor + custom layers) -> αJSD loss computation module -> DPM inference module (variational inference for posterior updates) -> Cluster assignment module -> Decoder network (for reconstruction)

- Critical path:
  1. Forward pass through encoder to get latent representation
  2. Compute αJSD between latent representation and DPM
  3. Update DPM parameters using variational inference
  4. Compute reconstruction loss
  5. Backpropagate combined loss

- Design tradeoffs:
  - Using pretrained ResNet18 vs end-to-end training: Trade accuracy for computational efficiency
  - Fixed truncation level T vs dynamic adjustment: Simplicity vs potential for better model selection
  - α parameter choice: Balance between symmetry and gradient magnitude

- Failure signatures:
  - Cluster assignment oscillates during training (indicates poor α tuning)
  - Reconstruction loss decreases but clustering accuracy plateaus (indicates loss imbalance)
  - All cluster weights converge to similar values (indicates poor DPM initialization)

- First 3 experiments:
  1. Verify αJSD implementation produces symmetric values for swapped inputs
  2. Test DPM inference with synthetic data to confirm cluster pruning behavior
  3. Run ablation study comparing αJSD vs KL divergence on a simple dataset like MNIST

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of α in αJSD affect the clustering performance across different datasets, and what is the optimal range for this parameter?
- Basis in paper: [explicit] The paper discusses using α = 0.65 to enable more biasing towards DPM for learning network weights, but does not explore the sensitivity of α on clustering performance.
- Why unresolved: The paper does not provide a systematic study of how varying α impacts clustering accuracy or the convergence of the number of clusters.
- What evidence would resolve it: Experiments showing clustering accuracy and estimated cluster numbers for a range of α values across multiple datasets.

### Open Question 2
- Question: Can the deep model selection approach be extended to other deep learning architectures beyond autoencoders, such as convolutional neural networks or generative adversarial networks?
- Basis in paper: [inferred] The paper focuses on autoencoder-based deep clustering and mentions other architectures (GANs, CNNs) in related work but does not explore their integration with Dirichlet Process Gaussian Mixture Models.
- Why unresolved: The methodology is demonstrated only for autoencoders, leaving open whether the approach generalizes to other network types.
- What evidence would resolve it: Experiments applying the αJSD and DPM framework to CNNs or GANs for unsupervised clustering tasks.

### Open Question 3
- Question: How does the truncation level T (maximum number of clusters) affect the performance and convergence of the deep model selection method, and is there a principled way to set this hyperparameter?
- Basis in paper: [explicit] The paper sets T = 200, 100, and 50 for different datasets but does not discuss the impact of this choice or provide guidance on selecting T.
- Why unresolved: The choice of T appears arbitrary and may influence both clustering accuracy and computational efficiency.
- What evidence would resolve it: A study showing clustering performance and convergence behavior for different truncation levels on various datasets.

### Open Question 4
- Question: Can the neighborhood information of images be effectively incorporated into the latent space learning to improve clustering performance, and what regularization techniques would be most effective?
- Basis in paper: [explicit] The conclusion mentions this as a possible future work, indicating that the current method does not exploit neighborhood information.
- Why unresolved: The paper focuses on global clustering in the latent space without considering local structure or neighborhood relationships.
- What evidence would resolve it: Experiments incorporating different neighborhood-based regularizations (e.g., graph-based, contrastive learning) and comparing their impact on clustering accuracy.

## Limitations
- The choice of α=0.65 appears arbitrary without systematic sensitivity analysis
- Experimental validation is limited to image datasets, leaving uncertainty about generalization to other domains
- The claim of truly automatic cluster number determination is only partially supported due to required truncation levels

## Confidence

**High confidence**: The theoretical framework for replacing KL divergence with α-JS divergence is well-established in the literature

**Medium confidence**: The experimental results show improvement over baselines, but the comparison scope is limited to specific datasets and methods

**Low confidence**: The claim that the method automatically determines optimal cluster numbers is only partially supported, as truncation levels (T) are still required as hyperparameters

## Next Checks

1. **α parameter sensitivity analysis**: Systematically vary α from 0.1 to 0.9 to determine its impact on clustering accuracy and verify the claimed improvement over KL divergence across the full parameter range.

2. **Cross-domain generalization**: Test the proposed method on non-image datasets (e.g., text or sensor data) to validate claims about general applicability beyond computer vision tasks.

3. **Truncation level impact study**: Conduct experiments varying T across different orders of magnitude to quantify its effect on model selection accuracy and determine whether truly infinite cluster representation is achieved.