---
ver: rpa2
title: 'Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input'
arxiv_id: '2408.15542'
source_url: https://arxiv.org/abs/2408.15542
tags:
- video
- arxiv
- videos
- long
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Kangaroo is a video-language model designed to handle long-context
  video inputs. It addresses challenges in video understanding due to insufficient
  high-quality training data and excessive visual feature compression.
---

# Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input

## Quick Facts
- arXiv ID: 2408.15542
- Source URL: https://arxiv.org/abs/2408.15542
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on video understanding benchmarks by handling long-context video inputs with high-resolution and extensive frame counts

## Executive Summary
Kangaroo is a video-language model designed to address limitations in processing long videos due to insufficient high-quality training data and excessive visual feature compression. It introduces a comprehensive data curation system to generate large-scale, high-quality datasets and employs a curriculum training strategy with gradually increasing resolution and frame count. Kangaroo achieves state-of-the-art performance across various video understanding benchmarks, excelling in long-video tasks and outperforming larger models on specialized benchmarks.

## Method Summary
Kangaroo employs a curriculum training pipeline that progressively increases video resolution and frame count to handle long-context inputs. The model uses a vision encoder (EVA-CLIP-L), a spatial-temporal patchify module, a multi-modal projector, and an LLM (Llama-3-8B-Instruct). Training begins with image-text pre-training, followed by video-text pre-training, pre-training refinement, instruction tuning, and long video tuning. The approach also includes a data curation system that filters low-quality videos and generates high-quality captions using multi-modal models, resulting in a large-scale dataset with better video-language alignment.

## Key Results
- Achieves state-of-the-art performance across multiple video understanding benchmarks including MVBench, MLVU, and MMBench-Video
- Excels in long-video tasks, outperforming larger models on specialized benchmarks like EgoSchema and TempCompass
- Demonstrates superior understanding of both local details and global context in long videos through high-resolution inputs and extensive frame counts

## Why This Works (Mechanism)

### Mechanism 1
- High-quality data curation overcomes the scarcity of training data for long video understanding
- The paper introduces a data curation system that filters low-quality videos and generates high-quality captions using multi-modal models, resulting in a large-scale dataset with better video-language alignment
- Core assumption: High-quality captions improve the model's ability to learn meaningful visual-language correlations
- Evidence anchors: [abstract] "Due to insufficient access to large-scale high-quality video data and the excessive compression of visual features, current methods exhibit limitations in effectively processing long videos." [section III.B] Describes the video filtering mechanisms including text coverage, face coverage, and static scene filters to remove low-quality videos. [section III.C] Details the video caption generation pipeline using frame-level and video-level captioners
- Break condition: If the filtering criteria are too strict, the dataset may become too small to cover diverse scenarios, limiting the model's generalization

### Mechanism 2
- Curriculum training with gradually increasing complexity enables the model to handle long-context videos
- The model is trained in stages, starting with image-text pre-training, followed by video-text pre-training, then increasing input resolution and frame count progressively, and finally fine-tuning on long videos
- Core assumption: Gradually increasing task difficulty allows the model to build foundational understanding before tackling complex long video inputs
- Evidence anchors: [abstract] "we design a curriculum training pipeline with gradually increasing resolution and number of input frames to accommodate long videos." [section IV.B] Describes the four-stage curriculum training strategy with increasing frame counts and resolution. [section IV.B, Stage III] Mentions dynamic frame sampling and sequence packing to handle varying video lengths
- Break condition: If the curriculum progression is too steep, the model may not converge or may overfit to specific video lengths

### Mechanism 3
- High-resolution inputs and extensive frame counts preserve visual details for long video understanding
- By increasing the resolution to 448x448 and extending the maximum number of input frames to 160, the model retains more visual information, which is crucial for understanding long videos
- Core assumption: More visual tokens lead to better perception of both local details and global context in videos
- Evidence anchors: [abstract] "we increase the resolution and number of input frames to improve the perception of global context and visual details in long videos." [section IV.A] Describes the use of high-resolution frames and temporal position embeddings. [section IV.B, Stage III] Specifies the increase in frame count to 64 and context length to 10K tokens during instruction tuning
- Break condition: If the computational cost becomes prohibitive, the model may not be practical for real-world applications

## Foundational Learning

- **Curriculum Learning**
  - Why needed here: Gradually increasing task difficulty helps the model build foundational understanding before tackling complex long video inputs
  - Quick check question: Why is it beneficial to start with image-text pre-training before video-text pre-training?

- **Data Filtering and Curation**
  - Why needed here: High-quality training data is essential for effective video-language alignment, especially for long videos
  - Quick check question: What are the consequences of not filtering out low-quality videos during dataset curation?

- **Temporal Position Embedding**
  - Why needed here: Encoding the temporal order of frames is crucial for understanding the sequence of events in a video
  - Quick check question: How does the use of actual timestamps instead of frame indices affect the model's understanding of video content?

## Architecture Onboarding

- **Component map:**
  - Video frames → Vision Encoder (EVA-CLIP-L) → Spatial-Temporal Patchify Module → Multi-Modal Projector → LLM (Llama-3-8B-Instruct)
  - Temporal Position Embedding: Encodes the temporal order of frames

- **Critical path:** Video frames → Vision Encoder → Spatial-Temporal Patchify Module → Multi-Modal Projector → LLM

- **Design tradeoffs:**
  - High resolution and frame count improve understanding but increase computational cost
  - Aggressive visual token compression reduces computational load but may lose critical information
  - Curriculum training ensures gradual learning but requires careful tuning of progression stages

- **Failure signatures:**
  - Poor performance on long videos: May indicate insufficient visual token retention or inadequate curriculum progression
  - Overfitting to specific video lengths: Could result from imbalanced training data or inappropriate frame sampling
  - High computational cost: Suggests the need for optimization in the patchify module or model scaling

- **First 3 experiments:**
  1. Test the impact of different frame sampling strategies on model performance
  2. Evaluate the effect of varying the number of input frames on understanding short vs. long videos
  3. Assess the model's performance when the patchify module is removed to observe the impact on visual token retention

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of Kangaroo compare when trained on datasets of varying quality and quantity?
- **Basis in paper:** [inferred] The paper mentions that the scarcity of high-quality video datasets limits current methods' performance. Kangaroo addresses this by curating a large-scale dataset with high-quality annotations.
- **Why unresolved:** The paper does not provide a direct comparison of Kangaroo's performance when trained on different dataset qualities and sizes.
- **What evidence would resolve it:** Experiments comparing Kangaroo's performance when trained on datasets of varying quality and quantity would provide insights into the impact of dataset curation on model performance.

### Open Question 2
- **Question:** What is the optimal balance between spatial and temporal resolution for video understanding tasks?
- **Basis in paper:** [explicit] The paper mentions increasing the resolution and number of input frames to improve perception of global context and visual details in long videos.
- **Why unresolved:** The paper does not explore the trade-offs between spatial and temporal resolution in detail.
- **What evidence would resolve it:** Systematic experiments varying spatial and temporal resolutions independently and jointly would help identify the optimal balance for different video understanding tasks.

### Open Question 3
- **Question:** How does Kangaroo's performance scale with increasing model size and computational resources?
- **Basis in paper:** [inferred] The paper presents results for an 8B parameter model, but does not explore the impact of scaling up the model size.
- **Why unresolved:** The paper does not provide a comprehensive analysis of how Kangaroo's performance changes with model size and computational resources.
- **What evidence would resolve it:** Training and evaluating Kangaroo with different model sizes and computational budgets would provide insights into the scalability of the approach.

## Limitations

- The computational overhead of processing high-resolution videos with extended frame counts remains a significant concern for practical deployment
- The curriculum training strategy's sensitivity to progression parameters may lead to inconsistent results if not carefully tuned
- The quality and generalizability of the curated datasets to real-world scenarios remain uncertain despite rigorous filtering

## Confidence

**High Confidence:**
- Kangaroo achieves state-of-the-art performance across various video understanding benchmarks
- The data curation system significantly improves the quality of training data compared to previous approaches

**Medium Confidence:**
- The curriculum training strategy effectively enables the model to handle long-context videos
- High-resolution inputs and extensive frame counts preserve visual details crucial for long video understanding

**Low Confidence:**
- Kangaroo outperforms larger models on specialized benchmarks without clear explanation of why this occurs
- The practical deployment feasibility of Kangaroo in real-world applications with limited computational resources

## Next Checks

1. **Dataset Quality Validation:** Conduct a thorough human evaluation of the curated datasets to assess the actual quality of filtered videos and generated captions. Compare the diversity and representativeness of the curated dataset against the original raw datasets to identify potential biases or gaps.

2. **Computational Efficiency Benchmark:** Measure the actual computational requirements (FLOPs, memory usage, inference time) of Kangaroo when processing long videos at high resolution. Compare these metrics against baseline models to quantify the trade-off between performance gains and computational overhead.

3. **Curriculum Sensitivity Analysis:** Systematically vary the progression stages in the curriculum training (e.g., frame count increments, resolution increases) and measure the impact on model performance. Identify the optimal progression parameters and determine the sensitivity of performance to deviations from these parameters.