---
ver: rpa2
title: In-context learning agents are asymmetric belief updaters
arxiv_id: '2402.03969'
source_url: https://arxiv.org/abs/2402.03969
tags:
- learning
- machine
- in-context
- trials
- blocks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined in-context learning in large language models
  (LLMs) using two-alternative forced choice (2AFC) tasks adapted from cognitive psychology.
  The research revealed that LLMs update their beliefs asymmetrically, learning more
  from better-than-expected outcomes than from worse-than-expected ones, a phenomenon
  known as optimism bias.
---

# In-context learning agents are asymmetric belief updaters

## Quick Facts
- arXiv ID: 2402.03969
- Source URL: https://arxiv.org/abs/2402.03969
- Reference count: 38
- Key finding: LLMs exhibit asymmetric belief updating, learning more from better-than-expected outcomes than worse-than-expected ones

## Executive Summary
This study investigates in-context learning in large language models using two-alternative forced choice (2AFC) tasks adapted from cognitive psychology. The research reveals that LLMs demonstrate optimism bias in their belief updating, learning more from positive outcomes than negative ones. This asymmetric learning pattern is highly sensitive to problem framing, with the bias reversing or disappearing under different experimental conditions. The findings suggest that in-context learning in LLMs is not purely statistical but influenced by cognitive-like mechanisms similar to those observed in human decision-making.

## Method Summary
The study employed two-alternative forced choice (2AFC) tasks adapted from cognitive psychology to examine in-context learning in LLMs. Researchers systematically varied problem framing to observe how different presentations affected belief updating patterns. To validate the empirical findings, they derived idealized in-context learning agents through meta-reinforcement learning, creating a theoretical framework to explain the observed asymmetric learning behavior. The experiments tested various scenarios including counterfactual feedback and agency implications to understand the boundary conditions of the optimism bias.

## Key Results
- LLMs demonstrate asymmetric belief updating, learning more from better-than-expected outcomes than worse-than-expected ones
- The optimism bias reverses when learning about counterfactual feedback and disappears when no agency is implied
- Idealized in-context learning agents derived through meta-reinforcement learning exhibit similar asymmetric updating patterns
- Problem framing significantly influences the direction and magnitude of belief updating in LLMs

## Why This Works (Mechanism)
The asymmetric belief updating observed in LLMs appears to stem from their in-context learning mechanism, which relies on pattern matching and adaptation within the context window rather than traditional parameter updates. When LLMs encounter outcomes that better than expected, they update their internal probability distributions more significantly because these outcomes represent larger deviations from their prior expectations. This mechanism is particularly pronounced in 2AFC tasks where the binary nature of outcomes creates clear expectation violations. The meta-reinforcement learning approach reveals that this behavior can be understood as an optimal strategy for certain types of decision problems where asymmetric updating provides computational efficiency or better long-term performance.

## Foundational Learning
- **Two-alternative forced choice (2AFC)**: A decision-making paradigm where subjects must choose between two options, providing clear binary outcomes for studying learning patterns. Why needed: Creates unambiguous expectation violations for measuring asymmetric belief updating. Quick check: Ensure tasks have clear correct/incorrect outcomes with measurable confidence.
- **Optimism bias in learning**: The tendency to update beliefs more strongly in response to positive outcomes than negative ones. Why needed: Provides the theoretical framework for understanding asymmetric belief updating. Quick check: Compare magnitude of updates for positive vs negative prediction errors.
- **Problem framing effects**: How the presentation of information influences cognitive processing and decision-making. Why needed: Explains why the same underlying learning mechanism produces different behaviors under different contexts. Quick check: Test identical learning tasks with varied contextual framing.
- **Meta-reinforcement learning**: A framework where agents learn how to learn, optimizing the learning process itself. Why needed: Provides theoretical justification for why asymmetric updating might be optimal. Quick check: Verify that derived learning rules match observed LLM behavior.

## Architecture Onboarding

**Component map:**
User query → LLM context processing → 2AFC decision → Outcome presentation → Belief updating → New context

**Critical path:**
The critical path for asymmetric belief updating flows from outcome presentation through the LLM's in-context learning mechanism. When outcomes are presented, the model updates its internal probability distribution based on the magnitude of expectation violation. Better-than-expected outcomes trigger larger updates because they represent more significant deviations from the model's prior. This updating occurs within the context window through pattern matching rather than parameter adjustment, making it sensitive to the specific framing of the outcome information.

**Design tradeoffs:**
The in-context learning approach trades computational efficiency for adaptability. Rather than fine-tuning parameters for each new task, the model relies on its pretraining to recognize patterns and adapt within the context window. This allows rapid learning but introduces sensitivity to problem framing and context presentation. The tradeoff manifests as the observed optimism bias - while potentially suboptimal for some tasks, asymmetric updating may be computationally efficient or optimal for others, as suggested by the meta-reinforcement learning analysis.

**Failure signatures:**
- Optimism bias reverses or disappears under different problem framings
- Learning performance degrades when agency implications are removed
- Asymmetric updating patterns break down in complex, multi-step decision tasks
- The model shows reduced learning when outcomes are presented in counterfactual contexts

**First experiments to run:**
1. Test the same 2AFC tasks with explicit probability distributions provided to the model
2. Vary the temporal spacing between decisions to measure learning decay
3. Introduce multi-step decision chains to assess how asymmetric updating compounds over time

## Open Questions the Paper Calls Out
None

## Limitations
- The optimism bias is highly sensitive to problem framing, raising questions about generalizability across diverse task domains
- The 2AFC paradigm may not fully capture the complexity of real-world decision-making scenarios that LLMs encounter
- The meta-reinforcement learning approach relies on simplifying assumptions that may not translate perfectly to actual LLM behavior

## Confidence
- Asymmetric belief updating in LLMs: Medium
- Optimism bias represents a fundamental characteristic of LLM reasoning: Low
- Asymmetric updating may be a rational response to certain problems: Low

## Next Checks
1. Test the asymmetric belief updating pattern across a broader range of task types and domains to assess generalizability beyond the 2AFC paradigm
2. Conduct ablation studies to isolate the specific components of problem framing that influence the direction and magnitude of the optimism bias
3. Compare the observed bias patterns in LLMs with human cognitive biases using identical experimental paradigms to better understand the relationship between artificial and biological learning systems