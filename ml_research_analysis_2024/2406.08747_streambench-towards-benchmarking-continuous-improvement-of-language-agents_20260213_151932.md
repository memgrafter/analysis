---
ver: rpa2
title: 'StreamBench: Towards Benchmarking Continuous Improvement of Language Agents'
arxiv_id: '2406.08747'
source_url: https://arxiv.org/abs/2406.08747
tags:
- agents
- performance
- agent
- streaming
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StreamBench, the first benchmark designed
  to evaluate the continuous improvement of large language model (LLM) agents over
  an input-feedback sequence in an online setting. The benchmark simulates a streaming
  environment where agents receive feedback and iteratively enhance their performance
  across diverse tasks like text-to-SQL, programming, tool use, medical diagnosis,
  and question answering.
---

# StreamBench: Towards Benchmarking Continuous Improvement of Language Agents

## Quick Facts
- arXiv ID: 2406.08747
- Source URL: https://arxiv.org/abs/2406.08747
- Reference count: 40
- Key outcome: Introduces StreamBench, the first benchmark for evaluating LLM agent improvement over streaming feedback sequences

## Executive Summary
This paper introduces StreamBench, the first benchmark designed to evaluate continuous improvement of large language model (LLM) agents in streaming scenarios where agents receive feedback and iteratively enhance their performance. The benchmark simulates online environments across diverse tasks including text-to-SQL, programming, tool use, medical diagnosis, and question answering. The authors propose several streaming baselines, with their cost-effective multi-agent method (MAM-StreamICL) outperforming other approaches while maintaining the average cost of a single agent.

## Method Summary
The method evaluates LLM agents in streaming scenarios where they process sequences of input-feedback pairs. The benchmark uses binary correctness feedback to guide improvement, with agents updating their components (prompts, memory, retrievers) based on feedback from previous instances. Key streaming baselines include GrowPrompt (dynamically updates prompt with correct examples), MemPrompt (uses retrieved examples from memory), Self-StreamICL (stores only correct self-outputs for future use), and MAM-StreamICL (multi-agent method sharing memory across agents). The benchmark covers five diverse datasets: Spider and CoSQL (text-to-SQL), BIRD (Python programming), ToolBench (tool use), DDXPlus (medical diagnosis), and HotpotQA (question answering).

## Key Results
- Streaming methods outperform non-streaming methods across all evaluated tasks and models
- MAM-StreamICL achieves the best performance while maintaining cost similar to a single agent
- Storing only correct self-generated outputs consistently boosts performance over zero-shot baselines
- Different LLM agents possess distinct strengths and weaknesses, benefiting from shared memory experiences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using only correct self-generated outputs in streaming improves LLM agent performance.
- **Mechanism:** Storing only correct (input, output) pairs in memory allows agents to learn effective patterns without being distracted by incorrect examples.
- **Core assumption:** The LLM can generalize from correct examples to improve future performance.
- **Evidence anchors:** Key findings include the importance of collecting correct self-generated outputs for improvement; using only correct self-output consistently boosts performance.
- **Break condition:** If tasks require learning from mistakes or if incorrect examples contain valuable contrastive information.

### Mechanism 2
- **Claim:** Sharing memory across multiple agents improves performance while maintaining cost efficiency.
- **Mechanism:** Different agents with distinct strengths benefit from each other's correct outputs through shared memory, with round-robin scheduling maintaining single-agent cost levels.
- **Core assumption:** Different agents possess complementary capabilities that can be leveraged through memory sharing.
- **Evidence anchors:** MAM-StreamICL benefits from sharing memory across multiple agents; each agent takes turn solving incoming inputs while benefiting from others.
- **Break condition:** If agents have highly overlapping capabilities or if coordination costs exceed benefits.

### Mechanism 3
- **Claim:** Streaming methods outperform non-streaming methods by leveraging information from past instances.
- **Mechanism:** Agents update their components based on feedback from previous instances, allowing adaptation and improvement over time rather than treating each instance independently.
- **Core assumption:** Information from past instances contains patterns that can be generalized to improve future performance.
- **Evidence anchors:** Streaming methods outperform non-streaming methods, though improvement extent varies across datasets.
- **Break condition:** If streaming sequences contain highly diverse tasks with no transferable patterns.

## Foundational Learning

- **Concept:** In-context learning (ICL)
  - Why needed here: Streaming methods like Self-StreamICL rely on using past correct examples as demonstrations for future predictions
  - Quick check question: Can you explain how in-context learning differs from traditional fine-tuning and why it's particularly suited for streaming scenarios?

- **Concept:** Feedback utilization
  - Why needed here: The benchmark uses binary feedback (correct/incorrect) to guide agent improvement
  - Quick check question: What are the advantages and disadvantages of using binary correctness feedback versus more detailed feedback like ground truth or natural language explanations?

- **Concept:** Multi-agent collaboration
  - Why needed here: MAM-StreamICL demonstrates how multiple agents can collaborate through shared memory
  - Quick check question: How does the round-robin scheduling in MAM-StreamICL ensure cost-effectiveness while enabling knowledge sharing among agents?

## Architecture Onboarding

- **Component map:**
  - Agent -> Environment -> Stream
  - Agent contains: LLM with prompt template, retriever, memory, optionally model weights
  - Memory: Vector database storing (input, output) pairs when feedback is positive

- **Critical path:**
  1. Receive input xt from stream
  2. Generate output ˆyt using current agent configuration
  3. Receive feedback fbt from environment
  4. Update agent components based on feedback
  5. Store correct (xt, ˆyt) pairs in memory for future use

- **Design tradeoffs:**
  - Memory vs. computation: Larger memory improves performance but increases retrieval cost
  - Correct vs. incorrect examples: Storing only correct examples improves performance but may miss learning opportunities
  - Single vs. multi-agent: Multi-agent sharing improves performance but requires coordination

- **Failure signatures:**
  - Performance degradation when switching from streaming to non-streaming baselines
  - Inconsistent improvements across different random seeds
  - High variance in performance across different task types

- **First 3 experiments:**
  1. Implement GrowPrompt baseline and verify it improves over zero-shot on one dataset
  2. Test Self-StreamICL ablation (storing only correct vs. all examples) to confirm correctness hypothesis
  3. Run MAM-StreamICL with two agents to verify multi-agent benefits before scaling to three agents

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions but highlights several areas for future work including exploring different feedback types beyond binary correctness, investigating the impact of distributional shifts in streaming sequences, and understanding the relationship between retriever quality and streaming performance.

## Limitations
- The benchmark focuses exclusively on binary correctness feedback, which may not reflect real-world scenarios with more nuanced feedback
- Limited evaluation to only four LLM providers may not generalize to other model families
- Task diversity, while covering five domains, may not capture all streaming scenarios relevant to real-world applications

## Confidence
- **High Confidence**: The core finding that streaming methods outperform non-streaming baselines (consistent across multiple datasets and models)
- **Medium Confidence**: The specific mechanisms for improvement (correct self-output storage and memory sharing) due to limited ablation studies
- **Low Confidence**: Generalization to production environments given the controlled nature of the benchmark and simplified feedback mechanism

## Next Checks
1. Test MAM-StreamICL with more than three agents to determine scalability limits and identify the point of diminishing returns
2. Implement a version of the benchmark with natural language feedback instead of binary correctness to assess robustness to different feedback types
3. Evaluate the streaming methods on a custom dataset with non-IID task distribution to test adaptation to more realistic streaming scenarios