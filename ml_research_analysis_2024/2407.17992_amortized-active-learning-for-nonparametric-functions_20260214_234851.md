---
ver: rpa2
title: Amortized Active Learning for Nonparametric Functions
arxiv_id: '2407.17992'
source_url: https://arxiv.org/abs/2407.17992
tags:
- data
- training
- learning
- policy
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an amortized active learning approach for nonparametric
  function regression that eliminates the need for repeated model training and acquisition
  optimization during deployment. The method trains a neural network policy upfront
  using a GP-based simulator, allowing for real-time data selection through simple
  forward passes.
---

# Amortized Active Learning for Nonparametric Functions

## Quick Facts
- arXiv ID: 2407.17992
- Source URL: https://arxiv.org/abs/2407.17992
- Authors: Cen-You Li; Marc Toussaint; Barbara Rakitsch; Christoph Zimmer
- Reference count: 40
- Key outcome: Amortized active learning approach eliminates repeated GP training and acquisition optimization, achieving comparable learning performance with orders of magnitude faster query times through neural network policies trained on synthetic GP functions.

## Executive Summary
This paper introduces an amortized active learning approach for nonparametric function regression that replaces the iterative process of training GP models and optimizing acquisition functions with a single upfront training of a neural network policy. The method trains on synthetic functions sampled from GP priors using a simulator, enabling real-time data selection through simple forward passes during deployment. Experiments on 1D and 2D benchmark functions and real datasets demonstrate that the amortized approach matches the modeling accuracy of traditional GP-based active learning methods while significantly reducing querying time.

## Method Summary
The method trains a neural network policy using a GP-based simulator to learn data selection strategies for active learning. During training, synthetic functions are sampled from GP priors using Fourier feature sampling for efficient function generation. The policy is trained to predict informative query points based on entropy or regularized entropy objectives, with the regularization term computed on a sparse grid to encourage exploration. During deployment, the trained policy directly proposes new data points without requiring further model training or acquisition optimization, achieving real-time data selection.

## Key Results
- The amortized approach achieves comparable learning performance to conventional GP-based active learning methods
- Query time is reduced by orders of magnitude compared to traditional iterative approaches
- The method demonstrates effective generalization from synthetic GP functions to real benchmark datasets
- Regularized entropy objectives provide better exploration behavior than standard entropy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method replaces repeated GP model training and acquisition optimization with a single upfront neural network training.
- Mechanism: By training a neural network policy on simulated active learning scenarios generated from GP priors, the system learns to directly predict informative data points without requiring iterative model updates or acquisition function optimization during deployment.
- Core assumption: The space of nonparametric functions can be sufficiently represented by sampling from GP priors, and a neural network can learn to generalize from these simulated functions to real-world functions.
- Evidence anchors:
  - [abstract]: "trains a neural network policy upfront using a GP-based simulator, allowing for real-time data selection through simple forward passes"
  - [section]: "To perform AL, however, one would face multiple challenges: (i) training models for every query can be nontrivial... (iii) optimizing an acquisition function can be difficult"
  - [corpus]: Weak - corpus contains related work on amortized active learning but lacks direct experimental comparison evidence
- Break condition: The neural network policy fails to generalize from simulated GP functions to real functions that don't follow GP assumptions, or the GP prior cannot adequately represent the function space of interest.

### Mechanism 2
- Claim: Using Fourier features allows efficient sampling of GP functions without the cubic complexity of standard GP sampling.
- Mechanism: The method employs decoupled function sampling with Fourier features to approximate GP functions, enabling linear-time function value computation at any point rather than requiring expensive GP posterior sampling.
- Core assumption: The kernel used has a Fourier transform (e.g., stationary kernels), allowing the function to be approximated as a linear combination of cosine functions.
- Evidence anchors:
  - [section]: "We address this issue by applying a decoupled function sampling technique... The idea is to sample Fourier features to approximate a GP function"
  - [section]: "As a result, an approximated function is a linear combination of cosine functions... and we can later compute the function value at any point x âˆˆ X in linear time"
  - [corpus]: Weak - corpus mentions related works but doesn't provide specific evidence about Fourier feature efficiency
- Break condition: When using non-stationary kernels that lack Fourier transforms, or when the Fourier feature approximation is insufficient for the function complexity.

### Mechanism 3
- Claim: The regularized entropy objective encourages the policy to select diverse points that track subsets of grid samples rather than only boundary points.
- Mechanism: By computing the regularization term only on a sparse set of grid samples and resampling these grid points in each training step, the method creates an objective that balances exploration across the space with avoiding overfitting to specific grid locations.
- Core assumption: A sparse grid sampling (with N_grid >> T) provides sufficient coverage of the input space to guide the policy toward informative regions while preventing it from collapsing to boundary-only selection.
- Evidence anchors:
  - [section]: "We thus wish to modify â„(ðœ‘)... compute the regularization term only on a sparse set of N_grid samples... This encourages {x_Ï†,1, ..., x_Ï†,T} to track subsets of X_grid"
  - [section]: "The intuition of this objective is two-fold: (i) it can be viewed as an entropy objective regularized by an additional search space indicator, or (ii) it can be viewed as an imitation objective"
  - [corpus]: Weak - corpus contains related works on active learning objectives but lacks specific evidence about this particular regularization approach
- Break condition: When N_grid is too small to provide meaningful coverage, or when the resampling frequency is insufficient to prevent the policy from overfitting to specific grid patterns.

## Foundational Learning

- Concept: Gaussian Process regression and its cubic complexity in time and space
  - Why needed here: The paper leverages GP priors for simulation and acknowledges that standard GP approaches suffer from O(N^3) complexity, which motivates the amortized approach
  - Quick check question: What is the computational complexity of inverting the covariance matrix in GP regression, and why does this matter for active learning?

- Concept: Bayesian experimental design and information-theoretic acquisition functions
  - Why needed here: The method builds on concepts like entropy and mutual information as acquisition criteria, adapting them into training objectives for the neural network policy
  - Quick check question: How do entropy and mutual information acquisition functions differ in their selection behavior, and what problem does mutual information address that entropy doesn't?

- Concept: Amortized inference and meta-learning in sequential decision-making
  - Why needed here: The core contribution is transforming active learning from an iterative optimization problem into a single inference step through meta-training, requiring understanding of amortized approaches
  - Quick check question: What distinguishes amortized inference from standard inference in the context of sequential decision-making problems?

## Architecture Onboarding

- Component map:
  - GP Simulator -> Neural Network Policy -> Trained Policy
  - Fourier feature sampling -> Function approximation -> Linear-time computation
  - Entropy/Regularized entropy objectives -> Policy training -> Real-time deployment

- Critical path:
  1. Sample GP hyperparameters and function realizations
  2. Generate initial observations and simulate AL iterations using the policy
  3. Compute loss based on entropy or regularized entropy objectives
  4. Backpropagate through the simulation to update policy parameters
  5. Deploy trained policy for real-time data selection

- Design tradeoffs:
  - Nonmyopic vs. Myopic training: Nonmyopic considers joint optimization of all T queries but requires recursive NN forwarding; myopic is simpler but potentially less effective
  - Entropy vs. Regularized entropy: Regularized entropy provides better exploration but requires additional grid sampling overhead
  - Fourier feature approximation: Enables efficient simulation but limits kernel choices to those with Fourier transforms

- Failure signatures:
  - Poor generalization to real functions: Policy performs well on training simulations but fails on actual benchmark tasks
  - Boundary collapse: Policy consistently selects points near the input space boundaries regardless of function characteristics
  - Grid overfitting: Policy's selections are strongly correlated with the grid sampling pattern used during training

- First 3 experiments:
  1. Train a policy on 1D sinusoidal functions with varying frequencies and evaluate on held-out frequencies to test generalization
  2. Compare the effect of different grid sizes (N_grid) in the regularized entropy objective on exploration behavior
  3. Benchmark the deployment time of the trained policy against standard GP active learning on a 2D benchmark function

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different loss functions (entropy, regularized entropy, entropy version 2, regularized entropy version 2) compare in terms of AL deployment performance across different benchmark tasks?
- Basis in paper: [explicit] The authors compare multiple loss functions (Eqs. 4-5, 10-11) and present ablation study results showing different policies perform better on different tasks
- Why unresolved: While the authors show that different policies excel on different tasks, they don't provide a systematic analysis of which loss functions work best for which types of functions or problem characteristics
- What evidence would resolve it: Systematic evaluation of each loss function across a diverse set of benchmark functions with varying characteristics (smoothness, dimensionality, periodicity) to identify patterns in which loss functions perform best for which function types

### Open Question 2
- Question: How does the amortized AL approach scale to higher-dimensional problems beyond 2D, and what are the limitations?
- Basis in paper: [inferred] The paper only demonstrates results on 1D and 2D problems, with 2D being the highest dimensionality tested. The NN policy structure and Fourier feature sampling approach may face challenges in higher dimensions
- Why unresolved: The authors don't explore dimensional scaling, and the complexity analysis in Appendix C suggests potential computational challenges as dimensionality increases
- What evidence would resolve it: Systematic evaluation of the amortized AL method on benchmark functions with dimensions 3-10, measuring both performance degradation and computational costs compared to conventional GP AL methods

### Open Question 3
- Question: How sensitive is the amortized AL method to the choice of hyperparameters for the GP prior and the Fourier feature approximation?
- Basis in paper: [explicit] The authors mention sampling hyperparameters (variance, lengthscale) from specific distributions and using 100 Fourier features, but don't explore sensitivity to these choices
- Why unresolved: While the authors provide their specific hyperparameter choices, they don't investigate how variations in these choices affect the final AL performance or the robustness of the trained policy
- What evidence would resolve it: Comprehensive sensitivity analysis varying the number of Fourier features, the hyperparameter sampling distributions, and grid sampling density (N_grid) to determine which choices have the most significant impact on AL performance

### Open Question 4
- Question: How does the amortized AL method perform when the real data deviates significantly from the GP prior assumptions used during training?
- Basis in paper: [explicit] The authors acknowledge that "failing this assumption (we however would not know a priori) may result in bad data selection" and note this is a limitation of their approach
- Why unresolved: The authors don't provide empirical evidence of how the method performs when the real function significantly violates the GP prior assumption, only mentioning it as a theoretical limitation
- What evidence would resolve it: Experiments testing the amortized AL method on functions with characteristics far from the GP prior assumptions (e.g., highly non-smooth functions, discontinuities, multimodal distributions) compared to conventional GP AL methods

## Limitations
- The method's performance depends on the GP prior adequately representing the true function space, which may not hold for functions with significant non-GP characteristics
- The neural network policy may fail to generalize when real functions deviate substantially from the synthetic functions used during training
- The specific neural network architecture details and hyperparameter choices are not fully specified, potentially affecting reproducibility

## Confidence
- High confidence: The amortized approach eliminates repeated GP training and acquisition optimization, achieving significant query time reduction
- Medium confidence: The Fourier feature sampling technique enables efficient GP function simulation, though the approximation quality depends on the number of features used
- Medium confidence: The regularized entropy objective improves exploration compared to standard entropy, but the optimal grid sampling parameters require empirical tuning

## Next Checks
1. Test policy generalization across diverse function classes beyond smooth 1D and 2D functions, including discontinuous and multimodal functions
2. Conduct ablation studies on grid sampling parameters (N_grid and resampling frequency) to determine their impact on exploration quality
3. Evaluate the method's performance when the true function deviates significantly from GP assumptions, such as with heteroscedastic noise or non-stationary kernels