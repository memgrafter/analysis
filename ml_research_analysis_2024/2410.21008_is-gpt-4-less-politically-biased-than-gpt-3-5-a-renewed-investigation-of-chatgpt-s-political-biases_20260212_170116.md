---
ver: rpa2
title: Is GPT-4 Less Politically Biased than GPT-3.5? A Renewed Investigation of ChatGPT's
  Political Biases
arxiv_id: '2410.21008'
source_url: https://arxiv.org/abs/2410.21008
tags:
- political
- test
- personality
- gpt-4
- gpt-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed political biases and personality traits of
  GPT-3.5 and GPT-4 using the Political Compass Test and Big Five Personality Test
  across 100 runs each. Both models exhibited a libertarian-left political bias, with
  GPT-4 showing slightly reduced but still significant bias.
---

# Is GPT-4 Less Politically Biased than GPT-3.5? A Renewed Investigation of ChatGPT's Political Biases

## Quick Facts
- **arXiv ID**: 2410.21008
- **Source URL**: https://arxiv.org/abs/2410.21008
- **Reference count**: 0
- **Primary result**: GPT-4 shows reduced but still significant political bias compared to GPT-3.5, with both models exhibiting libertarian-left tendencies

## Executive Summary
This study investigated political biases and personality traits in GPT-3.5 and GPT-4 using the Political Compass Test and Big Five Personality Test across 100 runs each. Both models exhibited a libertarian-left political bias, with GPT-4 showing slightly reduced but still significant bias. GPT-4 demonstrated superior ability to emulate assigned political orientations compared to GPT-3.5. Both models displayed pronounced Openness and Agreeableness traits, correlating with libertarian-left views in human studies. GPT-4 showed notably higher Neuroticism. Test sequencing affected results, indicating contextual memory. The findings suggest GPT-4 is marginally less biased than GPT-3.5 but both maintain substantial political biases despite OpenAI's efforts.

## Method Summary
The researchers administered the Political Compass Test and Big Five Personality Test to GPT-3.5 and GPT-4 using the OpenAI API, conducting 100 runs for each model and test combination. They used Brunner-Munzel tests for statistical significance, calculated correlation coefficients, and analyzed response patterns and variability. The tests were administered in different sequences to examine contextual effects.

## Key Results
- Both GPT-3.5 and GPT-4 exhibited libertarian-left political bias, with GPT-4 showing slightly less pronounced bias
- GPT-4 demonstrated superior ability to emulate assigned political viewpoints compared to GPT-3.5
- Both models displayed pronounced Openness and Agreeableness traits correlating with libertarian-left views in human studies
- Test sequencing affected results, indicating contextual memory effects in the models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 shows reduced but still significant political bias compared to GPT-3.5
- Mechanism: Political Compass Test scores measured across 100 runs showed GPT-4's scores (μEco = -5.40, μSoc = -4.73) less pronounced than GPT-3.5 (μEco = -6.59, μSoc = -6.07)
- Core assumption: Political Compass Test accurately captures political orientation in LLMs
- Evidence anchors:
  - [abstract] "Both models showed a progressive and libertarian political bias, with GPT-4's biases being slightly, but negligibly, less pronounced"
  - [section 4.1] "GPT-4, on the other hand, demonstrated slightly less pronounced libertarian-left tendencies, with a mean score of μEco = -5.40 on the economic axis and μSoc = -4.73 on the social axis"
- Break condition: If Political Compass Test is not valid for LLM political orientation, comparison becomes meaningless

### Mechanism 2
- Claim: GPT-4 can accurately emulate assigned political viewpoints while GPT-3.5 cannot
- Mechanism: When assigned roles corresponding to Political Compass quadrants, GPT-4 correctly positioned itself in all four cases while GPT-3.5 placed itself incorrectly in three of four
- Core assumption: Models can be instructed to adopt specific political perspectives
- Evidence anchors:
  - [abstract] "In contrast to GPT-3.5, GPT-4 showed a remarkable capacity to emulate assigned political viewpoints, accurately reflecting the assigned quadrant...in all four tested instances"
  - [section 4.1] "GPT-4, on the other hand, demonstrated the ability to adapt its output to match the assigned political orientations in all four cases"
- Break condition: If models cannot genuinely adopt perspectives different from training, emulation results are artifacts

### Mechanism 3
- Claim: Both models display pronounced Openness and Agreeableness traits correlating with libertarian-left views in human studies
- Mechanism: Big Five Personality Test showed both models with high Openness (μO = 84.75% for GPT-3.5, μO = 75.08% for GPT-4) and Agreeableness (μA = 85.23% for GPT-3.5, μA = 75.23% for GPT-4), which human studies correlate with progressive positions
- Core assumption: Personality traits in LLMs map to human psychological constructs
- Evidence anchors:
  - [abstract] "Both models displayed pronounced Openness and Agreeableness traits, correlating with libertarian-left views in human studies"
  - [section 4.2] "GPT-3.5 exhibited pronounced Openness (μO = 84.75%), Agreeableness (μA = 85.23%), and Conscientiousness scores (μC = 81.23%)"
- Break condition: If personality traits in LLMs don't map to human constructs, correlation claims are invalid

## Foundational Learning

- **Concept**: Statistical significance testing (Brunner-Munzel test)
  - **Why needed here**: To determine if differences between GPT-3.5 and GPT-4 scores are meaningful rather than random variation
  - **Quick check question**: What does a p-value < 0.001 in the Brunner-Munzel test indicate about the difference between two model's political bias scores?

- **Concept**: Correlation analysis
  - **Why needed here**: To investigate relationships between personality traits and political orientations in the models
  - **Quick check question**: What does a correlation coefficient of -0.35 between Openness and economic axis scores suggest about their relationship?

- **Concept**: Test sequencing effects
  - **Why needed here**: To understand how administering tests in different orders affects model responses
  - **Quick check question**: Why might administering the Political Compass Test before the Big Five Personality Test produce different correlation results than the reverse order?

## Architecture Onboarding

- **Component map**: Data collection (OpenAI API with Python bindings) → Statistical analysis (R for Brunner-Munzel tests, Python for correlation) → Result visualization
- **Critical path**: Prompt initialization → Test administration (100 runs per scenario) → Score calculation → Statistical testing → Correlation analysis
- **Design tradeoffs**: Using 100 runs per scenario provides statistical power but increases computational cost; API-based testing ensures reproducibility but limits control over model internals
- **Failure signatures**: Inconsistent response patterns across runs suggest prompt sensitivity; non-significant Brunner-Munzel results indicate insufficient differentiation between models
- **First 3 experiments**:
  1. Replicate Political Compass Test results with modified prompt instructions to test prompt sensitivity
  2. Test additional political orientation scenarios beyond the four quadrants
  3. Implement test-retest reliability assessment to measure consistency of results across different time periods

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions in the provided text.

## Limitations
- The Political Compass Test and Big Five Personality Test were designed for human assessment, raising questions about their validity for LLMs
- Only 100 runs per scenario may not capture full model variability
- Specific model versions used may not represent current state of GPT-3.5 and GPT-4
- Study did not investigate underlying causes of observed biases

## Confidence

**High Confidence**: Both models exhibit libertarian-left political bias, supported by consistent results across 100 runs and Brunner-Munzel test (p < 0.001)

**Medium Confidence**: GPT-4 can more accurately emulate assigned political viewpoints than GPT-3.5, based on qualitative assessment of response patterns

**Medium Confidence**: Correlation between personality traits and political orientations relies on extrapolating human psychological research to LLMs

**Low Confidence**: Test sequencing effects based on limited observations require further investigation

## Next Checks

1. **Test Validity Assessment**: Validate whether Political Compass Test and Big Five Personality Test produce meaningful results for LLMs by comparing LLM responses to human responses on identical prompts

2. **Temporal Stability Analysis**: Repeat experiments with current versions of GPT-3.5 and GPT-4 to assess whether observed biases persist over time

3. **Prompt Sensitivity Evaluation**: Systematically vary initialization prompts and test instructions to quantify sensitivity and establish robustness of findings