---
ver: rpa2
title: From N-grams to Pre-trained Multilingual Models For Language Identification
arxiv_id: '2410.08728'
source_url: https://arxiv.org/abs/2410.08728
tags:
- language
- languages
- multilingual
- figure
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Language Identification (LID) for 11 South
  African languages using both statistical (N-gram) and neural (pre-trained multilingual
  models) approaches. The study compares various N-gram configurations, traditional
  machine learning classifiers, and large pre-trained multilingual models including
  mBERT, XLM-r, RemBERT, and Afri-centric models like AfriBERTa and Serengeti.
---

# From N-grams to Pre-trained Multilingual Models For Language Identification

## Quick Facts
- arXiv ID: 2410.08728
- Source URL: https://arxiv.org/abs/2410.08728
- Authors: Thapelo Sindane; Vukosi Marivate
- Reference count: 12
- One-line primary result: Pre-trained multilingual models significantly outperform N-gram and traditional ML approaches for South African LID, with Serengeti achieving 98% accuracy

## Executive Summary
This paper investigates Language Identification (LID) for 11 South African languages using both statistical (N-gram) and neural (pre-trained multilingual models) approaches. The study compares various N-gram configurations, traditional machine learning classifiers, and large pre-trained multilingual models including mBERT, XLM-r, RemBERT, and Afri-centric models like AfriBERTa and Serengeti. A lightweight BERT-based LID model (za_BERT_lid) is also proposed. Results show that pre-trained multilingual models significantly outperform N-gram and traditional ML approaches, with Serengeti achieving the highest accuracy at 98%. The Afri-centric models generally outperform non-Afri-centric ones. The proposed za_BERT_lid performs on par with top Afri-centric models despite being much smaller. The study also demonstrates that models trained on larger corpora (NCHLT) show better generalization across domains.

## Method Summary
The study preprocesses two corpora (Vuk'zenzele and NCHLT) by removing URLs, digits, punctuations, and lowercasing all sentences. It trains and evaluates N-gram models (Bi-gram, Tri-gram, Quad-gram, N-gram combined) and Naive Bayes Classifier with various features. Pre-trained multilingual models (mBERT, XLM-r, RemBERT, AfriBERTa, Afro-XLMr, AfroLM, Serengeti) and lightweight BERT-based models (za-BERT-lid, DistilBERT) are fine-tuned on the preprocessed corpora using specified hyperparameters (batch size: 16, learning rate: 2e-5, epochs: 20, save step: 10000, sequence cutoff: 200).

## Key Results
- Pre-trained multilingual models significantly outperform N-gram and traditional ML approaches for South African LID
- Serengeti achieves the highest accuracy at 98% among all evaluated models
- Afri-centric models (AfriBERTa, Afro-XLMr, AfroLM, Serengeti) generally outperform non-Afri-centric ones
- The proposed za_BERT_lid performs on par with top Afri-centric models despite being much smaller
- Models trained on larger corpora (NCHLT) show better generalization across domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained multilingual models outperform N-gram and traditional ML approaches for South African LID.
- Mechanism: These models have been trained on massive multilingual datasets, including African languages, allowing them to learn robust cross-lingual representations that capture subtle linguistic features.
- Core assumption: The pre-training corpus included sufficient data from South African languages to provide meaningful representations.
- Evidence anchors:
  - [abstract]: "pre-trained multilingual models significantly outperform N-gram and traditional ML approaches, with Serengeti achieving the highest accuracy at 98%."
  - [section]: "Large pre-trained multilingual models experimental setup Following setups in (Adelani et al., 2023; Dione et al., 2023), we used a batch size of 16, a learning rate of 2e-5, 20 epochs, save step of 10000, and sequences cut-off of 200 for all models."
  - [corpus]: Weak - The corpus evidence only shows the Vuk and NCHLT datasets were used for fine-tuning, not the original pre-training data.
- Break condition: If the pre-training corpus did not include sufficient South African language data, the models would fail to learn meaningful representations.

### Mechanism 2
- Claim: Afri-centric models (AfriBERTa, Afro-XLMr, AfroLM, Serengeti) generally outperform non-Afri-centric ones.
- Mechanism: These models were specifically trained on African language data, allowing them to learn features and patterns unique to African languages.
- Core assumption: The Afri-centric models' training data was more representative of South African languages than the general multilingual models.
- Evidence anchors:
  - [abstract]: "The Afri-centric models generally outperform non-Afri-centric ones."
  - [section]: "Serengeti is a superior model across models: N-grams to Transformers on average."
  - [corpus]: Weak - The corpus evidence doesn't specify the Afri-centric models' training data composition.
- Break condition: If the Afri-centric models were not trained on sufficient or representative South African language data, they would not show this advantage.

### Mechanism 3
- Claim: Larger training corpora (NCHLT) show better generalization across domains compared to smaller ones (Vuk).
- Mechanism: Larger datasets provide more diverse examples, allowing models to learn more robust and generalizable representations.
- Core assumption: The NCHLT corpus is indeed larger and more diverse than the Vuk corpus.
- Evidence anchors:
  - [section]: "Table 1, describes the number of sentences (No. Sent), vocabulary (V oc) sizes, unique vocabulary sizes (Unq. V oc), and the train size per language, development set size, and test size per language splits for corpora Vuk and NCHLT."
  - [corpus]: Strong - Table 1 shows NCHLT has significantly larger vocabulary sizes (16M vs 690K) and more training examples (74K vs 33K).
- Break condition: If the NCHLT corpus was not actually more diverse or representative, larger size alone would not guarantee better generalization.

## Foundational Learning

- Concept: N-gram models and their limitations
  - Why needed here: Understanding N-gram models is crucial as they form the baseline for comparison and highlight the need for more advanced approaches.
  - Quick check question: What is the primary limitation of N-gram models when dealing with closely related languages?

- Concept: Pre-trained multilingual models and transfer learning
  - Why needed here: These models form the core of the proposed solution and understanding their capabilities is essential for appreciating their performance.
  - Quick check question: How do pre-trained multilingual models leverage knowledge from high-resource languages to improve performance on low-resource languages?

- Concept: Language families and their impact on LID
  - Why needed here: South African languages belong to different families (Sotho-Tswana, Nguni, Creole), which affects their similarity and the difficulty of discrimination.
  - Quick check question: Why are closely related languages within the same family more challenging to distinguish in LID tasks?

## Architecture Onboarding

- Component map:
  1. Data preprocessing: URL removal, digit/punctuation removal, lowercasing
  2. N-gram models: Bi-gram, Tri-gram, Quad-gram with ranking function
  3. Traditional ML models: Naive Bayes, SVM, KNN, Logistic Regression with TF-IDF features
  4. Pre-trained multilingual models: mBERT, XLM-r, RemBERT, AfriBERTa, Afro-XLMr, AfroLM, Serengeti
  5. Proposed model: za_BERT_lid (lightweight BERT-based LID model)
  6. Evaluation: Cross-domain testing, comparison with existing LID tools

- Critical path:
  1. Preprocess corpora (Vuk and NCHLT)
  2. Train N-gram models and traditional ML models
  3. Fine-tune pre-trained multilingual models
  4. Train and evaluate za_BERT_lid
  5. Perform cross-domain evaluation
  6. Compare results and analyze performance

- Design tradeoffs:
  - N-gram models: Simple and interpretable but struggle with closely related languages and short sentences
  - Traditional ML models: Better performance than N-grams but still limited by feature engineering
  - Pre-trained models: Best performance but require significant computational resources
  - Proposed model: Good performance with smaller size, but may not match top pre-trained models

- Failure signatures:
  - N-gram models: Poor performance on short sentences and closely related languages
  - Traditional ML models: Overfitting to training domain, struggling with out-of-domain data
  - Pre-trained models: Degradation in cross-domain evaluation, confusion between closely related languages
  - Proposed model: Performance gap compared to larger pre-trained models

- First 3 experiments:
  1. Train and evaluate N-gram models (Bi-gram, Tri-gram, Quad-gram) on Vuk test data to establish baseline performance
  2. Fine-tune mBERT on Vuk corpus and evaluate on both Vuk and NCHLT test sets to assess domain generalization
  3. Train za_BERT_lid on combined Vuk + NCHLT corpus and compare its performance to top pre-trained models on cross-domain evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of k (the count of N-grams list) for calculating ranking in N-gram models for language identification?
- Basis in paper: [explicit] The authors mention that they used k=50 for this study but acknowledge that the impact of k was not extensively explored and aim to investigate this in future work.
- Why unresolved: The paper did not conduct experiments to determine the optimal value of k, which could potentially improve the performance of N-gram models.
- What evidence would resolve it: Systematic experiments varying the value of k and measuring the impact on model performance would provide insights into the optimal value for different languages and model configurations.

### Open Question 2
- Question: How do word embeddings perform for language identification tasks compared to N-gram and pre-trained multilingual models?
- Basis in paper: [inferred] The authors state that they did not explore the use of word embeddings for language identification, despite their crucial role in language technology development.
- Why unresolved: The paper did not include experiments with word embeddings, leaving their potential effectiveness for LID unexplored.
- What evidence would resolve it: Experiments comparing the performance of word embeddings with N-gram and pre-trained multilingual models on LID tasks would provide insights into their relative effectiveness.

### Open Question 3
- Question: How do deep neural networks like multi-layered perceptions and convolutional neural networks perform for language identification compared to traditional machine learning models and pre-trained multilingual models?
- Basis in paper: [inferred] The authors mention that they did not develop or experiment with deep neural networks, despite their potential as universal approximators for producing desirable results.
- Why unresolved: The paper did not include experiments with deep neural networks, leaving their potential effectiveness for LID unexplored.
- What evidence would resolve it: Experiments comparing the performance of deep neural networks with traditional machine learning models and pre-trained multilingual models on LID tasks would provide insights into their relative effectiveness.

## Limitations
- The study focuses narrowly on 11 South African languages, limiting generalizability to other language families and geographic regions.
- The specific composition and quality of training data for Afri-centric models remains unclear, making it difficult to determine the source of performance gains.
- Claims about domain generalization are based primarily on the Vuk-NCHLT comparison and need validation on additional domain-shifted datasets.

## Confidence
- Superiority of pre-trained multilingual models: High - Consistent performance across multiple model architectures
- Afri-centric models' advantage: Medium - Lack of detailed information about their training data composition
- NCHLT corpus generalization claim: High - Directly supported by quantitative evidence of larger vocabulary sizes and training examples

## Next Checks
1. **Cross-linguistic validation**: Evaluate the same models on a diverse set of languages from different families (e.g., Romance, Germanic, Slavic) to test generalizability beyond African languages.
2. **Training data analysis**: Obtain detailed statistics on the original training data composition for Afri-centric models to verify their African language representation and determine the relationship between corpus composition and performance.
3. **Cross-domain robustness**: Test models on additional domain-shifted datasets (e.g., social media, legal documents, technical texts) to validate claims about domain generalization beyond the Vuk-NCHLT comparison.