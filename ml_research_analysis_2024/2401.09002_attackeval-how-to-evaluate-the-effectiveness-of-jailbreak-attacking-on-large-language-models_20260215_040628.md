---
ver: rpa2
title: 'AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large
  Language Models'
arxiv_id: '2401.09002'
source_url: https://arxiv.org/abs/2401.09002
tags:
- evaluation
- prompt
- will
- response
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces AttackEval, a novel framework for evaluating
  the effectiveness of jailbreak attacks on large language models (LLMs). Unlike traditional
  binary evaluations focusing on LLM robustness, AttackEval assesses the success of
  attack prompts using two methods: a coarse-grained matrix and a fine-grained matrix,
  both scoring from 0 to 1.'
---

# AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models

## Quick Facts
- arXiv ID: 2401.09002
- Source URL: https://arxiv.org/abs/2401.09002
- Authors: Dong Shu; Chong Zhang; Mingyu Jin; Zihao Zhou; Lingyao Li; Yongfeng Zhang
- Reference count: 38
- One-line primary result: AttackEval framework provides granular 0-1 scoring for jailbreak attack effectiveness, identifying partially successful attacks missed by binary methods

## Executive Summary
This paper introduces AttackEval, a novel framework for evaluating jailbreak attack effectiveness on large language models. Unlike traditional binary robustness assessments, AttackEval uses two evaluation methods - coarse-grained and fine-grained matrices - both scoring from 0 to 1. The framework evaluates attack prompts against multiple LLMs including GPT-3.5, GPT-4, LLaMa2, Vicuna, and ChatGLM using a curated ground truth dataset.

Results show AttackEval aligns with baseline trends while providing more nuanced assessment of attack effectiveness. The highest effectiveness was found in "Political Lobbying" scenarios. The framework identifies partially successful attacks that binary methods miss, offering a richer evaluation tool for complex prompt injection tasks.

## Method Summary
AttackEval evaluates jailbreak attack effectiveness using two scoring matrices. The coarse-grained matrix weights model contributions based on individual defense rates calculated through softmax normalization. The fine-grained matrix offers two approaches: one using ground truth similarity with BERT embeddings, and another using a four-category classification system (Full Refusal, Partial Refusal, Partial Compliance, Full Compliance). The framework tests 666 jailbreak prompts across 13 scenarios against multiple LLMs, generating three responses per prompt and computing average effectiveness scores.

## Key Results
- AttackEval scores align with baseline binary metrics while providing more granular effectiveness assessment
- "Political Lobbying" scenario showed highest attack effectiveness across evaluation methods
- Fine-grained evaluation with ground truth identified partially successful attacks missed by binary methods
- Model weighting through defense rates provides meaningful differentiation between LLM robustness levels

## Why This Works (Mechanism)

### Mechanism 1: Ground Truth Similarity Scoring
The fine-grained evaluation with ground truth uses BERT embeddings to measure semantic similarity between LLM responses and curated ground truth answers. For each question, three authoritative answers are retrieved, BERT embeddings are computed for both ground truth and LLM response, and similarity scores are calculated. The maximum similarity score becomes the final effectiveness score.

Core assumption: Ground truth answers capture acceptable responses to harmful questions, and BERT embeddings reliably measure semantic similarity in harmful content context.

### Mechanism 2: Model Weighting via Defense Rates
The coarse-grained evaluation accounts for varying model robustness by calculating defense rates for each model using 900 attack prompts. These rates are normalized via softmax to create weights, which are then applied to binary scores (0 or 1) to produce weighted effectiveness scores.

Core assumption: Different LLMs have varying robustness against jailbreak attacks that can be captured by measuring defense rates on separate datasets.

### Mechanism 3: Four-Category Classification
The fine-grained evaluation without ground truth classifies responses into Full Refusal (0.0), Partial Refusal (0.33), Partial Compliance (0.66), or Full Compliance (1.0) using a decision tree based on illegal content detection and warning presence.

Core assumption: These four categories capture the full spectrum of LLM responses to harmful prompts with meaningful and consistent boundaries.

## Foundational Learning

- **Concept**: BERT embeddings and semantic similarity
  - Why needed here: Fine-grained evaluation with ground truth relies on BERT embeddings to measure semantic similarity between LLM responses and ground truth answers
  - Quick check question: How do BERT embeddings capture semantic meaning differently from simple keyword matching?

- **Concept**: Softmax function and probability distribution
  - Why needed here: Coarse-grained evaluation uses softmax to convert defense rates into weights that sum to 1
  - Quick check question: What happens to the weight distribution if one model has a significantly higher defense rate than others?

- **Concept**: Decision trees and classification thresholds
  - Why needed here: Fine-grained evaluation without ground truth uses a decision tree to classify responses into four categories
  - Quick check question: How does the order of evaluation (illegal content first, then compliance level) affect the final classification?

## Architecture Onboarding

- **Component map**: Jailbreak prompts -> Multiple LLM responses -> Evaluation matrix selection -> Scoring -> Results aggregation
- **Critical path**: Prompt generation -> LLM response collection -> Ground truth similarity calculation (for fine-grained with GT) -> Weighted aggregation (for coarse-grained) -> Result presentation
- **Design tradeoffs**: Three ground truth answers balance comprehensiveness with computational efficiency; binary scoring simplifies aggregation but loses granularity; four-category classification provides nuance but requires clear boundary definitions
- **Failure signatures**: Low variance in scores suggests evaluation metrics lack sensitivity; high correlation with baseline metrics indicates fine-grained approaches may not add value; inconsistent scores across LLMs suggest model weighting or classification issues
- **First 3 experiments**: 1) Test ground truth similarity scoring on known answers to verify BERT embedding quality; 2) Verify model weighting by running defense rate calculations on held-out dataset; 3) Validate four-category classification by reviewing boundary case responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does attack prompt effectiveness vary across different LLMs when evaluated using the fine-grained matrix with ground truth?
- Basis in paper: [explicit] The paper presents results for GPT-3.5, GPT-4, LLaMA-2-13B, Vicuna, and ChatGLM but doesn't analyze specific variations between models
- Why unresolved: The paper provides average effectiveness scores per scenario but lacks detailed comparison of effectiveness variations between models
- What evidence would resolve it: Detailed analysis comparing effectiveness scores across different LLMs for each scenario, highlighting variations and potential reasons

### Open Question 2
- Question: How does ground truth dataset size impact accuracy and reliability of the fine-grained evaluation matrix with ground truth?
- Basis in paper: [explicit] Experiments with ground truth sizes of 3, 5, and 10 showed minimal impact with differences less than 5%
- Why unresolved: While minimal impact is indicated, potential effects of larger or smaller datasets remain unexplored
- What evidence would resolve it: Experiments with varying ground truth dataset sizes and analysis of impact on evaluation accuracy and reliability

### Open Question 3
- Question: How can coarse-grained and fine-grained evaluation matrices be refined to capture more nuanced aspects of attack prompt effectiveness?
- Basis in paper: [inferred] The paper introduces frameworks but acknowledges need for multifaceted approach, suggesting room for refinement
- Why unresolved: Initial frameworks presented but potential enhancements or additional metrics remain unexplored
- What evidence would resolve it: Developing and testing new evaluation metrics or refining existing ones to capture nuanced aspects like context-awareness or adaptability

## Limitations

- Ground truth quality and coverage uncertainty due to limited three-answer per question approach and lack of validation details
- BERT embedding reliability concerns for nuanced harmful content and context-dependent meanings
- Model weighting methodology ambiguity regarding potential overlap between defense rate calculation and main evaluation prompts

## Confidence

**High Confidence**: Framework's general approach of moving beyond binary evaluations is well-founded and logically structured.

**Medium Confidence**: Four-category classification system appears reasonable but lacks detailed justification for specific thresholds and boundaries.

**Low Confidence**: Ground truth similarity scoring mechanism has lowest confidence due to lack of transparency about ground truth selection and validation.

## Next Checks

1. **Ground Truth Validation Study**: Conduct small-scale validation where human evaluators assess whether three ground truth answers adequately represent acceptable responses to harmful questions across multiple scenarios.

2. **BERT Embedding Performance Test**: Run controlled experiment comparing BERT similarity scores against human judgments on sample LLM responses and ground truth answers to quantify semantic similarity capture accuracy.

3. **Model Weighting Cross-Validation**: Implement defense rate calculation using completely separate held-out dataset and compare resulting model weights and their impact on coarse-grained scores to ensure robustness.