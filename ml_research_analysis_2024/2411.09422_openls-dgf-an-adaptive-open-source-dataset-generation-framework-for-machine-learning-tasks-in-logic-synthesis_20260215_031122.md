---
ver: rpa2
title: 'OpenLS-DGF: An Adaptive Open-Source Dataset Generation Framework for Machine
  Learning Tasks in Logic Synthesis'
arxiv_id: '2411.09422'
source_url: https://arxiv.org/abs/2411.09422
tags:
- logic
- circuit
- dataset
- boolean
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OpenLS-DGF, a novel adaptive framework for
  generating logic synthesis datasets tailored for machine learning tasks. The framework
  addresses limitations in previous datasets that were task-specific or lacked integrated
  ML capabilities.
---

# OpenLS-DGF: An Adaptive Open-Source Dataset Generation Framework for Machine Learning Tasks in Logic Synthesis

## Quick Facts
- **arXiv ID**: 2411.09422
- **Source URL**: https://arxiv.org/abs/2411.09422
- **Reference count**: 37
- **Primary result**: Novel adaptive framework generating comprehensive logic synthesis datasets covering three fundamental steps (Boolean representation, logic optimization, technology mapping) for ML tasks

## Executive Summary
OpenLS-DGF addresses the critical gap in logic synthesis datasets by providing an adaptive framework that generates task-agnostic datasets covering the entire logic synthesis pipeline. Unlike previous task-specific approaches, this framework preserves intermediate circuit information in both Verilog and GraphML formats, enabling diverse downstream applications. The framework generates OpenLS-D-v1, containing over 966,000 Boolean circuits from 46 combinational designs across 7 logic types, supporting four key ML tasks: circuit classification (99.8% accuracy), circuit ranking (99.49% accuracy), QoR prediction (MAPE: 0.69-7.87%), and probability prediction (PE: 0.0002-0.011%).

## Method Summary
The framework implements a three-stage logic synthesis pipeline: Boolean representation (converting designs to various logic types), logic optimization (applying 1000 synthesis recipes), and technology mapping (generating ASIC/FPGA netlists with QoR metrics). Circuit information is preserved in intermediate files stored in both Verilog and GraphML formats. An adaptive circuit engine enables loading of GraphML files for final dataset packaging and sub-dataset extraction, facilitating task-specific feature engineering while maintaining full node correspondence for accurate relabeling.

## Key Results
- Generated OpenLS-D-v1 dataset with 46 combinational designs and over 966,000 Boolean circuits across 7 logic types
- Demonstrated versatility through four downstream tasks with high performance metrics
- Circuit classification achieved 99.8% accuracy using GNNs on GraphML representations
- QoR prediction showed MAPE of 0.69-7.87% across area and timing metrics
- Circuit ranking achieved 99.49% accuracy in identifying optimal synthesis recipes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OpenLS-DGF addresses the need for task-specific datasets by generating a unified, multi-purpose dataset covering three fundamental logic synthesis steps.
- Mechanism: The framework integrates Boolean representation, logic optimization, and technology mapping into a single adaptive pipeline, enabling extraction of task-specific sub-datasets from a common source.
- Core assumption: A comprehensive dataset capturing intermediate states in logic synthesis can be repurposed for multiple downstream ML tasks.
- Evidence anchors:
  - [abstract] "Previous dataset generation flows were tailored for specific tasks or lacked integrated machine learning capabilities. While OpenLS-DGF supports various machine learning tasks by encapsulating the three fundamental steps of logic synthesis"
  - [section] "OpenLS-DGF covers three fundamental logic synthesis steps—Boolean representation, logic optimization, and technology mapping—while preserving intermediate information in both Verilog and GraphML formats"
- Break condition: If downstream tasks require information not captured in the intermediate representations (e.g., timing for non-ASIC flows), the framework cannot adapt without modification.

### Mechanism 2
- Claim: The adaptive circuit engine enables efficient sub-dataset extraction by preserving original circuit information and supporting custom operations.
- Mechanism: Circuit information is stored in both Verilog and GraphML formats, and the circuit engine reconstructs circuits with full node correspondence, allowing task-specific feature extraction and relabeling.
- Core assumption: Maintaining full circuit information in multiple formats enables flexible feature engineering for diverse tasks.
- Evidence anchors:
  - [abstract] "It preserves the original information in the intermediate files that can be stored in both Verilog and machine-learning-friendly GraphML formats"
  - [section] "The framework also includes an adaptive circuit engine that facilitates the loading of GraphML files for final dataset packaging and sub-dataset extraction"
- Break condition: If GraphML or Verilog representations lose critical structural information during conversion, the engine cannot faithfully reconstruct circuits for accurate task-specific labeling.

### Mechanism 3
- Claim: The framework's support for multiple logic types and optimization sequences creates a diverse dataset that improves ML model generalization.
- Mechanism: By generating Boolean circuits across 7 logic types (AIG, OIG, XAG, MIG, PRIMARY, GTG) and 1000 optimization sequences, the dataset captures diverse circuit structures and optimization outcomes.
- Core assumption: Diversity in circuit representation and optimization improves model robustness across different logic synthesis scenarios.
- Evidence anchors:
  - [abstract] "OpenLS-D-v1 dataset comprises 46 combinational designs from established benchmarks, totaling over 966,000 Boolean circuits, with each design containing 21,000 circuits generated from 1000 synthesis recipes, including 7000 Boolean networks, 7000 ASIC netlists, and 7000 FPGA netlists"
  - [section] "The framework covers the three fundamental stages of logic synthesis: Boolean representation, Logic optimization, and Technology mapping"
- Break condition: If certain logic types or optimization sequences produce redundant or non-diverse circuit structures, the dataset diversity benefit diminishes.

## Foundational Learning

- Concept: Boolean circuit representation and functional completeness
  - Why needed here: Understanding how different logic types (AIG, OIG, XAG, MIG, PRIMARY, GTG) represent the same Boolean function is essential for interpreting dataset diversity and task formulation
  - Quick check question: What does it mean for a set of logic gates to be functionally complete, and why does this matter for logic synthesis datasets?

- Concept: Graph Neural Networks for circuit analysis
  - Why needed here: GNNs are used for circuit classification, ranking, and probability prediction tasks, so understanding their application to graph-structured circuit data is crucial
  - Quick check question: How does a GNN aggregate node features in a circuit graph, and why is this useful for circuit classification?

- Concept: Technology mapping and quality of results (QoR) metrics
  - Why needed here: Understanding how logic optimization affects physical design metrics (area, timing) is essential for interpreting QoR prediction tasks and circuit ranking
  - Quick check question: How does the choice of Boolean representation affect the quality of results after technology mapping?

## Architecture Onboarding

- Component map:
  - Preprocessing: Design parsing → GTG generation → AIG conversion
  - Optimization: 1000 synthesis recipes with diverse command sequences
  - Logic blasting: Conversion across 6 Boolean network types
  - Technology mapping: ASIC/FPGA netlist generation with QoR metrics
  - Dataset packaging: PyTorch conversion with circuit engine
  - Circuit engine: Verilog/GraphML loading, feature extraction, sub-dataset creation

- Critical path: Design input → GTG generation → AIG optimization → logic blasting → technology mapping → QoR analysis → dataset packaging
- Design tradeoffs:
  - Storage vs. flexibility: Raw files (~410GB) vs. compressed PyTorch files (~700GB) with different access patterns
  - Diversity vs. efficiency: 1000 optimization sequences provide coverage but increase generation time
  - Format choice: Verilog for human readability vs. GraphML for ML processing
- Failure signatures:
  - Missing intermediate files: Dataset generation pipeline breaks at specific steps
  - Incorrect node correspondence: Circuit engine cannot reconstruct original circuits
  - QoR inconsistency: Technology mapping produces unexpected results across logic types
- First 3 experiments:
  1. Generate a small dataset (1-2 designs) and verify intermediate file consistency across all steps
  2. Test circuit engine's ability to load GraphML files and reconstruct circuits for a simple classification task
  3. Validate QoR prediction by comparing predicted vs. actual area/timing for a known circuit and optimization sequence

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the OpenLS-D dataset be extended to support sequential circuit designs while maintaining the same level of diversity and task coverage?
  - Basis in paper: [inferred] The paper focuses exclusively on combinational designs and explicitly states "sequential Boolean circuits are not discussed in this work."
  - Why unresolved: Sequential circuits introduce additional complexity in terms of state elements, timing constraints, and verification methods that are not addressed in the current framework.
  - What evidence would resolve it: A follow-up study demonstrating successful generation of sequential circuit datasets with comparable diversity metrics and task performance as the combinational OpenLS-D dataset.

- **Open Question 2**: What is the optimal balance between dataset size and computational efficiency for different logic synthesis tasks, particularly for resource-constrained environments?
  - Basis in paper: [explicit] The paper notes that "downstream tasks in this study are not resource-intensive" but provides extensive computational requirements for dataset generation (76 hours for raw files, 65 hours for compression).
  - Why unresolved: While the paper demonstrates feasibility, it doesn't explore trade-offs between dataset comprehensiveness and practical deployment in different computational contexts.
  - What evidence would resolve it: Empirical studies comparing task performance across systematically varied dataset sizes under different hardware constraints, identifying minimum viable dataset sizes for each task type.

- **Open Question 3**: How does the performance of machine learning models trained on OpenLS-D generalize to real-world industrial designs that may have different characteristics than benchmark circuits?
  - Basis in paper: [inferred] The paper uses established benchmarks (IWLS2005, IWLS2015, OpenCores) but doesn't address real-world industrial circuit performance.
  - Why unresolved: Benchmark circuits, while diverse, may not capture all the characteristics and challenges present in industrial designs, such as specific timing constraints, power considerations, or design methodologies.
  - What evidence would resolve it: Validation studies applying models trained on OpenLS-D to actual industrial designs, measuring performance degradation and identifying any systematic biases in the benchmark-to-industrial design gap.

## Limitations

- The framework's reliance on specific toolchains (Yosys, ABC, LSILS, iEDA, LogicFactory) creates a brittle dependency chain where tool version mismatches could significantly impact reproducibility.
- The dataset generation process requires substantial computational resources (410GB raw files) and time (76 hours for raw files), potentially limiting accessibility for researchers with constrained infrastructure.
- The paper focuses exclusively on combinational designs, leaving sequential circuit support as an open question that introduces additional complexity.

## Confidence

- Dataset Generation Pipeline: High confidence - The three-stage logic synthesis framework is well-established and the intermediate file preservation mechanism is technically sound
- Downstream Task Performance: Medium confidence - While reported metrics are impressive, the lack of detailed hyperparameter specifications and model architectures introduces uncertainty in exact reproduction
- Framework Adaptability: Medium confidence - The circuit engine's ability to extract task-specific sub-datasets is theoretically sound, but practical effectiveness depends on the quality of preserved intermediate representations

## Next Checks

1. Reproduce the circuit classification task on a small subset (5-10 designs) to verify the framework's ability to generate and process GraphML files correctly
2. Conduct ablation studies by removing specific logic types or optimization sequences to quantify their impact on downstream task performance
3. Test the framework's robustness by running dataset generation with different tool versions to identify potential version-specific issues