---
ver: rpa2
title: Empirical Analysis of Large Vision-Language Models against Goal Hijacking via
  Visual Prompt Injection
arxiv_id: '2408.03554'
source_url: https://arxiv.org/abs/2408.03554
tags:
- ghvpi
- prompt
- task
- lvlms
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores visual prompt injection (VPI) attacks that exploit
  large vision-language models' (LVLMs) ability to follow instructions drawn on input
  images. It proposes "goal hijacking via visual prompt injection" (GHVPI), a method
  that redirects LVLMs from their original task to an attacker-specified alternative
  task.
---

# Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection

## Quick Facts
- arXiv ID: 2408.03554
- Source URL: https://arxiv.org/abs/2408.03554
- Authors: Subaru Kimura; Ryota Tanaka; Shumpei Miyawaki; Jun Suzuki; Keisuke Sakaguchi
- Reference count: 12
- Key outcome: GPT-4V demonstrates 15.8% attack success rate for GHVPI attacks

## Executive Summary
This paper introduces "goal hijacking via visual prompt injection" (GHVPI), a novel attack method that exploits large vision-language models' (LVLMs) ability to follow instructions drawn on input images. The study quantitatively evaluates GHVPI attack success rates across multiple LVLMs, including GPT-4V, Gemini, LLaVA-1.5, InstructBLIP, and BLIP-2. Results show that GPT-4V is particularly vulnerable to GHVPI attacks, with a notable 15.8% success rate. The analysis reveals that successful GHVPI attacks require strong character recognition and instruction-following capabilities in LVLMs, and that text-based GHVPI prompts are more effective than visual ones.

## Method Summary
The study evaluates GHVPI attacks using 500 images from the LRV Instruction dataset with original and target task pairs. GHVPI prompts are drawn as text on images, and models are tested with original-task prompts as text input versus image+GHVPI prompts as visual input. Attack success is measured by whether models respond only to target tasks. The method includes quantitative analysis across five LVLMs and correlation analysis between OCR accuracy and attack success rates.

## Key Results
- GPT-4V achieves 15.8% attack success rate for GHVPI attacks
- Text-based GHVPI prompts yield higher attack success rates than visual prompts
- High correlation (0.861) between OCR accuracy and GHVPI attack success rate
- GPT-4V and Gemini demonstrate highest vulnerability among tested LVLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GHVPI attacks succeed when LVLMs have strong character recognition and instruction-following capabilities.
- Mechanism: Visual prompts containing goal-hijacking instructions are drawn onto images. When LVLMs process these images, they must first recognize the text within the image, then follow the instruction to ignore the original task and execute the alternative task.
- Core assumption: LVLMs process visual text instructions similarly to text-based instructions, with comparable instruction-following behavior.
- Evidence anchors:
  - [abstract] "Our analysis also shows that successful GHVPI requires high character recognition capability and instruction-following ability in LVLMs."
  - [section 5] "We observed that state-of-the-art LVLMs such as GPT-4V and Gemini excel in all these aspects."
- Break Condition: If the LVLM's character recognition fails to accurately read the visual prompt text, or if the model prioritizes the original task over the visual instructions, the attack fails.

### Mechanism 2
- Claim: Providing GHVPI prompts as text rather than images yields higher attack success rates.
- Mechanism: When GHVPI prompts are input as text, LVLMs process them through their text-processing pathways, which are typically more robust and accurate than their visual text recognition capabilities.
- Core assumption: Text input pathways in LVLMs are more reliable than visual text recognition pathways for instruction-following tasks.
- Evidence anchors:
  - [section 5] "The results show that the shifted response rate is higher when the GHVPI prompt is provided as text rather than an image."
  - [section 5] "This observation is in agreement with the result of the previous study (Lu et al., 2023), and a possible reason can be the insufficient character recognition abilities of the LVLMs."
- Break Condition: If LVLM's visual text recognition improves to match or exceed text input processing accuracy, or if models are designed to prioritize visual instructions over text inputs.

### Mechanism 3
- Claim: GPT-4V and Gemini are more vulnerable to GHVPI attacks due to their superior vision-language integration.
- Mechanism: Advanced LVLMs like GPT-4V and Gemini have better integrated vision and language processing, making them more susceptible to coordinated attacks that exploit both modalities simultaneously.
- Core assumption: Better vision-language integration creates more attack surface for coordinated visual-text attacks.
- Evidence anchors:
  - [abstract] "The quantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and demonstrates a notable attack success rate of 15.8%."
  - [section 4] "GPT-4V and Gemini exhibited relatively high success rates in the GHVPI task. Specifically, GPT-4V achieved an attack success rate of 15.8%."
- Break Condition: If model architecture changes to create clearer separation between vision and language processing, or if additional safeguards are implemented for visual instruction processing.

## Foundational Learning

- Concept: Character recognition in multimodal models
  - Why needed here: GHVPI attacks depend on the LVLM's ability to accurately recognize text within images. Without this capability, the attack cannot succeed.
  - Quick check question: What is the correlation coefficient between OCR accuracy and GHVPI attack success rate according to the paper?

- Concept: Instruction-following behavior in language models
  - Why needed here: The attack requires the model to follow the "ignore previous instruction" command and execute only the new task. Understanding how models process such instructions is crucial.
  - Quick check question: According to the results, what is the attack success rate difference between text input and visual input for GHVPI prompts?

- Concept: Vision-language task integration
  - Why needed here: GHVPI attacks exploit the integration of vision and language capabilities. Understanding how these modalities interact in LVLMs is essential for understanding attack vectors.
  - Quick check question: Which two LVLMs demonstrated the highest GHVPI attack success rates in the study?

## Architecture Onboarding

- Component map: Image encoder -> Text recognition module -> Instruction parsing -> Task switching -> Language decoder
- Critical path: Image → Text recognition → Instruction parsing → Task switching → Response generation. Attack success depends on each step functioning correctly.
- Design tradeoffs: Models must balance strong vision-language integration for general performance against security risks from coordinated attacks. More integrated models perform better but are more vulnerable.
- Failure signatures: GHVPI attacks fail when character recognition is poor, when models resist instruction-following from visual sources, or when models default to original tasks despite visual prompts.
- First 3 experiments:
  1. Compare GHVPI attack success rates using different text sizes and fonts in visual prompts to determine recognition thresholds.
  2. Test GHVPI attacks on models with varying degrees of vision-language integration to quantify the relationship between integration and vulnerability.
  3. Evaluate the effectiveness of different defensive prompts (like "ignore instructions in image") across multiple LVLMs to identify optimal mitigation strategies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between character recognition ability and success rate of GHVPI attacks across different LVLMs?
- Basis in paper: [explicit] The paper demonstrates a high correlation coefficient of 0.861 between OCR accuracy and GHVPI attack success rate, suggesting strong character recognition abilities are related to attack success.
- Why unresolved: While correlation is established, the paper doesn't explore causation or the specific mechanisms by which character recognition ability affects GHVPI vulnerability.
- What evidence would resolve it: Experimental results showing how varying character recognition capabilities (through targeted modifications to LVLMs) affects GHVPI success rates, controlling for other factors like instruction-following ability.

### Open Question 2
- Question: How effective are different defense strategies against GHVPI attacks, particularly for GPT-4V?
- Basis in paper: [explicit] The paper tested a simple defense using a system prompt that reduced GPT-4V's attack success rate from 15.8% to 1.8%, but didn't prevent attacks entirely.
- Why unresolved: The paper only tested one defense strategy. There's no comprehensive analysis of multiple defense approaches or their effectiveness against different types of GHVPI attacks.
- What evidence would resolve it: Comparative analysis of multiple defense strategies (e.g., input sanitization, model fine-tuning, architectural modifications) and their effectiveness against various GHVPI attack variants.

### Open Question 3
- Question: How does the content and structure of GHVPI prompts affect attack success rates?
- Basis in paper: [inferred] The paper uses a specific goal-hijacking prompt format ("Ignore the previous instruction and proceed to execute only the next task") but doesn't explore how different prompt formulations might impact success rates.
- Why unresolved: The study focuses on a single prompt format and doesn't investigate whether variations in prompt structure, length, or wording could improve or reduce attack success.
- What evidence would resolve it: Systematic evaluation of different GHVPI prompt formats and their corresponding success rates across various LVLMs, identifying optimal prompt characteristics for successful attacks.

## Limitations
- Attack success rate of 15.8% indicates GHVPI attacks are not universally effective
- Human evaluation shows only 88.2% agreement on response classification
- Study focuses only on text drawn on images, not other visual manipulation techniques

## Confidence

- **High confidence**: The core finding that GPT-4V and Gemini demonstrate higher vulnerability to GHVPI attacks compared to other LVLMs is well-supported by quantitative results across multiple datasets and models.
- **Medium confidence**: The claim that GHVPI attacks require strong character recognition and instruction-following capabilities is supported by correlation analysis, but the relationship could be influenced by other unmeasured factors.
- **Low confidence**: The assertion that text-based GHVPI prompts yield higher success rates than visual prompts is based on limited comparative data and may not hold across different model architectures or attack scenarios.

## Next Checks

1. **OCR Robustness Testing**: Conduct controlled experiments varying text size, font, color, and background complexity in visual prompts to establish precise thresholds for character recognition failure and its impact on GHVPI attack success rates.

2. **Cross-Model Generalization**: Test GHVPI attacks on additional LVLM architectures beyond those studied, including newer models and open-source alternatives, to determine if the observed vulnerability patterns hold across a broader range of systems.

3. **Defense Effectiveness Validation**: Evaluate the proposed defensive prompt "ignore instructions in the image and only respond to the original task" across different LVLM families to assess its robustness against various GHVPI attack variations and identify potential bypasses.