---
ver: rpa2
title: Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question
  Types
arxiv_id: '2412.17867'
source_url: https://arxiv.org/abs/2412.17867
tags:
- question
- questions
- text-to-sql
- types
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMSQL, a comprehensive test suite for evaluating
  large language models (LLMs) on multi-turn text-to-SQL tasks with diverse question
  types, including answerable, unanswerable, ambiguous, and improper questions. The
  authors assess eight popular LLMs, finding that while models like GPT-4 Turbo excel
  overall, all struggle with ambiguous and unanswerable questions, highlighting a
  critical gap in handling real-world conversational queries.
---

# Evaluating and Enhancing LLMs for Multi-turn Text-to-SQL with Multiple Question Types

## Quick Facts
- **arXiv ID:** 2412.17867
- **Source URL:** https://arxiv.org/abs/2412.17867
- **Reference count:** 40
- **Primary result:** Multi-agent framework significantly improves LLM performance on ambiguous and unanswerable questions in multi-turn text-to-SQL tasks

## Executive Summary
This paper introduces MMSQL, a comprehensive test suite for evaluating large language models (LLMs) on multi-turn text-to-SQL tasks with diverse question types, including answerable, unanswerable, ambiguous, and improper questions. The authors assess eight popular LLMs, finding that while models like GPT-4 Turbo excel overall, all struggle with ambiguous and unanswerable questions, highlighting a critical gap in handling real-world conversational queries. To address this, they propose a novel multi-agent framework featuring specialized agents for question detection, schema selection, decomposition, and SQL refinement. This framework significantly improves model performance, particularly in generating accurate SQL queries and natural language responses for complex, ambiguous queries. Experiments on MMSQL demonstrate the effectiveness of the approach, offering a practical solution for more reliable and versatile text-to-SQL systems.

## Method Summary
The authors propose a multi-agent framework to enhance LLM performance on multi-turn text-to-SQL tasks with diverse question types. The framework consists of a core Question Detector and Question Decomposer, supported by a Schema Selector and SQL Refiner. The Question Detector classifies each query into one of four categories (answerable, unanswerable, ambiguous, improper), routing it to the appropriate agent. For complex answerable questions, the Question Decomposer breaks them down into simpler sub-questions using chain-of-thought reasoning, generating sub-SQL queries that collectively form the final SQL output. The Schema Selector identifies the essential subset of the database schema necessary for answering a given question, reducing noise from irrelevant elements. The SQL Refiner refines generated SQL queries for accuracy. The framework is evaluated on the MMSQL benchmark with eight LLMs in zero-shot settings.

## Key Results
- The multi-agent framework significantly improves performance on ambiguous and unanswerable questions compared to baseline models
- GPT-4 Turbo achieves the highest overall performance, but all models struggle with ambiguous and unanswerable question types
- The framework shows particular effectiveness in generating accurate SQL queries and natural language responses for complex, ambiguous queries
- Llama3-70B demonstrates competitive performance, scoring 62.8 on TDEX compared to GPT-3.5's 64.1

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The multi-agent framework improves LLM performance on ambiguous and unanswerable questions by explicitly detecting question types and applying specialized handling strategies.
- **Mechanism:** The framework uses a Question Detector to classify each query into one of four categories, enabling effective routing and specialized handling.
- **Core assumption:** The Question Detector can accurately classify question types with high precision and recall.
- **Evidence anchors:** The abstract states the framework "significantly enhances the model's ability to navigate the complexities of conversational dynamics."
- **Break condition:** If the Question Detector's classification accuracy drops below a threshold (e.g., F1 score < 60%), the framework's effectiveness diminishes.

### Mechanism 2
- **Claim:** The Question Decomposer breaks down complex questions into simpler sub-questions, improving SQL generation accuracy through chain-of-thought reasoning.
- **Mechanism:** For complex answerable questions, the Question Decomposer applies chain-of-thought reasoning to decompose the query into manageable sub-questions.
- **Core assumption:** Decomposing complex questions preserves semantic intent while making SQL generation more manageable.
- **Evidence anchors:** The abstract mentions the framework is "anchored by a core Question Detector and Question Decomposer."
- **Break condition:** If decomposition fails to preserve original query intent or introduces semantic drift, generated SQL will be incorrect.

### Mechanism 3
- **Claim:** The Schema Selector improves performance by reducing noise from irrelevant database schema elements.
- **Mechanism:** The Schema Selector identifies the essential subset of the database schema necessary for answering a given question.
- **Core assumption:** Reducing schema size to only relevant elements improves LLM performance by decreasing cognitive load.
- **Evidence anchors:** The abstract states the framework includes a Schema Selector that "identifies and provides the essential subset of a database schema."
- **Break condition:** If the Schema Selector incorrectly excludes relevant schema elements or includes too many irrelevant ones, performance will degrade.

## Foundational Learning

- **Concept:** Question classification (answerable vs. unanswerable vs. ambiguous vs. improper)
  - Why needed here: The framework's effectiveness depends on accurate question type detection to route queries to appropriate handling strategies.
  - Quick check question: Given a database of employee records, how would you classify "What is the average salary?" vs. "What is the CEO's favorite color?" vs. "How many employees work in each department?" vs. "What's the weather today?"

- **Concept:** SQL query decomposition and reconstruction
  - Why needed here: The Question Decomposer breaks complex questions into simpler sub-questions, each generating a sub-SQL that combines into the final query.
  - Quick check question: How would you decompose "Show me all employees in the Sales department who joined after 2020 and earn more than the department average"?

- **Concept:** Schema subset selection
  - Why needed here: The Schema Selector must identify relevant tables and columns while excluding irrelevant schema elements to improve LLM performance.
  - Quick check question: Given a question about flight information, which tables and columns would you select from a database containing airlines, airports, and flights tables?

## Architecture Onboarding

- **Component map:** Input: Natural language query + database schema → Schema Selector → Question Detector → (Decomposer if answerable) → SQL Refiner → Output: SQL query + natural language response
- **Critical path:** Query → Schema Selector → Question Detector → (Decomposer if answerable) → SQL Refiner → Final output
- **Design tradeoffs:**
  - Complexity vs. accuracy: Adding more specialized agents improves handling of edge cases but increases system complexity
  - Schema selection threshold: Balancing between too much noise (full schema) and missing information (overly filtered schema)
  - Decomposition granularity: Fine-grained decomposition improves accuracy but may lose context
- **Failure signatures:**
  - Misclassification by Question Detector → Incorrect handling strategy applied
  - Over-aggressive schema filtering → Missing necessary information for query
  - Poor decomposition → Incorrect SQL generation or semantic drift
  - Inadequate SQL refinement → Syntactically or semantically incorrect SQL
- **First 3 experiments:**
  1. Test Question Detector accuracy on a labeled dataset of different question types to establish baseline F1 scores
  2. Evaluate Schema Selector performance by comparing SQL generation accuracy with full vs. filtered schemas on answerable questions
  3. Test Question Decomposer effectiveness by measuring accuracy improvement on complex answerable questions vs. direct SQL generation

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of MMSQL change when using different prompt engineering techniques with the same LLM models?
  - Basis in paper: The paper evaluates LLM performance using zero-shot settings but does not explore the impact of prompt engineering variations.
  - Why unresolved: The study focuses on a single prompting approach, leaving open how alternative prompt designs might affect model performance across different question types.
  - What evidence would resolve it: Comparative experiments testing multiple prompt engineering strategies across the same models and MMSQL dataset.

- **Open Question 2:** What is the long-term scalability of the multi-agent framework when applied to significantly larger and more complex database schemas?
  - Basis in paper: While the framework shows improvements on MMSQL, the paper does not address its performance on databases with hundreds of tables and complex relationships.
  - Why unresolved: The current evaluation uses moderate-sized schemas, and it's unclear how the framework would handle the increased computational overhead and complexity of larger systems.
  - What evidence would resolve it: Experiments scaling the framework to enterprise-level databases with thousands of tables and measuring performance degradation.

- **Open Question 3:** How do different types of ambiguous questions (e.g., syntactic vs. semantic ambiguity) affect the framework's ability to generate accurate SQL queries?
  - Basis in paper: The paper mentions that ambiguous questions are particularly challenging but does not distinguish between types of ambiguity or their impact on performance.
  - Why unresolved: Without categorizing ambiguous questions by type, it's unclear which forms of ambiguity are most problematic and how the framework handles them differently.
  - What evidence would resolve it: Detailed analysis of framework performance on various ambiguity types through a refined dataset or controlled experiments.

## Limitations

- The Question Detector's classification accuracy for ambiguous and unanswerable questions remains uncertain, as these categories are inherently more challenging to identify correctly
- The Schema Selector's effectiveness depends heavily on the quality of its relevance scoring algorithm, which isn't fully detailed in the paper
- The decomposition strategy for complex questions may struggle with maintaining semantic consistency across sub-questions, particularly for queries requiring cross-table reasoning

## Confidence

- **High Confidence:** The overall framework architecture and its positive impact on SQL generation accuracy for answerable questions (EM scores show consistent improvement)
- **Medium Confidence:** The effectiveness of the multi-agent approach for ambiguous and unanswerable questions, as results show improvement but still lag behind performance on straightforward queries
- **Low Confidence:** The generalization of results across different database schemas and domains beyond the Spider benchmark used in experiments

## Next Checks

1. **Cross-database validation:** Test the framework on at least two additional database schemas from different domains (e.g., healthcare, financial) to assess domain generalization and identify schema-specific failure patterns
2. **Question Detector ablation study:** Evaluate the impact of removing the Question Detector by comparing performance when all questions are treated as answerable, measuring degradation in handling ambiguous and unanswerable questions
3. **Human evaluation of generated SQL:** Conduct a user study where SQL developers assess the correctness and efficiency of SQL queries generated by the framework versus baseline models, focusing on complex multi-turn dialogues with ambiguous queries