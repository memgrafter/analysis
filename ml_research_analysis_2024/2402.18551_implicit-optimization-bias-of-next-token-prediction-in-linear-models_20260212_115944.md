---
ver: rpa2
title: Implicit Optimization Bias of Next-Token Prediction in Linear Models
arxiv_id: '2402.18551'
source_url: https://arxiv.org/abs/2402.18551
tags:
- arxiv
- training
- bias
- implicit
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the implicit optimization bias of next-token
  prediction (NTP), the dominant training paradigm for modern language models. The
  key contribution is to show that gradient descent (GD) on the NTP loss, when the
  model is overparameterized, converges in direction to a max-margin solution within
  a data subspace, while diverging in norm in the orthogonal direction.
---

# Implicit Optimization Bias of Next-Token Prediction in Linear Models

## Quick Facts
- arXiv ID: 2402.18551
- Source URL: https://arxiv.org/abs/2402.18551
- Reference count: 40
- One-line primary result: Gradient descent on next-token prediction loss converges to max-margin solutions within data subspaces while diverging in orthogonal directions.

## Executive Summary
This paper investigates the implicit optimization bias of next-token prediction (NTP), the dominant training paradigm for modern language models. The key contribution is to show that gradient descent (GD) on the NTP loss, when the model is overparameterized, converges in direction to a max-margin solution within a data subspace, while diverging in norm in the orthogonal direction. Specifically, the authors introduce NTPH-compatibility and NTP-separability conditions, which determine when the NTP loss can reach its entropy lower bound. They show that under these conditions, the direction of GD converges to a max-margin direction specific to NTP, while the finite component of the parameters converges to a solution that matches the logits' differences to their log-odds. This extends prior work on implicit bias in one-hot classification to the NTP setting, highlighting key differences and prompting further research into optimization and generalization in NTP.

## Method Summary
The paper analyzes gradient descent on the next-token prediction loss in linear models with fixed context embeddings. The method involves checking NTPH-compatibility and NTP-separability conditions, then running gradient descent on the cross-entropy loss while monitoring parameter norm growth, alignment with the max-margin direction, and convergence of the finite component within the data subspace. The analysis assumes overparameterization (d > m) and uses a small learning rate to ensure convergence properties.

## Key Results
- Under NTPH-compatibility, GD converges to a solution in the data subspace that matches logits' differences to log-odds
- Under NTP-separability, GD parameters diverge in norm and align with the max-margin direction in the orthogonal subspace
- The regularization path converges to the NTP-SVM direction as the regularization parameter approaches zero

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent on the NTP loss selects parameters that equate logits' differences of in-support tokens to their log-odds within the data subspace F.
- Mechanism: The cross-entropy loss is convex in the decoding matrix W, and the sparsity patterns of distinct contexts define a data subspace F where NTPH-compatibility constraints apply. Within this subspace, the unique solution W⋆ satisfies (ez - ez')^T W⋆ h_j = log(ˆp_j,z / ˆp_j,z') for all z ≠ z' in the support set S_j, forcing the logits to match the empirical log-odds.
- Core assumption: The NTPH-compatibility condition holds, meaning there exists a finite W p in F that satisfies the logit constraints for all in-support tokens across all distinct contexts.
- Evidence anchors:
  - [abstract]: "Within the data subspace defined by the sparsity patterns of distinct contexts, GD selects parameters that equate the logits' differences of in-support tokens to their log-odds."
  - [section 3]: "We say that training data Tm are NTP-entropy-compatible if there exists V × d matrix W p satisfying: ∀j ∈ [m], z ≠ z' ∈ S_j : (e_z - e_z')^T W p h_j = log(ˆp_j,z / ˆp_j,z')."
- Break condition: If the training data are not NTPH-compatible, i.e., no finite W p exists in F that satisfies the logit constraints, then the CE loss cannot reach the entropy lower bound, and the implicit bias within F does not manifest as log-odds matching.

### Mechanism 2
- Claim: In the orthogonal subspace F⊥, gradient descent parameters diverge in norm and align with the direction that maximizes the NTP margin.
- Mechanism: The NTP-separability condition defines halfspace constraints in F⊥ that enforce (e_z - e_v)^T W d h_j ≥ 1 for all z in S_j and v not in S_j. The direction W d that satisfies these constraints while minimizing norm defines the NTP-SVM solution W mm. As GD iterates grow in norm, their direction in F⊥ converges to W mm, maximizing the margin between in-support and out-of-support tokens.
- Core assumption: The NTP-separability condition holds, meaning there exists a W d in F⊥ that satisfies the halfspace constraints for all contexts and token pairs.
- Evidence anchors:
  - [abstract]: "In the orthogonal subspace, the GD parameters diverge in norm and select the direction that maximizes a margin specific to NTP."
  - [section 4]: "NTP-SVM solves the following: W mm := arg min_W ||W|| subj. to W ∈ R^(V×d) satisfying (6a) and (6b)."
- Break condition: If the training data are not NTP-separable, i.e., no W d exists in F⊥ that satisfies the halfspace constraints, then the GD iterates do not converge in direction to W mm, and the implicit bias in F⊥ does not maximize the NTP margin.

### Mechanism 3
- Claim: The regularization path converges to the NTP-SVM direction W mm as the regularization parameter λ approaches zero.
- Mechanism: The regularized objective min_{||W|| ≤ B} CE(W) is strictly convex, ensuring a unique solution ̂W_B for each B. As B grows, the solution ̂W_B approaches the boundary ||W|| = B and aligns with W mm, as shown by comparing the loss of ̂W_B to that of a "genie" point W_B⋆ = W⋆ + R(B)W mm.
- Core assumption: The training data are both NTPH-compatible and NTP-separable, ensuring the existence of W⋆ in F and W mm in F⊥.
- Evidence anchors:
  - [section 4]: "Theorem 1 (Implicit bias of the regularization-path). Assume training data Tm is NTPH-compatible and NTP-separable. Let ̂W_B be defined as in (8). Then, it holds that lim_{B→∞} ⟨ ̂W_B / ||̂W_B||, W mm / ||W mm|| ⟩ = 1."
- Break condition: If either NTPH-compatibility or NTP-separability fails, the regularization path does not converge to the NTP-SVM direction, and the implicit bias of the regularization path is not characterized by W mm.

## Foundational Learning

- Concept: Cross-entropy loss minimization and its convexity in the decoding matrix W.
  - Why needed here: The paper's analysis relies on the convexity of the CE loss in W to establish the uniqueness of solutions in the regularization path and the convergence properties of GD iterates.
  - Quick check question: Is the CE loss convex in W when the context embeddings h(x) are fixed? (Yes, because the softmax function is convex in its input, and the CE loss is a composition of convex functions.)

- Concept: Softmax function and its properties, particularly its strict positivity.
  - Why needed here: The strict positivity of softmax outputs is crucial for understanding why the NTP-separability constraints involve inequalities rather than equalities, and why the CE loss approaches but never reaches the entropy lower bound for finite W.
  - Quick check question: Does the softmax function output strictly positive values for any finite input vector? (Yes, softmax(x)_i = exp(x_i) / sum_j exp(x_j) > 0 for all i, even if some x_j are very negative.)

- Concept: Linear separability and the concept of margin in classification.
  - Why needed here: The paper extends the concept of margin from one-hot classification to the NTP setting, defining the NTP margin as the minimum (e_z - e_v)^T W h_j over all contexts j and token pairs z in S_j, v not in S_j. This extension is essential for characterizing the implicit bias in the orthogonal subspace F⊥.
  - Quick check question: In the NTP setting, what is the margin between in-support tokens and out-of-support tokens for a given context embedding h_j? (The margin is min_{(z,v): z∈S_j, v∉S_j} (e_z - e_v)^T W h_j.)

## Architecture Onboarding

- Component map:
  - Training data -> Context embeddings -> Decoding matrix W -> NTP loss -> Gradient descent -> Parameter updates

- Critical path:
  1. Identify distinct contexts and their next-token distributions from the training data.
  2. Check NTPH-compatibility and NTP-separability conditions.
  3. If conditions hold, run GD on the NTP loss with a small learning rate.
  4. Monitor the norm of W and the alignment of W/||W|| with W mm.
  5. Verify convergence to W⋆ in the data subspace F.

- Design tradeoffs:
  - Overparameterization vs. model complexity: The analysis assumes d > m for NTPH-compatibility and NTP-separability. Increasing d beyond m improves the chances of satisfying these conditions but increases model complexity.
  - Fixed vs. learned context embeddings: The paper assumes fixed context embeddings for analytical tractability. Learning context embeddings jointly with the decoding matrix could capture more complex relationships but would complicate the analysis.

- Failure signatures:
  - CE loss does not approach the entropy lower bound: Indicates that the training data may not satisfy NTPH-compatibility or NTP-separability.
  - GD iterates do not align with W mm in F⊥: Suggests that the NTP-separability condition may not hold, or the learning rate may be too large.
  - Convergence to a different solution in F: Implies that the NTPH-compatibility condition may not hold, or there may be multiple solutions in F satisfying the constraints.

- First 3 experiments:
  1. Generate a synthetic dataset with m = 50 distinct contexts, d = 60 embedding dimension, V = 10 vocabulary size, and support sets of size 6. Run GD on the NTP loss and plot the CE loss, norm of W, and alignment with W mm as a function of iterations.
  2. Repeat experiment 1 with different values of d (e.g., d = m, d = 2m) to observe the effect of overparameterization on the convergence of GD iterates to W mm.
  3. Modify the synthetic dataset to violate NTPH-compatibility (e.g., by setting some conditional probabilities to zero) and observe the behavior of GD iterates. Verify that the CE loss does not approach the entropy lower bound and that the implicit bias in F is not characterized by log-odds matching.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact thresholds for NTP-separability under distributional assumptions?
- Basis in paper: [explicit] The paper mentions in Remark 1 that determining exact thresholds for NTP-separability would improve upon the sufficient condition of Lemma 1, analogous to previous work on one-hot separability.
- Why unresolved: The paper only provides a sufficient condition (d > m) for NTP-separability but acknowledges that the exact threshold is more intricate and remains an open question.
- What evidence would resolve it: Rigorous mathematical proofs or empirical studies that establish the precise conditions under which NTP-separability holds for various data distributions.

### Open Question 2
- Question: How does the implicit bias of gradient descent in NTP settings affect generalization?
- Basis in paper: [explicit] The paper suggests in the conclusion that studying generalization in NTP settings by examining statistical properties of the NTP-SVM solution is an interesting direction, similar to past research on one-hot classification.
- Why unresolved: The paper focuses on characterizing the optimization bias of gradient descent but does not delve into how this bias influences the generalization performance of the trained model.
- What evidence would resolve it: Theoretical analysis or empirical experiments that link the properties of the NTP-SVM solution (e.g., margin, norm) to the generalization ability of the model on unseen data.

### Open Question 3
- Question: What is the memory capacity of sequence-to-sequence architectures, such as transformers, in the context of NTP?
- Basis in paper: [explicit] The paper mentions in the conclusion that investigating the memory capacity of sequence-to-sequence architectures in NTP settings is an essential extension, as recent studies on transformer memory capacity do not apply here.
- Why unresolved: The paper does not provide a formal definition or analysis of memory capacity in NTP settings, and the existing literature on transformer memory capacity is not directly applicable.
- What evidence would resolve it: Rigorous mathematical analysis or empirical studies that establish bounds on the memory capacity of sequence-to-sequence architectures in NTP settings, considering factors such as sequence length, vocabulary size, and model architecture.

## Limitations
- The analysis relies on NTPH-compatibility and NTP-separability conditions, which are sufficient but not necessary for the observed implicit bias behavior.
- The assumption of fixed context embeddings limits the applicability of results to practical language models where context representations are typically learned end-to-end.
- The extension from linear models to practical transformer architectures remains an open question, as the implicit bias mechanisms may differ significantly.

## Confidence
- High Confidence: The convergence of the finite component of W to W⋆ within the data subspace F under NTPH-compatibility.
- Medium Confidence: The alignment of GD iterates with the max-margin direction W_mm in the orthogonal subspace F⊥ under NTP-separability.
- Low Confidence: The generalization of these results to practical language models with learned context embeddings and transformer architectures.

## Next Checks
1. Implement gradient descent on synthetic datasets satisfying NTPH-compatibility and NTP-separability conditions. Track the evolution of (a) CE loss approaching empirical entropy H, (b) parameter norm growth, and (c) alignment of W/||W|| with W_mm to validate theoretical predictions.
2. Construct synthetic datasets that violate NTPH-compatibility or NTP-separability. Observe and characterize the behavior of GD iterates in these cases to test the necessity of the compatibility conditions.
3. Extend the analysis to the case where context embeddings are also learned, not just the decoding matrix W. Investigate whether the implicit bias mechanisms identified for fixed embeddings still play a role when embeddings are learned.