---
ver: rpa2
title: Image space formalism of convolutional neural networks for k-space interpolation
arxiv_id: '2402.17410'
source_url: https://arxiv.org/abs/2402.17410
tags:
- image
- raki
- noise
- k-space
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an image-space formalism for RAKI (Robust
  Artificial Neural Networks for k-space Interpolation) to analytically study noise
  propagation and interpretability. By reformulating nonlinear activations in k-space
  as element-wise multiplications with activation masks, the method transforms convolutions
  in k-space to convolutions in image space, enabling analytical computation of g-factor
  maps.
---

# Image space formalism of convolutional neural networks for k-space interpolation

## Quick Facts
- arXiv ID: 2402.17410
- Source URL: https://arxiv.org/abs/2402.17410
- Authors: Peter Dawood; Felix Breuer; Istvan Homolya; Maximilian Gram; Peter M. Jakob; Moritz Zaiss; Martin Blaimer
- Reference count: 28
- Key outcome: Introduces image-space formalism for RAKI enabling analytical noise propagation analysis and interpretability

## Executive Summary
This paper presents an image-space formalism for RAKI (Robust Artificial Neural Networks for k-space Interpolation) that transforms nonlinear activations in k-space into element-wise multiplications with activation masks, enabling analytical computation of g-factor maps. The approach reveals that enhanced noise resilience in RAKI comes at the cost of image blurring and potential center autocorrelation artifacts. The negative slope parameter in leaky ReLU acts as Tikhonov-like regularization, allowing trade-offs between residual artifacts and noise resilience, particularly with limited training data.

## Method Summary
The method reformulates RAKI's complex-valued convolutions and nonlinear activations from k-space to image space using the convolution theorem. Activation masks are computed as the ratio of activated signal to original signal in k-space, allowing element-wise multiplication operations to be translated into convolution operations in image space. This enables explicit computation of Jacobian matrices for noise propagation analysis, providing analytical g-factor maps that quantify noise amplification. The framework was validated on 2D brain datasets (FLASH and TSE sequences) with acceleration factors R=4-6, using ACS lines for training data.

## Key Results
- Analytical g-factor maps match Monte Carlo simulations and auto-differentiation results, computed in ~135 seconds versus ~5,400 seconds for auto-differentiation
- Negative slope parameter in leaky ReLU acts as Tikhonov-like regularization, trading residual artifacts against noise resilience
- Enhanced noise resilience in RAKI leads to image blurring and potential center autocorrelation artifacts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Nonlinear activations in k-space can be analytically decomposed into image-space convolution operations
- Mechanism: By expressing nonlinear activation functions (e.g., â„‚LReLU) as element-wise multiplication with activation masks in k-space, and applying the convolution theorem, these operations translate to convolutions in image space
- Core assumption: The activation mask can be accurately computed as the ratio of activated signal to original signal in k-space
- Evidence anchors:
  - [abstract] "By reformulating nonlinear activations in k-space as element-wise multiplications with activation masks, the method transforms convolutions in k-space to convolutions in image space"
  - [section 2.2] "By formulating the action of â„‚LReLU to signal Üáˆºà¯žáˆ» as described in Eq. (5), the elementwise multiplication of Üáˆºà¯žáˆ» with ð€áˆºà¯žáˆ» in k-space can be translated into a corresponding complex-valued convolution operation in image space"
- Break condition: If the activation mask computation fails (e.g., division by zero or non-invertible activation functions), the image space translation breaks down

### Mechanism 2
- Claim: The negative slope parameter in leaky ReLU acts as a Tikhonov-like regularization parameter controlling the trade-off between residual artifacts and noise resilience
- Mechanism: Varying the negative slope parameter changes the degree of nonlinearity in the interpolation model. Lower values increase noise suppression but introduce image blurring and contrast loss, while higher values reduce these artifacts but increase noise amplification
- Core assumption: The relationship between negative slope parameter and noise/artifact trade-off is monotonic and predictable
- Evidence anchors:
  - [abstract] "The negative slope parameter in leaky ReLU can act as Tikhonov-like regularization to trade-off residual artifacts against noise resilience"
  - [section 4.5] "It is worth noting that the negative slope parameter Ü½can serve as a Tikhonov-like regularization parameter to control the degree of residual artifacts (i.e., apparent blurring and contrast loss) in trade off for slightly increased noise"
- Break condition: If the relationship between parameter and artifact/noise trade-off becomes non-monotonic or highly dataset-dependent, the regularization effect becomes unreliable

### Mechanism 3
- Claim: Analytical g-factor maps computed via the image space formalism accurately quantify noise propagation in RAKI
- Mechanism: By expressing the Jacobian of the de-aliased, coil-combined image relative to the aliased coil images algebraically, noise amplification can be quantified analytically using the standard g-factor formula
- Core assumption: First-order linear approximation of the nonlinear interpolation is sufficient for accurate noise propagation analysis
- Evidence anchors:
  - [abstract] "This approach quantifies noise amplification efficiently and reveals that enhanced noise resilience in RAKI leads to image blurring and a potential center autocorrelation artifact"
  - [section 2.3] "Thus, using the chain rule and the coil-combination weight matrix ð‘·, we can express the Jacobian of the de-aliased, coil-combined image relative to the aliased coil images, Û¸áˆºaccáˆ», explicitly"
- Break condition: If higher-order terms in the Jacobian become significant, the first-order approximation may break down and introduce errors in noise quantification

## Foundational Learning

- Concept: Convolution theorem and its application to k-space to image space transformations
  - Why needed here: The entire image space formalism relies on translating convolution operations between domains using the convolution theorem
  - Quick check question: What operation in k-space becomes element-wise multiplication in image space, and vice versa?

- Concept: Jacobian matrices and their role in noise propagation analysis
  - Why needed here: The analytical g-factor computation requires explicit Jacobian matrices of the reconstruction output relative to the input data
  - Quick check question: How does the chain rule apply when computing Jacobians through multiple hidden layers in a neural network?

- Concept: Regularization and its relationship to model complexity in neural networks
  - Why needed here: The negative slope parameter serves as a regularization mechanism, requiring understanding of how model complexity affects generalization and noise behavior
  - Quick check question: How does increasing nonlinearity in a neural network typically affect its noise resilience and potential for overfitting?

## Architecture Onboarding

- Component map: Input k-space data -> Complex-valued convolutional layers with â„‚LReLU activation -> Activation masks -> Image space convolutions -> Jacobian computation -> g-factor maps

- Critical path: k-space data â†’ convolution layers with â„‚LReLU â†’ activation masks â†’ image space convolutions â†’ Jacobian computation â†’ g-factor maps

- Design tradeoffs:
  - Memory vs. computation time: Analytical approach requires more memory but is faster than Monte Carlo
  - Nonlinearity degree: Higher nonlinearity increases noise resilience but introduces artifacts
  - Training data amount: Limited data requires careful regularization parameter tuning

- Failure signatures:
  - Memory overflow during Jacobian computation for high-resolution images
  - G-factor maps showing unexpected patterns suggesting Jacobian calculation errors
  - Residual artifacts appearing in reconstructions despite low noise enhancement

- First 3 experiments:
  1. Implement image space transformation for a single hidden layer and verify quasi-equivalence with k-space inference using NMSE comparison
  2. Test activation mask computation for different activation functions (ReLU, leaky ReLU with varying slopes) and verify element-wise multiplication property
  3. Compute analytical g-factor for a simple linear interpolation case and compare with known GRAPPA g-factor formula

## Open Questions the Paper Calls Out

- Question: How can the activation masks be designed to minimize the autocorrelation artifact while maintaining noise resilience?
  - Basis in paper: [explicit] The paper identifies that activation masks reveal autocorrelation patterns leading to center artifacts, and suggests that alternative activation functions with less autocorrelation could be explored
  - Why unresolved: The paper only identifies the problem and suggests a direction for future research, but does not provide concrete solutions or evaluate potential alternative activation functions
  - What evidence would resolve it: Testing various activation functions (e.g., different types of ReLU, Swish, GELU) in RAKI and quantifying both autocorrelation artifacts and noise resilience performance

- Question: What is the optimal way to adapt trained RAKI weights to varying coil profiles across different frames in a dynamic imaging series?
  - Basis in paper: [explicit] The paper notes that RAKI weights cannot simply be applied to different frames in a time series unlike GRAPPA, and suggests this as a future research direction
  - Why unresolved: The paper acknowledges the limitation but does not propose or test methods for adapting weights to different coil configurations
  - What evidence would resolve it: Developing and validating methods to adapt RAKI weights to new coil configurations, potentially using transfer learning or coil sensitivity estimation techniques

- Question: How would higher-order approximations of the nonlinear interpolation affect noise propagation analysis accuracy?
  - Basis in paper: [explicit] The paper uses first-order linear approximation for noise propagation analysis and mentions that higher-order terms could refine accuracy but are beyond the current scope
  - Why unresolved: The paper deliberately uses first-order approximation for tractability but acknowledges its limitations without exploring higher-order effects
  - What evidence would resolve it: Implementing and comparing second-order or higher-order approximations of the Jacobian calculation against first-order results across various noise levels and undersampling rates

## Limitations
- Memory requirements for computing full Jacobian matrices are prohibitive for high-resolution images, necessitating sparse approximations that could introduce errors
- The assumption that noise distributions remain Gaussian after nonlinear processing may not hold for all activation functions or network architectures
- The relationship between the negative slope parameter and artifact/noise trade-off may vary across different datasets or acquisition protocols

## Confidence
- **High Confidence**: The transformation of k-space convolutions to image space via convolution theorem and the accuracy of analytical g-factor maps compared to Monte Carlo simulations (correlation > 0.98)
- **Medium Confidence**: The regularization effect of the negative slope parameter as Tikhonov-like regularization, based on qualitative observations of artifact/noise trade-offs
- **Low Confidence**: The assumption that noise distributions remain Gaussian after nonlinear processing, as validated only by KS tests without examining potential non-Gaussian tails

## Next Checks
1. Test the framework on non-brain anatomical regions and different sequence types to verify generalizability of the negative slope regularization effect
2. Compare analytical g-factor computation with auto-differentiation results across multiple acceleration factors and undersampling patterns
3. Quantify the impact of sparse Jacobian approximations on g-factor accuracy by systematically varying sparsity thresholds