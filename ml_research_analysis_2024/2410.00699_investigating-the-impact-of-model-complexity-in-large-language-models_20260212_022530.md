---
ver: rpa2
title: Investigating the Impact of Model Complexity in Large Language Models
arxiv_id: '2410.00699'
source_url: https://arxiv.org/abs/2410.00699
tags:
- theorem
- where
- risk
- regression
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how model complexity affects fine-tuning
  performance in autoregressive large language models (LLMs). The authors model LLMs
  using Hidden Markov Models (HMMs) and analyze a head-tuning paradigm where only
  a linear head is trained on top of a frozen pre-trained LLM.
---

# Investigating the Impact of Model Complexity in Large Language Models

## Quick Facts
- arXiv ID: 2410.00699
- Source URL: https://arxiv.org/abs/2410.00699
- Authors: Jing Luo; Huiyuan Wang; Weiran Huang
- Reference count: 23
- This paper investigates how model complexity affects fine-tuning performance in autoregressive large language models (LLMs) through head-tuning.

## Executive Summary
This paper investigates how model complexity affects fine-tuning performance in autoregressive large language models (LLMs). The authors model LLMs using Hidden Markov Models (HMMs) and analyze a head-tuning paradigm where only a linear head is trained on top of a frozen pre-trained LLM. They prove that the risk of next-word prediction initially increases and then decreases as model complexity grows, exhibiting a "double descent" phenomenon. Notably, the initial descent is degenerate, meaning the optimal risk occurs at zero model size. The theoretical analysis is supported by experiments on HMM-generated data and a Transformer-based head tuning setup, confirming the predicted double descent behavior in practical settings.

## Method Summary
The authors analyze autoregressive LLMs using Hidden Markov Models (HMMs) to model sequential dependencies. They focus on a head-tuning paradigm where a frozen pre-trained model generates representations that are fed into a trainable linear head. The theoretical framework analyzes next-word prediction risk using bias-variance decomposition, examining how risk changes as the number of parameters (p) varies relative to the number of training samples (n). The analysis distinguishes between underparameterized (p < n) and overparameterized (p > n) regimes, deriving asymptotic expressions for risk in each regime. The model complexity is controlled by varying the dimensionality of the representation layer while keeping other components fixed.

## Key Results
- The risk of next-word prediction initially increases and then decreases with rising model complexity, showcasing a "double descent" phenomenon.
- The initial descent in the double descent curve is degenerate, with optimal risk occurring at zero model size rather than at some intermediate model size.
- The risk behavior is governed by the eigenvalues of the covariance matrix Σx of the input representations, with specific solutions to eigenvalue-based equations determining the bias-variance tradeoff.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The double descent phenomenon occurs due to the interplay between bias and variance as model complexity increases.
- Mechanism: As model size (p) increases relative to training samples (n), the least squares estimator transitions from underparameterized (p < n) to overparameterized (p > n) regimes. In the underparameterized regime, variance dominates risk, which increases toward the interpolation threshold (p ≈ n). Beyond this threshold, bias emerges but decreases faster than variance, causing overall risk to decline.
- Core assumption: The data follows a Hidden Markov Model (HMM) structure, and the head-tuning setup uses linear representations of tokens.
- Evidence anchors:
  - [abstract] "Our theoretical analysis reveals that the risk initially increases and then decreases with rising model complexity, showcasing a 'double descent' phenomenon."
  - [section] "Our findings reveal a phenomenon known as 'double descent' (Belkin et al., 2019), wherein the LLM's risk initially increases and then decreases with an increase in model complexity."
  - [corpus] The corpus contains several papers on double descent in deep learning, supporting the general phenomenon, but none specifically address HMM-based autoregressive models or head-tuning paradigms.

### Mechanism 2
- Claim: The initial descent in the double descent curve is degenerate, meaning optimal risk occurs at zero model size.
- Mechanism: When model size p approaches zero, the variance component of risk becomes negligible while the bias remains bounded. This creates a scenario where the minimum risk is achieved at p=0, rather than at some intermediate model size as in classical bias-variance tradeoff.
- Core assumption: The analysis assumes a head-tuning paradigm where all pre-trained parameters are frozen and only individual heads are trained.
- Evidence anchors:
  - [abstract] "In this case, the initial 'descent' is degenerate, signifying that the 'sweet spot' where bias and variance are balanced occurs when the model size is zero."
  - [section] "Our findings reveal a phenomenon known as 'double descent' (Belkin et al., 2019), wherein the LLM's risk initially increases and then decreases with an increase in model complexity. Notably, the initial 'descent' is degenerate..."
  - [corpus] No direct corpus evidence for degenerate initial descent; this appears to be a novel finding from the paper's specific theoretical framework.

### Mechanism 3
- Claim: The risk behavior is governed by the eigenvalues of the covariance matrix Σx of the input representations.
- Mechanism: The eigenvalues of Σx determine the bias-variance decomposition in the overparameterized regime. The solution c0 to a specific equation involving these eigenvalues governs how bias and variance scale with model complexity.
- Core assumption: The eigenvalues of Σx are bounded and do not accumulate near zero, and the overparameterization ratio γ = p/n stays within reasonable bounds.
- Evidence anchors:
  - [section] "The next-word prediction risk is intricately tied to the geometric properties of Σx. To elucidate, we introduce the eigenvalues of Σx as s1, s2, ..., sp, where s1 ≥ s2 ≥ ... ≥ sp ≥ 0."
  - [section] "Definition 1. Define γ = p/n, for γ ∈ R>1, define c0 to be the unique non-negative solution to [equation involving eigenvalues]."
  - [corpus] The corpus contains papers on ridge regression and eigenvalue analysis, supporting the general approach, but none specifically address the HMM-based setting.

## Foundational Learning

- Concept: Hidden Markov Models (HMMs)
  - Why needed here: The paper uses HMMs to model the autoregressive nature of LLMs, where input tokens are hidden states and their representations are observed states.
  - Quick check question: In the HMM framework, what represents the "hidden states" and what represents the "observed states" for the autoregressive LLM modeling?

- Concept: Bias-variance decomposition
  - Why needed here: The paper extends the classical bias-variance tradeoff to the multivariate regression setting of next-word prediction, decomposing risk into bias and variance components that behave differently across model complexity regimes.
  - Quick check question: How does the bias-variance decomposition in this paper differ from the classical univariate regression case?

- Concept: Double descent phenomenon
  - Why needed here: The paper's main theoretical contribution is showing that the risk exhibits double descent behavior, first increasing then decreasing with model complexity, with a degenerate initial descent.
  - Quick check question: What distinguishes the double descent phenomenon observed in this paper from the classical U-shaped risk curve in underparameterized regimes?

## Architecture Onboarding

- Component map: HMM with transition matrix A producing tokens z_i -> Linear mapping W producing x_i = z_iW + u_i -> Target generation y_i = z_iA + ξ_i -> Single-layer feed-forward network bB trained on (x_i, y_i)

- Critical path:
  1. Generate training data using HMM
  2. Apply linear representation to create input features
  3. Train head network using least squares (underparameterized) or ridge regression (overparameterized)
  4. Compute risk decomposition into bias and variance
  5. Analyze asymptotic behavior as p/n varies

- Design tradeoffs:
  - Linear representation assumption simplifies analysis but may not capture complex transformer behavior
  - HMM assumption captures sequential dependencies but is simpler than full language structure
  - Head-tuning simplifies fine-tuning but may limit performance compared to full fine-tuning

- Failure signatures:
  - Risk curve doesn't show double descent shape
  - Eigenvalue spectrum of Σx violates boundedness assumptions
  - Training error doesn't approach zero as expected
  - Bias-variance decomposition doesn't match theoretical predictions

- First 3 experiments:
  1. Verify double descent behavior by varying model size p while keeping n fixed, plotting risk vs p/n
  2. Test degenerate initial descent by comparing risk at p=0 versus small positive p values
  3. Validate eigenvalue analysis by computing and plotting the eigenvalue spectrum of Σx and comparing with theoretical predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would relaxing the Markov assumption in the HMM modeling affect the theoretical predictions of the double descent phenomenon?
- Basis in paper: Explicit. The authors mention that their theory could still hold when the HMM assumption is relaxed to more extended dependencies like zi = (zi−1, ..., zi−k)A + ε.
- Why unresolved: The authors only mention this as future work and do not provide theoretical analysis or empirical validation for this extended model.
- What evidence would resolve it: Theoretical analysis proving the double descent phenomenon persists under the extended dependency model, along with empirical validation on data generated from such models.

### Open Question 2
- Question: How does the double descent phenomenon in autoregressive LLMs compare to that observed in non-autoregressive models?
- Basis in paper: Inferred. The authors focus on autoregressive LLMs and derive unique characteristics of the double descent phenomenon in this context, but do not directly compare with non-autoregressive models.
- Why unresolved: The paper's theoretical framework is specifically designed for autoregressive models, and no comparative analysis with non-autoregressive models is provided.
- What evidence would resolve it: A comparative study analyzing the double descent behavior in both autoregressive and non-autoregressive models under similar conditions.

### Open Question 3
- Question: What is the impact of different fine-tuning strategies (e.g., prompt tuning) on the double descent phenomenon in LLMs?
- Basis in paper: Explicit. The authors acknowledge that their study does not encompass prompt tuning, which is a highly effective fine-tuning technique.
- Why unresolved: The theoretical framework and experiments are limited to head tuning, leaving the behavior of other fine-tuning strategies unexplored.
- What evidence would resolve it: Theoretical analysis and empirical experiments comparing the double descent phenomenon across various fine-tuning strategies, including prompt tuning.

## Limitations

- The theoretical analysis relies heavily on the HMM assumption, which may oversimplify the complex dynamics of actual large language models.
- The paper focuses on a specific head-tuning paradigm where all pre-trained parameters are frozen, which may not capture the full complexity of modern fine-tuning approaches.
- The eigenvalue analysis assumes specific properties of the covariance matrix Σx that may not hold in practical scenarios with high-dimensional data and complex representations.

## Confidence

**High Confidence:** The general double descent phenomenon in model complexity-risk relationships. This is supported by both theoretical analysis and experimental validation on multiple datasets and model architectures.

**Medium Confidence:** The degenerate initial descent finding. While the theoretical framework supports this claim and it is validated on synthetic HMM data, the practical implications and generalizability to real-world LLMs require further investigation.

**Low Confidence:** The specific quantitative predictions about risk behavior at different model complexity ratios. The theoretical bounds and approximations used in the analysis may not accurately capture the behavior in all practical scenarios.

## Next Checks

1. **Cross-architecture validation**: Test the double descent and degenerate initial descent predictions on multiple LLM architectures (BERT, RoBERTa, GPT variants) and head-tuning configurations to assess generalizability beyond the specific Transformer setup used in the paper.

2. **Real-world dataset testing**: Evaluate the theoretical predictions on actual language modeling tasks with real-world datasets (e.g., WikiText, BookCorpus) rather than synthetic HMM-generated data to assess practical relevance and identify potential discrepancies between theory and practice.

3. **Alternative fine-tuning method comparison**: Extend the analysis to other fine-tuning paradigms like full fine-tuning, LoRA, and prefix tuning to determine whether the observed risk-complexity relationships hold across different adaptation strategies, or if the head-tuning paradigm produces unique characteristics.