---
ver: rpa2
title: Aligning Large Language Models for Controllable Recommendations
arxiv_id: '2403.05063'
source_url: https://arxiv.org/abs/2403.05063
tags:
- user
- recommendation
- items
- item
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a two-stage approach to align large language
  models (LLMs) with controllable recommendation tasks. The first stage involves supervised
  learning using a variety of instruction-following tasks augmented with labels from
  a traditional recommender model.
---

# Aligning Large Language Models for Controllable Recommendations

## Quick Facts
- arXiv ID: 2403.05063
- Source URL: https://arxiv.org/abs/2403.05063
- Authors: Wensheng Lu; Jianxun Lian; Wei Zhang; Guanghua Li; Mingyang Zhou; Hao Liao; Xing Xie
- Reference count: 22
- Key outcome: Two-stage approach (SL + RL) aligns LLMs with controllable recommendation tasks, significantly improving instruction-following accuracy and reducing formatting errors.

## Executive Summary
This paper presents a novel two-stage approach to align large language models (LLMs) with controllable recommendation tasks. The first stage involves supervised learning using a variety of instruction-following tasks augmented with labels from a traditional recommender model. The second stage applies reinforcement learning with tailored reward signals to further improve instruction compliance and reduce formatting errors. Experiments on two real-world datasets show that the method significantly improves LLMs' ability to follow recommendation instructions while maintaining high accuracy, outperforming existing fine-tuned baselines.

## Method Summary
The proposed method involves a two-stage approach: (1) Supervised Learning (SL) with instruction-tuning tasks (I0-I4) and label augmentation using SASRec predictions; (2) Reinforcement Learning (RL) with PPO algorithm and tailored reward signals. The SL stage fine-tunes Llama-2-7b-chat on a variety of instruction-following tasks, while the RL stage further refines the model's ability to follow instructions and reduce formatting errors. The method is evaluated on two real-world datasets (Steam and Amazon Movie) using metrics such as HR@K, NDCG@K, TCP@K, CPA, and formatting quality (CorrectCount, RepeatItem, NonExist, InHistory).

## Key Results
- The two-stage approach significantly improves LLMs' ability to follow recommendation instructions while maintaining high accuracy.
- The method outperforms existing fine-tuned baselines on both recommendation accuracy and instruction-following metrics.
- RL fine-tuning with tailored reward signals effectively reduces formatting errors and improves instruction compliance.

## Why This Works (Mechanism)

### Mechanism 1
Label augmentation from SASRec injects domain knowledge into the LLM, enabling it to generate recommendations that meet instruction requirements even with sparse user history. The LLM is trained on responses where SASRec fills in k-1 items (beyond the ground-truth nth item), ensuring each output list aligns with the instruction while reflecting learned patterns from traditional recommendation. Core assumption: SASRec's top-k predictions contain items that are both relevant to the user and conform to instruction constraints.

### Mechanism 2
The RL stage refines the LLM's instruction-following generalization by applying rewards that penalize formatting errors and reward compliance with control intentions. A reward function combines item-level scores (preference and control effect) with list-level scores (ranking quality and control satisfaction), then applies PPO to optimize policy toward high-reward responses. Core assumption: The crafted reward rules accurately capture the desired behavior, and PPO can effectively navigate the reward landscape without collapsing into degenerate strategies.

### Mechanism 3
Separating instruction types (implicit, item-wise, list-wise) and training dedicated tasks allows the LLM to specialize in handling each category of control, improving controllability without sacrificing accuracy. Supervised learning includes tasks I0 (sequential), I1 (category control), I2 (proportion control), I3 (item search), and I4 (ShareGPT), each with specific label generation and reward design, so the LLM learns distinct patterns for each control type. Core assumption: The LLM can effectively distinguish and apply the correct control logic per instruction type during inference.

## Foundational Learning

- Concept: Supervised learning with augmented labels
  - Why needed here: User interaction data is sparse; augmenting with SASRec predictions provides richer, instruction-compliant examples for training.
  - Quick check question: How does the method ensure augmented labels still reflect user preferences while satisfying instruction constraints?

- Concept: Reinforcement learning with tailored rewards
  - Why needed here: Supervised learning alone cannot handle edge cases or penalize formatting errors; RL with PPO and specific reward signals further aligns the LLM with desired behaviors.
  - Quick check question: What is the difference between item-level and list-level rewards, and why are both necessary?

- Concept: Instruction categorization and task separation
  - Why needed here: Different control intentions (e.g., positive vs negative category control) require different handling; separating them in training improves controllability.
  - Quick check question: How does the method distinguish between I1+C and I1-C during both training and inference?

## Architecture Onboarding

- Component map: User history + instruction prompt -> LLM backbone (Llama-2-7b-chat with LoRA adapters) -> Supervised learning stage (label augmentation via SASRec + fine-tuning on instruction tasks) -> Reinforcement learning stage (PPO with item-level and list-level rewards + KL penalty) -> Formatted recommendation list

- Critical path:
  1. Construct augmented training samples using SASRec
  2. Fine-tune LLM on I0, I1, I2, I3, I4 tasks
  3. Apply RL fine-tuning with PPO and reward shaping
  4. Evaluate controllability and accuracy on held-out test sets

- Design tradeoffs:
  - Label augmentation vs. using only ground-truth: Augmentation improves data richness but may introduce noise if SASRec predictions are off.
  - Separate SL and RL stages vs. joint optimization: Separation simplifies debugging and reward design but may slow convergence.
  - Using LoRA adapters vs. full fine-tuning: LoRA is parameter-efficient but may limit model capacity to adapt fully.

- Failure signatures:
  - High RepeatItem or NonExist rates: RL reward shaping or label augmentation is misaligned.
  - Poor TCP/CPA in control tasks: Instruction type separation or reward calculation is incorrect.
  - Catastrophic forgetting: Insufficient ShareGPT data or overly aggressive RL fine-tuning.

- First 3 experiments:
  1. Train Oursv1 (SL only, I0) and evaluate HR@10/NDCG@10 on Movie dataset; expect improvement over Llama2-7b but below SASRec.
  2. Train Oursv2 (SL with all tasks) and compare TCP@10 for I1+C vs I1-C to confirm task separation works.
  3. Train Oursfull (SL + RL) and measure formatting metrics (RepeatItem, NonExist, InHistory) to verify RL reduces errors.

## Open Questions the Paper Calls Out
- How can catastrophic forgetting be minimized when fine-tuning LLMs for recommendation tasks while preserving their general capabilities?
- How do LLMs perform on more complex and diverse instruction types beyond category control, such as price range, release date, or brand preferences?
- What is the impact of the proposed method on recommendation performance in a live user environment compared to traditional recommender systems?

## Limitations
- The approach relies heavily on SASRec predictions for label augmentation, which may introduce noise if the traditional recommender's outputs do not align with user intent or instruction constraints.
- The effectiveness of the RL stage depends on precise reward shaping, which is not fully specified in the paper.
- The instruction categorization (I0-I4) assumes clear separation, but real-world instructions may be ambiguous or overlapping, potentially confusing the model.

## Confidence
- High: The overall two-stage approach (SL + RL) is well-justified and aligns with established practices in LLM alignment and controllable generation.
- Medium: The effectiveness of label augmentation and reward shaping depends on the quality of SASRec predictions and the precision of the reward rules, which are not fully detailed.
- Low: The handling of ambiguous or overlapping instruction categories (I0-I4) and the model's ability to generalize to unseen instruction types are uncertain.

## Next Checks
1. Test the model's robustness to noisy SASRec predictions by artificially degrading the quality of augmented labels and measuring the impact on instruction-following accuracy.
2. Conduct ablation studies to isolate the contributions of item-level vs. list-level rewards in the RL stage, and test whether the model exploits reward artifacts rather than genuinely improving instruction compliance.
3. Evaluate the model on a held-out set of ambiguous or overlapping instruction categories to assess its ability to correctly apply control logic when instruction types are not clearly separated.