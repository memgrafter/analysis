---
ver: rpa2
title: Are Self-Attentions Effective for Time Series Forecasting?
arxiv_id: '2405.16877'
source_url: https://arxiv.org/abs/2405.16877
tags:
- forecasting
- time
- series
- input
- cats
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether self-attention mechanisms are effective
  for time series forecasting. To address this, the authors propose a novel architecture
  called Cross-Attention-only Time Series transformer (CATS) that eliminates self-attention
  and relies solely on cross-attention.
---

# Are Self-Attentions Effective for Time Series Forecasting?

## Quick Facts
- arXiv ID: 2405.16877
- Source URL: https://arxiv.org/abs/2405.16877
- Reference count: 40
- Primary result: CATS eliminates self-attention and uses cross-attention with future horizons as queries, achieving superior forecasting performance with fewer parameters

## Executive Summary
This paper challenges the conventional use of self-attention in time series forecasting by proposing a novel architecture called Cross-Attention-only Time Series transformer (CATS). The authors argue that self-attention mechanisms, which are permutation-invariant, may not be optimal for preserving temporal information in time series data. CATS replaces self-attention with cross-attention where future horizon-dependent parameters serve as queries and past time series data act as keys and values. The model demonstrates superior performance across multiple datasets while using significantly fewer parameters than existing models.

## Method Summary
CATS is a Transformer-based architecture that eliminates self-attention entirely, relying solely on cross-attention mechanisms. The model uses learnable horizon-dependent queries that interact with past time series data (as keys and values) through multi-head cross-attention layers. A key innovation is parameter sharing across all forecasting horizons - the embedding layer, multi-head attention, and projection layer are shared, reducing parameter count from O(T) to O(1) scaling with horizon length. The model also introduces query-adaptive masking, where attention outputs are masked with increasing probability for each layer to force the model to focus on horizon-specific representations. The implementation is available at https://github.com/dongbeank/CATS.

## Key Results
- CATS achieves the lowest mean squared error across multiple real-world datasets including Weather, Traffic, Electricity, ETT, and M4
- The model uses significantly fewer parameters than existing time series forecasting models (355,320 vs. 404,672-3,121,568)
- Parameter sharing across forecasting horizons maintains performance while dramatically reducing model complexity
- Query-adaptive masking improves convergence speed and forecasting accuracy compared to standard dropout

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention with future horizons as queries preserves temporal information better than self-attention
- Mechanism: By using future horizon-dependent parameters as queries and past time series data as keys/values, the model directly models the dependency between past and future without the permutation-invariant properties of self-attention
- Core assumption: Temporal order in time series is critical for forecasting and should be explicitly preserved in the attention mechanism
- Evidence anchors:
  - [abstract]: "eliminating self-attention and leveraging cross-attention mechanisms instead"
  - [section 3]: "The version with linear embedding (Fig. 2c) demonstrates the clearest capture of temporal information, suggesting that the self-attention mechanism itself may not be necessary for capturing temporal information"
  - [corpus]: Weak evidence - related papers focus on alternative attention mechanisms but don't specifically validate this temporal preservation claim
- Break condition: If temporal dependencies are not the primary signal for forecasting (e.g., in highly stochastic processes), this mechanism may not provide advantages over self-attention

### Mechanism 2
- Claim: Parameter sharing across forecasting horizons significantly reduces model complexity while maintaining performance
- Mechanism: All horizon-dependent queries share the same embedding, multi-head attention, and projection layers, reducing parameters from O(T) to O(1) scaling with horizon length
- Core assumption: The transformation from past to future values follows similar patterns across different horizons, allowing shared parameters
- Evidence anchors:
  - [section 4.2]: "we propose parameter sharing across all possible layers — the embedding layer, multi-head attention, and projection layer — for every horizon-dependent query q"
  - [table 3]: Shows parameter count remains nearly constant (355,320) across forecasting horizons while non-sharing model increases from 404,672 to 3,121,568
  - [corpus]: Weak evidence - related parameter-efficient papers focus on spatial attention or segment attention rather than horizon-based sharing
- Break condition: If different forecasting horizons require fundamentally different transformations, parameter sharing could lead to underfitting

### Mechanism 3
- Claim: Query-adaptive masking improves model focus on forecasting targets by selectively disconnecting past information
- Mechanism: For each horizon, the direct connection from Multi-Head Attention to LayerNorm is masked with probability p, forcing the model to rely more on the query representation
- Core assumption: The model can overfit to historical patterns at the expense of learning horizon-specific representations
- Evidence anchors:
  - [section 4.2]: "To ensure the model focuses on each horizon-dependent query q, we introduce a new technique that masks the attention outputs"
  - [section B.3]: Shows query-adaptive masking outperforms dropout in convergence speed and performance
  - [corpus]: No direct evidence - this appears to be a novel technique not mentioned in related work
- Break condition: If the model cannot learn effective query representations without historical context, excessive masking could degrade performance

## Foundational Learning

- Concept: Attention mechanism and its mathematical formulation
  - Why needed here: Understanding how queries, keys, and values interact is fundamental to grasping why cross-attention is preferred over self-attention
  - Quick check question: What is the mathematical difference between self-attention and cross-attention in terms of query/key/value sources?

- Concept: Time series properties and forecasting challenges
  - Why needed here: The paper's contribution relies on understanding why standard Transformer approaches may fail for time series specifically
  - Quick check question: What properties of time series (stationarity, periodicity, etc.) make them different from other sequence data like text?

- Concept: Parameter efficiency and model scaling
  - Why needed here: The paper's main contribution includes significant parameter reduction while maintaining performance
  - Quick check question: How does parameter sharing across forecasting horizons change the scaling relationship between model size and forecast horizon?

## Architecture Onboarding

- Component map: Learnable queries (horizon-dependent parameters) -> Embedding (shared) -> Cross-Attention (multi-head, shared) -> LayerNorm (with masking) -> Projection (shared) -> Forecast output

- Critical path: Learnable queries → Embedding → Cross-Attention (with masking) → Projection → Forecast output

- Design tradeoffs:
  - Parameter sharing vs. horizon-specific specialization
  - Masking probability vs. information retention
  - Query complexity vs. model efficiency

- Failure signatures:
  - Constant or near-constant attention weights across different horizons (indicates masking is too aggressive)
  - Performance degradation with longer horizons (indicates insufficient horizon-specific capacity)
  - Training instability or slow convergence (indicates masking probability needs adjustment)

- First 3 experiments:
  1. Replace cross-attention with self-attention in CATS while keeping all other components identical to verify the importance of cross-attention
  2. Remove query-adaptive masking to measure its contribution to performance and training dynamics
  3. Vary the number of shared layers (e.g., share only embedding vs. share all layers) to find the optimal balance between efficiency and performance

## Open Questions the Paper Calls Out
None

## Limitations
- The temporal preservation mechanism relies on visual inspection of attention patterns rather than quantitative metrics
- Query-adaptive masking technique lacks extensive ablation analysis to determine optimal masking probabilities
- Parameter sharing assumptions may not generalize to domains where different forecasting horizons have fundamentally different characteristics

## Confidence

- **High Confidence:** Parameter reduction claims (well-quantified with clear scaling relationships in Table 3)
- **Medium Confidence:** Overall forecasting performance improvements (supported by multiple datasets but limited to MSE/MAE metrics)
- **Low Confidence:** The mechanism by which cross-attention preserves temporal information better than self-attention (based primarily on visual inspection of attention patterns rather than quantitative metrics)

## Next Checks

1. Conduct controlled ablation studies comparing temporal information preservation between CATS and self-attention variants using quantitative metrics beyond visual inspection
2. Test the query-adaptive masking technique across a broader range of masking probabilities and dataset characteristics to establish robustness
3. Evaluate model performance on datasets where different forecasting horizons have fundamentally different signal characteristics to test the validity of parameter sharing assumptions