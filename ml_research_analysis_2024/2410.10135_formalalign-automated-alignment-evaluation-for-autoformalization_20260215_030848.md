---
ver: rpa2
title: 'FormalAlign: Automated Alignment Evaluation for Autoformalization'
arxiv_id: '2410.10135'
source_url: https://arxiv.org/abs/2410.10135
tags:
- formal
- alignment
- evaluation
- language
- align
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FORMALALIGN, an automated framework for evaluating
  alignment between natural and formal languages in autoformalization. The core idea
  combines autoformalization sequence generation with representational alignment through
  a dual loss of cross-entropy and contrastive learning.
---

# FormalAlign: Automated Alignment Evaluation for Autoformalization

## Quick Facts
- **arXiv ID**: 2410.10135
- **Source URL**: https://arxiv.org/abs/2410.10135
- **Reference count**: 40
- **Primary result**: FORMALALIGN achieves 11.58% higher Alignment-Selection Score than GPT-4 on FormL4-Basic (99.21% vs 88.91%)

## Executive Summary
This paper introduces FORMALALIGN, an automated framework for evaluating alignment between natural and formal languages in autoformalization. The core idea combines autoformalization sequence generation with representational alignment through a dual loss of cross-entropy and contrastive learning. Evaluated on four benchmarks, FORMALALIGN achieves superior performance compared to GPT-4, with 11.58% higher Alignment-Selection Score on FormL4-Basic (99.21% vs 88.91%) and 3.19% higher on MiniF2F-Valid (66.39% vs 64.34%). The method significantly reduces manual verification needs while demonstrating strong generalization across different baseline models.

## Method Summary
FORMALALIGN trains a dual-task model using Mistral-7B, combining autoformalization sequence generation (cross-entropy loss) with representational alignment (contrastive loss). The model learns to generate formal mathematical statements from natural language while simultaneously learning embeddings that capture semantic alignment. The final alignment score combines a certainty score (generation confidence) and similarity score (cosine similarity of embeddings), with a threshold of 0.7 determining alignment decisions. The framework is trained on FormL4 and MMA datasets and evaluated on FormL4-Basic and MiniF2F-Valid benchmarks.

## Key Results
- FORMALALIGN achieves 99.21% Alignment-Selection Score on FormL4-Basic, outperforming GPT-4 by 11.58%
- On MiniF2F-Valid, FORMALALIGN reaches 66.39% AS, surpassing GPT-4 by 3.19%
- The method generalizes well across different baseline models, with consistent improvements over Phi2-2.7B, LLaMA2-7B, and DeepSeekMath-Base 7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-task training improves alignment detection
- Mechanism: The model learns both autoformalization generation (cross-entropy loss) and representational alignment (contrastive loss) simultaneously, creating mutual enhancement between tasks
- Core assumption: Representations that work well for generation also contain sufficient semantic information for alignment detection
- Evidence anchors:
  - [abstract]: "trains on both the autoformalization sequence generation task and the representational alignment between input and output, employing a dual loss that combines a pair of mutually enhancing autoformalization and alignment tasks"
  - [section]: "We jointly train an evaluator model with the autoformalization and alignment tasks, resulting in a FORMAL ALIGN model"
- Break condition: If the semantic information needed for alignment differs fundamentally from that needed for generation, the dual-task approach may fail

### Mechanism 2
- Claim: Contrastive learning enhances semantic discrimination
- Mechanism: The contrastive loss encourages embeddings of aligned pairs to be more similar than embeddings of misaligned pairs, creating a learned representation space where alignment can be detected
- Core assumption: The embedding space can capture semantic differences that surface-form metrics miss
- Evidence anchors:
  - [section]: "The contrastive loss encourages the cosine similarity cos(ui, vi) between the representations of corresponding informal-formal pairs to be higher than the cosine similarity cos(ui, vi') between non-corresponding pairs"
  - [section]: "This encourages the model to generate similar embeddings for corresponding pairs and distinct embeddings for non-corresponding pairs"
- Break condition: If the embedding space cannot capture the semantic differences relevant to alignment, contrastive learning will not help

### Mechanism 3
- Claim: Combined scoring captures both confidence and semantic alignment
- Mechanism: The final alignment score combines certainty (generation confidence) and similarity (embedding alignment), providing a more robust evaluation than either measure alone
- Core assumption: Both generation confidence and embedding similarity contain complementary information about alignment quality
- Evidence anchors:
  - [section]: "This score combines two metrics: the certainty score and the similarity score"
  - [section]: "This combined score reflects both the accuracy of the translation from informal to formal expressions and the alignment of the internal representations of the sequences"
- Break condition: If one component dominates the other or if both measures are correlated, the combination may not add value

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To create a representation space where aligned informal-formal pairs are close and misaligned pairs are far apart
  - Quick check question: What would happen to the model's alignment detection ability if we removed the contrastive loss component?

- Concept: Autoformalization sequence generation
  - Why needed here: To ensure the model understands the task of converting informal to formal mathematical statements, which provides the foundation for alignment evaluation
  - Quick check question: Why does training on the autoformalization task alone (without contrastive loss) still achieve reasonable performance?

- Concept: Embedding similarity metrics (cosine similarity)
  - Why needed here: To quantify the alignment between informal and formal representations in the learned embedding space
  - Quick check question: How would the alignment detection change if we used Euclidean distance instead of cosine similarity?

## Architecture Onboarding

- Component map:
  - Informal statement -> Autoformalization model -> Formal statement + Embeddings -> Scoring mechanism -> Alignment decision

- Critical path:
  1. Feed informal statement through model
  2. Generate formal statement (for training)
  3. Obtain embeddings for both informal and formal statements
  4. Compute certainty score (generation confidence)
  5. Compute similarity score (cosine similarity of embeddings)
  6. Combine scores to produce final alignment score

- Design tradeoffs:
  - Single model vs. separate models for generation and alignment: Single model saves parameters and training complexity but may require careful balancing of tasks
  - Certainty vs. similarity weighting: Equal weighting assumes both components are equally important
  - Threshold selection: 0.7 threshold balances precision and recall but may need adjustment for different datasets

- Failure signatures:
  - Low precision: Model flagging too many misaligned pairs as aligned (threshold too low or similarity component not discriminating well)
  - Low recall: Model missing actual misalignments (threshold too high or certainty component not capturing errors)
  - High BLEU but low alignment score: Model generates surface-similar text that lacks semantic alignment

- First 3 experiments:
  1. Ablation study: Remove contrastive loss and compare alignment detection performance
  2. Threshold sweep: Test different Valign thresholds and plot precision-recall curve
  3. Cross-dataset evaluation: Test model trained on FormL4 on MiniF2F to measure generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal temperature parameter τ for the contrastive loss in FORMALALIGN, and how does its value affect the model's performance across different datasets?
- Basis in paper: [inferred] The paper mentions a temperature parameter τ in the contrastive loss function (Equation 1) but does not specify its optimal value or analyze its impact on performance.
- Why unresolved: The paper does not provide a sensitivity analysis or ablation study on the temperature parameter, leaving its optimal value and impact on performance unclear.
- What evidence would resolve it: A comprehensive study varying the temperature parameter τ and evaluating its impact on alignment-selection score, precision, and recall across multiple datasets would provide insights into its optimal value and effects.

### Open Question 2
- Question: How does the performance of FORMALALIGN vary when using different pre-trained language models as the base architecture?
- Basis in paper: [explicit] The paper compares the performance of FORMALALIGN using Mistral-7B with other baseline models (Phi2-2.7B, LLaMA2-7B, DeepSeekMath-Base 7B) in Section 5.1, but does not explore a wider range of models or analyze the impact of model size and architecture.
- Why unresolved: The study is limited to a small set of baseline models, and the impact of using larger or differently architected models is not explored.
- What evidence would resolve it: Evaluating FORMALALIGN using a diverse range of pre-trained models, including larger and smaller models with different architectures, would provide insights into the generalizability and scalability of the approach.

### Open Question 3
- Question: How does the proposed alignment score Valign compare to other evaluation metrics, such as BLEU and BERTscore, in terms of capturing semantic alignment between natural and formal languages?
- Basis in paper: [explicit] The paper introduces the alignment score Valign in Section 3.3 and compares its performance to BLEU and BERTscore in a case study (Section H), but does not provide a comprehensive comparison across multiple datasets and misalignment types.
- Why unresolved: The case study is limited to a single example and does not provide a thorough comparison of the alignment score's performance against other metrics.
- What evidence would resolve it: A large-scale evaluation comparing the alignment score Valign to other metrics, such as BLEU and BERTscore, across multiple datasets and misalignment types would provide a comprehensive understanding of its strengths and limitations.

## Limitations
- Limited evaluation to two formal languages (Lean 4) may not capture broader generalizability
- Fixed threshold (θ = 0.7) for alignment detection lacks systematic justification
- Evaluation focuses on binary classification rather than assessing quality of generated formalizations

## Confidence
- **High Confidence**: The dual-task training architecture and the mathematical formulation of the loss functions are clearly specified and reproducible
- **Medium Confidence**: The reported performance improvements over GPT-4 are reliable, though the evaluation methodology could be more rigorous
- **Low Confidence**: The model's ability to generalize beyond the tested formal languages and mathematical domains remains uncertain

## Next Checks
1. Conduct ablation studies to determine the individual contributions of the certainty and similarity components to the final alignment score
2. Test the model on additional formal languages beyond Lean 4 to assess generalization capabilities
3. Implement cross-validation across different mathematical domains to evaluate domain robustness