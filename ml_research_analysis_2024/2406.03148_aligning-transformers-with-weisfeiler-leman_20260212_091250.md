---
ver: rpa2
title: Aligning Transformers with Weisfeiler-Leman
arxiv_id: '2406.03148'
source_url: https://arxiv.org/abs/2406.03148
tags:
- graph
- where
- matrix
- such
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a hierarchy of expressive pure transformers
  aligned with the k-dimensional Weisfeiler-Leman hierarchy. The authors improve theoretical
  expressivity for every order k0 and demonstrate feasibility in practice.
---

# Aligning Transformers with Weisfeiler-Leman

## Quick Facts
- **arXiv ID**: 2406.03148
- **Source URL**: https://arxiv.org/abs/2406.03148
- **Authors**: Luis Müller; Christopher Morris
- **Reference count**: 40
- **Primary result**: Transformers with k-WL expressive power for arbitrary k>1, requiring O(n^2k) runtime

## Executive Summary
This paper proposes a hierarchy of pure transformers aligned with the k-dimensional Weisfeiler-Leman (k-WL) hierarchy, improving theoretical expressivity for every order k>0 while maintaining practical feasibility. The authors introduce transformers that can simulate the k-WL algorithm by constructing appropriate token embeddings that encode both initial k-tuple colors and adjacency structure. They develop a theoretical framework for analyzing Laplacian and Spectral Positional Encodings (LPE/SPE), showing these enable node- and adjacency-identifying embeddings. The paper also introduces order transfer, allowing pre-training on lower-order tokenizations and fine-tuning on higher-order transformers. Experiments demonstrate competitive performance on large-scale molecular datasets and strong fine-tuning results on small molecular datasets.

## Method Summary
The method involves constructing transformers that simulate the k-WL hierarchy through specialized tokenization schemes. For order k, the tokenizer creates k-tuple tokens by concatenating node embeddings, structural embeddings (including degree information and node-level positional encodings), and atomic type embeddings. The structural embeddings must be "node-identifying" and "adjacency-identifying" to allow the self-attention mechanism to reconstruct the k-tuple adjacency matrix. The paper introduces two variants: k-GT with O(n^k) tokens and (k,s)-GT with O(n+s·m) tokens for sparse graphs. They develop Laplacian and Spectral Positional Encodings based on spectral graph theory, and propose order transfer for efficient training. The method is evaluated through pre-training on PCQM4Mv2 and fine-tuning on molecular datasets.

## Key Results
- Transformers with k-WL expressive power for arbitrary k>1, requiring O(n^2k) runtime
- (k,s)-GT variant reduces token count to O(n+s·m) while maintaining strictly higher expressivity than 1-WL for k>1
- Theoretical framework shows LPE and SPE are sufficiently node- and adjacency-identifying for use in k-GT
- Order transfer enables effective pre-training on lower-order tokenizations with successful fine-tuning on higher-order transformers
- Competitive performance on PCQM4Mv2 and strong fine-tuning results on small molecular datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers can simulate the k-WL hierarchy by using appropriately constructed token embeddings that encode both the initial k-tuple colors and the adjacency structure.
- Mechanism: The tokenizer builds embeddings for each k-tuple by concatenating node-level embeddings (including structural encodings and degrees) and atomic type embeddings. These embeddings are designed to be "node-identifying" and "adjacency-identifying," allowing the self-attention mechanism to reconstruct the k-tuple adjacency matrix during attention computation. The resulting attention matrix can then simulate the message aggregation steps of the k-WL algorithm.
- Core assumption: The structural embeddings can be constructed such that their inner products under learned projection matrices can distinguish whether two tuples share the same nodes (node-identifying) and whether two tuples are adjacent in the k-tuple sense (adjacency-identifying).
- Evidence anchors:
  - [abstract] "Transformers with k-WL expressive power for arbitrary k>1, requiring O(n^2k) runtime"
  - [section 3.2] "For order k and embedding dimension d of the tuple-level tokens, we first compute node-level tokens X(0,1) in Equation (1) and, in particular, the structural embeddings P(v) for each node v, with an embedding dimension of d and then concatenate node-level embeddings along the embedding dimension to construct tuple-level embeddings"
  - [corpus] Weak evidence for specific embedding construction details; main support comes from abstract and main text
- Break condition: If the structural embeddings cannot be made sufficiently node- and adjacency-identifying (e.g., for graphs with specific symmetries or pathological degree distributions), the attention mechanism cannot accurately reconstruct the k-tuple adjacency matrix, breaking the k-WL simulation.

### Mechanism 2
- Claim: Laplacian Positional Encodings (LPE) and Spectral Positional Encodings (SPE) are both sufficiently node- and adjacency-identifying, making them suitable for use in the k-GT architecture.
- Mechanism: LPE and SPE encode structural information from the graph Laplacian spectrum. LPE applies a DeepSet-like operation over eigenvector components paired with eigenvalues, while SPE applies a GIN-like operation to spectral features. Both encodings can approximate the matrix factorization of the Laplacian (L = UΣUT or L = PQT), which is node- and adjacency-identifying.
- Core assumption: The DeepSet and GIN-like operations in LPE and SPE, respectively, can approximate the required matrix factorizations arbitrarily closely for any graph.
- Evidence anchors:
  - [abstract] "develop a theoretical framework that allows the study of established positional encodings such as Laplacian PEs and SPE"
  - [section 4.1] "Theorem 7. Structural embeddings with LPE as node-level PE are sufficiently node- and adjacency-identifying"
  - [section 4.1] "Theorem 8. Structural embeddings with SPE as node-level PE are sufficiently node- and adjacency-identifying"
  - [corpus] Weak evidence; main support from stated theorems in the paper
- Break condition: If the graph Laplacian has repeated eigenvalues or the spectrum is degenerate in a way that prevents unique eigenvector factorization, the encodings may not be sufficiently identifying.

### Mechanism 3
- Claim: The (k, s)-GT variant reduces the number of tokens from nk to O(n + s·m) while maintaining expressivity strictly greater than the 1-WL for k > 1.
- Mechanism: The (k, s)-GT only considers k-tuples that induce subgraphs with at most s connected components. This restriction dramatically reduces the token count for sparse graphs while preserving the ability to distinguish more graphs than the 1-WL. The (2, 1)-GT with O(n + m) tokens has the same runtime as TokenGT but strictly higher expressivity.
- Core assumption: The (k, s)-WL variant (which only considers restricted tuples) remains strictly more powerful than the (k-1, s)-WL variant for k > 1.
- Evidence anchors:
  - [abstract] "Transformers aligned with the (k,s)-WL hierarchy, reducing token count to O(n+s·m)"
  - [section 3.2] "Morris et al. (2022) define the set of (k, s)-tuples V (G)^k_s... Morris et al. (2022) then define the (k, s)-LWL... We can now directly translate this modification to our token embeddings"
  - [corpus] Weak evidence; main support from abstract and section 3.2
- Break condition: If s is too small relative to k, the expressivity may drop below what's needed for practical applications, or if the graph has many small connected components, the token reduction benefit may be diminished.

## Foundational Learning

- Concept: Weisfeiler-Leman hierarchy and its variants (k-WL, δ-k-WL, (k,s)-WL)
  - Why needed here: The entire paper is built around aligning transformers with this hierarchy to establish and improve theoretical expressivity guarantees
  - Quick check question: What is the key difference between k-WL and δ-k-WL in terms of how they update tuple colors?

- Concept: Graph Laplacians and spectral graph theory
  - Why needed here: LPE and SPE are built on spectral properties of the graph Laplacian, and understanding these is crucial for implementing and debugging the positional encodings
  - Quick check question: Why does the graph Laplacian have real, non-negative eigenvalues for undirected graphs without self-loops?

- Concept: Multi-head attention mechanism in transformers
  - Why needed here: The paper's core contribution relies on understanding how attention can simulate message passing in GNNs, specifically how the attention matrix can be made to approximate adjacency or normalized adjacency matrices
  - Quick check question: How does the softmax operation on the attention matrix affect its ability to represent the adjacency matrix directly?

## Architecture Onboarding

- Component map:
  - Tokenizer -> Transformer layers -> Output head
  - Node embeddings + Structural embeddings + Atomic type embeddings -> k-tuple tokens -> Attention computation -> Feed-forward updates -> Task-specific output

- Critical path:
  1. Tokenization (most critical - determines expressivity)
  2. Attention computation (must approximate tuple adjacency)
  3. Feed-forward updates (must preserve expressivity)
  4. Output projection (task-specific)

- Design tradeoffs:
  - Token count vs. expressivity: Higher k increases expressivity but also token count (O(n^k) vs. O(n + s·m))
  - Embedding dimension vs. parameter efficiency: Larger d increases representational capacity but also parameters
  - Number of attention heads vs. runtime: More heads needed for higher k but increase O(n^2k) complexity
  - Choice of PE (LPE vs. SPE): LPE simpler but SPE offers stability benefits

- Failure signatures:
  - Poor performance on graphs with high symmetry (expressivity not translating to practical utility)
  - Memory issues with large graphs and high k values
  - Degraded performance when using SPE vs. LPE on certain tasks (stability vs. expressivity tradeoff)
  - No improvement from order transfer when downstream task benefits from higher-order features

- First 3 experiments:
  1. Verify tokenizer creates correct k-tuple embeddings by checking that initial colors respect atomic types and node labels
  2. Test attention mechanism can approximate tuple adjacency by computing attention scores and comparing to ground truth
  3. Validate expressivity by testing on BREC benchmark (1-WL to 4-WL indistinguishable graph pairs)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Laplacian and Spectral Positional Encodings (LPE and SPE) compare in practice when used with higher-order transformers (k-GT) for molecular learning tasks?
- Basis in paper: Explicit. The paper states that LPE and SPE are sufficiently node- and adjacency-identifying and evaluates both on downstream tasks.
- Why unresolved: The paper shows that SPE consistently leads to better performance on the BREC benchmark and fine-tuning experiments, but does not provide a clear explanation for this difference or determine which PE is universally better.
- What evidence would resolve it: Systematic ablation studies comparing LPE and SPE across diverse molecular datasets and graph types, along with theoretical analysis of their expressive power in different regimes.

### Open Question 2
- Question: Does the (k, s)-WL hierarchy converge to the full k-WL hierarchy for certain values of s, and if so, what is the minimal s required for convergence?
- Basis in paper: Explicit. The paper mentions that the (k, s)-WL is a more efficient variant of k-WL but does not characterize its relationship to the full k-WL.
- Why unresolved: While the paper shows that the (2, 1)-GT is strictly more expressive than 1-WL, it does not establish how quickly the (k, s)-WL hierarchy approaches the full k-WL hierarchy as s increases.
- What evidence would resolve it: Mathematical proofs establishing the relationship between (k, s)-WL and k-WL, or empirical studies showing the point at which increasing s no longer improves expressive power.

### Open Question 3
- Question: Can higher-order transformers (k-GT) effectively leverage their increased expressivity on real-world graph learning tasks beyond molecular datasets?
- Basis in paper: Inferred. The paper demonstrates strong performance on molecular datasets but only briefly mentions experiments on non-molecular datasets (CS and PHOTO).
- Why unresolved: The paper focuses primarily on molecular datasets where higher-order expressivity may be particularly valuable, but does not explore whether this advantage extends to other domains like social networks or citation graphs.
- What evidence would resolve it: Extensive experiments applying k-GT to diverse graph learning tasks including social networks, knowledge graphs, and citation networks, with comparison to state-of-the-art methods.

### Open Question 4
- Question: What is the theoretical relationship between the expressive power of transformers with node-level positional encodings (LPE/SPE) and the k-WL hierarchy?
- Basis in paper: Explicit. The paper shows that LPE and SPE enable node- and adjacency-identifying embeddings but does not fully characterize how this relates to k-WL expressivity.
- Why unresolved: While the paper demonstrates that LPE and SPE are sufficient for the k-GT to simulate k-WL, it does not establish whether these PEs are necessary or what their expressive power would be without the k-GT architecture.
- What evidence would resolve it: Theoretical analysis proving the expressive limits of transformers with LPE/SPE alone, without the k-GT tokenization scheme, and comparison to known bounds for k-WL.

## Limitations
- Exponential scaling with k makes high-order transformers computationally prohibitive for larger graphs
- Node- and adjacency-identifying properties of LPE/SPE rely on assumptions about unique eigenvector factorization that may not hold for graphs with repeated eigenvalues
- Order transfer framework shows mixed results with inconsistent improvements across different downstream tasks
- Limited empirical validation of (k,s)-GT expressivity gains beyond sparse graphs and specific graph categories

## Confidence
- **High confidence**: The theoretical alignment between transformers and k-WL hierarchy (Mechanism 1) is well-established mathematically, supported by proofs in sections 3.1-3.2 and the core equations for tokenization
- **Medium confidence**: The node- and adjacency-identifying properties of LPE and SPE (Mechanism 2) are theoretically proven, but the practical implementation details and neural network architectures for ρ and ϕ functions are underspecified
- **Medium confidence**: The (k,s)-GT expressivity claims (Mechanism 3) are supported by theoretical analysis, but empirical validation is limited to specific graph categories and sparse graphs
- **Low confidence**: The practical benefits of order transfer are not thoroughly validated, with only limited experimental results showing inconsistent improvements across different tasks

## Next Checks
1. **Expressivity validation on BREC benchmark**: Implement the full tokenization scheme and test on the BREC benchmark's graph pairs that are indistinguishable by 1-WL but should be separated by 2-GT. Verify that the (2,1)-GT correctly distinguishes at least 95% of these pairs.

2. **Positional encoding implementation audit**: Implement both LPE and SPE with the exact neural network architectures specified (or clarified) and test on a set of synthetic graphs with known Laplacian spectra. Verify that both encodings produce unique embeddings for non-isomorphic graphs and that SPE provides stability benefits for graphs with repeated eigenvalues.

3. **Order transfer ablation study**: Pre-train transformers at orders 2 and 3 on PCQM4Mv2, then fine-tune on molecular classification tasks. Compare performance against direct training at each order to quantify the practical benefits and identify which task characteristics determine successful order transfer.