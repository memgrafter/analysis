---
ver: rpa2
title: 'FairSample: Training Fair and Accurate Graph Convolutional Neural Networks
  Efficiently'
arxiv_id: '2401.14702'
source_url: https://arxiv.org/abs/2401.14702
tags:
- graph
- node
- fairness
- fairsample
- gcns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of training Graph Convolutional
  Networks (GCNs) that are both fair (satisfy demographic parity) and accurate. The
  key insight is that graph structure bias, node attribute bias, and model parameters
  all contribute to unfairness in GCNs.
---

# FairSample: Training Fair and Accurate Graph Convolutional Neural Networks Efficiently

## Quick Facts
- arXiv ID: 2401.14702
- Source URL: https://arxiv.org/abs/2401.14702
- Authors: Zicun Cong; Shi Baoxu; Shan Li; Jaewon Yang; Qi He; Jian Pei
- Reference count: 40
- Primary result: Achieves up to 65.5% improvement in fairness at the cost of at most 5.0% accuracy decrease

## Executive Summary
FairSample addresses the challenge of training Graph Convolutional Networks (GCNs) that are both fair and accurate by identifying and mitigating three sources of bias: graph structure, node attributes, and model parameters. The framework operates in two phases: first injecting edges between nodes with different sensitive values but similar features, then using a reinforcement learning-based computation graph sampler to select balanced and informative neighbors. Experiments on six real-world datasets demonstrate that FairSample achieves significant fairness improvements while maintaining high accuracy compared to state-of-the-art fair GCN methods.

## Method Summary
FairSample is a two-phase framework that jointly mitigates bias in graph structure, node attributes, and model parameters to achieve fair and accurate GCN training. Phase 1 uses an edge injector to add edges between nodes with different sensitive values but similar features and same class labels, improving inter-group connectivity. Phase 2 employs a reinforcement learning-based computation graph sampler that selects balanced and informative neighbors for training, ensuring fairness without sacrificing accuracy. The framework also includes a regularization objective to optimize fairness. The approach is evaluated on six real-world datasets, showing up to 65.5% improvement in fairness with at most 5.0% accuracy decrease compared to state-of-the-art methods.

## Key Results
- Achieves up to 65.5% improvement in demographic parity violation (∆DP) compared to state-of-the-art fair GCN methods
- Maintains accuracy with at most 5.0% decrease while significantly improving fairness
- Demonstrates effectiveness across six diverse real-world graph datasets including Cora, Citeseer, and Amazon
- Shows that jointly addressing graph structure, node attribute, and model parameter biases yields multiplicative improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph structure bias can be reduced by adding edges between nodes with different sensitive values but similar features
- Mechanism: The edge injector adds connections across sensitive groups while preserving utility by only linking similar nodes with the same class labels
- Core assumption: Nodes with similar features and the same class label are more likely to be informative when connected across sensitive groups
- Evidence anchors:
  - [abstract] "First, we inject edges across nodes that are in different sensitive groups but similar in node features"
  - [section] "Theorem 1 suggests that injecting inter-group edges into the input graph may improve the demographic parity of the GCN built on the modified graph"
- Break condition: If added edges introduce too much noise or connect dissimilar nodes, accuracy may degrade faster than fairness improves

### Mechanism 2
- Claim: Sampling can be learned to balance demographic parity and accuracy simultaneously
- Mechanism: Reinforcement learning trains a policy that combines stratified sampling (for fairness) and similarity sampling (for accuracy) to select neighbors for GCN training
- Core assumption: A learnable sampling policy can find an optimal tradeoff between balanced representation and informative features
- Evidence anchors:
  - [abstract] "Second, to enhance model fairness and retain model quality, we develop a learnable neighbor sampling policy using reinforcement learning"
  - [section] "Our key idea in fS is to sample the computation graphs T in the GCN fG such that each node in the sampled computation graph aggregates embeddings from a balanced and informative set of child nodes"
- Break condition: If the sampling policy overfits to training data or fails to generalize to unseen graph structures

### Mechanism 3
- Claim: Combining graph structure modification with learned sampling and regularization achieves better fairness-accuracy tradeoff than any single approach
- Mechanism: FairSample jointly optimizes three components: edge injection (structure), learned sampling (process), and fairness regularization (parameters)
- Core assumption: Addressing all three sources of bias simultaneously yields multiplicative improvements rather than additive ones
- Evidence anchors:
  - [abstract] "FairSample, a framework that jointly mitigates the three types of biases"
  - [section] "To address the bias in node features and model parameters, FairSample is complemented by a regularization objective to optimize fairness"
- Break condition: If any component introduces excessive computational overhead or conflicts with the others' objectives

## Foundational Learning

- Concept: Graph Convolutional Networks and feature aggregation
  - Why needed here: FairSample fundamentally modifies how GCNs aggregate features from neighbors, so understanding the base mechanism is essential
  - Quick check question: How does a GCN compute the embedding for a node using its neighbors in layer l?

- Concept: Demographic parity fairness
  - Why needed here: FairSample specifically targets demographic parity, requiring understanding of this fairness notion and its tradeoffs
  - Quick check question: What does it mean for a classifier to satisfy demographic parity across sensitive groups?

- Concept: Reinforcement learning for policy optimization
  - Why needed here: The computation graph sampler uses RL to learn an optimal sampling strategy
  - Quick check question: How does the log-derivative trick enable gradient-based learning of a sampling policy?

## Architecture Onboarding

- Component map:
  - Edge Injector: Adds inter-group edges between similar nodes
  - Pseudo Label Predictor: GCN that generates pseudo labels for unlabeled nodes
  - Computation Graph Sampler: RL-based policy that samples neighbors for training
  - GCN Classifier: Standard GCN trained with fairness regularization
  - Loss Function: Combines cross-entropy, demographic parity penalty, and RL reward

- Critical path: Edge injection → Pseudo label generation → Computation graph sampling → GCN training → Fairness evaluation

- Design tradeoffs:
  - Edge injection improves fairness but may introduce noise; limited to similar nodes with same labels
  - Sampling policy balances fairness vs accuracy; requires careful hyperparameter tuning
  - Regularization weight α controls fairness-accuracy tradeoff; too high degrades accuracy

- Failure signatures:
  - Fairness improves but accuracy drops significantly: Sampling policy may be too aggressive
  - Neither fairness nor accuracy improves: Edge injection may not be connecting appropriate nodes
  - Model becomes unstable during training: Adversarial components may need stability adjustments

- First 3 experiments:
  1. Test edge injection alone on a small homophilic graph to measure fairness improvement without sampling
  2. Test computation graph sampling with fixed edge set to isolate sampling effect
  3. Run full FairSample pipeline with varying α to find optimal fairness-accuracy balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FairSample be extended to optimize fairness notions beyond demographic parity, such as individual fairness or equalized odds?
- Basis in paper: [explicit] The authors mention this as a future direction, noting that "it is valuable to extend FairSample to improve the other fairness notions in GCNs, such as individual fairness [48]."
- Why unresolved: Extending to other fairness notions requires different theoretical analysis and potentially different techniques for mitigating bias in graph structures and model parameters.
- What evidence would resolve it: Empirical results showing FairSample's effectiveness on other fairness metrics across various datasets and graph structures.

### Open Question 2
- Question: What is the theoretical upper bound on the tradeoff between fairness and accuracy that FairSample can achieve?
- Basis in paper: [inferred] The authors observe a tradeoff between fairness and accuracy, but do not provide a theoretical analysis of its limits or bounds.
- Why unresolved: Characterizing the fundamental limits of fairness-accuracy tradeoffs requires theoretical analysis beyond the empirical results presented.
- What evidence would resolve it: Mathematical proofs establishing upper and lower bounds on fairness-accuracy tradeoffs achievable by FairSample under various conditions.

### Open Question 3
- Question: How does FairSample's performance scale with graph size and complexity?
- Basis in paper: [inferred] The authors report results on datasets with up to 67,797 nodes, but do not explore the performance limits with much larger or more complex graphs.
- Why unresolved: Understanding scalability is crucial for real-world applications, but requires extensive experimentation on very large graphs not covered in the paper.
- What evidence would resolve it: Experiments on graphs orders of magnitude larger than those studied, measuring runtime, memory usage, and fairness-accuracy tradeoffs.

## Limitations

- Performance validation limited to relatively small datasets (Cora, Citeseer, Pubmed, Amazon, Pokec, Freebase) without testing on larger, more complex graphs
- Edge injection strategy effectiveness depends heavily on the assumption that similar nodes across sensitive groups share the same class labels, which may not hold in all real-world scenarios
- The framework requires careful hyperparameter tuning to balance fairness and accuracy tradeoffs, which may be challenging in practice

## Confidence

- High confidence: The theoretical framework connecting graph structure bias, node attribute bias, and model parameter bias to unfairness in GCNs
- Medium confidence: The two-phase approach combining edge injection with learned sampling effectively improves fairness-accuracy tradeoff
- Medium confidence: The experimental results demonstrating significant fairness improvements on the tested datasets

## Next Checks

1. **Scalability test**: Evaluate FairSample on larger graphs (e.g., OGB datasets) to verify performance scales beyond the tested small datasets
2. **Label distribution analysis**: Test edge injection effectiveness when cross-group similar nodes have different class labels (challenging the core assumption)
3. **Ablation study**: Systematically disable each component (edge injection, sampling, regularization) to quantify individual contributions to fairness improvements