---
ver: rpa2
title: Strengthening Multimodal Large Language Model with Bootstrapped Preference
  Optimization
arxiv_id: '2403.08730'
source_url: https://arxiv.org/abs/2403.08730
tags:
- preference
- responses
- arxiv
- learning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of pretraining bias in Multimodal
  Large Language Models (MLLMs), where responses are overly similar to the pretraining
  corpus and neglect visual input. The proposed solution, Bootstrapped Preference
  Optimization (BPO), constructs preference datasets with negative responses bootstrapped
  from the model itself.
---

# Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization

## Quick Facts
- **arXiv ID:** 2403.08730
- **Source URL:** https://arxiv.org/abs/2403.08730
- **Reference count:** 40
- **Primary result:** Proposed BPO method improves MLLM performance by 41.4% on MM-Vet and 71.6% on LLaVA-Wild, surpassing larger baseline models

## Executive Summary
This paper addresses pretraining bias in Multimodal Large Language Models (MLLMs), where responses are overly similar to the pretraining corpus and neglect visual input. The authors propose Bootstrapped Preference Optimization (BPO), which constructs preference datasets with negative responses bootstrapped from the model itself. Two strategies are employed: image-weakened prompting to elicit pretraining bias, and LLM bias injection to modify ground truth responses. The preference dataset is then used for direct preference optimization, effectively suppressing pretrained LLM bias and enhancing grounding in visual inputs.

## Method Summary
The paper introduces Bootstrapped Preference Optimization (BPO) as a novel approach to address pretraining bias in MLLMs. BPO constructs a preference dataset containing negative responses generated through two mechanisms: image-weakened prompting, which uses image-unrelated prompts to elicit responses that ignore visual input, and LLM bias injection, which modifies ground truth responses to incorporate pretraining bias. These negative samples are paired with original responses to form preference pairs, which are then used for direct preference optimization. This process effectively suppresses the pretrained LLM bias and enhances the model's grounding in visual inputs, leading to improved performance across multiple benchmarks.

## Key Results
- BPO achieves a 41.4% total score on MM-Vet, demonstrating significant improvement in multimodal understanding
- On LLaVA-Wild, BPO attains a 71.6% total score, surpassing larger baseline models
- The method shows consistent performance gains across multiple benchmarks, validating its effectiveness in addressing pretraining bias

## Why This Works (Mechanism)
BPO works by creating a preference learning framework that explicitly targets pretraining bias. The image-weakened prompting strategy generates responses that ignore visual context, while LLM bias injection introduces pretraining corpus patterns into otherwise correct responses. By training on preference pairs that contrast these biased responses with grounded answers, the model learns to suppress its tendency to rely on memorized pretraining data and instead prioritize visual input. This direct preference optimization approach effectively re-aligns the model's behavior with multimodal understanding rather than pure language generation patterns from pretraining.

## Foundational Learning

**Preference Learning** - Why needed: To guide model behavior beyond supervised learning by ranking responses. Quick check: Verify preference dataset quality and diversity.

**Direct Preference Optimization** - Why needed: To optimize for preference satisfaction directly rather than proxy objectives. Quick check: Monitor KL divergence between policy updates.

**Bootstrapping** - Why needed: To generate challenging negative samples from the model itself. Quick check: Ensure bootstrapped samples represent actual model failures.

**Multimodal Grounding** - Why needed: To ensure visual inputs are properly incorporated into responses. Quick check: Validate visual consistency in generated responses.

**Bias Injection** - Why needed: To create controlled negative examples that isolate specific failure modes. Quick check: Confirm injected bias is detectable but not overwhelming.

## Architecture Onboarding

**Component Map:** Visual Encoder -> Fusion Module -> Language Model -> Preference Optimization Module -> Output Generator

**Critical Path:** Visual input → Visual Encoder → Fusion → Language Model → Response generation, with preference optimization applied to the Language Model parameters

**Design Tradeoffs:** BPO trades additional training compute for improved grounding, requiring careful balance between preference strength and catastrophic forgetting of useful pretraining knowledge

**Failure Signatures:** Over-suppression of pretraining knowledge leading to overly visual-dependent responses, or insufficient bias suppression resulting in continued pretraining bias

**First Experiments:**
1. Ablation study removing image-weakened prompting to quantify its contribution
2. Evaluation on held-out visual domains to test generalization
3. Human evaluation comparing baseline vs. BPO responses for bias detection

## Open Questions the Paper Calls Out

None

## Limitations

- The quality of bootstrapped negative samples directly impacts optimization effectiveness, yet validation of these samples is limited
- The evaluation framework could be more comprehensive to ensure robustness across different domains
- The approach may introduce new failure modes through the preference optimization process itself

## Confidence

**Core Claims:** Medium
- Performance improvements on benchmarks are well-documented
- The methodology is theoretically sound but relies on model-generated data
- Generalization beyond benchmark datasets requires further validation

## Next Checks

1. Conduct ablation studies removing either the image-weakened prompting or LLM bias injection components to quantify their individual contributions
2. Test the approach on held-out visual domains not present in the training data to assess generalization beyond benchmark datasets
3. Perform human evaluation studies to verify that improvements in benchmark scores correspond to actual improvements in multimodal understanding and reduced pretraining bias in real-world applications