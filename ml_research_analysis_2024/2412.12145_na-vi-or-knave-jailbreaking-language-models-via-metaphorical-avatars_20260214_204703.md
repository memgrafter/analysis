---
ver: rpa2
title: 'Na''vi or Knave: Jailbreaking Language Models via Metaphorical Avatars'
arxiv_id: '2412.12145'
source_url: https://arxiv.org/abs/2412.12145
tags:
- harmful
- llms
- jailbreak
- adversarial
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AVATAR is a black-box jailbreak method that uses adversarial metaphors
  to bypass LLM safety mechanisms. It maps harmful entities to innocuous ones via
  LLM imagination, then nests these metaphors into human-like interactions to extract
  harmful content.
---

# Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars

## Quick Facts
- arXiv ID: 2412.12145
- Source URL: https://arxiv.org/abs/2412.12145
- Reference count: 40
- AVATAR achieves >92% attack success rate on GPT-4 within 2 retries

## Executive Summary
AVATAR presents a novel black-box jailbreaking technique that exploits language models' imaginative capabilities to bypass safety mechanisms. By mapping harmful entities to innocuous metaphors and embedding these within human-like interactions, AVATAR successfully extracts restricted content from multiple advanced LLMs. The method demonstrates state-of-the-art performance and strong transferability across different model architectures, revealing potential vulnerabilities in how LLMs balance task completion with content safety.

## Method Summary
AVATAR operates through a two-stage process: first, it uses adversarial metaphors to transform harmful entities into seemingly benign concepts through LLM-driven imagination. Second, it nests these metaphorical mappings into human-like conversational interactions to extract harmful content while maintaining plausible deniability. The technique functions as a black-box attack, requiring no knowledge of the target model's internal architecture or safety mechanisms. AVATAR leverages the tension between LLMs' task-completion objectives and safety constraints, exploiting the models' tendency to engage with creative metaphorical reasoning even when it leads to prohibited content generation.

## Key Results
- Achieves over 92% attack success rate on GPT-4 within 2 retries
- Demonstrates strong transferability across multiple advanced LLMs
- Outperforms existing state-of-the-art jailbreaking methods

## Why This Works (Mechanism)
AVATAR exploits the inherent tension between language models' creative imaginative capabilities and their safety constraints. By mapping harmful entities to innocuous metaphors through adversarial reasoning, the technique bypasses safety filters that typically block direct requests for restricted content. The human-like interaction layer further obscures the malicious intent, making it difficult for safety mechanisms to distinguish between legitimate creative requests and jailbreaking attempts. This approach leverages the fundamental architectural tendency of LLMs to prioritize coherent task completion over strict content filtering when presented with metaphorical frameworks.

## Foundational Learning
- **Metaphorical Mapping**: Understanding how LLMs interpret and translate between conceptual domains is crucial for AVATAR's success. The technique relies on the model's ability to maintain consistent metaphorical relationships throughout interactions.
- **Safety Filter Bypass**: Knowledge of how LLMs' safety mechanisms detect and block harmful content is essential for designing effective metaphors that avoid triggering these filters.
- **Human-like Interaction Patterns**: Familiarity with conversational dynamics helps in crafting exchanges that maintain the metaphorical framework while extracting the desired content.
- **Black-box Attack Principles**: Understanding attack methodologies that don't require model internals is key to AVATAR's general applicability.
- **Imagination Exploitation**: Recognizing how LLMs use creative reasoning to bridge conceptual gaps enables the design of metaphors that lead to unintended content generation.
- **Transferability Analysis**: Understanding cross-model behavior helps predict and optimize attack success across different architectures.

## Architecture Onboarding

**Component Map**: Metaphor Generator -> Interaction Layer -> Content Extractor -> Safety Filter Bypass

**Critical Path**: The sequence Metaphor Generator -> Interaction Layer -> Content Extractor represents the core attack pipeline where each component builds upon the previous one to achieve the final goal of extracting restricted content.

**Design Tradeoffs**: The method balances metaphor complexity against interaction naturalness - more complex metaphors may provide better safety evasion but risk breaking conversational coherence, while simpler metaphors may be more natural but easier for safety systems to detect.

**Failure Signatures**: Attack failures typically manifest as either safety system intervention (content blocked), metaphorical breakdown (model loses the conceptual thread), or irrelevant responses (model misinterprets the metaphorical framework).

**Three First Experiments**:
1. Test single-turn metaphor application without interaction layer to isolate metaphor effectiveness
2. Vary metaphor complexity to find optimal balance between evasion and coherence
3. Test different harm categories to identify which types are most susceptible to metaphorical transformation

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Does not address whether metaphors might trigger alternative safety filters not measured in experiments
- Assumes LLM reliability in maintaining metaphorical consistency across multi-turn exchanges
- Relies heavily on specific evaluation criteria that may not generalize across all harmful content categories

## Confidence
Attack Effectiveness Claims: Medium
Transferability Claims: Medium
Fundamental Vulnerability Claims: Low

## Next Checks
1. Test AVATAR against models with explicitly different safety architectures (e.g., Claude, LLaMA with RLHF fine-tuning) to assess true generalizability beyond GPT-family models.

2. Evaluate whether safety systems that detect metaphorical or analogical reasoning can block AVATAR attacks, isolating whether the vulnerability is specifically about metaphors or general prompt manipulation.

3. Measure the coherence and quality degradation in AVATAR-generated responses across multiple turns to determine if the high success rate comes at the cost of meaningful output generation.