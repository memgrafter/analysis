---
ver: rpa2
title: Challenges Faced by Large Language Models in Solving Multi-Agent Flocking
arxiv_id: '2404.04752'
source_url: https://arxiv.org/abs/2404.04752
tags:
- agent
- agents
- flocking
- position
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  solve multi-agent flocking tasks, where multiple agents must stay close while avoiding
  collisions and maintaining formations. The authors test GPT-3.5-Turbo and GPT-4-Turbo
  in decentralized decision-making scenarios, where each agent independently uses
  an LLM to navigate.
---

# Challenges Faced by Large Language Models in Solving Multi-Agent Flocking

## Quick Facts
- arXiv ID: 2404.04752
- Source URL: https://arxiv.org/abs/2404.04752
- Reference count: 35
- Primary result: Large language models fail at multi-agent flocking tasks due to poor spatial reasoning and inability to maintain formations or distances

## Executive Summary
This paper investigates whether large language models can solve multi-agent flocking tasks where multiple agents must stay close while avoiding collisions and maintaining formations. The authors test GPT-3.5-Turbo and GPT-4-Turbo in decentralized decision-making scenarios where each agent independently uses an LLM to navigate. Experiments involve forming circles, triangles, and maintaining specific distances. Results show LLMs fail at multi-agent flocking due to poor spatial reasoning and inability to maintain formations or distances. Instead, agents either converge to a single point or diverge randomly. Even simple distance-keeping tasks fail, as LLMs cannot accurately interpret spatial coordinates.

## Method Summary
The study uses GPT-3.5-Turbo as the LLM for each agent with zero-shot prompting in a decentralized multi-agent flocking task. Agents are initialized at random positions and must form specific shapes (circle, triangle, line) while maintaining desired distances. Each round, agents receive prompts containing their current positions, flock shape, and distance constraints, then output their next position and reasoning. The environment updates positions based on LLM outputs and calculates Mean Absolute Error (MAE) measuring deviation from desired distances. No fine-tuning is performed, and the system tests increasingly simple scenarios from five-agent circle formation down to two-agent distance keeping.

## Key Results
- LLMs fail at multi-agent flocking, with agents converging to single points or diverging randomly instead of maintaining formations
- Even simple two-agent distance-keeping tasks fail as LLMs cannot accurately interpret spatial coordinates
- GPT-4-Turbo exhibits higher failure rates than GPT-3.5-Turbo due to format non-compliance issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail at multi-agent flocking because they cannot interpret spatial coordinates correctly to reason about relative positions.
- Mechanism: The LLM treats spatial coordinates as abstract tokens rather than geometric points, leading to incorrect spatial reasoning (e.g., mistaking negative signs or magnitudes).
- Core assumption: The LLM's token-level processing of coordinate tuples does not preserve geometric relationships.
- Evidence anchors:
  - [section]: "We think the LLMs are limited in reasoning the given position and distance information and making corresponding decisions. ... the LLM cannot reason the spatial positions giving only the coordinates of the points."
  - [abstract]: "Results show LLMs fail at multi-agent flocking due to poor spatial reasoning and inability to maintain formations or distances."
  - [corpus]: Weak/no direct evidence; this is an inference from test observations.
- Break condition: If the LLM is provided with visual or geometric representations instead of coordinate tuples, or if fine-tuned specifically on spatial reasoning tasks.

### Mechanism 2
- Claim: LLMs interpret "form a circle" or "maintain distance" as abstract concepts rather than geometric constraints, leading to convergence behavior.
- Mechanism: The LLM's language understanding maps flocking commands to clustering or consensus-like behavior (moving to average positions) instead of maintaining spatial formations.
- Core assumption: The LLM lacks grounding between linguistic descriptions of shapes and their geometric implementations.
- Evidence anchors:
  - [section]: "We tell the agents to form a circle and keep a desired distance of 5 units... Intuively, we would place the agents on the perimeter of the circle equidistant from the two closest neighbors. ... LLM takes the command of forming a circle as moving to an equidistant position from all other agents, which represents the center of the circle instead of some positions on the perimeter."
  - [abstract]: "agents either converge to a single point or diverge randomly."
  - [corpus]: Weak/no direct evidence; inference from behavioral observations.
- Break condition: If the LLM is given explicit geometric instructions or visual examples of the desired formation.

### Mechanism 3
- Claim: LLMs cannot maintain consistent distance reasoning across time steps, leading to oscillatory or divergent behavior in distance-keeping tasks.
- Mechanism: The LLM's reasoning about distance is context-dependent and fails to track relative positions consistently, causing agents to move away from each other when they should maintain distance.
- Core assumption: The LLM's reasoning is not persistent or spatially consistent across rounds.
- Evidence anchors:
  - [section]: "Instead of keeping the desired distance, drives further away from the stationary agent... the LLM of the active agent falsely reasons the situation and thinks the stationary agent is to the left of the active agent."
  - [abstract]: "Even simple distance-keeping tasks fail, as LLMs cannot accurately interpret spatial coordinates."
  - [corpus]: Weak/no direct evidence; inference from test observations.
- Break condition: If the LLM is provided with a memory mechanism or explicit distance-tracking logic.

## Foundational Learning

- Concept: Spatial reasoning with coordinate systems
  - Why needed here: Agents must interpret (x,y) positions and compute relative distances/vectors to navigate and maintain formations.
  - Quick check question: Given two points (3,4) and (7,1), what is the Euclidean distance between them?

- Concept: Decentralized decision-making in multi-agent systems
  - Why needed here: Each agent independently uses an LLM to decide movement based only on local information, requiring autonomous reasoning without central coordination.
  - Quick check question: In a decentralized system, can Agent A know Agent B's future position without communication?

- Concept: Flocking rules (separation, alignment, cohesion)
  - Why needed here: The task requires agents to stay close while avoiding collisions and maintaining formations, which are the core principles of flocking behavior.
  - Quick check question: Which flocking rule would cause an agent to move away from a neighbor that is too close?

## Architecture Onboarding

- Component map: Agent LLM -> Environment simulator -> Position tracker
- Critical path:
  1. Initialize random agent positions and flock target.
  2. For each round: Generate prompts → Call LLMs → Parse positions → Update positions → Check convergence.
  3. Terminate when max rounds reached or formation achieved.
- Design tradeoffs:
  - LLM choice (GPT-3.5 vs GPT-4): GPT-3.5 has lower failure rate but less reasoning power; GPT-4 has higher failure due to format non-compliance.
  - Prompt complexity: More detailed prompts might improve reasoning but increase token usage and latency.
  - Agent autonomy vs. coordination: Fully decentralized increases robustness but reduces global optimization.
- Failure signatures:
  - All agents converge to a single point (consensus failure).
  - Agents diverge randomly or oscillate (distance reasoning failure).
  - LLM ignores output format (GPT-4 case), causing system errors.
  - MAE remains high across rounds, indicating persistent spatial reasoning issues.
- First 3 experiments:
  1. Two-agent distance-keeping: Test basic spatial reasoning with minimal complexity.
  2. Three-agent triangle formation: Test shape understanding with simple coordination.
  3. Five-agent circle formation: Test complex shape and distance maintenance simultaneously.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning or specialized prompting techniques enable LLMs to successfully solve multi-agent flocking tasks?
- Basis in paper: [explicit] The authors conclude that current LLMs without fine-tuning lack spatial and collaborative reasoning, suggesting that enhanced models could potentially overcome these limitations
- Why unresolved: The paper only tests off-the-shelf LLMs (GPT-3.5-Turbo, GPT-4-Turbo) without any fine-tuning or specialized prompt engineering designed specifically for spatial reasoning tasks
- What evidence would resolve it: Successful implementation of multi-agent flocking using fine-tuned LLMs or with carefully designed spatial reasoning prompts, demonstrating proper formation maintenance and distance-keeping

### Open Question 2
- Question: How does incorporating visual information affect LLM performance in multi-agent flocking tasks?
- Basis in paper: [inferred] The authors suggest integrating visual data for enhanced spatial understanding, and another study cited in the paper found LLMs have poor performance reasoning about coordinate lists
- Why unresolved: All experiments use text-only prompts with coordinate information, without any visual context that might aid spatial reasoning
- What evidence would resolve it: Comparison of flocking performance between text-only prompts and prompts augmented with visual representations of agent positions and desired formations

### Open Question 3
- Question: What is the minimum complexity of spatial reasoning tasks that current LLMs can successfully solve?
- Basis in paper: [explicit] The authors systematically test increasingly simple scenarios (five agents forming circles, three agents forming triangles, two agents maintaining distance) and find failures at all levels
- Why unresolved: While the paper identifies LLMs' inability to solve these specific tasks, it doesn't establish the threshold at which LLMs can handle spatial reasoning tasks
- What evidence would resolve it: Systematic testing of LLM performance across a spectrum of spatial reasoning tasks of varying complexity, identifying the point where success rates significantly improve

## Limitations

- Experiments use zero-shot prompting without fine-tuning or specialized spatial reasoning modules
- Study focuses on two specific LLM variants (GPT-3.5-Turbo and GPT-4-Turbo) in simulated environments
- Results may not generalize to real-world deployment scenarios or other LLM architectures

## Confidence

- **High confidence**: LLMs fail at basic multi-agent flocking tasks as demonstrated by convergence to single points and failure to maintain distances/formations
- **Medium confidence**: The identified mechanisms (spatial reasoning limitations, geometric concept grounding issues, inconsistent distance tracking) are primary causes, though alternative explanations exist
- **Medium confidence**: Zero-shot prompting represents a reasonable baseline for assessing general LLM spatial reasoning capabilities

## Next Checks

1. **Fine-tuning validation**: Test whether fine-tuning the LLM on spatial reasoning and geometric formation tasks improves performance, distinguishing between fundamental architectural limitations and prompt engineering issues.

2. **Visual input comparison**: Repeat experiments using image-based prompts showing agent positions rather than coordinate tuples to determine if the failure stems from text-based spatial representation or fundamental geometric reasoning limitations.

3. **Hybrid architecture test**: Implement a hybrid approach where the LLM handles high-level decision-making while a geometric module computes precise movements, isolating whether LLMs can contribute meaningfully to multi-agent coordination when paired with specialized spatial processing.