---
ver: rpa2
title: Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors
arxiv_id: '2405.10529'
source_url: https://arxiv.org/abs/2405.10529
tags:
- adversarial
- visual
- prompt
- smoothvlm
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses patched visual prompt injection attacks on
  vision-language models (VLMs), where adversaries use adversarial patches to manipulate
  model outputs. The core method, SmoothVLM, leverages the observation that patched
  adversarial prompts are sensitive to pixel-wise randomization.
---

# Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors

## Quick Facts
- arXiv ID: 2405.10529
- Source URL: https://arxiv.org/abs/2405.10529
- Reference count: 17
- Key outcome: SmoothVLM reduces patched visual prompt injection attack success rates to below 5% on VLMs while maintaining 67.3% to 95.0% context recovery of benign images

## Executive Summary
This paper addresses the vulnerability of vision-language models (VLMs) to patched visual prompt injection attacks, where adversaries use adversarial patches to manipulate model outputs. The proposed defense, SmoothVLM, leverages the observation that patched adversarial prompts are sensitive to pixel-wise randomization. By applying randomized masking to visual prompts and using majority voting across multiple perturbed samples, SmoothVLM effectively defends against these attacks. The method achieves strong security (below 5% attack success rate) while maintaining reasonable usability (67.3% to 95.0% context recovery), and is computationally efficient compared to traditional randomized smoothing approaches.

## Method Summary
SmoothVLM defends against patched visual prompt injection attacks by exploiting the instability of adversarial patches to random pixel perturbations. The method applies random masking to a percentage q of the patch area, generating N perturbed versions of each input image. Each perturbed image is passed through the VLM independently, and the outputs are aggregated via majority voting. This approach disrupts the adversarial optimization process while maintaining the ability to recover benign visual content. The defense is specifically designed for VLMs where attacks target text generation rather than image classification, and demonstrates effectiveness against both standard and adaptive attacks.

## Key Results
- Attack success rate reduced to below 5% on both llava-1.5 and miniGPT4 models
- Context recovery of benign images maintained at 67.3% to 95.0% depending on perturbation level
- Requires only 10-20 runs per prompt compared to 100,000 in classic randomized smoothing
- Effective against both standard and adaptive attacks using Expectation Over Transformation (EOT)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SmoothVLM exploits the q-instability of patched adversarial prompts—random masking of the patch area disrupts the attack optimization process.
- Mechanism: Patch attacks are optimized to produce a target text sequence, but this optimization is brittle to pixel-wise randomness. By randomly masking a percentage q of the patch pixels and taking majority vote over N samples, the attack is likely to fail because at least half the samples will have enough perturbation to disrupt the adversarial goal.
- Core assumption: The adversarial patch must be at least partially masked to disrupt the attack, and the disruption is probabilistic but high for sufficient q and N.
- Evidence anchors:
  - [abstract]: "patched adversarial prompts exhibit sensitivity to pixel-wise randomization, a trait that remains robust even against adaptive attacks designed to counteract such defenses."
  - [section 3.3]: "Our investigations reveal vulnerabilities to randomized perturbations in the pixel space of the patched visual prompt injectors... random masking can consistently and effectively mitigate adversarial patch attacks with a sufficient amount of perturbation."
  - [corpus]: Weak; related papers focus on defenses for VLMs but do not provide direct empirical support for the q-instability claim.
- Break condition: If the attacker can place the patch outside the masked region or if the model is not sensitive to localized pixel corruption.

### Mechanism 2
- Claim: The majority vote aggregation over N perturbed samples ensures that even if some samples are still attackable, the most common output dominates and neutralizes the attack.
- Mechanism: Each perturbed image is passed through the VLM independently. The outputs are aggregated via majority vote. As long as the probability of a successful attack on a single perturbed image is below 0.5, increasing N drives the overall success rate toward zero.
- Core assumption: Outputs are conditionally independent given the perturbation distribution, and the vote threshold is set at >50%.
- Evidence anchors:
  - [abstract]: "by integrating majority voting with random perturbed visual prompts, our approach can defend the hidden visual prompt injectors with high probability."
  - [section 3.4]: "rather than passing a single perturbed prompt through the LLM, we obtain a collection of perturbed prompts with the same perturbation rate p, and then aggregate the predictions of this collection."
  - [corpus]: No direct empirical evidence in the corpus for the effectiveness of majority voting on VLM outputs; assumed from classic randomized smoothing literature.
- Break condition: If the model's outputs are highly correlated across perturbations or if the attack can be adapted to target the voting process.

### Mechanism 3
- Claim: The defense is robust to adaptive attacks (EOT) because the optimization landscape for VLM text generation is too complex to navigate under expectation over transformation.
- Mechanism: Adaptive attacks using EOT attempt to optimize the patch under the distribution of perturbations. However, because the loss for VLM text generation is computed over many tokens and iterations, EOT becomes computationally intractable and fails to find a robust adversarial patch.
- Core assumption: The attack optimization is already expensive (30+ minutes per example) and adding EOT multiplies this cost beyond feasibility.
- Evidence anchors:
  - [section 3.3]: "adaptive attacks are more challenging to launch in the era of multimodal language models... EOT will make the complexity at least an order higher, rendering the optimization intractable."
  - [figure 3 caption]: "The two figures demonstrate that EOT is extremely hard to optimize and subject to our identified q-instability as well."
  - [corpus]: No direct evidence; this is an argument based on computational complexity rather than empirical attack success.
- Break condition: If future attacks find more efficient optimization methods or if the EOT cost can be reduced.

## Foundational Learning

- Concept: Adversarial patch attacks in computer vision
  - Why needed here: SmoothVLM specifically defends against patch-based attacks, so understanding how patches work and why they are effective in physical settings is foundational.
  - Quick check question: What makes adversarial patches more practical in the real world compared to pixel-wise ℓ∞ attacks?

- Concept: Randomized smoothing and expectation over transformation
  - Why needed here: SmoothVLM is inspired by randomized smoothing; understanding how smoothing provides certified robustness and how EOT attacks try to break it is essential.
  - Quick check question: In classic randomized smoothing, how does the number of samples N relate to the confidence of the smoothed prediction?

- Concept: Multimodal model embeddings and next-token prediction
  - Why needed here: VLMs use image embeddings as prompts to generate text; the attack goal is to manipulate the generated text, not the image classification logits.
  - Quick check question: How does the VLM's text generation process differ from a standard image classifier in terms of attack surface?

## Architecture Onboarding

- Component map: Input image -> Perturbation generator (N times) -> VLM inference engine -> Voter (majority vote) -> Output selector -> Final result
- Critical path:
  1. Input image → Perturbation generator (N times)
  2. Each perturbed image → VLM inference
  3. All outputs → Voter (majority vote)
  4. Selected output → Final result
- Design tradeoffs:
  - More samples N → higher robustness but slower inference
  - Higher perturbation q → better defense but more distortion of benign images
  - Choice of perturbation type (mask vs swap vs replace) affects both robustness and recovery
- Failure signatures:
  - High attack success rate despite defense → perturbation q too low or N too small
  - Very low benign image recovery → perturbation q too high or inappropriate perturbation type
  - Slow inference → N set too high for available resources
- First 3 experiments:
  1. Vary N (2, 4, 6, 8, 10) with fixed q=10% to observe ASR drop
  2. Vary q (5%, 10%, 15%, 20%) with fixed N=10 to find the tradeoff point
  3. Compare perturbation types (mask, swap, replace) at q=10%, N=10 to find most effective

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SmoothVLM method's effectiveness scale with larger and more complex visual prompts beyond the tested 224×224 resolution?
- Basis in paper: [explicit] The paper states "In preliminary experiments on the latest LLaVA-v1.5-13b model (Liu et al., 2024, 2023a), which accepts 224 × 224 images" and notes "Patch attacks are also demonstrated to be much more physically achievable in the real world."
- Why unresolved: The experiments are limited to a specific model and resolution, and the paper acknowledges that larger adversarial patches are "much more physically achievable in the real world" without testing them.
- What evidence would resolve it: Testing SmoothVLM on larger resolution images and larger adversarial patches to verify if the 5% attack success rate threshold remains effective.

### Open Question 2
- Question: What is the theoretical upper bound on the perturbation percentage q that can be applied while still maintaining acceptable image quality and model performance?
- Basis in paper: [explicit] The paper states "We plot the distortion rate of VLM patch attack JIP (top row) and VAE (bottom row) for various values of the perturbation percentage q ∈ {5, 10, 15, 20} and the number of samples N ∈ {2, 4, 6, 8, 10}" and mentions "a minimal perturbation q=5% results in a higher distortion rate, suggesting that lower perturbation levels are insufficient to eliminate the concealed harmful context within the visual prompt and fail to restore the original visual semantics."
- Why unresolved: The paper only tests up to 20% perturbation and shows a trade-off between attack success rate and distortion rate, but doesn't establish an upper limit where image quality becomes unacceptable.
- What evidence would resolve it: Systematic testing with higher perturbation percentages (e.g., 25%, 30%, 40%) while measuring both attack success rates and human perception of image quality to find the practical upper bound.

### Open Question 3
- Question: How does SmoothVLM perform against adaptive attacks that specifically target the randomized smoothing mechanism itself?
- Basis in paper: [explicit] The paper states "Although our current focus is on visual prompt injections, the theoretical foundation of our work could potentially be extended to encompass both textual and visual prompts" and acknowledges limitations regarding "ℓp based adversarial attacks."
- Why unresolved: While the paper claims EOT attacks are ineffective, it doesn't comprehensively test against all possible adaptive attacks that could specifically target the randomization mechanism.
- What evidence would resolve it: Developing and testing adaptive attack strategies that specifically attempt to learn and exploit the randomization pattern used in SmoothVLM, such as attacks that model the perturbation distribution or use reinforcement learning to optimize attack strategies.

## Limitations

- The adaptive attack evaluation is weak - claims of EOT ineffectiveness are based on computational complexity arguments rather than empirical validation
- The perturbation mechanism doesn't explore whether attackers could adapt by placing patches in regions less likely to be masked
- Context recovery measurements lack detailed methodology and validation procedures

## Confidence

**High confidence**: The core observation that patched adversarial prompts exhibit sensitivity to pixel-wise randomization is well-supported by the experimental results. The basic mechanism of using majority voting over perturbed samples to reduce attack success rates is sound and demonstrates effectiveness across multiple perturbation types.

**Medium confidence**: The efficiency claims (10-20 runs vs 100,000 in classic randomized smoothing) are supported by the experimental setup, but the comparison assumes similar security guarantees without direct certification. The context recovery measurements are reported but lack detailed validation procedures.

**Low confidence**: The security claims against adaptive attacks are based primarily on computational complexity arguments rather than empirical evidence. The paper does not demonstrate that SmoothVLM would withstand attacks specifically designed to circumvent the randomization defense.

## Next Checks

1. **Adaptive attack validation**: Implement and evaluate a white-box adaptive attack where the attacker knows the defense mechanism and attempts to optimize the patch specifically to survive the randomization process. This would provide stronger evidence for the claimed robustness.

2. **Perturbation coverage analysis**: Systematically test whether attackers can evade the defense by placing patches in regions that are masked less frequently, or by using multiple distributed patches. This would validate whether the random masking strategy is robust to placement-aware attacks.

3. **Context preservation study**: Conduct a detailed analysis of what types of visual information are preserved versus lost under different perturbation levels and types. This should include qualitative examples showing specific cases where benign image understanding is degraded versus maintained.