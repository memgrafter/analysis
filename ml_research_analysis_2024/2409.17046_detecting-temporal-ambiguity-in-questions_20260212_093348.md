---
ver: rpa2
title: Detecting Temporal Ambiguity in Questions
arxiv_id: '2409.17046'
source_url: https://arxiv.org/abs/2409.17046
tags:
- search
- questions
- answer
- question
- ambiguous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TEMPAMBIQA, a manually annotated dataset
  of 8,162 open-domain questions designed to study the detection of temporally ambiguous
  questions. The dataset includes questions from existing QA datasets and labels them
  as either temporally ambiguous or unambiguous.
---

# Detecting Temporal Ambiguity in Questions

## Quick Facts
- arXiv ID: 2409.17046
- Source URL: https://arxiv.org/abs/2409.17046
- Reference count: 40
- Primary result: Qwen-110B outperforms other models for detecting temporal ambiguity in questions using search-based strategies

## Executive Summary
This paper introduces TEMPAMBIQA, a manually annotated dataset of 8,162 open-domain questions designed to study temporal ambiguity detection. The authors propose a novel approach using search strategies on disambiguated question variants (with specific years appended) to identify questions with different answers across time frames. They evaluate multiple search strategies against zero-shot and few-shot approaches using various LLMs, demonstrating that search-based methods effectively detect temporal ambiguity while balancing efficiency and performance.

## Method Summary
The authors create TEMPAMBIQA by annotating questions from existing QA datasets as temporally ambiguous or unambiguous. Their detection approach generates disambiguated question variants by appending specific years (e.g., "as of 2000") and uses search strategies to find pairs of variants with different answers. The search strategies include Linear Search (sequential year-by-year comparison), Skip-List Search (comparing answers at s intervals), Random Search (random year selection), and Divide and Conquer (midpoint comparison). They evaluate these strategies against LLM-based classification using zero-shot, few-shot, and fine-tuned BERT models.

## Key Results
- Qwen-110B consistently outperforms other LLMs including GPT-4 and LLaMA3 for temporal ambiguity detection
- Skip-List-2 search strategy achieves the best balance between efficiency and performance
- DAC (Divide and Conquer) method achieves linear search results with improved efficiency
- Few-shot learning with Mixtral-7B shows highest recall and F1 score, demonstrating effectiveness of minimal targeted training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal ambiguity can be detected by comparing answers to disambiguated variants across different time frames
- Mechanism: Constructs disambiguated questions by appending specific years, then uses search strategies to find pairs with different answers
- Core assumption: Different answers for the same question across years indicates temporal ambiguity
- Evidence anchors: [abstract] proposes search strategies based on disambiguated questions; [section 4.3] requires finding different answers from different time frames
- Break condition: LLM fails to provide different answers for genuinely ambiguous questions or hallucinates differences for unambiguous ones

### Mechanism 2
- Claim: Search strategies can efficiently detect temporal ambiguity by reducing comparisons needed
- Mechanism: Uses Skip-List, Random, and Divide and Conquer strategies instead of exhaustive linear search
- Core assumption: Temporal ambiguity can be detected with fewer comparisons without missing ambiguous questions
- Evidence anchors: [section 4.3] employs Skip-List to improve search efficiency; [section 6] DAC achieves same results as linear search with improved efficiency
- Break condition: Search strategies miss ambiguous questions due to large intervals or incorrectly flag unambiguous questions

### Mechanism 3
- Claim: LLMs can classify questions as temporally ambiguous or unambiguous
- Mechanism: Uses zero-shot, few-shot, and fine-tuned BERT models for classification
- Core assumption: LLMs understand temporal context and can distinguish ambiguous from unambiguous questions
- Evidence anchors: [abstract] establishes baseline methods including zero-shot and few-shot classification; [section 6] Mixtral-7B achieves highest recall in few-shot scenario
- Break condition: LLMs fail to generalize to new temporal contexts or show bias in classification

## Foundational Learning

- Concept: Temporal context
  - Why needed here: To understand how question meaning and answers change based on time frame
  - Quick check question: What is the answer to "Who was the president of the United States in 2020?" versus "Who is the president of the United States?"

- Concept: Disambiguation
  - Why needed here: To create unambiguous versions by specifying relevant time frame
  - Quick check question: How would you disambiguate "Who won the World Cup?" to make it unambiguous?

- Concept: Search strategies
  - Why needed here: To efficiently find pairs of disambiguated questions with different answers
  - Quick check question: What is the difference between linear search and binary search, and when would you use each?

## Architecture Onboarding

- Component map: Disambiguation Component -> Answer Equivalence Testing Component -> Search Component -> LLM Classification Component
- Critical path: Disambiguation -> Answer Equivalence Testing -> Search -> LLM Classification
- Design tradeoffs: Linear search is exhaustive but inefficient; search strategies are efficient but may miss some ambiguous questions; zero-shot/few-shot require no training but may be less accurate than fine-tuned models
- Failure signatures: LLM fails to provide different answers for genuinely ambiguous questions; search strategies miss ambiguous questions due to large intervals; LLM shows classification bias
- First 3 experiments:
  1. Test linear search on a small set of questions to establish baseline for temporal ambiguity detection
  2. Implement Skip-List Search and compare performance to linear search on same question set
  3. Train BERT model on subset of TEMPAMBIQA and evaluate on remaining questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance vary when using more granular time intervals (e.g., months instead of years) in disambiguation?
- Basis in paper: [inferred] Authors mention future work could explore refining granularity beyond yearly intervals
- Why unresolved: Paper only uses yearly intervals; no empirical data on finer time granularity impact
- What evidence would resolve it: Experiments with monthly/daily intervals comparing detection accuracy and efficiency with yearly intervals

### Open Question 2
- Question: How do dynamic time range determination strategies affect accuracy?
- Basis in paper: [explicit] Authors plan to explore dynamic strategies identifying timing of recurring events
- Why unresolved: Paper doesn't implement or test these dynamic strategies
- What evidence would resolve it: Implementing and testing dynamic time range strategies comparing performance with static ranges

### Open Question 3
- Question: What is the impact of training dataset size and diversity on fine-tuned model performance?
- Basis in paper: [inferred] Paper evaluates fine-tuned BERT but doesn't explore varying dataset size/diversity
- Why unresolved: Paper uses fixed dataset without experimenting with different sizes or compositions
- What evidence would resolve it: Experiments with different dataset sizes and compositions analyzing effect on model performance

### Open Question 4
- Question: How do search strategies compare in efficiency and accuracy with longer time ranges or complex temporal contexts?
- Basis in paper: [explicit] Paper compares strategies within fixed time range, not with longer/more complex contexts
- Why unresolved: Experiments limited to specific time range; no data on how strategies scale with complex scenarios
- What evidence would resolve it: Applying search strategies to datasets with extended/complex temporal contexts measuring efficiency and accuracy

## Limitations

- TEMPAMBIQA represents a filtered subset of questions rather than comprehensive temporal ambiguity sample
- Answer equivalence testing relies on LLM-generated answers, potentially introducing inconsistency or hallucination
- Search strategies trade completeness for speed and may miss ambiguity in certain temporal distributions

## Confidence

- Central claims about temporal ambiguity detection: Medium confidence
- Qwen-110B consistently outperforms other models: High confidence
- Search strategy efficiency claims: Medium confidence
- Major limitations: LLM behavior variability, dataset representativeness, reliance on LLM answers, completeness-speed tradeoff in search strategies

## Next Checks

1. **Cross-model validation**: Replicate temporal ambiguity detection experiments using multiple LLM providers and versions to verify results are not model-specific artifacts

2. **Edge case analysis**: Systematically test search strategies on questions with ambiguity occurring in narrow or non-linear temporal patterns to identify failure modes

3. **Human evaluation**: Conduct human annotation of TEMPAMBIQA sample to validate automatic temporal ambiguity labels and measure inter-annotator agreement, establishing ground truth for the task