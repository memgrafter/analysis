---
ver: rpa2
title: 'HLB: Benchmarking LLMs'' Humanlikeness in Language Use'
arxiv_id: '2409.15890'
source_url: https://arxiv.org/abs/2409.15890
tags:
- llms
- language
- human
- arxiv
- humanlikeness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HLB, a benchmark evaluating 20 large language
  models (LLMs) on their humanlikeness in language use. The benchmark consists of
  10 psycholinguistic experiments covering sound, word, syntax, semantics, and discourse
  levels.
---

# HLB: Benchmarking LLMs' Humanlikeness in Language Use

## Quick Facts
- **arXiv ID**: 2409.15890
- **Source URL**: https://arxiv.org/abs/2409.15890
- **Authors**: Xufeng Duan; Bei Xiao; Xuemei Tang; Zhenguang G. Cai
- **Reference count**: 34
- **Primary result**: HLB benchmark reveals LLMs' humanlikeness varies significantly across psycholinguistic tasks, with GPT-4o and Llama 3.1 showing highest alignment to human language patterns.

## Executive Summary
This paper introduces HLB (Humanlikeness Benchmark), a novel evaluation framework assessing 20 large language models on their humanlikeness in language use across 10 psycholinguistic experiments. The benchmark measures distributional similarity between human responses and LLM outputs using Jensen-Shannon divergence. Results show that while some models like GPT-4o and Llama 3.1 demonstrate relatively high humanlikeness, improvements in standard NLP metrics don't necessarily translate to greater humanlikeness, and some models show significant divergence from human patterns, particularly in semantic priming tasks.

## Method Summary
HLB evaluates LLMs on 10 psycholinguistic experiments covering sound symbolism, word associations, syntax processing, semantic relationships, and discourse comprehension. Human participants (N=2,000) completed these tasks through Qualtrics, generating response distributions for each item. LLMs generated 100 responses per item using default parameters. An auto-coding algorithm (using spaCy parsing and regex patterns) converted free-form responses into categorical distributions. Jensen-Shannon divergence measured the similarity between human and LLM response distributions, with lower divergence indicating higher humanlikeness. The benchmark provides humanlikeness scores at sound, word, syntax, semantics, and discourse levels.

## Key Results
- GPT-4o and Llama 3.1 models achieved the highest humanlikeness scores across most tasks
- Improvements in standard NLP metrics did not consistently correlate with greater humanlikeness
- Some models showed significant divergence from human patterns, particularly in semantic priming tasks
- Llama family models demonstrated overall increase in humanlikeness scores, with Meta-Llama-3.1-70B-Instruct achieving highest performance among Llama models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HLB captures fine-grained differences in humanlikeness that standard NLP benchmarks miss by focusing on distributional similarity of responses.
- Mechanism: By comparing response distributions between human participants and LLMs using Jensen-Shannon divergence, HLB measures alignment in the patterns of language use rather than just task completion accuracy.
- Core assumption: Human language behavior is characterized by consistent probabilistic response patterns across psycholinguistic tasks.
- Evidence anchors:
  - [abstract] "By comparing the response distributions between human participants and LLMs, we quantified humanlikeness through distributional similarity."
  - [section 3.4] "To quantify the humanlikeness of LLM responses, we used Jensen-Shannon (JS) divergence to compare the response distributions between human participants and LLMs."
  - [corpus] Found 25 related papers with average FMR 0.471, suggesting moderate corpus support for distributional methods.
- Break condition: If response distributions are too sparse or multimodal, JS divergence becomes unreliable; also breaks if human variability is too high relative to model differences.

### Mechanism 2
- Claim: Models trained on synthetic data can deviate from authentic human language patterns, making direct comparison with human data essential.
- Mechanism: HLB uses actual human responses as a gold standard, allowing detection of deviations caused by synthetic data loops.
- Core assumption: Human responses represent the true distribution of language use patterns, while synthetic data introduces systematic biases.
- Evidence anchors:
  - [abstract] "concerns have emerged that these models may deviate from authentic human language patterns, potentially losing the richness and creativity inherent in human communication."
  - [introduction] "concerns about models diverging from real-world human language patterns(del Rio-Chanona et al., 2024)."
  - [corpus] No direct evidence in corpus about synthetic data effects, so this is assumption-based.
- Break condition: If human response collection introduces its own biases (sampling, demographics), the comparison baseline becomes flawed.

### Mechanism 3
- Claim: LLMs' performance on standard NLP benchmarks does not necessarily translate to humanlike language use patterns.
- Mechanism: HLB reveals that improvements in task-based accuracy can coincide with decreased humanlikeness scores, highlighting the need for separate evaluation dimensions.
- Core assumption: Humanlike language use involves cognitive and communicative patterns beyond task completion.
- Evidence anchors:
  - [abstract] "we found that improvements in other performance metrics did not necessarily lead to greater humanlikeness, and in some cases, even resulted in a decline."
  - [section 4] "In contrast, the Llama family of models showed an overall increase in humanlikeness scores, with Meta-Llama-3.1-70B-Instruct achieving the highest performance among all Llama models."
  - [corpus] Corpus shows related work on psycholinguistic evaluation of LLMs, supporting this mechanism.
- Break condition: If the psycholinguistic tasks themselves are not truly representative of human language use, the dissociation between standard benchmarks and humanlikeness may be artificial.

## Foundational Learning

- Concept: Jensen-Shannon divergence and distributional similarity
  - Why needed here: Core metric for quantifying how closely LLM response distributions match human patterns
  - Quick check question: What is the main advantage of JS divergence over KL divergence for this benchmark?

- Concept: Psycholinguistic experimental design and priming effects
  - Why needed here: Understanding how to construct and interpret experiments like structural priming and semantic illusions
  - Quick check question: Why does structural priming measure humanlike syntax processing?

- Concept: Response coding and pattern extraction
  - Why needed here: Converting free-form LLM and human responses into comparable categorical distributions
  - Quick check question: How does the auto-coding algorithm ensure consistency across diverse response types?

## Architecture Onboarding

- Component map: Human data collection -> Auto-coding algorithm -> Distribution computation -> JS divergence calculation -> Humanlikeness scoring
- Critical path: Human data collection → Response coding → Distribution computation → JS divergence → Humanlikeness scoring
- Design tradeoffs: One-trial-per-run design vs. context effects; default model parameters vs. controlled diversity; broad linguistic coverage vs. task depth
- Failure signatures: Low Kappa scores in coding validation; zero variance in LLM responses; JS divergence near 1.0 (completely dissimilar)
- First 3 experiments:
  1. Sound-shape association (sound symbolism)
  2. Sound-gender association (phonological gender cues)
  3. Word length and predictivity (efficiency in communication)

## Open Questions the Paper Calls Out
None

## Limitations
- Data Collection Biases: Human participant pool may not be representative of global language patterns; one-trial-per-run design may reduce ecological validity
- Metric Reliability: JS divergence assumes stable response distributions, but some tasks show high variance in human responses that may not be well-captured by categorical distributions
- Model Configuration: Using default model parameters without systematic exploration of temperature, top-p, or other sampling controls introduces variability that may affect humanlikeness scores

## Confidence
- **High Confidence**: The methodology for computing humanlikeness through JS divergence and distributional similarity is sound and well-documented
- **Medium Confidence**: The claim that improvements in standard NLP metrics don't necessarily translate to greater humanlikeness is supported but requires additional validation
- **Low Confidence**: The interpretation that models displaying significant divergence from human patterns are "less humanlike" may oversimplify the relationship between statistical alignment and communicative effectiveness

## Next Checks
1. Replicate with controlled sampling parameters: Run the benchmark with systematically varied temperature and top-p values across all models to assess the impact of generation parameters on humanlikeness scores

2. Expand participant diversity: Collect human response data from geographically and demographically diverse populations to test the robustness of humanlikeness baselines across different language communities

3. Cross-task correlation analysis: Conduct a systematic analysis of how humanlikeness scores correlate across different psycholinguistic task types to identify whether models show consistent patterns of humanlike behavior or task-specific strengths and weaknesses