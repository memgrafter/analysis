---
ver: rpa2
title: Learning to Retrieve Iteratively for In-Context Learning
arxiv_id: '2406.14739'
source_url: https://arxiv.org/abs/2406.14739
tags:
- retriever
- iterative
- learning
- exemplars
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces iterative retrieval, a framework that enables
  retrievers to make iterative decisions through policy optimization. The authors
  address the problem of selecting optimal in-context learning (ICL) exemplars for
  semantic parsing tasks, which is a combinatorial optimization problem.
---

# Learning to Retrieve Iteratively for In-Context Learning

## Quick Facts
- arXiv ID: 2406.14739
- Source URL: https://arxiv.org/abs/2406.14739
- Authors: Yunmo Chen; Tongfei Chen; Harsh Jhamtani; Patrick Xia; Richard Shin; Jason Eisner; Benjamin Van Durme
- Reference count: 28
- The iterative retriever consistently outperforms baselines across various metrics and numbers of exemplars used for ICL

## Executive Summary
This paper introduces iterative retrieval, a framework that enables retrievers to make iterative decisions through policy optimization for in-context learning (ICL) exemplar selection. The authors address the combinatorial optimization problem of selecting optimal ICL exemplars for semantic parsing tasks. They propose a training procedure based on reinforcement learning that incorporates feedback from large language models (LLMs). The core innovation converts an off-the-shelf dense retriever into a stateful iterative retriever by adding only 4M additional parameters for state encoding, which achieves higher exact match (EM) and SMatch scores compared to baselines on semantic parsing datasets.

## Method Summary
The method introduces iterative retrieval as a framework for retrievers to make sequential decisions through policy optimization. The approach converts a dense retriever into a stateful iterative retriever by adding 4M parameters for state encoding. Training uses reinforcement learning with feedback signals from LLMs, treating exemplar selection as a sequential decision-making problem. The iterative retriever makes multiple retrieval decisions to select optimal exemplars for in-context learning on semantic parsing tasks, demonstrating improved performance over traditional one-shot retrieval methods.

## Key Results
- Iterative retriever achieves higher exact match (EM) and SMatch scores compared to BM25, Contriever, EPR, and CEIL baselines
- The method demonstrates generalization across different inference LLMs beyond the one used during training
- Consistent performance improvements of 1-3% absolute increase in EM/SMatch scores across SMCALFLOW, TreeDST, and MTOP datasets

## Why This Works (Mechanism)
The iterative approach works by allowing the retriever to refine its decisions through multiple retrieval steps, using state information to track progress and avoid redundant selections. The policy optimization framework treats exemplar selection as a sequential decision problem where each retrieval step can condition on previous choices. The LLM feedback provides rich, task-specific signals that guide the policy toward selections that improve downstream semantic parsing performance. The minimal additional parameters (4M) efficiently encode retrieval state without requiring full model fine-tuning, making the approach computationally practical.

## Foundational Learning
- **Reinforcement Learning**: Needed to optimize sequential retrieval decisions through policy gradients; quick check is verifying gradient flow through policy network
- **In-Context Learning (ICL)**: Core framework where selected exemplars guide model behavior; quick check is measuring performance variation with different exemplar sets
- **Dense Retrieval**: Foundation for converting retrievers to iterative versions; quick check is evaluating retrieval quality on retrieval benchmarks
- **Semantic Parsing**: Target task requiring structured output generation; quick check is computing exact match and SMatch metrics
- **State Encoding**: Mechanism for tracking retrieval history; quick check is verifying state representations capture retrieval context
- **LLM Feedback Integration**: Source of reward signals for policy optimization; quick check is testing feedback consistency across different exemplars

## Architecture Onboarding

**Component Map:**
Retriever Backbone -> State Encoder (4M params) -> Policy Network -> Action Selector -> Corpus -> Feedback Loop

**Critical Path:**
Retriever query -> Initial state encoding -> Policy action selection -> Exemplar retrieval -> LLM evaluation -> Reward signal -> Policy update

**Design Tradeoffs:**
- Parameter efficiency vs. retrieval performance: 4M additional parameters vs. full fine-tuning
- Feedback quality vs. training stability: LLM-based rewards vs. supervised signals
- State complexity vs. computational overhead: richer state representations vs. inference speed
- Sequential decision depth vs. training complexity: more steps vs. credit assignment difficulty

**Failure Signatures:**
- Policy collapse to degenerate exemplars (all selections converge to same examples)
- State forgetting where early retrieval decisions are overwritten by later ones
- Feedback noise causing policy to optimize for spurious correlations
- Overfitting to specific LLM feedback patterns that don't generalize

**First Experiments:**
1. Ablation test removing state encoding to measure performance degradation
2. Single-step retrieval comparison to verify iterative advantage
3. Cross-LLM generalization test using different inference models

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance gains are relatively modest (1-3% absolute improvement) over strong baselines
- RL training relies heavily on LLM feedback quality without exploring reward hacking scenarios
- The 4M parameter overhead, while small relative to full fine-tuning, may still be significant for resource-constrained applications

## Confidence
- **High confidence**: The iterative retrieval framework is technically sound and the core training methodology (policy optimization with LLM feedback) is correctly implemented
- **Medium confidence**: The generalization claims across different LLMs are supported but based on limited experiments with only one additional inference LLM
- **Medium confidence**: The combinatorial optimization framing is valid, but the practical impact of near-optimal exemplar selection on downstream task performance needs more thorough analysis

## Next Checks
1. Test the iterative retriever's performance when the feedback LLM differs substantially in architecture or training data from the one used during training, to verify true generalization
2. Conduct ablation studies removing the state encoding component to quantify the actual contribution of the 4M parameters to performance gains
3. Evaluate the method's robustness to noisy or adversarial exemplars in the corpus to assess real-world reliability beyond clean benchmark datasets