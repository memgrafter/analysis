---
ver: rpa2
title: TCG CREST System Description for the Second DISPLACE Challenge
arxiv_id: '2409.15356'
source_url: https://arxiv.org/abs/2409.15356
tags:
- diarization
- track
- speech
- speaker
- challenge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes speaker and language diarization systems developed
  for the Second DISPLACE Challenge 2024, focusing on multilingual and multi-speaker
  scenarios. The team investigated speech enhancement, voice activity detection, unsupervised
  domain categorization, and neural embedding extraction, using the SpeechBrain toolkit.
---

# TCG CREST System Description for the Second DISPLACE Challenge

## Quick Facts
- **arXiv ID:** 2409.15356
- **Source URL:** https://arxiv.org/abs/2409.15356
- **Reference count:** 0
- **Primary result:** Speaker diarization achieved 32.22% DER (7% relative improvement over baseline), language diarization achieved 43.76% DER (no improvement over baseline)

## Executive Summary
This paper presents the TCG CREST team's systems for the Second DISPLACE Challenge 2024, focusing on speaker and language diarization in multilingual, multi-speaker scenarios. The team explored various approaches including speech enhancement, voice activity detection, and neural embedding extraction using the SpeechBrain toolkit. Their final submissions used spectral clustering for both tasks, achieving a 7% relative improvement in speaker diarization but failing to improve upon the baseline in language diarization. The experiments were conducted on a high-performance machine with AMD Ryzen 9 7900X CPU, NVIDIA GeForce RTX 4090 GPU, and 128 GB RAM.

## Method Summary
The team used a pipeline approach for both tracks: speech enhancement (optional), neural SAD using Pyannote or Silero models, embedding extraction using pre-trained ECAPA-TDNN (Track 1) or ECAPA-TDNN plus XLS-R fusion (Track 2), fixed-length segmentation (2.0s with 0.4s overlap), and spectral clustering. For Track 1, VB-HMM-based re-segmentation was applied using Kaldi scripts. No fine-tuning of pre-trained models was performed due to resource constraints. The systems were evaluated using diarization error rate (DER) as the primary metric.

## Key Results
- Track 1 (Speaker diarization): Achieved 32.22% DER, representing a 7% relative improvement over the challenge baseline
- Track 2 (Language diarization): Achieved 43.76% DER, failing to improve upon the challenge baseline
- Speech enhancement techniques did not significantly improve performance in initial experiments
- Neural SAD models (Pyannote, Silero) were evaluated but not systematically validated for improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spectral clustering on ECAPA-TDNN embeddings achieves better DER than baseline for Track 1.
- Mechanism: ECAPA-TDNN provides compact, discriminative speaker representations that spectral clustering can effectively partition into speaker clusters.
- Core assumption: Speaker embeddings from ECAPA-TDNN trained on VoxCeleb are generalizable to the multilingual DISPLACE dataset.
- Evidence anchors:
  - [abstract]: "Our final submissions use spectral clustering for both the speaker and language diarization. We achieve about 7% relative improvement over the challenge baseline in Track 1."
  - [section]: "Our final submission in Track 1 consists of ECAPA-TDNN embeddings extracted with a pre-trained model trained on the VoxCeleb corpus."
- Break condition: If the DISPLACE dataset contains speakers or languages not well-represented in VoxCeleb, embedding quality degrades and clustering performance drops.

### Mechanism 2
- Claim: Weighted fusion of ECAPA-TDNN and XLS-R embeddings does not improve Track 2 performance.
- Mechanism: Language discrimination requires capturing both speaker-invariant and language-specific features; fusion with fixed weights (0.8/0.2) may not align these optimally.
- Core assumption: Fixed-weight fusion preserves complementary information from both models for language clustering.
- Evidence anchors:
  - [abstract]: "For Track 2, we fused the affinity scores obtained with embeddings from two types of embedding extractors... We did not obtain improvement over the challenge baseline in Track 2."
  - [section]: "The Track 2 submission... uses affinity matrix fusion with weight 0.8 for affinity matrix from ECAPA-TDNN embedding and 0.2 for affinity matrix from XLS-R embedding."
- Break condition: If language and speaker attributes are not orthogonal in the feature space, fusion weights cannot separate them effectively.

### Mechanism 3
- Claim: Pre-trained neural SAD models (Pyannote, Silero) are used but not validated for improvement.
- Mechanism: Neural SAD can provide more accurate speech/non-speech segmentation than statistical SAD, improving downstream diarization.
- Core assumption: Neural SAD outputs are cleaner than rVAD, leading to better speaker/language segment boundaries.
- Evidence anchors:
  - [section]: "For speech activity detection (SAD), we evaluated statistical SAD such as rVAD as well as neural SAD such as Pyannote and Silero."
  - [section]: "Our initial experiments with different speech enhancement methods did not significantly improve the performance over systems without speech enhancement techniques. We also attempted to extract SAD labels from the enhanced speech, but it did not help."
- Break condition: If SAD errors dominate the error budget, downstream embedding extraction and clustering cannot recover.

## Foundational Learning

- Concept: Speaker diarization pipeline stages
  - Why needed here: Understanding how speech enhancement → SAD → embedding extraction → clustering → re-segmentation fit together is critical for debugging and improving DER.
  - Quick check question: What is the role of re-segmentation (VB-HMM) in the speaker diarization pipeline, and why might it be skipped in language diarization?

- Concept: Embedding extraction and clustering
  - Why needed here: Choosing and tuning embedding models (ECAPA-TDNN, XLS-R) and clustering algorithms (spectral, AHC) directly impacts DER; knowing their strengths/weaknesses is key.
  - Quick check question: Why might spectral clustering outperform AHC for speaker diarization in multilingual data?

- Concept: Affinity matrix fusion
  - Why needed here: Language diarization requires separating languages, not speakers; fusion of speaker and language embeddings is a design choice that can fail if not aligned.
  - Quick check question: What are the risks of fixed-weight fusion for language diarization when speaker and language information are entangled?

## Architecture Onboarding

- Component map:
  Audio -> SAD (Pyannote/Silero) -> 2.0s segments with 0.4s overlap -> ECAPA-TDNN embeddings (Track 1) or ECAPA-TDNN+XLS-R fusion (Track 2) -> Affinity matrix -> Spectral clustering -> VB-HMM re-segmentation (Track 1) -> Speaker/language labels

- Critical path:
  1. Audio → SAD → segments
  2. Segments → embeddings → affinity matrix
  3. Affinity matrix → spectral clustering → cluster labels
  4. Labels → VB-HMM re-segmentation (Track 1)

- Design tradeoffs:
  - ECAPA-TDNN vs. XLS-R: speaker discrimination vs. language coverage
  - Fixed-weight fusion: simplicity vs. suboptimal alignment
  - No speech enhancement: faster but possibly noisier inputs

- Failure signatures:
  - High DER: likely SAD or embedding issues
  - Language confusion: embedding fusion not language-aligned
  - Cluster collapse: spectral clustering parameters or affinity matrix quality

- First 3 experiments:
  1. Compare DER with and without SAD (use ground truth speech segments)
  2. Test different fusion weights for Track 2 (e.g., 0.5/0.5, 0.9/0.1)
  3. Evaluate spectral clustering vs. AHC on the same embeddings for both tracks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does speech enhancement impact speaker and language diarization performance in multilingual scenarios?
- Basis in paper: [explicit] The authors note that initial experiments with different speech enhancement methods did not significantly improve performance over systems without speech enhancement techniques.
- Why unresolved: The paper indicates that speech enhancement techniques were explored but did not yield significant improvements, suggesting that the relationship between enhancement and diarization accuracy is not well understood in multilingual contexts.
- What evidence would resolve it: Systematic experiments comparing diarization performance with and without speech enhancement across different noise conditions and language mixtures would clarify the impact.

### Open Question 2
- Question: What is the optimal fusion strategy for combining embeddings from supervised (ECAPA-TDNN) and self-supervised (XLS-R) models in language diarization?
- Basis in paper: [explicit] The authors used a fixed weighting scheme (0.8 for ECAPA-TDNN and 0.2 for XLS-R) in their Track 2 submission, but did not explore other fusion strategies.
- Why unresolved: The paper does not explore different fusion strategies beyond simple weighted averaging, leaving open the question of whether alternative fusion methods might yield better performance.
- What evidence would resolve it: Experiments testing various fusion strategies (e.g., learned fusion, attention-based fusion, concatenation-based fusion) would determine the optimal approach.

### Open Question 3
- Question: How does fine-tuning pre-trained neural models on the specific DISPLACE challenge dataset affect diarization performance?
- Basis in paper: [explicit] The authors mention that due to resource and time constraints, they used publicly available pre-trained models and plan to investigate fine-tuning in the future.
- Why unresolved: The paper does not report on fine-tuning experiments, leaving open the question of whether domain adaptation would improve performance.
- What evidence would resolve it: Experiments comparing performance of pre-trained models versus fine-tuned models on the DISPLACE dataset would determine the benefit of adaptation.

## Limitations

- The paper lacks specific details on spectral clustering and VB-HMM hyperparameters, making exact reproduction difficult
- The Track 2 fusion mechanism using fixed weights (0.8/0.2) is not explained in terms of how affinity matrices were combined or why this specific ratio was chosen
- No ablation studies are provided to isolate the impact of SAD, speech enhancement, or embedding choice on final DER

## Confidence

- **Track 1 DER improvement (32.22%, 7% relative)**: Medium - the methodology is clear but lacks hyperparameter details and ablation evidence
- **Track 2 fusion failure (43.76%, no improvement)**: Medium - the fusion approach is described but mechanism and weight optimization are unclear
- **Speech enhancement and SAD experiments**: Low - stated that no improvement was found, but no systematic comparison or error analysis provided

## Next Checks

1. Conduct ablation study comparing DER with ground truth speech segments vs. Pyannote/Silero SAD outputs to quantify SAD impact
2. Test alternative fusion weight combinations (0.5/0.5, 0.9/0.1) and affinity matrix combination strategies for Track 2
3. Evaluate spectral clustering vs. AHC on identical embeddings for both tracks to verify clustering algorithm choice