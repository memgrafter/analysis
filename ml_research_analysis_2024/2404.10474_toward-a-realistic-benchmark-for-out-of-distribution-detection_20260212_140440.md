---
ver: rpa2
title: Toward a Realistic Benchmark for Out-of-Distribution Detection
arxiv_id: '2404.10474'
source_url: https://arxiv.org/abs/2404.10474
tags:
- classes
- detection
- samples
- datasets
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a benchmark for OOD detection that addresses
  the lack of realistic evaluation settings by considering semantic similarity between
  classes rather than dataset origin. The proposed benchmark uses ImageNet and Places365,
  assigning classes as ID or OOD based on semantic affinity.
---

# Toward a Realistic Benchmark for Out-of-Distribution Detection

## Quick Facts
- arXiv ID: 2404.10474
- Source URL: https://arxiv.org/abs/2404.10474
- Authors: Pietro Recalcati; Fabio Garcea; Luca Piano; Fabrizio Lamberti; Lia Morra
- Reference count: 40
- Primary result: Confidence-based techniques outperform classifier-based ones on near-OOD samples in semantic similarity-based benchmarks

## Executive Summary
This paper introduces a novel benchmark for out-of-distribution (OOD) detection that addresses limitations in traditional evaluation methods by focusing on semantic similarity between classes rather than dataset origin. The benchmark uses ImageNet and Places365 datasets, assigning classes as in-distribution (ID) or out-of-distribution (OOD) based on their semantic affinity measured through WordNet-based similarity metrics. The authors compare manual and automatic labeling methods and evaluate various OOD detection techniques, finding that confidence-based approaches like maximum softmax probability and temperature scaling generally outperform classifier-based methods on near-OOD samples. The benchmark is publicly available and aims to provide a more realistic assessment of OOD detection methods for real-world deployment scenarios.

## Method Summary
The benchmark assigns ID/OOD status based on semantic similarity between classes from different datasets (ImageNet and Places365) using WordNet-based metrics like Wu-Palmer and Path similarity. Classes are labeled as ID or OOD depending on their semantic affinity with the training set, with higher similarity scores indicating ID status. The evaluation compares multiple OOD detection methods including maximum softmax probability (MSP), temperature scaling (TS), maximum logit value (MLV), ODIN, and OODL with One-Class SVM. The benchmark includes both automatic semantic similarity-based labeling and manual annotation approaches, with performance measured using AUROC, FPR@95%TPR, and Detection Error metrics on ResNet50 models pre-trained on Places365.

## Key Results
- Confidence-based techniques (MSP, TS, MLV) outperform classifier-based methods (OODL) on near-OOD samples
- Performance of OOD detection methods varies significantly depending on the chosen benchmark and semantic similarity of test classes
- Manual annotation provides more nuanced semantic relationships than purely automatic WordNet-based metrics
- The benchmark reveals that traditional inter-dataset evaluation protocols underestimate semantic overlap between classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark assigns ID/OOD status based on semantic similarity between classes rather than dataset origin, which better reflects real-world deployment scenarios.
- Mechanism: By using WordNet-based similarity metrics (Wu-Palmer, Path) to compute semantic distance between ImageNet and Places365 classes, the benchmark captures near-OOD cases that traditional inter-dataset methods miss.
- Core assumption: Semantic similarity correlates with visual similarity and prediction confidence in pre-trained models.
- Evidence anchors:
  - [abstract] "assigns individual classes as in-distribution or out-of-distribution depending on the semantic similarity with the training set"
  - [section IV] "We argue that the standard inter-dataset evaluation protocol... ignores or underestimate the semantic overlap between classes"
  - [corpus] Weak evidence - related papers focus on topology and entropy-based methods but don't directly support semantic-based benchmarks
- Break condition: If semantic similarity doesn't correlate with visual similarity, the benchmark may misclassify classes, leading to unreliable OOD detection evaluation.

### Mechanism 2
- Claim: Confidence-based techniques (MSP, TS, MLV) outperform classifier-based ones (OODL) on near-OOD samples because they better handle semantic proximity.
- Mechanism: Softmax-based confidence scores capture subtle differences in prediction certainty for classes that are semantically similar but technically OOD.
- Core assumption: Near-OOD samples produce intermediate confidence scores that can be distinguished from both high-confidence ID and low-confidence far-OOD samples.
- Evidence anchors:
  - [abstract] "confidence-based techniques may outperform classifier-based ones on near-OOD samples"
  - [section VI] "confidence-based techniques like TS and MLV generally surpass SVM on the FACETS OOD Detection datasets"
  - [corpus] Weak evidence - related papers discuss entropy-based detection but don't specifically address confidence vs classifier performance on near-OOD
- Break condition: If near-OOD samples produce confidence scores indistinguishable from ID samples, confidence-based methods lose their advantage.

### Mechanism 3
- Claim: Manual annotation of OODness provides more realistic benchmarks than purely automatic methods because it captures nuanced semantic relationships.
- Mechanism: Human annotators can identify semantic relationships (meronym-holonym, scene-object) that automated WordNet metrics miss.
- Core assumption: Human judgment of semantic similarity better reflects real-world classification expectations than algorithmic similarity measures.
- Evidence anchors:
  - [section IV] "as an alternative to automatic path-based metrics, we manually annotated classes from the the ImageNet and SUN397 datasets"
  - [section V] "manual inspection is necessary to recognize and pair similar classes"
  - [corpus] No direct evidence in corpus papers about manual vs automatic annotation effectiveness
- Break condition: If manual annotations introduce subjective bias or don't align with actual model behavior, the benchmark loses validity.

## Foundational Learning

- Concept: Semantic similarity metrics (Wu-Palmer, Path similarity)
  - Why needed here: These metrics determine which classes are considered ID vs OOD in the benchmark
  - Quick check question: What is the range of values for Wu-Palmer similarity, and what does a score of 0.8 indicate about class relationship?

- Concept: Out-of-distribution detection evaluation metrics (AUROC, FPR@95%TPR, Detection Error)
  - Why needed here: These metrics quantify OOD detection performance across different benchmark configurations
  - Quick check question: If a method achieves 95% TPR at 20% FPR, what is its FPR@95%TPR score?

- Concept: Temperature scaling and input perturbation in ODIN
  - Why needed here: These techniques enhance confidence-based OOD detection by amplifying differences between ID and OOD samples
  - Quick check question: How does increasing the temperature T affect the softmax output distribution, and why does this help OOD detection?

## Architecture Onboarding

- Component map: WordNet semantic similarity engine (Wu-Palmer, Path metrics) -> Class labeling (automatic/manual) -> Dataset generation pipeline (Places365-Standard, ImageNet subsets) -> OOD detection method implementations (MSP, TS, MLV, ODIN, IP TS MLV, OODL) -> Evaluation framework (AUROC, FPR@95%TPR, Detection Error metrics)

- Critical path: Semantic similarity computation → Class labeling (automatic/manual) → Dataset creation → OOD detection method application → Performance evaluation

- Design tradeoffs:
  - Automatic vs manual labeling: Speed and consistency vs nuance and realism
  - Semantic vs visual similarity: Theoretical correctness vs practical model behavior
  - Confidence vs classifier-based methods: Simplicity vs explicit modeling

- Failure signatures:
  - Poor correlation between semantic similarity scores and actual model behavior
  - Inconsistent manual annotations across different annotators
  - OOD detection methods perform equally well across all benchmark variants
  - High computational cost preventing practical evaluation

- First 3 experiments:
  1. Compute semantic similarity matrix between all ImageNet and Places365 classes using Wu-Palmer and Path metrics, visualize correlation between metrics
  2. Implement simple baseline OOD detection using MSP on FACETS T1 dataset, measure performance across different semantic similarity thresholds
  3. Compare manual vs automatic labeling agreement rates on a subset of 100 classes, analyze sources of disagreement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we automate the process of assigning OOD labels at the image level rather than the class level, given that images within the same class may have vastly different semantic content?
- Basis in paper: [inferred] The paper mentions that even within the same class, images can have different scenes requiring some to be categorized as ID and others as OOD, and manual inspection of every image is impractical.
- Why unresolved: Current methods rely on class-level semantic similarity, which may not capture the variability within classes. Automated image-level OOD detection would require more sophisticated techniques that can analyze individual images' content and context.
- What evidence would resolve it: Development and validation of an automated image-level OOD detection method that outperforms class-level approaches on benchmark datasets, demonstrating improved performance in identifying truly OOD images within seemingly ID classes.

### Open Question 2
- Question: How can we improve the semantic distance metric used for automatic labeling of OOD classes to better capture the nuances of semantic relationships between classes from different datasets?
- Basis in paper: [explicit] The paper mentions that improving the semantic distance metric would enhance the labeling of WordNet-ImageNet datasets and facilitate more accurate comparison between concept proximity and output OOD scores.
- Why unresolved: Current path-based metrics like Wu-Palmer and Path similarity may not fully capture the complexity of semantic relationships, especially for concepts that are not directly connected in WordNet. More sophisticated metrics that consider contextual information and multiple semantic relationships are needed.
- What evidence would resolve it: Creation of a new semantic distance metric that outperforms existing ones on a benchmark of manually labeled class pairs, demonstrating improved correlation between semantic similarity and OOD detection performance.

### Open Question 3
- Question: How does the performance of OOD detection methods vary across different types of OOD samples (e.g., far-OOD vs. near-OOD) and what are the implications for real-world applications?
- Basis in paper: [explicit] The paper discusses how the measured efficacy of OOD detection techniques depends on the selected benchmark and how confidence-based techniques may outperform classifier-based ones on near-OOD samples.
- Why unresolved: While the paper provides some insights into how different methods perform on various benchmarks, a comprehensive analysis of their performance across different types of OOD samples and real-world scenarios is lacking. Understanding these variations is crucial for selecting appropriate OOD detection methods for specific applications.
- What evidence would resolve it: A large-scale study comparing the performance of various OOD detection methods on a diverse set of benchmarks, including both far-OOD and near-OOD samples, and evaluating their effectiveness in real-world applications with different levels of semantic similarity between ID and OOD classes.

## Limitations
- The benchmark's effectiveness depends on the correlation between semantic similarity and visual similarity, which may not hold for all class pairs
- Manual annotation introduces potential subjectivity and doesn't scale well to large datasets
- Results are primarily validated on ResNet50 models pre-trained on Places365 without extensive cross-architecture testing

## Confidence
- **High Confidence**: The observation that confidence-based techniques outperform classifier-based methods on near-OOD samples is well-supported by empirical results across multiple benchmark configurations.
- **Medium Confidence**: The claim that semantic similarity-based benchmarks better reflect real-world deployment scenarios is reasonable but depends on the specific application context and deployment environment.
- **Low Confidence**: The assertion that manual annotation provides more realistic benchmarks than automatic methods lacks direct comparative evidence within the paper itself.

## Next Checks
1. **Semantic-Visual Correlation Analysis**: Conduct a systematic study measuring the correlation between WordNet similarity scores and actual visual similarity as judged by human annotators, focusing on cases where automated and manual methods disagree.

2. **Cross-Architecture Generalization**: Evaluate the proposed benchmark across different model architectures (e.g., EfficientNet, Vision Transformers) and training conditions to verify that the semantic similarity-based approach provides consistent advantages over traditional inter-dataset methods.

3. **Real-World Deployment Validation**: Implement the benchmark evaluation in a controlled deployment scenario with actual OOD samples from the target domain, comparing detection performance against the benchmark predictions to assess ecological validity.