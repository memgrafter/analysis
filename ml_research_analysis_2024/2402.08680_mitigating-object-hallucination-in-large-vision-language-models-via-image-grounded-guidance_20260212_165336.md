---
ver: rpa2
title: Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded
  Guidance
arxiv_id: '2402.08680'
source_url: https://arxiv.org/abs/2402.08680
tags:
- guidance
- marine
- object
- hallucination
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of object hallucination in Large
  Vision-Language Models (LVLMs), where models generate descriptions of non-existent
  objects in images. The authors propose MARINE, a training-free and API-free framework
  that reduces hallucinations during inference by introducing image-grounded guidance.
---

# Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance

## Quick Facts
- arXiv ID: 2402.08680
- Source URL: https://arxiv.org/abs/2402.08680
- Authors: Linxi Zhao; Yihe Deng; Weitong Zhang; Quanquan Gu
- Reference count: 40
- Key outcome: MARINE reduces object hallucinations by 8.8 points on CHAIR and improves POPE accuracy by 6.7% using image-grounded guidance without retraining

## Executive Summary
This paper addresses object hallucination in Large Vision-Language Models (LVLMs), where models generate descriptions of non-existent objects in images. The authors propose MARINE, a training-free and API-free framework that reduces hallucinations during inference by introducing image-grounded guidance. MARINE leverages open-source vision models to extract object-level information, which is then aggregated and used to guide the LVLM's generation process through a classifier-free guidance mechanism. This approach effectively balances the original generation with guidance-based generation to reduce hallucinations while maintaining output quality.

## Method Summary
MARINE uses external object detection models (DETR, RAM++) to extract visual information from images, which is aggregated and converted into textual guidance. This guidance is combined with the LVLM's original visual tokens using classifier-free guidance, where the guidance strength γ controls the balance between original generation and image-grounded guidance. The framework can incorporate multiple object detection models and aggregate their outputs through intersection or union strategies. MARINE operates during inference without requiring model retraining or API calls.

## Key Results
- Reduces hallucination metrics by up to 8.8 points on CHAIR benchmark
- Improves POPE evaluation accuracy by 6.7% compared to baselines
- Adds only 1.98x computational overhead to original LVLM decoding time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MARINE reduces hallucination by leveraging external object detection models to provide additional visual grounding during inference.
- Mechanism: The framework uses open-source vision models like DETR and RAM++ to extract object-level information from the image. This information is aggregated and converted into textual guidance, which is then combined with the original LVLM generation using classifier-free guidance. The guidance strength γ controls the balance between original generation and image-grounded guidance.
- Core assumption: The additional visual context from specialized object detectors is more accurate than the visual encoder within the LVLM, and this accuracy can be transferred to the language generation process without requiring model retraining.
- Evidence anchors:
  - [abstract]: "This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content."
  - [section 4.1]: "Our approach leverages object detection models to extract detailed visual information from images."
  - [corpus]: Weak. The corpus contains related papers but none directly validate this specific mechanism of using external object detectors for guidance.
- Break condition: If the external object detectors consistently hallucinate objects themselves, or if the aggregation process introduces noise that degrades LVLM performance.

### Mechanism 2
- Claim: Classifier-free guidance allows MARINE to balance between following instructions and adhering to visual grounding without requiring additional training.
- Mechanism: MARINE formulates the guided generation as a combination of unconditional and conditional logits, where the conditional branch uses both original visual tokens and image-grounded guidance. The guidance strength γ determines how much weight the guidance receives.
- Core assumption: The linear interpolation of logits in the classifier-free guidance framework can effectively steer generation toward more grounded outputs while maintaining coherence.
- Evidence anchors:
  - [section 4.2]: "This linear combination of logits implies that the conditional generation on the additional image-grounded guidance acts as a controllable gate."
  - [section 4]: "This strikes a balance between a better ability to follow instructions to generate high-quality answers and the increased accuracy and detail in image descriptions."
  - [corpus]: Weak. The corpus mentions related hallucination mitigation work but doesn't specifically address classifier-free guidance for vision-language models.
- Break condition: If the guidance strength γ is set too high, the model may lose instruction-following ability; if too low, hallucination reduction may be minimal.

### Mechanism 3
- Claim: Using multiple image-grounded models and aggregating their outputs improves reliability and robustness of the guidance.
- Mechanism: MARINE can incorporate guidance from multiple vision models (e.g., DETR and RAM++) and aggregate their outputs either through intersection (only objects detected by both) or union (all detected objects).
- Core assumption: Consensus across multiple models provides more reliable guidance than any single model, and the aggregation method significantly impacts hallucination reduction.
- Evidence anchors:
  - [section 4.1]: "Our framework's compatibility with various vision models allows for the incorporation of multiple sources to enhance precision and robustness."
  - [section 5.3]: "The intersection-based approach consistently outperforms the union, significantly reducing hallucination."
  - [corpus]: Weak. The corpus doesn't provide evidence for the specific aggregation strategies or their comparative effectiveness.
- Break condition: If the models disagree significantly or if one model dominates the aggregation, the guidance quality may degrade.

## Foundational Learning

- Concept: Classifier-free guidance in diffusion models and language models
  - Why needed here: MARINE extends classifier-free guidance from single-modal to multi-modal settings, requiring understanding of how guidance strength affects the balance between unconditional and conditional generation.
  - Quick check question: What happens to the generated output when guidance strength γ approaches 1 versus when it approaches 0?

- Concept: Object detection and visual grounding in computer vision
  - Why needed here: The effectiveness of MARINE depends on the quality of object detection models and their ability to provide accurate visual grounding that can be translated into textual guidance.
  - Quick check question: How does the confidence threshold for object detection affect the trade-off between hallucination reduction and recall of actual objects?

- Concept: Multimodal model architecture and cross-modal alignment
  - Why needed here: Understanding how LVLMs process visual tokens and align them with language is crucial for designing effective guidance mechanisms that work within existing model architectures.
  - Quick check question: What are the typical sources of visual information loss in LVLMs that MARINE aims to address?

## Architecture Onboarding

- Component map: Image → DETR/RAM++ → Object lists → Aggregation module → Unified guidance text → Classifier-free guidance → LVLM → Output generation

- Critical path: 1) Input image → object detection models → object lists 2) Object lists → aggregation module → unified guidance text 3) Original visual tokens + guidance text + prompt → classifier-free guidance 4) Guided logits → token sampling → output generation

- Design tradeoffs:
  - Using multiple object detectors increases robustness but adds computational overhead
  - Higher confidence thresholds reduce noise but may miss relevant objects
  - Guidance strength must balance hallucination reduction with instruction adherence

- Failure signatures:
  - Hallucination persists despite guidance: indicates poor quality of object detection or ineffective aggregation
  - Output becomes incoherent: suggests guidance strength is too high or guidance text is noisy
  - Model ignores instructions: indicates guidance is overwhelming the original generation

- First 3 experiments:
  1. Test MARINE with a single object detector (DETR only) on CHAIR benchmark to verify basic hallucination reduction
  2. Vary guidance strength γ systematically to find optimal balance between hallucination reduction and instruction following
  3. Compare intersection vs. union aggregation strategies on the same benchmark to determine which provides better guidance quality

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the provided text.

## Limitations

- Lack of transparency in how visual information from multiple object detectors is aggregated into guidance text
- Potential for guidance models themselves to hallucinate objects, propagating errors to the LVLM
- Limited evaluation on out-of-distribution data to test cross-domain generalization

## Confidence

- Overall hallucination reduction effectiveness: Medium - comprehensive experimental results across multiple benchmarks and models, but critical implementation details remain unspecified
- Mechanism claims: Low to Medium - conceptual framework is sound but lacks ablation studies isolating component contributions
- Computational overhead claims: High - straightforward measurement with clear results
- Generalization claims: Low - limited testing on out-of-distribution data

## Next Checks

1. **Ablation Study on Aggregation Strategies**: Systematically compare MARINE's performance using individual object detectors (DETR only, RAM++ only) versus the combined approach with both intersection and union strategies on the CHAIR benchmark. This would quantify the marginal benefit of aggregation and identify which strategy works best under different conditions.

2. **Guidance Strength Sensitivity Analysis**: Conduct a fine-grained sweep of guidance strength γ values (e.g., 0.1 to 0.9 in increments of 0.1) on both hallucination metrics and instruction-following ability using a diverse set of prompts. This would reveal the optimal operating range and help identify failure modes at extreme values.

3. **Cross-Domain Generalization Test**: Evaluate MARINE on datasets from domains not represented in the training data of the object detection models (e.g., medical imaging, satellite imagery, or artistic renderings). This would assess whether the guidance mechanism generalizes beyond natural images and identify potential domain-specific failure patterns.