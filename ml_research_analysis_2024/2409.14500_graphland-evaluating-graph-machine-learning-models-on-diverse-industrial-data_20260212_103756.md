---
ver: rpa2
title: 'GraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial
  Data'
arxiv_id: '2409.14500'
source_url: https://arxiv.org/abs/2409.14500
tags:
- datasets
- graph
- node
- data
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GraphLand addresses the lack of diverse, realistic graph datasets
  for evaluating node property prediction models, particularly in industrial settings.
  It introduces 14 diverse graph datasets spanning multiple domains, sizes, and structural
  properties, including applications like fraud detection, CTR prediction, and road
  network speed estimation.
---

# GraphLand: Evaluating Graph Machine Learning Models on Diverse Industrial Data

## Quick Facts
- **arXiv ID**: 2409.14500
- **Source URL**: https://arxiv.org/abs/2409.14500
- **Reference count**: 30
- **Primary result**: GraphLand introduces 14 diverse graph datasets for evaluating GML models on industrial data, showing attention-based GNNs generally outperform classic ones while GBDTs with graph-based features are strong baselines.

## Executive Summary
GraphLand addresses the critical gap in realistic graph datasets for evaluating node property prediction models in industrial settings. The benchmark introduces 14 diverse graph datasets spanning multiple domains, sizes, and structural properties, including fraud detection, CTR prediction, and road network speed estimation. Experiments comparing GNNs, GBDTs, and graph foundation models reveal that attention-based GNNs generally outperform classic ones, but GBDTs with graph-based features are strong baselines, especially for regression tasks. The results highlight the importance of evaluating GML methods under realistic temporal and structural conditions, with temporal splits showing significant impact on model performance.

## Method Summary
GraphLand provides 14 graph datasets with node property prediction tasks collected from various industrial applications. The benchmark supports both transductive and inductive settings with temporal splits to reflect realistic distributional shifts. Models are evaluated using standard metrics (accuracy, average precision, R²) with extensive hyperparameter tuning via Optuna. GNNs are trained in full-batch mode for 1000 steps with Adam optimizer, while GBDTs are optimized with 100 iterations. All models are trained 10 times with different random seeds, and results report mean and standard deviation across runs.

## Key Results
- Attention-based GNNs (GAT and GT) outperform classic GNNs (GCN and GraphSAGE) on most datasets
- GBDTs with graph-based feature augmentation are strong baselines, especially for regression tasks
- Graph foundation models currently perform poorly on these datasets, indicating the need for better general-purpose models
- Temporal data splits significantly degrade GML model performance compared to random splits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal data splits significantly degrade GML model performance compared to random splits
- Mechanism: Temporal splits introduce distributional shifts across time that random splits do not, causing mismatch between training and test distributions in node features, labels, and graph structure
- Core assumption: Real-world graphs evolve over time, making temporal distributional shifts realistic and impactful
- Evidence anchors:
  - [abstract] "Temporal distributional shifts under transductive and inductive settings influence graph ML model performance"
  - [section 4] "temporal data splits may significantly affect the prediction problem and the model performance, as they often result in distributional shifts between train, validation, and test data"
  - [corpus] Weak evidence - corpus focuses on vision/text models, not GML temporal shifts

### Mechanism 2
- Claim: Attention-based GNNs outperform classic GNNs on diverse industrial datasets
- Mechanism: Attention mechanisms allow dynamic weighting of neighbor messages based on content, better handling diverse feature sets and non-homophilous graphs
- Core assumption: Industrial graphs often have rich, heterogeneous features and non-homophilous structure where neighbor importance varies
- Evidence anchors:
  - [section 5.2] "attention-based GNNs (GAT and GT) outperform classic GNNs (GCN and GraphSAGE) on most datasets"
  - [section 2] "It has been argued that in some GML datasets the graph structure does not actually help solve the considered tasks"
  - [corpus] Weak evidence - corpus doesn't discuss GNN variants or attention mechanisms

### Mechanism 3
- Claim: GBDT models with graph-based feature augmentation are strong baselines, especially for regression tasks
- Mechanism: GBDTs handle numerical features well, and NFA provides limited graph information through neighbor feature statistics, bridging graph-agnostic and graph-aware approaches
- Core assumption: Industrial datasets often have rich numerical features where GBDTs excel, and graph structure provides useful context
- Evidence anchors:
  - [section 5.2] "GBDTs provided with additional graph-based input features can sometimes be very strong baselines"
  - [section 5.2] "GBDTs with augmented features are strong baselines in realistic industrial settings"
  - [corpus] Weak evidence - corpus doesn't discuss GBDTs or feature augmentation

## Foundational Learning

- Concept: Graph neural networks (GNNs) as message-passing models
  - Why needed here: GNNs are the primary focus for evaluating graph machine learning on the proposed datasets
  - Quick check question: What are the three main steps in a GNN layer's forward pass?

- Concept: Temporal distributional shifts in graph data
  - Why needed here: The benchmark specifically investigates how temporal evolution affects model performance
  - Quick check question: What types of distributional shifts might occur in temporal graph data?

- Concept: Transductive vs inductive learning settings
  - Why needed here: The benchmark provides both settings to evaluate model generalization
  - Quick check question: How does the availability of test nodes during training differ between transductive and inductive settings?

## Architecture Onboarding

- Component map:
  - Data ingestion → Graph preprocessing (directed→undirected conversion) → Feature processing (numerical scaling, categorical encoding) → Model training (full-batch for GNNs, specific for GBDTs) → Evaluation with multiple random seeds

- Critical path:
  1. Load dataset and convert directed graphs to undirected if needed
  2. Preprocess features (standard scaling for numerical, one-hot for categorical)
  3. Prepare data splits (RL, RH, TH, THI as appropriate)
  4. Run hyperparameter optimization for chosen model
  5. Train model with best hyperparameters for 1000 steps
  6. Evaluate on test set and compute metrics with standard deviation

- Design tradeoffs:
  - Full-batch training vs subgraph sampling: Full-batch provides better results but higher memory usage
  - Attention-based vs classic GNNs: Attention better for heterogeneous features but more computationally expensive
  - GBDT with NFA vs GNNs: GBDTs faster for large graphs but may miss complex graph patterns

- Failure signatures:
  - NaN values during training: Often caused by high learning rates (especially 3×10^-3)
  - Memory errors on large datasets: Reduce hidden dimension or use subgraph sampling
  - Poor performance despite hyperparameter tuning: May indicate graph structure not beneficial for task

- First 3 experiments:
  1. Run ResMLP baseline on a small dataset to establish graph-agnostic performance
  2. Run GCN with default hyperparameters to verify basic GNN functionality
  3. Run LightGBM-NFA on a regression dataset to test feature augmentation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do GFMs perform when evaluated on graphs with different feature sets (beyond text) after proper fine-tuning or architectural modifications?
- Basis in paper: [explicit] The paper explicitly states that current GFMs "fail to produce competitive results" and notes that "currently available GFMs are still far from being able to compete with classic GNNs on realistic datasets with rich node attributes."
- Why unresolved: The paper only evaluated two GFMs (OpenGraph and AnyGraph) in an in-context learning setting without fine-tuning, and found their performance poor. The models tested (TS-GNN and GCOPE) either couldn't scale or lacked implementation support for node regression.
- What evidence would resolve it: Testing GFMs with proper fine-tuning, architectural adaptations for non-textual features, or newer models on the GraphLand datasets would show whether GFMs can become competitive with classic methods.

### Open Question 2
- Question: How much does the inductive setting performance gap between GNNs and transductive setting depend on the temporal evolution of the graph versus other factors?
- Basis in paper: [explicit] The paper shows "the considered models perform significantly worse in the inductive setting than in the transductive one" and asks "how much these differences can affect GNN performance."
- Why unresolved: While the paper compares transductive and inductive settings under the same temporal split, it doesn't isolate whether the performance drop is due to temporal evolution specifically or other factors like lack of complete graph information.
- What evidence would resolve it: Comparing inductive performance across random vs. temporal splits, or ablating temporal information while keeping the inductive constraint, would clarify the relative contributions.

### Open Question 3
- Question: What architectural modifications to GNNs could improve their robustness to temporal distributional shifts in graph-structured data?
- Basis in paper: [inferred] The paper observes that "GML methods can be very strongly affected by temporal distributional shifts" and highlights this as an "important research direction."
- Why unresolved: The paper tests standard GNN architectures without any specific modifications for temporal robustness, showing they degrade significantly under temporal splits.
- What evidence would resolve it: Developing and testing GNN variants with temporal awareness (e.g., time-aware attention, temporal convolutions, or meta-learning) on GraphLand's temporal splits would demonstrate whether such modifications help.

## Limitations

- The corpus search returned only 25 related papers with zero average citations, indicating a narrow research landscape
- The paper's claims about temporal distributional shifts in graph data are supported by weak evidence from the corpus, which focuses primarily on vision and text foundation models rather than graph ML
- Exact random seeds used for data splits and model initialization are not specified, which may affect reproducibility of results

## Confidence

- **High Confidence**: The claim that attention-based GNNs outperform classic GNNs is well-supported by the experimental results showing GAT and GT models consistently achieving higher scores across most datasets
- **Medium Confidence**: The claim about temporal splits degrading model performance is supported by experimental observations but lacks strong theoretical justification and corpus backing
- **Low Confidence**: The claim that graph foundation models perform poorly on these datasets is based on limited experimentation with a single model (GraphGPT) and may not generalize to other foundation models

## Next Checks

1. Replicate temporal split experiments: Conduct experiments comparing random vs temporal splits on synthetic graph datasets with known temporal dynamics to validate the distributional shift hypothesis

2. Expand model evaluation: Test additional graph foundation models (e.g., GraphFormers, GraphSAGE variants) on the GraphLand datasets to assess whether poor performance is model-specific or a general trend

3. Feature importance analysis: Perform ablation studies to quantify the contribution of graph structure versus node features in GBDT models with NFA augmentation, particularly for datasets where GBDTs outperform GNNs