---
ver: rpa2
title: 'STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited
  Data'
arxiv_id: '2407.03253'
source_url: https://arxiv.org/abs/2407.03253
tags:
- classi
- learning
- cation
- tweets
- tweet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a sentence transformer fine-tuning approach
  for tweet topic classification, addressing the challenge of limited labeled data.
  The method leverages pre-trained sentence transformers and fine-tunes them for the
  specific task of classifying tweets into topics.
---

# STF: Sentence Transformer Fine-Tuning For Topic Categorization With Limited Data

## Quick Facts
- arXiv ID: 2407.03253
- Source URL: https://arxiv.org/abs/2407.03253
- Authors: Kheir Eddine Daouadi; Yaakoub Boualleg; Oussama Guehairia
- Reference count: 40
- Primary result: Achieves 91.63% and 77.52% accuracy on two tweet topic classification datasets with limited labeled data

## Executive Summary
This paper proposes a sentence transformer fine-tuning approach for tweet topic classification, addressing the challenge of limited labeled data. The method leverages pre-trained sentence transformers and fine-tunes them for the specific task of classifying tweets into topics. Extensive parameter sensitivity analyses were conducted to optimize the fine-tuning process. Experiments on two benchmark datasets demonstrated that the proposed approach significantly outperforms existing state-of-the-art methods, achieving high accuracy without requiring large amounts of labeled tweets.

## Method Summary
The approach uses pre-trained Sentence Transformer models that are fine-tuned for tweet topic classification. The fine-tuning process employs Adam optimizer with learning rates between 1e-5 and 5e-5, batch sizes of 8-32, and max sequence length of 64. The model is trained for 2-20 epochs with early stopping based on validation loss. Tweet datasets undergo preprocessing to normalize URLs, mentions, hashtags, elongated words, and special characters before being split into train/validation/test sets.

## Key Results
- Achieves 91.63% accuracy on D1 dataset (1350 tweets, 6 classes)
- Achieves 77.52% accuracy on D2 dataset (1615 tweets, 5 classes)
- Significantly outperforms baseline models including MNB, LR, CNN, LSTM, and BiLSTM
- Demonstrates effectiveness with limited labeled data compared to other state-of-the-art approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning with pre-trained sentence transformers allows effective topic classification with minimal labeled data
- Mechanism: The model leverages pre-learned semantic representations from large-scale unsupervised training, then fine-tunes these representations to the specific topic classification task using a small labeled dataset
- Core assumption: Pre-trained sentence transformers capture general semantic relationships that transfer effectively to tweet topic classification
- Evidence anchors:
  - [abstract]: "the proposed STF does not require a huge amount of labeled tweets to achieve good accuracy, which is a limitation of many state-of-the-art approaches"
  - [section]: "Transfer learning achieves substantial robustness for small data resource scenarios. Transfer learning in Natural language processing [7] comprises two steps: A Pre-trained Language Model trained based on unlabeled data while the second step is fine-tuning for a particular task."
- Break condition: If the pre-trained representations do not capture domain-relevant semantics (e.g., informal tweet language patterns), fine-tuning will fail to achieve good performance

### Mechanism 2
- Claim: Fine-tuning hyperparameters significantly impact model performance and must be optimized per dataset
- Mechanism: Systematic exploration of learning rates, batch sizes, and epochs allows finding the optimal balance between underfitting and overfitting for each specific dataset
- Core assumption: The optimal hyperparameter configuration varies between datasets and must be empirically determined
- Evidence anchors:
  - [section]: "extensive parameter sensitivity analyses were established to fine-tune STF parameters' for our topic classification task to achieve the best performance results"
  - [section]: "We fine-tuned the models by trying the following hyperparameter and values : epoch=[2, 3, 4, 5, 10, 20], Batch_size=[8, 16, 32] and learning rate [ 1e−5, 2e−5, 3e−5, 4e−5, 5e−5]"
- Break condition: If hyperparameter search space is too narrow or optimization is insufficient, the model may not reach optimal performance

### Mechanism 3
- Claim: Sentence transformer models outperform traditional transformer models for tweet topic classification
- Mechanism: Sentence transformers are specifically trained on semantic similarity tasks, producing embeddings that better capture topical relationships in short text like tweets
- Core assumption: The semantic similarity training objective aligns better with topic classification needs than general language modeling
- Evidence anchors:
  - [section]: "As shown in Table 4, the pre-trained language model based on sentence transformers substantially outperforms the baselines systems for both datasets"
  - [section]: "It is important to note that for both datasets; Sentence transformers based models achieve better results than transformers based models, yielded overall Accuracy results of 91.63% and 77.52% for D1 and D2, respectively"
- Break condition: If the semantic similarity objective does not align with topical distinctions in tweets, sentence transformers may not provide advantages

## Foundational Learning

- Concept: Transfer Learning
  - Why needed here: The limited labeled data problem makes it impossible to train models from scratch; transfer learning allows leveraging pre-trained knowledge
  - Quick check question: What are the two main steps in transfer learning for NLP as described in the paper?

- Concept: Sentence Embeddings
  - Why needed here: Tweets are short texts requiring representations that capture semantic meaning rather than just word-level information
  - Quick check question: How do sentence embeddings differ from word embeddings in terms of what they represent?

- Concept: Fine-tuning vs. Feature Extraction
  - Why needed here: The paper uses fine-tuning rather than feature extraction, requiring understanding of when to update pre-trained weights
  - Quick check question: What is the key difference between fine-tuning and using a pre-trained model as a feature extractor?

## Architecture Onboarding

- Component map: Input preprocessing → Sentence transformer encoder → Classification head (feed-forward network) → Output layer → Hyperparameter optimization module → Cross-validation pipeline

- Critical path: Pre-trained model loading → Fine-tuning training loop → Hyperparameter optimization → Evaluation

- Design tradeoffs:
  - Model size vs. inference speed (large models achieve better accuracy but slower inference)
  - Fine-tuning duration vs. overfitting risk (longer training may overfit small datasets)
  - Hyperparameter search breadth vs. computational cost

- Failure signatures:
  - Overfitting: High training accuracy but low validation accuracy
  - Underfitting: Both training and validation accuracies remain low throughout training
  - Poor generalization: Performance drops significantly on new datasets

- First 3 experiments:
  1. Baseline comparison: Run all baseline models (MNB, LR, CNN, LSTM, BiLSTM) on both datasets to establish performance floor
  2. Transformer comparison: Compare different pre-trained transformer models (BERT, RoBERTa, ELECTRA, DistilBERT) with default hyperparameters
  3. Sentence transformer evaluation: Test various sentence transformer models with optimized hyperparameters to identify top performers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different sentence transformer architectures (e.g., paraphrase-mpnet-base-v2 vs all-roberta-large-v1) compare in performance for tweet topic classification under varying dataset sizes?
- Basis in paper: [explicit] The paper compares multiple sentence transformer models but does not analyze performance variations with different dataset sizes
- Why unresolved: The paper focuses on fixed dataset sizes and does not explore how performance scales with limited vs. moderate data
- What evidence would resolve it: Experiments showing accuracy trends across different dataset sizes for each sentence transformer model

### Open Question 2
- Question: What is the impact of tweet preprocessing steps (e.g., hashtag normalization, elongated word reduction) on the performance of sentence transformer fine-tuning?
- Basis in paper: [inferred] The paper describes preprocessing steps but does not analyze their individual or combined impact on STF performance
- Why unresolved: Preprocessing is treated as a fixed component rather than a variable affecting model performance
- What evidence would resolve it: Ablation studies comparing STF performance with and without specific preprocessing steps

### Open Question 3
- Question: How does the STF approach generalize to other social media platforms (e.g., Reddit, Facebook) or different languages?
- Basis in paper: [inferred] The paper only evaluates STF on English tweets, limiting generalizability claims to other platforms or languages
- Why unresolved: No cross-platform or multilingual experiments are conducted
- What evidence would resolve it: Experiments applying STF to datasets from other platforms or in different languages, comparing performance to baseline models

## Limitations

- Limited evaluation on only two tweet datasets raises questions about generalization to different domains, languages, or larger datasets
- Preprocessing implementation details are not fully specified, affecting reproducibility and potential performance variations
- Hyperparameter optimization scope is unclear regarding whether configurations were dataset-specific or universal

## Confidence

- High Confidence: STF achieves better accuracy than baseline models on tested datasets; transfer learning provides meaningful advantages for topic classification with limited data
- Medium Confidence: STF does not require large amounts of labeled data; sentence transformers outperform traditional transformer models for tweet classification
- Low Confidence: STF is "significantly" better than state-of-the-art methods (no statistical significance testing or absolute best model comparisons)

## Next Checks

1. Conduct statistical significance testing (paired t-tests or bootstrap confidence intervals) to determine whether accuracy improvements over baseline models are statistically significant

2. Test STF approach on additional tweet datasets from different domains to assess generalization capabilities and identify potential domain-specific limitations

3. Perform ablation studies comparing different pre-trained transformer architectures using both fine-tuning and feature extraction approaches to isolate the contribution of sentence transformer training objectives