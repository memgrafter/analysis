---
ver: rpa2
title: 'Generative Dense Retrieval: Memory Can Be a Burden'
arxiv_id: '2401.10487'
source_url: https://arxiv.org/abs/2401.10487
tags:
- retrieval
- documents
- document
- corpus
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of generative retrieval (GR),
  which struggles with fine-grained feature memory, scalability issues, and high update
  costs for new documents. To overcome these challenges, the authors propose Generative
  Dense Retrieval (GDR), a coarse-to-fine retrieval paradigm that combines the deep
  interaction of GR with the scalability of dense retrieval (DR).
---

# Generative Dense Retrieval: Memory Can Be a Burden

## Quick Facts
- arXiv ID: 2401.10487
- Source URL: https://arxiv.org/abs/2401.10487
- Reference count: 17
- Achieves average 3.0 R@100 improvement over advanced methods

## Executive Summary
Generative Dense Retrieval (GDR) addresses the limitations of generative retrieval (GR) by introducing a memory-efficient, coarse-to-fine retrieval paradigm. GDR combines the deep interaction capabilities of GR with the scalability and fine-grained matching of dense retrieval (DR). By shifting from full document memorization to cluster-level memorization combined with dense representations, GDR achieves better scalability and performance on the Natural Questions dataset.

## Method Summary
GDR implements a two-stage retrieval process where cluster identifiers (CIDs) are first generated using limited memory capacity through K-means clustering, followed by fine-grained intra-cluster matching using DR-style dense representations. The model employs a shared query encoder for both stages and uses cluster-adaptive negative sampling during training to improve intra-cluster discriminative ability. The document encoder is finetuned on the training set to generate cluster identifiers that better capture task-relevant semantic structure.

## Key Results
- Achieves average 3.0 R@100 improvement over advanced methods on Natural Questions dataset
- Demonstrates better scalability across different corpus sizes (334K, 1M, 2M, 4M documents)
- Outperforms generative retrieval in both accuracy and efficiency metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GDR reduces memory burden by shifting from full document memorization to cluster-level memorization combined with dense representations for fine-grained matching.
- Mechanism: The model first generates cluster identifiers (CID) using limited memory capacity, then uses DR-style dense representations for intra-cluster matching. This two-stage process reduces the memory required to encode the entire corpus.
- Core assumption: Document clusters can be reliably identified and distinguished using a shared query encoder, and the number of clusters can be kept fixed regardless of corpus size.
- Evidence anchors:
  - [abstract] "GDR first uses the limited memory volume to achieve inter-cluster matching from query to relevant document clusters. Memorizing-free matching mechanism from Dense Retrieval (DR) is then introduced to conduct fine-grained intra-cluster matching from clusters to relevant documents."
  - [section 3.1] "By conducting a coarse-to-fine retrieval process, GDR maximizes the advantages of memorizing mechanism in deep interaction and matching mechanism in fine-grained features discrimination..."
  - [corpus] Weak - no direct experimental comparison of memory usage or cluster stability across different corpus sizes.
- Break condition: If document clusters cannot be reliably distinguished, or if the number of clusters needs to scale with corpus size, the memory advantage disappears.

### Mechanism 2
- Claim: Cluster-adaptive negative sampling improves intra-cluster discriminative ability compared to standard negative sampling strategies.
- Mechanism: During training, GDR uses documents within the same cluster as the positive example as intra-cluster negatives, and in-batch negatives as inter-cluster negatives, with weighted loss to emphasize intra-cluster discrimination.
- Core assumption: Negative samples from the same cluster provide more informative training signals for fine-grained matching within clusters than random negatives or BM25-based negatives.
- Evidence anchors:
  - [section 3.3] "For a training pair (q, d+), we treat d âˆˆ CID(d+) as intra-cluster negatives Na and in-batch negatives as inter-cluster negatives Nr..."
  - [section 4.3] "We notice that GDR trained with the cluster-adaptive strategy outperforms that with widely used BM25 strategy by 1.1 on R@100."
  - [corpus] Weak - only one baseline comparison (BM25), no comparison to other advanced negative sampling strategies like ANCE.
- Break condition: If cluster boundaries are noisy or clusters contain many irrelevant documents, intra-cluster negatives may provide poor training signals.

### Mechanism 3
- Claim: Finetuning the document encoder on the training set to generate cluster identifiers improves retrieval performance compared to using a general-purpose BERT encoder.
- Mechanism: The document encoder is first trained on the training set using DR-style contrastive learning, then used to generate cluster identifiers that better capture the semantic structure relevant to the retrieval task.
- Core assumption: The semantic space learned by DR-style training better reflects the retrieval task than the general-purpose semantic space learned by BERT pretraining.
- Evidence anchors:
  - [section 3.2] "Compared to previous studies (Tay et al., 2022; Wang et al., 2022) using BERT (Devlin et al., 2019) as ED, our strategy can fully leverage the knowledge in the training set."
  - [section 4.3] "Both NCI and GDR trained with '-ours' perform significantly better than those trained with '-bert' across all the settings."
  - [corpus] Weak - no ablation study isolating the effect of finetuning from other differences in the implementation.
- Break condition: If the DR-style training doesn't generalize well to the generative retrieval task, or if the semantic space learned doesn't capture the relevant distinctions for retrieval.

## Foundational Learning

- Concept: Sequence-to-sequence modeling with attention mechanisms
  - Why needed here: GDR uses an encoder-decoder architecture to generate cluster identifiers, which requires understanding how attention mechanisms enable deep interaction between queries and documents.
  - Quick check question: How does the attention mechanism in a transformer encoder-decoder model enable the model to capture complex relationships between query tokens and document tokens during the generation of cluster identifiers?

- Concept: Contrastive learning for dense retrieval
  - Why needed here: GDR uses a document encoder trained with contrastive loss to extract dense representations for fine-grained matching, requiring understanding of how contrastive learning creates meaningful semantic spaces.
  - Quick check question: What is the role of the contrastive loss in training the document encoder, and how does it ensure that relevant documents are closer in the semantic space than irrelevant ones?

- Concept: K-means clustering for document organization
  - Why needed here: GDR uses K-means to organize documents into clusters based on their dense representations, requiring understanding of how clustering algorithms partition data and the implications for retrieval performance.
  - Quick check question: How does the choice of the number of clusters (k) in K-means affect the trade-off between the granularity of document organization and the memory burden on the generative model?

## Architecture Onboarding

- Component map:
  Query Encoder (EQ) -> Cluster Decoder (DC) -> Prefix Tree -> Document Encoder (ED) -> ANN Search

- Critical path:
  1. Query encoding by EQ
  2. Cluster identifier generation by DC with constrained beam search
  3. Intra-cluster matching using ED representations and ANN search
  4. Score combination and final ranking

- Design tradeoffs:
  - Memory vs. performance: Using fewer clusters reduces memory burden but may hurt recall if relevant documents span multiple clusters
  - Generation vs. matching: The autoregressive generation of CIDs enables deep interaction but is slower than direct matching
  - Negative sampling strategy: Cluster-adaptive sampling improves intra-cluster discrimination but requires accurate cluster assignments

- Failure signatures:
  - Poor R@100 scores: May indicate cluster assignments are not capturing relevant document relationships
  - High variance in R@100 across different seeds: May indicate instability in the clustering or negative sampling process
  - Slow inference speed: May indicate the ANN search is not properly optimized or the number of clusters is too large

- First 3 experiments:
  1. Ablation study on the number of clusters: Train GDR with different cluster counts (e.g., 1000, 5000, 10000) and evaluate R@100 to find the optimal balance between memory burden and retrieval performance.
  2. Comparison of negative sampling strategies: Train GDR with cluster-adaptive sampling, BM25 sampling, and random sampling, then compare R@100 to verify the effectiveness of the proposed strategy.
  3. Finetuning vs. non-finetuning ED: Train GDR with ED initialized from BERT vs. ED finetuned on the training set, then compare R@100 to verify the importance of the proposed strategy for generating cluster identifiers.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GDR change when using different clustering algorithms for the inter-cluster matching stage?
- Basis in paper: [inferred] The paper uses K-means clustering for constructing document clusters and generating cluster identifiers (CIDs). It also mentions that the total number of clusters is determined by the memory volume rather than the size of the document corpus.
- Why unresolved: The paper does not explore the impact of using alternative clustering algorithms (e.g., hierarchical clustering, DBSCAN) on the performance of GDR.
- What evidence would resolve it: Conduct experiments using different clustering algorithms for the inter-cluster matching stage and compare their performance in terms of recall and accuracy metrics.

### Open Question 2
- Question: What is the impact of using different query augmentation strategies on the performance of GDR?
- Basis in paper: [explicit] The paper mentions using DocT5Query to generate pseudo queries for document augmentation, but it does not explore the impact of using different query augmentation strategies on GDR's performance.
- Why unresolved: The paper does not investigate the effect of using alternative query augmentation strategies (e.g., using different query generation models or different numbers of augmented queries) on GDR's performance.
- What evidence would resolve it: Perform experiments using various query augmentation strategies and evaluate their impact on GDR's recall and accuracy metrics.

### Open Question 3
- Question: How does the performance of GDR scale when applied to larger document corpora (e.g., 10M or 100M documents)?
- Basis in paper: [explicit] The paper mentions that GDR has better scalability compared to generative retrieval methods, but it does not provide experimental results for larger document corpora.
- Why unresolved: The paper only tests GDR on document corpora up to 4M documents, and the training cost for larger corpora is mentioned as a limitation.
- What evidence would resolve it: Conduct experiments with larger document corpora (e.g., 10M or 100M documents) and evaluate GDR's performance in terms of recall, accuracy, and training/inference efficiency.

## Limitations

- Memory usage claims lack direct measurements and comparisons to standard generative retrieval baselines
- Cluster-adaptive negative sampling effectiveness supported by limited evidence (one baseline comparison)
- Finetuning strategy contribution not isolated through ablation studies

## Confidence

- **High confidence**: The basic architecture combining GR for inter-cluster matching and DR for intra-cluster matching is well-specified and experimentally validated with measurable performance improvements (average 3.0 R@100 gain).
- **Medium confidence**: The cluster-adaptive negative sampling strategy shows promising results but lacks comprehensive comparisons to other advanced sampling methods.
- **Low confidence**: The memory reduction claims are not directly measured, and the finetuning strategy's contribution is not isolated through ablation studies.

## Next Checks

1. **Memory usage measurement**: Implement memory profiling to quantify actual memory reduction compared to standard generative retrieval across different corpus sizes.
2. **Comprehensive negative sampling evaluation**: Compare cluster-adaptive sampling against multiple baselines (random, BM25, ANCE) across different cluster granularities.
3. **Finetuning ablation study**: Isolate the impact of document encoder finetuning by comparing against a frozen BERT baseline while controlling for other variables.