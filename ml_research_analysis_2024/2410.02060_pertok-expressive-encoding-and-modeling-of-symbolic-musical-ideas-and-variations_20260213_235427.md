---
ver: rpa2
title: 'PerTok: Expressive Encoding and Modeling of Symbolic Musical Ideas and Variations'
arxiv_id: '2410.02060'
source_url: https://arxiv.org/abs/2410.02060
tags:
- midi
- tokens
- expressive
- music
- musical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cadenza introduces a multi-stage generative framework that enables
  both unconditional symbolic music generation and expressive variations of input
  MIDI files. The core innovation is PerTok, a novel MIDI encoding method that separates
  compositional and performance information, reducing sequence length up to 59% and
  vocabulary size up to 95% while preserving expressive details like microtiming and
  velocity.
---

# PerTok: Expressive Encoding and Modeling of Symbolic Musical Ideas and Variations

## Quick Facts
- arXiv ID: 2410.02060
- Source URL: https://arxiv.org/abs/2410.02060
- Reference count: 0
- Authors: Julian Lenz; Anirudh Mani
- One-line primary result: Cadenza achieves 14.7% higher human ratings for expressivity compared to baselines while reducing sequence length up to 59% and vocabulary size up to 95%

## Executive Summary
Cadenza introduces a multi-stage generative framework for symbolic music generation that enables both unconditional MIDI music creation and expressive variations of input files. The core innovation is PerTok, a novel MIDI encoding method that separates compositional and performance information, significantly reducing sequence complexity while preserving expressive details like microtiming and velocity. The framework combines a Composer VAE for musical structure with a Performer transformer for expressive details, achieving state-of-the-art results in both musical quality and expressivity.

## Method Summary
Cadenza uses a two-stage architecture: a Composer VAE (transformer with Rotary Positional Embeddings and in-attention conditioning) learns compressed representations of musical ideas, while a Performer transformer separately models expressive performance attributes like velocity and microtiming. The novel PerTok encoding method reduces sequence length by 59% and vocabulary size by 95% by separating macro-level rhythmic information from micro-level expressive timing deviations. The Composer is trained on large score datasets while the Performer is trained on smaller performance datasets, allowing efficient learning of expressive variations.

## Key Results
- Cadenza matches state-of-the-art models in musical quality while sounding more expressive
- Human evaluations show 14.7% higher ratings for expressivity compared to baselines
- Variations are 70% musically related to inputs while providing novel inspiration
- Successfully learns distinct expressive characteristics from small datasets (MusicNet and HipHop)

## Why This Works (Mechanism)

### Mechanism 1
Separating composition and performance tokens in PerTok reduces sequence length and vocabulary size while preserving expressive detail. By encoding macro-level rhythmic information separately from micro-level expressive timing deviations, the model avoids the need for high-resolution temporal grids for all tokens, thereby reducing overall sequence complexity. This assumes expressive performance characteristics can be modeled independently of compositional structure without loss of musical coherence.

### Mechanism 2
The two-stage Composer-Perfomer architecture allows efficient learning of expressive variations with limited performance data. Composer VAE trained on large score datasets learns general musical structure, while Performer transformer trained on smaller performance datasets adds style-specific expressive details. This assumes composition and performance can be modeled as sequential, separable tasks without degrading overall musical quality.

### Mechanism 3
In-attention conditioning in the Composer VAE prevents posterior collapse while maintaining input-output similarity. Latent vectors are integrated into every attention layer through in-attention conditioning, ensuring the decoder cannot ignore the compressed representation. This assumes skip connections and latent vector integration can effectively prevent posterior collapse in sequence-to-sequence VAEs.

## Foundational Learning

- **Variational Autoencoder (VAE) architecture**: The Composer uses a VAE to learn compressed representations of musical ideas that can be decoded into variations. Quick check: What is the role of the KL divergence loss in a VAE, and why might it need modification in sequence modeling?

- **Transformer attention mechanisms**: Both Composer and Performer use transformer-based attention to model long-range dependencies in musical sequences. Quick check: How does rotary positional embedding (RoPE) differ from sinusoidal positional encoding in transformers?

- **MIDI encoding and tokenization**: PerTok requires understanding how MIDI data can be converted into discrete token sequences for transformer processing. Quick check: What are the trade-offs between fixed-grid quantization and variable quantization in MIDI tokenization?

## Architecture Onboarding

- **Component map**: PerTok tokenizer (composition + performance tokens) -> Composer VAE (encoder + decoder with in-attention conditioning) -> Performer transformer (bidirectional encoder for velocity/microtiming prediction) -> Latent space integration layer

- **Critical path**: 1. Tokenize input MIDI with PerTok 2. Encode with Composer VAE to obtain latent vector 3. Decode with Composer to generate compositional structure 4. Apply Performer to add expressive performance details

- **Design tradeoffs**: Sequence length vs. expressive detail (PerTok's separate performance tokens); Data efficiency vs. model capacity (two-stage vs. joint training); Posterior collapse vs. regularization (KL annealing in Composer)

- **Failure signatures**: Posterior collapse (generated variations nearly identical to inputs); Over-regularization (generated variations unrelated to inputs); Inconsistent performance (generated velocities/timings don't match training style)

- **First 3 experiments**: 1. Test PerTok encoding efficiency by comparing sequence lengths and vocabulary sizes against REMI on the same dataset 2. Validate Composer VAE latent space by interpolating between encoded musical ideas 3. Compare Performer model predictions against ground truth velocity/microtiming distributions on held-out data

## Open Questions the Paper Calls Out

- **Open Question 1**: Does Cadenza's performance tokenizer scale effectively to multi-track MIDI files while maintaining the same reduction in vocabulary size and sequence length? The paper focuses exclusively on single-track generation, leaving multi-track performance characteristics untested.

- **Open Question 2**: How does Cadenza's variation quality compare when trained on culturally diverse musical datasets versus Western classical and hip-hop data? The paper trains performer models on MusicNet (Western classical) and HipHop datasets, but does not explore other musical traditions.

- **Open Question 3**: What is the optimal balance between composer model capacity and performer model size for generating expressive variations? The paper presents a fixed architecture but acknowledges potential improvements through model substitution.

## Limitations
- Limited evaluation on diverse musical styles beyond Western classical and hip-hop
- No direct comparison of tokenization efficiency against established methods like REMI
- Small sample size for human evaluations (50 participants for expressivity comparisons)

## Confidence
- **High**: General architecture and methodology are well-described and follow established VAE and transformer principles
- **Medium**: Quantitative claims about sequence length/vocabulary reduction and expressivity improvements
- **Low**: Claims about the two-stage architecture's efficiency with limited data and the independence assumption between composition and performance modeling

## Next Checks
1. **Tokenization Efficiency Validation**: Implement PerTok and compare sequence lengths and vocabulary sizes against REMI on the same MIDI dataset (e.g., Lakh-MIDI) to verify the claimed 59% length reduction and 95% vocabulary size reduction.

2. **Latent Space Coherence Test**: Perform latent space interpolation between encoded musical ideas in the Composer VAE and evaluate whether generated variations remain musically coherent and related to the interpolation path.

3. **Expressive Fidelity Distribution Analysis**: Compare the Performer model's predicted velocity and microtiming distributions against ground truth distributions on held-out test sets from MusicNet and HipHop, using KL divergence and other distributional similarity metrics to quantify expressive fidelity.