---
ver: rpa2
title: 'Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline'
arxiv_id: '2412.21009'
source_url: https://arxiv.org/abs/2412.21009
tags:
- clip
- retrieval
- dataset
- person
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of identity-aware cross-modal
  retrieval, where the goal is to retrieve images of specific individuals in particular
  contexts based on natural language queries. The authors introduce a novel dataset,
  COCO-PFS, derived from COCO with faces replaced by controlled entities from VGGFace2
  using deepfake techniques.
---

# Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline

## Quick Facts
- arXiv ID: 2412.21009
- Source URL: https://arxiv.org/abs/2412.21009
- Reference count: 40
- Key outcome: Id-CLIP improves recall@1 by 55% compared to CLIP and 6% over CLIP-PAD on entity-in-context retrieval

## Executive Summary
This paper addresses the challenge of identity-aware cross-modal retrieval, where the goal is to retrieve images of specific individuals in particular contexts based on natural language queries. The authors introduce a novel dataset, COCO-PFS, derived from COCO with faces replaced by controlled entities from VGGFace2 using deepfake techniques. They propose Identity-aware CLIP (Id-CLIP), which enhances CLIP-PAD with visual prompt tuning to better discriminate between different identities. Id-CLIP improves recall@1 by 55% compared to CLIP and 6% over CLIP-PAD on entity-in-context retrieval, and achieves a 57% improvement on entity-only retrieval.

## Method Summary
The method introduces Id-CLIP, which enhances CLIP-PAD with visual prompt tuning for identity-aware cross-modal retrieval. The approach involves face extraction from images, face-to-token MLP projection to create identity embeddings, caption enrichment with [ENTITY] tokens, and CLIP encoding with cosine similarity scoring. The visual prompt tuning adds learnable tokens to the image encoder to focus on identity-specific features while keeping original CLIP weights frozen. The model is trained using Info-NCE loss with Adam optimizer for maximum 10 epochs, validating on entity-in-context retrieval tasks.

## Key Results
- Id-CLIP achieves 55% improvement in recall@1 over CLIP and 6% over CLIP-PAD on entity-in-context retrieval
- 57% improvement on entity-only retrieval task, isolating identity discrimination from context effects
- Demonstrates effectiveness of visual prompt tuning for adding identity discrimination to pre-trained CLIP models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Id-CLIP improves identity discrimination by replacing person names in captions with face-derived embeddings and fine-tuning visual backbone with prompt tuning.
- Mechanism: During training, the model learns to map facial features to a CLIP-compatible token embedding. The visual prompt tuning adds learnable tokens to the image encoder to better discriminate identity features without altering core CLIP weights.
- Core assumption: CLIP's pre-trained visual encoder lacks fine-grained facial discrimination, but this can be compensated by targeted visual prompt tuning.
- Evidence anchors: [abstract]: "Id-CLIP improves recall@1 by 55% compared to CLIP and 6% over CLIP-PAD on entity-in-context retrieval"; [section 4.1]: "We enrich the visual encoder of Id-CLIP with some additional learnable visual tokens"; [corpus]: No direct evidence; assumption based on general knowledge about CLIP limitations.
- Break condition: If the face recognition network (FRN) fails to provide discriminative facial features, the MLP projection cannot create meaningful token embeddings, breaking the identity-aware mechanism.

### Mechanism 2
- Claim: Visual Prompt Tuning (VPT) enables fine-grained identity discrimination without full fine-tuning of the CLIP model.
- Mechanism: VPT adds learnable tokens to the visual encoder's input, allowing the model to focus on identity-specific features while keeping the original CLIP weights frozen. This targeted approach improves retrieval performance without catastrophic forgetting.
- Core assumption: Adding learnable visual tokens to the input can effectively learn identity-specific features without disrupting the pre-trained CLIP representations.
- Evidence anchors: [section 4.1]: "The rationale behind Id-CLIP is that the person's identities are discriminated solely through their visual features, both at the query and image encoding stages"; [section 5.2]: "We employ the face crops of the face-swapped persons instead of the original ones as F"; [corpus]: No direct evidence; based on VPT literature [11].
- Break condition: If the number of prompt tuning tokens is insufficient or the training data lacks diversity, VPT cannot learn effective identity discrimination.

### Mechanism 3
- Claim: Face-swapping technique creates a controlled dataset that isolates identity discrimination from context variation.
- Mechanism: By replacing faces in COCO images with controlled identities from VGGFace2, the dataset ensures each identity appears in multiple contexts while maintaining consistent facial features. This controlled setup enables precise evaluation of identity-aware retrieval.
- Core assumption: The face-swapping technique preserves context while replacing identities, creating a valid testbed for identity-aware retrieval.
- Evidence anchors: [section 3]: "We used Roop tool, an advanced, easy-to-use face-swapping software, to reliably swap the faces"; [section 3]: "To make these swaps realistic, we don't select faces randomly. Instead, we use DeepFace tool to detect the gender and ethnicity of the original faces"; [corpus]: No direct evidence; assumption based on general understanding of face-swapping techniques.
- Break condition: If the face-swapping technique introduces artifacts or fails to preserve context, the dataset validity breaks down, making evaluation unreliable.

## Foundational Learning

- Concept: Cross-modal retrieval
  - Why needed here: The task requires finding images based on natural language queries, which is fundamentally a cross-modal retrieval problem.
  - Quick check question: What is the difference between image-to-text and text-to-image retrieval?

- Concept: Contrastive learning
  - Why needed here: CLIP and Id-CLIP use contrastive learning objectives to align image and text embeddings in a shared space.
  - Quick check question: How does InfoNCE loss encourage embeddings of matching pairs to be closer than non-matching pairs?

- Concept: Prompt tuning
  - Why needed here: Visual prompt tuning adds learnable tokens to the input without modifying the model weights, enabling efficient fine-tuning for identity discrimination.
  - Quick check question: What is the difference between prompt tuning and full fine-tuning of a pre-trained model?

## Architecture Onboarding

- Component map: Input: image + text query → CLIP visual encoder (with prompt tuning) + CLIP text encoder → similarity scoring → retrieval ranking
- Critical path: Face extraction → face-to-token MLP projection → [TOK] insertion in caption → CLIP encoding → cosine similarity → ranking
- Design tradeoffs: Using VPT instead of full fine-tuning preserves CLIP's general capabilities while adding identity discrimination, but requires more training data for effective learning
- Failure signatures: Poor identity discrimination (low recall@1), context insensitivity, over-reliance on name-based biases rather than facial features
- First 3 experiments:
  1. Compare Id-CLIP with original CLIP on entity-in-context retrieval using COCO-PFS validation set
  2. Test different [ENTITY] expansion strategies (TOK-only vs NAME+TOK) to find optimal inference
  3. Evaluate entity-only retrieval performance to isolate identity discrimination from context effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Id-CLIP scale with the size and diversity of the face gallery (VGGFace2), and what is the optimal gallery size for maximizing retrieval accuracy while maintaining computational efficiency?
- Basis in paper: [inferred] The paper mentions using VGGFace2 as the face gallery but does not explore how varying its size or diversity affects Id-CLIP's performance.
- Why unresolved: The authors do not provide experiments or analysis on the impact of gallery size or diversity on retrieval accuracy or computational costs.
- What evidence would resolve it: Experiments varying the number of identities in the gallery and measuring retrieval accuracy and computational efficiency would clarify the trade-offs.

### Open Question 2
- Question: To what extent do biases in the original CLIP model (e.g., ethnic or gender biases) influence the performance of Id-CLIP, and can these biases be mitigated without compromising identity recognition accuracy?
- Basis in paper: [explicit] The paper notes that CLIP leverages biases related to names and ethnic groups but does not explore how these biases affect Id-CLIP or how to mitigate them.
- Why unresolved: The authors do not investigate the impact of CLIP's inherent biases on Id-CLIP's performance or propose methods to address them.
- What evidence would resolve it: Analysis of Id-CLIP's performance across diverse demographics and experiments testing bias mitigation techniques would provide insights.

### Open Question 3
- Question: How does Id-CLIP perform on real-world datasets with uncontrolled identities and contexts, such as personal photo collections or video archives, compared to controlled datasets like COCO-PFS?
- Basis in paper: [inferred] The paper evaluates Id-CLIP on COCO-PFS, a controlled dataset, but does not test its performance on unstructured, real-world data.
- Why unresolved: The authors do not provide evidence of Id-CLIP's effectiveness in practical, real-world scenarios with diverse and unstructured data.
- What evidence would resolve it: Testing Id-CLIP on real-world datasets and comparing its performance to controlled datasets would demonstrate its generalizability.

## Limitations
- Dataset Validity: The quality and realism of face swaps could affect downstream performance, but quantitative metrics on face swap quality are not provided.
- Generalizability: The 55% improvement may not generalize to real-world scenarios where faces aren't controlled.
- Face Recognition Dependency: The entire identity-aware mechanism relies on the face recognition network, which could fail or provide poor embeddings.

## Confidence
**High Confidence Claims**:
- Id-CLIP architecture improves identity-aware retrieval on COCO-PFS dataset
- Visual prompt tuning can enhance CLIP for identity discrimination without full fine-tuning
- The face-swapping technique creates a controlled testbed for identity-aware retrieval

**Medium Confidence Claims**:
- The 55% improvement over CLIP and 6% over CLIP-PAD generalizes to real-world scenarios
- Visual prompt tuning is the optimal method for adding identity discrimination to CLIP
- The MLP projection effectively maps facial features to CLIP-compatible embeddings

**Low Confidence Claims**:
- The dataset covers sufficient diversity of identities and contexts for comprehensive evaluation
- The method performs well on identities not seen during training
- The face-swapping technique preserves all relevant contextual information

## Next Checks
1. **Dataset Quality Validation**: Conduct a human evaluation study to assess the quality and realism of face swaps in COCO-PFS. Measure face swap quality metrics (identity preservation, context preservation, visual artifacts) and correlate these with model performance to identify failure modes.

2. **Cross-Dataset Generalization**: Evaluate Id-CLIP on a real-world identity-aware retrieval dataset (e.g., Flickr-Faces-HQ with captions) to test generalization beyond the controlled COCO-PFS environment. Compare performance against the 55% improvement claimed on COCO-PFS.

3. **Ablation on FRN Quality**: Systematically vary the quality of face recognition embeddings (using different FRNs or adding noise) to quantify the dependency of Id-CLIP performance on face recognition quality. This would reveal whether the method is robust to FRN failures or overly dependent on perfect facial feature extraction.