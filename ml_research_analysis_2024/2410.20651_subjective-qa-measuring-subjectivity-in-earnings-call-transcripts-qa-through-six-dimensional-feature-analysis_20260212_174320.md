---
ver: rpa2
title: 'SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts'' QA Through
  Six-Dimensional Feature Analysis'
arxiv_id: '2410.20651'
source_url: https://arxiv.org/abs/2410.20651
tags:
- features
- dataset
- answer
- https
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'SubjECTive-QA introduces a dataset of 49,446 annotations on 2,747
  earnings call QA pairs, annotated across six subjective features: Clear, Assertive,
  Cautious, Optimistic, Specific, and Relevant. The dataset addresses the gap in evaluating
  misinformation and communication quality in QA settings by moving beyond factual
  correctness to assess tone, clarity, and specificity.'
---

# SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts' QA Through Six-Dimensional Feature Analysis

## Quick Facts
- arXiv ID: 2410.20651
- Source URL: https://arxiv.org/abs/2410.20651
- Reference count: 40
- Primary result: Introduces dataset of 49,446 annotations on 2,747 earnings call QA pairs across six subjective features, achieving 65.97% weighted F1 in transfer learning to political domains

## Executive Summary
SubjECTive-QA addresses the gap in evaluating misinformation and communication quality in QA settings by moving beyond factual correctness to assess tone, clarity, and specificity. The dataset introduces a six-dimensional subjective feature annotation framework that captures nuanced misinformation where responses are factually correct but misleading through omission, vagueness, or lack of technical detail. Models were benchmarked using PLMs and LLMs, with RoBERTa-base and Llama-3-70b-Chat achieving similar performance on clearer features like Relevant and Clear, while outperforming on more subjective features like Specific and Assertive. The framework demonstrates broader applicability through transfer learning to White House Press Briefings and Gaggles, yielding an average weighted F1 score of 65.97%.

## Method Summary
SubjECTive-QA constructs a dataset of 49,446 annotations on 2,747 earnings call QA pairs from 120 transcripts, annotated across six subjective features: Clear, Assertive, Cautious, Optimistic, Specific, and Relevant. The dataset is built through data collection, cleaning, and manual annotation by multiple annotators. Models including FinBERT, BERT, RoBERTa, Llama-3, and Mixtral are fine-tuned on the dataset using sequence classification with cross-entropy loss. Performance is evaluated using weighted F1 scores for each feature, with transfer learning tested on political press briefing domains.

## Key Results
- Dataset contains 49,446 annotations across 2,747 QA pairs from 120 earnings calls
- RoBERTa-base and Llama-3-70b-Chat achieve similar weighted F1 scores (2.17% mean difference) on objective features like Relevant and Clear
- Models outperform by 10.01% mean F1 difference on subjective features like Specific and Assertive
- Transfer learning to White House Press Briefings and Gaggles achieves 65.97% average weighted F1 score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The six-dimensional subjective feature annotation framework captures nuanced misinformation beyond factual errors by focusing on clarity, assertiveness, specificity, relevance, cautiousness, and optimism.
- Mechanism: The framework identifies subtle forms of misinformation where answers are factually correct but misleading through omission, vagueness, or lack of technical detail. By annotating long-form QA pairs across these six dimensions, the model can detect when a technically accurate response fails to address the question's intent or provide sufficient detail for informed decision-making.
- Core assumption: These six features are independent and collectively capture the key dimensions of subjective misinformation in formal QA settings.
- Evidence anchors:
  - [abstract] "a softer form of misinformation involves responses that are factually correct but lack certain features such as clarity and relevance"
  - [section] "The meticulous annotation of these transcripts with a six-label subjective feature rating system aids in capturing the dimensions of clarity, assertiveness, cautiousness, optimism, specificity, and relevance"

### Mechanism 2
- Claim: Domain-specific pretraining (FinBERT) improves model performance on subjective features requiring financial domain knowledge, while general-purpose models perform better on more objective features.
- Mechanism: FinBERT's pretraining on financial text enables it to recognize and interpret financial terminology and context-specific nuances, making it better suited for detecting specificity in technical financial details. General-purpose models like RoBERTa and BERT, with their broader pretraining, perform better on features like Clear and Relevant that rely more on general linguistic patterns.
- Core assumption: Financial domain knowledge is crucial for accurately assessing specificity and technical accuracy in earnings call responses.
- Evidence anchors:
  - [abstract] "the best-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar weighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity, such as Relevant and Clear"
  - [section] "For Specific, FinBERT had the highest weighted F1 score. This can be attributed to the fact that the other models are general-purpose models, whereas FinBERT is a domain-specific model for finance"

### Mechanism 3
- Claim: Transfer learning from earnings call transcripts to other QA domains like political press briefings demonstrates the broader applicability of the subjective feature detection framework.
- Mechanism: The subjective features identified in earnings calls (clarity, relevance, assertiveness, etc.) are transferable to other domains where nuanced communication analysis is critical, such as political press briefings. By fine-tuning models trained on earnings call data to these new domains, the framework can be applied to detect subjective misinformation and assess communication quality across various fields.
- Core assumption: The subjective features are relevant and meaningful across different QA domains beyond finance.
- Evidence anchors:
  - [abstract] "Furthermore, testing SubjECTive-QA's generalizability using QAs from White House Press Briefings and Gaggles yields an average weighted F1 score of 65.97%"
  - [section] "Our analysis of QA pairs from White House Press Briefings and Gaggles demonstrates the utility of our models in a political context"

## Foundational Learning

- Concept: Subjective misinformation and its detection
  - Why needed here: Understanding the concept of subjective misinformation, which goes beyond factual errors to include issues like vagueness, lack of relevance, and insufficient detail, is crucial for grasping the purpose and significance of the SubjECTive-QA dataset and framework.
  - Quick check question: Can you provide an example of subjective misinformation in a QA setting, where the answer is factually correct but still misleading or unhelpful?

- Concept: Natural Language Processing (NLP) and pretraining
  - Why needed here: Familiarity with NLP concepts, particularly pretraining and fine-tuning of language models like BERT, RoBERTa, and FinBERT, is essential for understanding how the models are trained and evaluated on the subjective features.
  - Quick check question: What is the difference between pretraining and fine-tuning in the context of NLP, and why is domain-specific pretraining beneficial for certain tasks?

- Concept: Transfer learning and its applications
  - Why needed here: Understanding transfer learning, which involves applying knowledge gained from one domain to another, is important for grasping how the subjective feature detection framework can be applied to domains beyond earnings calls, such as political press briefings.
  - Quick check question: How does transfer learning work, and what are some potential benefits and challenges of applying a model trained on one domain to a different domain?

## Architecture Onboarding

- Component map: Dataset of 49,446 annotations on 2,747 QA pairs -> Six subjective features (Clear, Assertive, Cautious, Optimistic, Specific, Relevant) -> PLMs and LLMs fine-tuning process -> Weighted F1 evaluation -> Transfer learning to political domains

- Critical path: Manual annotation of earnings call transcripts -> Model fine-tuning on subjective features -> Weighted F1 evaluation -> Transfer learning testing on new domains

- Design tradeoffs: Comprehensive subjective feature set vs. annotation complexity; domain-specific pretraining vs. generalization ability; weighted F1 metric vs. alternative evaluation measures

- Failure signatures: Annotator bias affecting training data quality; overfitting to financial domain language; subjective features missing key misinformation dimensions; poor transfer learning performance

- First 3 experiments:
  1. Reproduce benchmark results by fine-tuning RoBERTa-base, BERT, FinBERT-tone on SubjECTive-QA dataset and evaluating weighted F1 scores
  2. Conduct ablation study to determine relative importance of each subjective feature in detecting misinformation
  3. Apply best-performing models to customer service domain through transfer learning and evaluate performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SubjECTive-QA perform on earnings call transcripts from companies outside the United States, such as those listed on other major stock exchanges?
- Basis in paper: [inferred] from the limitation section stating that the dataset only encompasses companies listed on the New York Stock Exchange from 2007 to 2021.
- Why unresolved: The paper does not provide any empirical evidence or analysis on the performance of SubjECTive-QA on earnings call transcripts from companies outside the US. This is explicitly mentioned as a limitation, indicating that the dataset's applicability to other markets is unexplored.
- What evidence would resolve it: Testing SubjECTive-QA on earnings call transcripts from companies listed on other major stock exchanges (e.g., London Stock Exchange, Tokyo Stock Exchange) and evaluating the model's performance on these datasets would provide evidence of its generalizability.

### Open Question 2
- Question: How does the inclusion of audio features, such as pitch, intonation, and tone, impact the accuracy of the SubjECTive-QA model in detecting subjective features compared to using only written transcripts?
- Basis in paper: [explicit] from the limitations section mentioning that the work uses written transcripts of ECs rather than the original audio, and some aspects like pitch, intonation, and tone that may be clear in an audio extract will not be reflected in the presented dataset.
- Why unresolved: The paper acknowledges that certain subjective cues might be lost when using only written transcripts. However, it does not provide any empirical comparison between the performance of models trained on audio data versus text data.
- What evidence would resolve it: Conducting an experiment where the SubjECTive-QA model is trained and evaluated on both audio and text data from the same earnings calls, and comparing the performance metrics, would provide evidence of the impact of audio features.

### Open Question 3
- Question: What is the impact of annotator background (e.g., educational, geographic, gender) on the annotations in SubjECTive-QA, and how does this bias affect the model's performance?
- Basis in paper: [explicit] from the ethical considerations section acknowledging the presence of certain limitations and biases due to educational, geographic, and gender biases within the annotation and research work.
- Why unresolved: The paper recognizes the potential for bias but does not provide any quantitative analysis of how these biases affect the annotations or the model's performance. It also does not explore methods to mitigate these biases.
- What evidence would resolve it: Analyzing the correlation between annotator background and their annotations, and assessing the impact of these biases on the model's performance, would provide evidence of the effect of annotator background. Additionally, exploring techniques to reduce bias in the annotation process would be valuable.

## Limitations

- Dataset only encompasses companies listed on the New York Stock Exchange from 2007 to 2021, limiting generalizability to other markets
- Work uses written transcripts rather than original audio, potentially missing subjective cues like pitch, intonation, and tone
- Presence of educational, geographic, and gender biases within the annotation and research work that may affect model performance

## Confidence

- **High Confidence**: Dataset construction methodology, annotation process, and benchmark results for the earnings call domain
- **Medium Confidence**: Transfer learning results to political domains and generalizability of the six-dimensional framework
- **Low Confidence**: Claim that these six features collectively capture all key dimensions of subjective misinformation in formal QA settings

## Next Checks

1. Conduct inter-annotator agreement analysis to quantify annotation consistency and identify potential bias sources
2. Test the framework on additional QA domains (e.g., customer service, medical consultations) to assess broader generalizability
3. Perform feature correlation analysis to verify the independence assumption and identify potential feature groupings