---
ver: rpa2
title: Problem-Solving in Language Model Networks
arxiv_id: '2406.12374'
source_url: https://arxiv.org/abs/2406.12374
tags:
- agents
- networks
- correct
- agent
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study extends multi-agent debate to complex network topologies
  to improve large language model (LLM) question-answering performance. Agents are
  represented as nodes in graphs, connected via communication edges, with self-loops
  for self-reflection.
---

# Problem-Solving in Language Model Networks

## Quick Facts
- arXiv ID: 2406.12374
- Source URL: https://arxiv.org/abs/2406.12374
- Authors: Ciaran Regan; Alexandre Gournail; Mizuki Oka
- Reference count: 5
- Random networks achieve similar accuracy to fully connected networks while using 250× fewer tokens

## Executive Summary
This study extends multi-agent debate to complex network topologies to improve large language model (LLM) question-answering performance. Agents are represented as nodes in graphs, connected via communication edges, with self-loops for self-reflection. The research demonstrates that random networks achieve accuracy similar to fully connected networks while using significantly fewer tokens. Network consensus strongly correlates with correct answers—high consensus indicates correct answers, while disagreement typically indicates errors. The study also reveals how self-reflection benefits agents when surrounded by incorrect neighbors, but its impact weakens as more neighbors are correct, highlighting the balance between individuality and collaboration.

## Method Summary
The study implements multi-agent debate on networks where agents answer questions individually then re-evaluate based on neighbors' responses over 4 rounds, with majority vote determining final answer. Using GPT-3.5-Turbo on MMLU high school mathematics questions, researchers generated scale-free, random, fully connected, and fully disconnected 25-agent networks. They measured accuracy, consensus via Simpson index, and agent influence dynamics while testing how network topology affects collective problem-solving performance.

## Key Results
- Random networks achieve similar accuracy to fully connected networks while using 250× fewer input tokens per round
- High consensus among agents strongly correlates with correct answers, while disagreement typically indicates incorrect answers
- Biased agents at network hubs significantly impact performance, with correct hub agents doubling accuracy and incorrect hub agents halving it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random networks achieve similar accuracy to fully connected networks while using 250× fewer input tokens per round.
- Mechanism: Sparse connectivity preserves sufficient information spread while drastically reducing redundant communication overhead. Information propagates efficiently through short average path lengths in random networks, avoiding the quadratic edge growth of fully connected graphs.
- Core assumption: Information can spread effectively through sparse networks with average diameter around 4 rounds, reaching consensus without requiring all-to-all communication.
- Evidence anchors:
  - [abstract] "random networks achieve similar performance to fully connected networks while using significantly fewer tokens"
  - [section] "random networks achieve similar performance to fully connected networks, while using 250 times fewer input tokens per round of debate"
  - [corpus] Weak evidence - corpus contains related multi-agent work but no direct evidence about token efficiency differences
- Break condition: When average path length exceeds debate rounds, causing incomplete information spread and reduced consensus

### Mechanism 2
- Claim: High consensus among agents strongly correlates with correct answers, while disagreement typically indicates incorrect answers.
- Mechanism: When correct answers emerge, they spread through the network via neighbor interactions, creating uniform agreement. Incorrect answers fail to achieve consensus as they cannot overcome the spread of correct alternatives.
- Core assumption: Correct answers have inherent advantages in spreading through neighbor influence and self-reflection mechanisms.
- Evidence anchors:
  - [abstract] "a strong consensus among agents correlates with correct answers, whereas divided responses typically indicate incorrect answers"
  - [section] "when the system answers correctly, there is a higher consensus among agents, indicating greater certainty. Conversely, when the system answers incorrectly, the consensus is weaker, reflecting less certainty"
  - [corpus] Weak evidence - corpus contains related consensus work but no direct evidence about consensus-accuracy correlation
- Break condition: When noise or random agreement creates false consensus, or when multiple correct answers exist simultaneously

### Mechanism 3
- Claim: Biased agents at network hubs significantly impact system performance, with correct hub agents doubling accuracy and incorrect hub agents halving it.
- Mechanism: Hub nodes control information flow due to high connectivity. Correct hub agents inject accurate information into the network early, while incorrect hub agents spread misinformation to large portions of the network.
- Core assumption: Information flow in networks is dominated by high-degree nodes, making hub positions critical for system outcomes.
- Evidence anchors:
  - [abstract] "bias plays a strong role in system performance with correctly biased hub nodes boosting performance"
  - [section] "networks with correctly biased nodes at their hubs perform significantly better (worse) than their unbiased counterpart. In particular, networks with correctly biased hub nodes performed twice as well when compared to networks with incorrectly biased hubs"
  - [corpus] Weak evidence - corpus contains related multi-agent work but no direct evidence about hub bias effects
- Break condition: When network has redundant paths around hubs, or when other strong influencers counteract hub bias

## Foundational Learning

- Graph theory and network topology
  - Why needed here: Understanding how different network structures (fully connected, random, scale-free, disconnected) affect information spread and agent influence
  - Quick check question: What is the edge count formula for a fully connected network with n agents?

- Multi-agent consensus dynamics
  - Why needed here: Understanding how agents reach agreement through debate, self-reflection, and neighbor influence
  - Quick check question: What happens to consensus when agents have conflicting information sources?

- Statistical measures for agreement
  - Why needed here: Simpson index quantifies consensus levels and helps interpret system certainty
  - Quick check question: What Simpson index value indicates perfect agreement among all agents?

- Bias propagation in networks
  - Why needed here: Understanding how biased agents affect collective outcomes through their influence on neighbors
  - Quick check question: How does node centrality affect the spread of biased information?

## Architecture Onboarding

- Component map:
  - Agent nodes: LLM instances (GPT-3.5-Turbo) that solve problems and debate
  - Communication edges: Connections between agents for information exchange
  - Debate rounds: Iterative process where agents reconsider answers based on neighbors
  - Consensus mechanism: Majority voting after final round
  - Bias injection: Manual insertion of correct/incorrect answers into specific nodes
  - Performance metrics: Accuracy, consensus (Simpson index), influence analysis

- Critical path:
  1. Initialize network topology and agents
  2. First round: Individual problem solving
  3. Subsequent rounds: Agent reconsideration based on neighbors
  4. Final round: Majority vote for system answer
  5. Metrics calculation and analysis

- Design tradeoffs:
  - Connectivity vs efficiency: Fully connected maximizes information spread but is computationally expensive; random networks offer good tradeoff
  - Debate rounds vs convergence: More rounds improve accuracy but increase latency
  - Agent count vs token limits: More agents provide diversity but face context window constraints
  - Bias strength vs system robustness: Strong biased agents are influential but may reduce system reliability

- Failure signatures:
  - Accuracy degradation across rounds: Indicates poor self-reflection or negative neighbor influence
  - Low consensus with high accuracy: Suggests random agreement rather than true information spread
  - High consensus with low accuracy: Indicates groupthink or widespread incorrect bias
  - Oscillating answers between rounds: Shows unstable information dynamics

- First 3 experiments:
  1. Compare random vs fully connected networks on same question set to validate token efficiency claims
  2. Test hub bias effect by placing correct/incorrect agents at highest-degree nodes and measuring accuracy change
  3. Measure consensus-accuracy correlation by plotting Simpson index against system accuracy for all questions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multi-agent LLM systems scale with the number of agents when using random network topologies?
- Basis in paper: [explicit] The paper notes that random networks use 250× fewer tokens than fully connected networks while achieving similar accuracy, but does not explore performance scaling with larger agent counts.
- Why unresolved: The experiments were limited to 25 agents due to computational constraints, leaving the relationship between agent count and performance in random networks unexplored.
- What evidence would resolve it: Systematic experiments varying agent counts (e.g., 25, 50, 100, 200) while measuring accuracy and token usage across random network topologies.

### Open Question 2
- Question: What is the optimal balance between self-reflection and neighbor interaction for maximizing LLM agent performance in different network topologies?
- Basis in paper: [explicit] The paper identifies a "balance between self-reflection and interconnectedness" where self-reflection aids when local interactions are incorrect, but this balance's optimization across topologies remains unexplored.
- Why unresolved: The study analyzes influence patterns but doesn't systematically vary the weight or frequency of self-reflection versus neighbor interaction to find optimal configurations.
- What evidence would resolve it: Controlled experiments varying the emphasis on self-reflection versus neighbor interaction across different network topologies while measuring performance impact.

### Open Question 3
- Question: How do different network topologies affect the system's ability to detect and correct misinformation spread by biased agents?
- Basis in paper: [explicit] The paper shows that correctly biased hub nodes can double accuracy while incorrectly biased hubs significantly reduce performance, but doesn't explore how network structure affects misinformation correction dynamics.
- Why unresolved: The study examines static bias placement but doesn't investigate how different topologies enable or inhibit the system's ability to identify and override incorrect information from biased agents.
- What evidence would resolve it: Experiments introducing controlled misinformation into different network topologies and measuring the time and probability of correction across the network.

## Limitations

- The study focuses exclusively on mathematical questions from the MMLU dataset, leaving unclear whether network topology effects translate to other domains
- The fixed 4-round debate protocol may be suboptimal for different network structures - some topologies might benefit from more or fewer rounds
- Results depend on GPT-3.5-Turbo with specific prompt structures, and findings could vary significantly with different model sizes or prompting strategies

## Confidence

- High Confidence: The finding that network consensus correlates with answer accuracy has strong empirical support from the correlation analysis between Simpson index values and correctness rates across multiple network topologies
- Medium Confidence: The token efficiency advantage of random networks over fully connected networks is well-supported, though the exact 250× figure may depend on implementation details
- Medium Confidence: The impact of hub bias on system performance shows clear directional effects, but the specific "doubling/halving" accuracy figures may be dataset-specific

## Next Checks

1. Cross-domain validation: Test whether consensus-accuracy correlation holds for non-mathematical questions (e.g., MMLU science, humanities, or commonsense reasoning) to assess domain generalizability

2. Round count sensitivity: Systematically vary the number of debate rounds (1-8 rounds) across different network topologies to identify optimal round counts for each structure

3. Model size scaling: Repeat key experiments with GPT-4 and smaller models (e.g., GPT-3.5-turbo-0613) to determine whether network effects scale consistently with model capability