---
ver: rpa2
title: Ranking Large Language Models without Ground Truth
arxiv_id: '2402.14860'
source_url: https://arxiv.org/abs/2402.14860
tags:
- responses
- triplet
- evaluation
- which
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of ranking large language models
  (LLMs) without access to ground truth or reference responses. The core idea is to
  use triplets of models where each model judges the other two, with the goal of identifying
  the worst model in each triplet with high probability.
---

# Ranking Large Language Models without Ground Truth

## Quick Facts
- arXiv ID: 2402.14860
- Source URL: https://arxiv.org/abs/2402.14860
- Reference count: 40
- Primary result: Methods to rank LLMs without ground truth achieve higher RBO/MAP than baselines on summarization, multiple-choice, and dialog tasks

## Executive Summary
This paper addresses the challenge of ranking large language models without access to ground truth or reference responses. The authors propose using triplets of models where each model judges the other two, with the goal of identifying the worst model in each triplet with high probability. This approach is inspired by real-life scenarios where both experts and knowledgeable individuals can identify novices. Two methods are introduced: Greedy Triplet Ranking (GTR) and Full Triplet Ranking (FTR), both of which reliably recover close to true rankings without reference data.

## Method Summary
The paper proposes two methods for ranking LLMs without ground truth: Greedy Triplet Ranking (GTR) and Full Triplet Ranking (FTR). Both methods use triplets of models to identify the worst model in each triplet. GTR iteratively removes the worst model from each triplet until a complete ranking is achieved, while FTR uses reputation-based voting across all triplets to progressively rank models. The methods are evaluated on summarization, multiple-choice, and dialog tasks, showing improved performance over baseline methods like Most Common Answer (MCA) and a state-of-the-art LLM-as-a-judge approach.

## Key Results
- GTR and FTR methods outperform MCA and Prometheus-2 baselines on RBO and MAP metrics
- Methods particularly effective for generative tasks like summarization and dialog
- Performance degrades when models frequently agree on incorrect responses, especially in multiple-choice tasks with few options

## Why This Works (Mechanism)

### Mechanism 1
Models with higher accuracy can reliably identify models with lower accuracy as worse when judged in triplets. In a triplet (Mi, Mj, Mk) with accuracies ai > aj > ak, both Mi and Mj will rank Mk as worse because Mk's responses overlap less with their correct responses than their responses overlap with each other.

### Mechanism 2
Reputation-based voting in FTR can progressively rank models by weighting judges based on their cumulative performance. Each model's reputation score is updated based on how often it's voted as better than others.

### Mechanism 3
Greedy triplet ranking (GTR) can efficiently produce near-optimal rankings by iteratively removing the worst model from each triplet. GTR maintains a ranked list, starting with an arbitrary triplet, identifying the worst model, and progressively comparing remaining models.

## Foundational Learning

- **Concept**: Triplet comparison ranking
  - Why needed here: The core mechanism relies on comparing three models at a time to identify the worst performer without ground truth
  - Quick check question: Why does using only two models fail to identify which is worse without ground truth?

- **Concept**: Reputation-based iterative ranking
  - Why needed here: FTR requires understanding how to propagate relative model performance through weighted voting across multiple iterations
  - Quick check question: How does a model's reputation score change if it consistently wins triplet comparisons?

- **Concept**: Ranking evaluation metrics (RBO, MAP)
  - Why needed here: The methods need to be evaluated against true rankings derived from reference data using established metrics
  - Quick check question: What does an RBO score of 1.0 indicate about the agreement between estimated and true rankings?

## Architecture Onboarding

- **Component map**: Prompts -> Model responses -> Evaluation function -> GTR/FTR ranking algorithm -> Ranked list of models

- **Critical path**:
  1. Generate responses from all models for all prompts
  2. For each triplet comparison, use evaluation function to determine worst model
  3. For GTR: Iteratively remove worst models until ranking is complete
  4. For FTR: Compute reputation scores across all triplets until convergence
  5. Sort models by reputation score or final ranking

- **Design tradeoffs**:
  - GTR: O(n²) time complexity, fewer evaluations, potentially less accurate
  - FTR: O(n³) time complexity, more evaluations, potentially more accurate
  - MCA: Simple ensemble approach, works well for discrete responses, less effective for generative tasks

- **Failure signatures**:
  - GTR: Ranking fails when models have similar performance or when worst model cannot be reliably identified
  - FTR: Ranking fails when reputation scores don't converge or when initial comparisons are noisy
  - Both: Performance degrades with low variance in model outputs or high correlation in incorrect responses

- **First 3 experiments**:
  1. Small-scale validation: Run GTR on 3-5 models with synthetic data where true rankings are known, verify correct identification of worst model in each triplet
  2. Comparison with baseline: Implement MCA method and compare rankings on multiple-choice dataset with 2-3 answer options
  3. Noise sensitivity test: Introduce controlled noise into evaluation function and measure impact on GTR and FTR rankings using RBO metric

## Open Questions the Paper Calls Out

- **Open Question 1**: What are the exact sufficient conditions for the triplet approach to succeed when models can agree on incorrect responses?
- **Open Question 2**: How does the performance of the triplet approach scale with the size of the response set in multiple-choice tasks?
- **Open Question 3**: What is the impact of using different evaluation functions (e.g., NLI models, BERT scores) on the performance of the triplet approach?
- **Open Question 4**: How does the performance of FTR compare to more sophisticated matrix completion or Bayesian approaches?
- **Open Question 5**: What is the optimal set size for comparisons in each round, and how does it affect the bias-variance tradeoff?

## Limitations

- The approach relies on the assumption that models with higher accuracy can reliably identify models with lower accuracy in triplet comparisons
- Requires a large number of triplet comparisons, particularly for the FTR approach which has O(n³) time complexity
- Primarily tested on generative tasks like summarization and dialog, with limited validation on other LLM tasks

## Confidence

- **High Confidence**: The core mechanism of using triplet comparisons to identify the worst model without ground truth is well-founded and mathematically justified
- **Medium Confidence**: The claim that FTR outperforms GTR in accuracy due to reputation-based voting is supported by experiments but may be sensitive to initial conditions
- **Low Confidence**: The assertion that this approach works particularly well for generative tasks may be overstated, as the comparison is primarily against the MCA method

## Next Checks

1. **Cross-task validation**: Test GTR and FTR methods on code generation and reasoning tasks to assess generalizability beyond summarization and dialog tasks
2. **Robustness to noise**: Systematically vary the noise level in the evaluation function and measure how it affects ranking accuracy for both methods
3. **Efficiency-accuracy tradeoff**: Conduct a detailed analysis of the relationship between the number of triplet comparisons performed and ranking accuracy for both GTR and FTR methods across different model sets