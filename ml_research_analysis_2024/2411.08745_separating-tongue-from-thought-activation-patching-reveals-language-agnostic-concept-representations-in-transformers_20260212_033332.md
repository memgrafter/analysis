---
ver: rpa2
title: 'Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic
  Concept Representations in Transformers'
arxiv_id: '2411.08745'
source_url: https://arxiv.org/abs/2411.08745
tags:
- concept
- language
- source
- prompt
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents causal evidence that transformer-based language
  models develop language-agnostic concept representations. Through activation patching
  experiments on translation tasks, the authors demonstrate that language models first
  resolve the target language, then the concept to be translated, suggesting independent
  processing of these elements.
---

# Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers

## Quick Facts
- arXiv ID: 2411.08745
- Source URL: https://arxiv.org/abs/2411.08745
- Reference count: 40
- Primary result: Averaging concept representations across languages improves translation performance

## Executive Summary
This paper presents causal evidence that transformer-based language models develop language-agnostic concept representations. Through activation patching experiments on translation tasks, the authors demonstrate that language models first resolve the target language, then the concept to be translated, suggesting independent processing of these elements. The key finding shows that averaging concept representations across multiple languages improves translation performance rather than degrading it, supporting the hypothesis of language-agnostic concept representations. This improvement was observed across multiple models including Llama 2 7B, Llama 3 8B, Mistral 7B, Qwen 1.5 7B, Llama 2 70B, and Aya 23 8B. The authors further demonstrate that models can successfully generate natural language definitions from these mean representations, confirming their semantic coherence.

## Method Summary
The authors employed activation patching experiments to investigate concept representations in multilingual language models. They first identified where target language and concept representations are formed in the model architecture by systematically replacing activations between clean and corrupted model runs. Using this methodology, they demonstrated that language and concept representations are processed independently. The core experiment involved averaging concept representations across 100 concepts from multiple languages and using these averaged representations as inputs to translation models, which resulted in improved translation performance. They also tested whether these averaged representations retained semantic meaning by having models generate definitions from them.

## Key Results
- Averaging concept representations across languages improves translation performance across multiple models (Llama 2 7B, Llama 3 8B, Mistral 7B, Qwen 1.5 7B, Llama 2 70B, and Aya 23 8B)
- Models can generate coherent natural language definitions from averaged concept representations
- Activation patching reveals that language models first resolve target language, then concepts during translation

## Why This Works (Mechanism)
The mechanism underlying these findings appears to be that transformer models develop abstract, language-agnostic representations of concepts that are independent of the specific linguistic form used to express them. When a model processes multiple languages, it appears to create unified concept representations that capture the underlying semantic meaning rather than language-specific encodings. This allows for averaging across languages without loss of information, and may explain the observed performance improvements. The activation patching experiments reveal that these concept representations are formed at specific locations in the model architecture and can be causally linked to translation outputs.

## Foundational Learning
- **Activation patching methodology**: Technique for identifying causal relationships between model activations and outputs by swapping activations between different model runs
  - *Why needed*: To establish causal links between specific internal representations and model behavior
  - *Quick check*: Can you explain how swapping activations between clean and corrupted runs reveals causal relationships?

- **Concept representation averaging**: Process of computing mean representations across multiple languages for the same set of concepts
  - *Why needed*: To test whether language-agnostic representations exist and whether averaging preserves semantic information
  - *Quick check*: What would happen if representations were language-specific rather than language-agnostic?

- **Translation task architecture**: Understanding how translation models process source and target languages
  - *Why needed*: To identify where language and concept representations are formed in the model
  - *Quick check*: Where in a typical transformer architecture would you expect language and concept representations to be formed?

## Architecture Onboarding

**Component map**: Input tokens -> Embedding layer -> Encoder layers -> Concept representations formed -> Target language resolution -> Decoder layers -> Output tokens

**Critical path**: Concept resolution occurs before target language resolution, with both being independent processes that feed into the final translation output

**Design tradeoffs**: The study reveals that models can benefit from language-agnostic representations, suggesting that multilingual training may create more robust semantic representations than monolingual training

**Failure signatures**: If concept representations were language-specific, averaging would degrade performance; if language and concept resolution were not independent, activation patching would not reveal clear separation

**3 first experiments**:
1. Test activation patching on different layers to pinpoint where concept representations are formed
2. Verify that averaging works for different word types (verbs, adjectives) beyond the tested nouns
3. Check whether the improvement from averaging holds when using different numbers of concepts per language

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on translation tasks, which may not generalize to other language model capabilities or domains
- Experiments primarily examine noun concepts, leaving open questions about how well these findings extend to verbs, adjectives, or more complex linguistic structures
- The study uses only 100 concept representations per language for averaging experiments, which may not capture the full complexity of language-agnostic representations

## Confidence

**High confidence**: The core finding that averaging concept representations across languages improves translation performance is well-supported by empirical evidence across multiple models and datasets.

**Medium confidence**: The interpretation that language models first resolve target language then concepts, while supported by activation patching results, could have alternative explanations related to attention patterns or model architecture.

**Medium confidence**: The claim that models can generate coherent definitions from mean representations is demonstrated but relies on a limited evaluation approach.

## Next Checks

1. Test whether language-agnostic concept representations extend to non-translation tasks such as question answering or summarization across multiple languages.

2. Expand the concept representation averaging experiments to include a wider variety of word types (verbs, adjectives, phrases) and larger sample sizes per language.

3. Validate the activation patching results using alternative interpretability methods like sparse autoencoders or direct logit attribution to confirm the proposed processing order of language vs concept resolution.