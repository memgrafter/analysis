---
ver: rpa2
title: 'A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders'
arxiv_id: '2409.14507'
source_url: https://arxiv.org/abs/2409.14507
tags:
- feature
- latent
- absorption
- letter
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Feature absorption occurs in sparse autoencoders (SAEs) when hierarchical
  features split: a general latent fails to activate on certain cases where it should,
  and a more specific latent instead fires while contributing the missing feature
  direction. This phenomenon arises from optimizing for sparsity in SAEs and leads
  to seemingly interpretable latents having arbitrary false negatives in their mainline
  interpretation.'
---

# A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders

## Quick Facts
- arXiv ID: 2409.14507
- Source URL: https://arxiv.org/abs/2409.14507
- Reference count: 40
- Key outcome: Feature absorption occurs in SAEs when hierarchical features split, leading to latents with arbitrary false negatives in their mainline interpretation

## Executive Summary
This paper identifies and analyzes feature absorption in Sparse Autoencoders (SAEs) trained on large language models. Feature absorption occurs when hierarchical features split: a general latent fails to activate on certain cases where it should, and a more specific latent instead fires while contributing the missing feature direction. Through experiments on hundreds of LLM SAEs including Gemma Scope, Qwen2, and Llama models, the authors demonstrate that absorption rates can reach up to 40% for some letters, increasing with higher sparsity and wider SAEs. The phenomenon arises from optimizing for sparsity in SAEs and suggests that SAE latents may be unreliable classifiers for high-stakes tasks, complicating sparse circuit discovery.

## Method Summary
The study uses k-sparse probing, logistic regression probes, and ablation studies to detect feature absorption in SAEs trained on LLM activations. The methodology involves training logistic regression probes to identify first-letter features, using k-sparse probing to detect feature splitting, and conducting ablation studies on false negatives to identify absorbing latents. The absorption rate is calculated based on thresholds for cosine similarity, ablation dominance, and F1 score improvements. Experiments were conducted on Gemma-2-2B, Qwen2 0.5B, and Llama 3.2 1B models with various SAE widths and sparsity levels.

## Key Results
- Feature absorption rates increase with higher sparsity (lower L0) and wider SAEs, reaching up to 40% for some letters
- Absorption occurs when hierarchical features split, causing seemingly interpretable latents to have arbitrary false negatives
- Absorption is an optimal strategy for minimizing L1 loss in the presence of hierarchical features
- SAE latents may be unreliable classifiers for high-stakes tasks due to absorption phenomena

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature absorption is caused by optimizing for sparsity in SAEs when underlying features form a hierarchy.
- Mechanism: When a dense feature (e.g., "starts with S") always co-occurs with a sparse feature (e.g., "short"), the SAE can increase sparsity by absorbing the dense feature direction into a latent tracking the sparse feature, then not firing the main dense feature latent on cases where it should.
- Core assumption: The hierarchical feature relationship where child features imply parent features but not vice versa.
- Evidence anchors:
  - [abstract] "feature absorption occurs in sparse autoencoders (SAEs) when hierarchical features split"
  - [section] "Feature absorption is demonstrated in Figure 1, where the feature 'short' always fires alongside a feature representing 'starts with S'"
  - [corpus] Weak - only 5 related papers found with average FMR 0.442, suggesting this is a relatively novel finding

### Mechanism 2
- Claim: Absorption leads to seemingly interpretable latents having arbitrary false negatives in their mainline interpretation.
- Mechanism: The SAE learns gerrymandered latents that fail to fire on seemingly arbitrary cases where the latent should fire according to its mainline interpretation, creating "representation holes."
- Core assumption: SAE latents can be interpreted by examining their activation patterns on training data.
- Evidence anchors:
  - [abstract] "seemingly interpretable latents having arbitrary false negatives in their mainline interpretation"
  - [section] "the seemingly interpretable SAE latent 6510 fails to activate on arbitrary positive examples"
  - [corpus] Moderate - related work on feature composition and feature hedging suggests this is a recognized problem in SAE literature

### Mechanism 3
- Claim: Absorption is an optimal strategy for minimizing L1 loss in the presence of hierarchical features.
- Mechanism: The proof in Appendix A.2 shows that for hierarchical features, absorption maintains perfect reconstruction while decreasing sparsity loss, making it the optimal solution under L1 regularization.
- Core assumption: The SAE is optimizing for reconstruction loss plus L1 sparsity penalty.
- Evidence anchors:
  - [abstract] "absorption was detected using a metric combining k-sparse probing, logistic regression probes, and ablation studies"
  - [section] "Proof: hierarchical features cause absorption" with mathematical derivation
  - [corpus] Strong - multiple papers discuss SAE optimization and sparsity, though none specifically address this hierarchical absorption phenomenon

## Foundational Learning

- Concept: Linear Representation Hypothesis (LRH)
  - Why needed here: The paper assumes features are represented as linear directions in activation space, which is fundamental to how SAEs work
  - Quick check question: What does it mean for features to follow the LRH, and why is this important for SAE interpretability?

- Concept: Hierarchical feature relationships
  - Why needed here: Feature absorption specifically occurs when features form a hierarchy (child implies parent but not vice versa)
  - Quick check question: If feature A always implies feature B, but B doesn't always imply A, which is the parent and which is the child?

- Concept: K-sparse probing
  - Why needed here: Used to detect feature splitting and absorption by measuring how well combinations of SAE latents perform as classifiers
  - Quick check question: How does k-sparse probing help identify when multiple SAE latents are tracking related features?

## Architecture Onboarding

- Component map: SAE consists of encoder (W_enc, b_enc), decoder (W_dec, b_dec), ReLU nonlinearity, L1 sparsity penalty; plus auxiliary components like LR probes for feature detection and ablation studies for causal verification
- Critical path: Train SAE → Identify first-letter latents via LR probe cosine similarity → Use k-sparse probing to detect feature splitting → Run ablation studies to detect absorption → Calculate absorption rate
- Design tradeoffs: Higher sparsity (lower L0) increases absorption rate but improves feature separation; wider SAEs increase feature splitting and absorption; choice between L1 vs TopK architectures affects absorption patterns
- Failure signatures: Low recall of SAE latents on tasks they appear to track; latents with high probe cosine similarity but poor classification performance; activation patterns showing latents failing to fire on obvious positive examples
- First 3 experiments:
  1. Train SAE on LLM activations and compute F1 score for first-letter classification using top SAE latents
  2. Run k-sparse probing to detect feature splitting and identify false negative tokens
  3. Perform ablation studies on false negative tokens to identify absorbing latents and calculate absorption rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can structured sparsity approaches (like group lasso or hierarchical sparse coding) effectively mitigate feature absorption in SAEs?
- Basis in paper: [explicit] The paper discusses potential solutions to feature absorption, including structured sparsity techniques such as group lasso [13] or hierarchical sparse coding [14].
- Why unresolved: While the paper suggests these approaches as promising directions, it does not provide experimental validation or theoretical analysis of their effectiveness against feature absorption.
- What evidence would resolve it: Empirical results showing reduced feature absorption rates when using structured sparsity methods compared to standard L1 loss SAEs, across multiple SAE architectures and datasets.

### Open Question 2
- Question: How does feature absorption impact the reliability of SAEs for high-stakes classification tasks beyond first-letter identification?
- Basis in paper: [inferred] The paper highlights that feature absorption suggests SAE latents may be unreliable classifiers, particularly for applications requiring confidence in latents fully tracking behaviors like bias or deceptive behavior.
- Why unresolved: The analysis focuses on a specific task (first-letter identification) and does not explore how absorption affects SAE reliability in other high-stakes scenarios or different model architectures.
- What evidence would resolve it: Comparative studies measuring SAE performance and absorption rates across diverse classification tasks and domains, including bias detection, deception identification, and safety-critical applications.

### Open Question 3
- Question: Can the absorption phenomenon be leveraged to recover hierarchical relationships between features in LLMs?
- Basis in paper: [explicit] The paper mentions that allowing absorption to occur and using it as a way to recover hierarchies between features in an LLM is a possible future direction.
- Why unresolved: The paper only briefly mentions this possibility without exploring the methodology or potential benefits of using absorption patterns to infer feature hierarchies.
- What evidence would resolve it: Development of algorithms that successfully extract hierarchical feature relationships from absorption patterns in SAEs, validated against known ground truth hierarchies or human-annotated feature relationships.

## Limitations

- The study focuses primarily on first-letter features, which may not generalize to other hierarchical feature types or different feature relationships
- The practical impact of feature absorption on downstream safety applications remains to be quantified
- The optimal strategy claim is based on a simplified toy model that doesn't capture all constraints of real SAEs

## Confidence

- **High Confidence**: The detection methodology using k-sparse probing, logistic regression probes, and ablation studies is technically sound and reproducible
- **Medium Confidence**: The claim that absorption represents a fundamental interpretability failure of SAEs
- **Low Confidence**: The assertion that feature absorption is the optimal strategy for SAEs in general

## Next Checks

1. Test absorption detection on non-hierarchical features and different feature types (e.g., syntactic, semantic, factual knowledge) to determine if absorption is specific to hierarchical relationships or a broader phenomenon
2. Compare absorption rates across different SAE architectures (L1 vs TopK, orthogonal SAEs) to validate whether the phenomenon is tied to specific regularization choices or optimization strategies
3. Implement Meta-SAEs or structured sparsity approaches on the same datasets to measure whether these solutions actually reduce absorption rates as hypothesized, providing empirical validation of the proposed solutions