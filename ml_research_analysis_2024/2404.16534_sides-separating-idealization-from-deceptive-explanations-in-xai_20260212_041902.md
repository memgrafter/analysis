---
ver: rpa2
title: 'SIDEs: Separating Idealization from Deceptive Explanations in xAI'
arxiv_id: '2404.16534'
source_url: https://arxiv.org/abs/2404.16534
tags:
- idealization
- methods
- explanations
- explanation
- science
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework called SIDEs for evaluating
  whether xAI methods engage in successful idealizations or deceptive explanations.
  Drawing on idealization practices in the natural sciences and philosophy of science,
  SIDEs provides a modular workflow with key questions for reflection and qualitative
  evaluation.
---

# SIDEs: Separating Idealization from Deceptive Explanations in xAI

## Quick Facts
- arXiv ID: 2404.16534
- Source URL: https://arxiv.org/abs/2404.16534
- Reference count: 40
- Primary result: Introduces SIDEs framework to distinguish successful idealizations from deceptive explanations in xAI methods.

## Executive Summary
This paper introduces SIDEs, a framework for evaluating whether xAI methods engage in successful idealizations or deceptive explanations. Drawing on philosophical idealization practices from natural sciences, SIDEs provides a modular workflow with key questions for reflection and qualitative evaluation. The framework aims to identify idealization failure due to misalignment with purpose or rule failure. Through qualitative analysis of leading feature importance and counterfactual explanation methods, the paper finds widespread idealization failure and suggests remedies like adopting alternative idealization practices or developing new ones tailored to xAI.

## Method Summary
The SIDEs framework applies philosophical idealization theory to xAI methods through a four-phase workflow: identifying the method's purpose (epistemic vs. ethical), aligning with appropriate idealization practices, evaluating adherence to ideals and rules (including entailment variants), and validating user-facing explanations. The method uses qualitative analysis to compare xAI methods against idealization standards, examining purpose alignment, idealization practices, ideals and rules, and user-facing explanations to identify potential misalignment or rule failure.

## Key Results
- SIDEs framework successfully identifies idealization failure in leading xAI methods due to misalignment between purpose and idealization practices
- Feature importance methods like LIME and SHAP exhibit rule failure when tested against minimalist idealization standards
- The framework provides a foundation for novel evaluation methods and helps researchers determine if distortions introduced by xAI methods are successful idealizations or problematic deceptive explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SIDEs framework treats xAI distortions as *intentional idealizations* rather than simply "unfaithful" explanations, enabling a normative evaluation of whether those distortions are justified.
- Mechanism: It maps xAI methods onto philosophical idealization practices (e.g., Minimalist Idealization), then checks whether the distortions satisfy the practice's ideals (simplicity, difference-making) and rules (causal entailment).
- Core assumption: xAI methods are performing a form of idealization, even if not explicitly recognized as such.
- Evidence anchors:
  - [abstract] "Drawing on idealization practices in the natural sciences and philosophy of science, SIDEs provides a modular workflow with key questions for reflection and qualitative evaluation."
  - [section 4.2.1] "Evaluating idealization practices phase involves two central aspects... a justification step that can ground the legitimacy of the idealization practice."
- Break condition: If the distortions in an xAI method cannot be mapped to any coherent idealization practice, or if the practice's ideals/rules cannot be meaningfully operationalized, the framework fails.

### Mechanism 2
- Claim: By distinguishing between *purpose* and *idealization practice*, SIDEs avoids conflating different stakeholder needs and ensures the method's distortions serve the intended aim.
- Mechanism: The workflow first identifies the xAI method's purpose (epistemic vs. ethical), then aligns the idealization practice and rules to that purpose, and finally validates user-facing explanations against it.
- Core assumption: Purposes in xAI are diverse and sometimes conflicting, so alignment is critical.
- Evidence anchors:
  - [section 4.1] "Fine-grained aims are more useful for idealization evaluation... Ethical purposes include fairness, providing users with recourse..."
  - [section 5] "Misalignment in CE: [recourse] explanations have a different target: the relationship between model features and the world."
- Break condition: If purposes are conflated or the alignment step is skipped, the idealization may fail even if it passes internal mathematical checks.

### Mechanism 3
- Claim: The framework's modularity allows incremental evaluation—purpose, practice, ideals/rules, and user-facing explanations—so failure can be localized and remedied.
- Mechanism: Each phase can be evaluated independently; misalignment in one phase can be corrected by adopting a different idealization practice or redefining rules.
- Core assumption: Idealization failure is often due to misalignment rather than fundamental impossibility.
- Evidence anchors:
  - [section 6] "If it is too difficult to satisfy one idealization practice... then idealization success can occur by creating alignment with a different idealization practice."
  - [section 4.3.1] "Evaluating idealizations in the ideals and rules phase requires validating whether specific rules embody target ideals, and analyzing trade-offs between different ways of operationalizing ideals."
- Break condition: If multiple phases fail simultaneously, or if no suitable idealization practice exists for the given xAI method, the modular fix becomes infeasible.

## Foundational Learning

- Concept: **Idealization in philosophy of science**
  - Why needed here: SIDEs is built on the premise that xAI distortions are intentional idealizations; understanding how idealization works in science is key to applying the framework.
  - Quick check question: What is the difference between an idealization and a simple approximation in scientific modeling?
- Concept: **Minimalist Idealization (MinI)**
  - Why needed here: The paper uses MinI as a working hypothesis for evaluating xAI; knowing its norms (simplicity, difference-making) is essential.
  - Quick check question: In MinI, what does it mean for a feature to be a "difference-maker"?
- Concept: **Causal entailment vs. probabilistic entailment**
  - Why needed here: The framework discusses different ways to operationalize rules (map, elimination, prob), so understanding these notions of entailment is critical.
  - Quick check question: How does a "causal entailment" rule differ from a "probabilistic" rule in evaluating xAI methods?

## Architecture Onboarding

- Component map:
  - Purpose phase → Purpose alignment module
  - Idealization practices phase → Practice identification module
  - Ideals and rules phase → Rule evaluation engine (with map/elimination/prob variants)
  - User-facing explanations phase → User study integration layer
- Critical path: Purpose → Practices → Ideals/Rules → User-facing explanations; failure at any step stops the workflow.
- Design tradeoffs:
  - Local vs. global evaluation: map rule is global but less aligned with purpose; elimination is local but more aligned.
  - Complexity vs. interpretability: more sophisticated entailment rules may be more accurate but harder to explain to stakeholders.
  - Single vs. multiple idealization practices: MMI allows trade-offs but increases cognitive load.
- Failure signatures:
  - Purpose misalignment: xAI method claims to explain but actually serves a different aim (e.g., trust vs. recourse).
  - Rule failure: adversarial manipulation or low accuracy indicates the idealization distorts difference-makers.
  - User confusion: explanations are technically correct but misleading or unintuitive.
- First 3 experiments:
  1. Apply SIDEs to LIME on a loan dataset: identify purpose, map to MinI, test map rule, run user study.
  2. Compare LIME and SHAP via elimination rule on the same dataset to see which better isolates difference-makers.
  3. Implement a multi-model setup (LIME + SHAP + CE) and evaluate via MMI principles to reduce adversarial vulnerability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What experimental tests can be developed to evaluate the elimination rule for MinI in xAI methods, specifically for feature importance methods like LIME and SHAP?
- Basis in paper: [explicit] The paper discusses the elimination rule and its potential for evaluating MinI in xAI, but notes that no experimental test for elimination exists.
- Why unresolved: Developing such tests requires innovative approaches to operationalize the elimination rule and create benchmarks for comparison.
- What evidence would resolve it: A concrete experimental test that can be applied to feature importance methods to determine if they satisfy the elimination rule.

### Open Question 2
- Question: How can user-facing explanations in xAI be designed to align with the purpose of idealizations, conveying the ideals and norms of the idealizations to diverse stakeholders?
- Basis in paper: [explicit] The paper highlights the need for user-facing explanations to align with purpose and avoid misleading users, but notes that this requires innovation and further research.
- Why unresolved: Designing user-facing explanations that effectively communicate the purpose and limitations of idealizations to diverse stakeholders is a complex challenge that requires user-centered approaches and empirical testing.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of user-facing explanations in aligning with purpose and conveying the ideals and norms of idealizations to diverse stakeholders.

### Open Question 3
- Question: What are the implications of adopting multiple-model idealization (MMI) as an alternative idealization practice for xAI, and how can it be effectively implemented to address idealization failure?
- Basis in paper: [explicit] The paper suggests MMI as a potential solution to idealization failure, but notes that it requires further research and development.
- Why unresolved: Implementing MMI in xAI requires addressing challenges such as aggregation rules, user experience, and the potential for cognitive overload.
- What evidence would resolve it: Case studies or empirical research demonstrating the effectiveness of MMI in addressing idealization failure and improving the overall performance of xAI methods.

## Limitations
- The framework remains largely theoretical with limited empirical validation against real-world xAI deployments.
- Analysis relies heavily on philosophical idealization theories without robust quantitative validation of translation to actual xAI performance metrics.
- The framework's modularity, while conceptually appealing, lacks formal operationalization of evaluation rules into measurable criteria.

## Confidence
- High confidence in the philosophical foundation and conceptual mapping between idealization practices and xAI methods
- Medium confidence in the framework's ability to diagnose misalignment and rule failures based on qualitative analysis
- Low confidence in the practical applicability of proposed remedies without further empirical validation

## Next Checks
1. Conduct controlled experiments comparing SIDEs evaluations with user perception studies to verify that framework-identified idealization failures correlate with actual user confusion or misinterpretation
2. Develop quantitative metrics for operationalizing idealization rules (particularly entailment tests) that can be consistently applied across multiple xAI methods
3. Test the framework's modularity by applying it to a new xAI method not discussed in the paper, documenting where the workflow succeeds or breaks down