---
ver: rpa2
title: 'How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating
  Neural Networks Generalize with Narrow Teachers'
arxiv_id: '2402.06323'
source_url: https://arxiv.org/abs/2402.06323
tags:
- teacher
- theorem
- sample
- probability
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that uniform random weights can induce non-uniform
  bias in neural networks, leading to generalization even with narrow teachers. The
  key idea is that a flat prior over parameters creates a rich prior over functions
  due to redundancy in the network structure.
---

# How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating Neural Networks Generalize with Narrow Teachers

## Quick Facts
- arXiv ID: 2402.06323
- Source URL: https://arxiv.org/abs/2402.06323
- Authors: Gon Buzaglo; Itamar Harel; Mor Shpigel Nacson; Alon Brutzkus; Nathan Srebro; Daniel Soudry
- Reference count: 40
- Primary result: Uniform random parameter sampling induces a non-uniform prior over functions, leading to generalization bounds that depend on teacher model complexity rather than student model complexity.

## Executive Summary
This paper proves that uniform random weights in neural networks can induce non-uniform bias toward simpler functions, enabling generalization even when training to zero loss with narrow teacher models. The key insight is that parameter redundancy in neural networks creates a rich prior over functions, biasing toward those that can be represented with fewer effective parameters. This mechanism explains why over-parameterized networks generalize well despite interpolating training data. The paper provides theoretical bounds showing that sample complexity depends on the teacher model complexity rather than the student model width, particularly for quantized networks.

## Method Summary
The paper analyzes generalization of randomly initialized neural networks that interpolate training data through a "guess-and-check" approach. Parameters are sampled uniformly from a quantized space, and the first network that achieves zero training loss is selected. The analysis derives bounds on the probability that this random interpolator generalizes well, showing it depends on the complexity of an underlying teacher network rather than the student network width. The framework extends to continuous weights under angular margin assumptions and considers architectures with scaling layers to improve bounds.

## Key Results
- Uniform random parameter sampling induces non-uniform bias toward simpler functions due to parameter redundancy
- Sample complexity for quantized networks is proportional to teacher complexity times quantization bits, with weak dependence on student width
- Angular margin assumption enables similar results for continuous weights
- Scaling layers can further improve generalization bounds by increasing parameter redundancy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Uniform random parameter sampling induces a non-uniform prior over functions due to parameter redundancy in neural networks.
- **Mechanism:** A flat prior over the parameter space creates a rich prior over the function space because many parameter configurations map to the same function. This redundancy biases the induced function prior toward simpler functions requiring fewer effective parameters.
- **Core assumption:** The neural network architecture has inherent redundancy where different parameter configurations can produce equivalent functions.
- **Evidence anchors:**
  - [abstract] "such a 'flat' prior over the NN parametrization induces a rich prior over the NN functions, due to the redundancy in the NN structure"
  - [section 4] "Although this prior is uniform over parametrizations, it is easy to see that it induces a highly non-uniform prior over predictors"
  - [corpus] Weak evidence - no direct citations in corpus about redundancy-induced bias
- **Break condition:** The architecture has minimal redundancy (e.g., a single-layer network with unique parameter-to-function mapping).

### Mechanism 2
- **Claim:** Guess-and-check sampling from a uniform parameter prior achieves sample complexity proportional to the teacher model complexity rather than student model complexity.
- **Mechanism:** When sampling parameters uniformly and accepting only interpolators, the probability of finding a teacher-equivalent function is inversely proportional to the number of redundant parameter configurations that represent that function. This creates a bias toward simpler teacher functions.
- **Core assumption:** There exists a narrow "teacher" network that generates the training labels.
- **Evidence anchors:**
  - [abstract] "typical NN interpolators generalize well if there exists an underlying narrow 'teacher NN' that agrees with the labels"
  - [section 5] "the sample complexity of learning by sampling random NNs is almost the same as that of using a much narrower teacher"
  - [corpus] Weak evidence - no direct citations about guess-and-check achieving teacher-proportional complexity
- **Break condition:** The teacher network requires nearly as many parameters as the student, eliminating the complexity advantage.

### Mechanism 3
- **Claim:** Scaled neuron architectures with zero scaling parameters provide more redundancy than vanilla architectures, improving generalization bounds.
- **Mechanism:** Zero scaling parameters can deactivate entire neurons, creating more parameter configurations that represent the same function. This additional redundancy strengthens the bias toward simpler functions.
- **Core assumption:** The activation function satisfies σ(0) = 0, allowing zero scaling to completely deactivate neurons.
- **Evidence anchors:**
  - [section 5.2] "for Scaled Fully Connected Networks, we have that ˆC SFC = C⋆ + PL l=1 dl ≪ ˆC FC ≪ C"
  - [section 5] "considering NNs in which each neuron is multiplied by a scaling parameter can significantly improve the bound"
  - [corpus] Weak evidence - no direct citations about scaling parameter redundancy
- **Break condition:** The activation function does not satisfy σ(0) = 0, preventing complete neuron deactivation.

## Foundational Learning

- **Concept:** Parameter redundancy in neural networks
  - Why needed here: The paper's central argument relies on the idea that uniform parameter priors induce non-uniform function priors due to redundancy
  - Quick check question: Can you provide an example where two different parameter configurations produce the same neural network function?

- **Concept:** PAC-Bayes generalization bounds
  - Why needed here: The paper relates its results to PAC-Bayes analysis and shows how its bounds are tighter for interpolators
  - Quick check question: How does the PAC-Bayes bound differ when applied to posterior samples versus expectations over the posterior?

- **Concept:** Quantization and numerical precision
  - Why needed here: The main theoretical results assume quantized weights, and the sample complexity bounds depend on the number of quantization levels
  - Quick check question: What happens to the sample complexity bound if we increase the number of quantization bits from 8 to 16?

## Architecture Onboarding

- **Component map:** Parameter space P(θ) with uniform prior -> Function space P(h|θ) induced by parameter-to-function mapping -> Teacher network h⋆ with complexity C⋆ -> Student network with width D and depth L -> Guess-and-check algorithm that samples until finding an interpolator

- **Critical path:**
  1. Define teacher network with width D⋆ and depth L
  2. Sample student network parameters uniformly from Q-quantized space
  3. Check if sampled network interpolates training data
  4. If yes, evaluate generalization on test data
  5. Repeat to estimate probability of finding teacher-equivalent function

- **Design tradeoffs:**
  - Wider student networks provide more redundancy but increase parameter space
  - More quantization levels improve representational capacity but increase sample complexity
  - Deeper architectures may provide more redundancy but complicate analysis
  - Scaled neuron architectures trade additional parameters for better generalization bounds

- **Failure signatures:**
  - Generalization error remains high even with zero training loss
  - Sample complexity grows with student width rather than teacher width
  - No teacher-equivalent function exists in the sampled parameter space
  - Activation function does not satisfy σ(0) = 0, breaking neuron deactivation mechanism

- **First 3 experiments:**
  1. Implement two-layer fully connected network with quantized weights, compare generalization of guess-and-check vs. gradient descent for various width ratios
  2. Add scaling parameters to neurons and measure improvement in sample complexity bounds
  3. Test continuous weight case with angular margin assumption on synthetic Gaussian data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the generalization bound for random interpolating NNs hold for deeper architectures beyond two layers?
- Basis in paper: [explicit] The paper derives results for fully connected networks with arbitrary depth L, but the continuous weight case is only proven for two-layer networks.
- Why unresolved: The proof for continuous weights relies on specific properties of two-layer networks that may not extend to deeper architectures.
- What evidence would resolve it: Extending the continuous weight analysis to deeper architectures and proving generalization bounds.

### Open Question 2
- Question: How does the choice of activation function affect the generalization bounds for random interpolating NNs?
- Basis in paper: [explicit] The paper assumes σ(0)=0 for quantized networks and uses the leaky ReLU for continuous weights, but doesn't explore other activation functions.
- Why unresolved: Different activation functions may have different impacts on the redundancy and thus the generalization properties of the network.
- What evidence would resolve it: Analyzing the generalization bounds for various activation functions and comparing their performance.

### Open Question 3
- Question: Can the results be extended to non-realizable settings where the teacher NN doesn't perfectly classify the training data?
- Basis in paper: [explicit] The paper focuses on realizable settings with a perfect teacher NN, but mentions the possibility of extending to non-realizable cases.
- Why unresolved: The analysis relies heavily on the existence of a perfect teacher NN, which may not hold in many practical scenarios.
- What evidence would resolve it: Developing generalization bounds for random interpolating NNs in non-realizable settings with an imperfect teacher or irreducible error.

## Limitations

- The guess-and-check sampling approach may be computationally infeasible for practical neural network sizes due to exponential search space
- Results heavily depend on the teacher network being sufficiently narrow relative to the student, with unclear threshold for "narrow enough"
- Extension to continuous weights requires angular margin assumptions that may not hold in practice
- Binary classification with ±1 labels assumption may not generalize to multi-class problems

## Confidence

**High confidence**: The core mechanism that uniform parameter priors induce non-uniform function priors due to parameter redundancy is well-established theoretically and supported by the mathematical proofs in sections 4 and 5.

**Medium confidence**: The claim that sample complexity scales with teacher complexity rather than student complexity holds under the stated assumptions, but practical applicability depends on unknown constants and the efficiency of finding interpolators in high-dimensional spaces.

**Low confidence**: The extension to continuous weights with angular margin assumptions (section 5.2) introduces additional complexity that may limit practical applicability, and the improvement from scaling layers, while theoretically demonstrated, requires empirical validation.

## Next Checks

1. **Redundancy measurement**: Quantify the actual parameter redundancy in different neural network architectures by counting distinct functions versus parameter configurations, validating the core assumption about function space bias.

2. **Computational feasibility**: Implement the guess-and-check algorithm for small networks to empirically measure the probability of finding teacher-equivalent interpolators and the actual computational cost, comparing theoretical predictions with practice.

3. **Generalization to continuous weights**: Test the angular margin assumption on synthetic datasets with varying degrees of label noise to determine when the continuous weight generalization bound breaks down in practice.