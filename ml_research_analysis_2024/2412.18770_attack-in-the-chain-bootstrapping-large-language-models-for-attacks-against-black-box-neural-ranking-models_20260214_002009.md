---
ver: rpa2
title: 'Attack-in-the-Chain: Bootstrapping Large Language Models for Attacks Against
  Black-box Neural Ranking Models'
arxiv_id: '2412.18770'
source_url: https://arxiv.org/abs/2412.18770
tags:
- target
- document
- documents
- ranking
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AttChain, a novel black-box adversarial attack
  framework that leverages large language models (LLMs) to generate imperceptible
  adversarial examples against neural ranking models (NRMs). The method uses chain-of-thought
  prompting to iteratively identify anchor documents and dynamically assign perturbations
  to improve the ranking of target documents.
---

# Attack-in-the-Chain: Bootstrapping Large Language Models for Attacks Against Black-box Neural Ranking Models

## Quick Facts
- arXiv ID: 2412.18770
- Source URL: https://arxiv.org/abs/2412.18770
- Authors: Yu-An Liu; Ruqing Zhang; Jiafeng Guo; Maarten de Rijke; Yixing Fan; Xueqi Cheng
- Reference count: 10
- Primary result: AttChain achieves up to 99.6% attack success rate against neural ranking models while maintaining high imperceptibility

## Executive Summary
This paper presents AttChain, a novel black-box adversarial attack framework that leverages large language models (LLMs) to generate imperceptible adversarial examples against neural ranking models (NRMs). The method uses chain-of-thought prompting to iteratively identify anchor documents and dynamically assign perturbations to improve the ranking of target documents. Experiments on MS MARCO and TREC DL19 datasets demonstrate that AttChain significantly outperforms state-of-the-art attack methods, achieving up to 99.6% attack success rate and 91.2 boosted ranks while maintaining high imperceptibility and low spamicity detection rates. The results highlight the vulnerability of NRMs to LLM-based attacks and suggest the need for robust countermeasures.

## Method Summary
AttChain is a black-box adversarial attack framework that leverages large language models to attack neural ranking models. The method uses chain-of-thought prompting to iteratively identify anchor documents (documents ranked higher than the target) and dynamically assign perturbations to improve the target document's ranking. The attack process involves Zipf-based filtering to select anchor documents, discrepancy-oriented perturbation assignment, and iterative reasoning rounds to progressively improve rankings. The framework is evaluated on MS MARCO and TREC DL19 datasets against three NRMs (BERT, PROP, RankLLM) using metrics including attack success rate, boosted ranks, naturalness scores, and spamicity detection.

## Key Results
- Achieves up to 99.6% attack success rate against neural ranking models
- Boosts target document rankings by up to 91.2 positions on average
- Maintains high imperceptibility with low grammar error counts and spamicity detection rates
- Outperforms state-of-the-art attack methods including traditional and generative approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Zipf-based filtering strategy efficiently identifies relevant anchor documents by prioritizing higher-ranked documents based on human click behavior patterns.
- Mechanism: Documents are filtered using Zipf distribution where rank inversely correlates with frequency of selection, ensuring higher-ranked documents have greater probability of being retained as anchor candidates.
- Core assumption: Higher-ranked documents in search results follow Zipf distribution patterns similar to human click behavior.
- Evidence anchors:
  - [abstract] "We design a Zipf-based filtering strategy, in which higher-ranked documents are more likely to be retained as candidate documents"
  - [section] "Following this, the candidates CA with m anchor documents are filtered as follows: CA = Zipf( L[: Rank(f, q, di−1)], m, s)"
  - [corpus] Weak evidence - corpus contains related papers on ranking attacks but none specifically validate Zipf-based filtering for anchor selection
- Break condition: When document rankings deviate significantly from Zipf distribution patterns or when lower-ranked documents contain more relevant information for ranking improvement

### Mechanism 2
- Claim: Dynamic perturbation assignment based on ranking discrepancy enables efficient resource allocation for attacks.
- Mechanism: The number of perturbed words is calculated proportionally to the ranking difference between target and anchor documents, with larger gaps receiving more perturbations.
- Core assumption: Ranking improvements require proportional effort relative to current position gap from target.
- Evidence anchors:
  - [abstract] "We then dynamically assign the number of perturbation words to each node and prompt LLMs to execute attacks"
  - [section] "|pj i | = ⌈Rank (f, q, di) − Rank(f, q, dj)⌉ Rank (f, q, d) ϵ"
  - [corpus] Weak evidence - corpus shows related work on adversarial attacks but lacks specific validation of discrepancy-based perturbation allocation
- Break condition: When perturbation budget becomes exhausted or when ranking improvements plateau despite additional perturbations

### Mechanism 3
- Claim: Chain-of-thought prompting enables iterative reasoning that progressively improves document rankings through multiple interaction rounds.
- Mechanism: Each reasoning round targets phased ranking improvements by identifying new anchor documents and generating perturbations, creating a ladder-climbing effect toward higher rankings.
- Core assumption: Iterative reasoning with anchor document guidance can systematically improve document rankings.
- Evidence anchors:
  - [abstract] "Our approach starts by identifying anchor documents with higher ranking positions than the target document as nodes in the reasoning chain"
  - [section] "The process of identifying nodes and updating nodes is done iteratively"
  - [corpus] Moderate evidence - corpus contains papers on chain-of-thought prompting but none specifically for adversarial ranking attacks
- Break condition: When maximum number of reasoning rounds is reached or when no significant ranking improvement occurs in consecutive rounds

## Foundational Learning

- Concept: Neural ranking models and their vulnerability to adversarial attacks
  - Why needed here: Understanding how NRMs work and their weaknesses is essential for designing effective attack strategies
  - Quick check question: What makes neural ranking models more vulnerable to attacks compared to traditional retrieval models?

- Concept: Chain-of-thought prompting and its application in reasoning tasks
  - Why needed here: CoT prompting is the core mechanism that enables iterative reasoning and document improvement
  - Quick check question: How does chain-of-thought prompting differ from standard prompting approaches in LLMs?

- Concept: Zipf distribution and its application in modeling human behavior
  - Why needed here: Zipf distribution is used to filter anchor documents based on click behavior patterns
  - Quick check question: Why is Zipf distribution appropriate for modeling document selection in search results?

## Architecture Onboarding

- Component map:
  Input layer (Query, target document, ranked list from NRM) -> Filtering module (Zipf-based anchor document selection) -> Prompting module (Chain-of-thought reasoning with LLMs) -> Perturbation module (Dynamic word assignment and document modification) -> Verification module (Attack effectiveness evaluation and node selection) -> Output layer (Final adversarial document and attack metrics)

- Critical path: Query → NRM ranking → Anchor filtering → CoT prompting → Perturbation generation → Effectiveness verification → Next round iteration

- Design tradeoffs:
  - Computational efficiency vs. attack effectiveness: More anchor documents increase effectiveness but also computational cost
  - Perturbation budget vs. imperceptibility: Larger perturbations improve rankings but may reduce naturalness
  - Number of reasoning rounds vs. attack success rate: More rounds generally improve success but increase query costs

- Failure signatures:
  - Low attack success rate despite multiple rounds
  - High grammar error counts in generated documents
  - Spam detection triggering on adversarial examples
  - Plateau in ranking improvements across iterations

- First 3 experiments:
  1. Baseline effectiveness test: Run attack on simple NRMs with varying perturbation budgets to establish baseline success rates
  2. Anchor selection comparison: Compare Zipf-based filtering against random and top-1 anchor selection strategies
  3. CoT vs. direct prompting: Evaluate attack success rates using chain-of-thought prompting versus direct prompting approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are open-source LLMs compared to proprietary models like GPT-3.5 for adversarial attacks on neural ranking models?
- Basis in paper: [explicit] The authors note that "Adopting closed-source LLMs, i.e., GPT3.5 improves the attack effectiveness but incurs a relatively large cost. For future work, we plan to use, and analyze, open-source LLMs to achieve attacks."
- Why unresolved: The paper primarily uses GPT-3.5 and Llama3 for their experiments, but does not provide a comprehensive comparison with open-source alternatives in terms of attack effectiveness and cost.
- What evidence would resolve it: Conducting experiments using various open-source LLMs (e.g., LLaMA, Mistral) and comparing their attack performance, cost, and resource requirements against proprietary models like GPT-3.5.

### Open Question 2
- Question: Can the proposed attack framework be adapted to target generative information retrieval models beyond neural ranking models?
- Basis in paper: [explicit] The authors mention "For future work, we plan to use, and analyze, open-source LLMs to achieve attacks" and "This raises concerns about the use of NRMs in the age of AI-generated content being exploited by search engine optimization (SEO)."
- Why unresolved: The paper focuses on attacking neural ranking models, but the rapid advancement of generative IR models (e.g., ChatGPT-based retrieval) presents new challenges and opportunities for adversarial attacks.
- What evidence would resolve it: Adapting the AttChain framework to target generative IR models and evaluating its effectiveness in terms of attack success rate, imperceptibility, and potential countermeasures.

### Open Question 3
- Question: What are the most effective techniques for detecting and mitigating adversarial examples generated by LLMs in information retrieval systems?
- Basis in paper: [explicit] The authors discuss mitigation analysis, noting that "it is worthwhile to investigate how to recognize adversarial examples through other techniques, such as LLM-generated text detection."
- Why unresolved: The paper identifies the difficulty in distinguishing adversarial examples from original documents using perplexity and semantic similarity, highlighting the need for more robust detection methods.
- What evidence would resolve it: Developing and evaluating novel detection techniques, such as analyzing linguistic patterns, metadata, or leveraging additional contextual information to identify LLM-generated adversarial examples.

## Limitations
- The method relies heavily on the quality of LLM responses, which may vary across different providers and model versions
- The Zipf-based filtering strategy assumes document rankings follow predictable patterns that may not hold for long-tail queries or specialized domains
- The iterative attack requires multiple LLM queries, raising concerns about computational cost and potential detection through query pattern analysis

## Confidence

**High Confidence Claims:**
- The AttChain framework successfully implements a black-box attack mechanism using chain-of-thought prompting and dynamic perturbation assignment
- The method achieves state-of-the-art attack success rates on standard benchmark datasets (MS MARCO and TREC DL19)
- The Zipf-based filtering strategy effectively identifies relevant anchor documents for the attack process

**Medium Confidence Claims:**
- The imperceptibility metrics (grammar checking, spamicity detection, PPL) sufficiently demonstrate that generated adversarial examples remain natural
- The dynamic perturbation assignment strategy optimally balances attack effectiveness with resource efficiency
- The iterative reasoning approach provides significant advantages over single-round attack methods

**Low Confidence Claims:**
- The method generalizes well to all types of neural ranking models beyond the three tested (BERT, PROP, RankLLM)
- The attack remains undetectable in real-world search engine environments with sophisticated anti-spam measures
- The computational overhead of multiple LLM queries is acceptable for practical deployment scenarios

## Next Checks

1. **Cross-Model Generalization Test**: Evaluate AttChain's effectiveness against additional neural ranking models, including transformer-based dense retrievers and hybrid systems, to validate the claimed robustness across different architectures.

2. **Real-World Deployment Assessment**: Implement the attack in a controlled environment mimicking commercial search engines with integrated spam detection systems to evaluate actual detectability and computational overhead beyond controlled benchmarks.

3. **Long-Tail Query Performance**: Test the framework on long-tail and ambiguous queries where document ranking patterns may deviate from Zipf distribution assumptions, assessing whether the attack remains effective when standard ranking assumptions break down.