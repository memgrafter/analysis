---
ver: rpa2
title: LLM Robustness Against Misinformation in Biomedical Question Answering
arxiv_id: '2410.21330'
source_url: https://arxiv.org/abs/2410.21330
tags:
- answer
- accuracy
- context
- question
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates LLM robustness against misinformation in
  biomedical question answering using retrieval-augmented generation. The research
  compares four LLMs (Gemma 2, GPT-4o-mini, Llama 3.1, Mixtral) under three scenarios:
  vanilla answers, perfect RAG, and prompt-injection attacks.'
---

# LLM Robustness Against Misinformation in Biomedical Question Answering

## Quick Facts
- arXiv ID: 2410.21330
- Source URL: https://arxiv.org/abs/2410.21330
- Authors: Alexander Bondarenko; Adrian Viehweger
- Reference count: 21
- Key outcome: This study evaluates LLM robustness against misinformation in biomedical question answering using retrieval-augmented generation. The research compares four LLMs (Gemma 2, GPT-4o-mini, Llama 3.1, Mixtral) under three scenarios: vanilla answers, perfect RAG, and prompt-injection attacks. Results show Llama 3.1 achieves highest accuracy (0.651 vanilla, 0.802 perfect RAG), while accuracy gaps between models diminish with correct context. The study reveals LLMs are vulnerable to adversarial attacks, with accuracy drops up to 0.48 for vanilla responses and 0.63 for perfect RAG. Llama emerges as the most effective adversary across target models. The findings highlight that LLM robustness varies significantly by evaluation metric, emphasizing the need for domain-specific testing and guardrails before deployment.

## Executive Summary
This paper evaluates the robustness of four LLMs against misinformation in biomedical question answering using retrieval-augmented generation (RAG). The study compares Gemma 2, GPT-4o-mini, Llama 3.1, and Mixtral under three scenarios: vanilla answers without context, perfect RAG with correct context, and prompt-injection attacks with adversarial context. Results show Llama 3.1 achieves the highest accuracy across scenarios, while accuracy gaps between models diminish when correct context is provided. The study reveals significant vulnerabilities to adversarial attacks, with accuracy drops up to 0.48 for vanilla responses and 0.63 for perfect RAG. Interestingly, Llama emerges as the most effective adversary across target models, highlighting the complexity of assessing LLM resilience to misinformation.

## Method Summary
The researchers evaluate LLM robustness using the BioASQ dataset containing 1350 yes-no and 1050 free-form biomedical questions. Four LLMs (Gemma 2, GPT-4o-mini, Llama 3.1, Mixtral) are tested under three scenarios: vanilla answers without external context, perfect RAG with correct context from PubMed snippets, and prompt-injection attacks where adversarial LLMs generate misinformation. Evaluation metrics include accuracy (exact match for yes-no questions, LLM judge for free-form), attack success rate (ASR), accuracy under attack, and accuracy drop. The study employs adversarial LLMs acting as both attackers and targets, with effectiveness measured by how much misinformation reduces target model accuracy.

## Key Results
- Llama 3.1 achieves highest accuracy in both vanilla (0.651) and perfect RAG (0.802) scenarios
- Accuracy gaps between models almost disappear with correct context, suggesting RAG can mitigate size-related effectiveness differences
- Accuracy drops of up to 0.48 for vanilla answers and 0.63 for perfect RAG demonstrate significant vulnerability to adversarial attacks
- Llama is the most effective adversary, causing the largest accuracy drops across target models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing correct context to LLMs significantly improves answer accuracy and reduces model size-related effectiveness differences.
- Mechanism: Retrieval-augmented generation (RAG) allows LLMs to override their parametric knowledge with relevant external information, effectively boosting accuracy and mitigating size-related disparities.
- Core assumption: The external context provided is both relevant and factually correct, enabling the LLM to use it effectively.
- Evidence anchors:
  - [abstract] "However, the accuracy gap between the models almost disappears with 'perfect' RAG, suggesting its potential to mitigate the LLM's size-related effectiveness differences."
  - [section] "This observation emphasizes the fact that effective RAG-based approaches... not only are able to boost the LLM question answer accuracy but also diminish the model's size-related effectiveness."
  - [corpus] Weak - corpus neighbors focus on RAG systems but don't directly address size-related effectiveness gaps.
- Break condition: If the provided context is irrelevant, incorrect, or if the LLM cannot effectively integrate the context into its reasoning.

### Mechanism 2
- Claim: LLMs are vulnerable to adversarial attacks where incorrect context leads to significantly reduced accuracy.
- Mechanism: Malicious actors can generate factually incorrect context that, when injected into the LLM's prompt, misleads the model into generating wrong answers.
- Core assumption: The adversarial context is plausible enough to deceive the LLM, and the model lacks sufficient safeguards against misinformation.
- Evidence anchors:
  - [abstract] "Our results show that Llama 3.1... achieves the highest accuracy in both vanilla... and 'perfect' RAG... However, the accuracy gap between the models almost disappears with 'perfect' RAG..."
  - [section] "Interestingly, Llama is shown to be the most effective adversary, causing accuracy drops of up to 0.48 for vanilla answers and 0.63 for 'perfect' RAG across target models."
  - [corpus] Missing - no direct corpus evidence on adversarial context effectiveness.
- Break condition: If the LLM has robust fact-checking mechanisms or if the adversarial context is easily identifiable as incorrect.

### Mechanism 3
- Claim: The robustness of LLMs against misinformation varies significantly depending on the evaluation metric used.
- Mechanism: Different metrics (e.g., accuracy under attack, accuracy drop, attack success rate) capture different aspects of robustness, leading to inconsistent rankings of model resilience.
- Core assumption: The chosen evaluation metrics are appropriate and comprehensive for assessing LLM robustness against misinformation.
- Evidence anchors:
  - [abstract] "Our analysis reveals that robustness rankings vary depending on the evaluation measure, highlighting the complexity of assessing LLM resilience to adversarial attacks."
  - [section] "Interestingly, when we calculate the ASR scores... we observe that Llama and GPT-4o-mini are more successful as an adversary for the yes-no questions and Gemma and Mixtral in the scenario of the free-form questions."
  - [corpus] Weak - corpus neighbors don't address the variability in robustness rankings based on evaluation metrics.
- Break condition: If a single, universally accepted metric for LLM robustness is established, or if the variability is due to methodological flaws in the study.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is the core technique used to provide LLMs with external context, which is central to both improving accuracy and studying the impact of misinformation.
  - Quick check question: What are the two main strengths of RAG-based approaches mentioned in the paper?

- Concept: Adversarial Attacks on LLMs
  - Why needed here: Understanding how adversarial attacks work is crucial for evaluating the robustness of LLMs against misinformation.
  - Quick check question: How does the paper define the role of the adversarial LLM (‚Ñ≥ùê¥) in the prompt-injection attacks?

- Concept: Evaluation Metrics for LLM Robustness
  - Why needed here: The paper uses multiple metrics (accuracy under attack, accuracy drop, attack success rate) to assess robustness, highlighting the complexity of the evaluation.
  - Quick check question: What is the difference between accuracy under attack and accuracy drop as used in the paper?

## Architecture Onboarding

- Component map:
  BioASQ dataset -> LLMs (Gemma 2, GPT-4o-mini, Llama 3.1, Mixtral) -> Prompt templates (vanilla, perfect RAG, adversarial) -> Evaluation metrics (accuracy, attack success rate, accuracy drop) -> Adversarial context generator (LLM acting as ‚Ñ≥ùê¥)

- Critical path:
  1. Load and preprocess the BioASQ dataset.
  2. For each LLM, generate answers in three scenarios: vanilla, perfect RAG, and adversarial.
  3. Calculate evaluation metrics for each scenario and compare results.
  4. Analyze the robustness of each LLM against adversarial attacks.

- Design tradeoffs:
  - Using snippets vs. full abstracts for context: snippets are shorter and may lead to higher accuracy but might miss important information.
  - Assigning different roles to adversarial LLMs: changing the role from "medical professional" to "game player" affects the willingness to generate incorrect context.

- Failure signatures:
  - High accuracy in vanilla responses but low accuracy in perfect RAG suggests issues with context integration.
  - Consistently low accuracy across all scenarios indicates fundamental limitations of the LLM.
  - High attack success rate across all target models suggests vulnerabilities to adversarial attacks.

- First 3 experiments:
  1. Generate vanilla answers for all questions using each LLM and calculate accuracy.
  2. Generate perfect RAG answers by providing correct context and compare accuracy with vanilla responses.
  3. Generate adversarial context using each LLM as ‚Ñ≥ùê¥ and evaluate the target LLMs' accuracy under attack.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific defense mechanisms could be developed to identify and filter out incorrect synthetic information in retrieval-augmented generation systems?
- Basis in paper: [explicit] The authors conclude that future research should focus on "developing defense mechanisms against misinformation (e.g., identifying incorrect synthetic information)" and highlight the vulnerability of LLMs to prompt-injection attacks.
- Why unresolved: The paper identifies the problem but doesn't propose or test specific technical solutions for detecting adversarial context during the retrieval phase.
- What evidence would resolve it: Implementation and evaluation of specific detection algorithms that can distinguish between legitimate and malicious context in RAG systems, with measurable reduction in attack success rates.

### Open Question 2
- Question: How do different model architectures (e.g., transformer variants, attention mechanisms) affect robustness against misinformation in biomedical question answering compared to simple parameter count scaling?
- Basis in paper: [inferred] The study focuses on comparing models by parameter count (9B vs 70B vs 47B) and shows Llama 3.1 (70B) performs best, but doesn't explore architectural differences within or across model families.
- Why unresolved: The research attributes performance differences primarily to model size but doesn't investigate whether architectural choices (like different attention mechanisms or training objectives) contribute more significantly to robustness.
- What evidence would resolve it: Controlled experiments comparing models with similar parameter counts but different architectures on the same misinformation robustness benchmarks.

### Open Question 3
- Question: What is the relationship between a model's effectiveness on vanilla questions and its robustness against adversarial attacks in biomedical contexts?
- Basis in paper: [explicit] The authors observe that "robustness rankings vary depending on the evaluation measure" and note inconsistent results across different metrics, but don't establish a clear relationship between general effectiveness and adversarial robustness.
- Why unresolved: The paper shows that different models excel under different evaluation metrics but doesn't determine whether more effective models are inherently more or less robust to attacks.
- What evidence would resolve it: Correlation analysis between vanilla accuracy and robustness metrics across a larger sample of biomedical question-answering models.

## Limitations

- Evaluation is constrained to the biomedical domain using a specific dataset (BioASQ), which may not reflect performance in other domains
- The adversarial attack methodology relies on LLMs generating misinformation, which may not represent all real-world attack vectors
- The study uses snippet-level context rather than full abstracts, potentially missing critical information that could affect both accuracy and vulnerability to misinformation

## Confidence

- Medium: Our confidence in the claimed robustness differences between models is **Medium**. While the study demonstrates clear performance variations under different scenarios, several limitations affect generalizability. The evaluation is constrained to the biomedical domain using a specific dataset (BioASQ), which may not reflect performance in other domains. The adversarial attack methodology, while innovative, relies on LLMs generating misinformation, which may not represent all real-world attack vectors. Additionally, the study uses snippet-level context rather than full abstracts, potentially missing critical information that could affect both accuracy and vulnerability to misinformation.

- High: The finding that "accuracy gap between models almost disappears with 'perfect' RAG" is supported by the experimental results (**High confidence**), but the mechanism underlying this convergence requires further investigation. We cannot determine whether this reflects genuine improvement in smaller models or whether larger models become less effective with perfect context.

## Next Checks

1. **Cross-domain validation**: Replicate the experiment using non-biomedical datasets (e.g., legal or financial domains) to assess whether the observed robustness patterns hold across different knowledge domains.

2. **Full-text context evaluation**: Compare results using full abstracts versus snippets to determine if context completeness affects both accuracy and vulnerability to adversarial attacks differently across model sizes.

3. **Human-generated adversarial context**: Replace LLM-generated adversarial context with human-generated misinformation to evaluate whether model vulnerabilities differ when facing human-crafted attacks versus AI-generated ones.