---
ver: rpa2
title: 'FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection
  with LLMs through Data Pruning'
arxiv_id: '2406.18297'
source_url: https://arxiv.org/abs/2406.18297
tags:
- data
- training
- prompt
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates the application of open-source large language
  models (LLMs) to detect check-worthy statements from political transcriptions, addressing
  the challenge of identifying claims that need fact-checking. Eight prominent open-source
  LLMs (Llama2, Llama3, Mistral, Mixtral, Phi3-Mini-4K, Falcon, and Gemma-7b) were
  fine-tuned using prompt engineering and a two-step data pruning approach to improve
  training efficiency.
---

# FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning

## Quick Facts
- **arXiv ID**: 2406.18297
- **Source URL**: https://arxiv.org/abs/2406.18297
- **Reference count**: 25
- **Primary result**: Llama2-7b fine-tuned with data pruning achieved 0.82 F1-score and top leaderboard ranking in check-worthy statement detection

## Executive Summary
This paper presents FactFinders' approach to the CheckThat! 2024 task 1, focusing on detecting check-worthy statements from political transcriptions using open-source large language models. The authors evaluated eight prominent open-source LLMs and employed a two-step data pruning strategy to improve training efficiency while maintaining competitive performance. By identifying informative sentences and applying condensed nearest neighbor sampling, they reduced training data to 44% of the original while achieving strong results. Llama2-7b emerged as the best-performing model with an F1-score of 0.82, securing first place on the official leaderboard.

## Method Summary
The FactFinders team fine-tuned eight open-source LLMs (Llama2, Llama3, Mistral, Mixtral, Phi3-Mini-4K, Falcon, and Gemma-7b) for check-worthy statement detection using prompt engineering techniques. They implemented a two-step data pruning approach: first identifying informative sentences from the training corpus, then applying condensed nearest neighbor sampling to balance the dataset and reduce training data volume to 44% of the original. This pruning strategy aimed to improve training efficiency while maintaining model performance. The fine-tuned models were evaluated on a test set and submitted to the CheckThat! 2024 leaderboard for official ranking.

## Key Results
- Llama2-7b achieved the highest F1-score of 0.82 on the test set
- The model ranked first in the CheckThat! 2024 task 1 leaderboard
- Data pruning reduced training data to 44% while maintaining competitive performance
- Significant reduction in fine-tuning time achieved without sacrificing accuracy

## Why This Works (Mechanism)
The approach works by leveraging the strong reasoning capabilities of LLMs while addressing their computational inefficiency through intelligent data reduction. The two-step pruning process identifies the most informative examples, ensuring the model learns from high-quality, representative samples rather than being overwhelmed by redundant or less valuable data. This targeted learning approach allows the model to focus on distinguishing patterns that differentiate check-worthy statements from non-check-worthy ones, while the prompt engineering guides the model's attention toward relevant features in political transcriptions.

## Foundational Learning
- **Large Language Model Fine-tuning**: Adapting pre-trained LLMs to specific downstream tasks through additional training on domain-relevant data; needed to specialize general language understanding for check-worthy statement detection; quick check: verify that base model understands political discourse
- **Data Pruning Techniques**: Methods for reducing dataset size while preserving informational value; essential for computational efficiency and reducing overfitting risk; quick check: confirm pruned dataset maintains class balance
- **Condensed Nearest Neighbor Sampling**: A data reduction technique that selects representative samples closest to decision boundaries; critical for maintaining model performance with reduced data; quick check: validate that pruned samples cover the full feature space
- **Prompt Engineering**: Designing input prompts to guide LLM behavior and elicit desired outputs; necessary for steering model focus toward relevant features; quick check: test prompts with few examples before full fine-tuning
- **F1-Score Evaluation**: Harmonic mean of precision and recall; important metric for imbalanced classification tasks; quick check: ensure test set reflects real-world distribution
- **Check-worthy Statement Detection**: Identifying claims that warrant fact-checking; fundamental task for automated fact-checking systems; quick check: verify annotation consistency across training data

## Architecture Onboarding
**Component Map**: Raw Text Corpus -> Data Pruning (Informative Selection + CNN Sampling) -> Fine-tuned LLM -> Prediction Output

**Critical Path**: Political Transcriptions → Pruning Pipeline → Fine-tuning Process → Model Inference → F1-score Evaluation

**Design Tradeoffs**: Reduced training data (44%) vs. computational efficiency gains; model selection (8 LLMs) vs. evaluation complexity; prompt engineering complexity vs. model performance improvement

**Failure Signatures**: Over-pruning leading to information loss; class imbalance in pruned dataset; prompt engineering that doesn't align with task requirements; insufficient fine-tuning epochs

**First Experiments**: (1) Compare F1-scores across all eight fine-tuned LLMs on validation set; (2) Test pruning ratio sensitivity by varying reduction percentages; (3) Evaluate prompt engineering variations on model performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation confined to political transcriptions, limiting generalizability to other fact-checking domains
- Pruning performance not compared against alternative data reduction strategies
- No quantification of trade-off between pruning aggressiveness and model performance

## Confidence
- **High** confidence in Llama2-7b fine-tuning effectiveness (0.82 F1-score, leaderboard #1)
- **Medium** confidence in data pruning benefits (well-described but lacks comparative analysis)
- **Low** confidence in generalizability claims (single domain evaluation only)

## Next Checks
- Replicate pruning and fine-tuning pipeline on non-political text domains (health, science) to assess cross-domain robustness
- Compare proposed two-step pruning method against alternative data reduction strategies (active learning, entropy-based sampling)
- Conduct ablation studies to quantify impact of pruning ratio on accuracy, training efficiency, and bias propagation