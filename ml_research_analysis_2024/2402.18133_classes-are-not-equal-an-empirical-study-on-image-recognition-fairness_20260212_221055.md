---
ver: rpa2
title: 'Classes Are Not Equal: An Empirical Study on Image Recognition Fairness'
arxiv_id: '2402.18133'
source_url: https://arxiv.org/abs/2402.18133
tags:
- class
- classes
- accuracy
- imagenet
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an empirical study on image recognition fairness,
  revealing that extreme class accuracy disparity exists across various datasets,
  network architectures, and model capacities. The authors identify that unfairness
  stems from problematic representation rather than classifier bias, and propose the
  concept of Model Prediction Bias to explain the phenomenon.
---

# Classes Are Not Equal: An Empirical Study on Image Recognition Fairness

## Quick Facts
- arXiv ID: 2402.18133
- Source URL: https://arxiv.org/abs/2402.18133
- Reference count: 40
- Extreme class accuracy disparity exists across datasets, architectures, and model capacities

## Executive Summary
This paper reveals a significant fairness issue in image recognition where certain classes consistently achieve much lower accuracy than others, even on balanced datasets like ImageNet. Through systematic experiments, the authors demonstrate that this unfairness stems from problematic representation rather than classifier bias. They identify that harder classes have more diverse and overlapping feature distributions, leading to increased confusion during optimization. The study introduces the concept of Model Prediction Bias to explain why these classes receive lower accuracy through optimization dynamics dominated by false positives.

## Method Summary
The study evaluates image recognition fairness across multiple datasets (CIFAR-100, ImageNet, fine-grained datasets) using various architectures (ResNets, Vision Transformers, CLIP models). Models are trained with standard data augmentation techniques and evaluated on per-class accuracy. The authors employ k-NN evaluation and ETF classifier testing to verify that unfairness originates from representation issues. They analyze feature distributions, prediction biases, and optimization dynamics to understand the root causes of class-level disparities.

## Key Results
- Best class achieves 100% accuracy while worst class obtains only 16% accuracy on ImageNet with ResNet-50
- Unfairness persists across different network architectures and model capacities
- Data augmentation and representation learning algorithms can improve fairness to some degree
- Harder classes exhibit higher feature variance and more overlap with other classes in representation space

## Why This Works (Mechanism)

### Mechanism 1
Performance unfairness arises primarily from problematic representation rather than classifier bias. Harder classes have more diverse and overlapping features, causing false positives to dominate the learning signal during optimization.

### Mechanism 2
Model Prediction Bias explains why harder classes receive lower accuracy. During training, models exhibit higher prediction bias toward harder classes, overwhelming true positives in gradient signals and leading to poor optimization.

### Mechanism 3
Data diversity imbalance is a fundamental cause of unfairness. Harder classes cover more complex scenarios with diverse feature distributions, creating confusion in optimization even when class frequencies are balanced.

## Foundational Learning

- Concept: Representation learning and feature space analysis
  - Why needed here: Understanding how models learn representations is crucial to diagnosing why certain classes perform poorly despite balanced data.
  - Quick check question: Can you explain the difference between representation quality and classifier performance, and why poor representation leads to unfairness?

- Concept: Optimization dynamics and gradient-based learning
  - Why needed here: The paper's explanation of unfairness relies on understanding how false positives dominate gradients during training, affecting hard classes disproportionately.
  - Quick check question: How does the dominance of false positives in the gradient signal lead to poor optimization for specific classes?

- Concept: Data diversity and class complexity
  - Why needed here: The paper argues that harder classes have more diverse feature distributions, which is central to understanding the root cause of unfairness.
  - Quick check question: Why would a class with more diverse feature distributions be harder to learn, even with balanced sample counts?

## Architecture Onboarding

- Component map: Data pipeline -> Model backbone (CNN/ViT) -> Representation learning module -> Classifier head -> Evaluation
- Critical path: 1) Data preprocessing and augmentation 2) Forward pass through backbone and classifier 3) Loss computation 4) Backward pass and parameter updates 5) Periodic evaluation of per-class performance
- Design tradeoffs: Larger models vs smaller models for fairness analysis, including/excluding representation learning techniques, choice of evaluation metrics
- Failure signatures: High variance in per-class accuracy despite balanced data, correlation between class difficulty and feature variance, prediction bias toward harder classes
- First 3 experiments: 1) Reproduce basic unfairness on ImageNet with ResNet-50 2) Test k-NN evaluation for representation issues 3) Apply CutMix augmentation and measure accuracy gap reduction

## Open Questions the Paper Calls Out

### Open Question 1
How does class co-occurrence in datasets contribute to model prediction bias and unfairness? The paper mentions class co-occurrence but lacks detailed analysis of its specific impact on model prediction bias.

### Open Question 2
Can data augmentation strategies be specifically designed to address data diversity imbalance and improve fairness? The paper shows data augmentation improves fairness but doesn't explore strategies targeting data diversity issues.

### Open Question 3
How do vision-language models like CLIP and diffusion models compare in terms of fairness issues across different datasets? The paper demonstrates fairness issues in these models but doesn't compare them across datasets.

### Open Question 4
What is the relationship between model capacity and fairness, and how does it interact with data augmentation and representation learning techniques? The paper discusses model capacity effects but not its interaction with other fairness-improving techniques.

## Limitations

- The paper focuses primarily on representation-based unfairness without fully exploring other potential causes like optimization algorithms or dataset curation
- Model Prediction Bias mechanism lacks sufficient theoretical grounding and corpus support
- The study doesn't investigate whether fairness issues persist across different training procedures and hyperparameter settings

## Confidence

- Representation-based unfairness: Medium
- Model Prediction Bias mechanism: Low
- Data diversity as root cause: Medium

## Next Checks

1. Conduct ablation studies varying model architectures (CNNs vs Transformers) and training procedures to test robustness of the unfairness phenomenon.

2. Perform detailed gradient analysis to quantify how prediction bias magnitudes affect learning signals for hard vs easy classes.

3. Systematically control for data diversity by creating synthetic datasets with varying feature distributions while keeping class frequencies constant.