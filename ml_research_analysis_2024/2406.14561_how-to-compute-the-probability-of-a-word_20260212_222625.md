---
ver: rpa2
title: How to Compute the Probability of a Word
arxiv_id: '2406.14561'
source_url: https://arxiv.org/abs/2406.14561
tags:
- word
- language
- sbow
- probability
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper derives the correct method for computing the probability
  of a word in context using subword-based language models. It shows that many prior
  studies using bow-marked tokenisers (e.g., GPT models) have been computing these
  probabilities incorrectly by ignoring the need to marginalize over possible word-endings.
---

# How to Compute the Probability of a Word

## Quick Facts
- arXiv ID: 2406.14561
- Source URL: https://arxiv.org/abs/2406.14561
- Reference count: 25
- Key outcome: Many prior studies using bow-marked tokenisers (e.g., GPT models) have been computing word probabilities incorrectly by ignoring the need to marginalize over possible word-endings.

## Executive Summary
This paper addresses a fundamental issue in computing word probabilities from subword-based language models. While subword tokenisation enables efficient handling of large vocabularies, it creates challenges for accurately computing word-level probabilities. The authors derive the correct mathematical framework for computing p(wt|w<t) depending on whether the tokeniser uses end-of-word (eow) or beginning-of-word (bow) marking conventions. They demonstrate that many recent linguistic studies using bow-marked tokenisers (like GPT models) have been computing these probabilities incorrectly, leading to potential biases in empirical analyses.

## Method Summary
The authors derive two theorems for correctly computing word probabilities: Theorem 1 for eow-marked tokenisers allows direct computation using the product of subword probabilities, while Theorem 2 for bow-marked tokenisers requires marginalization over all possible word-ending subwords (Bug Fix 1). The paper also identifies two additional corner cases: Bug Fix 2 for non-eow-marked final words, and Bug Fix 3 for non-bow-marked first words. The authors empirically validate their framework by applying the corrected probability computations to sentence comprehension and lexical optimization tasks, comparing results with the previously used buggy methods.

## Key Results
- Many linguistic studies using bow-marked tokenisers have been computing word probabilities incorrectly by not marginalizing over possible word-endings.
- The corrected probability computation method leads to statistically significant changes in measured outcomes in sentence comprehension and lexical optimization tasks.
- Despite the computational corrections, the overall conclusions of the studied hypotheses remain unchanged, though with improved precision.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokeniser choice (eow vs bow) fundamentally changes how to correctly compute word probabilities from subword probabilities.
- Mechanism: Eow-marking tokenisers allow instantaneous decoding where each subword ending marks a word boundary, enabling direct probability computation via eq. (17). Bow-marking tokenisers only provide near-instantaneous decoding, requiring marginalization over possible word-endings (Bug Fix 1).
- Core assumption: Tokenisers can be classified as either eow or bow marking, and this classification determines the mathematical structure of the probability computation.
- Evidence anchors:
  - [abstract] "This paper derives the correct methods for computing word probabilities, highlighting issues when relying on language models that use beginning-of-word (bow)-marking tokenisers, e.g., the GPT family."
  - [section] "When using an end-of-word (eow)-marking tokeniser, computing p(wt|w<t) is simple. However, when using a beginning-of-word (bow)-marking tokeniser, correctly computing this value is not as straightforward."
  - [corpus] Weak - no direct corpus evidence provided for this mechanism
- Break condition: If a tokeniser doesn't clearly fall into eow or bow categories (e.g., uses both markings or neither), the mechanism breaks down.

### Mechanism 2
- Claim: The probability computation bug in bow-marked tokenisers stems from treating word boundaries as deterministic when they're actually probabilistic.
- Mechanism: When computing p(w | w<t) for bow-marked tokenisers, one must marginalize over all possible word-ending subwords rather than assuming a single continuation. This is captured in Bug Fix 1: dividing by the sum of probabilities of all bow-marked subwords.
- Core assumption: Each bow-marked subword can potentially end a word, and the probability of the word depends on the likelihood of each possible ending.
- Evidence anchors:
  - [abstract] "Many recent linguistic studies have been incorrectly computing these values... we show that many recent linguistic studies have been incorrectly computing these values."
  - [section] "When using a beginning-of-word (bow)-marking tokeniser, correctly computing this value is not as straightforward... we do observe statistically significant differences between the measured quantities when using the correct vs. buggy methods"
  - [corpus] Weak - no direct corpus evidence provided for this mechanism
- Break condition: If the bow-marked subword vocabulary is empty or contains only one element, the marginalization becomes trivial or undefined.

### Mechanism 3
- Claim: Tokeniser-specific corner cases (non-eow-marked final words, non-bow-marked first words) require additional probability computation corrections.
- Mechanism: These corner cases break the standard tokeniser assumptions, requiring special handling: Bug Fix 2 for final words without eow markings, and Bug Fix 3 for first words without bow markings.
- Core assumption: Tokenisers sometimes deviate from their standard marking patterns for practical reasons (e.g., avoiding whitespace issues), and these deviations must be accounted for in probability computations.
- Evidence anchors:
  - [section] "Several eow-marking tokenisers do not decompose exactly as in eq. (15), but treat the final word in a sequence differently... This mechanism allows tokenisers to avoid implying the existence of a white space that does not exist"
  - [section] "Just as eow-marking tokenisers often treat final words differently, bow-marking tokenisers treat the first word in a sequence differently to handle white space appropriately"
  - [corpus] Weak - no direct corpus evidence provided for this mechanism
- Break condition: If tokenisers handle these corner cases differently than described (e.g., using different marking patterns), the corrections may need adjustment.

## Foundational Learning

- Concept: Autoregressive probability computation in language models
  - Why needed here: The paper relies on decomposing word probabilities into products of subword conditional probabilities, which requires understanding how autoregressive models compute probabilities
  - Quick check question: Given a subword sequence s = [s1, s2, s3], how would an autoregressive language model compute p(s)?

- Concept: Marginalization over possible continuations
  - Why needed here: The key correction for bow-marked tokenisers involves marginalizing over all possible word-ending subwords, which requires understanding marginalization in probability theory
  - Quick check question: If p(a) = 0.3, p(b) = 0.4, and p(c) = 0.3 are the probabilities of three possible word-ending subwords, what is the total probability of the word?

- Concept: Tokenisation and detokenisation functions
  - Why needed here: The paper's entire framework depends on understanding how tokenisers map between words and subwords, and how these mappings affect probability computations
  - Quick check question: If a tokeniser maps the word "probability" to the subword sequence ["_prob", "ability"], what would the detokenisation function return for this subword sequence?

## Architecture Onboarding

- Component map: Language model -> Tokeniser -> Probability computation module -> Evaluation framework
- Critical path:
  1. Tokenise input text into subwords
  2. Compute subword probabilities from language model
  3. Apply appropriate probability computation method based on tokeniser type
  4. Evaluate corrected probabilities against empirical data
- Design tradeoffs:
  - Eow vs bow tokenisers: Eow is simpler but may require more subwords; bow may be more efficient but requires complex probability corrections
  - Exact vs. approximate marginalization: Exact marginalization is computationally expensive but accurate; approximations may be faster but introduce errors
- Failure signatures:
  - Incorrect probability estimates leading to poor model fit
  - Unexpected correlations between word length and predicted probabilities
  - Inconsistent results across different tokeniser types
- First 3 experiments:
  1. Implement probability computation for both eow and bow tokenisers, verify against synthetic data
  2. Test corrected probability computations on existing psycholinguistics datasets, compare with buggy version
  3. Analyze sensitivity of results to different tokeniser corner cases (final words, first words)

## Open Questions the Paper Calls Out

- Question: How does the probability correction affect performance on downstream NLP tasks beyond the studied psycholinguistics and lexical efficiency analyses?
- Question: What is the exact computational overhead of implementing Bug Fix 1 in practice, and how does it scale with model size and vocabulary?
- Question: How do different tokenization schemes (beyond eow and bow marking) affect the probability computation, and what are the general principles for handling arbitrary tokenization?

## Limitations

- The theoretical framework assumes tokenisers can be cleanly classified as either eow or bow marking, which may not hold for all implementations.
- The empirical validation focuses on a relatively small set of reading time datasets and language models, limiting generalizability.
- The handling of tokeniser corner cases relies on specific tokeniser conventions that may vary across implementations.

## Confidence

**High Confidence**: The core mathematical derivations for computing word probabilities from subword probabilities (Theorems 1 and 2) are correct and follow from basic probability theory.

**Medium Confidence**: The practical impact of the corrections on linguistic conclusions is demonstrated, but the effect sizes are small and need further validation across diverse datasets.

**Low Confidence**: The handling of tokeniser corner cases (Bug Fixes 2 and 3) relies on specific tokeniser conventions that may not generalize across all implementations.

## Next Checks

1. **Cross-tokeniser validation**: Test the probability computation methods across multiple tokeniser implementations (GPT-2, Pythia, SentencePiece, WordPiece) to verify that the theoretical framework holds for different marking conventions and that Bug Fixes 1-3 correctly handle implementation-specific edge cases.

2. **Scale validation**: Apply the corrected probability computation methods to larger, more diverse datasets beyond the reading time corpora used in the paper, including web-scale text and multiple languages, to assess whether the observed statistical differences in hypothesis testing remain significant at scale.

3. **Implementation verification**: Create a comprehensive test suite with synthetic and real examples covering all tokeniser corner cases (non-eow-marked final words, non-bow-marked first words, punctuation boundaries, clitics, etc.) to verify that the implementation correctly computes probabilities in all scenarios and that the margin of error remains within acceptable bounds.