---
ver: rpa2
title: 'SuperCorrect: Advancing Small LLM Reasoning with Thought Template Distillation
  and Self-Correction'
arxiv_id: '2410.09008'
source_url: https://arxiv.org/abs/2410.09008
tags:
- step
- reasoning
- arxiv
- llms
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SuperCorrect, a two-stage framework to improve
  mathematical reasoning and self-correction abilities of small language models (LLMs).
  The method extracts hierarchical thought templates from a large teacher model and
  employs cross-model collaborative direct preference optimization (DPO) to teach
  students how to identify and correct reasoning errors.
---

# SuperCorrect: Advancing Small LLM Reasoning with Thought Template Distillation and Self-Correction

## Quick Facts
- arXiv ID: 2410.09008
- Source URL: https://arxiv.org/abs/2410.09008
- Reference count: 40
- 7B SuperCorrect model surpasses DeepSeekMath-7B by 7.8% and Qwen2.5-Math-7B by 15.1% on MATH/GSM8K benchmarks

## Executive Summary
This paper introduces SuperCorrect, a two-stage framework to improve mathematical reasoning and self-correction abilities of small language models (LLMs). The method extracts hierarchical thought templates from a large teacher model and employs cross-model collaborative direct preference optimization (DPO) to teach students how to identify and correct reasoning errors. The approach significantly outperforms previous methods, achieving new state-of-the-art performance among all 7B models on MATH and GSM8K benchmarks.

## Method Summary
SuperCorrect uses a two-stage training approach: (1) Hierarchical thought-based supervised fine-tuning (HSFT) where the student learns from hierarchical templates (high-level thoughts + detailed solutions) extracted from a teacher LLM, and (2) Cross-model collaborative DPO where the student learns to identify and correct its own reasoning errors by observing teacher corrections. The framework breaks down complex reasoning into abstract principles and concrete steps, then teaches error-driven learning through teacher-provided correction traces.

## Key Results
- 7B SuperCorrect achieves new SOTA performance among all 7B models on MATH/GSM8K benchmarks
- Outperforms DeepSeekMath-7B by 7.8% and Qwen2.5-Math-7B by 15.1%
- Demonstrates significant improvements in self-correction ability and reasoning stability

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical thought templates provide granular reasoning guidance by decomposing complex problems into abstract principles and concrete steps. This enables smaller models to learn both the "what" and "how" of reasoning. Break condition: If teacher reasoning cannot be meaningfully decomposed into hierarchical structures.

### Mechanism 2
Cross-model collaborative DPO enables learning from teacher corrections rather than just correct answers. Students learn to identify and correct their own reasoning errors by observing teacher corrections of specific error steps, creating error-driven learning signals. Break condition: If teacher cannot consistently identify/correct errors or student cannot generalize from corrections.

### Mechanism 3
Thought-level correction optimization is more effective than instance-level preference optimization for complex reasoning tasks. By focusing on correcting specific erroneous reasoning steps rather than entire solutions, models learn to detect and fix errors at the most granular level where mistakes typically occur. Break condition: If errors are distributed across multiple steps or step-level corrections don't generalize.

## Foundational Learning

- **Knowledge Distillation**
  - Why needed here: SuperCorrect uses teacher models to extract reasoning patterns and corrections that are then distilled into smaller student models
  - Quick check question: What is the difference between traditional knowledge distillation (answer-level) and SuperCorrect's approach (thought-level)?

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: SuperCorrect builds upon CoT methodology but extends it with hierarchical templates and error correction mechanisms
  - Quick check question: How does hierarchical thought templates differ from standard CoT prompting in terms of information granularity?

- **Preference Optimization (DPO)**
  - Why needed here: The cross-model collaborative DPO uses preference learning between corrected and uncorrected reasoning steps to train the student model
  - Quick check question: What makes cross-model DPO different from standard DPO in terms of optimization targets?

## Architecture Onboarding

- **Component map**: Teacher LLM → Hierarchical Template Extractor → Student LLM (SFT stage) → Error Detection Pipeline → Correction Trace Generator → Student LLM (Cross-DPO stage)
- **Critical path**: 1. Extract hierarchical templates from teacher model, 2. Train student on hierarchical templates via SFT, 3. Generate student errors on test data, 4. Teacher provides correction traces for errors, 5. Train student via cross-model DPO using correction traces
- **Design tradeoffs**: Hierarchical templates add complexity but provide more granular guidance; cross-model approach requires additional teacher inference but enables error-driven learning; step-level correction focuses on granular improvements but may miss multi-step error patterns
- **Failure signatures**: Student model fails to produce coherent hierarchical thoughts after SFT stage; cross-DPO training diverges due to poor quality correction traces; accuracy improvements plateau despite increasing training data
- **First 3 experiments**: 1. Compare single-stage SFT vs two-stage SuperCorrect on MATH benchmark, 2. Ablation study: Remove hierarchical templates vs remove cross-DPO to isolate contribution of each component, 3. Test different teacher models (o1-mini vs GPT-4o) for correction trace quality and impact on student performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the hierarchical thought template approach scale to domains beyond mathematics? The framework was only tested on mathematical problems, and while generalization potential is suggested, empirical validation on other reasoning domains is lacking.

### Open Question 2
What is the impact of teacher model selection on student model performance? The paper uses SOTA LLMs as teachers but doesn't systematically investigate how different teacher model capabilities affect student outcomes or explore whether mismatched capabilities might harm performance.

### Open Question 3
How does the cross-model collaborative DPO approach compare to alternative self-correction methods? While the paper establishes superiority over single-model self-correction, it doesn't explore other approaches like peer-learning among multiple students or reward model-based approaches.

## Limitations
- Effectiveness depends heavily on teacher model's ability to consistently decompose reasoning into meaningful hierarchical structures, with limited analysis of template quality across diverse problem types
- Assumes teacher corrections can be reliably generated and that students can effectively learn from these corrections, without addressing cases where teacher corrections might introduce new errors
- Performance improvements are evaluated primarily on mathematical reasoning benchmarks, leaving unclear whether mechanisms generalize to other reasoning domains

## Confidence

- **High confidence**: Two-stage framework structure (SFT followed by cross-DPO) is clearly defined and implemented; benchmark performance improvements over baseline models are well-documented and statistically significant
- **Medium confidence**: Hierarchical thought template extraction process and its contribution to reasoning quality, as the paper describes the mechanism but provides limited analysis of template diversity or effectiveness across different problem types
- **Low confidence**: Generalizability of cross-model collaborative DPO beyond mathematical reasoning tasks, as the method hasn't been tested on other domains where reasoning error patterns may differ significantly

## Next Checks

1. **Template Quality Analysis**: Conduct an ablation study measuring student performance when trained on: (a) full hierarchical templates, (b) only high-level thoughts, (c) only detailed solutions, and (d) standard CoT prompts to quantify the actual contribution of hierarchical structure.

2. **Teacher Model Robustness Test**: Systematically introduce controlled reasoning errors into teacher model outputs and measure how this affects student model performance after cross-DPO training to reveal sensitivity to teacher model quality and consistency.

3. **Domain Transfer Experiment**: Apply the SuperCorrect framework to a non-mathematical reasoning task (such as commonsense reasoning or logical inference) using the same methodology to test whether the hierarchical template and cross-correction mechanisms generalize beyond mathematics.