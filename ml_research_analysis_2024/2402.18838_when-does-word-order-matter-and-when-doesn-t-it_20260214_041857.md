---
ver: rpa2
title: When does word order matter and when doesn't it?
arxiv_id: '2402.18838'
source_url: https://arxiv.org/abs/2402.18838
tags:
- order
- word
- tasks
- sentences
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates when word order is essential versus redundant
  for language models in natural language understanding tasks. The authors propose
  that linguistic redundancy explains why models appear insensitive to word order
  changes: when word order and other linguistic cues (like grammatical agreement)
  provide overlapping information, models can successfully handle scrambled sentences.'
---

# When does word order matter and when doesn't it?

## Quick Facts
- arXiv ID: 2402.18838
- Source URL: https://arxiv.org/abs/2402.18838
- Authors: Xuanda Chen; Timothy O'Donnell; Siva Reddy
- Reference count: 10
- This study investigates when word order is essential versus redundant for language models in natural language understanding tasks.

## Executive Summary
This paper investigates when word order is essential versus redundant for language models in natural language understanding tasks. The authors propose that linguistic redundancy explains why models appear insensitive to word order changes: when word order and other linguistic cues (like grammatical agreement) provide overlapping information, models can successfully handle scrambled sentences. They quantify word order redundancy using mutual information between scrambled and unscrambled sentences, estimated through a variational approximation with a reordering model and pre-trained language model. Results show that word order redundancy varies across tasks, with some tasks showing consistent predictions regardless of PMI changes while others drop to near random performance when PMI decreases.

## Method Summary
The authors create scrambled sentence pairs from Wikipedia and train a T5-based reordering model to restore original word order. They estimate PMI using the reordering model and a pre-trained language model (T5) through a variational approximation method. For NLU tasks, they run RoBERTa on both scrambled and unscrambled inputs, calculate consistency scores, and fit a Bayesian mixed-effects logistic regression with PMI and sentence length as predictors. The method is tested across multiple NLU tasks including SST-2, MRPC, QQP, RTE, COPA, BoolQ, and WinoGrande.

## Key Results
- Word order redundancy varies significantly across NLU tasks, with some showing consistent predictions regardless of PMI changes
- For RTE, prediction consistency drops near random levels when PMI decreases, while SST-2 shows consistent predictions even with low PMI
- Some sentences with low negative PMI can be reordered into grammatically correct but semantically altered versions
- The reordering model achieves high accuracy on CFG-generated test sentences, validating the PMI estimation approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Word order becomes dispensable when it provides redundant information that can be inferred from other linguistic cues.
- Mechanism: The model leverages grammatical agreement, animacy, and other structural cues to infer correct word order even when the surface order is scrambled.
- Core assumption: Linguistic redundancy exists in natural language where multiple cues provide overlapping information about syntactic structure.
- Evidence anchors:
  - [abstract] "linguistic redundancy can explain this phenomenon, whereby word order and other linguistic cues such as case markers provide overlapping and thus redundant information"
  - [section] "In these instances, both word order and grammatical agreement offer overlapping and thus redundant information"
- Break condition: When linguistic cues are insufficient or ambiguous, word order becomes essential for correct interpretation.

### Mechanism 2
- Claim: Mutual information between scrambled and unscrambled sentences quantifies the redundancy of word order.
- Mechanism: Higher mutual information indicates that scrambled sentences retain enough information to recover original word order through linguistic cues.
- Core assumption: Mutual information provides a valid measure of word order informativeness regardless of model architecture or training procedure.
- Evidence anchors:
  - [abstract] "We quantify how informative word order is using mutual information (MI) between unscrambled and scrambled sentences"
  - [section] "Our hypothesis is that when the MI between scrambled and unscrambled sentences is high, word order is less crucial for LMs to solve NLU tasks"
- Break condition: When PMI estimation is inaccurate or when sentences have low negative PMI that can be reordered into grammatically correct but semantically altered versions.

### Mechanism 3
- Claim: Task-specific sensitivity to word order varies based on the degree of linguistic redundancy required for successful completion.
- Mechanism: Tasks like SST-2 show consistent predictions regardless of PMI changes because sentiment can be inferred from word content alone, while RTE shows inconsistent performance when PMI decreases because logical relationships depend on word order.
- Core assumption: Different NLU tasks require different levels of structural information for accurate prediction.
- Evidence anchors:
  - [abstract] "for some tasks, like SST-2, LMs' prediction is almost always consistent with the original one even if the Pointwise-MI (PMI) changes, while for others, like RTE, the consistency is near random when the PMI gets lower"
  - [section] "Our results showed significant differences among the tasks in both intercepts and slopes"
- Break condition: When task requirements change or when new tasks are introduced that have different structural dependencies.

## Foundational Learning

- Concept: Mutual Information and Pointwise Mutual Information
  - Why needed here: These metrics quantify the redundancy of word order by measuring how much information scrambled sentences contain about unscrambled sentences
  - Quick check question: If two random variables have high mutual information, what does this imply about their relationship?

- Concept: Bayesian Mixed-Effects Logistic Regression
  - Why needed here: This statistical framework models the relationship between word order redundancy (predictor) and prediction consistency (response) while accounting for task-specific variations
  - Quick check question: What is the advantage of using random intercepts and slopes in a regression model when analyzing data across multiple tasks?

- Concept: Variational Approximation for MI Estimation
  - Why needed here: Direct computation of mutual information is intractable, so variational methods provide a practical way to estimate PMI using a reordering model and language model
  - Quick check question: Why is the reordering model trained to minimize KL divergence when estimating mutual information?

## Architecture Onboarding

- Component map: Reordering Model (RM) -> Language Model (LM) -> Mutual Information Estimator -> Regression Model
- Critical path:
  1. Create scrambled sentence pairs from corpus
  2. Train reordering model on (scrambled, unscrambled) pairs
  3. Use RM and LM to estimate PMI for each sentence
  4. Run LMs on NLU tasks with both scrambled and unscrambled inputs
  5. Calculate consistency scores and build regression model
- Design tradeoffs:
  - Using T5 for both RM and LM provides consistency but may limit exploration of other architectures
  - Random scrambling vs structured scrambling affects the validity of PMI estimates
  - Sampling vs exhaustive computation of all possible scramblings balances accuracy and computational cost
- Failure signatures:
  - High variance in PMI estimates across different random seeds
  - Low reordering model accuracy on certain sentence types
  - Inconsistent results when varying the number of scrambled versions per sentence
  - Poor regression model fit indicating missing variables or incorrect assumptions
- First 3 experiments:
  1. Test reordering model accuracy on CFG datasets with different argument structures
  2. Verify PMI estimation by comparing against known cases of word order importance
  3. Run baseline regression without PMI to establish task difficulty levels before adding redundancy effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the redundancy effect vary when using different scrambling methods beyond unigram word-level scrambling?
- Basis in paper: [explicit] The authors note their method is "agnostic to representation and processing" but only test unigram scrambling
- Why unresolved: The study uses a single scrambling method (unigram word-level), leaving open whether results generalize to other scrambling approaches
- What evidence would resolve it: Comparative experiments using different scrambling methods (e.g., n-gram scrambling, syntactic scrambling) showing consistency or variation in the redundancy effect across methods

### Open Question 2
- Question: What specific linguistic features beyond grammatical agreement and animacy contribute to word order recoverability?
- Basis in paper: [explicit] The authors mention "other linguistic cues" but only test grammatical agreement and animacy in their CFG evaluation
- Why unresolved: The probe model evaluation only examined two linguistic features, suggesting other features may play important roles
- What evidence would resolve it: Systematic analysis identifying which linguistic features (e.g., semantic roles, discourse structure, collocations) most strongly predict reordering success

### Open Question 3
- Question: How do model architecture choices affect sensitivity to word order redundancy?
- Basis in paper: [inferred] The study uses RoBERTa and T5 models, but doesn't compare with architectures like recurrent models or models with explicit position encoding
- Why unresolved: The authors use specific model architectures without exploring how architectural differences impact word order sensitivity
- What evidence would resolve it: Direct comparison of word order sensitivity across different model architectures (e.g., BERT, LSTM, Transformer variants) showing how architectural choices influence the redundancy effect

### Open Question 4
- Question: What is the relationship between PMI values and semantic preservation in reordered sentences?
- Basis in paper: [explicit] The authors observe that low-PMI sentences can be reordered into "grammatically correct but semantically altered" versions
- Why unresolved: The study quantifies PMI but doesn't systematically measure semantic preservation in reordered sentences
- What evidence would resolve it: Correlation analysis between PMI scores and semantic similarity metrics (e.g., S-BERT, human judgments) showing how PMI predicts semantic preservation after reordering

## Limitations
- PMI estimation reliability depends on the quality of the reordering model and language model, with potential inaccuracies when sentences have complex syntactic structures
- Results are based on specific NLU tasks that may not represent the full spectrum of language understanding requirements
- Random scrambling may not capture all types of word order changes that occur in natural language use

## Confidence
- High confidence: The mechanism that word order becomes dispensable when redundant information exists in other linguistic cues is well-supported by both theoretical arguments and empirical results across multiple tasks
- Medium confidence: The PMI estimation methodology using variational approximation is theoretically sound but has practical limitations in accuracy that could affect the quantitative results
- Low confidence: The claim that PMI values directly correspond to word order redundancy in all linguistic contexts, as the estimation method may not capture all forms of linguistic redundancy

## Next Checks
1. Cross-validation with alternative PMI estimation methods: Compare the variational approximation PMI estimates against direct computation methods on simplified sentence structures to validate the accuracy of the mutual information estimates
2. Controlled scrambling experiments: Systematically vary scrambling patterns (random vs. syntactic vs. semantic scrambling) to determine how different types of word order changes affect PMI estimates and model performance
3. Transfer learning analysis: Test whether models trained on high PMI datasets can successfully transfer to low PMI tasks, providing evidence for the redundancy hypothesis and identifying task-specific requirements for word order information