---
ver: rpa2
title: Evaluating Text Classification Robustness to Part-of-Speech Adversarial Examples
arxiv_id: '2408.08374'
source_url: https://arxiv.org/abs/2408.08374
tags:
- adversarial
- text
- examples
- dataset
- reviews
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a pipeline to generate adversarial examples
  for CNN-based text classification by systematically removing words based on their
  part-of-speech (POS). The method first analyzes the impact of different POS categories
  (nouns, verbs, adjectives) on classification accuracy through controlled deletion
  experiments, then trains an adversarial neural network to learn which specific words
  cause misclassification.
---

# Evaluating Text Classification Robustness to Part-of-Speech Adversarial Examples

## Quick Facts
- arXiv ID: 2408.08374
- Source URL: https://arxiv.org/abs/2408.08374
- Reference count: 16
- Key result: CNN classifiers show up to 84.53% accuracy reduction when nouns, verbs, and adjectives are systematically removed from reviews

## Executive Summary
This paper introduces a pipeline to generate adversarial examples for CNN-based text classification by systematically removing words based on their part-of-speech (POS). The method first analyzes the impact of different POS categories on classification accuracy through controlled deletion experiments, then trains an adversarial neural network to learn which specific words cause misclassification. Finally, the model generates adversarial examples by removing high-impact POS tokens. Experiments across IMDB, Amazon, and Yelp datasets show that targeted deletion of verbs, nouns, and adjectives can reduce CNN classification accuracy by up to 84.53% on the Yelp dataset, demonstrating significant vulnerabilities in how CNNs process linguistic features.

## Method Summary
The framework uses a three-phase pipeline: (1) baseline CNN training and controlled POS deletion experiments to analyze impact on accuracy, (2) training an adversarial neural network that learns which POS deletions cause misclassification by comparing original and manipulated reviews, and (3) generating adversarial examples by removing high-impact POS tokens identified by the adversarial network. The approach systematically tests deletions at 1%, 5%, 10%, and 15% perturbation levels across different POS categories, with experiments conducted on IMDB, Amazon, and Yelp review datasets using GloVe embeddings.

## Key Results
- 1% perturbation of targeted POS tokens achieved the highest accuracy reduction (54% on IMDB, 84.53% on Yelp)
- Verbs, nouns, and adjectives were identified as the most impactful POS categories for misclassification
- Accuracy reduction varied significantly across datasets, with Yelp showing the highest vulnerability
- Increasing perturbation levels beyond 1% did not consistently improve attack effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CNN text classifiers exhibit a structural bias against certain POS tokens, particularly nouns, verbs, and adjectives, making them vulnerable to targeted deletion attacks.
- Mechanism: The CNN model processes text as a sequence of tokens embedded in dense vector space. Local convolutional filters learn n-gram features, but if critical POS categories (verbs, nouns, adjectives) are systematically removed, the semantic signal degrades, causing misclassification. The experiments show that deleting even 1% of targeted POS can reduce accuracy from 97% to 54% on IMDB.
- Core assumption: The model's decision boundaries are heavily influenced by the presence of content-bearing words (nouns, verbs, adjectives) rather than function words, and removing these words shifts the embedding representation outside the model's learned decision region.
- Evidence anchors:
  - [abstract] "Our experiments highlight a distinct bias in CNN algorithms against certain parts of speech tokens within review datasets."
  - [section] "Our findings reveal that among the successful random attacks against the classifier, the classifier was most vulnerable to the removal of nouns and verbs followed by pronouns and adjectives."
  - [corpus] Weak—no direct corpus citation for POS-specific bias; inference drawn from experimental deletion results.
- Break condition: If the CNN uses attention mechanisms or contextual embeddings that allow it to recover semantics from surrounding tokens, targeted POS deletion may fail to consistently reduce accuracy.

### Mechanism 2
- Claim: The adversarial neural network learns to identify which specific POS tokens are most critical for misclassification by comparing predictions on original vs. manipulated reviews.
- Mechanism: The two-branch neural network ingests both the original and manipulated review. By concatenating their feature representations and training with labels indicating successful deception, the model learns a mapping from word deletions to classification failure. This learned mapping is then used to generate adversarial examples by prioritizing deletions that maximize the likelihood of misclassification.
- Core assumption: The decision boundary learned by the CNN is locally smooth enough that removing a small set of high-impact tokens will consistently flip the classification outcome, and this relationship can be approximated by the adversarial network.
- Evidence anchors:
  - [section] "By doing so, we mathematically proved the relationship between deleted words and the overall context of the review."
  - [section] "A label of 1 indicates that the manipulated review can successfully deceive the classifier algorithm and that the deleted words were crucial to the decision."
  - [corpus] Weak—no direct citation; relationship is inferred from training setup.
- Break condition: If the CNN's decision boundary is highly non-linear or discontinuous, the adversarial network may fail to generalize deletions to new reviews.

### Mechanism 3
- Claim: The perturbation effectiveness varies non-linearly with the percentage of POS tokens removed, and small, targeted deletions can be more effective than larger, less targeted ones.
- Mechanism: The experiments show that removing a small percentage (1%) of the most critical POS tokens can yield a larger accuracy drop than removing 5–15% of tokens indiscriminately. This suggests that the CNN relies heavily on a small subset of informative words, and that finding this subset is more important than the sheer number of deletions.
- Core assumption: The CNN's learned features are sparse and concentrated on a small set of high-information tokens; therefore, removing these tokens has a disproportionate effect on classification.
- Evidence anchors:
  - [section] "Interestingly, the 1% perturbation, which deletes the least amount of words, was the most effective, reducing the accuracy to 54%, a 44.32% reduction in accuracy compared to the baseline."
  - [section] "Overall, increasing the number of keywords deleted did not have a consistently positive impact on the performance."
  - [corpus] Weak—evidence is experimental rather than cited from external corpus.
- Break condition: If the CNN uses dense, distributed representations that are resilient to small token deletions, larger perturbations may be required for consistent misclassification.

## Foundational Learning

- Concept: Part-of-Speech (POS) tagging and its role in semantic representation
  - Why needed here: The attack relies on systematically removing tokens by POS category; understanding which POS carry more semantic weight (nouns, verbs, adjectives) is critical to crafting effective adversarial examples.
  - Quick check question: Which POS categories in the experiments were found to have the highest impact on CNN classification accuracy when removed?

- Concept: Convolutional Neural Networks for text classification
  - Why needed here: The vulnerability being exploited is specific to how CNNs process sequences of embedded tokens using local filters; understanding this architecture is key to reasoning about why POS deletions are effective.
  - Quick check question: How do CNNs for text differ from RNNs in terms of how they capture local vs. sequential dependencies?

- Concept: Adversarial example generation and black-box attack models
  - Why needed here: The framework assumes no access to model internals and instead learns attack patterns through output observation; understanding this threat model is essential for replicating or extending the approach.
  - Quick check question: What distinguishes a black-box adversarial attack from a white-box one in terms of information available to the attacker?

## Architecture Onboarding

- Component map: Baseline CNN training -> POS deletion analysis -> Adversarial NN training -> Adversarial example generation -> Attack evaluation
- Critical path: Baseline CNN -> POS deletion analysis -> Adversarial NN training -> Adversarial example generation -> Attack evaluation
- Design tradeoffs:
  - Deleting more tokens vs. targeting high-impact tokens: experiments show small, targeted deletions can outperform larger, indiscriminate ones.
  - Generalization across datasets: effectiveness varies (e.g., IMDB vs. Yelp), suggesting dataset-specific vulnerabilities