---
ver: rpa2
title: A comprehensive study on fidelity metrics for XAI
arxiv_id: '2401.10640'
source_url: https://arxiv.org/abs/2401.10640
tags:
- fidelity
- metrics
- these
- proposed
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study introduced a novel methodology to objectively verify\
  \ fidelity metrics for eXplainable AI (XAI) by using transparent decision trees\
  \ as ground truth. It compared four existing fidelity metrics\u2014Region Perturbation,\
  \ Faithfulness Correlation, Faithfulness Estimate, and Infidelity\u2014across two\
  \ synthetic image datasets."
---

# A comprehensive study on fidelity metrics for XAI

## Quick Facts
- arXiv ID: 2401.10640
- Source URL: https://arxiv.org/abs/2401.10640
- Authors: Miquel Miró-Nicolau; Antoni Jaume-i-Capó; Gabriel Moyà-Alcover
- Reference count: 39
- Primary result: All tested fidelity metrics showed significant deviations from perfect fidelity (up to 30% error) when evaluated against transparent decision trees, especially in the presence of out-of-distribution samples.

## Executive Summary
This study introduces a novel methodology for objectively verifying fidelity metrics in eXplainable AI by using transparent decision trees as ground truth. The authors compared four existing fidelity metrics—Region Perturbation, Faithfulness Correlation, Faithfulness Estimate, and Infidelity—across two synthetic image datasets. Results showed that all metrics significantly deviated from perfect fidelity, with particularly poor performance on out-of-distribution samples. The study concludes that current fidelity metrics are unreliable for real-world scenarios and recommends the proposed methodology as a benchmark for developing more robust metrics.

## Method Summary
The study used two synthetic image datasets (AIXI-Shape and TXUXIv3) to train transparent decision tree models, which provide perfect explanations due to their interpretability. Local explanations were extracted from decision paths using feature importance based on impurity criteria. Four fidelity metrics were then applied to these explanations and compared against the ground truth. The methodology specifically tested metric performance on out-of-distribution samples using the TXUXIv3 dataset with non-uniform backgrounds.

## Key Results
- All four fidelity metrics showed significant deviations from perfect fidelity (up to 30% error)
- Region Perturbation metric indicated very low fidelity, while Faithfulness Correlation and Faithfulness Estimate showed higher but still poor results
- Fidelity metrics demonstrated poor reliability, especially when handling out-of-distribution samples
- The proposed methodology successfully identified weaknesses in current fidelity metrics that would not be apparent without ground truth explanations

## Why This Works (Mechanism)

### Mechanism 1
Decision trees can serve as ground truth for XAI explanation fidelity because their decision paths are fully transparent and interpretable. By using decision trees as the underlying model, the study can generate explanations with known perfect fidelity, where any deviation in fidelity metrics indicates a flaw in the metric itself rather than uncertainty in the explanation.

### Mechanism 2
Synthetic datasets with known ground truth allow controlled evaluation of fidelity metrics by eliminating confounding factors present in real data. The AIXI-Shape and TXUXIv3 datasets provide controlled environments where the relationship between features and labels is defined, enabling isolation of metric performance from model complexity issues.

### Mechanism 3
Comparing multiple fidelity metrics against a ground truth reveals their relative reliability and identifies which metrics are more robust to out-of-distribution samples. By applying four different fidelity metrics to the same transparent model explanations, the study can identify which metrics consistently produce results close to expected perfect fidelity.

## Foundational Learning

- Concept: Decision tree interpretability and transparency
  - Why needed here: Understanding how decision trees work is crucial for appreciating why they can serve as ground truth for explanation fidelity
  - Quick check question: What makes a decision tree "transparent" compared to a deep neural network?

- Concept: Synthetic data generation and its role in controlled experiments
  - Why needed here: The study relies on synthetic datasets to create a controlled environment for evaluating fidelity metrics without the noise of real-world data
  - Quick check question: Why might synthetic datasets be preferred over real datasets when testing the fundamental properties of evaluation metrics?

- Concept: Out-of-distribution (OOD) samples and their impact on model explanations
  - Why needed here: The study specifically examines how fidelity metrics perform when faced with OOD samples, which is a critical real-world challenge
  - Quick check question: How do out-of-distribution samples typically affect the reliability of explanation methods?

## Architecture Onboarding

- Component map: Synthetic datasets (AIXI-Shape and TXUXIv3) -> Decision Tree Training -> Explanation Generation -> Fidelity Metric Calculation -> Analysis and Comparison
- Critical path: Data → Decision Tree Training → Explanation Generation → Fidelity Metric Calculation → Analysis and Comparison
- Design tradeoffs: Using decision trees provides perfect ground truth but limits the complexity of patterns that can be captured. Synthetic datasets offer control but may not fully represent real-world challenges.
- Failure signatures: If fidelity metrics show high variance across different samples, if results differ significantly between the two synthetic datasets, or if metrics produce unexpected values for known perfect explanations.
- First 3 experiments:
  1. Train decision trees on AIXI-Shape dataset and calculate all four fidelity metrics to establish baseline performance
  2. Repeat the same process on TXUXIv3 dataset to test sensitivity to out-of-distribution samples
  3. Systematically vary decision tree hyperparameters to test whether metric performance is sensitive to model complexity

## Open Questions the Paper Calls Out

### Open Question 1
Can new fidelity metrics be developed that are robust to out-of-distribution (OOD) samples in real-world scenarios? The authors concluded that current fidelity metrics are unreliable for real scenarios and recommended the development of new metrics to address the problems detected.

### Open Question 2
How can the proposed methodology for verifying fidelity metrics be extended to evaluate other properties of XAI methods, such as stability or robustness? The authors suggest that their methodology can serve as a quality benchmark for future metric developments.

### Open Question 3
What are the specific characteristics of OOD samples that cause the most significant impact on the performance of fidelity metrics? While the study identified the presence of OOD samples as a problem, it did not delve into the specific characteristics that exacerbate the issue.

## Limitations

- Reliance on synthetic datasets may not fully capture the complexity and distribution shifts present in real-world data
- Decision trees as ground truth constrain the types of patterns that can be evaluated, limiting generalizability
- The study does not address whether metric failures are due to fundamental flaws or artifacts of the synthetic data generation process

## Confidence

- High confidence: The methodology for using decision trees as ground truth is well-founded and the experimental results are internally consistent
- Medium confidence: The conclusion that current fidelity metrics are unreliable, as this requires generalization beyond the synthetic datasets
- Low confidence: The recommendation that this methodology should be used as a universal benchmark, given the limited scope of the evaluation

## Next Checks

1. Replicate the study using real-world image datasets (e.g., ImageNet, CIFAR) with inherently interpretable models like shallow decision trees to verify if the fidelity metric failures persist
2. Test the same fidelity metrics on post-hoc explanations generated for black-box models (CNNs, transformers) to determine if the observed issues are specific to the synthetic setup
3. Evaluate whether alternative explanation methods (e.g., counterfactual explanations, decision rules) show different fidelity metric behavior compared to feature importance-based explanations