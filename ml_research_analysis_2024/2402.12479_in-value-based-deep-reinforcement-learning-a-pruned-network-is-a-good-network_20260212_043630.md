---
ver: rpa2
title: In value-based deep reinforcement learning, a pruned network is a good network
arxiv_id: '2402.12479'
source_url: https://arxiv.org/abs/2402.12479
tags:
- learning
- network
- reinforcement
- deep
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that gradual magnitude pruning can dramatically
  improve the performance of deep RL value-based agents, even as the base network
  size scales up. Pruning to high sparsity levels (e.g., 95%) consistently yields
  33-60% performance gains over dense networks across DQN, Rainbow, IQN, Munchausen-IQN,
  and offline CQL agents.
---

# In value-based deep reinforcement learning, a pruned network is a good network

## Quick Facts
- arXiv ID: 2402.12479
- Source URL: https://arxiv.org/abs/2402.12479
- Authors: Johan Obando-Ceron; Aaron Courville; Pablo Samuel Castro
- Reference count: 40
- This paper shows that gradual magnitude pruning can dramatically improve the performance of deep RL value-based agents, even as the base network size scales up.

## Executive Summary
This paper demonstrates that gradual magnitude pruning consistently improves performance of deep RL value-based agents by 33-60% across multiple algorithms including DQN, Rainbow, IQN, Munchausen-IQN, and offline CQL. The performance gains scale with network width and are more pronounced with ResNet architectures compared to CNNs. Notably, pruned agents show better robustness to long training and high replay ratios, suggesting improved parameter efficiency and learning stability.

## Method Summary
The authors apply gradual magnitude pruning to value-based deep RL agents during training, removing low-magnitude weights to create sparse networks. Pruning starts at 20% and ends at 80% of training, with sparsity levels ranging from 90% to 99%. The method is tested across Atari 100k benchmark, full Atari suite, and offline RL datasets using DQN, Rainbow, IQN, Munchausen-IQN, and CQL agents with ResNet architectures. Performance is measured using human-normalized interquantile mean (IQM) scores and compared against dense baselines.

## Key Results
- Pruning to high sparsity levels (e.g., 95%) consistently yields 33-60% performance gains over dense networks
- Performance scales with network width, with wider networks benefiting more from pruning
- Pruned agents are more robust to long training and high replay ratios
- Gains cannot be explained by normalization or plasticity injection alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning reduces network capacity but improves parameter efficiency, allowing agents to scale better.
- Mechanism: Gradual magnitude pruning removes low-magnitude weights, forcing the network to use remaining parameters more effectively. This reduces redundancy and improves gradient utilization.
- Core assumption: The remaining parameters after pruning are sufficient to represent the learned value function.
- Evidence anchors:
  - [abstract] "pruning to high sparsity levels (e.g., 95%) consistently yields 33-60% performance gains"
  - [section 4.2] "networks trained with this technique produce stronger agents than their dense counterparts"
  - [corpus] No direct evidence about scaling efficiency, but related work on sparse training exists.
- Break condition: If pruned network performance drops below dense baseline, or if sparsity reduces parameter expressiveness below task requirements.

### Mechanism 2
- Claim: Pruning reduces gradient correlation and interference, improving learning stability.
- Mechanism: By removing redundant connections, pruning decreases the colinearity of gradients, leading to more diverse gradient directions and less interference between parameters during updates.
- Core assumption: Gradient interference is a significant factor limiting deep RL performance.
- Evidence anchors:
  - [section 5.2] "gradients exhibit a notable colinearity, whereas this colinearity is dramatically reduced in the pruned networks"
  - [section 5] "improved performance, and increased plasticity, is often associated with weaker gradient correlation"
  - [corpus] Weak evidence - the corpus papers don't discuss gradient correlation in RL context.
- Break condition: If gradient covariance remains high after pruning, or if gradient diversity doesn't correlate with performance improvements.

### Mechanism 3
- Claim: Pruning acts as implicit regularization, reducing overfitting and improving generalization.
- Mechanism: By removing parameters, pruning reduces model capacity and prevents overfitting to early data (primacy bias) and specific game features.
- Core assumption: Deep RL networks are prone to overfitting and need regularization beyond standard techniques.
- Evidence anchors:
  - [section 5.1] "gradual magnitude pruning surpasses all the other regularization methods at all levels of scale"
  - [section 4.6] "pruned networks are better at maintaining stable performance when trained for longer"
  - [corpus] Weak evidence - the corpus papers don't directly address overfitting in RL.
- Break condition: If pruning leads to underfitting (performance degradation) or if standard regularization techniques match pruning performance.

## Foundational Learning

- Concept: Temporal Difference Learning
  - Why needed here: Value-based RL relies on TD learning to estimate Q-values from observed rewards and next-state values.
  - Quick check question: What is the Bellman equation that TD learning tries to satisfy?

- Concept: Experience Replay
  - Why needed here: The paper uses replay buffers to break correlation between consecutive samples and improve sample efficiency.
  - Quick check question: How does the replay ratio (gradient updates per environment step) affect learning efficiency?

- Concept: Network Pruning Techniques
  - Why needed here: The paper applies gradual magnitude pruning, which requires understanding of weight importance criteria.
  - Quick check question: What is the difference between magnitude-based pruning and other pruning criteria like Hessian-based?

## Architecture Onboarding

- Component map: Environment -> Replay buffer -> Pruned Q-network -> Loss computation -> Gradients with pruning mask -> Parameter updates -> Target network update

- Critical path:
  1. Environment interaction â†’ store in replay buffer
  2. Sample batch from replay buffer
  3. Forward pass through pruned network
  4. Compute TD loss and gradients
  5. Apply pruning mask to gradients
  6. Update network parameters
  7. Periodically update target network

- Design tradeoffs:
  - Sparsity vs performance: Higher sparsity reduces parameters but may hurt expressiveness
  - Pruning schedule: Earlier pruning may help exploration but risks instability
  - Network width: Wider networks benefit more from pruning but increase computational cost

- Failure signatures:
  - Performance degradation after pruning begins
  - Training instability or divergence
  - Low effective rank of network parameters
  - High gradient correlation in final layers

- First 3 experiments:
  1. Run DQN with ResNet width=1, compare dense vs 95% pruned performance
  2. Vary pruning schedule (start/end frames) to find optimal timing
  3. Test different sparsity levels (90%, 95%, 99%) to identify sweet spot

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does pruning improve performance more with ResNet architectures compared to CNN architectures?
- Basis in paper: [explicit] The paper states that pruning with ResNet architecture results in significant performance improvements (50-60%) while with CNN architecture it yields little to no improvement.
- Why unresolved: The paper mentions this observation but does not provide a detailed explanation for the architectural differences in pruning effectiveness.
- What evidence would resolve it: Systematic ablation studies comparing different architectural components (e.g., residual connections, normalization layers) to isolate what makes ResNet more amenable to pruning.

### Open Question 2
- Question: What is the optimal pruning schedule for deep RL agents, especially at high replay ratios?
- Basis in paper: [inferred] The paper notes that pruned networks show sharper performance decline at high replay ratios, suggesting the pruning schedule may need adjustment.
- Why unresolved: The current pruning schedule was adapted from supervised learning and may not be optimal for RL settings with different training dynamics.
- What evidence would resolve it: Experiments varying pruning schedules (start/end points, pruning rate) across different replay ratios and network architectures.

### Open Question 3
- Question: How does pruning affect exploration and exploitation balance in RL agents?
- Basis in paper: [inferred] The paper shows pruning improves performance across various agents but doesn't analyze its impact on the exploration-exploitation tradeoff.
- Why unresolved: While performance metrics are reported, the underlying behavioral changes in agent decision-making are not examined.
- What evidence would resolve it: Analysis of action distributions, entropy, and value function smoothness before and after pruning to understand behavioral changes.

## Limitations

- The pruning mechanism's benefits appear robust across multiple algorithms and domains, though the exact contribution of each mechanism (capacity reduction, gradient interference reduction, regularization) remains unclear.
- The work doesn't explore alternative pruning criteria beyond magnitude, leaving open questions about whether other methods might perform better.
- While results show pruning improves parameter efficiency, the computational overhead of sparse operations and potential hardware limitations are not discussed.

## Confidence

- High confidence: Performance improvements (33-60%) and scaling with network width
- Medium confidence: Gradient correlation reduction mechanism
- Medium confidence: Regularization benefits over standard techniques

## Next Checks

1. Conduct ablation studies testing different pruning criteria (e.g., Hessian-based, gradient-based) to determine if magnitude pruning is optimal for RL
2. Measure and compare training compute efficiency (FLOPs, wall-clock time) between dense and sparse networks to verify practical benefits
3. Test pruning on continuous control tasks and offline RL benchmarks to verify generalization beyond ALE games