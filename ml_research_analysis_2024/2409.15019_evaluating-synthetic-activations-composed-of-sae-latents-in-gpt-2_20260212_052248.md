---
ver: rpa2
title: Evaluating Synthetic Activations composed of SAE Latents in GPT-2
arxiv_id: '2409.15019'
source_url: https://arxiv.org/abs/2409.15019
tags:
- activations
- activation
- step
- perturbations
- latents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether synthetic activations composed
  of Sparse Auto-Encoder (SAE) latents can mimic the behavior of real model-generated
  activations in GPT-2. Previous work showed that perturbing real activations causes
  step-function-like changes in final-layer activations, with distinct activation
  plateaus and directional sensitivity.
---

# Evaluating Synthetic Activations composed of SAE Latents in GPT-2

## Quick Facts
- arXiv ID: 2409.15019
- Source URL: https://arxiv.org/abs/2409.15019
- Reference count: 16
- Primary result: Synthetic activations composed of SAE latents can mimic directional sensitivity but fail to reproduce activation plateaus characteristic of real activations in GPT-2

## Executive Summary
This study investigates whether synthetic activations composed of Sparse Auto-Encoder (SAE) latents can mimic the behavior of real model-generated activations in GPT-2. Previous work showed that perturbing real activations causes step-function-like changes in final-layer activations, with distinct activation plateaus and directional sensitivity. The authors construct synthetic activations using three methods: randomly selecting latents, matching sparsity distributions, and controlling for sparsity and cosine similarity relationships. They find that synthetic activations with controlled sparsity and cosine similarity closely resemble real activations in directional sensitivity experiments, unlike simpler approaches. However, synthetic activations fail to reproduce the activation plateaus characteristic of real activations, suggesting that real activations cannot be explained by a simple "bag of SAE latents" lacking internal structure. The results highlight the importance of geometric and statistical properties of SAE latents in capturing model computation.

## Method Summary
The study compares real model-generated activations with synthetic activations constructed from SAE latents. The method involves perturbing base activations by adding increments toward target directions, then analyzing L2 distance changes at Layer 11. Synthetic activations are created using three approaches: random SAE latent selection, sparsity-matched SAE latents, and sparsity + cosine similarity-matched SAE latents. The directional sensitivity is measured by finding the maximum slope of L2 distance versus perturbation step, while activation plateaus are identified by the step at which L2 distance crosses a threshold of 20. The authors use Kolmogorov-Smirnov statistics to compare distributions of max slope steps and activation plateau steps across real, random, and synthetic activation types.

## Key Results
- Synthetic activations with controlled sparsity and cosine similarity closely resemble real activations in directional sensitivity experiments
- All synthetic activation methods fail to reproduce activation plateaus characteristic of real activations
- Random selection of SAE latents performs worse than controlled methods in directional sensitivity
- The failure to capture activation plateaus suggests real activations require internal structure beyond simple compositions of SAE latents

## Why This Works (Mechanism)
The study's mechanism relies on geometric properties of SAE latents and their relationships. When sparsity and cosine similarity between latents are properly controlled, synthetic activations can approximate the directional sensitivity of real activations because they capture the relevant feature dimensions. However, the failure to reproduce activation plateaus indicates that real activations involve geometric structures or interactions between latents that simple composition methods cannot capture. This suggests that the "bag of latents" assumption is insufficient to explain real activation behavior, pointing to more complex internal structures or dependencies in how features combine within the model.

## Foundational Learning
**SAE Latents**: Learned sparse representations extracted by Sparse Auto-Encoders from model activations, capturing interpretable features. Why needed: These serve as the building blocks for synthetic activations. Quick check: Verify latents have high sparsity and capture interpretable features.

**Directional Sensitivity**: The sensitivity of model activations to perturbations in specific directions, measured by max slope of L2 distance changes. Why needed: Indicates how model behavior changes with input perturbations. Quick check: Real activations should show higher directional sensitivity than random noise.

**Activation Plateaus**: Step-function-like behavior where small perturbations cause large changes in final-layer activations at specific thresholds. Why needed: Characterizes the geometric structure of activation space. Quick check: Plot L2 distance vs perturbation steps to identify plateaus visually.

**Cosine Similarity**: Measures the angle between vectors, indicating feature relationships. Why needed: Controls for feature interactions in synthetic activations. Quick check: Compute pairwise cosine similarities between SAE latents.

**Kolmogorov-Smirnov Statistics**: Non-parametric test comparing distributions. Why needed: Quantifies differences between real and synthetic activation behaviors. Quick check: KS statistic > 0.1 indicates significant distribution differences.

**L2 Distance**: Euclidean distance in activation space. Why needed: Measures similarity between perturbed and target activations. Quick check: L2 distance should increase monotonically with perturbation steps.

## Architecture Onboarding

**Component Map**: GPT-2 model (layers 1 and 11) -> SAE latents (from Bloom 2024) -> Perturbation engine -> L2 distance calculator -> KS statistics analyzer

**Critical Path**: Base activation collection at Layer 1 → Synthetic activation construction → Perturbation toward synthetic activations → L2 distance measurement at Layer 11 → Directional sensitivity and plateau analysis

**Design Tradeoffs**: Using SAE latents assumes they capture all relevant features, which may not be true if SAE latents miss some model computation. The threshold of 20 for activation plateaus is arbitrary and could affect results. Random selection is computationally cheaper but less accurate than controlled methods.

**Failure Signatures**: Synthetic activations fail to reproduce directional sensitivity when sparsity and cosine similarity relationships are not properly controlled. All synthetic methods fail to reproduce activation plateaus, suggesting missing geometric properties of real activations.

**First Experiments**:
1. Verify SAE latents have high sparsity and capture interpretable features from GPT-2 activations
2. Test directional sensitivity of random synthetic activations to establish baseline behavior
3. Compare L2 distance trajectories for real vs synthetic activations to visualize plateau differences

## Open Questions the Paper Calls Out
None

## Limitations
- The study assumes SAE latents capture all relevant features, which may not be true if SAE latents miss some model computation
- The threshold of 20 for activation plateau detection is arbitrary and could affect results
- The study doesn't explore whether other composition methods (e.g., weighted combinations, hierarchical structures) might better approximate real activations

## Confidence
- Real activations cannot be explained by simple compositions of SAE latents: Medium
- Controlled sparsity and cosine similarity relationships are necessary for directional sensitivity: High
- The lack of activation plateaus in synthetic activations reveals missing geometric properties: Medium

## Next Checks
1. Test whether adjusting the activation plateau threshold (e.g., 10, 30, 40) changes the fundamental conclusions about synthetic activation behavior
2. Implement and evaluate weighted combinations of SAE latents as synthetic activations to test whether internal structure beyond sparsity and cosine similarity relationships is needed
3. Apply the same synthetic activation methodology to a different layer (e.g., Layer 6) to verify whether the geometric properties of SAE latents vary systematically across layers