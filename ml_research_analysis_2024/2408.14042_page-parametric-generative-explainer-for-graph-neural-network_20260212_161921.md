---
ver: rpa2
title: 'PAGE: Parametric Generative Explainer for Graph Neural Network'
arxiv_id: '2408.14042'
source_url: https://arxiv.org/abs/2408.14042
tags:
- graph
- explanations
- features
- causal
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PAGE introduces a parameterized generative framework for explaining
  Graph Neural Networks without prior knowledge of internal details. The method trains
  an auto-encoder to generate explanatory substructures while using a discriminator
  to capture causality between latent features and model outputs.
---

# PAGE: Parametric Generative Explainer for Graph Neural Network

## Quick Facts
- arXiv ID: 2408.14042
- Source URL: https://arxiv.org/abs/2408.14042
- Reference count: 36
- Method achieves higher fidelity and accuracy than baseline methods while significantly improving efficiency

## Executive Summary
PAGE introduces a parameterized generative framework for explaining Graph Neural Network predictions without requiring prior knowledge of the model's internal details. The method uses an auto-encoder to generate explanatory substructures while a discriminator captures causality between latent features and model outputs. This approach eliminates the need for perturbation or sampling processes common in previous methods, operating at the sample level rather than node or edge level for better scalability to large graphs.

## Method Summary
PAGE trains an auto-encoder to compress graph features into latent space while using a discriminator to learn causal relationships between these features and model predictions. The framework operates in two stages: first jointly training the autoencoder and discriminator, then freezing the discriminator to constrain encoder learning. The encoder maps input graphs to latent features, the discriminator learns to map causal latent features to predictions, and the decoder generates explanatory subgraphs from causal features. This eliminates perturbation/sampling processes while maintaining high fidelity and accuracy.

## Key Results
- Achieves higher fidelity scores than baseline methods including GNNExplainer, PGExplainer, Gem, and OrphicX
- Demonstrates significantly improved efficiency by eliminating perturbation and sampling processes
- Shows consistent performance across multiple metrics including fidelity, accuracy, and sparsity constraints on both synthetic and real-world datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dimensionality reduction in latent space simplifies extraction of causal features
- Mechanism: The encoder compresses input features into lower-dimensional latent space Z, making causal features more distinguishable from spurious features
- Core assumption: Information compression preserves relevant causal relationships while reducing noise
- Evidence anchors:
  - [abstract] "Due to the dimensionality reduction of features in the latent space of the autoencoder, it becomes easier to extract causal features leading to the model's output"
  - [section 3.1.3] "The features in the latent space compressed by the encoder are divided into two parts: causally relevant features... and non-causal features"
- Break condition: If encoder fails to preserve causal information during compression, the causal features cannot be extracted effectively

### Mechanism 2
- Claim: Discriminator trained on latent causal features captures global causal relationships
- Mechanism: Well-trained discriminator learns to map causal latent features to model predictions, providing a constraint for the encoder
- Core assumption: Discriminator can learn the mapping between causal features and predictions better than sampling-based methods
- Evidence anchors:
  - [section 3.2] "We introduce an additional discriminator to capture the causality between latent causal features and the model's output"
  - [section 3.3] "The motivation is to ensure that the auto-encoder can completely rejuvenate the original graph structure and train the discriminator to learn the causal attribution from causal features to the predicted outcome"
- Break condition: If discriminator cannot learn the causal mapping accurately, it cannot provide useful constraints for the encoder

### Mechanism 3
- Claim: Two-stage training eliminates need for perturbation/sampling processes
- Mechanism: First stage trains both autoencoder and discriminator together; second stage freezes discriminator to constrain encoder learning
- Core assumption: Well-trained discriminator can effectively guide encoder to learn better causal features without requiring perturbations
- Evidence anchors:
  - [section 3.2] "The training process consists of two stages: In the first stage, autoencoder and discriminator are trained together... In the second stage, the parameters of the discriminator are frozen"
  - [section 4.7] "Compared to these methods, our approach eliminates any perturbation or sampling processes"
- Break condition: If two-stage training fails to converge properly, the discriminator may not provide effective guidance

## Foundational Learning

- Graph Neural Networks
  - Why needed here: Understanding how GNNs process graph-structured data is essential for interpreting their predictions
  - Quick check question: How does message passing work in a GCN layer?

- Autoencoders and Variational Autoencoders
  - Why needed here: PAGE uses autoencoders to compress graph features and generate explanations
  - Quick check question: What is the key difference between GAE and VGAE in terms of latent space representation?

- Information Theory and Mutual Information
  - Why needed here: The optimization objective maximizes mutual information between latent features and model outputs
  - Quick check question: How does mutual information relate to the concept of causality in this context?

## Architecture Onboarding

- Component map:
  - Input graph → Encoder (3-layer GCN) → Latent features Z
  - Latent features → Discriminator (2-layer MLP) → Prediction
  - Causal features Zc → Decoder (MLP + inner product) → Explanatory subgraph

- Critical path:
  1. Input graph → Encoder → Latent features Z
  2. Latent features → Discriminator → Prediction
  3. Causal features Zc → Decoder → Explanatory subgraph

- Design tradeoffs:
  - Using VGAE vs GAE: VGAE provides probabilistic latent representations but requires more computation
  - Fixed sparsity γ: Balances explanation comprehensiveness vs conciseness
  - Two-stage training: Improves efficiency but requires careful hyperparameter tuning

- Failure signatures:
  - Poor fidelity scores indicate discriminator is not capturing causal relationships correctly
  - Low accuracy on synthetic datasets suggests encoder is not extracting meaningful causal features
  - High training time compared to baselines indicates inefficiency in training process

- First 3 experiments:
  1. Test PAGE on BA-Shapes dataset with varying sparsity levels (γ = 0.3, 0.5, 0.7) to observe trade-off between fidelity and sparsity
  2. Compare fidelity scores when using GAE vs VGAE as base autoencoder to validate choice of VGAE
  3. Measure training and inference times on Mutagenicity dataset to confirm efficiency claims against baselines

## Open Questions the Paper Calls Out

- **Open Question 1**: How would PAGE perform on heterogeneous graphs with multiple edge types or node attributes?
  - Basis in paper: [inferred] The paper focuses on homogeneous graphs and mentions PAGE can work with node attributes but doesn't test heterogeneous graphs
  - Why unresolved: The experimental evaluation only covers homogeneous graphs with single edge types
  - What evidence would resolve it: Testing PAGE on benchmark heterogeneous graph datasets like OGB-Het or IMDB-Het would demonstrate its generalization capability

- **Open Question 2**: What is the impact of using different autoencoder architectures beyond the GCN-based encoder-decoder?
  - Basis in paper: [explicit] The paper states "we only explored autoencoders as interpreters" and suggests "An avenue for potential improvement could involve using more powerful generative models"
  - Why unresolved: The study is limited to GCN-based autoencoders without comparison to alternative generative architectures
  - What evidence would resolve it: Comparative experiments using transformer-based or graph attention network-based autoencoders would quantify the architectural impact

- **Open Question 3**: How does PAGE's performance scale with extremely large graphs containing millions of nodes and edges?
  - Basis in paper: [inferred] While PAGE claims efficiency improvements, the experiments are conducted on relatively small datasets (max ~4337 graphs with avg 32 nodes)
  - Why unresolved: The evaluation doesn't include large-scale graph datasets that would stress-test scalability claims
  - What evidence would resolve it: Performance testing on massive graph datasets like OGBN-Papers100M or social network graphs would validate scalability claims

## Limitations
- The framework's effectiveness depends heavily on the quality of the autoencoder's dimensionality reduction, which may fail to preserve critical causal relationships in complex graph structures
- The two-stage training procedure requires careful hyperparameter tuning, particularly the balance between fidelity and sparsity constraints through parameter γ
- The method operates at sample level rather than node/edge level, limiting its applicability for tasks requiring fine-grained explanations

## Confidence
- **High Confidence**: The efficiency claims compared to perturbation-based methods are well-supported by the elimination of sampling processes
- **Medium Confidence**: The fidelity improvements over baselines are demonstrated but could benefit from more diverse dataset testing
- **Medium Confidence**: The mechanism of causal feature extraction through dimensionality reduction is theoretically sound but requires further validation on more complex real-world scenarios

## Next Checks
1. Test PAGE on additional real-world datasets with varying graph complexities to validate generalization beyond the current four datasets
2. Conduct ablation studies removing the discriminator component to quantify its specific contribution to explanation quality
3. Implement stress tests with adversarial graph structures to evaluate robustness of the causal feature extraction mechanism