---
ver: rpa2
title: 'Apollo: A Lightweight Multilingual Medical LLM towards Democratizing Medical
  AI to 6B People'
arxiv_id: '2403.03640'
source_url: https://arxiv.org/abs/2403.03640
tags:
- medical
- data
- arxiv
- training
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Apollo, a suite of lightweight multilingual
  medical LLMs trained on ApolloCorpora, a high-quality dataset covering six major
  languages (English, Chinese, Hindi, Spanish, French, and Arabic) with a total population
  of 6.1 billion. The models, ranging from 0.5B to 7B parameters, achieve state-of-the-art
  performance among models of equivalent size, with Apollo-7B outperforming models
  up to 70B parameters.
---

# Apollo: A Lightweight Multilingual Medical LLM towards Democratizing Medical AI to 6B People

## Quick Facts
- arXiv ID: 2403.03640
- Source URL: https://arxiv.org/abs/2403.03640
- Reference count: 35
- Models achieve state-of-the-art performance among models of equivalent size, with Apollo-7B outperforming models up to 70B parameters

## Executive Summary
This work introduces Apollo, a suite of lightweight multilingual medical LLMs trained on ApolloCorpora, a high-quality dataset covering six major languages (English, Chinese, Hindi, Spanish, French, and Arabic) with a total population of 6.1 billion. The models, ranging from 0.5B to 7B parameters, achieve state-of-the-art performance among models of equivalent size, with Apollo-7B outperforming models up to 70B parameters. The training approach involves rewriting pre-training corpora into QA pairs and using adaptive sampling for smooth transitions between pretraining and instruction tuning. Additionally, Apollo models can enhance larger general LLMs' multilingual medical capabilities through proxy-tuning without direct fine-tuning, addressing privacy concerns.

## Method Summary
Apollo models are trained using a Mix Training approach that combines continued pretraining on medical corpora rewritten as QA pairs with adaptive sampling and instruction tuning. The process uses priority sampling with Pt(x) = π(x)/Σy∈D−St π(y) where π(x)=16 for pretraining data and π(x)=2 for instruction tuning data. Models are trained for 1 epoch on pretraining corpus and 2 epochs on instruction tuning corpus with batch size 256, learning rate 1e-5 (or 1e-4 for most models), and cosine scheduler warmup rate 0.03. The ApolloCorpora dataset contains 2.5B tokens across medical books, papers, encyclopedias, dialogues, exams, guidelines, web content, general instruction data, math, and code.

## Key Results
- Apollo-7B outperforms models up to 70B parameters on multilingual medical benchmarks
- Multilingual training improves performance compared to monolingual variants across all six languages
- Proxy-tuning enables larger models to gain medical capabilities without direct training on sensitive medical data
- Models achieve state-of-the-art performance among models of equivalent size

## Why This Works (Mechanism)

### Mechanism 1
Rewriting pre-training corpora into QA pairs preserves medical knowledge while improving instruction-following capability. Converting raw medical text into structured question-answer pairs enables the model to learn both domain knowledge and instruction format simultaneously during pretraining. Core assumption: Structured QA pairs maintain semantic equivalence with original text while providing explicit instruction-response patterns. Evidence: Inspired by (Cheng et al., 2023; Chen et al., 2023a), we use ChatGPT 2 to generate questions and answers for a certain paragraph.

### Mechanism 2
Multilingual training improves overall medical LLM performance compared to monolingual training. Joint training on medical data from multiple languages provides complementary medical knowledge that enhances cross-lingual understanding and generalization. Core assumption: Medical knowledge is largely language-neutral, so training on multiple languages provides additive benefits rather than conflicts. Evidence: The results show that the gap between open source and closed source is narrowing. The Apollo series models achieve the best performance of models of the same size.

### Mechanism 3
Proxy-tuning enables larger models to gain multilingual medical capabilities without direct training on sensitive medical data. Small, specialized models generate logit offsets that can be applied to larger base models, transferring medical knowledge while avoiding direct exposure of sensitive training data. Core assumption: Logit offsets from specialized models can effectively steer larger models' distributions without catastrophic forgetting or distribution mismatch. Evidence: These lite models could be used to improve the multi-lingual medical capabilities of larger models without fine-tuning in a proxy-tuning fashion.

## Foundational Learning

- **Multilingual data complementarity in medical knowledge**: Understanding how different languages' medical knowledge can enhance rather than conflict with each other is crucial for effective multilingual model training. Quick check: If a medical fact is expressed identically in two languages, what happens when both versions are used in training?

- **Instruction-following capability preservation during domain adaptation**: Medical LLMs must maintain the ability to follow instructions while acquiring specialized medical knowledge, which is why QA pair conversion is used. Quick check: How does converting medical text to QA format help preserve instruction-following ability compared to traditional continued pretraining?

- **Proxy-tuning mechanism for knowledge transfer**: Understanding how logit offsets can transfer specialized knowledge to larger models without direct training is key to the privacy-preserving aspect of this work. Quick check: What mathematical operation allows logit offsets from a small model to influence a larger model's predictions?

## Architecture Onboarding

- **Component map**: Data collection -> QA pair conversion -> Mix Training with adaptive sampling -> Model evaluation -> Proxy-tuning application
- **Critical path**: The critical path for creating a functional multilingual medical LLM is: data collection → QA pair conversion → mix training with adaptive sampling → model evaluation → proxy-tuning application. Each stage depends on successful completion of the previous one.
- **Design tradeoffs**: The choice of relatively small model sizes (0.5B-7B) trades raw capability for efficiency, privacy, and accessibility. The QA rewriting approach trades some pretraining efficiency for better instruction-following. The proxy-tuning approach trades some performance for privacy preservation.
- **Failure signatures**: Common failure modes include: (1) QA conversion introducing medical inaccuracies, (2) Multilingual training causing language-specific performance degradation, (3) Proxy-tuning causing distributional mismatch in larger models. These manifest as degraded accuracy on language-specific benchmarks or unexpected model behavior.
- **First 3 experiments**:
  1. Replicate the pilot study comparing multilingual vs monolingual training on a subset of languages to verify complementarity claims.
  2. Test QA pair conversion quality by comparing model performance on medical knowledge tasks with and without QA rewriting.
  3. Implement proxy-tuning on a smaller scale with a 1B parameter base model and 500M parameter specialized model to validate the mechanism before scaling up.

## Open Questions the Paper Calls Out

- **How does the integration of multilingual medical corpora impact the preservation of local medical knowledge and practices within the Apollo models?**: The paper acknowledges the potential risk of conflicts arising from integrating language-specific medical knowledge in multilingual training, stating that "The integration of medical knowledge across languages might dilute the local specificity of medicine due to differences in lifestyle and constitution across regions." The paper concludes that the benefits of multilingual training outweigh the risks but recognizes this as an area for future research.

- **What is the optimal balance between the scale of the model and its performance on multilingual medical tasks?**: While the paper demonstrates that Apollo-7B outperforms models up to 70B parameters, it does not provide a comprehensive analysis of the relationship between model scale and performance across different languages and medical tasks.

- **How can proxy-tuning be further optimized to enhance the multilingual medical capabilities of larger models without direct fine-tuning?**: The paper introduces proxy-tuning but notes that "We also notice a decline in English proficiency" and that further optimization is needed, particularly in addressing the decline in English proficiency observed in the experiments.

## Limitations

- Data quality control mechanisms for different data types, particularly web content and medical dialogues, are not explicitly specified beyond basic open-source protocol compliance
- The evaluation benchmark may not fully capture the diversity of medical knowledge across different healthcare systems and cultural contexts
- Extensive ablation studies showing how performance scales with different language combinations or how the model handles languages with varying linguistic distances from English are lacking

## Confidence

- **Multilingual Training Benefits**: Medium confidence - Supported by pilot study results but requires further validation of underlying mechanism and generalizability
- **QA Pair Rewriting Effectiveness**: Medium confidence - Theoretically sound but lacks direct comparative evidence against alternative approaches
- **Proxy-Tuning Mechanism**: Medium confidence - Results show improvements but mathematical robustness and generalizability across different architectures remain to be fully established

## Next Checks

1. Conduct controlled experiments training separate models on different subsets of the six languages to quantify the contribution of each language pair to overall performance
2. Compare the QA pair rewriting method against traditional continued pretraining and instruction tuning separately, measuring both medical knowledge retention and instruction-following capabilities
3. Test the proxy-tuning approach across different base model sizes (1B, 7B, 34B parameters) and architectures to evaluate its effectiveness and identify any distributional stability issues or catastrophic forgetting effects