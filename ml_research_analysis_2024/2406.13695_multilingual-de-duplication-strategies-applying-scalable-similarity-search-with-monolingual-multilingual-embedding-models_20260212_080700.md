---
ver: rpa2
title: 'Multilingual De-Duplication Strategies: Applying scalable similarity search
  with monolingual & multilingual embedding models'
arxiv_id: '2406.13695'
source_url: https://arxiv.org/abs/2406.13695
tags:
- multilingual
- embedding
- data
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of multilingual deduplication
  of textual data, specifically job advertisements, using advanced NLP tools. Two
  approaches were compared: a two-step method involving translation to English followed
  by embedding with mpnet, and a multilingual embedding model (distiluse).'
---

# Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models

## Quick Facts
- arXiv ID: 2406.13695
- Source URL: https://arxiv.org/abs/2406.13695
- Authors: Stefan Pasch; Dimitirios Petridis; Jannic Cutura
- Reference count: 6
- One-line primary result: Two-step approach (translation + English embedding) outperforms multilingual embedding model for deduplication accuracy.

## Executive Summary
This paper addresses the challenge of multilingual deduplication of job advertisements using advanced NLP techniques. The authors compare two approaches: a two-step method involving translation to English followed by embedding with mpnet, and a direct multilingual embedding model (distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%), particularly for less widely used languages, and could be further improved to 89% with expert rule filtering. While the multilingual model was faster (1 hour vs. 9 hours), the two-step method proved more effective for multilingual deduplication with current embedding models. Key limitations include token length constraints and computational efficiency.

## Method Summary
The study compared two approaches for multilingual deduplication of approximately 112,000 job advertisements in 24 languages. The first approach translated all job postings to English and embedded them using the mpnet model, while the second used the multilingual distiluse embedding model directly. Both approaches utilized FAISS for scalable similarity search, reducing pairwise comparisons from 1.9 billion to 61.5 million. Candidate pairs were filtered based on L2 distance thresholds, with expert rules further refining results based on domain knowledge (company name, language, location). The two-step approach achieved 82% F1 score versus 60% for the multilingual model, with expert rules potentially increasing accuracy to 89%.

## Key Results
- Two-step approach (translation + English embedding) achieved 82% F1 score vs. 60% for multilingual model
- FAISS indexing reduced pairwise comparisons by over 99% (1.9 billion to 61.5 million)
- Expert rule filtering improved F1 score by 7 percentage points (82% to 89%)
- Two-step approach took 9 hours vs. 1 hour for multilingual model due to sequential translation calls

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-step approach (translation → English embedding) outperforms multilingual embedding model for deduplication accuracy.
- Mechanism: By translating all job postings to English first, the system leverages a highly optimized monolingual embedding model (mpnet), which captures semantic similarity more effectively than current multilingual models, especially for underrepresented languages.
- Core assumption: Semantic nuances are better preserved through high-quality translation to English followed by embedding than through direct multilingual embeddings.
- Evidence anchors:
  - [abstract]: "The two-step approach achieved a higher F1 score (82% vs. 60%), particularly with less widely used languages..."
  - [section]: "Manual checks of a sub-sample revealed that the multilingual model struggled with less widely used languages. For instance, it falsely matched the Lithuanian job titles..."
  - [corpus]: Weak evidence; related papers focus on multilingual sentence encoders but do not directly validate the two-step translation approach.
- Break condition: If translation quality degrades significantly or if the embedding model's token limit causes loss of critical context, the two-step advantage may erode.

### Mechanism 2
- Claim: FAISS indexing reduces pairwise comparison complexity from O(n²) to near-linear, enabling scalable deduplication.
- Mechanism: By embedding all job postings and indexing them with FAISS, the system retrieves only the top k nearest neighbors for each posting, drastically reducing the number of pairwise L2 distance calculations needed.
- Core assumption: The top k nearest neighbors contain all relevant duplicates, and FAISS retrieval is accurate enough to avoid missing true matches.
- Evidence anchors:
  - [section]: "This reduces the number of individual comparisons from 1.9 billion to 61.5 million, a reduction of over 99%."
  - [section]: "A candidate pair is considered a semantic duplicate if the Euclidean distance (L2) is below a certain threshold."
  - [corpus]: Weak evidence; related papers discuss similarity search but do not validate FAISS for multilingual deduplication specifically.
- Break condition: If the L2 distance threshold is set too high, noise increases; if too low, true duplicates are missed.

### Mechanism 3
- Claim: Expert rule filtering based on domain knowledge improves F1 score by 7 percentage points (82% → 89%).
- Mechanism: After initial FAISS filtering, additional rules (e.g., matching on company name, language, location) refine candidate pairs to reduce false positives.
- Core assumption: Domain-specific attributes are reliable indicators of duplicate status beyond semantic similarity.
- Evidence anchors:
  - [abstract]: "which can be increased up to 89% by leveraging expert rules based on domain knowledge."
  - [section]: "We applied varying thresholds depending on whether company name, language of the text, and job location were identical..."
  - [corpus]: No direct evidence; this is a novel contribution not supported by corpus neighbors.
- Break condition: If expert rules are too strict or miss edge cases, precision and recall can suffer.

## Foundational Learning

- Concept: Semantic similarity vs. exact string matching
  - Why needed here: Deduplication must identify semantically equivalent postings even when wording or language differs.
  - Quick check question: Can two job ads with zero word overlap still be duplicates? (Yes, if they describe the same position in different languages or phrasing.)

- Concept: Tokenization and embedding length limits
  - Why needed here: Transformer models like mpnet truncate texts beyond 384 tokens, potentially losing critical context for long job descriptions.
  - Quick check question: What percentage of job postings were truncated in this study? (43%, affecting about 48,000 postings.)

- Concept: FAISS indexing and k-nearest neighbors retrieval
  - Why needed here: FAISS enables scalable similarity search by limiting comparisons to the top k neighbors, avoiding O(n²) complexity.
  - Quick check question: How many pairwise comparisons are avoided by using FAISS? (From 1.9 billion to 61.5 million, over 99% reduction.)

## Architecture Onboarding

- Component map:
  Data preprocessing → Translation API → English embedding (mpnet) / Multilingual embedding (distiluse) → FAISS indexing → L2 distance filtering → Expert rule filtering → Deduplication output
- Critical path:
  Translation (if using two-step) → Embedding → FAISS search → Threshold filtering → Expert rules
- Design tradeoffs:
  - Accuracy vs. runtime: Two-step approach is more accurate (82% F1) but slower (9h vs. 1h) due to sequential translation calls.
  - Token limit vs. context: Truncation beyond 384 tokens can lose critical information, impacting accuracy.
  - Neighbor limit vs. recall: Capping at 100 neighbors may miss some duplicates; L2 threshold filtering is more precise but slower.
- Failure signatures:
  - Low precision: Too many false positives after FAISS filtering; consider tightening L2 threshold or expert rules.
  - Low recall: Missing true duplicates; consider increasing neighbor count or relaxing L2 threshold.
  - Slow runtime: Translation or embedding bottlenecks; consider async processing or model optimization.
- First 3 experiments:
  1. Benchmark L2 distance threshold sweep (0.1 to 0.45) to find optimal precision-recall balance.
  2. Compare FAISS IVF vs. HNSW indexing for retrieval speed and accuracy.
  3. Test multilingual embedding vs. translation + English embedding on a held-out subset to confirm F1 differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would multilingual deduplication performance compare if using transformer models with longer token limits (e.g., 1024 or 2048 tokens) compared to the 384-token limit used in this study?
- Basis in paper: [explicit] The paper explicitly states that about one-third of observations exceeded the 384-token limit, resulting in truncation and potential loss of critical context.
- Why unresolved: The study only used models with a 384-token limit, so the impact of longer token limits on deduplication performance remains untested.
- What evidence would resolve it: Conducting the same multilingual deduplication experiments using transformer models with longer token limits and comparing the F1 scores to the current results.

### Open Question 2
- Question: Would asynchronous translation processing significantly reduce the runtime of the two-step approach to make it competitive with the multilingual model's runtime?
- Basis in paper: [explicit] The paper mentions that the two-step approach's 9-hour runtime is mainly due to sequential Google Translate API calls, which could be executed asynchronously in production.
- Why unresolved: The study used sequential translation calls, so the actual runtime reduction from asynchronous processing is unknown.
- What evidence would resolve it: Implementing the two-step approach with asynchronous translation processing and measuring the actual runtime reduction compared to the original 9-hour runtime.

### Open Question 3
- Question: How would the multilingual embedding model's performance change if it were fine-tuned on domain-specific data (e.g., job advertisements) rather than general multilingual corpora?
- Basis in paper: [explicit] The paper notes that the multilingual model struggled with less widely used languages, as evidenced by false matches in Lithuanian job titles.
- Why unresolved: The study used a general multilingual embedding model without domain-specific fine-tuning, so its performance on specialized datasets remains untested.
- What evidence would resolve it: Fine-tuning the multilingual embedding model on a large dataset of job advertisements in multiple languages and comparing its F1 score to the original multilingual model's performance.

## Limitations

- Study limited to job advertisements in 24 languages, may not generalize to other multilingual text domains
- Expert rules are not explicitly detailed, limiting reproducibility and assessment of potential bias
- Reliance on external translation APIs introduces latency and cost constraints affecting scalability

## Confidence

- **High Confidence**: The two-step approach achieving higher F1 scores (82% vs. 60%) is well-supported by results and manual validation. The FAISS indexing mechanism reducing pairwise comparisons from 1.9 billion to 61.5 million is clearly documented and reproducible.
- **Medium Confidence**: The claim that expert rules can increase F1 scores up to 89% is supported by results but lacks detailed implementation specifics, limiting independent verification.
- **Low Confidence**: The generalizability of these findings to other multilingual text domains (beyond job advertisements) and underrepresented languages not covered in the study remains uncertain.

## Next Checks

1. Replicate F1 Score Differences: Conduct an independent experiment comparing the two-step approach (translation + English embedding) against the multilingual embedding model on a held-out subset of the dataset to verify the reported F1 score gap (82% vs. 60%).

2. Test Token Truncation Impact: Analyze the effect of token truncation on deduplication accuracy by comparing results with and without truncation, particularly for job postings exceeding 384 tokens, to quantify the trade-off between context preservation and model constraints.

3. Evaluate Expert Rule Reproducibility: Document and test the specific expert rules used for filtering duplicates, including their thresholds and conditions, to assess their reproducibility and potential for automation or generalization to other domains.