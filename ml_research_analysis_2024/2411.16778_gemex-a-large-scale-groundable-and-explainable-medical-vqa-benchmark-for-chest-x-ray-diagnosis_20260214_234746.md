---
ver: rpa2
title: 'GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for
  Chest X-ray Diagnosis'
arxiv_id: '2411.16778'
source_url: https://arxiv.org/abs/2411.16778
tags:
- lung
- answer
- questions
- reason
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GEMeX, a large-scale medical visual question
  answering (Med-VQA) dataset for chest X-ray diagnosis that uniquely combines multimodal
  explainability with diverse question types. The dataset was constructed by refining
  anatomical regions and grounding reports from the Chest ImaGenome dataset, followed
  by generating questions using GPT-4o across four formats: open-ended, closed-ended,
  single-choice, and multiple-choice, each enriched with visual and textual explanations.'
---

# GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis

## Quick Facts
- arXiv ID: 2411.16778
- Source URL: https://arxiv.org/abs/2411.16778
- Reference count: 40
- Primary result: Large-scale Med-VQA dataset with 151,025 chest X-ray images and 1,605,575 questions across four diverse formats

## Executive Summary
GEMeX is a large-scale medical visual question answering dataset designed specifically for chest X-ray diagnosis. The dataset addresses limitations in existing Med-VQA benchmarks by providing detailed visual and textual explanations for each question-answer pair, supporting four distinct question formats, and ensuring precise visual-text correspondence through grounded report refinement. Built by refining the Chest ImaGenome dataset with radiologist input and generating questions using GPT-4o, GEMeX contains 1.6 million questions covering diverse clinical reasoning scenarios. The dataset demonstrates that while current large vision language models perform suboptimally on this complex benchmark, instruction tuning with GEMeX data significantly improves performance.

## Method Summary
The GEMeX dataset construction pipeline consists of three main stages: (1) refinement of the Chest ImaGenome dataset to improve anatomical region definitions and establish precise visual-text correspondence mappings with radiologist input, (2) generation of VQA pairs using GPT-4o across four question types (open-ended, closed-ended, single-choice, and multiple-choice) with in-context learning from a golden set and specific clinical rules to ensure multimodal explainability, and (3) post-processing to ensure format compliance and quality. The resulting dataset was evaluated using 10 representative large vision language models, revealing suboptimal performance that improved significantly after instruction tuning on the GEMeX training set using a question-type-aware approach.

## Key Results
- Constructed GEMeX dataset with 151,025 chest X-ray images and 1,605,575 questions
- Current large vision language models show suboptimal performance on GEMeX, highlighting dataset complexity
- Simple instruction tuning on GEMeX training set significantly improves model performance
- Dataset uniquely combines multimodal explainability with four diverse question types reflecting clinical needs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The grounded report refinement stage improves model performance by providing cleaner, more precise visual-text correspondences.
- Mechanism: By refining anatomical regions and ensuring one-to-one sentence-region mapping, the dataset eliminates ambiguity that could confuse models during training.
- Core assumption: Models benefit from clear, unambiguous mappings between visual regions and textual descriptions in medical images.
- Evidence anchors:
  - [section] "By collaborating with radiologists, we systematically redefine anatomical regions and establish more precise visual-text correspondence mappings, resulting in accurate region-grounded reports for each X-ray image."
  - [section] "In the original Chest ImaGenome, there are 29 significant pathological regions. However, in alignment with radiologists' practices, our dataset focuses on retaining core clinical regions..."
- Break condition: If models can already handle ambiguous mappings effectively, the refinement process may not provide significant benefit.

### Mechanism 2
- Claim: The multimodal explainability mechanism enhances model understanding by providing both visual and textual explanations.
- Mechanism: Each question-answer pair includes explicit reasoning and corresponding visual region annotations, forcing models to ground their reasoning in both modalities.
- Core assumption: Models that receive both visual and textual explanations during training will develop better multimodal reasoning capabilities than those trained on only one modality.
- Evidence anchors:
  - [abstract] "A multi-modal explainability mechanism that offers detailed visual and textual explanations for each question-answer pair, thereby enhancing answer comprehensibility"
  - [section] "Each question-answer pair is enriched with explicit reasoning and corresponding visual region annotations"
- Break condition: If models can achieve similar performance with only textual or only visual explanations, the multimodal requirement may be unnecessary.

### Mechanism 3
- Claim: The diverse question type distribution better reflects clinical needs and improves model generalization.
- Mechanism: By including open-ended, closed-ended, single-choice, and multiple-choice questions, the dataset forces models to handle various reasoning patterns and output formats.
- Core assumption: Clinical practice requires diverse question formats, and models trained on diverse formats will generalize better to real-world applications.
- Evidence anchors:
  - [abstract] "Four distinct question types—open-ended, closed-ended, single-choice, and multiple-choice—that better reflect diverse clinical needs"
  - [section] "each enriched with explicit reasoning and corresponding visual region annotations"
- Break condition: If models trained on uniform question types perform equally well on diverse clinical tasks, the diversity may not be necessary.

## Foundational Learning

- Concept: Multimodal alignment
  - Why needed here: Medical VQA requires understanding relationships between visual features and textual descriptions
  - Quick check question: Can you explain how CLIP-based models align visual and textual embeddings?

- Concept: Visual grounding
  - Why needed here: Accurate localization of abnormalities is crucial for clinical applications
  - Quick check question: How would you evaluate the accuracy of predicted bounding boxes against ground truth?

- Concept: Instruction following
  - Why needed here: Models must generate answers in specific formats (open-ended, closed-ended, etc.)
  - Quick check question: What techniques ensure models follow output format instructions consistently?

## Architecture Onboarding

- Component map: Data pipeline (refinement → generation → post-processing) → LVLM evaluation → fine-tuning → re-evaluation
- Critical path: Data refinement → VQA generation → model evaluation → fine-tuning loop
- Design tradeoffs: Precision vs. scale (detailed refinement is time-consuming but improves quality)
- Failure signatures: Poor visual grounding, format non-compliance, shortcut reasoning
- First 3 experiments:
  1. Evaluate LVLM performance on open-ended questions only
  2. Test visual grounding accuracy on single-choice questions
  3. Compare NLG metrics vs. GPTScore for model selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would multi-task training that incorporates GEMeX alongside other medical tasks (e.g., report generation, disease classification) affect the performance of large vision language models on Med-VQA tasks compared to the current task-specific fine-tuning approach?
- Basis in paper: [explicit] The paper states "the real potential lies in integrating our GEMeX into multi-task training (such as the second stage of training LLaV A-Med) in the future" and notes that the current fine-tuned model is "inherently task-specific, which means it may suffer from reduced accuracy on other datasets or lose its ability to engage in conversations."
- Why unresolved: The paper only demonstrates single-task fine-tuning on GEMeX and suggests future work on multi-task training, but does not provide empirical results comparing these approaches.
- What evidence would resolve it: Direct comparison of multi-task training incorporating GEMeX with single-task fine-tuning on GEMeX, measuring performance on both GEMeX and other medical datasets.

### Open Question 2
- Question: What specific architectural modifications to large vision language models would be most effective in improving their ability to perform visual grounding tasks on chest X-ray images, given that current models show poor visual grounding performance even when they can answer questions correctly?
- Basis in paper: [explicit] The paper notes that "Powerful LVLMs like GPT-4o-mini often rely on shortcut reasoning instead of true multimodal reasoning" and "they often fail to accurately visual grounding," suggesting current architectures have limitations in this area.
- Why unresolved: The paper only evaluates existing LVLMs without proposing or testing architectural modifications specifically designed to improve visual grounding capabilities.
- What evidence would resolve it: Empirical comparison of different architectural approaches (e.g., improved visual encoder designs, attention mechanisms, or grounding-specific modules) on the GEMeX dataset's visual grounding metrics.

### Open Question 3
- Question: How does the quality of questions generated by GPT-4o for GEMeX compare to manually created questions in terms of clinical accuracy, reasoning depth, and coverage of important diagnostic features, and what is the optimal balance between automated generation and human curation?
- Basis in paper: [explicit] The paper states that questions were "generated using GPT-4o across four formats" and mentions that the test set was "manually cleaned according to involved diseases," but does not directly compare the quality of automatically generated versus manually created questions.
- Why unresolved: The paper does not provide a systematic comparison of automatically generated questions versus manually curated ones, nor does it quantify the trade-offs between scalability and quality.
- What evidence would resolve it: Side-by-side evaluation of GPT-4o-generated questions versus manually created questions by medical experts on metrics like clinical accuracy, reasoning complexity, and diagnostic coverage, along with analysis of the resources required for each approach.

## Limitations

- Data generation quality control: The exact quality control mechanisms and inter-annotator agreement metrics for GPT-4o question generation are not specified
- Generalizability to other medical domains: The dataset focuses exclusively on chest X-ray diagnosis and may not transfer directly to other medical imaging modalities
- Evaluation metric completeness: Clinical utility requires additional considerations like safety-critical error analysis and integration with clinical workflows that are not addressed

## Confidence

**High Confidence**: The dataset construction pipeline is well-described and technically sound; the claim that GEMeX is the largest Med-VQA dataset with visual and textual explanations is verifiable through stated numbers.

**Medium Confidence**: The claim that instruction tuning on GEMeX significantly improves model performance is supported by results, but comparison baseline is limited; the mechanism by which multimodal explainability improves understanding is theoretically sound but not empirically validated through ablation studies.

**Low Confidence**: The claim that the four question types "better reflect diverse clinical needs" lacks direct clinical validation; while diversity is methodologically sound, clinical relevance is not empirically established.

## Next Checks

1. **Ablation study on explainability components**: Evaluate model performance with only visual explanations, only textual explanations, and both to empirically validate the claimed benefit of multimodal explainability.

2. **Clinical expert review of generated questions**: Have radiologists review a stratified sample of generated questions (across all types) to assess clinical validity, appropriateness, and potential safety concerns in the question-answer pairs.

3. **Cross-domain generalization test**: Fine-tune a model on GEMeX and evaluate its performance on a different medical imaging modality (e.g., X-ray vs. CT, or chest vs. abdominal imaging) to assess the domain-specificity of learned reasoning patterns.