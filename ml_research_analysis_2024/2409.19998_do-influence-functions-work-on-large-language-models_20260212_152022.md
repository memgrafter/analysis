---
ver: rpa2
title: Do Influence Functions Work on Large Language Models?
arxiv_id: '2409.19998'
source_url: https://arxiv.org/abs/2409.19998
tags:
- influence
- data
- arxiv
- functions
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether influence functions, a classical\
  \ method for identifying influential training data points, are effective when applied\
  \ to large language models (LLMs). Through extensive experiments across multiple\
  \ tasks\u2014including harmful data identification, class attribution, and backdoor\
  \ detection\u2014the authors find that influence functions consistently perform\
  \ poorly on LLMs, with low accuracy and coverage compared to simpler methods like\
  \ representation similarity matching."
---

# Do Influence Functions Work on Large Language Models?

## Quick Facts
- arXiv ID: 2409.19998
- Source URL: https://arxiv.org/abs/2409.19998
- Reference count: 24
- Primary result: Influence functions consistently fail on LLMs, with poor accuracy and coverage compared to simpler methods like representation similarity matching

## Executive Summary
This paper investigates whether influence functions, a classical method for identifying influential training data points, are effective when applied to large language models (LLMs). Through extensive experiments across multiple tasks—including harmful data identification, class attribution, and backdoor detection—the authors find that influence functions consistently perform poorly on LLMs, with low accuracy and coverage compared to simpler methods like representation similarity matching. The poor performance is attributed to three key factors: (1) approximation errors in computing inverse-Hessian vector products due to the scale of LLMs, (2) uncertainty in model convergence during fine-tuning, and (3) the fundamental mismatch between parameter changes and changes in LLM behavior. These findings suggest that influence functions are not suitable for explaining LLM behavior and highlight the need for alternative approaches to identify influential training data.

## Method Summary
The paper evaluates influence functions on LLMs by fine-tuning Llama2-7b-chat-hf and Mistral-7b-instruct-v0.3 models with LoRA adapters on various datasets (Advbench, Alpaca, Emotion, Grammars, MathQA, and backdoor trigger datasets). Influence scores are computed using DataInf, LiSSA, and Hessian-free methods, and compared against a representation similarity matching (RepSim) baseline. Performance is measured using accuracy and coverage metrics to identify how well each method can pinpoint influential training data points for specific test samples.

## Key Results
- Influence functions achieve consistently low accuracy and coverage rates across all tested tasks on LLMs
- Representation similarity matching (RepSim) outperforms influence functions on all benchmarks
- The poor performance of influence functions is attributed to approximation errors in inverse-Hessian vector products, uncertain model convergence, and the mismatch between parameter changes and behavioral changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Influence functions fail on LLMs because the inverse-Hessian vector products (iHVP) cannot be accurately computed due to the scale of LLMs.
- Mechanism: As LLM parameter count increases, the Hessian matrix becomes large and sparse with low rank properties. When computing (H + λI)^-1, even with damping, the result approaches an identity matrix scaled by 1/λ, making the influence calculation numerically unstable and ineffective.
- Core assumption: The Hessian matrix of LLMs exhibits sparsity and low-rank properties that increase with model size.
- Evidence anchors:
  - [abstract] "inevitable approximation errors when estimating the iHVP component due to the scale of LLMs"
  - [section 4.1] "When the rank of H satisfies rank(H) ≪ n, the inverse (H + λI)^-1 is close to I/λ"
  - [corpus] Weak evidence - corpus papers focus on efficiency improvements but don't address fundamental approximation failures
- Break condition: If new methods can compute iHVP accurately without approaching the identity matrix approximation

### Mechanism 2
- Claim: Influence functions fail because changes in model parameters don't reliably correlate with changes in LLM behavior.
- Mechanism: LLMs have vast parameter spaces where different parameter configurations can produce similar outputs. Parameter updates during fine-tuning may not reflect meaningful changes in model behavior, making parameter-based influence measures ineffective.
- Core assumption: Multiple parameter configurations can yield similar LLM behaviors
- Evidence anchors:
  - [abstract] "changes in model parameters do not necessarily correlate with changes in LLM behavior"
  - [section 4.3] "at least in this case, changes in the model's safety alignment are not reflected in parameter changes"
  - [section 4.3] "it is entirely possible that for a parameter-abundant complex function, such as LLMs, different parameter sets may yield similar behavior"
- Break condition: If parameter changes can be shown to reliably predict behavioral changes in LLMs

### Mechanism 3
- Claim: Influence functions fail because model convergence during fine-tuning is uncertain, making gradient information unreliable.
- Mechanism: As LLMs converge during fine-tuning, gradient directions become unstable and don't consistently point toward local minima. This instability in gradient updates makes influence calculations based on gradient products unreliable.
- Core assumption: Model convergence state affects gradient reliability
- Evidence anchors:
  - [abstract] "uncertain convergence during fine-tuning"
  - [section 4.2] "as the model converges, the direction of the gradient update no longer consistently moves toward the local minimum"
  - [section 4.2] "complex neural networks may have multiple local minima during optimization"
- Break condition: If stable convergence can be guaranteed or alternative gradient estimation methods prove reliable

## Foundational Learning

- Concept: Inverse-Hessian Vector Products (iHVP)
  - Why needed here: iHVP is the core computational challenge in applying influence functions to LLMs
  - Quick check question: Why does computing iHVP become more difficult as model size increases?

- Concept: Parameter-Efficient Fine-Tuning (PEFT)
  - Why needed here: PEFT methods like LoRA reduce trainable parameters, making influence functions more computationally feasible but introducing new approximation challenges
  - Quick check question: How does reducing trainable parameters affect the rank properties of the Hessian matrix?

- Concept: Model Convergence and Local Minima
  - Why needed here: Understanding how convergence affects gradient reliability is crucial for explaining influence function failures
  - Quick check question: Why might multiple local minima in the loss landscape make gradient-based influence measures unreliable?

## Architecture Onboarding

- Component map: Data preparation → Model fine-tuning (with PEFT) → Influence computation (Hessian-based or Hessian-free) → Evaluation (accuracy/coverage metrics)
- Key components: LoRA adapters, gradient computation, Hessian approximation, influence aggregation

- Critical path:
  1. Fine-tune LLM with PEFT on task-specific data
  2. Compute gradients for test and training samples
  3. Approximate inverse-Hessian vector products
  4. Calculate influence scores and rank training data
  5. Evaluate against ground truth influential data

- Design tradeoffs:
  - Accuracy vs. computational efficiency in iHVP approximation
  - Model convergence quality vs. training time
  - Parameter reduction (via PEFT) vs. influence computation accuracy

- Failure signatures:
  - High correlation between Hessian-based and Hessian-free methods (indicating identity matrix approximation)
  - Unstable influence scores across training epochs
  - Poor alignment between identified influential data and ground truth

- First 3 experiments:
  1. Compare influence function performance on small vs. large models to isolate scale effects
  2. Test influence function accuracy at different fine-tuning convergence stages
  3. Evaluate parameter change magnitude vs. behavioral change correlation across different fine-tuning datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can influence functions be made effective for LLMs through improved approximation techniques or alternative formulations?
- Basis in paper: Explicit - The paper identifies approximation errors in computing inverse-Hessian vector products (iHVP) as a key limitation, and suggests this is due to the scale of LLMs and their Hessian matrices exhibiting sparsity and low-rank properties.
- Why unresolved: The paper demonstrates that even state-of-the-art efficient Hessian-based methods and Hessian-free methods fail to perform well on LLMs, but does not explore whether fundamentally different mathematical formulations or approximation techniques could overcome these limitations.
- What evidence would resolve it: A study showing that modified influence function formulations (e.g., using different perturbation objectives, alternative regularization approaches, or novel approximation methods) achieve high accuracy and coverage rates across the tested tasks (harmful data identification, class attribution, backdoor detection) would demonstrate that influence functions can be made effective for LLMs.

### Open Question 2
- Question: Is there a fundamental mismatch between parameter changes and LLM behavior that cannot be resolved, or can this relationship be better understood and leveraged?
- Basis in paper: Explicit - The paper argues that influence functions measure parameter changes (∆θ) which do not necessarily reflect changes in LLM behavior, citing examples where fine-tuning produces significant behavioral changes without corresponding parameter changes.
- Why unresolved: The paper shows that parameter changes continue to grow during training even when model behavior stabilizes, suggesting that parameter space may have multiple minima with similar behavior. However, it does not explore whether there exists a better mapping between parameters and behavior that could be used for influence analysis.
- What evidence would resolve it: Research demonstrating a reliable method to map parameter changes to behavioral changes (e.g., through behavioral gradients, representation changes, or other intermediate representations) that enables accurate influence function computation would resolve this question. Conversely, theoretical proof that no such mapping exists would confirm a fundamental limitation.

### Open Question 3
- Question: What alternative methods could effectively identify influential training data for LLMs, and how do they compare to representation similarity matching?
- Basis in paper: Explicit - The paper concludes that alternative approaches are needed, having shown that representation similarity matching (RepSim) outperforms influence functions across all tested tasks, but does not explore other potential methods.
- Why unresolved: While RepSim is shown to be superior to influence functions, the paper does not systematically evaluate other approaches such as data valuation techniques (Shapley values, data imprints), causal influence methods, or newer interpretability techniques that might offer better performance or computational efficiency.
- What evidence would resolve it: A comprehensive comparison of multiple alternative methods (including but not limited to data valuation, causal analysis, and representation-based approaches) across the same benchmark tasks, demonstrating which methods achieve the best accuracy, coverage, and computational efficiency for identifying influential training data in LLMs.

## Limitations
- Scale dependency: Results focus on 7B parameter models, leaving uncertainty about performance on much larger models
- PEFT-specific effects: Use of LoRA may introduce artifacts not present with full fine-tuning
- Task specificity: Failure mechanisms may vary significantly across different types of LLM applications

## Confidence
- High confidence: Influence functions perform poorly on LLMs compared to simpler methods like RepSim, across multiple tasks and datasets
- Medium confidence: The three proposed mechanisms (iHVP approximation errors, convergence uncertainty, parameter-behavior mismatch) adequately explain the observed failures
- Medium confidence: The Hessian matrix exhibits low-rank properties that prevent accurate iHVP computation at LLM scale

## Next Checks
1. Scale extrapolation validation: Test influence function performance on both smaller models (1B parameters) and larger models (70B+ parameters) to determine if failure mechanisms scale predictably with model size.

2. Fine-tuning method comparison: Repeat key experiments using full fine-tuning instead of LoRA to isolate whether PEFT-specific factors contribute to the observed failures.

3. Alternative influence measures: Evaluate whether modified influence function formulations (e.g., using Fisher information matrix instead of Hessian) can overcome the identified failure modes, providing evidence for or against the proposed mechanisms.