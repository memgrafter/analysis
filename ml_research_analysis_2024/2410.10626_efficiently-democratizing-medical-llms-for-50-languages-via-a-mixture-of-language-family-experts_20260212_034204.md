---
ver: rpa2
title: Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language
  Family Experts
arxiv_id: '2410.10626'
source_url: https://arxiv.org/abs/2410.10626
tags:
- languages
- medical
- language
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting medical Large Language
  Models to local languages to reduce healthcare access barriers, particularly for
  low-resource languages where data scarcity remains a significant obstacle. The authors
  propose a novel approach combining a high-quality medical dataset construction,
  a language-specific Mixture of Experts (MoE) architecture with cross-lingual routing,
  and a "Spread Out in the End" information flow mechanism that applies sparse routing
  only in later layers while maintaining dense connections in earlier layers.
---

# Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts

## Quick Facts
- arXiv ID: 2410.10626
- Source URL: https://arxiv.org/abs/2410.10626
- Reference count: 40
- Primary result: Apollo-MoE series achieves 69.9 accuracy on high-resource languages and 58.3 on low-resource languages across 50 languages

## Executive Summary
This paper addresses the challenge of adapting medical Large Language Models to local languages to reduce healthcare access barriers, particularly for low-resource languages where data scarcity remains a significant obstacle. The authors propose a novel approach combining a high-quality medical dataset construction, a language-specific Mixture of Experts (MoE) architecture with cross-lingual routing, and a "Spread Out in the End" information flow mechanism that applies sparse routing only in later layers while maintaining dense connections in earlier layers. To efficiently scale to 50 languages, they introduce language family experts based on linguistic priors, reducing the number of expert layers from 50 to 7. The resulting Apollo-MoE series demonstrates strong multilingual performance across all 50 languages, with the 10B model (8.02B active) outperforming open-source models with similar parameter counts.

## Method Summary
The method combines three key innovations: a high-quality medical dataset spanning 12 high-resource languages expanded to 50 languages, a Post-MoE architecture that applies MoE only in later layers to maintain cross-lingual integration in early layers, and language family experts that group languages into 7 linguistic families to reduce parameter growth. The approach uses Hybrid-k routing that balances language-specific expertise with cross-lingual transfer, and the "Spread Out in the End" mechanism ensures early layers concentrate cross-lingual information flow while later layers enable language-specific specialization.

## Key Results
- Apollo-MoE 10B (8.02B active) achieves 69.9 accuracy on high-resource languages and 58.3 on low-resource languages
- Language family experts reduce expert layers from 50 to 7 while maintaining performance across 50 languages
- Post-MoE architecture with MoE in last 2 layers shows superior performance compared to full MoE and dense models
- Hybrid-k routing outperforms both pure language-specific and top-k routing strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language family experts reduce parameter growth while preserving multilingual performance.
- Mechanism: Grouping 50 languages into 7 linguistic families allows routing to language-specific experts within families rather than creating a unique expert per language. This reduces the number of expert layers from 50 to 7 while maintaining language-specific knowledge separation.
- Core assumption: Languages within the same family share sufficient linguistic priors to benefit from shared expert representation.
- Evidence anchors:
  - [abstract] "leverages linguistic priors, which enables scaling the number of languages without adding additional parameters"
  - [section 4.1] "we introduce the concept of language family experts, which groups the 50 languages into 7 established linguistic families"
  - [corpus] No direct corpus evidence found for this specific grouping mechanism.
- Break condition: If languages within families have insufficient shared features, performance would degrade despite reduced parameters.

### Mechanism 2
- Claim: "Spread Out in the End" routing enables cross-lingual integration in early layers and language-specific specialization in later layers.
- Mechanism: Early layers concentrate cross-lingual information flow where tokens share routing patterns across languages. Later layers exhibit language-specific divergence where tokens are routed to language-specific experts, enabling specialization.
- Core assumption: Information flow circuits follow a predictable pattern from cross-lingual integration to language-specific differentiation.
- Evidence anchors:
  - [abstract] "our routing analysis revealed a 'Spread Out in the End' information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence"
  - [section 3.2.2] "through the utilization of routing distribution analysis and Information Flow Circuits visualization, we observe that the information flow circuits exhibit cross-linguistic concentration in the early layers, while differentiation based on language occurs in the later layers"
  - [corpus] Related work on multilingual routing supports this layered specialization pattern.
- Break condition: If routing patterns don't follow this distribution, Post-MoE architecture would not improve performance.

### Mechanism 3
- Claim: Hybrid-k routing improves multilingual generalization compared to both language-specific and top-k routing alone.
- Mechanism: Hybrid-k ensures language-specific experts are activated while also allowing cross-lingual routing to other language experts, balancing interpretability with performance.
- Core assumption: The model can effectively leverage cross-lingual routing without sacrificing language-specific knowledge.
- Evidence anchors:
  - [section 3.1.1] "We propose new MoE consists of language-specific experts and hybrid routing, enhancing both language-specific expertise and transfer of general medical knowledge across languages"
  - [section 3.1.2] "Hybrid-k routing exhibits a clear advantages than Top-k in nearly all languages, especially in low-resource languages"
  - [corpus] No direct corpus evidence found for this specific routing mechanism.
- Break condition: If cross-lingual routing interferes with language-specific performance, Hybrid-k would underperform pure language-specific routing.

## Foundational Learning

- Concept: Mixture of Experts (MoE) architecture
  - Why needed here: Enables efficient scaling to multiple languages by activating only relevant experts per input
  - Quick check question: How does MoE differ from dense models in terms of parameter activation?

- Concept: Cross-lingual transfer learning
  - Why needed here: Allows models to leverage knowledge from high-resource languages to improve performance on low-resource languages
  - Quick check question: What linguistic features enable transfer between related languages?

- Concept: Circuit theory in neural networks
  - Why needed here: Provides framework for analyzing information flow patterns across layers and understanding routing behavior
  - Quick check question: How do directed acyclic graphs represent information flow in neural networks?

## Architecture Onboarding

- Component map: Input → Token classification → Router selection → Expert activation → Information processing → Output
- Critical path: Base transformer layers (dense) → MoE layers (sparse routing) → Language family experts (7 groups) → Router networks (per MoE layer) → Post-MoE configuration (MoE only in last 2 layers)
- Design tradeoffs: Parameter efficiency vs. language-specific performance; interpretability vs. pure performance
- Failure signatures: Poor performance on specific language families; routing imbalance across experts; catastrophic forgetting of original languages
- First 3 experiments:
  1. Implement Post-MoE with MoE only in last 2 layers and compare to full MoE and dense models
  2. Test language family grouping with different family definitions and measure impact on performance
  3. Compare Hybrid-k routing against pure language-specific and top-k routing across multiple language pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal depth and layer distribution for applying MoE routing in the Post-MoE architecture for larger models beyond 7B parameters?
- Basis in paper: [inferred] The paper notes that Post-MoE expansion in the last two layers has shown good results for the 7B model, but mentions that "For larger models, the optimal effect might not be achieved by expanding just the last two layers; a proportional calculation of layers may be required."
- Why unresolved: The paper only tested Post-MoE with expansion in the last 1-4 layers using Qwen2-0.5B and 1.5B models, leaving uncertainty about optimal configurations for larger models. The "Spread Out in the End" phenomenon observed in routing analysis provides theoretical motivation but doesn't specify exact layer distributions for different model sizes.
- What evidence would resolve it: Systematic experiments varying the number and distribution of MoE layers across different model sizes (e.g., 13B, 34B, 70B) while measuring performance on both high-resource and low-resource languages would identify optimal configurations.

### Open Question 2
- Question: How do language family experts perform compared to individual language-specific experts in terms of both performance and parameter efficiency as the number of languages scales beyond 50?
- Basis in paper: [explicit] The paper introduces language family experts to reduce expert layers from 50 to 7, demonstrating parameter efficiency, but notes "This method might facilitate scalable and robust multilingual performance across a wide range of languages" without providing empirical evidence for scaling beyond 50 languages.
- Why unresolved: The paper demonstrates the approach with 50 languages but doesn't test the limits of this approach or compare it directly with models using individual language-specific experts. The linguistic validity of grouping certain languages and potential performance trade-offs remain unexplored.
- What evidence would resolve it: Comparative experiments scaling to 100+ languages using both language family experts and individual language-specific experts, measuring performance, parameter counts, and efficiency metrics would reveal scalability limits and optimal grouping strategies.

### Open Question 3
- Question: What is the precise relationship between data quantity and model performance for low-resource languages, and at what point does performance plateau?
- Basis in paper: [explicit] The paper notes that "the model's performance across various languages improves with increasing data but eventually plateaus" and that "This indicates that the proposed method is not heavily data-dependent, reaching saturation with as few as 2,000 data examples."
- Why unresolved: While the paper observes plateauing behavior, it only tests up to 100% of the low-resource language data (2,000 examples per language). The exact shape of the learning curve, the minimum effective data threshold, and whether different language families require different amounts of data remain unknown.
- What evidence would resolve it: Detailed scaling studies varying data quantities from minimal samples (e.g., 50, 100, 500 examples) up to the full dataset for multiple low-resource languages, measuring performance curves and identifying saturation points for different language families.

## Limitations

- The linguistic validity of the 7-family grouping for 50 languages lacks empirical validation against alternative grouping strategies
- The Post-MoE architecture's performance claims rely on observational routing analysis rather than experimental ablation studies
- The evaluation focuses on medical QA tasks without demonstrating generalization to other medical domains or real-world clinical applications

## Confidence

- **High confidence**: The core observation that language family experts can reduce parameter growth while maintaining performance
- **Medium confidence**: The "Spread Out in the End" information flow mechanism shows interesting routing patterns but requires more rigorous experimental validation
- **Medium confidence**: The Hybrid-k routing claims are supported by comparative results but lack ablation studies isolating the routing mechanism's contribution

## Next Checks

1. **Language family grouping validation**: Conduct controlled experiments testing alternative language family definitions to empirically verify that the chosen 7-family structure is optimal rather than arbitrary.

2. **Routing mechanism ablation**: Implement and evaluate variants of the Post-MoE architecture with different routing distributions and strategies to isolate the contribution of each component to performance improvements.

3. **Cross-lingual transfer evaluation**: Design experiments measuring knowledge transfer from high-resource to low-resource languages by systematically removing training data from specific language families and evaluating performance degradation patterns.