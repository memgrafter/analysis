---
ver: rpa2
title: 'GLEAMS: Bridging the Gap Between Local and Global Explanations'
arxiv_id: '2408.05060'
source_url: https://arxiv.org/abs/2408.05060
tags:
- local
- gleams
- global
- explanations
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GLEAMS bridges the gap between local and global explanations for
  black-box models by recursively partitioning the input space and fitting linear
  models within each cell. The method generates Sobol points to sample the input space,
  builds a piecewise-linear global surrogate, and provides both local feature attributions
  and global feature importance rankings.
---

# GLEAMS: Bridging the Gap Between Local and Global Explanations

## Quick Facts
- arXiv ID: 2408.05060
- Source URL: https://arxiv.org/abs/2408.05060
- Reference count: 40
- Key outcome: GLEAMS bridges the gap between local and global explanations for black-box models by recursively partitioning the input space and fitting linear models within each cell.

## Executive Summary
GLEAMS is a novel method for explaining black-box models that generates both local and global explanations through recursive partitioning of the input space. The approach creates a global surrogate model by sampling Sobol points, building a piecewise-linear approximation, and using the resulting partition to provide constant-time explanations. The method demonstrates superior performance compared to LIME and SHAP on local and global monotonicity metrics while offering the unique advantage of generating explanations without re-sampling or querying the original model.

## Method Summary
GLEAMS works by generating Sobol points to sample the input space, recursively splitting this space into rectangular cells, and fitting linear models within each cell. The method uses a model-based recursive partitioning approach with score computations to determine optimal splits. Once the global surrogate model is built, local explanations are obtained by reading coefficients from the appropriate cell, and global explanations are computed as weighted averages of absolute local coefficients across all cells. The approach handles continuous features and provides counterfactual explanations by computing feature value variations along cell boundaries.

## Key Results
- Outperforms LIME and SHAP on local and global monotonicity metrics
- Provides constant-time explanations without re-sampling or querying the original model
- Demonstrates comparable performance to state-of-the-art methods with additional benefits
- Successfully evaluated on Wine, House sell, and Parkinson datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GLEAMS generates both local and global explanations by recursively partitioning the input space and fitting linear models within each cell.
- Mechanism: The method creates a global surrogate model by sampling Sobol points, building a piecewise-linear approximation, and using the resulting partition to provide constant-time explanations.
- Core assumption: The input space is a hyper-rectangle where each variable has a prescribed range, and the black-box model can be well-approximated by linear models within rectangular cells.
- Evidence anchors: [abstract] "GLEAMS bridges the gap between local and global explanations for black-box models by recursively partitioning the input space and fitting linear models within each cell." [section] "The core idea of GLEAMS is to recursively split the input space in rectangular cells on which the black-box model can be well-approximated by linear models." [corpus] Weak - the corpus neighbors discuss general interpretability but don't specifically address recursive partitioning for linear approximation.
- Break condition: The mechanism breaks when the black-box model's decision surface is too complex to be approximated by piecewise-linear models within reasonable partition sizes, or when features are not continuous.

### Mechanism 2
- Claim: GLEAMS provides explanations in constant time without re-sampling or querying the original model.
- Mechanism: Once the global surrogate model is built, local explanations are obtained by reading coefficients from the appropriate cell, and global explanations are computed from aggregated cell-level information.
- Core assumption: The partition structure allows for efficient lookup of which cell contains a given example, and the local linear models remain valid within their respective cells.
- Evidence anchors: [abstract] "GLEAMS outperforms LIME and SHAP on local and global monotonicity metrics while offering constant-time explanations without re-sampling or querying the original model." [section] "At query time, each new example to explain Œæ ‚àà ùí≥ is rooted to the corresponding hyper-rectangle R in the tree structure, and thus associated to a linear model Œ≤^Œæ." [corpus] Weak - corpus doesn't specifically address constant-time explanation generation.
- Break condition: The mechanism breaks when the partition becomes too deep (requiring multiple lookups) or when the tree structure becomes unbalanced, increasing query time.

### Mechanism 3
- Claim: GLEAMS combines local interpretability with global model understanding through a unified framework.
- Mechanism: Local explanations are given by feature coefficients within each cell, while global feature importance is computed as a weighted average of absolute local coefficients across all cells.
- Core assumption: Local linear models capture meaningful feature importance that can be aggregated to provide global insights, and the weighting by cell volume appropriately reflects feature importance across the input space.
- Evidence anchors: [abstract] "provides both local feature attributions and global feature importance rankings" [section] "We define the global importance of feature j as the aggregation of the local importance of feature j on all cells of the partition ùí´ of ùí≥." [corpus] Weak - corpus doesn't specifically address the unification of local and global explanations.
- Break condition: The mechanism breaks when local feature importance varies wildly across cells without a coherent global pattern, or when the aggregation method doesn't capture the true global importance.

## Foundational Learning

- Concept: Sobol sequences for space-filling sampling
  - Why needed here: GLEAMS requires evenly spread measurement points across the input space to build an accurate global surrogate model.
  - Quick check question: What property of Sobol sequences makes them preferable to random sampling for building the global surrogate?

- Concept: Recursive partitioning and tree structures
  - Why needed here: The method recursively splits the input space into rectangular cells, creating a tree structure that enables efficient lookup and constant-time explanations.
  - Quick check question: How does the tree structure enable constant-time explanations once the global surrogate is built?

- Concept: Model-based recursive partitioning
  - Why needed here: GLEAMS uses a variant of model-based recursive partitioning based on score computations to determine where to split the input space.
  - Quick check question: What criterion does GLEAMS use to decide whether to split a given cell in the partition?

## Architecture Onboarding

- Component map:
  - Sobol point generator ‚Üí Measurement point collection ‚Üí Recursive partition builder ‚Üí Tree structure ‚Üí Query engine ‚Üí Explanation generator
  - Local explanation module: Extracts coefficients from leaf node
  - Global explanation module: Aggregates coefficients across all leaves
  - Counterfactual explanation module: Computes feature value variations along cell boundaries

- Critical path:
  1. Generate Sobol points and query black-box model
  2. Build recursive partition with linear models in each leaf
  3. Store tree structure for efficient querying
  4. For each new example, find containing leaf and extract local explanation
  5. Compute global explanations by aggregating across all leaves

- Design tradeoffs:
  - Number of Sobol points (N) vs. accuracy of global surrogate
  - Minimum points per leaf (nmin) vs. granularity of partition
  - R¬≤ threshold vs. complexity of resulting tree
  - Tradeoff between constant-time explanations and accuracy of approximation

- Failure signatures:
  - Poor local monotonicity scores indicate inadequate partitioning or insufficient Sobol points
  - Degraded global monotonicity suggests the aggregation method doesn't capture true global importance
  - High variance in local R¬≤ values across leaves indicates inconsistent model approximation quality
  - Deep trees with many small leaves suggest the black-box model is too complex for piecewise-linear approximation

- First 3 experiments:
  1. Verify Sobol point generation covers the input space uniformly by visualizing point distribution
  2. Test the partition building on a known piecewise-linear function to verify the surrogate matches the ground truth
  3. Measure query time for explanations on held-out data to confirm constant-time performance claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does increasing the number of Sobol points (N) affect the quality of GLEAMS explanations, particularly for higher-dimensional datasets?
- Basis in paper: [inferred] The authors mention that increasing N would likely improve results for higher-dimensional datasets but did not have time to run experiments with higher N.
- Why unresolved: The authors did not conduct experiments with higher N values to verify the impact on explanation quality for complex models and higher-dimensional data.
- What evidence would resolve it: Running GLEAMS with varying N values on datasets with increasing dimensionality and comparing explanation quality metrics (local/global monotonicity, recall) would provide empirical evidence of the relationship between N and explanation accuracy.

### Open Question 2
- Question: How does GLEAMS perform when explaining models with mixed feature types (categorical and continuous)?
- Basis in paper: [explicit] The authors state that a main limitation of GLEAMS is the assumption that all features are continuous and mention extending it to mixed features as future work.
- Why unresolved: GLEAMS currently only handles continuous features, and its performance on models with categorical features remains untested.
- What evidence would resolve it: Implementing GLEAMS to handle categorical features and evaluating its explanation quality on datasets with mixed feature types would demonstrate its effectiveness in this broader setting.

### Open Question 3
- Question: What is the optimal stopping criterion for the recursive partitioning in GLEAMS to balance explanation quality and computational efficiency?
- Basis in paper: [inferred] The authors use an R¬≤ threshold of 0.95 and a minimum number of points per leaf, but these are chosen empirically and may not be optimal for all scenarios.
- Why unresolved: The paper does not explore different stopping criteria or provide a theoretical justification for the chosen values.
- What evidence would resolve it: Systematically varying the R¬≤ threshold and minimum points per leaf, then evaluating the trade-off between explanation quality (monotonicity, recall) and computational cost would identify optimal stopping criteria for different types of models and datasets.

## Limitations
- Method assumes all features are continuous, limiting applicability to datasets with categorical variables
- Performance may degrade on high-dimensional datasets due to increased complexity of the model surface
- Critical hyperparameters are set empirically without clear guidance on their impact or optimal values

## Confidence
- Local and global explanation quality claims: **Medium** - Supported by empirical results but limited dataset diversity
- Constant-time explanation generation: **High** - Theoretically sound given the tree structure, though query complexity depends on tree depth
- Unified framework advantage: **Medium** - Demonstrated practically but not compared against specialized local/global methods in all aspects

## Next Checks
1. **Sensitivity analysis of hyperparameters**: Systematically vary N, œÅ, and nmin to quantify their impact on explanation quality and runtime performance
2. **Benchmark against specialized methods**: Compare GLEAMS not just with LIME/SHAP but also with methods designed specifically for either local or global explanations
3. **Scalability testing**: Evaluate performance on high-dimensional datasets (10+ features) to assess the method's limits and verify the claimed degradation patterns