---
ver: rpa2
title: In-context learning and Occam's razor
arxiv_id: '2410.14086'
source_url: https://arxiv.org/abs/2410.14086
tags:
- prequential
- learning
- data
- length
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper establishes that in-context learning (ICL) in sequence\
  \ models can be understood as an implicit form of Occam\u2019s razor, where the\
  \ next-token prediction objective is equivalent to minimizing a prequential code\
  \ length. This code length jointly accounts for model complexity and training error,\
  \ promoting simple models that generalize well."
---

# In-context learning and Occam's razor

## Quick Facts
- arXiv ID: 2410.14086
- Source URL: https://arxiv.org/abs/2410.14086
- Reference count: 40
- The paper establishes that in-context learning (ICL) in sequence models can be understood as an implicit form of Occam's razor, where the next-token prediction objective is equivalent to minimizing a prequential code length.

## Executive Summary
This paper presents a theoretical and empirical framework connecting in-context learning (ICL) in sequence models to Occam's razor through prequential coding. The authors demonstrate that the next-token prediction objective used to train ICL learners is mathematically equivalent to minimizing prequential code length, which jointly accounts for model complexity and training error. Through experiments on synthetic regression and classification tasks, they show that ICL learners generalize better than those trained on train-risk objectives or standard SGD, particularly in low-data regimes. The work provides both theoretical insights into why ICL works and practical guidance for improving ICL through architectural choices and training setup.

## Method Summary
The method involves training sequence models (Transformers, Mamba) as meta-learners on distributions of tasks, where the training objective is next-token prediction loss that implements prequential coding. For each task, context sequences are constructed from input-output pairs, passed through the sequence model to obtain predictions, and the negative log-likelihood is computed. The meta-learner is trained to minimize cumulative prediction loss across tasks. The approach is compared against train-risk ICL (trained to minimize average training loss) and SGD baselines on synthetic datasets including linear regression, sinusoidal regression, Mastermind classification, and Hidden Markov Models.

## Key Results
- ICL consistently outperforms train-risk ICL and SGD in generalization for short context lengths and low-data regimes
- Architecture choice significantly impacts ICL performance, with interactions between model capacity and task distribution
- Large pretrained LLMs can fail on novel tasks despite scale, indicating task-generalization remains challenging
- The prequential coding objective provides a theoretical foundation for why ICL prefers simpler models that generalize well

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICL is a meta-learning process that optimizes for compression via prequential coding, jointly minimizing training error and model complexity
- Mechanism: The next-token prediction objective is mathematically equivalent to minimizing prequential code length, which upper bounds Kolmogorov complexity
- Core assumption: The learner can generalize to novel tasks while preserving low prequential code length
- Evidence: Theoretical equivalence established, experiments show better generalization in low-data regimes
- Break condition: Overfitting to single tasks or insufficient model capacity prevents code length minimization

### Mechanism 2
- Claim: ICL prefers simpler models in low-data regimes because prequential coding requires fast adaptation with minimal complexity
- Mechanism: Simpler models assign higher likelihood to next tokens faster, reducing cumulative prediction loss more than complex overfit models
- Core assumption: Learner capacity scales with parameters and next-token prediction enforces online learning dynamics
- Evidence: Consistent generalization advantage for short context lengths
- Break condition: In high-data regimes, complexity dominates and simpler models may underfit

### Mechanism 3
- Claim: Architecture critically affects ICL's ability to minimize prequential code length and generalize
- Mechanism: Different architectures have varying inductive biases that influence compression efficiency and generalization
- Core assumption: Architecture's expressivity and optimization dynamics are compatible with prequential coding objective
- Evidence: Substantial performance differences across architectures, LLM failure on Mastermind
- Break condition: Inefficient implementation of prequential coding objective due to architectural limitations

## Foundational Learning

- Concept: Kolmogorov complexity and its relationship to data compression
  - Why needed here: Frames ICL as compression problem where minimizing prequential code length approximates minimizing Kolmogorov complexity
  - Quick check question: Why does minimizing negative log-likelihood relate to data compression in the context of Kolmogorov complexity?

- Concept: Meta-learning and distinction between training error minimization vs. prequential code length minimization
  - Why needed here: Explains how ICL differs from standard supervised learning and why it generalizes better in low-data regimes
  - Quick check question: How does the prequential coding objective differ from standard train-risk minimization?

- Concept: Next-token prediction as online learning and its connection to prequential coding
  - Why needed here: Explains mechanism by which sequence models implicitly learn to compress data efficiently
  - Quick check question: How does next-token prediction objective implement prequential coding algorithm for data compression?

## Architecture Onboarding

- Component map: Meta-learner TÏ• (sequence model) -> Context encoder (processes input-output pairs) -> Prediction head (maps activations to predictions) -> Loss function (next-token prediction implementing prequential coding)

- Critical path: 1. Construct context sequence from training examples, 2. Pass context through sequence model to get activations, 3. Apply prediction head to get next-token predictions, 4. Compute next-token prediction loss, 5. Backpropagate through sequence model to update parameters

- Design tradeoffs: Model capacity vs. overfitting (larger models risk overfitting in low-data regimes), Architecture choice (Transformers vs. state-space models have different inductive biases), Context length handling (affects compression efficiency and generalization)

- Failure signatures: Poor generalization in low-data regimes (underfitting or insufficient capacity), Overfitting to training tasks (memorized task distribution), Inconsistent performance across task types (architectural mismatch)

- First 3 experiments: 1. Compare prequential ICL vs. train-risk ICL on simple regression with varying context lengths, 2. Test different architectures (Transformer with/without bottleneck, Mamba) on same task distribution, 3. Evaluate pretrained LLM on novel task (Mastermind) to demonstrate task-generalization limitations

## Open Questions the Paper Calls Out

- Question: How can sequence model architectures be designed to explicitly target prequential code length during ICL?
  - Basis: Section 5 discusses role of model architecture in minimizing prequential code length
  - Why unresolved: Identifies current limitations but doesn't propose specific architectural modifications
  - Evidence needed: Experimental results comparing ICL performance using novel architectures versus standard ones

- Question: Can combining ICL with gradient-based methods through "mixture of learners" improve generalization across data regimes?
  - Basis: Section 5 suggests combining ICL's sample efficiency with SGD's performance on large datasets
  - Why unresolved: Mentioned as promising approach but not implemented or tested
  - Evidence needed: Empirical results showing improved performance with hybrid approach

- Question: How does ordering of data presented in-context affect prequential code length and model complexity?
  - Basis: Section 5 raises questions about optimal curricula for data ordering in ICL
  - Why unresolved: Identified as important factor but not investigated experimentally
  - Evidence needed: Studies showing how different data orderings impact ICL performance and compression efficiency

## Limitations

- Experiments rely entirely on synthetic datasets which may not capture real-world task complexity and variability
- Comparison between ICL and SGD baselines may not be entirely fair due to different context construction approaches
- Computational efficiency of ICL versus other meta-learning approaches is not investigated, crucial for practical applications
- Analysis assumes minimizing prequential code length directly translates to better generalization, relationship may not hold for all task distributions

## Confidence

- **High Confidence**: Empirical demonstration that ICL outperforms train-risk ICL and SGD in low-data regimes across multiple synthetic tasks; ablation studies on architecture and pretraining conditions provide robust evidence
- **Medium Confidence**: Theoretical framing of ICL as minimizing prequential code length as implementation of Occam's razor; mathematical equivalence established but practical implications require further validation
- **Low Confidence**: Claim that large pretrained models fail at novel tasks due to inability to minimize prequential code length; Mastermind example provides suggestive evidence but lacks comprehensive comparison

## Next Checks

1. Test ICL framework on real-world datasets with varying data regimes (few-shot image classification, tabular regression) to verify prequential coding advantage extends beyond synthetic data

2. Compare ICL against established meta-learning algorithms (MAML, ProtoNets) on same task distributions to isolate specific benefits of prequential coding objective versus other meta-learning approaches

3. Measure and compare computational cost (FLOPs, memory usage, wall-clock time) of ICL versus standard supervised learning and meta-learning baselines across different context lengths and model sizes to assess practical viability