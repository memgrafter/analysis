---
ver: rpa2
title: 'NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation'
arxiv_id: '2402.15852'
source_url: https://arxiv.org/abs/2402.15852
tags:
- navigation
- arxiv
- navid
- tokens
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NaVid, a video-based large vision language
  model (VLM) for vision-and-language navigation (VLN). NaVid uses only monocular
  RGB video as input to output next-step actions for robot navigation, without requiring
  maps, odometers, or depth sensors.
---

# NaVid: Video-based VLM Plans the Next Step for Vision-and-Language Navigation

## Quick Facts
- arXiv ID: 2402.15852
- Source URL: https://arxiv.org/abs/2402.15852
- Authors: Jiazhao Zhang; Kunyu Wang; Rongtao Xu; Gengze Zhou; Yicong Hong; Xiaomeng Fang; Qi Wu; Zhizheng Zhang; He Wang
- Reference count: 40
- Key outcome: NaVid achieves state-of-the-art performance in vision-and-language navigation using only monocular RGB video, reaching approximately 66% success rate in real-world indoor navigation across four diverse scenes.

## Executive Summary
NaVid introduces a video-based large vision language model for vision-and-language navigation (VLN) that operates using only monocular RGB video input. The system encodes historical and current observations as spatio-temporal video tokens, enabling effective context for decision making without requiring maps, odometers, or depth sensors. Extensive experiments demonstrate state-of-the-art performance in both simulation and real-world environments, with particular strength in cross-dataset and Sim2Real transfer capabilities.

The method addresses the challenge of navigating in previously unseen environments based on natural language instructions by leveraging the temporal coherence of video sequences. By processing the video stream as a sequence of tokens rather than individual frames, NaVid captures the spatial-temporal context necessary for effective navigation planning. This approach eliminates the need for additional sensors while maintaining competitive performance against sensor-rich baselines.

## Method Summary
NaVid employs a video-based large vision language model architecture that processes monocular RGB video sequences to generate next-step navigation actions. The system encodes both historical and current visual observations into spatio-temporal video tokens, which serve as the input representation for the VLM. This encoding captures the temporal evolution of the environment and the agent's trajectory, providing rich contextual information for decision-making. The model outputs discrete navigation actions (forward, turn left, turn right, stop) based on the current video context and the language instruction. The approach is trained on large-scale video-text pairs and fine-tuned on VLN-specific datasets, enabling zero-shot transfer to unseen environments.

## Key Results
- Achieves state-of-the-art performance on VLN benchmarks, outperforming existing methods in both seen and unseen environments
- Demonstrates strong Sim2Real transfer, successfully navigating in real-world indoor environments with 66% success rate across four diverse scenes
- Shows superior cross-dataset generalization, maintaining performance when transferred between different navigation datasets
- Performs navigation tasks using only monocular RGB video input, eliminating the need for additional sensors like depth cameras or odometry

## Why This Works (Mechanism)
NaVid's effectiveness stems from its ability to capture spatio-temporal context through video tokenization, which provides richer environmental information than single-frame processing. By encoding the temporal evolution of observations, the model develops a better understanding of spatial relationships and navigational affordances. The video-based approach allows the model to implicitly learn motion patterns, object permanence, and environmental layouts that are critical for navigation. The VLM architecture enables joint reasoning over visual and linguistic information, allowing the model to ground language instructions in visual observations effectively. The elimination of sensor requirements makes the system more practical for real-world deployment while maintaining competitive performance.

## Foundational Learning
- Vision-and-Language Navigation (VLN): The task of navigating in physical environments based on natural language instructions; needed to understand the problem domain and evaluation metrics.
- Spatio-temporal Video Encoding: The process of converting video sequences into token representations that capture both spatial and temporal information; critical for understanding how NaVid processes visual input.
- Large Vision Language Models (VLMs): Neural architectures that jointly process visual and textual information; fundamental to understanding NaVid's core approach.
- Sim2Real Transfer: The ability of models trained in simulation to perform effectively in real-world environments; important for evaluating practical deployment potential.
- Cross-dataset Generalization: The model's ability to perform well on datasets different from its training data; key metric for assessing robustness.

## Architecture Onboarding

**Component Map:**
Raw RGB Video -> Spatio-temporal Video Encoder -> Video Tokens -> VLM Backbone -> Language Instruction Encoder -> Cross-modal Fusion -> Action Prediction

**Critical Path:**
Video stream captured by monocular camera → Spatio-temporal encoding → VLM processes video tokens and language instruction → Cross-modal attention fusion → Action output generation

**Design Tradeoffs:**
- Sensor simplicity vs. performance: Using only RGB video reduces hardware requirements but may limit performance compared to depth-aware methods
- Model complexity vs. real-time capability: Large VLM architecture provides rich reasoning but may impact inference speed
- Training data diversity vs. specialization: Broader pretraining enables better generalization but may reduce task-specific optimization

**Failure Signatures:**
- Incorrect action prediction when visual context is ambiguous or lighting conditions are poor
- Failure to handle long-range dependencies in instructions due to context length limitations
- Performance degradation in highly dynamic environments with moving obstacles

**First Experiments:**
1. Evaluate NaVid on standard VLN benchmarks (R2R, R4R) to establish baseline performance
2. Test cross-dataset generalization by training on one dataset and evaluating on another
3. Conduct Sim2Real transfer evaluation by deploying in real-world environments after simulation training

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does NaVid handle extremely long navigation instructions (e.g., >100 steps) compared to its performance on shorter instructions?
- Basis in paper: The paper mentions that NaVid experiences a performance drop under very long horizon instructions due to the long context tokens problem.
- Why unresolved: The paper only provides performance analysis up to 90 steps, and the exact performance degradation at longer instruction lengths is not quantified.
- What evidence would resolve it: Detailed performance metrics (success rate, SPL, etc.) for instruction lengths beyond 90 steps, and comparison with state-of-the-art methods on these longer instructions.

### Open Question 2
- Question: Can NaVid's performance be further improved by incorporating additional sensory inputs beyond RGB video, such as depth information or LiDAR data?
- Basis in paper: The paper emphasizes NaVid's ability to achieve state-of-the-art performance using only RGB video, but does not explore the potential benefits of additional sensory inputs.
- Why unresolved: The paper focuses on demonstrating the capabilities of a video-based VLM for VLN, but does not investigate whether incorporating other sensory modalities could enhance performance.
- What evidence would resolve it: Experimental results comparing NaVid's performance with and without additional sensory inputs (e.g., depth, LiDAR) on the same benchmark tasks.

### Open Question 3
- Question: How does NaVid's performance generalize to outdoor environments and more complex, dynamic scenes compared to indoor environments?
- Basis in paper: The paper primarily focuses on indoor environments and does not explicitly address the model's performance in outdoor or dynamic settings.
- Why unresolved: The paper's experiments are limited to indoor scenes, and it is unclear how well NaVid would adapt to the challenges of outdoor environments (e.g., varying lighting, weather conditions, dynamic obstacles).
- What evidence would resolve it: Experimental results evaluating NaVid's performance on outdoor navigation tasks and in dynamic environments, comparing it to existing methods and highlighting its strengths and limitations.

## Limitations
- Performance evaluation is primarily limited to indoor environments, with unclear generalization to outdoor or highly dynamic settings
- 66% real-world success rate indicates a significant 34% failure rate that may be problematic for safety-critical applications
- Computational efficiency and real-time performance constraints are not extensively addressed, which are crucial for practical deployment
- Generalization to languages other than English and complex linguistic structures remains unexplored

## Confidence
- **High confidence**: The core technical approach of using spatio-temporal video tokens for VLN tasks is well-supported by experimental results across multiple datasets and real-world testing
- **Medium confidence**: The claimed superiority over state-of-the-art methods is supported, but some comparisons lack ablation studies to isolate the contribution of the video-based approach from other architectural choices
- **Medium confidence**: The practical deployment implications are promising but require further validation in more diverse and challenging real-world scenarios

## Next Checks
1. Conduct extensive testing in outdoor and large-scale environments to evaluate performance beyond controlled indoor settings
2. Perform detailed computational efficiency analysis including inference latency, memory usage, and power consumption for real-time navigation
3. Test the model's robustness to environmental changes such as varying lighting conditions, dynamic obstacles, and different camera perspectives to assess true generalizability