---
ver: rpa2
title: Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their
  Defenses
arxiv_id: '2406.01288'
source_url: https://arxiv.org/abs/2406.01288
tags:
- arxiv
- llms
- inst
- preprint
- jailbreaking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Improved Few-Shot Jailbreaking (I-FSJ), a
  method that efficiently jailbreaks aligned language models using few-shot demonstrations.
  The core idea involves injecting special system tokens like [/INST] into demonstrations
  and employing demo-level random search from a collected demo pool.
---

# Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses

## Quick Facts
- arXiv ID: 2406.01288
- Source URL: https://arxiv.org/abs/2406.01288
- Authors: Xiaosen Zheng; Tianyu Pang; Chao Du; Qian Liu; Jing Jiang; Min Lin
- Reference count: 40
- Primary result: I-FSJ achieves >80% (mostly >95%) attack success rates on Llama-2-7B and Llama-3-8B without multiple restarts, even when enhanced by strong defenses.

## Executive Summary
This paper introduces Improved Few-Shot Jailbreaking (I-FSJ), a method that efficiently jailbreaks aligned language models using few-shot demonstrations. The core idea involves injecting special system tokens like [/INST] into demonstrations and employing demo-level random search from a collected demo pool. I-FSJ achieves impressive attack success rates against Llama-2-7B and Llama-3-8B models, even when these models are enhanced by strong defenses like perplexity detection and SmoothLLM. The method consistently achieves nearly 100% ASRs across various aligned LLMs and advanced defenses, demonstrating high effectiveness and robustness.

## Method Summary
I-FSJ works by constructing a pool of jailbreaking demonstrations and using a demo-level random search strategy to select and inject these demonstrations into prompts. The method introduces special system tokens (e.g., [/INST]) to structure the demonstrations, making them more effective at bypassing the model's safety mechanisms. Unlike traditional few-shot jailbreaking methods that require multiple restarts, I-FSJ achieves high success rates in a single attempt by carefully selecting and combining demonstrations. The approach is designed to be robust against common defenses, including perplexity-based detection and SmoothLLM, which typically aim to identify and block jailbreaking attempts.

## Key Results
- I-FSJ achieves >80% (mostly >95%) attack success rates on Llama-2-7B and Llama-3-8B without multiple restarts.
- The method maintains high effectiveness even when models are enhanced by strong defenses like perplexity detection and SmoothLLM.
- I-FSJ consistently achieves nearly 100% ASRs across various aligned LLMs and advanced defenses.

## Why This Works (Mechanism)
The mechanism behind I-FSJ's success lies in its ability to exploit the structure and context of few-shot demonstrations. By injecting special system tokens like [/INST], the method creates a clear separation between the demonstration examples and the target prompt, making it harder for the model to distinguish between legitimate and malicious inputs. The demo-level random search ensures that the most effective demonstrations are selected and combined, increasing the likelihood of bypassing the model's safety mechanisms. This approach is particularly effective against defenses that rely on detecting patterns or anomalies in the input, as the structured demonstrations can blend in more naturally with the prompt.

## Foundational Learning
- **Few-shot jailbreaking**: A technique that uses a small number of demonstrations to manipulate the behavior of a language model. *Why needed*: To efficiently jailbreak models without requiring extensive fine-tuning or multiple restarts. *Quick check*: Verify that the method works with minimal demonstrations.
- **System tokens**: Special tokens like [/INST] used to structure prompts and demonstrations. *Why needed*: To create a clear separation between examples and target prompts, making it harder for models to detect jailbreaking attempts. *Quick check*: Test the impact of different system token configurations.
- **Demo-level random search**: A strategy that selects and combines demonstrations from a pool to maximize jailbreaking success. *Why needed*: To identify the most effective demonstrations without exhaustive search. *Quick check*: Evaluate the success rate of random vs. targeted demonstration selection.
- **Perplexity detection**: A defense mechanism that identifies jailbreaking attempts by analyzing the perplexity of the input. *Why needed*: To understand how I-FSJ bypasses this common defense. *Quick check*: Test the method against perplexity-based defenses.
- **SmoothLLM**: A defense that smooths the model's responses to reduce the effectiveness of jailbreaking. *Why needed*: To assess I-FSJ's robustness against advanced defenses. *Quick check*: Evaluate the method's performance against SmoothLLM.
- **Attack success rate (ASR)**: The percentage of successful jailbreaking attempts. *Why needed*: To quantify the effectiveness of I-FSJ. *Quick check*: Measure ASR across different models and defenses.

## Architecture Onboarding
- **Component map**: I-FSJ -> Demonstration Pool -> Demo-level Random Search -> Special System Tokens -> Aligned LLM
- **Critical path**: The method constructs a demonstration pool, selects demonstrations via random search, injects system tokens, and combines them with the target prompt to jailbreak the aligned LLM.
- **Design tradeoffs**: The method trades computational efficiency for effectiveness by using few-shot demonstrations and demo-level random search instead of exhaustive search or fine-tuning.
- **Failure signatures**: The method may fail if the demonstration pool is insufficient or if the system tokens are not properly structured, leading to detection by advanced defenses.
- **First experiments**:
  1. Test I-FSJ with varying numbers of demonstrations to identify the optimal pool size.
  2. Evaluate the impact of different system token configurations on attack success rates.
  3. Assess the method's performance against a broader range of defenses, including adversarial training.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation is primarily focused on Meta's Llama series, limiting generalizability to other model architectures.
- The robustness claims against advanced defenses are supported by experiments with only perplexity detection and SmoothLLM, with limited scope for other defenses.
- The paper does not address potential ethical implications or scalability in real-world scenarios.

## Confidence
- **Effectiveness against Llama-2-7B and Llama-3-8B**: High
- **Generalizability to other models**: Medium
- **Robustness against diverse defenses**: Medium

## Next Checks
1. Test I-FSJ against a broader range of state-of-the-art defenses, including those based on adversarial training and multi-modal input filtering.
2. Evaluate the method's effectiveness on non-Llama models (e.g., GPT-4, Claude) to assess generalizability.
3. Conduct experiments to measure the method's performance under varying prompt lengths and system token configurations to identify potential limitations.