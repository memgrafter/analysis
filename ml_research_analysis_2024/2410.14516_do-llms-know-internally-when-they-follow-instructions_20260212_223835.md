---
ver: rpa2
title: Do LLMs "know" internally when they follow instructions?
arxiv_id: '2410.14516'
source_url: https://arxiv.org/abs/2410.14516
tags:
- instruction
- instruction-following
- arxiv
- dimension
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLMs encode internal representations
  that correlate with instruction-following success. The authors analyze internal
  states across different tokens and layers using linear probes to identify a specific
  dimension in the input embedding space that predicts instruction-following success,
  which they call the "instruction-following dimension." They find that this dimension
  generalizes well across unseen tasks but not across unseen instruction types.
---

# Do LLMs "know" internally when they follow instructions?

## Quick Facts
- arXiv ID: 2410.14516
- Source URL: https://arxiv.org/abs/2410.14516
- Authors: Juyeon Heo; Christina Heinze-Deml; Oussama Elachqar; Kwan Ho Ryan Chan; Shirley Ren; Udhay Nallasamy; Andy Miller; Jaya Narain
- Reference count: 40
- Primary result: LLMs encode internal representations that correlate with instruction-following success, identified through linear probes as an "instruction-following dimension"

## Executive Summary
This paper investigates whether LLMs internally encode representations that correlate with instruction-following success. The authors analyze internal states across different tokens and layers using linear probes to identify a specific dimension in the input embedding space that predicts instruction-following success. They demonstrate that modifying representations along this dimension improves instruction-following success rates without compromising response quality. Further analysis reveals that this dimension is more closely related to prompt phrasing rather than the inherent difficulty of the task or instructions.

## Method Summary
The authors use linear probing on input embeddings from multiple LLaMA and Mistral models to identify an instruction-following dimension that predicts success. They validate this dimension through representation engineering by adjusting embeddings along this direction and measuring improvements in success rates. The IFEval-simple dataset contains 5 instruction types (keywords:existence, keywords:forbidden, keywords:frequency, startend:end checker, detectable content:number placeholders) with 100 tasks each. They conduct sensitivity analysis to interpret the dimension and evaluate performance using AUC scores, success rates, and quality ratios.

## Key Results
- Linear probes identify an instruction-following dimension that generalizes well across unseen tasks (AUC > 0.5) but not across unseen instruction types
- Representation engineering along this dimension improves instruction-following success rates compared to random changes while maintaining response quality
- Sensitivity analysis shows the instruction-following dimension is more closely related to prompt phrasing than task familiarity or instruction difficulty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs encode internal representations that correlate with instruction-following success, identifiable using linear probes
- Core assumption: Internal representations contain sufficient information to distinguish successful from failed instruction-following
- Evidence: Linear probes achieve AUC scores well above chance (0.5) for task generalization

### Mechanism 2
- Claim: Modifying representations along the instruction-following dimension improves success rates without compromising quality
- Core assumption: The identified dimension is causally related to instruction-following behavior
- Evidence: Representation engineering using R_updated = R_original + α × D shows improvements over random adjustments

### Mechanism 3
- Claim: The instruction-following dimension relates more to prompt phrasing than task familiarity or difficulty
- Core assumption: How instructions are phrased affects their internal encoding
- Evidence: Sensitivity analysis reveals stronger correlations with phrasing modifications

## Foundational Learning

- Concept: Linear probing
  - Why needed here: Used to identify the instruction-following dimension by training a simple linear classifier to distinguish success from failure
  - Quick check question: What is the primary purpose of using linear probes in this paper?

- Concept: Representation engineering
  - Why needed here: Validates causal relationship between identified dimension and instruction-following by modifying representations and measuring impact
  - Quick check question: How does representation engineering differ from simply training a better model?

- Concept: AUC (Area Under the ROC Curve)
  - Why needed here: Evaluates linear probe performance in predicting instruction-following success
  - Quick check question: What does an AUC score of 0.5 indicate in linear probe evaluation?

## Architecture Onboarding

- Component map: Data preparation -> Model selection -> Analysis pipeline -> Evaluation metrics
- Critical path: 1) Prepare IFEval-simple dataset 2) Extract input embeddings from multiple tokens/layers 3) Train linear probes 4) Apply representation engineering 5) Conduct sensitivity analysis
- Design tradeoffs: Simple verifiable instructions vs. complex real-world scenarios; task consistency across instruction types; focus on early layers for task generalization
- Failure signatures: Low AUC scores near 0.5; representation engineering failing to improve success rates; sensitivity analysis showing correlations with task familiarity instead of phrasing
- First 3 experiments: 1) Train linear probes and evaluate AUC for task generalization 2) Apply representation engineering to failure cases 3) Conduct sensitivity analysis by modifying prompt phrasing

## Open Questions the Paper Calls Out

- Does the instruction-following dimension generalize to more diverse instruction types beyond the 25 categories tested?
- Is there a fundamental difference in how models encode task familiarity versus instruction-following success?
- Can the instruction-following dimension be made more robust to prompt phrasing variations through targeted fine-tuning?
- How does the instruction-following dimension interact with other known dimensions in LLMs?

## Limitations

- Generalization is asymmetric: the dimension transfers well across unseen tasks but fails to generalize across unseen instruction types
- Analysis relies on a simplified dataset (IFEval-simple) with only 5 instruction types and 100 tasks per type
- Representation engineering modifies only input embeddings, not model weights, limiting potential improvements
- Findings may not extend to larger frontier models beyond the 7B-13B parameter range studied

## Confidence

- High Confidence: Identification of instruction-following dimension using linear probes and validation through representation engineering
- Medium Confidence: Claim that dimension relates more to prompt phrasing than task difficulty
- Low Confidence: Broader implications about fundamental insights into LLM instruction processing

## Next Checks

1. Test cross-model validation by applying instruction-following dimension from one model family to completely different architectures using zero-shot probing

2. Analyze dynamic behavior by tracking how the instruction-following dimension evolves during generation across multiple decoding steps

3. Conduct ablation studies on prompt phrasing by systematically varying phrasing while keeping tasks constant to directly test phrasing's primary role