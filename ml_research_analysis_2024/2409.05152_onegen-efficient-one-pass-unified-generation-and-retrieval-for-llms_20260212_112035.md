---
ver: rpa2
title: 'OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs'
arxiv_id: '2409.05152'
source_url: https://arxiv.org/abs/2409.05152
tags:
- retrieval
- generation
- onegen
- training
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OneGen, a novel framework that unifies generation
  and retrieval tasks for large language models (LLMs). Traditional approaches treat
  these tasks separately, requiring separate models and increasing computational overhead.
---

# OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs

## Quick Facts
- arXiv ID: 2409.05152
- Source URL: https://arxiv.org/abs/2409.05152
- Reference count: 40
- One-line primary result: OneGen unifies generation and retrieval tasks in LLMs, improving performance and efficiency over pipeline approaches

## Executive Summary
This paper introduces OneGen, a novel framework that unifies generation and retrieval tasks for large language models (LLMs) within a single forward pass. Traditional approaches treat these tasks separately, requiring distinct models and increasing computational overhead. OneGen addresses this by incorporating retrieval tokens generated autoregressively into the LLM's output, enabling it to handle both generation and retrieval simultaneously. The framework is evaluated on Retrieval-Augmented Generation (RAG) for question answering and Entity Linking tasks, demonstrating significant improvements in accuracy and efficiency compared to existing methods.

## Method Summary
OneGen is a unified framework that enables LLMs to handle both generation and retrieval tasks in a single forward pass. It fine-tunes pre-trained LLMs with a combined loss function (L = λgLg + λrLr) where Lg is the language model loss and Lr is the contrastive loss for retrieval. The framework introduces special tokens ([RQ] for query representation, [RD] for document representation) and a <CON> token to enable continuous generation. BPR (Bayesian Personalized Ranking) is used as the loss function for retrieval to handle multiple positive samples per batch. The model is trained on reconstructed datasets that include these special tokens, and evaluated on RAG and Entity Linking tasks using accuracy, F1 score, and Micro F1 score metrics.

## Key Results
- OneGen improves RAG performance by 1.5 points on average across four single-hop QA datasets
- OneGen improves RAG performance by 3.3 points on average for multi-hop QA
- OneGen improves Entity Linking accuracy by 3.2 points on average across six out-of-domain datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The [RQ] and [RD] tokens allow the model to learn both generative and retrieval representations within the same context without interfering with each other.
- **Mechanism:** The model uses separate loss functions (Lg for generation, Lr for retrieval) and token roles to ensure that retrieval tokens only participate in representation fine-tuning while other tokens continue with standard language model objectives.
- **Core assumption:** The LLM's inherent encoding capabilities are sufficient to learn meaningful representations for retrieval when trained with contrastive loss.
- **Evidence anchors:**
  - [abstract]: "incorporating retrieval tokens generated autoregressively" and "incorporating retrieval tokens generated autoregressively"
  - [section 3.2]: "For tokens where role(xi) = RET, optimization employs Lr: Lr = 1|D| ..."
  - [corpus]: Weak - no direct corpus evidence found for this specific mechanism
- **Break condition:** If the model cannot learn meaningful representations for retrieval using only contrastive loss on single tokens, or if the retrieval tokens interfere with generation capabilities.

### Mechanism 2
- **Claim:** Using BPR loss instead of InfoNCE enables larger batch sizes and multiple positive samples per batch without requiring complex gradient caching.
- **Mechanism:** BPR's pairwise loss formulation allows gradient accumulation and supports multiple positive samples, making it more efficient than InfoNCE which requires large batch sizes and careful temperature tuning.
- **Core assumption:** BPR is less restrictive than InfoNCE and won't negatively impact generative capabilities.
- **Evidence anchors:**
  - [section 3.2]: "we employ the hyperparameter-free BPR (Rendle et al., 2009), a pair-wise loss function"
  - [section 4.3.3]: "Our results, presented in Table 4, indicate the BPR consistently surpasses InfoNCE in performance"
  - [corpus]: No direct corpus evidence found for this specific comparison
- **Break condition:** If BPR proves insufficient for learning high-quality representations or if it significantly degrades generative performance compared to InfoNCE.

### Mechanism 3
- **Claim:** The unified forward pass eliminates the need for separate retrieval and generation passes, reducing inference time and computational overhead.
- **Mechanism:** By incorporating retrieval tokens directly into the generation process, the model can perform both tasks in a single forward pass rather than requiring separate passes for query generation and document retrieval.
- **Core assumption:** The time saved by eliminating separate passes outweighs any additional computation from including retrieval tokens in the generation process.
- **Evidence anchors:**
  - [abstract]: "enables a single LLM to handle both tasks simultaneously in a unified forward pass"
  - [section 4.3.1]: "OneGen's inference process is efficient, with a notably greater increase in speed as query length extends"
  - [corpus]: No direct corpus evidence found for this specific efficiency claim
- **Break condition:** If the overhead of processing retrieval tokens during generation exceeds the time saved from eliminating separate passes, or if the single-pass approach degrades performance.

## Foundational Learning

- **Concept:** Autoregressive token generation
  - **Why needed here:** The model generates retrieval tokens ([RQ], [RD]) in an autoregressive manner, which requires understanding how next-token prediction works in sequence models
  - **Quick check question:** How does the model decide which token to generate next during the autoregressive process?

- **Concept:** Contrastive learning
  - **Why needed here:** The retrieval component uses contrastive loss (BPR) to learn document and query representations by distinguishing between positive and negative samples
  - **Quick check question:** What is the difference between contrastive loss and standard cross-entropy loss in this context?

- **Concept:** Token role assignment
  - **Why needed here:** Different tokens serve different purposes (generation, context, retrieval) and this affects which loss function is applied to each token
  - **Quick check question:** How does the model determine whether a token should contribute to Lg or Lr during training?

## Architecture Onboarding

- **Component map:** LLM backbone (e.g., Llama2-7B) -> Expanded vocabulary with special tokens ([RQ], [RD], [CON]) -> Two loss functions (Lg for generation, Lr for retrieval) -> Data reconstruction pipeline to add retrieval tokens to training data -> Inference engine that handles unified generation and retrieval

- **Critical path:** Training → Data reconstruction → Token role assignment → Loss computation → Parameter updates → Inference with unified forward pass

- **Design tradeoffs:**
  - Using single model vs. separate models (efficiency vs. specialization)
  - Autoregressive retrieval vs. bidirectional retrieval (simplicity vs. representation quality)
  - BPR vs. InfoNCE (efficiency vs. potential performance)

- **Failure signatures:**
  - Poor retrieval performance indicates issues with contrastive learning or token representation
  - Degraded generation quality suggests retrieval tokens are interfering with language modeling
  - Slow inference may indicate the unified approach isn't providing efficiency benefits

- **First 3 experiments:**
  1. Train a minimal version with only [RQ] tokens and compare to baseline without retrieval
  2. Test different values of λr (retrieval loss weight) to find optimal balance between generation and retrieval
  3. Evaluate inference speed with varying query lengths to verify efficiency gains over pipeline approaches

## Open Questions the Paper Calls Out

- **Question:** How does OneGen perform when trained with parameter-efficient fine-tuning methods like LoRA or QLoRA compared to full-parameter fine-tuning?
  - **Basis in paper:** [explicit] The paper mentions that it remains unknown whether parameter-efficient fine-tuning methods could bring benefits for OneGen training, as the study utilized full-parameter fine-tuning.
  - **Why unresolved:** The paper does not provide any experiments or comparisons with parameter-efficient fine-tuning methods.
  - **What evidence would resolve it:** Experiments comparing OneGen performance using full-parameter fine-tuning vs. LoRA/QLoRA on the same tasks and datasets would provide evidence.

- **Question:** How does OneGen's performance scale with more diverse and extensive training data?
  - **Basis in paper:** [explicit] The paper acknowledges the absence of performance evaluations in more diverse and extensive data scenarios, noting that using more diverse datasets might produce a model with enhanced capabilities.
  - **Why unresolved:** The paper only used limited training data for its experiments, and the impact of more diverse data is not explored.
  - **What evidence would resolve it:** Experiments training OneGen on larger, more diverse datasets and comparing performance to the current results would provide evidence.

- **Question:** How would integrating OneGen with Mixture of Experts (MoE) models affect its performance and efficiency?
  - **Basis in paper:** [explicit] The paper states that the efficacy of OneGen within MoE models has not been tested, and it's possible that MoE architectures could significantly influence the routing of retrieval and generation tasks.
  - **Why unresolved:** The paper does not explore the integration of OneGen with MoE models.
  - **What evidence would resolve it:** Experiments comparing OneGen performance with and without MoE integration on the same tasks and datasets would provide evidence.

- **Question:** What are the underlying mechanisms by which LLMs trained using OneGen achieve simultaneous retrieval and generation in a single forward pass without mutual interference?
  - **Basis in paper:** [explicit] The paper acknowledges that the underlying mechanisms remain unclear.
  - **Why unresolved:** The paper does not provide a detailed analysis of the mechanisms enabling simultaneous retrieval and generation.
  - **What evidence would resolve it:** A theoretical analysis or empirical studies investigating the mechanisms of simultaneous retrieval and generation in OneGen would provide evidence.

## Limitations

- The evaluation relies heavily on synthetic data augmentation (Self-RAG dataset) which may not fully capture real-world complexity
- The paper reports strong performance gains but comparisons are limited to relatively few published approaches
- The ablation studies provide useful insights but don't fully isolate the contributions of individual components like BPR loss

## Confidence

**High confidence:** The core technical contribution of unifying generation and retrieval in a single forward pass is well-demonstrated through both theoretical framework and empirical results. The implementation details for the combined loss function and token role assignment are sufficiently specified for reproduction.

**Medium confidence:** The performance improvements over baseline methods are substantial but rely on comparisons with a limited set of published approaches. The efficiency gains are supported by evidence but lack comprehensive benchmarking across different hardware and workload scenarios.

**Low confidence:** The effectiveness of BPR loss versus alternative contrastive losses like InfoNCE is demonstrated within the paper's experimental setup but lacks external validation. The data reconstruction process introduces complexity that could affect reproducibility without more detailed specifications.

## Next Checks

1. **Ablation study with alternative contrastive losses:** Replicate the main experiments using InfoNCE or other contrastive losses to isolate the specific contribution of BPR to the observed performance gains.

2. **Cross-dataset generalization test:** Evaluate the model on additional RAG and EL datasets not included in the original evaluation to assess robustness and generalizability of the performance improvements.

3. **Efficiency benchmarking across hardware:** Conduct comprehensive timing and memory usage measurements across different GPU configurations and batch sizes to validate the claimed efficiency advantages over pipeline approaches under various deployment scenarios.