---
ver: rpa2
title: 'Preference Consistency Matters: Enhancing Preference Learning in Language
  Models with Automated Self-Curation of Training Corpora'
arxiv_id: '2408.12799'
source_url: https://arxiv.org/abs/2408.12799
tags:
- preference
- dataset
- data
- self-curation
- proxy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of inconsistent annotations in
  preference learning datasets, which arise from variability among annotators and
  the multi-dimensional nature of preferences. The authors propose a self-curation
  method that leverages proxy models trained on the preference dataset to identify
  and exclude data with inconsistent preferences.
---

# Preference Consistency Matters: Enhancing Preference Learning in Language Models with Automated Self-Curation of Training Corpora

## Quick Facts
- arXiv ID: 2408.12799
- Source URL: https://arxiv.org/abs/2408.12799
- Reference count: 33
- Performance improvements up to 33% across various learning algorithms and proxy capabilities

## Executive Summary
This paper addresses a fundamental challenge in preference learning: inconsistent annotations in preference datasets that arise from annotator variability and the multi-dimensional nature of preferences. The authors propose a self-curation method that uses proxy models to identify and exclude data with inconsistent preferences, significantly improving performance across multiple preference learning algorithms. The approach demonstrates up to 33% improvement without relying on heuristics, offering a more reliable solution to preference learning inconsistencies.

## Method Summary
The authors introduce a self-curation method that leverages proxy models trained on preference datasets to detect inconsistent annotations. The approach involves training a proxy model using the Bradley-Terry model to score response quality, then identifying discrepancies between the proxy model's predictions and original annotations to detect inconsistent preferences. The target model (e.g., Llama-2 7B) is then trained using DPO or other preference learning methods on the curated subset of consistent preferences. This automated approach eliminates the need for manual filtering or heuristic-based selection of consistent data.

## Key Results
- Performance improvements of up to 33% across various learning algorithms and proxy capabilities
- Self-curation consistently yields better performance regardless of base model used, as long as the proxy has sufficient capacity
- The method shows significant improvements on in-distribution evaluation while maintaining competitive performance on out-of-distribution tasks

## Why This Works (Mechanism)
The method works by addressing the fundamental issue of inconsistent annotations in preference datasets. By training proxy models to identify and exclude data points where preferences are inconsistent with learned patterns, the approach ensures that the target model learns from more reliable training data. This automated curation process is more effective than heuristic-based filtering because it directly leverages the model's ability to identify patterns of consistency within the preference data itself.

## Foundational Learning
- **Preference learning**: Understanding how to train models to distinguish between preferred and non-preferred responses
  - Why needed: Core capability for training alignment and instruction-following in LLMs
  - Quick check: Can the model rank responses based on quality?

- **Bradley-Terry model**: Probabilistic model for pairwise comparison ranking
  - Why needed: Provides theoretical foundation for preference scoring
  - Quick check: Does the model correctly predict pairwise preferences?

- **Direct Preference Optimization (DPO)**: Optimization method for learning from pairwise preferences
  - Why needed: Standard approach for preference-based fine-tuning
  - Quick check: Does the model improve on preference-based evaluation metrics?

- **Self-curation**: Automated identification and exclusion of inconsistent training data
  - Why needed: Improves data quality without manual intervention
  - Quick check: Does curated data lead to better downstream performance?

- **Proxy model training**: Using a model to identify training data quality
  - Why needed: Enables automated quality assessment of training data
  - Quick check: Does proxy model performance correlate with target model improvement?

- **Preference dataset consistency**: Ensuring annotations reflect coherent preferences
  - Why needed: Foundation for reliable preference learning
  - Quick check: Are annotations internally consistent across similar examples?

## Architecture Onboarding

**Component Map:**
Proxy Model (trained on full dataset) -> Consistency Scoring -> Data Curation -> Target Model Training

**Critical Path:**
1. Train proxy model on full preference dataset
2. Identify inconsistent preferences using proxy predictions
3. Curate dataset by excluding inconsistent examples
4. Train target model on curated dataset using DPO

**Design Tradeoffs:**
- Model capacity vs. computational efficiency for proxy training
- Threshold sensitivity vs. data retention
- In-distribution performance vs. generalization capability

**Failure Signatures:**
- Poor proxy model performance indicates base model lacks capacity to learn consistent patterns
- Overfitting during proxy training suggests excessive epochs (should be single epoch)
- Performance degradation on out-of-distribution tasks may indicate over-optimization

**3 First Experiments:**
1. Train proxy model with single epoch and evaluate on held-out validation set
2. Test different threshold values (including default λ=0) to find optimal curation level
3. Compare performance of curated vs. full dataset across multiple base model sizes

## Open Questions the Paper Calls Out
**Open Question 1**: What is the optimal threshold for self-curation that maximizes preference learning performance? The paper suggests exploring thresholds larger than zero, with preliminary results indicating ~10% data exclusion might be optimal, but systematic experiments are needed.

**Open Question 2**: How does self-curation affect the quality and diversity of responses when evaluated on tasks outside the training distribution? While the method shows strong in-distribution performance, detailed analysis of out-of-distribution response quality and diversity is needed.

**Open Question 3**: How does the capability of the proxy model impact the long-term performance of self-curation? The paper investigates different base models but doesn't explore longitudinal effects of varying proxy capabilities on sustained performance.

## Limitations
- Reliance on proxy model quality fundamentally constrains the method's effectiveness
- Optimal threshold selection (λ) remains ambiguous and dataset-dependent
- The claimed "up to 33%" improvement represents a maximum rather than typical result across all configurations

## Confidence
- Self-curation offers a straightforward and reliable solution: Medium confidence
- Performance improvements across various learning algorithms and proxy capabilities: Medium confidence
- The method is straightforward without relying on heuristics: Low confidence

## Next Checks
1. Conduct ablation studies to determine optimal λ threshold values across different dataset characteristics and model sizes
2. Test method effectiveness with alternative preference learning algorithms beyond DPO
3. Evaluate performance with smaller base models (3B and below) to determine minimum capacity requirements