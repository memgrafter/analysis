---
ver: rpa2
title: Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting
arxiv_id: '2402.12220'
source_url: https://arxiv.org/abs/2402.12220
tags:
- learning
- ne-tuning
- language
- performance
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in parameter-efficient
  fine-tuning (PEFT) methods for large pre-trained models. The authors propose using
  Bayesian transfer learning techniques, specifically Laplace approximations (diagonal
  and Kronecker-factored), to regularize PEFT methods like LoRA.
---

# Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting

## Quick Facts
- **arXiv ID**: 2402.12220
- **Source URL**: https://arxiv.org/abs/2402.12220
- **Authors**: Haolin Chen; Philip N. Garner
- **Reference count**: 40
- **Primary result**: Laplace approximations (diagonal/EWC and Kronecker-factored/KFAC) applied to LoRA regularization mitigate catastrophic forgetting in PEFT across language and speech tasks.

## Executive Summary
This paper addresses catastrophic forgetting in parameter-efficient fine-tuning (PEFT) methods like LoRA by introducing Bayesian transfer learning techniques. The authors propose using Laplace approximations to regularize PEFT methods, specifically applying diagonal (EWC) and Kronecker-factored (KFAC) approximations to preserve pre-training knowledge during fine-tuning. Their approach balances the likelihood of fine-tuning data with a prior that penalizes deviation from pre-trained weights, enabling effective knowledge preservation without degrading task performance.

Experiments on GLUE benchmark and WikiText datasets for language modeling, as well as VCTK dataset for speech synthesis, demonstrate that the proposed method successfully mitigates catastrophic forgetting. The Kronecker-factored approximation outperforms diagonal methods in preserving pre-training knowledge, while maintaining strong fine-tuning performance. The method also shows promise in speech synthesis tasks, maintaining speaker similarity across different accents while adapting to target speakers.

## Method Summary
The authors apply Bayesian transfer learning to LoRA-based PEFT using Laplace approximations. They estimate the Fisher information matrix (FIM) from a subset of pre-training data, then incorporate this into the fine-tuning loss as a regularization term that penalizes deviation from pre-trained weights. Two variants are explored: diagonal (EWC) and Kronecker-factored (KFAC) approximations. The regularization strength λ controls the tradeoff between fine-tuning performance and knowledge preservation. The method is evaluated on language modeling (GLUE benchmark, WikiText) and speech synthesis tasks (VCTK dataset), measuring both task performance and preservation of pre-training capabilities.

## Key Results
- KFAC-based regularization outperforms diagonal methods in preserving pre-training knowledge across both language and speech tasks
- The method successfully mitigates catastrophic forgetting without degrading fine-tuning performance on GLUE benchmark tasks
- In speech synthesis, the approach maintains speaker similarity across different accents while adapting to target speakers
- Varying regularization strength λ reveals an optimal balance between fine-tuning performance and knowledge preservation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian PEFT using Laplace approximations regularizes the parameter shift during LoRA fine-tuning, preserving pre-training knowledge without degrading task performance.
- Mechanism: The Bayesian transfer learning framework maximizes the posterior p(θ|DA, DB) by balancing the likelihood of the fine-tuning data LB(θ) with a prior term that penalizes deviation from the pre-trained weights θ0. This prior is approximated using the Fisher information matrix (FIM), which captures the importance of each parameter for the pre-training task. The Kronecker-factored approximation (KFAC) improves over diagonal methods by modeling parameter interactions within each layer.
- Core assumption: The parameter shift ∆Wl during LoRA fine-tuning can be expressed as a differentiable function of the low-rank matrices Al and Bl, enabling the Bayesian prior to be applied.
- Evidence anchors:
  - [abstract] "We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably."
  - [section] Equation 11: LP EFT (θ) = LB(θ) + λ/L ∑ vec(∆Wl)⊤ Flvec(∆Wl), showing how the parameter shift is incorporated into the loss.
  - [corpus] Weak: No direct corpus evidence for this specific mechanism, but related works on PEFT regularization support the general approach.
- Break condition: If the LoRA parameter shift cannot be expressed differentiably, or if the FIM estimation is inaccurate due to insufficient pre-training data, the regularization may fail.

### Mechanism 2
- Claim: KFAC's Kronecker-factored approximation of the Hessian provides more accurate knowledge preservation than diagonal methods like EWC by capturing parameter interactions within layers.
- Mechanism: KFAC approximates the Fisher information matrix for each layer as a Kronecker product of two smaller matrices, FlKF AC = Al ⊗ Gl. This allows the regularization to account for correlations between parameters, leading to more effective preservation of the pre-training knowledge compared to diagonal approximations that only consider individual parameter variances.
- Core assumption: The Kronecker product approximation accurately represents the parameter interactions within each layer, and the assumption of independence between layers is acceptable for regularization purposes.
- Evidence anchors:
  - [abstract] "using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones."
  - [section] Equation 8-9: FlKF AC = Al ⊗ Gl and the efficient computation of the quadratic penalty term.
  - [corpus] Weak: No direct corpus evidence, but related works on KFAC in continual learning support the claim.
- Break condition: If the Kronecker product approximation is poor for the specific model or task, or if the assumption of layer independence is violated, the KFAC method may not outperform diagonal methods.

### Mechanism 3
- Claim: The regularization strength λ controls the tradeoff between fine-tuning performance and pre-training knowledge preservation, with optimal values balancing both objectives.
- Mechanism: As λ increases, the penalty for deviating from the pre-trained weights becomes stronger, leading to better preservation of the pre-training knowledge but potentially worse fine-tuning performance. The optimal λ is selected based on achieving a good balance between these two objectives, typically where further increasing λ would significantly degrade the fine-tuning performance.
- Core assumption: There exists an optimal λ that balances the fine-tuning and knowledge preservation objectives, and this value can be determined empirically through hyperparameter sweeps.
- Evidence anchors:
  - [abstract] "Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance."
  - [section] Table II: Comparison of performance with varying regularization strength, showing the tradeoff between fine-tuning accuracy and pre-training knowledge preservation.
  - [corpus] Weak: No direct corpus evidence, but the concept of regularization strength controlling the tradeoff is well-established in machine learning.
- Break condition: If the optimal λ is too high, the fine-tuning performance may suffer significantly. If it is too low, the knowledge preservation effect may be insufficient.

## Foundational Learning

- Concept: Laplace approximation for posterior estimation
  - Why needed here: The true posterior over the pre-trained model parameters is intractable, so the Laplace approximation is used to approximate it with a Gaussian distribution around the MAP estimate, enabling the Bayesian transfer learning framework.
  - Quick check question: What is the key insight behind the Laplace approximation, and how does it relate to the Fisher information matrix?

- Concept: Kronecker product and its properties
  - Why needed here: The Kronecker-factored approximation of the Hessian relies on the properties of the Kronecker product to efficiently compute the regularization term, which is crucial for the KFAC method's effectiveness.
  - Quick check question: How does the Kronecker product enable efficient computation of the Hessian approximation, and what are the key assumptions behind this approximation?

- Concept: Low-rank adaptation (LoRA) and parameter-efficient fine-tuning
  - Why needed here: LoRA is the PEFT method used in this work, and understanding its mechanism is essential for applying the Bayesian transfer learning framework to regularize the parameter shift during fine-tuning.
  - Quick check question: What is the key hypothesis behind LoRA, and how does it achieve parameter efficiency compared to full fine-tuning?

## Architecture Onboarding

- Component map: Pre-trained model -> LoRA adapters -> Bayesian regularization -> Fine-tuning task -> Evaluation metrics
- Critical path:
  1. Pre-train the base model on a large dataset
  2. Sample a subset of the pre-training data to estimate the Fisher information matrix (EWC or KFAC)
  3. Initialize LoRA adapters for the fine-tuning task
  4. Fine-tune the model with the Bayesian regularization loss
  5. Evaluate the fine-tuning performance and pre-training knowledge preservation
- Design tradeoffs:
  - Computational cost vs. knowledge preservation: KFAC provides better preservation but requires more pre-training data and computation compared to EWC
  - Fine-tuning performance vs. knowledge preservation: Increasing the regularization strength λ improves knowledge preservation but may degrade fine-tuning performance
  - Parameter efficiency vs. performance: LoRA achieves good performance with fewer trainable parameters compared to full fine-tuning, but may not match the best performance of full fine-tuning
- Failure signatures:
  - Poor knowledge preservation: High perplexity on pre-training data after fine-tuning, significant degradation in zero-shot performance for speech synthesis
  - Degraded fine-tuning performance: Lower accuracy or higher perplexity on the fine-tuning task compared to unregularized LoRA
  - Unstable training: Large fluctuations in the loss or metrics during fine-tuning, indicating an inappropriate choice of λ or Hessian estimation
- First 3 experiments:
  1. Fine-tune a pre-trained OPT model on a text classification task with LoRA and no regularization, then evaluate the forgetting of pre-training knowledge by measuring the perplexity on a held-out pre-training dataset.
  2. Repeat experiment 1 with EWC regularization, varying the regularization strength λ to find the optimal balance between fine-tuning performance and knowledge preservation.
  3. Repeat experiment 2 with KFAC regularization, comparing its performance to EWC and analyzing the impact of the Kronecker product approximation on knowledge preservation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Bayesian PEFT methods scale with increasingly larger model sizes (e.g., 1B, 10B, 100B parameters)?
- Basis in paper: [inferred] The paper tests on OPT-350M and OPT-1.3B models and notes that larger models forget less, but does not explore beyond this scale.
- Why unresolved: The paper explicitly states that larger TTS models were inaccessible due to hardware constraints, leaving the scalability question open.
- What evidence would resolve it: Systematic experiments on progressively larger models (1B, 10B, 100B+ parameters) measuring catastrophic forgetting mitigation effectiveness and computational overhead.

### Open Question 2
- Question: Can the Bayesian PEFT framework be extended to regularization methods beyond LoRA, such as adapters or prefix-tuning, and what are the trade-offs?
- Basis in paper: [explicit] The paper states that methods like (IA)3 and Orthogonal Butterfly "fit in the framework" but would require extra computation, and that bottleneck adapters cannot be directly regularized.
- Why unresolved: The paper only implements and tests LoRA, leaving the generalization to other PEFT methods as a theoretical possibility without empirical validation.
- What evidence would resolve it: Implementation and comparative experiments of Bayesian regularization on various PEFT methods (adapters, prefix-tuning, etc.) measuring both forgetting mitigation and computational/memory costs.

### Open Question 3
- Question: What is the minimum amount of pre-training data required to accurately estimate the Hessian for effective Bayesian regularization?
- Basis in paper: [explicit] The paper conducts experiments with varying sample sizes (20, 200, 2000, 20000) and finds KFAC requires more data than EWC for accurate estimation.
- Why unresolved: While the paper explores sample size effects, it does not determine a definitive minimum threshold or provide guidelines for different model sizes and tasks.
- What evidence would resolve it: A comprehensive study varying pre-training data sizes across different model architectures and tasks to establish minimum effective sample sizes and identify diminishing returns.

## Limitations
- Empirical evaluation is limited to specific model architectures (OPT and StyleTTS 2) and datasets, raising questions about generalizability
- Incomplete specification of StyleTTS 2 Hessian estimation implementation details, particularly regarding which modules are included
- Subjective evaluation for speech synthesis relies on Prolific listeners without detailed qualification criteria or sample selection processes documented
- Computational overhead of KFAC estimation and its scalability to larger models remains unexplored

## Confidence
- **High Confidence**: The core finding that Bayesian regularization via Laplace approximations can mitigate catastrophic forgetting in LoRA-based PEFT is well-supported by experimental results across both language and speech tasks.
- **Medium Confidence**: The claim that regularization strength λ controls the tradeoff between fine-tuning performance and knowledge preservation is supported by experimental results, but optimal selection methodology could benefit from more rigorous analysis.
- **Low Confidence**: The assertion that KFAC's performance advantage stems specifically from capturing parameter interactions within layers lacks direct ablation studies or analysis to confirm this mechanism.

## Next Checks
1. **Ablation study on Hessian approximation quality**: Compare KFAC against full Hessian approximation (where computationally feasible) to isolate whether the Kronecker product structure or the overall approximation quality drives performance improvements.

2. **Generalization test across model architectures**: Apply the Bayesian PEFT framework to additional pre-trained models (e.g., BERT, GPT-2, Whisper) and tasks beyond those studied to assess the method's broader applicability and identify potential failure modes.

3. **Scalability analysis**: Evaluate the computational overhead and memory requirements of KFAC vs EWC on larger model scales (e.g., OPT-6.7B, T5-XXL) to determine practical limitations and identify potential optimization strategies for large-scale deployment.