---
ver: rpa2
title: 'LRM-Zero: Training Large Reconstruction Models with Synthesized Data'
arxiv_id: '2406.09371'
source_url: https://arxiv.org/abs/2406.09371
tags:
- data
- training
- reconstruction
- lrm-zero
- zeroverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LRM-Zero, a Large Reconstruction Model trained
  entirely on procedurally synthesized 3D data, achieving high-quality sparse-view
  3D reconstruction competitive with models trained on real-world data. The core of
  LRM-Zero is Zeroverse, a procedural 3D dataset created from simple primitive shapes
  with random texturing and augmentations like height fields, boolean differences,
  and wireframes.
---

# LRM-Zero: Training Large Reconstruction Models with Synthesized Data

## Quick Facts
- arXiv ID: 2406.09371
- Source URL: https://arxiv.org/abs/2406.09371
- Authors: Desai Xie; Sai Bi; Zhixin Shu; Kai Zhang; Zexiang Xu; Yi Zhou; Sören Pirk; Arie Kaufman; Xin Sun; Hao Tan
- Reference count: 40
- Primary result: Large Reconstruction Model trained entirely on procedurally synthesized 3D data achieves competitive sparse-view reconstruction quality

## Executive Summary
This paper introduces LRM-Zero, a Large Reconstruction Model trained entirely on procedurally synthesized 3D data called Zeroverse. Unlike previous 3D datasets that approximate real-world data, Zeroverse ignores global semantics but contains rich geometric and texture details. The authors demonstrate that LRM-Zero achieves reconstruction quality similar to models trained on Objaverse, with only a 1.12 PSNR gap on the GSO benchmark. This work shows that 3D reconstruction can potentially be addressed without real-world semantics, offering a solution to data scarcity and licensing issues in 3D vision.

## Method Summary
LRM-Zero is trained on Zeroverse, a procedural 3D dataset created from simple primitive shapes with random texturing and augmentations including height fields, boolean differences, and wireframes. The model uses a GS-LRM architecture (pure-transformer based, predicts Gaussian Splatting parameters) and is trained for 80K steps on 64 A100 GPUs. The training pipeline involves generating synthetic 3D objects, rendering multi-view images, training the transformer to predict Gaussian Splatting parameters, and evaluating on real-world benchmarks. Key design choices include reducing perceptual loss weight from 0.5 to 0.2 for stability and using gradient clipping to prevent divergence.

## Key Results
- LRM-Zero trained on Zeroverse achieves 30.52 PSNR on GSO benchmark, only 1.12 lower than Objaverse-trained model (31.64 PSNR)
- Boolean difference augmentation improves metrics significantly, especially for thin structures
- Wireframe augmentation enables reconstruction of objects with thin structures that would otherwise fail
- Training with 400K Zeroverse objects is sufficient for competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local geometric and texture details in synthetic data are sufficient to train a large reconstruction model to reconstruct real-world objects.
- Mechanism: The model learns to predict shape from multi-view images by exploiting local correspondences (e.g., cross-view patch matching) rather than global semantic understanding.
- Core assumption: 3D reconstruction tasks depend more on local visual features than on global semantics, especially in sparse-view settings.
- Evidence anchors: [abstract] "Zeroverse completely ignores realistic global semantics but is rich in complex geometric and texture details that are locally similar to or even more intricate than real objects."

### Mechanism 2
- Claim: Procedural augmentation of simple primitives improves reconstruction performance by introducing realistic geometric diversity.
- Mechanism: Augmentations add curvature, concavity, and thin structures that the model must learn to represent, improving generalization to real objects with similar features.
- Core assumption: Real-world objects contain curved surfaces, concave regions, and thin structures; synthetic models lacking these fail to reconstruct such shapes.
- Evidence anchors: [section] "We find that each type of augmentation provides visible structural improvements for the reconstructions, and most of the improvements are reflected in the metrics."

### Mechanism 3
- Claim: Co-design of dataset complexity and model hyperparameters stabilizes training of large reconstruction models on synthetic data.
- Mechanism: Matching the complexity of synthetic data (via augmentation ratios) with appropriate model and training configurations prevents gradient explosion and divergence.
- Core assumption: Synthetic data can be more complex than real data; model architecture and training hyperparameters must be tuned jointly for stability.
- Evidence anchors: [section] "As both data complexity and model hyperparameters can affect the training stability, a model-data co-design is helpful in our experiments, i.e., the model's hyperparameters and data properties are tuned jointly."

## Foundational Learning

- Concept: Sparse-view 3D reconstruction and its dependence on local geometric cues.
  - Why needed here: The success of LRM-Zero relies on the insight that sparse-view reconstruction can be solved without semantic understanding, using local geometry and texture.
  - Quick check question: What is the minimum number of views needed for a reconstruction model to hallucinate missing geometry without semantic priors?

- Concept: Procedural data generation and augmentation for 3D shapes.
  - Why needed here: Zeroverse is built by composing primitives and applying augmentations; understanding this pipeline is essential for modifying or extending the dataset.
  - Quick check question: How do height fields, boolean differences, and wireframes each contribute to geometric diversity in synthetic 3D data?

- Concept: Training stability in large-scale neural networks, especially transformers.
  - Why needed here: The paper highlights that improper data or hyperparameter choices can cause training instability; engineers must recognize and mitigate such risks.
  - Quick check question: What are common signs of training instability (e.g., gradient explosion) in transformer-based 3D reconstruction models?

## Architecture Onboarding

- Component map: Primitive shape sampling -> Texturing -> Augmentation (height field, boolean diff, wireframe) -> Rendering -> Patchification -> Transformer -> Gaussian Splatting parameters
- Critical path: Generate synthetic 3D objects with diverse geometry/texture → Render multi-view images (32 views per object, 512x512) → Train transformer to predict Gaussian Splatting parameters → Fine-tune at higher resolution → Evaluate on real-world benchmarks
- Design tradeoffs: Data complexity vs. training stability; Model size vs. convergence; Local detail vs. semantic generalization
- Failure signatures: Training divergence (gradient explosion); Poor reconstruction of concave or thin structures; Overfitting to synthetic textures; Underperformance on sparse-view benchmarks
- First 3 experiments: 1) Train LRM-Zero with no augmentations; verify baseline performance drop. 2) Add height field augmentation only; measure impact on reconstruction quality. 3) Jointly vary augmentation ratios and perceptual loss weight; observe stability and performance trade-offs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact boundary between semantic and intrinsic tasks in 3D vision, and how can we determine whether a task requires semantic understanding or can be learned from procedural data?
- Basis in paper: The paper discusses that 3D reconstruction relies more on local visual clues than global semantics, but acknowledges that tasks like single-view reconstruction require semantic understanding.
- Why unresolved: This is a fundamental theoretical question about the nature of 3D vision tasks that requires empirical validation across multiple tasks and datasets to establish clear criteria for what constitutes semantic versus intrinsic information.

### Open Question 2
- Question: How can we design procedural 3D data that captures real-world semantic distributions while maintaining the benefits of synthetic data?
- Basis in paper: The authors note that Zeroverse lacks global semantics and may not be suitable for semantic-rich tasks.
- Why unresolved: Current procedural methods create data without semantic structure, but real-world applications require semantic understanding. Finding the right balance between semantic content and procedural generation remains an open challenge.

### Open Question 3
- Question: What is the optimal training configuration for LRM models on procedural data, and how does it differ from training on real-world data?
- Basis in paper: The authors found that procedural data requires different hyperparameters (e.g., reduced perceptual loss weight, different data mixing ratios).
- Why unresolved: The paper only explored a limited range of hyperparameters and data scales. The complex interaction between data complexity, model architecture, and training dynamics for procedural data remains underexplored.

## Limitations
- The paper doesn't address potential domain gaps in texture appearance or lighting conditions between synthetic and real data
- Analysis of why synthetic data generalizes is primarily theoretical rather than empirically validated through controlled ablation studies
- Stability claims depend heavily on careful hyperparameter tuning with limited discussion of sensitivity to dataset composition changes

## Confidence

- **High Confidence**: The core empirical finding that LRM-Zero trained on Zeroverse achieves competitive reconstruction performance (within 1.12 PSNR of Objaverse-trained model) is well-supported by benchmark results.
- **Medium Confidence**: The mechanism explaining why local geometric details suffice for reconstruction (local cues vs. global semantics) is plausible but lacks direct empirical validation through controlled experiments.
- **Medium Confidence**: The dataset augmentation contributions are demonstrated through qualitative examples and metric improvements, but the analysis doesn't systematically isolate individual augmentation effects or their interactions.

## Next Checks
1. Conduct controlled experiments varying view sparsity (1-4 views) to test the hypothesis that reconstruction depends primarily on local geometric cues rather than global semantics.
2. Systematically ablate each augmentation type (height fields, boolean differences, wireframes) while keeping others constant to isolate their individual contributions to reconstruction quality.
3. Test training stability across a broader range of dataset complexities by varying augmentation ratios and measuring convergence behavior and final performance trade-offs.