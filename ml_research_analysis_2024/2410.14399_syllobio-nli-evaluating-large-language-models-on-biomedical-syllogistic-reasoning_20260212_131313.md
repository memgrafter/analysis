---
ver: rpa2
title: 'SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning'
arxiv_id: '2410.14399'
source_url: https://arxiv.org/abs/2410.14399
tags:
- syllogistic
- reasoning
- generalized
- pathway
- member
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SylloBio-NLI, a framework for evaluating
  large language models (LLMs) on biomedical syllogistic reasoning. It leverages external
  ontologies to generate 28 syllogistic argument schemes instantiated with human genome
  pathways, assessing LLMs on textual entailment and premise selection tasks.
---

# SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning

## Quick Facts
- arXiv ID: 2410.14399
- Source URL: https://arxiv.org/abs/2410.14399
- Reference count: 40
- Zero-shot LLMs achieve 23-70% accuracy on biomedical syllogistic reasoning tasks

## Executive Summary
This paper introduces SylloBio-NLI, a framework for evaluating large language models (LLMs) on biomedical syllogistic reasoning. It leverages external ontologies to systematically instantiate 28 syllogistic argument schemes with human genome pathways from Reactome, assessing models on textual entailment and premise selection tasks. Experiments reveal that zero-shot LLMs perform poorly, with accuracies ranging from 23% for disjunctive syllogism to 70% for generalized modus ponens. Few-shot prompting shows inconsistent improvements across models, with some like Gemma improving by +14% while others like Llama-3 improve by +43%. The study highlights significant sensitivity to lexical variations and overall poor robustness, indicating current LLMs are far from achieving the consistency required for safe biomedical NLI applications.

## Method Summary
The SylloBio-NLI framework generates biomedical syllogistic arguments by leveraging Reactome ontology to instantiate 28 logical argument schemes with real gene-pathway relationships. The study evaluates 8 open-source LLMs including Gemma-7B, Llama-3, Mistral-7B, Mixtral-8x7B, and BioMistral-7B using both zero-shot and few-shot prompting strategies. Models are tested on two tasks: textual entailment (determining if conclusions follow from premises) and premise selection (identifying relevant premises from distractors). Performance is measured using accuracy, F1-score, recall, precision, reasoning accuracy, and faithfulness metrics. The framework also includes robustness analysis through lexical variations and distractor experiments to assess model sensitivity to superficial changes in biomedical text.

## Key Results
- Zero-shot LLMs achieve poor performance (23-70% accuracy) across all 28 syllogistic schemes
- Few-shot prompting improves performance inconsistently (+14% for Gemma, +43% for Llama-3)
- Models show high sensitivity to superficial lexical variations and struggle with robustness
- Gemma-7b exhibits strong negative correlation (r = -0.951, p < 0.001) between reasoning accuracy and distractor complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: External ontologies like Reactome enable systematic generation of domain-specific syllogistic arguments
- Mechanism: Ontologies provide structured relationships (e.g., gene-pathway memberships) that can be directly mapped onto formal syllogistic schemes, ensuring both logical validity and domain soundness
- Core assumption: The hierarchical and taxonomic structure of biomedical ontologies is sufficiently detailed to instantiate all 28 syllogistic variants without requiring additional expert annotation
- Evidence anchors:
  - [abstract] "leverages external ontologies to systematically instantiate diverse syllogistic arguments for biomedical NLI."
  - [section] "Reactome’s hierarchical structure... enables a systematic instantiation of the syllogistic arguments."
  - [corpus] Weak - no direct citations from related papers but domain knowledge mapping is supported by Reactome literature
- Break condition: If the ontology lacks sufficient granularity or missing intermediate pathway levels, instantiation fails for certain schemes

### Mechanism 2
- Claim: Syllogistic reasoning is content-independent; models should generalize reasoning patterns regardless of domain-specific knowledge
- Mechanism: By evaluating across different biomedical predicates and entities, the study tests whether models abstract the underlying logical forms rather than memorizing factual associations
- Core assumption: Models trained on general corpora have been exposed to enough logical reasoning patterns to transfer to specialized domains
- Evidence anchors:
  - [abstract] "the ability to perform syllogistic reasoning should be content-independent... independent of its concrete instantiation."
  - [section] "the syllogistic schemes... are designed to assess the interpretation of fine-grained logical operators including quantifiers, implications and negation."
  - [corpus] Assumption - related work shows models often fail to generalize beyond training data (Eisape et al., Kim et al.)
- Break condition: If models rely heavily on factual correlations rather than logical form, performance drops when factual correctness is removed

### Mechanism 3
- Claim: Few-shot prompting can elicit reasoning patterns not present in zero-shot settings
- Mechanism: Providing exemplars guides the model to follow the logical structure required for syllogistic inference, reducing reliance on surface patterns
- Core assumption: The few-shot exemplars are sufficiently representative of the target logical forms and the model can generalize from them
- Evidence anchors:
  - [abstract] "we found that few-shot prompting can boost the performance of different LLMs, including Gemma (+14%) and LLama-3 (+43%)."
  - [section] "FS improves for all schemes only for Gemma-7b and Llama-3."
  - [corpus] Assumption - related work (Wei et al., Huang & Chang) supports ICL effectiveness, but inconsistency here suggests domain specificity matters
- Break condition: If exemplars are too domain-specific or not representative, models fail to generalize or performance may degrade

## Foundational Learning

- Concept: First-order logic (FOL) representation of syllogisms
  - Why needed here: Formal schemes in FOL provide the precise logical structure that must be mapped to natural language
  - Quick check question: Can you translate "Every member of F is a member of G" into ∀x(Fx → Gx)?

- Concept: Domain ontologies and hierarchical relationships
  - Why needed here: Ontologies supply the entities and predicates required to instantiate syllogisms with realistic biomedical content
  - Quick check question: What is the relationship between a pathway and its sub-pathways in Reactome?

- Concept: Prompt engineering (zero-shot vs few-shot)
  - Why needed here: Different prompting strategies significantly affect model performance on logical inference tasks
  - Quick check question: What key difference in output format is expected between Task 1 and Task 2 prompts?

## Architecture Onboarding

- Component map: Ontological data loader → Syllogistic scheme generator → NL template mapper → LLM evaluator → Metrics aggregator
- Critical path: Data loading → Instantiation → Prompt formatting → LLM inference → Result evaluation
- Design tradeoffs: High ontology coverage vs. manual annotation effort; Few-shot exemplars vs. model overfitting to examples
- Failure signatures: Empty outputs, irrelevant text, CoT-style reasoning when instructions demand single-token answers
- First 3 experiments:
  1. Load Reactome pathways and verify gene-pathway membership extraction
  2. Instantiate one syllogistic scheme (e.g., generalized modus ponens) and check NL output
  3. Run zero-shot evaluation on a single model and verify metric calculation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on biomedical syllogistic reasoning compare to their performance on syllogistic reasoning in general domain datasets?
- Basis in paper: [inferred] The paper focuses exclusively on biomedical domain syllogistic reasoning, comparing zero-shot and few-shot settings but not benchmarking against general domain performance
- Why unresolved: The authors did not conduct comparative experiments using general domain syllogistic reasoning datasets like GLoRE or other benchmarks
- What evidence would resolve it: Direct comparison experiments measuring LLM performance on both biomedical and general domain syllogistic reasoning tasks using identical models and evaluation protocols

### Open Question 2
- Question: Can fine-tuning LLMs on biomedical syllogistic reasoning data improve their performance beyond what's achievable with few-shot prompting?
- Basis in paper: [explicit] The paper only evaluates zero-shot and few-shot prompting approaches, finding that few-shot prompting improves performance inconsistently across models
- Why unresolved: The authors did not explore fine-tuning as an intervention strategy for improving biomedical syllogistic reasoning capabilities
- What evidence would resolve it: Controlled experiments comparing few-shot prompting versus fine-tuning on biomedical syllogistic reasoning datasets, measuring performance across the 28 syllogistic schemes

### Open Question 3
- Question: What specific aspects of model architecture (beyond size and pre-training regime) contribute to differences in biomedical syllogistic reasoning performance?
- Basis in paper: [inferred] The paper notes that performance varies across different model families (Llama, Mistral, Gemma) and pre-training regimes but doesn't analyze architectural differences
- Why unresolved: The authors did not conduct architectural ablation studies or analyze which specific architectural features correlate with better syllogistic reasoning performance
- What evidence would resolve it: Systematic comparison of models with varying architectural features (attention mechanisms, activation functions, etc.) while controlling for size and pre-training data, measuring their impact on syllogistic reasoning accuracy

## Limitations
- Performance significantly varies across models (23-70% accuracy), indicating current LLMs lack robust logical reasoning capabilities
- Significant sensitivity to superficial lexical variations suggests poor robustness in biomedical applications
- Inconsistent effectiveness of few-shot prompting across different models indicates this approach may not be universally reliable

## Confidence
- High confidence: The mechanism that external ontologies enable systematic generation of domain-specific syllogistic arguments is well-supported by the structured nature of Reactome and the successful instantiation of all 28 schemes
- Medium confidence: The claim that syllogistic reasoning is content-independent receives mixed support - while models should theoretically generalize reasoning patterns, the significant performance gaps suggest practical limitations in current LLMs
- Medium confidence: The few-shot prompting mechanism shows inconsistent results across models, with improvements ranging from +14% to +43%, indicating context-dependent effectiveness

## Next Checks
1. Test the ontology-based instantiation pipeline with a different biomedical database (e.g., KEGG or BioCyc) to verify that Reactome coverage is not artificially limiting syllogistic scheme instantiation
2. Conduct ablation studies on the few-shot exemplars to determine which specific examples most effectively improve reasoning performance across different model architectures
3. Implement a controlled experiment varying the complexity of ontological relationships while holding logical form constant to isolate whether performance degradation stems from logical complexity or domain knowledge requirements