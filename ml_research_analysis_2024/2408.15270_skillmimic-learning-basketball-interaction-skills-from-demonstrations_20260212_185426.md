---
ver: rpa2
title: 'SkillMimic: Learning Basketball Interaction Skills from Demonstrations'
arxiv_id: '2408.15270'
source_url: https://arxiv.org/abs/2408.15270
tags:
- skills
- interaction
- skill
- learning
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SkillMimic, a data-driven framework for learning
  diverse basketball interaction skills from demonstrations. The core innovation is
  a unified Human-Object Interaction (HOI) imitation reward that eliminates the need
  for skill-specific reward engineering, enabling a single policy to master multiple
  basketball skills including dribbling, layup, shooting, and ball pickup.
---

# SkillMimic: Learning Basketball Interaction Skills from Demonstrations

## Quick Facts
- arXiv ID: 2408.15270
- Source URL: https://arxiv.org/abs/2408.15270
- Reference count: 40
- Key outcome: A unified HOI imitation reward eliminates skill-specific reward engineering while achieving higher success rates than baseline methods

## Executive Summary
SkillMimic introduces a data-driven framework for learning diverse basketball interaction skills from demonstrations using a unified Human-Object Interaction (HOI) imitation reward. The approach eliminates the need for skill-specific reward engineering by combining kinematic, relative motion, and contact graph rewards multiplicatively. Trained on two basketball datasets (BallPlay-V and BallPlay-M), SkillMimic achieves significantly higher success rates than baseline methods and demonstrates strong generalization as training data scales.

## Method Summary
SkillMimic uses reinforcement learning with state-only trajectories to learn basketball interaction skills from demonstrations. The method employs a unified HOI imitation reward combining kinematic, relative motion, and contact graph components multiplicatively. A Contact Graph reward explicitly models binary contact states between nodes (hands, body, ball) to ensure physically plausible interactions. The approach uses a single policy network (3-layer MLP) trained with PPO in Isaac Gym simulator, with an adaptive velocity regularization to suppress unnatural motions.

## Key Results
- 86.7% pickup success rate versus 0% for AMP*-style rewards
- 80.25% success rate for high-level task composition (consecutive scoring)
- Strong generalization demonstrated as training data scales from 1 to 131 clips for pickup skill
- Unified policy achieves comparable performance to skill-specific policies without per-skill tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified HOI imitation reward eliminates skill-specific reward engineering while maintaining learning performance
- Core assumption: Multiplicative combination of reward components prevents dominance by any single component
- Evidence: Unified reward achieves comparable performance to skill-specific approaches
- Break condition: Reward scaling issues could lead to imbalanced learning

### Mechanism 2
- Claim: Contact Graph reward provides critical supervision for precise physical interactions
- Core assumption: Physical contact cannot be reliably inferred from kinematics alone
- Evidence: Kinematic rewards alone yield poor HOI performance
- Break condition: Noisy contact state detection could destabilize learning

### Mechanism 3
- Claim: Skill diversity and generalization improve with larger demonstration datasets
- Core assumption: More diverse demonstrations provide better state-action space coverage
- Evidence: Performance improves as dataset grows from 1 to 40 clips
- Break condition: Insufficient policy capacity for increased complexity

## Foundational Learning

- Concept: Reinforcement Learning with State-Only Trajectories
  - Why needed: More tolerant to noise and data-efficient than behavior cloning
  - Quick check: What distinguishes state-only from state-action trajectory learning?

- Concept: Contact Modeling for Physical Interactions
  - Why needed: Kinematic rewards cannot ensure proper physical contact
  - Quick check: Why are kinematic rewards insufficient for HOI tasks?

- Concept: Multi-Skill Policy Learning
  - Why needed: Single policy must handle multiple distinct basketball skills
  - Quick check: What challenges arise in multi-skill policy learning?

## Architecture Onboarding

- Component map: Motion capture/vision → SMPL-X/object tracking → HOI state transitions → Random skill initialization → State observation → IS policy → Physics simulation → Unified HOI imitation reward → Skill evaluation → HLC training

- Critical path: Data collection → HOI state extraction → IS policy training with unified reward → Skill evaluation → HLC training for high-level tasks

- Design tradeoffs: Unified vs. skill-specific rewards (engineering effort vs. fine-tuning capability), contact graph complexity (precision vs. computational overhead), policy capacity (skill diversity vs. training requirements)

- Failure signatures: Kinematic local optima (incorrect body parts for control), mode collapse (repetitive behaviors), contact errors (physically implausible interactions), reward imbalance (biased skill development)

- First 3 experiments:
  1. Train IS policy on single skill with/without contact graph reward to verify contact modeling importance
  2. Train mixed skills vs. individual skills to test skill switching and cross-learning benefits
  3. Scale training data from 1 to 40 clips for pickup skill to validate generalization improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SkillMimic's performance scale when learning HOI skills from datasets with multiple object types of varying geometries?
- Basis: Paper acknowledges need for more general environmental perception approaches
- Why unresolved: Experiments focus on single basketball object
- Resolution evidence: Experiments on multi-object datasets with varied geometries

### Open Question 2
- Question: What is the minimum HOI demonstration data required, and how does performance degrade with noisy or partial demonstrations?
- Basis: Paper shows quantity effects but not quality effects
- Why unresolved: Doesn't investigate data quality or minimum requirements
- Resolution evidence: Systematic experiments varying data quality and quantity

### Open Question 3
- Question: How well does SkillMimic transfer to real-world humanoid robots?
- Basis: Authors identify as important future direction but provide no experimental results
- Why unresolved: Only demonstrates simulation results
- Resolution evidence: Real-world experiments on humanoid robots with performance analysis

## Limitations

- Limited to basketball-specific interactions with a single ball object
- No real-world validation or analysis of sim-to-real transfer challenges
- Exact implementation details of Contact Graph reward and hyperparameter values remain unclear

## Confidence

- **High Confidence**: Success rate improvements over baselines, scalability benefits, high-level task composition
- **Medium Confidence**: Unified reward effectiveness across all skills, contact graph modeling necessity
- **Low Confidence**: Generalization beyond basketball domain, scalability to more complex scenarios

## Next Checks

1. Conduct ablation studies on unified reward components to quantify individual contributions
2. Test approach on non-basketball interaction tasks to validate cross-domain generalization
3. Systematically vary λcg parameter to analyze contact graph reward sensitivity