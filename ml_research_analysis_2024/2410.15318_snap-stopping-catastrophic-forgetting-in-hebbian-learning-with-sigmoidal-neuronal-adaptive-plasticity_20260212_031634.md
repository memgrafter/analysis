---
ver: rpa2
title: 'SNAP: Stopping Catastrophic Forgetting in Hebbian Learning with Sigmoidal
  Neuronal Adaptive Plasticity'
arxiv_id: '2410.15318'
source_url: https://arxiv.org/abs/2410.15318
tags:
- weight
- growth
- learning
- layer
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Sigmoidal Neuronal Adaptive Plasticity (SNAP)
  to address catastrophic forgetting in neural networks by mimicking biological long-term
  potentiation. Unlike standard linear weight updates, SNAP uses sigmoid-shaped weight
  growth where synapses consolidate and stabilize at high strengths, reducing their
  plasticity.
---

# SNAP: Stopping Catastrophic Forgetting in Hebbian Learning with Sigmoidal Neuronal Adaptive Plasticity

## Quick Facts
- arXiv ID: 2410.15318
- Source URL: https://arxiv.org/abs/2410.15318
- Reference count: 40
- Hebbian networks with neuron-wise sigmoid growth prevent forgetting completely across sequential tasks

## Executive Summary
This work introduces Sigmoidal Neuronal Adaptive Plasticity (SNAP) to address catastrophic forgetting in neural networks by mimicking biological long-term potentiation. Unlike standard linear weight updates, SNAP uses sigmoid-shaped weight growth where synapses consolidate and stabilize at high strengths, reducing their plasticity. Two variants are tested: synapse-wise (individual weights) and neuron-wise (shared within neurons). Experiments on sequential MNIST and FashionMNIST tasks show that Hebbian networks with neuron-wise sigmoidal growth completely prevent forgetting, achieving near-perfect retention across tasks. Linear and exponential growth perform significantly worse, with linear showing moderate forgetting.

## Method Summary
SNAP modifies weight updates in neural networks by applying a sigmoid scaling factor that reduces plasticity as weights grow larger, mimicking biological long-term potentiation consolidation. The method implements two variants: synapse-wise where each weight has independent plasticity, and neuron-wise where all weights to a neuron share plasticity based on their collective norm. The scaling factors are |Wij|(1 - |Wij|) for synapse-wise and ∥Wi∥²(1 - ∥Wi∥²) for neuron-wise. The approach is tested on sequential MNIST and FashionMNIST tasks where networks learn two-digit classes sequentially, measuring retention across all classes after training.

## Key Results
- Hebbian networks with neuron-wise sigmoidal growth achieve near-perfect retention across all sequential tasks
- Linear weight growth shows moderate forgetting, while exponential growth performs poorly
- SNAP provides minimal benefit for SGD-based learning as weights often consolidate before reaching critical strengths
- Synapse-wise SNAP offers only slight improvement over linear growth in preventing forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sigmoidal weight growth mimics biological LTP by reducing plasticity at high weight strengths, stabilizing learned information.
- Mechanism: Weight updates are scaled by |Wij|(1 - |Wij|) for synapse-wise or ∥Wi∥²(1 - ∥Wi∥²) for neuron-wise, so large weights grow slowly and small weights grow quickly.
- Core assumption: Biological LTP reduces synaptic plasticity after consolidation, preventing catastrophic forgetting.
- Evidence anchors:
  - [abstract] "synapses consolidate and stabilize at high strengths, reducing their plasticity."
  - [section] "during LTP, learning does not strengthen further, but the acquired knowledge becomes resistant to forgetting."
- Break condition: If weights never reach the stabilization zone due to insufficient training or inappropriate hyperparameters, forgetting will still occur.

### Mechanism 2
- Claim: Synapse-wise vs neuron-wise SNAP differs in plasticity allocation, with neuron-wise offering uniform plasticity per neuron and synapse-wise offering fine-grained control.
- Mechanism: Synapse-wise applies the sigmoid scaling per individual weight, while neuron-wise applies it to the norm of all incoming weights to a neuron.
- Core assumption: Consolidation of weights can be modeled at either the individual synapse level or the collective neuron input level.
- Evidence anchors:
  - [section] "In the first, synapse-wise SNAP... plasticity is defined at the level of each individual weight... In the second, neuron-wise SNAP... plasticity is defined at the level of the neuron."
  - [section] "we can turn any learning rule which provides weight changes δWij into a rule which has sigmoidal weight growth by applying weight changes ∆Wij instead where ∆Wij = |Wij|(1 − |Wij|)δWij."
- Break condition: Neuron-wise may oversimplify plasticity patterns if individual synapses require different consolidation thresholds.

### Mechanism 3
- Claim: SNAP prevents catastrophic forgetting in Hebbian learning but not in SGD because Hebbian learning can stabilize consolidated weights, whereas SGD continues to shift weights during new tasks.
- Mechanism: In Hebbian learning, once weights consolidate under SNAP, lateral inhibition and orthogonal projection prevent interference; in SGD, ongoing gradient updates can still disrupt consolidated weights.
- Core assumption: Hebbian learning dynamics allow stabilization after consolidation, while SGD does not.
- Evidence anchors:
  - [abstract] "SNAP provides only slight improvements for SGD-based learning, as weights often consolidate before reaching critical strengths."
  - [section] "we see that for Hebbian learning, neuron-wise Sigmoidal weight growth completely prevents any forgetting of previous tasks while training on new tasks. The same cannot be said of SGD trained networks."
- Break condition: If SGD training is stopped before significant forgetting occurs, SNAP might appear effective, but this is not true continual learning.

## Foundational Learning

- Concept: Hebbian learning and its extension to Sanger's rule
  - Why needed here: The paper compares Hebbian learning with SGD and uses Sanger's rule for the hidden layer, so understanding the basic Hebbian update and its orthogonal extension is critical.
  - Quick check question: What is the difference between basic Hebbian learning and Sanger's rule in terms of weight updates?
- Concept: Lateral inhibition and sparse activation
  - Why needed here: The model uses lateral inhibition (parameterized by λ) to enforce sparse hidden layer activations, which is essential for contrastive Hebbian learning and preventing interference.
  - Quick check question: How does increasing λ affect the sparsity of hidden layer activations in the model?
- Concept: Catastrophic forgetting and continual learning
  - Why needed here: The core problem SNAP addresses is catastrophic forgetting, so understanding why standard linear weight growth fails in sequential task learning is foundational.
  - Quick check question: Why does linear weight growth cause catastrophic forgetting when training on sequential tasks?

## Architecture Onboarding

- Component map:
  - Input layer: 784 units (MNIST/FashionMNIST flattened)
  - Hidden layer: 64 or 96 units with Hebbian learning + lateral inhibition
  - Output layer: 10 units with supervised Hebbian learning
  - Weight growth: Linear, sigmoid, or exponential applied synapse-wise or neuron-wise
- Critical path:
  1. Forward pass with lateral inhibition to compute hidden activations
  2. Compute weight updates using Hebbian/Sanger rule for hidden, supervised Hebbian for output
  3. Apply SNAP scaling to weight updates
  4. Update weights
  5. Repeat for each batch/task
- Design tradeoffs:
  - Synapse-wise SNAP offers fine-grained control but increases memory and computation per weight
  - Neuron-wise SNAP is simpler and more biologically plausible but may miss important synapse-level dynamics
  - Sigmoid growth prevents forgetting but may slow learning if consolidation occurs too early
- Failure signatures:
  - High variance in weight updates → unstable learning, possible divergence
  - Weights saturate at 0 or 1 too quickly → no learning occurs
  - Forgetting still occurs despite SNAP → insufficient consolidation or inappropriate hyperparameters
- First 3 experiments:
  1. Train a simple Hebbian network on i.i.d. MNIST with linear weight growth to establish baseline
  2. Repeat with sigmoid weight growth (both synapse-wise and neuron-wise) to verify learning capability
  3. Test on sequential MNIST tasks to observe forgetting with and without SNAP

## Open Questions the Paper Calls Out

- Question: How does SNAP's performance vary across different network architectures beyond the tested MLP?
  - Basis in paper: [inferred] The experiments only tested MLP with one hidden layer; no mention of convolutional networks or deeper architectures.
  - Why unresolved: The paper's experiments are limited to a specific network architecture, and no theoretical analysis is provided about SNAP's behavior in other architectures.
  - What evidence would resolve it: Experiments comparing SNAP's performance across different architectures (CNNs, ResNets, deeper MLPs) on the same sequential learning tasks.

- Question: Why does SNAP prevent catastrophic forgetting in Hebbian learning but only marginally help in SGD-based learning?
  - Basis in paper: [explicit] "When we look into why it solve catastrophic forgetting for SGD, it is because the network reaches high accuracies before the weights have grown to be close to 1 in size, i.e. before the weights reach their consolidation phase."
  - Why unresolved: The paper identifies this timing issue but doesn't explore potential modifications to SNAP that could make it more effective for SGD.
  - What evidence would resolve it: Experiments testing modified SNAP variants that trigger consolidation at different weight thresholds or with different timing mechanisms for SGD.

- Question: What is the optimal threshold for weight consolidation in SNAP to balance plasticity and stability?
  - Basis in paper: [inferred] The paper uses sigmoid functions but doesn't explore different threshold values or functions for consolidation.
  - Why unresolved: The paper uses standard sigmoid functions without investigating how different consolidation points might affect learning and forgetting.
  - What evidence would resolve it: Systematic experiments varying the consolidation threshold in the sigmoid function and measuring the trade-off between new learning and retention of old tasks.

## Limitations
- SNAP's effectiveness is specific to Hebbian learning dynamics and may not generalize to other architectures
- The biological fidelity of sigmoid weight growth as a model of LTP consolidation lacks direct empirical validation
- Performance on SGD is limited because weights consolidate before reaching critical strengths for effective learning

## Confidence
- High: SNAP prevents catastrophic forgetting in Hebbian learning with neuron-wise sigmoid growth
- Medium: SNAP provides only marginal benefits for SGD due to premature weight consolidation
- Low: The biological fidelity of sigmoid weight growth as a model of LTP consolidation

## Next Checks
1. Test SNAP with different lateral inhibition strengths (λ) to determine sensitivity thresholds and identify the minimum λ required for effective forgetting prevention.
2. Compare SNAP performance against modern continual learning baselines (e.g., Elastic Weight Consolidation, Synaptic Intelligence) in both Hebbian and SGD settings to contextualize its effectiveness.
3. Implement SNAP with varying sigmoid steepness parameters to explore the tradeoff between consolidation speed and learning capacity, determining optimal curves for different task complexities.