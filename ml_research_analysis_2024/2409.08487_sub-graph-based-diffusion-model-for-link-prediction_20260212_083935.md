---
ver: rpa2
title: Sub-graph Based Diffusion Model for Link Prediction
arxiv_id: '2409.08487'
source_url: https://arxiv.org/abs/2409.08487
tags:
- diffusion
- graph
- link
- prediction
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a sub-graph based diffusion model for link
  prediction, treating it as conditional likelihood estimation of enclosing sub-graphs.
  The key innovation is decomposing the likelihood estimation via Bayesian formula
  to separate sub-graph structure and node feature estimation, enabling both inductive
  learning and strong generalization.
---

# Sub-graph Based Diffusion Model for Link Prediction

## Quick Facts
- arXiv ID: 2409.08487
- Source URL: https://arxiv.org/abs/2409.08487
- Reference count: 40
- Primary result: State-of-the-art cross-dataset link prediction with rank 1.4 and strong generalization from only 1% training data

## Executive Summary
This paper introduces a sub-graph based diffusion model (SGD IFF) for link prediction that treats the task as conditional likelihood estimation of enclosing sub-graphs. The key innovation is a Bayesian decomposition that separates sub-graph structure and node feature estimation, enabling cross-dataset transfer without retraining. By combining structure diffusion for graph topology and feature diffusion for node attributes, the model achieves state-of-the-art performance on cross-dataset link prediction, demonstrates strong generalization with limited training data (only 1% of training set), and shows improved robustness against adversarial attacks compared to discriminative baselines.

## Method Summary
The method treats link prediction as conditional likelihood estimation of enclosing sub-graphs using a Bayesian decomposition: p(y|A, X) = p(X|A, y)·p(A|y)·p(y) / Σc p(A, X|y=c)·p(y=c). This separates structure estimation (p(A|y)) from feature estimation (p(X|A, y)), allowing the structure component to be shared across datasets while adapting only the feature component per dataset. The model uses discrete status transition noise for structure diffusion and Gaussian noise for feature diffusion, with a transformer-based network for denoising. The final connection probability is computed by fusing structure and feature likelihood scores using learnable parameters.

## Key Results
- Achieves rank 1.4 on cross-dataset link prediction, outperforming previous state-of-the-art methods
- Maintains strong performance with only 1% of training data, demonstrating excellent generalization
- Shows improved robustness against adversarial attacks compared to discriminative baselines like GNNs and VGAE
- Enables cross-dataset transfer without retraining by sharing structure components across datasets

## Why This Works (Mechanism)

### Mechanism 1
The Bayesian decomposition allows separation of graph structure and node feature modeling, enabling cross-dataset transfer. By decomposing p(y|A, X) into p(X|A, y)·p(A|y)·p(y) / Σc p(A, X|y=c)·p(y=c), the model separates structure generation (p(A|y)) from node features (p(X|A, y)). This allows sharing the structure component across datasets while adapting only the feature component per dataset. Core assumption: node features are incompatible across datasets but graph structure patterns are transferable.

### Mechanism 2
Diffusion models can be adapted for discrete graph structures using discrete status transition noise while maintaining graph-theoretic properties. The structure diffusion model uses discrete status transition noise to maintain sparsity and connectivity during forward/reverse Markov chains. Orbit features (DRNL) distinguish sub-graphs with similar adjacency matrices but different center node locations. Core assumption: discrete noise transitions can effectively capture graph structure distribution.

### Mechanism 3
Combining structure and feature diffusion models with learned fusion parameters provides robustness against adversarial attacks. The model estimates both log P(A|y) and log P(X|A, y) separately, then fuses them using learnable parameters {η1, η2, δ} to compute final connection probabilities. This dual modeling approach provides more robust likelihood estimation than single-modality approaches. Core assumption: modeling both components independently provides complementary information that enhances robustness.

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPMs)**: Understanding forward/reverse diffusion processes and ELBO optimization is essential for adapting DDPMs to link prediction. Quick check: What are the key differences between forward and reverse processes in DDPMs, and how does the ELBO objective work?

- **Graph Neural Networks and Sub-graph Learning**: SGNNs are used for sub-graph representation learning, requiring understanding of how node features are augmented with structural features and how pairwise relations are captured. Quick check: How do sub-graph GNNs like SEAL address limitations of vanilla GNNs for link prediction?

- **Bayesian Theorem and Likelihood Decomposition**: The core innovation involves decomposing likelihood estimation using Bayesian formula, requiring understanding of conditional probabilities and separation of structure from feature modeling. Quick check: How does the Bayesian decomposition enable cross-dataset transfer?

## Architecture Onboarding

- **Component map**: Sub-graph Generator -> Structure Diffusion Model -> Feature Diffusion Model -> Fusion Layer -> Connection Probability

- **Critical path**: 1) Generate k-hop enclosing sub-graph S for node pair (u,v) 2) Apply structure diffusion to estimate log P(A|y) 3) Apply feature diffusion to estimate log P(X|A, y) 4) Fuse scores using learnable parameters {η1, η2, δ} 5) Apply softmax to obtain connection probability P(y|A, X)

- **Design tradeoffs**: Discrete vs continuous noise (discrete preserves graph properties but may be harder to optimize); Transformer vs GCN for structure (transformer handles global dependencies but has higher computational cost); Fusion parameters vs fixed weights (learnable fusion provides flexibility but adds parameters)

- **Failure signatures**: Poor cross-dataset transfer (indicates structure component not sufficiently generalizable); Training instability (may indicate issues with discrete noise transitions or improper ELBO calculation); Adversarial vulnerability (suggests fusion parameters not effectively balancing components)

- **First 3 experiments**: 1) Cross-dataset transfer test: Train on Cora, test on Citeseer/Pubmed/Router/NS/USAir 2) Limited training data test: Reduce training set to 1% and measure performance 3) Adversarial attack test: Apply random flip and embedding attacks to evaluate robustness

## Open Questions the Paper Calls Out

- **Open Question 1**: How does transferability performance change when using larger subgraphs (k > 1) for structure diffusion? The paper only tests k=1 hop subgraphs, leaving the impact of larger subgraphs on cross-dataset transferability unexplored.

- **Open Question 2**: What is the impact of different orbit feature choices beyond DRNL on structure diffusion model's performance? The paper mentions "There are many choices for the orbit features" but only tests DRNL.

- **Open Question 3**: How does SGD IFF perform on dynamic graphs where structure evolves over time? All experiments use static graphs, leaving temporal dynamics and concept drift unaddressed.

- **Open Question 4**: What is the minimum required training data size for SGD IFF to outperform discriminative baselines? The paper tests 1% training data but doesn't explore the threshold where generative approaches become superior.

## Limitations
- Cross-dataset transfer relies heavily on assumption that graph structure patterns are transferable while node features are not, which may not hold for all graph types
- Computational cost of generating k-hop enclosing sub-graphs with DRNL orbit features may limit scalability to very large graphs
- Effectiveness of discrete status transition noise for graph structures needs more thorough validation across diverse graph types

## Confidence
- **High confidence**: Basic framework of using diffusion models for link prediction and experimental methodology (cross-dataset transfer, limited training data tests)
- **Medium confidence**: Bayesian decomposition approach and claimed advantages for cross-dataset transfer (strong experimental validation but theoretical justification could be more rigorous)
- **Low confidence**: Robustness claims against adversarial attacks (only two attack types tested, comparison with generative baselines could be more comprehensive)

## Next Checks
1. **Cross-dataset structure transferability validation**: Conduct ablation studies where graph structure patterns are deliberately modified across datasets to test if claimed transferability advantage holds under varying degrees of structural similarity
2. **Scalability analysis**: Evaluate performance and training time on progressively larger graphs (from Cora/Citeseer size to 10K+ node graphs) to quantify computational overhead of sub-graph generation and orbit feature computation
3. **Adversarial robustness stress test**: Systematically test against multiple attack types (metattack, graph injection, feature perturbation) across varying attack strengths to comprehensively validate claimed robustness advantages