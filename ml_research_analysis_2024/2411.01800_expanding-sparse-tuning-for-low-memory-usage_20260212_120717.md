---
ver: rpa2
title: Expanding Sparse Tuning for Low Memory Usage
arxiv_id: '2411.01800'
source_url: https://arxiv.org/abs/2411.01800
tags:
- memory
- snell
- usage
- performance
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SNELL addresses high memory usage in sparse tuning by decomposing
  the tunable matrix into two low-rank matrices, avoiding costly storage of the original
  matrix and eliminating the need to store weight indexes. It extends LoRA with nonlinear
  kernel functions to increase the rank of the merged matrix, improving performance.
---

# Expanding Sparse Tuning for Low Memory Usage

## Quick Facts
- arXiv ID: 2411.01800
- Source URL: https://arxiv.org/abs/2411.01800
- Authors: Shufan Shen; Junshu Sun; Xiangyang Ji; Qingming Huang; Shuhui Wang
- Reference count: 40
- Primary result: SNELL achieves state-of-the-art performance on 24 visual tasks with LoRA-level memory usage

## Executive Summary
SNELL (Sparse tuning with kerNELized LoRA) addresses the high memory usage problem in sparse tuning by decomposing the tunable matrix into two low-rank matrices, eliminating the need to store weight indexes through a competition-based sparsification mechanism. It extends LoRA with nonlinear kernel functions to increase the rank of the merged matrix, improving performance while maintaining low memory consumption. Experiments show SNELL outperforms existing parameter-efficient methods on 24 downstream visual tasks while using comparable memory to LoRA.

## Method Summary
SNELL is a parameter-efficient fine-tuning method that decomposes the adaptation matrix into two learnable low-rank matrices (B and A) combined with a nonlinear kernel function. The method uses competition-based sparsification to select task-relevant weights based on their absolute values during end-to-end optimization, avoiding storage of binary masks. By applying nonlinear kernel functions to the merging process, SNELL increases the effective rank of the merged matrix, improving performance while maintaining memory efficiency. The method is evaluated on vision Transformer backbones across FGVC and VTAB-1k benchmark tasks.

## Key Results
- Achieves state-of-the-art performance on 24 visual tasks while maintaining LoRA-level memory usage
- Successfully scales to large models including ViT-L/16 and ViT-H/14
- Demonstrates effectiveness across diverse downstream tasks with varying data scales
- Outperforms existing sparse tuning methods including BitFit and LoRA in both performance and memory efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SNELL decomposes the tunable matrix into two low-rank matrices to reduce memory usage by avoiding storage of the original full matrix
- Mechanism: Instead of storing the entire adaptation matrix, SNELL stores only two smaller low-rank matrices B ∈ Rm×r and A ∈ Rn×r. The merged adaptation matrix is computed as ∆W = BϕA⊤ where ϕ is a nonlinear kernel function
- Core assumption: The nonlinear kernel function can approximate the full-rank adaptation matrix well enough while using only low-rank components
- Evidence anchors:
  - [abstract]: "SNELL decomposes the tunable matrix for sparsification into two learnable low-rank matrices, saving from the costly storage of the whole original matrix"
  - [section 3.2]: "Compared to storing the whole adaptation matrix, storing low-rank matrices in the optimizer results in lower memory usage"
  - [corpus]: Weak evidence - no direct citations found for this specific decomposition mechanism
- Break condition: If the kernel function cannot accurately reconstruct the full-rank adaptation matrix, performance will degrade significantly

### Mechanism 2
- Claim: The competition-based sparsification mechanism selects task-relevant weights based on absolute values without additional storage
- Mechanism: Weights compete based on their absolute values during end-to-end optimization. A dynamic threshold ∆ws (the weight with the ⌈smn⌉-th smallest absolute value) is used to zero out less important weights, eliminating the need to store a binary mask
- Core assumption: Weights that contribute more to loss reduction will naturally develop larger absolute values during optimization
- Evidence anchors:
  - [abstract]: "A competition-based sparsification mechanism is further proposed to avoid the storage of tunable weight indexes"
  - [section 3.3]: "Our objective is to encourage the weights to compete based on their contributions to performance improvement"
  - [corpus]: Weak evidence - no direct citations found for this specific competition-based approach
- Break condition: If the absolute value does not correlate well with weight importance for the downstream task, the sparsification will select suboptimal weights

### Mechanism 3
- Claim: Applying nonlinear kernel functions to the merging process increases the rank of the merged matrix, improving performance
- Mechanism: Instead of using the linear kernel (inner product) like LoRA, SNELL uses nonlinear kernel functions κ(Bi,·, Aj,·) = ϕ(Bj,·)⊤ϕ(Ai,·) to map to higher-dimensional spaces, creating a higher-rank merged matrix
- Core assumption: Higher-rank merged matrices can capture more complex relationships and improve sparse tuning performance
- Evidence anchors:
  - [abstract]: "we extend the low-rank decomposition by applying nonlinear kernel functions to the whole-matrix merging. Consequently, we gain an increase in the rank of the merged matrix"
  - [section 3.2]: "By replacing κl(·, ·) with more complex non-linear kernel functions, we can approximate relations in higher-dimensional spaces Rd and obtain matrices with rank larger than r"
  - [section 4.3]: "Compared with the linear kernel function, nonlinear kernel functions can reconstruct the full-rank target matrix more accurately based on the low-rank matrices"
- Break condition: If the nonlinear kernel function is too complex, it may increase optimization difficulty and hurt training stability

## Foundational Learning

- Concept: Low-rank matrix decomposition
  - Why needed here: SNELL stores two low-rank matrices instead of the full adaptation matrix to reduce memory usage
  - Quick check question: If we have a matrix of size 1000×1000 and use rank 8 decomposition, how many parameters do we store compared to the original matrix?

- Concept: Kernel trick and Mercer's theorem
  - Why needed here: Nonlinear kernel functions are used to increase the effective rank of the merged matrix while still using low-rank components
  - Quick check question: What property must a kernel function satisfy according to Mercer's theorem to be valid for this approach?

- Concept: Sparsification and binary masks
  - Why needed here: Understanding why traditional sparse tuning requires additional memory for storing weight indexes, which SNELL avoids
  - Quick check question: In traditional sparse tuning, if we have a 1000×1000 weight matrix with 10% sparsity, how much additional memory is needed to store the binary mask?

## Architecture Onboarding

- Component map: Pre-trained weight matrix W0 → [B, A optimization] → [Kernel merging] → [Competition-based sparsification] → ∆Ws → W = W0 + ∆Ws
- Critical path: W0 → [B, A optimization] → [Kernel merging] → [Competition-based sparsification] → ∆Ws → W = W0 + ∆Ws
- Design tradeoffs:
  - Memory vs. Performance: Lower rank r saves memory but may hurt performance; nonlinear kernels help but add complexity
  - Sparsity ratio s: Higher sparsity saves memory but may lose important weights; must be tuned per task
  - Kernel function choice: Simple kernels are stable but less expressive; complex kernels are more powerful but harder to optimize
- Failure signatures:
  - Memory usage not reduced: Check if B and A are being stored correctly instead of full matrix
  - Poor performance: Verify kernel function is appropriate and sparsification threshold is well-chosen
  - Training instability: Check if kernel function is too complex or learning rate is inappropriate
- First 3 experiments:
  1. Verify memory savings: Compare memory usage of SNELL vs. LoRA vs. full fine-tuning on a small model
  2. Test kernel effectiveness: Compare performance of linear vs. nonlinear kernels with fixed sparsity
  3. Validate sparsification: Test different sparsity ratios on a single task to find optimal balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of kernel function affect the performance of SNELL across different types of downstream tasks (e.g., natural images vs. specialized images)?
- Basis in paper: [explicit] The paper discusses the use of different kernel functions, such as piecewise linear, sigmoid, and RBF, and compares their effectiveness in fitting randomly generated matrices and in fine-tuning pre-trained models. The results suggest that the piecewise linear kernel performs better than the linear kernel, while the sigmoid and RBF kernels lead to severe performance degradation.
- Why unresolved: While the paper provides some insights into the performance of different kernel functions, it does not extensively explore how these choices impact performance across various types of downstream tasks. This leaves open the question of whether certain kernel functions are more suitable for specific task categories.
- What evidence would resolve it: Conducting experiments that systematically evaluate the performance of different kernel functions across a diverse set of downstream tasks, particularly those with distinct characteristics, would provide evidence to determine the optimal kernel function choices for each task type.

### Open Question 2
- Question: What is the impact of the sparsity ratio on the generalization ability of SNELL across different downstream tasks?
- Basis in paper: [explicit] The paper discusses the use of a sparsity ratio to control the number of tunable parameters in SNELL. It mentions that different tasks may have different optimal sparsity ratios, but it does not extensively explore how the sparsity ratio affects the generalization ability of the model.
- Why unresolved: While the paper provides some insights into the optimal sparsity ratio for individual tasks, it does not investigate how the sparsity ratio influences the model's ability to generalize to unseen data across different tasks. This leaves open the question of whether there is an optimal sparsity ratio that balances performance and generalization.
- What evidence would resolve it: Conducting experiments that evaluate the generalization ability of SNELL with different sparsity ratios across multiple downstream tasks would provide evidence to determine the impact of the sparsity ratio on the model's ability to generalize.

### Open Question 3
- Question: How does SNELL scale to even larger models, such as those with billions of parameters, and what are the computational challenges associated with this scaling?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of SNELL on models of varying sizes, including ViT-L/16 and ViT-H/16. However, it does not explore the scalability of SNELL to models with billions of parameters or discuss the computational challenges that may arise in such cases.
- Why unresolved: While the paper provides evidence of SNELL's scalability to moderately large models, it does not address the potential challenges and limitations that may arise when scaling to models with billions of parameters. This leaves open the question of whether SNELL can effectively handle such large-scale models and what computational considerations need to be taken into account.
- What evidence would resolve it: Conducting experiments that evaluate the performance and computational efficiency of SNELL on models with billions of parameters, along with an analysis of the associated computational challenges, would provide evidence to determine the scalability of SNELL to extremely large models.

## Limitations
- The competition-based sparsification mechanism relies on the assumption that absolute weight values correlate with importance, which may not hold for all architectures
- Scalability claims to extremely large models (billions of parameters) are not thoroughly validated
- Memory savings calculations may not account for all practical overheads including gradient storage and optimizer states

## Confidence

**High Confidence:** The core memory reduction mechanism through low-rank matrix decomposition is well-established and mathematically sound.

**Medium Confidence:** The performance improvements from nonlinear kernel functions are supported by experimental results, but the relationship between kernel choice, rank, and performance could benefit from more extensive ablation studies.

**Low Confidence:** The generalizability of the competition-based sparsification mechanism across different domains and model architectures is not thoroughly validated.

## Next Checks

1. **Ablation study on kernel function choice:** Systematically test different nonlinear kernel functions (RBF, polynomial, piecewise linear with varying parameters) across multiple model architectures and task types to establish which kernels work best for which scenarios.

2. **Robustness to weight initialization:** Evaluate SNELL's performance when weights are initialized with different strategies (random, pre-trained, zero-initialized) to test whether the competition mechanism consistently identifies important weights regardless of initialization.

3. **Memory usage in production settings:** Measure actual GPU memory consumption during training with SNELL versus baseline methods, including all overheads (gradients, optimizer states, activation memory) to validate claimed memory savings under realistic conditions.