---
ver: rpa2
title: 'Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2'
arxiv_id: '2408.05147'
source_url: https://arxiv.org/abs/2408.05147
tags:
- saes
- gemma
- layer
- loss
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Gemma Scope introduces a comprehensive suite of over 400 JumpReLU
  sparse autoencoders (SAEs) trained on all layers and sublayers of Google's Gemma
  2 models (2B, 9B, and 27B), totaling more than 30 million learned features. The
  primary goal is to make large-scale SAE research more accessible to the broader
  community by releasing trained weights and evaluation results under a permissive
  license.
---

# Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2

## Quick Facts
- arXiv ID: 2408.05147
- Source URL: https://arxiv.org/abs/2408.05147
- Reference count: 40
- Primary result: Released over 400 JumpReLU sparse autoencoders (SAEs) trained on all layers and sublayers of Google's Gemma 2 models (2B, 9B, and 27B), totaling more than 30 million learned features

## Executive Summary
Gemma Scope introduces a comprehensive suite of over 400 JumpReLU sparse autoencoders (SAEs) trained on all layers and sublayers of Google's Gemma 2 models (2B, 9B, and 27B), totaling more than 30 million learned features. The primary goal is to make large-scale SAE research more accessible to the broader community by releasing trained weights and evaluation results under a permissive license. The SAEs were trained on 4-16B tokens each, using over 20% of GPT-3's training compute, and are designed to decompose neural network activations into interpretable features.

## Method Summary
The researchers trained over 400 JumpReLU sparse autoencoders across all layers and sublayers of Google's Gemma 2 models (2B, 9B, and 27B). Each SAE was trained on 4-16B tokens, with the total compute expenditure exceeding 20% of GPT-3's training budget. The autoencoders were designed to decompose neural network activations into interpretable features, with the project focusing on accessibility by releasing both weights and evaluation results under a permissive license. The training covered residual stream, attention, and MLP sublayers across different model sizes.

## Key Results
- Residual stream SAEs demonstrated higher delta loss than attention or MLP SAEs
- SAE performance varied significantly by sequence position within the models
- Over 30 million features were learned across all trained autoencoders
- Transferability between base and instruction-tuned models showed moderate feature stability

## Why This Works (Mechanism)
Assumption: The JumpReLU activation function encourages sparse, interpretable feature representations by creating sharp nonlinearities that promote selective neuron activation. The large-scale training (4-16B tokens per SAE) allows the autoencoders to capture diverse feature patterns across different layers and sublayers of the Gemma models.

## Foundational Learning
- Sparse Autoencoders: Why needed - to decompose neural activations into interpretable features; Quick check - verify sparsity metrics and feature reconstruction quality
- JumpReLU Activation: Why needed - to encourage sparse, interpretable feature representations; Quick check - compare feature distributions with standard ReLU
- Delta Loss Evaluation: Why needed - to measure SAE reconstruction quality; Quick check - validate against alternative reconstruction metrics
- Feature Transferability: Why needed - to assess generalization across model variants; Quick check - test feature stability across additional model pairs
- Sequence Position Effects: Why needed - to understand positional biases in feature learning; Quick check - analyze feature activation patterns across different token positions

## Architecture Onboarding

Component map:
Token inputs -> Gemma 2 models (2B/9B/27B) -> Residual/Attention/MLP layers -> SAE encoders -> Feature decompositions

Critical path:
Token processing through Gemma 2 -> Layer activation extraction -> SAE encoding -> Feature interpretation

Design tradeoffs:
- JumpReLU vs standard activation functions (sparsity vs representational capacity)
- Training compute allocation (4-16B tokens per SAE vs full dataset training)
- Model coverage (all layers vs selective layer training)
- Evaluation metrics (delta loss vs semantic interpretability)

Failure signatures:
- Low delta loss but poor feature interpretability
- High computational cost without proportional interpretability gains
- Feature instability across similar model variants
- Positional biases that limit general applicability

First experiments:
1. Validate feature reconstruction quality on held-out data
2. Compare feature interpretability across different SAE widths
3. Test transferability to smaller/larger models within the Gemma family

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out open questions, suggesting either the authors consider the current work comprehensive or the open questions section was not included in the available summary.

## Limitations
- Scale limitations may not capture full feature diversity from full GPT-3 scale training
- JumpReLU activation choices may bias feature representations
- Evaluation metrics may not fully capture semantic interpretability
- Feature transferability generalization to other architectures remains uncertain

## Confidence

High confidence:
- Technical implementation details and training scale are well-documented
- Release of weights and evaluation results is verifiable

Medium confidence:
- Comparative performance metrics (delta loss differences) are measurable
- Interpretability and feature quality assessments use standard but potentially incomplete metrics

## Next Checks

1. Cross-validate the transferability claims by testing feature stability across additional model pairs with varying architectures and training objectives
2. Conduct ablation studies on the JumpReLU activation function to quantify how architectural choices affect feature quality and interpretability
3. Implement alternative evaluation metrics beyond delta loss (such as feature clustering coherence or downstream task performance) to assess whether residual stream SAE superiority holds under different evaluation frameworks