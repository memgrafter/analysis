---
ver: rpa2
title: 'SPLATE: Sparse Late Interaction Retrieval'
arxiv_id: '2404.13950'
source_url: https://arxiv.org/abs/2404.13950
tags:
- retrieval
- splate
- late
- sparse
- colbertv2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the efficiency bottleneck in late interaction
  retrieval models like ColBERTv2, which require re-ranking large numbers of candidate
  documents. The authors propose SPLATE (Sparse Late Interaction Retrieval), a method
  that adapts frozen ColBERTv2 embeddings using a lightweight SPLADE module to generate
  sparse representations.
---

# SPLATE: Sparse Late Interaction Retrieval

## Quick Facts
- arXiv ID: 2404.13950
- Source URL: https://arxiv.org/abs/2404.13950
- Reference count: 40
- Primary result: Sparse late interaction retrieval achieves comparable effectiveness to PLAID ColBERTv2 while re-ranking only 50 documents in under 10ms

## Executive Summary
This paper addresses the efficiency bottleneck in late interaction retrieval models like ColBERTv2, which require re-ranking large numbers of candidate documents. The authors propose SPLATE (Sparse Late Interaction Retrieval), a method that adapts frozen ColBERTv2 embeddings using a lightweight SPLADE module to generate sparse representations. This allows candidate generation to use traditional sparse retrieval techniques rather than approximate dense scoring.

SPLATE achieves comparable effectiveness to PLAID ColBERTv2 by re-ranking only 50 documents, which can be retrieved in under 10ms. The approach is particularly appealing for CPU environments as it leverages traditional sparse retrieval infrastructure. The method is evaluated on MS MARCO, BEIR, and LoTTE benchmarks, showing strong performance across in-domain and out-of-domain scenarios while maintaining the interpretable nature of sparse retrieval.

## Method Summary
SPLATE combines frozen ColBERTv2 embeddings with sparse retrieval through a SPLADE-style adaptation layer. The method freezes the ColBERTv2 model and passes its outputs through a lightweight SPLADE module to generate sparse representations. This allows traditional sparse retrieval methods to be used for candidate generation instead of approximate nearest neighbor search. The approach maintains the late interaction paradigm while dramatically reducing the number of documents that need to be re-ranked, from thousands to only 50 documents. The sparse representation captures query-document interactions efficiently while leveraging the semantic understanding from the pre-trained ColBERTv2 model.

## Key Results
- SPLATE achieves comparable effectiveness to PLAID ColBERTv2 with only 50 documents re-ranked
- Retrieval of 50 candidates completed in under 10ms, demonstrating significant efficiency gains
- Strong performance maintained across MS MARCO, BEIR, and LoTTE benchmarks for both in-domain and out-of-domain scenarios
- Preserves the interpretable nature of sparse retrieval while benefiting from pre-trained semantic embeddings

## Why This Works (Mechanism)
SPLATE works by bridging the gap between dense late interaction models and efficient sparse retrieval. By freezing ColBERTv2 embeddings and applying a SPLADE-style transformation, the method creates sparse representations that can be indexed and retrieved using traditional inverted index structures. This eliminates the need for computationally expensive approximate nearest neighbor search while preserving the semantic understanding captured in the frozen embeddings. The approach effectively converts the late interaction paradigm into a format compatible with high-performance sparse retrieval infrastructure.

## Foundational Learning

**Late Interaction Models**: These models compute fine-grained interactions between query and document embeddings at retrieval time, providing superior effectiveness but requiring expensive re-ranking operations. Understanding this is crucial because SPLATE builds directly on this paradigm while addressing its efficiency limitations.

**Sparse Retrieval**: Traditional information retrieval using inverted indexes and bag-of-words representations. This forms the foundation for SPLATE's efficiency improvements, as it leverages well-optimized sparse retrieval infrastructure.

**Embedding Freezing**: The practice of using pre-trained embeddings without fine-tuning. SPLATE relies on this to maintain semantic understanding while adding sparse adaptation, making it essential to understand the tradeoffs involved.

**Approximate Nearest Neighbor Search**: The computational bottleneck that SPLATE aims to avoid. Understanding this technique helps appreciate the efficiency gains achieved by SPLATE's sparse approach.

## Architecture Onboarding

**Component Map**: User Query -> ColBERTv2 Embeddings (Frozen) -> SPLADE Sparse Adaptation -> Inverted Index -> Top-50 Documents -> ColBERTv2 Re-ranking

**Critical Path**: The user query flows through the ColBERTv2 model (without updating weights), then through the SPLADE adaptation layer to generate sparse representations. These sparse representations are used to retrieve the top 50 candidates via inverted index, which are then re-ranked using the original ColBERTv2 model.

**Design Tradeoffs**: The method trades some semantic precision for dramatic efficiency gains. While dense representations can capture nuanced semantic relationships, the sparse adaptation provides sufficient signal for candidate retrieval while enabling use of highly optimized sparse retrieval infrastructure.

**Failure Signatures**: Poor performance may occur when queries require deep semantic understanding that sparse representations cannot capture, or when the frozen ColBERTv2 embeddings are not well-suited to the domain. The method may also underperform on queries where traditional sparse retrieval already performs well.

**First Experiments**:
1. Compare retrieval effectiveness using different numbers of re-ranked documents (10, 50, 100) to find the optimal tradeoff
2. Test on specialized domains where ColBERTv2's frozen embeddings may be suboptimal
3. Benchmark retrieval times across different hardware configurations to verify CPU efficiency claims

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Relies on frozen ColBERTv2 embeddings, inheriting any biases or limitations from that model's training
- Sparse adaptation may not fully capture nuanced semantic relationships compared to dense representations
- 10ms retrieval time claim assumes specific hardware configurations and may vary across environments

## Confidence

**High confidence**: The computational efficiency improvements and retrieval time reductions are well-supported by the experimental setup and benchmarks

**Medium confidence**: The claim of "comparable effectiveness" to PLAID ColBERTv2 holds for the tested benchmarks but may not generalize to all retrieval scenarios

**Medium confidence**: The assertion that this approach is particularly suitable for CPU environments is reasonable but requires validation across diverse hardware configurations

## Next Checks

1. Test SPLATE's effectiveness on specialized domains (e.g., biomedical, legal) where ColBERTv2's frozen embeddings may be suboptimal
2. Evaluate performance with varying candidate set sizes (beyond the fixed 50 documents) to determine robustness across different recall requirements
3. Benchmark on hardware configurations beyond the tested environment to verify the claimed CPU efficiency benefits across real-world deployment scenarios