---
ver: rpa2
title: 'FairDD: Fair Dataset Distillation'
arxiv_id: '2411.19623'
source_url: https://arxiv.org/abs/2411.19623
tags:
- fairdd
- dataset
- groups
- datasets
- vanilla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FairDD addresses bias amplification in dataset distillation by
  introducing synchronized matching across protected attribute (PA) groups. Instead
  of matching synthetic samples to the entire dataset distribution dominated by majority
  groups, FairDD aligns them equally with each PA-wise subgroup.
---

# FairDD: Fair Dataset Distillation

## Quick Facts
- arXiv ID: 2411.19623
- Source URL: https://arxiv.org/abs/2411.19623
- Reference count: 40
- Reduces DEOM from 100.0 to ~17.0 while maintaining 94.6% accuracy

## Executive Summary
FairDD addresses bias amplification in dataset distillation by introducing synchronized matching across protected attribute (PA) groups. The method aligns synthetic samples equally with each PA-wise subgroup rather than matching to the entire dataset distribution dominated by majority groups. This prevents synthetic datasets from collapsing into majority group patterns while preserving target attribute (TA) accuracy. Experiments across multiple benchmarks demonstrate FairDD significantly improves fairness metrics without sacrificing classification accuracy, and requires no architectural changes to existing dataset distillation approaches.

## Method Summary
FairDD is a fairness-aware dataset distillation framework that modifies the matching objective of existing DD methods to account for protected attributes. Instead of matching synthetic samples to the entire dataset distribution, FairDD groups samples by their protected attributes and aligns synthetic samples equally with each PA-wise subgroup. This synchronized matching ensures minority groups are not overlooked during distillation. The framework is compatible with any dataset distillation method (DM, DC, etc.) and maintains target attribute coverage through theoretical constraints. FairDD requires prior knowledge of protected attributes and can be implemented with minimal changes to existing DD codebases.

## Key Results
- DEOM (Difference of Equalized Odds Max) reduced from 100.0 to approximately 17.0 across benchmarks
- Maintains high classification accuracy (>94.6%) while improving fairness
- Outperforms baselines (DM, DC) on fairness metrics across C-MNIST, CelebA, CIFAR10-S datasets
- Shows consistent performance across different network architectures (ConvNet, AlexNet, VGG11, ResNet18)
- Effective even with extreme minority representation (1/500 ratio)

## Why This Works (Mechanism)
FairDD prevents bias amplification by ensuring synthetic samples represent all protected attribute groups proportionally during distillation. Traditional DD methods match to the dominant majority distribution, causing minority groups to be underrepresented or lost entirely in the condensed dataset. By grouping samples by protected attributes and aligning synthetic samples equally with each group, FairDD preserves minority representation. The synchronized matching constraint ensures comprehensive coverage of the target attribute space while preventing the synthetic dataset from collapsing toward majority patterns. This PA-imbalance-agnostic approach maintains accuracy by preserving the full target attribute distribution.

## Foundational Learning
- **Dataset Distillation (DD)**: Technique to create small synthetic datasets that retain the training efficacy of large datasets
  - *Why needed*: Enables efficient model training on condensed data while preserving performance
  - *Quick check*: Verify synthetic dataset trains models to similar accuracy as original dataset

- **Protected Attributes (PA)**: Sensitive features (e.g., gender, color) that should be treated fairly in ML systems
  - *Why needed*: Basis for fairness evaluation and the synchronized matching mechanism
  - *Quick check*: Ensure PA groups are correctly identified and balanced in synthetic dataset

- **Target Attributes (TA)**: Attributes of interest for the main classification task (e.g., digit identity)
  - *Why needed*: Primary objective that must be preserved while improving fairness
  - *Quick check*: Verify TA accuracy remains comparable to baseline DD methods

- **Equalized Odds Metrics (DEOM/DEOA)**: Fairness metrics measuring difference in true/false positive rates across groups
  - *Why needed*: Quantify fairness improvements achieved by FairDD
  - *Quick check*: Lower DEOM/DEOA values indicate better fairness performance

## Architecture Onboarding

**Component Map**: Datasets -> FairDD Framework -> Synthetic Dataset -> Model Training -> Fairness & Accuracy Evaluation

**Critical Path**: FairDD Synchronized Matching -> PA Grouping -> Equal Alignment -> Synthetic Dataset Generation -> Model Training

**Design Tradeoffs**: 
- Pros: Compatible with existing DD methods, no architectural changes required, maintains accuracy while improving fairness
- Cons: Requires PA prior knowledge, computational overhead from PA-wise matching, performance depends on PA/TA definitions

**Failure Signatures**:
- Poor fairness improvement: Incorrect PA grouping or synchronized matching implementation
- Significant accuracy drop: TA coverage compromised by overly restrictive alignment
- Inconsistent results: PA/TA definitions not properly aligned with dataset characteristics

**First Experiments**:
1. Implement FairDD on C-MNIST with color as PA and digit as TA, compare DEOM with vanilla DD
2. Test FairDD+DM vs FairDD+DC on CIFAR10-S, measure fairness vs accuracy tradeoff
3. Apply FairDD to CelebA with gender as PA, evaluate on both facial attribute and gender classification tasks

## Open Questions the Paper Calls Out

**Open Question 1**: How does FairDD perform when protected attributes are unavailable or must be inferred?
The paper acknowledges this limitation, stating "Since FairDD relies on PA's prior information to conduct attribute-wise matching, it is valuable to explore the scenario where PA is unavailable." This is identified as future work without empirical results or theoretical analysis.

**Open Question 2**: What is the performance impact of FairDD when scaling to extremely imbalanced datasets where minority groups represent less than 1% of the data?
The ablation study shows FairDD maintains performance at 1/500 representation, but real-world scenarios may have even more extreme imbalances not tested in the paper.

**Open Question 3**: How does FairDD's performance compare when using Vision Transformers as the backbone network versus convolutional networks?
The paper explores this in section J, noting performance degradation with ViTs but still showing improvement over vanilla DM. The smaller improvement margins suggest architectural incompatibility issues requiring further investigation.

## Limitations
- Requires prior knowledge of protected attributes, limiting applicability to scenarios where PA must be inferred
- Computational overhead from PA-wise matching not quantified in experiments
- Performance may vary significantly depending on specific PA/TA definitions and dataset characteristics
- Limited testing on datasets with extreme minority representation (<0.1%)

## Confidence

**Theoretical Claims**: High confidence in PA-imbalance-agnostic formulation, though TA coverage proof requires verification
**Experimental Results**: Medium confidence due to implementation details of synchronized matching not fully specified
**Generalization Claims**: Medium confidence supported by diverse experimental setup but limited by tested architectures
**Practical Applicability**: High confidence in compatibility with existing DD methods and no architectural changes required

## Next Checks

1. Verify synchronized matching implementation by comparing synthetic dataset PA distribution to original dataset PA distribution
2. Test FairDD performance on datasets with multiple overlapping protected attributes (e.g., both gender and race)
3. Measure computational overhead of PA-wise matching compared to vanilla DD methods across different dataset sizes