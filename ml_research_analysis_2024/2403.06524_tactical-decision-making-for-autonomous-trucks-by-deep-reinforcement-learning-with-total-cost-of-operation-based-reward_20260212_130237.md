---
ver: rpa2
title: Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning
  with Total Cost of Operation Based Reward
arxiv_id: '2403.06524'
source_url: https://arxiv.org/abs/2403.06524
tags:
- uni00000013
- reward
- learning
- euros
- vehicle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes a deep reinforcement learning (DRL) framework
  for tactical decision making in autonomous trucks, specifically for Adaptive Cruise
  Control (ACC) and lane change maneuvers in a highway scenario. The research demonstrates
  the benefit of separating high-level decision-making processes and low-level control
  actions between the reinforcement learning agent and low-level controllers based
  on physical models.
---

# Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward

## Quick Facts
- arXiv ID: 2403.06524
- Source URL: https://arxiv.org/abs/2403.06524
- Reference count: 36
- New architecture reduces collision rates from 29.4% to 1.6% and increases target success rate from 70.6% to 97.8%

## Executive Summary
This paper proposes a deep reinforcement learning (DRL) framework for tactical decision making in autonomous trucks, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in highway scenarios. The research demonstrates the benefit of separating high-level decision-making from low-level control actions between the RL agent and physical model-based controllers. Three DRL algorithms (DQN, A2C, and PPO) are evaluated using both basic safety-focused and complex multi-objective Total Cost of Operation (TCOP) reward functions. The new architecture significantly outperforms the baseline, with curriculum learning techniques showing particular promise for training with complex reward functions.

## Method Summary
The study implements a DRL framework where the RL agent selects high-level tactical actions (adjust time gap, change lane) while physical model-based controllers (IDM for longitudinal, LC2013 for lateral) execute low-level control. Three RL algorithms are evaluated in a SUMO simulation environment: DQN (discrete actions, experience replay), A2C (continuous actions, parallel workers), and PPO (continuous actions, clipped surrogate objective). Two reward structures are tested: a basic safety-focused reward and a complex TCOP-based reward incorporating realistic costs (energy, driver, collision) and revenue (target completion). Curriculum learning is investigated with reward normalization to improve training stability on the complex TCOP task.

## Key Results
- New architecture reduces collision rates from 29.4% to 1.6% and increases target success rate from 70.6% to 97.8%
- PPO outperforms DQN and A2C across all architectures and reward functions
- Curriculum learning with normalized TCOP rewards achieves 100% success rate, 27.6 m/s average speed, and 1.01 TCOP score
- Separation of high-level decision-making and low-level control proves crucial for safety and learning efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating high-level decision-making from low-level control improves safety and learning efficiency.
- Mechanism: The RL agent selects high-level tactical actions while physical model-based controllers handle low-level execution, preventing suboptimal fine-grained control learning and reducing collision risk.
- Core assumption: Physical controllers (IDM, LC2013) reliably handle low-level vehicle dynamics without introducing hazards.
- Evidence anchors:
  - [abstract]: "Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models."
  - [section 2.3.2]: Describes new architecture with RL agent choosing high-level actions and physical controllers handling low-level execution.

### Mechanism 2
- Claim: Using a multi-objective TCOP-based reward function encourages cost-effective and safe driving strategies.
- Mechanism: Reward function incorporates realistic costs (energy, driver, collision penalties) and revenue (target completion), guiding agent to optimize safety and economic efficiency.
- Core assumption: TCOP reward components accurately reflect real-world costs and are appropriately weighted.
- Evidence anchors:
  - [abstract]: "study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck"
  - [section 2.4]: Defines TCOP reward function with detailed cost and revenue components.

### Mechanism 3
- Claim: Curriculum learning with reward normalization improves training stability and performance on complex TCOP tasks.
- Mechanism: Gradually increasing task complexity and normalizing conflicting reward components helps agent learn balanced policy.
- Core assumption: Normalized reward function effectively resolves conflicts between competing objectives during training.
- Evidence anchors:
  - [section 2.5]: Describes curriculum learning approach and need for normalization.
  - [section 3.3.2]: Shows improved success rate and average speed with normalized rewards in curriculum learning.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: RL framework modeled as MDP, requiring understanding of states, actions, rewards, and transition dynamics.
  - Quick check question: What are the components of the MDP tuple (S, A, T, R, Î³) as defined in the paper?

- Concept: Reinforcement Learning Algorithms (DQN, A2C, PPO)
  - Why needed here: Paper evaluates three RL algorithms for tactical decision making, requiring understanding of their differences.
  - Quick check question: How does PPO's surrogate objective function with clipping differ from A2C's advantage-based update?

- Concept: Reward Function Design
  - Why needed here: Paper uses basic and complex TCOP-based reward functions, requiring understanding of how reward shaping affects behavior.
  - Quick check question: Why is reward normalization necessary when using the TCOP-based reward function?

## Architecture Onboarding

- Component map:
  SUMO simulation environment -> RL agent (high-level tactical decisions) -> Low-level controllers (IDM for longitudinal, LC2013 for lateral) -> Vehicle dynamics

- Critical path:
  1. RL agent observes state (ego vehicle and surrounding vehicles' positions, speeds, lane info)
  2. RL agent selects high-level action (set time gap, change lane)
  3. Low-level controllers execute action (IDM computes acceleration, LC2013 initiates lane change)
  4. SUMO simulates environment and returns next state and reward
  5. RL agent learns from reward signal

- Design tradeoffs:
  - Separation of high-level and low-level control: Improves safety and learning efficiency but adds complexity
  - TCOP-based reward function: Encourages cost-effective driving but requires careful weighting and normalization
  - Curriculum learning: Improves training stability but requires well-defined curriculum stages

- Failure signatures:
  - High collision rate: Indicates poor safety or ineffective low-level controllers
  - Low success rate: Suggests agent not learning to reach target or is overly conservative
  - Unstable training: May indicate reward function issues or insufficient exploration

- First 3 experiments:
  1. Evaluate baseline vs. new architecture with basic reward function using PPO
  2. Test TCOP-based reward function with different weight values in new architecture
  3. Compare curriculum learning with and without reward normalization using TCOP-based reward function

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed RL framework generalize to more complex traffic scenarios such as uphill, downhill, and merging traffic?
- Basis in paper: [explicit] The paper suggests that an interesting future direction would be to explore transfer learning methods for generalizing tactical decision-making to diverse traffic scenarios.
- Why unresolved: Current study focuses on highway scenario and does not investigate framework's performance in more complex environments.
- What evidence would resolve it: Experiments demonstrating RL agent's ability to adapt to and perform well in uphill, downhill, and merging traffic scenarios, ideally using transfer learning techniques.

### Open Question 2
- Question: What is the impact of different reward normalization techniques on the learning efficiency and final performance of the RL agent in the TCOP-based reward function?
- Basis in paper: [explicit] The paper investigates training with complex TCOP-based reward function using different approaches, including normalizing reward components.
- Why unresolved: While paper compares performance with and without normalization, it does not explore impact of different normalization techniques on learning process.
- What evidence would resolve it: Comparative study of various reward normalization methods, evaluating their effects on learning speed, convergence, and final policy performance.

### Open Question 3
- Question: How does the performance of the RL framework compare to traditional rule-based systems in terms of safety and cost-effectiveness in real-world autonomous truck applications?
- Basis in paper: [inferred] The paper emphasizes importance of safety and cost-effectiveness, suggesting comparison with traditional systems could provide valuable insights.
- Why unresolved: Study primarily focuses on RL framework's performance within simulation environment and does not compare it to traditional rule-based systems in real-world scenarios.
- What evidence would resolve it: Field tests or simulations comparing RL framework's performance to traditional rule-based systems in terms of safety metrics (e.g., collision rates) and cost-effectiveness (e.g., energy consumption, operational costs) in real-world autonomous truck applications.

## Limitations

- Evaluation conducted entirely in simulation (SUMO) with no real-world validation
- Highway scenario is simplified with controlled traffic patterns and no adverse weather conditions
- Three RL algorithms represent limited sample of available methods
- TCOP reward function parameters based on assumptions that may not reflect all real-world operational contexts

## Confidence

- High confidence: Architecture separation benefits (clear performance improvements in collision reduction and success rate)
- Medium confidence: TCOP reward function effectiveness (simulation results are positive but lack real-world validation)
- Medium confidence: Curriculum learning improvements (shown in simulation but dependent on reward normalization assumptions)

## Next Checks

1. Test policy transfer from SUMO simulation to a high-fidelity driving simulator (e.g., CARLA) to evaluate real-world robustness before physical deployment
2. Conduct ablation studies varying TCOP reward weights across different traffic densities to verify stability of learned policies
3. Implement cross-validation across diverse highway scenarios (varying speed limits, truck types, weather conditions) to assess generalizability