---
ver: rpa2
title: Learning Goal-Conditioned Representations for Language Reward Models
arxiv_id: '2407.13887'
source_url: https://arxiv.org/abs/2407.13887
tags:
- reward
- learning
- state
- goal
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a goal-conditioned contrastive learning approach
  for training reward models (RMs) in language alignment tasks. By encouraging representations
  to align with goal states from preferred trajectories and diverge from dispreferred
  ones, the method improves RM performance on math reasoning (up to 0.09 AUROC gain)
  and natural language alignment (2.3% accuracy gain on Helpful-Harmless).
---

# Learning Goal-Conditioned Representations for Language Reward Models

## Quick Facts
- arXiv ID: 2407.13887
- Source URL: https://arxiv.org/abs/2407.13887
- Reference count: 40
- Key outcome: Introduces goal-conditioned contrastive learning for reward models, achieving up to 0.09 AUROC improvement on math reasoning and 2.3% accuracy gain on natural language alignment, with fine-grained control capabilities for filtering low-quality tokens and steering model outputs.

## Executive Summary
This paper presents a novel approach to training reward models (RMs) for language alignment tasks using goal-conditioned contrastive learning. The method learns representations that align with goal states from preferred trajectories while diverging from dispreferred ones, enabling improved performance and steerability. By encoding dense signals about promising trajectories at intermediate steps, the approach enhances both generalization and the ability to control model outputs through Q-value based filtering and guided decoding.

## Method Summary
The method combines a preference ranking objective with a contrastive loss that encourages representations to align with goal states from preferred trajectories. During training, state-action pairs from sampled trajectories are compared to goal states using cosine similarity as Q-values. The combined loss function balances preference ranking with contrastive learning (parameterized by λ), enabling the reward model to learn representations that capture the likelihood of achieving desired outcomes at intermediate steps. This allows for fine-grained control during inference through evaluation of token-level Q-values for filtering and steering.

## Key Results
- Achieves up to 0.09 AUROC improvement on math reasoning benchmarks (GSM8k, MATH, Asdiv)
- Improves natural language alignment accuracy by 2.3% on Helpful-Harmless dataset
- Enables up to 55% reduction in low-quality tokens through Q-value based filtering
- Increases helpfulness by 9.6% and complexity by 21.6% through guided decoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Goal-conditioned contrastive learning improves reward model representations by aligning them with future goal states along preferred trajectories and diverging them from dispreferred ones.
- Mechanism: Uses a contrastive loss that increases similarity between state-action representations and positive goal states from preferred trajectories, while decreasing similarity with negative goal states from dispreferred trajectories.
- Core assumption: Representations that better capture the expected reward or likelihood of achieving a goal state at intermediate steps will lead to improved reward model performance and generalization.
- Evidence anchors:
  - [abstract]: "by increasing the representation similarity of future states along sampled preferred trajectories and decreasing the similarity along randomly sampled dispreferred trajectories"
  - [section 3.2]: "forcing the reward model's output scores to be entangled with an approximated Q-function has the potential to improve the reward model's credit assignment and preference ranking accuracy"
- Break condition: If the contrastive loss dominates the preference ranking loss (λ too high), the reward model may lose its ability to accurately rank preferred vs dispreferred sequences.

### Mechanism 2
- Claim: The learned representations enable fine-grained control and steerability by allowing evaluation of the likelihood that a particular action will achieve a desired goal state.
- Mechanism: Q-values computed as cosine similarities between intermediate state representations and goal state representations provide a measure of how likely a token is to lead to a desired outcome.
- Core assumption: The learned representations capture meaningful semantic relationships between intermediate states and goal states that can be exploited for steering model outputs.
- Evidence anchors:
  - [abstract]: "we show this way of training RM representations enables improved steerability because it allows us to evaluate the likelihood of an action achieving a particular goal-state"
  - [section 4.1.3]: "we use these Q-values to filter generations...discard it if the Q-value for any of the tokens is less than 0"
- Break condition: If the goal state representation is poorly chosen or too generic, the Q-values may not provide meaningful steering signals.

### Mechanism 3
- Claim: The method improves reward model generalization by encoding dense signals about which trajectories are promising at different points in the sequence.
- Mechanism: By training on intermediate representations rather than just sequence-level rewards, the model learns to make better credit assignment decisions and generalize across different types of problems.
- Core assumption: Dense intermediate supervision signals are more informative than sparse sequence-level signals for learning generalizable representations.
- Evidence anchors:
  - [abstract]: "Enforcing this loss on representations from intermediate steps of the sequence helps encode a dense signal as to which trajectories are more promising at different points in the sequence"
  - [section 4.1.2]: "AUROC scores for both reward models tend to increase as they see more of the generated sequence, but Q-Function 7b Reward largely maintains higher AUROC across different percentiles"
- Break condition: If the preference ranking dataset is too small or biased, the intermediate supervision may amplify existing biases rather than improve generalization.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The method relies on contrasting positive and negative examples to learn meaningful representations that capture the relationship between intermediate states and goal states.
  - Quick check question: How does contrastive learning differ from supervised learning in terms of the supervision signal it provides?

- Concept: Goal-conditioned reinforcement learning
  - Why needed here: The approach is inspired by goal-conditioned RL, where the agent learns to achieve specific goal states rather than maximizing cumulative reward.
  - Quick check question: What is the relationship between the Q-function in goal-conditioned RL and the representations learned by this method?

- Concept: Reward modeling for language alignment
  - Why needed here: The method builds on existing reward modeling approaches but extends them with representation learning to improve performance and steerability.
  - Quick check question: How does the standard preference ranking objective differ from the combined objective used in this method?

## Architecture Onboarding

- Component map:
  Feature extractor (ϕ) -> Reward projection head (r') -> Contrastive loss module -> Preference ranking loss module

- Critical path:
  1. Input prompt and completion sequence
  2. Extract hidden state representations at each token position
  3. Sample source and goal states for contrastive loss
  4. Compute contrastive loss and preference ranking loss
  5. Backpropagate combined loss to update model parameters

- Design tradeoffs:
  - Balance between contrastive loss (λ) and preference ranking loss
  - Choice of goal state representation (mean of preferred completions vs other methods)
  - Sampling strategy for source and goal states (random vs late sampling)

- Failure signatures:
  - If λ is too high: Reward model may lose ability to accurately rank preferred vs dispreferred sequences
  - If goal state is poorly chosen: Q-values may not provide meaningful steering signals
  - If sampling strategy is suboptimal: May not capture meaningful state-action relationships

- First 3 experiments:
  1. Implement baseline reward model with standard preference ranking loss
  2. Add contrastive loss with goal states from preferred completions
  3. Experiment with different values of λ to find optimal balance between losses

## Open Questions the Paper Calls Out

- How can we derive more informative goal states during inference for the reward model?
  - Basis in paper: [explicit] The paper notes that during inference, the goal state is computed as the mean representation of all preferred completions in the training set, which may not be as precise as the training-time goal states sampled from specific preferred trajectories.
  - Why unresolved: The authors acknowledge this discrepancy but do not propose a solution for constructing more precise inference-time goal states without requiring additional annotations.
  - What evidence would resolve it: Experiments comparing different methods of constructing inference-time goal states (e.g., using prototypes, clustering, or other techniques) and their impact on reward model performance and downstream

## Limitations

- The performance gains are measured against a relatively small number of datasets, and generalization beyond tested mathematical and alignment tasks is not established.
- The robustness of Q-values for steering across different domains and prompt types is not thoroughly validated.
- The optimal balance between contrastive loss and preference ranking loss is not systematically explored.

## Confidence

**High Confidence**: The core technical contribution of combining contrastive learning with goal-conditioned representations for reward models is well-specified and implementable.

**Medium Confidence**: The claims about improved generalization and steerability are supported by experimental results but could benefit from additional validation across diverse domains.

**Low Confidence**: The assertion that the method works "because it allows us to evaluate the likelihood of an action achieving a particular goal-state" is somewhat circular and lacks strong evidence that Q-values capture meaningful semantic relationships.

## Next Checks

**Check 1: Cross-Domain Generalization**: Test the learned representations on language tasks outside of math reasoning and natural language alignment, such as creative writing, code generation, or summarization. Evaluate whether the Q-values remain meaningful and whether the steering capabilities transfer to these new domains.

**Check 2: Ablation on Intermediate Supervision**: Systematically vary the frequency and positions at which intermediate representations are used in the contrastive loss. Compare against baselines that use only final sequence representations to isolate the contribution of dense intermediate supervision to performance gains.

**Check 3: Reward Hacking Analysis**: Design adversarial prompts or completions specifically crafted to exploit the representation learning mechanism. Test whether the learned representations can be manipulated to produce high Q-values for undesirable outputs, and evaluate the effectiveness of the proposed decoupling strategy using an MLP projection layer.