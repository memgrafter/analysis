---
ver: rpa2
title: 'An analysis of HOI: using a training-free method with multimodal visual foundation
  models when only the test set is available, without the training set'
arxiv_id: '2408.05772'
source_url: https://arxiv.org/abs/2408.05772
tags:
- unseen
- rare
- human-object
- classes
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores Human-Object Interaction (HOI) detection using
  only test data, without training data, by employing multimodal visual foundation
  models in a training-free manner. The study tests three setups: using paired ground
  truth, randomly recombining ground truth pairs, and using bounding boxes from grounding
  DINO.'
---

# An analysis of HOI: using a training-free method with multimodal visual foundation models when only the test set is available, without the training set

## Quick Facts
- arXiv ID: 2408.05772
- Source URL: https://arxiv.org/abs/2408.05772
- Authors: Chaoyi Ai
- Reference count: 7
- One-line primary result: Zero-shot/few-shot HOI detection using multimodal models achieves mAP scores from 13.56 to 49.56 on HICO-DET, with rare classes showing distinct properties compared to non-rare classes.

## Executive Summary
This paper explores Human-Object Interaction (HOI) detection using only test data, without access to training data, by employing multimodal visual foundation models in a training-free manner. The study tests three experimental setups: using paired ground truth bounding boxes, randomly recombining ground truth pairs, and using bounding boxes from grounding DINO. The results show that both rare and non-rare HOI classes exist in the environment with distinct properties, but the zero-shot/few-shot capabilities of current multimodal vision foundation models are not fully realized.

## Method Summary
The method employs multimodal visual foundation models in a training-free manner to detect HOI relationships using only test data. Three experimental setups are tested: (1) paired ground truth human-object bounding boxes, (2) randomly recombined ground truth pairs, and (3) bounding boxes extracted by grounding DINO. The models output verb probability distributions for each human-object pair, which are evaluated using mAP on the HICO-DET dataset.

## Key Results
- mAP scores range from 13.56 to 49.56 depending on the model and setup used
- Rare HOI classes show insensitivity to random human-object pairings while non-rare classes are sensitive
- Current multimodal vision foundation models have not fully realized zero-shot/few-shot capabilities for HOI detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using multimodal visual foundation models in a training-free manner can detect HOI relationships even without access to training data.
- Mechanism: The model takes paired human-object bounding boxes as input, combines them into a union area, and feeds them to a multimodal visual foundation model alongside text prompts. The model outputs a probability distribution over verbs, allowing classification of the interaction.
- Core assumption: The multimodal model has sufficient generalization capability to map visual features to textual verb labels without explicit training on the HOI dataset.
- Evidence anchors:
  - [abstract] "using multimodal visual foundation model in a training-free manner"
  - [section] "The model using the paired ground truth is depicted in Figure 2... the output is processed through a softmax function, which yields a probability distribution for different verbs."
- Break condition: The multimodal model lacks the ability to generalize beyond its pretraining distribution, resulting in poor verb prediction accuracy.

### Mechanism 2
- Claim: Randomly recombining ground truth human and object bounding boxes can reveal whether HOI classes are sensitive to arbitrary pairings.
- Mechanism: Instead of using true paired bounding boxes, the method creates arbitrary combinations of all human and all object boxes from ground truth. These random pairs are fed into the multimodal model to test if the model can still predict meaningful verbs.
- Core assumption: The sensitivity of rare vs non-rare classes to random combinations will differ, revealing intrinsic properties of the classes.
- Evidence anchors:
  - [section] "we used randomly composed pairs... 'human' refers to all possible bounding boxes of humans from the ground truth, and 'object' pertains to all possible bounding boxes from the ground truth."
  - [section] "rare classes display insensitivity to arbitrary human-object interactions, while non-rare classes are sensitive to these combinations."
- Break condition: If the multimodal model predicts verbs with similar confidence regardless of whether pairs are true or random, the distinction between rare and non-rare classes disappears.

### Mechanism 3
- Claim: Grounding DINO extracted bounding boxes can serve as unpaired features for HOI detection in the absence of ground truth pairs.
- Mechanism: Grounding DINO is used to extract object bounding boxes without pairing them with humans. These unpaired boxes are then combined with ground truth human boxes and processed through the multimodal model to predict verbs.
- Core assumption: Even without explicit human-object pairing, the multimodal model can infer interactions from the spatial and contextual features of the boxes.
- Evidence anchors:
  - [section] "grounding DINO(Liu et al. 2023), which are unpaired, using a method similar to the second setup to ascertain the verb outcomes."
  - [section] "the output bounding boxes from the GroundingDINO model are used in any arbitrary combination."
- Break condition: If the multimodal model cannot infer interactions from unpaired boxes, verb prediction accuracy will drop significantly compared to paired setups.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The paper evaluates the zero-shot capabilities of multimodal models for HOI detection without training on the dataset.
  - Quick check question: Can a model classify new classes without any training examples of those classes?

- Concept: Long-tail distribution
  - Why needed here: HOI datasets like HICO-DET have a long-tail distribution where rare classes have very few samples, affecting model performance.
  - Quick check question: How does the performance on rare classes compare to non-rare classes in long-tail datasets?

- Concept: Open vocabulary
  - Why needed here: The multimodal models are expected to handle open vocabulary tasks, predicting verbs that may not be in their training set.
  - Quick check question: Can the model predict verbs not explicitly seen during pretraining?

## Architecture Onboarding

- Component map: Feature extractor -> Multimodal visual foundation model -> Softmax layer -> Evaluation module

- Critical path:
  1. Extract bounding boxes (paired or unpaired)
  2. Create union area or arbitrary combinations
  3. Input to multimodal model with text prompts
  4. Obtain verb logits
  5. Apply softmax to get probabilities
  6. Calculate mAP against ground truth

- Design tradeoffs:
  - Using paired vs unpaired boxes: Paired boxes maintain spatial context but require ground truth; unpaired boxes are more flexible but lose direct pairing information.
  - Model choice: Larger models (ViT-L/14) generally perform better but are computationally expensive.
  - Random combinations: Useful for understanding class properties but may introduce noise.

- Failure signatures:
  - Similar verb predictions for both true and random pairs indicate the model is not learning meaningful interactions.
  - Low mAP across all setups suggests the multimodal model cannot generalize to HOI tasks.
  - High variance in predictions indicates instability in the model's reasoning.

- First 3 experiments:
  1. Test paired ground truth with CLIP ViT-B/16 to establish baseline performance.
  2. Test random combinations with the same model to compare sensitivity between rare and non-rare classes.
  3. Test Grounding DINO features with CLIP ViT-L/14 to evaluate unpaired feature performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or algorithmic modifications could enhance the zero-shot/few-shot capabilities of multimodal vision foundation models for HOI detection?
- Basis in paper: [explicit] The paper concludes that current models have not fully realized the potential of zero-shot and few-shot learning in multimodal vision foundation models for HOI detection.
- Why unresolved: The paper does not provide specific modifications or improvements that could be made to enhance these capabilities. It only identifies the current limitations.
- What evidence would resolve it: Experimental results demonstrating improved performance in zero-shot/few-shot HOI detection tasks after implementing specific architectural or algorithmic modifications.

### Open Question 2
- Question: How do the properties of rare and non-rare HOI classes differ in terms of their sensitivity to random combinations of humans and objects?
- Basis in paper: [explicit] The paper finds that rare classes are insensitive to random combinations, while non-rare classes are sensitive to these combinations in the default setting.
- Why unresolved: The paper does not explore the underlying reasons for this difference in sensitivity or provide a detailed analysis of the properties that cause this behavior.
- What evidence would resolve it: A detailed analysis of the features and characteristics of rare vs. non-rare HOI classes that explains their differing sensitivities to random combinations.

### Open Question 3
- Question: How does the performance of multimodal vision foundation models in HOI detection vary across different datasets beyond HICO-DET?
- Basis in paper: [explicit] The paper only tests the models on the HICO-DET dataset.
- Why unresolved: The paper does not provide any comparative analysis or results from other datasets, limiting the generalizability of the findings.
- What evidence would resolve it: Experimental results showing the performance of the models on multiple HOI datasets, allowing for a comparative analysis across different data distributions and complexities.

## Limitations

- The study only evaluates on the HICO-DET dataset, limiting generalizability to other HOI datasets.
- The interpretation of rare vs non-rare class behaviors requires further validation to confirm the underlying reasons for their differing sensitivities.
- The broader claims about the need for further development of open vocabulary capabilities in multimodal models are based on limited testing.

## Confidence

- High confidence: The experimental methodology and implementation details are clearly specified, and the results showing varying mAP scores across different setups are reliable.
- Medium confidence: The interpretation of rare vs non-rare class behaviors and the implications for model capabilities are reasonable but require further validation.
- Low confidence: The broader claims about the need for further development of open vocabulary capabilities in multimodal models, as this would require testing on additional datasets and model architectures.

## Next Checks

1. **Cross-dataset validation**: Test the same methodology on other HOI datasets (e.g., V-COCO, HOI-A) to verify whether the observed patterns of rare vs non-rare class behaviors generalize across datasets.

2. **Alternative model comparison**: Implement the same experimental setup using different multimodal foundation models (e.g., Flamingo, GIT) to determine if the observed limitations are model-specific or inherent to the approach.

3. **Controlled random pairing analysis**: Conduct a more systematic analysis of random pairings by varying the ratio of true to random pairs and measuring the degradation in mAP to better quantify the models' sensitivity to correct human-object pairings.