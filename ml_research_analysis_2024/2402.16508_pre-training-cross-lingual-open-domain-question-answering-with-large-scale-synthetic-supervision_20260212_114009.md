---
ver: rpa2
title: Pre-training Cross-lingual Open Domain Question Answering with Large-scale
  Synthetic Supervision
arxiv_id: '2402.16508'
source_url: https://arxiv.org/abs/2402.16508
tags:
- question
- answer
- retrieval
- language
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLASS, a unified model that performs cross-lingual
  passage retrieval and multilingual open-domain question answering using a single
  encoder-decoder architecture. The approach employs a two-stage unsupervised pre-training
  method that leverages the cross-lingual link structure within Wikipedia to synthesize
  supervisory signals.
---

# Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision

## Quick Facts
- arXiv ID: 2402.16508
- Source URL: https://arxiv.org/abs/2402.16508
- Reference count: 35
- Primary result: State-of-the-art performance on cross-lingual open-domain QA using a unified retrieval+QA model trained without labeled data

## Executive Summary
This paper introduces CLASS, a unified model that performs cross-lingual passage retrieval and multilingual open-domain question answering using a single encoder-decoder architecture. The approach employs a two-stage unsupervised pre-training method that leverages the cross-lingual link structure within Wikipedia to synthesize supervisory signals. The first stage involves cross-lingual retrieval pre-training using cloze-style queries and a knowledge distillation process to align retrieval distributions between English and multilingual models. The second stage employs multilingual QA pre-training using anchor texts and a question transformation technique via large language models to generate natural questions from cloze-style ones. The method does not require additional tools like machine translation and achieves state-of-the-art performance on both supervised and zero-shot language adaptation settings, including the XOR-T YDI QA benchmark and MKQA dataset.

## Method Summary
CLASS is a unified model that combines cross-lingual passage retrieval and multilingual open-domain question answering. It uses a two-stage unsupervised pre-training approach. Stage 1 employs cross-lingual retrieval pre-training using cloze-style queries and knowledge distillation from an English teacher model to align retrieval distributions across languages. Stage 2 uses multilingual QA pre-training with anchor texts from Wikipedia hyperlinks and question transformation via LLMs to generate natural questions. The model is fine-tuned on NQ and XOR-TYDI QA datasets without requiring additional tools like machine translation.

## Key Results
- Achieves state-of-the-art performance on cross-lingual open-domain QA benchmarks
- Effective in both supervised and zero-shot language adaptation settings
- Outperforms previous methods on XOR-T YDI QA benchmark and MKQA dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model learns to map queries in different languages into a shared semantic space via knowledge distillation from an English expert.
- Mechanism: A well-trained English retriever generates retrieval distributions over English passages for English queries. The multilingual model is trained to match these distributions for both English and target language queries, ensuring cross-lingual alignment.
- Core assumption: The English retriever is a reliable teacher and the cross-lingual link structure in Wikipedia provides enough parallel context for alignment.
- Evidence anchors:
  - [abstract] "a knowledge distillation process to align retrieval distributions between English and multilingual models"
  - [section] "A well-trained English model θEn is employed to teach a multilingual model θM L using parallel queries... through KL divergence loss"
- Break condition: If the English teacher model is biased or the parallel queries are not semantically aligned, the distillation will propagate errors across languages.

### Mechanism 2
- Claim: Natural questions are synthesized from cloze-style queries to improve QA fine-tuning.
- Mechanism: Anchor texts from hyperlinks in target language Wikipedia pages are extracted as answer candidates. A large language model rewrites these cloze queries into natural questions using In-Context Learning.
- Core assumption: Anchor texts are reliable answer spans and LLMs can transform them into natural questions that match real user queries.
- Evidence anchors:
  - [abstract] "generate more natural questions to supervise answer generation"
  - [section] "we employ anchor texts with hyperlinks as answer candidates... we employ large language models (LLMs) for query transformation via In-Context Learning"
- Break condition: If anchor texts are not valid answers or LLM transformations introduce semantic drift, the training signal becomes noisy.

### Mechanism 3
- Claim: End-to-end pre-training jointly improves retrieval and QA within a single transformer.
- Mechanism: The reader generates cross-attention scores used as retrieval targets, and the retriever learns from the answer generation loss. This backpropagates retrieval signals through the reader.
- Core assumption: The decoder's cross-attention scores are a reliable relevance signal for the retriever and that joint training does not destabilize either component.
- Evidence anchors:
  - [abstract] "A unified model capable of performing both cross-lingual retrieval and multilingual open-domain QA tasks"
  - [section] "The reader optimises the negative log-likelihood... The final loss combines reader and retriever loss: Le2e = α · LKL + Lreader"
- Break condition: If cross-attention scores are not discriminative enough, the retriever will not learn meaningful relevance rankings.

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: To transfer the English retriever's ability to the multilingual model without requiring large amounts of multilingual labeled data.
  - Quick check question: If the English teacher retriever is 90% accurate, what is the best-case accuracy the multilingual model can achieve after distillation?

- Concept: Cross-Modal Alignment
  - Why needed here: To ensure that queries in different languages map to the same semantic representation space for effective retrieval.
  - Quick check question: What happens to retrieval accuracy if the multilingual model's embeddings for the same concept in two languages are orthogonal?

- Concept: In-Context Learning
  - Why needed here: To generate diverse, natural-language questions from cloze-style queries without fine-tuning a separate model.
  - Quick check question: How many meta-examples are typically needed to stabilize the LLM's query transformation behavior?

## Architecture Onboarding

- Component map: Encoder (B layers as retriever, rest + decoder as reader) → Cross-attention → Decoder → Answer generation. Parallel query mining → Query transformation → Pre-training stages.
- Critical path: Parallel query mining → Cross-lingual retrieval pre-training → Multilingual QA pre-training → Fine-tuning.
- Design tradeoffs: Using a single encoder-decoder model simplifies deployment but requires careful joint training to avoid one component dominating; mining parallel queries from Wikipedia avoids labeled data but limits scalability to languages with weak Wikipedia coverage.
- Failure signatures: Low retrieval accuracy indicates misalignment in knowledge distillation; poor QA results suggest noisy query transformations or ineffective joint training.
- First 3 experiments:
  1. Validate that the English teacher retriever produces consistent distributions for parallel queries.
  2. Test the query transformation LLM on a small sample to confirm semantic preservation.
  3. Run joint training with only retrieval loss first, then add QA loss, monitoring for divergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CLASS scale with the size of the training corpus beyond the current implementation?
- Basis in paper: [inferred] The paper mentions that the model was trained on 15 languages and achieved state-of-the-art performance, but does not explore scaling to more languages or larger datasets.
- Why unresolved: The authors note that scaling to more diverse languages and larger models is an interesting research topic for future work, indicating that the current study did not address this aspect.
- What evidence would resolve it: Conducting experiments with additional languages and larger training datasets to evaluate the performance gains or limitations of CLASS.

### Open Question 2
- Question: What is the impact of the pre-training framework on the model's ability to handle cross-lingual evidence reasoning and answer generation?
- Basis in paper: [explicit] The authors observe that while CLASS improves cross-lingual retrieval, it does not translate English evidence into answers in the target language as effectively, suggesting a gap in cross-lingual evidence reasoning.
- Why unresolved: The paper identifies this as a limitation but does not provide solutions or further analysis on how to improve this aspect of the model.
- What evidence would resolve it: Investigating methods to enhance the model's capability to reason over cross-lingual evidence and generate accurate answers in the target language.

### Open Question 3
- Question: How does the choice of question transformation technique affect the quality of generated questions and subsequent QA performance?
- Basis in paper: [explicit] The paper discusses the use of large language models for query transformation but does not compare different techniques or their impact on QA results.
- Why unresolved: While the paper mentions the use of LLMs for query transformation, it does not explore alternative methods or their effectiveness in generating high-quality questions.
- What evidence would resolve it: Conducting experiments with different query transformation techniques and evaluating their impact on the quality of generated questions and QA performance.

## Limitations
- Approach relies heavily on Wikipedia's cross-lingual link structure, limiting scalability to low-resource languages
- Performance depends on quality of English teacher retriever and effective knowledge distillation transfer
- Synthetic QA generation involves multiple error-prone steps with limited analysis of error propagation
- Effectiveness on languages beyond 15 tested is uncertain due to varying Wikipedia coverage

## Confidence
- **High Confidence**: The basic architecture combining retrieval and QA in a single model, and the overall two-stage pre-training methodology
- **Medium Confidence**: The effectiveness of knowledge distillation for cross-lingual retrieval alignment, and the quality of synthetic question generation through LLMs
- **Low Confidence**: Claims about robustness to low-resource languages and the absence of need for additional tools like machine translation

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of knowledge distillation vs. end-to-end training to the final performance
2. Evaluate model performance on languages with varying levels of Wikipedia coverage to test scalability claims
3. Analyze the quality of synthetic questions by measuring semantic drift between original cloze queries and LLM-transformed questions, and assess whether this drift correlates with QA performance degradation