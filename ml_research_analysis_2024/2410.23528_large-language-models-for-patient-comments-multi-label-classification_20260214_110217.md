---
ver: rpa2
title: Large Language Models for Patient Comments Multi-Label Classification
arxiv_id: '2410.23528'
source_url: https://arxiv.org/abs/2410.23528
tags:
- patient
- comments
- classification
- issues
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research leverages Large Language Models (LLMs) to automate
  multi-label classification of patient feedback from HCAHPS surveys. It introduces
  a PHI detection framework for data privacy, then applies GPT-4 Turbo in zero-shot
  and few-shot settings for classification into 10 predefined topics.
---

# Large Language Models for Patient Comments Multi-Label Classification

## Quick Facts
- arXiv ID: 2410.23528
- Source URL: https://arxiv.org/abs/2410.23528
- Authors: Hajar Sakai; Sarah S. Lam; Mohammadsadegh Mikaeili; Joshua Bosire; Franziska Jovin
- Reference count: 0
- Primary result: GPT-4 Turbo achieves 76.12% F1-score in zero-shot multi-label classification of patient comments

## Executive Summary
This research explores leveraging Large Language Models (LLMs) to automate multi-label classification of patient feedback from HCAHPS surveys. The study introduces a PHI detection framework for data privacy and applies GPT-4 Turbo in zero-shot and few-shot settings for classification into 10 predefined topics. Results demonstrate that GPT-4 Turbo significantly outperforms traditional methods and pre-trained language models, achieving an F1-score of 76.12% and weighted F1-score of 73.61% in the zero-shot setting. The classification outputs are associated with structured patient data, revealing strong links between positive feedback and higher satisfaction ratings.

## Method Summary
The study uses HCAHPS patient comments (1,089 comments) annotated by 5 annotators across 3 rounds. A PHI detection framework using regular expressions de-identifies patient data before feeding it to GPT-4 Turbo. The LLM performs multi-label classification using prompt engineering (zero-shot, 1-shot, 3-shot, 5-shot settings) across 10 predefined topics based on patient journey mapping. Performance is evaluated using F1-score and weighted F1-score metrics.

## Key Results
- GPT-4 Turbo achieves 76.12% F1-score and 73.61% weighted F1-score in zero-shot setting
- Outperforms traditional methods and pre-trained language models on the same dataset
- Classification outputs show strong associations with structured patient data and satisfaction ratings

## Why This Works (Mechanism)

### Mechanism 1
GPT-4 Turbo's zero-shot performance exceeds traditional ML and PLM baselines due to its ability to interpret contextual patient feedback without fine-tuning. Zero-shot learning allows the model to classify feedback using its pre-existing general language understanding, avoiding the data labeling bottleneck. The model's pre-training covers the semantic domain of patient feedback sufficiently well.

### Mechanism 2
The PHI detection framework preserves privacy without degrading classification accuracy through regular expression-based detection and redaction of identifiable elements. This allows safe input to the LLM without removing semantic meaning. PHI removal does not significantly alter the semantic content necessary for classification.

### Mechanism 3
Multi-label classification better captures the complexity of patient feedback than binary or single-label approaches. Assigning multiple relevant topics to a single comment reflects the multifaceted nature of patient experiences, improving diagnostic insight. Patient comments often contain multiple simultaneous concerns or positive aspects.

## Foundational Learning

- **Concept: Multi-label text classification (MLTC)**
  - Why needed: Patient feedback often addresses multiple aspects of care in a single comment; single-label classification would oversimplify
  - Quick check: What distinguishes multi-label classification from multi-class classification in NLP?

- **Concept: Prompt engineering (zero-shot and few-shot)**
  - Why needed: Avoids the need for labeled training data, reducing cost and time while still achieving strong performance
  - Quick check: How does few-shot learning differ from zero-shot learning in LLM contexts?

- **Concept: PHI (Protected Health Information) and HIPAA compliance**
  - Why needed: Patient feedback contains sensitive identifiers that must be removed before sharing with external models
  - Quick check: Which types of information are considered PHI under HIPAA?

## Architecture Onboarding

- **Component map**: PHI Detection Module → Input Sanitization → GPT-4 Turbo API → Multi-label Classification Output → Statistical Association Analysis
- **Supporting**: Topic Definition & Annotation Process → Evaluation Metrics (F1, AUC) → Data Triangulation with Structured Data
- **Critical path**: PHI detection → classification → evaluation → association analysis
- **Design tradeoffs**: Zero-shot vs. few-shot (saves tokens/time vs. improves precision/cost); Regular expressions for PHI (fast/interpretable vs. ML-based detection)
- **Failure signatures**: Low F1 scores on certain topics suggest ambiguous topic definitions; High PHI false positives indicate over-redaction; Inconsistent classification suggests prompt sensitivity
- **First 3 experiments**:
  1. Test PHI detection accuracy on labeled test set of synthetic patient comments
  2. Compare zero-shot vs. few-shot classification performance on balanced subset of topics
  3. Run association analysis between classification topics and structured survey data

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of GPT-4 Turbo compare to fine-tuned domain-specific LLMs trained on healthcare data?
- **Open Question 2**: What is the optimal number of topics for multi-label classification that balances granularity with classification accuracy?
- **Open Question 3**: How does the classification performance vary across different patient demographics and hospital stay characteristics?

## Limitations
- Based on single dataset of 1,089 HCAHPS comments, limiting generalizability
- PHI detection relies on regular expressions that may miss complex PHI patterns
- Prompt engineering approach lacks detailed specification for exact replication
- Does not address potential biases in GPT-4 Turbo model affecting different demographics

## Confidence
- **High Confidence**: GPT-4 Turbo's superior performance compared to traditional methods and PLMs
- **Medium Confidence**: Effectiveness of PHI detection framework in preserving privacy
- **Medium Confidence**: Association between classification outputs and structured patient data

## Next Checks
1. Evaluate PHI detection robustness on diverse synthetic patient comments to assess false positive/negative rates
2. Systematically vary prompt templates and topic definitions to test classification performance sensitivity
3. Analyze classification outputs across patient demographics to identify potential model biases