---
ver: rpa2
title: Evaluating World Models with LLM for Decision Making
arxiv_id: '2411.08794'
source_url: https://arxiv.org/abs/2411.08794
tags:
- self
- world
- format
- name
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a comprehensive evaluation of world models
  with large language models (LLMs) for decision making. The authors introduce three
  main tasks: policy verification, action proposal, and policy planning, and evaluate
  them on 31 diverse environments.'
---

# Evaluating World Models with LLM for Decision Making

## Quick Facts
- arXiv ID: 2411.08794
- Source URL: https://arxiv.org/abs/2411.08794
- Reference count: 40
- Key outcome: GPT-4o significantly outperforms GPT-4o-mini on policy verification, action proposal, and policy planning tasks across 31 diverse environments, with performance degrading for long-term decision-making

## Executive Summary
This paper presents a comprehensive evaluation framework for world models implemented with large language models (LLMs) in decision-making tasks. The authors introduce three main tasks - policy verification, action proposal, and policy planning - and evaluate them across 31 diverse environments. Using GPT-4o and GPT-4o-mini as backbone models, the study reveals that GPT-4o consistently outperforms its smaller counterpart, particularly in tasks requiring domain knowledge. The research also identifies key limitations, including performance degradation for long-term planning and instabilities when combining different world model functionalities.

## Method Summary
The evaluation framework tests world models with LLMs on three core tasks: policy verification (checking if action sequences lead to task completion), action proposal (generating top-K potential actions), and policy planning (finding complete policies using world model predictions). The authors use 31 environments from prior work, implementing world models that predict next states, rewards, and valid actions through in-context learning from environment rules. Experiments compare GPT-4o and GPT-4o-mini across various settings, including different ρ values for verification/planning and K values for action proposal, with results averaged over 30 runs using temperature 0 for generation.

## Key Results
- GPT-4o significantly outperforms GPT-4o-mini across all three tasks, especially for domain-knowledge intensive environments
- World model performance decreases for long-term decision-making tasks due to prediction error accumulation
- Combining different world model functionalities introduces additional performance instabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based world models improve decision-making by providing accurate next-state and reward predictions tailored to the agent's policy path.
- Mechanism: The world model uses in-context learning from environment rules to predict state transitions relevant to the agent's intended policy, enabling more focused evaluation than general simulation.
- Core assumption: The LLM can extract relevant state transitions from rule-based prompts without extensive fine-tuning.
- Evidence anchors:
  - [abstract] "LLMs also serve as the world model for deliberative reasoning in Reasoning via Planning (RAP) and Tree of Thought (ToT)"
  - [section] "The world model considered in this work mainly follows the design in (Wang et al., 2024), where the representation of the state includes the objects in the environments and their properties"
- Break condition: If rule-based prompts become too complex or the LLM fails to follow instructions, prediction accuracy degrades significantly.

### Mechanism 2
- Claim: Action proposal functionality enhances decision-making by generating potential actions aligned with task completion.
- Mechanism: The world model generates top-K actions based on current state and previous actions, then uses text embedding similarity to validate actions against valid action sets.
- Core assumption: The LLM can rank potential actions by their relevance to task completion when given sufficient context.
- Evidence anchors:
  - [abstract] "We introduce the action proposal of the world model and the policy planning can be completed with the policy verification and action proposal"
  - [section] "One key issue for the action proposal is that the action generated by the world model may not be valid for the game at the current state"
- Break condition: When the action space becomes extremely large (>500 actions) or the task requires highly specific domain knowledge.

### Mechanism 3
- Claim: Combining policy verification and action proposal enables planning without separate critic modules.
- Mechanism: The world model verifies partial policies and generates next actions, creating a closed-loop planning system that evaluates complete action sequences.
- Core assumption: The world model's predictions remain consistent enough across multiple steps to enable planning.
- Evidence anchors:
  - [abstract] "Planning with world models can find the policies solely"
  - [section] "With the prediction of the next states and the action proposal, we can leverage planning methods or search methods to find the policies"
- Break condition: When prediction errors accumulate over long horizons, causing planning failure.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: Understanding the formal framework for decision-making problems and how world models fit into this framework
  - Quick check question: What are the five components of an MDP tuple?

- Concept: Large Language Model prompting techniques
  - Why needed here: World models rely on effective prompt engineering to generate predictions and actions
  - Quick check question: What are the key elements that should be included in prompts for world model predictions?

- Concept: Text embedding similarity
  - Why needed here: Used to validate generated actions against valid action sets in the action proposal task
  - Quick check question: How does cosine similarity between embeddings help in action validation?

## Architecture Onboarding

- Component map: Environment wrapper -> World model interface -> Policy verification module -> Action proposal module -> Planning orchestrator

- Critical path: State → World model prediction → Action validation → Policy verification → Planning decision

- Design tradeoffs:
  - Model size vs. response time (GPT-4o vs. GPT-4o-mini)
  - Prompt complexity vs. accuracy
  - Action generation quantity vs. computational cost

- Failure signatures:
  - Empty JSON responses from LLM
  - Invalid action generation despite valid context
  - Prediction drift over long horizons

- First 3 experiments:
  1. Test world model on simple state prediction with known rule sets
  2. Validate action proposal accuracy with ground truth action spaces
  3. Run policy verification on short action sequences with ρ=0.25

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performances of world models with LLMs change when evaluated on environments requiring advanced scientific knowledge versus common daily tasks?
- Basis in paper: [explicit] The paper mentions that GPT-4o outperforms GPT-4o-mini, especially in tasks requiring domain knowledge, such as "bandage," "hang," and "campfire."
- Why unresolved: The paper does not provide a detailed analysis of the types of tasks that are more challenging for LLMs or the reasons behind their performance differences in scientific versus daily tasks.
- What evidence would resolve it: Comparative performance metrics of world models across tasks categorized by scientific complexity and daily routine, along with qualitative analysis of LLM responses.

### Open Question 2
- Question: What are the underlying causes of the decreased performance of LLMs in long-term decision-making tasks, and how can this issue be mitigated?
- Basis in paper: [explicit] The paper observes that the performance of the world model with LLM decreases for long-term decision-making tasks.
- Why unresolved: The paper does not explore the specific factors contributing to this decline or propose solutions to improve performance in long-term scenarios.
- What evidence would resolve it: Detailed studies on the error accumulation in LLM predictions over extended sequences and experiments testing different strategies to enhance long-term decision-making capabilities.

### Open Question 3
- Question: How does the integration of different functionalities of the world model affect the overall stability and reliability of decision-making processes?
- Basis in paper: [explicit] The paper notes that the combination of different functionalities of the world model introduces additional instabilities in performance.
- Why unresolved: The paper does not investigate the specific interactions between functionalities or suggest methods to enhance stability.
- What evidence would resolve it: Experiments isolating the effects of individual functionalities and their combinations on task performance, along with analyses of the sources of instability.

## Limitations

- The evaluation framework relies heavily on the quality and diversity of the 31 environments from Wang et al. (2023; 2024), which are not fully specified in this paper
- The study focuses on rule-based policies and environments, which may not generalize to more complex, stochastic, or continuous action spaces
- The evaluation does not address potential issues with LLM hallucination or the stability of predictions over very long horizons

## Confidence

**High Confidence**: The observation that GPT-4o outperforms GPT-4o-mini across all three tasks is well-supported by the experimental results and consistent with general expectations about model capabilities.

**Medium Confidence**: The finding that performance degrades for long-term decision-making tasks is plausible given known LLM limitations with context length and attention mechanisms, but the specific degradation rates and thresholds need further validation.

**Medium Confidence**: The claim about additional instabilities when combining different world model functionalities is supported by experimental observations but requires more systematic analysis to determine whether these are fundamental limitations or implementation artifacts.

## Next Checks

1. **Model Ablation Study**: Conduct controlled experiments isolating the effects of context length, prompt complexity, and temperature settings to determine which factors most significantly impact the performance gap between GPT-4o and GPT-4o-mini.

2. **Robustness Testing**: Evaluate world model performance across environments with varying levels of domain complexity, action space sizes (especially testing the >500 action threshold mentioned), and stochasticity to better understand generalizability limits.

3. **Long Horizon Validation**: Extend the policy planning experiments to test horizons significantly longer than currently evaluated (e.g., 20+ steps) to quantify the exact nature and rate of performance degradation, and test whether techniques like chain-of-thought reasoning or state summarization can mitigate these issues.