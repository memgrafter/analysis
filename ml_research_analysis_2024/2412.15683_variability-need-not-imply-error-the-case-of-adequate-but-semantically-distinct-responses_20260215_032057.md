---
ver: rpa2
title: 'Variability Need Not Imply Error: The Case of Adequate but Semantically Distinct
  Responses'
arxiv_id: '2412.15683'
source_url: https://arxiv.org/abs/2412.15683
tags:
- question
- probar
- answer
- responses
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reliably estimating when a
  language model can produce correct responses to prompts, particularly in open-ended
  settings where prompts elicit multiple adequate but semantically distinct responses.
  The authors propose PROBAR (Probability of Adequate Responses), a confidence measure
  based on the probability an LM assigns to responses judged adequate by a task-specific
  classifier.
---

# Variability Need Not Imply Error: The Case of Adequate but Semantically Distinct Responses

## Quick Facts
- arXiv ID: 2412.15683
- Source URL: https://arxiv.org/abs/2412.15683
- Reference count: 40
- PROBAR consistently outperforms baselines in terms of AUROC (0.7-0.9 range) and precision-coverage tradeoffs, particularly for ambiguous prompts where semantic variation does not imply error.

## Executive Summary
This paper addresses the problem of reliably estimating when a language model can produce correct responses to prompts, particularly in open-ended settings where prompts elicit multiple adequate but semantically distinct responses. The authors propose PROBAR (Probability of Adequate Responses), a confidence measure based on the probability an LM assigns to responses judged adequate by a task-specific classifier. They evaluate PROBAR against semantic entropy and other baselines for selective prediction across QA datasets (Abg-COQA, AmbigQA) and next-word prediction (Provo Corpus) using OPT models. PROBAR consistently outperforms baselines in terms of AUROC (0.7-0.9 range) and precision-coverage tradeoffs, particularly for ambiguous prompts where semantic variation does not imply error. The method shows robustness across prompts with varying degrees of open-endedness.

## Method Summary
The authors propose PROBAR (Probability of Adequate Responses) as a confidence measure that estimates the probability an LM assigns to adequate responses. For each prompt, they generate 10 unbiased samples using the LM, classify each response as adequate or inadequate using a task-specific adequacy classifier, and compute PROBAR as the proportion of adequate responses. They compare PROBAR to baseline uncertainty metrics (entropy, semantic entropy, P(Adequate)) using AUROC and precision-coverage plots across three datasets: Abg-COQA (context-ambiguous and non-ambiguous prompts), AmbigQA (ambiguous prompts), and Provo Corpus (next-word prediction). The evaluation uses OPT models ranging from 2.7B to 30B parameters.

## Key Results
- PROBAR consistently outperforms entropy, semantic entropy, and P(Adequate) baselines in AUROC (0.7-0.9 range) across all tested datasets
- PROBAR shows robust performance across prompts with varying degrees of ambiguity/open-endedness
- Particularly effective for ambiguous prompts where semantic variation does not imply error, outperforming semantic entropy which fails in such cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PROBAR correlates with model reliability because it measures the probability that sampled responses are judged adequate, which captures semantic variability beyond surface form.
- Mechanism: PROBAR approximates the probability that an LM assigns to adequate responses by sampling responses, using a task-specific adequacy classifier, and computing the proportion of responses judged adequate.
- Core assumption: The adequacy classifier accurately reflects whether a response is semantically adequate to the prompt, even when multiple distinct meanings are valid.
- Evidence anchors:
  - [abstract] PROBAR consistently outperforms baselines in terms of AUROC (0.7-0.9 range) and precision-coverage tradeoffs, particularly for ambiguous prompts where semantic variation does not imply error.
  - [section] Kuhn et al. (semantic entropy; 2022b) regard semantic variation amongst sampled responses as evidence that the model ‘struggles’ with the prompt and that the LM is likely to err. We argue that semantic variability need not imply error—this being especially intuitive in open-ended settings, where prompts elicit multiple adequate but semantically distinct responses.
- Break condition: If the adequacy classifier fails to distinguish between semantically adequate and inadequate responses, PROBAR loses its discriminative power.

### Mechanism 2
- Claim: Semantic entropy fails on ambiguous prompts because it clusters responses by semantic equivalence, which cannot distinguish between semantically distinct but adequate responses.
- Mechanism: SE maps sampled responses to semantic clusters using an NLI model and estimates entropy over those clusters. High entropy indicates low confidence.
- Core assumption: The NLI model can accurately cluster responses by semantic equivalence, and semantic homogeneity implies model reliability.
- Evidence anchors:
  - [section] Kuhn et al. (2022b) associate variation in meaning with propensity for error, showing their semantic entropy to outperform entropy’s potential for selective prediction. However, we do not regard semantic variation as propensity for error in open-ended settings.
  - [section] For the second ambiguous question (x = ‘What is a date?’) allows for multiple, semantically distinct answers (due to ambiguities inherent to ‘date’). Here, SE makes model D appear semantically certain (single cluster), while models C and E exhibit complete semantic uncertainty (uniform distribution over clusters). As it turns out, SE cannot distinguish models C and E, despite only the former having no inadequate responses.
- Break condition: If prompts have no ambiguity or underspecification, semantic homogeneity does imply model reliability, and SE works well.

### Mechanism 3
- Claim: PROBAR's performance is robust to prompt open-endedness because it focuses on adequacy rather than semantic similarity, capturing when a model can respond correctly despite multiple valid answers.
- Mechanism: PROBAR estimates the probability of adequate responses using a task-specific classifier, which can capture different notions of adequacy (e.g., plausibility, support from document, grammatical correctness) beyond semantic equivalence.
- Core assumption: The adequacy classifier can be trained to capture the relevant notion of adequacy for the task and prompt type.
- Evidence anchors:
  - [abstract] We evaluate PROBAR as a measure of confidence in selective prediction with OPT models (in two QA datasets and in next-word prediction, for English) and find PROBAR to outperform semantic entropy across prompts with varying degrees of ambiguity/open-endedness.
  - [section] We argue that rather than semantic similarity amongst sampled responses, we need to reason about whether or not sampled responses are generally adequate to the prompt—see Figure 1. We propose to estimate the Probability that an LM assigns to Adequate Responses (PROBAR) and regard that as a notion of confidence in the LM’s ability to respond to a prompt.
- Break condition: If the adequacy classifier is poorly trained or the notion of adequacy is not well-defined for the task, PROBAR's performance degrades.

## Foundational Learning

- Concept: Uncertainty quantification in language models
  - Why needed here: PROBAR is an uncertainty quantifier that estimates the probability of adequate responses. Understanding uncertainty quantification methods is crucial for interpreting PROBAR's results and comparing it to baselines like entropy and semantic entropy.
  - Quick check question: What is the difference between aleatoric and epistemic uncertainty, and how do they relate to PROBAR?

- Concept: Sampling-based approximations
  - Why needed here: PROBAR uses Monte Carlo sampling to estimate the probability of adequate responses. Understanding sampling methods and their limitations is important for interpreting PROBAR's results and designing experiments.
  - Quick check question: How does the number of samples affect the accuracy of PROBAR's estimate, and what is the trade-off between sample size and computational cost?

- Concept: Task-specific classifiers
  - Why needed here: PROBAR relies on a task-specific adequacy classifier to judge whether responses are adequate. Understanding how to design and train such classifiers is crucial for implementing PROBAR in different tasks and domains.
  - Quick check question: What are the key considerations when designing an adequacy classifier for a specific task, and how can we evaluate its performance?

## Architecture Onboarding

- Component map:
  LM generator -> Sampling algorithm -> Adequacy classifier -> PROBAR estimator -> Evaluation protocol

- Critical path:
  - Generate N sampled responses using the LM and sampling algorithm
  - Classify each response as adequate or inadequate using the adequacy classifier
  - Compute PROBAR as the proportion of adequate responses
  - Use PROBAR as a confidence score to accept or reject the LM's prediction
  - Evaluate PROBAR's performance using metrics like AUROC and precision-coverage tradeoffs

- Design tradeoffs:
  - Sample size vs. computational cost: Larger sample sizes improve PROBAR's accuracy but increase computational cost
  - Adequacy classifier complexity vs. accuracy: More complex classifiers may capture finer-grained notions of adequacy but require more training data and computational resources
  - Task-specific vs. general adequacy classifiers: Task-specific classifiers may perform better on their target tasks but require more effort to design and train

- Failure signatures:
  - PROBAR fails to distinguish between semantically adequate and inadequate responses: The adequacy classifier is not well-trained or the notion of adequacy is not well-defined
  - PROBAR is insensitive to prompt open-endedness: The adequacy classifier only captures surface-level features or is too lenient in its judgments
  - PROBAR's performance degrades on certain prompt types: The adequacy classifier is not robust to the specific characteristics of those prompts (e.g., length, domain, style)

- First 3 experiments:
  1. Implement PROBAR on a small QA dataset with known ambiguities and evaluate its performance compared to entropy and semantic entropy.
  2. Ablation study: Vary the sample size and adequacy classifier complexity to understand their impact on PROBAR's performance.
  3. Cross-task evaluation: Apply PROBAR to different task types (e.g., QA, summarization, dialogue) and assess its generalization ability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PROBAR perform when responses are expected to be longer than those typically generated for short-answer QA tasks?
- Basis in paper: [inferred] The authors note that "we only evaluated PROBAR in a setting where only short responses were expected" and acknowledge that "if one needed to employ an adequacy classifier in a QA setting with longer generations expected, they would need to re-assess the classifier's performance."
- Why unresolved: The current evaluation focuses on short-response QA tasks (Abg-COQA, AmbigQA) and next-word prediction. The performance of PROBAR for tasks requiring longer, more complex responses remains unknown.
- What evidence would resolve it: Experiments applying PROBAR to tasks with longer expected responses (e.g., open-ended dialogue, long-form QA, story generation) with appropriate adequacy classifiers evaluated on these longer response types.

### Open Question 2
- Question: Can PROBAR be effectively approximated by learning to predict it directly, similar to how Kossen et al. (2024) predict semantic entropy?
- Basis in paper: [explicit] The authors mention this possibility in their limitations section, stating "future research could investigate whether one could learn to predict PROBAR (similar to how Kossen et al. (2024) predict SE)."
- Why unresolved: The computational expense of sampling-based confidence methods like PROBAR motivates exploring whether it can be learned directly. This has not been investigated in the current work.
- What evidence would resolve it: Development and evaluation of a model that predicts PROBAR values directly from the prompt and/or a few sampled responses, comparing its performance and computational efficiency to the sampling-based approach.

### Open Question 3
- Question: What complementary information, if any, can be gained by simultaneously considering PROBAR alongside semantic entropy?
- Basis in paper: [explicit] The authors discuss this potential in their discussion section, suggesting that "simultaneously high SE and high PROBAR might detect prompts for which the LM models plausible variability well, while simultaneously high SE and low PROBAR might detect prompts about which the model is rather ignorant."
- Why unresolved: While the authors propose this hypothesis, they do not empirically investigate the relationship between PROBAR and semantic entropy across different types of prompts and model behaviors.
- What evidence would resolve it: Systematic analysis of cases where PROBAR and semantic entropy give different predictions, categorizing the types of prompts and model behaviors where each metric is more informative.

## Limitations
- PROBAR's performance critically depends on the adequacy classifier's quality, yet comprehensive error analysis of these classifiers is missing
- With only 10 samples per prompt, PROBAR estimates may be noisy, particularly for prompts where adequate responses are rare
- Results are demonstrated primarily on three datasets (Abg-COQA, AmbigQA, Provo Corpus), limiting generalizability to other domains

## Confidence
- High Confidence: PROBAR outperforms entropy and semantic entropy on AUROC metrics for the tested datasets (0.7-0.9 range)
- Medium Confidence: PROBAR shows robust performance across prompts with varying degrees of open-endedness
- Low Confidence: Claims about PROBAR's superiority over P(Adequate) as a baseline, due to limited comparison details

## Next Checks
1. Ablation on Sample Size: Systematically evaluate PROBAR's performance across different sample sizes (5, 10, 20, 50) to quantify the trade-off between estimation accuracy and computational cost.
2. Classifier Error Analysis: Conduct detailed error analysis of the adequacy classifiers, including false positive/negative rates across different prompt types and domains.
3. Cross-Domain Evaluation: Test PROBAR on at least two additional task types (e.g., summarization, dialogue) to assess generalizability beyond the QA and next-word prediction tasks examined.