---
ver: rpa2
title: Poro 34B and the Blessing of Multilinguality
arxiv_id: '2404.01856'
source_url: https://arxiv.org/abs/2404.01856
tags:
- language
- finnish
- data
- poro
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of training large generative
  models for smaller languages due to limited training data. The authors propose a
  multilingual training approach, training Poro 34B, a 34 billion parameter model
  on 1 trillion tokens of Finnish, English, and programming languages, including 8
  billion tokens of Finnish-English translation pairs.
---

# Poro 34B and the Blessing of Multilinguality

## Quick Facts
- arXiv ID: 2404.01856
- Source URL: https://arxiv.org/abs/2404.01856
- Reference count: 14
- Primary result: 34B parameter multilingual model trained on Finnish, English, and programming languages achieving state-of-the-art Finnish performance and competitive English/code results

## Executive Summary
This paper introduces Poro 34B, a large-scale multilingual generative model trained on 1 trillion tokens spanning Finnish, English, and programming languages. The model addresses the challenge of limited training data for smaller languages by leveraging multilingual training. Poro 34B achieves state-of-the-art performance on Finnish benchmarks (66.28 average score vs 49.69 for best monolingual Finnish model) while maintaining competitive performance on English and code tasks. Notably, the model demonstrates strong translation capabilities, outperforming dedicated translation models and matching GPT-4 levels on English-Finnish and Finnish-English translation tasks.

## Method Summary
Poro 34B is a 34 billion parameter transformer model trained on a diverse corpus of 1 trillion tokens across three domains: Finnish, English, and programming languages. The training incorporated 8 billion Finnish-English translation pairs to enhance cross-lingual capabilities. The model employs standard transformer architecture with attention mechanisms and is trained using next-token prediction objectives. The multilingual approach leverages transfer learning across languages to improve performance on resource-constrained Finnish while maintaining strong English and code capabilities.

## Key Results
- Achieved 66.28 average score on Finnish benchmarks, substantially outperforming the best monolingual Finnish model (49.69)
- Competitive performance on English tasks with 50.57 average score and code tasks with 41.80 average score
- Translation capabilities that outperform dedicated translation models and match GPT-4 performance in English-Finnish and Finnish-English translation

## Why This Works (Mechanism)
The multilingual training approach enables knowledge transfer across languages, allowing the model to leverage abundant English and programming language data to improve Finnish performance. The inclusion of translation pairs during training creates cross-lingual connections that enhance zero-shot and few-shot translation capabilities. The large scale (34B parameters) provides sufficient capacity to model multiple languages and domains simultaneously without catastrophic forgetting.

## Foundational Learning
- **Multilingual pretraining**: Training on multiple languages improves low-resource language performance through transfer learning; quick check: compare monolingual vs multilingual performance on Finnish benchmarks
- **Scale effects in language models**: Larger models show better few-shot learning and generalization; quick check: evaluate performance scaling with parameter count
- **Cross-lingual representation learning**: Translation pairs help align semantic representations across languages; quick check: measure semantic similarity between aligned representations

## Architecture Onboarding

**Component Map:**
Input Tokenizer -> Embedding Layer -> Transformer Blocks (34B parameters) -> Output Head

**Critical Path:**
Tokenization → Embedding → Multi-head Attention → Feed-forward Network → Layer Normalization → Output Projection

**Design Tradeoffs:**
- Large parameter count (34B) provides capacity but increases computational cost
- Multilingual training improves Finnish performance but may slightly reduce English-only specialization
- Translation pair inclusion enhances cross-lingual abilities but requires additional data curation

**Failure Signatures:**
- Performance degradation on Finnish-specific tasks when translation pairs are removed
- Reduced code generation accuracy when English-only training data is reduced
- Loss of Finnish cultural/linguistic nuance when model size is reduced below critical threshold

**First Experiments:**
1. Fine-tune on Finnish-only subset and compare to full multilingual model
2. Remove translation pairs and evaluate impact on zero-shot translation
3. Test cross-lingual transfer to additional low-resource languages

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Lack of detailed training methodology and hyperparameter specifications
- Unclear evaluation protocols and whether multiple runs were averaged
- No ablation studies showing contribution of different data sources to final performance

## Confidence
- Finnish benchmark improvements (66.28 vs 49.69): Medium confidence - verifiable but dependent on evaluation setup
- English and code task performance: Medium confidence - competitive but not leading-edge
- Translation performance matching GPT-4: Low confidence - direct comparison difficult without standardized evaluation

## Next Checks
1. Replicate the training setup with publicly available datasets to verify claimed improvements under similar conditions
2. Conduct ablation studies to determine the contribution of different data sources (Finnish, English, programming languages, translation pairs) to final performance
3. Test the model's generalization to additional low-resource languages not included in original training data to validate multilinguality benefits