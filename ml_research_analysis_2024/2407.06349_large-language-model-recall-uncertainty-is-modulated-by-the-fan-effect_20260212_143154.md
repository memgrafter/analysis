---
ver: rpa2
title: Large Language Model Recall Uncertainty is Modulated by the Fan Effect
arxiv_id: '2407.06349'
source_url: https://arxiv.org/abs/2407.06349
tags:
- effect
- effects
- arxiv
- human
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) exhibit
  human-like fan effects, a cognitive phenomenon where response uncertainty increases
  with overlapping features. Two experiments were conducted using Mistral and SOLAR
  models to measure recall uncertainty via token probabilities under varying typicality
  and feature overlap conditions.
---

# Large Language Model Recall Uncertainty is Modulated by the Fan Effect

## Quick Facts
- arXiv ID: 2407.06349
- Source URL: https://arxiv.org/abs/2407.06349
- Reference count: 11
- Large language models exhibit human-like fan effects in recall uncertainty

## Executive Summary
This paper investigates whether large language models (LLMs) exhibit fan effects, a cognitive phenomenon where response uncertainty increases with overlapping features. Two experiments using Mistral and SOLAR models demonstrate that LLM recall uncertainty, measured via token probabilities, is influenced by the fan effect. Results show that both models exhibit patterns consistent with human behavior: higher uncertainty when querying typical items from pretraining data, and stronger effects with repeated person features than place features. The findings indicate that fan effects are expressions of typicality and highlight the need for human-consistent uncertainty measures in LLMs.

## Method Summary
The paper conducts two experiments: one measuring typicality effects from pretraining data and another measuring in-context effects from repeated person-place features. It uses token probabilities (particularly for "present" and "absent" completions) as a proxy for recall uncertainty. The experiments employ in-context learning with counterfactual prompting to obtain probabilities for presence/absence judgments, analyzing correlations between probabilities and typicality/frequency measures to detect fan effects. PopulationLM is used to create perturbed populations for robustness testing.

## Key Results
- LLM recall uncertainty correlates with fan values in both pretraining and in-context experiments
- Differential fan effects are stronger for person features than place features
- Fan effects disappear when uncertainty is mitigated, suggesting they are expressions of uncertainty
- The phenomenon is consistent across multiple model architectures (Mistral, SOLAR, RoBERTa, GPT-2, Llama-2, Llama-3)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fan effects arise from typicality-based uncertainty modulation in LLM token probabilities
- Mechanism: When querying absent items, LLMs experience uncertainty that increases with typicality. This manifests as lower probability assigned to "absent" for more typical items due to the model's uncertainty about whether it simply missed seeing the item during processing.
- Core assumption: LLM uncertainty behavior is analogous to human cognitive uncertainty in categorization tasks
- Evidence anchors:
  - [abstract] "LLM recall uncertainty, measured via token probability, is influenced by the fan effect"
  - [section] "The fan effect is canonically explained as a modulation of human uncertainty based on the categorical distance from an exemplar"
  - [corpus] Weak evidence - corpus neighbors focus on attention and memory but don't directly support this mechanism
- Break condition: When uncertainty is mitigated (e.g., queried items are present in context), the fan effect disappears

### Mechanism 2
- Claim: Differential fan effects occur due to asymmetric feature importance in concept representation
- Mechanism: When concepts combine person and place features, the fan effect is stronger for person features than place features because person features carry more discriminative weight in the model's learned representations. This creates asymmetric uncertainty modulation.
- Core assumption: Feature type influences concept representation strength differently in the model
- Evidence anchors:
  - [abstract] "stronger effects with repeated person features than place features, aligning with differential fan effects in humans"
  - [section] "In the TAG, we see a stronger correlation with the fan value of the person feature than with the fan value of the place feature"
  - [corpus] No direct evidence - corpus neighbors don't address feature asymmetry
- Break condition: When feature types are swapped or when both features have equal fan values

### Mechanism 3
- Claim: Fan effects are expressions of the same underlying phenomenon as typicality effects
- Mechanism: Both fan effects and typicality effects arise from how frequently features or items appear in training data. High fan values (frequent features) create uncertainty because they can apply to multiple concepts, while low fan values create confidence because they're more distinctive.
- Core assumption: Fan effects and typicality effects share a common computational basis in the model
- Evidence anchors:
  - [abstract] "fan effects and typicality are expressions of the same phenomena"
  - [section] "fan effects are found by Silber and Fisher (1989) in probabilistic categories created by COBWEB to be a special case of another phenomenon known as the typicality effect"
  - [corpus] Weak evidence - corpus neighbors discuss memory but not the typicality-fan relationship
- Break condition: When items are presented in isolation without feature overlap

## Foundational Learning

- Concept: Token probability as uncertainty measure
  - Why needed here: The paper uses token probabilities (particularly for "present" and "absent" completions) as a proxy for recall uncertainty, which is central to detecting fan effects
  - Quick check question: Why does the paper use token probabilities instead of direct confidence scores to measure uncertainty?

- Concept: Typicality vs. frequency
  - Why needed here: The paper distinguishes between typicality (how representative an item is within its category) and raw frequency, showing that typicality modulates uncertainty even when frequency is controlled
  - Quick check question: How does typicality differ from simple frequency in the context of fan effects?

- Concept: In-context learning vs. pretraining knowledge
  - Why needed here: The paper demonstrates fan effects from both pretraining (typicality) and in-context presentation (repeated features), requiring understanding of how these knowledge sources interact
  - Quick check question: What's the difference between fan effects induced by pretraining data versus those induced in-context?

## Architecture Onboarding

- Component map: Model token prediction layer -> Attention mechanism -> PopulationLM perturbed populations -> Token probability extraction via minicons
- Critical path: 1) Construct prompts with specific feature overlap patterns, 2) Run through model populations, 3) Extract token probabilities for canary words ("present"/"absent"), 4) Calculate correlations between probabilities and fan values
- Design tradeoffs: Using median aggregation across populations trades computational cost for robustness against outliers. Using canary words trades precision for interpretability.
- Failure signatures: If no significant correlations appear, this could indicate: 1) The model doesn't learn fan effects, 2) The feature types chosen don't induce strong enough effects, 3) The uncertainty measure isn't capturing the right phenomenon
- First 3 experiments:
  1. Replicate the bird category typicality experiment with a different model to verify generalizability
  2. Test with person names instead of occupations to see if differential fan effects strengthen
  3. Add a time-delay simulation by injecting semantic noise between concept presentation and query

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do SOLAR and Mistral exhibit divergent patterns in TPG predictions when uncertainty is mitigated?
- Basis in paper: [explicit] The paper notes that SOLAR and Mistral show different disruption patterns in TPG predictions when uncertainty is mitigated.
- Why unresolved: The paper does not provide a clear explanation for why these models diverge in their behavior under mitigated uncertainty.
- What evidence would resolve it: Comparative analysis of model architectures and training processes, or experiments that manipulate uncertainty mitigation methods.

### Open Question 2
- Question: How do fan effects manifest in humans under mitigated uncertainty, and how does this compare to LLM behavior?
- Basis in paper: [inferred] The paper suggests that human cognitive experiments could explore fan effects under mitigated uncertainty, as this is not well-documented.
- Why unresolved: There is no existing cognitive science literature addressing fan effects in humans under mitigated uncertainty, making direct comparisons difficult.
- What evidence would resolve it: Conducting human experiments that simulate mitigated uncertainty and comparing results with LLM behavior.

### Open Question 3
- Question: What is the impact of using unique identifiers like names instead of occupations on differential fan effects in LLMs?
- Basis in paper: [inferred] The paper suggests that using names instead of occupations might yield stronger differential fan effects, based on the mental models mechanism.
- Why unresolved: The paper does not empirically test this hypothesis, leaving the potential impact of identifier type on fan effects unexplored.
- What evidence would resolve it: Experiments comparing fan effects using names versus occupations as identifiers in LLMs.

## Limitations

- Limited feature type exploration (only person/place features tested)
- Uncertainty measure validation not rigorously established
- No investigation of how model size or architecture variations affect fan effect strength
- Limited exploration of feature interaction effects beyond simple repetition

## Confidence

**Claim 1: LLMs exhibit fan effects similar to humans** - High confidence
The experimental results show consistent correlations between token probabilities and fan values across multiple models and experimental conditions. The patterns align with established human cognitive psychology findings.

**Claim 2: Fan effects are expressions of typicality** - Medium confidence
While the paper establishes correlation between fan effects and typicality, the causal mechanism linking these phenomena requires further validation. The theoretical connection is supported by references to human cognitive literature but needs direct experimental verification.

**Claim 3: Differential fan effects exist between feature types** - Medium confidence
The in-context experiments show stronger person feature effects than place features, but this could be influenced by feature selection rather than inherent feature type differences. More systematic feature type comparisons are needed.

## Next Checks

1. **Alternative uncertainty measures validation**: Replicate key experiments using multiple uncertainty metrics (entropy, prediction confidence scores, calibration error) to verify that token probability is the most appropriate uncertainty proxy.

2. **Multi-feature complexity test**: Design experiments with compound features (e.g., person+occupation+location) to test how fan effects scale with feature dimensionality and interaction complexity.

3. **Cross-linguistic replication**: Test whether fan effects manifest similarly across different language models (e.g., English vs. multilingual models) to determine if the phenomenon is language-specific or universal to LLM architectures.