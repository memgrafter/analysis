---
ver: rpa2
title: 'WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series Forecasting'
arxiv_id: '2410.22649'
source_url: https://arxiv.org/abs/2410.22649
tags:
- wavelet
- time
- series
- domain
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WaveRoRA, a novel approach for multivariate
  time series forecasting that addresses limitations in existing Transformer-based
  models. The authors propose using wavelet transforms to extract features in the
  wavelet domain, which captures both time and frequency information, allowing for
  better analysis of local characteristics at different scales.
---

# WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2410.22649
- Source URL: https://arxiv.org/abs/2410.22649
- Authors: Aobo Liang; Yan Sun; Nadra Guizani
- Reference count: 40
- Key outcome: WaveRoRA outperforms state-of-the-art models while maintaining lower computational costs

## Executive Summary
WaveRoRA introduces a novel approach for multivariate time series forecasting that combines wavelet transforms with a new attention mechanism called Rotary Route Attention (RoRA). The method addresses limitations in existing Transformer-based models by leveraging wavelet domain features to capture both time and frequency information, while using RoRA to overcome the quadratic computational complexity of standard Softmax attention. Extensive experiments on eight real-world datasets demonstrate that WaveRoRA achieves state-of-the-art performance with significant improvements in MSE and MAE metrics while maintaining computational efficiency.

## Method Summary
WaveRoRA transforms time series into the wavelet domain using Discrete Wavelet Transform (DWT), then applies RoRA attention to capture inter-series dependencies. The method uses instance normalization on input time series, followed by J-level DWT decomposition (typically 4-level with Sym3 wavelet). Multi-scale wave embedding layers map wavelet coefficients to uniform dimension tokens, which are processed through N-layer WaveRoRA encoders with RoRA attention. The model employs routing tokens to aggregate information with linear complexity, combined with rotary position embeddings for relative positional information. Multi-scale wavelet predictors map embeddings back to wavelet coefficients, and Inverse DWT (IDWT) reconstructs final predictions. The approach maintains lower computational costs compared to standard Transformer models while achieving superior forecasting accuracy.

## Key Results
- WaveRoRA reduces MSE by 5.91% and MAE by 3.50% compared to the current SOTA Transformer-based model (iTransformer)
- Model outperforms state-of-the-art on 7 of 8 tested datasets
- Most significant improvements observed on datasets with many variables and pronounced periodic characteristics
- Maintains lower computational complexity compared to standard Transformer-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Wavelet transforms capture both time and frequency information, allowing for better analysis of local characteristics at different scales.
- Mechanism: Discrete Wavelet Transform (DWT) decomposes a time series into multi-scale high-pass and low-pass components, preserving time-frequency localization that DFT lacks.
- Core assumption: Wavelet coefficients from different decomposition levels contain complementary information about periodic and trend components that are not adequately captured by either time or frequency domain features alone.
- Evidence anchors:
  - [abstract] "The wavelet domain integrates both time and frequency information, allowing for the analysis of local characteristics of signals at different scales."
  - [section] "Wavelet transform decomposes a signal into different frequency and time scales by using scaled and shifted versions of a mother wavelet [22]."
  - [corpus] Weak - no direct citations about wavelet effectiveness in multivariate time series forecasting

### Mechanism 2
- Claim: Rotary Route Attention (RoRA) achieves linear complexity while maintaining expressivity through routing tokens and rotary position embeddings.
- Mechanism: RoRA uses a small number of routing tokens to aggregate information from KV matrices and redistribute it to the Q matrix, combined with rotary position embeddings to inject relative positional information.
- Core assumption: The routing tokens can effectively summarize and redistribute information without losing critical dependencies, and rotary embeddings can provide sufficient positional information without the full quadratic complexity.
- Evidence anchors:
  - [abstract] "RoRA utilizes rotary position embeddings to inject relative positional information to sequence tokens and introduces a small number of routing tokens r to aggregate information from the KV matrices and redistribute it to the Q matrix, offering linear complexity."
  - [section] "RoRA could take advantages of the nonlinearity of Softmax and the mapping mechanism of R."
  - [corpus] Weak - limited corpus evidence on rotary embeddings combined with routing tokens

### Mechanism 3
- Claim: Combining wavelet domain features with RoRA enables superior capture of inter-series dependencies compared to time-domain or frequency-domain approaches alone.
- Mechanism: The wavelet learning framework maps input to wavelet embeddings, then applies RoRA to capture inter-series dependencies in the wavelet domain, leveraging both high-pass and low-pass components.
- Core assumption: Inter-series dependencies manifest differently across wavelet decomposition levels, and RoRA can effectively model these scale-specific dependencies.
- Evidence anchors:
  - [abstract] "We further propose WaveRoRA, which leverages RoRA to capture inter-series dependencies in the wavelet domain."
  - [section] "RoRA uses the relative positional information to facilitate in identifying the interactions between variables."
  - [corpus] Weak - no direct evidence of RoRA's effectiveness for inter-series dependency modeling

## Foundational Learning

- Concept: Discrete Wavelet Transform and its implementation
  - Why needed here: Understanding how DWT decomposes time series into multi-scale components is crucial for implementing the wavelet learning framework
  - Quick check question: How does the downsampling factor change across DWT decomposition levels, and why is this important for reconstruction?

- Concept: Attention mechanisms and computational complexity
  - Why needed here: RoRA is designed to overcome the quadratic complexity of standard Softmax attention, so understanding this limitation is essential
  - Quick check question: What is the computational complexity of standard Softmax attention versus linear attention, and how does sequence length affect this?

- Concept: Positional encoding techniques in transformers
  - Why needed here: Rotary position embeddings are a key component of RoRA, so understanding how they inject positional information is critical
  - Quick check question: How do rotary position embeddings differ from sinusoidal positional encodings, and what advantages do they offer for sequence modeling?

## Architecture Onboarding

- Component map:
  Input normalization → DWT decomposition → Multi-scale wave embedding → Series-wise token transposition → RoRA encoding → Wavelet-wise normalization → Wave predictor → IDWT reconstruction

- Critical path:
  The flow from DWT decomposition through wave embedding to RoRA encoding and back through wave predictors to IDWT reconstruction is the critical path for both accuracy and computational efficiency

- Design tradeoffs:
  - Wavelet basis selection vs. computational cost: More sophisticated wavelets may improve accuracy but increase complexity
  - Number of DWT levels (J) vs. information retention: Higher J captures finer details but may lose trend information
  - Number of routing tokens (r) vs. expressiveness: Larger r captures more dependencies but approaches quadratic complexity

- Failure signatures:
  - Poor reconstruction quality: May indicate insufficient wavelet decomposition levels or inappropriate wavelet basis
  - Slow training/inference: Could suggest too many routing tokens or inefficient implementation of RoRA
  - Overfitting on high-dimensional data: Might indicate insufficient dropout or regularization in wave embedding layers

- First 3 experiments:
  1. Vary DWT decomposition levels (J) on a single dataset to find the optimal balance between capturing periodic information and retaining trend information
  2. Compare RoRA with standard Softmax attention on datasets with different numbers of variables to evaluate the routing mechanism's effectiveness
  3. Test different wavelet bases (Haar, Symlet, Coiflet) on datasets with varying periodicity characteristics to determine optimal basis selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed wavelet learning framework be effectively integrated with other types of backbones beyond those capturing inter-series dependencies?
- Basis in paper: [explicit] The authors mention that the wavelet learning framework currently fits well for models capturing inter-series dependencies and suggest future work could explore combining wavelet domain features with other types of backbones.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis on how the wavelet learning framework performs when integrated with different types of neural network architectures.
- What evidence would resolve it: Empirical studies comparing the performance of wavelet-based models with various backbone architectures (e.g., CNNs, RNNs, Graph Neural Networks) on multivariate time series forecasting tasks.

### Open Question 2
- Question: Is there an automatic approach to adaptively select the best wavelet function for specific time series data patterns?
- Basis in paper: [explicit] The authors note that different wavelet functions exhibit potentials for processing MTS data with varying patterns and propose finding an automatic approach to select the best wavelet function for specific data as future research.
- Why unresolved: The paper only evaluates a few manually selected wavelet functions (Haar, Symlet, Coiflet) and does not provide any methodology or evidence for automatic wavelet selection.
- What evidence would resolve it: Development and validation of a method that can automatically choose the optimal wavelet function based on the characteristics of the input time series data, along with experimental results showing improved forecasting accuracy.

### Open Question 3
- Question: How does the proposed WaveRoRA model perform on applications beyond time series forecasting, such as Large Time Series Models and AIOps?
- Basis in paper: [explicit] The authors mention the potential of applying WaveRoRA to applications like Large Time Series Model and AIOps in the future.
- Why unresolved: The paper only focuses on the performance of WaveRoRA on multivariate time series forecasting tasks and does not provide any evidence or analysis of its effectiveness on other applications.
- What evidence would resolve it: Experimental results and comparative analysis of WaveRoRA's performance on diverse applications like Large Time Series Models and AIOps, demonstrating its versatility and effectiveness beyond time series forecasting.

## Limitations

- Key implementation details of RoRA mechanism (routing token aggregation and redistribution) remain underspecified
- Performance improvements, while statistically significant on aggregate, show mixed results across individual datasets
- Choice of wavelet basis (Sym3) and decomposition levels (J=4) appears arbitrary without empirical justification across different dataset characteristics

## Confidence

**High Confidence** (Supporting Evidence: Experimental Results)
- WaveRoRA achieves state-of-the-art performance on 7 of 8 datasets tested
- The model maintains lower computational complexity compared to standard Transformer-based approaches
- MSE and MAE improvements are consistent across different prediction horizons (H = 96, 192, 336, 720)

**Medium Confidence** (Supporting Evidence: Theoretical Justification, Limited Empirical Validation)
- Wavelet domain transformation effectively captures time-frequency characteristics
- Rotary position embeddings combined with routing tokens provide sufficient positional information while maintaining linear complexity
- The multi-scale wavelet learning framework appropriately handles different frequency components

**Low Confidence** (Limited Supporting Evidence, Complex Implementation Details)
- The specific implementation details of RoRA mechanism are not fully specified
- The choice of hyperparameters (wavelet basis, decomposition levels) lacks systematic justification
- The routing mechanism's ability to capture complex inter-series dependencies across diverse datasets requires further validation

## Next Checks

1. **Implementation Verification**: Reproduce the RoRA mechanism implementation with focus on the routing token aggregation and redistribution process. Compare performance with a baseline that uses standard Softmax attention to verify the claimed linear complexity benefits.

2. **Ablation Study on Wavelet Parameters**: Systematically vary the wavelet basis functions and decomposition levels across datasets with different characteristics (periodic vs. non-periodic, high vs. low dimensionality) to determine optimal configurations and validate the claimed benefits of wavelet domain transformation.

3. **Computational Complexity Analysis**: Measure actual runtime and memory usage during training and inference across different dataset sizes and compare with theoretical complexity claims. Verify that the routing mechanism maintains linear complexity as sequence length and variable count increase.