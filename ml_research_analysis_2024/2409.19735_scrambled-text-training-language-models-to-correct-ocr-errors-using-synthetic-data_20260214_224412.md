---
ver: rpa2
title: 'Scrambled text: training Language Models to correct OCR errors using synthetic
  data'
arxiv_id: '2409.19735'
source_url: https://arxiv.org/abs/2409.19735
tags:
- data
- corruption
- text
- synthetic
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that synthetic data can be used to effectively
  fine-tune language models for correcting OCR errors in historical documents. The
  approach uses a Markov corruption process to generate synthetic corrupted text,
  which is then used to train models that reduce character error rates by 55% and
  word error rates by 32% compared to baseline models.
---

# Scrambled text: training Language Models to correct OCR errors using synthetic data

## Quick Facts
- arXiv ID: 2409.19735
- Source URL: https://arxiv.org/abs/2409.19735
- Reference count: 40
- Language models trained on synthetic data reduce character error rate by 55% and word error rate by 32% over baseline models

## Executive Summary
This paper presents a novel approach to training language models for OCR error correction using synthetic data. The method employs a Markov corruption model to learn character-level error patterns from real OCR datasets, then generates synthetic corrupted text that mimics actual OCR errors. By fine-tuning a Llama-3.1 8B model on this synthetic data, the approach achieves significant improvements in OCR correction accuracy, outperforming models trained on real data and reducing character error rates by 55% and word error rates by 32%.

## Method Summary
The approach generates synthetic training data by first creating 11,000 synthetic 19th century newspaper articles using GPT-4o, then corrupting them using a Markov model that learns conditional character-level error probabilities from real OCR datasets (NCSE, SMH, CA, BLN600). The corrupted text is used to fine-tune a Llama-3.1 8B model using LoRA with Rank 64, learning rate 5e-5, batch size 16, and context window 1024 for 1 epoch. The model is evaluated on the NCSE test set using Character Error Rate (CER) and Word Error Rate (WER) metrics, with results compared against baseline models and models trained on real data.

## Key Results
- Models trained on synthetic data reduce CER by 55% and WER by 32% over baseline models
- Synthetic data outperforms real data for training OCR correction models
- Optimal corruption levels are found between CER 5-20% for effective training
- More tokens-per-observation outperforms more observations for a fixed token budget

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data trained with learned OCR corruption patterns can improve LM correction accuracy.
- Mechanism: A Markov corruption model learns conditional character-level error probabilities from real OCR datasets. The model then generates synthetic corrupted text that mimics actual OCR errors, providing training data where ground truth is always available.
- Core assumption: OCR errors follow learnable statistical patterns that can be captured at the character level and applied consistently across different texts.
- Evidence anchors:
  - [abstract] Models trained on synthetic data reduce the character error rate by 55% and word error rate by 32% over the base LM and outperform models trained on real data.
  - [section 3.2.1] The Markov model takes a single correct character x and outputs a sequence of characters y with a length greater than or equal to 0, learning conditional probability distributions of passing through the model unaltered, being deleted, substituted, and having a new character inserted after it.
  - [corpus] Weak - corpus shows related papers on OCR error correction but no direct evidence about Markov-based synthetic data approaches.

### Mechanism 2
- Claim: Training with under-corrupted data (CER 5-20%) is more effective than heavily corrupted data.
- Mechanism: Low-to-moderate corruption levels provide sufficient error patterns for the LM to learn correction while maintaining enough readable context for effective learning. High corruption levels degrade into gibberish that overwhelms the LM's ability to extract useful patterns.
- Core assumption: There exists an optimal corruption level where errors are present but not overwhelming, allowing the LM to learn meaningful correction patterns.
- Evidence anchors:
  - [abstract] Key findings include; training on under-corrupted data is better than over-corrupted data.
  - [section 4] Models that perform well in the low corruption group all have very low target CER values, typically 0.05 or 0.1.
  - [corpus] Weak - corpus doesn't provide specific evidence about optimal corruption levels for training.

### Mechanism 3
- Claim: More tokens-per-observation outperforms more observations for a fixed token budget.
- Mechanism: Longer text sequences provide richer context for the LM to learn correction patterns, while shorter sequences may lack sufficient context to capture long-range dependencies important for OCR correction.
- Core assumption: OCR correction benefits from longer context windows where the LM can leverage surrounding text to disambiguate corrupted characters/words.
- Evidence anchors:
  - [abstract] More tokens-per-observation outperforms more observations for a fixed token budget.
  - [section 4.1] Looking at the right panel of Figure 5 shows how CER changes with increased tokens per observation. Here, the CER reduces up to the maximum of 200 tokens per observation.
  - [corpus] Weak - corpus doesn't provide evidence about token-per-observation effects on model performance.

## Foundational Learning

- Concept: Markov models and conditional probability distributions
  - Why needed here: The corruption function is based on a character-level Markov model that learns P(y|x) - the probability of observing corrupted character sequence y given correct character x.
  - Quick check question: How would you calculate the probability of a 3-character corrupted sequence given a single correct character using the Markov corruption model?

- Concept: Character Error Rate (CER) and Word Error Rate (WER) metrics
  - Why needed here: These metrics are used to evaluate both the corruption function's output and the LM's correction performance, requiring understanding of substitution, deletion, and insertion error types.
  - Quick check question: If a ground truth word "example" becomes "exmple" after OCR, what are the S, D, and I values for calculating CER?

- Concept: Parameter-efficient fine-tuning (LoRA) and quantization
  - Why needed here: The approach uses LoRA for efficient fine-tuning of the 8B parameter Llama model, requiring understanding of how low-rank adaptations work and how quantization reduces memory requirements.
  - Quick check question: What is the primary benefit of using LoRA over full fine-tuning when working with large language models?

## Architecture Onboarding

- Component map:
  - Synthetic text generation -> Markov corruption -> training data creation -> LoRA fine-tuning -> evaluation on test set

- Critical path: synthetic text generation → corruption model training → corrupted text generation → LoRA fine-tuning → evaluation on test set

- Design tradeoffs:
  - Synthetic vs real data: Synthetic provides unlimited high-quality training data but may not capture all real error patterns; real data is limited but authentic
  - Corruption level selection: Higher corruption matches test data better but reduces training signal quality
  - Token-per-observation vs number of observations: Longer sequences provide better context but fewer training examples

- Failure signatures:
  - Model performance plateaus despite increased training data → corruption model may not capture real error patterns
  - High CER on test set despite good training performance → distribution mismatch between synthetic and real errors
  - Memory errors during training → quantization/LoRA parameters need adjustment

- First 3 experiments:
  1. Test corruption levels: Train models with CER 0.05, 0.1, 0.2, 0.3 and evaluate on NCSE dataset to find optimal corruption level
  2. Context length validation: Train models with 100, 200, 400 tokens per observation (fixed total tokens) to confirm tokens-per-observation effect
  3. Real vs synthetic comparison: Train one model on synthetic data (CER 0.1, WER 0.2) and one on real BLN600 data, compare performance on NCSE test set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of synthetic data-trained models vary across different types of historical documents beyond newspapers?
- Basis in paper: [inferred] The paper focuses specifically on newspaper datasets but discusses broader implications for OCR correction in historical archives
- Why unresolved: The experiments were limited to newspaper corpora from the 19th century, leaving uncertainty about performance on other document types
- What evidence would resolve it: Comparative experiments using synthetic data training on diverse historical document types (books, manuscripts, legal documents, etc.) with similar error characteristics

### Open Question 2
- Question: What is the optimal corruption distribution function beyond the Markov model for simulating OCR errors?
- Basis in paper: [explicit] The paper notes its Markov corruption function is "very simple" and may explain poor performance on highly corrupted text
- Why unresolved: The paper acknowledges the simplicity of its corruption model but does not explore more sophisticated alternatives
- What evidence would resolve it: Experiments comparing multiple corruption models (e.g., incorporating word-level transpositions, context-aware errors) across various CER levels

### Open Question 3
- Question: Does model specialization for high vs. low corruption levels improve overall OCR correction performance compared to a single model approach?
- Basis in paper: [explicit] The paper finds models perform differently on high and low corruption subsets and suggests "training a mixture of experts model" might work better
- Why unresolved: While the paper identifies this as a potential improvement, it does not implement or test a mixture-of-experts approach
- What evidence would resolve it: Direct comparison of single-model approaches against mixture-of-experts architectures on datasets with diverse corruption distributions

### Open Question 4
- Question: What is the mechanism by which language models correct OCR errors - are they essentially stochastic parrots or do they demonstrate genuine language understanding?
- Basis in paper: [explicit] The paper discusses this question in relation to Kallini et al.'s work on impossible languages and whether LMs "gain some understanding of the structure of the 'impossible language' that is the OCR errors"
- Why unresolved: The paper identifies this as an open question but does not provide evidence for either mechanism
- What evidence would resolve it: Controlled experiments testing model performance on corrupted text with various semantic and syntactic relationships to the ground truth

## Limitations

- The corruption model training process lacks transparency, making it difficult to assess whether synthetic corruption accurately captures real OCR error patterns
- Evaluation is limited to English-language historical newspapers, raising questions about generalizability to other languages and document types
- Computational requirements for fine-tuning an 8B parameter model may be prohibitive for many research groups

## Confidence

**High Confidence (8-10/10):**
- Synthetic data can improve OCR error correction performance compared to baseline models
- Models trained on synthetic data outperform models trained on real data for this task
- Optimal corruption levels exist between CER 5-20% for effective training

**Medium Confidence (5-7/10):**
- More tokens-per-observation outperforms more observations for a fixed token budget
- Heavily corrupted texts should be excluded from training sets
- The scrambledtext library's Markov corruption model effectively captures OCR error patterns

**Low Confidence (1-4/10):**
- The specific corruption distributions learned from real datasets are optimal for all historical OCR correction tasks
- The approach generalizes well to non-English historical documents
- The 8B parameter model size is optimal for this task

## Next Checks

1. **Corruption Model Validation**: Train the Markov corruption model on multiple real OCR datasets (NCSE, SMH, CA, BLN600) and analyze the learned distributions for consistency and accuracy. Compare synthetic errors generated from these distributions against actual OCR errors in held-out test data to quantify distribution mismatch.

2. **Cross-Domain Generalization Test**: Apply the best-performing model (trained on synthetic data with optimal corruption levels) to OCR correction datasets from different domains - such as handwritten documents, non-English texts, or documents from different historical periods - to assess generalizability beyond the training domain.

3. **Resource Efficiency Analysis**: Repeat the experiments with progressively smaller Llama models (7B, 3B, 1.5B) using the same fine-tuning approach and synthetic data generation pipeline. Compare performance trade-offs against computational requirements to identify the most resource-efficient model size for this task.