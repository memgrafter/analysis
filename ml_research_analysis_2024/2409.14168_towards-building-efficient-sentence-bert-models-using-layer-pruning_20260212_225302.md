---
ver: rpa2
title: Towards Building Efficient Sentence BERT Models using Layer Pruning
arxiv_id: '2409.14168'
source_url: https://arxiv.org/abs/2409.14168
tags:
- pruning
- layer
- sbert
- bert
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that layer pruning effectively reduces
  SBERT model size while preserving embedding quality, outperforming similarly sized
  scratch-trained models. By systematically removing less critical layers and fine-tuning
  with NLI and STS training, pruned models maintain competitive performance to full
  models despite 50-80% size reduction.
---

# Towards Building Efficient Sentence BERT Models using Layer Pruning

## Quick Facts
- arXiv ID: 2409.14168
- Source URL: https://arxiv.org/abs/2409.14168
- Authors: Anushka Shelke; Riya Savant; Raviraj Joshi
- Reference count: 3
- This study demonstrates that layer pruning effectively reduces SBERT model size while preserving embedding quality, outperforming similarly sized scratch-trained models.

## Executive Summary
This study investigates layer pruning as a strategy to create efficient Sentence BERT (SBERT) models that maintain high-quality embeddings while significantly reducing computational demands. The researchers systematically remove layers from pre-trained BERT models and fine-tune them using a two-phase approach involving Natural Language Inference (NLI) and Semantic Textual Similarity (STS) training. Their findings demonstrate that pruned models, despite having 50-80% fewer layers, perform competitively with fully layered versions and consistently outperform scratch-trained models of similar size.

## Method Summary
The researchers employ a systematic layer pruning approach on BERT models, specifically targeting top layers for removal while retaining critical lower layers. They implement a two-phase fine-tuning process: first training on the IndicXNLI dataset for NLI tasks, then fine-tuning on the STSb benchmark for STS tasks. The evaluation includes multiple pruning configurations (2, 6, and 12 layers) and compares pruned models against scratch-trained counterparts of equivalent size. The study focuses on Marathi and Hindi language models, using MahaBERT-v2 and Muril as base models.

## Key Results
- Pruned models maintain competitive performance to full models despite 50-80% size reduction
- Top-layer pruning strategy was found most effective for preserving embedding quality
- Pruned models consistently outperform similarly sized, scratch-trained models
- The two-phase NLI+STS fine-tuning approach successfully compensates for information loss from pruning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer pruning preserves embedding quality by removing redundant layers while retaining critical ones.
- Mechanism: Systematic removal of top layers in BERT models, followed by NLI+STS fine-tuning, maintains competitive performance to full models.
- Core assumption: Top layers contribute less to embedding quality than lower layers in transformer architectures.
- Evidence anchors:
  - [abstract] "Our findings show that pruned models, despite fewer layers, perform competitively with fully layered versions."
  - [section] "The top-layer pruning strategy was found most effective."
  - [corpus] Weak - related papers discuss layer pruning but don't specifically validate top-layer effectiveness.
- Break condition: If bottom or middle layers are pruned instead of top layers, performance drops significantly.

### Mechanism 2
- Claim: Fine-tuning with NLI and STS training compensates for information loss from pruning.
- Mechanism: Two-phase training (NLI then STS) enhances model's ability to capture semantic relationships despite reduced layers.
- Core assumption: Fine-tuning can recover performance lost from layer removal by focusing on task-specific representations.
- Evidence anchors:
  - [abstract] "Through a two-phase SBERT fine-tuning process involving Natural Language Inference (NLI) and Semantic Textual Similarity (STS), we evaluate the impact of layer reduction on embedding quality."
  - [section] "We compare 2-layer and 6-layer models created through layer pruning... Our observations show that the pruned models consistently outperform the scratch-trained models."
  - [corpus] Weak - related papers mention fine-tuning but don't specifically analyze NLI+STS two-phase approach.
- Break condition: If models are not fine-tuned after pruning, performance degrades significantly.

### Mechanism 3
- Claim: Pruned models outperform scratch-trained models of similar size due to better initialization.
- Mechanism: Starting from a pre-trained base and pruning retains learned representations better than training small models from scratch.
- Core assumption: Pre-trained representations provide better starting point than random initialization for small models.
- Evidence anchors:
  - [abstract] "Moreover, pruned models consistently outperform similarly sized, scratch-trained models, establishing layer pruning as an effective strategy for creating smaller, efficient embedding models."
  - [section] "Therefore, instead of developing new models from the ground up, it is more effective to start with a larger model and apply pruning techniques."
  - [corpus] Weak - related papers discuss pruning but don't directly compare pruned vs scratch-trained models.
- Break condition: If scratch-trained models use knowledge distillation from larger models, the performance gap may narrow.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how layers contribute to embedding quality is crucial for effective pruning
  - Quick check question: What is the primary function of attention heads in transformer layers?

- Concept: Siamese and triplet network structures
  - Why needed here: SBERT uses these structures to generate sentence embeddings, understanding them is key to grasping the model modifications
  - Quick check question: How do siamese networks differ from traditional feedforward networks in handling sentence pairs?

- Concept: Natural Language Inference and Semantic Textual Similarity tasks
  - Why needed here: These are the two-phase fine-tuning tasks that enable pruned models to maintain performance
  - Quick check question: What is the difference between NLI and STS tasks in terms of their training objectives?

## Architecture Onboarding

- Component map:
  Input: Marathi/Hindi text sentences -> Pruned BERT layers (2-12 layers depending on configuration) -> Mean or CLS token pooling -> Sentence embeddings for similarity comparison

- Critical path:
  1. Load pre-trained BERT model (Muril or MahaBERT-v2)
  2. Apply top-layer pruning to reduce layers
  3. Fine-tune with NLI training on IndicXNLI dataset
  4. Further fine-tune with STS training on STSb dataset
  5. Evaluate on translated Marathi STSb test set

- Design tradeoffs:
  - Model size vs. performance: 2-layer pruned models vs 12-layer full models
  - Pruning strategy: Top-layer pruning vs middle/bottom-layer pruning
  - Fine-tuning approach: Two-phase (NLI+STS) vs single-phase training

- Failure signatures:
  - Large drop in embedding similarity scores after pruning
  - Poor performance on classification tasks despite good similarity scores
  - Inconsistent results across different pruning configurations

- First 3 experiments:
  1. Compare top-layer pruning (removing layers 7-12) vs no pruning on MahaBERT-v2 with 2-phase fine-tuning
  2. Test different layer combinations (2, 6, 12 layers) using top-layer pruning strategy
  3. Compare pruned models against scratch-trained MahaBERT-Small and MahaBERT-Smaller of equivalent size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different layer pruning strategies (top, middle, bottom) affect the performance of SBERT models on downstream tasks beyond STS and NLI, such as text classification or named entity recognition?
- Basis in paper: [explicit] The paper mentions evaluating pruned models on STS and NLI tasks but does not explore their performance on other NLP tasks.
- Why unresolved: The study focuses primarily on STS and NLI tasks, leaving the generalizability of pruned models to other tasks unexplored.
- What evidence would resolve it: Testing pruned SBERT models on a variety of NLP tasks (e.g., text classification, NER) and comparing their performance to full models and scratch-trained models would provide insights into their broader applicability.

### Open Question 2
- Question: What is the optimal number of layers to prune for different SBERT models to achieve the best balance between model size reduction and performance retention?
- Basis in paper: [inferred] The paper experiments with pruning different numbers of layers (e.g., 2, 6, 12) but does not determine an optimal number for each model type.
- Why unresolved: The study provides results for specific layer configurations but does not systematically determine the optimal pruning depth for each model.
- What evidence would resolve it: Conducting a more extensive analysis of model performance across a wider range of layer configurations for each SBERT model would help identify the optimal pruning strategy.

### Open Question 3
- Question: How does the pruning strategy affect the interpretability and explainability of SBERT models?
- Basis in paper: [explicit] The paper does not address the interpretability or explainability of pruned models.
- Why unresolved: The focus of the study is on performance and efficiency, not on understanding how pruning affects the model's decision-making process.
- What evidence would resolve it: Analyzing the attention patterns or feature importance in pruned models compared to full models could reveal how pruning impacts interpretability.

### Open Question 4
- Question: Can the layer pruning approach be effectively applied to other transformer-based models beyond SBERT, such as GPT or T5?
- Basis in paper: [inferred] The paper discusses layer pruning in the context of SBERT but does not explore its applicability to other transformer architectures.
- Why unresolved: The study is limited to SBERT models, leaving the generalizability of the pruning approach to other architectures untested.
- What evidence would resolve it: Applying the same layer pruning techniques to other transformer models and evaluating their performance on relevant tasks would determine the broader applicability of the approach.

## Limitations

- The comparison between pruned and scratch-trained models lacks direct ablation studies that would isolate the specific contribution of layer pruning versus fine-tuning strategy.
- The claim that top-layer pruning is "most effective" is based on relative performance but doesn't provide systematic comparison across all possible pruning strategies.
- The evaluation focuses primarily on embedding quality and classification accuracy, without examining robustness to adversarial examples or cross-domain generalization.

## Confidence

**High Confidence**:
- The general principle that layer pruning can reduce model size while maintaining performance is well-established in the literature and supported by the experimental results
- The two-phase fine-tuning approach (NLI then STS) is effective for adapting pruned models to semantic tasks

**Medium Confidence**:
- Top-layer pruning is specifically more effective than other strategies - this requires more systematic ablation studies
- Pruned models consistently outperform scratch-trained models of similar size - while demonstrated, the margin and conditions need further exploration
- The 50-80% size reduction claim is supported but may vary significantly across different base models and pruning strategies

## Next Checks

1. **Ablation study on pruning strategy**: Systematically test middle-layer and bottom-layer pruning approaches alongside top-layer pruning, using identical fine-tuning procedures, to definitively establish which strategy provides optimal performance across different layer counts.

2. **Cross-linguistic generalization test**: Apply the top-layer pruning and two-phase fine-tuning approach to at least two additional low-resource languages (e.g., Tamil, Telugu) to validate whether the effectiveness generalizes beyond Hindi and Marathi.

3. **Robustness evaluation**: Test pruned models against adversarial sentence pairs and cross-domain datasets to assess whether the size reduction compromises model robustness or generalization beyond the specific evaluation tasks used in the paper.