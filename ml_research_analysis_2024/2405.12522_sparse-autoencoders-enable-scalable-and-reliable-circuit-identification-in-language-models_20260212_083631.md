---
ver: rpa2
title: Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in
  Language Models
arxiv_id: '2405.12522'
source_url: https://arxiv.org/abs/2405.12522
tags:
- head
- circuit
- examples
- heads
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an efficient method for discovering interpretable
  circuits in large language models using discrete sparse autoencoders. The approach
  trains sparse autoencoders on carefully designed positive and negative examples,
  where the model can only correctly predict the next token for the positive examples.
---

# Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models

## Quick Facts
- arXiv ID: 2405.12522
- Source URL: https://arxiv.org/abs/2405.12522
- Authors: Charles O'Neill; Thang Bui
- Reference count: 40
- Primary result: Method achieves higher precision and recall in recovering ground-truth circuits compared to state-of-the-art baselines, while reducing runtime from hours to seconds

## Executive Summary
This paper introduces an efficient method for discovering interpretable circuits in large language models using discrete sparse autoencoders (SAEs). The approach trains SAEs on carefully designed positive and negative examples, where the model can only correctly predict the next token for the positive examples. By discretizing the learned representations into integer codes and measuring the overlap between codes unique to positive examples for each attention head, the method enables direct identification of heads involved in circuits without expensive ablations or architectural modifications. On three well-studied tasks - indirect object identification, greater-than comparisons, and docstring completion - the proposed method achieves higher precision and recall in recovering ground-truth circuits compared to state-of-the-art baselines, while reducing runtime from hours to seconds.

## Method Summary
The method trains discrete sparse autoencoders on attention head outputs from positive examples (requiring a specific circuit) and negative examples (not requiring the circuit). After training, activations are discretized into integer codes via argmax. For node-level identification, the method counts codes unique to positive examples per head; for edge-level identification, it counts co-occurrences of codes between head pairs unique to positive examples. A softmax normalization followed by thresholding produces a binary mask of heads in the circuit. The approach requires only 5-10 text examples per task and eliminates the need for expensive ablations or architectural modifications.

## Key Results
- Achieves higher precision and recall than state-of-the-art baselines for recovering ground-truth circuits
- Reduces runtime from hours to seconds for circuit identification
- Requires only 5-10 text examples per task to learn robust representations
- Successfully identifies circuits for indirect object identification, greater-than comparisons, and docstring completion tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Discrete sparse autoencoders (SAEs) trained on attention head outputs learn compressed representations that highlight circuit-specific computations.
- **Mechanism**: The SAE learns to compress head activations into a small set of features. When trained on positive examples (which require the circuit) and negative examples (which don't), it must differentiate between them to minimize reconstruction error. This forces the learned features to encode distinctions tied to the circuit.
- **Core assumption**: The difference between positive and negative examples can be captured in a compressed feature space without losing the signal needed to distinguish circuit-relevant heads.
- **Evidence anchors**:
  - [abstract]: "We hypothesise that learned representations of attention head outputs will signal when a head is engaged in specific computations."
  - [section]: "The key insight behind our approach is that by training SAEs on carefully designed examples of a task that requires the language model to use a specific circuit (and examples where it doesn't), the learned representations should capture circuit-specific behaviour."
  - [corpus]: Weak—no direct comparison of feature-based vs raw activation methods in related work.
- **Break condition**: If the positive/negative distinction is too subtle or the SAE cannot compress it effectively, the learned codes will not separate heads by circuit involvement.

### Mechanism 2
- **Claim**: Counting codes unique to positive examples identifies heads involved in the circuit.
- **Mechanism**: After training, each head's activation is mapped to a discrete code via argmax over features. Heads that activate codes appearing only in positive examples are inferred to be part of the circuit. The softmax normalization and threshold convert counts into a binary mask.
- **Core assumption**: Codes that are unique to positive examples are indicative of circuit-relevant computation, and their frequency correlates with head importance.
- **Evidence anchors**:
  - [abstract]: "By discretising the learned representations into integer codes and measuring the overlap between codes unique to positive examples for each head, we enable direct identification of attention heads involved in circuits."
  - [section]: "We then compute a vector u ∈ R^nheads, where each element ui represents the number of unique codes that appear only in the positive examples... Intuitively, a high value of ui indicates that the i-th head activates a large proportion of codes that are unique to positive examples."
  - [corpus]: Moderate—vector quantization approaches in related work support discrete representations but do not specifically validate code uniqueness for circuit discovery.
- **Break condition**: If codes are polysemantic or the uniqueness criterion fails to isolate circuit-specific heads, the method will misclassify heads.

### Mechanism 3
- **Claim**: Edge-level identification via co-occurrence patterns reveals which head pairs work together in the circuit.
- **Mechanism**: For each head pair, the method counts co-occurrences of specific codes in positive vs negative examples. Pairs with high positive-only co-occurrences are considered part of the circuit. The top k pairs determine the final head mask.
- **Core assumption**: Circuit-relevant computation involves coordinated activation of head pairs, and this coordination is detectable via code co-occurrence differences.
- **Evidence anchors**:
  - [abstract]: "For edge-level circuit identification... we count the number of co-occurrences of codes between heads for the top-k co-occurrences, and then again take the softmax and thresholding with θ."
  - [section]: "We then compute a matrix U, where each entry Uh1,h2 represents the number of co-occurrences that appear in the positive examples but not in the negative examples for the head pair (h1, h2)."
  - [corpus]: Weak—related work focuses on node importance but does not validate co-occurrence-based edge detection.
- **Break condition**: If the circuit does not involve coordinated head pairs, or if noise dominates the co-occurrence signal, the method will fail to recover true edges.

## Foundational Learning

- **Concept**: Sparse autoencoders and dictionary learning
  - Why needed here: The SAE learns a compressed dictionary of features from attention head outputs; understanding how this compression works is key to interpreting the learned codes.
  - Quick check question: If the bottleneck dimension is much smaller than the input, what does that imply about the autoencoder's compression goal?
- **Concept**: Attention head mechanics in transformers
  - Why needed here: Circuit discovery targets specific heads; knowing how heads read from and write to the residual stream is necessary to understand their role in computation.
  - Quick check question: What does it mean for an attention head to "write" to the residual stream?
- **Concept**: Circuit identification vs ablation studies
  - Why needed here: This method bypasses expensive ablations by using learned features; understanding why ablations are costly clarifies the method's efficiency claim.
  - Quick check question: Why is choosing a metric for ablation sensitivity a challenge in circuit discovery?

## Architecture Onboarding

- **Component map**: Input examples -> Tokenization -> GPT-2 forward pass -> Cached attention head outputs -> Sparse autoencoder training -> Discretization via argmax -> Code matrices for positive/negative sets -> Unique code counting or co-occurrence counting -> Softmax + threshold -> Binary head mask
- **Critical path**:
  1. Cache attention head activations for all examples
  2. Train SAE on a small subset (10 examples)
  3. Apply argmax to obtain discrete codes
  4. Count unique positive codes per head or co-occurrences per head pair
  5. Normalize with softmax and apply threshold θ
  6. Output binary mask of heads in circuit
- **Design tradeoffs**:
  - Fewer training examples → faster but risk underfitting
  - More bottleneck features → richer representation but more noise
  - Softmax across heads vs layers → affects head importance ranking
- **Failure signatures**:
  - ROC AUC near 0.5 → no signal in codes, possibly due to poor SAE training or indistinguishable examples
  - Low F1 at all thresholds → codes not aligned with ground truth circuit
  - High variance across runs → unstable training or hyperparameter sensitivity
- **First 3 experiments**:
  1. Train SAE on 10 positive/negative examples, plot histogram of unique positive codes per head vs ground truth
  2. Sweep threshold θ, plot ROC curve, verify AUC > 0.8
  3. Repeat with 100 examples, compare ROC AUC to confirm robustness to sample size

## Open Questions the Paper Calls Out
None

## Limitations
- Code discretization artifacts may lose circuit-specific signal through argmax quantization
- Ground truth dependence assumes completeness and accuracy of manually constructed circuits
- Hyperparameter sensitivity not thoroughly explored across bottleneck dimensions, sparsity weights, and thresholds
- Scalability to larger models not evaluated, computational advantage may diminish with expensive SAE training

## Confidence
- **High Confidence**: Core methodology and implementation are technically sound; runtime improvement claim is supported
- **Medium Confidence**: Comparative performance on three evaluated tasks is well-supported but generalizability is uncertain
- **Low Confidence**: Claims about interpretability of learned codes are not directly validated

## Next Checks
1. **Code Interpretability Analysis**: Take the top 10 codes from heads identified as circuit-relevant and perform manual analysis to determine if these codes correspond to interpretable features related to the task. Compare this to codes from heads not in the circuit.

2. **Cross-Model Transfer Test**: Apply the method trained on GPT-2 XL to identify the same circuits in GPT-2 Small or GPT-2 Medium. Evaluate whether the code-based approach transfers across model scales without retraining SAEs.

3. **Adversarial Example Test**: Construct positive examples that require the circuit but are semantically different from the training examples. Test whether the method still correctly identifies the circuit-relevant heads, validating that it learned the circuit mechanism rather than memorizing specific patterns.