---
ver: rpa2
title: 'DAISY: Data Adaptive Self-Supervised Early Exit for Speech Representation
  Models'
arxiv_id: '2406.05464'
source_url: https://arxiv.org/abs/2406.05464
tags:
- exit
- early
- speech
- layer
- daisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DAISY introduces a data-adaptive early exit strategy for self-supervised
  speech models, using the entropy of self-supervised loss to decide when to exit
  inference, eliminating the need for task-specific training or fine-tuning. The method
  achieves comparable or better performance than HuBERT on MiniSUPERB tasks (e.g.,
  6.96% WER on ASR, 82.63% accuracy on SID) while reducing inference time by up to
  31.51%.
---

# DAISY: Data Adaptive Self-Supervised Early Exit for Speech Representation Models

## Quick Facts
- arXiv ID: 2406.05464
- Source URL: https://arxiv.org/abs/2406.05464
- Authors: Tzu-Quan Lin; Hung-yi Lee; Hao Tang
- Reference count: 0
- DAISY achieves 6.96% WER on ASR and 82.63% accuracy on SID while reducing inference time by up to 31.51%

## Executive Summary
DAISY introduces a data-adaptive early exit strategy for self-supervised speech models that uses entropy of self-supervised loss to determine when to exit inference. The method eliminates the need for task-specific training or fine-tuning while achieving comparable or better performance than HuBERT on MiniSUPERB tasks. DAISY adapts to noise levels, using fewer layers for clean data and more layers for noisy data, demonstrating significant inference time savings.

## Method Summary
DAISY implements a three-stage approach: first, linear classifiers are trained with a simplified HuBERT loss on each layer of a frozen HuBERT model; second, downstream classifiers are trained using weighted sums of hidden states up to the early exit layer; third, inference uses entropy-based early exit decisions with configurable thresholds. The method leverages the observation that entropy decreases linearly with layer depth and correlates with noise levels, enabling efficient early exits without task-specific supervision.

## Key Results
- Achieves 6.96% WER on ASR task, comparable to HuBERT baseline
- Reduces inference time by up to 31.51% while maintaining performance
- Demonstrates noise adaptivity: improves WER from 26.1% (fixed exit) to 20.67% on noisy test sets
- Achieves 82.63% accuracy on SID task with early exit mechanism

## Why This Works (Mechanism)

### Mechanism 1
Entropy of self-supervised loss correlates with downstream task difficulty, enabling early exit without task-specific training. The entropy of predictions from early exit branches reflects uncertainty in feature representation, with lower entropy indicating sufficient information for downstream tasks. Core assumption: entropy reliably proxies task-specific uncertainty. Evidence: paper observes linear entropy decrease with depth and correlation with noise levels. Break condition: if entropy fails to correlate with task difficulty, early exit decisions become unreliable.

### Mechanism 2
Linear relationship between layer index and average entropy enables simple threshold-based decisions. The entropy of predictions decreases linearly with depth, allowing threshold setting based on mean entropy scaling. Core assumption: linear entropy-layer relationship holds across datasets. Evidence: paper observes surprising linearity in entropy function. Break condition: if relationship becomes non-linear, threshold approach fails.

### Mechanism 3
Noise adaptivity emerges because noisy samples have higher entropy, requiring more layers. When input contains noise, self-supervised model produces higher entropy predictions at earlier layers, routing noisy samples through more layers. Core assumption: noise levels predictably affect entropy. Evidence: paper shows models forward to deeper layers on low signal-to-noise ratio data. Break condition: if noise doesn't consistently increase entropy, adaptivity benefit disappears.

## Foundational Learning

- Self-supervised learning in speech models
  - Why needed here: DAISY relies on self-supervised loss to train early exit branches without task-specific supervision
  - Quick check question: What is the primary objective used to train HuBERT models, and how does it differ from supervised ASR training?

- Entropy as uncertainty measure
  - Why needed here: DAISY uses entropy of softmax outputs to decide when to exit; understanding entropy helps interpret confidence
  - Quick check question: Given probability distribution [0.1, 0.9], compute its entropy and explain what it indicates about model confidence

- Early exit mechanisms in deep networks
  - Why needed here: DAISY implements early exit by attaching branches to intermediate layers and deciding exit based on entropy thresholds
  - Quick check question: What is the key difference between DAISY's early exit approach and traditional methods that use task-specific classifiers at each branch?

## Architecture Onboarding

- Component map: Input speech waveform -> HuBERT backbone (frozen, 12 layers) -> Early exit branches (linear classifier + entropy calculator per layer) -> Downstream classifier (weighted sum of hidden states) -> Exit decision logic (entropy threshold comparison)

- Critical path: 1) Forward through HuBERT layers until entropy threshold met, 2) Compute weighted sum of hidden states up to exit layer, 3) Apply downstream classifier to obtain final prediction

- Design tradeoffs: Fixed threshold vs. adaptive threshold (fixed is simpler but may not generalize), number of exit branches (more branches allow finer granularity but increase memory), entropy vs. other confidence measures (entropy is simple but may not be optimal)

- Failure signatures: Consistently high entropy across all layers indicates poor feature learning, early exit on clean data with performance drop suggests threshold too aggressive, late exit on noisy data with performance drop suggests threshold too conservative, large variance in exit layers indicates instability

- First 3 experiments: 1) Verify entropy decreases linearly with layer depth on clean dataset, 2) Test early exit performance on clean vs. noisy versions to confirm noise adaptivity, 3) Compare DAISY with fixed-layer early exit on MiniSUPERB tasks to quantify tradeoffs

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several implications arise from the analysis. The method's reliance on entropy correlation with downstream performance suggests questions about generalization to other tasks and model architectures. The heuristic approach to threshold selection indicates potential for more principled optimization methods. The noise adaptivity claims based on synthetic noise raise questions about performance on real-world noisy conditions.

## Limitations
- Core assumption that entropy correlates with downstream task difficulty lacks direct empirical validation across diverse tasks
- Requires training HuBERT from scratch to obtain quantized targets, creating significant reproducibility barriers
- Noise adaptivity claims based on artificially added MUSAN noise may not generalize to real-world noise characteristics

## Confidence
- High confidence: Entropy-layer linear relationship observation and its use for threshold-based early exit decisions
- Medium confidence: Performance claims on MiniSUPERB tasks are convincing but limited to specific benchmark
- Low confidence: Claim that DAISY eliminates need for task-specific training is somewhat overstated

## Next Checks
1. Cross-task entropy correlation validation: Test whether entropy values from early exit branches actually correlate with task-specific uncertainty measures across at least 5 diverse speech tasks beyond MiniSUPERB
2. Real-world noise adaptivity test: Evaluate DAISY on audio with naturally occurring noise to verify noise adaptivity claims hold in practical scenarios
3. Alternative confidence measures comparison: Implement and compare DAISY with early exit strategies using other confidence measures to determine if entropy is optimal or merely convenient