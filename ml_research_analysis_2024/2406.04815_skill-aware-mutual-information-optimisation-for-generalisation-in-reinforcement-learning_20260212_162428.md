---
ver: rpa2
title: Skill-aware Mutual Information Optimisation for Generalisation in Reinforcement
  Learning
arxiv_id: '2406.04815'
source_url: https://arxiv.org/abs/2406.04815
tags:
- context
- learning
- tasks
- test
- skills
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of zero-shot generalisation
  in Meta-Reinforcement Learning (Meta-RL) when tasks require different optimal skills.
  The authors propose Skill-aware Mutual Information (SaMI), an optimisation objective
  that distinguishes context embeddings based on skills, enabling RL agents to identify
  and execute different skills across tasks.
---

# Skill-aware Mutual Information Optimisation for Generalisation in Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.04815
- Source URL: https://arxiv.org/abs/2406.04815
- Reference count: 40
- Zero-shot generalisation in Meta-RL improved by skill-aware MI optimisation and SaNCE estimator

## Executive Summary
This paper addresses the challenge of zero-shot generalisation in Meta-Reinforcement Learning when tasks require different optimal skills. The authors propose Skill-aware Mutual Information (SaMI), an optimisation objective that distinguishes context embeddings based on skills, enabling RL agents to identify and execute different skills across tasks. They also introduce Skill-aware Noise Contrastive Estimation (SaNCE), a data-efficient K-sample estimator to optimise SaMI. Experimental validation on modified MuJoCo and Panda-gym benchmarks shows that RL agents using SaMI achieve substantially improved zero-shot generalisation to unseen tasks, with higher returns and success rates.

## Method Summary
The method introduces Skill-aware Mutual Information (SaMI) as an optimisation objective that encourages context embeddings to capture skill-related information rather than just environmental features. The key innovation is Skill-aware Noise Contrastive Estimation (SaNCE), a K-sample estimator that improves data efficiency compared to standard InfoNCE. SaNCE samples trajectories based on skill-aware ranking rather than simple temporal ordering, allowing the model to distinguish between different optimal behaviours across tasks. The approach is implemented on top of SAC with an LSTM context encoder producing 6-dimensional embeddings. The contrastive batch size is 12 for MuJoCo and 256 for Panda-gym environments, with training conducted for 1.6 million timesteps.

## Key Results
- RL agents using SaMI achieved substantially improved zero-shot generalisation to unseen tasks
- Higher returns and success rates compared to baseline methods on modified MuJoCo and Panda-gym benchmarks
- Context encoder trained with SaNCE demonstrated greater robustness to reduced sample sizes, suggesting potential to overcome the log-K curse

## Why This Works (Mechanism)
SaMI works by explicitly optimizing for skill-awareness in context embeddings, allowing the agent to distinguish between different optimal behaviours required across tasks. Unlike traditional MI-based methods that focus on environmental features, SaMI ensures that context embeddings capture the underlying skills needed for success. SaNCE provides an efficient K-sample estimation approach that reduces the sample complexity required for effective learning, enabling better generalization from limited training data.

## Foundational Learning
- **Mutual Information Maximization**: Why needed - To capture relevant information in context embeddings; Quick check - Verify that MI between context and trajectory increases during training
- **Contrastive Learning**: Why needed - To distinguish between positive and negative samples effectively; Quick check - Confirm that contrastive loss decreases while positive sample similarity increases
- **Skill-aware Sampling**: Why needed - To focus on behavioural differences rather than just environmental variations; Quick check - Ensure sampled trajectories represent distinct skill patterns

## Architecture Onboarding

**Component Map**: Environment -> SAC Agent -> Context Encoder (LSTM) -> SaNCE Loss -> Skill-aware MI Objective

**Critical Path**: State observations → Context embedding → Policy action → Environment transition → Trajectory sampling → Contrastive loss computation → Context encoder update

**Design Tradeoffs**: Model-free (SAC) vs model-based approaches; standard InfoNCE vs SaNCE; fixed vs adaptive skill-aware sampling strategies

**Failure Signatures**: Poor generalization due to collapsed context embeddings; ineffective skill discrimination; insufficient exploration leading to limited skill diversity

**First Experiments**:
1. Verify basic functionality by running TESAC baseline on Hopper or Panda-gym Pick&Place
2. Implement SaNCE by replacing InfoNCE loss with skill-aware sampling strategy
3. Test context embedding quality using UMAP/t-SNE visualization for distinct skill clusters

## Open Questions the Paper Calls Out
- How does SaMI performance compare when using a model-based RL approach versus the current model-free SAC base algorithm? The paper notes that prediction models are more effective when transition functions change across tasks and suggests model-based approaches as future work.
- What is the impact of introducing independence assumptions (like DOMINO) on SaMI's performance in environments where environmental features are correlated? The current implementation assumes independence of environmental features, which may limit performance in more complex scenarios.

## Limitations
- The skill-aware trajectory sampling strategy is described at a high level, making faithful implementation challenging
- The momentum encoder update mechanism is underspecified in the paper
- Relatively simple nature of the modified MuJoCo and Panda-gym benchmarks raises questions about scalability to more complex, real-world tasks

## Confidence
- High confidence: The basic approach of using skill-aware MI optimization for Meta-RL generalization is sound and well-motivated
- Medium confidence: The experimental results showing improved zero-shot generalization on the tested benchmarks are likely reproducible given the open-sourced code
- Medium confidence: The claim about SaNCE's improved sample efficiency is supported by the results but needs more extensive validation

## Next Checks
1. Reproduce the core results on Hopper and Panda-gym Pick&Place to verify the claimed improvements in zero-shot generalization
2. Conduct ablation studies on buffer sizes and contrastive batch sizes to test the claim about overcoming the log-K curse
3. Implement and test the method on a more complex Meta-RL benchmark (e.g., MetaWorld) to assess scalability beyond the current environments