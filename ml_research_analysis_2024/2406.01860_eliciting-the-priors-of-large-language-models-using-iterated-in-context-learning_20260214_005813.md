---
ver: rpa2
title: Eliciting the Priors of Large Language Models using Iterated In-Context Learning
arxiv_id: '2406.01860'
source_url: https://arxiv.org/abs/2406.01860
tags:
- gpt-4
- learning
- priors
- iterated
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a prompt-based method to elicit prior distributions
  from Large Language Models (LLMs) using iterated in-context learning. The approach
  chains successive inferences from LLMs to implement a Markov chain Monte Carlo algorithm
  that supports sampling from the prior distribution.
---

# Eliciting the Priors of Large Language Models using Iterated In-Context Learning

## Quick Facts
- arXiv ID: 2406.01860
- Source URL: https://arxiv.org/abs/2406.01860
- Authors: Jian-Qiao Zhu; Thomas L. Griffiths
- Reference count: 40
- Key outcome: Introduces a prompt-based method to elicit prior distributions from LLMs using iterated in-context learning, validated against human priors and applied to speculative events.

## Executive Summary
This paper introduces a novel method to elicit prior distributions from Large Language Models (LLMs) using iterated in-context learning. The approach chains successive inferences from LLMs to implement a Markov chain Monte Carlo algorithm that supports sampling from the prior distribution. Experiments validated the method by eliciting priors from GPT-4 for tasks with known human priors (causal learning, proportion estimation, and everyday quantities), showing qualitative alignment with human priors and outperforming generic priors in explaining GPT-4's decisions. The method was also applied to elicit priors for speculative events (superhuman AI, zero carbon emissions, Mars colonization), yielding plausible distributions. The results suggest that LLMs encode human-like priors that can be recovered through this approach, with implications for understanding LLM decision-making and using LLMs as cultural technologies or in automated science.

## Method Summary
The method uses iterated in-context learning where GPT-4 makes predictions based on current data, then samples new data from a likelihood function based on that prediction, repeating this process for multiple chains. The approach implements a Markov chain Monte Carlo algorithm where the stationary distribution on hypotheses is the prior. For each task, 100 chains with 12 iterations each are run using random seeds and temperature 1. The method applies different likelihood functions depending on the task: noisy-OR and noisy-AND-NOT for causal strengths, binomial for proportion estimation, and uniform for speculative events. The recovered priors are compared against known human priors and used to explain GPT-4's decision-making.

## Key Results
- Recovered priors from GPT-4 qualitatively align with human priors across multiple domains (causal strengths, proportions, everyday quantities)
- Elicited priors outperform generic priors in explaining GPT-4's posterior inferences on held-out data
- Method successfully produces plausible distributions for speculative events (superhuman AI, zero carbon emissions, Mars colonization)
- Avoids model collapse by using fixed-weight LLMs rather than training on generated data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterated in-context learning implements a Markov chain Monte Carlo algorithm that converges to the prior distribution of the LLM.
- Mechanism: The method chains successive inferences where each LLM response conditions on the previous output, forming a Markov chain whose stationary distribution is the prior.
- Core assumption: LLMs perform approximate Bayesian inference such that p(h|d) âˆ p(d|h)p(h), making iterated learning equivalent to Gibbs sampling.
- Evidence anchors:
  - [abstract] "our approach is based on iterated learning, a Markov chain Monte Carlo method in which successive inferences are chained in a way that supports sampling from the prior distribution"
  - [section 2] "This process implements a Gibbs sampler for the joint distribution p(d,h) = p(d|h)p(h), a form of Markov chain Monte Carlo. The stationary distribution on hypotheses is thus the prior p(h)"
  - [corpus] Weak - no direct corpus evidence for MCMC convergence in LLMs, though the connection to Gibbs sampling is theoretical
- Break condition: If LLMs do not perform true Bayesian inference, the Markov chain will not converge to the prior distribution

### Mechanism 2
- Claim: The method avoids model collapse by using fixed-weight LLMs rather than training on generated data.
- Mechanism: Since in-context learning uses fixed weights without updating model parameters, there is no forgetting of improbable events that typically causes model collapse.
- Core assumption: Model collapse occurs specifically when models are trained on their own generated outputs, not when using in-context learning
- Evidence anchors:
  - [section 3] "This implementation of iterated learning also avoids the model collapse problem because there is no explicit training"
  - [section 3] "To bridge the gap between neural networks and Bayesian learners, seeded iterated learning methods have been proposed [22, 23]" (contrasting with our approach)
  - [corpus] Weak - the paper references model collapse literature but doesn't provide empirical evidence for this specific claim
- Break condition: If in-context learning implicitly updates weights or the LLM adapts to its own outputs over iterations

### Mechanism 3
- Claim: The recovered priors align with human priors because LLMs have learned from human-generated text that encodes these priors.
- Mechanism: LLMs trained on large corpora of human text implicitly learn the statistical patterns and biases (priors) that humans use in reasoning about various domains
- Core assumption: The training data distribution contains sufficient examples of human reasoning patterns that the LLM can extract and represent as priors
- Evidence anchors:
  - [section 4] "The priors recovered from GPT-4 also align with the human priors" (multiple domains: causal strengths, proportions, everyday quantities)
  - [section 5] "The distributions recovered from GPT-4 suggest the model has plausible priors for these speculative events"
  - [corpus] Moderate - corpus contains multiple papers on LLM priors and human-LLM alignment, but no direct evidence for how priors are encoded during training
- Break condition: If the LLM's training data is biased, incomplete, or filtered in ways that don't represent typical human reasoning

## Foundational Learning

- Concept: Bayesian inference and prior distributions
  - Why needed here: The entire method relies on understanding how prior distributions shape posterior inferences and how these can be recovered through MCMC methods
  - Quick check question: What is the relationship between p(h), p(d|h), and p(h|d) in Bayesian inference?

- Concept: Markov chain Monte Carlo and Gibbs sampling
  - Why needed here: The method is explicitly described as implementing a Gibbs sampler through iterated learning
  - Quick check question: What conditions must be met for a Markov chain to converge to its stationary distribution?

- Concept: In-context learning vs. fine-tuning
  - Why needed here: The paper distinguishes its approach from model training approaches that suffer from model collapse
  - Quick check question: How does in-context learning differ fundamentally from updating model weights during training?

## Architecture Onboarding

- Component map:
  - LLM (fixed weights, GPT-4) -> Prompt template (covers story and task description) -> Likelihood function (problem-specific probability model) -> Random seed generator (for initial conditions and sampling) -> Iteration controller (manages chain progression)

- Critical path:
  1. Initialize with random seed and first data point
  2. Generate LLM response based on current data
  3. Apply likelihood function to generate next data point
  4. Repeat for fixed number of iterations
  5. Analyze distribution of final responses

- Design tradeoffs:
  - Temperature setting: Higher temperature increases exploration but may slow convergence
  - Number of iterations: More iterations improve convergence but increase computational cost
  - Chain count: More chains provide better statistical coverage but require more queries
  - Prompt engineering: More detailed prompts may improve quality but reduce generality

- Failure signatures:
  - Chains not converging (distribution still changing after many iterations)
  - LLM refusing to answer or providing default responses
  - Generated data points outside expected ranges
  - Inconsistent responses across chains with same seed

- First 3 experiments:
  1. Simple proportion estimation (coin flips) to verify basic MCMC convergence
  2. Everyday quantity prediction (lifespan) to test alignment with known human priors
  3. Causal strength estimation with multiple cover stories to validate across domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the specific form of the likelihood function affect the quality of prior elicitation from LLMs?
- Basis in paper: [explicit] The paper uses uniform likelihood functions for speculative events and different likelihood functions (noisy-OR, noisy-AND-NOT, binomial) for other tasks, but doesn't systematically compare the impact of likelihood choice.
- Why unresolved: The paper demonstrates that iterated learning works with various likelihood functions, but doesn't investigate whether certain forms are more effective at recovering priors or how sensitive the method is to likelihood specification.
- What evidence would resolve it: Systematic experiments comparing prior recovery quality across different likelihood function specifications for the same tasks, measuring convergence rates and fidelity to known priors.

### Open Question 2
- Question: What are the computational limits of iterated in-context learning for prior elicitation as model size and task complexity increase?
- Basis in paper: [inferred] The paper uses 100 chains with 12 iterations for most tasks, but doesn't explore how these parameters scale with task difficulty or model size.
- Why unresolved: The paper demonstrates the method works for a range of tasks but doesn't establish boundaries on the number of iterations needed, the number of chains required, or how these requirements change with task complexity.
- What evidence would resolve it: Experiments varying chain length, number of chains, and task complexity to establish scaling relationships and identify computational bottlenecks.

### Open Question 3
- Question: How do different sampling temperatures during inference affect the quality of prior elicitation?
- Basis in paper: [explicit] The paper fixes temperature at 1 but notes this is "consistent with the idea of sampling from the posterior" without exploring temperature effects.
- Why unresolved: The paper uses a single temperature setting without investigating whether different temperatures yield better prior recovery or how temperature interacts with the iterated learning process.
- What evidence would resolve it: Systematic experiments varying temperature across a range of values and measuring the impact on prior recovery quality and convergence behavior.

## Limitations

- Limited empirical validation that chains actually converge to stationary distributions using rigorous MCMC diagnostics
- Reliance on GPT-4 specifically without testing whether the method generalizes to other LLMs or model versions
- Speculative events application lacks ground truth for validation, making it impossible to assess accuracy

## Confidence

**High Confidence**
- The iterated in-context learning framework can be implemented to chain successive inferences
- GPT-4 can be prompted to make predictions about various tasks (causal learning, proportions, quantities)
- The method successfully produces distributions that qualitatively resemble human priors in tested domains

**Medium Confidence**
- The recovered distributions represent the LLM's prior beliefs rather than prompt artifacts
- The method avoids model collapse through in-context learning rather than parameter updates
- The distributions for speculative events are plausible and reflect reasonable uncertainty

**Low Confidence**
- The Markov chain implementation guarantees convergence to true prior distributions
- The LLM's internal inference mechanism matches the assumed Bayesian framework
- The method is robust to prompt engineering choices and likelihood function specifications

## Next Checks

1. **Convergence Diagnostics**: Implement standard MCMC convergence tests (Gelman-Rubin statistic, trace plots, autocorrelation analysis) on the generated chains to verify they reach stationary distributions before sampling. Run chains with different random seeds and compare final distributions using statistical tests.

2. **Cross-Model Validation**: Apply the same methodology to multiple LLM architectures (GPT-3.5, Claude, LLaMA) and versions to test whether prior distributions are consistent across models or vary significantly. This would validate whether the method reveals model-specific priors rather than generic prompting artifacts.

3. **Prompt Sensitivity Analysis**: Systematically vary prompt templates, temperature settings, and chain lengths to determine how sensitive the recovered priors are to implementation choices. Test whether minor prompt modifications produce dramatically different distributions, which would indicate the method is capturing prompt artifacts rather than true model priors.