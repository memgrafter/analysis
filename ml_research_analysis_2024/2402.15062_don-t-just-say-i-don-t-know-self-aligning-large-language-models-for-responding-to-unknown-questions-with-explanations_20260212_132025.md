---
ver: rpa2
title: Don't Just Say "I don't know"! Self-aligning Large Language Models for Responding
  to Unknown Questions with Explanations
arxiv_id: '2402.15062'
source_url: https://arxiv.org/abs/2402.15062
tags:
- question
- unknown
- questions
- answer
- known
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of large language models (LLMs)
  providing hallucinated answers to unknown questions. The authors propose a self-alignment
  method called Self-Aligned to improve LLMs' ability to detect unanswerability and
  generate appropriate responses with explanations for different types of unknown
  questions.
---

# Don't Just Say "I don't know"! Self-aligning Large Language Models for Responding to Unknown Questions with Explanations

## Quick Facts
- arXiv ID: 2402.15062
- Source URL: https://arxiv.org/abs/2402.15062
- Authors: Yang Deng; Yong Zhao; Moxin Li; See-Kiong Ng; Tat-Seng Chua
- Reference count: 21
- One-line primary result: Self-Aligned method outperforms baselines on unknown question detection, classification, and response generation across four types of unknown questions.

## Executive Summary
This paper addresses the challenge of large language models (LLMs) providing hallucinated answers to unknown questions. The authors propose a self-alignment method called Self-Aligned that improves LLMs' ability to detect unanswerability and generate appropriate responses with explanations for different types of unknown questions. The method uses a two-stage class-aware self-augmentation approach to generate unknown question-response data from known question-answer data, followed by disparity-driven self-curation to filter low-quality data. Experimental results on two datasets across four types of unknown questions show that Self-Aligned outperforms existing baselines in three task formulations.

## Method Summary
The Self-Aligned method employs a two-stage class-aware self-augmentation approach followed by disparity-driven self-curation and supervised fine-tuning. First, guided question rewriting transforms known questions into specific types of unknown questions using seed data as few-shot demonstrations. Second, conditioned response generation creates appropriate responses with explanations for these unknown questions. The method then filters these pairs using disparity-driven self-curation, which scores the semantic difference between unknown question-response pairs and their known counterparts. The curated data is used to fine-tune the base LLM, with the process potentially iterating for further improvement.

## Key Results
- Self-Aligned outperforms existing baselines on unknown question detection with higher F1 scores
- The method achieves better macro-Precision, Recall, and F1 scores for unknown question classification
- For open-ended response generation, Self-Aligned shows higher win rates against baselines in GPT-4 comparisons and human evaluations on Honesty, Comprehensibility, and Helpfulness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The self-alignment method improves LLM's ability to recognize unknown questions by iteratively refining the model using disparity-driven self-curation.
- Mechanism: The method first generates unknown question-response pairs through guided question rewriting and conditioned response generation. It then filters these pairs using disparity-driven self-curation, which scores the semantic difference between unknown question-response pairs and their known counterparts. The curated data is used to fine-tune the base model, which is then iteratively improved by repeating the augmentation and curation process.
- Core assumption: The base model can effectively distinguish semantic differences between known and unknown question-response pairs, and this difference correlates with the quality of the unknown question-response pair.
- Evidence anchors:
  - [abstract]: "we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown questions as desired."
  - [section 3.3]: "we propose a disparity-driven self-curation approach to measure the semantic difference between the unknown question-response pair... and its known question-answer pair counterpart."
- Break condition: If the base model cannot accurately assess semantic disparity or if the semantic difference does not correlate with answer quality, the self-curation process may select low-quality data, reducing the effectiveness of the method.

### Mechanism 2
- Claim: The two-stage class-aware self-augmentation approach generates high-quality unknown question-response data from known question-answer data.
- Mechanism: The method uses seed data to demonstrate how to rewrite known questions into specific types of unknown questions (guided question rewriting). It then uses class-aware prompts to instruct the base model to generate appropriate responses with explanations for these unknown questions (conditioned response generation). This approach leverages the base model's knowledge of known questions to create plausible unknown questions and responses.
- Core assumption: The base model can effectively rewrite known questions into unknown questions of specific types while maintaining semantic plausibility, and it can generate appropriate responses with explanations for these unknown questions.
- Evidence anchors:
  - [abstract]: "the Self-Aligned method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data."
  - [section 3.2.1]: "the seed data is adopted as few-shot demonstrations for the in-context learning of unknown question rewriting."
- Break condition: If the base model struggles to generate semantically plausible unknown questions or appropriate responses with explanations, the quality of the self-augmented data will be poor, limiting the effectiveness of the method.

### Mechanism 3
- Claim: The iterative self-alignment process progressively improves the model's performance on unknown questions.
- Mechanism: After the initial fine-tuning on curated unknown question-response data, the updated model (M(1)) is used to generate higher-quality data in subsequent iterations. This process continues, with each iteration potentially improving the quality of the curated data and the model's performance.
- Core assumption: Each iteration of self-alignment can generate higher-quality unknown question-response data than the previous iteration, leading to progressive improvement in the model's performance.
- Evidence anchors:
  - [abstract]: "we further employ iterative self-alignment to continually augment and curate higher-quality data with the improved model."
  - [section 3.5]: "The base model in turn can be fine-tuned with the new data to get a new updated base model."
- Break condition: If the iterative process leads to overfitting or if the quality of the curated data plateaus or decreases after a certain number of iterations, further iterations will not improve the model's performance.

## Foundational Learning

- Concept: Semantic similarity and disparity measurement
  - Why needed here: The method relies on the base model's ability to measure the semantic disparity between known and unknown question-response pairs to filter low-quality data.
  - Quick check question: How would you explain the difference between semantic similarity and semantic disparity in the context of this method?

- Concept: Few-shot learning and in-context learning
  - Why needed here: The method uses a small amount of seed data as few-shot demonstrations for guided question rewriting, relying on the base model's in-context learning capabilities.
  - Quick check question: What are the key differences between few-shot learning and in-context learning, and how are they applied in this method?

- Concept: Supervised fine-tuning and parameter-efficient tuning
  - Why needed here: The method uses supervised fine-tuning to train the base model on curated unknown question-response data, employing parameter-efficient techniques like LoRA for efficient training.
  - Quick check question: How does parameter-efficient tuning like LoRA differ from full fine-tuning, and why is it beneficial in this context?

## Architecture Onboarding

- Component map: Seed data -> Guided Question Rewriting -> Conditioned Response Generation -> Disparity-driven Self-Curation -> Supervised Fine-tuning -> (Iterative loop back to Guided Question Rewriting)
- Critical path: The critical path is the iterative cycle of data augmentation (guided question rewriting and conditioned response generation) followed by data curation (disparity-driven self-curation) and model fine-tuning. The quality of the curated data directly impacts the effectiveness of the fine-tuning and subsequent iterations.
- Design tradeoffs: The method trades computational cost for improved performance. Iterative self-alignment can lead to better results but requires more computational resources. The choice of self-curation strategy (disparity-driven vs. principle-driven) also involves a tradeoff between effectiveness and implementation complexity.
- Failure signatures: Poor performance on unknown question detection or classification may indicate issues with the guided question rewriting or conditioned response generation stages. If the method does not improve the model's performance on known questions, it may be overfitting to unknown questions or the self-curation process may be filtering too aggressively.
- First 3 experiments:
  1. Implement and evaluate the guided question rewriting stage using a small set of seed data and known questions to assess the quality of the generated unknown questions.
  2. Implement and evaluate the conditioned response generation stage to assess the quality of the generated responses for unknown questions.
  3. Implement and evaluate the disparity-driven self-curation stage to assess its effectiveness in filtering low-quality unknown question-response pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Self-Aligned method perform on open-ended response generation for known questions compared to unknown questions?
- Basis in paper: [explicit] The paper mentions evaluating the Self-Aligned method on both known and unknown questions for open-ended response generation, and compares the results.
- Why unresolved: The paper does not provide detailed results or analysis of the Self-Aligned method's performance specifically on known questions, making it difficult to assess its impact on known question answering.
- What evidence would resolve it: Detailed evaluation results and analysis of the Self-Aligned method's performance on known questions for open-ended response generation, compared to its performance on unknown questions.

### Open Question 2
- Question: How does the Self-Aligned method's performance on unknown question detection and classification vary across different base models (Vicuna vs. LLaMA-2)?
- Basis in paper: [explicit] The paper mentions comparing the performance of Vicuna and LLaMA-2 as base models for the Self-Aligned method on unknown question detection and classification tasks.
- Why unresolved: While the paper provides some comparison results, it does not delve into the reasons behind the performance differences between the two base models or explore ways to further improve the method's performance with different base models.
- What evidence would resolve it: A detailed analysis of the performance differences between Vicuna and LLaMA-2 as base models for the Self-Aligned method, including insights into the factors contributing to the differences and potential strategies to optimize the method for each base model.

### Open Question 3
- Question: How does the Self-Aligned method handle more complex or nuanced types of unknown questions, such as those involving sarcasm, irony, or implicit assumptions?
- Basis in paper: [inferred] The paper mentions the Self-Aligned method's ability to handle four types of unknown questions (incomplete, futuristic, incorrect, and ambiguous), but does not explicitly discuss its performance on more complex or nuanced question types.
- Why unresolved: The paper's evaluation focuses on relatively straightforward types of unknown questions, leaving the method's effectiveness on more challenging question types unclear.
- What evidence would resolve it: Experiments and analysis of the Self-Aligned method's performance on a diverse set of complex and nuanced unknown question types, including sarcasm, irony, and implicit assumptions, to assess its robustness and adaptability to different question styles.

## Limitations
- The method's effectiveness depends on the base model's ability to accurately assess semantic disparity, which may not always correlate with answer quality
- Significant computational resources are required for iterative self-alignment, limiting practical applicability for resource-constrained environments
- Evaluation focuses primarily on four specific types of unknown questions, leaving questions about generalization to other forms of unanswerable queries

## Confidence
- High Confidence: The core mechanism of using disparity-driven self-curation to filter low-quality unknown question-response pairs is well-supported by experimental results
- Medium Confidence: The iterative self-alignment process shows promise, but optimal number of iterations and potential diminishing returns need further validation
- Low Confidence: Scalability to larger, more diverse datasets and performance on question types beyond the four studied categories remain uncertain

## Next Checks
1. Conduct an ablation study comparing disparity-driven self-curation with alternative filtering approaches to quantify the specific contribution of disparity-based filtering
2. Systematically evaluate performance gains across multiple self-alignment iterations to determine optimal iteration count and identify potential diminishing returns
3. Test the Self-Aligned method on additional datasets containing different types of unknown questions (such as commonsense reasoning gaps) to assess broader applicability