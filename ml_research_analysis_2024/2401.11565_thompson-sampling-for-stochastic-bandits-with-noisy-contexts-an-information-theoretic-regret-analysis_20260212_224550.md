---
ver: rpa2
title: 'Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic
  Regret Analysis'
arxiv_id: '2401.11565'
source_url: https://arxiv.org/abs/2401.11565
tags:
- distribution
- where
- context
- 'true'
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of stochastic linear contextual
  bandits with noisy contexts, where the agent observes a corrupted version of the
  true context through an unknown noise channel. The key idea is to design a Thompson
  sampling algorithm that approximates the action policy of an oracle with access
  to the reward model, channel parameter, and predictive distribution of the true
  context.
---

# Thompson Sampling for Stochastic Bandits with Noisy Contexts: An Information-Theoretic Regret Analysis

## Quick Facts
- arXiv ID: 2401.11565
- Source URL: https://arxiv.org/abs/2401.11565
- Reference count: 40
- One-line result: Thompson Sampling algorithm for linear contextual bandits with noisy contexts achieves O(d√T) Bayesian regret using de-noising approach

## Executive Summary
This paper addresses stochastic linear contextual bandits where the agent observes corrupted versions of true contexts through an unknown noise channel. The key contribution is a Thompson Sampling algorithm that approximates the action policy of an oracle with access to the true context by de-noising observed noisy contexts and using a sampling distribution that approximates the true posterior. For Gaussian bandits with Gaussian context noise, the algorithm achieves O(d√T) Bayesian regret, where d is the context dimension and T is the time horizon. The paper also extends the analysis to settings with delayed true contexts, showing that delayed observations lead to lower Bayesian regret.

## Method Summary
The method uses modified Thompson Sampling with a de-noising approach to handle noisy context observations. In each round, the agent estimates the predictive distribution of the true context from the observed noisy context using a de-noising step based on past observations. It then samples from an approximate posterior distribution of the reward parameter and selects the action that maximizes the expected reward. The algorithm is specifically analyzed for Gaussian bandits with Gaussian context noise, with regret bounds derived using information-theoretic techniques. The approach is also extended to handle delayed true contexts, where the agent observes the true context after a delay following the reward observation.

## Key Results
- Thompson Sampling with de-noising achieves O(d√T) Bayesian regret for Gaussian bandits with Gaussian context noise
- Posterior mismatch between sampling distribution and true posterior is quantified by KL divergence in the regret bound
- Delayed true contexts lead to lower Bayesian regret compared to immediate noisy contexts
- Algorithm outperforms baselines on synthetic Gaussian and logistic bandit problems as well as MovieLens dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Thompson Sampling approximates oracle policy via de-noising predictive distribution from noisy contexts.
- **Mechanism:** The algorithm estimates a posterior predictive distribution P(c_t|ĉ_t, H_{t-1}) from noisy contexts and uses this to approximate the oracle's action policy that would otherwise have access to the true context. This is done by first inferring the posterior distribution of the noise channel parameter γ* from past noisy contexts, then using this to compute the predictive distribution of the true context given the current noisy context.
- **Core assumption:** The context distribution P(c) and noise channel P(ĉ|c, γ*) are known to the agent.
- **Evidence anchors:**
  - [abstract]: "Our objective is to design an action policy that can 'approximate' that of an oracle, which has access to the reward model, the channel parameter, and the predictive distribution of the true context from the observed noisy context."
  - [section II]: "In each round, the environment samples a true context vector c_t from a context distribution that is known to the agent. The agent, however, does not observe the true context, but observes a noisy version ĉ_t of the true context, obtained as the output of a noise channel P(ĉ_t|c_t, γ*) parameterized by the noise channel parameter γ* that is unknown to the agent."
- **Break condition:** If the context distribution or noise channel model is misspecified, the de-noising step will produce biased estimates leading to poor action selection.

### Mechanism 2
- **Claim:** Sampling distribution approximates true posterior, with mismatch quantified by KL divergence.
- **Mechanism:** Since the agent cannot observe true contexts, it cannot evaluate the exact posterior P(θ*|H_{t-1}, r, a, ĉ). Instead, it uses a sampling distribution P̄(θ*|H_{t-1}, r, a, ĉ) that approximates the true posterior. The information-theoretic analysis shows that the Bayesian regret includes an error term proportional to the expected KL divergence between these distributions.
- **Core assumption:** A tractable approximation to the true posterior can be found that is sufficiently close in KL divergence.
- **Evidence anchors:**
  - [abstract]: "The algorithm uses a de-noising approach to estimate the predictive distribution from observed noisy contexts and employs a sampling distribution that approximates the true posterior."
  - [section III-A]: "For the modified-TS step, we sample θ_t from a multivariate Gaussian distribution P̄(θ*|H_{t-1}, r, a, ĉ) = N(μ_{t-1}, Σ_{t-1}^{-1}) whose inverse covariance matrix and mean respectively evaluate as..."
- **Break condition:** If the sampling distribution is too far from the true posterior (large KL divergence), the approximation error will dominate the regret bound.

### Mechanism 3
- **Claim:** Delayed true contexts reduce Bayesian regret by eliminating posterior mismatch.
- **Mechanism:** When the agent observes true contexts with delay, it can evaluate the exact posterior distribution with known contexts, eliminating the need for approximation. This results in tighter regret bounds compared to the setting with only noisy contexts.
- **Core assumption:** The delay in observing true contexts is bounded and does not prevent effective learning.
- **Evidence anchors:**
  - [abstract]: "We also extend this problem to a scenario where the agent observes the true context with some delay after receiving the reward and show that delayed true contexts lead to lower Bayesian regret."
  - [section IV]: "Due to availability of delayed true contexts, there is no posterior mismatch in Algorithm 2. Hence, we apply (Neu et al., 2022, Cor. 3) to yield the following upper bound on RT_{d,CB}."
- **Break condition:** If the delay is too long, the agent may not have timely information to make effective decisions.

## Foundational Learning

- **Concept:** Bayesian regret and information-theoretic analysis
  - **Why needed here:** The paper uses Bayesian framework with priors on reward and channel parameters, and derives information-theoretic bounds on cumulative regret rather than frequentist bounds.
  - **Quick check question:** What is the difference between Bayesian regret and frequentist regret, and why would one choose a Bayesian framework for this problem?

- **Concept:** Thompson Sampling and posterior sampling
  - **Why needed here:** The algorithm is based on Thompson Sampling, which requires sampling from posterior distributions to select actions based on probability matching.
  - **Quick check question:** How does Thompson Sampling differ from Upper Confidence Bound (UCB) methods in contextual bandits?

- **Concept:** Kullback-Leibler divergence and mutual information
  - **Why needed here:** The regret analysis uses KL divergence to quantify the mismatch between the sampling distribution and true posterior, and mutual information to bound the error from approximating the predictive distribution.
  - **Quick check question:** How does KL divergence measure the difference between two probability distributions, and why is it appropriate for measuring posterior approximation error?

## Architecture Onboarding

- **Component map:** Noisy context observation → De-noising → Posterior approximation → Sampling → Action selection → Reward observation
- **Critical path:** The algorithm processes noisy context observations through de-noising to estimate the predictive distribution, approximates the posterior for the reward parameter, samples from this approximation, and selects actions based on sampled parameters.
- **Design tradeoffs:** 
  - Exact vs. approximate posterior: Exact computation requires true contexts, so approximation is necessary; tradeoff is between computational tractability and accuracy
  - Delayed vs. immediate contexts: Delayed contexts eliminate approximation error but may slow learning; tradeoff is between regret reduction and timeliness
- **Failure signatures:**
  - High regret despite many iterations: Could indicate poor posterior approximation or misspecified noise model
  - Instability in action selection: Could indicate numerical issues in computing predictive distributions
  - Slow convergence: Could indicate insufficient exploration or poor choice of priors
- **First 3 experiments:**
  1. Implement the basic Thompson Sampling algorithm with Gaussian bandits and verify it matches theoretical regret bounds on synthetic data
  2. Add the de-noising component and test with varying levels of noise to verify robustness
  3. Implement delayed context version and compare regret bounds to immediate context version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Thompson Sampling algorithm perform under non-Gaussian reward distributions, such as Bernoulli or heavy-tailed distributions, in the noisy context setting?
- Basis in paper: [explicit] The paper mentions that the regret analysis is specifically tailored for Gaussian bandits, but empirical results are shown for logistic bandits with Bernoulli rewards, suggesting potential extension.
- Why unresolved: The regret analysis in the paper is focused on Gaussian bandits due to analytical tractability, and the extension to other distributions remains theoretical without formal analysis.
- What evidence would resolve it: Formal regret bounds and convergence analysis for non-Gaussian reward distributions, along with empirical validation across diverse reward models.

### Open Question 2
- Question: What is the impact of delayed true contexts on the regret bounds when the noise in the context channel is non-Gaussian or has time-varying parameters?
- Basis in paper: [inferred] The paper extends the analysis to delayed true contexts but only under Gaussian noise assumptions. It suggests that delayed contexts reduce regret, but this is not explored for non-Gaussian or dynamic noise settings.
- Why unresolved: The current analysis assumes stationary Gaussian noise, and the effect of non-Gaussian or time-varying noise on regret is not addressed.
- What evidence would resolve it: Analytical and empirical studies comparing regret bounds under delayed contexts for various noise distributions, including time-varying scenarios.

### Open Question 3
- Question: How sensitive is the proposed algorithm to the choice of the prior distributions for the reward parameter and the noise channel parameter?
- Basis in paper: [explicit] The algorithm uses Gaussian priors for both the reward parameter and noise channel parameter, but the sensitivity to prior misspecification is not discussed.
- Why unresolved: The paper assumes known prior distributions but does not explore the impact of incorrect or poorly specified priors on algorithm performance.
- What evidence would resolve it: Sensitivity analysis experiments showing regret performance under different prior specifications, including misspecified or non-informative priors.

### Open Question 4
- Question: Can the proposed Thompson Sampling algorithm be adapted to settings where the context distribution itself is unknown or time-varying?
- Basis in paper: [inferred] The algorithm assumes a known context distribution, which may not hold in dynamic or non-stationary environments. The paper does not explore adaptive methods for unknown or changing context distributions.
- Why unresolved: The current framework relies on a fixed context distribution, and extending it to adaptive or online learning of the context distribution is not addressed.
- What evidence would resolve it: Development of an adaptive version of the algorithm that learns the context distribution online, along with corresponding regret bounds and empirical validation.

## Limitations
- Information-theoretic regret analysis relies heavily on KL divergence bounds between sampling and true posteriors, which may be loose in practice
- Computational complexity of de-noising step and sampling procedures not fully characterized for large-scale problems
- Theoretical analysis limited to Gaussian bandits, with only empirical results for logistic bandits

## Confidence

- **High confidence:** The core mechanism of using Thompson Sampling with de-noised contexts to approximate an oracle policy is well-grounded and the Gaussian case analysis appears sound. The extension to delayed contexts reducing regret is logically consistent with the theoretical framework.
- **Medium confidence:** The information-theoretic bounds for the general case with KL divergence penalties are mathematically rigorous but may be loose in practice. The empirical results show promising performance but are limited to relatively small-scale problems.
- **Low confidence:** The scalability of the algorithm to high-dimensional contexts and large action spaces, particularly the computational feasibility of the de-noising and sampling procedures, is not well-established.

## Next Checks

1. **Empirical validation of regret bounds:** Run experiments across a wider range of noise levels and problem dimensions to empirically verify that the Bayesian regret follows the predicted O(d√T) scaling and to measure the actual KL divergence between the sampling and true posteriors.

2. **Scalability analysis:** Implement the algorithm with high-dimensional contexts (d > 100) and measure computational time for the de-noising step and sampling procedures to identify bottlenecks and assess practical feasibility.

3. **Robustness to model misspecification:** Test the algorithm when the assumed context distribution or noise channel model is misspecified, measuring performance degradation to understand failure modes and identify when the approach breaks down.