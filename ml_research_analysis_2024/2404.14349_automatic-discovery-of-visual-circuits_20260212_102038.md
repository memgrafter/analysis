---
ver: rpa2
title: Automatic Discovery of Visual Circuits
arxiv_id: '2404.14349'
source_url: https://arxiv.org/abs/2404.14349
tags:
- circuit
- neurons
- circuits
- concept
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLA (Cross-Layer Attribution), a method for
  automatically discovering functional subgraphs ("circuits") in vision models that
  underlie specific visual concepts. The approach uses attribution scores between
  neurons in successive layers to iteratively build and refine a subgraph corresponding
  to an input concept.
---

# Automatic Discovery of Visual Circuits

## Quick Facts
- arXiv ID: 2404.14349
- Source URL: https://arxiv.org/abs/2404.14349
- Authors: Achyuta Rajaram; Neil Chowdhury; Antonio Torralba; Jacob Andreas; Sarah Schwettmann
- Reference count: 40
- Key outcome: CLA successfully recovers known circuits, identifies compositional concept circuits in synthetic datasets, and defends CLIP from adversarial attacks with 97% to 13% reduction in attack success

## Executive Summary
This paper introduces CLA (Cross-Layer Attribution), a method for automatically discovering functional subgraphs ("circuits") in vision models that underlie specific visual concepts. CLA computes attribution scores between neurons in successive layers to iteratively build and refine a subgraph corresponding to an input concept. The method successfully recovers a known car-detection circuit in InceptionV1, identifies intermediate concept circuits in a synthetic compositional dataset that causally affect model predictions, and demonstrates practical utility by defending CLIP from text-based adversarial attacks through circuit pruning.

## Method Summary
CLA automatically discovers visual circuits by computing attribution scores between neurons in successive layers, then iteratively refining circuits through layer-by-layer selection of neurons that maximize total attribution to/from neighboring circuit layers. The method takes a few example images of a target concept and traces functional connectivity across the model's layers to identify the subgraph responsible for representing that concept. Circuit validation is performed through interventions that zero activations in identified circuit neurons while preserving clean activations outside the circuit, measuring the causal effect on model predictions.

## Key Results
- CLA successfully recovers a manually-identified car-detection circuit in InceptionV1 that was previously discovered through extensive manual analysis
- On the CatFish synthetic dataset, CLA identifies intermediate concept circuits (e.g., "car" and "cat" for "CatCar" class) that causally affect model predictions, with circuits showing high overlap across related output classes
- Applying circuit pruning to defend CLIP from text-based adversarial attacks reduces attack success from 97% to 13% while affecting only 6% of model edges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLA recovers functional subgraphs by measuring cross-layer attribution between neurons in successive layers
- Mechanism: The method computes attribution scores for each neuron pair (m in layer li, n in layer li+1) by multiplying the activation magnitude |ai,m| with the gradient of the downstream activation ∂∥li+1,n(ai)∥2/∂ai,m. This creates an input-distribution-dependent functional connectivity graph that identifies which neurons in layer li influence neurons in layer li+1 for the given concept.
- Core assumption: Functional connectivity between neurons can be meaningfully measured through attribution scores that combine both relevance (activation magnitude) and influence (gradient information)
- Evidence anchors:
  - [abstract]: "Our approach instead focuses on a functional notion of connectivity, based on which features influence the computation of other features in the input distribution"
  - [section]: "The score for a pair of neurons (m ∈ li, n ∈ li+1) takes into account both relevance and influence, by multiplying together two terms: one corresponding to the magnitude of the activations, and one corresponding to the gradient of activations across layers"
  - [corpus]: Weak evidence - no direct corpus support for this specific attribution mechanism

### Mechanism 2
- Claim: Iterative refinement of circuits by sweeping through layers converges to stable subgraphs representing the concept
- Mechanism: After initial selection of top-k neurons per layer based on total attribution scores, the algorithm iteratively re-selects neurons in each layer to maximize the sum of attributions to neurons in the circuit from both the previous and next layers. This bidirectional refinement continues until the circuit stops changing
- Core assumption: The true concept circuit can be found by greedily selecting neurons that maximize attribution to/from neighboring circuit layers
- Evidence anchors:
  - [section]: "We iteratively refine the resulting circuit by 'sweeping' through the layers, re-selecting neurons to maximise the total sum of their attribution scores to the neurons within the circuit in the successive and previous layers"
  - [abstract]: "We introduce a new method for identifying these subgraphs: specifying a visual concept using a few examples, and then tracing the interdependence of neuron activations across layers"
  - [corpus]: Weak evidence - no direct corpus support for the iterative refinement convergence properties

### Mechanism 3
- Claim: Circuit pruning interventions validate that identified circuits causally affect model behavior for the target concept
- Mechanism: The intervention zeros activations in circuit neurons while preserving clean activations outside the circuit. For edge pruning, only the first two layers of the circuit are ablated. This prevents information flow through the circuit while maintaining other model functionality, allowing measurement of the circuit's specific contribution to predictions
- Core assumption: Removing a circuit's information flow while preserving other pathways will reveal the circuit's unique contribution to concept representation
- Evidence anchors:
  - [section]: "We can inhibit the identified circuit by edge pruning: corrupting (zeroing) all paths between the first and second layers of the circuit... If the circuit is exhaustive, this intervention should cause the model to fail to represent a specific visual concept"
  - [abstract]: "We find that our approach extracts circuits that causally affect model output, and that editing these circuits can defend large pretrained models from adversarial attacks"
  - [corpus]: Weak evidence - no direct corpus support for this specific intervention methodology

## Foundational Learning

- Concept: Attribution methods in neural networks
  - Why needed here: CLA builds on attribution techniques to measure functional connectivity between internal layers, extending methods like Grad-CAM that attribute outputs to inputs
  - Quick check question: What's the difference between input attribution (like Grad-CAM) and cross-layer attribution used in CLA?

- Concept: Circuit analysis and mechanistic interpretability
  - Why needed here: The paper's goal is to automatically discover interpretable computational subgraphs, which requires understanding how circuits represent specific functions in neural networks
  - Quick check question: How does CLA's definition of circuits (intermediate concept representations) differ from traditional weight-based circuit discovery?

- Concept: Adversarial attack defense through model editing
  - Why needed here: The CLIP defense application demonstrates how circuit identification enables targeted model interventions to remove spurious correlations
  - Quick check question: Why might circuit pruning be more efficient than traditional adversarial training for defense?

## Architecture Onboarding

- Component map:
  - Attribution Matrix Calculator -> Circuit Builder -> Intervention Module -> Dataset Handler

- Critical path: Input examples → Attribution matrix computation → Circuit refinement (until convergence) → Circuit validation through intervention → Application to defense or analysis

- Design tradeoffs:
  - Topological restriction vs. flexibility: CLA assumes circuits follow sequential layer order, which is efficient but may miss circuits with complex connectivity patterns
  - k parameter selection: Larger k captures more comprehensive circuits but increases computational cost and may include spurious neurons
  - Attribution method choice: The specific attribution formulation balances relevance and influence but may not capture all functional dependencies

- Failure signatures:
  - Circuit convergence to trivial solutions (e.g., always selecting the same neurons regardless of input)
  - Interventions that don't significantly affect model behavior despite circuit identification
  - High overlap between random neuron selection and CLA-selected circuits, indicating poor discrimination

- First 3 experiments:
  1. Replicate CLA on a simple known circuit (like the car circuit) to verify basic functionality
  2. Test circuit stability across different k values to understand parameter sensitivity
  3. Apply edge pruning to a small circuit and measure effect on intermediate concept representation before scaling to full models

## Open Questions the Paper Calls Out

The paper explicitly calls out several open questions that would benefit from further investigation, including how CLA's performance compares to weight-based circuit discovery methods across different model architectures, what the optimal circuit size (k parameter) should be for maximizing causal effect while minimizing collateral damage, and how CLA can be extended to discover multi-hop circuits involving non-adjacent layers.

## Limitations

- CLA assumes circuits follow sequential layer order, which may miss circuits with complex connectivity patterns
- The method's effectiveness depends heavily on the k parameter for neuron selection, introducing a tradeoff between comprehensiveness and computational efficiency
- The attribution-based approach may struggle with polysemantic neurons that serve multiple functions depending on context

## Confidence

- **High Confidence**: CLA successfully recovers known circuits and demonstrates measurable effects on model behavior through interventions
- **Medium Confidence**: The claim that CLA circuits causally affect predictions is supported but requires more extensive validation across different architectures and concepts
- **Low Confidence**: The scalability of CLA to extremely large models (>100B parameters) and its effectiveness on natural datasets beyond synthetic CatFish compositions remains unproven

## Next Checks

1. Test CLA on a known circuit from a different vision architecture (e.g., ResNet or Vision Transformer) to assess generalizability beyond InceptionV1
2. Evaluate circuit stability and overlap when varying the k parameter across multiple orders of magnitude to understand the method's sensitivity
3. Apply CLA to a real-world compositional dataset (e.g., objects with varying backgrounds or lighting conditions) to test performance beyond synthetic compositions