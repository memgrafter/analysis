---
ver: rpa2
title: 'MINER: Mining the Underlying Pattern of Modality-Specific Neurons in Multimodal
  Large Language Models'
arxiv_id: '2410.04819'
source_url: https://arxiv.org/abs/2410.04819
tags:
- neurons
- modality
- arxiv
- preprint
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MINER, a framework for identifying modality-specific
  neurons (MSNs) in multimodal large language models (MLLMs). MINER addresses the
  challenge of understanding how different modalities (e.g., text, image, audio) are
  processed by MLLMs by proposing a method to mine and analyze MSNs.
---

# MINER: Mining the Underlying Pattern of Modality-Specific Neurons in Multimodal Large Language Models

## Quick Facts
- **arXiv ID**: 2410.04819
- **Source URL**: https://arxiv.org/abs/2410.04819
- **Reference count**: 33
- **Primary result**: Identifies modality-specific neurons in MLLMs and shows that deactivating just 2% significantly degrades performance

## Executive Summary
This paper introduces MINER, a framework for identifying modality-specific neurons (MSNs) in multimodal large language models (MLLMs). The framework addresses the challenge of understanding how different modalities are processed by proposing a method to mine and analyze MSNs through four stages: modality separation, importance score calculation, importance score aggregation, and modality-specific neuron selection. Experiments on six benchmarks and two representative MLLMs (Qwen2-VL and Qwen2-Audio) demonstrate that deactivating only 2% of MSNs significantly reduces model performance, revealing insights into multimodal processing mechanisms.

## Method Summary
MINER proposes a systematic approach to identify modality-specific neurons in MLLMs through four key stages. First, it separates input modalities during processing to isolate their distinct representations. Second, it calculates importance scores for each neuron based on their contribution to modality-specific processing. Third, it aggregates these importance scores across different layers and attention heads to identify consistently modality-specific neurons. Finally, it selects neurons that show high importance scores specifically for one modality while maintaining low scores for others. The framework uses token-level cross-attention scores as a proxy for modality-specificity and employs ablation studies to validate the functional importance of identified MSNs.

## Key Results
- Deactivating only 2% of MSNs reduces Qwen2-VL performance from 0.56 to 0.24 and Qwen2-Audio from 0.69 to 0.31
- Different modalities converge in the lower layers of the model architecture
- Two phenomena identified: semantic probing (neurons responding to semantic features) and semantic telomeres (layer-specific semantic representations)

## Why This Works (Mechanism)
MINER works by leveraging the inherent attention mechanisms in MLLMs to identify neurons that consistently respond to specific modalities. The framework exploits the fact that token-level cross-attention scores reflect the importance of neurons in processing particular modalities. By aggregating these scores across layers and attention heads, MINER can identify neurons that show strong modality-specific activation patterns. The ablation experiments validate that these identified neurons are functionally important for maintaining multimodal performance, as their removal causes substantial performance degradation.

## Foundational Learning
- **Modality separation**: Understanding how different input types are processed independently in MLLMs - needed to isolate modality-specific effects
- **Cross-attention mechanisms**: How attention heads process different modalities - needed to understand token-level importance
- **Neuron importance scoring**: Methods for quantifying neuron contributions - needed to identify key modality-specific neurons
- **Layer-wise convergence**: How different modalities' representations evolve through network depth - needed to understand processing hierarchies
- **Ablation studies**: Systematic removal of components to test functional importance - needed to validate MSN identification
- **Semantic probing**: Analyzing neurons for semantic feature detection - needed to understand what MSNs actually encode

## Architecture Onboarding

**Component map**: Input Modalities → Encoder Layers → Cross-Attention Heads → Neuron Importance Scoring → MSNs → Performance Evaluation

**Critical path**: Modality Separation → Importance Score Calculation → Aggregation → Neuron Selection → Ablation Validation

**Design tradeoffs**: Token-level attention scores provide granular analysis but may miss higher-level semantic interactions; 2% ablation threshold balances impact demonstration with model viability

**Failure signatures**: If MSNs cannot be clearly identified, may indicate either highly distributed processing or limitations in the importance scoring method; poor ablation results may suggest incorrect MSN identification

**3 first experiments**:
1. Validate MSN identification by comparing against random neuron ablation baselines
2. Test MSN importance across different MLLM architectures to assess generalizability
3. Examine layer-wise distribution of MSNs to confirm convergence observations

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Methodological assumptions may not capture complex cross-modal neuron activation patterns
- Experimental validation limited to two specific MLLM architectures (Qwen2-VL and Qwen2-Audio)
- 2% ablation threshold may not represent full impact across different model scales

## Confidence
- **High**: Core findings on MSN identification and performance impact from ablation studies
- **Medium**: Layer-wise convergence observations dependent on specific architectural configurations
- **Low**: Semantic probing and semantic telomeres phenomena due to limited quantitative validation

## Next Checks
1. Test MINER across diverse MLLM architectures (e.g., GPT-4V, Gemini) to assess generalizability
2. Implement complementary analysis using integrated gradients or causal tracing to validate MSN modality-specificity
3. Conduct ablation studies with varying percentages of deactivated neurons beyond 2% to establish comprehensive impact relationships