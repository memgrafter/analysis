---
ver: rpa2
title: Enhancing Adversarial Robustness through Multi-Objective Representation Learning
arxiv_id: '2410.01697'
source_url: https://arxiv.org/abs/2410.01697
tags:
- adversarial
- morel
- robustness
- learning
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving adversarial robustness
  in deep neural networks without requiring architectural changes or test-time data
  purification. The authors propose MOREL (Multi-Objective Representation Learning),
  a framework that learns robust feature representations by aligning natural and adversarial
  features in a shared embedding space during training.
---

# Enhancing Adversarial Robustness through Multi-Objective Representation Learning

## Quick Facts
- arXiv ID: 2410.01697
- Source URL: https://arxiv.org/abs/2410.01697
- Reference count: 36
- Primary result: MOREL achieves superior accuracy-robustness trade-off against white-box and black-box attacks on CIFAR-10, CIFAR-100, and Tiny-ImageNet without architectural changes

## Executive Summary
This paper addresses the challenge of improving adversarial robustness in deep neural networks without requiring architectural changes or test-time data purification. The authors propose MOREL (Multi-Objective Representation Learning), a framework that learns robust feature representations by aligning natural and adversarial features in a shared embedding space during training. MOREL employs a multi-objective optimization approach, balancing two competing objectives: learning robust features and maintaining high classification accuracy. The method uses cosine similarity and multi-positive contrastive losses to encourage similar features for same-class inputs, regardless of perturbations.

## Method Summary
MOREL is a framework that learns robust feature representations by aligning natural and adversarial features in a shared embedding space during training. It uses a multi-objective optimization approach with Conic Scalarization to balance robust feature learning (via cosine similarity and multi-positive contrastive losses) and classification accuracy. The method processes features through an encoder, linear projection, class-adaptive multi-head attention module, and classifier, with loss computed on both natural and adversarial examples.

## Key Results
- MOREL significantly outperforms existing adversarial training methods like TRADES, MART, and LOAT in terms of accuracy-robustness trade-off
- Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet18 and WideResNet34-10 architectures
- Demonstrates superior performance against white-box attacks (PGD-20, PGD-100, CW∞) and black-box attacks (AutoAttack, SquareAttack)
- Achieves better robustness without requiring architectural changes or test-time data purification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MOREL aligns natural and adversarial features in a shared embedding space, making perturbations less likely to flip class boundaries.
- Mechanism: By minimizing cosine similarity loss between natural and adversarial features and clustering same-class features via multi-positive contrastive loss, MOREL creates compact, well-separated class regions in embedding space that require larger perturbations to cross.
- Core assumption: Features from same-class inputs (clean or adversarial) should be similar, and dissimilar across classes, regardless of small perturbations.
- Evidence anchors:
  - [abstract] "aligns natural and adversarial features using cosine similarity and multi-positive contrastive losses to encourage similar features for same-class inputs"
  - [section 3.1] "constrain the model to produce features that are as similar as possible for input images within the same class, and as dissimilar as possible from feature distributions of other classes"
  - [corpus] Weak: Corpus lacks direct references to cosine similarity-based feature alignment; only general multi-objective adversarial robustness.

### Mechanism 2
- Claim: MOREL balances robustness and accuracy as a multi-objective optimization problem using Conic Scalarization.
- Mechanism: By optimizing two objectives (robust feature learning and classification accuracy) simultaneously via CS(k, γ, a), MOREL finds a Pareto-optimal trade-off point that improves both metrics relative to single-objective baselines.
- Core assumption: Adversarial training can be improved by explicitly balancing two competing objectives rather than optimizing robustness alone.
- Evidence anchors:
  - [abstract] "multi-objective optimization approach, balancing two competing objectives: learning robust features and maintaining high classification accuracy"
  - [section 3.1] "We approach this challenge as a multi-objective optimization problem... effectively balancing these objectives to enhance the accuracy-robustness trade-off"
  - [section 4.4] "Figure 3 provides a visualization of how the loss terms and performance against PGD-20 evolve as we adjust the values of k1 and k2"

### Mechanism 3
- Claim: The class-adaptive multi-head attention module captures global context dependencies within same-class feature groups, enhancing representational quality.
- Mechanism: Me processes features grouped by class (Eq. 5) with attention, allowing cross-image interaction within classes to produce richer, more discriminative embeddings before contrastive loss computation.
- Core assumption: Within-class feature dependencies contain useful information for clustering and discrimination that standard concatenation would miss.
- Evidence anchors:
  - [section 3.1] "a class-adaptive multi-head attention module Me enables interaction within each lower-dimensional feature group, resulting in richer feature representations"
  - [section 3.1] "This approach takes advantage of the global context understanding property of the attention mechanism... to capture dependencies and relationships across features within the same group"
  - [corpus] Weak: No direct corpus evidence for class-adaptive attention in adversarial robustness; general ViT attention references only.

## Foundational Learning

- Concept: Multi-objective optimization and Pareto optimality
  - Why needed here: MOREL explicitly balances two conflicting objectives (robustness vs accuracy) rather than optimizing one at the expense of the other
  - Quick check question: What is the difference between weighted sum and conic scalarization in multi-objective optimization?

- Concept: Contrastive learning principles (similarity and separation in embedding space)
  - Why needed here: MOREL uses multi-positive contrastive loss to pull same-class features together and push different-class features apart
  - Quick check question: How does contrastive loss differ from standard classification loss in terms of what it optimizes in the embedding space?

- Concept: Adversarial training and PGD attacks
  - Why needed here: MOREL builds on adversarial training foundations but improves it through feature-level alignment rather than just input-level augmentation
  - Quick check question: What is the role of the PGD attack in adversarial training, and how does it differ from FGSM?

## Architecture Onboarding

- Component map:
  Encoder g(·) -> Linear projection Le -> Class-adaptive multi-head attention Me -> Classifier h(·)

- Critical path:
  1. Forward pass through encoder → Le projection → class grouping → Me attention → T/T' features
  2. Compute Lcosine between Le outputs of clean and adversarial features
  3. Compute Lcsl using Me outputs (T normalized)
  4. Combine with baseline loss L2 using Conic Scalarization
  5. Backward pass updates encoder and Le (Me is only for training)

- Design tradeoffs:
  - Batch size vs robustness: Larger batches provide more negative samples but hurt robustness due to increased gradient complexity
  - Attention computation cost: O(n²×b) per batch vs potential representational gains
  - Feature type choice: Using adversarial features (T') vs natural features (T) in Lcsl affects robustness differently

- Failure signatures:
  - Poor robustness despite training: Likely issues with batch size, loss weighting (k values), or attention module implementation
  - Degraded clean accuracy: Imbalance in multi-objective optimization favoring robustness too heavily
  - Training instability: Incorrect implementation of normalization, attention, or projection layers

- First 3 experiments:
  1. Train MOREL with batch size 8 and default hyperparameters, compare against baseline TRADES on CIFAR-10
  2. Vary k1 in Conic Scalarization to find optimal robustness-accuracy balance (test k1=0.1, 0.3, 0.5, 0.7, 0.9)
  3. Remove Me module (set α=0) to measure its contribution to robustness improvements

## Open Questions the Paper Calls Out
None

## Limitations
- The class-adaptive multi-head attention module (Me) introduces significant computational overhead that may limit scalability to larger models or datasets
- The framework requires careful hyperparameter tuning (k values, α) to balance objectives effectively, with no clear guidance on optimal settings across different datasets
- The paper does not address potential distributional shift issues when applying the method to out-of-distribution data or domain adaptation scenarios

## Confidence
- **High confidence** in the core mechanism of feature alignment through cosine similarity and contrastive losses
- **Medium confidence** in the specific contributions of the class-adaptive attention module
- **Medium confidence** in the generalizability across architectures

## Next Checks
1. Perform detailed ablation studies on the class-adaptive attention module (Me) by systematically varying the number of attention heads and measuring impact on robustness gains versus computational cost
2. Test MOREL's performance on different model architectures (e.g., EfficientNet, ConvNeXt) and non-vision domains to assess generalizability beyond the current experimental scope
3. Conduct transfer learning experiments where MOREL is pre-trained on CIFAR-10 and fine-tuned on CIFAR-100/Tiny-ImageNet to evaluate cross-dataset robustness and identify potential overfitting to specific data distributions