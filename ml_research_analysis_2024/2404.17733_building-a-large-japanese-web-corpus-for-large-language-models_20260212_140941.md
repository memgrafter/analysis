---
ver: rpa2
title: Building a Large Japanese Web Corpus for Large Language Models
arxiv_id: '2404.17733'
source_url: https://arxiv.org/abs/2404.17733
tags:
- japanese
- text
- https
- language
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper builds a large Japanese web corpus by extracting and\
  \ refining text from Common Crawl archives (21 snapshots, 63.4 billion pages, 2020-2023),\
  \ resulting in 312.1 billion characters from 173 million pages\u2014the largest\
  \ Japanese LLM corpus available. The authors use rapid language detection to filter\
  \ for Japanese text, then apply quality filtering, deduplication, and host filtering\
  \ to ensure high-quality data."
---

# Building a Large Japanese Web Corpus for Large Language Models

## Quick Facts
- **arXiv ID**: 2404.17733
- **Source URL**: https://arxiv.org/abs/2404.17733
- **Reference count**: 39
- **Primary result**: Built the largest Japanese web corpus (312.1B characters from 173M pages) and achieved 6.6-8.1 point improvements on Japanese benchmarks through continual pre-training

## Executive Summary
This paper presents the construction of a large-scale Japanese web corpus for training large language models (LLMs). The authors extracted and refined text from 21 snapshots of Common Crawl archives (2020-2023), resulting in 312.1 billion characters from 173 million web pages. The corpus underwent rapid Japanese detection, quality filtering, deduplication, and host filtering to ensure high-quality data. The effectiveness was validated by continual pre-training Llama 2, Mistral, and Mixtral models, achieving consistent improvements of 6.6-8.1 points on Japanese benchmark datasets, with the largest gain of 7.0 points for Llama 2 13B.

## Method Summary
The authors extracted text from Common Crawl WARC files using Trafilatura, then applied rapid Japanese detection to filter non-Japanese pages and reduce processing time. Precise Japanese detection using a linear binary classifier was followed by quality filtering based on character percentages, sentence length, and NG expressions. The corpus underwent deduplication using MinHash, host filtering to remove harmful content, punctuation normalization, and footer trimming. The processed corpus was then used for continual pre-training of various LLM models (Llama 2, Mistral 7B, Mixtral 8x7B) to evaluate its effectiveness on Japanese benchmark datasets.

## Key Results
- Built the largest Japanese web corpus with 312.1 billion characters from 173 million pages
- Achieved 6.6-8.1 point improvements on Japanese benchmark datasets across model sizes
- Obtained state-of-the-art performance, with 7.0 point improvement for Llama 2 13B compared to other corpora

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Rapid language detection before full text extraction reduces total processing time by filtering out non-Japanese pages early.
- **Mechanism**: The system checks HTML language attributes and title tags for Japanese indicators. Only pages passing this filter undergo full text extraction and precise language detection.
- **Core assumption**: The rapid detection has sufficiently high precision that the time saved on skipped pages outweighs the overhead of running the filter.
- **Evidence anchors**:
  - [section] "The precision, recall, and F1 score were 0.888, 0.967, and 0.926, respectively" from the rapid detection evaluation.
  - [section] "the processing time for Steps 1–4 was about 15 times faster than Steps 1, 3, and 4 (without rapid language detection)"
- **Break condition**: If rapid detection precision drops significantly or the proportion of Japanese pages increases substantially, the time savings would diminish.

### Mechanism 2
- **Claim**: Quality filtering rules effectively remove low-value Japanese text while preserving useful content.
- **Mechanism**: The system applies 7 character-level rules (e.g., minimum hiragana percentage, maximum katakana percentage) and repetition detection to eliminate irrelevant pages like e-commerce sites and RSS feeds.
- **Core assumption**: The manually tuned thresholds effectively separate high-quality Japanese text from noise without overly aggressive filtering.
- **Evidence anchors**:
  - [section] "we observe that web pages that may be unuseful for training LLMs (e.g., e-commerce sites) have disappeared" after quality filtering.
  - [section] Specific rules described: "Rule 2 ensures that a text includes a certain amount of function words" and "Rule 3 rejects a web page containing a lot of product or service names"
- **Break condition**: If the Japanese web ecosystem changes (e.g., more katakana usage becomes common) or if the rules are too strict and remove valuable content.

### Mechanism 3
- **Claim**: Strict deduplication using MinHash with high Jaccard coefficient threshold preserves model efficiency while maintaining content diversity.
- **Mechanism**: Documents with Jaccard coefficient ≥ 0.9 are considered duplicates, keeping only the most recent version to prevent overfitting on repeated content.
- **Core assumption**: High threshold (0.9) effectively removes true duplicates while preserving near-duplicate content that might provide useful variation.
- **Evidence anchors**:
  - [section] "we adopted a setting where b = 20, r = 40 so that a pair of documents with a Jackard coefficient of 0.9 can be approximately detected as a duplicate with a probability of 92.5%"
  - [section] "the non-duplicate rate of web pages collected between March and June 2023 ranged from 77.8 to 87.9 percent"
- **Break condition**: If the threshold is too high and removes genuinely distinct content, or too low and fails to remove meaningful duplicates.

## Foundational Learning

- **Concept**: Japanese text characteristics (hiragana/katakana/kanji distribution)
  - **Why needed here**: Quality filtering rules depend on understanding what constitutes "normal" Japanese text composition
  - **Quick check question**: What percentage of hiragana characters would you expect in typical Japanese text, and why is this relevant to filtering?

- **Concept**: Web crawling and WARC/WET formats
  - **Why needed here**: Understanding the source data format is essential for implementing the extraction pipeline
  - **Quick check question**: What's the key difference between WARC and WET formats, and why did the authors choose WARC?

- **Concept**: Deduplication techniques (MinHash, Jaccard coefficient)
  - **Why needed here**: The deduplication step requires understanding how to efficiently identify similar documents
  - **Quick check question**: How does MinHash enable efficient duplicate detection at scale, and what does the Jaccard coefficient threshold of 0.9 mean in practice?

## Architecture Onboarding

- **Component map**: WARC file reader -> Rapid language detector -> HTML text extractor -> Precise language detector -> Quality filter -> Deduplicator -> Host filter -> Punctuation normalizer -> Footer remover
- **Critical path**: The end-to-end pipeline from WARC download to final corpus cleaning
- **Design tradeoffs**: WARC vs WET format (accuracy vs processing speed), aggressive vs conservative filtering, high vs low deduplication threshold
- **Failure signatures**: 
  - Low Japanese text yield → rapid detection too strict or source data issues
  - Poor model performance → quality filters too aggressive or deduplication too strict
  - Processing bottlenecks → text extraction or language detection inefficiencies
- **First 3 experiments**:
  1. Test rapid detection accuracy on a small WARC sample to validate the 15x speedup claim
  2. Run quality filtering on a subset with manual verification to tune thresholds
  3. Validate deduplication effectiveness by checking if known duplicate pairs are correctly identified

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the quality of the presented corpus compare to other Japanese corpora when used for training models from scratch rather than continual pre-training?
- **Basis in paper**: [explicit] The paper acknowledges this limitation in the "Future directions" section, stating "although our study focused on the continual pre-training setting, we want to evaluate the presented corpus by training Japanese LLMs from scratch."
- **Why unresolved**: The paper only evaluated the corpus through continual pre-training on existing models, which requires significant computational resources and may not fully capture the corpus's potential for initial model training.
- **What evidence would resolve it**: Training multiple Japanese LLMs from scratch using different corpora (including the presented corpus) and comparing their performance on Japanese benchmarks would provide definitive evidence.

### Open Question 2
- **Question**: What is the optimal ratio of Japanese to English tokens for continual pre-training Japanese LLMs to maximize performance in both languages?
- **Basis in paper**: [explicit] The paper mentions changing the ratio from 9:1 to 72:28 for Mixtral 8x7B Instruct and notes improvements in English-Japanese translation but degradation in Japanese-English translation, indicating the ratio significantly impacts performance.
- **Why unresolved**: The paper only tested a limited set of ratios and observed mixed results, suggesting that the optimal balance between maintaining English proficiency and enhancing Japanese capabilities remains unclear.
- **What evidence would resolve it**: Systematic experimentation with various Japanese-to-English token ratios across different model sizes and evaluating their performance on both language benchmarks would identify the optimal balance.

### Open Question 3
- **Question**: Can lightweight evaluation methods accurately predict the effectiveness of pre-training corpora without requiring full LLM training?
- **Basis in paper**: [explicit] The "Future directions" section states "training an LLM on a pre-training corpus requires huge computations. Therefore, we want to explore a lightweight method for assessing the effectiveness of pre-training corpora without building LLMs."
- **Why unresolved**: Current evaluation requires computationally expensive full model training, creating a barrier to efficiently assessing and comparing different corpora during development.
- **What evidence would resolve it**: Developing and validating proxy metrics or small-scale experiments that correlate strongly with full-scale training outcomes would enable efficient corpus evaluation.

## Limitations
- The paper doesn't provide detailed breakdowns of content sources or potential biases in the corpus
- Quality filtering thresholds weren't tested for sensitivity, making robustness unclear
- Evaluation focuses on general Japanese benchmarks without testing domain-specific performance

## Confidence
- **High Confidence**: The corpus construction methodology (rapid detection, quality filtering, deduplication) is technically sound and the reported processing speed improvements (15x faster) are well-supported by the precision/recall metrics provided.
- **Medium Confidence**: The performance improvements (6.6-8.1 points across benchmarks) are credible given the corpus size and quality, but the evaluation could benefit from additional ablation studies to isolate the contribution of different corpus components.
- **Low Confidence**: The claim of achieving "state-of-the-art performance across model sizes" lacks context about competing approaches and doesn't address potential overfitting to the specific benchmark datasets used.

## Next Checks
1. **Threshold Sensitivity Analysis**: Conduct experiments varying the quality filtering thresholds (hiragana/katakana ratios, sentence length) to quantify the trade-off between corpus size and model performance, providing error bars on the reported improvements.
2. **Domain-Specific Evaluation**: Test the pre-trained models on specialized Japanese datasets (technical documents, medical texts, legal corpora) to assess whether the web corpus improvements generalize beyond general language benchmarks.
3. **Ablation Study on Corpus Components**: Systematically remove different corpus components (Wikipedia, RefinedWeb, arXiv portions) from the training data to measure their individual contributions to the reported performance gains, addressing potential overfitting concerns.