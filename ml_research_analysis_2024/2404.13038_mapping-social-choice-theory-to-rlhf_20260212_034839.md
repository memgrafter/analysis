---
ver: rpa2
title: Mapping Social Choice Theory to RLHF
arxiv_id: '2404.13038'
source_url: https://arxiv.org/abs/2404.13038
tags:
- social
- rlhf
- choice
- preferences
- alternatives
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper bridges social choice theory and RLHF, arguing that
  core differences in problem settings necessitate adaptation of SCT''s technical
  results for RLHF. The authors identify key differences including: RLHF''s infinite,
  sampled alternative space versus SCT''s finite fixed set; partial evaluator feedback
  in RLHF versus full voter information in SCT; and RLHF''s goal of a generalizable
  reward function versus SCT''s focus on selecting a winner.'
---

# Mapping Social Choice Theory to RLHF

## Quick Facts
- arXiv ID: 2404.13038
- Source URL: https://arxiv.org/abs/2404.13038
- Authors: Jessica Dai; Eve Fleisig
- Reference count: 7
- Key outcome: Bridges social choice theory and RLHF by adapting SCT axioms to RLHF's infinite alternative space and partial feedback, proposing preference modeling voting rules and new axiom formulations.

## Executive Summary
This paper establishes a theoretical bridge between social choice theory and reinforcement learning from human feedback by identifying core differences in their problem settings and adapting SCT concepts for RLHF evaluation. The authors argue that while both domains involve aggregating preferences, RLHF's infinite alternative space, partial evaluator feedback, and goal of learning generalizable reward functions necessitate careful reformulation of SCT's technical results. They propose a "preference modeling voting rule" that maps alternatives to real-valued scores and adapt SCT axioms like unanimity, Condorcet consistency, and consistency to account for these differences, creating a framework for analyzing RLHF challenges such as evaluator disagreement and cognitive bias.

## Method Summary
The paper reformulates RLHF as a preference modeling voting rule that maps alternatives (prompt-response pairs) to real-valued scores representing population assessment. It adapts SCT axioms to the RLHF context by creating ε-threshold versions that account for infinite spaces and population-level preferences rather than fixed voters. The approach involves separating generalization (learning to score unseen alternatives) from aggregation (combining preferences), allowing SCT's axiomatic framework to be applied to the aggregation problem while recognizing that standard SCT doesn't address the generalization challenge central to RLHF. The method proposes new definitions for unanimity, Condorcet consistency, and consistency adapted to scoring rules over infinite spaces.

## Key Results
- Identifies key differences between SCT and RLHF including infinite vs. finite alternatives, partial vs. full feedback, and reward function vs. winner selection goals
- Proposes preference modeling voting rule that maps alternatives to real-valued scores for population assessment
- Adapts SCT axioms (unanimity, Condorcet consistency, consistency) to RLHF context using ε-threshold approach
- Demonstrates that SCT offers theoretical underpinnings to analyze RLHF challenges like evaluator disagreement and cognitive bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper's mapping between SCT and RLHF works because it identifies that both domains fundamentally deal with aggregating individual preferences into a global representation, even though their technical instantiations differ.
- Mechanism: By reformulating the RLHF problem as a "preference modeling voting rule" that maps alternatives to real-valued scores, the paper creates a bridge that allows SCT axioms to be adapted and applied to RLHF evaluation.
- Core assumption: Preferences in both domains can be modeled in a comparable mathematical framework despite differences in problem settings.
- Evidence anchors:
  - [abstract]: "We analyze the problem settings of social choice and RLHF, identify key differences between them, and discuss how these differences may affect the RLHF interpretation of well-known technical results in social choice."
  - [section]: "We propose a reformulation of the social choice problem that lets us interpolate between both settings."
- Break condition: If preferences in RLHF cannot be meaningfully modeled using the same mathematical framework as SCT preferences, or if the infinite nature of RLHF alternatives cannot be reconciled with SCT's finite alternatives framework.

### Mechanism 2
- Claim: The paper's approach works because it distinguishes between generalization (learning to score unseen alternatives) and aggregation (deciding how to combine preferences), allowing for targeted application of SCT concepts.
- Mechanism: By separating these two subproblems, the paper can apply SCT's axiomatic approach to the aggregation problem while recognizing that standard SCT doesn't address the generalization challenge that's central to RLHF.
- Core assumption: The aggregation and generalization problems in RLHF can be meaningfully separated and analyzed independently.
- Evidence anchors:
  - [section]: "RLHF research has focused almost exclusively on the generalization problem. This perspective, though reasonable under the assumption that evaluators do not meaningfully disagree on their preferences, does not account for meaningful and widespread disagreement between evaluators."
  - [corpus]: Weak - corpus doesn't provide direct evidence about this specific mechanism, but related work on "Adaptive Preference Aggregation" suggests this distinction is being explored in the field.
- Break condition: If the generalization and aggregation problems in RLHF are too tightly coupled to be separated, or if treating them independently leads to suboptimal solutions.

### Mechanism 3
- Claim: The paper's approach works because it adapts SCT axioms to the RLHF context by reformulating them to account for infinite spaces and population-level preferences rather than fixed voters.
- Mechanism: By creating new definitions like (a, ε)-unanimity and (a, ε)-Condorcet consistency that work with real-valued scores and population distributions, the paper makes SCT's axiomatic framework applicable to RLHF.
- Core assumption: SCT axioms can be meaningfully adapted to contexts with infinite alternatives and population-level preferences.
- Evidence anchors:
  - [section]: "To account for these differences, we propose definitions for unanimity, consistency, and Condorcet consistency for the RLHF setting as follows."
  - [section]: "A core tenet of social choice is that voting rules can be axiomatically analyzed; i.e., absent the notion of some 'ground truth,' there are particular principles that the final output should follow."
- Break condition: If the adapted axioms lose their fundamental meaning when applied to RLHF, or if they cannot capture the essential properties needed for RLHF evaluation.

## Foundational Learning

- Concept: Social Choice Theory (SCT) fundamentals
  - Why needed here: Understanding SCT axioms like unanimity, consistency, and Condorcet consistency is essential for grasping how the paper adapts these concepts to RLHF.
  - Quick check question: What is the difference between a voting rule that is unanimous and one that is strategyproof?

- Concept: Reinforcement Learning from Human Feedback (RLHF) framework
  - Why needed here: Understanding how RLHF works - particularly the preference modeling and reward function learning aspects - is crucial for understanding the paper's mapping.
  - Quick check question: How does the Bradley-Terry model relate to preference modeling in RLHF?

- Concept: Axiomatic analysis vs. distortion metrics
  - Why needed here: The paper discusses two different approaches from SCT for evaluating voting rules, and understanding both is key to following the analysis.
  - Quick check question: What is the key difference between evaluating a voting rule axiomatically versus using distortion metrics?

## Architecture Onboarding

- Component map: Problem setting analysis -> Preference modeling voting rule -> Axiom adaptation -> Evaluation perspectives
- Critical path: Problem setting analysis → Preference modeling voting rule formulation → Axiom adaptation → Evaluation perspective analysis
- Design tradeoffs:
  - Fixed vs. infinite alternatives: SCT assumes finite alternatives while RLHF has infinite alternatives
  - Full information vs. sampled feedback: SCT assumes full voter information while RLHF has partial evaluator feedback
  - Winner selection vs. reward function: SCT focuses on selecting a winner while RLHF aims to create a generalizable reward function
- Failure signatures:
  - Inability to meaningfully map RLHF preferences to SCT preference models
  - Adapted axioms that don't capture meaningful properties of RLHF systems
  - Separation of generalization and aggregation problems that doesn't reflect actual RLHF behavior
- First 3 experiments:
  1. Test the preference modeling voting rule formulation by implementing it on a simple RLHF dataset and comparing results to standard RLHF approaches
  2. Implement the adapted axioms (ε-unanimity, ε-Condorcet consistency, ε-consistency) and test them on RLHF-trained reward models
  3. Analyze the distortion perspective by creating a utility-based evaluation framework for RLHF reward models and comparing it to standard RLHF evaluation metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed axioms for preference modeling (unanimity, Condorcet consistency, and consistency) be mathematically formalized and proven to hold or fail for specific RLHF algorithms?
- Basis in paper: [explicit] The paper proposes new axioms for RLHF but does not prove their validity for existing algorithms.
- Why unresolved: The axioms are newly proposed and require rigorous mathematical analysis to determine their applicability and limitations in the RLHF context.
- What evidence would resolve it: Formal proofs or counterexamples demonstrating whether specific RLHF algorithms satisfy or violate the proposed axioms.

### Open Question 2
- Question: How can the distortion perspective from social choice theory be adapted to measure the suboptimality of RLHF reward models in practice?
- Basis in paper: [explicit] The paper discusses the concept of distortion in social choice and suggests applying a similar perspective to RLHF, but does not provide a concrete method for doing so.
- Why unresolved: The distortion metric requires a way to quantify the "hidden context" or true utilities of alternatives, which is challenging in the RLHF setting due to the infinite space of prompts and completions.
- What evidence would resolve it: A proposed method for defining and measuring distortion in RLHF, along with empirical studies comparing the performance of different RLHF algorithms using this metric.

### Open Question 3
- Question: What are the implications of evaluator disagreement and cognitive biases for the generalization and axiomatic properties of RLHF reward models?
- Basis in paper: [explicit] The paper highlights the challenges of evaluator disagreement and cognitive biases in RLHF and suggests that social choice theory can provide insights into these issues.
- Why unresolved: The impact of evaluator disagreement and cognitive biases on RLHF performance is not well understood, and there is a need for more research to quantify their effects and develop methods to mitigate them.
- What evidence would resolve it: Empirical studies measuring the impact of evaluator disagreement and cognitive biases on RLHF reward model performance, as well as the development of techniques to account for or reduce these effects.

## Limitations
- The infinite alternative space in RLHF creates significant computational challenges for checking axiom satisfaction
- Adapted axioms may be too permissive or too strict depending on ε-threshold values, potentially failing to capture meaningful evaluator disagreement
- The separation of generalization and aggregation problems may not reflect actual RLHF system behavior where these components are tightly coupled

## Confidence

- **High confidence**: The identification of key differences between SCT and RLHF problem settings is well-supported and clearly articulated.
- **Medium confidence**: The preference modeling voting rule formulation provides a reasonable theoretical bridge, but practical implementation challenges remain significant.
- **Low confidence**: The adapted axiom formulations, while mathematically coherent, require extensive empirical validation to demonstrate their utility in real RLHF systems.

## Next Checks
1. Implement the ε-unanimity and ε-Condorcet consistency axioms on a real RLHF dataset and measure violation rates across different reward model architectures
2. Design controlled experiments to test whether separating generalization and aggregation problems leads to better or worse alignment outcomes compared to integrated approaches
3. Conduct a distortion analysis comparing standard RLHF evaluation metrics with utility-based metrics derived from the adapted SCT framework