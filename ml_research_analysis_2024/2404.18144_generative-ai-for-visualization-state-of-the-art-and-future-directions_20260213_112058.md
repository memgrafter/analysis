---
ver: rpa2
title: 'Generative AI for Visualization: State of the Art and Future Directions'
arxiv_id: '2404.18144'
source_url: https://arxiv.org/abs/2404.18144
tags:
- data
- visualization
- generation
- visual
- genai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides a comprehensive overview of Generative AI
  (GenAI) applications in data visualization, categorizing 81 papers across four stages:
  data enhancement, visual mapping generation, stylization, and interaction. The study
  identifies key challenges and research opportunities, emphasizing the need for improved
  evaluation metrics, diverse datasets, and better integration of GenAI methods with
  traditional visualization pipelines.'
---

# Generative AI for Visualization: State of the Art and Future Directions

## Quick Facts
- arXiv ID: 2404.18144
- Source URL: https://arxiv.org/abs/2404.18144
- Reference count: 40
- This survey provides a comprehensive overview of Generative AI applications in data visualization, categorizing 81 papers across four stages: data enhancement, visual mapping generation, stylization, and interaction.

## Executive Summary
This survey comprehensively examines the state of Generative AI (GenAI) applications in data visualization, organizing 81 research papers into four pipeline stages: data enhancement, visual mapping generation, stylization, and interaction. The authors identify key challenges including evaluation metric limitations, dataset diversity constraints, and the gap between end-to-end GenAI approaches and traditional rule-based visualization methods. The paper highlights promising future directions such as developing more robust evaluation frameworks, creating diverse heterogeneous datasets, and exploring new paradigms for human-AI collaboration in visualization tasks.

## Method Summary
The authors conducted a comprehensive survey of GenAI applications in visualization by collecting 81 research papers through search-based and reference-driven methods. Papers were categorized according to four visualization pipeline stages and analyzed based on the GenAI methods employed (sequence, tabular, spatial, graph generation). The survey synthesizes algorithms used across different visualization tasks, discusses current limitations, and identifies future research opportunities. The methodology focuses on understanding how GenAI transforms traditional visualization pipelines and where these approaches show the most promise or face significant challenges.

## Key Results
- GenAI enables end-to-end visualization generation by learning implicit mapping patterns from real-world data
- Vision-language models can bridge the gap between data and visualization through multi-modal representations
- Reinforcement learning frameworks enable personalized and adaptive visualization through interactive feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative AI enables end-to-end visualization generation by learning implicit mapping patterns from real-world data.
- Mechanism: GenAI models, particularly large language models and diffusion models, can learn complex relationships between data structures and visual representations without explicit rule-based programming. By training on diverse visualization datasets, these models capture design principles and user intent patterns.
- Core assumption: Real-world visualization data contains sufficient variety and quality to teach the model effective design patterns.
- Evidence anchors:
  - [abstract] "The advent of GenAI introduces a paradigm shift, promising not only increased efficiency but also a more intuitive and accessible approach to visualization"
  - [section 4.1.1] "Some studies formulate the problem of visualization code generation as sequence-to-sequence generation"
- Break Condition: If training data lacks diversity or contains systematic biases, the model will fail to generalize to real-world visualization tasks.

### Mechanism 2
- Claim: GenAI can bridge the gap between data and visualization by learning multi-modal representations.
- Mechanism: Vision-language models can process both data tables and visual elements simultaneously, enabling more accurate chart question answering and interactive visualization systems.
- Core assumption: Multi-modal models can effectively fuse information from different data types to produce coherent visualizations.
- Evidence anchors:
  - [section 6.2.1] "Some recent studies simplifies the chart question answering pipeline with unified vision-language model"
  - [section 7.3] "LIDA makes an early effort towards a more integrated GenAI4VIS pipeline"
- Break Condition: If the model cannot properly align different modalities, it will produce inaccurate or irrelevant visualizations.

### Mechanism 3
- Claim: GenAI enables personalized and adaptive visualization through reinforcement learning and interactive feedback.
- Mechanism: Reinforcement learning frameworks can optimize visualization layouts and styles based on user preferences and interaction patterns.
- Core assumption: User feedback can be effectively translated into reward signals for the learning algorithm.
- Evidence anchors:
  - [section 5.4.1] "Tang et al. proposed a reinforcement learning framework and introduced an authoring tool, PlotThread"
  - [section 7.1] "evaluating the robustness and consistency of AI-generated visualizations across different scenarios is a key metric"
- Break Condition: If the reward function doesn't capture user preferences accurately, the system will produce suboptimal results.

## Foundational Learning

- Concept: Data structures in visualization (tabular, spatial, graph, sequence)
  - Why needed here: Different GenAI methods are optimized for different data structures, and understanding these relationships is crucial for effective application
  - Quick check question: What type of GenAI method would be most appropriate for generating chemical molecule structures?

- Concept: Evaluation metrics for AI-generated visualizations
  - Why needed here: Traditional visualization evaluation metrics may not capture the unique challenges of AI-generated content
  - Quick check question: What additional metrics beyond visual similarity should be considered when evaluating AI-generated charts?

- Concept: Visualization pipeline stages (data enhancement, visual mapping, stylization, interaction)
  - Why needed here: Understanding where GenAI can be applied in the visualization process helps identify appropriate use cases
  - Quick check question: Which stage of the visualization pipeline would benefit most from end-to-end GenAI approaches?

## Architecture Onboarding

- Component map:
  - Data preprocessing → GenAI model selection → Training/Inference → Post-processing → Evaluation
  - Key components: Data loaders, model architectures, training pipelines, evaluation metrics, visualization rendering

- Critical path:
  - Data preparation → Model training → Validation → Visualization generation → User interaction
  - Focus on data quality and model performance at each stage

- Design tradeoffs:
  - End-to-end vs. hybrid approaches: End-to-end offers simplicity but may lack precision; hybrid approaches offer more control but increased complexity
  - Model size vs. performance: Larger models generally perform better but require more resources
  - Training data diversity vs. task specificity: More diverse data improves generalization but may reduce task-specific performance

- Failure signatures:
  - Poor data quality leading to biased or inaccurate visualizations
  - Overfitting to training data resulting in poor generalization
  - Model hallucination producing unrealistic or misleading visualizations
  - Inconsistency between different components of the visualization pipeline

- First 3 experiments:
  1. Data augmentation experiment: Test different data augmentation techniques to improve model robustness
  2. Model architecture comparison: Compare different GenAI architectures (e.g., transformer vs. diffusion) for a specific visualization task
  3. User study: Evaluate the effectiveness of AI-generated visualizations compared to traditional approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more robust evaluation frameworks that go beyond pixel-wise metrics to assess the quality of data restoration in data embedding tasks for visualization?
- Basis in paper: [explicit] The paper mentions that current evaluation of data restoration quality is limited to pixel-wise metrics like RMSE, which cannot fully reflect the accuracy of data restoration due to the absence of original data in the evaluation.
- Why unresolved: The trade-off between embedding capacity and image quality poses challenges in developing more comprehensive evaluation metrics that can accurately assess data restoration precision.
- What evidence would resolve it: Development and validation of new evaluation metrics that consider practical scenarios, precision requirements, and the unique characteristics of visualization data restoration tasks.

### Open Question 2
- Question: How can we improve the interpretability and disentanglement of dimensions extracted by Disentangled Representation Learning (DRL) methods for data inference in visualization tasks?
- Basis in paper: [explicit] The paper states that DRL methods, while useful for extracting interpretable dimensions, cannot guarantee perfect interpretability and disentanglement, limiting the customizable data exploration for users.
- Why unresolved: The choice of β in β-VAE remains largely heuristic, and even when dimensions are meaningful, they may not align with users' intended exploration, restricting the effectiveness of DRL-based data inference.
- What evidence would resolve it: Development of more sophisticated DRL architectures or training strategies that can automatically learn interpretable and disentangled dimensions aligned with users' exploration goals, validated through user studies.

### Open Question 3
- Question: How can we better integrate specific domain knowledge related to visualization and data analysis into large language models (LLMs) for improved performance in natural language to visualization (NL2VIS) tasks?
- Basis in paper: [explicit] The paper discusses the limitations of current LLM-based NL2VIS solutions, which often do not incorporate specific domain knowledge, resulting in suboptimal performance and lack of semantic correctness in generated visualizations.
- Why unresolved: Integrating domain-specific knowledge into LLMs requires careful consideration of visualization principles, data analysis requirements, and the development of specialized training datasets or expert systems to guide the models.
- What evidence would resolve it: Demonstration of improved NL2VIS performance using LLMs enhanced with domain-specific knowledge, validated through user studies comparing the quality, accuracy, and interpretability of generated visualizations against traditional methods.

## Limitations

- Paper collection methodology relies on search-based and reference-driven approaches with unclear specific search criteria, potentially introducing selection bias
- Categorization of papers into four pipeline stages may oversimplify complex multi-stage approaches and edge cases in classification methodology are not fully addressed
- Analysis of challenges and future directions is based on reasonable interpretations but lacks empirical validation and specific quantitative support

## Confidence

- **High confidence**: The categorization framework for visualization pipeline stages is well-established and clearly explained
- **Medium confidence**: The analysis of current challenges and future directions is based on reasonable interpretations of the surveyed literature, though specific quantitative support is limited
- **Medium confidence**: The identification of key limitations (evaluation metrics, dataset diversity, integration challenges) aligns with general observations in the field but lacks empirical validation

## Next Checks

1. **Methodological validation**: Replicate the paper collection process using identical search criteria and compare the resulting corpus composition to verify selection bias is minimal
2. **Categorization consistency**: Have multiple independent reviewers categorize a sample of papers into the four pipeline stages and calculate inter-rater reliability to assess classification consistency
3. **Evaluation framework assessment**: Conduct a systematic review of existing evaluation metrics for AI-generated visualizations to identify gaps and validate the survey's claims about metric limitations