---
ver: rpa2
title: Dissecting Fine-Tuning Unlearning in Large Language Models
arxiv_id: '2410.06606'
source_url: https://arxiv.org/abs/2410.06606
tags:
- knowledge
- unlearning
- methods
- language
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why fine-tuning-based unlearning methods
  in large language models appear effective despite not truly erasing targeted knowledge.
  Through activation patching and parameter restoration experiments on LLaMA2-7B-chat
  and OLMo-7B, the authors find that these methods do not modify the knowledge stored
  in MLP value vectors, but rather alter knowledge retrieval mechanisms by adjusting
  MLP coefficients and attention components.
---

# Dissecting Fine-Tuning Unlearning in Large Language Models

## Quick Facts
- arXiv ID: 2410.06606
- Source URL: https://arxiv.org/abs/2410.06606
- Reference count: 12
- Key outcome: Fine-tuning-based unlearning methods in LLMs primarily adjust knowledge retrieval mechanisms rather than erasing knowledge, making them vulnerable to recovery attacks and causing collateral damage to unrelated capabilities.

## Executive Summary
This paper investigates why fine-tuning-based unlearning methods in large language models appear effective despite not truly erasing targeted knowledge. Through systematic experiments on LLaMA2-7B-chat and OLMo-7B models, the authors demonstrate that these methods do not modify knowledge stored in MLP value vectors but rather alter knowledge retrieval mechanisms by adjusting MLP coefficients and attention components. The research reveals that MLP coefficients in the final layers are primarily responsible for the observed unlearning effects, and that current fine-tuning approaches inevitably impact unrelated knowledge while attempting to unlearn target concepts. The findings show these methods are vulnerable to recovery attacks and fundamentally unsuitable for true unlearning, as they adjust how models access knowledge rather than erasing it.

## Method Summary
The authors employ activation patching and parameter restoration experiments to analyze how fine-tuning-based unlearning methods affect different model components. They systematically test knowledge retrieval by modifying specific components and measuring changes in model behavior, comparing the effectiveness of unlearning when different parts of the model are altered or restored. The experiments use global behavior tests to evaluate collateral damage to unrelated knowledge and capabilities, providing a comprehensive assessment of the trade-offs inherent in current unlearning approaches.

## Key Results
- Fine-tuning-based unlearning methods do not erase knowledge stored in MLP value vectors but alter knowledge retrieval mechanisms
- MLP coefficients in final layers are primarily responsible for unlearning effects
- Current methods cause inevitable collateral damage to unrelated knowledge while unlearning target concepts
- These approaches are vulnerable to recovery attacks and fundamentally unsuitable for true unlearning

## Why This Works (Mechanism)
Fine-tuning-based unlearning methods work by adjusting the weights and coefficients that control how models access stored knowledge rather than modifying the knowledge itself. When a model is fine-tuned to "unlearn" specific information, the training process typically adjusts the parameters that govern knowledge retrieval - particularly MLP coefficients and attention mechanisms - while leaving the actual stored knowledge in MLP value vectors largely untouched. This creates the appearance of unlearning because the model becomes less capable of retrieving the targeted information, even though the information remains encoded in the model's weights. The final layers of the model, which handle the most abstract representations and knowledge integration, are especially sensitive to these adjustments, making them critical for the observed unlearning effects.

## Foundational Learning
- **Knowledge representation in LLMs**: Understanding how information is encoded and stored in transformer models is crucial for analyzing unlearning effectiveness. Quick check: Can you explain the difference between knowledge storage and knowledge retrieval mechanisms?
- **Activation patching methodology**: This technique allows researchers to isolate the contribution of specific components by replacing activations and measuring performance changes. Quick check: How does activation patching help identify which components are critical for specific behaviors?
- **Parameter restoration techniques**: Understanding how to systematically restore parameters helps reveal which modifications are responsible for observed effects. Quick check: What does it mean when restoring a parameter doesn't recover the original behavior?

## Architecture Onboarding
- **Component map**: Input -> Embedding -> Attention layers -> MLP layers -> Output
- **Critical path**: The knowledge retrieval path involves attention mechanisms that identify relevant information followed by MLP layers that process and integrate this information, with final layers having the most significant impact on abstract knowledge representation.
- **Design tradeoffs**: Fine-tuning methods must balance between effectively suppressing target knowledge and preserving general capabilities, but current approaches fail to achieve this balance due to their fundamental mechanism of adjusting retrieval rather than erasing knowledge.
- **Failure signatures**: The primary failure mode is the appearance of unlearning without actual knowledge erasure, evidenced by the ability to recover target knowledge through parameter restoration or activation manipulation.
- **First experiments**: 1) Test activation patching on MLP value vectors vs coefficients, 2) Systematically restore parameters layer by layer to identify critical components, 3) Evaluate global behavior changes when targeting specific knowledge

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions for future research.

## Limitations
- Experiments limited to 7B parameter models, limiting generalizability to larger architectures
- Fixed perturbation threshold (Â±0.3 standard deviations) may not capture optimal knowledge retrieval dynamics across different components
- Evaluation focuses on single-turn knowledge retrieval, leaving uncertainty about multi-turn interaction scenarios
- Global behavior tests use relatively small evaluation sets that may not comprehensively characterize capability trade-offs

## Confidence
- **High confidence**: The observation that fine-tuning-based unlearning primarily adjusts knowledge retrieval rather than erasing knowledge
- **Medium confidence**: The specific claim that MLP coefficients in final layers are primarily responsible for unlearning effects
- **Medium confidence**: The assertion that current methods are vulnerable to recovery attacks, based on the experimental setup used

## Next Checks
1. Replicate activation patching experiments across multiple perturbation thresholds and larger model families (e.g., 13B-70B parameter models) to verify the robustness of MLP coefficient findings
2. Conduct ablation studies systematically removing or restoring individual MLP layers to isolate which components are truly critical for knowledge retrieval
3. Design controlled experiments testing recovery attack resistance with different fine-tuning hyperparameters and regularization techniques to map the vulnerability landscape