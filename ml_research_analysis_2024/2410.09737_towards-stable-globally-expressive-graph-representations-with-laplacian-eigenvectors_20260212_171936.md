---
ver: rpa2
title: Towards Stable, Globally Expressive Graph Representations with Laplacian Eigenvectors
arxiv_id: '2410.09737'
source_url: https://arxiv.org/abs/2410.09737
tags:
- graph
- concat
- neural
- eigenvectors
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the stability and expressive power limitations
  of graph neural networks (GNNs) that use Laplacian eigenvectors as node features.
  While Laplacian eigenvectors contain valuable global positional information, previous
  approaches either lose expressivity or suffer from instability due to hard-splitting
  of eigenspaces.
---

# Towards Stable, Globally Expressive Graph Representations with Laplacian Eigenvectors

## Quick Facts
- arXiv ID: 2410.09737
- Source URL: https://arxiv.org/abs/2410.09737
- Reference count: 40
- Key outcome: Introduces OGE-Aug method combining learnable orthogonal group invariant representations with smooth eigenvalue filtering to achieve both stability and expressive power in graph representation learning

## Executive Summary
This paper addresses the fundamental tension between stability and expressive power in graph neural networks that use Laplacian eigenvectors as node features. While Laplacian eigenvectors contain valuable global positional information, previous approaches suffer from instability due to hard-splitting of eigenspaces or lose expressivity through over-simplification. The authors propose OGE-Aug, a novel framework that uses learnable orthogonal group invariant representations for each Laplacian eigenspace combined with a smooth filtering approach to handle numerically close eigenvalues. Theoretical analysis proves the stability of OGE-Aug, and extensive experiments demonstrate state-of-the-art or highly competitive performance across multiple graph benchmarks.

## Method Summary
OGE-Aug processes graphs by first computing their Laplacian eigenvectors, then applying a learnable orthogonal group (O(p))-invariant encoder to each eigenspace. Unlike previous approaches that hard-split eigenspaces or use different encoders for different dimensions, OGE-Aug uses a single universal encoder with weighted contributions based on eigenspace dimensions. The method handles numerically close eigenvalues through a smooth filtering function ρ that prevents drastic changes in output due to small perturbations. This design ensures both stability (proven through Lipschitz continuity analysis) and expressive power (demonstrated through competitive performance on molecular property prediction and protein contact prediction tasks).

## Key Results
- Achieves state-of-the-art performance on QM9 molecular property prediction with 0.0046 MAE for alpha-related property
- Outperforms previous spectral methods on ZINC12k with 0.0407 MAE for penalized logP
- Shows significant improvements on PCQM-Contact protein contact prediction with MRR of 0.8598

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Smooth eigenvalue filtering prevents hard splits that cause instability
- Mechanism: Uses a continuous smoothing function ρ(|λi - λj|) instead of hard partitioning eigenvalues into separate eigenspaces
- Core assumption: Small perturbations in eigenvalues should not cause drastic changes in network output
- Evidence anchors:
  - [abstract] "our method deals with numerically close eigenvalues in a smooth fashion, ensuring its better robustness against perturbations"
  - [section 4] "hard split of Laplacian eigenvalues according to their numerical identity, which suffers from great instability when the graph structure is perturbed"
  - [corpus] Weak - corpus neighbors don't directly address this smoothing mechanism
- Break condition: If ρ becomes too flat, all eigenspaces become indistinguishable, losing expressive power

### Mechanism 2
- Claim: Single O(n)-invariant encoder with weighted sum maintains stability while preserving expressivity
- Mechanism: Instead of using different encoders for different eigenspace dimensions, uses one universal encoder f and weights contributions by eigenspace dimensions
- Core assumption: The weighted sum is insensitive to exact eigenspace splitting
- Evidence anchors:
  - [abstract] "our method utilizes learnable O(p)-invariant representations for each Laplacian eigenspace"
  - [section 4] "the dependency on eigenspace dimensions µj appears only in the form of a weighted sum, which is insensitive to the exact splitting of Laplacian eigenspaces"
  - [corpus] Weak - corpus neighbors focus on spectral features but don't discuss this weighted sum approach
- Break condition: If the weighted sum becomes dominated by a single eigenspace, others may be effectively ignored

### Mechanism 3
- Claim: Cartesian tensor representations provide universal expressivity within stability constraints
- Mechanism: Uses second-order Cartesian tensors as the O(n)-invariant encoder instead of requiring full universality
- Core assumption: Second-order tensors provide sufficient expressivity for practical graph learning tasks
- Evidence anchors:
  - [section 4] "we adopt as f a Cartesian tensor based point cloud network with Cartesian tensors up to the second order used"
  - [section 5] "there have been a few works establishing theoretically the universality of some of the aforementioned architectures"
  - [corpus] Weak - corpus neighbors don't discuss Cartesian tensor approaches
- Break condition: If second-order tensors prove insufficient for the task, may need higher-order representations at computational cost

## Foundational Learning

- Concept: Graph Laplacian eigendecomposition and eigenspaces
  - Why needed here: The method fundamentally operates on Laplacian eigenvectors and their properties
  - Quick check question: What is the relationship between graph Laplacian eigenvalues and graph connectivity?

- Concept: Orthogonal group invariance and equivariance
  - Why needed here: The method requires processing eigenvectors in a way that's invariant to orthogonal transformations within eigenspaces
  - Quick check question: How does O(p) invariance differ from permutation invariance in graph learning?

- Concept: Universal graph representation and expressiveness
  - Why needed here: The theoretical foundation relies on understanding when a graph representation is "universal"
  - Quick check question: What does it mean for a graph representation to be universal, and why is this important?

## Architecture Onboarding

- Component map: Input graph -> Laplacian computation -> OGE-Aug processing -> feature augmentation -> base model -> output
- Critical path: Input graph → Laplacian computation → OGE-Aug processing → feature augmentation → base model → output
- Design tradeoffs: Full universality (n exp(O(n)) complexity) vs. practical second-order tensors (manageable complexity)
- Failure signatures: Poor performance on graphs with similar spectra but different structures; instability on perturbed graphs
- First 3 experiments:
  1. Test on QM9 with varying δ values to see stability-accuracy tradeoff
  2. Compare with BasisNet and SignNet on ZINC12k for baseline performance
  3. Evaluate on PCQM-Contact to verify long-range interaction learning capability

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but based on the analysis, several important questions remain:

## Limitations
- The trade-off between smoothing parameter δ and model performance is not fully characterized
- The method's performance on dynamic graphs with frequent structural changes remains unexplored
- Complexity of O(n)-invariant encoders for larger eigenspaces is noted as problematic but lacks concrete solutions

## Confidence
- **High**: Stability claims under Laplacian perturbations (supported by theoretical analysis)
- **Medium**: Expressiveness improvements (based on competitive experimental results)
- **Medium**: Generalizability across different graph types (demonstrated on molecular and contact prediction tasks)

## Next Checks
1. Conduct ablation studies varying the smoothing parameter δ systematically to map the stability-accuracy tradeoff curve
2. Test the method on dynamic graph datasets with frequent structural changes to evaluate robustness in practical scenarios
3. Extend experiments to include higher-order Cartesian tensors to verify if second-order representations are indeed sufficient for all tasks