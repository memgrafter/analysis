---
ver: rpa2
title: Indiscriminate Disruption of Conditional Inference on Multivariate Gaussians
arxiv_id: '2411.14351'
source_url: https://arxiv.org/abs/2411.14351
tags:
- attacker
- problem
- attacks
- decisionmaker
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how an attacker can disrupt conditional inference
  on multivariate Gaussian (MVG) models by corrupting observed data. The attacker
  aims to maximally disrupt the decisionmaker's conditional distribution while ensuring
  plausibility of the corrupted data.
---

# Indiscriminate Disruption of Conditional Inference on Multivariate Gaussians

## Quick Facts
- arXiv ID: 2411.14351
- Source URL: https://arxiv.org/abs/2411.14351
- Reference count: 5
- Key outcome: Attacker can significantly disrupt conditional inference on multivariate Gaussian models by corrupting observed data, with attacks increasing KL divergence from 0 to 255.3 in real estate valuation examples

## Executive Summary
This paper demonstrates how attackers can disrupt conditional inference on multivariate Gaussian (MVG) models by corrupting observed data. The attacker aims to maximize KL divergence between true and corrupted conditional distributions while maintaining data plausibility. Two attack settings are analyzed: white-box (attacker knows MVG parameters) and grey-box (attacker has Bayesian prior over parameters). The white-box attack reduces to a quadratic program whose convexity properties determine solution methods, while the grey-box attack uses stochastic programming techniques like Sample Average Approximation and Stochastic Gradient Ascent.

## Method Summary
The white-box attack formulates a quadratic program that maximizes a weighted sum of KL divergence between true and corrupted conditionals plus a plausibility penalty. Convexity analysis based on eigenvalue structure determines whether to use convex QP methods or specialized non-convex algorithms. The grey-box attack uses Sample Average Approximation to approximate expectations over parameter distributions or employs Stochastic Gradient Ascent methods (AdaGrad, RMSProp, Adam) to iteratively update perturbations. Three applications demonstrate effectiveness: real estate valuation (ZHVI), loan interest rate estimation, and 2D object tracking.

## Key Results
- White-box attacks achieve significant disruption (KL divergence increases from 0 to 255.3 in ZHVI example)
- Grey-box attacks are slightly more aggressive than white-box due to parameter uncertainty
- Computational times remain reasonable (under 10 seconds for complex grey-box instances)
- Eigenvalue analysis provides structural characterization of when objective is convex or concave
- SAA and SGA methods successfully solve high-dimensional grey-box problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attack exploits the closed-form conditional distribution of multivariate Gaussians to craft adversarial perturbations.
- Mechanism: By corrupting observed evidence z, the attacker maximizes KL divergence between true and corrupted conditionals using quadratic objective functions that are either convex, concave, or neither depending on eigenvalue structure.
- Core assumption: The attacker knows the MVG parameters (white-box) or has a Bayesian prior (grey-box), enabling computation of the conditional distribution's sensitivity to perturbations.
- Evidence anchors:
  - [abstract] "reduces to a quadratic program that maximizes a weighted sum of the KL divergence between true and corrupted conditionals and a plausibility penalty"
  - [section 2.3] "Maximizing disruption DKL(PY|z′ ||PY|z) is equivalent to maximizing ϕ1(z) = zT Qz + vT z"
- Break condition: If the attacker's knowledge is severely incomplete or the prior is misspecified, the grey-box attacks lose efficacy.

### Mechanism 2
- Claim: Convexity/concavity structure of the objective function determines solvability and solution method.
- Mechanism: Eigenvalue analysis of Q and Σ⁻¹[ZZ] characterizes when the objective is convex (solvable via QP) or non-convex (requires specialized methods).
- Core assumption: The attacker can compute eigenvalues of precision and covariance matrices to determine convexity.
- Evidence anchors:
  - [section 3.1] "Problem WB's objective function is concave if for every i∈ {1, · · · , |Z|}, there exists m, n ≥ 1 such that i = m + n − 1, 1 ≤ m + n − 1 ≤ |Z|, and w1ρm − w2ζn ≤ 0"
  - [section 3.1] "Problem WB's objective function is convex if for every i ∈ {1, · · ·, |Z|}, there exists m, n ≥ 1 such that i = m + n − |Z|, 1 ≤ m + n − |Z| ≤ | Z|, and 0 ≤ w1ρm − w2ζn"
- Break condition: When neither convex nor concave, global optimality requires expensive non-convex optimization.

### Mechanism 3
- Claim: Sample Average Approximation (SAA) and Stochastic Gradient Ascent (SGA) enable tractable grey-box attacks.
- Mechanism: SAA samples parameter distributions to approximate expectations, while SGA iteratively updates perturbations using gradient estimates from sampled parameters.
- Core assumption: The attacker can sample from the prior distribution over unknown parameters and compute conditional KL divergence for each sample.
- Evidence anchors:
  - [section 4.2.1] "the average of these randomly sampled functions is then utilized to estimate E[w1ϕ1(z) + w2ϕ2(z)]"
  - [section 4.2.2] "∇[w1ϕ1(z) + w2ϕ2(z)] = 2[w1Q − w2Σ⁻¹[ZZ]]z + w1v + 2w2Σ⁻¹[ZZ]µ[Z]"
- Break condition: High-dimensional parameter spaces or expensive sampling procedures make grey-box attacks computationally prohibitive.

## Foundational Learning

- Concept: Multivariate Gaussian conditional distributions
  - Why needed here: The entire attack framework relies on the closed-form conditional distribution PY|z = N(µY|z, ΣY|z) for computing KL divergence
  - Quick check question: Given PY,Z ~ N(µ, Σ), write the conditional mean µY|z in terms of the precision matrix Λ and observed evidence z

- Concept: Convex vs. non-convex optimization
  - Why needed here: The attacker must choose between efficient QP methods (convex) or expensive global optimization (non-convex) based on objective structure
  - Quick check question: Given Q and Σ⁻¹[ZZ] with eigenvalues ρ and ζ, under what condition is w1Q - w2Σ⁻¹[ZZ] negative semidefinite?

- Concept: Stochastic programming and sampling methods
  - Why needed here: Grey-box attacks require approximating expectations over unknown parameters using SAA or SGA methods
  - Quick check question: What is the key difference between Sample Average Approximation and Stochastic Gradient Ascent approaches?

## Architecture Onboarding

- Component map:
  - Parameter extraction: Compute µ, Σ, Λ from MVG model
  - Convexity analysis: Eigenvalue decomposition of Q and Σ⁻¹[ZZ]
  - Attack formulation: Build objective w1ϕ1(z) + w2ϕ2(z) with constraints
  - Solution method selection: Choose QP, specialized algorithms, or heuristics based on convexity
  - SAA/SGA pipeline: Sample parameters, compute gradients/objectives, update perturbations

- Critical path: Parameter extraction → Convexity analysis → Solution method selection → Attack computation
- Design tradeoffs: White-box attacks are more effective but require complete parameter knowledge; grey-box attacks trade effectiveness for operational security
- Failure signatures: Poor convergence in SGA, unexpected objective values, attacks that don't push z to feasible boundaries
- First 3 experiments:
  1. Verify convexity conditions on synthetic MVGs with known eigenvalues
  2. Compare white-box attack effectiveness vs random noise baseline
  3. Test SAA convergence with varying sample sizes J on grey-box problem

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but implies several areas for future research:

- How to design robust MVG-based inference systems that are resistant to such attacks
- Whether alternative statistical distances beyond KL divergence provide different attack characteristics
- How to extend these attack methods to non-Gaussian models and other probabilistic inference tasks

## Limitations

- Computational scalability: Grey-box attacks require sampling from parameter distributions, becoming expensive as dimensionality increases
- Perfect model assumption: White-box attacks assume exact MVG model specification, which rarely holds in practice
- Hyperparameter sensitivity: Stochastic optimization methods require careful tuning of learning rates and decay parameters

## Confidence

High confidence in white-box attack formulation and theoretical foundations. Medium confidence in grey-box methods due to computational complexity considerations in high-dimensional settings. Low confidence in scalability claims without extensive validation on larger problems.

## Next Checks

1. Verify convexity conditions empirically across a range of MVG models with varying eigenvalue structures to confirm the theoretical characterization holds in practice.
2. Conduct scalability experiments measuring computation time and attack effectiveness as the number of observed variables increases beyond the tested examples.
3. Test attack robustness when the assumed MVG model is misspecified, examining how parameter uncertainty affects both white-box and grey-box attack performance.