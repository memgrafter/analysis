---
ver: rpa2
title: Understanding Reasoning Ability of Language Models From the Perspective of
  Reasoning Paths Aggregation
arxiv_id: '2402.03268'
source_url: https://arxiv.org/abs/2402.03268
tags:
- reasoning
- paths
- random
- walk
- path
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how pre-trained language models (LMs) acquire
  reasoning abilities through next-token prediction pre-training. The authors propose
  that LMs learn to reason by aggregating indirect reasoning paths seen during pre-training,
  formalized as random walk paths on knowledge/reasoning graphs.
---

# Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation

## Quick Facts
- arXiv ID: 2402.03268
- Source URL: https://arxiv.org/abs/2402.03268
- Authors: Xinyi Wang; Alfonso Amayuelas; Kexun Zhang; Liangming Pan; Wenhu Chen; William Yang Wang
- Reference count: 33
- Key outcome: Language models learn to reason by aggregating indirect reasoning paths seen during pre-training, formalized as random walk paths on knowledge/reasoning graphs.

## Executive Summary
This paper investigates how pre-trained language models acquire reasoning abilities through next-token prediction pre-training. The authors propose that LMs learn to reason by aggregating indirect reasoning paths seen during pre-training, formalized as random walk paths on knowledge/reasoning graphs. They demonstrate this hypothesis through experiments on logical reasoning over knowledge graphs and chain-of-thought reasoning for math word problems, showing that LMs can effectively utilize random walk paths for reasoning and that there is an optimal random walk path length for training.

## Method Summary
The authors propose a framework where language models acquire reasoning abilities by aggregating random walk paths seen during pre-training. For logical reasoning, they pre-train LMs from scratch on random walk paths generated from knowledge graphs, analyzing the KL divergence between LM output distributions and weighted/unweighted sums of random walk path probabilities. For chain-of-thought reasoning, they continue training pre-trained LMs on random walk reasoning paths constructed from existing CoT data. They evaluate performance across multiple datasets including GSM8K, AQUA, and SV AMP for math reasoning tasks.

## Key Results
- LMs learn to assign weights to logical rules rather than using simple unweighted aggregation
- There is an optimal random walk path length for training LMs that varies by dataset
- Random walk path training consistently improves reasoning performance compared to vanilla supervised fine-tuning
- LMs can efficiently utilize unlabeled reasoning paths for reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Language models learn to reason by aggregating indirect reasoning paths seen during pre-training, formalized as random walk paths on knowledge/reasoning graphs.
- **Mechanism:** The model treats pre-training data as samples from random walks on a reasoning graph. During inference, it can "jump" between concepts by aggregating probabilities of relevant random walk paths, effectively performing reasoning without explicit fine-tuning.
- **Core assumption:** The pre-training corpus contains sufficient reasoning paths that connect related concepts, and the model can learn to weight these paths appropriately.
- **Evidence anchors:**
  - [abstract] "We propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time."
  - [section 2.3] "We hypothesize that, at inference time, this enables an LM to jump from one concept to another during its reasoning process"
  - [corpus] Weak evidence - the corpus neighbors are mostly unrelated to the paper's core topic of reasoning paths aggregation.
- **Break condition:** If the pre-training data lacks sufficient reasoning paths between related concepts, or if the model cannot learn appropriate weights for different paths, this mechanism would fail.

### Mechanism 2
- **Claim:** The KL divergence between LM output distributions and weighted/unweighted sums of random walk path probabilities is a reasonable way to explain how LMs reason.
- **Mechanism:** By comparing the LM's predicted distributions to various aggregations of random walk probabilities, we can understand how well the LM has learned to assign weights to logical rules and reason over knowledge graphs.
- **Core assumption:** The random walk aggregation algorithms (weighted and unweighted) provide reasonable baselines for comparison with LM reasoning.
- **Evidence anchors:**
  - [section 2.4] "To better understand the similarity between LM and the random walk aggregation algorithm... we propose to compute and analyze the KL divergence between them"
  - [section 2.4] "The KL divergence reflects how LM assigns probabilities to possible logical rules based on the given prompt"
  - [corpus] Weak evidence - corpus neighbors do not directly address KL divergence analysis.
- **Break condition:** If the random walk aggregation algorithms are poor representations of reasoning, or if KL divergence is not a meaningful measure of similarity between distributions, this mechanism would break.

### Mechanism 3
- **Claim:** There is an optimal random walk path length for training LMs, which is associated with the intrinsic reasoning length of different datasets.
- **Mechanism:** Too short random walks lack sufficient reasoning steps, while too long random walks introduce noise. An intermediate path length balances these effects, leading to optimal reasoning performance.
- **Core assumption:** The intrinsic reasoning length of a dataset can be estimated from the average length of reasoning chains in that dataset.
- **Evidence anchors:**
  - [section 2.5] "there is usually an optimal random walk path length for training LMs"
  - [section 3.3] "we observe that each dataset has a performance peak at a certain random walk length"
  - [section 3.3] "The average length of CoTs in AQUA, GSM8, and SV AMP training sets are 4.79, 3.72, and 1.36, respectively"
  - [corpus] No direct evidence in corpus neighbors.
- **Break condition:** If the relationship between path length and reasoning performance is not consistent across datasets, or if other factors dominate the effect of path length, this mechanism would break.

## Foundational Learning

- **Concept: Random walk on graphs**
  - Why needed here: The paper formalizes reasoning paths as random walks on knowledge/reasoning graphs, so understanding random walks is fundamental to grasping the main idea.
  - Quick check question: In a random walk on a graph, how do you compute the probability of reaching a particular node from a starting node?

- **Concept: KL divergence between probability distributions**
  - Why needed here: The paper uses KL divergence to compare LM output distributions with aggregations of random walk probabilities, so understanding KL divergence is crucial for interpreting the results.
  - Quick check question: What does it mean if the KL divergence between two distributions is small? What about if it's large?

- **Concept: Knowledge graphs and logical rules**
  - Why needed here: The paper analyzes logical reasoning over knowledge graphs, where entities are connected by relationships that can be inferred using logical rules. Understanding knowledge graphs and logical rules is necessary for grasping the KG reasoning experiments.
  - Quick check question: In a knowledge graph, how would you represent the fact "Paris is the capital of France"? How might you infer this fact from other triples in the graph?

## Architecture Onboarding

- **Component map:** Knowledge graph construction -> Random walk generation -> LM pre-training -> KL divergence computation -> Random walk reasoning path construction -> LM fine-tuning

- **Critical path:** 1. Construct knowledge graph from data 2. Generate random walk paths from KG 3. Pre-train LM on random walk paths 4. Format test triples as prompts 5. Compute KL divergence between LM and aggregation distributions

- **Design tradeoffs:**
  - Random walk path length: Longer paths may capture more reasoning but introduce more noise
  - Number of clusters for latent reasoning graph: More clusters may better capture distinct reasoning patterns but increase computational cost
  - Weighting scheme for logical rules: More complex weighting may improve performance but increase computational cost and risk overfitting

- **Failure signatures:**
  - KL divergence between LM and aggregation distributions remains high across all path lengths
  - LM performance does not improve with longer random walk paths beyond a certain point
  - Clustering of reasoning states results in many small or empty clusters

- **First 3 experiments:**
  1. Pre-train LM on random walk paths of length 1 (individual triples) vs length 5 from a small knowledge graph, compare KL divergence and accuracy
  2. Vary the maximum path length for computing aggregation distributions (1-10) and plot KL divergence vs path length
  3. Construct latent reasoning graph from CoT data, generate random walk reasoning paths, and fine-tune LM, compare performance to supervised fine-tuning baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal random walk path length for training language models on knowledge graphs, and how does it vary across different KG datasets?
- Basis in paper: [explicit] The paper demonstrates that there is an optimal random walk path length for training LMs on KGs, and that this optimal length varies across datasets (e.g., Countries peaks at 3, UMLS peaks at 2).
- Why unresolved: The paper only tests a limited range of path lengths (1-20) and on a small number of KG datasets. It's unclear how the optimal path length scales with KG size and complexity.
- What evidence would resolve it: Systematic experiments training LMs on KGs of varying size and complexity, testing a wider range of path lengths, and measuring the resulting reasoning performance.

### Open Question 2
- Question: How does the performance of language models trained on random walk paths compare to other knowledge graph reasoning methods, such as rule-based or GNN-based approaches?
- Basis in paper: [inferred] The paper only compares LMs trained on random walks to simple baseline methods (e.g., unweighted aggregation). It doesn't compare to more sophisticated KG reasoning methods.
- Why unresolved: A comprehensive comparison would require implementing and evaluating multiple KG reasoning methods on the same datasets, which is beyond the scope of this paper.
- What evidence would resolve it: Experiments training and evaluating LMs on random walks alongside rule-based methods (e.g., PRA), GNN-based methods (e.g., R-GCN), and other LM-based methods, on a variety of KG reasoning tasks.

### Open Question 3
- Question: Can the insights from this paper be applied to improve the reasoning abilities of large language models beyond the specific tasks studied (e.g., KG reasoning, math word problems)?
- Basis in paper: [explicit] The paper suggests that the random walk training method could be applied to the vast amount of pre-training data used to train LLMs, potentially improving their general reasoning abilities.
- Why unresolved: The paper only tests the method on small-scale tasks and with smaller LMs. It's unclear how well it would scale to the massive pre-training corpora and model sizes used for LLMs.
- What evidence would resolve it: Experiments applying the random walk training method to large-scale pre-training corpora, training LLMs of various sizes, and evaluating their reasoning performance on a diverse set of tasks beyond KGs and math word problems.

## Limitations
- Model complexity vs. interpretability trade-off: While the framework is interpretable, the actual reasoning process within LMs remains largely opaque
- Dataset generalization concerns: Optimal path length varies significantly across datasets, suggesting need for dataset-specific tuning
- Pre-training data dependency: The approach assumes sufficient reasoning paths exist in pre-training data, which may not hold for all domains

## Confidence

**High Confidence:** The empirical demonstration that random walk path length affects reasoning performance consistently across datasets. The KL divergence analysis showing LMs learn weighted logical rules rather than simple unweighted aggregation.

**Medium Confidence:** The core hypothesis that LMs reason by aggregating random walk paths seen during pre-training. While supported by experiments, alternative explanations (such as LMs learning statistical patterns rather than true reasoning) cannot be fully ruled out.

**Low Confidence:** The claim about efficient utilization of unlabeled reasoning paths for real-world pre-training. The paper provides limited evidence for how this would scale or perform in truly unsupervised settings.

## Next Checks

1. **Cross-domain Transfer Experiment:** Test the approach on a dataset from a completely different domain (e.g., medical reasoning or legal reasoning) to verify whether the random walk path aggregation mechanism generalizes beyond math word problems and knowledge graphs.

2. **Ablation on Path Quality:** Systematically vary the quality and relevance of random walk paths (e.g., using noisy or adversarial paths) to determine how sensitive LM reasoning performance is to the quality of aggregated paths versus their mere presence.

3. **Comparison with Alternative Reasoning Mechanisms:** Implement and compare against other reasoning approaches like explicit rule learning or neuro-symbolic methods on the same datasets to better understand the relative strengths and weaknesses of the random walk aggregation hypothesis.