---
ver: rpa2
title: Towards Effective Discrimination Testing for Generative AI
arxiv_id: '2412.21052'
source_url: https://arxiv.org/abs/2412.21052
tags:
- genai
- testing
- fairness
- systems
- discrimination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights the disconnect between current GenAI fairness
  testing methods and regulatory goals, which can lead to discriminatory outcomes
  being missed in practice. It shows through four case studies that traditional fairness
  metrics, red teaming approaches, single-turn evaluations, and static parameter testing
  often fail to capture the complexity of real-world GenAI deployments, especially
  in adaptive or dynamic environments.
---

# Towards Effective Discrimination Testing for Generative AI

## Quick Facts
- arXiv ID: 2412.21052
- Source URL: https://arxiv.org/abs/2412.21052
- Reference count: 35
- Key outcome: Traditional GenAI fairness testing methods often fail to capture discriminatory outcomes in practice, highlighting the need for context-specific, robust evaluation frameworks.

## Executive Summary
This paper identifies a critical disconnect between current generative AI fairness testing methods and regulatory goals, demonstrating through four case studies how traditional approaches often miss discriminatory outcomes. The authors show that common evaluation techniques—including equalized performance metrics, red teaming, single-turn evaluations, and static parameter testing—fail to capture the complexity of real-world GenAI deployments. They emphasize that GenAI systems require new evaluation frameworks that account for downstream effects, interaction modes, and deployment contexts to ensure equitable and responsible operation.

## Method Summary
The authors conduct four case studies to evaluate discrimination in different GenAI contexts: (1) a resume summarization task where model outputs influence simulated hiring decisions, (2) red teaming evaluations testing model robustness to offensive prompts, (3) multi-turn conversation analysis examining fairness across interaction modes, and (4) image generation testing for NSFW content disparities. Each case uses controlled experiments with synthetic data and LLM-based simulations to measure discriminatory outcomes, comparing traditional fairness metrics against actual downstream effects.

## Key Results
- Traditional fairness metrics like ROUGE can miss discriminatory outcomes by focusing on text similarity rather than downstream allocative effects
- Red teaming evaluations are highly sensitive to the choice of red language model, leading to inconsistent fairness assessments
- Single-turn evaluation settings do not generalize to multi-turn interactions, where context can amplify or introduce biases
- User-modifiable parameters (like guidance scale in diffusion models) can dramatically increase representational harms for specific groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fairness testing methods based on equalized performance metrics (like ROUGE) can fail to detect discriminatory outcomes in GenAI systems because the metrics do not capture downstream allocative effects.
- Mechanism: When GenAI models produce text that is used by downstream decision-makers, the model's fairness is determined by how its output influences those decisions, not by how well it matches a reference text. Traditional metrics like ROUGE only measure textual similarity, not how the summary affects a hiring manager's choice.
- Core assumption: The downstream decision-maker's behavior is influenced by subtle biases in the GenAI output that are not captured by standard evaluation metrics.
- Evidence anchors:
  - [abstract] "This paper highlights the disconnect between current GenAI fairness testing methods and regulatory goals, which can lead to discriminatory outcomes being missed in practice."
  - [section 4.1] "We study a case where an LLM is used to summarize resumes... and scored for ROUGE against a ground truth summary... we then use an LLM to simulate decisions of a hiring manager of whether or not to offer an in-person interview"
  - [corpus] Weak - no direct corpus evidence, but the mechanism is supported by the experiment showing that Llama-2-7B has higher ROUGE scores but leads to more discriminatory outcomes.
- Break condition: If the downstream decision-maker uses a completely different evaluation criteria that is independent of the GenAI output, or if the GenAI output has no influence on the decision.

### Mechanism 2
- Claim: Red teaming evaluations are highly sensitive to the choice of red language model (RedLM), leading to inconsistent and unreliable fairness assessments.
- Mechanism: Different RedLMs generate different sets of adversarial prompts, which elicit different responses from candidate models. The attack success rate (ASR) varies significantly based on the RedLM used, causing the perceived fairness of models to change drastically.
- Core assumption: The RedLM's prompt generation capability and style significantly influence the ASR and thus the fairness ranking of candidate models.
- Evidence anchors:
  - [section 4.2] "We produce 1000 attacks (i.e., question templates) each using a set of 7 RedLMs, and rank the fairness of a set of 4 candidate chatbots based on their responses to these red teaming prompts"
  - [section 4.2] "Attack success rate for each pair of candidate and target model is shown in Figure 4. Given full view of these ASR scores across RedLMs, it seems clear that Llama3-8B offers the least robust protection against offensive speech towards women. However, if a developer were to select Mistral-7B as the RedLM—seemingly a high-quality, reasonable choice—they would mistakenly conclude that Llama3-8B is actually the least discriminatory against women"
  - [corpus] Weak - no direct corpus evidence, but the mechanism is supported by the experiment showing inconsistent fairness rankings.
- Break condition: If red teaming becomes standardized with a fixed, well-vetted RedLM and prompt generation process, or if ASR becomes insensitive to RedLM choice.

### Mechanism 3
- Claim: Fairness assessments in simple, single-turn evaluation settings do not generalize to more complex, multi-turn interaction modes, leading to undetected bias in real-world deployments.
- Mechanism: In multi-turn interactions, the context of the conversation influences the model's responses, potentially amplifying or introducing biases that are not present in single-turn evaluations. The fairness ranking of models can change drastically depending on the interaction mode.
- Core assumption: The model's behavior is context-dependent, and the accumulated context in multi-turn interactions can trigger different biases than in single-turn settings.
- Evidence anchors:
  - [section 4.3] "We use datasets from two different domains, education (GSM8K) and health (MedQuad), in order to simulate multi-turn exchanges... Results are presented in Figure 5, illustrating how discrimination measurements in the single-turn setting do not generalize to the multi-turn setting"
  - [section 4.3] "Instead, we see that the perceived fairness of the candidate models can change drastically across settings: while Gemma-2-2B (red line) appears more discriminatory under a single-turn evaluation, it in fact seems consistently less so than Gemma-2-9B in the multi-turn setting"
  - [corpus] Weak - no direct corpus evidence, but the mechanism is supported by the experiment showing inconsistent fairness rankings across interaction modes.
- Break condition: If multi-turn interactions are adequately simulated in evaluation settings, or if models exhibit consistent behavior across interaction modes.

## Foundational Learning

- Concept: Disparate Impact Doctrine
  - Why needed here: The paper discusses how traditional discrimination law, particularly the disparate impact doctrine, applies to ML classification models but struggles with GenAI systems that do not make direct allocative decisions.
  - Quick check question: What is the key difference between disparate treatment and disparate impact in discrimination law?

- Concept: Fairness Metrics in ML
  - Why needed here: The paper critiques common fairness metrics like equalized performance and ROUGE, showing how they can fail to capture discriminatory outcomes in GenAI systems.
  - Quick check question: What are some limitations of using ROUGE as a fairness metric in GenAI systems?

- Concept: Red Teaming in AI Safety
  - Why needed here: The paper examines red teaming as a method for detecting bias in GenAI systems, highlighting its variability and sensitivity to RedLM choice.
  - Quick check question: What is attack success rate (ASR) in the context of red teaming, and why is it a problematic metric for fairness assessment?

## Architecture Onboarding

- Component map:
  - Resume Generation: GPT-4 generates synthetic resumes for the job of Social Worker
  - Resume Summarization: Open source LLMs (e.g., Llama-2-7B, Gemma-2-2B) summarize the resumes
  - Decision Simulation: Llama-3-70B-instruct simulates a hiring manager's decision based on the summaries
  - Red Teaming: RedLMs generate adversarial prompts, candidate models respond, and Detoxify scores the toxicity of responses
  - Multi-Turn Simulation: Gemma-2-9b-it generates responses to domain questions, creating conversation histories
  - NSFW Scoring: Falconsai/nsfw image detection model scores the NSFW content of generated images

- Critical path:
  1. Resume Generation → Resume Summarization → Decision Simulation (for hiring bias case)
  2. RedLM Prompt Generation → Candidate Model Response → Toxicity Scoring (for red teaming case)
  3. Domain Response Generation → Conversation Creation → RedLM Prompt Appending → Candidate Model Response → Toxicity Scoring (for multi-turn case)
  4. Image Generation → NSFW Scoring (for diffusion model case)

- Design tradeoffs:
  - Using synthetic data (resumes) allows for controlled experiments but may not fully capture real-world complexity
  - Simulating decisions with LLMs provides a scalable way to test bias but may not perfectly reflect human decision-making
  - Using a single metric (e.g., ROUGE) for evaluation is simple but may miss important nuances in fairness
  - Red teaming with multiple RedLMs provides a more comprehensive assessment but introduces variability and complexity

- Failure signatures:
  - High ROUGE scores but discriminatory outcomes in hiring decisions
  - Inconsistent fairness rankings across different RedLMs in red teaming
  - Different fairness assessments in single-turn vs. multi-turn interaction modes
  - Disproportionate NSFW scores for certain demographic groups in image generation

- First 3 experiments:
  1. Generate synthetic resumes with stereotypical names, summarize them with different LLMs, and simulate hiring decisions to test for bias
  2. Use multiple RedLMs to generate adversarial prompts, have candidate models respond, and compare fairness rankings based on attack success rate
  3. Create multi-turn conversation histories using domain data, append red teaming prompts, and compare fairness assessments in single-turn vs. multi-turn settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective and standardized approach to measure discrimination in generative AI systems that can reliably detect harmful downstream outcomes across diverse deployment scenarios?
- Basis in paper: explicit
- Why unresolved: The paper identifies a significant gap between existing bias assessment methods and regulatory goals, particularly in how current metrics like ROUGE fail to capture downstream discriminatory effects. The authors highlight the need for more holistic, context-specific evaluation suites that can better predict real-world impacts.
- What evidence would resolve it: Development and validation of a standardized evaluation framework that combines multiple metrics (e.g., sentiment analysis, keyword detection, downstream outcome simulation) and demonstrates consistent correlation with discriminatory behaviors in diverse real-world deployment settings.

### Open Question 2
- Question: How can red teaming methodologies be made robust and reproducible to prevent arbitrary fairness rankings and ensure consistent identification of discriminatory behavior across different testing conditions?
- Basis in paper: explicit
- Why unresolved: The paper demonstrates that red teaming results are highly sensitive to choices like the red language model, leading to inconsistent fairness assessments. The authors suggest combining multiple techniques but acknowledge the need for more robust frameworks.
- What evidence would resolve it: A comprehensive study comparing multiple red teaming approaches across various models and protected groups, demonstrating that a standardized protocol produces consistent rankings and successfully identifies discriminatory models regardless of testing conditions.

### Open Question 3
- Question: How does user-modifiable parameter settings in generative AI systems affect discriminatory outcomes, and what frameworks can be developed to test and mitigate these effects?
- Basis in paper: explicit
- Why unresolved: The paper shows that parameters like guidance scale in text-to-image models can dramatically increase representational harms for specific groups, yet there's no clear framework for testing these parameter effects or determining liability.
- What evidence would resolve it: A systematic analysis mapping parameter space to discriminatory outcomes across multiple generative AI modalities, coupled with guidelines for safe parameter ranges and testing protocols that account for user modifications.

### Open Question 4
- Question: What evaluation approaches can effectively capture discrimination in complex interaction modes like multi-turn conversations and multi-modal systems?
- Basis in paper: inferred
- Why unresolved: The paper demonstrates that fairness assessments in single-turn settings do not generalize to multi-turn interactions, and highlights the need for methods that can evaluate complex deployment conditions.
- What evidence would resolve it: Development of benchmark datasets and evaluation metrics specifically designed for multi-turn and multi-modal interactions that demonstrate superior predictive validity for real-world discriminatory outcomes compared to single-turn evaluations.

### Open Question 5
- Question: How can liability for discriminatory outcomes be effectively allocated between developers and deployers of generative AI systems given the complexity of these systems and the separation of roles?
- Basis in paper: inferred
- Why unresolved: The paper discusses the legal uncertainty around liability, particularly when users modify systems post-deployment, and notes that current regulatory frameworks assign obligations to both parties without clear guidance on responsibility.
- What evidence would resolve it: Case studies of real-world discriminatory incidents involving generative AI that clearly delineate the causal chain from system design to deployment to user modification, informing clearer liability frameworks and testing responsibilities.

## Limitations
- Empirical validation relies entirely on simulation rather than human studies, limiting ecological validity
- Case studies focus on specific domains (hiring, toxicity, education/health) and may not generalize to other GenAI applications
- Paper does not resolve the fundamental challenge of evaluating outcomes in GenAI systems that don't make direct decisions

## Confidence
- **High Confidence**: The core observation that traditional ML fairness testing methods are misaligned with GenAI's non-deterministic, text-based outputs
- **Medium Confidence**: The specific findings about red teaming variability and multi-turn evaluation limitations
- **Medium Confidence**: The broader claim that current regulatory frameworks struggle with GenAI discrimination testing

## Next Checks
1. **Human-in-the-loop validation**: Replicate the resume summarization and hiring simulation experiments with actual human decision-makers to assess whether LLM-simulated discrimination correlates with human bias patterns

2. **Cross-domain generalizability testing**: Apply the proposed testing framework to additional GenAI applications (e.g., code generation, creative writing, customer service) to identify whether the identified limitations persist across different task types and output modalities

3. **Regulatory alignment analysis**: Systematically map current regulatory requirements (e.g., EU AI Act, NIST AI RMF) against the testing gaps identified in this work, identifying specific areas where guidance needs updating for GenAI systems