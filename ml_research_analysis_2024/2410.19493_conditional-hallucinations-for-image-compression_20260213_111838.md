---
ver: rpa2
title: Conditional Hallucinations for Image Compression
arxiv_id: '2410.19493'
source_url: https://arxiv.org/abs/2410.19493
tags:
- image
- compression
- hific
- distortion
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel image compression method that dynamically
  balances the degree of hallucination based on image content. The key idea is to
  predict user preferences on hallucinations using a trained classifier and adjust
  the perceptual weight in the reconstruction loss accordingly.
---

# Conditional Hallucinations for Image Compression

## Quick Facts
- arXiv ID: 2410.19493
- Source URL: https://arxiv.org/abs/2410.19493
- Authors: Till Aczel; Roger Wattenhofer
- Reference count: 40
- Primary result: ConHa outperforms state-of-the-art compression methods across all bitrates in user studies

## Executive Summary
This paper introduces a novel image compression method that dynamically balances the degree of hallucination based on image content. The key idea is to predict user preferences on hallucinations using a trained classifier and adjust the perceptual weight in the reconstruction loss accordingly. This approach results in a Conditionally Hallucinating compression model (ConHa) that outperforms state-of-the-art image compression methods. The primary result is that ConHa consistently outperforms the previous state-of-the-art compression model, HiFiC, across all bitrates in a user study, as evidenced by higher bootstrapped Elo Scores.

## Method Summary
The method trains a preference model using ResNet50 features to predict whether an image benefits more from hallucinated details or exact reconstruction. This prediction (w) scales the perceptual loss term during compression model training, allowing the model to emphasize realism for texture-like content and fidelity for text/edges. The compression model uses a two-stage training process: first optimizing rate, MSE, and LPIPS losses for 1 million iterations, then incorporating the GAN component for another million iterations. The final model is evaluated against baselines using both user studies and computational metrics on the CLIC 2024 test set.

## Key Results
- ConHa outperforms Hyperprior and HiFiC at low bitrates
- ConHa outperforms HiFiC alone at medium and high bitrates
- User study shows consistent Elo Score improvements across all bitrates compared to previous state-of-the-art

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The conditional hallucination model achieves better perceptual quality by dynamically adjusting the GAN discriminator weight based on image content.
- Mechanism: The model uses a trained preference classifier (MP) to predict whether an image benefits more from hallucinated details or exact reconstruction. This prediction (w) scales the perceptual loss term, allowing the model to emphasize realism for texture-like content and fidelity for text/edges.
- Core assumption: A single scalar weight (w) is sufficient to capture the optimal hallucination level for an entire image, and this can be predicted from the uncompressed image.
- Evidence anchors:
  - [abstract] "By using this prediction to adjust the perceptual weight in the reconstruction loss, we develop a Conditionally Hallucinating compression model (ConHa) that outperforms state-of-the-art image compression methods."
  - [section 3.1] "To align the compression model's loss term with human preferences, obtaining human labels for all crops of all training images is too expensive. Instead, we train a preference model MP on the labeled data, namely a binary classifier, which can then be utilized during the compression model training."
  - [corpus] No direct corpus evidence; this is a novel mechanism not well-covered in related work.
- Break condition: If the preference model's predictions do not correlate well with actual user preferences, or if the image context is too limited to make accurate predictions.

### Mechanism 2
- Claim: The two-stage training process allows the model to first learn basic reconstruction before optimizing for perceptual quality.
- Mechanism: The compression model is first trained with rate, MSE, and LPIPS losses for 1 million iterations, then continues with GAN component incorporation for another million iterations. This staged approach stabilizes training and prevents early collapse to unrealistic hallucinations.
- Core assumption: Separating the perceptual and GAN optimization into stages improves convergence compared to joint optimization from the start.
- Evidence anchors:
  - [section 3.2] "We adopt the HiFiC approach [1] for our compression model training... First, we train the model for 1 million iterations with the rate, MSE, and LPIPS losses. Then we continue training for another 1 million iterations with the GAN component incorporated."
  - [corpus] No direct corpus evidence; this is a standard practice in GAN-based image compression literature.
- Break condition: If the two-stage approach leads to mode collapse or fails to capture fine-grained perceptual details that could be learned jointly.

### Mechanism 3
- Claim: User studies provide more reliable evaluation than computational metrics for perceptual quality in image compression.
- Mechanism: The paper conducts a 2AFC user study with 40 participants, collecting 1531 comparisons. Bootstrapped Elo scores are used to aggregate results, which show ConHa outperforming HiFiC across all bitrates.
- Core assumption: Human perceptual preferences are the ultimate ground truth for evaluating compression quality, and computational metrics like PSNR, MS-SSIM, LPIPS, and FID are insufficient proxies.
- Evidence anchors:
  - [section 4.2] "Computational distortion metrics often fail to predict human preferences accurately, highlighting the need for user studies... At all bitrates, our model consistently outperforms HiFiC, the previous state-of-the-art compression model."
  - [corpus] No direct corpus evidence; this is a common finding in compression literature but not explicitly supported here.
- Break condition: If the user study sample size or methodology introduces bias, or if computational metrics improve significantly in correlating with human perception.

## Foundational Learning

- Concept: Rate-distortion-perception trade-off in learned image compression
  - Why needed here: The paper extends the standard rate-distortion trade-off by incorporating a perceptual term controlled by a GAN discriminator. Understanding this framework is essential to grasp how ConHa modifies the loss function.
  - Quick check question: What is the difference between optimizing for rate-distortion versus rate-distortion-perception in image compression?

- Concept: Variational Autoencoders (VAEs) and latent quantization for compression
  - Why needed here: The compression model uses an autoencoder architecture with learned quantization, similar to VAEs. Understanding how this differs from traditional codecs is crucial for implementing the model.
  - Quick check question: How does learned quantization in VAEs enable compression, and why is it necessary for this application?

- Concept: Generative Adversarial Networks (GANs) and their role in perceptual optimization
  - Why needed here: The GAN discriminator ensures the compressed images remain in-distribution, preventing out-of-distribution artifacts. Understanding GAN training dynamics is important for the two-stage training process.
  - Quick check question: What is the role of the GAN discriminator in the rate-distortion-perception loss, and how does it interact with the generator during training?

## Architecture Onboarding

- Component map:
  - Input image → Encoder → Quantizer → Generator → Output image
  - Preference Model (MP): ResNet50-based classifier predicting hallucination weight (w)
  - GAN Discriminator (D): Ensures reconstructed images are in-distribution

- Critical path: Input image → Encoder → Quantizer → Generator → Output image
  - The preference model runs once per image during training to determine weight w
  - The GAN discriminator is trained alternately with the generator

- Design tradeoffs:
  - Fixed vs. conditional perceptual weight: ConHa-fixed uses average w across dataset, while ConHa adjusts per image
  - Two-stage vs. joint training: Separates basic reconstruction from perceptual optimization
  - Preference model complexity: Uses ResNet50 features vs. lighter alternatives for speed

- Failure signatures:
  - If w predictions are consistently wrong, the model may hallucinate inappropriately
  - If GAN training is unstable, the model may generate unrealistic artifacts
  - If preference model overfits to DIV2K, performance may degrade on other datasets

- First 3 experiments:
  1. Train ConHa-fixed (fixed w) and compare to ConHa (conditional w) to validate the importance of content-awareness
  2. Test ablation: Remove GAN discriminator to see impact on perceptual quality vs. hallucinations
  3. Evaluate preference model accuracy by testing its predictions against human judgments on held-out data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more accurate distortion metrics that align with human perception and capture subtle differences in compressed images?
- Basis in paper: [explicit] The paper states that "Distortion metrics often fail to accurately capture human perception" and that existing metrics do not align with human evaluations in user studies.
- Why unresolved: Current metrics like PSNR and MS-SSIM often poorly align with user studies and can even negatively correlate with human preferences among the best-performing models. The paper highlights that automated metrics do not capture the performance gap between ConHa and other baselines as indicated by user studies.
- What evidence would resolve it: Development of new distortion metrics that consistently correlate with human preferences in user studies across various image types and compression scenarios. Validation through extensive user studies comparing these new metrics against human judgments.

### Open Question 2
- Question: Can the conditional hallucination approach be extended to other domains beyond image compression, such as video or 3D data compression?
- Basis in paper: [inferred] The paper focuses on image compression but discusses the concept of balancing distortion and perception based on content, which could be applicable to other types of data.
- Why unresolved: The paper only demonstrates the effectiveness of the approach for image compression. Extending this to other domains would require adapting the methodology to handle different data structures and perceptual qualities specific to those domains.
- What evidence would resolve it: Successful implementation and user study validation of the conditional hallucination approach in video compression and 3D data compression, showing improved perceptual quality and user preferences compared to existing methods.

### Open Question 3
- Question: How does the conditional hallucination model perform in real-time or resource-constrained environments where computational efficiency is crucial?
- Basis in paper: [inferred] The paper does not address the computational efficiency of the model, which is important for practical deployment in real-time applications or devices with limited resources.
- Why unresolved: The paper focuses on the perceptual quality improvements but does not discuss the computational overhead introduced by the additional components like the preference model and the dynamic adjustment of hallucination levels.
- What evidence would resolve it: Benchmarking the model's performance in terms of speed and resource usage in real-time applications or on resource-constrained devices, and comparing it to existing methods to determine if the perceptual quality improvements justify any additional computational costs.

## Limitations

- The preference model's accuracy is critical but not extensively validated against held-out human judgments
- The two-stage training approach lacks comparison to alternative optimization strategies that might achieve similar results
- The user study sample size (40 participants) may not fully represent diverse user preferences across different demographic groups

## Confidence

- High confidence in user study methodology and Elo score aggregation
- Medium confidence in preference model's ability to generalize beyond DIV2K dataset
- Low confidence in claim that this approach represents the "optimal" solution for hallucination balancing

## Next Checks

1. Conduct a follow-up user study with a larger and more diverse participant pool to validate whether the Elo score advantages hold across different user demographics and cultural contexts.

2. Test the preference model's predictions against held-out human judgments on the CLIC 2024 test set to quantify its accuracy and identify failure modes.

3. Compare the two-stage training approach against joint optimization from the start, as well as other perceptual loss formulations, to determine if the staged approach is indeed optimal or if simpler alternatives could achieve similar results.