---
ver: rpa2
title: Revisiting Random Walks for Learning on Graphs
arxiv_id: '2407.01214'
source_url: https://arxiv.org/abs/2407.01214
tags:
- random
- walk
- graph
- walks
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work revisits random walk neural networks (RWNNs) as an alternative
  to message passing neural networks for graph learning. The authors formalize RWNNs
  as a combination of random walk algorithms, recording functions, and reader neural
  networks.
---

# Revisiting Random Walks for Learning on Graphs

## Quick Facts
- arXiv ID: 2407.01214
- Source URL: https://arxiv.org/abs/2407.01214
- Reference count: 40
- One-line primary result: RWNNs based on pre-trained language models outperform previous methods on graph isomorphism learning and achieve competitive results on real-world transductive classification tasks.

## Executive Summary
This paper revisits random walk neural networks (RWNNs) as an alternative to message passing neural networks (MPNNs) for graph learning. The authors formalize RWNNs as a combination of random walk algorithms, recording functions, and reader neural networks, establishing them as universal approximators of graph functions in probability. By using anonymized walk records and rapidly mixing random walks, RWNNs achieve probabilistic invariance even when using language models as reader networks. Experiments demonstrate that RWNNs overcome the over-smoothing problem inherent in MPNNs while exhibiting probabilistic under-reaching instead of over-squashing.

## Method Summary
The method formalizes RWNNs as a three-component architecture: a random walk algorithm that generates vertex sequences on graphs, a recording function that converts these walks into machine-readable format, and a reader neural network that processes the recorded information. The key innovation is achieving probabilistic invariance by using anonymized walk records (without vertex names) combined with random walks that mix rapidly to stationary distribution. This allows the use of powerful language models as readers while maintaining isomorphism invariance. The approach ensures universal approximation by designing random walks that traverse sufficient graph information to enable graph reconstruction up to isomorphism.

## Key Results
- RWNNs achieve probabilistic invariance while avoiding over-smoothing, with over-squashing manifesting as probabilistic under-reaching
- RWNNs based on pre-trained language models outperform previous methods on graph isomorphism learning benchmarks
- RWNNs achieve competitive results on real-world transductive classification tasks while maintaining theoretical guarantees

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RWNNs achieve probabilistic invariance even when the reader neural network lacks symmetry
- Mechanism: By using invariant random walk algorithms and invariant recording functions, any reader neural network (even non-invariant ones) processes the walk record in a way that guarantees probabilistic invariance of the overall RWNN output
- Core assumption: The random walk algorithm must be invariant in probability and the recording function must be invariant
- Evidence anchors:
  - [abstract]: "we can design them to be isomorphism invariant while capable of universal approximation of graph functions in probability"
  - [section]: "Xθ(·) is invariant in probability, if its random walk algorithm is invariant in probability and its recording function is invariant"
  - [corpus]: Weak - no direct corpus evidence, but related to invariant random walks in learning on graphs
- Break condition: If either the random walk algorithm or recording function fails to maintain invariance, the probabilistic invariance guarantee breaks

### Mechanism 2
- Claim: RWNNs avoid over-smoothing while over-squashing manifests as probabilistic under-reaching
- Mechanism: Unlike MPNNs where feature mixing through adjacency matrix causes exponential convergence to stationary distribution, RWNNs decouple feature processing from graph topology. Over-squashing occurs when distant vertices become exponentially unlikely to be reached during random walks
- Core assumption: The parallelism between MPNN feature mixing and random walk transition matrices holds for linearized models
- Evidence anchors:
  - [abstract]: "over-smoothing in message passing is alleviated by construction in RWNNs, while over-squashing manifests as probabilistic under-reaching"
  - [section]: "Theorem 3.5. The simple RWNN outputs h(l) → x⊤π as l → ∞" and "Theorem 3.6. Let h(l)u be output of the simple RWNN queried with u"
  - [corpus]: Weak - related concepts exist in random walks for learning on graphs but not direct evidence for this specific mechanism
- Break condition: If the random walk becomes too localized or the walk length is insufficient, under-reaching prevents capturing distant structural information

### Mechanism 3
- Claim: RWNNs can universally approximate graph functions in probability by ensuring random walks record sufficient graph information
- Mechanism: If random walks traverse all vertices (with named neighbors) or all edges (without named neighbors), the recorded information allows a sufficiently powerful reader neural network to reconstruct the graph up to isomorphism, enabling universal approximation
- Core assumption: Cover times of random walks are finite and the reader neural network is universal
- Evidence anchors:
  - [abstract]: "capable of universal approximation of graph functions in probability"
  - [section]: "Theorem 3.2. An RWNN Xθ(·) with a sufficiently powerful fθ is a universal approximator of graph-level functions in probability"
  - [corpus]: Moderate - related to learning long range dependencies on graphs via random walks and random search neural networks for efficient graph learning
- Break condition: If the random walk length is insufficient to cover the graph or the reader neural network lacks sufficient expressive power, universal approximation fails

## Foundational Learning

- Concept: Isomorphism invariance
  - Why needed here: RWNNs must respect graph symmetries to generalize properly across isomorphic graphs
  - Quick check question: What property ensures that RWNNs produce the same output distribution for isomorphic graphs?

- Concept: Markov chain theory and cover times
  - Why needed here: Cover times determine how long random walks need to be to ensure sufficient graph information is recorded
  - Quick check question: How does the minimum degree local rule (MDLR) improve vertex cover time compared to uniform random walks?

- Concept: Probabilistic invariance and universal approximation
  - Why needed here: These concepts allow RWNNs to achieve expressive power while maintaining symmetry constraints
  - Quick check question: What is the difference between deterministic invariance and probabilistic invariance in graph neural networks?

## Architecture Onboarding

- Component map: Random walk algorithm → Recording function → Reader neural network → Output prediction
- Critical path: The random walk algorithm and recording function must maintain invariance; the reader neural network must be sufficiently powerful
- Design tradeoffs: Using named neighbors recording increases vertex cover time efficiency but requires more storage; using language models provides expressive power but increases computational cost
- Failure signatures: Poor performance on isomorphic graphs indicates invariance issues; failure to capture long-range dependencies suggests under-reaching problems
- First 3 experiments:
  1. Test anonymization recording on a simple graph to verify invariance property
  2. Compare MDLR vs uniform random walks on lollipop graphs to measure cover time improvements
  3. Evaluate RWNN with MLP reader vs transformer reader on a small graph classification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum walk length required to achieve high accuracy on real-world graph tasks, given the trade-off between computation cost and information coverage?
- Basis in paper: [explicit] The authors discuss cover times and their relationship to task performance in Section 2 and Appendix A.8, noting that while cover times provide worst-case upper bounds, in practice shorter walks may suffice for recovering useful information.
- Why unresolved: The paper provides theoretical upper bounds based on cover times but does not empirically determine the minimum walk length needed for various tasks.
- What evidence would resolve it: Experimental results showing task performance versus walk length for various graph datasets and tasks, identifying the point of diminishing returns.

### Open Question 2
- Question: How do different random walk algorithms (e.g., uniform, non-backtracking, node2vec) compare in terms of task performance when used with anonymized recording and language models?
- Basis in paper: [explicit] The authors discuss various random walk algorithms and their properties in Section 2, including non-backtracking walks and the minimum degree local rule (MDLR).
- Why unresolved: The paper focuses on MDLR walks with non-backtracking but does not provide a comprehensive comparison with other walk algorithms.
- What evidence would resolve it: Empirical comparison of task performance using different random walk algorithms with the same recording function and reader network architecture.

### Open Question 3
- Question: Can language models pre-trained on graph pseudocorpora outperform standard language models when used as reader networks for RWNNs?
- Basis in paper: [explicit] The authors mention the possibility of training a language model on a large pseudocorpus of random walks to build a foundation model for graphs in the Limitations section.
- Why unresolved: The paper does not explore this possibility, focusing instead on using existing pre-trained language models.
- What evidence would resolve it: Experimental comparison of task performance using language models pre-trained on graph pseudocorpora versus standard pre-trained language models.

## Limitations
- The theoretical analysis assumes connected, simple, undirected graphs, limiting applicability to directed or disconnected graph structures
- The probabilistic invariance guarantee depends critically on random walk mixing times, which may be prohibitively slow for graphs with high conductance bottlenecks
- The use of pre-trained language models as reader networks introduces computational overhead and potential overfitting on smaller datasets

## Confidence
- Probabilistic invariance mechanism: **High** - The theoretical framework is well-established and supported by rigorous proofs
- Universal approximation capability: **Medium** - While theoretically sound, practical limitations of finite random walks and finite neural networks may restrict real-world performance
- Over-smoothing avoidance: **High** - The mechanism is mathematically proven and distinct from MPNNs
- Language model reader effectiveness: **Medium** - Empirically demonstrated but with computational trade-offs

## Next Checks
1. **Conductance sensitivity analysis**: Systematically vary graph conductance and measure the impact on mixing times and classification accuracy to quantify the practical limits of probabilistic invariance
2. **Inductive learning evaluation**: Test RWNNs on graph prediction tasks with unseen graph structures to validate generalization beyond the transductive setting
3. **Computational efficiency benchmarking**: Compare the training and inference time of RWNNs with language model readers against standard MPNNs across varying graph sizes and walk lengths