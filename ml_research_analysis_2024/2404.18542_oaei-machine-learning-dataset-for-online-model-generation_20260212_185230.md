---
ver: rpa2
title: OAEI Machine Learning Dataset for Online Model Generation
arxiv_id: '2404.18542'
source_url: https://arxiv.org/abs/2404.18542
tags:
- systems
- matching
- oaei
- alignment
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a machine learning dataset for OAEI ontology
  matching tracks to enable online model adaptation. The dataset provides stratified
  train/validation/test splits (20%/10%/70%) for Anatomy, BioDiv, KnowledgeGraph,
  and Bio-ML tracks.
---

# OAEI Machine Learning Dataset for Online Model Generation

## Quick Facts
- arXiv ID: 2404.18542
- Source URL: https://arxiv.org/abs/2404.18542
- Authors: Sven Hertling; Ebrahim Norouzi; Harald Sack
- Reference count: 11
- Key outcome: Dataset enables online model adaptation for OAEI ontology matching tracks, achieving up to 13.3% F1-measure improvement through confidence threshold tuning

## Executive Summary
This paper introduces a machine learning dataset for OAEI ontology matching tracks that enables online model adaptation. The dataset provides stratified train/validation/test splits (20%/10%/70%) for Anatomy, BioDiv, KnowledgeGraph, and Bio-ML tracks. A use case demonstrates tuning confidence thresholds for three OAEI 2023 systems (Matcha, LogMap, OLaLa), achieving F1-measure improvements up to 13.3% on BioDiv track. The dataset supports online model generation where systems adapt to given input alignments rather than using pre-trained models.

## Method Summary
The authors create a machine learning dataset for OAEI ontology matching by stratifying reference alignments based on entity type (class, property, instance), relation type, and difficulty level (easy, medium, hard). They split the reference alignment into 20% training, 10% validation, and 70% test sets while maintaining the same distribution across all criteria. The dataset requires systems to generate hard negative examples dynamically rather than providing pre-generated negatives. The use case demonstrates confidence threshold tuning using supervised partial (SPart) and supervised complete (SComp) approaches, where systems optimize F1-measure on validation data and evaluate on test data.

## Key Results
- Achieved F1-measure improvements up to 13.3% (OLaLa on BioDiv track) through confidence threshold tuning
- Dataset enables fair evaluation of online model adaptation approaches for ontology matching systems
- Stratified splits maintain entity type, relation type, and difficulty distributions across train/validation/test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stratified train/validation/test splits enable fair ML evaluation for ontology matching systems.
- Mechanism: The dataset creator stratifies reference alignments by entity type, relation type, and difficulty level to ensure each split maintains the same distribution as the original reference alignment.
- Core assumption: The difficulty classification based on label matching complexity accurately reflects the actual matching difficulty.
- Evidence anchors: [section] mentions using 20%/10%/70% splits and stratifying by entity type, relation type, and difficulty.

### Mechanism 2
- Claim: Online model generation improves system adaptability compared to offline pre-trained models.
- Mechanism: Systems adapt their confidence thresholds using the provided training and validation alignments during execution, rather than using pre-trained models.
- Core assumption: The input alignment provided to the system is representative of the overall matching task.
- Evidence anchors: [abstract] states the dataset supports online model generation where systems adapt to given input alignments.

### Mechanism 3
- Claim: Excluding negative examples from the dataset forces systems to generate hard negatives dynamically, improving model robustness.
- Mechanism: Rather than providing pre-generated negative examples, the dataset requires systems to create hard negatives on the fly by assuming at most one correct correspondence per entity.
- Core assumption: Systems can effectively generate hard negatives that reflect their specific matching capabilities.
- Evidence anchors: [section] describes not providing negatives but letting systems create them on the fly.

## Foundational Learning

- Concept: Stratified sampling
  - Why needed here: Ensures that training, validation, and test sets maintain the same distribution as the original reference alignment across entity types, relation types, and difficulty levels.
  - Quick check question: If a reference alignment has 60% class-class correspondences, 30% class-property, and 10% instance-instance, what percentage of each should appear in the stratified training set?

- Concept: Online vs offline model training
  - Why needed here: Online training allows systems to adapt to the specific characteristics of input alignments, while offline training produces static models that may not generalize well to new tasks.
  - Quick check question: What is the key difference between a system that downloads and packages a pre-trained model versus one that tunes parameters based on input alignment during execution?

- Concept: Confidence threshold optimization
  - Why needed here: Fine-tuning confidence thresholds using training and validation data can significantly improve precision-recall tradeoffs for ontology matching systems.
  - Quick check question: If increasing the confidence threshold improves precision but decreases recall, what metric should be optimized to find the best threshold value?

## Architecture Onboarding

- Component map: Dataset loader -> Stratification manager -> Online tuner -> Evaluation module
- Critical path: Load dataset → Stratify reference alignment → Split into train/validation/test → Provide training and validation to system → System tunes parameters → Evaluate on test set
- Design tradeoffs: Providing negative examples vs. forcing systems to generate them dynamically; using micro vs. macro averaging for evaluation; choosing between supervised partial vs. supervised complete threshold optimization approaches
- Failure signatures: Poor performance on test set despite good validation performance (overfitting); inability to improve over default thresholds (stratification may not capture important variations); inconsistent results across different runs (randomness in stratification)
- First 3 experiments:
  1. Load the Anatomy track dataset and verify that the stratified splits maintain the same entity type distribution as the original reference alignment
  2. Implement a simple threshold tuner that optimizes F1-measure on the validation set, then evaluate on the test set
  3. Compare performance when using supervised partial (SPart) vs. supervised complete (SComp) approaches for threshold optimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal train/validation/test split ratio for machine learning datasets in ontology matching that balances having enough training data with maintaining fair evaluation?
- Basis in paper: [explicit] The paper uses 20%/10%/70% splits and mentions the reason for low training/validation fractions is the extreme imbalance between correct and incorrect correspondences
- Why unresolved: The paper doesn't experimentally compare different split ratios or provide theoretical justification for why 20/10/70 is optimal versus alternatives like 60/20/20 or 80/10/10
- What evidence would resolve it: Empirical studies comparing multiple split ratios across different OAEI tracks showing which ratio maximizes system performance while maintaining fair evaluation

### Open Question 2
- Question: How can negative examples be effectively generated for ontology matching systems without relying on predefined negative samples?
- Basis in paper: [explicit] The paper states "we do not provide negatives... but let the system create them on the fly" and describes a method assuming "at most one entity exists in the target graph"
- Why unresolved: The paper only describes one heuristic approach but doesn't evaluate its effectiveness or compare it to other negative sampling strategies
- What evidence would resolve it: Comparative studies showing the impact of different negative sampling strategies on model performance across various OAEI tracks

### Open Question 3
- Question: What are the limits of online model adaptation for ontology matching systems compared to offline pre-training?
- Basis in paper: [inferred] The paper contrasts "online model learning" versus "offline model generation" and aims to "enable a fair comparison for ML-based systems," but only demonstrates threshold tuning rather than full model adaptation
- Why unresolved: The paper only shows threshold tuning improvements and states "We hope to see larger improvements if systems use it to train their whole approach" without providing such evidence
- What evidence would resolve it: Comprehensive evaluation comparing fully online-trained systems against offline-trained systems across multiple OAEI tracks showing performance differences and limitations of each approach

## Limitations
- The dataset and use case only explore confidence threshold optimization, while systems may have many other tunable parameters that could benefit from online adaptation
- The difficulty classification (easy, medium, hard) is based on label matching complexity heuristics, which may not accurately reflect true matching difficulty in all cases
- The approach of letting systems generate hard negatives dynamically assumes systems can effectively implement this, but specific strategies are not detailed

## Confidence
- **High confidence**: The dataset creation methodology and stratified sampling approach are well-specified and reproducible
- **Medium confidence**: The effectiveness of online threshold tuning will depend on the specific characteristics of each system and input alignment
- **Medium confidence**: The assumption that at most one correct correspondence per entity holds for the OAEI tracks, though this may not generalize to all ontology matching scenarios

## Next Checks
1. Verify stratification quality: Check that the stratified train/validation/test splits maintain the same entity type, relation type, and difficulty distributions as the original reference alignments across all four tracks
2. Compare tuning strategies: Implement and compare both supervised partial (SPart) and supervised complete (SComp) threshold optimization approaches to determine which works better for different system types and alignment completeness levels
3. Test generalization: Evaluate whether the online threshold tuning approach that works well for Anatomy and BioDiv tracks also generalizes to the KnowledgeGraph and Bio-ML tracks with different characteristics