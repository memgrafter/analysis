---
ver: rpa2
title: 'VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation'
arxiv_id: '2406.17681'
source_url: https://arxiv.org/abs/2406.17681
tags:
- fiber
- total
- question
- blue
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of data contamination in language
  model evaluation, where models may have been trained on benchmark test sets. To
  mitigate this, the authors propose a dynamic variable perturbation method that extracts
  variables from test questions and samples new values to create unique test cases
  for each evaluation.
---

# VarBench: Robust Language Model Benchmarking Through Dynamic Variable Perturbation

## Quick Facts
- arXiv ID: 2406.17681
- Source URL: https://arxiv.org/abs/2406.17681
- Reference count: 40
- Authors show that LLMs trained on static benchmarks may have memorized patterns, and propose variable perturbation to create unique test cases that reduce contamination effects.

## Executive Summary
This paper addresses data contamination in language model evaluation, where models may have been trained on benchmark test sets. The authors propose a dynamic variable perturbation method that extracts variables from test questions and samples new values to create unique test cases for each evaluation. Applied to GSM8K, ARC, CommonsenseQA, and TruthfulQA benchmarks, the method shows significant performance drops for both open- and closed-source models on perturbed datasets compared to original benchmarks, indicating potential training data contamination. The approach provides a more accurate assessment of true model capabilities by preventing reliance on memorization of specific numerical patterns.

## Method Summary
The method involves extracting variables from test questions, generating delexicalized versions, defining value ranges, and sampling new values to create unique test cases for each evaluation. The process uses LLMs to identify variables and generate solution functions, then applies these to create perturbed datasets. The authors evaluate multiple LLMs (open-source like Mistral, Llama, Phi; closed-source like GPT-4) on both original and perturbed versions of four benchmarks to measure the impact of contamination.

## Key Results
- Open- and closed-source LLMs show significant performance drops on dynamically generated test sets compared to original benchmarks
- Variable perturbation is more effective than alternative strategies like paraphrasing or shuffling choices
- Closed-source models like GPT-4o show smaller performance drops than open-source models, suggesting possible differences in scale or reasoning capabilities
- The approach effectively reduces token-level contamination but may not fully address semantic-level contamination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sampling new values for variables ensures models cannot rely on memorization of specific numbers
- Mechanism: Delexicalizing questions and generating new values from defined ranges creates unique test cases, reducing exact data contamination
- Core assumption: Models trained on static benchmarks may have memorized numerical patterns rather than learned general problem-solving
- Evidence anchors: [abstract] Significant performance drops on perturbed datasets; [section 4.1] Process involves extraction, delexicalization, and sampling; [corpus] Dynamic-KGQA addresses similar contamination concerns
- Break condition: If variable ranges are too narrow or predictable, models may still perform well by generalizing from seen patterns

### Mechanism 2
- Claim: Removing exact token sequences mitigates contamination at semantic level
- Mechanism: Replacing specific numbers and phrases with variables eliminates exact matches while preserving semantic structure
- Core assumption: Contamination occurs at both token and semantic/information levels
- Evidence anchors: [section 6.2] Some models show smaller drops, suggesting semantic contamination; [section 8] Variable replacement more effective than paraphrasing/shuffling; [corpus] Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables supports dynamic evaluation
- Break condition: If models generalize across semantic patterns, they may still perform well despite variable perturbation

### Mechanism 3
- Claim: Dynamic evaluation provides accurate assessment of true model capabilities
- Mechanism: Creating unique test cases ensures performance reflects current reasoning ability rather than memorization
- Core assumption: Static benchmarks cannot reliably measure capabilities if training data includes benchmarks
- Evidence anchors: [abstract] Significant performance drops demonstrate contamination; [section 5] Evaluation uses different prompting strategies and random seeds; [corpus] Is Your Benchmark (Still) Useful? addresses similar concerns
- Break condition: If models effectively generalize from seen patterns or variable ranges lack diversity

## Foundational Learning

- Concept: Data contamination in language model evaluation
  - Why needed here: Understanding how training data leakage affects benchmark reliability is crucial for developing mitigation strategies
  - Quick check question: Why might a model perform well on a benchmark but fail on a similar task with different values?

- Concept: Variable perturbation and delexicalization
  - Why needed here: These techniques are the core mechanisms for creating contamination-resistant benchmarks
  - Quick check question: How does replacing specific numbers with variables help prevent memorization-based performance?

- Concept: Prompt engineering and evaluation frameworks
  - Why needed here: Proper evaluation setup is essential for measuring the true impact of variable perturbation
  - Quick check question: What role do different prompting strategies play in assessing model reasoning capabilities?

## Architecture Onboarding

- Component map: Variable Extraction -> Delexicalization -> Value Range Generation -> Dynamic Evaluation
- Critical path: Extracting variables from questions, generating delexicalized versions, defining value ranges, sampling new values for each evaluation
- Design tradeoffs: Balancing variable range complexity with need for diverse test cases; ensuring delexicalization doesn't alter semantic meaning
- Failure signatures: Models performing well despite perturbation indicate ranges too narrow or semantic contamination exists
- First 3 experiments:
  1. Test variable extraction and delexicalization on small subset to ensure semantic preservation
  2. Evaluate model performance on original vs. perturbed datasets to measure contamination impact
  3. Vary value ranges and prompting strategies to assess robustness and identify failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does variable perturbation effectively mitigate contamination across all language tasks, or is its impact limited to specific domains like mathematical reasoning?
- Basis in paper: Explicit - Shows significant impact on GSM8K but smaller drops on ARC and CommonsenseQA
- Why unresolved: Paper demonstrates varying effectiveness across task types without comprehensive analysis
- What evidence would resolve it: Comprehensive study across diverse language tasks including code generation, summarization, and question answering

### Open Question 2
- Question: What are the underlying reasons for performance differences between open-source and closed-source models on perturbed benchmarks?
- Basis in paper: Explicit - Notes closed-source models show smaller performance drops, speculates about scale or reasoning capabilities
- Why unresolved: Paper suggests but doesn't definitively explain the performance gap
- What evidence would resolve it: Analysis of training data and architectural differences between open-source and closed-source models

### Open Question 3
- Question: How robust are LLMs to different perturbation types beyond variable replacement?
- Basis in paper: Explicit - Compares variable perturbation to paraphrasing, shuffling, replacement; finds variable replacement most effective
- Why unresolved: Study focuses on limited perturbation methods without exploring full spectrum
- What evidence would resolve it: Systematic evaluation of LLM robustness to various perturbation techniques

### Open Question 4
- Question: What is the optimal balance between perturbation intensity and task difficulty for robust benchmarks?
- Basis in paper: Inferred - Discusses creating GSM+ through perturbation but doesn't address intensity vs. difficulty tradeoff
- Why unresolved: Excessive perturbation may create nonsensical questions while insufficient perturbation may not mitigate contamination
- What evidence would resolve it: Empirical study exploring relationship between perturbation intensity, task difficulty, and model performance

## Limitations

- Effectiveness may be limited for verbal reasoning tasks where variable ranges don't significantly alter semantic structure
- Does not fully address semantic-level contamination that may persist despite token-level changes
- Focuses primarily on mathematical and multiple-choice benchmarks, limiting generalizability to other task types
- Performance still depends on quality of variable extraction and appropriateness of value ranges

## Confidence

**High Confidence**: Claims about static benchmarks being contaminated and variable perturbation creating unique evaluation instances are well-supported by experimental results showing performance drops.

**Medium Confidence**: Assertion that variable perturbation provides more accurate assessment of true capabilities is supported but could be strengthened with more diverse benchmarks and clearer baselines.

**Low Confidence**: Claims about completely eliminating data contamination effects are not supported, as paper acknowledges some semantic-level contamination may persist.

## Next Checks

1. **Range Validation**: Systematically test how different value range sizes and distributions affect model performance to identify optimal settings balancing challenge and semantic preservation

2. **Cross-Task Generalization**: Apply variable perturbation to non-mathematical benchmarks (code generation, text summarization) to assess generalizability and identify task-specific limitations

3. **Semantic Contamination Analysis**: Design controlled experiments with paraphrased or semantically altered questions to quantify extent of semantic-level contamination persisting despite variable perturbation