---
ver: rpa2
title: 'Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation'
arxiv_id: '2409.12941'
source_url: https://arxiv.org/abs/2409.12941
tags:
- reasoning
- questions
- retrieval
- articles
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FRAMES, a novel evaluation dataset for Retrieval-Augmented
  Generation (RAG) systems that tests factuality, retrieval, and reasoning capabilities
  in an integrated framework. Unlike existing benchmarks that assess these abilities
  in isolation, FRAMES provides 824 challenging multi-hop questions requiring information
  integration from 2-15 Wikipedia articles, covering numerical, tabular, temporal,
  and post-processing reasoning types.
---

# Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID**: 2409.12941
- **Source URL**: https://arxiv.org/abs/2409.12941
- **Reference count**: 18
- **Primary result**: FRAMES dataset introduces 824 multi-hop questions testing factuality, retrieval, and reasoning in integrated framework

## Executive Summary
This paper introduces FRAMES, a novel evaluation dataset for Retrieval-Augmented Generation (RAG) systems that tests factuality, retrieval, and reasoning capabilities in an integrated framework. Unlike existing benchmarks that assess these abilities in isolation, FRAMES provides challenging multi-hop questions requiring information integration from 2-15 Wikipedia articles. The dataset includes quality controls such as temporal disambiguation and ensuring a large output space to prevent guesswork. Experiments with state-of-the-art LLMs show poor performance on single-step evaluations (0.408 accuracy for Gemini-Pro-1.5-0514 without retrieval), but significant improvement with multi-step retrieval pipelines (0.66 accuracy, >50% improvement).

## Method Summary
The paper proposes FRAMES, a dataset of 824 multi-hop questions requiring integration of information from Wikipedia articles. The evaluation uses three baseline approaches: naive prompting, BM25-retrieved prompts, and oracle prompts with ground truth articles. A multi-step retrieval and reasoning framework iteratively retrieves and reasons across multiple steps, generating search queries, retrieving relevant documents, and updating context. The system uses BM25 scoring for document retrieval and LLM-based auto-rating for answer evaluation.

## Key Results
- Single-step evaluation with Gemini-Pro-1.5-0514 achieves 0.408 accuracy without retrieval
- Multi-step retrieval pipeline achieves 0.66 accuracy (>50% improvement)
- Oracle prompt with all gold Wikipedia articles achieves 0.729 accuracy
- 6 non-parallelizable inference calls required per question in multi-step pipeline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-hop reasoning improves when models are forced to iteratively retrieve and reason across multiple steps
- **Mechanism**: The multi-step retrieval framework compels models to generate search queries, retrieve relevant documents, and update context iteratively. This prevents models from attempting to answer complex questions in a single step where they may lack necessary information
- **Core assumption**: Models can generate useful search queries when given appropriate instructions and examples
- **Evidence anchors**:
  - [abstract]: "The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (>50% improvement)"
  - [section 3.2]: "we observe a very promising trend with these changes, where the model performance (0.66) through iterations reaches close to the oracle performance (0.73) by the end of five iterations of retrievals"
  - [corpus]: Weak - no direct evidence in corpus papers about multi-step iterative reasoning improvements

### Mechanism 2
- **Claim**: Providing ground truth articles as context significantly improves model performance on complex reasoning tasks
- **Mechanism**: When all relevant Wikipedia articles are included in the prompt, models achieve higher accuracy by having access to all necessary facts, though they may still struggle with the reasoning component
- **Core assumption**: Models can effectively utilize provided context when it contains all necessary information for answering
- **Evidence anchors**:
  - [section 3.1]: "we observe an accuracy of 0.729 for Gemini-Pro-1.5-0514 when all the gold Wikipedia articles are provided in the context, which we call Oracle Prompt"
  - [abstract]: "the model failed to reason through the different facts to provide a correct answer to the question" even with oracle context
  - [corpus]: Weak - no direct evidence in corpus papers about oracle context performance

### Mechanism 3
- **Claim**: Temporal disambiguation and preventing guesswork through large output spaces improve dataset quality and evaluation reliability
- **Mechanism**: Adding temporal context to questions prevents ambiguity from changing information over time, while removing binary questions ensures models cannot achieve high accuracy through random guessing
- **Core assumption**: Temporal information and output space constraints are necessary for meaningful evaluation of RAG systems
- **Evidence anchors**:
  - [section 2]: "Annotators added extra context to disambiguate answers that could change over time" and "We removed questions with binary answers ('yes' or 'no') to prevent LLMs from achieving 50% accuracy through random guessing"
  - [abstract]: The dataset includes "quality controls such as temporal disambiguation and ensuring a large output space to prevent guesswork"
  - [corpus]: Weak - no direct evidence in corpus papers about temporal disambiguation or output space design choices

## Foundational Learning

- **Concept: Multi-hop reasoning**
  - **Why needed here**: FRAMES requires integrating information from 2-15 Wikipedia articles, which demands understanding how to chain reasoning across multiple documents
  - **Quick check question**: Can you explain how answering "How many years earlier would Punxsutawney Phil have to be canonically alive to have made a Groundhog Day prediction in the same state as the US capitol?" requires information from multiple sources?

- **Concept: Retrieval-augmented generation (RAG) architecture**
  - **Why needed here**: The paper evaluates how well models can retrieve relevant information and incorporate it into their generation process for complex questions
  - **Quick check question**: What are the key differences between single-step and multi-step RAG approaches, and why might multi-step be more effective for complex queries?

- **Concept: BM25 scoring and information retrieval**
  - **Why needed here**: The experiments use BM25 to retrieve Wikipedia articles based on generated queries, so understanding how this scoring works is essential for interpreting results
  - **Quick check question**: How does BM25 scoring determine which documents are most relevant to a given query, and what are its limitations for complex multi-hop questions?

## Architecture Onboarding

- **Component map**: Question generator -> Wikipedia article corpus -> Retrieval component (BM25) -> LLM model -> Auto-rating component -> Multi-step pipeline orchestrator

- **Critical path**:
  1. Receive question
  2. Generate search queries (k queries per step)
  3. Retrieve top n_docs articles per query using BM25
  4. Update context with new articles
  5. Repeat steps 2-4 for n iterations
  6. Generate final answer using complete context
  7. Auto-rate answer against ground truth

- **Design tradeoffs**:
  - Single-step vs multi-step: Single-step is faster but less effective; multi-step improves accuracy but increases computational cost
  - BM25 vs learned retrievers: BM25 is simpler and reproducible but may miss semantically relevant documents
  - Context length limits: Larger contexts improve recall but may exceed model capacity or increase computation
  - Query diversity: More diverse queries improve coverage but require better query generation strategies

- **Failure signatures**:
  - Low accuracy despite multi-step retrieval: Indicates poor query generation or retrieval component failure
  - High oracle accuracy but low single-step accuracy: Suggests models need better context utilization strategies
  - Inconsistent auto-rating results: May indicate issues with the evaluation prompt or ground truth quality
  - Computational inefficiency: May indicate need for parallelization or optimization of retrieval steps

- **First 3 experiments**:
  1. Replicate single-step evaluation with Gemini-Pro-1.5-0514 using naive prompt, BM25-R(n_docs=2), and BM25-R(n_docs=4) to establish baseline performance
  2. Implement multi-step pipeline with (k=3, n=3, n_docs=5) and compare against single-step results to measure improvement
  3. Test different query generation strategies (with vs without planning instructions) to determine optimal approach for multi-step reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can we effectively mitigate pretraining data contamination when evaluating RAG systems on Wikipedia-derived datasets?
- **Basis in paper**: Explicit
- **Why unresolved**: The paper acknowledges this as a significant limitation, noting that LLMs may have encountered Wikipedia information during pretraining, potentially inflating performance metrics. However, no concrete solutions are proposed beyond noting the issue.
- **What evidence would resolve it**: Development and testing of novel contamination detection methods, synthetic data generation approaches, or time-based filtering strategies that could isolate truly novel evaluation scenarios.

### Open Question 2
- **How can we reduce the computational cost of multi-step retrieval while maintaining or improving accuracy?**
- **Basis in paper**: Explicit
- **Why unresolved**: The paper shows that while multi-step retrieval improves accuracy (from 0.408 to 0.66), it requires 6 non-parallelizable inference calls per question, which is computationally expensive. The authors suggest this needs improvement but don't propose specific solutions.
- **What evidence would resolve it**: New retrieval architectures or pruning strategies that maintain high accuracy with fewer inference steps, demonstrated through controlled experiments comparing efficiency and performance.

### Open Question 3
- **How can we improve LLM reasoning capabilities for numerical, tabular, and post-processing tasks beyond simple fact retrieval?**
- **Basis in paper**: Explicit
- **Why unresolved**: The paper identifies that even with perfect retrieval (oracle setting), models struggle significantly with numerical, tabular, and post-processing reasoning tasks, with ~80% of errors in these categories. No specific solutions are proposed.
- **What evidence would resolve it**: Novel training methodologies, architectural modifications, or reasoning frameworks that demonstrate measurable improvements in these specific reasoning types while maintaining performance in other areas.

## Limitations
- The evaluation relies on LLM-based auto-rating for answer assessment, which introduces potential bias and inconsistency in scoring
- The dataset creation process includes quality controls, but specific human annotation guidelines and their impact on question difficulty remain underspecified
- The temporal disambiguation approach may introduce unintended constraints that limit question generality

## Confidence
- **High Confidence**: The core finding that multi-step retrieval significantly improves performance over single-step approaches is well-supported by the 50%+ accuracy improvement (0.408 to 0.66) across multiple models.
- **Medium Confidence**: The claim that models fail to reason effectively even with oracle context is supported but could benefit from deeper analysis of reasoning failures and error patterns.
- **Low Confidence**: The dataset quality improvements through temporal disambiguation and output space constraints are asserted but lack comparative analysis showing these features meaningfully improve evaluation reliability over existing benchmarks.

## Next Checks
1. Conduct ablation studies removing temporal disambiguation to measure its actual impact on model performance and question difficulty.
2. Implement human evaluation of a subset of auto-rated answers to quantify scoring reliability and identify systematic biases in the evaluation process.
3. Test the multi-step pipeline with learned retrievers (e.g., dense retrieval) versus BM25 to determine if retrieval quality improvements translate to better reasoning performance.