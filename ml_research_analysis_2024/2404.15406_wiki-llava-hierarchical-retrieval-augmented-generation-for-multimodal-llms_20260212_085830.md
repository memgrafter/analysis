---
ver: rpa2
title: 'Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs'
arxiv_id: '2404.15406'
source_url: https://arxiv.org/abs/2404.15406
tags:
- knowledge
- external
- arxiv
- llav
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Wiki-LLaVA is the first retrieval-augmented multimodal large language
  model that integrates external multimodal knowledge into LLaVA via a hierarchical
  retrieval pipeline. It retrieves relevant documents and passages from an external
  knowledge base using image and text queries, then provides them as additional context
  to the LLM.
---

# Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs

## Quick Facts
- arXiv ID: 2404.15406
- Source URL: https://arxiv.org/abs/2404.15406
- Authors: Davide Caffagni; Federico Cocchi; Nicholas Moratelli; Sara Sarto; Marcella Cornia; Lorenzo Baraldi; Rita Cucchiara
- Reference count: 40
- Primary result: Wiki-LLaVA is the first retrieval-augmented multimodal LLM that integrates external multimodal knowledge into LLaVA via hierarchical retrieval, showing up to 3.4 points accuracy improvement on InfoSeek dataset.

## Executive Summary
Wiki-LLaVA introduces the first retrieval-augmented multimodal large language model that integrates external multimodal knowledge into LLaVA through a hierarchical retrieval pipeline. The method retrieves relevant documents and passages from an external knowledge base using image and text queries, then provides them as additional context to the LLM. This enables the model to answer knowledge-intensive visual questions that require external data beyond its pre-trained knowledge.

The approach is evaluated on Encyclopedic-VQA and InfoSeek datasets, demonstrating significant accuracy improvements over standard LLaVA when relevant passages are retrieved. However, the performance is highly sensitive to retrieval quality, with larger knowledge bases degrading accuracy due to retrieval noise. Fine-tuning with a mix of dataset-specific and LLaVA-Instruct data helps preserve performance on standard multimodal benchmarks.

## Method Summary
Wiki-LLaVA integrates external multimodal knowledge into LLaVA through a hierarchical retrieval pipeline that combines image and text queries. The system first retrieves relevant documents from a large external knowledge base using image and text queries, then extracts relevant passages from these documents. These retrieved passages are provided as additional context to the LLM during inference. The model is fine-tuned on a combination of dataset-specific data and LLaVA-Instruct data to maintain performance on standard multimodal benchmarks while improving knowledge-intensive visual question answering capabilities.

## Key Results
- Wiki-LLaVA shows up to 3.4 points accuracy improvement on InfoSeek dataset using three retrieved passages
- Significant accuracy improvements over standard LLaVA when relevant passages are retrieved
- Performance highly sensitive to retrieval quality, with larger knowledge bases degrading accuracy due to retrieval noise
- Fine-tuning with mixed data helps preserve performance on standard multimodal benchmarks

## Why This Works (Mechanism)
The hierarchical retrieval pipeline enables Wiki-LLaVA to access external multimodal knowledge beyond its pre-trained capabilities. By retrieving relevant documents and passages based on both image and text queries, the system provides additional context to the LLM that helps answer knowledge-intensive visual questions. The integration of external knowledge through retrieval augmentation addresses the limitations of pre-trained multimodal models that are constrained by their original training data.

## Foundational Learning

**Multimodal Retrieval Augmentation**: Combines visual and textual information for knowledge retrieval - needed for integrating external knowledge into visual question answering; quick check: retrieval accuracy on relevant documents.

**Hierarchical Retrieval Pipeline**: Two-stage retrieval process (document retrieval followed by passage extraction) - needed to efficiently search large knowledge bases; quick check: retrieval precision at document and passage levels.

**Knowledge Base Scaling Effects**: Impact of knowledge base size on retrieval quality and model performance - needed to understand practical deployment limits; quick check: accuracy degradation vs. knowledge base size.

**Fine-tuning Strategies**: Balancing dataset-specific learning with pre-trained capabilities - needed to maintain general performance while improving specialized tasks; quick check: performance on standard vs. knowledge-intensive benchmarks.

## Architecture Onboarding

**Component Map**: Image Encoder -> Text Encoder -> Document Retriever -> Passage Extractor -> LLM with Context Augmentation -> Answer Generator

**Critical Path**: Image/Text Input → Document Retrieval → Passage Extraction → Context Generation → LLM Inference → Answer Output

**Design Tradeoffs**: The hierarchical retrieval adds computational overhead but enables access to external knowledge; retrieval quality vs. knowledge base size tradeoff affects accuracy; fine-tuning mix balances specialized and general capabilities.

**Failure Signatures**: Poor retrieval quality leads to degraded accuracy; large knowledge bases increase noise and reduce performance; insufficient fine-tuning mix can harm standard benchmark performance.

**First Experiments**: 1) Baseline LLaVA performance on knowledge-intensive tasks; 2) Retrieval accuracy at document and passage levels; 3) Accuracy sensitivity to number of retrieved passages.

## Open Questions the Paper Calls Out
None

## Limitations
- Strong dependence on retrieval quality, with larger knowledge bases degrading accuracy due to retrieval noise
- Evaluation limited to two specific datasets (Encyclopedic-VQA and InfoSeek) rather than broader multimodal tasks
- Unresolved trade-off between retrieval precision and knowledge base size
- Computational overhead of hierarchical retrieval pipeline not quantified

## Confidence
- High: Retrieval augmentation improves performance on knowledge-intensive visual QA when relevant information is retrieved
- High: Hierarchical retrieval pipeline design effectively integrates external knowledge
- High: Retrieval quality sensitivity demonstrated through experimental results
- Medium: Generalizability of method beyond evaluated datasets
- Low: Claims about computational efficiency and scalability

## Next Checks
1. Test accuracy degradation with varying knowledge base sizes to quantify retrieval noise effects
2. Evaluate performance on additional multimodal datasets beyond Encyclopedic-VQA and InfoSeek
3. Measure computational overhead of hierarchical retrieval pipeline compared to standard LLaVA