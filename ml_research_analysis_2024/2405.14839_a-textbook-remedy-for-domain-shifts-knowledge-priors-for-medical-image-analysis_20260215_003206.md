---
ver: rpa2
title: 'A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis'
arxiv_id: '2405.14839'
source_url: https://arxiv.org/abs/2405.14839
tags:
- datasets
- medical
- concept
- lesion
- skin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses domain shift problems in medical image classification,
  where models trained on one dataset often fail when applied to different hospitals
  or patient populations. The authors find that existing visual backbones lack appropriate
  priors for medical images, as evidenced by poor performance of untrained networks
  compared to pixels as features.
---

# A Textbook Remedy for Domain Shifts: Knowledge Priors for Medical Image Analysis

## Quick Facts
- **arXiv ID**: 2405.14839
- **Source URL**: https://arxiv.org/abs/2405.14839
- **Reference count**: 40
- **Primary result**: KnoBo achieves 32.4% improvement over fine-tuned models on confounded medical image datasets by incorporating textbook and PubMed-derived medical knowledge

## Executive Summary
This paper addresses the challenge of domain shift in medical image classification, where models trained on one dataset often fail when applied to different hospitals or patient populations. The authors demonstrate that existing visual backbones lack appropriate priors for medical images, as untrained networks perform no better than pixels as features. To address this, they propose Knowledge-enhanced Bottlenecks (KnoBo), which incorporates medical knowledge from textbooks and PubMed into concept bottleneck models. Across 20 datasets covering chest X-rays and skin lesions, KnoBo significantly outperforms fine-tuned models on confounded datasets, with PubMed-derived knowledge showing the best performance.

## Method Summary
KnoBo uses retrieval-augmented language models to generate interpretable medical concepts from textbook and PubMed sources, then trains classifiers to recognize these concepts from medical images. The approach constrains model reasoning to clinically relevant factors, reducing reliance on spurious correlations that cause domain shift failures. The method consists of a structure prior for concept generation, bottleneck predictor for concept grounding functions and linear layer, and parameter prior for knowledge-based regularization. KnoBo demonstrates that incorporating explicit medical knowledge significantly improves robustness to domain shifts while maintaining interpretability.

## Key Results
- KnoBo outperforms fine-tuned models on confounded datasets by 32.4% on average
- PubMed-derived knowledge sources outperform textbooks and Wikipedia on both diversity and prediction performance
- KnoBo ranks top-1 in eight datasets and second-best in the other two for out-of-domain accuracy
- The approach maintains strong performance on unconfounded datasets while excelling on domain-shifted data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Existing visual backbones lack appropriate priors for medical images, making them no more effective than pixels as features
- Mechanism: Deep networks trained on natural images develop priors that don't transfer to medical imaging domains where visual patterns differ fundamentally
- Core assumption: The deep image prior phenomenon that benefits natural image classification does not extend to medical domains
- Evidence anchors:
  - [abstract] "existing visual backbones lack an appropriate prior from the architecture for reliable generalization in these settings"
  - [section 3] "Across architecture, these untrained models are higher quality featurizers of natural images than directly using pixels as features. In contrast, across multiple medical modalities, the deep image prior in current major visual backbones is no more effective than using pixels (and often worse)"
  - [corpus] Weak - the paper only evaluates performance without investigating why the priors fail to transfer
- Break condition: If medical image visual patterns share sufficient similarity with natural images that deep network architectures develop useful transferable priors

### Mechanism 2
- Claim: Incorporating medical knowledge through concept bottlenecks improves robustness to domain shifts
- Mechanism: Concept bottleneck models constrain reasoning to clinically relevant factors, reducing reliance on spurious correlations
- Core assumption: Medical domain shifts often involve confounding variables that models can learn to ignore when reasoning with explicit medical concepts
- Evidence anchors:
  - [abstract] "KnoBo outperforms fine-tuned models on confounded datasets by 32.4% on average"
  - [section 6.1] "KnoBo outperforms baselines in OOD and domain-average accuracy by large margins, ranking top-1 in eight datasets and second-best in the other two"
  - [corpus] Weak - the paper shows correlation between knowledge incorporation and performance but doesn't establish causal mechanisms
- Break condition: If confounding variables are too complex or numerous for concept bottlenecks to capture effectively

### Mechanism 3
- Claim: Retrieval-augmented generation from medical corpora produces more diverse and useful concept bottlenecks than prompting alone
- Mechanism: Language models generate better concepts when grounded in relevant medical documents rather than generating from scratch
- Core assumption: Medical concepts require domain-specific context that generic prompting cannot provide
- Evidence anchors:
  - [abstract] "PubMed is a promising resource for making medical models less sensitive to domain shift, outperforming other resources on both diversity of information and final prediction performance"
  - [section 6.2] "The retrieval-augmented bottlenecks perform better than those generated by prompting, especially for skin lesions, where more specific knowledge is required because prompting lacks diversity"
  - [corpus] Strong - the paper provides quantitative comparisons across multiple knowledge sources
- Break condition: If the retrieval corpus contains insufficient relevant information for the target concepts

## Foundational Learning

- Concept: Domain shift and its impact on model generalization
  - Why needed here: The paper's core contribution addresses robustness to domain shifts in medical imaging
  - Quick check question: What distinguishes in-distribution from out-of-distribution evaluation in the context of medical imaging?

- Concept: Concept bottleneck models and their interpretability advantages
  - Why needed here: KnoBo builds upon concept bottleneck architecture to incorporate medical knowledge
  - Quick check question: How do concept bottleneck models differ from standard end-to-end classification models?

- Concept: Retrieval-augmented generation and its application to concept design
  - Why needed here: The paper uses RAG to generate concept bottlenecks from medical documents
  - Quick check question: What is the iterative process used to expand the concept bottleneck using RAG?

## Architecture Onboarding

- Component map: Structure Prior (medical corpus retrieval and concept generation) -> Bottleneck Predictor (concept grounding functions and linear layer) -> Parameter Prior (knowledge-based regularization)

- Critical path: 1) Retrieve relevant documents from medical corpus, 2) Generate concepts using LLM, 3) Train concept grounding functions on pretraining data, 4) Learn linear layer with parameter prior regularization

- Design tradeoffs: Using interpretable concepts improves robustness but requires access to pretraining data with image-text pairs; retrieval-augmented generation improves concept quality but adds computational overhead

- Failure signatures: Poor performance on confounded datasets indicates insufficient medical knowledge incorporation; reduced performance on unconfounded datasets suggests over-regularization or concept mismatch

- First 3 experiments:
  1. Verify deep image prior failure by comparing linear probe performance using features from untrained networks versus pixels on medical images
  2. Test concept grounding function accuracy by evaluating concept classification performance on pretraining data
  3. Compare OOD performance between KnoBo and fine-tuned baselines on confounded datasets to validate robustness improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different medical foundation models compare to KnoBo in terms of robustness to domain shifts across various medical imaging modalities?
- Basis in paper: [inferred] The paper discusses the potential of KnoBo to improve robustness to domain shifts by incorporating medical knowledge priors. However, it also mentions the limitations of current medical foundation models and the need for larger-scale pretraining data.
- Why unresolved: The paper does not directly compare KnoBo to other medical foundation models. It focuses on demonstrating the effectiveness of KnoBo's approach of using knowledge priors from medical documents.
- What evidence would resolve it: A comprehensive evaluation of KnoBo against state-of-the-art medical foundation models on a diverse set of medical imaging tasks with various domain shifts.

### Open Question 2
- Question: How can the concept grounding process in KnoBo be improved to handle rare or novel medical conditions with limited annotated data?
- Basis in paper: [explicit] The paper mentions that KnoBo assumes the availability of medical multimodal datasets, limiting applications to rare conditions. It also discusses the challenge of deriving visual concepts from text-only data, which can affect the effectiveness of grounding functions for lesion concepts.
- Why unresolved: The paper does not provide a detailed solution for handling rare or novel medical conditions. It suggests the need for larger-scale and higher-quality pretraining data but does not explore alternative approaches.
- What evidence would resolve it: Development and evaluation of novel concept grounding techniques that can effectively leverage limited annotated data for rare or novel medical conditions, potentially through few-shot learning or active learning approaches.

### Open Question 3
- Question: What is the impact of using different types of medical knowledge sources (e.g., clinical guidelines, research articles, patient records) on the performance and interpretability of KnoBo?
- Basis in paper: [explicit] The paper evaluates the performance of KnoBo using different knowledge sources, including PubMed, textbooks, and Wikipedia. It finds that PubMed outperforms other sources in terms of diversity of information and final prediction performance.
- Why unresolved: The paper does not explore the impact of using different types of medical knowledge sources on the interpretability of KnoBo. It also does not investigate the potential benefits of combining multiple knowledge sources or incorporating more diverse types of medical knowledge.
- What evidence would resolve it: A comprehensive analysis of the impact of different types of medical knowledge sources on the performance and interpretability of KnoBo, including a comparison of using individual sources versus combining multiple sources.

## Limitations

- Requires pretraining data with image-text pairs, limiting applicability to modalities without large-scale annotated datasets
- Performance gains on unconfounded datasets suggest potential over-regularization or concept mismatch in some cases
- Concept generation quality depends heavily on the coverage and relevance of medical knowledge sources used

## Confidence

- High confidence: The core claim that KnoBo improves robustness to domain shifts is well-supported by the experimental results across 20 datasets, showing consistent improvements in OOD accuracy
- Medium confidence: The mechanism by which medical knowledge incorporation improves robustness is demonstrated but not fully explained - the paper shows correlation but limited causal analysis of why specific concepts improve performance
- Medium confidence: The comparison between different knowledge sources (textbooks vs. PubMed) is robust, but the specific factors driving PubMed's superior performance require further investigation

## Next Checks

1. **Causal Analysis of Concept Selection**: Conduct ablation studies to identify which specific concepts contribute most to robustness improvements, and test whether removing these concepts reverts performance to baseline levels

2. **Cross-Modality Transfer**: Evaluate whether concepts generated for one medical imaging modality (e.g., chest X-rays) can be effectively applied to other modalities (e.g., skin lesions) to assess the generality of the approach

3. **Knowledge Source Quality Analysis**: Systematically analyze the relationship between the quality and relevance of medical knowledge sources and model performance by testing KnoBo with progressively more curated knowledge bases