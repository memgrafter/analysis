---
ver: rpa2
title: Recurrent Drafter for Fast Speculative Decoding in Large Language Models
arxiv_id: '2403.09919'
source_url: https://arxiv.org/abs/2403.09919
tags:
- beam
- tokens
- draft
- step
- morning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Recurrent Drafter (ReDrafter), an advanced
  speculative decoding approach that accelerates large language model (LLM) inference.
  The method leverages a recurrent neural network (RNN) as a draft model conditioned
  on LLM hidden states, uses beam search with dynamic tree attention to eliminate
  duplicated prefixes, and trains through knowledge distillation from the LLM.
---

# Recurrent Drafter for Fast Speculative Decoding in Large Language Models

## Quick Facts
- arXiv ID: 2403.09919
- Source URL: https://arxiv.org/abs/2403.09919
- Reference count: 11
- Primary result: Achieves 2.8x speedup on Nvidia H100 GPUs for Vicuna inference in MT-Bench

## Executive Summary
This paper introduces Recurrent Drafter (ReDrafter), an advanced speculative decoding approach that accelerates large language model (LLM) inference by using a recurrent neural network (RNN) as a draft model conditioned on LLM hidden states. The method employs beam search with dynamic tree attention to eliminate duplicated prefixes and trains through knowledge distillation from the LLM. ReDrafter achieves significant speedups of up to 2.8x on Nvidia H100 GPUs and demonstrates effectiveness across different hardware platforms, including Apple Silicon chips using MLX, where it achieves up to 2.3x speedup on Metal GPUs.

## Method Summary
ReDrafter accelerates LLM inference by using an RNN as a draft model that generates candidate tokens conditioned on the LLM's hidden states. The method implements beam search with dynamic tree attention to efficiently manage the search space while eliminating duplicated prefixes. The RNN draft model is trained through knowledge distillation from the LLM, learning to predict the LLM's output distribution. During inference, the draft model proposes candidate sequences, which are then verified by the slower but more accurate LLM. This approach balances the speed of the draft model with the quality assurance of the LLM, achieving substantial inference speedups while maintaining output quality.

## Key Results
- Achieves 2.8x speedup on Nvidia H100 GPUs for Vicuna inference in MT-Bench
- Demonstrates 2.3x speedup on Apple Silicon Metal GPUs using MLX framework
- Shows optimal beam width scaling with hardware capabilities (beam width=1 on M1 Max, beam width=50+ on A100/H100)

## Why This Works (Mechanism)
ReDrafter works by leveraging the complementary strengths of fast draft models and accurate verification models. The RNN draft model quickly generates candidate sequences based on the LLM's hidden states, while the dynamic tree attention mechanism efficiently manages the search space by eliminating duplicated prefixes. The knowledge distillation training process ensures the draft model learns to approximate the LLM's output distribution, making its predictions reliable enough for the verification step. This architecture creates a synergistic relationship where the draft model provides speed and the LLM provides quality assurance, resulting in significant overall speedup.

## Foundational Learning
- **Speculative decoding**: Why needed - to accelerate LLM inference by using faster draft models; Quick check - compare inference speed with and without speculative decoding
- **Knowledge distillation**: Why needed - to train the RNN draft model to approximate LLM output distribution; Quick check - measure KL divergence between draft model and LLM predictions
- **Dynamic tree attention**: Why needed - to efficiently manage search space and eliminate duplicated prefixes; Quick check - verify compression ratio and search space reduction
- **Beam search**: Why needed - to explore multiple candidate sequences simultaneously; Quick check - measure tokens accepted per step at different beam widths
- **Recurrent neural networks**: Why needed - to capture sequential dependencies in draft generation; Quick check - compare performance with other draft model architectures
- **Conditional generation**: Why needed - to generate drafts conditioned on LLM hidden states; Quick check - verify draft quality improves with better conditioning

## Architecture Onboarding

Component Map: LLM -> RNN Draft Model -> Beam Search with Dynamic Tree Attention -> Verification -> Output

Critical Path: Draft generation → Beam search with dynamic attention → Verification by LLM → Output selection

Design Tradeoffs: Speed vs accuracy tradeoff between draft model and LLM, beam width vs computational overhead, compression ratio vs search space coverage

Failure Signatures: Insufficient speedup due to verification overhead, out-of-memory errors with large beam widths, quality degradation from inaccurate draft predictions

First Experiments:
1. Verify dynamic tree attention correctly eliminates duplicated prefixes while maintaining output quality
2. Test draft model accuracy by measuring KL divergence between draft predictions and LLM outputs
3. Benchmark speedup at different beam widths to identify optimal configuration for target hardware

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal beam width for ReDrafter scale with GPU computational power across different hardware platforms?
- Basis in paper: The paper observes that optimal speedup occurs at beam width=1 on M1 Max and beam width=3 on M2 Ultra, while PyTorch experiments on A100/H100 show optimal beam width=50+.
- Why unresolved: The paper only compares three specific hardware configurations and doesn't provide a systematic study of how beam width optimization varies across a broader range of GPU capabilities.
- What evidence would resolve it: A comprehensive study testing ReDrafter across multiple GPU generations with varying computational power, mapping the relationship between GPU FLOPS and optimal beam width.

### Open Question 2
- Question: What is the theoretical limit of speedup achievable with speculative decoding when the draft model prediction accuracy approaches that of the LLM?
- Basis in paper: The paper shows speedups up to 2.8x on H100 and 2.3x on M2 Ultra, but doesn't analyze the fundamental limits of the approach as draft model accuracy improves.
- Why unresolved: The paper focuses on practical implementation but doesn't explore the theoretical performance ceiling of speculative decoding when draft models become nearly perfect predictors.
- What evidence would resolve it: Mathematical analysis deriving the speedup limit based on draft model accuracy, computational overhead, and verification costs.

### Open Question 3
- Question: How does dynamic tree attention performance scale with beam width and sequence length in terms of compression ratio?
- Basis in paper: The paper demonstrates 30-60% compression with beam widths 5-70 and fixed beam length=5, but doesn't explore how this scales with larger parameters.
- Why unresolved: The paper only provides a limited empirical study of compression ratios and doesn't analyze the scaling behavior for larger beam widths or sequence lengths.
- What evidence would resolve it: Systematic experiments measuring compression ratios across a wide range of beam widths (1-100+) and sequence lengths (1-20+), with analysis of the scaling patterns.

## Limitations
- Incomplete specification of RNN draft model training hyperparameters (learning rate, batch size, epochs)
- Lack of detailed implementation description for dynamic tree attention algorithm
- Potential computational overhead of beam search verification could offset speed gains

## Confidence
- High Confidence: Core methodology of using RNN as draft model conditioned on LLM hidden states
- Medium Confidence: Reported speedup metrics (2.3x-2.8x) and hardware platform consistency
- Medium Confidence: Effectiveness of knowledge distillation from LLM to RNN

## Next Checks
1. Implement ReDrafter with beam search and verify dynamic tree attention correctly eliminates duplicated prefixes while maintaining output quality
2. Train RNN draft model using knowledge distillation with various hyperparameters to identify optimal settings for maximum speedup
3. Conduct controlled experiments comparing ReDrafter performance against standard autoregressive decoding across different batch sizes and model scales