---
ver: rpa2
title: 'TUBench: Benchmarking Large Vision-Language Models on Trustworthiness with
  Unanswerable Questions'
arxiv_id: '2410.04107'
source_url: https://arxiv.org/abs/2410.04107
tags:
- unanswerable
- code
- image
- question
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces TUBench, a comprehensive benchmark designed
  to evaluate the trustworthiness of large vision-language models (LVLMs) when faced
  with unanswerable questions. The benchmark includes four diverse datasets: Unanswerable
  Code Reasoning (UCR), Unanswerable Visual Question Answering (UVQA), Unanswerable
  GeoQA (UGeoQA), and Unanswerable TabMWP (UTabMWP).'
---

# TUBench: Benchmarking Large Vision-Language Models on Trustworthiness with Unanswerable Questions

## Quick Facts
- arXiv ID: 2410.04107
- Source URL: https://arxiv.org/abs/2410.04107
- Reference count: 40
- Top-performing model (Gemini-1.5-Pro) achieves 69.2% average accuracy in determining question answerability

## Executive Summary
TUBench introduces a comprehensive benchmark for evaluating large vision-language models' (LVLMs) trustworthiness when confronted with unanswerable questions. The benchmark comprises four diverse datasets: Unanswerable Code Reasoning (UCR), Unanswerable Visual Question Answering (UVQA), Unanswerable GeoQA (UGeoQA), and Unanswerable TabMWP (UTabMWP), totaling 2,354 questions with balanced answerable and unanswerable splits. Through evaluation of 28 leading LVLMs, the study reveals significant challenges in model performance, particularly in identifying unanswerable questions and avoiding hallucinations. The benchmark demonstrates that even state-of-the-art models struggle with these fundamental trustworthiness challenges.

## Method Summary
The benchmark evaluation employs a systematic approach to assess LVLMs' ability to handle unanswerable questions across four distinct domains. Each dataset is carefully curated to contain both answerable and unanswerable questions, with answerable questions serving as controls. The evaluation framework measures two key aspects: the model's ability to correctly identify whether a question is answerable, and its tendency to hallucinate responses when faced with unanswerable questions. The study uses a consistent evaluation protocol across all 28 LVLMs, measuring performance metrics including accuracy in determining question answerability and hallucination rates.

## Key Results
- Current LVLMs struggle significantly with identifying unanswerable questions, with top model achieving only 69.2% average accuracy
- Substantial hallucination issues persist across all evaluated models when faced with unanswerable questions
- Performance varies considerably across different domains, with geometric reasoning and tabular reasoning showing particular challenges
- The benchmark reveals that existing LVLMs cannot reliably distinguish between answerable and unanswerable questions in many scenarios

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focus on a critical but underexplored aspect of LVLM trustworthiness: handling unanswerable questions. By creating controlled environments where the answerability status is known, the benchmark provides clear ground truth for evaluating model behavior. The diversity of question types across four domains ensures that models cannot rely on domain-specific heuristics, forcing them to develop more robust understanding of answerability. The inclusion of both answerable and unanswerable questions in each dataset allows for direct comparison of model behavior across different question types.

## Foundational Learning
1. **Vision-Language Model Architecture**: Understanding transformer-based architectures that process both visual and textual inputs is essential, as TUBench evaluates models' ability to integrate these modalities when determining answerability.
2. **Question Classification**: Knowledge of supervised and unsupervised classification techniques is needed to understand how models distinguish between answerable and unanswerable questions.
3. **Hallucination Detection**: Familiarity with hallucination metrics and detection methods is crucial for interpreting the benchmark's evaluation of model trustworthiness.

## Architecture Onboarding

**Component Map**: LVLM -> Question Input -> Answerability Classifier -> Response Generator -> Hallucination Detector

**Critical Path**: The evaluation pipeline processes questions through the LVLM, first assessing answerability classification, then examining the response quality to detect hallucinations.

**Design Tradeoffs**: The benchmark balances between controlled experimental conditions (known answerability) and ecological validity (diverse question types). This tradeoff ensures reliable measurement while maintaining relevance to real-world scenarios.

**Failure Signatures**: Models that incorrectly classify unanswerable questions as answerable, or that provide hallucinated responses to unanswerable questions, represent the primary failure modes identified by the benchmark.

**First 3 Experiments**: 1) Evaluate baseline performance on single-domain datasets to establish domain-specific challenges. 2) Test cross-domain generalization by training on one dataset and evaluating on others. 3) Examine the impact of prompt engineering on answerability detection accuracy.

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the main study.

## Limitations
- Benchmark covers only four specific domains (code, visual QA, geographic QA, and tabular reasoning), potentially missing other unanswerable question scenarios
- Relatively small total dataset size (2,354 questions) may limit statistical power and generalizability
- Focus on English-language questions may miss cultural and linguistic variations in how unanswerable questions manifest

## Confidence

**Major claims about current LVLMs struggling with unanswerable questions**: **High confidence** - This aligns with existing literature on model hallucination and is supported by evaluation of 28 diverse models.

**Claims about specific performance metrics (e.g., 69.2% accuracy for Gemini-1.5-Pro)**: **Medium confidence** - While the evaluation methodology appears sound, the limited dataset size and potential question selection bias could affect precision.

**Claims about benchmark comprehensiveness and difficulty**: **Medium confidence** - The four datasets show diversity, but coverage may be insufficient to claim comprehensive difficulty assessment across all LVLM applications.

## Next Checks

1. Replicate the benchmark evaluation with additional LVLMs and expanded question sets across more diverse domains to test generalizability
2. Conduct human evaluation studies to verify the unanswerable nature of questions and assess ground truth accuracy
3. Test model performance on cross-dataset generalization - whether models that perform well on one unanswerable question type can transfer to others