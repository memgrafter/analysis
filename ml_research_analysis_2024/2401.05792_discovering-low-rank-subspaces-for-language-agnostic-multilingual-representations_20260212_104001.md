---
ver: rpa2
title: Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations
arxiv_id: '2401.05792'
source_url: https://arxiv.org/abs/2401.05792
tags:
- language
- lsar
- languages
- original
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents LSAR, a method to discover and remove language-specific
  factors from multilingual embeddings to improve language-agnostic semantic tasks.
  The core idea is to identify a low-rank subspace encoding language-specific signals
  via singular value decomposition on monolingual corpora, then project embeddings
  into the null space to remove these factors without finetuning.
---

# Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations

## Quick Facts
- arXiv ID: 2401.05792
- Source URL: https://arxiv.org/abs/2401.05792
- Reference count: 40
- Key outcome: LSAR method improves cross-lingual retrieval and classification tasks by removing language-specific syntactic signals from multilingual embeddings

## Executive Summary
This paper introduces LSAR, a method that discovers and removes language-specific factors from multilingual embeddings to improve language-agnostic semantic tasks. The approach uses singular value decomposition on monolingual corpora to identify a low-rank subspace encoding language-specific signals, then projects embeddings into the null space to remove these factors without requiring any finetuning. LSAR consistently improves performance across multiple cross-lingual tasks and multilingual language models, with up to 19% relative gains on Tatoeba sentence retrieval.

## Method Summary
LSAR identifies language-specific signals in multilingual embeddings by computing mean embeddings across languages from monolingual corpora, then applying SVD to find the low-rank subspace that captures language identity information. The method projects original embeddings into the null space of this subspace to remove language-specific factors while preserving semantic content. The approach requires no labeled data or finetuning, making it broadly applicable across different multilingual models and tasks. The key parameter is the rank r, which determines how many language-specific components to remove.

## Key Results
- LSAR achieves up to 19% relative improvement on Tatoeba sentence retrieval task
- Consistent gains across multiple multilingual models (mBERT, XLM, XLM-R, LABSE)
- Removes language-specific syntactic signals while preserving semantic meaning
- Outperforms LABSE on cross-lingual retrieval despite LABSE using supervised parallel data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Low-rank subspace captures language-specific syntactic signals
- **Mechanism:** SVD decomposition on mean embeddings across languages isolates directions of maximal variance in language identity
- **Core assumption:** Language-specific signals manifest as shared variance patterns across languages' mean embeddings
- **Evidence anchors:**
  - [abstract] "we discover that there exists a low-rank subspace that primarily encodes information irrelevant to semantics (e.g., syntactic information)"
  - [section] "we observe that the subspace encodes a great deal of syntactic information"
- **Break condition:** If language-specific variance is distributed across full space rather than concentrated in low-rank directions

### Mechanism 2
- **Claim:** Projecting onto null space removes language-specific signals while preserving semantic content
- **Mechanism:** Orthogonal projection eliminates components aligned with language-specific subspace, retaining only language-agnostic semantic directions
- **Core assumption:** Semantic content is approximately orthogonal to language-specific syntactic signals
- **Evidence anchors:**
  - [abstract] "we can directly project the original embeddings into the null space to boost language agnosticism without finetuning"
  - [section] "the larger r is, the more language-specific signals we can identify"
- **Break condition:** If semantic content shares basis vectors with language-specific subspace, causing information loss

### Mechanism 3
- **Claim:** Cross-lingual semantic similarity improves when language identity signals are suppressed
- **Mechanism:** Removing language-specific variance allows embeddings from different languages with similar meanings to cluster together
- **Core assumption:** Semantic similarity is stronger signal than language identity for cross-lingual tasks
- **Evidence anchors:**
  - [abstract] "empirical results show that applying our method consistently leads to improvements over commonly used ML-LMs"
  - [section] "empirical results show that applying our method consistently leads to improvements over commonly used ML-LMs"
- **Break condition:** If language identity is actually useful signal for the downstream task

## Foundational Learning

- **Concept:** Singular Value Decomposition (SVD) decomposition
  - Why needed here: Core mathematical operation to identify low-rank subspace structure
  - Quick check question: What property of SVD makes it suitable for finding directions of maximal variance in data?
- **Concept:** Null space projection
  - Why needed here: Mathematical operation to remove identified language-specific components
  - Quick check question: How does projecting onto null space differ from simple subtraction of mean embeddings?
- **Concept:** Cross-lingual semantic similarity
  - Why needed here: Target property that should improve after language-specific signal removal
  - Quick check question: Why might removing language identity actually help semantic retrieval tasks?

## Architecture Onboarding

- **Component map:** Monolingual corpus -> Mean embedding calculator -> SVD decomposition engine -> Null space projector -> Evaluation pipeline
- **Critical path:** Extract embeddings -> Compute mean embeddings -> SVD decomposition -> Null space projection -> Evaluate downstream tasks
- **Design tradeoffs:**
  - Rank r vs. information retention: Higher r removes more language-specific signals but risks semantic loss
  - Monolingual corpus quality vs. subspace identification: Poor corpora may not capture true language-specific patterns
  - Computational cost vs. accuracy: Full SVD vs. approximate methods
- **Failure signatures:**
  - Performance degradation on semantic tasks (over-removal of information)
  - Minimal improvement in cross-lingual retrieval (subspace not capturing relevant signals)
  - Language clustering persists in embeddings (projection insufficient)
- **First 3 experiments:**
  1. Apply LSAR with r=1 on Tatoeba and measure retrieval accuracy changes
  2. Vary r from 1 to L-1 and plot performance curve to find optimal rank
  3. Visualize language clustering before/after projection using t-SNE to verify language-agnosticism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of components (r) to remove for language-agnostic representations across different tasks and models?
- Basis in paper: [explicit] The paper mentions that LSAR consistently achieves best results with r = l-1 (number of languages minus one), but also notes that classification tasks are more sensitive to semantic information and may require fewer components removed.
- Why unresolved: The paper only tests a few values of r (1, 2, and l-1) and shows that optimal values vary by task and model. A more comprehensive analysis across different domains and model architectures could reveal more nuanced patterns.
- What evidence would resolve it: Systematic experiments varying r across diverse tasks (semantic, syntactic, sentiment analysis), model families (BERT, RoBERTa, T5), and languages with different typological properties would identify generalizable patterns for optimal r.

### Open Question 2
- Question: What specific syntactic information is encoded in the language-specific subspace, and how does it relate to universal grammar theories?
- Basis in paper: [explicit] The paper shows the subspace is highly correlated with syntactic language similarities from the URIEL database, but does not identify which specific syntactic features drive this correlation.
- Why unresolved: While the paper demonstrates the subspace captures syntactic information, it does not break down which particular syntactic dimensions (e.g., word order, morphological complexity, agreement patterns) contribute most to the language-specific signal.
- What evidence would resolve it: Correlation analysis between the subspace basis vectors and specific typological features from databases like WALS, combined with controlled experiments manipulating individual syntactic properties in synthetic data, would identify the key syntactic dimensions encoded.

### Open Question 3
- Question: How does LSAR's unsupervised approach compare to supervised methods that use parallel data or translation pairs for alignment?
- Basis in paper: [explicit] The paper mentions LABSE as a supervised baseline but only briefly notes that LSAR's improvements over it are marginal, without detailed comparison of when each approach excels.
- Why unresolved: The paper demonstrates LSAR's effectiveness without parallel data, but does not systematically compare its performance to supervised alignment methods across different scenarios (low-resource languages, distant language pairs, specialized domains).
- What evidence would resolve it: Head-to-head comparisons of LSAR versus supervised alignment methods across diverse language pairs, resource conditions, and downstream tasks would reveal the trade-offs between data requirements and performance, potentially identifying domains where unsupervised methods are preferable.

## Limitations
- The claim that the identified subspace "primarily encodes syntactic information" lacks rigorous quantitative validation
- Method's performance gains are demonstrated primarily on sentence retrieval and classification tasks
- Choice of rank r appears somewhat arbitrary without systematic justification
- Assumption that semantic content is orthogonal to language-specific syntactic signals is not rigorously tested

## Confidence

- **High Confidence:** The core methodology (SVD-based subspace identification and null space projection) is mathematically sound and the implementation details are clearly specified.
- **Medium Confidence:** Empirical improvements on Tatoeba, LAReQA, and Amazon Reviews are demonstrated, but the attribution of gains specifically to syntactic signal removal versus other factors remains uncertain.
- **Low Confidence:** The claim that the identified subspace "primarily encodes syntactic information" lacks rigorous quantitative validation beyond indirect evidence.

## Next Checks

1. **Subspace Content Analysis:** Conduct systematic quantitative analysis of what linguistic features the low-rank subspace captures by correlating subspace directions with specific syntactic properties (POS tags, dependency structures) and semantic properties (word embeddings, sentence similarity) across multiple languages.

2. **Rank Sensitivity Study:** Systematically vary the rank parameter r from 1 to L-1 across all tested models and measure the trade-off curve between language-specific signal removal (measured via NMI) and semantic task performance to identify optimal rank selection strategies.

3. **Cross-Lingual Semantic Preservation Test:** Design controlled experiments where semantically similar sentences in different languages are embedded before and after LSAR projection, then measure both cross-lingual similarity improvements and potential semantic degradation through human evaluation or semantic probing tasks.