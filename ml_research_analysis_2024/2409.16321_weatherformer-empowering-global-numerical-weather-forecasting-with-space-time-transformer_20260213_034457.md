---
ver: rpa2
title: 'WeatherFormer: Empowering Global Numerical Weather Forecasting with Space-Time
  Transformer'
arxiv_id: '2409.16321'
source_url: https://arxiv.org/abs/2409.16321
tags:
- weather
- states
- weatherformer
- prediction
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes WeatherFormer, a data-driven numerical weather
  prediction framework based on space-time transformer blocks. The method introduces
  a Position-aware Adaptive Fourier Neural Operator (PAFNO) to model spatio-temporal
  weather dynamics while encoding positional information.
---

# WeatherFormer: Empowering Global Numerical Weather Forecasting with Space-Time Transformer

## Quick Facts
- arXiv ID: 2409.16321
- Source URL: https://arxiv.org/abs/2409.16321
- Authors: Junchao Gong; Tao Han; Kang Chen; Lei Bai
- Reference count: 11
- Achieves RMSE of 181/366 for Z500 (3/5 days) and ACC of 0.99/0.96 on WeatherBench

## Executive Summary
WeatherFormer introduces a novel transformer-based framework for data-driven numerical weather prediction that combines space-time factorization with position-aware adaptive Fourier neural operators. The framework achieves state-of-the-art performance on WeatherBench, outperforming existing deep learning methods and approaching the accuracy of advanced physical models. Key innovations include an SF-Block that factorizes spatial and temporal attention to reduce computational complexity, and PAFNO that encodes positional information efficiently in the frequency domain.

## Method Summary
WeatherFormer is a transformer-based numerical weather prediction framework that processes weather data through a series of space-time transformer blocks. The core architecture features SF-Blocks that separate spatial and temporal processing using PAFNO for efficient token mixing, followed by a convolutional decoder. The model is trained on the WeatherBench dataset with 32×64 resolution, using 6-hour time intervals and 69 dynamic weather states plus 2 constant variables. Two augmentation strategies—Earth Rotation and noise augmentation—are employed to enhance performance and reduce training costs.

## Key Results
- WeatherFormer achieves RMSE of 181/366 for Z500 (3/5 days) and ACC of 0.99/0.96 on WeatherBench
- Outperforms all existing deep learning methods for data-driven NWP on the benchmark
- Approaches the accuracy of advanced physical models while using only historical data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Space-time factorization reduces computational complexity while preserving spatiotemporal modeling capability
- Mechanism: The SF-Block splits self-attention into spatial and temporal mixers, reducing complexity from O(B²T²HW) to O(BT²HW) + O(BTHW²)
- Core assumption: Temporal and spatial dependencies can be modeled independently without significant information loss
- Evidence anchors: [section] "our SF-Block factorizes the self-attention module in a traditional transformer block by introducing a spatial mixer and a temporal mixer to separately model the spatial and temporal relationship"

### Mechanism 2
- Claim: PAFNO introduces position information without quadratic parameter growth
- Mechanism: PAFNO uses learnable coefficients λn that weight frequency filters, encoding relative positions while maintaining O(N) parameters
- Core assumption: Position information can be effectively encoded in frequency domain weights rather than positional embeddings
- Evidence anchors: [section] "we employ a set of learnable position-related coefficients {λn}N n=1 to assign a coefficient for the token at each position"

### Mechanism 3
- Claim: Data augmentation strategies improve generalization and reduce training costs
- Mechanism: Earth rotation augmentation exploits rotational equivariance; noise augmentation simulates prediction errors and reduces error accumulation
- Core assumption: Weather data exhibits rotational symmetry and error patterns that can be simulated
- Evidence anchors: [section] "earth rotation augmentation is applied to exploit rotation equivariance and noise augmentation to obtain a comparable multi-step performance with half of training consumption"

## Foundational Learning

- Concept: Discrete Fourier Transform (DFT) and Inverse DFT (IDFT)
  - Why needed here: PAFNO relies on transforming tokens to frequency domain for efficient mixing, then back to spatial domain
  - Quick check question: What is the computational complexity of DFT/IDFT compared to naive token mixing?

- Concept: Self-attention mechanism in transformers
  - Why needed here: Understanding the baseline that SF-Block factorizes; knowing how position embeddings normally work
  - Quick check question: How does standard self-attention scale with sequence length and what are its memory requirements?

- Concept: WeatherBench dataset structure and evaluation metrics
  - Why needed here: WeatherFormer is evaluated on WeatherBench; understanding RMSE and ACC calculations is crucial
  - Quick check question: How does latitude weighting work in WeatherBench's RMSE calculation and why is it used?

## Architecture Onboarding

- Component map: Input (weather states) -> Patch embedding -> SF-Blocks (spatial mixer + temporal mixer with PAFNO) -> Convolution decoder -> Output (predicted weather states)
- Critical path: Tokenization -> SF-Block processing -> Position encoding -> Output generation
- Design tradeoffs: SF-Block factorization trades some modeling fidelity for reduced computation; PAFNO trades position modeling quality for parameter efficiency
- Failure signatures: Poor long-term predictions suggest error accumulation; spatially inconsistent predictions suggest position encoding issues
- First 3 experiments:
  1. Test SF-Block alone (without PAFNO) to verify factorization benefits
  2. Test PAFNO with fixed coefficients vs learned coefficients to validate position encoding
  3. Test with and without augmentations to quantify their impact on generalization

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Limited to the WeatherBench dataset which may not fully capture real-world weather forecasting complexities
- SF-Block factorization assumes temporal and spatial dependencies can be modeled independently
- PAFNO's position encoding relies on learned coefficients that may not generalize across different weather patterns

## Confidence
- **High confidence** in core architecture's ability to achieve state-of-the-art performance on WeatherBench
- **Medium confidence** in generalizability of PAFNO's position encoding approach
- **Medium confidence** in effectiveness of augmentation strategies

## Next Checks
1. Conduct ablation study of augmentation strategies by training WeatherFormer with Earth Rotation only, noise augmentation only, and without any augmentations
2. Evaluate WeatherFormer on a different weather dataset (e.g., ERA5) to assess cross-dataset generalization
3. Analyze error propagation by comparing WeatherFormer versus baselines over extended prediction horizons (7-10 days)