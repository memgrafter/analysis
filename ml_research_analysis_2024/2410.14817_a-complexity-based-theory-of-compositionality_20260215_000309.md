---
ver: rpa2
title: A Complexity-Based Theory of Compositionality
arxiv_id: '2410.14817'
source_url: https://arxiv.org/abs/2410.14817
tags:
- compositionality
- complexity
- representation
- language
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a formal, information-theoretic definition
  of compositionality called "representational compositionality," which addresses
  the lack of a rigorous, measurable mathematical framework for compositionality in
  AI and cognitive science. The key idea is that a compositional representation is
  both expressive and can be significantly compressed as a simple function of discrete
  constituent parts, analogous to sentences in natural language.
---

# A Complexity-Based Theory of Compositionality

## Quick Facts
- arXiv ID: 2410.14817
- Source URL: https://arxiv.org/abs/2410.14817
- Reference count: 34
- Key outcome: Introduces "representational compositionality" as a formal, information-theoretic definition measuring how well representations can be compressed as a function of discrete constituent parts

## Executive Summary
This paper addresses the lack of a rigorous mathematical framework for measuring compositionality in AI and cognitive science by introducing a complexity-based definition called "representational compositionality." The key insight is that a compositional representation is one that can be significantly compressed as a function of discrete constituent parts, analogous to how sentences in natural language combine words. The authors formalize this using Kolmogorov complexity, measuring the ratio K(Z)/K(Z|W) where Z is the representation and W is a compressed description using discrete symbolic sequences. Through experiments on synthetic and real-world data, they validate their definition and demonstrate its ability to distinguish between compositional and non-compositional representations.

## Method Summary
The paper defines representational compositionality using Kolmogorov complexity to measure how well representations can be compressed as functions of discrete constituent parts. The core metric is C(Z) = K(Z)/K(Z|W), where K(Z) is the complexity of the representation and K(Z|W) is the complexity of the representation given a compressed description using discrete sentences. To make this tractable, the authors propose using discrete auto-encoders with learned priors to estimate the necessary complexity terms, combined with prequential coding for complexity estimation. They validate their approach on synthetic datasets with controlled parameters, emergent languages from multi-agent training, and natural language systems across five languages.

## Key Results
- The compositionality measure successfully distinguishes between compositional and non-compositional representations on synthetic lookup table data
- Compositionality increases with disentanglement, sentence length, and expressivity in controlled synthetic experiments
- Emergent languages developed through iterated learning show significantly higher compositionality than those without iterated learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compositionality is measured through compressibility of representations as a function of discrete constituent parts.
- Mechanism: Representational compositionality is formalized as a ratio K(Z)/K(Z|W), where K(Z) is the complexity of the representation and K(Z|W) is the complexity of the representation given a compressed description using discrete sentences. Representations that can be significantly compressed using simple semantics (low K(Z|W)) are more compositional.
- Core assumption: The shortest program that outputs a compositional representation has a particular form involving discrete symbolic sequences (sentences) and a simple semantics function.
- Evidence anchors:
  - [abstract]: "Intuitively, representational compositionality states that a compositional representation satisfies three properties... Second, it must be possible to re-describe the representation as a function of discrete symbolic sequences with re-combinable parts... Third, the function that relates these symbolic sequences to the representation—analogous to semantics in natural language—must be simple."
  - [section]: "We will argue that a natural way to think about compositional representations is: representations Z that can be significantly compressed as a function of constituent parts."
- Break condition: If the assumed program form for compressing representations is incorrect, the estimated K(Z) will be far greater than the true Kolmogorov complexity, leading to inaccurate compositionality measurements.

### Mechanism 2
- Claim: The simplicity of the semantics function f is central to compositionality, explaining systematicity and generalization.
- Mechanism: Representational compositionality increases when the semantics function f mapping discrete sentences to high-dimensional representations is simple (low K(f)). Simple semantics enable systematic generalization to novel combinations of known concepts.
- Core assumption: Simple semantics functions (low K(f)) correspond to structure-preserving mappings that maintain pairwise distances between sentences and their representations.
- Evidence anchors:
  - [abstract]: "Third, the function that relates these symbolic sequences to the representation—analogous to semantics in natural language—must be simple."
  - [section]: "If a model is compositional with respect to a set of features in its training data, it need not observe all possible combinations of those features in order to generalize to novel ones."
- Break condition: If the semantics function is complex or arbitrary (high K(f)), the compositionality will be low regardless of the representation's expressivity.

### Mechanism 3
- Claim: Compositionality emerges from competing pressures for expressivity and compression.
- Mechanism: Representational compositionality balances the expressivity of the representation (high K(Z)) with its compressibility as a function of constituent parts (low K(Z|W)). Highly expressive representations that can be compressed using simple semantics are most compositional.
- Core assumption: There exists a trade-off between the expressivity of representations and their compressibility using discrete symbolic sequences.
- Evidence anchors:
  - [abstract]: "Representational compositionality therefore formalizes a hypothesis in cognitive science that compositionality emerges from competing pressures for expressivity and compression."
  - [section]: "When a representation is highly expressive (high K(Z)) but can nevertheless be compressed as a simple function of constituent parts (low K(Z|W)), representational compositionality says that the representation is highly compositional."
- Break condition: If either the expressivity is too low (low K(Z)) or the compressibility is too difficult (high K(Z|W)), the compositionality will be low.

## Foundational Learning

- Concept: Kolmogorov complexity and algorithmic information theory
  - Why needed here: The definition of representational compositionality relies on Kolmogorov complexity to measure the compressibility of representations and semantics functions.
  - Quick check question: What is the relationship between Kolmogorov complexity and compression? (Answer: Kolmogorov complexity measures the length of the shortest program that can generate an object, which is directly related to how compressible that object is.)

- Concept: Context-free grammars and hierarchical structure
  - Why needed here: The paper uses context-free grammars to generate synthetic representations and demonstrates how grammar complexity affects compositionality.
  - Quick check question: How does the width and depth of a context-free grammar relate to its complexity? (Answer: Increasing the width or depth of a context-free grammar increases the number of production rules needed, which increases the complexity of the semantics function f and thus decreases compositionality.)

- Concept: Emergent communication and iterated learning
  - Why needed here: The paper applies the compositionality definition to language systems that emerge in multi-agent settings, particularly those that develop through iterated learning.
  - Quick check question: What is iterated learning and how does it affect compositionality in emergent languages? (Answer: Iterated learning is a process where model parameters are periodically reset and retrained on data from previous generations, creating an inductive bias for simpler, more compositional language systems.)

## Architecture Onboarding

- Component map: Data (Z, W) -> Encoder e(z) -> Discrete sentences W -> Decoder f(w) -> Representations Z
- Critical path: 1) Obtain representations Z and sentences W, 2) Train discrete auto-encoder, 3) Estimate complexity terms using prequential coding, 4) Calculate compositionality C(Z) = K(Z)/K(Z|W)
- Design tradeoffs:
  - Discrete vs. continuous representations: Current framework focuses on discrete symbolic sequences, but continuous compositionality could be explored
  - Model complexity vs. estimation accuracy: Simpler models are easier to estimate but may not capture complex compositional structures
  - Sentence structure vs. flexibility: Fixed sentence structures enable better estimation but may limit the types of compositional representations that can be measured
- Failure signatures:
  - High K(Z) with low C(Z): Representation is expressive but cannot be compressed using discrete sentences (lacks compositional structure)
  - Low K(Z) with high C(Z): Representation is simple but the measure may not capture true compositionality (e.g., constant representations)
  - Inconsistent C(Z) across seeds: Estimation instability or sensitivity to initialization
- First 3 experiments:
  1. Implement discrete auto-encoder on synthetic lookup table representations and verify C(Z) matches intuition (increasing with disentanglement, sentence length, and expressivity)
  2. Apply compositionality definition to context-free grammar representations and test relationship between grammar complexity and C(Z)
  3. Measure C_L(Z) for emergent languages from multi-agent training with and without iterated learning to reproduce compositionality differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the compositional decomposition of Z as a function of discrete symbolic sequences W across different choices of programming languages or computational frameworks?
- Basis in paper: [inferred] The paper discusses that Kolmogorov complexity depends on the programming language, but argues that differences are bounded by a constant independent of the object. However, this assumption needs empirical validation.
- Why unresolved: The paper does not empirically test the sensitivity of the compositionality measure to different computational frameworks or programming languages.
- What evidence would resolve it: Experiments comparing compositionality scores across multiple programming languages or computational frameworks (e.g., Python, C++, Turing machines) for the same representations would demonstrate the stability of the measure.

### Open Question 2
- Question: Can the representational compositionality metric be reliably applied to representations from real-world neural networks without predefined symbolic mappings, and what are the practical challenges in estimating K(Z) in such cases?
- Basis in paper: [explicit] The paper acknowledges that measuring C(Z) requires finding pw, W, and f that jointly minimize K(Z), which is intractable, and outlines potential strategies using deep learning tools.
- Why unresolved: The paper provides theoretical arguments and synthetic experiments but does not demonstrate the practical feasibility of applying the metric to real-world representations without predefined symbolic mappings.
- What evidence would resolve it: Successful application of the metric to representations from diverse neural networks (e.g., vision, language models) without predefined symbolic mappings, along with a detailed analysis of the practical challenges and limitations encountered.

### Open Question 3
- Question: How does the representational compositionality of natural language systems compare across languages with different grammatical structures, and what insights can be gained about the nature of compositionality in human cognition?
- Basis in paper: [explicit] The paper applies the metric to natural language systems (English, French, Spanish, German, Japanese) and finds roughly equivalent compositionality, with Japanese having slightly higher relative compositionality.
- Why unresolved: The paper uses sentence embeddings as proxies for meanings, which may not fully capture the nuances of human thought, and the sample size of languages studied is limited.
- What evidence would resolve it: A larger-scale study comparing the compositionalities of diverse natural languages using more direct measures of meaning (e.g., brain activity patterns) and incorporating a wider range of languages with different grammatical structures would provide deeper insights into the relationship between compositionality and human cognition.

## Limitations

- The theoretical framework relies on Kolmogorov complexity, which is uncomputable in general, requiring approximation methods that may not capture true compositionality
- The definition focuses on discrete symbolic sequences, potentially limiting applicability to continuous or structured representations like graphs or images
- The practical challenges of applying the metric to real-world neural networks without predefined symbolic mappings remain largely unexplored

## Confidence

- **High Confidence**: The core mathematical framework and experimental validation on synthetic data are sound. The distinction between compositional and non-compositional representations is well-supported.
- **Medium Confidence**: The extension to natural language and emergent communication systems shows promising results but relies on specific implementation choices that may affect the outcomes.
- **Low Confidence**: The theoretical guarantees for the estimation methods and their behavior on real-world, complex representations require further investigation.

## Next Checks

1. **Robustness to Compression Schemes**: Test alternative compression methods (e.g., different auto-encoder architectures, continuous compression) and verify that compositionality estimates remain consistent across methods.
2. **Generalization to Structured Data**: Apply the framework to structured representations like graphs or images and validate whether the compositionality measures align with human intuitions about structure and modularity.
3. **Scaling Analysis**: Investigate how compositionality estimates behave as representation dimensionality, dataset size, and vocabulary size increase, and determine whether the measures remain meaningful in high-dimensional settings.