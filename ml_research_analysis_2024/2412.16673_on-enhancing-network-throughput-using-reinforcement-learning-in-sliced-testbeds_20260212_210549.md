---
ver: rpa2
title: On Enhancing Network Throughput using Reinforcement Learning in Sliced Testbeds
arxiv_id: '2412.16673'
source_url: https://arxiv.org/abs/2412.16673
tags:
- network
- throughput
- rate
- learning
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the eMBB-Agent, a reinforcement learning approach
  that enhances network slicing throughput to meet Service-Level Agreements (SLAs).
  The agent uses Deep Q-Networks (DQN) to analyze application transmission variables
  and adjust the reception window in a discrete action space.
---

# On Enhancing Network Throughput using Reinforcement Learning in Sliced Testbeds

## Quick Facts
- arXiv ID: 2412.16673
- Source URL: https://arxiv.org/abs/2412.16673
- Reference count: 4
- Primary result: eMBB-Agent uses DQN to enhance network slicing throughput, with simpler models (NN-2) achieving faster convergence and better performance than deeper models

## Executive Summary
This paper presents the eMBB-Agent, a reinforcement learning approach that enhances network slicing throughput to meet Service-Level Agreements (SLAs). The agent uses Deep Q-Networks (DQN) to analyze application transmission variables and adjust the reception window in a discrete action space. Experimental results demonstrate that DQN model complexity inversely correlates with network throughput optimization efficiency, with simpler models achieving faster convergence times and better performance, particularly in error-prone channels.

## Method Summary
The eMBB-Agent is implemented using NS3 network simulator with two FTP applications on 10 Mbps and 2 Mbps links. The DQN agent observes network state variables (congestion window, packet size, etc.) and takes discrete actions to modify the reception window. Three DQN architectures (NN-2, NN-4, NN-8) with learning rates of 0.01 and 0.001 were tested across 10 simulation runs each, with packet loss rates of 0% and 20%. The agent's performance was evaluated based on throughput, convergence time, and network stability metrics.

## Key Results
- Simpler DQN models (NN-2) achieved faster convergence and better throughput than deeper models (NN-4, NN-8)
- Network error rate and learning rate hyperparameters had no statistically significant effect on throughput optimization
- eMBB-Agent successfully analyzed application transmission variables and adjusted reception window within discrete action space to improve throughput

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simpler DQN models (NN-2) achieve faster convergence and better throughput than deeper models (NN-4, NN-8) in network slicing environments.
- Mechanism: Shallow networks require fewer training steps to converge because they have fewer parameters to update, allowing quicker adaptation to the discrete action space for adjusting the reception window.
- Evidence: Fig. 4b shows NN-4 exhibits 20.09% lower convergence time than NN-2 in error-free channels, while NN-2 reaches convergence 10.34% faster than NN-4 in 20% error channels.

### Mechanism 2
- Claim: Network error rate and learning rate hyperparameters do not significantly affect throughput optimization in this RL-based network slicing approach.
- Mechanism: The eMBB-Agent's discrete action space and reward structure are robust to variations in channel conditions and learning rate, maintaining stable performance across different error rates and learning rates.
- Evidence: Statistical analysis in Tables 3 and 4 shows neither error rate nor learning rate had statistically significant effects on throughput.

### Mechanism 3
- Claim: The eMBB-Agent improves throughput by analyzing application transmission variables and adjusting the reception window within a discrete action space.
- Mechanism: The agent observes network state variables (like congestion window, packet size) and takes discrete actions to modify the reception window, directly influencing throughput by managing how much data the receiver can accept at any given time.
- Evidence: The agent analyzes vertical application variables and proposes actions within a discrete space to increase or decrease the reception window.

## Foundational Learning

- Concept: Reinforcement Learning and Q-Learning
  - Why needed here: The eMBB-Agent uses DQN, which is based on Q-learning principles, to learn optimal actions for throughput optimization.
  - Quick check question: What is the difference between Q-learning and Deep Q-Networks (DQN)?

- Concept: Network Slicing and Service-Level Agreements (SLAs)
  - Why needed here: The agent's purpose is to enhance network slicing throughput to meet SLAs, which requires understanding how network slicing works and what SLAs typically require.
  - Quick check question: What are the key performance metrics that SLAs typically define for network slicing?

- Concept: Congestion Control and TCP Window Management
  - Why needed here: The agent adjusts the reception window, which is directly related to TCP congestion control mechanisms.
  - Quick check question: How does adjusting the TCP reception window affect throughput and congestion in a network?

## Architecture Onboarding

- Component map: NS3 simulator -> eMBB-Agent (DQN) -> Vertical application (FTP) -> Network throughput
- Critical path: 1) Agent observes network state through NS3, 2) Agent selects action from discrete action space, 3) Action is applied to adjust reception window, 4) Network state changes and new state is observed, 5) Reward is calculated based on throughput improvement, 6) Agent updates Q-values through DQN training
- Design tradeoffs: Shallow vs. deep neural networks: faster convergence vs. potential for more complex decision-making; Discrete vs. continuous action space: simpler implementation vs. finer control granularity; Simulation vs. real-world deployment: controlled testing vs. practical validation
- Failure signatures: Poor convergence, suboptimal throughput, unstable performance, overfitting to simulation environment
- First 3 experiments: 1) Baseline test: run system with no RL agent to establish baseline throughput, 2) Simple DQN test: implement NN-2 model and verify convergence and throughput improvement, 3) Error rate test: introduce different error rates (0%, 10%, 20%) and observe agent performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the eMBB-Agent's performance scale when deployed in real-world 5G/6G network environments with dynamic traffic patterns and multi-user scenarios?
- Basis in paper: The authors acknowledge their analysis was conducted in a simulated environment using NS3 and suggest future research should explore functionality in real-world settings with more complex and dynamic situations.
- Why unresolved: The study's findings are based on controlled simulations, and authors explicitly note the need to test in real-world environments to understand practical implications across various operational scenarios.
- What evidence would resolve it: Deployment and testing of eMBB-Agent in actual 5G/6G testbeds or production networks, measuring throughput, latency, and reliability under realistic multi-user and dynamic traffic conditions.

### Open Question 2
- Question: What is the impact of varying the DQN model architecture beyond the tested configurations (NN-2, NN-4, NN-8) on convergence time and throughput optimization efficiency?
- Basis in paper: The authors observed an inverse correlation between the number of layers in the DQN model and the efficiency of optimizing network flow, suggesting that increased complexity leads to decreased throughput.
- Why unresolved: The study only tested three specific DQN configurations and found that simpler models (NN-2) performed better, but did not explore a broader range of architectures or depth variations.
- What evidence would resolve it: Systematic experimentation with a wider range of DQN architectures (varying depth, width, and activation functions) to identify optimal configurations for different network conditions and error rates.

### Open Question 3
- Question: How does the eMBB-Agent's throughput optimization performance compare against state-of-the-art traditional congestion control algorithms and other AI-based approaches in identical network conditions?
- Basis in paper: The authors conducted experiments comparing different DQN configurations but did not benchmark their approach against existing solutions like SmartCC, QTCP, or other AI-enabled congestion control methods.
- Why unresolved: While the study demonstrates the effectiveness of their RL-based approach, it lacks comparative analysis with established algorithms to quantify relative performance improvements.
- What evidence would resolve it: Head-to-head benchmarking of eMBB-Agent against traditional algorithms (e.g., TCP Cubic, BBR) and other AI-based solutions under identical network conditions, measuring throughput, convergence time, and stability.

## Limitations
- The paper lacks detailed specifications of the DQN state and action spaces, making exact replication challenging
- No comparison with traditional TCP congestion control algorithms or other baseline approaches is provided
- Real-world deployment feasibility is not addressed, limiting practical applicability assessment

## Confidence
- **High Confidence**: The core finding that simpler DQN models converge faster than deeper ones is well-supported by the experimental data and statistical analysis
- **Medium Confidence**: The claim that network error rate and learning rate have no significant effect is supported by statistical tests, but the tested ranges may be too narrow to draw definitive conclusions
- **Low Confidence**: The practical significance of throughput improvements relative to standard TCP congestion control is unclear without proper baseline comparisons

## Next Checks
1. **Statistical Significance Verification**: Replicate the statistical analysis using the reported data to verify that the claimed non-significant effects of error rate and learning rate are accurate
2. **Baseline Comparison Implementation**: Implement a standard TCP congestion control algorithm (e.g., CUBIC or BBR) as a baseline to quantify the relative performance improvement of the DQN approach
3. **Parameter Sensitivity Analysis**: Systematically vary the learning rate and error rate beyond the tested ranges (0.0001-0.1 and 0%-50%) to determine if the robustness claims hold under more extreme conditions