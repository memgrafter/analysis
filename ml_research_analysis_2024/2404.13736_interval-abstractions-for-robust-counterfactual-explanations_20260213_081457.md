---
ver: rpa2
title: Interval Abstractions for Robust Counterfactual Explanations
arxiv_id: '2404.13736'
source_url: https://arxiv.org/abs/2404.13736
tags:
- robustness
- robust
- which
- methods
- interval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating robust counterfactual
  explanations (CEs) for machine learning models, focusing on ensuring their validity
  under plausible model changes. The authors propose a novel interval abstraction
  technique to formally verify the robustness of CEs under a set of norm-bounded model
  perturbations.
---

# Interval Abstractions for Robust Counterfactual Explanations

## Quick Facts
- arXiv ID: 2404.13736
- Source URL: https://arxiv.org/abs/2404.13736
- Authors: Junqi Jiang; Francesco Leofante; Antonio Rago; Francesca Toni
- Reference count: 40
- Primary result: Novel interval abstraction technique provides provable robustness guarantees for counterfactual explanations under model perturbations

## Executive Summary
This paper addresses the challenge of generating counterfactual explanations (CEs) that remain valid under plausible model changes, such as those occurring during retraining. The authors propose a novel interval abstraction technique that enables formal verification of CE robustness across an infinite set of norm-bounded model perturbations. This approach, called ∆-robustness, provides provable guarantees that CEs remain valid when model parameters change slightly. The method is applicable to both binary and multi-class classification tasks and is evaluated on various datasets, demonstrating superior robustness compared to existing approaches.

## Method Summary
The method uses interval abstraction to represent the range of possible model outputs under perturbations, enabling exhaustive verification without enumerating infinite model variations. The core technique involves constructing an interval abstraction I(θ,∆) that over-approximates all models obtainable from Mθ via perturbations in ∆. This abstraction is encoded as a Mixed Integer Linear Programming (MILP) problem to compute output ranges and check ∆-robustness. Two algorithms are introduced: an iterative algorithm that augments existing methods and a complete RNCE method that leverages dataset structure for efficient robust CE generation.

## Key Results
- RNCE algorithm achieves 100% ∆-validity while maintaining plausibility on tested datasets
- Proposed method outperforms existing approaches in terms of robustness under model perturbations
- Interval abstraction technique provides formal guarantees without requiring exhaustive model enumeration
- Method successfully handles both binary and multi-class classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interval abstraction allows exhaustive verification of CE robustness under infinite model perturbations.
- Mechanism: The interval abstraction I(θ,∆) represents the range of possible model outputs under all perturbations in ∆. By computing over-approximations of these output intervals via MILP, we can check if a counterfactual explanation x′ remains valid across all perturbed models without enumerating them.
- Core assumption: The interval abstraction accurately over-approximates the output ranges of the model family under ∆ perturbations.
- Evidence anchors:
  - [abstract] "we propose a novel interval abstraction technique for parametric machine learning models, which allows us to obtain provable robustness guarantees for CEs under a possibly infinite set of plausible model changes ∆."
  - [section 4.2] "I(θ,∆) over-approximates the set of models Mθ′ that can be obtained from Mθ via ∆."
- Break condition: If the interval abstraction is not a tight over-approximation, the robustness guarantees become overly conservative or fail.

### Mechanism 2
- Claim: ∆-robustness provides formal guarantees that CEs remain valid under model changes.
- Mechanism: By defining ∆-robustness as requiring I(θ,∆)(x′) = 1 (for binary classification), we ensure that for all models Mθ′ ∈ ∆, the CE x′ produces the desired classification. This is verified by checking if the lower bound of the output interval exceeds the decision threshold.
- Core assumption: The classification semantics of the interval abstraction correctly capture the behavior of all models in ∆.
- Evidence anchors:
  - [section 4.3] "We say that a counterfactual explanation x′ is: ∆-robust iff I(θ,∆)(x′) = 1"
  - [section 6] "Definition 15... We say that a counterfactual explanation x′ is ∆-robust if v(k+1)1 lb ≥ 0."
- Break condition: If the interval abstraction's classification semantics are undefined (output interval contains the decision boundary), robustness cannot be determined.

### Mechanism 3
- Claim: RNCE algorithm generates provably robust and plausible CEs by leveraging interval abstraction and dataset structure.
- Mechanism: RNCE uses interval abstraction to filter dataset instances that are ∆-robust, then builds a k-d tree for efficient nearest-neighbor search. This ensures returned CEs are both robust and plausibly from the data manifold.
- Core assumption: The training dataset contains at least one instance that is ∆-robust for each input requiring a CE.
- Evidence anchors:
  - [section 7.2] "RNCE is complete if there exists an (x′, y′) ∈ D such that I(θ,∆)(x′) ̸= Mθ(x)."
  - [section 8.4] "Our RNCE algorithm (both configurations) generates the most robust CEs among the compared methods, showing 100% vr and 100% targeted ∆-validity."
- Break condition: If no ∆-robust instance exists in the dataset, RNCE cannot guarantee completeness.

## Foundational Learning

- Concept: Interval arithmetic and over-approximation
  - Why needed here: Interval abstraction relies on propagating intervals through the model to compute output ranges under perturbations.
  - Quick check question: How does interval arithmetic ensure that all possible model behaviors under ∆ are captured without enumerating them?

- Concept: Mixed Integer Linear Programming (MILP) for verification
  - Why needed here: MILP is used to compute the minimum/maximum values of nodes in the interval abstraction, enabling formal verification of ∆-robustness.
  - Quick check question: What role do binary variables play in the MILP encoding for ReLU activations?

- Concept: Counterfactual explanation properties (validity, proximity, plausibility)
  - Why needed here: Understanding these properties is crucial for evaluating the quality of CEs and how robustness affects them.
  - Quick check question: Why might optimizing for robustness potentially compromise proximity or plausibility?

## Architecture Onboarding

- Component map:
  Interval abstraction module -> MILP verifier -> CE generation algorithms -> Evaluation pipeline

- Critical path:
  1. Build interval abstraction I(θ,∆) for model Mθ and perturbation set ∆.
  2. For each candidate CE x′, solve MILP to check if I(θ,∆)(x′) = 1 (∆-robustness test).
  3. If not robust, modify CE generation parameters and repeat until robust CE found or iteration limit reached.

- Design tradeoffs:
  - Conservative vs. precise interval abstraction: Tighter bounds improve accuracy but increase MILP complexity.
  - Robustness vs. cost: Finding CEs robust to larger perturbations may require larger changes to inputs, reducing proximity.
  - Search space exploration: Iterative algorithm may miss robust CEs if search space is too constrained; RNCE is complete but requires dataset traversal.

- Failure signatures:
  - MILP solver fails to find solution: May indicate overly conservative interval bounds or infeasible robustness requirements.
  - CE generation loop exceeds iteration limit: Suggests current search strategy cannot find robust CEs within given constraints.
  - No ∆-robust instances in dataset (for RNCE): Dataset lacks sufficient diversity to support robust CEs.

- First 3 experiments:
  1. Implement interval abstraction for a simple logistic regression model and verify output ranges under small perturbations.
  2. Test MILP encoding on the interval abstraction to check ∆-robustness of a known counterfactual.
  3. Run RNCE on a small dataset with a neural network classifier, comparing CEs generated with and without robustness guarantees.

## Open Questions the Paper Calls Out

The paper acknowledges several open questions regarding the theoretical foundations and practical implications of ∆-robust counterfactual explanations. Key questions include how the choice of p-norm in the distance metric affects robustness guarantees, the computational complexity of RNCE as dataset size scales, and the practical impact of soundness requirements on CE feasibility. The authors also highlight the need for further exploration of how different perturbation sets affect the trade-off between robustness and other CE quality metrics.

## Limitations

- Interval abstraction may introduce conservatism if bounds are not tight, leading to overly restrictive robustness guarantees
- MILP-based verification is computationally expensive, limiting scalability to larger models and datasets
- RNCE's completeness guarantee depends on dataset coverage, which may not hold for all inputs or perturbation magnitudes
- The approach requires access to model parameters, limiting applicability to black-box scenarios

## Confidence

- **High confidence**: The core mechanism of using interval abstraction for robustness verification is well-established theoretically. The empirical results showing improved robustness over baseline methods are convincing.
- **Medium confidence**: The practical effectiveness of RNCE's completeness guarantee in real-world scenarios remains to be validated, as dataset coverage limitations could impact performance.
- **Low confidence**: The scalability claims to larger neural networks and datasets require further empirical validation, particularly regarding computational feasibility.

## Next Checks

1. **Bound Tightness Analysis**: Quantify the gap between the interval over-approximation and actual model output ranges on benchmark problems to assess conservatism.

2. **Scalability Benchmark**: Test the MILP verification on incrementally larger neural networks to identify computational bottlenecks and establish practical limits.

3. **Dataset Coverage Evaluation**: Systematically analyze the proportion of inputs for which RNCE can find robust CEs across different datasets and perturbation magnitudes.