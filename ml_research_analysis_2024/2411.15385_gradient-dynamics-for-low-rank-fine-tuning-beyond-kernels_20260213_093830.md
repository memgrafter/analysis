---
ver: rpa2
title: Gradient dynamics for low-rank fine-tuning beyond kernels
arxiv_id: '2411.15385'
source_url: https://arxiv.org/abs/2411.15385
tags:
- then
- have
- gradient
- learning
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the problem of low-rank fine-tuning of two-layer
  neural networks, focusing on understanding why gradient descent converges to useful
  low-rank perturbations even when kernel approximations break down. The authors consider
  a student-teacher setting where the teacher model is a rank-1 perturbation of a
  base model, and the goal is to recover this perturbation using online gradient descent.
---

# Gradient dynamics for low-rank fine-tuning beyond kernels

## Quick Facts
- arXiv ID: 2411.15385
- Source URL: https://arxiv.org/abs/2411.15385
- Reference count: 40
- The paper studies why gradient descent converges to useful low-rank perturbations in two-layer neural networks even when kernel approximations break down

## Executive Summary
This paper analyzes the problem of low-rank fine-tuning in two-layer neural networks, focusing on understanding why gradient descent converges to useful low-rank perturbations even when kernel approximations break down. The authors consider a student-teacher setting where the teacher model is a rank-1 perturbation of a base model, and the goal is to recover this perturbation using online gradient descent. Under mild assumptions on the activation function and base model, they prove that online SGD efficiently converges to the ground truth perturbation direction in different regimes, with complexity that does not depend on fine-grained properties of the activation's Hermite expansion.

## Method Summary
The method involves online gradient descent on a two-layer student network that is initialized at a pre-trained base model. The student model has the form f_Œ∏ÃÉ(x) = Œª^T œÉ((W + Œæ cÃÉ ≈©^T)x) where W is frozen pre-trained weights, cÃÉ is frozen, and only ≈© is trained. The algorithm maintains spherical projection to keep ≈© in the orthogonal complement of span(W). The loss function is (f*(x) - f_Œ∏ÃÉ(x))¬≤ where f* is the teacher model. The key insight is that even in nonlinear regimes where the perturbation is large, the population gradient provides a consistent signal toward the ground-truth perturbation direction.

## Key Results
- Online SGD converges to the correct low-rank perturbation direction in O(ÃÉdk‚Å¥/Œµ‚Å¥) iterations when the perturbation is large and base model has orthogonal features
- In the more general regime with separated features, convergence occurs in O(ÃÉdk¬≥/Œµ‚Å¥) iterations
- The complexity does not depend on fine-grained properties of the activation's Hermite expansion, unlike in generalized linear model regression
- Learning from scratch requires significantly more iterations than fine-tuning in this setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent converges to the correct low-rank perturbation even when the teacher model is not well-approximated by its linearization around the base model.
- Mechanism: The population gradient drives the dynamics, and its sign and magnitude are governed by the Hermite expansion of the activation function and the geometry of the pre-trained features. Even in nonlinear regimes, this gradient provides a consistent signal toward the ground-truth perturbation direction.
- Core assumption: The perturbation direction is orthogonal to the span of the pre-trained features, and the activation function has sufficiently fast Hermite coefficient decay.
- Evidence anchors:
  - [abstract] "when the rank-1 perturbation is comparable in norm to the weight matrix of f, the training dynamics are nonlinear. Nevertheless, in this regime we prove under mild assumptions that a student model... will converge to the teacher"
  - [section] Proposition 1: "the population gradient can be reduced to a term in the direction of spherical projection of u onto ÀÜu, scaled by some non-linear function that only depends on ‚ü®ÀÜu, u‚ü©"
  - [corpus] Weak. Corpus contains low-rank adaptation methods but lacks direct analysis of nonlinear gradient dynamics in the fine-tuning regime.
- Break condition: If the perturbation direction is not orthogonal to the span of pre-trained features, multiple global optima can arise (Example 1).

### Mechanism 2
- Claim: The complexity of fine-tuning does not depend on the information exponent of the activation function, unlike in generalized linear model regression.
- Mechanism: In the fine-tuning setting, the function ‚Ñé(ùëö) that governs the population gradient dynamics is shaped by interactions between the base model weights and the perturbation, not just the activation's Hermite expansion. This interaction masks the sensitivity to the information exponent.
- Core assumption: The base model has non-trivial features (not zero), and the perturbation scale is bounded.
- Evidence anchors:
  - [abstract] "Importantly, unlike in the GLM setting, the complexity does not depend on fine-grained properties of the activation's Hermite expansion"
  - [section] "the behavior of ‚Ñé(ùëö) in our fine-tuning setting when ùúâ is bounded is very different, and the dynamics will not be sensitive to the information exponent"
  - [corpus] Weak. Corpus papers focus on low-rank optimization and adaptation but do not analyze Hermite coefficient sensitivity.
- Break condition: If the base model is trivial (W=0), the setting reduces to GLM regression and the information exponent barrier reappears.

### Mechanism 3
- Claim: Fine-tuning is strictly more efficient than learning from scratch in the two-layer setting.
- Mechanism: The pre-trained base model provides a favorable initialization that allows gradient descent to find the perturbation direction with far fewer samples than required for learning the full teacher model from scratch.
- Core assumption: The teacher model is a low-rank perturbation of the base model, and the perturbation direction can be learned efficiently.
- Evidence anchors:
  - [abstract] "We also prove that in our setting, learning the teacher model 'from scratch' can require significantly more iterations"
  - [section] Theorem 8: "fine-tuning with online SGD learns the teacher network perturbation u in ùëÇ(ùëëùëò2/ùúÄ4) samples, whereas training from scratch using any CSQ algorithm requires at least ùëÇ(ùëëùëù/2) queries"
  - [corpus] Weak. Corpus contains LoRA and low-rank adaptation but lacks rigorous sample complexity separation proofs.
- Break condition: If the teacher model cannot be expressed as a low-rank perturbation of the base model, the separation disappears.

## Foundational Learning

- Concept: Hermite analysis and orthogonal polynomial expansions
  - Why needed here: The population gradient and loss are evaluated under Gaussian inputs, and Hermite polynomials provide the natural basis for such computations. The decay of Hermite coefficients controls the behavior of ‚Ñé(ùëö).
  - Quick check question: What is the first non-zero Hermite coefficient of the ReLU activation function?

- Concept: Anti-concentration inequalities for low-influence polynomials
  - Why needed here: To show that the randomized components of the gradient (due to random c and ÀÜc) do not vanish, ensuring ‚Ñé(0) is non-negligible and ‚Ñé(ùëö) maintains a lower bound during training.
  - Quick check question: What is the influence of a coordinate in a quadratic polynomial, and how does it affect anti-concentration?

- Concept: Martingale concentration and online learning theory
  - Why needed here: To control the error terms from stochastic gradients and finite-sample effects, ensuring that the population gradient drift dominates the noise over the training horizon.
  - Quick check question: In Freedman's inequality, what are the roles of the variance and bounded difference parameters?

## Architecture Onboarding

- Component map:
  Input -> Gaussian samples x ~ N(0, I_d)
  Base model -> f_Œ∏0(x) = Œª^T œÉ(W x) with pre-trained W
  Teacher model -> f_Œ∏(x) = Œª^T œÉ((W + Œî)x) where Œî = Œæ c u^T is rank-1
  Student model -> f_Œ∏ÃÉ(x) = Œª^T œÉ((W + Œæ cÃÉ ≈©^T)x) with frozen cÃÉ and trained ≈©
  Loss -> (f*(x) - f_Œ∏ÃÉ(x))¬≤
  Algorithm -> Online SGD on ≈© with spherical projection to keep it in the orthogonal complement of span(W)

- Critical path:
  1. Initialize ≈©0 uniformly from the orthogonal complement of span(W)
  2. At each step, sample (x, f*(x)), compute gradient, update ≈© with projection
  3. Track ‚ü®≈©_t, u‚ü© to measure alignment with ground truth
  4. Stop when ‚ü®≈©_T, u‚ü©¬≤ ‚â• 1 - Œµ

- Design tradeoffs:
  - Freezing cÃÉ vs. joint training: Freezing simplifies analysis and still works well; joint training may converge faster but introduces coupling
  - Orthogonal vs. separated features: Orthogonality simplifies proofs but angular separation is more realistic; separated case requires handling higher-order interactions
  - Activation choice: Most smooth activations work; Hermite coefficient decay rate affects convergence bounds but not qualitative behavior

- Failure signatures:
  - If ‚ü®≈©_t, u‚ü© plateaus at low values: likely violation of Assumption 2 (perturbation not orthogonal) or poor initialization
  - If training oscillates: learning rate too high or gradient variance dominates signal
  - If convergence is extremely slow: perturbation scale Œæ too small or activation has very slow Hermite decay

- First 3 experiments:
  1. Verify convergence with frozen cÃÉ and orthogonal W, varying Œæ to see transition from kernel to nonlinear regime
  2. Test joint training of cÃÉ and ≈© to confirm faster convergence and robustness to initialization
  3. Replace orthogonal W with separated features to validate angular separation assumption and bound dependence on separation parameter

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the orthogonality assumption (Assumption 2) be lifted without creating multiple global optima?
- Basis in paper: [explicit] The paper discusses in Appendix C.1 that removing Assumption 2 leads to multiple global optima
- Why unresolved: The authors provide an example showing multiple global optima exist when Assumption 2 fails, but do not prove whether this is a fundamental barrier or if alternative analysis techniques could handle this case
- What evidence would resolve it: A proof showing that (a) no algorithm can efficiently converge to the ground truth without Assumption 2, or (b) a new analysis technique that handles non-orthogonal perturbations

### Open Question 2
- Question: What is the worst-case computational complexity of fine-tuning beyond correlational statistical query algorithms?
- Basis in paper: [explicit] The authors discuss separations between fine-tuning and learning from scratch but ask "are there computational-statistical gaps in this setting?"
- Why unresolved: The paper only proves lower bounds for correlational statistical query algorithms, not for general algorithms including gradient-based methods
- What evidence would resolve it: Either a worst-case lower bound for general algorithms or an algorithm that achieves better complexity than shown in the paper

### Open Question 3
- Question: How does the complexity of fine-tuning scale with the magnitude of the perturbation parameter ùúâ?
- Basis in paper: [explicit] The authors mention in Section 5 that "we expect fine-tuning to get more difficult when the correlations are small (e.g. O(1/‚àöd))" and show empirical evidence of this transition
- Why unresolved: The authors provide empirical evidence of a transition but do not provide rigorous theoretical bounds on when this transition occurs
- What evidence would resolve it: A theoretical proof characterizing the exact scaling of complexity with ùúâ and identifying the threshold where fine-tuning becomes as hard as feature learning

### Open Question 4
- Question: Can the analysis be extended to handle joint training of both the perturbation direction ùë¢ and the coefficient vector ùëê?
- Basis in paper: [explicit] The authors note in Section 1.2.2 that "we would like to note that there are potentially algorithms beyond gradient descent" and show empirical results suggesting joint training is more efficient
- Why unresolved: The current analysis only handles the case where ùëê is frozen at initialization, while practical LoRA implementations jointly train both parameters
- What evidence would resolve it: Either a theoretical analysis of the joint dynamics or an explanation of why the frozen-ùëê analysis provides a good approximation to the joint case

### Open Question 5
- Question: Can the results be extended to deeper networks and higher-rank perturbations?
- Basis in paper: [explicit] The authors mention in Section 5 that "it would be interesting to understand the worst-case complexity of fine-tuning" and discuss going "beyond two layers and rank-1 perturbations" as a future direction
- Why unresolved: The current analysis is limited to two-layer networks with rank-1 perturbations, while practical LoRA often uses higher ranks and deeper models
- What evidence would resolve it: Either a proof extending the analysis to deeper networks/higher ranks or a counterexample showing why such extension is fundamentally difficult

## Limitations

- The theoretical analysis relies heavily on specific assumptions about the base model (orthogonal or well-separated features) and perturbation direction (orthogonal to feature span) that may not hold in practical fine-tuning scenarios.
- The results are proven for online SGD with a fixed learning rate and specific initialization, but the behavior under other optimization algorithms or different initializations remains unexplored.
- The analysis focuses on the direction of the perturbation but does not address the magnitude or scale factor, which could be important for practical fine-tuning applications.

## Confidence

- **High Confidence**: The main theoretical results showing convergence to the correct perturbation direction under the stated assumptions. The proofs appear rigorous and the mathematical framework is sound.
- **Medium Confidence**: The practical implications and relevance to real-world fine-tuning scenarios. While the theory is solid, the gap between theoretical assumptions and practical settings needs further investigation.
- **Low Confidence**: The lower bound showing learning from scratch is harder than fine-tuning. This is based on a specific adversarial construction and may not reflect typical practical scenarios.

## Next Checks

1. **Empirical Validation**: Implement the online SGD algorithm with the specific assumptions (orthogonal features, frozen cÃÉ) and verify the theoretical convergence rates on synthetic data with different activation functions and perturbation scales.

2. **Assumption Relaxation**: Test the algorithm when the perturbation direction is not exactly orthogonal to the feature span, or when the base model features are not perfectly orthogonal but only well-separated, to understand the robustness of the theoretical results.

3. **Practical Comparison**: Compare the sample complexity of fine-tuning (as analyzed) versus learning from scratch on a real-world task (like adapting a pre-trained language model) to assess whether the theoretical separation observed in the paper translates to practical performance gains.