---
ver: rpa2
title: In-Dataset Trajectory Return Regularization for Offline Preference-based Reinforcement
  Learning
arxiv_id: '2412.09104'
source_url: https://arxiv.org/abs/2412.09104
tags:
- reward
- offline
- learning
- policy
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes In-Dataset Trajectory Return Regularization
  (DTR) to address reward bias in offline preference-based reinforcement learning
  (PbRL). The core idea is to combine conditional sequence modeling with Decision
  Transformer and TD-Learning to balance trajectory return regularization with reward-based
  exploration.
---

# In-Dataset Trajectory Return Regularization for Offline Preference-based Reinforcement Learning

## Quick Facts
- arXiv ID: 2412.09104
- Source URL: https://arxiv.org/abs/2412.09104
- Reference count: 40
- Outperforms state-of-the-art baselines by up to 11% on MuJoCo and Adroit tasks

## Executive Summary
This paper proposes In-Dataset Trajectory Return Regularization (DTR) to address reward bias in offline preference-based reinforcement learning (PbRL). The method combines conditional sequence modeling with Decision Transformer and TD-Learning to balance trajectory return regularization with reward-based exploration. A key innovation is ensemble normalization of multiple reward models to balance differentiation and accuracy. Experiments on MuJoCo locomotion and Adroit manipulation tasks show DTR outperforms state-of-the-art baselines, achieving up to 11% higher performance on average compared to the best baseline.

## Method Summary
DTR addresses reward bias in offline PbRL by combining Decision Transformer with TD-Learning. The method trains an ensemble of MLP reward models on preference data using the Bradley-Terry model, applies ensemble normalization to balance reward differentiation and accuracy, then uses these normalized rewards to train both a DT policy (for in-dataset regularization) and a TD3-style Q-network (for exploration). During inference, the policy uses RTG-based rollout with fixed decrement values instead of learned rewards. The training dynamically adjusts coefficients to balance conservatism and exploration throughout learning.

## Key Results
- Achieves up to 11% higher performance on average compared to the best baseline
- Demonstrates robustness to small preference datasets
- Provides theoretical guarantees for policy performance bounds
- Outperforms state-of-the-art baselines on MuJoCo locomotion and Adroit manipulation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-dataset trajectory return regularization prevents overestimation of rewards from causing suboptimal trajectory stitching in offline PbRL.
- Mechanism: Decision Transformer conditions on high in-dataset trajectory returns, restricting the policy to trajectories that actually exist in the dataset. This conservative constraint mitigates risk of stitching together trajectory fragments based on overestimated rewards that don't correspond to real data patterns.
- Core assumption: In-dataset trajectories contain sufficient information about high-return behaviors that align with human preferences, even if individual reward labels are noisy or biased.
- Evidence anchors:
  - [abstract] "DTR employs Decision Transformer and TD-Learning to strike a balance between maintaining fidelity to the behavior policy with high in-dataset trajectory returns and selecting optimal actions based on high reward labels."
  - [section] "CSM agent gets correct actions that align closely with the behavior policy conditioned on high in-dataset trajectory return."
  - [corpus] Found 25 related papers, but none specifically address in-dataset regularization as a mechanism for handling reward bias in PbRL.

### Mechanism 2
- Claim: Ensemble normalization of multiple reward models balances reward differentiation and accuracy by reducing individual model estimation errors.
- Mechanism: Multiple reward models are trained independently from the same preference data. Each model's predictions are normalized individually to prevent scale inconsistencies, then averaged. This approach reduces variance while preserving reward signal differences that guide policy learning.
- Core assumption: Different reward models will make uncorrelated errors, so averaging their normalized outputs provides a more reliable signal than any single model.
- Evidence anchors:
  - [section] "We propose ensemble normalization, which first normalizes the estimates of each ensemble and then averages them: rˆψ = Mean(Norm(rˆψ1), ···, Norm(rˆψN))"
  - [section] "We observe that the trained reward model labels different state-action pairs with only minor differences... While direct reward normalization can amplify these differences, it also increases uncertainty."
  - [corpus] Limited direct evidence for ensemble normalization in PbRL context, though ensemble methods are common in other RL settings.

### Mechanism 3
- Claim: Dynamic coefficient adjustment balances exploration and conservatism throughout training, preventing premature convergence to suboptimal policies.
- Mechanism: The coefficient λ(step) in the policy loss starts small to prioritize learning from in-dataset trajectories, then gradually increases to allow more exploration via TD-learning. This prevents the policy from becoming too conservative too early while still maintaining stability.
- Core assumption: Early training should prioritize reliable in-dataset information, while later training can safely incorporate more exploration once the model has learned basic behavioral patterns.
- Evidence anchors:
  - [section] "we dynamically integrate the policy gradient to train the policy network... we gradually increase the proportion of policy gradient to facilitate trajectory stitching near the optimal range within the distribution."
  - [section] "We minimize the following policy loss: Lπ = LDT − λ(step) Eτt∼D ∑i=t−H+1t Qϕ(si, aˆi)"
  - [corpus] No direct corpus evidence for this specific dynamic coefficient approach in PbRL literature.

## Foundational Learning

- Concept: Preference-based reinforcement learning with Bradley-Terry model
  - Why needed here: DTR builds on the standard PbRL framework of learning rewards from pairwise preferences using the Bradley-Terry model, then applying those rewards in offline RL.
  - Quick check question: What does the Bradley-Terry model assume about the relationship between preference labels and underlying rewards?

- Concept: Offline reinforcement learning with pessimism
  - Why needed here: DTR operates in the offline setting where the policy must learn from fixed datasets without environment interaction, requiring pessimism to handle out-of-distribution state-action pairs.
  - Quick check question: Why is pessimism particularly important when using learned reward models in offline RL?

- Concept: Conditional sequence modeling with Decision Transformer
  - Why needed here: DT forms the core of DTR's in-dataset regularization component, using sequence modeling to learn policies conditioned on return-to-go.
  - Quick check question: How does Decision Transformer's conditioning on return-to-go differ from traditional RL value function approaches?

## Architecture Onboarding

- Component map: Preference data → ensemble of MLP reward models → ensemble normalization → Decision Transformer and TD3-style Q-network → policy inference with RTG-based rollout
- Critical path: Preference data → reward model ensemble → normalized rewards → DT and Q-network training → policy inference with RTG-based rollout
- Design tradeoffs: The paper balances conservatism (DT-based in-dataset regularization) against exploration (TD-learning), and reward differentiation (ensemble normalization) against accuracy (averaging multiple models)
- Failure signatures: Poor performance may indicate either too much conservatism (insufficient exploration), too little (reward bias causing bad trajectory stitching), or reward model issues (overfitting to preference data or poor generalization)
- First 3 experiments:
  1. Verify reward model training on preference data achieves reasonable accuracy (>96%) before using it to label the offline dataset.
  2. Test DT-only variant on a simple environment to confirm it learns reasonable in-dataset behaviors before adding Q-network.
  3. Validate ensemble normalization improves reward differentiation compared to single model normalization using visualization of reward distributions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of the tradeoff between reward differentiation and accuracy in ensemble normalization?
- Basis in paper: [explicit] The paper introduces ensemble normalization to balance reward differentiation and inaccuracy, but does not explore its theoretical limits.
- Why unresolved: The paper only empirically shows that ensemble normalization improves performance but does not provide a theoretical framework for understanding the tradeoff between differentiation and accuracy.
- What evidence would resolve it: A theoretical analysis or empirical study showing the optimal balance point between reward differentiation and accuracy in ensemble normalization.

### Open Question 2
- Question: How does the performance of DTR scale with the size of the offline dataset when the preference dataset is small?
- Basis in paper: [inferred] The paper shows DTR performs well with small preference datasets, but does not explore how performance scales with the size of the offline dataset.
- Why unresolved: The experiments focus on the impact of the preference dataset size, but do not vary the size of the offline dataset to understand its impact on DTR's performance.
- What evidence would resolve it: Experiments varying the size of the offline dataset while keeping the preference dataset small, and measuring the impact on DTR's performance.

### Open Question 3
- Question: What is the impact of the context length H on DTR's performance in tasks with longer horizons?
- Basis in paper: [explicit] The paper discusses the impact of context length H on performance in MuJoCo-medium-replay tasks, but does not explore its impact in tasks with longer horizons.
- Why unresolved: The experiments focus on tasks with relatively short horizons, and do not explore how DTR's performance changes with longer horizons.
- What evidence would resolve it: Experiments testing DTR's performance in tasks with longer horizons, varying the context length H, and measuring the impact on performance.

## Limitations
- Lack of ablation studies on the critical dynamic coefficient λ(step) scheduling mechanism
- Limited empirical validation of ensemble normalization compared to alternative approaches
- Theoretical analysis provides policy performance bounds but doesn't account for practical challenges of reward model estimation errors
- RTG-based rollout strategy assumes fixed decrement values without sensitivity analysis

## Confidence

- **High confidence**: The core claim that DTR outperforms state-of-the-art baselines on standard benchmarks (MuJoCo and Adroit tasks) is well-supported by the experimental results showing up to 11% higher performance.
- **Medium confidence**: The mechanism of ensemble normalization balancing differentiation and accuracy is theoretically justified but lacks comprehensive empirical validation against alternative approaches.
- **Medium confidence**: The dynamic coefficient adjustment preventing premature convergence is plausible but not thoroughly validated through ablation studies.

## Next Checks

1. **Ablation study on λ(step) scheduling**: Systematically vary the dynamic coefficient scheduling parameters to identify optimal settings and validate the claim about balancing exploration and conservatism.
2. **Ensemble normalization comparison**: Compare ensemble normalization against alternative approaches (single model normalization, no normalization, other ensemble methods) to quantify its specific contribution to performance.
3. **RTG decrement sensitivity analysis**: Test different fixed RTG decrement values during inference to determine sensitivity and identify optimal values for different task types.