---
ver: rpa2
title: Computationally Efficient RL under Linear Bellman Completeness for Deterministic
  Dynamics
arxiv_id: '2406.11810'
source_url: https://arxiv.org/abs/2406.11810
tags:
- udcurlymod
- alt1
- parall
- brack
- summation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops the first computationally efficient reinforcement
  learning algorithm for the linear Bellman complete setting with deterministic dynamics.
  The algorithm injects random noise into least squares regression problems only in
  the null space of historical data to achieve optimism while avoiding error amplification.
---

# Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics

## Quick Facts
- arXiv ID: 2406.11810
- Source URL: https://arxiv.org/abs/2406.11810
- Reference count: 40
- One-line primary result: First computationally efficient RL algorithm for linear Bellman complete setting with deterministic dynamics achieving polynomial regret bound

## Executive Summary
This paper develops the first computationally efficient reinforcement learning algorithm for the linear Bellman complete setting with deterministic dynamics. The algorithm injects random noise into least squares regression problems only in the null space of historical data to achieve optimism while avoiding error amplification. By leveraging deterministic dynamics and a span argument, the algorithm ensures accurate value function estimates within the data span, enabling efficient exploration via null-space noise. The approach achieves a regret bound of $\tilde{O}(d^{5/2}H^{5/2} + d^{2}H^{3/2}\sqrt{T})$ for feature dimension $d$, horizon $H$, and $T$ rounds, handling large action spaces, stochastic rewards, and random initial states.

## Method Summary
The algorithm uses a span argument combined with null-space randomization to achieve computational efficiency in linear Bellman complete RL with deterministic dynamics. It relies on three key components: (1) a D-optimal design for feature space sampling, (2) constrained squared loss minimization oracles, and (3) noise injection confined to the null space of historical data. The algorithm maintains a covariance matrix and leverages the span of observed trajectories to bound regret, paying H regret when trajectories fall outside the historical data span, which can happen at most dH times due to dimensionality constraints.

## Key Results
- Achieves polynomial regret bound $\tilde{O}(d^{5/2}H^{5/2} + d^{2}H^{3/2}\sqrt{T})$ without exponential parameter blow-up
- First computationally efficient algorithm for linear Bellman complete RL with deterministic dynamics
- Handles large action spaces, stochastic rewards, and random initial states while maintaining polynomial sample complexity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Injecting noise only in the null space of historical data enables optimism while avoiding error amplification.
- Mechanism: The algorithm adds Gaussian noise to the least squares regression parameters but restricts it to the null space of the data covariance matrix. This ensures that within the span of observed data, the value function estimates remain accurate while optimism is maintained in unexplored directions.
- Core assumption: Deterministic dynamics ensure that value functions can be learned exactly for trajectories within the data span.
- Evidence anchors:
  - [abstract] "Our key technical contribution is to carefully design the noise to only act in the null space of the training data to ensure optimism while circumventing a subtle error amplification issue."
  - [section] "Our key observation is that deterministic dynamics simplifies the learning process by ensuring accurate value estimates within the data span, allowing noise injection to be confined to the null space."
- Break condition: If dynamics become stochastic, the value function cannot be learned exactly within the data span, invalidating this mechanism.

### Mechanism 2
- Claim: The span argument bounds regret by limiting how often trajectories fall outside historical data.
- Mechanism: When a trajectory segment is not in the span of historical data, the algorithm pays H regret for that segment. Since dimension cannot exceed d for any h∈[H], this can happen at most dH times, providing a polynomial bound on regret.
- Core assumption: The feature space spans Rd, ensuring that the span dimension cannot grow indefinitely.
- Evidence anchors:
  - [section] "By definition, when Espan_t does not hold, there exists h∈[H] such that dim span(Dt,h) = dim span(Dt-1,h) + 1... Since the dimension of spans cannot exceed d for any h∈[H], the case that Espan_t does not hold cannot happen for more than dH times."
  - [section] "Together, these techniques leads to our polynomial sample complexity bound."
- Break condition: If the feature space does not span Rd, the span dimension could remain low indefinitely, preventing the bound from holding.

### Mechanism 3
- Claim: The algorithm achieves polynomial sample complexity without ℓ2-norm boundedness assumptions on parameters.
- Mechanism: Unlike prior methods that require bounded parameter norms, this approach uses the span argument to avoid exponential blow-up. The algorithm does not need to truncate value functions, which would break linearity in Bellman completeness.
- Core assumption: The Bellman backup of a truncated value function is not necessarily linear, making truncation infeasible under linear Bellman completeness.
- Evidence anchors:
  - [section] "Prior RLSVI algorithms used truncation on value functions to explicitly avoid such an exponential blow-up. However, truncation does not work for linear Bellman completeness setting since the Bellman backup on a truncated value function is not necessarily a linear function anymore."
  - [section] "In fact, the search for a computationally efficient algorithm with large action spaces is open even when the transition dynamics are deterministic."
- Break condition: If the MDP structure allows safe truncation without breaking linearity, simpler methods might work.

## Foundational Learning

- Concept: Linear Bellman Completeness
  - Why needed here: This is the fundamental assumption that enables the algorithm to work. It ensures that Bellman backups of linear value functions remain linear, allowing the use of linear function approximation throughout.
  - Quick check question: Can you verify that for any linear value function Q(s,a) = ⟨θ, φ(s,a)⟩, the Bellman backup E[max_a' ⟨θ, φ(s',a')⟩] can be expressed as ⟨Tθ, φ(s,a)⟩ for some linear operator T?

- Concept: Deterministic vs Stochastic Dynamics
  - Why needed here: The algorithm crucially relies on deterministic dynamics to learn value functions exactly within the data span. Stochastic dynamics would require expectation over next states, breaking the exact learning property.
  - Quick check question: How would the algorithm need to change if we replaced the deterministic transition T(s,a) with a stochastic transition distribution P(s'|s,a)?

- Concept: Span Argument in Reinforcement Learning
  - Why needed here: This is the key technique that allows the algorithm to avoid exponential blow-up in parameter norms. By focusing on whether trajectories are in or out of the data span, the algorithm can provide polynomial bounds without requiring bounded parameters.
  - Quick check question: Can you explain why the dimension of the span can increase by at most one each time a trajectory goes out of the span, and why this limits the total number of such events to dH?

## Architecture Onboarding

- Component map: D-optimal design -> constrained squared loss minimization oracle -> null-space noise injection -> span-based regret analysis
- Critical path: The most critical path is the interaction between the noise injection and the span argument. Noise must be added only in the null space to ensure optimism while the span argument must correctly track when trajectories leave the data span to bound regret.
- Design tradeoffs: The algorithm trades off computational efficiency against the deterministic dynamics assumption. While this enables polynomial complexity, it limits applicability to environments with stochastic dynamics. The noise injection mechanism must carefully balance exploration with maintaining accurate estimates within the data span.
- Failure signatures: Key failure modes include: (1) If the feature space doesn't span Rd, the span argument breaks down, (2) If the noise injection leaks into the span space, optimism is lost, (3) If the squared loss minimization oracle is inaccurate, value function estimates degrade, and (4) If dynamics become stochastic, the core mechanism fails.
- First 3 experiments:
  1. Implement the algorithm on a simple tabular MDP with deterministic dynamics and verify that regret scales as expected with d and H.
  2. Test the null-space noise injection by measuring value function accuracy within the data span versus exploration in null space directions.
  3. Verify the span argument by tracking how often trajectories leave the data span and confirming it stays below dH.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the algorithm be extended to stochastic dynamics while maintaining computational efficiency?
- Basis in paper: [explicit] The paper states "Extending our algorithm to stochastic dynamics remains an open challenge" and the algorithm relies on deterministic dynamics for accurate value function estimation within the data span.
- Why unresolved: The span argument that ensures accurate value function estimation within the data span breaks down with stochastic transitions, making it difficult to confine noise injection to the null space while maintaining optimism.
- What evidence would resolve it: A modified algorithm that provably works with stochastic dynamics, showing regret bounds comparable to the deterministic case, or a lower bound proving computational intractability for stochastic dynamics under linear Bellman completeness.

### Open Question 2
- Question: How does the exponential blow-up in parameter norms affect practical implementation, and can it be mitigated without violating linearity?
- Basis in paper: [explicit] The paper acknowledges that parameter norms can grow exponentially due to noise injection, but claims this doesn't affect sample efficiency due to the span argument. Prior methods used truncation, which doesn't work here.
- Why unresolved: While theoretically the span argument handles the blow-up, practical implementation might face numerical stability issues or computational overhead when parameters become extremely large.
- What evidence would resolve it: Empirical studies comparing numerical stability and runtime across different parameter ranges, or theoretical analysis of how the blow-up affects finite-precision computation.

### Open Question 3
- Question: Is the D-optimal design assumption necessary, or can the algorithm work with other exploration strategies?
- Basis in paper: [explicit] The algorithm requires a D-optimal design for the feature space, but the paper doesn't explore whether this is a fundamental requirement or just one way to ensure good exploration.
- Why unresolved: The D-optimal design is used to initialize the covariance matrix and ensure invertibility, but other exploration strategies might achieve similar results without requiring explicit knowledge of the feature space.
- What evidence would resolve it: An algorithm variant that replaces the D-optimal design with an adaptive exploration strategy, showing similar theoretical guarantees without requiring the design upfront.

## Limitations

- The algorithm fundamentally relies on deterministic dynamics, making extension to stochastic environments an open challenge
- Requires linear Bellman completeness, a restrictive assumption that may not hold in many practical problems
- Practical implementation may face numerical stability issues due to exponential parameter norm blow-up, despite theoretical guarantees

## Confidence

**High Confidence**: The regret bound of $\tilde{O}(d^{5/2}H^{5/2} + d^{2}H^{3/2}\sqrt{T})$ for the deterministic case, as this follows from rigorous mathematical proofs in the span argument framework.

**Medium Confidence**: The computational efficiency claims, as the algorithm relies on linear regression oracles that may be expensive in practice, particularly for high-dimensional feature spaces or when the convex feasibility solver requires many iterations.

**Low Confidence**: Claims about handling large action spaces, as the algorithm's performance in practice may degrade when the action space is truly enormous, despite the theoretical guarantees.

## Next Checks

1. **Implementation Validation**: Implement Algorithm 1 on a simple deterministic MDP (e.g., a grid world with deterministic transitions) and verify that the regret scales as predicted by the theoretical bound. Track the number of times trajectories leave the data span to confirm it stays below dH.

2. **Noise Injection Verification**: Conduct experiments to verify that noise is correctly constrained to the null space. Measure value function accuracy within the data span versus exploration in null space directions, ensuring that the algorithm maintains optimism without degrading estimates in explored regions.

3. **Oracle Performance Testing**: Test the random-walk-based convex feasibility solver (Algorithm 2) on high-dimensional instances to measure its mixing time and accuracy. Compare its performance against exact solvers on smaller instances to understand the practical implications of using an approximate oracle.