---
ver: rpa2
title: 'PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness
  with Single-task Models'
arxiv_id: '2406.07801'
source_url: https://arxiv.org/abs/2406.07801
tags:
- speech
- polyspeech
- tasks
- tokens
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PolySpeech, a unified multitask speech model
  that integrates speech recognition, synthesis, and classification tasks using a
  multi-modal language model architecture. The core innovation involves using semantic
  speech representations and a novel tokenization method (SSET) to improve speech
  generation quality while maintaining competitive performance across all tasks.
---

# PolySpeech: Exploring Unified Multitask Speech Models for Competitiveness with Single-task Models

## Quick Facts
- arXiv ID: 2406.07801
- Source URL: https://arxiv.org/abs/2406.07801
- Reference count: 0
- Key result: Unified multitask speech model achieves comparable performance to single-task training with 2.0% CER on 26k-hour Mandarin ASR and 94.4% LID accuracy

## Executive Summary
PolySpeech introduces a unified multitask speech model that integrates speech recognition, synthesis, and classification tasks using a multi-modal language model architecture. The model leverages semantic speech representations and a novel tokenization method (SSET) to achieve high-quality speech generation while maintaining competitive performance across all tasks. Experiments demonstrate that multitask training achieves comparable performance to single-task training, with particular benefits for language identification tasks. The model achieves 2.0% CER on 26k-hour Mandarin ASR, maintains high speaker similarity (SECS > 0.9) in TTS, and improves LID accuracy from 90.9% to 94.4% compared to single-task training.

## Method Summary
PolySpeech is a 160M parameter decoder-only Transformer LM that processes both text and speech inputs using semantic embeddings from chinese-hubert-base or acoustic EnCodec tokens. The model uses multitask optimization with random task selection per batch and gradient accumulation. For TTS, a novel Semantic Speech Embedding Tokenization (SSET) method employs convolutional encoder-decoder with RVQ to generate 2048 discrete tokens. Speaker embeddings are extracted from speech prompts for timbre control. Training uses Adam optimizer with 2k warm-up steps on 26k hours of Mandarin data for ASR/TTS, plus additional data for LID/GID tasks.

## Key Results
- Achieves 2.0% CER on 26k-hour Mandarin ASR test set
- Maintains SECS > 0.9 for speaker similarity in TTS
- Improves LID accuracy from 90.9% to 94.4% with multitask training
- Demonstrates comparable performance to single-task models across all four tasks (ASR, TTS, LID, GID)

## Why This Works (Mechanism)

### Mechanism 1
Semantic speech representations preserve more acoustic information than k-means discretized tokens. The SSET codec method maintains high-fidelity speech information by using continuous semantic embeddings instead of discrete k-means clusters, preventing tone inaccuracies in Mandarin. Core assumption: Semantic embeddings contain richer acoustic-phonetic information than k-means discretized tokens.

### Mechanism 2
Multitask training improves LID performance by maintaining semantic discrimination abilities through ASR task. Joint optimization with ASR task helps the model maintain better semantic discrimination, which transfers to improved language identification. Core assumption: Semantic features learned for ASR are transferable to LID task.

### Mechanism 3
Speech prompts enable high-quality TTS for any speaker by controlling acoustic conditions. The semantic speech token decoder uses speech prompts to extract speaker embeddings, which control timbre and acoustic features during synthesis. Core assumption: Speaker embeddings extracted from prompts contain sufficient acoustic information for timbre reproduction.

## Foundational Learning

- **Transformer decoder architecture and causal masking**: PolySpeech uses decoder-only Transformer as core structure for autoregressive prediction. Quick check: How does the causal mask prevent information leakage during inference?
- **Speech representation discretization methods**: Different tokenization approaches (HuBERT vs EnCodec vs k-means) significantly impact performance. Quick check: What are the trade-offs between semantic embeddings and acoustic tokens for speech generation?
- **Multitask learning optimization strategies**: PolySpeech uses task-balanced batching and gradient accumulation across different speech tasks. Quick check: How does random task selection with data volume weighting affect convergence?

## Architecture Onboarding

- **Component map**: Multi-modal Transformer LM (core) → Input tokenizers (text/speech) → Task ID embedding → Output de-tokenizers (text/speech classification)
- **Critical path**: Speech input → Semantic embedding → LM prediction → Speech output generation
- **Design tradeoffs**: Semantic vs acoustic representations (quality vs complexity), multitask vs single-task training (generalization vs specialization)
- **Failure signatures**: CER degradation in ASR indicates tokenization issues; SECS drop in TTS indicates speaker embedding problems; LID accuracy drop indicates semantic feature loss
- **First 3 experiments**: 1) Compare HuBERT vs EnCodec tokenization on ASR baseline; 2) Test SSET codec quality by reconstructing speech from tokens; 3) Validate speaker embedding extraction from prompts for TTS

## Open Questions the Paper Calls Out

### Open Question 1
Does multitask training with PolySpeech consistently improve performance for specific speech tasks across different language families and acoustic conditions? The paper states that multitask optimization "is especially beneficial for specific tasks" but only tested on Mandarin Chinese and a few other languages, making it unclear if benefits generalize to other language families or diverse acoustic environments.

### Open Question 2
What is the optimal task mixing ratio for multitask training in PolySpeech to maximize performance across all tasks? The paper uses data-volume-based probability for task selection without investigating whether different mixing ratios could yield better overall performance or task-specific improvements.

### Open Question 3
How does PolySpeech's performance scale with model size compared to single-task models? The current 160M parameter model demonstrates competitive performance, but scaling experiments comparing PolySpeech to similarly-sized single-task models are not provided.

## Limitations

- Claims about SSET codec superiority lack direct ablation studies comparing it to k-means discretization on identical datasets
- The 26k-hour Mandarin corpus composition remains partially opaque with unspecified "internal Mandarin dialog corpora"
- Multitask benefits for LID are promising but the transfer mechanism remains theoretical without analysis of which semantic features specifically improve LID
- Speaker embedding quality for TTS across "any given speaker" is demonstrated on limited in-domain data

## Confidence

**High Confidence**: Core multitask architecture works as described - PolySpeech successfully performs ASR, TTS, LID, and GID tasks simultaneously with functional competence.
**Medium Confidence**: Claims about SSET codec providing "high-quality speech" and multitask training achieving "comparable performance" are demonstrated but could benefit from more extensive ablation studies.
**Low Confidence**: Explanations for why multitask training particularly benefits LID and claims about speaker embedding quality for TTS across diverse speakers are under-validated.

## Next Checks

1. **Ablation Study Required**: Conduct systematic comparison of SSET vs k-means discretized tokens on identical Mandarin speech data, measuring both ASR accuracy and speech reconstruction quality with objective metrics (PESQ, STOI) and subjective listening tests.

2. **Cross-Domain Speaker Generalization**: Test TTS speaker embedding quality on speakers from different domains than training data (e.g., read speech vs spontaneous conversation), measuring SECS and conducting ABX preference tests to verify "any given speaker" coverage.

3. **Semantic Feature Transfer Analysis**: Perform feature attribution analysis to identify which HuBERT layers and semantic features contribute most to LID performance, comparing multitask vs single-task models using layer-wise relevance propagation to validate the claimed transfer mechanism from ASR to LID.