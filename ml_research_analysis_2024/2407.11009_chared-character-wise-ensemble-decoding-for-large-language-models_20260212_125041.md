---
ver: rpa2
title: 'CharED: Character-wise Ensemble Decoding for Large Language Models'
arxiv_id: '2407.11009'
source_url: https://arxiv.org/abs/2407.11009
tags:
- char
- language
- performance
- character
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Character-wise Ensemble Decoding (CharED),
  an inference-time method for combining outputs from multiple large language models
  (LLMs) with different vocabularies and tokenizations. CharED converts subword-level
  models into character-level ones by finding marginal character distributions from
  each model's next-token probabilities, then performing a weighted average of these
  distributions to generate outputs character-by-character.
---

# CharED: Character-wise Ensemble Decoding for Large Language Models

## Quick Facts
- arXiv ID: 2407.11009
- Source URL: https://arxiv.org/abs/2407.11009
- Reference count: 24
- Primary result: Combines multiple LLMs with different vocabularies at inference time through character-level marginalization, achieving Pareto-optimal performance across coding, mathematics, and toxicity benchmarks

## Executive Summary
This paper introduces Character-wise Ensemble Decoding (CharED), an inference-time method that combines outputs from multiple large language models with different vocabularies and tokenizations. CharED converts subword-level models into character-level ones by finding marginal character distributions from each model's next-token probabilities, then performs weighted averaging of these distributions to generate outputs character-by-character. The method requires no fine-tuning and works with any pair of LLMs regardless of vocabulary or model size.

Experimental results demonstrate that CharED successfully combines complementary strengths of paired models across three domains: coding (HumanEval), mathematics (GSM8K), and toxicity avoidance (ToxiGen). The best performance came from combining DeepSeek Coder and WizardMath, which retained ~68% accuracy on both HumanEval and GSM8K at α=0.5. Even the worst combination (WizardMath + Llama2Chat on GSM8K and ToxiGen) demonstrated some skill transfer from both constituent models.

## Method Summary
CharED is an inference-time ensembling method that combines two LLMs with different tokenizations by converting them to a shared character-level representation. The algorithm works by first extracting next-token probability distributions from both models, then decomposing these into marginal character probabilities by summing probabilities of tokens starting with each character. These character distributions are weighted-averaged using parameter α to generate character-by-character outputs. The method bypasses the need for shared vocabularies or tokenization schemes that traditional shallow fusion requires, enabling simple ensemble combinations regardless of original model differences.

## Key Results
- Best performance: DeepSeek Coder + WizardMath retained ~68% accuracy on both HumanEval and GSM8K at α=0.5
- All pairwise combinations showed skill transfer from both constituent models
- Worst combination (WizardMath + Llama2Chat) still demonstrated complementary benefits
- Achieved Pareto-optimal performance curves, outperforming individual models across various α values
- Successfully combined models across coding, mathematics, and toxicity avoidance domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CharED converts subword-level models into character-level ones at decoding time by finding marginal character distributions from each model's next-token probabilities.
- Mechanism: The method decomposes each model's token probability distribution into marginal character probabilities by summing probabilities of tokens starting with each character. These marginal distributions are then weighted averaged based on parameter α to generate character-by-character outputs.
- Core assumption: LLMs trained with subword tokenization still encode sufficient character-level information that can be extracted through marginalization.
- Evidence anchors:
  - [abstract]: "finds the marginal distribution of each character for an individual model and performs a weighted average to generate an output, character by character"
  - [section]: "by decomposing next token output probabilities from two separate LLMs into marginal next-character probabilities"
  - [corpus]: Weak evidence - the corpus mentions "Breaking the Ceiling of the LLM Community by Treating Token Generation as a Classification for Ensembling" but doesn't directly support the character marginalization mechanism.

### Mechanism 2
- Claim: CharED enables ensembling of models with different vocabularies and tokenizations by converting them to a shared character-level representation.
- Mechanism: Since all models are converted to character-level probability distributions before averaging, the method bypasses the need for shared vocabularies or tokenization schemes that traditional shallow fusion requires.
- Core assumption: Character-level representations provide a universal common ground for combining model outputs regardless of original tokenization differences.
- Evidence anchors:
  - [abstract]: "averaging outputs from multiple LLMs and illustrate its improved performance across multiple domains compared to its constituent models alone"
  - [section]: "This character level conversion means all models then share vocabulary, making them simpler to ensemble"
  - [corpus]: Weak evidence - while the corpus discusses various ensemble methods, it doesn't specifically address vocabulary-agnostic ensemble approaches.

### Mechanism 3
- Claim: The weighted averaging of character probabilities allows CharED to combine complementary strengths of different models across domains.
- Mechanism: By varying the weight parameter α, CharED can shift the balance between constituent models, enabling optimization for specific tasks where one model may excel over another.
- Core assumption: Different LLMs develop complementary capabilities during training that can be effectively combined through simple weighted averaging at the character level.
- Evidence anchors:
  - [abstract]: "find our proposed model able to combine complimentary strengths of multiple LLMs, regardless of vocabulary, tokenization, or model size"
  - [section]: "we find in all three cases that the combined model is able to confer benefits from both individual models"
  - [corpus]: Weak evidence - the corpus mentions ensemble methods but doesn't specifically validate the effectiveness of weighted character-level averaging for combining complementary strengths.

## Foundational Learning

- Concept: Marginal probability distribution
  - Why needed here: Understanding how to convert token-level probabilities to character-level probabilities by summing over all tokens starting with each character
  - Quick check question: Given tokens "cat" (0.3), "car" (0.2), and "dog" (0.5), what are the marginal probabilities for characters 'c' and 'd'?

- Concept: Weighted arithmetic mean
  - Why needed here: The core operation for combining character distributions from different models using parameter α
  - Quick check question: If model A gives P('a')=0.7 and model B gives P('a')=0.3, what is the combined probability when α=0.4?

- Concept: Tokenization schemes (BPE, WordPiece, SentencePiece)
  - Why needed here: Understanding why traditional ensemble methods fail with different tokenizations and how CharED overcomes this limitation
  - Quick check question: Why can't shallow fusion be directly applied to models using different tokenization schemes like BPE vs WordPiece?

## Architecture Onboarding

- Component map:
  Input layer: Two separate LLM prompts (l1, l2) -> Token probability extraction: Query both models for next-token probabilities -> Marginalization module: Convert token probabilities to character probabilities -> Weighted averaging: Combine character distributions using parameter α -> Sampling/greedy selection: Choose next character -> State management: Track partial tokens and refresh when complete -> Output layer: Character-by-character generated sequence

- Critical path:
  1. Query both models for initial token probabilities
  2. Marginalize to character probabilities
  3. Weighted average character distributions
  4. Select next character (greedy or sampled)
  5. Update token states and refresh if needed
  6. Repeat until EOS character

- Design tradeoffs:
  - Greedy vs sampling selection: Greedy is faster but may miss diverse outputs; sampling is slower but can explore more possibilities
  - k value for token probability extraction: Higher k captures more uncertainty but increases computation
  - Token refresh strategy: Immediate refresh vs waiting for partial token completion affects efficiency

- Failure signatures:
  - No improvement over individual models: Likely indicates poor α choice or insufficient complementary strengths
  - Degraded performance: May indicate information loss during marginalization or poor model compatibility
  - Slow inference: Could be caused by large k values or inefficient state management

- First 3 experiments:
  1. Single-model validation: Run CharED with α=1 on a single model to verify it produces identical output to the original model
  2. Simple ensemble test: Combine two models with known complementary strengths (e.g., coding + math) on their respective benchmarks
  3. α sweep analysis: Test multiple α values on a benchmark to identify optimal weighting and validate Pareto curve behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CharED scale when combining more than two models, particularly in cases where multiple models have complementary strengths?
- Basis in paper: [inferred] The paper mentions that "This lays the groundwork for future experiments investigating the combination of more than two models" and suggests that exploring combinations of multiple models is of interest.
- Why unresolved: The paper only evaluates pairwise combinations of models, leaving the performance and behavior of ensembling more than two models unexplored.
- What evidence would resolve it: Experiments combining three or more models on the same benchmarks (HumanEval, GSM8K, ToxiGen) to measure whether additional models provide incremental performance gains or introduce diminishing returns.

### Open Question 2
- Question: What is the impact of using alternative averaging mechanisms (e.g., geometric mean, weighted arithmetic-geometric combinations) compared to the current arithmetic mean in CharED?
- Basis in paper: [explicit] The paper states "While the current averaging mechanism in CHAR ED uses arithmetic means, further exploring more sophisticated variants such as geometric means or weighted combinations of arithmetic and geometric means is of interest."
- Why unresolved: The paper uses only arithmetic averaging but explicitly suggests investigating other averaging methods without providing experimental results.
- What evidence would resolve it: Comparative experiments using different averaging mechanisms (arithmetic, geometric, hybrid) on the same benchmark tasks to determine which produces optimal performance.

### Open Question 3
- Question: How does CharED perform on more complex compositional tasks that require integrating multiple skills (e.g., mathematical reasoning within code generation or combining factual knowledge with reasoning)?
- Basis in paper: [inferred] The paper mentions testing on coding, math, and toxicity benchmarks separately but doesn't explore compositional tasks that combine these skills.
- Why unresolved: The current evaluation uses separate benchmarks for different domains, but real-world applications often require combining multiple capabilities simultaneously.
- What evidence would resolve it: Experiments on benchmarks that require multi-skill integration, such as program synthesis problems with mathematical constraints or reasoning tasks that require both language understanding and numerical computation.

## Limitations

- Tokenization information loss: The marginalization process may not preserve sufficient information from original models, particularly for rare characters or domain-specific tokens
- Limited benchmark scope: Evaluation covers only coding, mathematics, and toxicity avoidance tasks, with unknown effectiveness on other domains like reasoning or multilingual tasks
- Computational overhead: Requires running two models in parallel at inference time, potentially doubling computational costs and latency

## Confidence

**High Confidence**: The core methodology of converting token probabilities to character probabilities through marginalization is well-defined and mathematically sound. The implementation of this conversion step is straightforward and verifiable.

**Medium Confidence**: The empirical results showing performance improvements across benchmarks are promising, but the evaluation scope is limited. The Pareto-optimal curves and complementary strength findings are based on specific model pairs and tasks, requiring broader validation.

**Low Confidence**: Claims about the method's effectiveness across different model sizes, tokenization schemes, and domains are largely untested. The scalability and generalizability of CharED beyond the evaluated 7B parameter models on coding, math, and toxicity tasks remains speculative.

## Next Checks

1. **Information Preservation Analysis**: Conduct ablation studies measuring the KL divergence between original token distributions and reconstructed distributions after CharED's marginalization and sampling process. This would quantify information loss and validate the core assumption that character-level marginalization preserves sufficient model information.

2. **Cross-Domain Generalization Test**: Evaluate CharED on additional benchmarks beyond coding, math, and toxicity - including reasoning tasks (BBH, AGIEval), long-form generation (NarrativeQA, WikiHop), and multilingual tasks. This would test the breadth of the method's effectiveness across diverse LLM capabilities.

3. **Scalability Assessment**: Test CharED with larger model sizes (13B, 33B, 70B parameters) and different architecture families (transformers, state-space models). Measure both performance scaling and computational overhead to understand practical deployment constraints and identify optimal use cases for the method.