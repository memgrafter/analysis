---
ver: rpa2
title: Learning in complex action spaces without policy gradients
arxiv_id: '2410.06317'
source_url: https://arxiv.org/abs/2410.06317
tags:
- learning
- policy
- action
- qmle
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the belief that policy gradient methods
  are inherently better for complex action spaces than action-value methods. The authors
  identify three core principles underlying the success of policy gradients: Monte
  Carlo approximation of summation/integration, amortized maximization using maximum
  likelihood estimation (MLE), and action-in architectures for representation learning.'
---

# Learning in complex action spaces without policy gradients

## Quick Facts
- arXiv ID: 2410.06317
- Source URL: https://arxiv.org/abs/2410.06317
- Reference count: 40
- Key outcome: QMLE achieves competitive performance with state-of-the-art policy gradient methods on continuous control tasks with up to 38 action dimensions

## Executive Summary
This paper challenges the prevailing belief that policy gradient methods are inherently better suited for complex action spaces than action-value methods. The authors identify three core principles that enable policy gradients to scale: Monte Carlo approximation for summation/integration, amortized maximization through maximum likelihood estimation, and action-in architectures for representation learning. They propose Q-learning with Maximum Likelihood Estimation (QMLE), a framework that incorporates these principles into action-value learning, achieving competitive performance compared to state-of-the-art methods like DMPO and D4PG in the DeepMind Control Suite.

## Method Summary
QMLE is a framework that extends Q-learning to complex action spaces by incorporating three key mechanisms: (1) Monte Carlo sampling to approximate the arg max operation over actions, (2) cached refinements of arg max approximations using maximum likelihood estimation (MLE) with an ensemble of arg max predictors, and (3) action-in architectures that take both states and actions as inputs for scalable action-value inference. The method uses prioritized experience replay and balances between uniform sampling and learned arg max predictors to explore the action space effectively. QMLE can handle both continuous and discrete action spaces, with flexibility in the choice of arg max predictor distributions.

## Key Results
- QMLE matches or exceeds state-of-the-art policy gradient methods (DMPO, D4PG) on 18 continuous control tasks from DeepMind Control Suite
- Performance gains from amortized maximization via MLE range from 1.3% to 45.1% across different tasks
- QMLE with action-in architectures outperforms action-out architectures, especially in high-dimensional action spaces (up to 38 dimensions)
- Ablation studies confirm the importance of each principle: Monte Carlo approximation, amortized maximization, and action-in architectures

## Why This Works (Mechanism)

### Mechanism 1
Policy gradients scale to complex action spaces by using Monte Carlo methods to approximate summation/integration over actions. Instead of computing the full sum over all actions, they estimate the expectation using a single sampled action (on-policy) or a small set of sampled actions (off-policy). This replaces an intractable integral with a tractable estimator. The sampled estimator converges to the true expectation as the number of samples increases.

### Mechanism 2
Policy gradients achieve amortized maximization through a special form of maximum likelihood estimation. The policy gradient update weights the log-likelihood gradient by the action-value Q(s,a), effectively performing a modified MLE that favors actions with higher Q-values. This creates a learned policy that approximates the arg max over actions. Each policy update improves the current approximate maximizer of an interdependent action-value function.

### Mechanism 3
Action-in architectures enable representation learning and generalization across the joint state-action space, which is critical for scaling to complex action spaces. By treating both states and actions as inputs, action-in architectures allow backpropagation to learn representations over the joint space. This enables generalization to unseen actions and states, whereas action-out architectures are limited in their capacity for generalizing across actions.

## Foundational Learning

- **Markov Decision Process (MDP)**: The paper is about reinforcement learning in MDPs, so understanding the MDP framework (states, actions, transitions, rewards) is fundamental to grasping the problem setup and solution approaches. Quick check: What tuple defines an MDP and what does each component represent?

- **Policy Gradient Theorem**: The paper builds on the policy gradient theorem to explain why policy gradients work and how they can be adapted to action-value methods. Quick check: What is the relationship between the policy gradient and the action-value function according to the policy gradient theorem?

- **Maximum Likelihood Estimation (MLE)**: The paper uses MLE as a key mechanism for amortized maximization in both policy gradients and their proposed QMLE framework. Quick check: How does maximum likelihood estimation work to find the parameters of a distribution that best fit observed data?

## Architecture Onboarding

- **Component map**: QMLE consists of three main components: (1) an action-value function approximator Q(s,a), (2) an ensemble of arg max predictors fθ(.|s) that model action distributions, and (3) a sampling mechanism for generating action candidates. The action-value function uses an action-in architecture, and the arg max predictors are trained via MLE on cached arg max approximations.

- **Critical path**: For each training step: (1) sample actions using the arg max predictors and uniform sampling, (2) approximate the greedy action by finding the action with maximum Q-value among sampled actions, (3) execute this action in the environment and store the transition, (4) sample a batch from replay buffer, (5) for each transition, sample actions to approximate the target max, (6) compute TD target and update Q-network, (7) update arg max predictors using MLE on cached arg max approximations.

- **Design tradeoffs**: The main tradeoff is between sampling budget (mtarget, mgreedy) and computational cost. More samples give better arg max approximations but increase inference cost. The uniform sampling ratio (ρ0) versus learned arg max predictors (ρ1...ρk) trades off exploration vs. exploitation of learned approximations.

- **Failure signatures**: (1) Poor performance in high-dimensional action spaces suggests insufficient sampling budget or ineffective arg max predictors. (2) Instability or divergence may indicate learning rate issues or insufficient exploration. (3) Suboptimal policies suggest the action-in architecture isn't generalizing well or the arg max predictors are converging to local optima.

- **First 3 experiments**:
  1. Implement QMLE on a simple continuous bandit problem (like the 2D bimodal example) to verify the basic mechanism works and compare with DPG.
  2. Test QMLE on a low-dimensional continuous control task (e.g., Pendulum) with varying sampling budgets to understand the tradeoff between accuracy and computation.
  3. Implement the ablation study without amortized maximization (only uniform sampling) on a moderate-dimensional task to confirm the importance of the MLE-based arg max predictors.

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the analysis, several important questions remain unresolved:

### Open Question 1
How does the performance of QMLE scale with increasing action space complexity beyond the 38 dimensions tested in the DeepMind Control Suite? The paper states "Our results showed that QMLE performs strongly in continuous control problems with up to 38 action dimensions" but doesn't explore higher dimensions. Testing higher dimensions would require either creating more complex tasks or finding new benchmarks.

### Open Question 2
What is the optimal balance between local sampling (around current delta parameters) and global sampling (uniform over full action space) for different types of reward landscapes? The illustrative example shows that local sampling can get stuck in local optima while global sampling finds global optima, but the paper doesn't systematically explore this trade-off across different problem types.

### Open Question 3
How does the choice of arg max predictor distribution family (delta vs. factored categorical vs. other distributions) impact performance across different action space types? The paper mentions using both delta and factored categorical distributions in experiments and notes the framework is flexible regarding ensemble composition, but doesn't systematically compare different distribution families.

### Open Question 4
What is the computational overhead of QMLE compared to policy gradient methods when scaling to extremely large action spaces (e.g., millions or billions of discrete actions)? The paper demonstrates QMLE working with up to 338 discrete actions but doesn't explore the scaling behavior at much larger scales.

## Limitations
- Theoretical analysis is primarily heuristic rather than rigorous, relying on intuitive arguments rather than formal proofs
- Empirical evaluation covers only 18 tasks from DeepMind Control Suite and doesn't test extreme cases like very high-dimensional actions (>38 dimensions)
- Sampling-based arg max approximation introduces a hyperparameter tuning burden that isn't fully explored

## Confidence
- **High confidence**: The identification of three core principles (Monte Carlo approximation, amortized maximization, action-in architectures) underlying policy gradient success - these are well-established concepts with clear theoretical grounding
- **Medium confidence**: The claim that QMLE can match or exceed policy gradient performance in complex action spaces - supported by experimental results but limited to a specific benchmark suite
- **Medium confidence**: The assertion that action-in architectures are critical for scalability - theoretically sound but the ablation study only compares against action-out architectures, not against other approaches

## Next Checks
1. Test QMLE on a benchmark with significantly higher action dimensionality (e.g., 50+ dimensions) to evaluate scalability limits and determine if the sampling budget needs to scale exponentially with dimension
2. Implement a version of QMLE without the amortized maximization component (pure uniform sampling) on a multimodal task to verify the claimed 45% improvement from the MLE-based arg max predictors
3. Compare QMLE against a state-of-the-art policy gradient method (like DMPO) on a set of tasks where action-out architectures fail, to confirm the claimed advantage of action-in architectures for complex action spaces