---
ver: rpa2
title: Online Reinforcement Learning with Passive Memory
arxiv_id: '2410.14665'
source_url: https://arxiv.org/abs/2410.14665
tags:
- memory
- regret
- proof
- density
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes an online reinforcement learning algorithm
  that uses pre-collected passive memory to improve performance. The key contributions
  include: (1) proving that the performance of the regularized linear programming
  formulation of RL depends on the quality of passive memory, with regret bounded
  by the KL divergence between the optimal and memory distributions; (2) establishing
  a minimax lower bound for discounted online RL with finite horizon; (3) providing
  a regret upper bound that is near-minimax optimal, showing that using passive memory
  with good coverage of the optimal policy''s occupancy measure leads to improved
  regret; and (4) demonstrating that the approach works for both discrete and continuous
  state-action spaces, with specific density estimation techniques (plug-in for discrete,
  kernel for continuous) achieving the theoretical bounds.'
---

# Online Reinforcement Learning with Passive Memory

## Quick Facts
- arXiv ID: 2410.14665
- Source URL: https://arxiv.org/abs/2410.14665
- Reference count: 40
- One-line primary result: Near-minimax optimal online RL algorithm using passive memory with regret bounds depending on KL divergence between optimal and memory distributions

## Executive Summary
This paper introduces an online reinforcement learning algorithm that leverages pre-collected passive memory to improve performance. The key insight is that the quality of passive memory, measured by KL divergence between the optimal policy's occupancy measure and the memory distribution, directly impacts the sub-optimality of regret. The algorithm combines regularized linear programming formulations of RL with density estimation techniques, providing theoretical guarantees for both discrete and continuous state-action spaces.

## Method Summary
The method uses a regularized linear programming formulation of RL that incorporates passive memory through KL divergence regularization. The algorithm employs online mirror descent with density estimation error bounds, using plug-in estimators for discrete spaces and kernel density estimators for continuous spaces. The regret bound combines standard online learning regret with density estimation error, achieving near-minimax optimality when passive memory has good coverage of the optimal policy's occupancy measure.

## Key Results
- Proves performance of regularized LP formulation depends on KL divergence between optimal and memory distributions
- Establishes minimax lower bound for discounted online RL with finite horizon
- Provides near-minimax optimal regret upper bound that scales with density estimation error and KL divergence
- Generalizes approach to both discrete (plug-in estimator) and continuous (kernel estimator) state-action spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Performance improvement depends on quality of passive memory, measured by KL