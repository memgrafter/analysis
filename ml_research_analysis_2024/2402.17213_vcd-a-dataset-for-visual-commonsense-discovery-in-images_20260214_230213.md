---
ver: rpa2
title: 'VCD: A Dataset for Visual Commonsense Discovery in Images'
arxiv_id: '2402.17213'
source_url: https://arxiv.org/abs/2402.17213
tags:
- commonsense
- visual
- image
- vcdm
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VCD, a task for discovering fine-grained
  visual commonsense across multiple object types in images. The authors construct
  VCDD, a dataset with 100K+ images and 14M object-commonsense pairs spanning explicit
  (directly observable) and implicit (inferred) commonsense across property, action,
  and space categories.
---

# VCD: A Dataset for Visual Commonsense Discovery in Images

## Quick Facts
- arXiv ID: 2402.17213
- Source URL: https://arxiv.org/abs/2402.17213
- Authors: Xiangqing Shen; Fanfan Wang; Siwei Wu; Rui Xia
- Reference count: 40
- Primary result: Introduces VCD task and VCDD dataset for visual commonsense discovery; proposes VCDM model outperforming GPT-4V on implicit commonsense extraction

## Executive Summary
This paper introduces VCD, a novel task for discovering fine-grained visual commonsense across multiple object types in images. The authors construct VCDD, a large-scale dataset with 100K+ images and 14M object-commonsense pairs spanning explicit and implicit commonsense across property, action, and space categories. They propose VCDM, a generative model combining vision-language models with instruction tuning to extract diverse commonsense triples. Automatic and human evaluations demonstrate VCDM's superiority over baselines including GPT-4V in implicit commonsense discovery, with applications showing improved performance on downstream visual understanding tasks.

## Method Summary
The authors define Visual Commonsense Discovery (VCD) as extracting object-centric commonsense triples from images. They construct VCDD by mapping Visual Genome annotations and ConceptNet knowledge to defined commonsense types, then develop VCDM - a generative model that combines an OFA vision-language backbone with instruction tuning. The model learns to correlate instructions with images during training and generalizes to discover both explicit (directly observable) and implicit (inferred) commonsense during inference. The approach uses carefully crafted templates combining image, object name, bounding box, and commonsense type information to standardize generation across different commonsense categories.

## Key Results
- VCDM outperforms GPT-4V on implicit commonsense discovery tasks
- Human evaluation shows VCDM achieves higher correctness and completeness than baseline models
- Applications to downstream tasks (commonsense evaluation, VQA) demonstrate improved visual understanding and reasoning
- VCDD dataset encompasses 100K+ images with 14M object-commonsense pairs across property, action, and space categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VCDM successfully integrates vision-language models with instruction tuning to discover both explicit and implicit visual commonsense
- Mechanism: VCDM leverages an OFA vision-language model with object localization, enhanced by instruction tuning using templates that combine image, object name, bounding box, and commonsense type information
- Core assumption: Vision-language models pretrained on image-text pairs can generalize commonsense knowledge beyond visible content when provided appropriate instructions
- Evidence anchors: [abstract] states VCDM combines vision-language model with instruction tuning; [section] shows it learns to correlate instructions and images during training

### Mechanism 2
- Claim: VCD dataset construction effectively bridges structured commonsense knowledge bases and visually grounded representations
- Mechanism: The dataset maps Visual Genome triples to commonsense types using part-of-speech rules, extracts additional commonsense from regional phrases through parsing, and retrieves implicit commonsense from ConceptNet based on object synsets
- Core assumption: Visual Genome annotations and ConceptNet knowledge can be effectively aligned to create comprehensive explicit and implicit commonsense datasets
- Evidence anchors: [abstract] states VCDD spans explicit and implicit commonsense across property, action, and space categories

### Mechanism 3
- Claim: The explicit/implicit commonsense distinction is meaningful for visual understanding
- Mechanism: Explicit commonsense is directly observable (e.g., "car is yellow"), while implicit requires inference (e.g., "car is used for traveling")
- Core assumption: Humans use both observable and inferable knowledge when interpreting visual scenes
- Evidence anchors: [abstract] introduces three-level taxonomy integrating Seen and Unseen commonsense; [section] distinguishes between directly observable and inference-based commonsense

## Foundational Learning

- Concept: Part-of-speech tagging and dependency parsing
  - Why needed here: Used to map regional phrases in Visual Genome to commonsense triples by identifying syntactic structures
  - Quick check question: How would you parse "a thin man behind the yellow car" to extract the triple (man, /E/S/Relatedness, behind car)?

- Concept: Synset generation and word sense disambiguation
  - Why needed here: Used to enhance coverage of implicit commonsense retrieval from ConceptNet by considering different senses of object names
  - Quick check question: Why is it important to lemmatize object names before retrieving synsets from ConceptNet?

- Concept: Instruction tuning for vision-language models
  - Why needed here: Enables VCDM to follow specific instructions for discovering different types of visual commonsense from images
  - Quick check question: What are the key components that must be included in the instruction template for VCDM to work effectively?

## Architecture Onboarding

- Component map: Input (Image, object name, bounding box, instruction) -> OFA Encoder (ResNet + Transformer) -> Transformer Decoder -> Output (commonsense triples)

- Critical path:
  1. Parse input instruction to identify commonsense type and object
  2. Encode image region specified by bounding box
  3. Generate commonsense triples autoregressively
  4. Post-process output to maintain diversity using [sep] tokens for multiple triples

- Design tradeoffs:
  - Model scale vs. computational efficiency (tiny to large variants)
  - Number of generated triples vs. quality (sampling strategy)
  - Explicit vs. implicit commonsense discovery (different instruction templates)
  - Object localization precision vs. generation speed

- Failure signatures:
  - Poor bounding box localization leading to wrong object context
  - Failure to follow instruction templates resulting in irrelevant outputs
  - Over-generation of commonsense triples leading to noise
  - Bias toward explicit commonsense at the expense of implicit knowledge

- First 3 experiments:
  1. Test VCDM with and without bounding box information to measure impact of object localization
  2. Compare performance on explicit vs. implicit commonsense discovery to validate distinction
  3. Evaluate different model scales (tiny to large) to find optimal tradeoff between performance and efficiency

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the work:
- How to extend the approach to other modalities beyond images
- How to improve the boundary between explicit and implicit commonsense
- How to scale the approach to even larger datasets and more diverse commonsense types

## Limitations
- The boundary between explicit and implicit commonsense may be subjective and lead to annotation inconsistencies
- Dataset construction relies heavily on parsing Visual Genome and ConceptNet, potentially introducing bias toward common objects
- Evaluation metrics (BLEU, ROUGE, METEOR) may not fully capture semantic quality and diversity of commonsense triples

## Confidence
- **High Confidence**: Overall task formulation and dataset construction methodology are well-defined and reproducible
- **Medium Confidence**: Effectiveness of instruction tuning for VCDM in discovering diverse commonsense types
- **Medium Confidence**: The explicit/implicit commonsense distinction is meaningful though boundaries may be subjective

## Next Checks
1. Conduct systematic study with human annotators to validate explicit/implicit commonsense distinction by categorizing sample triples and measuring inter-annotator agreement
2. Evaluate VCDM's performance on images from domains not well-represented in Visual Genome (medical imaging, satellite imagery) to test generalization
3. Perform ablation studies on downstream tasks to quantify contribution of explicit vs. implicit commonsense discovered by VCDM