---
ver: rpa2
title: Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement
  in LLMs
arxiv_id: '2407.03181'
source_url: https://arxiv.org/abs/2407.03181
tags:
- dcot
- cots
- answer
- reasoning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diverse Chain of Thought (DCoT), a method
  that fine-tunes language models to generate multiple reasoning chains within a single
  inference step. Unlike prior approaches that generate independent chains in parallel,
  DCoT enables within-inference refinement by allowing models to build upon previous
  reasoning paths.
---

# Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement in LLMs

## Quick Facts
- arXiv ID: 2407.03181
- Source URL: https://arxiv.org/abs/2407.03181
- Authors: Haritz Puerto; Tilek Chubakov; Xiaodan Zhu; Harish Tayyar Madabushi; Iryna Gurevych
- Reference count: 40
- One-line primary result: DCoT improves performance over standard CoT baselines, particularly on tasks with large output spaces, by enabling within-inference refinement without external feedback.

## Executive Summary
This paper introduces Diverse Chain of Thought (DCoT), a method that fine-tunes language models to generate multiple reasoning chains within a single inference step. Unlike prior approaches that generate independent chains in parallel, DCoT enables within-inference refinement by allowing models to build upon previous reasoning paths. The authors construct training datasets where each question is paired with multiple diverse reasoning chains, concatenated into a single sequence. Experiments across various model families and scales (1.3B to 70B parameters) show that DCoT improves performance over standard Chain of Thought baselines, particularly on tasks with large output spaces like numeric and span extraction tasks.

## Method Summary
DCoT fine-tunes models using instruction-tuning on specially formatted data where each question is paired with multiple diverse reasoning chains concatenated into a single sequence. The method uses LoRA with 4-bit quantization, training for 3 epochs with best checkpoint selected based on dev set performance. Models tested include Phi 1.5 (1.3B), Phi 2 (2.7B), LLaMA-2 7B, LLaMA-2 13B, and LLaMA-2 70B. The training data is constructed by generating CoTs for each question in 9 training datasets using GPT-3.5 turbo with high temperature (0.7) and specified CoT triggers.

## Key Results
- DCoT achieves consistent improvements over standard CoT baselines across all tested model families and scales
- Performance gains are particularly pronounced for tasks with large output spaces (numeric and span extraction tasks)
- Manual evaluations confirm that improvements stem from the model's ability to revise initial reasoning chains within the same inference step

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on diverse CoTs enables within-inference refinement without external feedback
- Mechanism: Models learn to generate multiple reasoning chains in a single inference step, where each subsequent chain has access to previous chains and can revise earlier reasoning
- Core assumption: Access to previous reasoning chains during generation allows models to identify and correct errors in earlier reasoning
- Evidence anchors:
  - [abstract] "DCoT allows LLMs to gain the ability to perform within-inference refinement of reasoning chains without requiring external feedback"
  - [section 3] "we propose to concatenate CoTs into a single sequence, forming training pairs of the format (question, [CoTs])"
  - [corpus] Weak evidence - no direct citations support within-inference refinement specifically

### Mechanism 2
- Claim: Sequential generation of CoTs within a single inference step is more effective than parallel generation with post-hoc ensembling
- Mechanism: The model can condition subsequent CoTs on previous ones, creating a revision process rather than independent attempts
- Core assumption: Information from previous reasoning chains improves the quality of subsequent chains
- Evidence anchors:
  - [section 1] "unlike prior work that primarily operate on parallel CoT generations"
  - [section 4.6] "we observe that the second CoT of DCoT@2 exhibits a different reasoning pattern from the first in 15 cases"
  - [corpus] Weak evidence - corpus lacks specific studies comparing sequential vs parallel CoT generation

### Mechanism 3
- Claim: The method is particularly effective for tasks with large result state spaces
- Mechanism: Larger solution spaces provide more opportunities for revision and alternative reasoning paths
- Core assumption: Tasks with binary or symbolic outputs have limited room for meaningful revision
- Evidence anchors:
  - [abstract] "These improvements are particularly impactful for tasks with a large result space, such as those involving numeric answers"
  - [section 4.1] "performance on binary and symbolic tasks presents a more mixed picture"
  - [corpus] Weak evidence - corpus lacks direct analysis of result state space effects

## Foundational Learning

- Concept: Chain of Thought (CoT) prompting
  - Why needed here: DCoT builds on CoT methodology, understanding the baseline is essential for grasping the innovation
  - Quick check question: What distinguishes DCoT from standard CoT prompting in terms of inference procedure?

- Concept: Supervised fine-tuning vs instruction tuning
  - Why needed here: DCoT uses instruction tuning on specially formatted data, understanding this distinction is crucial for implementation
  - Quick check question: How does the training data format for DCoT differ from standard instruction-tuning datasets?

- Concept: Temperature and sampling strategies
  - Why needed here: The paper discusses self-consistency as an extension, requiring understanding of how sampling affects reasoning diversity
  - Quick check question: Why might high-temperature sampling be useful when generating multiple CoTs for self-consistency?

## Architecture Onboarding

- Component map: Training pipeline (data generation → fine-tuning → evaluation), inference pipeline (single-step multi-chain generation), evaluation framework (task-specific metrics)
- Critical path: Data generation → fine-tuning with DCoT format → evaluation with k=2 chains → analysis of improvement patterns
- Design tradeoffs: k=2 provides good balance of improvement vs cost, but may miss deeper revisions; higher k increases computation without proportional gains
- Failure signatures: Uniform CoT outputs across attempts, no improvement from k=2 to k=1, performance degradation on tasks where CoT typically helps
- First 3 experiments:
  1. Compare DCoT@1 vs CoT

## Open Questions the Paper Calls Out
None

## Limitations
- Data Quality Uncertainty: Relies on GPT-3.5-turbo for training CoT generation without explicit quality control mechanisms
- Task Coverage Limitation: Analysis of effectiveness on tasks with large output spaces remains correlational rather than experimentally validated
- Mechanism Validation Gap: Core claims about within-inference refinement are supported by qualitative observations rather than systematic analysis

## Confidence
- High Confidence: Empirical observation that DCoT improves performance over standard CoT across tested model families and tasks
- Medium Confidence: Claims about sequential generation being more effective than parallel generation and improvements on large output space tasks
- Low Confidence: Specific mechanism of within-inference refinement and claim that models actively revise earlier reasoning

## Next Checks
1. Conduct ablation studies comparing DCoT with shuffled chain orders and with identical chains to isolate whether improvement comes from accessing previous chains or simply seeing multiple reasoning approaches
2. Perform controlled experiments on tasks with varying output space sizes while holding other task characteristics constant to validate the specific relationship between output space size and DCoT effectiveness
3. Implement a quantitative measure of revision behavior by analyzing the edit distance or semantic similarity between the first and second CoTs generated by DCoT models, and correlate this with performance improvements across tasks