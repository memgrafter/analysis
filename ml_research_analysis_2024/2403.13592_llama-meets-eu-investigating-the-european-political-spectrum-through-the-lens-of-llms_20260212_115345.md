---
ver: rpa2
title: 'Llama meets EU: Investigating the European Political Spectrum through the
  Lens of LLMs'
arxiv_id: '2403.13592'
source_url: https://arxiv.org/abs/2403.13592
tags:
- political
- party
- parties
- llama
- setting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates political biases in large language models
  (LLMs) using the European political context. The authors use debates from the European
  Parliament and the EUandI questionnaire to audit Llama Chat's political knowledge
  and reasoning capabilities.
---

# Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs

## Quick Facts
- arXiv ID: 2403.13592
- Source URL: https://arxiv.org/abs/2403.13592
- Reference count: 20
- Key outcome: This paper investigates political biases in large language models (LLMs) using the European political context. The authors use debates from the European Parliament and the EUandI questionnaire to audit Llama Chat's political knowledge and reasoning capabilities. They also fine-tune Llama Chat on speeches from specific EU political parties to assess how adaptation affects the model's political alignment. Results show that Llama Chat has considerable political knowledge and can reason in context. The adapted models are substantially re-aligned towards the respective parties' positions, with better results for parties with consistent ideologies compared to "big tent" parties. This work lays the foundation for using chat-based LLMs as data-driven conversational engines to assist research in political science.

## Executive Summary
This paper investigates political biases in large language models (LLMs) using the European political context. The authors use debates from the European Parliament and the EUandI questionnaire to audit Llama Chat's political knowledge and reasoning capabilities. They also fine-tune Llama Chat on speeches from specific EU political parties to assess how adaptation affects the model's political alignment. Results show that Llama Chat has considerable political knowledge and can reason in context. The adapted models are substantially re-aligned towards the respective parties' positions, with better results for parties with consistent ideologies compared to "big tent" parties. This work lays the foundation for using chat-based LLMs as data-driven conversational engines to assist research in political science.

## Method Summary
The authors use the EU Debates corpus (87k speeches from European Parliament) and EUandI questionnaire (22 political statements with 5-option responses) to evaluate Llama 2 13B chat model. They conduct contextualized auditing experiments using three settings: party name, party justification, and guessing party from justification. The model is fine-tuned on party-specific speeches using LoRA with learning rate 2e-4 for 10 epochs, then evaluated on EUandI to measure political alignment across 7 thematic categories using radar plots.

## Key Results
- Llama Chat shows considerable knowledge of national parties' positions and is capable of reasoning in context
- Adapted, party-specific models are substantially re-aligned towards respective positions
- Better results for parties with consistent ideologies compared to "big tent" parties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on party-specific speech data can realign a model's political stance toward that party's ideology.
- Mechanism: LoRA fine-tuning adapts the model's weight parameters to match the linguistic patterns and topic distributions in party speeches, which reflect the party's political positions.
- Core assumption: The model's original stance is sufficiently different from the target party that adaptation produces measurable change.
- Evidence anchors:
  - [abstract] "The adapted, party-specific, models are substantially re-aligned towards respective positions"
  - [section] "We fine-tune Llama Chat on the speeches from the EUD EBATES dataset using Low-Rank Adaptation (LoRA)"
  - [corpus] Weak - no direct evidence that LoRA produces the claimed realignment; correlation may not imply causation.
- Break condition: If party speeches are not representative of their ideology, or if the model's original stance is already close to the target party's stance, adaptation will not produce significant change.

### Mechanism 2
- Claim: Models can predict political positions using contextual information like party names or justifications.
- Mechanism: The model uses its pre-trained knowledge to infer political positions from contextual cues, either party names (leveraging known associations) or justifications (reasoning from stated positions).
- Core assumption: The model has sufficient prior knowledge about political parties and their positions.
- Evidence anchors:
  - [abstract] "Llama Chat shows considerable knowledge of national parties' positions and is capable of reasoning in context"
  - [section] "Setting A: Given the results in Setting A, where contextualization solely relies on parties' names, accuracy, i.e., the ability of a model to predict a party's official position on a given statement, varies"
  - [corpus] Weak - the corpus shows political debate data but doesn't directly support the model's reasoning capability.
- Break condition: If the model lacks sufficient prior knowledge about parties or their positions, or if contextual cues are insufficient, prediction accuracy will be low.

### Mechanism 3
- Claim: Models can identify which party a given justification belongs to.
- Mechanism: The model uses its knowledge of party positions and reasoning ability to match justifications to the most likely party.
- Core assumption: Party positions are sufficiently distinct that the model can differentiate between them based on justifications.
- Evidence anchors:
  - [abstract] "Llama Chat shows considerable knowledge of national parties' positions and is capable of reasoning in context"
  - [section] "In contrast, we see an exception when it comes to parties affiliated with ID"
  - [corpus] Weak - the corpus provides party speeches but doesn't directly support the model's ability to identify parties from justifications.
- Break condition: If party positions are too similar or justifications are ambiguous, the model will struggle to correctly identify the party.

## Foundational Learning

- Concept: Political spectrum and party ideologies
  - Why needed here: Understanding the political context is crucial for interpreting the model's behavior and the study's findings.
  - Quick check question: Can you explain the difference between left-wing and right-wing parties, and how EU politics differs from US politics?

- Concept: Fine-tuning and alignment
  - Why needed here: The study uses fine-tuning to adapt the model's political stance, so understanding these concepts is essential.
  - Quick check question: What is the difference between fine-tuning and alignment, and how do they relate to changing a model's behavior?

- Concept: Evaluation metrics for political bias
  - Why needed here: The study uses accuracy and thematic categories to evaluate the model's political knowledge and reasoning, so understanding these metrics is important.
  - Quick check question: How would you design an evaluation metric to measure a model's political bias, and what are the potential challenges?

## Architecture Onboarding

- Component map: EU Debates corpus -> LoRA fine-tuning -> Llama Chat model -> EUandI questionnaire evaluation
- Critical path: Fine-tuning Llama Chat on party speeches using LoRA, then evaluating political alignment with EUandI
- Design tradeoffs: LoRA fine-tuning allows efficient adaptation but may limit the extent of political realignment
- Failure signatures: Low accuracy in contextualized auditing or minimal change in political stance after fine-tuning
- First 3 experiments:
  1. Evaluate the baseline Llama Chat model's political knowledge using the EUAND I questionnaire.
  2. Fine-tune the model on speeches from a specific party and evaluate its political stance.
  3. Compare the political stance of the adapted model to the original party's position using the EUAND I questionnaire.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the size of a large language model significantly impact its ability to accurately represent political stances across diverse European contexts?
- Basis in paper: Explicit. The authors note they only tested Llama Chat models and state "Our study is limited to 13-billion-parameter-sized Llama Chat models" and "Unfortunately, we lack the compute infrastructure to experiment with the available 70-billion-parameter-sized models."
- Why unresolved: The authors only tested one model size and explicitly state they couldn't test larger models. The paper doesn't provide comparative data across different model sizes.
- What evidence would resolve it: Testing the same political stance analysis tasks across multiple model sizes (7B, 13B, 70B parameters) to compare accuracy in representing European political positions.

### Open Question 2
- Question: How does fine-tuning on parliamentary speeches from specific political parties affect the model's ability to maintain factual accuracy while adopting party-aligned positions?
- Basis in paper: Explicit. The authors note "The adapted models can be seen as data-driven mirrors of the parties' ideologies, but are by no means 'perfect', and thus may misrepresent them."
- Why unresolved: The paper doesn't quantify the trade-off between political alignment and factual accuracy, only noting that misrepresentation is possible.
- What evidence would resolve it: Systematic testing of fine-tuned models on fact-checking tasks alongside political stance analysis to measure accuracy degradation.

### Open Question 3
- Question: How does the temporal aspect of parliamentary speeches affect the accuracy of political stance analysis when comparing to contemporary questionnaire responses?
- Basis in paper: Explicit. The authors note "In our adaptation experiments, we use debates from 2009-2023, while the EUAND I questionnaire and parties' responses represent the public pre-EU-elections debate in 2019."
- Why unresolved: The paper acknowledges this as a potential source of misalignment but doesn't investigate how time gaps affect accuracy.
- What evidence would resolve it: Chronological analysis testing model accuracy when trained on speeches from different time periods relative to the questionnaire data.

## Limitations
- Findings are based on a single model (Llama 2 13B) and specific political context (European Parliament)
- Manual annotation process for reconciling model responses with party positions introduces potential subjectivity
- Difficult to determine whether changes result from learning specific party positions or simply memorizing examples

## Confidence
- High Confidence: Core methodology and technical implementation
- Medium Confidence: Interpretation of results regarding party adaptation effects
- Low Confidence: Broader claims about using LLMs as conversational engines for political science research

## Next Checks
1. Replicate fine-tuning and evaluation experiments with at least two additional LLMs (e.g., Mistral, Llama 3) to assess whether observed adaptation effects generalize across model architectures.
2. Conduct rigorous statistical analysis of adaptation effects, including significance testing for differences in accuracy across parties and thematic categories.
3. Implement inter-annotator agreement metrics for the manual annotation process, including multiple annotators rating model responses against party positions.