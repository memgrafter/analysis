---
ver: rpa2
title: 'ProverbEval: Exploring LLM Evaluation Challenges for Low-resource Language
  Understanding'
arxiv_id: '2411.05049'
source_url: https://arxiv.org/abs/2411.05049
tags:
- language
- english
- prompt
- native
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ProverbEval, a benchmark for evaluating
  large language models (LLMs) on low-resource languages using culture-specific proverbs.
  The authors collected proverbs and their meanings in four Ethiopian languages (Amharic,
  Afaan Oromo, Tigrinya, Ge''ez) and English, creating three tasks: multiple choice,
  fill-in-the-blank, and proverb generation.'
---

# ProverbEval: Exploring LLM Evaluation Challenges for Low-resource Language Understanding

## Quick Facts
- arXiv ID: 2411.05049
- Source URL: https://arxiv.org/abs/2411.05049
- Reference count: 18
- LLMs show up to 50% performance variance based on answer choice order in low-resource language evaluations

## Executive Summary
This paper introduces ProverbEval, a benchmark designed to evaluate large language models (LLMs) on low-resource languages using culture-specific proverbs. The authors collected proverbs and their meanings in four Ethiopian languages (Amharic, Afaan Oromo, Tigrinya, Ge'ez) and English, creating three tasks: multiple choice, fill-in-the-blank, and proverb generation. The study reveals significant challenges in LLM evaluation for low-resource languages, including performance variance due to answer choice order, the importance of native language prompts, and the critical role of tokenizer quality over model size.

## Method Summary
The authors created ProverbEval by collecting proverbs and their meanings across four Ethiopian languages and English. They developed three evaluation tasks: multiple choice questions about proverb meanings, fill-in-the-blank completion tasks, and proverb generation from descriptions. Using ElutherAI's Language Model Evaluation Harness, they evaluated open-source and closed-source LLMs in zero-shot settings across these tasks. The evaluation employed accuracy metrics for multiple choice and fill-in-the-blank tasks, and ChrF, BLEU, and TER scores for generation tasks, comparing performance across native language and English prompts.

## Key Results
- LLMs show up to 50% performance variance depending on answer choice order in multiple-choice tasks
- Native language prompts significantly improve performance for generation tasks compared to English descriptions
- Monolingual evaluations outperform cross-lingual ones in generation tasks, with tokenizer quality proving more important than model size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenization quality is more important than model size for low-resource language understanding.
- Mechanism: Better tokenizers reduce subword fertility, leading to improved comprehension of morphologically rich low-resource languages.
- Core assumption: Subword fertility correlates inversely with model performance on language understanding tasks.
- Evidence anchors:
  - [abstract] "tokenizer quality is crucial"
  - [section] "This directly correlates with Figure 2 that the model with the lowest subword fertility is the better and Gemma models are better multilingual models"
  - [corpus] Weak - no direct corpus evidence supporting this specific mechanism

### Mechanism 2
- Claim: Choice order significantly affects model performance in multiple-choice tasks.
- Mechanism: Models exhibit sensitivity to answer position, with performance varying by up to 50% depending on whether correct answers appear first or last.
- Core assumption: LLMs have positional bias in their attention mechanisms that affects answer selection.
- Evidence anchors:
  - [abstract] "performance variances of up to 50%, depending on the order in which answer choices were presented"
  - [section] "As shown in Table 3, smaller models show a difference of accuracy close to 30% and 50% when the answers are provided in the first choice"
  - [corpus] Weak - corpus shows related papers but no direct evidence about choice order sensitivity

### Mechanism 3
- Claim: Native language prompts improve performance for language understanding tasks.
- Mechanism: Models process instructions more effectively when given in the target language, reducing cross-lingual interference.
- Core assumption: Models have stronger comprehension when prompt language matches task language.
- Evidence anchors:
  - [abstract] "Native language proverb descriptions significantly improve tasks such as proverb generation"
  - [section] "Using in-language prompt results min 0 and max ±3 differences between native and english multiple choice"
  - [corpus] Weak - corpus mentions related benchmarks but no direct evidence about native prompt benefits

## Foundational Learning

- Concept: Subword tokenization and fertility
  - Why needed here: Understanding how tokenization affects model performance on morphologically rich languages
  - Quick check question: What does subword fertility measure and why does lower value indicate better tokenization for low-resource languages?

- Concept: Zero-shot vs few-shot learning evaluation
  - Why needed here: The paper compares model performance with and without examples, requiring understanding of evaluation paradigms
  - Quick check question: How do zero-shot and few-shot evaluations differ in terms of model exposure to task examples?

- Concept: Cross-lingual vs monolingual evaluation
  - Why needed here: The study distinguishes between evaluating models in their native language versus translated prompts
  - Quick check question: What are the key differences between cross-lingual and monolingual evaluation approaches?

## Architecture Onboarding

- Component map: Data collection → Task generation → Evaluation harness → Model interface → Result analysis
- Critical path: Proverb collection → Task creation → Model evaluation → Result analysis → Benchmark release
- Design tradeoffs:
  - Multiple-choice vs open-ended: Multiple-choice easier to score but may miss nuanced understanding
  - Native vs translated prompts: Native better for monolingual tasks, translated enables cross-lingual evaluation
  - Choice order randomization: Reduces bias but increases evaluation complexity
- Failure signatures:
  - High variance across choice orders indicates model sensitivity issues
  - Poor performance on generation tasks suggests tokenization or instruction-following problems
  - Minimal difference between native and English prompts may indicate lack of true multilingual understanding
- First 3 experiments:
  1. Test a single model across all tasks with randomized choice order to establish baseline variance
  2. Compare native vs English prompt performance on multiple-choice task for one language pair
  3. Evaluate generation task performance using ChrF metric with native vs English descriptions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs vary across different task types (multiple-choice, fill-in-the-blank, generation) when evaluated on low-resource languages?
- Basis in paper: [explicit] The paper introduces three distinct tasks and evaluates LLMs across these tasks for low-resource languages, noting varying performance levels.
- Why unresolved: The paper presents results but does not provide a comprehensive comparative analysis across all three task types for each language.
- What evidence would resolve it: A detailed comparative analysis of LLM performance across all task types for each low-resource language, highlighting strengths and weaknesses.

### Open Question 2
- Question: What is the impact of using native language descriptions versus English descriptions on the quality of generated proverbs by LLMs?
- Basis in paper: [explicit] The paper observes that models perform better when descriptions are provided in native languages compared to English for proverb generation tasks.
- Why unresolved: While the paper notes this observation, it does not explore the underlying reasons or provide a quantitative comparison of the quality differences.
- What evidence would resolve it: A detailed analysis of the quality differences in generated proverbs, including human evaluation or automated metrics, comparing native and English descriptions.

### Open Question 3
- Question: How does the sensitivity to choice order in multiple-choice tasks differ between monolingual and cross-lingual evaluations?
- Basis in paper: [explicit] The paper finds that models show sensitivity to choice order, with larger effects in cross-lingual tasks.
- Why unresolved: The paper provides results but does not explore the reasons behind the difference in sensitivity or its implications for LLM evaluation.
- What evidence would resolve it: An in-depth study of the factors contributing to choice order sensitivity, including linguistic and cultural aspects, and their impact on evaluation design.

## Limitations

- The study focuses on four Ethiopian languages, limiting generalizability to other language families
- The use of automated metrics (ChrF, BLEU, TER) may not fully capture the nuanced understanding required for culturally-specific proverbs
- Potential cultural bias in proverb selection and translation accuracy between languages is not thoroughly examined

## Confidence

**High Confidence:** The finding that larger models generally outperform smaller ones in low-resource language tasks aligns with established patterns in LLM research. The observation that tokenizer quality significantly impacts performance has strong theoretical support from subword tokenization literature.

**Medium Confidence:** The claim about native language prompts improving performance has reasonable supporting evidence but may vary significantly based on model architecture and training data composition. The 50% variance in multiple-choice tasks due to answer ordering is well-documented in the results but requires further investigation into underlying mechanisms.

**Low Confidence:** The assertion that monolingual evaluations consistently outperform cross-lingual ones for generation tasks needs more extensive validation across different language families and model types. The cultural specificity of proverbs may introduce evaluation artifacts that aren't fully accounted for.

## Next Checks

1. **Cross-architectural choice order sensitivity test:** Evaluate whether the observed 50% variance due to answer ordering persists across different model architectures (decoder-only, encoder-decoder, transformer variants) to determine if this represents a fundamental limitation or architecture-specific bias.

2. **Extended language family validation:** Test ProverbEval across additional low-resource language families (e.g., Slavic, Southeast Asian, Indigenous American languages) to assess whether the observed patterns generalize beyond the Ethiopian language family and reveal potential typological dependencies.

3. **Cultural bias and idiom comprehension control:** Design parallel evaluation tasks using culturally neutral idioms versus culturally specific proverbs to isolate whether performance differences stem from language complexity or cultural context understanding, controlling for vocabulary and syntactic complexity.