---
ver: rpa2
title: 'MrT5: Dynamic Token Merging for Efficient Byte-level Language Models'
arxiv_id: '2410.20771'
source_url: https://arxiv.org/abs/2410.20771
tags:
- mrt5
- sequence
- byt5
- deletion
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MrT5, a byte-level language model that improves
  efficiency over ByT5 through dynamic token deletion. MrT5 inserts a learned gating
  mechanism after a fixed encoder layer to selectively remove tokens, thereby shortening
  sequences and reducing computation.
---

# MrT5: Dynamic Token Merging for Efficient Byte-level Language Models

## Quick Facts
- arXiv ID: 2410.20771
- Source URL: https://arxiv.org/abs/2410.20771
- Reference count: 36
- MrT5 achieves up to 75% sequence length reduction with minimal accuracy loss on downstream tasks

## Executive Summary
MrT5 introduces a dynamic token deletion mechanism for byte-level language models that selectively removes tokens after a fixed encoder layer, reducing sequence length and computational cost. The approach uses learned deletion gates during training with soft deletion and a regularization loss to control deletion rates, then applies hard deletion during inference. Experiments show MrT5 outperforms random and fixed deletion baselines while matching or exceeding ByT5 accuracy on tasks like XNLI and character-level benchmarks, with up to 55% inference speedup. Multilingual training further improves deletion efficiency across diverse scripts.

## Method Summary
MrT5 builds on ByT5's byte-level approach by inserting a learned gating mechanism after a fixed encoder layer to selectively remove tokens from the sequence. During training, soft deletion is applied where tokens are multiplied by learned deletion probabilities, and a regularization loss controls the deletion rate. During inference, hard deletion removes tokens entirely, shortening sequences and reducing computation. The model is trained to minimize both the standard language modeling loss and a deletion regularization loss, allowing it to learn which tokens are most expendable while maintaining task performance.

## Key Results
- Achieves up to 75% sequence length reduction with minimal accuracy degradation
- Outperforms random and fixed deletion baselines on downstream tasks
- Reduces inference runtimes by up to 55% on a V100 GPU compared to ByT5

## Why This Works (Mechanism)
MrT5 works by learning to identify and remove redundant or less informative tokens from byte-level sequences, effectively compressing the input while preserving task-relevant information. The learned deletion gates adapt to different scripts and languages, with multilingual training enabling more efficient deletion patterns across character sets. The soft deletion during training allows gradients to flow through the model while learning which tokens can be safely removed, and the regularization loss prevents excessive deletion that would harm performance.

## Foundational Learning

**Byte-level tokenization**: Why needed - Byte-level models avoid vocabulary limitations and out-of-vocabulary issues. Quick check - Verify the model processes raw bytes rather than word pieces or characters.

**Sequence length reduction**: Why needed - Long sequences in byte-level models lead to high computational costs. Quick check - Confirm the model achieves significant length reduction without catastrophic performance loss.

**Learned gating mechanisms**: Why needed - Static deletion strategies cannot adapt to task-specific token importance. Quick check - Compare learned deletion patterns against random/fixed baselines.

## Architecture Onboarding

**Component map**: Input bytes -> Encoder layers -> Deletion gates -> (Soft deletion during training / Hard deletion during inference) -> Remaining tokens -> Downstream task layers

**Critical path**: The deletion mechanism after fixed encoder layers is the key innovation - tokens flow through initial encoding, then through learned gates that decide which to keep or remove before final processing.

**Design tradeoffs**: The model trades additional parameters for deletion gates and training complexity (soft deletion + regularization) against significant inference efficiency gains. This creates a more complex training pipeline but simpler, faster inference.

**Failure signatures**: Excessive deletion leading to accuracy drops, failure to generalize deletion patterns across scripts, or deletion gates that become stuck at extreme values (always delete or never delete).

**First experiments**: 1) Verify deletion rates can be controlled through the regularization parameter, 2) Compare learned deletion patterns against random/fixed baselines on a validation task, 3) Measure inference speedup at different deletion rates to find optimal trade-offs.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims based on synthetic deletion baselines may not reflect real-world efficiency gains
- 55% speedup reported on single V100 GPU without specifying batch size or sequence length distributions
- Additional deletion parameters and training complexity could offset efficiency gains depending on implementation
- Study focuses on English and multilingual benchmarks but doesn't explore extreme deletion rates (>75%) or very long sequences (>8k tokens)

## Confidence
- High confidence: The core mechanism of learned token deletion is technically sound and ablation results showing superiority over random/fixed deletion are convincing
- Medium confidence: The claimed performance trade-offs (75% reduction with minimal accuracy loss) are supported by experiments but may vary with different sequence lengths and hardware
- Medium confidence: The multilingual benefits are demonstrated but analysis of script-specific deletion patterns is preliminary

## Next Checks
1. Benchmark MrT5 on production-scale workloads with varying batch sizes and sequence lengths to verify claimed inference speedups translate to real-world scenarios
2. Evaluate the model's robustness to extreme deletion rates (>75%) and on very long sequences (>8k tokens) to establish practical limits
3. Conduct ablation studies isolating the contribution of additional deletion parameters to total model size and inference efficiency