---
ver: rpa2
title: Cross-layer Attention Sharing for Pre-trained Large Language Models
arxiv_id: '2408.01890'
source_url: https://arxiv.org/abs/2408.01890
tags:
- attention
- lisa
- layers
- heads
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the inter-layer redundancy of attention weights
  in large language models (LLMs) and proposes a method called LISA to reduce this
  redundancy. LISA uses tiny feed-forward networks to align attention heads between
  adjacent layers and low-rank matrices to approximate differences in layer-wise attention
  weights.
---

# Cross-layer Attention Sharing for Pre-trained Large Language Models

## Quick Facts
- arXiv ID: 2408.01890
- Source URL: https://arxiv.org/abs/2408.01890
- Reference count: 40
- This paper analyzes inter-layer redundancy in LLMs and proposes LISA to reduce redundant attention calculations by 53%-84% while maintaining high response quality.

## Executive Summary
This paper addresses the computational redundancy in large language models by identifying that attention weights between adjacent layers are highly similar. The authors propose LISA (Layer-wise Integrated Sparse Attention), which shares attention weights across layers using tiny feed-forward networks for head alignment and low-rank matrices for difference compensation. LISA achieves significant memory compression (6x reduction in Q and K matrices) and throughput improvements (19.5%-40.1%) across LLaMA3-8B, LLaMA2-7B, and LLaMA2-13B models while maintaining performance on 13 evaluation benchmarks.

## Method Summary
LISA introduces two key components to enable safe attention sharing: an attention heads alignment module using small feed-forward networks to remap similar heads across layers, and a difference compensation module using low-rank matrices to capture layer-specific variations. The method trains only these newly introduced parameters while freezing pre-trained weights, using knowledge distillation with Huber loss and language modeling loss. LISA selectively applies sharing to layers with high attention similarity (JS divergence < 0.1) and compensates for differences in sensitive layers using low-rank approximations.

## Key Results
- Achieves 6× compression of Q and K matrices across all tested models
- Reduces redundant attention calculations by 53%-84% across layers
- Improves throughput by 19.5%, 32.3%, and 40.1% for LLaMA3-8B, LLaMA2-7B, and LLaMA2-13B respectively
- Maintains performance on 13 benchmarks including PIQA, BoolQ, WinoGrande, CoQA, OBQA, MMLU, HellaSwag, TriviaQA, Natural Questions, and GSM8K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inter-layer attention weight similarity is high enough to enable safe sharing.
- Mechanism: Adjacent layers compute similar attention scores due to overlapping receptive fields and internal feature similarity.
- Core assumption: High JS divergence < 0.1 indicates practical similarity for sharing.
- Evidence anchors:
  - [abstract] "highly similar attention patterns persist within most layers" and "attention weights of most layers are highly similar, especially in adjacent layers"
  - [section] Figure 3 shows JS divergence scores sustained at a degree lower around 0.1, with diagonal cells below 0.05
  - [corpus] Weak evidence - corpus lacks specific quantitative claims about attention similarity metrics
- Break condition: When similarity drops below ~0.2 (per experiments showing poor performance for DS when similarity is lower).

### Mechanism 2
- Claim: Attention heads can be aligned across layers to preserve similarity during sharing.
- Mechanism: Feed-forward networks realign heads by learning permutation matrices that map similar heads across layers.
- Core assumption: Attention heads lack inherent positional relationships, so explicit alignment is necessary.
- Evidence anchors:
  - [abstract] "Directly sharing the weight matrix without carefully rearranging the attention heads proves to be ineffective"
  - [section] "Most attention heads can be aligned with a highly similar one in the shared matrix" and Figure 5(d) showing similarity-based alignment recovers high similarity
  - [corpus] No direct corpus evidence for head alignment necessity
- Break condition: When alignment networks cannot learn effective mappings, likely in early layers where patterns diverge.

### Mechanism 3
- Claim: Difference compensation prevents performance collapse in sensitive layers.
- Mechanism: Low-rank matrices approximate layer-specific attention differences that remain after sharing.
- Core assumption: Small deviations in sensitive layers cause performance collapse, requiring explicit compensation.
- Evidence anchors:
  - [abstract] "Shallow layers are vulnerable to small deviations in attention weights" and need for "remedy for differences"
  - [section] "even small distortions...can lead to notable performance degradation" and Figure 6 showing shallow layers are sensitive
  - [corpus] No corpus evidence for difference compensation approach
- Break condition: When compensation cannot adequately model the residual differences, particularly in complex reasoning tasks.

## Foundational Learning

- Concept: Jensen-Shannon divergence as similarity metric
  - Why needed here: Quantifies distributional similarity between attention weights across layers
  - Quick check question: What JS divergence threshold indicates attention weights are practically similar enough for sharing?

- Concept: Low-rank matrix approximation
  - Why needed here: Enables compact representation of layer-specific attention differences
  - Quick check question: How does reducing rank from d to r (e.g., 128→20) affect reconstruction quality of attention differences?

- Concept: Feed-forward networks for permutation learning
  - Why needed here: Learns head alignment mappings without assuming positional correspondence
  - Quick check question: What activation function works best for head alignment FFNs (ReLU vs SiLU)?

## Architecture Onboarding

- Component map:
  Standard attention (Q,K,V,Softmax) -> Attention heads alignment module (FFN-based head remapping) -> Difference compensation module (low-rank projections W_Q^LR, W_K^LR) -> Integration layer (combining aligned shared weights with compensated differences)

- Critical path: Shared attention matrix → Alignment FFN → Low-rank difference → Final attention scores

- Design tradeoffs:
  - Higher compression (smaller r) saves more memory but requires more complex difference modeling
  - More FFN layers improve alignment quality but increase trainable parameters
  - Selective layer application balances efficiency vs. performance

- Failure signatures:
  - Performance collapse in shallow layers (suggests insufficient compensation)
  - Degradation on complex tasks (indicates missing layer-specific features)
  - Inconsistent gains across models (suggests sensitivity to base model architecture)

- First 3 experiments:
  1. Measure JS divergence between adjacent layers on validation data to identify sharing candidates
  2. Train LISA with only alignment module (no compensation) to isolate alignment impact
  3. Test different r values (e.g., 20, 32, 64) to find optimal compression-efficiency tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the upper bound of inter-layer redundancy in attention mechanisms beyond transformer-based LLMs, particularly in architectures like RNNs, CNNs, or graph neural networks?
- Basis in paper: [inferred] The paper analyzes inter-layer redundancy in transformer-based LLMs and suggests investigating other modalities, but does not provide data or analysis for other architectures.
- Why unresolved: The study is limited to transformer-based LLMs, and the authors explicitly state a plan to investigate other modalities, leaving the question open.
- What evidence would resolve it: Comparative studies measuring attention/activation similarity across layers in RNNs, CNNs, and graph neural networks, alongside empirical tests of redundancy-reduction methods in these architectures.

### Open Question 2
- Question: How does the effectiveness of LISA scale with sequence length, particularly for very long sequences (e.g., >4096 tokens), and what are the limits of its compression efficiency?
- Basis in paper: [inferred] The paper mentions efficiency gains and memory savings but does not provide data on performance with very long sequences or analyze the limits of compression.
- Why unresolved: While LISA's benefits are demonstrated, the scalability with sequence length and potential bottlenecks are not explored in detail.
- What evidence would resolve it: Systematic experiments varying sequence lengths up to extreme values, measuring throughput, memory usage, and accuracy, to identify the point where LISA's efficiency plateaus or degrades.

### Open Question 3
- Question: Can LISA be adapted to pre-training from scratch for transformer models with different attention mechanisms (e.g., sparse attention, multi-query attention), and what are the trade-offs in performance and efficiency?
- Basis in paper: [explicit] The paper discusses adapting LISA for pre-training from scratch and mentions sparse attention and multi-query attention as related work, but does not provide experimental results for these adaptations.
- Why unresolved: The paper suggests potential adaptations but does not empirically test LISA with alternative attention mechanisms during pre-training.
- What evidence would resolve it: Experiments applying LISA during pre-training of models with sparse attention or multi-query attention, comparing performance, efficiency, and convergence to standard pre-training.

### Open Question 4
- Question: What is the impact of LISA on the robustness of LLMs to adversarial attacks or distribution shifts, particularly in scenarios where input patterns deviate significantly from training data?
- Basis in paper: [inferred] The paper evaluates LISA's performance on standard benchmarks but does not test its behavior under adversarial conditions or distribution shifts.
- Why unresolved: The study focuses on standard task performance and efficiency, without exploring robustness to adversarial inputs or out-of-distribution data.
- What evidence would resolve it: Adversarial attack experiments and out-of-distribution evaluation showing how LISA-equipped models perform compared to baselines under stress conditions.

## Limitations

- The effectiveness of LISA depends critically on attention similarity thresholds, but the paper doesn't systematically explore sensitivity to different JS divergence thresholds across model families.
- While head alignment is empirically shown to be necessary, the paper lacks theoretical explanation for why certain attention heads map more readily than others or under what conditions alignment might fail.
- The low-rank difference compensation approach works for tested models but its scalability and effectiveness for much larger models or different architectural choices remains unproven.

## Confidence

- **High**: Attention similarity exists between adjacent layers (supported by JS divergence metrics)
- **Medium**: Alignment networks effectively preserve similarity during sharing (empirical but not theoretical validation)
- **Medium**: Difference compensation prevents performance collapse (demonstrated but limited to specific model scales)

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the JS divergence threshold for sharing eligibility and measure the resulting performance tradeoff curve to identify the optimal similarity threshold for different model families.

2. **Alignment Robustness Testing**: Test LISA on models with deliberately modified attention patterns (e.g., sparse attention, specialized heads) to determine when alignment mechanisms break down and what architectural features make alignment more or less effective.

3. **Cross-Architecture Generalization**: Apply LISA to transformer variants beyond standard LLMs (such as Vision Transformers or hybrid architectures) to assess whether the observed attention similarity patterns and sharing effectiveness generalize across different task domains.