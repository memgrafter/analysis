---
ver: rpa2
title: Are You Using Reliable Graph Prompts? Trojan Prompt Attacks on Graph Neural
  Networks
arxiv_id: '2410.13974'
source_url: https://arxiv.org/abs/2410.13974
tags:
- graph
- prompt
- task
- attack
- backdoor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first backdoor attack on graph prompt
  learning (GPL) without poisoning graph neural network (GNN) encoders. The proposed
  Trojan Graph Prompt Attack (TGPA) framework injects backdoors into learnable graph
  prompts by optimizing both prompt parameters and an adaptive trigger generator to
  build associations between trigger-attached nodes and target classes.
---

# Are You Using Reliable Graph Prompts? Trojan Prompt Attacks on Graph Neural Networks

## Quick Facts
- arXiv ID: 2410.13974
- Source URL: https://arxiv.org/abs/2410.13974
- Authors: Minhua Lin; Zhiwei Zhang; Enyan Dai; Zongyu Wu; Yilong Wang; Xiang Zhang; Suhang Wang
- Reference count: 40
- Primary result: First backdoor attack on graph prompt learning that achieves up to 97.1% attack success rate without poisoning GNN encoders

## Executive Summary
This paper introduces the first backdoor attack on graph prompt learning (GPL) that operates without modifying pre-trained graph neural network (GNN) encoders. The Trojan Graph Prompt Attack (TGPA) framework injects backdoors by optimizing learnable graph prompts and an adaptive trigger generator, creating associations between trigger-attached nodes and target classes. The attack maintains high clean accuracy while achieving strong attack success rates across multiple datasets and GPL methods.

## Method Summary
TGPA operates through a bi-level optimization framework that first trains a clean graph prompt and task header, then generates triggers using a feature-aware MLP-based generator. The trojan graph prompt and trigger generator are optimized in a poisoned dataset where trigger-attached nodes are labeled as target classes. To ensure backdoor effectiveness after downstream fine-tuning, the framework employs adversarial training with a finetuning-resistant loss that makes the trojan prompt robust to perturbations within an ε-ball.

## Key Results
- Achieves attack success rates up to 97.1% on Cora dataset while maintaining clean accuracy
- Adaptive triggers outperform fixed subgraph triggers across all tested GPL methods
- Finetuning-resistant mechanism reduces ASR drop to less than 5% after prompt fine-tuning
- Effective across GraphPrompt, GraphPrompt-CLS, and GPL-CLS methods

## Why This Works (Mechanism)

### Mechanism 1
Backdoors can be injected into graph prompts without modifying GNN encoders by training a trojan prompt that builds associations between trigger-attached nodes and target classes. The trojan graph prompt is trained in a poisoned dataset where trigger-attached nodes are labeled as the target class. The task header learns to associate (trojan prompt, trigger) with the target class while maintaining clean accuracy on benign samples.

### Mechanism 2
A feature-aware trigger generator creates powerful, unnoticeable triggers that achieve high attack success rates while maintaining clean accuracy. An MLP-based trigger generator produces node features and structure for triggers based on the representations of target nodes after prompting. The triggers are optimized to maximize backdoor loss while minimizing noticeability.

### Mechanism 3
Finetuning-resistant graph prompt poisoning ensures backdoor effectiveness even after downstream users fine-tune prompts or task headers on clean datasets. Adversarial training is applied to make the trojan prompt robust to perturbations within an ε-ball. The finetuning-resistant loss ensures low backdoor loss for any updates to the prompt within this constraint.

## Foundational Learning

- **Graph Neural Networks (GNNs) and message-passing mechanism**: Understanding GNNs is fundamental to grasping why trojan prompt attacks are effective against GPL. *Quick check*: How does the message-passing mechanism in GNNs work, and why does freezing the encoder make it vulnerable to prompt-based attacks?

- **Graph Prompt Learning (GPL) and its two main approaches**: The paper distinguishes between prompt-as-tokens and prompt-as-graphs approaches, which affects attack strategy. *Quick check*: What are the key differences between prompt-as-tokens and prompt-as-graphs in GPL, and how does this distinction impact backdoor injection methods?

- **Backdoor attacks and trigger generation**: Understanding traditional backdoor attacks helps explain why simple adaptations fail and why the proposed approach is novel. *Quick check*: Why do traditional graph backdoor attacks that modify GNN parameters fail when applied to GPL, and what makes trigger generation more complex in this context?

## Architecture Onboarding

- **Component map**: Pretrained GNN Encoder (frozen) -> Clean Graph Prompt Generator -> Trojan Graph Prompt Generator -> Feature-aware Trigger Generator -> Task Header (with limited parameters) -> Downstream Task Dataset
- **Critical path**: Clean prompt generation → Trigger generation → Trojan prompt poisoning → Fine-tuning resistance
- **Design tradeoffs**: Balance between attack success rate and clean accuracy; Trigger complexity vs. noticeability; Adversarial training strength vs. model utility preservation; Prompt capacity vs. fine-tuning resistance
- **Failure signatures**: Low attack success rate despite high clean accuracy (backdoor not properly embedded); High attack success rate but low clean accuracy (backdoor too aggressive); Backdoor effectiveness lost after fine-tuning (finetuning-resistant mechanism insufficient); Triggers easily detected by defenses (trigger generation not sufficiently unnoticeable)
- **First 3 experiments**: 1) Test basic trojan prompt attack without fine-tuning resistance on Cora dataset using GraphPrompt method; 2) Evaluate trigger generator effectiveness by comparing adaptive triggers vs. fixed subgraph triggers; 3) Test finetuning resistance by fine-tuning trojan prompt on clean dataset and measuring attack success rate retention

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed trojan graph prompt attack generalize to graph-level datasets such as molecular graphs? The current work focuses on node-level datasets, and extending the attack to graph-level datasets involves different challenges and considerations.

### Open Question 2
What are the most effective defense mechanisms against trojan graph prompt attacks? The paper only explores a preliminary defense strategy involving edge pruning, which is not sufficient to counter the attack effectively.

### Open Question 3
How does the trojan graph prompt attack perform in scenarios with limited access to graph data or computational resources? The attack's performance under resource constraints is not explored, and it is unclear how it would adapt to such conditions.

## Limitations

- Missing sensitivity analysis for critical hyperparameters λ and ε in the finetuning-resistant loss
- Limited testing against modern graph neural network backdoor defenses and detection methods
- Unclear generalization to all GPL variants beyond the three methods evaluated

## Confidence

**High Confidence** (Strong experimental support):
- The basic trojan prompt attack mechanism works as described
- Adaptive triggers outperform fixed subgraph triggers
- Clean accuracy is maintained while achieving high attack success rates

**Medium Confidence** (Reasonable support but with gaps):
- The finetuning-resistant mechanism provides robustness
- The attack works across multiple GPL methods
- The trigger generator produces effective, unnoticeable triggers

**Low Confidence** (Limited or indirect support):
- Attack effectiveness against deployed defenses
- Performance on larger, more complex graph datasets
- Generalization to all possible GPL architectures

## Next Checks

1. **Defense Robustness Test**: Evaluate TGPA against at least three state-of-the-art graph neural network backdoor defenses (e.g., spectral signatures, activation clustering, and graph-based anomaly detection) to assess real-world effectiveness.

2. **Parameter Sensitivity Analysis**: Conduct systematic experiments varying λ and ε in the finetuning-resistant loss to establish the attack's robustness to hyperparameter changes and identify optimal settings.

3. **Cross-Architecture Generalization**: Test TGPA on GPL methods beyond the three evaluated (GraphPrompt, GraphPrompt-CLS, GPL-CLS), including methods that use prompt-as-tokens approaches, to validate the attack's broader applicability.