---
ver: rpa2
title: 'DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents'
arxiv_id: '2407.03300'
source_url: https://arxiv.org/abs/2407.03300
tags:
- discrete
- latents
- disco-diff
- latent
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Discrete-Continuous Latent Variable Diffusion
  Models (DisCo-Diff), which enhances standard diffusion models by incorporating learnable
  discrete latent variables alongside continuous latents. The discrete latents are
  inferred through an encoder and conditioned on the main diffusion model to simplify
  its complex denoising task, reducing the curvature of the generative ODE and improving
  learning efficiency.
---

# DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents

## Quick Facts
- arXiv ID: 2407.03300
- Source URL: https://arxiv.org/abs/2407.03300
- Reference count: 40
- Primary result: Achieves SOTA FID scores of 1.65 (ImageNet-64) and 1.98 (ImageNet-128) using ODE samplers

## Executive Summary
DisCo-Diff introduces a novel approach to enhancing continuous diffusion models by incorporating learnable discrete latent variables. The method uses an encoder to infer discrete latents from clean data, which are then conditioned on the diffusion model to simplify the denoising task by reducing the curvature of the generative ODE. Unlike existing discrete latent approaches, DisCo-Diff trains both the discrete latents and the diffusion model end-to-end without requiring pre-trained networks. An autoregressive model is subsequently trained to sample these discrete latents during inference.

## Method Summary
DisCo-Diff jointly trains a discrete-conditional diffusion model with an encoder that infers discrete latents, and an autoregressive model to sample these latents at inference time. The discrete latents are trained end-to-end with the diffusion model objective, ensuring they encode information that simplifies denoising. The approach uses a small number of discrete latents with modest codebook sizes, making autoregressive modeling tractable. During training, continuous relaxation (Gumbel-Softmax) is used for backpropagation, while discrete sampling is used during inference.

## Key Results
- Achieves state-of-the-art FID scores of 1.65 and 1.98 on class-conditioned ImageNet-64 and ImageNet-128 respectively
- Demonstrates significant improvements over standard diffusion models on 2D toy data and molecular docking tasks
- Shows that discrete latents effectively capture data modes and reduce ODE curvature compared to continuous latents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete latents reduce curvature of the generative ODE, making the denoising task easier.
- Mechanism: The discrete latents partition the complex multimodal data distribution into simpler sub-distributions, each with lower curvature trajectories in the ODE space.
- Core assumption: The data distribution can be decomposed into simpler modes that are well-captured by discrete latents.
- Evidence anchors:
  - [abstract] "The discrete latents significantly simplify learning the DM’s complex noise-to-data mapping by reducing the curvature of the DM’s generative ODE."
  - [section] "The discrete latents learn to capture the different modes, and DisCo-Diff’s DM component models the individual modes. The DM’s ODE trajectories for different latents are now almost perfectly straight, indicating a simple conditional score function."
- Break condition: If the discrete latents fail to capture meaningful modes, the ODE curvature reduction would be minimal and the denoising task would remain complex.

### Mechanism 2
- Claim: End-to-end learning of discrete latents with the DM objective ensures they encode information beneficial for denoising.
- Mechanism: The encoder learns discrete latents from clean data that directly help the denoiser reconstruct data more accurately. This joint optimization ensures latents are trained to reduce the DM's score matching loss, especially at high noise levels where denoising is most challenging.
- Core assumption: The discrete latents inferred by the encoder from clean data contain information that simplifies the denoising task.
- Evidence anchors:
  - [abstract] "The discrete latents are inferred through an encoder and learnt end-to-end together with the DM."
  - [section] "By learning them jointly with the DM objective itself, they are directly trained to help the DM learn better denoisers and lower curvature generative ODEs."
- Break condition: If the encoder fails to learn useful discrete latents or if the joint optimization doesn't align with the denoising objective, the latents won't provide meaningful simplification.

### Mechanism 3
- Claim: Using few discrete latents with small codebooks simplifies modeling their distribution compared to large discrete latent spaces.
- Mechanism: DisCo-Diff uses only a moderate number of latents (e.g., 10) with small codebooks (e.g., 100), making it easy to train an autoregressive model over their distribution.
- Core assumption: A small set of discrete latents can capture global data structure while remaining easy to model.
- Evidence anchors:
  - [abstract] "We only use a small set of discrete latents with relatively small codebooks, which makes the additional training of the autoregressive model easy."
  - [section] "DisCo-Diff, in contrast, carefully combines its discrete latents with the continuous latents (Gaussian prior) of the DM and effectively separates the modeling of discrete and continuous variations within the data. It requires only a few discrete latents."
- Break condition: If the data complexity requires more discrete latents or larger codebooks, the autoregressive modeling becomes more difficult and may not provide the intended simplification.

## Foundational Learning

- Concept: Diffusion models and score matching
  - Why needed here: DisCo-Diff builds upon continuous diffusion models, so understanding how DMs learn to reverse the noising process through score estimation is fundamental.
  - Quick check question: What is the relationship between the denoiser network and the score function in a diffusion model?

- Concept: Variational autoencoders with discrete latents
  - Why needed here: DisCo-Diff can be interpreted as a VAE with discrete latents and a diffusion model as decoder, so understanding how discrete latents work in VAEs is relevant.
  - Quick check question: How does the Kullback-Leibler divergence regularization in VAEs control information flow through continuous latents?

- Concept: Gumbel-Softmax relaxation for discrete variables
  - Why needed here: DisCo-Diff uses Gumbel-Softmax to enable backpropagation through the discrete latent sampling process during training.
  - Quick check question: What role does the temperature parameter play in the Gumbel-Softmax distribution?

## Architecture Onboarding

- Component map:
  Encoder (Eϕ) -> Denoiser (Dθ) -> Autoregressive model (Aψ)

- Critical path:
  1. Training stage 1: Jointly optimize encoder and denoiser using denoising score matching with discrete latents
  2. Training stage 2: Train autoregressive model to model discrete latent distribution
  3. Inference: Sample discrete latents from autoregressive model, then run denoiser with ODE solver

- Design tradeoffs:
  - Number of discrete latents vs. codebook size: More latents capture more structure but make autoregressive modeling harder
  - Cross-attention placement: Adding cross-attention at multiple resolutions vs. only at bottleneck
  - Temperature in Gumbel-Softmax: Higher temperature adds regularization but reduces discrete sampling quality

- Failure signatures:
  - Poor sample quality: Discrete latents not capturing meaningful modes
  - Encoder collapse: Continuous relaxation too strong, latents become uninformative
  - Overfitting in second stage: Autoregressive model memorizing discrete latent patterns

- First 3 experiments:
  1. 2D mixture of Gaussians: Validate ODE curvature reduction and discrete latent mode capture
  2. ImageNet-64 with small model: Test discrete latent conditioning and autoregressive modeling
  3. Ablation: Replace discrete latents with continuous latents to confirm discrete advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of discrete latent variables and codebook size affect the trade-off between model performance and auto-regressive model complexity?
- Basis in paper: [explicit] The paper mentions that using a modest number of latents (e.g., 10-30) and codebook size (e.g., 50-100) significantly enhances performance on the complex ImageNet dataset, and that increasing the number of latents and codebook size might further reduce reconstruction error but also complicates the auto-regressive model's task.
- Why unresolved: The paper does not provide an extensive study on the optimal number of latents and codebook size for different datasets and tasks. It only mentions that a configuration of 10 latents with a codebook size of 100 significantly enhances performance on ImageNet.
- What evidence would resolve it: A systematic study varying the number of discrete latents and codebook sizes on different datasets and tasks, and evaluating the impact on both model performance and the complexity of the auto-regressive model.

### Open Question 2
- Question: How do discrete latent variables interact with and complement semantic conditioning information like class labels or text prompts in diffusion models?
- Basis in paper: [explicit] The paper states that conditioning information like class labels or text prompts often helps to simplify the complex mapping by offering the DM's denoiser additional cues for more accurate denoising. However, it also suggests that discrete latents can further reduce the complexity and capture variations complementary to class semantics.
- Why unresolved: The paper provides some evidence of this interaction in the ImageNet experiments, showing that samples sharing the same discrete latent exhibit similar characteristics and there are noticeable distinctions for different discrete latents under the same class. However, a more in-depth analysis of how discrete latents interact with different types of conditioning information is not provided.
- What evidence would resolve it: A detailed analysis of how discrete latent variables interact with and complement different types of conditioning information (e.g., class labels, text prompts) in various tasks, and how this interaction affects the overall model performance.

### Open Question 3
- Question: Can the DisCo-Diff framework be extended to other continuous flow models, such as flow-matching or rectified flow, and what would be the impact on their performance?
- Basis in paper: [explicit] The paper mentions that due to the close relation between diffusion models and flow matching, it expects discrete latents to behave similarly in flow-matching and improve performance. It also suggests that the idea of DisCo-Diff could be applied to other continuous flow models.
- Why unresolved: The paper does not provide any experimental results or analysis on applying the DisCo-Diff framework to other continuous flow models. It only suggests that this could be a potential future direction.
- What evidence would resolve it: Experiments applying the DisCo-Diff framework to other continuous flow models, such as flow-matching or rectified flow, and evaluating the impact on their performance compared to the original models without discrete latents.

## Limitations

- The optimal configuration of discrete latents and codebook sizes for different datasets and tasks remains unexplored
- The interaction between discrete latents and semantic conditioning information (class labels, text prompts) needs more in-depth analysis
- Extension to other continuous flow models (flow-matching, rectified flow) has not been experimentally validated

## Confidence

- **High confidence**: The discrete latent mechanism for reducing ODE curvature (supported by 2D experiments)
- **Medium confidence**: State-of-the-art FID results (single run reported, hyperparameters not fully detailed)
- **Medium confidence**: Universal applicability claim (limited comparative analysis with pre-trained alternatives)

## Next Checks

1. **Scalability test**: Evaluate DisCo-Diff on larger image resolutions (e.g., 256x256) to assess whether the discrete latent benefits scale with data complexity and dimensionality.

2. **Ablation on discrete count**: Systematically vary the number of discrete latents and codebook sizes to identify the optimal configuration and test the claimed advantage of using "few" discrete latents.

3. **Training stability analysis**: Monitor encoder collapse and discrete latent quality throughout training by tracking the mutual information between discrete latents and generated samples, and the denoiser's reliance on discrete conditioning.