---
ver: rpa2
title: One-stage Prompt-based Continual Learning
arxiv_id: '2402.16189'
source_url: https://arxiv.org/abs/2402.16189
tags:
- prompt
- learning
- continual
- os-prompt
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a one-stage prompt-based continual learning
  (PCL) framework to address the high computational cost of existing PCL methods.
  Prior PCL methods require two Vision Transformer (ViT) feed-forward stages: one
  for generating a prompt query and another for the backbone ViT.'
---

# One-stage Prompt-based Continual Learning

## Quick Facts
- arXiv ID: 2402.16189
- Source URL: https://arxiv.org/abs/2402.16189
- Reference count: 40
- Primary result: Proposed method outperforms prior two-stage PCL methods by ~1.4% while maintaining ~50% computational cost reduction during inference.

## Executive Summary
This paper introduces a one-stage prompt-based continual learning (PCL) framework that addresses the high computational cost of existing PCL methods. Prior PCL methods require two Vision Transformer (ViT) feed-forward stages: one for generating a prompt query and another for the backbone ViT. The proposed method uses an intermediate layer's token embedding as the prompt query, eliminating the need for a separate query ViT stage and reducing computational cost by ~50%. Additionally, a Query-Pool Regularization (QR) loss is introduced to improve representation power, applied only during training. Experiments on CIFAR-100, ImageNet-R, and DomainNet benchmarks show that the proposed method outperforms prior two-stage PCL methods by ~1.4% while maintaining ~50% computational cost reduction during inference.

## Method Summary
The proposed one-stage PCL framework directly uses the intermediate layer's token embedding as a prompt query, eliminating the need for an additional feed-forward stage for query ViT. This design results in ~50% computational cost reduction. The framework further introduces a Query-Pool Regularization (QR) loss that regulates the relationship between the prompt query and the prompt pool to improve representation power. The QR loss is only applied during training time, ensuring no computational overhead at inference. The method uses a pre-trained Vision Transformer and refines the model by training learnable tokens on the given data in a sequential manner, with minimal memory overhead due to the small prompt pool size.

## Key Results
- Outperforms prior two-stage PCL methods by ~1.4% on benchmark datasets
- Achieves ~50% computational cost reduction during inference
- Query-Pool Regularization (QR) loss improves representation power without adding inference overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using intermediate layer token embedding as prompt query removes the need for a separate query ViT feedforward stage.
- Mechanism: The token embedding from an early ViT layer is already computed during the main backbone feedforward pass. By reusing this embedding as the prompt query instead of computing a new one via a separate frozen query ViT, the method eliminates one full ViT feedforward computation.
- Core assumption: Early-layer token embeddings maintain stable, consistent representations across continual learning stages, even as later layers and prompt parameters are updated.
- Evidence anchors:
  - [abstract] "we introduce a one-stage PCL framework by directly using the intermediate layer’s token embedding as a prompt query. This design removes the need for an additional feed-forward stage for query ViT"
  - [section] "We directly use the intermediate layer’s token embedding as a prompt query... This design removes the need for an additional feed-forward stage for query ViT, resulting in ~50% computational cost reduction"
  - [corpus] No direct corpus evidence on intermediate layer stability; paper provides internal analysis (Figure 2) showing early layers remain stable.
- Break condition: If intermediate token embeddings shift significantly during continual learning, the prompt query representation would become inconsistent, leading to degraded performance or catastrophic forgetting.

### Mechanism 2
- Claim: Query-Pool Regularization (QR) loss improves representation power without adding inference overhead.
- Mechanism: During training, the QR loss encourages similarity between the similarity matrix derived from the intermediate token embedding query and that derived from the final-layer [CLS] token (reference query). This regularizes the prompt pool to maintain stronger, more discriminative representations. Since the QR loss is only applied during training, it incurs no additional computational cost at inference.
- Core assumption: Aligning the query-prompt pool similarity structure with that of the high-capacity final-layer representation improves prompt pool representational power without harming stability.
- Evidence anchors:
  - [abstract] "We further introduce a Query-Pool Regularization (QR) loss that regulates the relationship between the prompt query and the prompt pool to improve representation power... The QR loss is only applied during training time, so there is no computational overhead at inference"
  - [section] "The QR loss ensures that the query-pool relationship becomes similar to that of the final layer’s [CLS] token, thereby improving representation power... the QR loss is applied during the training phase, ensuring that there is no computational overhead during inference"
  - [corpus] No corpus evidence directly addressing QR loss; internal ablation studies in paper support its effectiveness.
- Break condition: If the regularization forces the intermediate query to mimic the final-layer behavior too strongly, it could suppress the natural task-specific adaptation that makes the intermediate query effective.

### Mechanism 3
- Claim: Prompt-based continual learning with a small learnable prompt pool achieves high performance while preventing catastrophic forgetting without data replay.
- Mechanism: Instead of fine-tuning all model parameters, only a small set of prompt tokens is updated per task. The prompt pool is partitioned per task, and prompts are selected based on similarity to the prompt query. This constrains the adaptation space, reducing interference between tasks while retaining most pre-trained knowledge.
- Core assumption: Pre-trained ViT backbone features are sufficiently general and robust that selective prompt adaptation can encode new task knowledge without overwriting old knowledge.
- Evidence anchors:
  - [abstract] "PCL utilizes a pre-trained Vision Transformer (ViT) [8] and refines the model by training learnable tokens on the given data... This strategy enables a model to learn the information of the training data in a sequential manner with less memory overhead"
  - [section] "PCL adopts a prompt pool-based training scheme where different prompts are selected and trained for each continual learning stage. This strategy enables a model to learn the information of the training data in a sequential manner with less memory overhead, as the prompt pool requires minimal resources"
  - [corpus] Prior PCL works (L2P, DualPrompt, CodaPrompt) cited as state-of-the-art, but no direct corpus evidence on prompt-based CL mechanism specifics.
- Break condition: If tasks are too dissimilar or the prompt pool size is insufficient, catastrophic forgetting may occur despite prompt-based adaptation.

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture and multi-head self-attention.
  - Why needed here: The entire framework relies on manipulating ViT token embeddings and prompt tokens via prefix-tuning in self-attention layers.
  - Quick check question: Can you describe how a ViT processes an image and how prefix-tuning modifies the self-attention computation?

- Concept: Catastrophic forgetting and continual learning evaluation metrics.
  - Why needed here: Understanding why methods like ER, LWF, and prompt-based approaches exist, and how to interpret average accuracy (AN) and forgetting (FN) metrics.
  - Quick check question: What is the difference between class-incremental and task-incremental continual learning, and why does task identity being unknown at inference make the problem harder?

- Concept: Gradient-based optimization and parameter-efficient fine-tuning.
  - Why needed here: The method freezes most ViT parameters and only updates prompt tokens; understanding this constraint is key to grasping the efficiency gains.
  - Quick check question: How does updating only a small fraction of parameters (like prompts) reduce computational and memory cost compared to full fine-tuning?

## Architecture Onboarding

- Component map:
  Input image -> Backbone ViT -> Intermediate layer token embeddings
  Intermediate [CLS] token -> Prompt query (used directly)
  Prompt pool (per layer) -> Keys and values for prompts
  Cosine similarity -> Weighted summation -> Prompt tokens
  Prompt tokens -> Prefix-tuned into backbone ViT layers 1-5
  Optional reference ViT (OS-Prompt++) -> Final-layer [CLS] token -> QR loss computation (training only)

- Critical path:
  Image -> ViT backbone (forward pass) -> Intermediate [CLS] token -> Prompt query -> Prompt pool selection -> Prefix-tuned self-attention -> Final prediction

- Design tradeoffs:
  - Accuracy vs. efficiency: OS-Prompt sacrifices slight accuracy for ~50% GFLOPs reduction; OS-Prompt++ recovers accuracy but keeps training cost similar to prior methods.
  - Prompt query stability vs. representational capacity: Intermediate layer query is stable but weaker; QR loss bridges this gap without inference overhead.
  - Training efficiency vs. inference efficiency: OS-Prompt has both lower training and inference cost; OS-Prompt++ matches training cost of prior methods but maintains inference gains.

- Failure signatures:
  - Performance drops >1% relative to two-stage PCL -> possible intermediate layer instability or poor prompt pool regularization.
  - Large forgetting rates despite prompt tuning -> prompt pool size or task separation strategy may be insufficient.
  - Training instability -> QR loss weight λ may be mis-tuned or reference ViT mismatch.

- First 3 experiments:
  1. Verify intermediate layer token stability: Train a model with OS-Prompt on a small continual learning task and measure layer-wise feature distances across tasks (as in Figure 2).
  2. Test QR loss ablation: Train OS-Prompt and OS-Prompt++ on a 2-3 task split of CIFAR-100 and compare accuracy and FN; remove QR loss to confirm its contribution.
  3. Benchmark GFLOPs and latency: Profile OS-Prompt and OS-Prompt++ on a GPU, comparing against L2P/DualPrompt/CodaPrompt to confirm ~50% inference cost reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt formation strategies (L2P vs DualPrompt vs CodaPrompt) compare when used within the one-stage PCL framework?
- Basis in paper: [explicit] The authors tested OS-Prompt with L2P and DualPrompt formation strategies and found they performed worse than CodaPrompt-based OS-Prompt
- Why unresolved: The paper only tested three specific formation strategies and didn't explore the full space of possible strategies or their interaction with one-stage vs two-stage frameworks
- What evidence would resolve it: Systematic comparison of multiple prompt formation strategies (including novel ones) across both one-stage and two-stage PCL frameworks on multiple benchmarks

### Open Question 2
- Question: What is the optimal layer selection strategy for extracting token embeddings as prompt queries in the one-stage framework?
- Basis in paper: [explicit] The authors used intermediate layers (1-5) based on observations about feature stability, but didn't systematically explore different layer selections or develop a principled method for choosing layers
- Why unresolved: The paper used empirical observations to select layers but didn't provide theoretical justification or explore alternative layer selection strategies
- What evidence would resolve it: Analysis of how different layer selections affect performance, stability, and computational efficiency, potentially leading to an adaptive layer selection method

### Open Question 3
- Question: How can the computational efficiency of OS-Prompt++ be improved during training while maintaining its performance benefits?
- Basis in paper: [explicit] The authors note that OS-Prompt++ has higher training computational cost due to the reference ViT feedforward, but propose early exit strategies as a potential solution
- Why unresolved: The paper identifies the efficiency gap during training but only suggests early exit as a potential direction without implementing or evaluating it
- What evidence would resolve it: Implementation and evaluation of early exit strategies or other efficiency improvements during training, comparing training time and performance against OS-Prompt and baseline methods

## Limitations
- The exact intermediate layer selection for prompt query extraction is not precisely specified, which could impact performance stability.
- The reference ViT R(·) used for Query-Pool Regularization in OS-Prompt++ is not fully detailed - whether it's a separate model or a frozen version of the same backbone remains unclear.
- The mechanism by which aligning intermediate and final-layer similarity matrices improves prompt pool representation lacks theoretical grounding in the broader literature.

## Confidence
- Confidence: Medium
  The proposed one-stage PCL framework demonstrates clear computational advantages by eliminating the need for a separate query ViT stage, but several implementation details remain underspecified. The exact intermediate layer selection for prompt query extraction (described as "layer 1-5" but not precisely defined) could significantly impact performance stability. Additionally, the reference ViT R(·) used for Query-Pool Regularization in OS-Prompt++ is not fully detailed - whether it's a separate model or a frozen version of the same backbone remains unclear.

- Confidence: High
  The computational cost reduction claim (~50% GFLOPs) appears well-supported by the framework's design, as removing an entire ViT feedforward stage logically halves the inference computation. However, this assumes the intermediate layer token embedding remains sufficiently stable and representative across continual learning stages, which is not extensively validated beyond internal analysis.

- Confidence: Medium
  The Query-Pool Regularization loss shows promise for improving representation power without inference overhead, but its effectiveness depends on careful hyperparameter tuning (λ weight). The paper provides ablation studies, but the mechanism by which aligning intermediate and final-layer similarity matrices improves prompt pool representation lacks theoretical grounding in the broader literature.

## Next Checks
1. **Intermediate Layer Stability Analysis**: Systematically test OS-Prompt across different intermediate layer choices (layers 1-5) on a 3-5 task continual learning split of CIFAR-100, measuring both accuracy degradation and layer-wise feature distance changes. This will identify the optimal layer for prompt query extraction and validate the stability assumption.

2. **QR Loss Ablation and Sensitivity**: Conduct controlled experiments removing the QR loss from OS-Prompt++ on ImageNet-R, comparing average accuracy and forgetting rates. Additionally, perform a hyperparameter sweep on λ to determine sensitivity and identify optimal regularization strength.

3. **End-to-End Computational Profiling**: Implement and profile both OS-Prompt and OS-Prompt++ on a standard GPU, measuring actual GFLOPs, memory usage, and inference latency. Compare against L2P, DualPrompt, and CodaPrompt implementations under identical conditions to verify the claimed ~50% inference cost reduction.