---
ver: rpa2
title: Rethinking Time Series Forecasting with LLMs via Nearest Neighbor Contrastive
  Learning
arxiv_id: '2412.04806'
source_url: https://arxiv.org/abs/2412.04806
tags:
- time
- series
- forecasting
- learning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large language models
  (LLMs) for time series forecasting, where the continuous nature of time series data
  and the need for effective prompt formulation pose significant difficulties. The
  authors propose NNCL-TLLM, a method that leverages nearest neighbor contrastive
  learning (NNCL) to formulate prompts that better represent time series characteristics
  while aligning with word token embeddings.
---

# Rethinking Time Series Forecasting with LLMs via Nearest Neighbor Contrastive Learning

## Quick Facts
- arXiv ID: 2412.04806
- Source URL: https://arxiv.org/abs/2412.04806
- Reference count: 40
- Key outcome: Achieves competitive or superior performance over state-of-the-art methods in long-term, short-term, and few-shot forecasting tasks, with up to 5% reduction in MSE in few-shot settings.

## Executive Summary
This paper addresses the challenge of adapting large language models (LLMs) for time series forecasting by proposing NNCL-TLLM, a method that leverages nearest neighbor contrastive learning (NNCL) to formulate prompts that better represent time series characteristics while aligning with word token embeddings. The approach involves generating neighborhood-aware time series compatible text prototypes (TCTPs) using LLM word token embeddings, followed by fine-tuning only the layer normalization and positional embeddings of the LLM to reduce computational cost. The method achieves competitive or superior performance over state-of-the-art methods in long-term, short-term, and few-shot forecasting tasks, with up to 5% reduction in MSE in few-shot settings.

## Method Summary
NNCL-TLLM formulates prompts by concatenating time series patch embeddings with top-k nearest neighbor TCTPs from a support set. The method uses GPT-2 as backbone, learns TCTPs through neighborhood-aware contrastive learning, and finetunes only layer normalization and positional embeddings. Time series are divided into overlapping patches, embedded, and matched with TCTPs to create information-rich prompts. The model minimizes contrastive loss to align TCTPs with time series embeddings while preserving LLM's learned prior through selective fine-tuning.

## Key Results
- Achieves average MSE of 0.402 and MAE of 0.423 across four long-term forecasting horizons on ETTh1 dataset
- Outperforms baselines in few-shot forecasting with up to 5% MSE reduction when trained on 10% of data
- Demonstrates competitive performance on short-term forecasting with SMAPE of 1.22 and MASE of 1.01 on ECL dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NNCL-TLLM aligns LLM word token embeddings with time series embeddings by learning neighborhood-aware time series compatible text prototypes (TCTPs).
- Mechanism: The model creates TCTPs that are representatives of the word token embeddings in their neighborhood in the embedding space. During training, the distance between each word token embedding and its nearest TCTP embedding is minimized, and the contrastive loss aligns the TCTP with the time series embedding.
- Core assumption: There is no explicit mapping between time series and word tokens, so a neighborhood-based approach is needed to create compatible representations.
- Evidence anchors:
  - [abstract]: "formulate the prompt which utilizes the word token embeddings while effectively representing the characteristics of the time series"
  - [section]: "These TCTPs become time series compatible through the end-to-end finetuning of the entire framework with NNCL."

### Mechanism 2
- Claim: The method achieves competitive or superior performance in time series forecasting by leveraging the strong prior learned by LLMs and effectively formulating prompts using NNCL.
- Mechanism: The model formulates prompts by concatenating time series patch embeddings with the top-k nearest neighbor TCTPs from the support set. This information-rich embedding as the input prompt to finetune the LLM ensures that the LLM has not only the temporal context from time series embedding but also the corresponding time series compatible textual representations.
- Core assumption: LLMs trained on abundant text data have a robust capability to recognize patterns in sequences, which can be transferred to time series forecasting.
- Evidence anchors:
  - [abstract]: "Our comprehensive experiments demonstrate that NNCL-TLLM outperforms in few-shot forecasting while achieving competitive or superior performance over the state-of-the-art methods in long-term and short-term forecasting tasks."

### Mechanism 3
- Claim: The method reduces computational cost and maintains the learned prior of LLMs by only finetuning layer normalization and positional embeddings.
- Mechanism: By finetuning only the layer normalization and positional embeddings of the LLM, the model reduces the number of trainable parameters while preserving the self-attention and feed-forward layers that encapsulate most of the knowledge learned during pre-training.
- Core assumption: The self-attention and feed-forward layers of LLMs contain most of the learned knowledge, so finetuning only the normalization and positional embeddings is sufficient to adapt the model.
- Evidence anchors:
  - [abstract]: "We then fine-tune the layer normalization and positional embeddings of the LLM, keeping the other layers intact, reducing the trainable parameters and decreasing the computational cost."

## Foundational Learning

- Concept: Time series patching
  - Why needed here: Time series patching allows the model to capture relevant information from long time series with a look-back window while minimizing computational costs. It divides the input time series into overlapping patches, which are then embedded to create a manageable representation for the LLM.
  - Quick check question: What is the purpose of dividing the time series into overlapping patches before embedding?

- Concept: Nearest neighbor contrastive learning (NNCL)
  - Why needed here: NNCL is used to formulate prompts that align with both time series and text. It involves creating a support set of TCTPs and using the nearest neighbors of the time series embedding in this set to compute a contrastive loss, which aligns the TCTP with the time series.
  - Quick check question: How does the contrastive loss in NNCL help align the TCTP with the time series embedding?

- Concept: Layer normalization and positional embeddings in LLMs
  - Why needed here: These components are finetuned to adapt the LLM for time series forecasting while preserving the learned prior. Layer normalization helps stabilize the training process, and positional embeddings provide information about the order of the elements in the sequence.
  - Quick check question: Why are only the layer normalization and positional embeddings finetuned in this method, and not the other layers of the LLM?

## Architecture Onboarding

- Component map:
  Input -> Patch embedding layer -> Linear layer -> Neighborhood aware TCTP learning -> NNCL -> LLM (finetuned) -> Output projection

- Critical path:
  1. Input univariate time series is normalized and divided into overlapping patches.
  2. Patch embeddings are created and passed through a linear layer to generate the time series embedding.
  3. Neighborhood aware TCTPs are learned using the word token embeddings of the LLM.
  4. The contrastive loss aligns the TCTP with the time series embedding.
  5. The prompt is formulated by concatenating the time series patch embeddings with the top-k nearest neighbor TCTPs.
  6. The LLM is finetuned using the formulated prompt (only layer normalization and positional embeddings are updated).
  7. The output projection layer generates the final forecast.
  8. The MSE loss is computed between the forecast and the ground truth.

- Design tradeoffs:
  - Finetuning only layer normalization and positional embeddings reduces computational cost but may limit the model's ability to adapt to time series patterns.
  - Using a support set for NNCL increases the model's capacity but also increases memory usage.
  - The choice of the number of TCTPs and the size of the support set affects the model's performance and computational requirements.

- Failure signatures:
  - If the model's performance is poor, it could be due to insufficient alignment between the TCTP and time series embeddings, inadequate finetuning of the LLM, or suboptimal hyperparameters.
  - If the model is computationally expensive, it could be due to a large number of TCTPs, a large support set, or inefficient implementation.

- First 3 experiments:
  1. Verify that the patch embedding layer correctly divides the time series into overlapping patches and creates patch embeddings.
  2. Check that the neighborhood aware TCTP learning creates TCTPs that are representatives of word token embeddings in their neighborhood.
  3. Ensure that the contrastive loss aligns the TCTP with the time series embedding by visualizing the embeddings before and after training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does NNCL-TLLM perform when incorporating channel dependencies in multivariate time series forecasting, and what architectural modifications would be needed to capture cross-channel information?
- Basis in paper: [explicit] The authors explicitly identify incorporating channel dependencies of multivariate time series as a limitation in the conclusion, stating "A limitation that needs to be addressed is incorporating the channel dependencies of multivariate time series."
- Why unresolved: The current method uses channel independence, processing each univariate time series separately, which may miss important cross-channel correlations in multivariate data.
- What evidence would resolve it: Comparative experiments showing performance differences between channel-independent and channel-dependent variants on multivariate datasets, along with architectural details for capturing cross-channel information.

### Open Question 2
- Question: What is the optimal balance between the number of neighborhood-aware TCTPs and the size of the support set for maximizing forecasting accuracy across different time series domains?
- Basis in paper: [explicit] The authors mention conducting experiments with varying values for the number of TCTPs ([1000, 5000, 8000]) and support set sizes ([x10, x20, x50, x70, x100] times the number of TCTPs), but do not provide a systematic analysis of the optimal balance.
- Why unresolved: The paper reports parameter sensitivity analysis but doesn't determine optimal values or investigate how these parameters interact across different domains and forecasting tasks.
- What evidence would resolve it: Comprehensive sensitivity analysis across multiple datasets showing optimal parameter combinations, including interaction effects between TCTP count and support set size.

### Open Question 3
- Question: How does the performance of NNCL-TLLM compare when using different LLM backbones beyond GPT-2, particularly larger models or those with different tokenization schemes?
- Basis in paper: [explicit] The authors mention that GPT-2 was used as the backbone in all experiments and briefly compare with Llama-2 in computational efficiency analysis, but don't explore different LLM architectures systematically.
- Why unresolved: The paper focuses exclusively on GPT-2 despite the general approach being potentially applicable to other LLMs, leaving questions about scalability and transfer to larger models.
- What evidence would resolve it: Comparative experiments using multiple LLM backbones (different sizes, architectures, tokenization schemes) on the same forecasting tasks, with performance and computational efficiency analysis.

## Limitations

- The method is currently limited to univariate time series forecasting and does not incorporate channel dependencies in multivariate time series.
- The computational efficiency claims are not fully validated with concrete measurements of training/inference time and memory usage.
- The optimal balance between the number of TCTPs and the size of the support set is not systematically analyzed across different domains and forecasting tasks.

## Confidence

- High confidence in the core mechanism: The neighborhood-aware TCTP learning and contrastive alignment approach is theoretically sound and well-grounded in the literature on metric learning and prototype-based representations.
- Medium confidence in performance claims: The reported improvements (up to 5% MSE reduction in few-shot settings) are significant but based on a specific set of benchmarks.
- Low confidence in computational efficiency claims: Without concrete measurements of training/inference time and memory usage, the assertion that this method is computationally efficient remains unverified.

## Next Checks

1. **TCTP quality assessment**: Visualize the learned TCTPs in embedding space using t-SNE or UMAP to verify they form meaningful clusters that align with time series patterns, addressing the core assumption of Mechanism 1.

2. **Ablation study on finetuning scope**: Systematically compare performance when finetuning different combinations of LLM components (only normalization, only positional embeddings, both, plus a few transformer layers) to validate the claimed computational efficiency of Mechanism 3.

3. **Few-shot generalization test**: Evaluate the method's performance when trained on 1% or even 0.1% of the training data to determine the actual limits of few-shot capability and identify potential overfitting to the support set.