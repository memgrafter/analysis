---
ver: rpa2
title: Influential Language Data Selection via Gradient Trajectory Pursuit
arxiv_id: '2410.16710'
source_url: https://arxiv.org/abs/2410.16710
tags:
- data
- selection
- algorithm
- gradient
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Gradient Trajectory Pursuit (GTP), a data selection
  algorithm that jointly selects influential language data through matching gradient
  trajectories in a subspace. The method uses a pursuit process equivalent to starting
  from a top-k initialization and iteratively de-duplicating samples, while also supporting
  distributed computation across machines.
---

# Influential Language Data Selection via Gradient Trajectory Pursuit

## Quick Facts
- arXiv ID: 2410.16710
- Source URL: https://arxiv.org/abs/2410.16710
- Authors: Zhiwei Deng; Tao Li; Yang Li
- Reference count: 12
- Primary result: Achieves 82.1% accuracy on ALFWorld with 20% of data, a 38.9% improvement over random selection

## Executive Summary
This paper introduces Gradient Trajectory Pursuit (GTP), a data selection algorithm that identifies influential language data points by matching gradient trajectories in a subspace. The method uses joint selection via L0-norm regularized optimization to automatically de-duplicate samples while reducing memory and computation costs through gradient projection. Experiments demonstrate that GTP outperforms top-k selection and competitive algorithms on both in-domain (ALFWorld agent tasks) and target-domain (instruction tuning) benchmarks, achieving full performance on targeted tasks while using only 0.5% of the full dataset.

## Method Summary
GTP selects influential data by matching gradient trajectories computed during warmup training. The algorithm projects gradients onto a low-dimensional subspace using PCA, then jointly selects data points through iterative compressive sampling pursuit with L0-norm regularization. This joint selection automatically de-duplicates similar samples while optimizing for gradient trajectory matching. The method supports both single-machine and distributed implementations, with the distributed version (DCSP) partitioning data across machines to achieve linear scaling.

## Key Results
- Achieves 82.1% accuracy on ALFWorld with only 20% of data, a 38.9% improvement over random selection
- Reaches full performance on targeted tasks using just 0.5% of the full dataset
- Outperforms top-k selection and competitive algorithms on both in-domain and target-domain benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint selection via L0-norm regularized objective automatically de-duplicates samples, improving subset quality
- Mechanism: The L0-norm regularization enforces sparsity on weight vector w, meaning only a subset of data points get non-zero weights. When multiple samples have similar gradient trajectories, the joint optimization will assign non-zero weights to only one representative, implicitly removing duplicates
- Core assumption: Similar gradient trajectories correspond to redundant information; the optimization can distinguish and prefer one over the others
- Evidence anchors: [abstract] "joint selection instead of independent top-k selection, which automatically de-duplicates samples"; [section] "We introduce a set of weights wi for each data point and define the gradient as ∇θlogp(θ|DS)=PN i=1wi∇θlogp((xi,yi);θ)... Due to sparsity enforced in L0 norm, the indices of final weights w that are non-negative is the final selected set S."
- Break condition: If gradient trajectories are not correlated with data redundancy, or if the L0-norm optimization fails to converge to a sparse solution, de-duplication may not occur

### Mechanism 2
- Claim: Matching gradient trajectories in a subspace reduces storage and computation cost while preserving essential information for selection
- Mechanism: By projecting gradients onto a low-dimensional subspace (computed via PCA), the algorithm reduces the dimensionality of the gradient space from model parameter size to subspace dimension. This makes storage feasible and speeds up the selection process
- Core assumption: The optimization process occurs in a subspace, and projecting gradients does not lose critical information for identifying influential samples
- Evidence anchors: [abstract] "we project the gradients onto a small subspace and greatly reduces the memory cost (both on disk and RAM) during selection"; [section] "we project the model gradients onto a subspace before matching, both to reduce the storage and computation cost and to rule out potential noise signals"
- Break condition: If the subspace dimension is too small to capture important gradient variations, or if the PCA subspace is not representative of the optimization dynamics, the selection quality may degrade

### Mechanism 3
- Claim: The pursuit process iteratively improves subset quality by adjusting data weights, starting from a top-k initialization and refining through de-duplication
- Mechanism: The algorithm starts with a top-k selection, then iteratively solves a non-negative least squares problem to adjust weights. This process removes redundancy by reducing weights of similar samples and increasing weights of complementary ones
- Core assumption: The initial top-k selection has redundancy, and iterative refinement can identify and correct this
- Evidence anchors: [abstract] "The pursuit process is equivalent to starting from a top-k initialization, iteratively determines the proper combination of data samples and implicitly performs de-duplication"; [section] "In figure 4, we plot the accuracies of subsets of 1000 samples selected after each GTP iteration. Through progressively solving the data weights w, the performance gradually improves"
- Break condition: If the iterative process converges too quickly without significant improvement, or if the residual norm does not decrease, the refinement may not be effective

## Foundational Learning

- Concept: Gradient-based data selection and influence scores
  - Why needed here: The algorithm relies on gradient information to identify influential data points. Understanding how gradient influence scores correlate with model performance is crucial for grasping the algorithm's rationale
  - Quick check question: What is the difference between feature-based and gradient-based data selection methods, and why might gradient-based methods be preferred for capturing optimization dynamics?

- Concept: Compressive sensing and matching pursuit algorithms
  - Why needed here: The algorithm uses iterative compressive sampling matching pursuit to solve the L0-norm regularized objective. Knowledge of these algorithms is necessary to understand the selection process and its efficiency
  - Quick check question: How does iterative compressive sampling matching pursuit differ from orthogonal matching pursuit, and what are the advantages in terms of computation time and scalability?

- Concept: Subspace methods and dimensionality reduction (e.g., PCA)
  - Why needed here: The algorithm projects gradients onto a subspace to reduce computational cost. Understanding subspace methods and PCA is essential for grasping how the algorithm handles high-dimensional gradient data
  - Quick check question: Why is projecting gradients onto a subspace beneficial for data selection, and what are the potential trade-offs in terms of information loss?

## Architecture Onboarding

- Component map: Warmup training -> Gradient extraction and storage -> Subspace computation -> Selection algorithm -> Evaluation

- Critical path:
  1. Warmup training (generates parameter states)
  2. Gradient extraction (computes gradients for all data points)
  3. Subspace computation (projects gradients onto low-dimensional subspace)
  4. Selection algorithm (jointly selects influential data points)
  5. Evaluation (measures performance on target tasks)

- Design tradeoffs:
  - Joint vs. independent selection: Joint selection (GTP) automatically de-duplicates samples but is more computationally intensive than top-k selection
  - Subspace dimension: Higher subspace dimensions may capture more information but increase computational cost; lower dimensions are faster but may lose important details
  - Distributed vs. single-machine: Distributed implementation scales better but introduces communication overhead

- Failure signatures:
  - Slow convergence of the pursuit algorithm (residual norm does not decrease)
  - Poor performance on target tasks despite high gradient similarity scores
  - Excessive memory usage during gradient storage (indicates subspace dimension is too high or number of timesteps is too large)

- First 3 experiments:
  1. Run GTP on a small subset of data (e.g., 1% of full dataset) to verify the basic functionality and measure computation time
  2. Compare the selected subset's performance with random and top-k selection on a simple benchmark (e.g., MMLU) to validate the effectiveness
  3. Vary the subspace dimension and number of timesteps to study their impact on selection quality and computation time

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation: the theoretical relationship between matching objective and generalization gap, extension to mixed precision training scenarios, and systematic analysis of subspace dimension selection across different tasks and model sizes.

## Limitations
- Computational overhead of storing gradient trajectories across multiple timesteps for large-scale language models is not fully addressed
- Claim of automatic de-duplication assumes gradient similarity correlates with data redundancy without empirical validation
- Distributed implementation's linear scaling claims assume negligible communication overhead, which may not hold in practice
- Evaluation focuses on specific benchmarks without exploring robustness across diverse domains or failure cases

## Confidence

**High Confidence**: The mathematical formulation of GTP is sound, and the iterative pursuit process is well-defined. The empirical improvements over top-k selection on the tested benchmarks are statistically significant.

**Medium Confidence**: The mechanism of automatic de-duplication through joint selection is plausible but relies on assumptions about gradient trajectory similarity that are not thoroughly validated. The computational benefits of subspace projection are claimed but not extensively benchmarked against alternative dimensionality reduction methods.

**Low Confidence**: The claim that 0.5% of data achieves full performance is based on specific datasets and may not generalize to other domains. The distributed implementation's scaling claims are theoretical and lack empirical validation across different cluster configurations.

## Next Checks

1. **De-duplication Validation**: Run ablation studies comparing GTP with and without the joint selection component on datasets with known redundancies to quantify the de-duplication benefit

2. **Subspace Sensitivity Analysis**: Systematically vary the subspace dimension and number of timesteps to measure the trade-off between selection quality and computational cost across multiple datasets

3. **Distributed Scaling Benchmark**: Implement DCSP on clusters with varying numbers of machines and measure actual scaling behavior versus the claimed linear relationship, documenting communication overhead