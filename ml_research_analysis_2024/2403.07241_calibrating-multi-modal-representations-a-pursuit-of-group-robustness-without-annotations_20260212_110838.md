---
ver: rpa2
title: 'Calibrating Multi-modal Representations: A Pursuit of Group Robustness without
  Annotations'
arxiv_id: '2403.07241'
source_url: https://arxiv.org/abs/2403.07241
tags:
- clip
- spurious
- group
- dataset
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies mitigating spurious correlations in pre-trained
  vision-language models like CLIP without requiring group annotations. The authors
  systematically analyze the presence of spurious correlations in CLIP and verify
  that last-layer retraining can improve group robustness.
---

# Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations

## Quick Facts
- arXiv ID: 2403.07241
- Source URL: https://arxiv.org/abs/2403.07241
- Reference count: 40
- Primary result: Achieves state-of-the-art robust classification by mitigating spurious correlations in CLIP without group annotations

## Executive Summary
This paper addresses the challenge of spurious correlations in pre-trained vision-language models like CLIP, particularly when group annotations are unavailable. The authors propose a lightweight representation calibration method that first generates a calibration set using the pretrained CLIP and then refines sample representations through contrastive learning, all without requiring group labels. Extensive experiments across multiple benchmarks demonstrate significant improvements in worst-group accuracy and average accuracy, achieving state-of-the-art results in robust classification tasks.

## Method Summary
The paper proposes a Contrastive Feature Recalibration (CFR) method that operates in two stages: first, a calibration set is formulated by selecting samples misclassified by an ERM-tuned CLIP model; second, sample representations within this set are refined through contrastive learning that pulls samples toward their class centroids while pushing them away from opposing class centroids. The method combines this calibration loss with a holistic data integration approach that includes cosine similarity loss across the entire training dataset. The approach leverages CLIP's pretrained encoders while only fine-tuning the final layers, making it computationally efficient while maintaining strong performance across multiple robustness benchmarks.

## Key Results
- Achieves state-of-the-art performance on Waterbirds, CelebA, CheXpert, MetaShift, and Colored-MNIST benchmarks
- Significantly improves worst-group accuracy (WGA) while maintaining high average accuracy across all tested datasets
- Demonstrates that representation calibration can effectively mitigate spurious correlations without requiring group annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Last-layer retraining improves group robustness by recalibrating the feature representation without altering the pretrained feature extractor.
- Mechanism: The pretrained CLIP learns robust visual features, but the final classification layer can overfit to spurious correlations in the training data. Retraining only the last layer adjusts the decision boundaries to reduce reliance on these spurious features.
- Core assumption: The feature representations learned by the pretrained CLIP are sufficiently robust and only the final classification layer needs adjustment to mitigate spurious correlations.
- Evidence anchors:
  - [abstract] "We first, following recent work on Deep Feature Reweighting (DFR), verify that last-layer retraining can greatly improve group robustness on pretrained CLIP."
  - [section] "To tackle the challenge of spurious correlations without incurring substantial computational overhead, we draw inspiration from [23] to calibrate feature representation quality by re-training of CLIP's final layer."
  - [corpus] Weak evidence; no direct corpus support for this specific mechanism.
- Break condition: If the pretrained CLIP's feature representations themselves are biased or not robust, last-layer retraining alone will be insufficient.

### Mechanism 2
- Claim: Contrastive learning with carefully selected positive and negative samples recalibrates representations to be more aligned with class centroids and less influenced by spurious attributes.
- Mechanism: By pulling anchor samples closer to their class centroids and pushing them away from opposing class centroids, the model learns to focus on features that are truly indicative of the class rather than spurious correlations.
- Core assumption: The centroids of classes in the feature space represent the "true" class characteristics, and moving samples towards these centroids will improve robustness.
- Evidence anchors:
  - [abstract] "We advocate a lightweight representation calibration method for fine-tuning CLIP, by first generating a calibration set using the pretrained CLIP, and then calibrating representations of samples within this set through contrastive learning, all without the need for group labels."
  - [section] "Our proposed CFR method is capable of recalibrating the feature of a given sample. This is achieved by pulling the sample's feature closer to the centroid of its designated class while simultaneously pushing it away from centroids associated with opposing classes."
  - [corpus] Weak evidence; no direct corpus support for this specific mechanism.
- Break condition: If the centroids themselves are influenced by spurious correlations, moving samples towards them will not improve robustness.

### Mechanism 3
- Claim: Using samples misclassified by the pretrained CLIP for the calibration set focuses the recalibration on the most problematic cases where spurious correlations are likely to be strongest.
- Mechanism: Samples that the pretrained CLIP misclassifies are likely relying heavily on spurious features. By recalibrating these samples, the model learns to correct its most significant errors.
- Core assumption: Misclassifications by the pretrained CLIP are indicative of reliance on spurious correlations.
- Evidence anchors:
  - [abstract] "CFR employs an intuitive strategy, selecting samples misclassified by the pretrained CLIP."
  - [section] "Our strategy involves training only the projection head of CLIP using cross-entropy loss, subsequently selecting samples that are misclassified by this ERM-tuned CLIP for further rectification."
  - [corpus] Weak evidence; no direct corpus support for this specific mechanism.
- Break condition: If the pretrained CLIP's misclassifications are not due to spurious correlations but other factors, this strategy will not be effective.

## Foundational Learning

- Concept: Spurious correlations in machine learning
  - Why needed here: The paper addresses the problem of models relying on spurious features (features that correlate with the target in training data but are not causally related) which can lead to poor generalization and reduced robustness, especially for underrepresented groups.
  - Quick check question: Can you explain what spurious correlations are and why they are problematic for model generalization?

- Concept: Contrastive learning
  - Why needed here: The paper proposes a contrastive learning approach to recalibrate sample representations, pulling them closer to their class centroids and pushing them away from opposing class centroids.
  - Quick check question: How does contrastive learning work, and how can it be used to improve the quality of feature representations?

- Concept: Representation calibration
  - Why needed here: The paper introduces a representation calibration method that involves generating a calibration set and refining sample representations through contrastive learning to improve group robustness without group annotations.
  - Quick check question: What is representation calibration, and how can it be used to improve model robustness?

## Architecture Onboarding

- Component map:
  Pretrained CLIP (visual and text encoders) -> Projection Head (trained with cross-entropy loss) -> Calibration Set (misclassified samples) -> Contrastive Feature Recalibration (CFR) with positive and negative batches -> Holistic Data Integration (calibration + cosine similarity loss)

- Critical path:
  1. Pretrained CLIP generates initial representations.
  2. Projection head is trained to select misclassified samples.
  3. Calibration set is created from these misclassified samples.
  4. CFR refines representations using contrastive learning.
  5. Holistic data integration combines calibration and cosine similarity losses.
  6. Model achieves improved group robustness.

- Design tradeoffs:
  - Last-layer retraining vs. full model fine-tuning: Last-layer retraining is computationally efficient but may be limited if feature representations are also biased.
  - Positive and negative sample selection strategies: Different strategies (DPS+RNS, DPS+NNS, RPS+RNS, RPS+NNS) offer varying levels of performance and computational cost.
  - Size of calibration set: A larger calibration set may improve performance but also increase computational cost.

- Failure signatures:
  - No improvement in worst-group accuracy (WGA) despite recalibration: Indicates that the recalibration strategy is not effectively addressing the spurious correlations.
  - Degradation in overall accuracy: Suggests that the recalibration is overcorrecting and harming the model's ability to learn from genuine features.
  - Overfitting to the calibration set: Indicates that the model is not generalizing well beyond the calibration set.

- First 3 experiments:
  1. Verify the presence of spurious correlations in the pretrained CLIP using t-SNE or UMAP visualizations on benchmark datasets.
  2. Evaluate the effectiveness of last-layer retraining on the pretrained CLIP's group robustness using WGA as the metric.
  3. Compare the performance of different sample selection strategies (DPS+RNS, DPS+NNS, RPS+RNS, RPS+NNS) for the CFR method using WGA and average accuracy as metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CFR scale with larger calibration set sizes, and what is the optimal size for different dataset characteristics?
- Basis in paper: [inferred] The paper mentions that determining the size and distribution of the calibration set beforehand is challenging and discusses the risk of overfitting with a limited-sized calibration set. It also notes the use of Holistic Data Integration to mitigate this risk.
- Why unresolved: The paper does not provide a systematic study on how calibration set size affects performance across different datasets or provide guidelines for optimal size selection.
- What evidence would resolve it: A comprehensive ablation study varying calibration set sizes across multiple benchmarks, showing performance curves and identifying optimal sizes for different dataset characteristics (e.g., number of classes, dataset size, degree of spurious correlation).

### Open Question 2
- Question: Can the DPS strategy be further improved by incorporating uncertainty estimates or model confidence scores in the positive sample selection process?
- Basis in paper: [explicit] The paper describes the DPS strategy as selecting samples correctly predicted by the pre-trained CLIP and using an EMA estimate of the class centroid, but notes this could include misaligned samples.
- Why unresolved: The paper acknowledges potential limitations of the current DPS approach but does not explore whether incorporating uncertainty or confidence metrics could lead to better positive sample selection.
- What evidence would resolve it: Comparative experiments showing performance of DPS variants that incorporate model uncertainty (e.g., entropy-based selection, confidence thresholding) against the baseline DPS strategy across multiple datasets.

### Open Question 3
- Question: How does the choice of negative sampling strategy (RNS vs NNS) affect performance when using different batch sizes or when fine-tuning larger CLIP variants (e.g., CLIP-ViT-L/14@336px)?
- Basis in paper: [explicit] The paper notes that NNS may require larger batch sizes to be effective and observes that RNS performs better than NNS for CLIP-ResNet50, but this finding may be batch size dependent.
- Why unresolved: The paper only explores NNS with a fixed batch size and observes its sensitivity to hyperparameters, but does not systematically investigate the interaction between batch size, model architecture, and negative sampling strategy.
- What evidence would resolve it: A grid search experiment varying batch sizes and negative sampling strategies across different CLIP architectures, showing performance trade-offs and identifying optimal configurations for each setting.

## Limitations
- The method relies on CLIP's pretrained feature representations being sufficiently robust, which may not hold for all datasets or scenarios.
- The effectiveness of the contrastive learning approach depends on the quality of the calibration set and the chosen positive/negative sampling strategies.
- The paper does not provide a thorough analysis of the computational cost of the proposed method compared to other approaches.

## Confidence
- High confidence: The paper's systematic analysis of spurious correlations in CLIP and the effectiveness of last-layer retraining
- Medium confidence: The proposed CFR method's ability to improve group robustness, as the results are promising but may not generalize to all datasets
- Low confidence: The paper's claim of achieving state-of-the-art results without group annotations, as the comparison with other methods is limited

## Next Checks
1. Evaluate the proposed method on additional datasets with known spurious correlations to assess its generalizability.
2. Compare the computational cost of the CFR method with other approaches for mitigating spurious correlations, such as full model fine-tuning or data augmentation.
3. Analyze the quality of the calibration set and the impact of different positive/negative sampling strategies on the method's performance.