---
ver: rpa2
title: Is it the model or the metric -- On robustness measures of deeplearning models
arxiv_id: '2412.09795'
source_url: https://arxiv.org/abs/2412.09795
tags:
- ieee
- robustness
- adversarial
- icassp
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates the robustness of deep learning models
  for deepfake detection by comparing two metrics: robust accuracy (RA) and robust
  ratio (RR). RA measures the proportion of correct classifications under adversarial
  attacks, while RR captures the stability of model output probabilities under input
  perturbations, even when class labels remain unchanged.'
---

# Is it the model or the metric -- On robustness measures of deeplearning models

## Quick Facts
- arXiv ID: 2412.09795
- Source URL: https://arxiv.org/abs/2412.09795
- Authors: Zhijin Lyu; Yutong Jin; Sneha Das
- Reference count: 24
- Primary result: Robust Ratio (RR) reveals model robustness variations that Robust Accuracy (RA) misses, particularly under different tolerance levels

## Executive Summary
This work investigates deep learning model robustness for deepfake detection by comparing two metrics: robust accuracy (RA) and robust ratio (RR). While RA measures correct classifications under adversarial attacks, RR captures the stability of model output probabilities under input perturbations, even when class labels remain unchanged. The authors demonstrate that RR provides a more nuanced evaluation of model robustness by identifying optimal operating regions that RA cannot detect. Using three deepfake detection models (Meso4, Meso4Inception, ResNet34) and three attack methods (FGSM, PGD, CW) on image and video datasets, the study shows that RR exhibits different behavior patterns across tolerance levels and dataset types.

## Method Summary
The authors evaluate model robustness using three deepfake detection architectures (Meso4, Meso4Inception, ResNet34) on image and video datasets (UADFV, FF++, Celeb-DF-v2). They apply three adversarial attack methods (FGSM, PGD, CW) at tolerance levels from 0 to 0.2. RA is computed as the proportion of correct classifications under attack, while RR measures the proportion of samples where perturbations result in bounded changes to normalized output. The study systematically compares these metrics across different models, attack types, and tolerance levels to identify optimal operating regions for each combination.

## Key Results
- RA shows similar performance across all models and attack types, while RR reveals significant variations under different tolerance levels
- RR provides more granular evaluation by capturing probability stability even when predicted classes remain unchanged
- Video datasets exhibit more linear RR-tolerance relationships compared to image datasets, potentially due to progressive perturbation application across frames
- The study identifies optimal operating regions where models perform best under specific tolerance and bound values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robust Ratio (RR) captures stability of model output probabilities under perturbations, even when class labels remain unchanged
- Mechanism: RR measures the proportion of examples where small perturbations in input result in bounded changes in the normalized output or probability, allowing quantification of how stable the continuous output remains within a desired perturbation range
- Core assumption: Small input perturbations can cause changes in model output probabilities without changing the predicted class label
- Evidence anchors:
  - [abstract] "RR captures the proportion of examples where arbitrary input perturbations result in equivalent changes in normalized continuous (or probability) outcome of the model"
  - [section] "RR allows the quantification of how stable the continuous output remains within a desired perturbation range, even if the final predicted label does not change"
  - [corpus] No direct evidence found in corpus papers, but related work on probabilistic robustness exists
- Break condition: When perturbations are large enough to change the predicted class label, making the bounded probability change less meaningful

### Mechanism 2
- Claim: Different models can have similar Robust Accuracy (RA) but varying RR under different tolerance levels
- Mechanism: RA only measures whether the predicted class remains correct under adversarial perturbations, while RR captures the stability of the probability distribution, revealing different model behaviors that RA misses
- Core assumption: Model robustness has multiple dimensions that RA alone cannot capture
- Evidence anchors:
  - [abstract] "despite similar RA between models, the models show varying RR under different tolerance (perturbation) levels"
  - [section] "RA shows similar performances for all models and over all attack types, for both images and videos. In contrast, RR shows that the performance of models is largely influenced by the tolerance and bound values"
  - [corpus] No direct evidence found in corpus papers, but similar concepts discussed in related robustness work
- Break condition: When all models behave identically under all perturbation levels, making RR redundant

### Mechanism 3
- Claim: Video datasets show more linear relationship between RR and tolerance compared to image datasets
- Mechanism: Temporal dimension in video datasets may result in smoother and more cumulative changes in perturbations across frames, leading to gradual accumulation of perturbation effects
- Core assumption: Adversarial perturbations are applied more progressively or iteratively across video frames compared to direct application on entire image frames
- Evidence anchors:
  - [abstract] "RR exhibits a more linear relationship with tolerance for video datasets compared to image datasets. We hypothesize that this difference may be attributed to variations in the implementation of adversarial attacks across the two types of datasets"
  - [section] "we suspect that in video datasets, the adversarial perturbations are applied more progressively or iteratively across frames, leading to a gradual accumulation of the perturbation over time"
  - [corpus] No direct evidence found in corpus papers, but related work on video perturbations exists
- Break condition: When video and image datasets show similar perturbation patterns, making the temporal difference negligible

## Foundational Learning

- Concept: Adversarial attacks and their types (FGSM, PGD, CW)
  - Why needed here: The paper compares model robustness under three different attack methods, requiring understanding of how these attacks work
  - Quick check question: What is the key difference between FGSM (fast gradient sign method) and PGD (projected gradient descent) in terms of how they generate adversarial examples?

- Concept: Deepfake detection models (Meso4, Meso4Inception, ResNet34)
  - Why needed here: The paper evaluates robustness of three specific deepfake detection architectures
  - Quick check question: What architectural feature distinguishes Meso4Inception from the base Meso4 model?

- Concept: Robustness metrics (RA vs RR)
  - Why needed here: The paper introduces RR as a complementary metric to RA and compares their effectiveness
  - Quick check question: How does RR differ from RA in terms of what aspect of model behavior it measures?

## Architecture Onboarding

- Component map:
  Data pipeline: Image and video datasets (UADFV, FF++, Celeb-DF-v2) → Face detection and resizing → Model inference → Adversarial perturbation generation → Metric calculation → Results visualization

- Critical path: Data preprocessing → Model inference → Adversarial perturbation generation → Metric calculation → Results visualization

- Design tradeoffs:
  - RA vs RR: RA is simpler but less informative; RR is more comprehensive but computationally heavier
  - Tolerance levels: Higher tolerance provides more perturbation information but may exceed realistic bounds
  - Dataset choice: Images vs videos have different perturbation characteristics affecting metric interpretation

- Failure signatures:
  - Similar RA but different RR indicates models have different probability stability characteristics
  - Non-linear RR vs tolerance relationship may indicate model sensitivity to specific perturbation magnitudes
  - Video datasets showing different RR patterns than images suggests temporal effects in perturbation propagation

- First 3 experiments:
  1. Replicate RA and RR calculations on a small subset of one dataset to verify metric implementations
  2. Compare RA values across all three models for one attack type to confirm similar RA behavior
  3. Plot RR vs tolerance for all models on image dataset to observe variation in optimal operating regions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different adversarial attack implementations across image and video datasets contribute to the observed linear relationship between RR and tolerance in video datasets?
- Basis in paper: [explicit] The paper hypothesizes that video datasets show a more linear RR-tolerance relationship due to progressive perturbation application across frames versus direct application to entire frames in image datasets
- Why unresolved: The paper only provides a hypothesis about the implementation differences without experimental validation of how the attack methods are actually implemented differently between image and video datasets
- What evidence would resolve it: Comparative analysis of attack implementation details (perturbation application method, frame-by-frame vs. full-frame) across both dataset types, with controlled experiments isolating these factors

### Open Question 2
- Question: How does RR interact with alternative robustness metrics like robustness-score and average-confidence-of-class, and what are the trade-offs between these complementary metrics?
- Basis in paper: [explicit] The paper mentions these alternative metrics are similar to RR in providing granular evaluation of model behavior under adversarial perturbations and states they will explore the interaction in future work
- Why unresolved: The paper acknowledges these metrics exist and are similar but does not provide any empirical comparison or analysis of their relationships or relative strengths
- What evidence would resolve it: Systematic comparison of RR with robustness-score and average-confidence-of-class using the same experimental setup, including correlation analysis and case studies where metrics diverge

### Open Question 3
- Question: What are the optimal tolerance and bound values for different application scenarios (e.g., high-noise clinical data vs. carefully curated benchmarks) when using RR as a robustness measure?
- Basis in paper: [explicit] The paper states that defining optimum regions can aid in choosing models suitable to application scenarios but does not provide specific guidance on tolerance/bound values
- Why unresolved: The paper demonstrates RR varies with tolerance levels but does not establish thresholds or ranges for different real-world use cases
- What evidence would resolve it: Empirical studies mapping RR performance across different tolerance/bound combinations to specific application requirements, including sensitivity analysis for different noise levels and accuracy tolerances

## Limitations
- The hypothesis about progressive perturbation application in video datasets lacks direct experimental validation
- The choice of tolerance bounds (0-0.2) appears arbitrary with minimal sensitivity analysis
- Demographic grouping implications for robustness measurements are not addressed

## Confidence
- **High confidence**: The fundamental mechanism of RR measuring bounded changes in normalized outputs (Mechanism 1) is well-supported by the abstract and core definitions. The observation that RA shows similar results across models while RR reveals variations (Mechanism 2) is directly demonstrated in the results.
- **Medium confidence**: The linear relationship hypothesis for video datasets (Mechanism 3) is plausible but lacks direct experimental evidence. The paper presents the observation but doesn't validate the underlying temporal perturbation mechanism.
- **Low confidence**: The demographic grouping implications for robustness metrics are not addressed, and the sensitivity analysis for tolerance bounds is minimal.

## Next Checks
1. Conduct controlled experiments varying perturbation application methods on video datasets to directly test whether progressive frame-by-frame application produces more linear RR-tolerance relationships compared to global video perturbations.
2. Perform sensitivity analysis of RR metric stability across different tolerance bounds to determine if the 0-0.2 range is optimal or if results are sensitive to this choice.
3. Replicate the study with additional deepfake detection architectures beyond the three tested models to verify whether the RA-RR divergence pattern holds across a broader model space.