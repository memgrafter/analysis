---
ver: rpa2
title: 'SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using
  Decoder-Only Transformers'
arxiv_id: '2411.13428'
source_url: https://arxiv.org/abs/2411.13428
tags:
- data
- time
- should
- series
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SynEHRgy, a method for generating synthetic
  Electronic Health Records (EHRs) that includes mixed data types such as ICD codes
  and irregularly-sampled time series across multiple patient visits. The core innovation
  is a novel tokenization strategy for numerical variables, which enables decoder-only
  transformer models to effectively learn and generate high-quality synthetic EHR
  data.
---

# SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using Decoder-Only Transformers
## Quick Facts
- arXiv ID: 2411.13428
- Source URL: https://arxiv.org/abs/2411.13428
- Reference count: 40
- Primary result: Novel decoder-only transformer approach for generating synthetic EHRs with mixed data types, achieving superior fidelity, utility, and privacy compared to state-of-the-art models on MIMIC-III dataset

## Executive Summary
SynEHRgy introduces a novel method for generating synthetic Electronic Health Records (EHRs) that handle mixed data types including ICD codes and irregularly-sampled time series across multiple patient visits. The core innovation is a specialized tokenization strategy for numerical variables that enables decoder-only transformer models to effectively learn and generate high-quality synthetic EHR data. The approach was evaluated on the MIMIC-III dataset and demonstrated superior performance compared to state-of-the-art models in terms of fidelity, utility, and privacy, making it particularly valuable for healthcare applications where data sharing is restricted due to privacy concerns.

## Method Summary
SynEHRgy leverages decoder-only transformer models to generate synthetic EHRs by first converting structured patient records into a sequence of tokens through a novel tokenization strategy. This strategy discretizes numerical variables into bins while preserving the semantic meaning of continuous measurements. The model is trained autoregressively to predict the next token in a patient's visit sequence, learning both the temporal dependencies and the relationships between different data types. The architecture handles mixed-type data by encoding categorical variables (like ICD codes) and discretized numerical measurements within the same token space, allowing the transformer to learn joint distributions across all features. During generation, the model can produce realistic synthetic patient trajectories that maintain the statistical properties and temporal patterns of real EHR data.

## Key Results
- Achieved high fidelity in preserving ICD code sequences and time series correlations in synthetic EHR generation
- Demonstrated competitive utility in downstream tasks, specifically mortality prediction on MIMIC-III dataset
- Showed strong privacy protection against membership inference attacks while maintaining data utility

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge of generating synthetic data that preserves complex temporal and cross-variable dependencies present in real EHRs. The decoder-only transformer architecture, combined with the novel tokenization approach for numerical variables, allows the model to learn the joint distribution of mixed-type data effectively. By treating the generation as a sequence prediction problem, the model captures the sequential nature of patient visits while maintaining the statistical properties of each variable type. The tokenization strategy bridges the gap between continuous measurements and the discrete token space required by transformers, enabling the model to generate realistic numerical values while preserving the underlying distributions.

## Foundational Learning
- **Decoder-only transformers**: These architectures are needed because they can handle the autoregressive generation of sequences while maintaining attention over previously generated tokens. Quick check: Verify the model uses masked self-attention to prevent looking ahead in the sequence.
- **Tokenization of numerical variables**: Essential for converting continuous measurements into discrete tokens that transformers can process. Quick check: Confirm the binning strategy preserves meaningful distinctions between measurement values.
- **Mixed-type data handling**: Required because EHRs contain both categorical (ICD codes) and numerical (vital signs, lab results) data. Quick check: Validate that the model maintains correlations between different variable types in generated samples.
- **Irregular time series processing**: Important for handling the asynchronous nature of clinical measurements across patient visits. Quick check: Ensure the model captures temporal dependencies despite irregular sampling intervals.
- **Privacy-utility tradeoff**: Critical consideration for synthetic data generation in healthcare where data sharing is restricted. Quick check: Verify that privacy metrics don't significantly degrade when utility is maintained.
- **Autoregressive generation**: Enables sequential generation of patient visit records while maintaining temporal coherence. Quick check: Confirm that generated sequences follow realistic temporal patterns observed in real data.

## Architecture Onboarding
**Component Map**: Patient records -> Tokenizer -> Decoder-only Transformer -> Synthetic EHR generator
**Critical Path**: Data preprocessing (tokenization) → Model training (autoregressive learning) → Synthetic data generation → Quality evaluation (fidelity, utility, privacy)
**Design Tradeoffs**: The use of decoder-only transformers simplifies the architecture compared to encoder-decoder models but requires careful attention masking. The tokenization strategy balances discretization granularity with model capacity, trading off between preserving numerical precision and maintaining manageable vocabulary size.
**Failure Signatures**: Poor tokenization can lead to loss of numerical precision and unrealistic synthetic measurements. Insufficient model capacity may result in mode collapse, where the generator produces limited variation in synthetic samples. Inadequate privacy evaluation could lead to privacy leaks despite good utility scores.
**First Experiments**: 1) Evaluate tokenization quality by comparing distribution statistics between real and synthetic numerical variables, 2) Test model capacity by varying transformer layer depth and width, 3) Assess temporal coherence by measuring how well generated sequences maintain realistic visit patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Single-dataset evaluation on MIMIC-III limits generalizability to other clinical settings and patient populations
- Privacy evaluation focuses on membership inference attacks without comprehensive coverage of reconstruction or differential privacy guarantees
- Tokenizer design for numerical variables may introduce bias in discretization, potentially affecting time series quality

## Confidence
- High confidence: Core technical contribution is well-implemented with convincing ablation studies for fidelity metrics
- Medium confidence: Utility preservation claims are supported but limited to single downstream task
- Low confidence: Generalizability claims to other EHR datasets are not empirically supported

## Next Checks
1. Evaluate SynEHRgy on multiple heterogeneous EHR datasets (eICU, outpatient settings) to assess cross-domain performance and robustness to different data distributions
2. Conduct comprehensive privacy analysis including reconstruction attacks, attribute inference, and formal differential privacy guarantees for clinical deployment standards
3. Test utility preservation across multiple downstream clinical tasks beyond mortality prediction, including diagnosis prediction, length of stay forecasting, and treatment recommendation scenarios