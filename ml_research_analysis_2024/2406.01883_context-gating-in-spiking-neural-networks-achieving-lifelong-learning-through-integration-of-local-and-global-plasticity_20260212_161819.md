---
ver: rpa2
title: 'Context Gating in Spiking Neural Networks: Achieving Lifelong Learning through
  Integration of Local and Global Plasticity'
arxiv_id: '2406.01883'
source_url: https://arxiv.org/abs/2406.01883
tags:
- learning
- task
- network
- neural
- plasticity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in spiking neural networks
  (SNN) during lifelong learning of multiple tasks. It proposes a framework called
  Context Gated SNN (CG-SNN) that integrates local and global plasticity to achieve
  context-dependent gating, inspired by the prefrontal cortex's role in cognitive
  control.
---

# Context Gating in Spiking Neural Networks: Achieving Lifelong Learning through Integration of Local and Global Plasticity

## Quick Facts
- **arXiv ID**: 2406.01883
- **Source URL**: https://arxiv.org/abs/2406.01883
- **Reference count**: 40
- **Primary result**: CG-SNN achieves context-dependent gating in SNNs for lifelong learning, reducing catastrophic forgetting and matching human cognitive control patterns

## Executive Summary
This paper addresses catastrophic forgetting in spiking neural networks (SNN) during lifelong learning of multiple tasks. The authors propose Context Gated SNN (CG-SNN), a framework that integrates local and global plasticity inspired by the prefrontal cortex's role in cognitive control. CG-SNN uses STDP or Hebbian learning for local plasticity to strengthen task-specific connections, while backpropagation maintains generalization. Experiments on a tree classification task demonstrate that CG-SNN outperforms baseline SNNs and aligns with human-like behavior in task selectivity and accuracy.

## Method Summary
The CG-SNN framework integrates local and global plasticity mechanisms to achieve context-dependent gating. The model uses iterative training between global backpropagation (for generalization) and local plasticity rules (STDP for single-spike, Oja's rule for multi-spike architectures). Sluggish neurons model human learning bias by carrying information across samples. The architecture includes input encoding, context gating units, hidden layers with task-selective neurons, and output layers for binary classification. The method is evaluated on a synthetic tree classification task with two tasks: categorizing trees by leaf density and branch density.

## Key Results
- CG-SNN demonstrates accuracy around 90% for the first task and 100% for the second task
- The model shows better resistance to catastrophic forgetting compared to baseline SNNs
- CG-SNN aligns with human cognitive control patterns in both blocked and interleaved learning scenarios
- The framework achieves task selectivity comparable to human behavior data

## Why This Works (Mechanism)

### Mechanism 1
Local STDP plasticity strengthens connections between task neurons and hidden neurons, enabling context-dependent gating. STDP updates weights based on spike timing differences between task neurons and hidden neurons, selectively strengthening task-relevant pathways while weakening task-irrelevant ones. Core assumption: Spike timing differences between task neurons and hidden neurons can effectively distinguish task relevance. Evidence anchors: [abstract], [section]. Break condition: If spike timing differences become too small to distinguish task relevance, STDP cannot effectively gate context.

### Mechanism 2
Global backpropagation plasticity maintains generalization capability while preserving task-specific information. Backpropagation updates all weights to minimize task-specific error while iterative training with local plasticity preserves context information. Core assumption: Global plasticity can maintain task performance while local plasticity handles context-specific gating. Evidence anchors: [abstract], [section]. Break condition: If global plasticity updates overwhelm local plasticity changes, context gating information may be lost.

### Mechanism 3
Context gating reduces interference between tasks by selectively activating task-relevant pathways. Task-specific neurons gate input dimensions, ensuring only relevant features activate for each task, reducing cross-task interference. Core assumption: Selective activation of task-relevant pathways can prevent catastrophic forgetting. Evidence anchors: [abstract], [section]. Break condition: If gating becomes too restrictive, the network may lose ability to generalize across related tasks.

## Foundational Learning

- **Spiking Neural Networks and LIF neuron dynamics**: Understanding SNN spike timing, membrane potential dynamics, and firing mechanisms is essential for implementing context gating. Quick check: How does the LIF neuron model differ from the IF neuron model in terms of spike generation and refractory behavior?

- **Spike-Timing-Dependent Plasticity (STDP)**: STDP is the local plasticity mechanism that enables context-dependent weight updates based on spike timing. Quick check: What are the mathematical formulations for LTP and LTD in STDP, and how do they depend on the timing difference between pre- and postsynaptic spikes?

- **Backpropagation through time for SNNs**: Global plasticity requires gradient-based weight updates, which must account for the temporal dynamics of spiking neurons. Quick check: How is the surrogate gradient function used to handle the non-differentiability of spike firing in backpropagation through time?

## Architecture Onboarding

- **Component map**: Input layer (tree features + task context) → Encoding → Context Gating units → Hidden layer (task-selective neurons) → Output layer (binary classification)

- **Critical path**: Input → Encoding → Context Gating → Hidden Layer → Output, with iterative training between local (STDP/Hebbian) and global (backpropagation) plasticity

- **Design tradeoffs**:
  - Single-spike vs multi-spike architecture: Single-spike simpler but may miss weak signals; multi-spike captures more temporal information but requires more complex training
  - STDP vs Hebbian for local plasticity: STDP more biologically plausible but may be unstable; Hebbian more stable but less biologically accurate
  - Frequency of local vs global updates: More local updates may improve context retention but slow overall training

- **Failure signatures**:
  - Catastrophic forgetting: Accuracy drops sharply on previous tasks after learning new ones
  - Context confusion: Network fails to distinguish between tasks despite context signals
  - Overfitting to context: Network becomes too reliant on context signals and fails to generalize

- **First 3 experiments**:
  1. Train on single task, measure accuracy and neuron selectivity to verify basic functionality
  2. Train on two tasks sequentially without context gating, measure catastrophic forgetting
  3. Train on two tasks with context gating, compare retention of first task vs no-gating baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does the iterative training between global and local plasticity in CG-SNN affect the balance between context learning and generalization learning? The paper mentions that the iterative training is designed to balance context learning and generalization learning, but does not provide detailed analysis or experiments on how this balance is achieved. What evidence would resolve it: Experiments comparing the performance of CG-SNN with different ratios of global and local plasticity iterations, and analysis of how these ratios affect the model's ability to retain old tasks and learn new tasks.

### Open Question 2
How does the introduction of sluggish neurons in CG-SNN model the human's learning bias under interleaved training? The paper mentions that sluggish neurons are used to carry information from previous samples over to the current sample, by applying an exponentially moving average to input information of task units. What evidence would resolve it: Experiments comparing the performance of CG-SNN with and without sluggish neurons under interleaved training, and analysis of how the level of sluggishness affects the model's ability to retain old tasks and learn new tasks.

### Open Question 3
How does the CG-SNN framework compare to other lifelong learning methods in terms of task-selectivity and accuracy? The paper mentions that the proposed CG-SNN model has better task-selectivity and accuracy than other methods during lifelong learning. What evidence would resolve it: Experiments comparing the task-selectivity and accuracy of CG-SNN with other lifelong learning methods, such as regularization-based and memory replay methods, under the same experimental settings.

## Limitations

- The claims about context gating mechanisms rely heavily on a single synthetic tree classification dataset, limiting generalizability to real-world scenarios
- The biological plausibility claims, while inspired by prefrontal cortex function, lack direct neurophysiological validation
- The model's performance improvements over baselines are demonstrated only in controlled experimental settings without comparison to other established lifelong learning methods in SNNs

## Confidence

**High Confidence**: The CG-SNN architecture design and the basic premise that context-dependent gating can reduce interference between tasks are well-supported by the experimental results. The demonstration that context gating improves resistance to catastrophic forgetting on the tree classification task is robust.

**Medium Confidence**: Claims about biological plausibility and alignment with human cognitive control patterns are supported by the experimental design but require more direct validation against neurophysiological data. The superiority of CG-SNN over baseline methods is demonstrated but in a limited experimental setting.

**Low Confidence**: The scalability of the CG-SNN framework to more complex, real-world lifelong learning scenarios is speculative without additional experiments on diverse datasets and task types.

## Next Checks

1. **Generalizability Test**: Evaluate CG-SNN performance on established lifelong learning benchmarks like Split MNIST or Split CIFAR-100 to assess performance beyond the synthetic tree dataset.

2. **Biological Validation**: Conduct neurophysiological experiments to measure prefrontal cortex activity during similar cognitive control tasks, comparing human neural patterns with CG-SNN internal representations.

3. **Computational Efficiency Analysis**: Measure and compare the computational cost (training time, memory usage) of CG-SNN against baseline SNN methods to assess practical deployment feasibility.