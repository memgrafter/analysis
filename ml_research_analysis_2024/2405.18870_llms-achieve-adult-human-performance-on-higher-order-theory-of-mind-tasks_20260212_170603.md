---
ver: rpa2
title: LLMs achieve adult human performance on higher-order theory of mind tasks
arxiv_id: '2405.18870'
source_url: https://arxiv.org/abs/2405.18870
tags:
- performance
- human
- order
- gpt-4
- statements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares large language models (LLMs) on higher-order
  theory of mind (ToM) tasks using a novel benchmark based on the Imposing Memory
  Task. The benchmark includes 7 stories and 20 statements targeting ToM orders 2-6
  and factual recall tasks of equivalent syntactic complexity.
---

# LLMs achieve adult human performance on higher-order theory of mind tasks

## Quick Facts
- arXiv ID: 2405.18870
- Source URL: https://arxiv.org/abs/2405.18870
- Reference count: 24
- Key outcome: GPT-4 and Flan-PaLM achieve adult-level or near-adult-level performance on higher-order theory of mind tasks, with GPT-4 exceeding adult performance on 6th-order inferences

## Executive Summary
This paper evaluates large language models on higher-order theory of mind tasks using a novel benchmark based on the Imposing Memory Task. The study finds that GPT-4 and Flan-PaLM achieve adult-level or near-adult-level performance on ToM tasks overall, with GPT-4 exceeding adult performance on 6th-order inferences. Performance varied significantly across orders for all models, with GPT-4 showing an unusual increase in accuracy from orders 4-6. The results suggest a link between model size, fine-tuning, and ToM abilities, with implications for LLM applications in social interaction contexts.

## Method Summary
The study uses a novel benchmark consisting of 7 short stories (~200 words each) describing social interactions of 3-5 characters, accompanied by 20 true/false statements (10 ToM statements targeting orders 2-6, 10 factual statements of equivalent syntactic complexity). Human benchmark data was gathered via Qualtrics survey with participants reading stories and answering one statement each. LLM performance was evaluated using API log probabilities for candidate tokens ('True', 'False', 'TRUE', 'FALSE', 'true', 'false') to determine responses, which were then compared to human benchmark performance.

## Key Results
- GPT-4 and Flan-PaLM achieved adult-level or near-adult-level performance on ToM tasks overall
- GPT-4 exceeded adult performance on 6th-order inferences
- All models performed better on factual recall tasks than ToM tasks
- Performance varied significantly across ToM orders for all models, with GPT-4 showing unusual accuracy increase from orders 4-6

## Why This Works (Mechanism)

### Mechanism 1
Log probability-based candidate scoring produces more robust ToM performance estimates than single-token selection. The method aggregates probabilities for semantically equivalent tokens (e.g., 'True', 'true', 'TRUE') to reduce sensitivity to tokenization variations and linguistic form. Core assumption: The model's probability distribution for different capitalizations of the same word is consistent and meaningful for intent inference.

### Mechanism 2
Instruction fine-tuning combined with scale enables higher-order ToM performance in LLMs. Flan-PaLM and GPT-4 (larger, fine-tuned models) outperform smaller, non-fine-tuned models like PaLM and LaMDA on ToM tasks. Core assumption: Scale provides general reasoning capacity while instruction tuning specializes it for ToM-like inference patterns.

### Mechanism 3
GPT-4's multimodal pretraining contributes to superior ToM performance. Visual behavioral signals (e.g., raised eyebrows) in stories provide grounding for mental state inference beyond text alone. Core assumption: Multimodal input creates richer representations of social cues than text-only training.

## Foundational Learning

- Log probability aggregation
  - Why needed here: Ensures robust ToM response measurement across tokenization variations
  - Quick check question: Why does summing probabilities for 'True', 'true', and 'TRUE' provide a better estimate than selecting the single highest probability token?

- Theory of Mind hierarchy and orders
  - Why needed here: The benchmark explicitly tests recursive mental state reasoning up to 6th order
  - Quick check question: What distinguishes a 3rd-order ToM statement from a 2nd-order statement in terms of mental state embedding?

- Benchmark contamination awareness
  - Why needed here: The paper explicitly designed novel stories to avoid pretraining overlap
  - Quick check question: How does the paper ensure its ToM benchmark wasn't included in pretraining data?

## Architecture Onboarding

- Component map: LLM API → Logprob extraction → Candidate aggregation → Binary classification → Statistical comparison
- Critical path: Story + statement → API call → Logprob vector → Probability aggregation → Threshold decision → Result storage
- Design tradeoffs: Candidate set size vs. computational cost; aggregation method vs. sensitivity to tokenization; prompt simplicity vs. task clarity
- Failure signatures: Uniform responses (LaMDA's all-true pattern); anchoring effects (PaLM, GPT-3.5 sensitivity to response order); order-dependent performance drops
- First 3 experiments:
  1. Test baseline performance using single-token selection vs. candidate aggregation on identical inputs
  2. Compare instruction-tuned vs. base model performance on identical ToM tasks across orders
  3. Evaluate GPT-4 vs. Flan-PaLM on multimodal vs. text-only story variants

## Open Questions the Paper Calls Out

### Open Question 1
Does LLM performance on higher-order theory of mind tasks translate to real-world social interactions and applications? The study only tests performance on a specific benchmark and does not examine how this translates to actual social scenarios.

### Open Question 2
What is the exact relationship between model size, fine-tuning, and the emergence of higher-order ToM capabilities in LLMs? The study only compares a limited number of models with different architectures and training approaches.

### Open Question 3
Are there inherent differences in how LLMs and humans process higher-order ToM tasks, beyond just performance outcomes? The study only compares performance metrics and does not investigate the underlying cognitive processes.

## Limitations

- Benchmark contamination risk: 5 of 7 stories may have been in pretraining data for some models, though authors argue performance differences across models suggest minimal impact
- Order-specific performance patterns: GPT-4 shows unusual accuracy increase from orders 4-6, which deviates from expected cognitive load patterns
- Cross-cultural validity: Benchmark developed using US-based participants and English language materials; theory of mind reasoning may vary across cultural contexts

## Confidence

**High confidence**: LLMs show measurable ToM capabilities above chance level; factual recall tasks are easier than ToM tasks
**Medium confidence**: GPT-4 achieves or exceeds adult human performance on higher-order ToM tasks; relationship between model size/fine-tuning and ToM performance
**Low confidence**: GPT-4's multimodal pretraining specifically contributes to superior 5th-6th order performance

## Next Checks

1. Independent benchmark replication: Recreate the ToM benchmark with entirely novel stories and statements not present in any public corpus to eliminate contamination concerns
2. Cross-cultural validation: Administer the benchmark to non-Western populations and test models trained on multilingual/multicultural data
3. Ablation study of model components: Test models with systematic variations - same size but different training approaches, same architecture but different scale, and multimodal vs text-only variants