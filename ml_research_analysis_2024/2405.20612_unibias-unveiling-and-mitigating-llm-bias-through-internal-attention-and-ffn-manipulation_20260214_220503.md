---
ver: rpa2
title: 'UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and
  FFN Manipulation'
arxiv_id: '2405.20612'
source_url: https://arxiv.org/abs/2405.20612
tags:
- attention
- label
- bias
- biased
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the internal mechanisms causing bias in
  large language models (LLMs) during in-context learning (ICL). Through mechanistic
  interpretability, the authors analyze how feedforward neural networks (FFNs) and
  attention heads contribute to label prediction bias.
---

# UniBias: Unveiling and Mitigating LLM Bias through Internal Attention and FFN Manipulation

## Quick Facts
- arXiv ID: 2405.20612
- Source URL: https://arxiv.org/abs/2405.20612
- Authors: Hanzhang Zhou; Zijian Feng; Zixiao Zhu; Junlang Qian; Kezhi Mao
- Reference count: 40
- One-line primary result: UniBias achieves 3.39% average accuracy improvement over standard ICL by eliminating biased FFN vectors and attention heads

## Executive Summary
This paper investigates the internal mechanisms causing bias in large language models (LLMs) during in-context learning (ICL). Through mechanistic interpretability, the authors analyze how feedforward neural networks (FFNs) and attention heads contribute to label prediction bias. They identify that specific FFN vectors and attention heads consistently introduce biased influences toward certain labels, regardless of input variations.

To mitigate these biases, the authors propose UniBias, an inference-only method that identifies and eliminates biased FFN vectors and attention heads based on three criteria: relatedness to label prediction, biased distribution favoring certain labels, and low variance across samples. Extensive experiments on 12 NLP datasets demonstrate that UniBias significantly improves ICL performance compared to standard ICL and state-of-the-art calibration methods.

## Method Summary
UniBias is an inference-only method that identifies and eliminates biased FFN vectors and attention heads in LLMs during in-context learning. The method projects FFN vectors and attention head outputs into vocabulary space using logit lens to assess their contributions to label prediction. Biased components are identified based on three criteria: relatedness to label prediction, biased distribution favoring certain labels, and low variance across samples. These components are then masked during inference to mitigate their influence on predictions.

## Key Results
- UniBias achieves an average accuracy of 70.46% on 12 NLP datasets using Llama-2 7B, compared to 67.07% for standard ICL (3.39% improvement)
- The method effectively alleviates prompt brittleness, reducing performance variations under different prompt design settings from 8-26% to less than 4%
- UniBias demonstrates consistent performance improvements across diverse datasets including SST-2, MNLI, WiC, COPA, CR, AGNews, MR, RTE, SST-5, TREC, ARC, and MMLU

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Certain FFN vectors have consistent, context-independent preference for specific label names, creating vanilla label bias.
- Mechanism: FFN value vectors project into vocabulary space via logit lens; their weighted sum produces uncontextual logits favoring certain labels.
- Core assumption: Residual stream view holds—each FFN contributes additively to logits independent of input context.
- Evidence anchors:
  - [abstract] "they identify that specific FFN vectors and attention heads consistently introduce biased influences toward certain labels, regardless of input variations."
  - [section 2.2] "by projecting the FFN value vectors into the vocabulary space, we compute the logits for various label names for each FFN vector."
  - [corpus] weak—no direct citation found, but similar FFN logit lens studies exist.
- Break condition: If FFN contributions are not additive or if residual connections alter contributions based on context.

### Mechanism 2
- Claim: Specific attention heads consistently attend to the last example in the prompt, causing recency bias.
- Mechanism: Attention weights from a biased head disproportionately focus on final example's label tokens, regardless of test sample label.
- Core assumption: Attention outputs are independent and interpretable; logit lens on head output reveals label preference.
- Evidence anchors:
  - [abstract] "they identify that specific FFN vectors and attention heads consistently introduce biased influences toward certain labels, regardless of input variations."
  - [section 2.2] "specific heads consistently prioritize the example at the end of the prompt, providing an internal perspective on the origin of recency bias."
  - [corpus] weak—no exact match, but related work on positional bias exists.
- Break condition: If attention weights shift with context or if other heads dominate final prediction.

### Mechanism 3
- Claim: UniBias can eliminate biased components by masking them, restoring balanced predictions.
- Mechanism: Identify biased FFNs/heads via low-variance label logits across samples, mask them during inference, leaving residual stream unbiased.
- Core assumption: Masking biased components does not disrupt model's general capability on other inputs.
- Evidence anchors:
  - [abstract] "UniBias projects FFN vectors and attention head outputs into the vocabulary space to assess their contributions."
  - [section 3.1-3.3] "we mitigate their impact by masking these biased components."
  - [corpus] weak—no exact prior work on this masking strategy.
- Break condition: If masking causes catastrophic forgetting or if residual stream compensation fails.

## Foundational Learning

- Concept: Transformer residual stream interpretation (Elhage et al. 2021)
  - Why needed here: Understanding that each layer's output adds to hidden state lets us isolate contributions of individual FFNs and attention heads.
  - Quick check question: If a layer's output is h_out = h_in + Δ, what does Δ represent in terms of model contribution?

- Concept: Logit lens (Nostalgebraist 2020)
  - Why needed here: Projects hidden states or FFN vectors into vocabulary space to interpret semantic content without running full forward pass.
  - Quick check question: Given embedding matrix E and hidden state h, how do you compute vocabulary logits?

- Concept: Bias detection via coefficient variance (Geva et al. 2021)
  - Why needed here: Low variance across samples indicates component is not adapting to input, revealing bias.
  - Quick check question: If CV(m) = σ(m)/µ(m) is near zero, what does that say about m's behavior across samples?

## Architecture Onboarding

- Component map: Input -> Tokenizer -> Llama-2 transformer layers (Multi-head Attention + FFN) -> Output logits
- Critical path: Token embedding -> attention+FFN forward pass -> logit lens projection -> bias identification (threshold grid search) -> mask application -> inference
- Design tradeoffs: Masking reduces bias but may slightly hurt general capability; threshold tuning balances sensitivity vs false positives; support set size affects accuracy of bias detection
- Failure signatures: Over-masking -> performance drop; under-masking -> bias persists; threshold mis-tuning -> noisy bias detection; high variance in CV metric -> unstable identification
- First 3 experiments:
  1. Run UniBias on SST-2 with 1-shot ICL, record accuracy vs baseline
  2. Vary support set size (5, 10, 20, 50 samples) and measure bias mitigation consistency
  3. Compare masking only FFNs vs only attention heads vs both on a subset of datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the identification of biased FFN vectors and attention heads in LLMs vary across different tasks and datasets, and what are the underlying mechanisms that cause these variations?
- Basis in paper: [explicit] The paper mentions that different datasets with similar tasks could share common biased LLM components, and that there may be global biased components that mitigate bias across multiple tasks and diverse prompt design settings.
- Why unresolved: The paper does not provide a detailed analysis of how the identification of biased components varies across different tasks and datasets, nor does it explore the underlying mechanisms that cause these variations.
- What evidence would resolve it: Conducting experiments to identify biased components across a wide range of tasks and datasets, and analyzing the patterns and similarities in these biased components to uncover the underlying mechanisms that cause variations.

### Open Question 2
- Question: What are the potential applications of using biased FFN vectors and attention heads as sensors for guiding effective prompt generation, and how can this approach improve the performance and robustness of LLMs in various tasks?
- Basis in paper: [inferred] The paper suggests that the biased FFN vectors and attention heads identified could potentially serve as sensors for guiding effective prompt generation, but it does not explore this application in detail.
- Why unresolved: The paper does not provide concrete examples or experiments to demonstrate the potential applications of using biased components as sensors for prompt generation, nor does it discuss how this approach can improve LLM performance and robustness.
- What evidence would resolve it: Conducting experiments to use biased components as sensors for prompt generation, and evaluating the impact on LLM performance and robustness across various tasks and datasets.

### Open Question 3
- Question: How can the UniBias method be further improved to reduce its reliance on grid searching with a small set of labeled training samples, and what are the potential trade-offs between efficiency and effectiveness in this context?
- Basis in paper: [explicit] The paper acknowledges that the UniBias method relies on grid searching with a small set of labeled training samples, and suggests that future research could focus on reducing this reliance.
- Why unresolved: The paper does not provide specific strategies or techniques to improve the UniBias method in terms of reducing its reliance on grid searching, nor does it discuss the potential trade-offs between efficiency and effectiveness.
- What evidence would resolve it: Developing and testing new techniques to improve the UniBias method, such as using fewer labeled samples or incorporating unsupervised learning methods, and evaluating the trade-offs between efficiency and effectiveness in various scenarios.

## Limitations

- The method's effectiveness depends on the interpretability of residual stream contributions from individual FFN vectors and attention heads, which may not hold for all model architectures or tasks.
- The grid search for threshold values in identifying biased components is not specified in detail, potentially introducing sensitivity to implementation choices.
- The paper does not thoroughly investigate the potential negative effects of masking components on the model's general capabilities or performance on other tasks.

## Confidence

- **High Confidence**: The observation that certain FFN vectors and attention heads consistently introduce biased influences toward specific labels across various input samples. This is directly supported by empirical analysis in the paper.
- **Medium Confidence**: The effectiveness of UniBias in improving ICL performance and reducing prompt brittleness. While results are promising, the lack of detailed threshold specification and potential sensitivity to implementation details introduce some uncertainty.
- **Low Confidence**: The generalizability of the method to a broader range of tasks and models beyond those tested. The specific biases addressed may not be representative of all potential biases in LLMs.

## Next Checks

1. **Threshold Sensitivity Analysis**: Perform a systematic grid search over different threshold values to determine the stability and robustness of UniBias's bias identification. Analyze how varying thresholds affect both bias mitigation and overall performance.

2. **Cross-Model Evaluation**: Apply UniBias to other LLM architectures (e.g., GPT models, other transformer variants) and diverse datasets to assess generalizability. Compare the effectiveness of bias mitigation across different model sizes and types.

3. **Component Interaction Study**: Investigate how masking biased FFN vectors and attention heads affects the interactions between remaining components. Use techniques like ablation studies to understand if residual stream compensation occurs and how it impacts model behavior on non-biased tasks.