---
ver: rpa2
title: Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only
  Training
arxiv_id: '2401.02347'
source_url: https://arxiv.org/abs/2401.02347
tags:
- image
- text
- clip
- captioning
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the modality gap in CLIP's embedding space,
  finding that local image subregions have closer proximity to paired captions than
  global image features, and that the modality gap follows a zero-mean Gaussian distribution.
  To address this, the authors propose MacCap, a zero-shot image captioning framework
  that uses text-only training.
---

# Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training

## Quick Facts
- arXiv ID: 2401.02347
- Source URL: https://arxiv.org/abs/2401.02347
- Authors: Longtian Qiu; Shan Ning; Xuming He
- Reference count: 22
- Primary result: State-of-the-art zero-shot image captioning on MSCOCO and Flickr30k via text-only training

## Executive Summary
This paper analyzes the modality gap in CLIP's embedding space, finding that local image subregions have closer proximity to paired captions than global image features, and that the modality gap follows a zero-mean Gaussian distribution. To address this, the authors propose MacCap, a zero-shot image captioning framework that uses text-only training. MacCap employs a subregion feature aggregation to integrate global and local image information, creating a tighter alignment with text embeddings. It also incorporates noise injection and CLIP reranking to boost performance. The method achieves state-of-the-art results on image captioning benchmarks like MSCOCO and Flickr30k, and extends to zero-shot visual question answering.

## Method Summary
The paper proposes MacCap, a zero-shot image captioning framework that leverages text-only training to address the modality gap in CLIP embeddings. MacCap uses subregion feature aggregation to combine global and local image features, creating a tighter alignment with text embeddings. It also incorporates noise injection and CLIP reranking to enhance performance. The approach achieves state-of-the-art results on MSCOCO and Flickr30k, and demonstrates effectiveness on zero-shot VQA.

## Key Results
- Achieves state-of-the-art zero-shot image captioning results on MSCOCO and Flickr30k
- Demonstrates effectiveness on zero-shot visual question answering
- Shows that local image subregions align more closely with captions than global features

## Why This Works (Mechanism)
The method addresses the modality gap in CLIP embeddings by leveraging local image subregions, which align more closely with captions than global features. Text-only training allows the model to learn alignment without requiring paired image-text data. Subregion feature aggregation, noise injection, and CLIP reranking further enhance performance by integrating global and local information and improving robustness.

## Foundational Learning
- **Modality Gap**: The discrepancy between image and text embeddings in CLIP's space. Why needed: Understanding this gap is crucial for improving cross-modal alignment. Quick check: Analyze the distribution of distances between image and text embeddings.
- **Subregion Feature Aggregation**: Combining local and global image features to create a tighter alignment with text embeddings. Why needed: Local features better capture fine-grained details relevant to captions. Quick check: Compare performance with and without subregion aggregation.
- **Zero-Shot Learning**: Training a model to perform tasks without task-specific labeled data. Why needed: Enables generalization to new tasks without additional training data. Quick check: Evaluate performance on unseen datasets.

## Architecture Onboarding
- **Component Map**: Image Encoder (CLIP) -> Subregion Feature Aggregation -> Text Encoder (CLIP) -> Caption Generation -> CLIP Reranking
- **Critical Path**: Image encoding and subregion aggregation are critical for aligning local features with text embeddings. Text encoding and caption generation rely on this alignment.
- **Design Tradeoffs**: Using CLIP's pre-trained embeddings without finetuning limits adaptation to domain-specific tasks but enables zero-shot generalization.
- **Failure Signatures**: Poor performance on non-photographic imagery (e.g., medical or remote sensing) due to reliance on CLIP's original representations.
- **First Experiments**:
  1. Evaluate MacCap's performance on non-photographic datasets (e.g., medical, remote sensing).
  2. Conduct ablation studies to isolate the contributions of subregion aggregation, noise injection, and CLIP reranking.
  3. Test the robustness of the method to varying text prompt quality and domain-specific vocabulary.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on CLIP's pre-trained embeddings without finetuning may restrict adaptation to domain-specific tasks.
- Performance on non-photographic imagery (e.g., medical or remote sensing) is untested.
- The zero-mean Gaussian assumption for the modality gap is not theoretically justified.

## Confidence
- **High**: Analysis of the modality gap and local vs. global feature alignment; empirical improvements on established captioning benchmarks.
- **Medium**: Generalization to other vision-language tasks and domains; robustness to dataset biases.
- **Low**: Theoretical justification of the Gaussian gap assumption; adaptation potential beyond CLIP's representations.

## Next Checks
1. Evaluate MacCap's performance on non-photographic datasets (e.g., medical, remote sensing) to test domain generalization.
2. Conduct ablation studies to isolate the contributions of subregion aggregation, noise injection, and CLIP reranking.
3. Test the robustness of the method to varying text prompt quality and domain-specific vocabulary.