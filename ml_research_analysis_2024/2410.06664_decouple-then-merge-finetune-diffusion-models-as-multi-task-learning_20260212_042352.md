---
ver: rpa2
title: 'Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning'
arxiv_id: '2410.06664'
source_url: https://arxiv.org/abs/2410.06664
tags:
- diffusion
- finetuning
- training
- timesteps
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of gradient conflicts in diffusion
  model training across different timesteps, which can degrade image generation performance.
  The authors propose a Decouple-then-Merge (DeMe) framework that first finetunes
  separate diffusion models for non-overlapping timestep ranges to avoid negative
  interference, then merges these specialized models into a single model in parameter
  space.
---

# Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning

## Quick Facts
- arXiv ID: 2410.06664
- Source URL: https://arxiv.org/abs/2410.06664
- Authors: Qianli Ma; Xuefei Ning; Dongrui Liu; Li Niu; Linfeng Zhang
- Reference count: 40
- Key outcome: FID reductions of 0.91 on CIFAR10, 3.42 on LSUN-Church, and 0.62 on LSUN-Bedroom compared to standard DDPM, without additional inference costs

## Executive Summary
This paper addresses the issue of gradient conflicts in diffusion model training across different timesteps, which can degrade image generation performance. The authors propose a Decouple-then-Merge (DeMe) framework that first finetunes separate diffusion models for non-overlapping timestep ranges to avoid negative interference, then merges these specialized models into a single model in parameter space. During finetuning, they introduce three techniques: channel-wise projection to capture timestep-specific features, consistency loss to preserve original knowledge, and probabilistic sampling to balance learning across timesteps. The merged model achieves significant improvements in generation quality without additional inference costs. Experimental results show FID reductions of 0.91 on CIFAR10, 3.42 on LSUN-Church, and 0.62 on LSUN-Bedroom compared to standard DDPM, and similar improvements on text-to-image tasks using Stable Diffusion.

## Method Summary
The Decouple-then-Merge (DeMe) framework addresses gradient conflicts in diffusion models by first dividing the timestep range [0, T) into N non-overlapping ranges and finetuning separate models for each range. During finetuning, three techniques are employed: channel-wise projection to capture timestep-specific features, consistency loss to preserve original knowledge from the pretrained model, and probabilistic sampling to balance learning across timesteps. After finetuning, the specialized models are merged into a single model using model merging with optimized coefficients found through grid search. This approach effectively mitigates gradient conflicts while preserving the benefits of knowledge sharing across timesteps, resulting in improved generation quality without additional inference costs.

## Key Results
- FID reductions of 0.91 on CIFAR10, 3.42 on LSUN-Church, and 0.62 on LSUN-Bedroom compared to standard DDPM
- Significant improvements on text-to-image tasks using Stable Diffusion on LAION-Aesthetics V2 dataset
- Optimal performance achieved with N=4 partitions, showing diminishing returns beyond this point
- Channel-wise projection, consistency loss, and probabilistic sampling each contribute to performance improvements

## Why This Works (Mechanism)
The method works by recognizing that gradient conflicts occur when training diffusion models across all timesteps simultaneously, as different timesteps require different types of denoising knowledge. By decoupling the training into specialized models for non-overlapping timestep ranges, each model can focus on learning the specific denoising patterns for its assigned timesteps without interference. The channel-wise projection layer allows each specialized model to develop timestep-specific feature representations. The consistency loss ensures that while each model specializes, it doesn't forget the broader knowledge from the pretrained model. Finally, the probabilistic sampling strategy balances the learning process, preventing any single timestep from dominating the training. The model merging step combines these specialized capabilities into a single, more capable model.

## Foundational Learning

**Diffusion Models**: Why needed - Core technology being improved; Quick check - Understanding forward and reverse processes, noise schedule, and denoising objective

**Model Merging**: Why needed - Essential technique for combining specialized models; Quick check - Understanding how parameters from different models can be linearly combined

**Gradient Conflicts**: Why needed - The fundamental problem being solved; Quick check - Recognizing how simultaneous optimization across different tasks can lead to suboptimal solutions

**FID Score**: Why needed - Primary evaluation metric; Quick check - Understanding Fréchet Inception Distance and its interpretation for image generation quality

## Architecture Onboarding

**Component Map**: Pretrained diffusion model -> [DeMe Pipeline: Partition timesteps -> Finetune specialized models (with channel-wise projection, consistency loss, probabilistic sampling) -> Merge models] -> Improved generation model

**Critical Path**: Timestep partitioning → Specialized finetuning (with all three techniques) → Model merging → Performance evaluation

**Design Tradeoffs**: Number of partitions (N) vs. computational overhead; probabilistic sampling rate (p) vs. knowledge sharing; model merging complexity vs. performance gains

**Failure Signatures**: Poor performance with too few partitions (insufficient decoupling) or too many (excessive overhead); suboptimal results with incorrect p values limiting knowledge sharing

**First Experiments**: 1) Test different N values (2, 4, 6, 8) on CIFAR10 to find optimal partition count; 2) Vary p values (0.1, 0.3, 0.5, 0.7) to optimize knowledge sharing; 3) Compare DeMe with baseline finetuning across multiple datasets

## Open Questions the Paper Calls Out

**Open Question 1**: How does the optimal value of N (number of timestep ranges) scale with different diffusion model architectures and training datasets? The paper only tests N values up to 7 on CIFAR10 with one model size, leaving the relationship between N, model capacity, and dataset complexity unexplored.

**Open Question 2**: What is the theoretical relationship between the orthogonality of task vectors and the effectiveness of the model merging process? The paper observes task vectors are "generally close to orthogonal" but provides no theoretical justification for why orthogonality is beneficial or how it relates to merging performance.

**Open Question 3**: How does the probabilistic sampling strategy (parameter p) interact with different diffusion model loss functions and prediction targets? The paper shows p affects performance but only tests with standard diffusion loss, without exploring whether optimal p values should be loss-function specific.

## Limitations

- Specific implementation details of the channel-wise projection layer are not fully specified, affecting reproducibility
- Optimal hyperparameters (N and p) appear task-specific and may require extensive tuning
- Model merging relies on computationally expensive grid search without detailed specifications
- Limited exploration of how results scale across different model sizes and dataset complexities

## Confidence

- **High confidence**: The core methodology of decoupling-then-merging diffusion models addresses a well-identified problem
- **Medium confidence**: The reported FID improvements and effectiveness of the three proposed techniques
- **Low confidence**: Specific implementation details of the merging algorithm and exact hyperparameters used

## Next Checks

1. **Implement channel-wise projection layer**: Verify exact implementation details, including initialization and integration with diffusion model architecture

2. **Grid search validation**: Replicate the grid search procedure for finding optimal merging coefficients, documenting search space and computational requirements

3. **Hyperparameter sensitivity analysis**: Conduct experiments varying N and p values to establish their impact on performance and identify optimal ranges for different tasks