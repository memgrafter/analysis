---
ver: rpa2
title: 'MMDS: A Multimodal Medical Diagnosis System Integrating Image Analysis and
  Knowledge-based Departmental Consultation'
arxiv_id: '2410.15403'
source_url: https://arxiv.org/abs/2410.15403
tags:
- medical
- facial
- large
- accuracy
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMDS integrates multimodal medical image and video analysis with
  department-specific knowledge base routing to enhance diagnostic accuracy. The system
  uses a specialized multimodal model achieving 72.59% accuracy on facial emotion
  recognition (FER2013), 93% accuracy in facial paralysis recognition (30% higher
  than GPT-4o), and 83.3% grading accuracy for facial paralysis videos.
---

# MMDS: A Multimodal Medical Diagnosis System Integrating Image Analysis and Knowledge-based Departmental Consultation

## Quick Facts
- **arXiv ID**: 2410.15403
- **Source URL**: https://arxiv.org/abs/2410.15403
- **Reference count**: 0
- **Primary result**: 72.59% accuracy on FER2013 emotion recognition; 93% facial paralysis recognition (30% higher than GPT-4o); 83.3% grading accuracy for facial paralysis videos

## Executive Summary
This paper introduces MMDS (Multimodal Medical Diagnosis System), a comprehensive medical diagnosis platform that integrates multimodal image and video analysis with a department-specific knowledge base routing mechanism. The system combines medical image parsing, facial emotion recognition, and facial paralysis analysis with a large language model to generate professional diagnoses. The core innovation lies in the department-specific knowledge base routing management mechanism, which categorizes queries by medical department and retrieves information only from the corresponding departmental knowledge base, significantly improving retrieval accuracy in RAG processes. The system achieves state-of-the-art performance on facial paralysis recognition and demonstrates strong generalization capabilities across multiple medical diagnostic tasks.

## Method Summary
MMDS is built on a fine-tuned multimodal medical model (InternLM-XComposer2-VL with LoRA) trained on a combination of medical image datasets (LLaVA-Med, XrayGLM), facial datasets, and FER2013 emotion recognition data. The model is trained for 6 epochs with batch size 8 and learning rate 5e-5 on A100 GPUs using fp16 precision. A novel department-specific knowledge base routing mechanism categorizes queries by medical department and retrieves information only from the corresponding departmental knowledge base during the RAG process. The system also includes a medical video parser that analyzes facial movement videos for facial paralysis severity grading, extracting frames at 1-2 fps and integrating multimodal analysis with external data to generate comprehensive medical reports.

## Key Results
- 72.59% accuracy on FER2013 facial emotion recognition dataset
- 93% accuracy in facial paralysis recognition, 30% higher than GPT-4o
- 83.3% grading accuracy for facial paralysis videos (tested on 30 videos)
- 84.41% accuracy on MedQA Chinese evaluation dataset with 7B-level model

## Why This Works (Mechanism)

### Mechanism 1: Department-specific Knowledge Base Routing
The large language model categorizes queries by medical department and retrieves information only from the corresponding departmental knowledge base, reducing semantic overlap and improving relevance. This filtering mechanism significantly improves retrieval accuracy in RAG processes. The core assumption is that medical queries and knowledge base data can be accurately classified into distinct departments with minimal semantic overlap.

### Mechanism 2: Specialized Multimodal Medical Model
The model is fine-tuned on specialized datasets including medical images, facial emotion recognition data, and dedicated facial paralysis datasets. This training approach enables the model to recognize both normal facial expressions and pathological conditions, achieving 30% higher accuracy than GPT-4o in facial paralysis recognition. The core assumption is that training data from multiple sources provides complementary information that improves overall performance.

### Mechanism 3: Medical Video Parser for Facial Paralysis Grading
The system extracts frames at 1-2 frames per second, analyzes each frame using the multimodal medical model, and integrates results with external data (historical information, user queries) to generate comprehensive assessments. The core assumption is that facial paralysis severity can be accurately assessed by analyzing multiple frames over time, capturing dynamic facial movements.

## Foundational Learning

- **Multimodal learning and fusion**: Why needed - The system integrates information from medical images, videos, and text to make comprehensive diagnoses. Quick check - How does the system combine visual and textual information during the analysis process?
- **Knowledge base retrieval and routing**: Why needed - The system uses a department-specific knowledge base to provide accurate medical information based on the query context. Quick check - What determines which department's knowledge base is queried during the retrieval process?
- **Facial paralysis grading systems**: Why needed - The system needs to understand the House-Brackmann scale and how to assess facial paralysis severity from visual data. Quick check - What are the key differences between House-Brackmann grades I and VI?

## Architecture Onboarding

- **Component map**: User Input -> Image/Video Analysis -> Department Classification -> Knowledge Base Retrieval -> Response Generation
- **Critical path**: User input → Image/Video Analysis → Department Classification → Knowledge Base Retrieval → Response Generation
- **Design tradeoffs**: Specialized training vs. general-purpose models; accuracy vs. computational cost; comprehensive analysis vs. response time
- **Failure signatures**: Incorrect department classification leads to irrelevant knowledge base retrieval; poor video quality affects grading accuracy; ambiguous facial expressions confuse emotion recognition
- **First 3 experiments**:
  1. Test the multimodal medical model on FER2013 dataset to verify emotion recognition accuracy
  2. Test facial paralysis recognition on the constructed evaluation dataset to verify the 92% accuracy claim
  3. Test the knowledge base routing mechanism on MedQA dataset to verify the 4 percentage point improvement in accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MMDS's facial paralysis recognition and grading change with varying video quality, lighting conditions, and camera angles? The paper mentions testing on 30 facial paralysis videos but does not discuss performance variations under different recording conditions.

### Open Question 2
How does MMDS's multimodal medical model compare to other specialized medical models in terms of accuracy and computational efficiency when analyzing different types of medical images (e.g., X-rays, MRIs, CT scans)? The paper describes MMDS's multimodal model but does not provide comparative analysis with other specialized medical image analysis models.

### Open Question 3
How does the department-specific knowledge base routing mechanism in MMDS scale and perform when dealing with a larger number of medical departments and more complex, multi-department cases? The paper introduces a novel department-specific knowledge base routing mechanism but does not explore its performance in more complex, multi-department scenarios.

## Limitations

- Small test set size (only 30 facial paralysis videos) may not represent real-world diversity
- Limited comparative analysis with other specialized medical image analysis models
- No detailed discussion of demographic biases in training data or real-world clinical validation

## Confidence

- Multimodal model accuracy claims (72.59% FER2013, 93% facial paralysis recognition): Medium confidence
- Knowledge base routing improvement (4 percentage point MedQA gain): Medium confidence
- Video parser grading accuracy (83.3% on 30 videos): Low confidence
- Overall system effectiveness: Medium confidence

## Next Checks

1. **Independent replication on expanded test set**: Re-evaluate the multimodal medical model on a larger, more diverse dataset of at least 100 facial paralysis videos from multiple medical institutions to verify the 93% accuracy claim and assess generalizability across different demographics and lighting conditions.

2. **Ablation study for knowledge base routing**: Conduct controlled experiments comparing the department-specific routing mechanism against alternative retrieval strategies (e.g., flat retrieval, hierarchical routing) using the same MedQA benchmark to isolate the contribution of the routing mechanism to the observed performance improvements.

3. **Real-world clinical pilot study**: Deploy MMDS in a controlled clinical setting with actual patients presenting with facial paralysis and other conditions, comparing diagnostic accuracy against established clinical standards and measuring both inter-rater reliability and agreement with specialist diagnoses across a representative sample of cases.