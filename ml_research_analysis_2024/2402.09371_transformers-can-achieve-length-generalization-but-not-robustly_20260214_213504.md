---
ver: rpa2
title: Transformers Can Achieve Length Generalization But Not Robustly
arxiv_id: '2402.09371'
source_url: https://arxiv.org/abs/2402.09371
tags:
- length
- accuracy
- generalization
- digit
- fire
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that Transformers can achieve length generalization
  on the decimal addition task by combining the right position encoding (FIRE) with
  data formatting techniques (reversed format, index hints, randomized positions).
  The resulting model can extrapolate to sequences 2.5x longer than training length
  (100 digits when trained on 40), achieving over 98% accuracy.
---

# Transformers Can Achieve Length Generalization But Not Robustly

## Quick Facts
- **arXiv ID**: 2402.09371
- **Source URL**: https://arxiv.org/abs/2402.09371
- **Reference count**: 40
- **Primary result**: Transformers can achieve length generalization on decimal addition using FIRE position encoding and specific data formatting, but results are highly sensitive to initialization and training order

## Executive Summary
This paper investigates whether Transformers can generalize to longer sequences than those seen during training, focusing on the decimal addition task. The authors demonstrate that by combining FIRE position encoding with specific data formatting techniques (reversed format, index hints, randomized positions), Transformers can successfully extrapolate to sequences 2.5x longer than training length. However, this generalization capability is surprisingly fragile - performance varies dramatically based on random initialization and training data order, revealing that success depends critically on the synergy between position encoding and data format rather than just model scale or regularization.

## Method Summary
The authors test Transformers on decimal addition with varying position encodings and data formats. They train models on sequences up to 40 digits and evaluate on 100-digit sequences. The key innovation is combining FIRE (Fourier) position encoding with data formatting techniques including reversed input order (smallest digits first), index hints (explicit position markers), and randomized position assignments during training. The study systematically ablates each component to identify what enables successful length generalization, finding that the combination of FIRE encoding with appropriate formatting is essential for achieving high accuracy on longer sequences.

## Key Results
- FIRE position encoding combined with reversed format and index hints enables 98%+ accuracy on 100-digit addition when trained only on 40-digit sequences
- Length generalization is highly sensitive to random weight initialization, with different seeds producing vastly different performance
- The order of training data significantly impacts generalization ability, with certain orderings leading to successful extrapolation while others fail completely
- Models with standard sinusoidal or learned position encodings fail to generalize, highlighting the importance of the specific FIRE encoding design

## Why This Works (Mechanism)
The success mechanism relies on the Fourier-based structure of FIRE position encoding creating frequency patterns that can be extrapolated beyond training lengths when combined with data formats that make positional relationships explicit. The reversed format (processing numbers from least significant digit first) reduces the complexity of carry propagation across long distances. Index hints provide explicit positional information that helps the model learn length-independent arithmetic patterns. Randomized positions during training force the model to learn position-robust representations rather than memorizing specific alignments. Together, these create a representation space where arithmetic operations can be performed consistently regardless of sequence length.

## Foundational Learning
- **Position encoding importance**: Why needed - Transformers lack inherent sequence awareness; quick check - try training without any position encoding
- **Fourier vs sinusoidal encoding**: Why needed - Fourier basis functions have better extrapolation properties; quick check - compare performance of Fourier vs sinusoidal on extrapolation tasks
- **Data format impact**: Why needed - input representation significantly affects model's ability to learn generalizable patterns; quick check - test different input formats (normal vs reversed) on same model
- **Carry propagation in arithmetic**: Why needed - long addition requires managing dependencies across sequence positions; quick check - analyze attention patterns for carry handling
- **Extrapolation vs interpolation**: Why needed - generalizing beyond training distribution requires different learning dynamics; quick check - test performance on intermediate lengths between training and test
- **Random seed sensitivity**: Why needed - optimization landscape may have multiple local minima with vastly different generalization properties; quick check - run multiple seeds and analyze successful vs unsuccessful patterns

## Architecture Onboarding

### Component Map
Transformer -> Position Encoding (FIRE) -> Data Formatting (reversed/index hints/randomized) -> Length Generalization

### Critical Path
1. Input encoding with FIRE position encoding
2. Data formatting applied to training examples
3. Model training with specific data ordering
4. Evaluation on longer sequences

### Design Tradeoffs
- FIRE encoding provides better extrapolation but may be more computationally expensive than sinusoidal
- Reversed format simplifies carry propagation but deviates from standard numerical notation
- Index hints improve generalization but add explicit positional bias
- Randomization during training improves robustness but may slow convergence

### Failure Signatures
- Standard sinusoidal encodings consistently fail at extrapolation
- Models without data formatting show catastrophic performance drop on longer sequences
- Certain random seeds produce models that memorize training patterns without generalization
- Normal (most-significant-digit-first) format without hints leads to poor carry handling

### First Experiments
1. Train identical models with FIRE vs sinusoidal encoding on reversed format data
2. Compare reversed format vs normal format with and without index hints
3. Run 10 different random seeds with identical hyperparameters to quantify variability

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions in the provided content.

## Limitations
- Findings are specific to decimal addition and may not generalize to other algorithmic tasks like multiplication or sorting
- Evaluation is limited to a single extrapolation factor (2.5x) without systematic exploration of boundaries
- The paper doesn't investigate why certain initializations succeed while others fail, or whether success patterns can be predicted
- Analysis focuses on aggregate accuracy without detailed examination of failure modes or error patterns

## Confidence
- **High confidence**: Empirical results showing FIRE + formatting achieves length generalization on decimal addition
- **Medium confidence**: Generalizability of techniques to other arithmetic or algorithmic tasks
- **Low confidence**: Ability to consistently reproduce successful results across different experimental conditions

## Next Checks
1. Test FIRE encoding and formatting techniques on multiplication and subtraction tasks to verify generalization beyond addition
2. Systematically vary extrapolation ratios (training on 10 digits, testing on 25; training on 50, testing on 125) to map successful generalization boundaries
3. Analyze successful vs unsuccessful runs to identify initialization patterns or architectural choices correlating with robust generalization performance