---
ver: rpa2
title: 'ActFusion: a Unified Diffusion Model for Action Segmentation and Anticipation'
arxiv_id: '2412.04353'
source_url: https://arxiv.org/abs/2412.04353
tags:
- action
- segmentation
- mask
- anticipation
- masking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ActFusion, a unified diffusion model for
  joint temporal action segmentation and long-term action anticipation. The key innovation
  is an anticipative masking strategy that treats visible video frames as conditioning
  for segmentation and masked (future) frames as tokens for anticipation.
---

# ActFusion: a Unified Diffusion Model for Action Segmentation and Anticipation

## Quick Facts
- **arXiv ID:** 2412.04353
- **Source URL:** https://arxiv.org/abs/2412.04353
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art performance on both temporal action segmentation and long-term action anticipation using a single unified diffusion model.

## Executive Summary
This paper introduces ActFusion, a unified diffusion model that jointly performs temporal action segmentation (TAS) and long-term action anticipation (LTA) for video understanding. The key innovation is an anticipative masking strategy that treats visible video frames as conditioning for segmentation and masked (future) frames as tokens for anticipation. By training on both observed and masked sequences, the model learns to denoise action labels while handling visible and invisible parts simultaneously. Extensive experiments on 50 Salads, Breakfast, and GTEA datasets show that ActFusion achieves state-of-the-art performance on both tasks with a single model, outperforming task-specific approaches and demonstrating bi-directional benefits between segmentation and anticipation.

## Method Summary
ActFusion is built on a diffusion model architecture that uses an encoder-decoder structure based on ASFormer. The model takes pre-extracted I3D video features and applies different masking strategies during training: anticipative masking (masking future frames), random masking (masking random clips), boundary masking (masking boundary frames), and relation masking (masking related frames). The encoder processes the masked video features, while the decoder performs iterative denoising to predict frame-wise action labels. The model is trained with multiple losses including cross-entropy, boundary smoothing, and boundary alignment. During inference, no masking is used for segmentation while anticipative masking is used for anticipation, allowing the same model to perform both tasks effectively.

## Key Results
- ActFusion achieves state-of-the-art performance on both TAS and LTA tasks across 50 Salads, Breakfast, and GTEA datasets.
- The unified model outperforms task-specific approaches on both segmentation and anticipation tasks.
- Joint learning provides bi-directional benefits: segmentation helps anticipation and anticipation helps segmentation.
- Anticipative masking is critical for the unified approach, significantly outperforming random masking and other strategies.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Anticipative masking enables the model to jointly learn both action segmentation and anticipation by treating visible frames as conditioning and masked (future) frames as tokens for prediction.
- **Mechanism:** During training, a late portion of video frames is masked and replaced with learnable tokens. The model learns to denoise action labels conditioned on both visual features and these mask tokens. During inference, no masking is used for segmentation while anticipative masking is used for anticipation.
- **Core assumption:** The model can effectively learn temporal action relations by simultaneously classifying visible tokens and predicting future actions of masked tokens through the diffusion process.
- **Evidence anchors:**
  - [abstract] "The key idea to unification is to train the model to effectively handle both visible and invisible parts of the sequence in an integrated manner; the visible part is for temporal segmentation, and the invisible part represents future anticipation."
  - [section] "The key to our unified approach lies in training the model to effectively process both visible and invisible parts of the sequence, where the visible part corresponds to observed video frames and the invisible part represents future frames to be anticipated."

### Mechanism 2
- **Claim:** Joint learning of segmentation and anticipation provides bi-directional benefits - segmentation helps anticipation and anticipation helps segmentation.
- **Mechanism:** The encoder loss on observed frames (Lenc) provides strong supervision for visible parts, which improves the model's ability to anticipate future actions. The anticipative masking strategy (MA) during training helps the model learn better temporal context that improves segmentation performance.
- **Core assumption:** Learning to segment actions from visible frames provides useful inductive bias for anticipating future actions, and learning to anticipate future actions provides useful temporal context for segmenting current actions.
- **Evidence anchors:**
  - [section] "Table 3 shows the results. Specifically, we exclude the encoder loss Lenc in Eq. 11, while omitting the decoder loss Ldec in Eq. 12 on the observed frames... By comparing (1) and (3) in Table 3, we observe a significant performance drop when removing the loss on the observed frames."
  - [section] "Comparing (1) and (2) in Table 4a, we find that using anticipation masking helps improve the performance of TAS."

### Mechanism 3
- **Claim:** Random masking improves robustness against visual ambiguities by forcing the model to learn reliable action recognition even when some parts of the video are missing or unclear.
- **Mechanism:** During training, random clips of the video are masked out, requiring the model to learn to recognize actions based on partial information. This makes the model more robust when dealing with ambiguous or incomplete visual inputs.
- **Core assumption:** The model can learn to infer missing information from context when random portions of the video are masked, making it more robust to real-world scenarios where video quality may be imperfect.
- **Evidence anchors:**
  - [section] "Random masking aims to provide robustness in prediction when parts of video frames are missing or ambiguous [59]."
  - [section] "Random masking M R significantly reduces performance in both TAS and LTA, as shown in row (3) of Table 4."

## Foundational Learning

- **Concept:** Diffusion models and denoising processes
  - Why needed here: ActFusion is built on diffusion models that learn to generate action sequences from Gaussian noise through iterative denoising. Understanding the forward and reverse diffusion processes is crucial for implementing the model.
  - Quick check question: What is the difference between the forward diffusion process (adding noise) and the reverse diffusion process (denoising) in diffusion models?

- **Concept:** Temporal action segmentation and long-term action anticipation
  - Why needed here: The paper addresses two related but distinct tasks - segmenting observed actions and anticipating future actions. Understanding the problem setup and evaluation metrics for both tasks is essential.
  - Quick check question: How do temporal action segmentation and long-term action anticipation differ in terms of input requirements and output predictions?

- **Concept:** Masking strategies in self-supervised learning
  - Why needed here: ActFusion uses multiple masking strategies (anticipative, random, boundary, relation) to train the model effectively. Understanding how different masking strategies affect learning is important for implementation.
  - Quick check question: What is the purpose of using different masking strategies during training versus inference in ActFusion?

## Architecture Onboarding

- **Component map:** Input I3D features [T, C] → Encoder (modified ASFormer with NE layers) → Decoder (ND layers) → Output action label predictions

- **Critical path:**
  1. Apply masking strategy to input features
  2. Pass masked features through encoder to get embeddings
  3. Add noise to ground truth action labels
  4. Pass noisy labels and embeddings through decoder
  5. Apply losses (cross-entropy, boundary smoothing, boundary alignment)
  6. During inference, start with Gaussian noise and iteratively denoise using decoder

- **Design tradeoffs:**
  - Masking before vs. after encoder: Applying masking before encoder allows unified handling of visible/invisible parts but may reduce segmentation accuracy compared to masking after encoder
  - Learnable vs. zero mask tokens: Learnable tokens perform better as they can be embedded with visual tokens
  - Position of mask tokens: Providing mask tokens to encoder is more effective than providing them only to decoder

- **Failure signatures:**
  - Poor performance on LTA: May indicate issues with anticipative masking strategy or decoder's ability to handle future predictions
  - Low segmentation accuracy: Could suggest encoder isn't properly learning from masked inputs or mask tokens aren't effective
  - Training instability: Might occur if masking is too aggressive or if diffusion process parameters aren't properly tuned

- **First 3 experiments:**
  1. Test different masking strategies (none, anticipative, random) on a small dataset to verify the masking implementation
  2. Compare performance with and without learnable mask tokens to validate their importance
  3. Test encoder conditioning with different features (masked features F', embeddings E, encoder predictions) to find optimal setup

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation based on the evaluation results and discussion.

## Limitations
- The model relies on pre-extracted I3D features, so performance gains may not directly translate when using raw video input or different feature extractors.
- While joint learning is shown to help both tasks, the paper does not clearly separate whether gains come from architectural synergy or simply from more diverse training signals.
- The ablation study for the diffusion process itself is missing, leaving the actual contribution of the diffusion architecture versus the masking strategy unclear.

## Confidence
- **High Confidence**: The core experimental results (Table 1 and Table 2) showing state-of-the-art performance on 50 Salads, Breakfast, and GTEA are well supported by the reported metrics and ablation studies.
- **Medium Confidence**: The claim that anticipative masking is essential for both TAS and LTA performance is supported by ablation, but the underlying mechanism for how visible and invisible parts interact during training could be more explicitly validated.
- **Low Confidence**: The claim that the diffusion architecture itself is critical (beyond the masking strategy) is weakly supported, as there is no ablation comparing to a non-diffusion baseline.

## Next Checks
1. **Validate diffusion contribution**: Replace the diffusion decoder with a standard transformer decoder and compare performance on TAS and LTA to isolate the contribution of the diffusion architecture from the masking strategy.
2. **Test cross-dataset generalization**: Evaluate ActFusion on a more complex dataset (e.g., EPIC-KITCHENS) with longer, more varied action sequences to check robustness beyond the reported datasets.
3. **Probe joint learning benefits**: Conduct a controlled experiment where the model is trained on TAS and LTA separately but with identical masking and compare to the unified model to isolate the effect of shared parameters versus task-specific heads.