---
ver: rpa2
title: Conditional and Modal Reasoning in Large Language Models
arxiv_id: '2401.17169'
source_url: https://arxiv.org/abs/2401.17169
tags:
- llama
- instruct
- claude
- code
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a systematic study of logical reasoning in 29
  large language models (LLMs) on inference patterns involving conditionals and epistemic
  modals. Using hand-crafted stimuli with nonsense predicates to isolate logical form,
  the authors evaluate models' ability to distinguish valid from invalid inferences
  (e.g., Modus Tollens, Disjunctive Syllogism) and to reason consistently about modal
  interactions (e.g., "might not" vs.
---

# Conditional and Modal Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2401.17169
- Source URL: https://arxiv.org/abs/2401.17169
- Authors: Wesley H. Holliday; Matthew Mandelkern; Cedegao E. Zhang
- Reference count: 40
- Key outcome: Systematic evaluation of 29 LLMs on conditional and modal reasoning reveals persistent logical fallacies across all models, with performance improving with size and chain-of-thought prompting but fundamental gaps remaining

## Executive Summary
This paper presents a systematic evaluation of logical reasoning in large language models (LLMs) on inference patterns involving conditionals and epistemic modals. Using hand-crafted stimuli with nonsense predicates to isolate logical form, the authors test 29 models across 8 inference patterns, including Modus Ponens, Modus Tollens, and various modal interactions. The study finds that while zero-shot chain-of-thought prompting significantly improves performance (with top models reaching ~90% accuracy), all models exhibit systematic failures on certain modal/conditional fallacies. Models often overgeneralize from simple Boolean cases to invalid modal instances, and even the best performers show logically inconsistent judgments across related patterns. Performance correlates highly with general benchmarks like Chatbot Arena Elo ratings, suggesting logical reasoning is predictive of overall capability.

## Method Summary
The authors evaluated 29 large language models on 8 inference patterns involving conditionals and epistemic modals, using hand-crafted stimuli with nonsense predicates to isolate logical form. Models were tested at temperatures 0 and 1, with and without zero-shot chain-of-thought prompting, across multiple trials. Performance was measured against expert philosophical judgments about validity. The evaluation compared model responses to human reasoning patterns and correlated results with general benchmark scores (MMLU, GSM8K, Chatbot Arena Elo).

## Key Results
- All tested LLMs commit basic modal/conditional fallacies despite improvements from chain-of-thought prompting
- Performance improves with model size, with top models achieving ~90% accuracy on some patterns
- Models exhibit logical inconsistencies, often accepting invalid patterns like DSmu while rejecting related valid patterns
- Performance on logical reasoning tasks correlates strongly (0.75-0.85) with general benchmark scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot chain-of-thought prompting significantly improves performance on logical reasoning tasks
- Mechanism: Forcing structured reasoning process moves beyond surface-level pattern matching
- Core assumption: Model's internal reasoning capabilities activate when explicitly prompted to show work
- Evidence anchors: Abstract notes dramatic improvement with CoT prompting; statistically significant paired t-test results
- Break condition: If model fails to generate meaningful chain of thought or produces flawed reasoning steps

### Mechanism 2
- Claim: LLMs overgeneralize from simple Boolean conditional patterns to invalid modal cases
- Mechanism: Apply rules learned from basic conditional inference to complex modal interactions
- Core assumption: Training data contains many Boolean examples but fewer nuanced modal interactions
- Evidence anchors: Models fail on MTmu, DSmu, CMP patterns; accept DSmu while rejecting DSmi even in GPT-4
- Break condition: If model specifically trained on nuanced modal interactions

### Mechanism 3
- Claim: Performance correlates highly with general benchmark scores
- Mechanism: Logical reasoning is component of general reasoning ability measured by benchmarks
- Core assumption: Logical reasoning is fundamental aspect of general reasoning, not separate skill
- Evidence anchors: Correlations of 0.81, 0.85, and 0.75 with Chatbot Arena Elo, MMLU, and GSM8K
- Break condition: If logical reasoning found to be separable skill

## Foundational Learning

- Concept: Distinction between logical validity and material validity in conditional reasoning
  - Why needed here: Traditional benchmarks assume material conditional interpretation, but modal analysis is more accurate for natural language
  - Quick check question: Why is "If it's raining, then it's not raining hard. Therefore, if it's raining hard, then it's not raining" not logically valid under modal analysis?

- Concept: Interaction between epistemic modals and logical connectives
  - Why needed here: Models make mistakes when reasoning about sentences combining modals with "and", "or", and conditionals
  - Quick check question: Why doesn't "The envelope might be upstairs and the envelope might be under a bed" entail "The envelope might be upstairs under a bed"?

- Concept: Overgeneralization in machine learning
  - Why needed here: Models overgeneralize from valid Boolean patterns to invalid modal instances
  - Quick check question: How might training data lead models to overgeneralize Modus Tollens validity from Boolean to modal cases?

## Architecture Onboarding

- Component map: Data generation -> LLM evaluation (temperature/prompting variations) -> Response analysis (human judgments/benchmarks)
- Critical path: Hand-craft stimuli with nonsense predicates → Evaluate LLMs across temperature and prompting settings → Compare responses to expert judgments and benchmarks
- Design tradeoffs: Nonsense predicates control world knowledge but create artificial tasks; temperature sweep reveals variability but increases cost
- Failure signatures: Consistent failures on specific patterns despite chain-of-thought prompting suggest fundamental gaps; inconsistent responses across related patterns indicate overgeneralization
- First 3 experiments:
  1. Replicate core finding with uncontroversial patterns (DS, MP, MT, AC) using different prompting approaches
  2. Test overgeneralization with modal-disjunction interactions (MTmu, DSmu, CMP)
  3. Explore pure modal reasoning patterns (MuDistOr, MiAg, NSFC, WSFC)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications could enable consistent recognition of modal/conditional fallacies while preserving valid patterns?
- Basis in paper: Models overgeneralize from Boolean to modal cases with inconsistent judgments
- Why unresolved: Models lack explicit semantic representations of modal operators and conditionals
- What evidence would resolve it: Training studies comparing performance with architectural modifications or targeted fine-tuning

### Open Question 2
- Question: How do human experimental judgments compare to expert judgments and LLM performance on complex modal/conditional inferences?
- Basis in paper: Paper cites expert philosophical judgments but plans to compare against experimental data
- Why unresolved: Only expert judgments cited rather than empirical human subject data
- What evidence would resolve it: Controlled experiments comparing human subjects to both expert judgments and LLM performance

### Open Question 3
- Question: What's the relationship between performance on these tasks and ability to handle novel inference patterns?
- Basis in paper: Models may not be reliable on novel patterns; modal/conditional patterns are less frequent in training data
- Why unresolved: Paper shows correlation with benchmarks but doesn't test truly out-of-distribution patterns
- What evidence would resolve it: Testing on logically valid patterns never seen in training data

### Open Question 4
- Question: Would explicit modal logic semantics in neurosymbolic approaches enable consistent reasoning?
- Basis in paper: Neurosymbolic approaches don't implement sophisticated modal semantics discussed
- Why unresolved: Current neurosymbolic approaches lack proper modal semantics
- What evidence would resolve it: Implementing theorem provers with proper modal semantics and testing performance

## Limitations

- Use of nonsense predicates creates artificial tasks that may not reflect natural language reasoning
- Temperature variability raises questions about whether inconsistency reflects genuine reasoning variation or sampling artifacts
- High correlation with Chatbot Arena Elo may reflect general capability proxies rather than specific logical competence

## Confidence

**High Confidence**: All models commit basic modal/conditional fallacies; chain-of-thought prompting consistently improves performance
**Medium Confidence**: Models overgeneralize from Boolean to modal patterns; correlation with general benchmarks
**Low Confidence**: Performance on nonsense predicates predicts real-world reasoning capabilities

## Next Checks

1. Replicate findings using naturally phrased conditional and modal statements to test performance outside nonsense predicate paradigm
2. Run identical model instances across multiple days to quantify response stability and distinguish genuine reasoning variation from sampling artifacts
3. Fine-tune base model on small corpus of correct modal/conditional reasoning examples to determine if targeted intervention reduces observed fallacies