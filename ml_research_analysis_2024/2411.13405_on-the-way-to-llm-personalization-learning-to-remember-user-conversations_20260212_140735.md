---
ver: rpa2
title: 'On the Way to LLM Personalization: Learning to Remember User Conversations'
arxiv_id: '2411.13405'
source_url: https://arxiv.org/abs/2411.13405
tags:
- conversation
- questions
- conversations
- user
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores injecting knowledge of prior user conversations
  into LLMs to enable future personalization research. It proposes PLUM, a pipeline
  that augments user conversations as question-answer pairs, then fine-tunes a LoRA
  adapter with a weighted cross-entropy loss.
---

# On the Way to LLM Personalization: Learning to Remember User Conversations

## Quick Facts
- arXiv ID: 2411.13405
- Source URL: https://arxiv.org/abs/2411.13405
- Reference count: 31
- Primary result: PLUM achieves 81.5% accuracy on yes/no questions about conversations

## Executive Summary
This paper explores injecting knowledge of prior user conversations into LLMs to enable future personalization research. It proposes PLUM, a pipeline that augments user conversations as question-answer pairs, then fine-tunes a LoRA adapter with a weighted cross-entropy loss. The method achieves an accuracy of 81.5% across 100 conversations, performing competitively with RAG baselines (83.5%). Key findings include the importance of 10 epochs per conversation, the weighted cross-entropy loss, and maintaining balance between positive and negative samples.

## Method Summary
PLUM augments user conversations by generating question-answer pairs using Llama 3 8B Instruct, creating both positive (yes) and negative (no) samples for each conversation. A LoRA adapter (rank=16, alpha=64) is fine-tuned sequentially on each conversation for 10 epochs using a weighted cross-entropy loss that scales question and answer tokens by 位=10. The model is evaluated on yes/no questions about conversation content, achieving 81.5% accuracy while maintaining performance on baseline tasks.

## Key Results
- PLUM achieves 81.5% accuracy on yes/no questions about conversations (vs 75.0% baseline)
- Outperforms RAG baseline with 83.5% accuracy
- Weighted cross-entropy loss significantly improves performance over standard CE
- Sequential training prevents catastrophic forgetting while enabling memory disposal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted cross-entropy loss on question-answer tokens forces the model to focus on conversational content rather than system prompts.
- Mechanism: By scaling the CE loss on qi and ai by 位=10, the gradient signal prioritizes learning the semantic content of what was discussed over matching boilerplate text.
- Core assumption: The model can distinguish between content tokens and structural tokens, and will adjust learning accordingly when loss weighting is applied.
- Evidence anchors:
  - [section]: "we empirically derive a custom loss based on the CE loss... scaling up the CE loss on the question and answer tokens... weighting qi and ai focuses the model on the actual question and answer"
  - [abstract]: "finetune a low-rank adaptation adapter with a weighted cross entropy loss"
- Break condition: If the model cannot differentiate content from structure, or if 位 is too extreme causing gradient instability.

### Mechanism 2
- Claim: Maintaining positive-negative balance prevents directional bias in responses.
- Mechanism: Balanced sampling ensures the model learns both "yes" and "no" responses equally, avoiding overfitting to one answer type.
- Core assumption: The model's default behavior without negative samples is to always answer "yes", and this bias can be corrected through balanced training.
- Evidence anchors:
  - [section]: "we propose maintaining a balance between positive and negative samples so that the model does not err in one direction"
  - [section]: "increasing the number of positive or negative samples by 25% per conversation deteriorates results... the model's accuracy to respond with 'yes' or 'no' increases for the respective cases, but decreases for the opposite"
- Break condition: If sampling becomes imbalanced during training or if negative samples are too dissimilar from positive ones.

### Mechanism 3
- Claim: Conversation-by-conversation sequential training prevents catastrophic forgetting while enabling memory disposal.
- Mechanism: By completing all epochs on one conversation before moving to the next, the model solidifies knowledge of that conversation before overwriting it with new information.
- Core assumption: LLMs can retain conversation-specific knowledge through LoRA adapters when trained in isolation rather than mixing all conversations simultaneously.
- Evidence anchors:
  - [section]: "conversations are sequential in nature... we should finish finetuning on one conversation before moving on to the next... allows for discarding the conversation after all of its samples have been iterated over"
  - [section]: "We measure the model's accuracy over time to detect issues such as catastrophic forgetting... We observe no sign of catastrophic forgetting"
- Break condition: If conversations share too much semantic overlap, or if the model capacity is exceeded.

## Foundational Learning

- Concept: Cross-entropy loss and its weighted variants
  - Why needed here: Understanding how scaling different token sequences affects gradient flow and model focus
  - Quick check question: What happens to the gradient when you multiply the loss for certain tokens by 10 versus the rest?

- Concept: Parameter-efficient fine-tuning (LoRA) mechanics
  - Why needed here: LoRA modifies weight updates through low-rank decomposition rather than full parameter changes
  - Quick check question: How does a rank-16 LoRA adapter differ from full fine-tuning in terms of parameter count and update dynamics?

- Concept: Catastrophic forgetting in sequential learning
  - Why needed here: The paper's approach depends on preventing old knowledge loss when learning new conversations
  - Quick check question: Why might training on conversation 1 then conversation 2 cause the model to "forget" conversation 1?

## Architecture Onboarding

- Component map: Data augmentation pipeline -> LoRA adapter -> Weighted CE loss -> Sequential training loop
- Critical path:
  1. Generate conversation data (user prompt + model response)
  2. Generate balanced question-answer pairs (positive/negative, open/closed)
  3. Initialize LoRA adapter on base LLM
  4. For each conversation: train for 10 epochs with weighted CE
  5. Evaluate on held-out yes/no questions
- Design tradeoffs:
  - LoRA vs full fine-tuning: Parameter efficiency vs potential performance ceiling
  - Weighted vs standard CE: Better content focus vs simpler implementation
  - Sequential vs batch training: Prevents forgetting vs slower convergence
- Failure signatures:
  - Oscillating yes/no accuracy: Indicates loss weighting or epoch count issues
  - All "no" responses: Suggests negative samples dominate or loss is misconfigured
  - Random guessing: Points to insufficient training or poor data quality
- First 3 experiments:
  1. Train with standard CE loss (no weighting) to verify improvement from weighted version
  2. Vary epoch count (e=1,5,15,20) to find optimal training duration per conversation
  3. Test with only positive samples to demonstrate importance of negative balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of epochs per conversation affect the model's ability to remember user conversations, and what is the optimal number of epochs for different conversation types?
- Basis in paper: [explicit] The paper mentions that the number of epochs per conversation is highly important and that e=10 provides the best trade-off between the accuracy of 'yes' and 'no' questions. However, it also suggests that some conversations may be easier to learn than others.
- Why unresolved: The paper does not provide a detailed analysis of how the number of epochs affects the model's performance for different types of conversations. It only mentions that e=10 is optimal for the general case.
- What evidence would resolve it: Conducting experiments with varying numbers of epochs for different types of conversations and analyzing the results would provide insights into the optimal number of epochs for each type.

### Open Question 2
- Question: How does the weighted cross-entropy loss function contribute to the model's ability to remember user conversations, and are there alternative loss functions that could further improve performance?
- Basis in paper: [explicit] The paper proposes a custom weighted cross-entropy loss function and shows that it significantly improves the model's performance compared to the standard cross-entropy loss. However, it does not explore other potential loss functions.
- Why unresolved: The paper only tests the proposed weighted cross-entropy loss and does not compare it to other loss functions that could potentially improve the model's performance.
- What evidence would resolve it: Conducting experiments with different loss functions and comparing their performance to the proposed weighted cross-entropy loss would provide insights into whether there are alternative loss functions that could further improve the model's ability to remember user conversations.

### Open Question 3
- Question: How does the balance between positive and negative samples affect the model's ability to remember user conversations, and what is the optimal balance for different types of conversations?
- Basis in paper: [explicit] The paper mentions that maintaining a balance between positive and negative samples is important for reinforcing the knowledge boundary and preventing hallucination. However, it does not provide a detailed analysis of how the balance affects the model's performance.
- Why unresolved: The paper only mentions the importance of balancing positive and negative samples but does not explore how different balances affect the model's performance for different types of conversations.
- What evidence would resolve it: Conducting experiments with varying balances of positive and negative samples for different types of conversations and analyzing the results would provide insights into the optimal balance for each type.

## Limitations

- Evaluation scope is limited to yes/no questions about conversation content rather than broader personalization capabilities
- Data generation process relies on automatically generated Q/A pairs without detailed quality control specifications
- Method tested only on 100 conversations with Llama 3 8B, scalability and generalization unclear

## Confidence

**High Confidence** (supported by direct evidence and ablation studies):
- Weighted cross-entropy loss improves focus on conversational content compared to standard loss
- Maintaining positive-negative balance prevents directional bias in responses
- 10 epochs per conversation provides optimal training duration
- Sequential conversation training prevents catastrophic forgetting

**Medium Confidence** (supported by results but with limitations):
- PLUM achieves competitive performance with RAG baselines (81.5% vs 83.5%)
- LoRA adapter with rank=16 and alpha=64 is effective for this task
- The method can remember and recall specific conversation details

**Low Confidence** (limited evidence or future work claims):
- The approach can reduce redundancy in subsequent conversations (future work)
- PLUM can extend reasoning capabilities through conversation memory (future work)
- Performance generalizes to other datasets or conversation types beyond yes/no questions

## Next Checks

1. **Ablation Study on Loss Weighting**: Systematically vary 位 from 1 to 20 in increments of 2, measuring both yes/no accuracy and baseline task performance. This would verify the optimal weighting and check if extreme values cause gradient instability or degradation in general capabilities.

2. **Catastrophic Forgetting Analysis**: Train on conversations 1-50, then continue training on conversations 51-100 while periodically evaluating on conversations 1-50. Measure accuracy decay over time and compare against batch training baselines to quantify forgetting.

3. **Negative Sample Quality Analysis**: Create controlled experiments with varying negative sample quality - using completely unrelated conversations, slightly modified positive questions, and completely random negatives. Measure how different negative qualities affect accuracy and bias, determining the minimum quality threshold needed for effective training.