---
ver: rpa2
title: LLMs Are Prone to Fallacies in Causal Inference
arxiv_id: '2406.12158'
source_url: https://arxiv.org/abs/2406.12158
tags:
- causal
- relations
- position
- temporal
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  infer causal relations from relational data in text beyond simply memorizing explicitly
  stated causal facts. To test this, the authors finetune LLMs on synthetic data containing
  temporal, spatial, and counterfactual relations, then evaluate their ability to
  infer causal relations.
---

# LLMs Are Prone to Fallacies in Causal Inference

## Quick Facts
- arXiv ID: 2406.12158
- Source URL: https://arxiv.org/abs/2406.12158
- Reference count: 40
- Primary result: LLMs learn position heuristics and post hoc fallacies when inferring causal relations from text

## Executive Summary
This paper investigates whether large language models can infer causal relations from relational data in text beyond simply memorizing explicitly stated causal facts. The authors finetune LLMs on synthetic data containing temporal, spatial, and counterfactual relations, then evaluate their ability to infer causal relations. They find that LLMs are susceptible to inferring causal relations from the relative position of event mentions (position heuristic) and from temporal precedence (post hoc fallacy). While LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they struggle to infer causal relations from counterfactuals.

## Method Summary
The authors generate synthetic datasets from causal graphs containing temporal, spatial, and counterfactual relations between events. They then finetune LLAMA 2 models (7B, 13B, 70B) on these datasets using LoRA adapters. The models are evaluated on multiple-choice tasks to predict causal relations from the relational data. The evaluation includes testing for position heuristics by comparing performance when event positions match vs. don't match between training and evaluation, and testing for post hoc fallacy by randomizing event positions in the training data.

## Key Results
- LLMs learn position heuristics when finetuning data supports sequential order of events
- Even after mitigating position heuristic, LLMs still suffer from post hoc fallacy
- LLMs can correctly deduce absence of causal relations from temporal and spatial relations
- LLMs struggle to infer causal relations from counterfactuals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs learn position heuristics when finetuning data supports sequential order of events.
- Mechanism: When events X and Y appear in fixed relative order (e.g., X always before Y) in training, the model associates position with causality rather than relational content.
- Core assumption: The pretraining data contains similar position-causality patterns, reinforcing this heuristic.
- Evidence anchors:
  - [abstract]: "LLMs are susceptible to inferring causal relations from the order of two entity mentions in text (e.g. X mentioned before Y implies X causes Y)."
  - [section]: "Table 1 shows accuracy on DX→Y... models infer the causal edge only when the relative position of the two events under test matches during finetuning and evaluation."
  - [corpus]: "Across all causal relations, we find that when X, Y co-occur within the context window, 60.77% of the times X occurs before Y."
- Break condition: Randomizing event mention order in training data (e.g., adding 10% paraphrased variations) breaks the heuristic.

### Mechanism 2
- Claim: LLMs exhibit post hoc fallacy when position heuristic is removed.
- Mechanism: After mitigating position bias, models overgeneralize from temporal precedence to infer causation, mistaking correlation for causation.
- Core assumption: Temporal relations are common in pretraining data, making this overgeneralization likely.
- Evidence anchors:
  - [abstract]: "if the order is randomized, LLMs still suffer from the post hoc fallacy, i.e. X occurs before Y (temporal relation) implies X causes Y."
  - [section]: "Figure 5 (left) shows the error rate... all models incorrectly infer the causal relation better than random guessing (33.3%)."
  - [corpus]: Weak corpus evidence; no direct temporal-causality co-occurrence statistics provided.
- Break condition: Including explicit causal/non-causal statements in training data can reduce this fallacy.

### Mechanism 3
- Claim: LLMs struggle to infer causal relations from counterfactuals.
- Mechanism: Counterfactual reasoning requires understanding necessity/sufficiency, which is not captured by simple text patterns.
- Core assumption: Counterfactuals are less frequent and more complex in pretraining data than temporal/spatial relations.
- Evidence anchors:
  - [abstract]: "we find that while LLMs can correctly deduce the absence of causal relations from temporal and spatial relations, they have difficulty inferring causal relations from counterfactuals."
  - [section]: "Table 2 shows the results... models can correctly deduce the absence of causal relations from temporal relations and spatial relations better than random guessing, but cannot deduce causal relations from either positive counterfactual or negative counterfactuals."
  - [corpus]: No explicit corpus evidence on counterfactual frequency or patterns.
- Break condition: Scaling model size does not improve counterfactual reasoning performance.

## Foundational Learning

- Concept: Temporal precedence does not imply causation (post hoc fallacy).
  - Why needed here: To understand why LLMs incorrectly infer causality from temporal order after position bias is removed.
  - Quick check question: If event A occurs before event B, can we always conclude A causes B? Why or why not?

- Concept: Spatial locality principle (events in different locations do not directly cause each other).
  - Why needed here: To grasp how LLMs correctly infer absence of causal relations from spatial relations.
  - Quick check question: If two events occur in different cities, what can we infer about their causal relationship?

- Concept: Counterfactual reasoning (understanding necessity and sufficiency of causes).
  - Why needed here: To appreciate why LLMs struggle with inferring causal relations from counterfactual statements.
  - Quick check question: How does a counterfactual statement like "If X had not happened, Y would not have happened" help establish causality?

## Architecture Onboarding

- Component map:
  - Data generation pipeline → Causal graphs → Relation graphs → Scenario generation → Template verbalization
  - Finetuning module → LoRA adapter on LLAMA 2 → Evaluation module → Multiple-choice causal relation prediction

- Critical path: Data generation → Finetuning → Evaluation → Analysis of position bias and fallacies

- Design tradeoffs:
  - Synthetic data vs real data: Synthetic data allows controlled experiments but may not capture all real-world nuances.
  - Position randomization: Balancing between mitigating position heuristic and maintaining realistic event orderings.
  - Template variety: More templates improve robustness but increase computational cost.

- Failure signatures:
  - Over-reliance on event mention order (position heuristic)
  - Inferring causation from temporal precedence (post hoc fallacy)
  - Inability to infer causation from counterfactuals

- First 3 experiments:
  1. Finetune LLAMA 2-7B on temporal relations with fixed event order, evaluate on causal relation prediction with matched/unmatched positions.
  2. Repeat experiment with randomized event orders in training data, observe reduction in position heuristic.
  3. Finetune on counterfactual relations, evaluate ability to infer causal relations, compare to temporal/spatial relations.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic data may not represent real-world causal reasoning challenges
- Single-hop causal relations may oversimplify real-world causal chains
- Limited to English and specific causal relation types
- Position randomization at 10% may be insufficient to fully eliminate position heuristics

## Confidence

**High Confidence** (supported by robust experimental evidence):
- LLMs learn position heuristics when event order is consistent in training data
- This position heuristic can be mitigated through training data randomization
- LLMs correctly deduce absence of causal relations from spatial relations

**Medium Confidence** (supported by evidence but with notable caveats):
- LLMs suffer from post hoc fallacy after position bias is removed
- LLMs struggle with counterfactual reasoning for causal inference
- Results may not generalize to more complex causal reasoning scenarios

## Next Checks

1. **Real-World Data Validation**: Test the same experimental setup on real-world text corpora with naturally occurring causal, temporal, spatial, and counterfactual relations to verify if the observed patterns hold beyond synthetic data.

2. **Multi-Hop Causal Chain Evaluation**: Design experiments to evaluate LLM performance on multi-hop causal chains (A→B→C→D) to assess whether the observed limitations in single-hop reasoning extend to more complex causal reasoning tasks.

3. **Cross-Lingual and Domain Transfer**: Evaluate the same models on causal reasoning tasks in different languages and domains (e.g., scientific literature, news articles, technical documentation) to assess the generalizability of the findings across linguistic and domain boundaries.