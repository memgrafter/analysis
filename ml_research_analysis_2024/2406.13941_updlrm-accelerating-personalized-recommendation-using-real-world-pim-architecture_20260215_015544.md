---
ver: rpa2
title: 'UpDLRM: Accelerating Personalized Recommendation using Real-World PIM Architecture'
arxiv_id: '2406.13941'
source_url: https://arxiv.org/abs/2406.13941
tags:
- memory
- embedding
- time
- lookup
- cache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes UpDLRM, a system that accelerates deep learning
  recommendation models (DLRMs) by utilizing real-world processing-in-memory (PIM)
  hardware, specifically UPMEM DPU. The main idea is to offload the memory-intensive
  embedding operations to DPUs, leveraging their parallel nature to reduce recommendation
  latency.
---

# UpDLRM: Accelerating Personalized Recommendation using Real-World PIM Architecture

## Quick Facts
- arXiv ID: 2406.13941
- Source URL: https://arxiv.org/abs/2406.13941
- Authors: Sitian Chen; Haobin Tan; Amelie Chi Zhou; Yusen Li; Pavan Balaji
- Reference count: 20
- Primary result: Up to 4.6x speedup in inference time compared to CPU-only and CPU-GPU hybrid implementations

## Executive Summary
UpDLRM is a system that accelerates deep learning recommendation models (DLRMs) by utilizing real-world processing-in-memory (PIM) hardware, specifically UPMEM DPU. The main idea is to offload the memory-intensive embedding operations to DPUs, leveraging their parallel nature to reduce recommendation latency. The authors address the challenge of partitioning embedding tables across DPUs to achieve workload balance and efficient data caching. Evaluations on six real-world datasets demonstrate that UpDLRM achieves up to 4.6x speedup in inference time compared to CPU-only and CPU-GPU hybrid counterparts.

## Method Summary
The paper proposes UpDLRM, a system that accelerates DLRMs by utilizing real-world PIM hardware, specifically UPMEM DPU, to offload memory-intensive embedding operations and reduce recommendation latency. The method involves partitioning embedding tables into tiles smaller than 64MB to fit in DPU memory, employing uniform, non-uniform, and cache-aware partitioning methods to balance workload and optimize data caching, and evaluating performance using UpDLRM on UPMEM DPU hardware. The system is tested on six real-world datasets with varying hotness levels and average reduction frequencies.

## Key Results
- Up to 4.6x speedup in inference time compared to CPU-only and CPU-GPU hybrid implementations
- Cache-aware non-uniform partitioning reduces memory traffic while balancing workload across DPUs
- Uniform EMT partitioning with optimized tile size balances workload and maximizes DPU throughput

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Offloading embedding lookups to DPUs reduces CPU memory bandwidth contention and accelerates inference.
- Mechanism: Embedding layers require irregular memory accesses that bottleneck CPU memory bandwidth. By partitioning embedding tables and executing lookups on DPUs, these accesses are served by DPU memory banks in parallel, freeing CPU resources for other tasks.
- Core assumption: DPU memory bandwidth is sufficiently higher than CPU memory bandwidth for embedding operations, and data movement overhead between CPU and DPU is minimal.
- Evidence anchors:
  - [abstract] "utilizes real-world processing-in-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth and reduce recommendation latency"
  - [section] "by offloading the memory-bound embedding operations to DPUs, we can reduce the resource contention on CPU memory bandwidth"
  - [corpus] Weak evidence - no direct comparisons of DPU vs CPU bandwidth in cited papers
- Break condition: If inter-DPU communication becomes frequent due to poor partitioning, data movement overhead through CPU memory could negate bandwidth benefits.

### Mechanism 2
- Claim: Uniform EMT partitioning with optimized tile size balances workload and maximizes DPU throughput.
- Mechanism: By dividing embedding tables into uniform tiles of size optimized based on MRAM read latency characteristics (32B sweet spot), each DPU receives equal workload. This ensures all DPUs process lookups simultaneously without idle time.
- Core assumption: Access patterns are balanced across embedding table rows, and MRAM read latency is predictable based on data transfer size.
- Evidence anchors:
  - [section] "we prefer to have ùëÅùëê*4B ‚â§ 32B, namely ùëÅùëê ‚â§ 8" based on MRAM read latency study
  - [section] "uniform EMT partitioning can result in the same embedding lookup time on each DPU"
  - [corpus] Weak evidence - latency characteristics are specific to UPMEM DPU architecture
- Break condition: Real-world datasets show highly imbalanced access patterns (e.g., 340x difference between popular and least popular blocks), making uniform partitioning suboptimal.

### Mechanism 3
- Claim: Cache-aware non-uniform partitioning reduces memory traffic while balancing workload across DPUs.
- Mechanism: By identifying frequently co-occurring item combinations and caching their partial sums, memory lookups are reduced. The partitioning algorithm assigns items to DPUs considering both embedding table capacity and cache capacity to maintain balance.
- Core assumption: Item access follows power-law distribution with predictable co-occurrence patterns, and cache storage can be effectively shared with embedding tables within 64MB MRAM limit.
- Evidence anchors:
  - [section] "the co-occurrence feature of multiple items in real recommendation systems" and cache list generation
  - [section] "cache-aware non-uniform partitioning as shown in Algorithm 1" that balances EMT+cache accesses
  - [corpus] Moderate evidence - GRACE algorithm mentioned but specific to UPMEM DPU context
- Break condition: If cache hit rate is low or co-occurrence patterns change rapidly, caching provides minimal benefit while consuming valuable MRAM space.

## Foundational Learning

- Concept: Processing-in-Memory (PIM) architecture
  - Why needed here: Understanding how DPUs integrate processing units with memory banks is essential to grasp why embedding lookups can be accelerated compared to traditional CPU-GPU architectures.
  - Quick check question: What are the three main memory components in each DPU and their respective sizes?

- Concept: Embedding table partitioning and memory alignment
  - Why needed here: The effectiveness of UpDLRM depends on correctly partitioning large embedding tables into tiles that fit within 64MB MRAM while considering MRAM read latency characteristics (8B alignment, 2048B max).
  - Quick check question: Given a 10GB embedding table and 256 DPUs, what is the maximum number of rows per tile if each row is 4 bytes and we maintain the 32B sweet spot?

- Concept: Workload balancing in parallel systems
  - Why needed here: Understanding bin-packing and greedy assignment algorithms is crucial for implementing non-uniform and cache-aware partitioning methods that distribute access frequency across DPUs.
  - Quick check question: If you have 1000 items with varying access frequencies and 10 DPUs, what greedy strategy would you use to assign items to minimize maximum DPU load?

## Architecture Onboarding

- Component map:
  - CPU -> DPU (256 units) -> UPMEM DIMM -> CPU
  - CPU hosts main program, manages DPU orchestration, performs final aggregation of partial sums
  - DPU has 64MB MRAM, 24KB IRAM, 64KB WRAM; runs lookup operations in parallel
  - UPMEM DIMM contains 8-16 PIM chips, each with 8 DPUs; connects to CPU via standard DDR4 interface
  - EMT: Embedding tables partitioned into tiles stored across DPUs
  - Cache: Partial sums of frequently co-occurring items stored in MRAM alongside embedding vectors

- Critical path:
  1. Pre-processing: Generate indices/offsets, partition EMTs, initialize cache
  2. CPU-DPU communication: Send lookup requests from CPU to DPUs
  3. DPU lookup: Parallel embedding lookups and partial sum computation within DPUs
  4. DPU-CPU communication: Return partial results to CPU
  5. Aggregation: CPU combines partial results into final embedding vectors

- Design tradeoffs:
  - Partition granularity vs. communication overhead: Smaller tiles reduce DPU lookup time but increase communication frequency
  - Cache capacity vs. EMT capacity: Larger cache reduces lookups but limits embedding table storage
  - Uniform vs. non-uniform partitioning: Simpler implementation vs. better workload balance for skewed access patterns
  - Tasklet count per DPU: More tasklets improve pipeline utilization but increase complexity

- Failure signatures:
  - Low speedup despite DPU acceleration: Check if inter-DPU communication is excessive or if cache hit rate is poor
  - CPU bottleneck: Verify if DPU-CPU communication time dominates total latency
  - DPU underutilization: Examine if workload imbalance causes some DPUs to finish early while others remain busy
  - Memory overflow: Ensure partitioned tiles fit within 64MB MRAM considering both EMT and cache storage

- First 3 experiments:
  1. Baseline performance comparison: Run DLRM-CPU, DLRM-Hybrid, and UpDLRM on a medium-hot dataset (e.g., meta1) to establish speedup baseline and identify bottlenecks
  2. Partitioning method evaluation: Test uniform, non-uniform, and cache-aware partitioning on the same dataset with varying ùëÅùëê values (2, 4, 8) to find optimal configuration
  3. Cache sensitivity analysis: Vary cache capacity from 40% to 100% of required storage size on a high-hot dataset (e.g., GoodReads) to quantify caching benefits and identify optimal cache size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does UpDLRM's performance scale with the number of DPUs, especially when partitioning embedding tables across a large number of DPUs?
- Basis in paper: [inferred] The paper mentions that each DPU has a limited memory capacity of 64MB, and larger embedding tables need to be partitioned across multiple DPUs. However, it does not provide a detailed analysis of how performance scales with the number of DPUs.
- Why unresolved: The paper does not discuss the impact of increasing the number of DPUs on performance, such as potential communication overhead or load balancing issues.
- What evidence would resolve it: Experimental results showing UpDLRM's performance with varying numbers of DPUs, including a detailed breakdown of latency components and any observed bottlenecks.

### Open Question 2
- Question: How does the cache-aware partitioning method perform when the popularity distribution of items changes over time?
- Basis in paper: [inferred] The paper mentions that the cache-aware partitioning method considers the co-occurrence feature of multiple items in real recommendation systems and adopts existing caching techniques to reduce memory traffic. However, it does not address how the method adapts to changing item popularity distributions.
- Why unresolved: The paper does not discuss the adaptability of the cache-aware partitioning method to dynamic changes in item popularity, which is common in real-world recommendation systems.
- What evidence would resolve it: An analysis of the cache-aware partitioning method's performance under varying item popularity distributions, including its ability to adapt and maintain efficient partitioning.

### Open Question 3
- Question: How does UpDLRM's energy efficiency compare to CPU-only and CPU-GPU hybrid architectures, considering the power consumption of the UPMEM DPU modules?
- Basis in paper: [explicit] The paper mentions that the UPMEM technical paper indicates a potential reduction of 60% in energy consumption for the PIM platform. However, it does not provide a direct comparison of UpDLRM's energy efficiency with other architectures.
- Why unresolved: The paper focuses on inference time speedup and does not provide a comprehensive analysis of energy consumption, which is crucial for real-world deployment considerations.
- What evidence would resolve it: A detailed energy consumption analysis of UpDLRM, CPU-only, and CPU-GPU hybrid architectures, including measurements of power usage and energy efficiency metrics.

## Limitations

- Limited validation of cache-aware partitioning effectiveness across different recommendation domains
- No empirical analysis of performance scaling with varying numbers of DPUs
- Lack of energy efficiency comparison with CPU-only and CPU-GPU hybrid architectures

## Confidence

- **High Confidence**: The fundamental mechanism of offloading embedding lookups to DPUs is sound, given established PIM literature and the memory-bound nature of embedding operations.
- **Medium Confidence**: The partitioning strategies (uniform and non-uniform) are reasonable but lack empirical validation on datasets with varying access pattern skew.
- **Low Confidence**: The cache-aware partitioning's effectiveness is primarily theoretical, with limited experimental validation of co-occurrence pattern exploitation and cache hit rates.

## Next Checks

1. **Skew Sensitivity Analysis**: Test UpDLRM performance on datasets with varying power-law coefficients (e.g., Œ±=0.5 to Œ±=2.0) to validate workload balancing effectiveness across different access distributions.

2. **Cache Hit Rate Measurement**: Implement instrumentation to measure actual cache hit rates during inference and quantify the relationship between cache capacity utilization and speedup.

3. **Communication Overhead Profiling**: Conduct detailed profiling of CPU-DPU and DPU-CPU communication times to verify that memory bandwidth improvements are not negated by data movement costs.