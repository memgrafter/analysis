---
ver: rpa2
title: 'MMSci: A Dataset for Graduate-Level Multi-Discipline Multimodal Scientific
  Understanding'
arxiv_id: '2407.04903'
source_url: https://arxiv.org/abs/2407.04903
tags:
- scientific
- figure
- data
- dataset
- figures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMSci, a comprehensive multimodal dataset
  for scientific figure understanding, covering 72 scientific disciplines with 742k
  figures from peer-reviewed Nature Communications articles. The dataset addresses
  the gap in existing benchmarks by focusing on complex, graduate-level scientific
  figures beyond simple charts, including schematics, microscopic images, and experimental
  results.
---

# MMSci: A Dataset for Graduate-Level Multi-Discipline Multimodal Scientific Understanding

## Quick Facts
- arXiv ID: 2407.04903
- Source URL: https://arxiv.org/abs/2407.04903
- Authors: Zekun Li; Xianjun Yang; Kyuri Choi; Wanrong Zhu; Ryan Hsieh; HyeonJung Kim; Jin Hyuk Lim; Sungyoung Ji; Byungju Lee; Xifeng Yan; Linda Ruth Petzold; Stephen D. Wilson; Woosang Lim; William Yang Wang
- Reference count: 40
- Key outcome: MMSci dataset with 742k figures from Nature Communications across 72 disciplines shows Qwen2-VL-7B fine-tuned on task-specific data outperforms both GPT-4o and human experts on multiple-choice questions

## Executive Summary
MMSci is a comprehensive multimodal dataset for scientific figure understanding, containing 742k figures from peer-reviewed Nature Communications articles across 72 scientific disciplines. The dataset addresses the gap in existing benchmarks by focusing on complex, graduate-level scientific figures beyond simple charts, including schematics, microscopic images, and experimental results. The authors evaluate 19 models on figure captioning and multiple-choice tasks, revealing significant performance gaps and demonstrating that task-specific fine-tuning and continuous pre-training on interleaved article and figure data substantially improves model performance.

## Method Summary
The MMSci dataset was constructed by collecting articles and figures from Nature Communications, extracting sub-captions, categorizing figure types, and splitting data into benchmark sets. The authors evaluated 19 models on figure captioning and multiple-choice tasks, fine-tuned Qwen2-VL-7B on task-specific data, and pre-trained LLaMA2-7B on interleaved article text and figure images. The benchmark tasks included figure captioning with quality metrics (BLEU, ROUGE, METEOR, BERTScore, CIDEr, FACTSCORE, G-EVAL) and multiple-choice accuracy evaluation against human experts.

## Key Results
- Qwen2-VL-7B fine-tuned on MMSci task-specific data achieved higher multiple-choice accuracy than both GPT-4o and human experts
- Continuous pre-training on interleaved article and figure data substantially improved downstream material generation performance
- Abstract-grounded captioning consistently improved generation quality across all models by providing essential context
- Significant performance gaps were observed between open-source and proprietary models on graduate-level scientific figure understanding tasks

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning Qwen2-VL-7B on task-specific MMSci data outperforms both GPT-4o and human experts in multiple-choice evaluations. Task-specific multimodal fine-tuning on interleaved article and figure data enables the model to learn nuanced semantic associations between complex scientific figures and their corresponding captions across 72 disciplines. Core assumption: The training data captures diverse figure types and caption styles found in graduate-level scientific literature, allowing generalization across disciplines. Evidence anchors: [abstract] "Fine-tuning Qwen2-VL-7B with our task-specific data achieved better performance than GPT-4o and even human experts in multiple-choice evaluations." Break condition: If fine-tuning data doesn't represent diversity of figure types and caption styles across disciplines, model performance would degrade.

### Mechanism 2
Continuous pre-training on interleaved article text and figure images substantially enhances downstream task performance in materials science. Interleaving text and images during pre-training allows the model to learn multimodal representations that capture relationships between scientific concepts and their visual manifestations, improving material generation capabilities. Core assumption: The interleaved data structure reflects how scientific knowledge is presented in research articles, enabling effective learning of multimodal associations. Evidence anchors: [abstract] "Continuous pre-training on our interleaved article and figure data substantially enhanced the model's downstream task performance in materials science." Break condition: If interleaving doesn't capture semantic relationships between text and images, the model may not learn meaningful multimodal representations.

### Mechanism 3
Grounding figure captions in article abstracts consistently improves generation quality across all models. Providing contextual information from abstracts helps models disambiguate complex scientific figures by giving them background knowledge about research context and methodology. Core assumption: Abstracts contain sufficient contextual information to disambiguate figure content without requiring full article access. Evidence anchors: [section] "Grounding captions in article abstracts consistently improves generation quality across all models by providing essential context." Break condition: If abstracts don't contain relevant contextual information for figures, or if the model relies too heavily on abstract text without properly analyzing the figure itself, performance may not improve.

## Foundational Learning

- Concept: Multimodal representation learning
  - Why needed here: The dataset requires models to understand both visual and textual scientific information simultaneously, necessitating learning of cross-modal representations.
  - Quick check question: Can the model effectively map visual features from figures to semantic concepts described in text?

- Concept: Domain adaptation
  - Why needed here: The dataset spans 72 scientific disciplines, requiring models to adapt their understanding across diverse scientific domains.
  - Quick check question: Can the model transfer knowledge learned from one scientific discipline to another when interpreting figures?

- Concept: Figure-text alignment
  - Why needed here: The benchmark tasks require precise matching between figures and their corresponding captions, necessitating understanding of semantic alignment.
  - Quick check question: Can the model accurately determine which caption corresponds to which figure when presented with multiple options?

## Architecture Onboarding

- Component map: Data ingestion pipeline -> Preprocessing module -> Model evaluation framework -> Training pipeline -> Benchmark assessment
- Critical path: 1. Data collection and preprocessing 2. Benchmark task construction 3. Model evaluation and comparison 4. Fine-tuning on task-specific data 5. Pre-training on interleaved data 6. Downstream task evaluation
- Design tradeoffs: Using peer-reviewed articles ensures high quality but limits dataset size compared to preprint repositories; focusing on graduate-level complexity creates challenging benchmarks but may limit practical applications; providing both captioning and multiple-choice tasks offers comprehensive evaluation but increases annotation complexity
- Failure signatures: Models performing at random-chance levels on multiple-choice questions indicate insufficient understanding of scientific content; large performance gaps between open-source and proprietary models suggest limitations in current open-source architectures; poor performance on specific figure types indicates domain-specific knowledge gaps
- First 3 experiments: 1. Fine-tune Qwen2-VL-7B on task-specific MMSci data for one epoch and evaluate on multiple-choice benchmark 2. Pre-train LLaMA2-7B on interleaved MMSci data and evaluate on material generation task 3. Compare abstract-grounded captioning performance against full-article grounding for different model types

## Open Questions the Paper Calls Out

### Open Question 1
How do models perform on multimodal scientific understanding tasks across different scientific domains beyond materials science? The paper discusses evaluating models across 72 scientific disciplines but focuses detailed analysis on materials science. This remains unresolved because the paper primarily emphasizes materials science as the most represented domain and provides detailed results only for this field. What evidence would resolve it: Comprehensive performance analysis of models across all 72 scientific disciplines in MMSci dataset.

### Open Question 2
What is the optimal balance between using full article context versus abstracts for scientific figure captioning tasks? While the paper shows both approaches improve performance, it doesn't determine the optimal balance or when each approach is most beneficial. This remains unresolved because the paper only demonstrates that both approaches improve performance without identifying when each is most beneficial. What evidence would resolve it: Systematic comparison of captioning quality using abstracts alone, full articles, and hybrid approaches across different figure types and scientific domains.

### Open Question 3
How can the MMSci dataset be extended to create additional tasks for evaluating scientific knowledge comprehension beyond figure understanding? The paper states "The dataset offers rich resources that could be leveraged to create additional tasks for assessing scientific scientific knowledge comprehension" but doesn't explore what these might be or how to implement them. This remains unresolved because the paper acknowledges potential for additional tasks but doesn't explore what these might be or how to implement them. What evidence would resolve it: Specific proposals for new tasks (e.g., multi-step reasoning, hypothesis generation, experimental design) with example implementations using MMSci data.

## Limitations
- Dataset relies on Nature Communications articles, potentially introducing publication bias toward specific research methodologies and geographic regions
- Human expert performance baseline of 86.2% may not fully represent variability in scientific expertise across all 72 disciplines
- Focus on graduate-level complexity may limit applicability for undergraduate or professional development contexts

## Confidence
- High Confidence: Dataset construction methodology and benchmark creation are well-documented and reproducible
- Medium Confidence: Claim that Qwen2-VL-7B outperforms human experts requires further validation across different expert populations
- Medium Confidence: Effectiveness of abstract-grounded captioning improvements may vary depending on quality and completeness of article abstracts across different scientific disciplines

## Next Checks
1. Conduct blind evaluation of model performance by domain experts across multiple scientific disciplines to verify claimed superiority over human performance and assess potential discipline-specific biases
2. Perform systematic analysis of figure types and scientific domains represented in the dataset to quantify potential coverage gaps and assess generalizability of model performance across underrepresented disciplines
3. Implement independent reproduction of fine-tuning and pre-training procedures using different model architectures to validate robustness of reported performance improvements and identify architecture-specific limitations