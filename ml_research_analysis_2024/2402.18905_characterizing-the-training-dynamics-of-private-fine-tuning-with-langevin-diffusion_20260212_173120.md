---
ver: rpa2
title: Characterizing the Training Dynamics of Private Fine-tuning with Langevin diffusion
arxiv_id: '2402.18905'
source_url: https://arxiv.org/abs/2402.18905
tags:
- theorem
- fine-tuning
- linear
- relu
- langevin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approximation technique to study the
  training dynamics of differentially private fine-tuning using Langevin diffusion.
  The key insight is that the misalignment between randomly initialized linear heads
  and pre-trained backbone features causes feature distortion during DP fine-tuning.
---

# Characterizing the Training Dynamics of Private Fine-tuning with Langevin diffusion

## Quick Facts
- arXiv ID: 2402.18905
- Source URL: https://arxiv.org/abs/2402.18905
- Authors: Shuqi Ke; Charlie Hou; Sewoong Oh; Giulia Fanti
- Reference count: 40
- Key outcome: Sequential fine-tuning (DP-LP-FFT) mitigates feature distortion caused by random head initialization, outperforming DP-LP and DP-FFT alone, especially at lower privacy budgets.

## Executive Summary
This paper introduces a new approximation technique to study the training dynamics of differentially private fine-tuning using Langevin diffusion. The key insight is that the misalignment between randomly initialized linear heads and pre-trained backbone features causes feature distortion during DP fine-tuning. By linearizing the Langevin diffusion around the noise term rather than the parameters, the authors derive approximate upper and lower bounds on training loss for two-layer ReLU networks. Their theory predicts that running linear probing before full fine-tuning can mitigate this distortion. Empirical results validate these theoretical predictions across various architectures and datasets, showing that DP-LP-FFT can outperform both DP-LP and DP-FFT alone, particularly at lower privacy budgets.

## Method Summary
The paper models DP-SGD dynamics using Langevin diffusion and introduces a zeroth-order approximation technique that linearizes around the noise term rather than the parameters. This allows deriving approximate upper and lower bounds on training loss for DP-LP and DP-FFT. The theoretical framework analyzes two-layer ReLU networks and uses Lojasiewicz inequalities and eigenvalue analysis to establish convergence bounds. The empirical validation involves pre-training backbone models (ResNet-50, ViT, MobileNet-v3) on ImageNet-1K using self-supervised methods, then fine-tuning on private data (STL-10, CIFAR-10) using DP-SGD with different privacy budgets, comparing DP-LP-FFT against DP-LP and DP-FFT alone.

## Key Results
- DP-LP-FFT consistently outperforms both DP-LP and DP-FFT alone across multiple architectures (ResNet-50, ViT, MobileNet-v3) and datasets (STL-10, CIFAR-10).
- Feature distortion occurs during DP-FFT due to misalignment between pre-trained backbone and randomly initialized linear head, with feature quality initially degrading before improving.
- Theoretical predictions about privacy budget allocation are validated: at lower privacy budgets (ε < 1), more budget should be allocated to DP-LP phase.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomly initialized linear heads cause feature distortion during DP-FFT because they misalign with pre-trained backbone features.
- Mechanism: The randomly initialized linear head introduces a gradient signal that is misaligned with the pre-trained backbone features. This causes the backbone to adapt in directions that temporarily degrade feature quality before alignment improves.
- Core assumption: Pre-trained features are well-aligned with their original data distribution but not yet aligned with the new fine-tuning data distribution.
- Evidence anchors:
  - [abstract] "We identify the cause of the distortion as the misalignment between the pre-trained backbone and the randomly initialized linear head."
  - [section 3] "Theorem 3.3(Random initialization causes feature distortion)... DP-FFT distortsw j reducing its alignment with the data cluster."
- Break condition: If the pre-trained features are already well-aligned with the fine-tuning data distribution, or if the linear head is not randomly initialized.

### Mechanism 2
- Claim: DP-LP first mitigates feature distortion by aligning the linear head with backbone features before full fine-tuning.
- Mechanism: Linear probing (freezing the backbone and only training the linear head) allows the head to adapt to the pre-trained features without causing the backbone to distort. This alignment reduces the initial gradient misalignment when switching to DP-FFT.
- Core assumption: The linear probing phase provides sufficient time for the linear head to align with the pre-trained backbone features.
- Evidence anchors:
  - [abstract] "We prove that a sequential fine-tuning strategy can mitigate the feature distortion: first-linear-probing-then-fine-tuning (DP-LP-FFT)."
  - [section 3] "Theorem 3.4(DP-LP first mitigates feature distortion)... after running DP-LP for time∆t, switching to full fine-tuning ensures that DP-FFT does not distort the pre-trained features."
- Break condition: If the linear probing phase is too short to achieve alignment, or if the backbone features are too sensitive to noise during the LP phase.

### Mechanism 3
- Claim: The zeroth-order approximation of Langevin diffusion simplifies analysis while preserving essential noise effects, enabling convergence bounds for DP fine-tuning.
- Mechanism: By linearizing around the noise term rather than the parameters, the approximation converts complex stochastic differential equations into more tractable ordinary differential equations, while still capturing the impact of DP noise on loss dynamics.
- Core assumption: The zeroth-order approximation error is bounded and does not significantly affect the theoretical predictions.
- Evidence anchors:
  - [abstract] "A new approximation scheme allows us to derive approximate upper and lower bounds on the training loss of DP-LP and DP-FFT."
  - [section 2] "Theorem 2.2(Zeroth order approximation error)... E[∥θt − ˜θt∥2]≤(σ(2p)1/2t1/2+2nCt)2"
- Break condition: If the approximation error grows too large relative to the true dynamics, or if higher-order effects become significant.

## Foundational Learning

- Concept: Stochastic differential equations and Langevin diffusion
  - Why needed here: The paper models DP-SGD dynamics using Langevin diffusion, requiring understanding of SDEs to follow the theoretical analysis.
  - Quick check question: What is the key difference between ordinary differential equations and stochastic differential equations?

- Concept: Differential privacy and DP-SGD
  - Why needed here: The paper analyzes differentially private fine-tuning, requiring understanding of DP mechanisms and how DP-SGD introduces noise.
  - Quick check question: How does the noise multiplier in DP-SGD relate to the privacy budget (epsilon)?

- Concept: Neural collapse and feature representation learning
  - Why needed here: The paper discusses how features collapse during pre-training and how this affects fine-tuning, requiring understanding of representation learning dynamics.
  - Quick check question: What is neural collapse and how does it affect transfer learning performance?

## Architecture Onboarding

- Component map: Pre-trained backbone -> Linear head initialization -> Langevin diffusion with clipping -> Zeroth-order approximation -> Convergence bounds calculation -> Feature quality evaluation (kNN accuracy)

- Critical path: To reproduce the results, one must: (1) implement the two-layer network with ReLU, (2) implement Langevin diffusion with gradient clipping, (3) implement the zeroth-order approximation, (4) implement the convergence bounds calculation, and (5) implement the feature quality evaluation using kNN accuracy.

- Design tradeoffs: The paper uses a simplified two-layer network model instead of deeper architectures to enable theoretical analysis. This tradeoff allows for rigorous proofs but may limit generalizability to more complex architectures.

- Failure signatures: If the theoretical predictions don't match empirical results, possible causes include: (1) the approximation error being too large, (2) the assumptions about data separability or feature collapsing being violated, or (3) the two-layer model being too simplistic for the target architecture.

- First 3 experiments:
  1. Verify feature distortion: Pre-train a simple network, then fine-tune with DP-FFT and measure feature quality over time to confirm initial degradation followed by improvement.
  2. Test DP-LP mitigation: Run DP-LP for varying epochs before DP-FFT and measure feature distortion reduction.
  3. Validate convergence bounds: Implement the theoretical bounds for DP-LP and DP-FFT and compare with empirical loss curves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the zeroth-order approximation error bound scale favorably with network depth and dimensionality?
- Basis in paper: [explicit] The paper derives an error bound for zeroth-order approximation in Theorem 2.2, but does not analyze its scaling with network architecture.
- Why unresolved: The paper only provides a general error bound without examining how the approximation error changes as we move from 2-layer networks to deeper architectures or higher-dimensional feature spaces.
- What evidence would resolve it: Empirical studies comparing zeroth-order approximation error across networks of varying depths and dimensions, or theoretical analysis of error bounds as a function of architectural parameters.

### Open Question 2
- Question: Can the feature distortion phenomenon be completely eliminated by more sophisticated initialization strategies rather than just using linear probing?
- Basis in paper: [inferred] The paper shows that random initialization causes feature distortion (Theorem 3.3) and that linear probing can mitigate this distortion (Theorem 3.4), but does not explore whether alternative initialization methods could prevent distortion entirely.
- Why unresolved: The theoretical analysis assumes random initialization and does not investigate whether different initialization schemes (such as those based on pre-trained features or meta-learning approaches) could avoid the misalignment problem.
- What evidence would resolve it: Comparative experiments testing various initialization strategies (including feature-based and learned initializations) against linear probing, or theoretical analysis of initialization schemes that guarantee no feature distortion.

### Open Question 3
- Question: How do the convergence bounds for DP-LP and DP-FFT change when applied to networks with more than two layers or different activation functions?
- Basis in paper: [explicit] The paper derives convergence bounds specifically for 2-layer ReLU networks in Section 4, with additional results for 2-layer linear networks without approximation.
- Why unresolved: The analysis is restricted to simple architectures, and the paper does not address how the convergence behavior generalizes to deeper networks or different activation functions commonly used in practice.
- What evidence would resolve it: Extension of the theoretical analysis to deeper architectures and different activation functions, or comprehensive empirical studies comparing convergence across various network types.

### Open Question 4
- Question: What is the optimal allocation of privacy budget between DP-LP and DP-FFT phases for architectures beyond WideResNet and ResNet?
- Basis in paper: [explicit] The paper suggests that at lower privacy budgets, more budget should be allocated to DP-LP (Theorem 5.1), but only empirically tests this on specific architectures.
- Why unresolved: The theoretical predictions are based on simplified models and have not been validated across the full spectrum of modern neural network architectures.
- What evidence would resolve it: Systematic experiments testing privacy budget allocation across diverse architectures (transformers, vision transformers, efficient networks) and theoretical extensions that account for architectural differences.

### Open Question 5
- Question: How does the feature distortion phenomenon manifest in the parameter-efficient fine-tuning (PEFT) methods like LoRA under differential privacy?
- Basis in paper: [explicit] The paper briefly mentions experiments with DP-LoRA in Section 5.3, but does not provide a theoretical analysis of feature distortion in these methods.
- Why unresolved: The paper focuses on full fine-tuning and linear probing, leaving open questions about how feature distortion operates when only adapting low-rank matrices or other parameter-efficient modifications.
- What evidence would resolve it: Theoretical extension of the feature distortion analysis to PEFT methods, or comprehensive experiments comparing feature quality dynamics in DP-LP, DP-FFT, and DP-PEFT methods.

## Limitations

- The theoretical analysis relies heavily on simplifying assumptions about two-layer ReLU networks that may not fully capture the complexity of modern deep architectures.
- The zeroth-order approximation technique, while enabling tractable analysis, introduces approximation errors whose magnitude relative to the true dynamics remains unclear.
- The paper's focus on linear head misalignment as the primary cause of feature distortion may overlook other potential factors such as data distribution shifts or architectural mismatches between pre-training and fine-tuning phases.

## Confidence

- **High confidence**: The empirical validation showing DP-LP-FFT outperforming both DP-LP and DP-FFT alone across multiple architectures and datasets (ResNet-50, ViT, MobileNet-v3 on STL-10 and CIFAR-10). The consistency of results across different privacy budgets (ε ∈ [0.3, 10]) and the measurable feature quality improvements during DP-LP phases are well-supported.

- **Medium confidence**: The theoretical claims about feature distortion mechanisms and the effectiveness of sequential fine-tuning. While the proofs are mathematically rigorous for the two-layer model, the extension to deeper networks relies on empirical validation rather than theoretical guarantees.

- **Low confidence**: The specific privacy budget allocation recommendations (favoring DP-LP as ε decreases). The theoretical analysis suggests this tradeoff, but the empirical evidence, while supportive, is limited to a specific range of privacy budgets and may not generalize to extreme privacy regimes.

## Next Checks

1. **Approximation error analysis**: Quantify the zeroth-order approximation error by comparing theoretical predictions against full numerical simulations of Langevin diffusion for the two-layer model across varying noise levels and time scales.

2. **Generalization to deeper architectures**: Test the feature distortion hypothesis and DP-LP-FFT effectiveness on deeper pre-trained models (e.g., ResNet-101, Vision Transformer with varying depths) to validate whether the two-layer theoretical insights extend beyond simplified architectures.

3. **Extreme privacy budget regimes**: Evaluate the sequential fine-tuning strategy at both very high privacy budgets (ε > 10) and very low privacy budgets (ε < 0.1) to test the robustness of the theoretical privacy budget allocation recommendations across the full privacy-utility tradeoff spectrum.