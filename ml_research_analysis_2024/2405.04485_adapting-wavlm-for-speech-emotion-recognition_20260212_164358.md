---
ver: rpa2
title: Adapting WavLM for Speech Emotion Recognition
arxiv_id: '2405.04485'
source_url: https://arxiv.org/abs/2405.04485
tags:
- pooling
- speech
- emotion
- gender
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates fine-tuning strategies for WavLM Large on
  the MSP Podcast Corpus for speech emotion recognition. It evaluates the impact of
  time-dimensional pooling methods (STD, attention), gender conditioning, and text
  conditioning on model performance.
---

# Adapting WavLM for Speech Emotion Recognition

## Quick Facts
- arXiv ID: 2405.04485
- Source URL: https://arxiv.org/abs/2405.04485
- Reference count: 0
- Primary result: F1-macro score of 0.35 achieved using ensemble of five diverse models

## Executive Summary
This paper investigates fine-tuning strategies for WavLM Large on the MSP Podcast Corpus for speech emotion recognition. The study evaluates the impact of time-dimensional pooling methods, gender conditioning, and text conditioning on model performance. STD pooling with gender conditioning achieved the best individual model performance (F1-macro 0.32), while an ensemble of five diverse models reached 0.35 F1-macro on the test set. The research demonstrates that variance-based pooling and speaker-specific conditioning can enhance SER performance, while text conditioning did not provide benefits in this context.

## Method Summary
The study fine-tuned WavLM Large using weighted layer-wise averaging, followed by time-dimensional pooling (STD or attention) and conditioning with gender embeddings. The model was trained using AdamW optimizer with learning rate 1e-5 and weight decay 0.01 for 20 epochs. Text conditioning used Sentence Transformers embeddings. A final ensemble combined predictions from five diverse models using constrained optimization.

## Key Results
- STD pooling with gender conditioning achieved F1-macro score of 0.32 on the test set
- Attention pooling with gender conditioning reached F1-macro of 0.31
- Final ensemble of five diverse models achieved F1-macro score of 0.35
- Text conditioning did not improve performance despite high-quality transcriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing standard average pooling with STD pooling improves F1-macro scores by capturing variability in emotional speech.
- Mechanism: STD pooling computes both mean and standard deviation of WavLM outputs, preserving temporal variance that correlates with emotional expressiveness.
- Core assumption: Emotional content is better represented by regions with high feature variability rather than uniform averages.
- Evidence anchors:
  - [abstract]: "STD pooling with gender conditioning achieved the best F1-macro score of 0.32"
  - [section]: "STD pooling focuses on regions with high variability. It can emphasize distinct features that differ from neighboring regions, which can be beneficial for emotion classification."
  - [corpus]: Weak correlation; corpus shows related work using variance-based pooling but no direct citation for SER.
- Break condition: If emotional content is uniformly distributed across time frames, variance pooling offers no advantage.

### Mechanism 2
- Claim: Gender conditioning improves SER performance by providing speaker-specific context.
- Mechanism: Gender embeddings are point-wise multiplied with pooled WavLM output, allowing the model to learn gender-dependent emotional patterns.
- Core assumption: Emotional expression varies systematically by speaker gender, and the model can leverage this information.
- Evidence anchors:
  - [abstract]: "STD pooling with gender conditioning achieved the best F1-macro score of 0.32"
  - [section]: "combining the WavLM output with gender information can enhance the accuracy of the SER model"
  - [corpus]: Weak; corpus shows gender conditioning used in ASR but limited evidence for SER.
- Break condition: If gender has no systematic effect on emotional expression in the dataset, conditioning degrades performance.

### Mechanism 3
- Claim: Model ensemble fusion improves overall F1-macro by leveraging complementary strengths of diverse configurations.
- Mechanism: Weighted sum of model predictions, trained with constrained optimization, captures diverse decision boundaries.
- Core assumption: Different pooling methods and conditioning strategies capture different aspects of emotional content.
- Evidence anchors:
  - [abstract]: "A final ensemble of five diverse models achieved an F1-macro score of 0.35"
  - [section]: "Fusing the predictions from multiple models improve the generalization ability and can lead to improved performance"
  - [corpus]: Moderate; corpus includes multiple ensemble-based SER papers but no direct evidence for this specific configuration.
- Break condition: If models are too similar or correlations between errors are high, ensemble offers no benefit.

## Foundational Learning

- Concept: Self-supervised learning (SSL) in speech processing
  - Why needed here: WavLM is pre-trained using SSL, and understanding SSL is crucial for effective fine-tuning.
  - Quick check question: What is the key difference between supervised and self-supervised pre-training in speech models?

- Concept: Transfer learning and fine-tuning strategies
  - Why needed here: The paper evaluates multiple fine-tuning approaches for WavLM, requiring understanding of how to adapt pre-trained models.
  - Quick check question: Why might layer-wise weighted averaging outperform simple last-layer selection in fine-tuning?

- Concept: Pooling mechanisms in sequential data
  - Why needed here: Different pooling strategies (average, STD, attention) are compared for their impact on SER performance.
  - Quick check question: How does standard deviation pooling differ from average pooling in capturing temporal patterns?

## Architecture Onboarding

- Component map:
  WavLM Large (upstream) -> Layer-wise averaging -> Time-dimensional pooling -> Conditioning blocks -> Projection layer -> Classification head

- Critical path: WavLM → Layer-wise averaging → Time pooling → Conditioning → Projection → Classification

- Design tradeoffs:
  - STD pooling vs. attention pooling: STD is faster but less context-aware
  - Gender conditioning vs. no conditioning: Adds speaker context but requires gender labels
  - Text conditioning: May add semantic context but showed no improvement here
  - Label smoothing: Can help with ambiguous labels but degraded performance in this case

- Failure signatures:
  - Low F1-macro with high variance across runs: Likely overfitting or unstable training
  - Gender conditioning degrades performance: Gender information may not be relevant to this dataset
  - Text conditioning provides no improvement: Text encoder may not capture emotional nuances

- First 3 experiments:
  1. Baseline with average pooling and no conditioning
  2. STD pooling with gender conditioning (sum/2)
  3. Attention pooling with gender conditioning (multiplication)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the size of the gender embedding affect the performance of gender-conditioned SER models?
- Basis in paper: [inferred] The paper uses a lookup table for gender embeddings but does not experiment with different embedding sizes.
- Why unresolved: The study fixed the embedding size without exploring how varying dimensions might impact results.
- What evidence would resolve it: Experiments comparing F1-macro scores across different gender embedding sizes.

### Open Question 2
- Question: Would fine-tuning the WavLM CNN feature encoder improve SER performance?
- Basis in paper: [explicit] The paper explicitly states that "We keep the layers of WavLM CNN feature encoder frozen during all experiments."
- Why unresolved: The study did not explore whether unfreezing these layers would lead to better results.
- What evidence would resolve it: Comparing F1-macro scores with and without fine-tuning the CNN feature encoder.

### Open Question 3
- Question: Does the choice of text encoder significantly impact the effectiveness of text conditioning in SER?
- Basis in paper: [inferred] The paper uses Sentence Transformers but notes that "the chosen text encoder's insensitivity to emotional speech" may explain poor results.
- Why unresolved: Only one text encoder was tested, and its emotional sensitivity was not evaluated.
- What evidence would resolve it: Testing multiple text encoders with varying capabilities in capturing emotional nuances and comparing results.

## Limitations

- The study relies on externally generated gender labels for the test set, introducing potential noise and bias
- Text conditioning showed no improvement despite the availability of high-quality transcriptions
- The MSP Podcast Corpus represents a specific domain that may not reflect emotional expression patterns in other speech contexts
- The ensemble achieved only a marginal improvement over the best individual model

## Confidence

- High confidence: The observed performance improvements from STD pooling over average pooling are well-supported by the experimental results and theoretical rationale
- Medium confidence: The benefit of gender conditioning is demonstrated on the MSP Podcast Corpus but may not generalize to other datasets
- Medium confidence: The ensemble approach shows promise, but the modest improvement (0.32 to 0.35 F1-macro) suggests diminishing returns

## Next Checks

1. Test the gender conditioning approach on a different SER dataset with explicit gender annotations to verify generalizability
2. Implement ablation studies removing gender conditioning to quantify its exact contribution to the 0.32 F1-macro score
3. Evaluate the ensemble approach with different model diversity strategies (beyond just pooling and conditioning variations) to determine optimal ensemble composition