---
ver: rpa2
title: 'Train More Parameters But Mind Their Placement: Insights into Language Adaptation
  with PEFT'
arxiv_id: '2412.12674'
source_url: https://arxiv.org/abs/2412.12674
tags:
- language
- adaptation
- shot
- lora
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores parameter-efficient fine-tuning (PEFT) methods
  for adapting a small English instruction-tuned LLM (Llama-3.2-1B) to Icelandic using
  unstructured text data. The study addresses the challenge of improving generation
  performance for medium-resourced languages when machine-translated instruction data
  is insufficient due to missing language-specific knowledge.
---

# Train More Parameters But Mind Their Placement: Insights into Language Adaptation with PEFT

## Quick Facts
- **arXiv ID**: 2412.12674
- **Source URL**: https://arxiv.org/abs/2412.12674
- **Reference count**: 15
- **Primary result**: LoRA in feed-forward layers and bottleneck adapters with sufficient parameters yield best results for adapting 1B-parameter LLMs to Icelandic using unstructured text data.

## Executive Summary
This paper investigates parameter-efficient fine-tuning (PEFT) methods for adapting a small English instruction-tuned LLM to Icelandic using unstructured text data. The study systematically compares various PEFT approaches including LoRA, IA3, bottleneck adapters, and prefix tuning to identify optimal configurations for language adaptation when machine-translated instruction data is insufficient. Through extensive ablation studies, the research demonstrates that increasing trainable parameters consistently improves adaptation performance, while the placement of LoRA modules and choice of PEFT method significantly impact results. The findings reveal that prefix tuning and IA3 methods are ineffective for unstructured text adaptation, while LoRA in feed-forward layers and bottleneck adapters with adequate parameters show the most promise.

## Method Summary
The study adapts Llama-3.2-1B-Instruct to Icelandic using CC100 corpus data with CCNet filtering. Multiple PEFT methods are evaluated: LoRA with various ranks (8-1024) in attention and feed-forward layers, IA3, bottleneck adapters with reduction factors (4-64), and prefix tuning. Models are trained with 5e-5 learning rate, linear scheduler, batch size 4, and causal language modeling objective on 1,024 token contexts. Performance is measured using BERTScore F1 and ROUGE-L on the RÚV Radio News dataset for 0-shot, 1-shot, and 5-shot summarization tasks.

## Key Results
- Increasing trainable parameters consistently improves language adaptation performance across all PEFT methods
- LoRA placed in feed-forward layers outperforms LoRA in attention layers for language adaptation
- Bottleneck adapters with sufficient parameters show strong performance comparable to LoRA
- Prefix tuning and IA3 methods are ineffective for unstructured text adaptation tasks
- Restricting adaptation to final layers can mitigate performance degradation on longer context lengths

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing the number of trainable parameters consistently improves language adaptation performance
- Mechanism: More parameters provide greater learning capacity, allowing the model to better capture language-specific patterns without overfitting to limited data
- Core assumption: The relationship between parameter count and adaptation quality is monotonic and not subject to diminishing returns within tested ranges
- Evidence anchors:
  - [abstract] "Through ablation studies using various parameter-efficient fine-tuning (PEFT) methods and setups, we find that increasing the number of trainable parameters leads to better and more robust language adaptation"
  - [section] "Our results show that across architectures, more trainable parameters lead to better scores, showing, perhaps unsurprisingly, that sufficient learning capacity is crucial for language adaptation"
  - [corpus] Weak evidence - corpus shows related papers on PEFT and parameter efficiency but no direct parameter-count experiments
- Break condition: When parameter count becomes so large that it causes overfitting to the adaptation corpus or computational constraints prevent effective training

### Mechanism 2
- Claim: LoRA placement in feed-forward layers outperforms LoRA in attention layers for language adaptation
- Mechanism: Feed-forward layers capture more general language patterns while attention layers are more specialized for task-specific processing learned during instruction tuning
- Core assumption: The adaptation task (language-specific generation) benefits more from modifying general language processing than from modifying attention mechanisms
- Evidence anchors:
  - [abstract] "LoRAs placed in the feed-forward layers and bottleneck adapters show promising results with sufficient parameters"
  - [section] "As shown in Table 1, the best-performing method are LoRAs in the feed-forward layers" and "LoRA in the attention module is the most heavily affected setup"
  - [corpus] Weak evidence - corpus mentions LoRA but doesn't discuss layer-specific performance differences
- Break condition: When adaptation requires task-specific attention modifications rather than general language pattern learning

### Mechanism 3
- Claim: Restricting adaptation to final layers preserves longer-context processing capabilities
- Mechanism: Final layers contain the most task-specific adaptations from instruction tuning, so preserving them maintains the model's ability to handle longer contexts
- Core assumption: Longer-context processing capabilities are primarily stored in the final layers of the model
- Evidence anchors:
  - [abstract] "some adapted models struggle with longer context lengths, an issue that can be mitigated by adapting only the final layers"
  - [section] "We hypothesise this is a result of limiting the context length to 1,024 tokens during the adaptation process" and "the second hypothesis, however, appears plausible: restricting LoRA modules to the last two layers yields the best 5-shot results"
  - [corpus] Weak evidence - corpus shows papers on long-context LLMs but no direct evidence about layer-specific context processing
- Break condition: When language-specific knowledge requires modification of earlier layers or when the model architecture doesn't store context processing capabilities primarily in final layers

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: The adaptation process risks overwriting instruction-tuning capabilities with language-specific knowledge
  - Quick check question: What mechanism does PEFT use to prevent catastrophic forgetting when adapting models to new languages?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: Understanding different PEFT methods (LoRA, IA3, bottleneck adapters, prefix tuning) and their tradeoffs is crucial for selecting appropriate adaptation strategies
  - Quick check question: How does LoRA differ from prefix tuning in terms of parameter count and inference overhead?

- Concept: Transformer architecture
  - Why needed here: Understanding where different components (attention, feed-forward, normalization) are located helps explain why placement matters for adaptation
  - Quick check question: Which Transformer component typically has the most parameters and might be most influential for language adaptation?

## Architecture Onboarding

- Component map: Llama-3.2-1B (1B parameters, 16 layers, hidden size 2048) -> PEFT methods (LoRA, IA3, bottleneck adapters, prefix tuning) -> Icelandic CC100 corpus (250K chunks, 12.5M tokens) -> RÚV Radio News dataset (evaluation)

- Critical path: Data preprocessing → PEFT method selection → Training (1,024 token context) → Evaluation (0-shot, 1-shot, 5-shot summarization)

- Design tradeoffs:
  - Parameter count vs. inference efficiency (LoRA vs. bottleneck adapters)
  - Layer coverage vs. preservation of original capabilities
  - Context length during adaptation vs. target deployment context length

- Failure signatures:
  - Performance degradation on longer contexts indicates adaptation context length mismatch
  - Poor 1-shot/5-shot performance suggests interference with in-context learning abilities
  - Prefix tuning causing substantial performance drops indicates format mismatch issues

- First 3 experiments:
  1. Compare LoRA in feed-forward vs. attention layers with same rank (e.g., rank 256) on 0-shot summarization
  2. Test bottleneck adapters with different reduction factors (64, 16, 4) on same task
  3. Evaluate layer exclusion strategies: all layers vs. only final 2 layers on 5-shot performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of trainable parameters for language adaptation using PEFT methods, and does this vary by language family or model size?
- Basis in paper: [explicit] The paper states "A higher number of trainable parameters is better" and shows performance improvements with increased parameters, but doesn't identify an optimal point or explore variations across different languages or model sizes.
- Why unresolved: The paper only tested a limited range of parameter counts (8-1024 ranks for LoRA) and only one language (Icelandic), leaving open questions about whether there's an optimal parameter count and how this might vary across different contexts.
- What evidence would resolve it: Systematic experiments varying parameter counts across multiple language families and model sizes, including testing whether performance plateaus at certain thresholds or follows diminishing returns patterns.

### Open Question 2
- Question: Why do prefix tuning and (IA)3 methods perform poorly for language adaptation with unstructured text data, and could they be made effective with different approaches?
- Basis in paper: [explicit] The paper states "Prefix tuning hurts the model's capabilities" and that "(IA)3 does not transfer well to language adaptation," but doesn't investigate the underlying reasons for this failure.
- Why unresolved: The paper identifies the poor performance but doesn't conduct detailed analysis of why these methods fail specifically for language adaptation tasks versus their success in other contexts.
- What evidence would resolve it: Detailed error analysis comparing intermediate representations, attention patterns, and gradient flows between successful methods (LoRA, bottleneck adapters) and failed methods (prefix tuning, (IA)3) during language adaptation.

### Open Question 3
- Question: How can language adaptation methods be designed to preserve the model's ability to handle longer contexts while maintaining language-specific improvements?
- Basis in paper: [explicit] The paper observes that "Some adapted models struggle with longer context lengths" and proposes a partial solution of "restricting adapter placement to the top layers," but this is presented as a mitigation rather than a complete solution.
- Why unresolved: The paper identifies the problem and offers one mitigation strategy but doesn't explore the fundamental mechanisms causing context degradation or develop comprehensive solutions.
- What evidence would resolve it: Experiments comparing context handling across different adapter placements, architectures, and training strategies, potentially including methods that explicitly preserve context-handling capabilities or dynamically adjust to context length.

## Limitations

- The study focuses exclusively on a single 1B-parameter model and one target language (Icelandic), limiting generalizability to other model sizes and language families.
- Evaluation is limited to one downstream task (summarization), with no investigation of performance on other language-related tasks like translation or question answering.
- The adaptation data consists of unstructured web text rather than diverse linguistic phenomena that might stress different aspects of language modeling.

## Confidence

- **High confidence**: The finding that increasing trainable parameters improves adaptation performance is well-supported by systematic ablation studies across multiple PEFT methods.
- **Medium confidence**: The claim that LoRA in feed-forward layers outperforms LoRA in attention layers is supported by the main experiments but relies on comparisons with limited rank values.
- **Low confidence**: The mechanism explaining why restricting adaptation to final layers preserves longer-context capabilities is speculative, based on observed correlations rather than controlled experiments.

## Next Checks

1. **Cross-model validation**: Test whether the optimal PEFT configurations (LoRA in feed-forward layers, bottleneck adapters with sufficient parameters) generalize to larger models (e.g., 8B or 70B parameters) and different instruction-tuned architectures beyond Llama-3.2-1B.

2. **Multi-task evaluation**: Assess adapted models on diverse language tasks including translation, question answering, and dialogue to determine whether the observed summarization improvements transfer to other language generation capabilities or create task-specific interference.

3. **Layer-wise analysis**: Conduct controlled experiments that systematically vary which layers are adapted (not just final layers vs. all layers) while measuring both adaptation quality and longer-context performance to better understand the relationship between layer selection and capability preservation.