---
ver: rpa2
title: 'Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental
  Learning'
arxiv_id: '2409.10956'
source_url: https://arxiv.org/abs/2409.10956
tags:
- learning
- incremental
- task
- scenario
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a new challenging IL scenario, Versatile
  Incremental Learning (VIL), where incoming tasks can increase classes, domains,
  or both without prior knowledge. To address VIL, the authors propose ICON, an incremental
  learning framework with two key components: Cluster-based Adaptation Shift Control
  (CAST), which regularizes learning direction by preventing interference with previously
  learned knowledge using clustering of parameter shifts, and Incremental Classifier
  (IC), which dynamically expands output nodes to handle intra-class domain confusion
  and inter-domain class confusion.'
---

# Versatile Incremental Learning: Towards Class and Domain-Agnostic Incremental Learning

## Quick Facts
- **arXiv ID**: 2409.10956
- **Source URL**: https://arxiv.org/abs/2409.10956
- **Reference count**: 40
- **Primary result**: ICON achieves state-of-the-art performance in Versatile Incremental Learning (VIL) across three benchmarks, outperforming existing methods in average accuracy and forgetting metrics.

## Executive Summary
This paper introduces Versatile Incremental Learning (VIL), a new challenging scenario in incremental learning where incoming tasks can increase classes, domains, or both without prior knowledge. The authors propose ICON, an incremental learning framework with two key components: Cluster-based Adaptation Shift Control (CAST) and Incremental Classifier (IC). CAST regularizes learning direction using clustering of parameter shifts to prevent catastrophic forgetting, while IC dynamically expands output nodes to handle intra-class domain confusion. Experiments on iDigits, CORe50, and DomainNet benchmarks show ICON significantly outperforms existing state-of-the-art methods in the VIL setting, achieving higher average accuracy and lower forgetting across all scenarios.

## Method Summary
ICON is an incremental learning framework designed for the VIL scenario. It uses a frozen Vision Transformer backbone with adapter modules for task-specific adaptation. The two main components are CAST, which clusters historical adapter weight shifts and regularizes current learning to be orthogonal to dissimilar previous task shifts, and IC, which dynamically expands classifier output nodes for classes that perform poorly in new domains based on a dynamic threshold. The framework trains with a combined loss function that includes CAST regularization, cross-entropy loss, and knowledge distillation to preserve previous knowledge while learning new information.

## Key Results
- ICON outperforms existing state-of-the-art methods significantly in the VIL setting across all three benchmarks
- The approach achieves higher average accuracy and lower forgetting in class-incremental learning (CIL), domain-incremental learning (DIL), and VIL scenarios
- ICON performs well in cross-domain incremental learning, demonstrating its versatility in handling both class and domain changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAST prevents catastrophic forgetting by regularizing learning direction to avoid collision with dissimilar previous tasks
- Mechanism: Computes current adapter weight shifts, clusters historical shifts using K-Means, and regularizes current shift to be orthogonal to shifts in dissimilar clusters
- Core assumption: Shifts in adapter weights capture learning direction, and orthogonality between dissimilar task shifts preserves knowledge
- Evidence anchors: [abstract], [section 3.2], [corpus] Weak - orthogonal regularization is common in continual learning
- Break condition: If task similarity changes over time, clustering may not align with actual knowledge overlap

### Mechanism 2
- Claim: IC prevents intra-class domain confusion by dynamically expanding output nodes for classes that become difficult in new domains
- Mechanism: Compares accuracy across previously seen domains versus new domain, increments output nodes when accuracy drops below threshold, uses maximum logit for inference
- Core assumption: Different domains for same class produce separable feature distributions that can be modeled by distinct classifier heads
- Evidence anchors: [abstract], [section 3.3], [corpus] Weak - node expansion used in other CIL methods
- Break condition: If domains are too similar, expansion may be unnecessary; if too different, expansion may not separate them sufficiently

### Mechanism 3
- Claim: CAST and IC create synergistic regularization that handles both inter-domain class confusion and intra-class domain confusion
- Mechanism: CAST handles feature adapter level regularization through clustering and orthogonalization, IC handles classifier level confusion through node expansion and distillation
- Core assumption: Feature-level regularization and classifier-level expansion address orthogonal sources of catastrophic forgetting in VIL
- Evidence anchors: [section 3.4], [section 4.3], [corpus] Weak - multi-level regularization is common in continual learning
- Break condition: If one mechanism dominates the other in importance for a given dataset, synergy may not materialize

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - Why needed here: The paper's premise is solving catastrophic forgetting in VIL where both classes and domains increase unpredictably
  - Quick check question: What happens to the weights of a neural network when it's fine-tuned on a new task without any regularization?

- **Concept**: Domain adaptation and domain shift
  - Why needed here: VIL deals with learning new domains for existing classes, requiring understanding of how domain shifts affect model performance
  - Quick check question: How does the feature distribution of the same class change when captured under different environmental conditions?

- **Concept**: Knowledge distillation and regularization
  - Why needed here: Both CAST and IC rely on regularization techniques - CAST through orthogonal regularization and IC through knowledge distillation
  - Quick check question: What is the difference between L2 regularization and knowledge distillation in terms of what they preserve during training?

## Architecture Onboarding

- **Component map**: Vision Transformer backbone (frozen) -> Adapter modules (trainable) -> Classifier head (trainable, dynamic expansion) -> Shift pool (stores historical shifts) -> K-Means clustering module -> Dynamic thresholding module

- **Critical path**:
  1. Initialize with frozen ViT, empty shift pool, and base classifier
  2. For each task: compute adapter shifts, cluster historical shifts, apply CAST regularization
  3. Evaluate classifier performance per class across domains, apply IC expansion where needed
  4. Train with combined CAST + IC loss, update shift pool
  5. Repeat for subsequent tasks

- **Design tradeoffs**:
  - Adapter size vs. memory/computation: Larger adapters provide more capacity but increase memory usage and computation
  - Number of clusters in CAST: Too few clusters may over-regularize, too many may under-regularize
  - Dynamic threshold Î³ in IC: Higher values make expansion more conservative, lower values more aggressive

- **Failure signatures**:
  - Performance plateaus early: Likely over-regularization from CAST or insufficient classifier capacity
  - Performance drops on previously learned domains: Likely under-regularization or forgetting in IC
  - Memory usage grows uncontrollably: Likely too many classifier expansions or shift pool not being pruned

- **First 3 experiments**:
  1. Run baseline fine-tuning on VIL scenario to establish catastrophic forgetting baseline
  2. Test CAST alone (without IC) to measure impact of direction regularization
  3. Test IC alone (without CAST) to measure impact of classifier expansion on domain confusion

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of ICON change when the number of classes or domains per task varies in the VIL scenario?
  - Basis in paper: The paper mentions room for improvement regarding scenarios where the number of classes and domains in a task can change
  - Why unresolved: Current implementation focuses on fixed numbers of classes and domains per task
  - What evidence would resolve it: Experiments showing ICON performance with varying numbers of classes and domains per task in VIL scenario

- **Open Question 2**: What is the impact of different clustering algorithms on the effectiveness of CAST in the VIL scenario?
  - Basis in paper: The paper uses K-Means but does not explore other clustering algorithms or their impact
  - Why unresolved: Only K-Means results are reported without comparison to other methods like DBSCAN or hierarchical clustering
  - What evidence would resolve it: Comparative experiments using different clustering algorithms for CAST

- **Open Question 3**: How does ICON perform in VIL scenarios with a larger number of tasks compared to current benchmarks?
  - Basis in paper: The paper suggests ICON can benefit from longer task sequences but doesn't provide experiments with significantly larger numbers of tasks
  - Why unresolved: Current experiments use limited number of tasks (up to 40 in CORe50)
  - What evidence would resolve it: Experiments with ICON on VIL benchmarks with substantially larger number of tasks (e.g., 100+)

## Limitations
- The paper doesn't provide specific hyperparameter values for adapters, learning rates, and balancing weights, making exact reproduction challenging
- Computational overhead and memory requirements of maintaining shift pool and dynamic node expansion are not discussed
- Scalability to very large numbers of classes and domains, and long-term stability of shift clustering approach are not thoroughly explored

## Confidence
- **High confidence**: The core concept of VIL as a more challenging IL scenario is well-justified and the general approach of using adapter shifts for regularization is sound
- **Medium confidence**: The effectiveness of CAST and IC is demonstrated through experiments, but the exact contribution of each component is difficult to isolate
- **Low confidence**: The scalability of the approach to very large numbers of classes and domains, and the long-term stability of the shift clustering approach

## Next Checks
1. Conduct ablation studies to quantify the individual contribution of CAST vs IC components in different VIL scenarios
2. Test the approach on datasets with more domains per class to evaluate scalability of the dynamic node expansion
3. Measure computational overhead and memory usage as the number of tasks increases, particularly focusing on shift pool maintenance