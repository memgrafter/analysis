---
ver: rpa2
title: 'LLMAuditor: A Framework for Auditing Large Language Models Using Human-in-the-Loop'
arxiv_id: '2402.09346'
source_url: https://arxiv.org/abs/2402.09346
tags:
- probes
- llms
- auditing
- framework
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLMAuditor, a two-phase human-in-the-loop
  framework for auditing large language models (LLMs). In Phase 1, human evaluators
  iteratively refine a prompt template to generate diverse, relevant probes from user
  questions.
---

# LLMAuditor: A Framework for Auditing Large Language Models Using Human-in-the-Loop

## Quick Facts
- arXiv ID: 2402.09346
- Source URL: https://arxiv.org/abs/2402.09346
- Reference count: 40
- Primary result: Human-in-the-loop probe validation improves LLM auditing accuracy (BERT-S from 88.14 to 91.77 for Falcon)

## Executive Summary
LLMAuditor introduces a two-phase framework for auditing large language models using human-in-the-loop validation. The framework generates diverse, relevant probes from user questions in Phase 1, then uses a different LLM to answer these validated probes in Phase 2, enabling systematic auditing. Experiments show significant improvements in semantic similarity (BERT-S from 88.14 to 91.77 for Falcon) and F1-score (43.93 to 60.16 for Falcon) compared to baseline methods. The approach reduces hallucination and provides verifiable, transparent auditing while avoiding circular self-validation.

## Method Summary
The framework uses Mistral 7B to generate 5 diverse probes per question from the TruthfulQA dataset, validated by human assessors using relevance and diversity criteria. After iterative refinement of the prompt template guided by inter-annotator agreement metrics, validated probes are answered by Falcon and Llama 2 models. Results are evaluated using BERT-S score, F1-score, and BLEU metrics to measure semantic similarity and hallucination. The process leverages structured prompt templates with explicit criteria and temperature control to balance probe diversity and consistency.

## Key Results
- BERT-S scores improved from 88.14 to 91.77 for Falcon and 86.57 to 90.07 for Llama 2
- F1-scores increased from 43.93 to 60.16 for Falcon and 36.07 to 52.41 for Llama 2
- The framework reduced hallucination in generated responses compared to baseline methods
- Human-in-the-loop validation achieved inter-annotator agreement thresholds through iterative refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-in-the-loop validation of probes improves semantic alignment between generated probes and original questions.
- Mechanism: Human assessors evaluate probe relevance and diversity using a structured codebook, iteratively refining the prompt template until agreement thresholds are met.
- Core assumption: Human judgment can effectively identify and improve probe quality to reduce semantic drift.
- Evidence anchors:
  - [abstract] Experiments show improved BERT-S scores (88.14 → 91.77 for Falcon) after probe refinement.
  - [section] "The first annotation round yielded 0.61 for diversity and 0.46 for relevance... Throughout four annotation rounds, we made iterative refinements... achieving the threshold for diversity and relevance."
- Break condition: If inter-annotator agreement metrics do not reach thresholds, the probe generation process fails.

### Mechanism 2
- Claim: Using two different LLMs avoids circular self-validation.
- Mechanism: LLM1 generates diverse probes validated by humans; LLM2 answers these probes, exposing inconsistencies.
- Core assumption: Different LLMs will produce different outputs for the same probe, revealing model-specific biases or hallucinations.
- Evidence anchors:
  - [abstract] "This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs."
  - [section] "Using the same LLM to generate such probing questions leads to a circular self-validation loop."
- Break condition: If LLM1 and LLM2 are too similar in training or architecture, the auditing power diminishes.

### Mechanism 3
- Claim: Structured prompt templates with explicit criteria yield higher quality probes.
- Mechanism: Iterative refinement guided by human evaluation and clear criteria headings improves probe diversity and relevance.
- Core assumption: Explicit, structured instructions help LLMs generate probes that meet human-defined quality standards.
- Evidence anchors:
  - [section] "We iteratively removed sections of the instructions that did not improve performance until we arrived at a minimal set of instructions... These instructions produced the highest-quality outputs."
  - [abstract] "The novelty of our research stems from the development of a comprehensive, general-purpose framework that includes a HIL verified prompt template for auditing responses generated by LLMs."
- Break condition: If the structured format is not followed, probe quality degrades.

## Foundational Learning

- Concept: Inter-annotator agreement metrics (Cohen's kappa, Krippendorff's alpha)
  - Why needed here: To quantify consistency between human evaluators during probe validation.
  - Quick check question: If two annotators give identical ratings, what is Cohen's kappa?

- Concept: Semantic similarity evaluation using BERT embeddings
  - Why needed here: To measure how closely generated probes match original questions and how consistently LLMs answer them.
  - Quick check question: What does a BERT-S score of 0.9 indicate about semantic similarity?

- Concept: Prompt engineering and temperature control in LLMs
  - Why needed here: To control probe diversity and relevance during generation.
  - Quick check question: How does lowering temperature affect the randomness of LLM outputs?

## Architecture Onboarding

- Component map: User questions → LLM1 (probe generator) → Human evaluators (validation) → Revised prompt template → LLM2 (audited model) → Response evaluation
- Critical path: Probe generation (LLM1 + human validation) → Probe answering (LLM2) → Performance evaluation (BERT-S, F1, BLEU)
- Design tradeoffs:
  - More human validation rounds → higher probe quality but increased time/cost
  - Lower LLM temperature → more consistent probes but less diversity
  - Different LLM architectures for LLM1/LLM2 → better auditing but potential domain mismatch
- Failure signatures:
  - Low inter-annotator agreement → probe validation fails
  - High semantic dissimilarity between probes and questions → probe generation ineffective
  - Similar LLM1/LLM2 architectures → reduced auditing sensitivity
- First 3 experiments:
  1. Run LLM1 with initial prompt template; evaluate inter-annotator agreement; iterate until thresholds met.
  2. Generate probes using validated template; compare semantic dissimilarity with original questions.
  3. Use LLM2 to answer probes; evaluate hallucination using BERT-S, F1, BLEU against ground truth.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine the optimal number of probes to generate for effective LLM auditing without introducing diminishing returns?
- Basis in paper: [explicit] The paper mentions "While we chose to create 5 probes for our experiments, one could vary that to suite their needs."
- Why unresolved: The paper doesn't provide guidance on the relationship between probe quantity and auditing effectiveness or the point at which additional probes provide minimal benefit.
- What evidence would resolve it: Systematic experiments comparing auditing accuracy and efficiency across different probe quantities (e.g., 3, 5, 10, 15) while measuring both performance gains and computational costs.

### Open Question 2
- Question: How can we automatically validate the effectiveness of generated probes without human-in-the-loop evaluation?
- Basis in paper: [explicit] The paper emphasizes the importance of human validation but notes this may not be scalable, stating "if external assessors are used, extra attention must be paid to have them treated fairly and compensated appropriately for their labor."
- Why unresolved: While HIL improves probe quality, it's resource-intensive and may not be practical for large-scale auditing applications.
- What evidence would resolve it: Development and validation of automated metrics that correlate strongly with human judgments of probe quality, or demonstration that a smaller subset of HIL-validated probes can be used to train automated validators.

### Open Question 3
- Question: How does the proposed auditing framework perform across different types of LLM tasks beyond question answering, such as code generation or creative writing?
- Basis in paper: [inferred] The paper demonstrates the framework on question answering from the TruthfulQA dataset but notes "while we applied our method of auditing to testing an LLM for potential issue of hallucination, the proposed framework is general enough for other forms of auditing."
- Why unresolved: The experiments are limited to factual question answering, and the framework's effectiveness for other task types remains untested.
- What evidence would resolve it: Empirical evaluation of the framework on diverse LLM tasks, measuring auditing accuracy and identifying any necessary adaptations for different domains.

## Limitations
- The prompt template structure and human evaluation criteria are not fully specified, making exact replication challenging.
- The study relies on a small dataset (TruthfulQA) and only three LLM models, limiting generalizability.
- Human-in-the-loop validation introduces scalability concerns and potential subjectivity affecting consistency.

## Confidence
- **High Confidence**: The core mechanism of using different LLMs for probe generation and answer evaluation is well-supported by experimental results showing improved semantic similarity and F1-scores.
- **Medium Confidence**: The effectiveness of human-in-the-loop probe validation is supported by improved metrics but depends heavily on the quality and consistency of human evaluators, which is not fully characterized.
- **Medium Confidence**: The claim about reducing hallucination is supported by the reported metrics, but the relationship between probe refinement and hallucination reduction could be more explicitly demonstrated.

## Next Checks
1. **Probe Template Specification**: Implement the exact prompt template structure described in the paper and verify if it consistently produces diverse, relevant probes across different question types and domains.

2. **Cross-Model Auditing Sensitivity**: Test the auditing framework using LLM pairs with varying degrees of architectural similarity (e.g., same architecture but different training data, different architectures from the same family) to quantify the relationship between model similarity and auditing effectiveness.

3. **Human Evaluator Consistency**: Conduct inter-annotator agreement studies with multiple independent evaluator teams using the same probe validation criteria to assess the robustness and reproducibility of the human-in-the-loop component.