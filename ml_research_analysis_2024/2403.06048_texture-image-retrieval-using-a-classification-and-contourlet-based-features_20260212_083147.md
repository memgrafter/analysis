---
ver: rpa2
title: Texture image retrieval using a classification and contourlet-based features
arxiv_id: '2403.06048'
source_url: https://arxiv.org/abs/2403.06048
tags:
- image
- texture
- images
- retrieval
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a machine learning-enhanced CBIR framework for
  texture images using a novel variant of the Redundant Contourlet transform (RCT-Plus)
  for richer directional feature extraction, combined with statistical modeling via
  generalized Gaussian distribution. Images are first classified using kNN or SVM,
  then ranked within the predicted class using KLD or Euclidean distance.
---

# Texture image retrieval using a classification and contourlet-based features

## Quick Facts
- arXiv ID: 2403.06048
- Source URL: https://arxiv.org/abs/2403.06048
- Authors: Asal Rouhafzay; Nadia Baaziz; Mohand Said Allili
- Reference count: 16
- Primary result: Proposes ML-CBIR framework using RCT-Plus and GGD modeling, achieving up to 99.86% AR on VisTex-40 and 97.96% on Kylberg-27.

## Executive Summary
This paper presents a machine learning-enhanced content-based image retrieval (CBIR) framework for texture images that combines a novel Redundant Contourlet transform variant (RCT-Plus) with statistical modeling via generalized Gaussian distribution (GGD). The method first classifies query images using kNN or SVM classifiers, then ranks images within the predicted class using Kullback-Leibler Divergence (KLD) or Euclidean distance. On benchmark datasets VisTex-40 and Kylberg-27, the approach achieves state-of-the-art retrieval performance, outperforming traditional CBIR by up to 32.64% and surpassing the current state-of-the-art LDPVBP by 16.52%.

## Method Summary
The proposed ML-CBIR framework operates in two phases: offline training and online retrieval. In the offline phase, RCT-Plus decomposition (3 scales, 8 directions) is applied to all database images, and subband coefficients are modeled using GGD or energy statistics to create feature vectors. These features are used to train kNN (k=1) or SVM (linear kernel) classifiers via 10-fold cross-validation. During online retrieval, a query image is first classified to predict its texture class, then images from that class are ranked using KLD (for GGD features) or Euclidean distance (for energy features), and the top-ranked images are returned as results.

## Key Results
- Achieves 99.86% average retrieval rate on VisTex-40 dataset
- Achieves 97.96% average retrieval rate on Kylberg-27 dataset
- Outperforms traditional CBIR methods by up to 32.64% and state-of-the-art LDPVBP by 16.52%
- Reduces computational cost by avoiding full database comparisons through classification-first approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Classification first, then similarity ranking within the predicted class reduces the search space and computational load compared to full-database comparisons.
- **Mechanism**: The ML-CBIR system splits retrieval into two phases: (1) classify the query into one of the known texture classes using kNN or SVM, and (2) only compare the query to images within that class using KLD or Euclidean distance. This avoids comparing the query to all images in the database.
- **Core assumption**: Texture classes in the database are well-separated and the classifier can reliably predict the correct class for most queries.
- **Evidence anchors**:
  - [abstract] "A query is then first classified to select the best texture class after which the retained class images are ranked to select top ones."
  - [section 2.2] "During the Online phase, the search and retrieval process to any given query image is performed through query image classification. Firstly, the trained classifier is applied to the given query feature vector in order to predict its class membership (class label)."
  - [corpus] **Missing direct evidence**: The corpus does not discuss the two-phase classification-then-ranking mechanism or computational savings; this is an inference from the paper's description.
- **Break condition**: If texture classes are highly overlapping or the classifier has low accuracy, the system will misclassify queries, leading to irrelevant retrievals and negating computational savings.

### Mechanism 2
- **Claim**: Using RCT-Plus transform with richer directional selectivity provides more discriminative texture features than traditional contourlet or wavelet transforms.
- **Mechanism**: RCT-Plus is a variant of the Redundant Contourlet transform that adds augmented directional selectivity, redundancy, and Gaussian filtering, yielding more detailed directional sub-bands. These sub-bands are modeled using GGD (or energy), producing compact feature vectors (e.g., 50 components) that capture fine texture details.
- **Core assumption**: The additional directional information extracted by RCT-Plus is relevant and enhances discrimination between texture classes more than previous methods.
- **Evidence anchors**:
  - [section 3.1] "RCT-Plus is a new variant of the contourlet transform [13] in which augmented directional selectivity, redundancy and Gaussian filtering for multiscale decomposition are meant to enhance texture characterization and extract a richer directional information in the image."
  - [section 4.B] "We also decided to set the size of the RCT-Plus decomposition at 3 scale levels and 8 directions for all experiments, because this choice of parameters allowed us to achieve the best texture retrieval results in our previous work [5]."
  - [corpus] **Weak evidence**: Corpus papers do not discuss RCT-Plus specifically; this claim is supported only by the paper's own results and prior work.
- **Break condition**: If the extra directional information is redundant or noisy for certain textures, it may not improve discrimination and could even hurt performance.

### Mechanism 3
- **Claim**: GGD modeling of RCT-Plus sub-bands combined with symmetric Kullback-Leibler Divergence (KLD) provides a more accurate similarity measure than simple Euclidean distance on raw features.
- **Mechanism**: Each RCT-Plus sub-band is modeled as a GGD; the KLD between the query's and target's GGDs is computed for all sub-bands and summed to give an overall distance. This captures the distributional differences between textures more effectively than raw feature differences.
- **Core assumption**: Texture sub-band distributions are well-modeled by GGD and KLD accurately reflects perceptual similarity for textures.
- **Evidence anchors**:
  - [section 3.2] "The symmetric Kullback-Leibler Divergence (KLD) is a statistical measure of how different two probability distributions over the same event space are... The summation of all KLD values is considered as the distance measure between two images."
  - [section 4.B] "In most cases, kNN and SVM learning models were more accurate than Decision trees and Discriminant analysis models. Moreover, in most cases, kNN and SVM achieved high Accuracy levels when using GGD1 or GGD2 feature extraction methods."
  - [corpus] **Missing evidence**: The corpus does not contain studies comparing KLD to Euclidean distance for texture retrieval; this is inferred from the paper's experimental setup.
- **Break condition**: If the GGD model does not fit the sub-band distributions well (e.g., heavy tails, multimodal), KLD will be less meaningful and Euclidean distance might perform better.

## Foundational Learning

- **Concept**: Supervised machine learning for classification (kNN, SVM).
  - **Why needed here**: The CBIR system needs to first assign a query to a known texture class before ranking within that class; this requires a trained classifier that can generalize from labeled training data.
  - **Quick check question**: What is the difference between kNN and SVM in terms of decision boundary and training complexity?

- **Concept**: Statistical modeling of subband distributions (GGD, energy).
  - **Why needed here**: Texture images have structured variations at multiple scales and orientations; modeling the distribution of these subbands captures texture patterns better than raw pixel values.
  - **Quick check question**: How does the GGD's shape parameter β affect the tail behavior of the modeled distribution?

- **Concept**: Similarity metrics (KLD, Euclidean distance).
  - **Why needed here**: To rank images within the predicted class, we need a distance measure that reflects perceptual similarity for textures; KLD is a probabilistic divergence, while ED is a simple norm.
  - **Quick check question**: When would KLD be preferred over Euclidean distance for comparing texture feature vectors?

## Architecture Onboarding

- **Component map**: Feature extraction -> RCT-Plus decomposition -> GGD/energy modeling -> Classifier training -> Query classification -> KLD/ED ranking -> TopN output
- **Critical path**: Query → Feature extraction → Classification → Ranking → Output. Any failure in early stages (e.g., classification) propagates downstream.
- **Design tradeoffs**:
  - Feature extraction complexity vs. retrieval accuracy (RCT-Plus is more complex but yields better features).
  - Classifier choice (kNN is simple and fast for small k; SVM can be more accurate but requires kernel selection).
  - Similarity metric (KLD is more computationally intensive but may capture distributional differences better than ED).
- **Failure signatures**:
  - High miss-classification rate → Many queries get irrelevant TopN results.
  - Low retrieval rate even within correct class → Feature extraction or similarity metric inadequate.
  - Long query response time → Feature extraction or ranking too slow.
- **First 3 experiments**:
  1. Train and test the classifier alone (no ranking) on a small subset of VisTex-40 to verify accuracy and class separability.
  2. Replace RCT-Plus with a standard contourlet transform and compare retrieval rates to isolate the impact of the richer directional features.
  3. Swap KLD for Euclidean distance in the ranking phase to assess whether the distributional modeling provides measurable gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the ML-CBIR framework scale with larger databases (e.g., millions of images) compared to traditional CBIR methods?
- Basis in paper: [inferred] The paper mentions computational efficiency of the Online search phase but does not test on large-scale databases.
- Why unresolved: The experiments were conducted on databases of moderate size (640 and 1080 images). Scaling effects on query classification accuracy, retrieval speed, and memory usage were not evaluated.
- What evidence would resolve it: Experiments measuring classification accuracy, retrieval time, and memory usage on databases with millions of images, comparing ML-CBIR to traditional CBIR.

### Open Question 2
- Question: How robust is the ML-CBIR framework to noise, compression artifacts, and other common image degradations?
- Basis in paper: [inferred] The paper evaluates retrieval performance on clean datasets but does not test robustness to image quality issues.
- Why unresolved: The retrieval experiments used pristine images from standard datasets. The impact of image quality degradation on feature extraction, classification, and retrieval accuracy was not studied.
- What evidence would resolve it: Experiments evaluating retrieval accuracy on noisy, compressed, or otherwise degraded versions of the test images, comparing ML-CBIR to traditional CBIR.

### Open Question 3
- Question: What is the impact of using multi-class membership prediction instead of single-class prediction on retrieval performance?
- Basis in paper: [explicit] The paper mentions extending the search space by considering multi-class membership prediction as future work.
- Why unresolved: The current framework uses single-class prediction. The potential benefits and trade-offs of multi-class prediction (e.g., improved recall vs. increased computational cost) were not investigated.
- What evidence would resolve it: Experiments comparing retrieval performance (e.g., average retrieval rate, precision, recall) and computational cost of single-class vs. multi-class prediction variants of the ML-CBIR framework.

## Limitations

- The claim of 99.86% retrieval rate is based on a single experimental setup without ablation studies to isolate component contributions or statistical significance testing.
- RCT-Plus transform implementation details are not specified, making exact reproduction difficult and claims of superiority over traditional contourlet transforms lack direct comparative evidence.
- The paper does not report classification accuracy, which is critical for evaluating the reliability of the two-phase classification-then-ranking approach.

## Confidence

- **High confidence**: The general framework of ML-enhanced CBIR (classification + ranking) is well-established and correctly described. The datasets (VisTex-40, Kylberg-27) are public and their use is verifiable.
- **Medium confidence**: The retrieval rates (99.86% on VisTex-40, 97.96% on Kylberg-27) are reported, but without statistical tests or ablation studies, it's difficult to attribute these results solely to the proposed method's innovations.
- **Low confidence**: The specific implementation and advantages of RCT-Plus are not detailed enough for independent verification. Claims of superiority over LDPVBP (16.52% improvement) and traditional CBIR (32.64% improvement) are not supported by direct, controlled comparisons in the paper.

## Next Checks

1. **Classification accuracy audit**: Re-run the experiments and report the classification accuracy (not just retrieval rate) for kNN and SVM on both datasets. If accuracy is below 95%, the two-phase approach's reliability is questionable.

2. **Ablation study on RCT-Plus**: Replace RCT-Plus with a standard contourlet transform (same number of scales and directions) and compare retrieval rates. This isolates whether the augmented directional selectivity provides measurable gains.

3. **Statistical significance testing**: Perform paired t-tests or Wilcoxon signed-rank tests between the proposed method and the best baseline (LDPVBP) across multiple runs or cross-validation folds to determine if the 16.52% improvement is statistically significant.