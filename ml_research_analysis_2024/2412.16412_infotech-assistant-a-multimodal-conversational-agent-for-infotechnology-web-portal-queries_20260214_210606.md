---
ver: rpa2
title: 'InfoTech Assistant: A Multimodal Conversational Agent for InfoTechnology Web
  Portal Queries'
arxiv_id: '2412.16412'
source_url: https://arxiv.org/abs/2412.16412
tags:
- assistant
- data
- infotech
- responses
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The InfoTech Assistant is a multimodal chatbot for bridge evaluation
  and infrastructure technology queries, leveraging web scraping, RAG, and large language
  models to provide accurate, contextually relevant responses. It integrates textual
  and visual data from the InfoTechnology web portal, using a structured JSON database
  and a Flask backend connected to Llama 3.1 via LLM Studio.
---

# InfoTech Assistant: A Multimodal Conversational Agent for InfoTechnology Web Portal Queries

## Quick Facts
- **arXiv ID:** 2412.16412
- **Source URL:** https://arxiv.org/abs/2412.16412
- **Reference count:** 40
- **Primary result:** Multimodal chatbot leveraging web scraping, RAG, and large language models to provide accurate, contextually relevant responses for infrastructure technology queries with approximately 95% accuracy on domain-specific tasks

## Executive Summary
The InfoTech Assistant is a multimodal conversational agent designed to handle bridge evaluation and infrastructure technology queries by integrating textual and visual data from the InfoTechnology web portal. The system employs web scraping to collect content, stores it in a structured JSON database, and uses retrieval-augmented generation (RAG) with Llama 3.1 via LLM Studio to deliver accurate, contextually relevant responses. A Flask backend provides the interface, enabling users to interact through both text and images. The assistant demonstrates high accuracy (~95%) on domain-specific tasks and successfully handles complex multimodal queries, making it a dependable tool for infrastructure professionals seeking technical information.

## Method Summary
The InfoTech Assistant combines web scraping, structured data storage, and multimodal RAG to enable conversational queries about infrastructure technology. Content is scraped from the InfoTechnology web portal and stored in a JSON database, which is then indexed for efficient retrieval. The system uses LLM Studio to interface with Llama 3.1, providing both textual and visual information in responses. A Flask backend manages user interactions and orchestrates the multimodal pipeline. The evaluation methodology relies on automated metrics, including cosine similarity scores, to assess response quality, with results showing approximately 95% accuracy on domain-specific tasks.

## Key Results
- Achieved approximately 95% accuracy on domain-specific infrastructure technology tasks
- High cosine similarity scores confirm response quality for multimodal queries
- Successfully handles complex queries involving both textual and visual data from the InfoTechnology portal

## Why This Works (Mechanism)
The system works by integrating real-time web scraping with a structured JSON database and RAG architecture. Web scraping enables dynamic content collection from the InfoTechnology portal, while the JSON database ensures efficient retrieval and indexing. The multimodal pipeline, powered by Llama 3.1 via LLM Studio, processes both text and image inputs, allowing the assistant to deliver contextually relevant responses. The Flask backend orchestrates the interaction flow, ensuring seamless user experience. This architecture allows the system to bridge the gap between static documentation and dynamic, user-driven queries in the infrastructure domain.

## Foundational Learning
- **Web Scraping**: Needed to dynamically collect up-to-date content from the InfoTechnology portal; quick check: verify scraped content matches portal structure and content
- **Structured JSON Database**: Required for efficient storage and retrieval of multimodal data; quick check: ensure database schema supports both text and image metadata
- **RAG (Retrieval-Augmented Generation)**: Essential for combining retrieved context with generative models to produce accurate answers; quick check: validate retrieved context relevance to user queries
- **Multimodal Processing**: Enables handling of both text and image inputs for comprehensive query support; quick check: test system with mixed text-image queries
- **Flask Backend**: Provides the web interface and request orchestration; quick check: confirm API endpoints respond correctly to multimodal inputs
- **Cosine Similarity for QA**: Used as an automated metric to assess response quality; quick check: compare similarity scores against human-annotated ground truth

## Architecture Onboarding

**Component Map:**
User Interface -> Flask Backend -> Web Scraper -> JSON Database -> RAG Pipeline (LLM Studio + Llama 3.1) -> Response Generator

**Critical Path:**
User query → Flask backend → Web scraper (if new content needed) → JSON database retrieval → RAG pipeline → LLM Studio → Llama 3.1 → Response generation → User output

**Design Tradeoffs:**
- Web scraping vs. static data: scraping allows real-time updates but introduces fragility if portal structure changes
- JSON database vs. other storage: JSON is simple and human-readable but may not scale as well as specialized databases
- Cosine similarity vs. human evaluation: automated metric is fast but lacks qualitative nuance

**Failure Signatures:**
- Web scraping failures: missing or malformed data if portal structure changes
- Database retrieval issues: incomplete or outdated information due to synchronization lags
- RAG inaccuracies: irrelevant context retrieval leading to incorrect responses
- LLM response errors: hallucinations or off-topic answers due to model limitations

**3 First Experiments:**
1. Validate end-to-end query flow with a simple text-only question to confirm all components communicate correctly
2. Test multimodal query handling by submitting a query with both text and image input
3. Perform a controlled evaluation of response accuracy using a curated set of known answers

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology and test datasets lack full detail, making it difficult to assess generalizability of the ~95% accuracy
- Web scraping introduces potential reliability issues due to external website structure changes
- High cosine similarity scores lack external validation and comparison to human-annotated ground truth
- Multimodal capabilities have not been thoroughly tested for edge cases involving complex visual queries or degraded image quality

## Confidence
- **High confidence**: The system architecture and technical implementation details are clearly described and follow established best practices in multimodal RAG systems
- **Medium confidence**: The reported accuracy and performance metrics, while promising, lack sufficient methodological detail for full verification
- **Low confidence**: The generalizability of results to real-world deployment scenarios and the long-term sustainability of the web scraping approach

## Next Checks
1. Conduct a user study with infrastructure professionals to evaluate the system's practical utility and identify usability issues not captured by automated metrics
2. Perform stress testing with adversarial queries and degraded image inputs to assess robustness under challenging conditions
3. Implement a continuous monitoring system to track performance degradation when the source web portal undergoes structural changes, and develop automated adaptation mechanisms