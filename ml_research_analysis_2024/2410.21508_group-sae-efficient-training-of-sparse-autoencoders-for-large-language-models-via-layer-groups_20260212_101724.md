---
ver: rpa2
title: 'Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models
  via Layer Groups'
arxiv_id: '2410.21508'
source_url: https://arxiv.org/abs/2410.21508
tags:
- saes
- layers
- training
- groups
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Group-SAE, a method to reduce the computational
  overhead of training sparse autoencoders (SAEs) for large language models (LLMs)
  by grouping similar layers and training one SAE per group. The approach leverages
  the similarity of residual stream representations between contiguous layers, using
  angular distance to cluster layers and train a single SAE for each group.
---

# Group-SAE: Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups
## Quick Facts
- **arXiv ID:** 2410.21508
- **Source URL:** https://arxiv.org/abs/2410.21508
- **Reference count:** 40
- **Primary result:** Reduces SAE training costs by up to 50% with minimal impact on reconstruction quality and interpretability

## Executive Summary
This paper introduces Group-SAE, a method to reduce the computational overhead of training sparse autoencoders (SAEs) for large language models (LLMs) by grouping similar layers and training one SAE per group. The approach leverages the similarity of residual stream representations between contiguous layers, using angular distance to cluster layers and train a single SAE for each group. To select the optimal number of groups, the authors propose the Average Maximum Angular Distance (AMAD), an empirical metric that balances efficiency and reconstruction quality. Experiments on Pythia models show that Group-SAE achieves up to 50% reduction in training costs with minimal impact on reconstruction quality and comparable downstream task performance. Interpretability scores remain on par with or slightly better than baseline SAEs. The method provides an efficient and scalable strategy for training SAEs in modern LLMs.

## Method Summary
Group-SAE addresses the computational burden of training sparse autoencoders for LLMs by exploiting layer similarity. The method clusters contiguous layers based on angular distance of their residual stream representations, then trains a single SAE for each cluster group rather than individual SAEs per layer. The clustering process uses cosine similarity to measure representation similarity between layers, grouping together those with high angular proximity. To determine the optimal number of groups, the authors introduce AMAD (Average Maximum Angular Distance), which quantifies the trade-off between efficiency gains and reconstruction quality degradation. During training, each SAE is optimized to reconstruct the aggregated activations from all layers within its assigned group, significantly reducing the total number of SAEs that need to be trained while maintaining performance.

## Key Results
- Achieves up to 50% reduction in SAE training costs compared to per-layer training
- Maintains comparable reconstruction quality with minimal degradation in downstream task performance
- Achieves interpretability scores on par with or slightly better than baseline SAEs

## Why This Works (Mechanism)
Group-SAE exploits the observation that residual stream representations between contiguous layers in LLMs often exhibit high similarity. By grouping these similar layers and training shared SAEs, the method reduces redundancy in representation learning while preserving the essential features needed for reconstruction and interpretability. The angular distance metric effectively captures the geometric similarity between layer representations, ensuring that grouped layers have comparable activation patterns. This similarity allows a single SAE to effectively model multiple layers' features, reducing the total parameter count and training computational requirements. The AMAD metric provides an empirical balance between the number of groups (affecting efficiency) and the reconstruction quality, ensuring that the grouping strategy maintains sufficient model capacity for accurate feature extraction.

## Foundational Learning
**Angular Distance:** Measures the angle between two vectors in high-dimensional space, used here to quantify similarity between layer representations. Why needed: Provides a scale-invariant measure of representation similarity that captures geometric relationships. Quick check: Verify cosine similarity between layer activations remains stable across training epochs.

**Sparse Autoencoder (SAE):** Neural network architecture with bottleneck structure that learns compressed representations with sparse activations. Why needed: Enables feature extraction and interpretability in LLMs by identifying meaningful activation patterns. Quick check: Confirm reconstruction loss and sparsity penalty balance during training.

**Residual Stream:** The continuous flow of activations through transformer layers, carrying information forward through the network. Why needed: Understanding residual stream similarity is crucial for determining which layers can share SAEs. Quick check: Visualize activation distributions across layers to identify similarity patterns.

**Cosine Similarity:** Measures the cosine of the angle between two vectors, ranging from -1 to 1. Why needed: Provides the mathematical foundation for angular distance calculations between layer representations. Quick check: Compute pairwise similarity matrices to visualize layer clustering structure.

## Architecture Onboarding
**Component Map:** Input Activations -> Layer Grouping (Angular Distance) -> SAE Training (Grouped) -> Feature Extraction -> Interpretability Analysis
**Critical Path:** Layer representations → Angular distance computation → Clustering → SAE training → Feature analysis
**Design Tradeoffs:** Fewer groups = higher efficiency but potential reconstruction quality loss; more groups = better quality but reduced efficiency gains
**Failure Signatures:** Excessive reconstruction error, degraded downstream performance, inconsistent interpretability scores across groups
**First Experiments:** 1) Compute angular distance matrix for layer representations, 2) Test AMAD metric sensitivity to group count, 3) Validate reconstruction quality degradation with increasing group size

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
The AMAD metric lacks theoretical grounding for why this particular angular distance-based measure optimally balances efficiency and reconstruction quality. The angular distance similarity assumption between contiguous layers may not hold for all LLM architectures or training regimes, particularly for models with non-standard residual connections or attention patterns. The evaluation focuses primarily on Pythia models, leaving uncertainty about generalization to other architectures.

## Confidence
**High confidence in:** Layer clustering effectiveness and training cost reduction claims, as these are directly measurable and well-supported by experimental results.
**Medium confidence in:** Downstream task performance comparisons and interpretability score claims, due to limited task diversity and potential domain-specific variations.
**Low confidence in:** The theoretical justification for AMAD as an optimal metric and the universal applicability of angular distance similarity assumptions across different model architectures.

## Next Checks
1. Test Group-SAE on diverse LLM architectures including decoder-only, encoder-decoder, and models with different attention mechanisms to validate generalization claims.
2. Conduct ablation studies systematically varying the number of groups beyond what AMAD suggests to better understand the efficiency-reconstruction quality trade-off curve.
3. Evaluate on a broader suite of downstream tasks including code generation, mathematical reasoning, and multilingual benchmarks to comprehensively assess performance impacts.