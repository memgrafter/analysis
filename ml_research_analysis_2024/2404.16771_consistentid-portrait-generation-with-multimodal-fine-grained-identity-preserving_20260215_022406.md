---
ver: rpa2
title: 'ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving'
arxiv_id: '2404.16771'
source_url: https://arxiv.org/abs/2404.16771
tags:
- facial
- consistentid
- fine-grained
- image
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ConsistentID, a method for identity-preserving
  portrait generation that uses multimodal fine-grained facial prompts with only a
  single reference image. The approach combines a multimodal facial prompt generator
  that integrates facial features, descriptions, and overall context with an ID-preservation
  network optimized by facial attention localization.
---

# ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving

## Quick Facts
- arXiv ID: 2404.16771
- Source URL: https://arxiv.org/abs/2404.16771
- Reference count: 40
- Primary result: Identity-preserving portrait generation using single reference image with multimodal fine-grained facial prompts

## Executive Summary
ConsistentID introduces a novel approach for identity-preserving portrait generation that leverages multimodal fine-grained facial prompts. The method combines detailed facial descriptions generated by LLaVA1.5 with local facial features extracted via BiSeNet, fused through a facial encoder to create comprehensive identity representations. An ID-preservation network with facial attention localization strategy ensures consistent identity across generated images while maintaining high quality and inference speed.

## Method Summary
The method consists of a multimodal facial prompt generator that integrates facial features, descriptions, and context using LLaVA1.5 for detailed facial descriptions and BiSeNet for facial region segmentation. These fine-grained features are combined with overall facial ID features from CLIP and InsightFace through a projection module. The ID-preservation network uses modified cross-attention with an Lloc loss that aligns attention maps with facial segmentation masks to prevent identity blending across regions. Training uses the Adam optimizer on 8 NVIDIA 3090 GPUs with a learning rate of 1 × 10⁻⁴.

## Key Results
- Achieves superior identity fidelity compared to existing methods using only a single reference image
- Maintains high-quality generation with fast inference speeds
- Introduces new FGIS metric for evaluating identity consistency
- Trained on FGID dataset with over 500,000 fine-grained facial images

## Why This Works (Mechanism)

### Mechanism 1: Fine-grained Multimodal Feature Integration
Detailed facial descriptions and local facial features contain more identity-relevant information than coarse global features. LLaVA1.5 generates detailed facial descriptions while BiSeNet extracts facial region images, which are fused with overall face features in a facial encoder to create multimodal facial prompts. Break condition: If facial region segmentation fails or descriptions are too generic, the fine-grained advantage disappears.

### Mechanism 2: Facial Attention Localization
Cross-attention maps without localization will mix identity information across regions, causing ID inconsistency. An Lloc loss aligns cross-attention maps with segmentation masks of facial regions, ensuring each region's attention stays localized. Break condition: If segmentation masks are inaccurate or attention maps become too sparse, localization fails.

### Mechanism 3: Complementary Overall and Fine-grained ID Features
Overall facial ID preserves global identity while fine-grained features preserve local details; combining both captures complementary information. CLIP-based overall facial ID features are combined with fine-grained multimodal features through a projection module and facial encoder. Break condition: If projection module fails to align features or if one type dominates during fusion, the complementary benefit is lost.

## Foundational Learning

- Concept: Cross-attention mechanism in diffusion models
  - Why needed here: ConsistentID modifies how cross-attention maps are used for ID preservation by localizing them to facial regions
  - Quick check question: How does cross-attention work in standard diffusion models, and what changes when you add localization constraints?

- Concept: Multimodal fusion strategies
  - Why needed here: The facial encoder fuses visual facial features with textual descriptions using self-attention and token replacement
  - Quick check question: What are different ways to fuse visual and textual embeddings, and why might token replacement be effective here?

- Concept: Facial segmentation and landmark detection
  - Why needed here: BiSeNet provides facial region masks that guide attention localization and feature extraction
  - Quick check question: How does BiSeNet work, and what are the challenges in obtaining accurate facial region segmentation?

## Architecture Onboarding

- Component map: Reference image → Fine-grained feature extraction → Multimodal fusion → ID preservation → Generated image
- Critical path: Single reference image flows through multimodal prompt generation, facial attention localization, and UNet-based generation
- Design tradeoffs: LLaVA1.5 adds computational overhead but provides richer descriptions; localization loss adds training complexity but improves ID consistency; combining overall and fine-grained features increases model complexity but captures complementary information
- Failure signatures: Poor ID consistency (check segmentation quality and attention localization), low image quality (check base model capability and training stability), slow inference (profile each module's inference time)
- First 3 experiments: 1) Ablation: Remove facial attention localization and compare ID consistency metrics, 2) Ablation: Use only overall facial ID vs. only fine-grained ID vs. both, compare metrics, 3) Ablation: Remove LLaVA1.5 descriptions and use simple prompts, compare generation quality

## Open Questions the Paper Calls Out

- How does the model perform on facial identities outside the 15 types included in the FGID dataset? The paper acknowledges the dataset may not fully represent all possible facial identities but lacks quantitative results on unseen identities.

- What is the impact of using different segmentation models on the quality of generated images? The paper uses BiSeNet but does not compare performance with other segmentation models.

- How does the model handle extreme variations in facial expressions or poses? The paper discusses fine-grained control but lacks experiments on extreme facial variations.

## Limitations

- Lack of detailed architectural specifications for critical components like the multimodal facial prompt generator and ID-preservation network
- Heavy dependence on proprietary FGID dataset with over 500,000 images, raising reproducibility concerns
- High computational requirements (8 NVIDIA 3090 GPUs) limiting accessibility for many research groups

## Confidence

- High Confidence: The general framework of combining multimodal features for identity preservation is technically sound and aligns with established practices in personalized generation
- Medium Confidence: The specific implementation of facial attention localization and its claimed benefits are plausible but lack strong empirical support
- Low Confidence: The newly introduced FGIS metric and its superiority over existing evaluation methods have not been validated by independent research

## Next Checks

1. Ablation Study on Attention Localization: Remove the facial attention localization mechanism (Lloc loss) and compare identity consistency metrics (FaceSim, FGIS) against the full ConsistentID model

2. Dataset Dependency Analysis: Test the model's performance using publicly available datasets (e.g., FFHQ, CelebA) instead of the proprietary FGID dataset

3. Independent Metric Validation: Implement the FGIS metric independently and apply it to existing identity preservation methods to establish whether it provides meaningful differentiation beyond established metrics like FaceSim and CLIP-based similarity scores