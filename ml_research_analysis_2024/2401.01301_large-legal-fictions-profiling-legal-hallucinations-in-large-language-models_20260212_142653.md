---
ver: rpa2
title: 'Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models'
arxiv_id: '2401.01301'
source_url: https://arxiv.org/abs/2401.01301
tags:
- case
- legal
- llms
- hallucination
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically profiles legal hallucinations in large
  language models (LLMs), providing the first empirical evidence of these phenomena
  across multiple dimensions of the legal system. The authors develop a typology of
  legal hallucinations and create a set of tasks ranging from simple case existence
  checks to complex doctrinal reasoning queries.
---

# Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models

## Quick Facts
- arXiv ID: 2401.01301
- Source URL: https://arxiv.org/abs/2401.01301
- Reference count: 40
- Primary result: Legal hallucinations occur between 58% (ChatGPT 4) and 88% (Llama 2) of the time across multiple legal tasks

## Executive Summary
This paper provides the first systematic empirical profiling of legal hallucinations in large language models across multiple dimensions of the legal system. Using four popular LLMs (ChatGPT 4, ChatGPT 3.5, PaLM 2, and Llama 2), the authors evaluate 14 legal knowledge tasks ranging from simple case existence checks to complex doctrinal reasoning. They find alarmingly high hallucination rates across all models, with performance varying significantly by court hierarchy, jurisdiction, case prominence, and year. The study reveals that models perform best on Supreme Court cases but worst on district court cases, and identifies additional failure modes including contra-factual bias and poor calibration. These findings raise serious concerns about rapid integration of LLMs into legal tasks, particularly for under-resourced litigants who could benefit most from these tools.

## Method Summary
The authors construct a test dataset of federal court cases (USDC, USCOA, SCOTUS) sampled from published volumes and merge this with metadata from multiple sources (Caselaw Access Project, Supreme Court Database, Appeals Courts Database, Library of Congress, and Shepard's Citations). They query four LLMs with 14 legal knowledge tasks using both reference-based queries (with ground truth metadata) and reference-free queries (detecting self-contradictions). For reference-based tasks, they use temperature 0 for deterministic responses; for reference-free tasks, they use temperature 1 to detect contradictions. They evaluate responses for hallucinations using ground truth metadata or GPT-4-based contradiction detection, then calculate hallucination rates, calibration error, and contra-factual bias across different task types and model configurations.

## Key Results
- Legal hallucinations occur between 58% (ChatGPT 4) and 88% (Llama 2) of the time when models are asked specific questions about federal court cases
- Hallucinations vary by court hierarchy, with models performing best on SCOTUS cases and worst on USDC cases
- Two additional failure modes identified: contra-factual bias (accepting incorrect legal premises) and poor model calibration (inability to predict hallucinations)
- Models struggle most with complex legal tasks including factual background, procedural posture, subsequent history, core legal question, and central holding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Legal hallucinations are more frequent when models are asked about lower-level courts due to reduced prominence and training data coverage.
- Mechanism: The model's learned probability distribution over legal entities reflects training corpus frequencies, which favor high-profile SCOTUS cases over less salient district court cases.
- Core assumption: Training corpus contains disproportionately more references to prominent legal cases.
- Evidence anchors: Abstract finding that hallucinations vary by court hierarchy with best performance on SCOTUS and worst on USDC cases.

### Mechanism 2
- Claim: Models accept incorrect legal premises when prompts contain factual errors, leading to contra-factual bias.
- Mechanism: The model's sycophantic tendency causes it to agree with the user's premise rather than correct it, prioritizing prompt fidelity over factual accuracy.
- Core assumption: The model is tuned to minimize prompt hallucinations at the expense of factual hallucinations.
- Evidence anchors: Abstract finding that LLMs fail to correct incorrect legal assumptions in contra-factual question setups.

### Mechanism 3
- Claim: Model calibration is poor for legal queries, with overconfidence in hallucinated responses.
- Mechanism: The model's confidence scores are poorly correlated with actual accuracy, leading users to trust incorrect answers.
- Core assumption: Calibration error is consistent across legal tasks and not task-specific.
- Evidence anchors: Abstract finding that LLMs cannot always predict when they are producing legal hallucinations, with Llama 2 showing particularly poor calibration (ECE = 0.421).

## Foundational Learning

- Concept: Token-level probability distributions in LLMs
  - Why needed here: Understanding how models generate responses and why hallucinations occur
  - Quick check question: If a model has a flat probability distribution over next tokens, what is the likelihood of hallucination?

- Concept: Temperature parameter effects on stochasticity
  - Why needed here: Explains why reference-free querying uses temperature > 0 to detect contradictions
  - Quick check question: How does increasing temperature from 0 to 1 affect the determinism of model outputs?

- Concept: Expected calibration error (ECE) calculation
  - Why needed here: Evaluating whether models know when they are hallucinating
  - Quick check question: What does an ECE of 0.421 indicate about a model's calibration?

## Architecture Onboarding

- Component map: Query generator -> LLM query -> Reference oracle (for reference-based) or Contradiction detector (for reference-free) -> Result aggregation
- Critical path: Query generation → Model response → Validation/correction detection → Result aggregation
- Design tradeoffs:
  - Reference-based vs reference-free: Direct accuracy measurement vs lower-bound estimates
  - Temperature settings: 0 for deterministic responses vs 1 for contradiction detection
  - Prompt engineering: Zero-shot vs few-shot for different evaluation goals
- Failure signatures:
  - High abstention rates indicate model uncertainty or safety tuning
  - Systematic biases toward certain justices or jurisdictions indicate training corpus skew
  - Poor calibration indicates confidence-accuracy misalignment
- First 3 experiments:
  1. Test model responses to fake case existence queries to validate Existence task results
  2. Compare calibration before and after temperature scaling to assess ECE improvements
  3. Vary prompt examples in few-shot settings to test in-context learning effects on hallucination rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between an LLM's temperature parameter and its hallucination rate in legal contexts, and how can this relationship be mathematically modeled?
- Basis in paper: The paper discusses how increasing temperature can induce more hallucinated responses, but does not provide a formal model of this relationship.
- Why unresolved: While the paper demonstrates that higher temperatures lead to more hallucinations, it does not quantify this relationship or develop a predictive model.
- What evidence would resolve it: Controlled experiments varying temperature across multiple legal tasks while measuring hallucination rates, followed by statistical modeling of the temperature-hallucination relationship.

### Open Question 2
- Question: How do retrieval-augmented generation (RAG) methods perform specifically on the complex legal tasks (factual background, procedural posture, subsequent history, core legal question, and central holding) where hallucination rates are highest?
- Basis in paper: The paper mentions RAG as a potential solution but does not test its effectiveness on the most challenging tasks.
- Why unresolved: The paper focuses on baseline performance without external knowledge bases, leaving open whether RAG can address the worst-performing task categories.
- What evidence would resolve it: Direct comparison of hallucination rates on high-complexity tasks between baseline LLMs and RAG-augmented versions using identical test sets.

### Open Question 3
- Question: What is the optimal balance between an LLM's fidelity to user prompts versus fidelity to legal facts, and how does this balance vary across different types of legal users (pro se litigants vs. experienced attorneys)?
- Basis in paper: Section 2.3 discusses the trade-off between prompt fidelity and factual fidelity as a normative consideration without providing empirical guidance on optimal balance.
- Why unresolved: The paper identifies this as a key tension but does not empirically investigate how different user groups would prefer this trade-off to be resolved.
- What evidence would resolve it: User studies with different legal sophistication levels testing responses to prompts containing both correct and incorrect legal premises, measuring user satisfaction and accuracy preferences.

## Limitations
- Reliance on metadata-based ground truth assumes complete and accurate metadata, which may not always hold
- Contradiction detection approach for reference-free tasks uses GPT-4 as judge, introducing potential circularity
- Study focuses on U.S. federal court cases, limiting generalizability to other legal systems or domains
- Four LLMs tested represent a small fraction of available models, results may not extend to newer models

## Confidence

- High confidence: General phenomenon of legal hallucinations across all tested models (58-88% hallucination rates)
- Medium confidence: Mechanisms explaining hierarchy-based variation and contra-factual bias
- Low confidence: Specific calibration error values and their practical implications for legal practice

## Next Checks
1. Conduct a replication study using actual legal practitioners to manually verify model responses, particularly for reference-free tasks where GPT-4 serves as judge
2. Test additional LLMs (including open-source models with legal fine-tuning) to assess whether findings generalize beyond the four models studied
3. Perform an ablation study on prompt engineering approaches (zero-shot vs few-shot, different temperature settings) to quantify their impact on hallucination rates and calibration