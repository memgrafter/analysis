---
ver: rpa2
title: 'Trust but Verify: Programmatic VLM Evaluation in the Wild'
arxiv_id: '2410.13121'
source_url: https://arxiv.org/abs/2410.13121
tags:
- image
- arxiv
- scene
- preprint
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PROVE, a new benchmarking paradigm for evaluating
  Vision-Language Models (VLMs) on open-ended queries. The key challenge addressed
  is the difficulty in quantifying hallucinations in free-form responses, as it requires
  visually verifying each claim within the response.
---

# Trust but Verify: Programmatic VLM Evaluation in the Wild

## Quick Facts
- arXiv ID: 2410.13121
- Source URL: https://arxiv.org/abs/2410.13121
- Reference count: 40
- Primary result: Introduces PROVE benchmark with 10.5k QA pairs programmatically verified using scene graphs

## Executive Summary
This paper addresses the challenge of evaluating hallucinations in Vision-Language Models (VLMs) when responding to open-ended queries. The key insight is that hallucinations are difficult to quantify because they require visually verifying each claim in a free-form response. To solve this, PROVE uses hyper-detailed image captions to construct high-fidelity scene-graph representations, which are then used to generate diverse question-answer pairs with accompanying verification programs. These programs can be executed over the scene graph to verify the correctness and groundedness of each QA pair. The final benchmark consists of 10.5k visually grounded QA pairs, and a programmatic evaluation strategy measures both helpfulness and truthfulness of model responses within a unified scene graph-based framework.

## Method Summary
PROVE constructs scene graphs from hyper-detailed image captions using LLM-extracted tuples, then generates diverse question-answer pairs and verification programs that can be executed over these scene graphs. The benchmark curation process involves LLM-generated QA pairs with verification programs, followed by programmatic and text-based filtering to ensure quality. The evaluation framework computes hscore (helpfulness based on recall) and tscore (truthfulness based on precision) by comparing model responses against the scene graph representation. The method relies on existing models for text-embeddings, scene graph tuple extraction, and image-text entailment.

## Key Results
- Very few VLMs achieve a good balance between helpfulness and truthfulness
- Larger and more recent models show higher helpfulness but not necessarily higher truthfulness
- Human study validates the quality of the benchmark and proposed metrics
- Models that prioritize helpfulness often sacrifice truthfulness, and vice versa

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-fidelity scene graph representations enable programmatic verification of VLM responses
- Mechanism: Scene graphs from detailed captions enable algorithmic comparison against model responses
- Core assumption: Scene graph tuples accurately capture all relevant visual information
- Evidence anchors: [abstract], [section 3.1]
- Break condition: If scene graph extraction misses critical visual details

### Mechanism 2
- Claim: Separating helpfulness and truthfulness evaluation provides more nuanced model assessment
- Mechanism: hscore based on recall and tscore based on precision capture different aspects of performance
- Core assumption: Helpfulness and truthfulness are independent dimensions
- Evidence anchors: [abstract], [section 3.2]
- Break condition: If metrics become highly correlated in practice

### Mechanism 3
- Claim: Programmatic filtering ensures high-quality, visually grounded QA pairs
- Mechanism: Two rounds of filtering (programmatic verification + LLM checks) ensure reliable evaluation data
- Core assumption: LLMs can reliably identify low-quality QA pairs
- Evidence anchors: [section 3.1]
- Break condition: If filtering removes too many valid QA pairs or fails to catch low-quality ones

## Foundational Learning

- Concept: Scene graph representation of visual scenes
  - Why needed here: Provides structured, queryable representation for programmatic verification
  - Quick check question: What are the three types of tuples that comprise scene graphs, and how do they relate to each other?

- Concept: Precision and recall metrics in information retrieval
  - Why needed here: Used to evaluate truthfulness (precision) and helpfulness (recall) of VLM responses
  - Quick check question: How do precision and recall metrics differ in what they measure about model performance?

- Concept: Visual entailment
  - Why needed here: Determines if response tuples are consistent with actual image, not just caption
  - Quick check question: What is visual entailment, and why is it important for evaluating VLM truthfulness?

## Architecture Onboarding

- Component map: Scene graph → QA generation → Filtering → Evaluation
- Critical path: Each step depends on the previous one working correctly
- Design tradeoffs: High-precision filtering vs. recall, using external models vs. custom solutions
- Failure signatures: High hscore/low tscore indicates answering but making factual errors
- First 3 experiments:
  1. Verify scene graph construction accuracy by comparing against human annotations
  2. Test LLM-based filtering by having humans evaluate filtered vs. unfiltered pairs
  3. Benchmark simple VLM to establish baseline performance and validate metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve caption recall while maintaining precision to capture more image aspects?
- Basis in paper: Discussion on limitations of high-recall captions potentially missing model hallucinations
- Why unresolved: Even high-recall captions may not capture every aspect of an image
- What evidence would resolve it: Techniques enhancing caption recall while preserving precision, validated through improved hallucination detection

### Open Question 2
- Question: Can we develop more robust scene graph representations less dependent on off-the-shelf models?
- Basis in paper: Discussion on PROVE's reliance on off-the-shelf models and their limitations
- Why unresolved: PROVE inherits limitations from external models used for text-embeddings and entailment
- What evidence would resolve it: Alternative scene graph methods not relying on external models, showing improved reliability

### Open Question 3
- Question: How effective are hallucination mitigation strategies on PROVE, and can they achieve Pareto improvements?
- Basis in paper: Explicitly mentioned as future research direction
- Why unresolved: Paper doesn't evaluate these strategies on PROVE
- What evidence would resolve it: Empirical studies showing improvements in both helpfulness and truthfulness

## Limitations

- Scene graph extraction quality directly impacts benchmark reliability and evaluation accuracy
- Heavy reliance on external models (text-embeddings, entailment) introduces potential cascading errors
- LLM-based filtering may introduce bias or remove valid evaluation pairs
- The precision-recall formulation assumes tuple-based answers sufficiently capture response quality

## Confidence

- Scene graph-based verification mechanism: Medium
- Separation of helpfulness and truthfulness metrics: Medium
- LLM-based filtering effectiveness: Low

## Next Checks

1. Compare scene graph tuples against human annotations on a subset of images to quantify extraction accuracy
2. Run the filtering pipeline with different LLM configurations to assess sensitivity to filtering parameters
3. Test model evaluation on synthetically generated scene graphs with known ground truth to validate the hscore/tscore computation