---
ver: rpa2
title: Rehearsal-Free Continual Federated Learning with Synergistic Synaptic Intelligence
arxiv_id: '2412.13779'
source_url: https://arxiv.org/abs/2412.13779
tags:
- learning
- data
- federated
- fedssi
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedSSI introduces a novel regularization approach for continual
  federated learning that addresses catastrophic forgetting without requiring data
  rehearsal or extensive computational overhead. The method enhances Synaptic Intelligence
  by incorporating both local and global data information through a personalized surrogate
  model, which dynamically balances local and global knowledge based on data heterogeneity.
---

# Rehearsal-Free Continual Federated Learning with Synergistic Synaptic Intelligence

## Quick Facts
- **arXiv ID**: 2412.13779
- **Source URL**: https://arxiv.org/abs/2412.13779
- **Reference count**: 26
- **Key outcome**: FedSSI introduces a novel regularization approach for continual federated learning that addresses catastrophic forgetting without requiring data rehearsal or extensive computational overhead.

## Executive Summary
FedSSI addresses catastrophic forgetting in continual federated learning by introducing a personalized surrogate model (PSM) that dynamically balances local and global knowledge based on data heterogeneity. Unlike existing methods that require data rehearsal or extensive computational resources, FedSSI leverages Synaptic Intelligence with a novel mechanism that incorporates both local and global data information through the PSM. The method achieves significant improvements over state-of-the-art approaches, demonstrating up to 12.47% better final accuracy while maintaining robustness across varying levels of data heterogeneity in both class-incremental and domain-incremental learning scenarios.

## Method Summary
FedSSI enhances Synaptic Intelligence for federated learning by introducing a personalized surrogate model that computes parameter importance weights using both local and global information. The PSM is trained with a momentum-like update controlled by λ, which dynamically adjusts based on data heterogeneity α to balance local adaptation and global knowledge retention. During training, each client calculates surrogate losses for previous tasks using the PSM, then updates their local model with both new task loss and the surrogate loss weighted by importance parameters. The server aggregates updated models using FedAvg, with the entire process requiring no data rehearsal while maintaining computational efficiency.

## Key Results
- Achieves up to 12.47% improvement in final accuracy compared to state-of-the-art methods
- Maintains robustness across varying levels of data heterogeneity (α = 0.1, 1.0, 10.0, 100)
- Demonstrates effectiveness in both class-incremental and domain-incremental learning scenarios
- Outperforms traditional regularization baselines including FL+LwF, FL+EWC, FL+OGD, and FL+SI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The personalized surrogate model (PSM) dynamically balances local and global knowledge based on data heterogeneity.
- Mechanism: PSM incorporates both local and global information through a momentum-like update term controlled by λ. When data is highly heterogeneous (α low), λ decreases, giving more weight to global knowledge from the aggregated model. When data is homogeneous (α high), λ increases, allowing more local adaptation.
- Core assumption: There exists a positive correlation between data heterogeneity (α) and the optimal λ value for balancing local/global information.
- Evidence anchors:
  - [abstract] "incorporating both local and global data information through a personalized surrogate model, which dynamically balances local and global knowledge based on data heterogeneity"
  - [section] "we can empirically and theoretically judge there exists a positive correlation between α and λ"
  - [corpus] Weak evidence - no direct citations supporting this specific mechanism
- Break condition: If the correlation between α and optimal λ doesn't hold, PSM would either overfit to local data in heterogeneous settings or fail to adapt in homogeneous settings.

### Mechanism 2
- Claim: Synaptic Intelligence's importance weights computed with PSM better reflect true parameter importance across heterogeneous data.
- Mechanism: Traditional SI computes importance weights based solely on local data distribution. FedSSI uses PSM to compute these weights, which incorporates both local and global information. This creates more accurate importance weights that reflect the true contribution of parameters to previous tasks across the federated network.
- Core assumption: Importance weights computed with global information are more accurate than those computed with only local information in heterogeneous settings.
- Evidence anchors:
  - [abstract] "each client will calculate the surrogate loss for previous tasks based not only on information from the local dataset but also on its correlation to the global dataset"
  - [section] "Since this is purely local training assisted by an already converged global model, the training of the PSM is very fast"
  - [corpus] Weak evidence - no direct citations supporting this specific mechanism
- Break condition: If PSM training fails to converge or if global information doesn't improve importance weight estimation, the regularization would be ineffective.

### Mechanism 3
- Claim: The momentum-like update in PSM training ensures convergence while balancing global and local information.
- Mechanism: The PSM update includes a term q(λ)(vt-1k,s-1 - wt-1) that acts like momentum, incorporating global model information. As λ decreases, this term dominates, pulling PSM toward the global model. As λ increases, the local gradient term dominates, allowing more local adaptation.
- Core assumption: The momentum-like term in PSM update ensures stable convergence while maintaining the balance between global and local information.
- Evidence anchors:
  - [section] "We can draw an analogy to momentum methods in optimization. Momentum-based methods leverage past updates to guide the current update direction"
  - [section] "Proposition 1 and Table 3" showing empirical correlation between α and λ
  - [corpus] Weak evidence - no direct citations supporting this specific mechanism
- Break condition: If the momentum term destabilizes PSM training or if the balance mechanism fails, PSM would either diverge or provide no benefit over standard SI.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper addresses catastrophic forgetting in the federated setting where models need to learn new tasks without forgetting previous ones
  - Quick check question: Why does training on new tasks typically cause a model to perform worse on previously learned tasks?

- Concept: Data heterogeneity in federated learning
  - Why needed here: The paper specifically addresses how data heterogeneity (Non-IID distributions across clients) affects continual learning performance
  - Quick check question: What happens to federated learning performance when each client has a different data distribution?

- Concept: Regularization-based approaches to continual learning
  - Why needed here: FedSSI is a regularization-based method that modifies synaptic intelligence to work in federated settings
  - Quick check question: How do regularization-based methods prevent catastrophic forgetting without storing old data?

## Architecture Onboarding

- Component map:
  Global model (wt) -> Personalized Surrogate Model (PSM, vt-k) -> Importance weights (Ωt-k,i) -> Local model (wt-k)

- Critical path:
  1. Server sends global model wt-1 to selected clients
  2. Each client trains PSM vt-1-k using both local data and global model information
  3. Client computes importance weights Ωt-k using PSM
  4. Client trains local model wt-k with new task loss + surrogate loss
  5. Server aggregates updated models wt-k into new global model wt

- Design tradeoffs:
  - Memory: No data rehearsal needed vs. additional PSM storage
  - Communication: One extra model parameter exchange per client per task
  - Computation: PSM training adds overhead but is claimed to be minimal (1/40 of task training)
  - Privacy: PSM is computed locally, no additional privacy concerns

- Failure signatures:
  - PSM training diverges (check loss curves)
  - Importance weights become unstable (monitor Ω values)
  - Performance drops significantly with increased data heterogeneity
  - Communication overhead becomes prohibitive at scale

- First 3 experiments:
  1. Reproduce FL+SI baseline to establish performance without FedSSI modifications
  2. Test FedSSI with varying λ values on homogeneous data to verify local/global balance mechanism
  3. Test FedSSI on highly heterogeneous data to confirm catastrophic forgetting is mitigated compared to baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical relationship between the data heterogeneity parameter α and the hyper-parameter λ in FedSSI?
- Basis in paper: [explicit] The paper states "We can empirically and theoretically judge there exists a positive correlation between α and λ" and discusses how λ controls the balance between local and global information in the personalized surrogate model.
- Why unresolved: The paper only establishes an empirical observation of correlation without providing a formal mathematical relationship or formula connecting these parameters.
- What evidence would resolve it: A mathematical proof or experimental validation demonstrating the precise functional relationship between α and λ would resolve this question.

### Open Question 2
- Question: How does FedSSI perform on more complex datasets or larger-scale federated learning scenarios?
- Basis in paper: [inferred] The paper tests on datasets ranging from CIFAR10 to Tiny-ImageNet and Digit10, but does not explore more complex datasets or scenarios with significantly larger numbers of clients or tasks.
- Why unresolved: The paper's experiments focus on moderate-scale scenarios and do not push the boundaries of dataset complexity or system scale.
- What evidence would resolve it: Extensive experiments on larger-scale federated learning systems with more complex datasets would provide evidence of FedSSI's scalability and performance limits.

### Open Question 3
- Question: What is the impact of FedSSI on model convergence speed and training stability in highly heterogeneous federated learning environments?
- Basis in paper: [explicit] The paper mentions "FedSSI may introduce additional communication rounds" and shows trade-offs between communication efficiency and accuracy, but does not deeply analyze convergence behavior in extreme heterogeneity cases.
- Why unresolved: The paper provides performance metrics but does not thoroughly investigate the convergence dynamics or stability of FedSSI under varying levels of data heterogeneity.
- What evidence would resolve it: Detailed convergence analysis with convergence rate measurements and stability tests across different heterogeneity levels would resolve this question.

## Limitations

- The positive correlation between data heterogeneity (α) and optimal λ value is empirically observed but lacks theoretical guarantees
- Computational efficiency claims about PSM training are based on limited experimental evidence and may not generalize to larger models
- The method's performance on more complex datasets or larger-scale federated learning scenarios remains untested

## Confidence

- High confidence: FedSSI successfully mitigates catastrophic forgetting in federated settings compared to baselines (supported by multiple datasets and metrics)
- Medium confidence: The PSM mechanism effectively balances local and global information based on data heterogeneity (empirical correlation shown but theoretical foundation weak)
- Low confidence: The claim about computational efficiency of PSM training relative to task training (limited experimental evidence)

## Next Checks

1. Test FedSSI on datasets with different heterogeneity patterns (beyond Dirichlet distribution) to verify the α-λ correlation mechanism's robustness
2. Measure actual PSM training time and memory usage across different model sizes to validate computational efficiency claims
3. Implement ablation studies removing the momentum-like term in PSM updates to quantify its contribution to stability