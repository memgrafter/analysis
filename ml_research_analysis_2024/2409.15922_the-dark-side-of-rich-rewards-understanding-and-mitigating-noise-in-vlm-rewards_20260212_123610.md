---
ver: rpa2
title: 'The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards'
arxiv_id: '2409.15922'
source_url: https://arxiv.org/abs/2409.15922
tags:
- reward
- rewards
- 'false'
- agent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "VLMs used as reward signals often fail in complex tasks due to\
  \ noisy rewards. Analysis showed that false positive rewards\u2014where unintended\
  \ trajectories are spuriously rewarded\u2014are more harmful than false negatives."
---

# The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM Rewards

## Quick Facts
- **arXiv ID**: 2409.15922
- **Source URL**: https://arxiv.org/abs/2409.15922
- **Reference count**: 40
- **Primary result**: BiMI reward function improved VLM-guided RL performance by up to 364% in Minigrid and 88% in Montezuma by reducing false positive rewards

## Executive Summary
Vision-Language Models (VLMs) as reward signals often produce noisy rewards that mislead reinforcement learning agents, particularly through false positive rewards that reinforce incorrect behaviors. This paper systematically analyzes how VLM reward noise affects learning and introduces BiMI, a novel reward function that reduces false positives by issuing one-time binary rewards above a high-confidence threshold and penalizing frequent but misleading signals. Experiments across three environments demonstrate that BiMI significantly outperforms standard VLM rewards and effectively combines with intrinsic rewards, addressing a critical bottleneck in VLM-guided RL.

## Method Summary
The paper fine-tunes CLIP and X-CLIP VLM backbones on environment-specific instruction-trajectory pairs to create reward models (Pixl2R, ELLM-). RL agents are trained using PPO with different reward configurations: VLM-only, intrinsic-only (DEIR), and combined. The proposed BiMI reward function uses conformal prediction to set a high threshold for binary reward issuance and incorporates a mutual information term to downweight frequent signals. Performance is evaluated using a score metric (geometric mean of success rates across subtasks) and AUC (learning speed).

## Key Results
- False positive rewards are more detrimental to learning than false negatives, misleading agents into reinforcing suboptimal behaviors
- BiMI reduced false positives by 65% and improved performance by up to 364% in Minigrid and 88% in Montezuma's Revenge
- Combining BiMI with intrinsic rewards achieved synergistic gains, with intrinsic rewards filling gaps between anchor points provided by BiMI
- Standard VLM rewards often cause reward hacking and suboptimal learning due to state entanglement and composition insensitivity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: False positive rewards mislead RL agents into reinforcing suboptimal behaviors, while false negatives merely reduce feedback frequency without misleading.
- Mechanism: False positives assign rewards to trajectories that don't fulfill the intended instruction, causing the agent to learn incorrect state-action associations. False negatives withhold rewards from correct trajectories, but the agent still learns the correct associations when rewards are given.
- Core assumption: The agent's learning process is more sensitive to spurious positive signals than to missing positive signals.
- Evidence anchors:
  - [abstract] "false positive rewards -- instances where unintended trajectories are incorrectly rewarded -- are more detrimental than false negatives"
  - [section] "Our analysis centered around two classes of noise: false positives... and false negatives... we posit that false positive rewards are not only more prevalent but potentially more detrimental to the learning process than false negatives"
  - [corpus] Weak evidence; related work focuses on reward modeling but doesn't directly compare false positive vs false negative impact

### Mechanism 2
- Claim: Cosine similarity metrics suffer from state entanglement and composition insensitivity, leading to false positive rewards.
- Mechanism: Cosine similarity measures lexical similarity in embedding space but fails to capture state transitions and temporal ordering. This causes the metric to reward trajectories that are semantically similar to instructions but lead to wrong states or execute actions in incorrect order.
- Core assumption: The VLM embedding space preserves semantic similarity but not state transition information or temporal relationships.
- Evidence anchors:
  - [abstract] "the widely used cosine similarity metric is prone to false positive estimates"
  - [section] "cosine similarity metric... erroneously pays more attention to lexical-level similarity while lacking comprehension of the underlying state transitions"
  - [corpus] Weak evidence; related work mentions cosine similarity but doesn't explicitly analyze state entanglement issues

### Mechanism 3
- Claim: Binary Mutual Information (BIMI) reward reduces false positives by issuing one-time binary rewards above a high-confidence threshold and penalizing frequent but misleading signals.
- Mechanism: BIMI uses conformal prediction to set a high threshold for reward issuance, ensuring only highly confident matches receive rewards. The mutual information term downweights rewards from frequently occurring signals, preventing over-reliance on common but potentially misleading associations.
- Core assumption: Reducing the frequency of reward signals while maintaining high confidence will decrease false positives without significantly harming learning.
- Evidence anchors:
  - [abstract] "We introduce BIMI (BinaryMutualInformation Reward), a novel reward function designed to mitigate noise"
  - [section] "BIMI uses binary signals to directly reduce the occurrence of false positives and incorporates a mutual information term to prevent overfitting to noisy signal sources"
  - [corpus] Moderate evidence; related work mentions BIMI but doesn't detail its mechanism for false positive reduction

## Foundational Learning

- Concept: Heuristic-guided reinforcement learning and the concept of heuristic admissibility
  - Why needed here: The paper frames VLM rewards as heuristics and analyzes their impact on learning convergence using HuRL framework
  - Quick check question: Why does a pessimistic heuristic (h(s) ≤ V*(s)) provide better convergence guarantees than an optimistic one in RL?

- Concept: Conformal prediction and empirical quantile calculation
  - Why needed here: BIMI uses conformal prediction to set the threshold for binary reward issuance
  - Quick check question: How does conformal prediction ensure that the threshold will correctly identify true positive pairs with high probability?

- Concept: Mutual information maximization and its role in preventing overfitting to noisy signals
  - Why needed here: BIMI incorporates a mutual information term to downweight frequent but potentially misleading reward signals
  - Quick check question: Why does maximizing mutual information between trajectories and instructions help prevent overfitting to noisy reward signals?

## Architecture Onboarding

- Component map: VLM encoder → Cosine similarity scoring → Reward thresholding (BIMI) → RL policy training loop
- Critical path: Instruction → VLM embedding → Trajectory embedding → Similarity score → Threshold check → Reward signal → Policy update
- Design tradeoffs: BIMI trades off reward frequency for higher confidence, potentially slowing learning but improving accuracy
- Failure signatures: Agent fails to learn if threshold is too high, or learns incorrect behaviors if threshold is too low
- First 3 experiments:
  1. Test BIMI reward function on simple instruction-following task with synthetic false positive rewards to verify reduction in false positives
  2. Compare learning curves of BIMI vs standard VLM rewards on Montezuma's Revenge to measure performance improvement
  3. Ablation study: test BIMI with only binary component vs only mutual information component to isolate their individual contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VLM-based reward models degrade as the complexity of language instructions increases, particularly for tasks involving conditional or ambiguous instructions?
- Basis in paper: [inferred] The paper mentions that most existing work on VLM-based reward models focuses on linear sequences of language instructions, excluding more complex cases. The authors suggest that future research should investigate conditional and ambiguous instructions.
- Why unresolved: The paper does not provide experimental results or analysis for tasks involving conditional or ambiguous instructions. It only briefly mentions this as a limitation and future research direction.
- What evidence would resolve it: Experimental results comparing VLM-based reward models' performance on tasks with linear vs. conditional/ambiguous instructions, showing the degradation in performance as instruction complexity increases.

### Open Question 2
- Question: What is the optimal balance between false positive reduction and false negative tolerance in the BIMI reward function, and how does this balance affect overall agent performance across different environments?
- Basis in paper: [explicit] The paper introduces BIMI to reduce false positives but acknowledges it may increase false negatives. The authors show that BI alone improved performance by 36.5%, while MI added 23% further improvement (excluding Crafter). However, the optimal balance is not explored.
- Why unresolved: The paper does not systematically explore different thresholds or trade-offs between false positive reduction and false negative tolerance. It only tests one configuration (90th percentile).
- What evidence would resolve it: A systematic ablation study varying the confidence threshold in BIMI and measuring its impact on agent performance across multiple environments, identifying the optimal trade-off point.

### Open Question 3
- Question: How do false positive rewards from VLM-based reward models affect the exploration-exploitation trade-off in reinforcement learning, and can intrinsic rewards effectively compensate for this noise?
- Basis in paper: [explicit] The paper shows that combining BIMI with intrinsic rewards (DEIR) led to significant performance improvements (e.g., 65% gain in Montezuma). It also demonstrates that intrinsic rewards fill gaps between anchor points provided by BIMI.
- Why unresolved: While the paper shows positive synergy between BIMI and intrinsic rewards, it does not provide a theoretical framework explaining how false positive rewards disrupt the exploration-exploitation balance or how intrinsic rewards compensate for this disruption.
- What evidence would resolve it: Theoretical analysis or experimental results quantifying how false positive rewards affect the agent's exploration behavior and how intrinsic rewards modify this behavior to achieve better performance.

## Limitations

- The analysis relies on synthetic false positive/negative simulations rather than directly measuring true error rates in real VLM reward outputs
- Experiments focus on three specific environments and two VLM backbones, limiting generalizability to other domains
- Critical implementation details like threshold selection process and fine-tuning procedures are underspecified

## Confidence

- **High confidence**: The core claim that false positive rewards are more detrimental than false negatives is well-supported by ablation studies
- **Medium confidence**: The effectiveness of BiMI in reducing false positives and improving learning outcomes is demonstrated, but component contributions are not fully isolated
- **Low confidence**: The claim that cosine similarity inherently suffers from state entanglement lacks direct empirical validation

## Next Checks

1. Collect and analyze actual VLM reward outputs on held-out trajectories to quantify true false positive/negative rates and correlate with learning performance
2. Implement BiMI with alternative VLM backbones (e.g., SigLIP, BLIP) to verify effectiveness beyond CLIP/X-CLIP
3. Design controlled experiments isolating binary thresholding and mutual information components of BiMI to quantify individual contributions