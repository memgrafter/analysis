---
ver: rpa2
title: 'Insight: A Multi-Modal Diagnostic Pipeline using LLMs for Ocular Surface Disease
  Diagnosis'
arxiv_id: '2410.00292'
source_url: https://arxiv.org/abs/2410.00292
tags:
- clinical
- data
- llms
- diagnosis
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a multi-modal diagnostic pipeline for ocular
  surface diseases using large language models. The pipeline combines quantified meibography
  imaging data with clinical metadata through a visual translator and LLM-based summarizer,
  enhanced by domain-specific clinician knowledge.
---

# Insight: A Multi-Modal Diagnostic Pipeline using LLMs for Ocular Surface Disease Diagnosis

## Quick Facts
- arXiv ID: 2410.00292
- Source URL: https://arxiv.org/abs/2410.00292
- Authors: Chun-Hsiao Yeh; Jiayun Wang; Andrew D. Graham; Andrea J. Liu; Bo Tan; Yubei Chen; Yi Ma; Meng C. Lin
- Reference count: 40
- Primary result: Achieves up to 89.9% accuracy in dry eye diagnosis, outperforming GPT-4 and baselines

## Executive Summary
This paper introduces a multi-modal diagnostic pipeline for ocular surface diseases that combines quantified meibography imaging data with clinical metadata through a visual translator and LLM-based summarizer. The approach integrates domain-specific clinician knowledge through fine-tuning to improve diagnostic reasoning. Evaluated across OSD diagnosis benchmarks, the pipeline demonstrates superior performance compared to existing methods, with clinician studies confirming better diagnostic reasoning and clinical accuracy.

## Method Summary
The pipeline processes meibography images through an instance segmentation network with ResNet50 backbone to quantify meibomian gland morphology (atrophy percentage, density, contrast, length/width/tortuosity). GPT-4 generates structured Q&A clinical report summaries from the combined morphology and metadata. The LLM is fine-tuned using TRL with 4-bit QLoRA on Llama2 models using DE clinical trial criteria and real clinician diagnoses. The system is trained on 3,513 entries from CRC and DREAM datasets using 4 NVIDIA RTX 3090 GPUs, achieving diagnostic accuracy through multi-modal integration.

## Key Results
- Achieves 89.9% accuracy in dry eye diagnosis on test benchmarks
- Outperforms GPT-4 and other baseline methods across OSD diagnosis tasks
- Clinician study confirms superior diagnostic reasoning and clinical accuracy compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual translator converts complex meibography images into structured quantitative features for LLM processing
- Mechanism: Instance segmentation isolates individual meibomian glands, then morphological metrics (atrophy %, density, contrast, length/width/tortuosity) are computed
- Core assumption: Gland-level segmentation and quantification accurately reflect clinical disease states and are interpretable by LLMs
- Evidence anchors: Visual translator description with instance segmentation and quantification process
- Break condition: If segmentation fails to isolate glands accurately, or metrics don't correlate with disease severity

### Mechanism 2
- Claim: LLM-based summarizer contextualizes fragmented metadata and morphology into coherent clinical report summaries
- Mechanism: GPT-4 prompted with task description, supporting examples, and raw clinical data to synthesize Q&A pairs
- Core assumption: GPT-4 can reliably translate raw clinical data into clinically meaningful Q&A summaries
- Evidence anchors: GPT-4 prompting structure described for generating clinical reports
- Break condition: If GPT-4 produces inconsistent or clinically inaccurate summaries

### Mechanism 3
- Claim: Fine-tuning LLMs on DE clinical trial criteria and real clinician diagnoses refines their reasoning
- Mechanism: Two-stage fine-tuning on DE-specific clinical terms and real clinician diagnosis cases
- Core assumption: Domain-specific fine-tuning improves LLM diagnostic accuracy and clinical reasoning
- Evidence anchors: Two-stage fine-tuning process described with DE trial criteria and clinician cases
- Break condition: If fine-tuning data doesn't cover disease spectrum or contains biases

## Foundational Learning

- Concept: Multimodal integration - combining image-derived features with clinical metadata
  - Why needed here: OSD diagnosis requires correlating visual gland morphology with clinical measurements
  - Quick check question: Can the system handle cases where only metadata is available and still produce reasonable diagnoses?

- Concept: Instance segmentation for medical imaging
  - Why needed here: Individual gland isolation is essential for accurate morphological quantification
  - Quick check question: Does segmentation model maintain accuracy across different eyelid regions and disease severities?

- Concept: Few-shot learning and prompting strategies for LLMs
  - Why needed here: Summarizer relies on GPT-4's ability to generalize from limited examples
  - Quick check question: How sensitive is summarizer's output to changes in prompt format or example quality?

## Architecture Onboarding

- Component map: Image → Visual Translator → Summarizer → LLM Fine-tuning → Diagnosis
- Critical path: Visual translator processes images, summarizer contextualizes data, fine-tuned LLM generates diagnosis
- Design tradeoffs: Segmentation accuracy vs computational cost; fine-tuning on synthetic vs real data; prompt complexity vs output consistency
- Failure signatures: Low segmentation IoU indicates visual translator failure; inconsistent summarizer outputs indicate prompting issues; poor diagnostic accuracy indicates inadequate fine-tuning
- First 3 experiments:
  1. Validate visual translator by comparing segmentation outputs against clinician annotations on held-out images
  2. Test summarizer consistency by running multiple prompts with same metadata and checking output variance
  3. Evaluate fine-tuning impact by comparing diagnostic accuracy before and after DE trial criteria integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MDPipe's performance compare when using different visual translator architectures?
- Basis in paper: [inferred] Paper mentions ResNet50 but does not compare to other architectures
- Why unresolved: No ablation studies or comparisons with alternative visual translator models
- What evidence would resolve it: Experimental results comparing performance using different segmentation network architectures

### Open Question 2
- Question: What is the impact of dataset size and diversity on MDPipe's diagnostic accuracy?
- Basis in paper: [inferred] Paper mentions efforts to obtain additional data but does not evaluate diversity impact
- Why unresolved: No analysis of performance across different demographic groups or assessment of increasing diversity
- What evidence would resolve it: Systematic evaluation of performance across different age groups, ethnicities, and genders

### Open Question 3
- Question: How does MDPipe's clinical reasoning compare to specialist ophthalmologists in complex cases?
- Basis in paper: [explicit] Clinician preference study only compares to GPT-4, not specialist ophthalmologists
- Why unresolved: Clinician study only compares MDPipe to GPT-4, not human experts
- What evidence would resolve it: Direct comparison against specialist ophthalmologists on complex cases with overlapping symptoms

### Open Question 4
- Question: What is the effect of different fine-tuning strategies on performance and efficiency?
- Basis in paper: [explicit] Paper mentions using 4-bit QLoRA but does not compare to other approaches
- Why unresolved: No performance or efficiency comparisons between different fine-tuning methods
- What evidence would resolve it: Experimental results comparing accuracy and efficiency using different fine-tuning methods

### Open Question 5
- Question: How does MDPipe handle rare ocular surface diseases or atypical presentations?
- Basis in paper: [inferred] Paper focuses on common OSDs but does not address rare diseases
- Why unresolved: No evaluation on rare ocular conditions or atypical presentations
- What evidence would resolve it: Evaluation on rare ocular surface diseases and atypical presentations with analysis of failure modes

## Limitations
- Visual translator's segmentation accuracy and generalizability are primary uncertainties with weak validation evidence
- GPT-4 summarizer's effectiveness is assumed rather than independently validated against clinician-written reports
- CRC dataset with only 1,039 total images may not capture full spectrum of OSD presentations, particularly severe cases

## Confidence
- High confidence: Multi-modal integration approach combining quantified imaging data with clinical metadata is methodologically sound
- Medium confidence: Overall diagnostic accuracy claims are supported by benchmark comparisons
- Low confidence: Assumption that GPT-4 can reliably generate clinically accurate summaries lacks direct validation

## Next Checks
1. Validate visual translator by comparing segmentation outputs against independent clinician annotations on held-out test images
2. Run GPT-4 summarizer multiple times with identical clinical metadata inputs to measure output variance and consistency
3. Conduct ablation study comparing diagnostic accuracy before and after each fine-tuning stage to assess impact and overfitting risks