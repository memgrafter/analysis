---
ver: rpa2
title: 'The Invalsi Benchmarks: measuring Linguistic and Mathematical understanding
  of Large Language Models in Italian'
arxiv_id: '2403.18697'
source_url: https://arxiv.org/abs/2403.18697
tags:
- invalsi
- italian
- llama
- instruct
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces three new benchmarks for evaluating large
  language models (LLMs) on mathematical and language understanding tasks in Italian:
  Invalsi MATE, Invalsi ITA, and Olimpiadi MATE. The first two are based on validated
  Italian school assessment tests, while the third is derived from the Italian national
  math Olympics.'
---

# The Invalsi Benchmarks: measuring Linguistic and Mathematical understanding of Large Language Models in Italian

## Quick Facts
- arXiv ID: 2403.18697
- Source URL: https://arxiv.org/abs/2403.18697
- Reference count: 17
- LLMs achieve 71% accuracy on mathematical understanding and 88% on language understanding in Italian benchmarks

## Executive Summary
This paper introduces three new benchmarks for evaluating large language models on mathematical and language understanding tasks in Italian: Invalsi MATE, Invalsi ITA, and Olimpiadi MATE. The benchmarks are based on validated Italian school assessment tests and national math Olympics. The authors evaluate 10 LLMs and find that Llama 3.1 models achieve the highest accuracy, outperforming both English-only pre-trained models and Italian students on certain tasks. The study reveals that multilingual pre-training provides strong performance improvements in Italian, while fine-tuning on Italian alone does not close the gap created by multilingual pre-training.

## Method Summary
The authors evaluate 10 large language models on three Italian benchmarks using a likelihood-based evaluation method with chain-of-thought prompting. The benchmarks include Invalsi MATE (400 mathematical questions), Invalsi ITA (1,117 language understanding questions), and Olimpiadi MATE (619 mathematical questions). Models are evaluated in zero-shot settings without fine-tuning on the benchmarks. The likelihood-based approach scores answers by computing the likelihood of the correct option given the model's prefix probability distribution.

## Key Results
- Llama 3.1 405b instruct achieves 71% accuracy on Invalsi MATE, 88% on Invalsi ITA, and 45% on Olimpiadi MATE
- Multilingual pre-training improves Italian performance compared to English-only pre-training
- Fine-tuning on Italian alone does not close the performance gap created by multilingual pre-training
- Llama 3.1 outperforms Italian students on Invalsi MATE while most models outperform students on Invalsi ITA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multilingual pre-training improves Italian performance compared to English-only pre-training.
- **Mechanism:** Models exposed to multiple languages during pre-training develop better generalization across linguistic structures, including Italian.
- **Core assumption:** Multilingual pre-training corpora contain sufficient Italian data to improve language understanding.
- **Evidence anchors:**
  - [abstract] "multilingual pre-training improves Italian performance compared to English-only pre-training"
  - [section 4.2] "multilingual models are stronger than English-first ones even when having fewer parameters"
  - [corpus] Weak - no direct comparison of training data composition
- **Break condition:** If Italian data is underrepresented in multilingual corpora, the advantage disappears.

### Mechanism 2
- **Claim:** Fine-tuning on Italian alone does not close the performance gap created by multilingual pre-training.
- **Mechanism:** Pre-training establishes foundational language representations that fine-tuning cannot fully override.
- **Core assumption:** Fine-tuning datasets are insufficient to re-architect language representations established during pre-training.
- **Evidence anchors:**
  - [abstract] "fine-tuning on Italian alone does not close this gap"
  - [section 4.2] "camoscio 2 70b instruct and llamantino 2 70b chat only show marginal gains"
  - [corpus] Weak - no detailed analysis of fine-tuning dataset size/quality
- **Break condition:** If fine-tuning datasets are substantially larger and more comprehensive than pre-training data.

### Mechanism 3
- **Claim:** Mathematical reasoning transfers better across languages than language understanding.
- **Mechanism:** Mathematical concepts are more universal and less language-dependent than linguistic nuances.
- **Core assumption:** Mathematical reasoning relies more on logical structures than language-specific features.
- **Evidence anchors:**
  - [abstract] "We provide early results that LLMs can transfer mathematical reasoning across languages better than language understanding"
  - [section 4.2] "larger difference between llama 3.1 70b instruct in Invalsi MATE is larger than in Invalsi ITA"
  - [corpus] Weak - no explicit comparison of mathematical vs linguistic task performance across languages
- **Break condition:** If mathematical reasoning is more language-dependent than assumed.

## Foundational Learning

- **Concept:** Likelihood-based evaluation method
  - **Why needed here:** Provides objective measurement of model performance on multiple choice and true/false questions
  - **Quick check question:** How does likelihood-based evaluation compare to human evaluation in terms of reliability and scalability?

- **Concept:** Multilingual vs monolingual pre-training differences
  - **Why needed here:** Explains why multilingual models outperform English-only models on Italian benchmarks
  - **Quick check question:** What specific linguistic features benefit from multilingual pre-training?

- **Concept:** Chain-of-thought prompting
  - **Why needed here:** Helps models show reasoning steps for mathematical problems
  - **Quick check question:** How does chain-of-thought prompting affect performance on different question types?

## Architecture Onboarding

- **Component map:** Data loading -> Model inference -> Answer extraction -> Performance aggregation
- **Critical path:** Data loading -> Model inference -> Answer extraction (bottleneck is inference time for large models)
- **Design tradeoffs:** Likelihood-based evaluation vs human evaluation (speed vs accuracy)
- **Failure signatures:** Random performance on true/false questions indicates model confusion; poor fill-the-gap performance suggests limited language generation capabilities
- **First 3 experiments:**
  1. Compare likelihood-based vs human evaluation on small subset
  2. Test different prompting strategies for mathematical reasoning
  3. Analyze performance breakdown by question type and grade level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying reason for multilingual pre-training providing stronger performance in Italian than English-only pre-training, even when English-only models are larger?
- Basis in paper: Inferred from the finding that "multilingual pre-training provides strong performance improvements in Italian" and "fine-tuning on Italian does not appear to help on mathematical understanding."
- Why unresolved: The paper observes this performance gap but does not investigate the underlying mechanisms. It could be due to better handling of linguistic nuances, more diverse training data, or other factors not explicitly analyzed.
- What evidence would resolve it: A controlled experiment comparing models with identical architecture and training data composition, but varying only in the proportion of multilingual vs. English-only pre-training, could isolate the effect of multilingual pre-training.

### Open Question 2
- Question: Why do LLMs show a larger performance gap on Invalsi MATE (mathematical understanding) compared to Invalsi ITA (language understanding) when comparing Llama 3.1 70b instruct's performance in Italian versus English?
- Basis in paper: The paper notes that "the gap between llama 3.1 70b instruct in Invalsi MATE is larger than in Invalsi ITA," suggesting that LLMs can transfer mathematical reasoning across languages better than language understanding.
- Why unresolved: The paper does not provide a detailed analysis of why mathematical reasoning is more transferable across languages than language understanding for LLMs.
- What evidence would resolve it: A detailed analysis of the types of reasoning and language comprehension required for each task, along with experiments isolating the effects of linguistic vs. logical reasoning on performance, could shed light on this discrepancy.

### Open Question 3
- Question: What is the maximum achievable accuracy on the Olimpiadi MATE benchmark, and how does it compare to human expert performance on similar mathematical problem-solving tasks?
- Basis in paper: The paper shows that current LLMs achieve a maximum accuracy of 45% on Olimpiadi MATE, indicating that "current LLMs are currently not able to consistently solve complex mathematical problems in Italian." However, it does not compare this to human expert performance.
- Why unresolved: The paper does not provide a benchmark for human expert performance on the Olimpiadi MATE tasks, making it difficult to assess the relative capabilities of LLMs.
- What evidence would resolve it: A study evaluating human experts on the same Olimpiadi MATE tasks, along with a comparison of their performance to the best-performing LLMs, would provide a clearer picture of the current limitations of LLMs in complex mathematical problem-solving.

## Limitations

- Potential dataset overlap with model training data, despite using datasets released after model training cutoff dates
- Reliability of likelihood-based evaluation for multilingual tasks compared to human evaluation
- Limited analysis of fine-tuning dataset characteristics and their impact on performance

## Confidence

- **High Confidence:** The comparative performance analysis between multilingual and English-only models is robust, supported by consistent results across multiple benchmarks and model sizes.
- **Medium Confidence:** The claim that fine-tuning on Italian alone doesn't close the multilingual gap requires further validation, as the study doesn't provide detailed analysis of fine-tuning dataset characteristics.
- **Low Confidence:** The transferability of mathematical reasoning across languages is suggested but not conclusively demonstrated, as the study lacks direct comparison of mathematical vs. linguistic task performance across multiple languages.

## Next Checks

1. Conduct a comprehensive analysis of training data overlap between evaluated models and benchmark datasets to verify the validity of zero-shot evaluation claims.
2. Implement human evaluation on a subset of benchmark questions to validate the reliability of likelihood-based scoring, particularly for complex Italian language constructs.
3. Design controlled experiments comparing mathematical reasoning performance across languages with matched linguistic complexity to isolate the effects of language transfer versus task complexity.