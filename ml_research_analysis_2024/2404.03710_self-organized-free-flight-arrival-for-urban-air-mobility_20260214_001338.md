---
ver: rpa2
title: Self-organized free-flight arrival for urban air mobility
arxiv_id: '2404.03710'
source_url: https://arxiv.org/abs/2404.03710
tags:
- uni00000013
- uni00000003
- uni00000018
- uni00000014
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A deep reinforcement learning policy is developed for decentralized
  urban air mobility vertiport arrival operations. Aircraft operate freely in a circular
  airspace and follow a shared neural network policy to avoid collisions and enter
  the vertiport when permitted.
---

# Self-organized free-flight arrival for urban air mobility

## Quick Facts
- arXiv ID: 2404.03710
- Source URL: https://arxiv.org/abs/2404.03710
- Authors: Martin Waltz; Ostap Okhrin; Michael Schultz
- Reference count: 30
- Key outcome: Deep reinforcement learning policy for decentralized urban air mobility vertiport arrival operations learns to form clockwise holding patterns and safely enter vertiport when permitted, showing high safety and efficiency with robust performance under sensor noise and varying traffic conditions.

## Executive Summary
This paper presents a decentralized deep reinforcement learning approach for urban air mobility vertiport arrival operations. The system uses a shared recurrent neural network policy that enables multiple aircraft to coordinate collision-free behavior without explicit communication, relying only on local observations. Aircraft follow a learned policy to maintain safe distances, form holding patterns, and enter the vertiport when permitted. The approach is validated through extensive simulations and real-world tests with small drones, demonstrating high safety, efficiency, and robustness to environmental variations.

## Method Summary
The approach employs multi-agent reinforcement learning with parameter sharing across aircraft. Each aircraft uses a recurrent neural network (LSTM-TD3 algorithm) to process local observations and historical information, enabling decentralized decision-making without communication. Training uses curriculum learning, starting with few aircraft and gradually increasing traffic density up to 25 vehicles. A balanced replay buffer ensures equal learning of both staying-in-airspace and entering-vertiport tasks. The policy learns continuous heading adjustments to maintain safe separation, form clockwise holding patterns, and enter the vertiport when signaled.

## Key Results
- Policy learns to form clockwise holding patterns and safely enter vertiport when permitted
- High safety achieved with no accidents in most scenarios, though 5 incidents occurred in highest density scenarios
- Efficient operation demonstrated with linear increases in airspace time as traffic density grows
- Policy shows robustness to sensor noise (up to 100m positional noise) and variations in inbound traffic distribution
- Successful transfer to small-scale Crazyflie drones without retraining required

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The shared recurrent policy enables scalable multi-agent coordination without communication.
- Mechanism: Each aircraft uses the same neural network policy parameterized by local observations and history. The recurrent LSTM component integrates temporal information across multiple time steps, allowing each agent to infer the likely future positions of others without explicit message passing. This enables decentralized, collision-free behavior even as the number of aircraft scales.
- Core assumption: Local observations contain sufficient information to infer surrounding aircraft intentions when combined with recurrent history.
- Evidence anchors:
  - [abstract] "Each aircraft is considered an individual agent and follows a shared policy, resulting in decentralized actions that are based on local information."
  - [section 4.4] "We assume that each of the Nt aircraft follows the same shared policy but takes decentralized actions based on its local observation."
  - [corpus] Weak - no direct corpus papers address recurrent parameter sharing for UAM, though similar approaches exist in maritime traffic.
- Break condition: If observation noise or limited sensor range prevents accurate inference of nearby aircraft intentions, the shared policy cannot coordinate effectively.

### Mechanism 2
- Claim: Curriculum learning with gradually increasing traffic density enables effective training.
- Mechanism: Training starts with few aircraft (3-8), allowing the policy to learn basic collision avoidance and vertiport entry. As training progresses, more aircraft are added (up to 25), forcing the policy to adapt to increasingly complex scenarios. This staged complexity prevents the policy from being overwhelmed early and builds robust skills incrementally.
- Core assumption: Skills learned in simpler scenarios transfer effectively to more complex scenarios.
- Evidence anchors:
  - [section 4.5] "We pursue a curriculum learning strategy... to increase the number of vehicles in the airspace gradually."
  - [section 5.1] "The blue curve labeled 'CL' represents the curriculum learning strategy... For comparison, we include the green curve labeled 'No CL', where 25 vehicles are generated immediately after the start of training, resulting in high initial complexity that blocks learning."
  - [corpus] Weak - no corpus papers explicitly use curriculum learning for UAM, though the concept is well-established in RL literature.
- Break condition: If skills learned in simple scenarios don't transfer to complex ones, or if the policy overfits to specific traffic patterns from early training.

### Mechanism 3
- Claim: The balanced replay buffer ensures adequate learning of both staying-in-airspace and entering-vertiport tasks.
- Mechanism: The replay buffer is constructed so that 80% of experience involves staying in the airspace (σO,t = -1) and 20% involves entering the vertiport (σO,t = 1). This prevents the policy from overfitting to either task and ensures it learns both components equally well. The main agent is selected to maintain this balance.
- Core assumption: Both tasks are equally important and require balanced representation in training data.
- Evidence anchors:
  - [section 4.6] "During training, we randomly select one of the Nt agents in the airspace as the main agent... For the initial 200 steps of each training episode, all aircraft are instructed not to enter the VTOL zone (σO,t = -1). Subsequently, only the main agent receives an entrance signal (σO,t = 1)."
  - [section 4.6] "we thus deviate from the minimum Euclidean distance rule for selecting the next landing aircraft during training."
  - [corpus] Weak - no corpus papers address replay buffer balancing for UAM, though the general technique is well-known in RL.
- Break condition: If the 80/20 split is suboptimal for the specific task distribution, or if the main agent selection doesn't adequately represent the full state space.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The UAM arrival problem is formalized as an MDP where each aircraft chooses actions (heading changes) to maximize long-term rewards (safety and efficiency).
  - Quick check question: What are the components of an MDP tuple (S, A, P, R, γ) and how do they map to the UAM problem?

- Concept: Partial Observable MDP (POMDP)
  - Why needed here: Aircraft only have local observations of surrounding vehicles, not the full state of the airspace. The LSTM component helps handle this partial observability.
  - Quick check question: How does the POMDP framework extend the MDP to handle partial observability, and why is this relevant for decentralized aircraft coordination?

- Concept: Curriculum Learning
  - Why needed here: Starting with simple scenarios (few aircraft) and gradually increasing complexity prevents the policy from being overwhelmed and enables effective learning.
  - Quick check question: What is curriculum learning, and how does it differ from training on complex scenarios from the beginning?

## Architecture Onboarding

- Component map:
  Observation space -> Spatial LSTM -> Temporal LSTM -> Fully connected layers -> Action space
  (own state + up to 24 surrounding aircraft states) -> (each neighbor) -> (multi-step history) -> (continuous heading change)

- Critical path:
  1. Aircraft observes local state and up to 24 nearest neighbors
  2. Spatial LSTM processes each neighbor's information
  3. Temporal LSTM aggregates multi-step history
  4. Fully connected layers output continuous action
  5. Action applied to heading, new state observed
  6. Experience stored in replay buffer
  7. Periodically sampled for training

- Design tradeoffs:
  - Shared policy vs. individual policies: Shared policy enables scalability but may limit individual optimization
  - Recurrent vs. feed-forward: Recurrent handles partial observability but increases complexity
  - Continuous vs. discrete actions: Continuous allows smoother control but requires more sophisticated algorithms
  - Local vs. global observations: Local enables decentralization but may miss important global patterns

- Failure signatures:
  - Policy consistently produces heading changes that point directly at other aircraft (collision avoidance failing)
  - Aircraft oscillate back and forth without making progress (comfort reward too strong or action space too limited)
  - Aircraft cluster near vertiport but never enter (goal reward not properly weighted)
  - Training returns plateau early (curriculum too aggressive or replay buffer unbalanced)

- First 3 experiments:
  1. Single aircraft in empty airspace: Verify policy produces smooth, continuous actions and aircraft doesn't crash into vertiport boundary
  2. Two aircraft approaching head-on: Verify policy produces evasive maneuvers and prevents collisions
  3. Multiple aircraft in holding pattern: Verify policy produces clockwise/anticlockwise motion and allows safe vertiport entry when signaled

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the decentralized deep reinforcement learning approach perform if communication between aircraft were allowed, enabling cooperative behavior?
- Basis in paper: [explicit] The paper mentions that relaxing the non-cooperative assumption and allowing communication could leverage shared intentions among vehicles, potentially enhancing safety.
- Why unresolved: The current work assumes a non-cooperative setting where aircraft cannot communicate, leaving the potential benefits of cooperative communication unexplored.
- What evidence would resolve it: Implementing a cooperative variant of the deep reinforcement learning approach with inter-vehicle communication and comparing its safety and efficiency performance against the current non-cooperative method.

### Open Question 2
- Question: What is the impact of using different vehicle dynamics models (e.g., aircraft with hovering capabilities) on the performance and safety of the learned policy?
- Basis in paper: [inferred] The paper assumes vehicles cannot hover and can only adjust heading, which simplifies the problem but may not reflect all potential UAM vehicles.
- Why unresolved: The policy is specifically trained and validated under the assumption of non-hovering vehicles, and its performance with other vehicle dynamics is unknown.
- What evidence would resolve it: Training and evaluating the deep reinforcement learning policy using different vehicle dynamics models, including those with hovering capabilities, and comparing the resulting safety and efficiency metrics.

### Open Question 3
- Question: How does the performance of the deep reinforcement learning policy scale with the number of vertiports and the complexity of the airspace network?
- Basis in paper: [explicit] The paper focuses on a single vertiport with a circular airspace, leaving the scalability to multiple vertiports unexplored.
- Why unresolved: The current approach is designed for a single vertiport scenario, and its effectiveness in a more complex network of vertiports is not addressed.
- What evidence would resolve it: Extending the deep reinforcement learning approach to handle multiple vertiports and evaluating its performance in terms of safety, efficiency, and scalability in a more complex airspace network.

## Limitations

- Neural network architecture details and exact hyperparameters are not fully specified, making reproduction challenging
- Training relies on simulation data without extensive real-world validation beyond limited drone tests
- Safety metrics show 5 incidents in highest density scenarios, indicating potential limitations in extreme conditions
- Policy may overfit to specific training progression used in curriculum learning approach

## Confidence

- High confidence: The basic reinforcement learning framework and training methodology are sound and well-established in the literature
- Medium confidence: The policy's ability to learn effective holding patterns and vertiport entry is demonstrated, but real-world performance may differ
- Low confidence: The exact neural network architecture and hyperparameters that led to successful training are not fully specified, making reproduction challenging

## Next Checks

1. Implement the full neural network architecture with exact LSTM configurations and validate that similar performance can be achieved with the described training procedure
2. Test the policy in higher density scenarios (25+ aircraft) to identify performance degradation points and failure modes
3. Conduct extensive real-world flights with the Crazyflie drones in varied weather conditions and environments to assess robustness beyond the controlled experimental setup