---
ver: rpa2
title: Dynamic Anisotropic Smoothing for Noisy Derivative-Free Optimization
arxiv_id: '2405.01731'
source_url: https://arxiv.org/abs/2405.01731
tags:
- function
- optimization
- window
- algorithm
- smoothing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel algorithm for noisy derivative-free
  optimization that dynamically adapts the shape of the smoothing kernel to match
  the Hessian of the objective function. The algorithm extends ball smoothing and
  Gaussian smoothing methods by incorporating anisotropic curvature information, significantly
  reducing gradient estimation error.
---

# Dynamic Anisotropic Smoothing for Noisy Derivative-Free Optimization

## Quick Facts
- arXiv ID: 2405.01731
- Source URL: https://arxiv.org/abs/2405.01731
- Authors: Sam Reifenstein; Timothee Leleu; Yoshihisa Yamamoto
- Reference count: 17
- Primary result: Novel algorithm dynamically adapts smoothing kernel shape to match Hessian, significantly reducing gradient estimation error in noisy derivative-free optimization

## Executive Summary
This paper introduces Dynamic Anisotropic Smoothing (DAS), a novel algorithm for noisy derivative-free optimization that dynamically adapts the shape of the smoothing kernel to match the Hessian of the objective function. The method extends traditional ball and Gaussian smoothing by incorporating anisotropic curvature information, enabling more accurate gradient estimation in the presence of noise. DAS simultaneously updates the optimization parameters and the smoothing window's shape and size using gradient ascent on a smoothed objective function.

## Method Summary
DAS is a derivative-free optimization algorithm that uses a multivariate Gaussian distribution with adaptive covariance matrix L to sample points around the current estimate x. The algorithm estimates the gradient of a smoothed objective function using these samples, then updates both x and L using gradient ascent. The window size and shape are dynamically adjusted based on the curvature of the objective function, with a growth term that prevents the window from collapsing too quickly. The method is particularly effective for tuning NP-hard combinatorial optimization solvers, where the fitness function exhibits heterogeneous curvature.

## Key Results
- DAS outperforms state-of-the-art derivative-free and Bayesian optimization methods on artificial test functions with varying dimensions and noise levels
- The algorithm demonstrates improved performance in tuning NP-hard combinatorial optimization solvers, particularly for SAT and Ising/QUBO problems
- Numerical experiments show DAS effectively handles heterogeneous curvature of fitness functions while maintaining robustness to noise

## Why This Works (Mechanism)

### Mechanism 1
Dynamic adaptation of the smoothing kernel shape to match the Hessian reduces gradient estimation error. The algorithm maintains a covariance matrix L in the sampling distribution and updates it using gradient ascent on a smoothed objective function, where the smoothing kernel's shape is adapted to approximate the Hessian of the objective function. This allows the sampling distribution to align with the curvature of the objective function, improving gradient estimation accuracy. The core assumption is that the Hessian is constant or slowly varying near the optimum.

### Mechanism 2
Sampling according to a Gaussian distribution with covariance L provides an unbiased estimate of the gradient of the smoothed objective function. The algorithm samples points from a Gaussian distribution with mean x and covariance L, then uses these samples to estimate the gradient of the smoothed objective function. The gradient estimates are unbiased because the expectation of the sampled gradients equals the true gradient. The core assumption is that the sampling distribution is correctly parameterized by L and objective function evaluations are unbiased.

### Mechanism 3
Dynamic adjustment of the sampling window size and shape allows the algorithm to balance exploration and exploitation in the presence of noise. The algorithm starts with a large sampling window to explore the parameter space and gradually shrinks it as it approaches the optimum. This allows coarse-grained progress early on and fine-grained progress later. The dynamic adjustment of window shape also adapts to anisotropic curvature of the objective function. The core assumption is that the objective function is smooth enough for Gaussian smoothing to provide a good approximation.

## Foundational Learning

- Gradient ascent/descent: Used to optimize the smoothed objective function. Quick check: What is the update rule for gradient ascent?
- Covariance matrix and multivariate Gaussian distribution: Used to sample points from the sampling distribution. Quick check: How do you sample from a multivariate Gaussian distribution with a given mean and covariance matrix?
- Hessian matrix and its role in optimization: The algorithm adapts the sampling kernel shape to match the Hessian. Quick check: What information does the Hessian matrix provide about the curvature of a function?

## Architecture Onboarding

- Component map: Sampling distribution -> Objective function evaluator -> Gradient estimator -> Update rule
- Critical path: Sampling -> Objective function evaluation -> Gradient estimation -> Update L and x
- Design tradeoffs: Larger sampling window size allows better exploration but slower convergence; smaller window size allows faster convergence but may get stuck in local optima; more samples per iteration provide better gradient estimates but increase computational cost
- Failure signatures: Algorithm gets stuck in local optima, fails to converge or converges slowly, gradient estimates are inaccurate or noisy
- First 3 experiments: 1) Test on simple quadratic function with known Hessian to verify kernel shape adaptation; 2) Test on noisy quadratic function to verify noise handling; 3) Test on high-dimensional non-quadratic function to verify complex curvature handling

## Open Questions the Paper Calls Out

### Open Question 1
How does the convergence rate of DAS compare to second-order optimization methods when gradient information is available? The paper focuses on derivative-free settings and does not compare to gradient-based methods. This remains unresolved because the algorithm is designed for noisy, derivative-free scenarios. Theoretical analysis and numerical experiments comparing DAS to second-order methods would provide insights into relative performance.

### Open Question 2
What is the theoretical justification for the choice of the growth term 位 in equation (9) and its impact on the algorithm's performance? The paper mentions 位 controls window size growth but lacks theoretical justification for its choice. This remains unresolved because the choice appears heuristic. Theoretical analysis of convergence properties as a function of 位 and numerical experiments varying 位 would provide insights into optimal selection.

### Open Question 3
How does DAS perform on combinatorial optimization problems with discrete parameters, and can it be extended to handle such problems effectively? The paper focuses on continuous optimization problems and demonstrates effectiveness on tuning continuous parameters of combinatorial solvers, but does not address discrete parameter problems. This remains unresolved because many combinatorial problems have discrete parameters. Numerical experiments on discrete problems and theoretical analysis of behavior in such settings would provide insights.

## Limitations
- Theoretical analysis relies on assumptions about bounded gradients and convergence conditions that may not hold in practice
- Empirical evaluation is limited in scope and lacks comprehensive comparison across diverse problem domains
- Algorithm's performance depends on hyperparameter selection, which is not fully characterized

## Confidence

- High confidence: Dynamic adaptation of smoothing kernel shape to approximate Hessian is well-supported by theoretical analysis and numerical experiments
- Medium confidence: Effectiveness in handling noise and improving optimization performance is supported by empirical results but could be strengthened with more extensive testing
- Low confidence: Theoretical guarantees on convergence and error bounds are not rigorously proven and assumptions may not hold in practice

## Next Checks
1. Conduct comprehensive empirical study comparing DAS to other state-of-the-art derivative-free optimization methods on diverse benchmark problems, including high-dimensional and non-convex functions
2. Derive rigorous mathematical proofs of convergence and error bounds for DAS algorithm under various assumptions on objective function and noise characteristics
3. Investigate sensitivity of algorithm to hyperparameters (initial window size, learning rates) and develop practical guidelines for their selection