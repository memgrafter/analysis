---
ver: rpa2
title: Test-Time Multimodal Backdoor Detection by Contrastive Prompting
arxiv_id: '2405.15269'
source_url: https://arxiv.org/abs/2405.15269
tags:
- backdoor
- detection
- clip
- attack
- bdetclip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first test-time backdoor detection method
  for CLIP, named BDetCLIP, addressing the vulnerability of multimodal contrastive
  learning models to backdoor attacks. The method exploits the observation that backdoored
  images exhibit insensitive alignment between visual representations and text prompts
  when class description texts are significantly changed.
---

# Test-Time Multimodal Backdoor Detection by Contrastive Prompting

## Quick Facts
- arXiv ID: 2405.15269
- Source URL: https://arxiv.org/abs/2405.15269
- Authors: Yuwei Niu; Shuo He; Qi Wei; Zongyu Wu; Feng Liu; Lei Feng
- Reference count: 40
- Primary result: First test-time backdoor detection method for CLIP achieving AUROC up to 0.998

## Executive Summary
This paper introduces BDetCLIP, the first test-time backdoor detection method specifically designed for multimodal contrastive learning models like CLIP. The method exploits a key observation: backdoored images exhibit insensitive alignment between visual representations and class description texts when the textual prompts are significantly changed. By leveraging GPT-4 to generate class-related (benign) and class-perturbed random (malignant) text prompts, BDetCLIP computes the distribution difference in cosine similarity between images and these prompts to detect backdoor samples. Extensive experiments demonstrate superior performance across multiple attack types and datasets while requiring no parameter updates during inference.

## Method Summary
BDetCLIP generates benign and malignant text prompts using GPT-4, then detects backdoor samples by computing the distribution difference in cosine similarity between images and these prompts. For each test image, cosine similarity is calculated between the image and all benign prompts, and between the image and all malignant prompts. The distribution difference, measured by the Ω(x) statistic, serves as the detection criterion - backdoored samples exhibit smaller differences due to their weak semantic alignment with class descriptions. A threshold determined from clean validation data classifies samples as backdoored or clean.

## Key Results
- Achieves AUROC up to 0.998 across multiple attack types and datasets
- Outperforms state-of-the-art unimodal detection methods in both effectiveness and efficiency
- Requires no parameter updates during inference, making it suitable for real-world deployment
- Successfully detects backdoor samples from diverse attack methods including BadNet, Blended, ISSBA, WaNet, and BadCLIP variants

## Why This Works (Mechanism)

### Mechanism 1
Backdoored images produce visual representations with less semantic alignment to class description texts compared to clean images. The backdoor trigger is a non-semantic pixel pattern that does not capture the rich semantic attributes present in class description texts. When class prompts change semantically, clean images' representations shift significantly while backdoored images' representations remain relatively unchanged due to their reduced semantic content.

### Mechanism 2
The distribution difference in cosine similarity between images and two types of class prompts can effectively distinguish backdoored from clean images. Clean images show large differences between benign and malignant similarity distributions due to their strong semantic alignment with meaningful prompts, while backdoored images show smaller differences due to their weak semantic alignment.

### Mechanism 3
GPT-4 can generate high-quality class-related and class-perturbed random texts that effectively create the benign and malignant prompt distributions needed for detection. GPT-4's in-context learning capabilities allow it to generate multiple fine-grained attribute-based sentences for a given class (benign prompts) and random sentences unrelated to the class (malignant prompts).

## Foundational Learning

- Concept: Contrastive learning and CLIP architecture
  - Why needed here: Understanding how CLIP aligns visual and textual representations is crucial for understanding why backdoor triggers affect this alignment differently for clean vs. backdoored images.
  - Quick check question: How does CLIP compute similarity between images and text, and what role does the cosine similarity play in this process?

- Concept: Backdoor attacks in machine learning
  - Why needed here: Understanding how backdoor attacks work and their effects on model behavior is essential for designing detection mechanisms that can identify compromised models.
  - Quick check question: What distinguishes a backdoor attack from other types of adversarial attacks, and how does the trigger affect model behavior during inference?

- Concept: Statistical distribution analysis and thresholding
  - Why needed here: The detection method relies on comparing distributions of similarity scores and setting appropriate thresholds to classify samples, requiring understanding of statistical analysis techniques.
  - Quick check question: How do you determine an appropriate threshold for classifying samples based on a distribution difference metric?

## Architecture Onboarding

- Component map:
  Input images -> GPT-4 prompt generation -> CLIP visual and textual encoders -> Cosine similarity computation -> Distribution difference calculation -> Threshold comparison -> Classification

- Critical path:
  1. For each test image, compute cosine similarity with all benign prompts
  2. For each test image, compute cosine similarity with all malignant prompts
  3. Calculate distribution difference Ω(x)
  4. Compare Ω(x) with threshold ϵ
  5. Output classification result

- Design tradeoffs:
  - Number of class-specific benign prompts (m): More prompts provide better discrimination but increase computation time
  - Text length of malignant prompts: Shorter random sentences preserve more contrast but may be less random
  - Choice of LLM: GPT-4 provides high-quality prompts but has cost and availability constraints; open-source models may be cheaper but potentially less effective

- Failure signatures:
  - High false positive rate: Threshold set too low or prompts not generating sufficient semantic contrast
  - High false negative rate: Threshold set too high or prompts not capturing enough semantic information
  - Performance degradation with different CLIP architectures: Method may need tuning for different visual encoders

- First 3 experiments:
  1. Test on a small set of known clean and backdoored images to verify the distribution difference property holds
  2. Vary the number of benign prompts (m) to find the optimal tradeoff between performance and efficiency
  3. Test with different text lengths for malignant prompts to optimize the semantic contrast while maintaining randomness

## Open Questions the Paper Calls Out

### Open Question 1
How does BDetCLIP perform when defending against backdoor attacks with multiple semantic triggers? The paper mentions considering "semantic backdoor triggers" but only provides results for one semantic trigger ("Hello Kitty").

### Open Question 2
Can BDetCLIP maintain effectiveness when backdoor triggers are designed to be semantically aligned with class descriptions? The paper's defense relies on triggers being "insensitive to significant changes in class description texts" - this assumption may break down if triggers are designed to be semantically coherent with class attributes.

### Open Question 3
How does the performance of BDetCLIP scale when applied to large-scale vision-language models beyond CLIP? The paper states "only the CLIP model is considered because existing backdoor research on multimodal contrastive learning commonly considers CLIP as a representative victim model."

## Limitations
- Performance may degrade if backdoor triggers incorporate semantic meaning related to the target class
- Reliance on GPT-4 introduces dependency constraints and potential failure points
- Effectiveness may vary across different CLIP architectures and requires potential tuning

## Confidence
- High confidence: The empirical demonstration of distribution difference between clean and backdoored images (AUROC up to 0.998)
- Medium confidence: The generalizability of detection effectiveness across all possible backdoor attack variants
- Medium confidence: The necessity of GPT-4 specifically versus alternative language models

## Next Checks
1. Test BDetCLIP's effectiveness against backdoor triggers that incorporate semantic meaning related to the target class, measuring whether the distribution difference property still holds.

2. Evaluate BDetCLIP performance using open-source language models (e.g., GPT-3.5, LLaMA) for prompt generation to assess the impact of model choice on detection accuracy.

3. Conduct a time-to-failure analysis by progressively adding backdoor samples to the test set, measuring how quickly BDetCLIP's performance degrades as the proportion of backdoored samples increases.