---
ver: rpa2
title: 'FTuner: A Fast Dynamic Shape Tensors Program Auto-Tuner for Deep Learning
  Compilers'
arxiv_id: '2407.21418'
source_url: https://arxiv.org/abs/2407.21418
tags:
- ftuner
- ukernel
- shape
- tensor
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FTuner, a fast dynamic shape tensor program
  auto-tuner for deep learning compilers. Dynamic shape tensors, where input dimensions
  vary at runtime, pose significant challenges for existing deep learning compilers
  that typically optimize for fixed shapes.
---

# FTuner: A Fast Dynamic Shape Tensors Program Auto-Tuner for Deep Learning Compilers

## Quick Facts
- **arXiv ID**: 2407.21418
- **Source URL**: https://arxiv.org/abs/2407.21418
- **Reference count**: 40
- **Primary result**: Reduces tuning time by two orders of magnitude compared to state-of-the-art approaches while achieving comparable performance to vendor libraries on nearly half of tested shapes

## Executive Summary
This paper introduces FTuner, a fast dynamic shape tensor program auto-tuner for deep learning compilers that addresses the challenge of optimizing tensor programs when input dimensions vary at runtime. Unlike existing approaches that rely on expensive cost model training or padding tensors to fixed sizes, FTuner uses an analytic hardware information model to generate abstract computational units called uKernels. The compilation phase creates high-performance uKernels through hardware alignment and parallelism analysis, while the runtime phase combines these uKernels to construct programs with minimal padding for any input shape.

## Method Summary
FTuner introduces uKernels as abstract computational units that patch together small tensors to match dynamic input shapes. Instead of using large design spaces or training cost models, FTuner determines uKernel shapes using an analytic hardware information model that analyzes hardware alignment constraints, parallelism limits, and multi-axis properties. During compilation, FTuner generates high-performance uKernels based on these hardware characteristics. At runtime, different uKernels are combined to create programs that minimize padding, and the Synthesis Index Analysis (SIA) metric is used to evaluate and select the best-performing program without measurement-on-device.

## Key Results
- Achieves comparable performance to vendor libraries on nearly half of tested dynamic tensor shapes
- Provides a 3% speedup over existing model-training auto-tuners
- Reduces tuning time by two orders of magnitude compared to state-of-the-art approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Using abstract computational units (uKernel) eliminates the need for cost model training
- **Mechanism**: uKernels are generated analytically from hardware alignment constraints and performance metrics rather than through iterative search
- **Core assumption**: Hardware characteristics can be expressed as deterministic constraints that yield optimal uKernel configurations
- **Evidence anchors**: [abstract] "Instead of using a large design space or training a cost model, we use an abstract computational unit called the uKernel to patch together small, various-sized tensors to match the shape of the input tensor"; [section] "We determine the shape of the uKernel using an analytic hardware information model"
- **Break condition**: Hardware models become inaccurate due to thermal effects, voltage/frequency scaling, or non-deterministic behavior

### Mechanism 2
- **Claim**: Multi-axis tiling reduces padding more effectively than single-kernel approaches
- **Mechanism**: By selecting different uKernel sizes for each tensor axis, FTuner achieves exact divisibility without padding in at least one dimension
- **Core assumption**: Dynamic tensor axes can be decomposed into combinations of available uKernel sizes
- **Evidence anchors**: [section] "We iterate through all uKernels... First, we check if Kτ is a factor of H (line 5)... H is reduced by Kτ in the next search round, making H=46 as shown in line 10"; [section] "This process continues until we find the perfect division, i.e., H=(7,8), stored in the CombinSet, as shown in line 18"
- **Break condition**: When all tensor dimensions are prime numbers that don't match available uKernel sizes

### Mechanism 3
- **Claim**: Synthesis Index Analysis (SIA) provides near-optimal performance without measurement-on-device
- **Mechanism**: Weighted sum of compute-to-memory ratio, padding threshold, and resource utilization predicts performance
- **Core assumption**: The three metrics in SIA are strongly correlated with actual execution time
- **Evidence anchors**: [section] "We calculate the SIA score using a weighted sum of three metrics... According to the SIA score rank, all uKernel-based programs in the uProg are"; [section] "Experiment results show that the techniques used by FTuner outperform state-of-the-art compilers on dynamic tensors"
- **Break condition**: When hardware behavior is non-linear or when microarchitectural effects dominate performance

## Foundational Learning

- **Concept**: Hardware alignment constraints for GPU kernels
  - **Why needed here**: uKernel generation depends on understanding shared memory, register tile sizes, and thread block dimensions
  - **Quick check question**: What is the typical alignment requirement for float32 data on NVIDIA GPUs?

- **Concept**: Dynamic shape tensor representation
  - **Why needed here**: FTuner must handle tensors where one or more dimensions vary at runtime
  - **Quick check question**: How does padding affect the computational efficiency of dynamic shape tensors?

- **Concept**: Performance metrics for GPU kernels
  - **Why needed here**: SIA uses compute-to-memory ratio, padding threshold, and resource utilization to rank programs
  - **Quick check question**: Which metric would be most important for a memory-bound operator?

## Architecture Onboarding

- **Component map**: ONNX graph processing and operator fusion -> uKernel generation through hardware alignment, parallelism constraints, and multi-axis analysis -> Program construction via combination algorithm and evaluation via SIA -> TVM-based code generation
- **Critical path**: 
  1. ONNX graph → fused operators
  2. Operator → tensor program → uKernel candidates
  3. uKernel filtering → high-performance uKernel set
  4. Runtime input shape → program construction → SIA evaluation → execution
- **Design tradeoffs**:
  - Pre-computation vs runtime flexibility: uKernels are generated at compile time but combined at runtime
  - Search space vs tuning time: Hardware alignment constraints reduce search space but may miss some optimal configurations
  - Precision vs performance: SIA provides fast evaluation but may not capture all hardware effects
- **Failure signatures**:
  - Poor performance on prime-numbered dimensions
  - Suboptimal SM utilization on small tensor shapes
  - Runtime errors when input shapes exceed pre-generated uKernel ranges
- **First 3 experiments**:
  1. Measure compilation time for Dense operator with 128 shapes vs HAOTuner
  2. Compare padding percentage between FTuner and Roller on V100
  3. Verify SIA accuracy by comparing predicted vs measured performance on a small program set

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but several areas remain unexplored based on the experimental results and methodology:

1. Can FTuner's uKernel approach be extended to support other types of dynamic shape tensors beyond sequence length, such as variable image sizes or video frames with different resolutions?
2. How would FTuner perform on edge devices or embedded systems with more constrained hardware resources compared to the NVIDIA GPUs tested in the evaluation?
3. Can the uKernel generation process in FTuner be further optimized to reduce the initial compilation time, especially for models with many dynamic shapes?

## Limitations

- The exact implementation details of the RegTileRule(), SmemTileRule(), and CrossPick() algorithms are not specified, making faithful reproduction challenging
- The specific hyperparameters and default values for stride size and compute-to-memory ratio thresholds are not provided
- The paper doesn't adequately address how the hardware analytic model handles non-deterministic GPU behavior like thermal throttling or voltage/frequency scaling

## Confidence

- Hardware analytic model claims: Medium
- Multi-axis tiling effectiveness: Medium
- SIA metric accuracy: Low
- Performance improvement claims: Medium

## Next Checks

1. Measure compilation time reduction for Dense operator with 128 shapes compared to HAOTuner on the same hardware platform
2. Compare padding percentages between FTuner and Roller on V100 GPU across a diverse set of dynamic tensor shapes
3. Validate SIA accuracy by comparing predicted vs measured performance on a small program set with ground truth measurements from actual device execution