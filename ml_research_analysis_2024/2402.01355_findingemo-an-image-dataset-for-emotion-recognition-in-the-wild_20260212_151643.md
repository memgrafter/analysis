---
ver: rpa2
title: 'FindingEmo: An Image Dataset for Emotion Recognition in the Wild'
arxiv_id: '2402.01355'
source_url: https://arxiv.org/abs/2402.01355
tags:
- emotion
- images
- image
- arousal
- valence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FindingEmo is a new dataset of 25k images annotated for emotion
  recognition in complex social scenes. Unlike existing datasets that focus on single
  individuals or faces, it annotates entire scenes for valence, arousal, and emotion
  using Plutchik's Wheel of Emotions.
---

# FindingEmo: An Image Dataset for Emotion Recognition in the Wild

## Quick Facts
- **arXiv ID**: 2402.01355
- **Source URL**: https://arxiv.org/abs/2402.01355
- **Reference count**: 40
- **Primary result**: New 25k-image dataset for emotion recognition in complex social scenes using Plutchik's Wheel of Emotions

## Executive Summary
FindingEmo is a new dataset of 25k images annotated for emotion recognition in complex social scenes. Unlike existing datasets that focus on single individuals or faces, it annotates entire scenes for valence, arousal, and emotion using Plutchik's Wheel of Emotions. The dataset was collected via web scraping and crowdsourced annotations from 655 participants. Baseline results using popular ImageNet architectures show moderate performance, with VGG16 performing best for emotion classification and Visual Transformers excelling at arousal and valence prediction. The dataset provides a challenging benchmark for emotion recognition in naturalistic social settings and highlights the complexity of human emotion perception in context.

## Method Summary
FindingEmo was created by web scraping 25,000 images from online sources, focusing on complex social scenes with multiple people. Each image was annotated by 655 crowdsourced participants who rated valence (pleasantness), arousal (intensity), and mapped emotions using Plutchik's Wheel of Emotions. The dataset explicitly targets emotion recognition in the wild, capturing the complexity of real-world social interactions where emotions are influenced by contextual elements beyond individual facial expressions. The annotations capture both the emotional content of the scene and the complex interplay between people and their environment.

## Key Results
- VGG16 architecture achieved the best performance for emotion classification on the FindingEmo dataset
- Visual Transformers outperformed other architectures for predicting arousal and valence values
- Baseline performance was characterized as moderate, indicating the dataset's challenging nature for emotion recognition
- The dataset demonstrates the difficulty of recognizing emotions in complex social scenes compared to individual face-based approaches

## Why This Works (Mechanism)
The dataset works by capturing the naturalistic complexity of emotional scenes through crowdsourced annotations. By collecting ratings from 655 participants for valence, arousal, and specific emotions using Plutchik's framework, the dataset aggregates diverse human perceptions of emotional content in social contexts. This approach acknowledges that emotion recognition in the wild involves understanding relationships between multiple people, contextual cues, and environmental factors that influence emotional interpretation beyond isolated facial expressions.

## Foundational Learning

**Crowdsourcing methodology**: Why needed - To gather diverse human perspectives on emotional content in images. Quick check - Verify inter-rater reliability metrics and consensus thresholds for annotation quality.

**Plutchik's Wheel of Emotions**: Why needed - Provides a structured framework for categorizing emotional states. Quick check - Validate that the 8 basic emotions and their combinations adequately capture the range of emotions perceived in complex social scenes.

**Emotion dimensions (valence and arousal)**: Why needed - These continuous dimensions provide a nuanced understanding of emotional intensity and pleasantness. Quick check - Ensure annotation scales are properly calibrated and normalized across the 655 participants.

## Architecture Onboarding

**Component map**: Web scraping -> Image preprocessing -> Crowdsourced annotation (valence/arousal/emotion) -> Dataset compilation -> Model training (ImageNet architectures) -> Performance evaluation

**Critical path**: The most important sequence is: image collection → annotation collection → quality validation → model training → baseline evaluation. Each step depends on the previous one, with annotation quality being particularly critical for downstream model performance.

**Design tradeoffs**: The choice of Plutchik's Wheel over other emotion frameworks represents a tradeoff between theoretical grounding and practical annotation feasibility. The decision to use crowdsourced annotations provides scale and diversity but introduces potential noise compared to expert annotations.

**Failure signatures**: Poor inter-rater reliability would indicate annotation inconsistency. Low baseline performance could suggest either genuine dataset difficulty or annotation quality issues. Misalignment between facial expression analysis and scene-level annotations would indicate the added value (or lack thereof) of context.

**First experiments**: 
1. Compute inter-rater reliability metrics across the 655 participants
2. Compare annotation distributions against established emotion datasets
3. Evaluate whether scene-level annotations improve prediction accuracy compared to face-only approaches

## Open Questions the Paper Calls Out
None

## Limitations
- The validation methodology for crowdsourced annotations lacks detailed inter-rater reliability metrics or consensus thresholds
- Performance claims require validation against established emotion recognition benchmarks to distinguish dataset difficulty from annotation noise
- Plutchik's Wheel of Emotions may not align with natural human perception of emotions in complex social scenes

## Confidence

| Claim | Confidence |
|-------|------------|
| FindingEmo provides a challenging benchmark for emotion recognition in naturalistic social settings | Medium |
| Baseline performance reflects genuine dataset difficulty rather than annotation noise | Medium |
| Scene-level annotations provide advantages over face-based emotion recognition | Low |

## Next Checks
1. Compute and report inter-rater reliability metrics (Krippendorff's alpha or Cohen's kappa) for the crowdsourced annotations to establish label quality
2. Compare baseline performance against established emotion recognition datasets (like AffectNet or GoEmotions) using identical model architectures
3. Conduct ablation studies to determine whether scene-level annotations provide meaningful advantages over face-based emotion recognition approaches in this dataset