---
ver: rpa2
title: 'Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU:
  Are We There Yet?'
arxiv_id: '2403.01929'
source_url: https://arxiv.org/abs/2403.01929
tags:
- language
- computational
- languages
- task
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically compares three learning paradigms\u2014\
  supervised fine-tuning (SFT), supervised instruction tuning (SIT), and in-context\
  \ learning (ICL)\u2014for few-shot multilingual natural language understanding (NLU)\
  \ across six languages (high- and low-resource), three tasks, and various domain\
  \ setups. While ICL is popular due to its simplicity, the results show that supervised\
  \ methods consistently outperform ICL, even with limited training data, across all\
  \ tasks and languages."
---

# Analyzing and Adapting Large Language Models for Few-Shot Multilingual NLU: Are We There Yet?

## Quick Facts
- arXiv ID: 2403.01929
- Source URL: https://arxiv.org/abs/2403.01929
- Reference count: 40
- Large language models (LLMs) consistently outperform in-context learning (ICL) for few-shot multilingual natural language understanding (NLU) when fine-tuned or instruction-tuned.

## Executive Summary
This paper systematically compares three learning paradigms—supervised fine-tuning (SFT), supervised instruction tuning (SIT), and in-context learning (ICL)—for few-shot multilingual natural language understanding (NLU) across six languages, three tasks, and various domain setups. While ICL is popular due to its simplicity, the results show that supervised methods consistently outperform ICL, even with limited training data, across all tasks and languages. SIT, in particular, offers the best trade-off between performance and resource efficiency, with lower computational and inference costs than ICL. Additionally, the study investigates the impact of target language adaptation of LLMs, finding that while adaptation improves generation fluency in the target language, it does not significantly enhance NLU capabilities, especially for low-resource languages. These findings highlight the importance of multilingual pretraining and supervised fine-tuning for effective few-shot NLU.

## Method Summary
The study evaluates three learning paradigms (SFT, SIT, ICL) on multilingual NLU tasks using the Multi3NLU++ dataset for intent detection and value extraction, and XNLI for natural language inference across six languages. Models tested include XLM-R, LaBSE, Flan-T5, mT0, LLaMA-2, and GPT-3.5 with varying training data sizes (30-1000 examples). The research analyzes performance metrics alongside computational, inference, and financial costs, while also examining the effects of target language adaptation through QLoRA on LLaMA-2 for ICL tasks.

## Key Results
- Supervised methods (SFT and SIT) consistently outperform ICL across all tasks and languages, even with limited training data
- SIT provides the best balance of performance and resource efficiency, with lower computational and inference costs than ICL
- Target language adaptation improves generation fluency but does not significantly enhance NLU capabilities, particularly for low-resource languages
- Multilingual pretraining remains crucial for effective few-shot NLU performance

## Why This Works (Mechanism)
The superior performance of supervised methods stems from their ability to adapt model parameters to specific tasks and languages through parameter updates, rather than relying solely on prompt engineering and context. This parameter adaptation allows the models to learn task-specific representations that generalize better across languages. SIT's effectiveness comes from its ability to learn instruction-following capabilities during training, making it more efficient at task execution during inference. The limited impact of target language adaptation on NLU capabilities suggests that generation fluency and understanding are governed by different underlying mechanisms in LLMs.

## Foundational Learning
- **Multilingual Pretraining**: Why needed - Provides cross-lingual representations that enable transfer learning across languages; Quick check - Compare performance of multilingual vs monolingual models on cross-lingual transfer tasks
- **Few-shot Learning**: Why needed - Enables model adaptation with limited labeled data; Quick check - Measure performance degradation as training data decreases
- **Instruction Following**: Why needed - Critical for SIT effectiveness; Quick check - Test model's ability to follow novel instructions not seen during training
- **Parameter Efficient Fine-tuning**: Why needed - Reduces computational costs while maintaining performance; Quick check - Compare full fine-tuning vs parameter efficient methods on task performance and resource usage

## Architecture Onboarding
- **Component Map**: Input Data -> Model Selection -> Learning Paradigm (SFT/SIT/ICL) -> Training/Adaptation -> Evaluation -> Cost Analysis
- **Critical Path**: Multi3NLU++/XNLI datasets → Model choice (XLM-R/LaBSE/Flan-T5/mT0/LLaMA-2/GPT-3.5) → Learning paradigm selection → Training/adaptation → Performance evaluation across languages/tasks
- **Design Tradeoffs**: SFT offers good performance but higher computational costs vs ICL's simplicity but lower performance; SIT balances both but requires careful instruction design
- **Failure Signatures**: ICL underperformance due to context limitations; target adaptation improving generation but not understanding; high computational costs for full fine-tuning
- **First Experiments**: 1) Baseline ICL performance across all tasks/languages; 2) SFT performance comparison with varying training data sizes; 3) SIT performance with different instruction templates

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the underlying reasons for the performance gap between SIT and ICL, potential hybrid approaches combining supervised fine-tuning and in-context learning, alternative adaptation strategies beyond QLoRA for improving NLU capabilities in low-resource languages, and the specific requirements of real-world applications that influence learning paradigm choice based on resource constraints.

## Limitations
- Limited to three specific tasks and six languages, which may not capture broader NLU challenges
- Exact implementation details of instruction templates for ICL tasks not fully specified
- Financial cost analysis lacks detailed breakdowns for precise replication
- Study focuses on resource efficiency but doesn't explore model compression or efficient inference strategies

## Confidence
- **High Confidence**: Supervised methods consistently outperform ICL across all tasks and languages
- **Medium Confidence**: SIT offers the best trade-off between performance and resource efficiency
- **Low Confidence**: Target language adaptation's limited impact on NLU capabilities, especially for low-resource languages

## Next Checks
1. Implement and test the exact ICL instruction templates as used in the original study to verify performance gap persistence
2. Conduct experiments varying fine-tuning and QLoRA adaptation hyperparameters to assess their impact on relative performance
3. Perform detailed breakdown of computational, inference, and financial costs across different hardware configurations to validate claimed resource efficiency