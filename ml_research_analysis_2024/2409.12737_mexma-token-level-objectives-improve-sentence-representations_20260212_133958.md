---
ver: rpa2
title: 'MEXMA: Token-level objectives improve sentence representations'
arxiv_id: '2409.12737'
source_url: https://arxiv.org/abs/2409.12737
tags:
- sentence
- mexma
- tokens
- latn
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEXMA integrates token-level and sentence-level objectives to improve
  cross-lingual sentence representations. It uses sentence embeddings to predict masked
  tokens across languages, updating both tokens and sentence representations directly.
---

# MEXMA: Token-level objectives improve sentence representations

## Quick Facts
- arXiv ID: 2409.12737
- Source URL: https://arxiv.org/abs/2409.12737
- Reference count: 40
- Key outcome: MEXMA achieves 9.60% xsim++ error rate vs 12.08% for SONAR, improving cross-lingual sentence representations through token-level objectives

## Executive Summary
MEXMA addresses the limitation of current cross-lingual sentence encoders that use only sentence-level objectives, which can lead to loss of lexical information and degraded sentence representations. The approach integrates token-level and sentence-level objectives by using sentence embeddings to predict masked tokens across languages, with both sentence representations and individual tokens directly updating the encoder. MEXMA demonstrates superior performance on bitext mining (9.60% error rate) and classification tasks (65.35% accuracy) compared to state-of-the-art models while using fewer parameters (277M vs 471M for LaBSE).

## Method Summary
MEXMA uses a symmetrical cross-unmasking architecture with four encoder instances (clean/masked for each language) to predict masked tokens in one language using sentence representations from another language. The model employs token-level MLM objectives, sentence-level alignment losses (MSE or contrastive), and a KoLeo loss for space utilization. Training uses AdamW optimizer for 300k steps with batch size 2400 and a 40% masking ratio on 81 languages from the NLLB-200 corpus.

## Key Results
- Achieves 9.60% xsim++ error rate on bitext mining vs 12.08% for SONAR
- Improves SentEval classification accuracy to 65.35% vs 63.02% for SONAR
- Uses 277M parameters vs 471M for LaBSE while maintaining performance
- Creates more focused sentence representations with skewed attention distributions (entropy ~2.5 vs ~3.4 for LaBSE)

## Why This Works (Mechanism)

### Mechanism 1
Token-level gradients improve sentence representation quality by preventing degradation of lexical information. Traditional cross-lingual sentence encoders only update through sentence-level objectives, losing token-level information. MEXMA's cross-unmasking task provides gradients for both tokens and sentence representations, preserving lexical information that would otherwise be lost.

### Mechanism 2
Cross-lingual token prediction forces semantic alignment of individual tokens across languages. By using sentence representations from one language to predict masked tokens in another language, MEXMA creates implicit alignment that is reinforced by explicit alignment losses. This encourages the model to learn that tokens with similar meanings across languages should have similar representations.

### Mechanism 3
Skewed attention distribution focusing on important tokens creates better sentence representations than uniform pooling. MEXMA uses CLS token attention pooling, creating a skewed distribution over tokens. Analysis shows LaBSE has more uniform attention while MEXMA has more skewed attention, and when MEXMA uses uniform pooling, performance drops significantly.

## Foundational Learning

- **Masked Language Modeling (MLM)**: MEXMA uses MLM heads to predict masked tokens, so understanding how MLM works is essential for implementing the cross-unmasking objective. *Quick check*: What is the difference between BERT's MLM objective and the cross-unmasking objective used in MEXMA?

- **Contrastive Learning**: The paper compares MEXMA to contrastive approaches like LaBSE and shows MEXMA can be combined with contrastive losses. Understanding how contrastive learning aligns representations is important for understanding the design space. *Quick check*: How does InfoNCE contrastive loss differ from the MSE alignment loss used in MEXMA?

- **Attention Mechanisms and Pooling**: MEXMA uses CLS token attention pooling to create sentence representations. Understanding how attention distributions affect representation quality is crucial for the analysis of why MEXMA works. *Quick check*: What happens to sentence representation quality when you replace attention pooling with mean pooling?

## Architecture Onboarding

- **Component map**: Input → Masking → Encoding → Cross-unmasking prediction → Token/CLS gradients → Encoder update
- **Critical path**: The encoder receives gradients from both token predictions and sentence alignment, making this the critical learning path
- **Design tradeoffs**: Token-level vs sentence-level objectives (token-level preserves lexical information but increases complexity); Symmetrical vs asymmetrical architecture (symmetrical enables clean-to-clean alignment but requires more parameters); MSE vs contrastive alignment (MSE enables non-contrastive training but may be less effective for some tasks)
- **Failure signatures**: Poor mining performance suggests token alignment isn't working; Poor downstream performance suggests sentence representations aren't capturing relevant information; High entropy in attention distribution suggests uniform pooling rather than focused attention; Tokens matching same-language tokens rather than translations suggests lack of cross-lingual alignment
- **First 3 experiments**:
  1. Train MEXMA without token-level gradients (only sentence-level updates) and compare to full MEXMA on mining tasks
  2. Replace MSE alignment with contrastive loss and evaluate impact on both mining and downstream tasks
  3. Test different masking ratios (20%, 40%, 60%) to find optimal balance between task difficulty and representation quality

## Open Questions the Paper Calls Out

### Open Question 1
How does MEXMA's token alignment across languages impact the quality of sentence representations for low-resource languages? The paper doesn't provide specific analysis or results showing how token alignment impacts sentence representation quality for low-resource languages.

### Open Question 2
Can MEXMA's approach be effectively scaled to handle a larger number of languages and modalities beyond text? The paper mentions the possibility of exploring MEXMA's scalability but doesn't provide experimental results or analysis on scaling to more languages or modalities.

### Open Question 3
How does the masking ratio used in MEXMA affect its performance across different languages and tasks? While the paper mentions that 30%-60% seems to be the best operating region, it doesn't provide a detailed analysis of how the masking ratio affects MEXMA's performance across different languages and tasks.

## Limitations

- Data scaling and generalization concerns with 15-25 million sentence pairs per language pair, with unclear performance on truly low-resource languages
- Architecture complexity requiring four encoder instances increases memory requirements and training complexity
- Alignment quality measurement relying on nearest-neighbor search may not capture all aspects of semantic alignment or handle challenging linguistic phenomena

## Confidence

**High Confidence**:
- Token-level gradients improve sentence representation quality by preventing degradation of lexical information
- MEXMA achieves better performance on bitext mining and classification tasks compared to baselines
- MEXMA creates more focused sentence representations with skewed attention distributions

**Medium Confidence**:
- Cross-lingual token prediction forces semantic alignment of individual tokens across languages
- MEXMA achieves better token alignment across languages compared to baselines
- Skewed attention distributions are beneficial compared to uniform pooling

**Low Confidence**:
- The specific optimal masking ratio of 40% for the cross-unmasking task
- MEXMA can maintain performance while scaling to smaller models (277M parameters)
- Generalizability of performance gains to languages or domains not represented in training data

## Next Checks

1. **Ablation Study on Token-Level Gradients**: Train a variant of MEXMA that removes the token-level objective while keeping the sentence-level cross-unmasking task. Compare performance on bitext mining and downstream tasks to determine whether token-level gradients are essential.

2. **Cross-Lingual Alignment Evaluation on Challenging Phenomena**: Evaluate token alignment quality on sentences containing polysemous words, idiomatic expressions, and words with context-dependent meanings. Use more sophisticated alignment metrics that account for hubness.

3. **Memory and Computational Efficiency Analysis**: Implement a more memory-efficient version of MEXMA that uses a single encoder with conditional masking rather than four separate encoder instances. Measure whether performance gains can be maintained with reduced computational requirements.