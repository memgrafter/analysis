---
ver: rpa2
title: 'VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding
  and Grounding?'
arxiv_id: '2404.05955'
source_url: https://arxiv.org/abs/2404.05955
tags:
- uni00000013
- mllms
- uni00000048
- uni00000011
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VisualWebBench, a comprehensive benchmark
  designed to evaluate the capabilities of Multimodal Large Language Models (MLLMs)
  in web page understanding and grounding. The benchmark consists of seven tasks across
  three levels of granularity: website, element, and action, covering 1.5K instances
  from 139 real websites.'
---

# VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?

## Quick Facts
- arXiv ID: 2404.05955
- Source URL: https://arxiv.org/abs/2404.05955
- Reference count: 29
- Introduces VisualWebBench to evaluate MLLMs' web page understanding across 7 tasks and 3 granularity levels

## Executive Summary
VisualWebBench is a comprehensive benchmark introduced to evaluate Multimodal Large Language Models (MLLMs) on web page understanding and grounding tasks. The benchmark consists of seven tasks across three levels of granularity - website, element, and action - covering 1.5K instances from 139 real websites. The authors evaluate 14 open-source MLLMs, commercial models including Gemini Pro, Claude-3 series, and GPT-4V(ision), revealing significant performance gaps. GPT-4V(ision) achieves the highest average score of 64.6, while the best open-source model LLaVA-1.6-34B scores 50.5. The study identifies key limitations in current MLLMs, particularly inadequate grounding in text-rich environments and poor performance with low-resolution image inputs.

## Method Summary
The authors constructed VisualWebBench by collecting 1.5K instances from 139 real websites, covering seven web page understanding tasks across three granularity levels: website-level tasks (categorization, main function identification), element-level tasks (target element localization, text attribute identification, relational position finding), and action-level tasks (action prediction, position generation). The benchmark uses high-resolution screenshots (1280 pixels width) to provide detailed visual information. The evaluation framework measures model performance through automated scoring systems, with human annotation used to establish ground truth for all tasks. The study compares commercial models (GPT-4V, Claude-3 series, Gemini Pro) against 14 open-source MLLMs, analyzing performance differences and identifying key challenges.

## Key Results
- GPT-4V(ision) achieves the highest average score of 64.6 across all VisualWebBench tasks
- Best open-source model LLaVA-1.6-34B scores 50.5, showing significant performance gap
- Models struggle particularly with text-rich environments and low-resolution image inputs
- GUI agent MLLMs show better grounding performance in position generation tasks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of web page understanding tasks at multiple granularities, using real websites rather than synthetic data. By employing high-resolution screenshots, VisualWebBench captures the detailed visual information necessary for accurate web page comprehension. The automated scoring system combined with human-annotated ground truth ensures reliable performance measurement. The comparison across diverse model families (commercial vs open-source, GUI agents vs general MLLMs) reveals systematic performance patterns and limitations in current MLLM architectures when applied to web-specific tasks.

## Foundational Learning
**Multimodal Large Language Models (MLLMs)**: AI models that can process and understand both visual and textual information, essential for web page understanding tasks where visual layout and textual content must be integrated. Quick check: Can the model process both image and text inputs simultaneously?

**Grounding in Text-Rich Environments**: The ability to accurately locate and interpret textual elements within complex visual contexts, critical for web pages where text is often overlaid on images or embedded in complex layouts. Quick check: Can the model correctly identify and extract text from screenshots with varied fonts and backgrounds?

**High-Resolution Input Processing**: The capacity to handle detailed visual information beyond standard 448x448 resolution, necessary for capturing fine-grained web page elements. Quick check: Does the model maintain performance when input resolution increases?

## Architecture Onboarding
**Component Map**: Image Input -> Visual Encoder -> Multimodal Fusion -> Text Decoder -> Output Generation
**Critical Path**: Visual preprocessing (resolution adjustment) → Visual feature extraction → Multimodal attention fusion → Task-specific decoding
**Design Tradeoffs**: High resolution improves detail capture but increases computational cost; text grounding requires specialized attention mechanisms but adds complexity
**Failure Signatures**: Poor performance on text-rich tasks indicates inadequate visual-text alignment; low-resolution failures suggest insufficient detail capture
**First Experiments**: 1) Test text extraction accuracy on varied font/background combinations 2) Compare performance across 224x224, 448x448, and 1024x1024 resolutions 3) Evaluate grounding accuracy in complex layouts with overlapping elements

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does increasing image resolution beyond 448x448 affect MLLM performance on web tasks, and what is the optimal resolution for balancing performance and computational cost?
- Basis in paper: The paper discusses the impact of image resolution on model performance, noting that most MLLMs can only process low-resolution images (typically 448x448), while VisualWebBench uses high-resolution screenshots (1280 pixels in width). It shows that higher input resolutions generally lead to better scores.
- Why unresolved: The paper only explores a limited range of resolutions and does not identify an optimal resolution that balances performance gains with computational efficiency.
- What evidence would resolve it: A comprehensive study testing a wider range of resolutions, measuring performance on VisualWebBench and computational costs, to determine the optimal trade-off point.

### Open Question 2
- Question: What specific pretraining techniques could improve MLLMs' grounding capabilities in text-rich web environments?
- Basis in paper: The paper highlights inadequate grounding in text-rich environments as a limitation of current MLLMs and suggests that GUI agent MLLMs, which undergo grounding pretraining, perform better in generating positions of target elements.
- Why unresolved: The paper does not explore specific pretraining techniques or architectures that could enhance grounding capabilities.
- What evidence would resolve it: Experimental results comparing different pretraining approaches, such as contrastive learning, attention mechanisms, or specialized architectures, on their impact on grounding performance in VisualWebBench.

### Open Question 3
- Question: How do MLLMs' performance on VisualWebBench correlate with their ability to handle real-world web navigation and interaction tasks?
- Basis in paper: The paper notes that while MLLMs perform well on general multimodal tasks like MMMU, their performance on web-specific tasks like VisualWebBench is lower, suggesting a gap in web-related capabilities.
- Why unresolved: The paper does not investigate how VisualWebBench performance translates to actual web navigation and interaction tasks.
- What evidence would resolve it: A study comparing MLLM performance on VisualWebBench with their performance on real-world web navigation benchmarks or user studies, to establish a correlation between the two.

## Limitations
- Benchmark focuses on specific web page tasks and may not comprehensively represent all real-world web interaction scenarios
- Performance evaluations are based on specific image resolutions and may not generalize to all web page rendering conditions
- The gap between commercial and open-source models may be influenced by factors beyond pure capability, such as fine-tuning approaches and access to proprietary data

## Confidence
- **High confidence**: Benchmark construction methodology and evaluation of 14+ models across 1.5K instances from 139 real websites provides robust empirical evidence for performance comparisons
- **Medium confidence**: Reported limitations in text-rich environments and low-resolution handling are well-documented but may not fully capture all contextual challenges
- **Medium confidence**: Claim that VisualWebBench serves as a valuable resource for developing better MLLMs is reasonable but will require community validation over time

## Next Checks
1. Test model performance across varying image resolutions and compression levels to quantify the impact on text-rich environment understanding
2. Evaluate model performance on dynamically generated web content versus static pages to assess generalization capabilities
3. Conduct cross-cultural web page understanding tests using websites from different global regions to identify potential cultural bias or limitation patterns