---
ver: rpa2
title: "The Effect of Quantization in Federated Learning: A R\xE9nyi Differential\
  \ Privacy Perspective"
arxiv_id: '2405.10096'
source_url: https://arxiv.org/abs/2405.10096
tags:
- privacy
- quantization
- gaussian
- quantized
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes how quantization affects privacy in federated\
  \ learning (FL) systems. The authors examine quantized Gaussian mechanisms using\
  \ R\xE9nyi Differential Privacy (RDP) and show that lower quantization bit levels\
  \ provide improved privacy protection."
---

# The Effect of Quantization in Federated Learning: A Rényi Differential Privacy Perspective

## Quick Facts
- arXiv ID: 2405.10096
- Source URL: https://arxiv.org/abs/2405.10096
- Reference count: 21
- Primary result: Lower quantization bit levels provide improved privacy protection in FL systems

## Executive Summary
This paper analyzes how quantization affects privacy in federated learning systems through the lens of Rényi Differential Privacy (RDP). The authors examine quantized Gaussian mechanisms and demonstrate that lower quantization bit levels provide improved privacy protection compared to non-quantized Gaussian noise. The work bridges communication efficiency and privacy enhancement, showing that quantization not only reduces communication overhead but also strengthens privacy guarantees. Theoretical findings are validated through Membership Inference Attacks, with experiments on CIFAR-10 using ResNet-18 architecture demonstrating that 4-bit quantization achieves better privacy protection while maintaining reasonable utility.

## Method Summary
The method involves implementing FedAvg with stochastic quantization and RDP analysis. Gaussian noise is added to weight differences from the global model, followed by k-level stochastic quantization. The privacy budget is calculated using RDP for the quantized Gaussian mechanism, and empirical validation is performed using Membership Inference Attacks with the LiRA method. The experimental setup uses CIFAR-10 with ResNet-18, 100 clients, and various quantization levels to evaluate the privacy-utility tradeoff.

## Key Results
- Lower quantization levels lead to better privacy protection by reducing the Kullback-Leibler divergence between neighboring distributions
- MIA accuracy decreases with lower quantization levels, empirically validating theoretical privacy benefits
- 4-bit quantization achieves better privacy protection (lower MIA accuracy) compared to unquantized models while maintaining reasonable utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization after Gaussian noise addition can provide tighter privacy guarantees than the original Gaussian mechanism.
- Mechanism: The quantized Gaussian mechanism restricts the output space to discrete levels, which reduces the maximum divergence between outputs for neighboring inputs. This results in a lower privacy budget for the same sensitivity.
- Core assumption: The quantization levels are sufficiently fine to preserve utility while being coarse enough to limit information leakage.
- Evidence anchors:
  - [abstract] "we demonstrate that lower quantization bit levels provide improved privacy protection"
  - [section III-B] "we obtain the RDP of the quantized Gaussian mechanism in the following theorem"
  - [corpus] Weak - related papers focus on quantization for communication efficiency rather than privacy benefits

### Mechanism 2
- Claim: Lower quantization bit levels lead to better privacy protection by reducing the Kullback-Leibler divergence between neighboring distributions.
- Mechanism: As quantization becomes coarser (fewer bits), the probability mass functions of outputs for neighboring inputs become more similar, reducing the maximum possible divergence measured by RDP.
- Core assumption: The relationship between quantization level and privacy budget is monotonic, as derived in Theorem 2 and Lemma 3.
- Evidence anchors:
  - [section III-B] "Our analysis demonstrates lower quantization levels lead to better privacy protection"
  - [section IV-A] "we observe that the privacy budget of quantized Gaussian... increases monotonically with quantization level k"
  - [corpus] Missing - no direct evidence in corpus papers about this specific relationship

### Mechanism 3
- Claim: Membership Inference Attacks (MIA) accuracy decreases with lower quantization levels, empirically validating the theoretical privacy benefits.
- Mechanism: Lower quantization reduces the information leakage that enables MIAs to distinguish between training and non-training samples based on model loss distributions.
- Core assumption: MIA accuracy is a reliable proxy for actual privacy leakage in the system.
- Evidence anchors:
  - [abstract] "The numerical results align with our theoretical analysis, confirming that quantization can indeed enhance privacy protection"
  - [section IV-C] "a positive correlation between quantization level and MIA accuracy can also be observed; a larger quantization level leads to a higher MIA accuracy"
  - [corpus] Weak - related papers mention quantization for efficiency but not for attack resistance

## Foundational Learning

- Concept: Rényi Differential Privacy (RDP)
  - Why needed here: RDP provides a tighter and more computationally convenient way to analyze the privacy budget of the quantized Gaussian mechanism compared to traditional (ε, δ)-DP.
  - Quick check question: What advantage does RDP have over traditional DP when analyzing mechanisms with Gaussian noise?

- Concept: Kullback-Leibler (KL) divergence
  - Why needed here: KL divergence is used to bound the privacy budget for α=1 in the RDP analysis of quantized Gaussian mechanisms.
  - Quick check question: How does KL divergence relate to the privacy budget when α=1 in RDP?

- Concept: Membership Inference Attacks (MIA)
  - Why needed here: MIA provides an empirical method to validate theoretical privacy guarantees by measuring how well an attacker can distinguish between training and non-training samples.
  - Quick check question: What assumption about loss distributions does the Likelihood Ratio MIA method make?

## Architecture Onboarding

- Component map:
  Local update → Gaussian noise addition → k-level stochastic quantization → encoding/transmission → aggregation

- Critical path:
  1. Client performs local training
  2. Client computes weight difference from global model
  3. Client clips and adds Gaussian noise
  4. Client applies k-level stochastic quantization
  5. Client encodes and transmits updates
  6. Server decodes and aggregates updates
  7. Server updates global model
  8. Privacy analysis calculates RDP bounds
  9. MIA simulation validates privacy claims

- Design tradeoffs:
  - Higher quantization levels: Better utility, worse privacy protection
  - Lower quantization levels: Better privacy protection, worse utility
  - Noise scale: Higher noise improves privacy but degrades utility
  - Clipping threshold: Affects both sensitivity and utility

- Failure signatures:
  - High MIA accuracy despite quantization: Possible implementation error in quantization or noise addition
  - Rapid utility degradation with low quantization: Quantization levels too coarse for the task
  - Privacy budget calculations not matching empirical results: Errors in RDP analysis or MIA implementation

- First 3 experiments:
  1. Verify monotonicity: Test privacy budget calculation across quantization levels (k=2, 4, 8, 16) with fixed noise and clipping parameters
  2. Validate RDP bounds: Compare theoretical RDP bounds with empirical results from MIA across different noise scales
  3. Utility-privacy tradeoff analysis: Measure accuracy and MIA accuracy across a grid of quantization levels and noise scales to map the Pareto frontier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the trade-off between privacy protection and model utility vary with different quantization levels in federated learning?
- Basis in paper: [explicit] The paper discusses the correlation between quantization level and MIA accuracy, indicating that lower quantization levels lead to better privacy protection but potentially lower model utility.
- Why unresolved: The paper provides experimental results showing this trade-off but does not offer a theoretical framework to predict or optimize the balance between privacy and utility for different quantization levels.
- What evidence would resolve it: A theoretical model or formula that quantifies the trade-off between privacy protection and model utility as a function of quantization level would resolve this question.

### Open Question 2
- Question: What is the impact of different noise scales on the effectiveness of quantization in enhancing privacy protection?
- Basis in paper: [explicit] The paper mentions that MIA accuracy decreases with increasing noise scale, but it does not explore how different noise scales interact with quantization levels to affect privacy protection.
- Why unresolved: The experiments focus on a fixed noise scale, and the theoretical analysis does not address how varying noise scales might influence the benefits of quantization.
- What evidence would resolve it: Experimental results showing the relationship between noise scale, quantization level, and privacy protection, along with a theoretical analysis of this interaction, would resolve this question.

### Open Question 3
- Question: How do different quantization methods (e.g., stochastic vs. deterministic) affect the privacy guarantees in federated learning?
- Basis in paper: [inferred] The paper focuses on stochastic quantization but does not compare its privacy benefits to other quantization methods, such as deterministic quantization.
- Why unresolved: The paper does not provide a comparative analysis of different quantization methods in terms of their impact on privacy protection.
- What evidence would resolve it: A comparative study of various quantization methods, including their privacy budgets and effectiveness in preventing attacks like MIA, would resolve this question.

## Limitations
- The experimental validation focuses on a single dataset (CIFAR-10) and model architecture (ResNet-18), limiting generalizability
- The assumption that KL divergence between neighboring distributions monotonically decreases with coarser quantization may not hold in all scenarios
- While lower quantization levels improve privacy bounds mathematically, the practical impact on real-world attacks beyond MIA is unclear

## Confidence
- **High Confidence**: The theoretical derivation of RDP bounds for quantized Gaussian mechanisms is mathematically sound and well-established within the differential privacy literature.
- **Medium Confidence**: The experimental validation showing reduced MIA accuracy with lower quantization levels is convincing but limited to one attack type and dataset.
- **Medium Confidence**: The claim that quantization provides both communication efficiency and enhanced privacy protection is supported by evidence but requires broader validation across different FL scenarios.

## Next Checks
1. **Broader Attack Surface**: Validate the privacy benefits of quantization against multiple attack types (e.g., property inference attacks, reconstruction attacks) beyond just membership inference to ensure the privacy gains aren't attack-specific.
2. **Cross-Dataset Generalization**: Test the quantization-privacy relationship on diverse datasets (text, tabular, medical) and model architectures to verify the theoretical findings aren't dataset-specific.
3. **Utility-Privacy Tradeoff Mapping**: Conduct a systematic grid search across quantization levels, noise scales, and clipping thresholds to map the complete Pareto frontier and identify optimal configurations for different utility requirements.