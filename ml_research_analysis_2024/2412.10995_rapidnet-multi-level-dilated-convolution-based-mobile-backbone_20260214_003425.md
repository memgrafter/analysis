---
ver: rpa2
title: 'RapidNet: Multi-Level Dilated Convolution Based Mobile Backbone'
arxiv_id: '2412.10995'
source_url: https://arxiv.org/abs/2412.10995
tags:
- convolution
- dilated
- vision
- convolutions
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing efficient CNN-based
  mobile vision architectures that can compete with state-of-the-art hybrid CNN-ViT
  and CNN-ViG models. The proposed method introduces Multi-Level Dilated Convolutions
  (MLDC) to enable larger theoretical receptive fields with lower computational cost
  than standard convolutions.
---

# RapidNet: Multi-Level Dilated Convolution Based Mobile Backbone

## Quick Facts
- **arXiv ID:** 2412.10995
- **Source URL:** https://arxiv.org/abs/2412.10995
- **Reference count:** 40
- **Primary result:** RapidNet-Ti achieves 76.3% top-1 accuracy on ImageNet-1K with 0.9 ms inference latency on iPhone 13 mini

## Executive Summary
This paper introduces RapidNet, a pure CNN-based mobile architecture that competes with state-of-the-art hybrid CNN-ViT and CNN-ViG models through the use of Multi-Level Dilated Convolutions (MLDC). The key innovation is MLDC, which combines parallel dilated convolutions at different scales to achieve larger receptive fields with lower computational cost than standard convolutions. The architecture incorporates inverted residual blocks, large kernel feedforward networks, and reparameterizable depthwise convolutions to optimize for both accuracy and mobile deployment. Experimental results demonstrate that RapidNet outperforms competing models across ImageNet-1K classification, COCO object detection, instance segmentation, and ADE20K semantic segmentation while maintaining competitive latency on mobile devices.

## Method Summary
RapidNet is a CNN-based mobile vision architecture that uses Multi-Level Dilated Convolutions (MLDC) to expand receptive fields without the computational cost of large kernels or self-attention mechanisms. The architecture consists of four stages with inverted residual blocks, where MLDC blocks are integrated in the later stages to balance accuracy and latency. The design also includes a large kernel feedforward network to compensate for the lack of self-attention, and reparameterizable depthwise convolutions to reduce inference latency. Knowledge distillation from RegNetY-16GF is used during training, along with comprehensive data augmentation techniques. The model is specifically optimized for mobile deployment, with latency measured on iPhone 13 mini NPU.

## Key Results
- RapidNet-Ti achieves 76.3% top-1 accuracy on ImageNet-1K with 0.9 ms inference latency on iPhone 13 mini
- RapidNet-M achieves 42.0 AP on COCO object detection, outperforming MobileNetV3-Large
- RapidNet-B achieves 43.1 AP box and 39.3 AP mask on COCO tasks, matching MobileViG-B performance
- RapidNet achieves competitive results across all tasks while maintaining lower latency than most competing models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-Level Dilated Convolutions (MLDC) expand receptive fields without the computational cost of large kernels or graph convolutions.
- **Mechanism:** Two parallel dilated convolutions with different dilation factors (2 and 3) are summed after batch normalization and GeLU activation, allowing processing at two receptive field sizes (5x5 and 7x7) while maintaining 3x3 parameter efficiency.
- **Core assumption:** The combination of features from different receptive fields provides complementary information that improves feature extraction compared to single-scale processing.
- **Evidence anchors:**
  - [abstract] "Using Multi-Level Dilated Convolutions allows for a larger theoretical receptive field than standard convolutions"
  - [section] "Two dilated convolutions at different levels of dilation can be used to process features with different theoretical receptive fields in parallel"
  - [corpus] Weak evidence - related papers discuss dilated convolutions but don't specifically validate parallel multi-scale processing
- **Break condition:** If the feature combination from different receptive fields introduces harmful interference or if the computational savings are negligible compared to alternatives like deformable convolutions.

### Mechanism 2
- **Claim:** The large kernel feedforward network (LK FFN) compensates for the lack of self-attention by expanding receptive fields in early stages.
- **Mechanism:** A 7x7 depthwise convolution is integrated into the feedforward network, followed by a two-layer MLP, expanding the receptive field without the quadratic complexity of self-attention.
- **Core assumption:** Expanding receptive fields through convolution can approximate some benefits of self-attention for capturing long-range dependencies in early network stages.
- **Evidence anchors:**
  - [section] "Since we do not use self-attention token mixers or the graph-based mixing of MobileViG, we need to employ another efficient method to expand our theoretical receptive field"
  - [section] "Convolutional FFN blocks have been shown to exhibit greater robustness compared to standard FFN blocks"
  - [corpus] Weak evidence - related papers discuss large kernel convolutions but don't specifically validate their role as self-attention alternatives
- **Break condition:** If the 7x7 convolution becomes a bottleneck or if the receptive field expansion is insufficient for the target tasks.

### Mechanism 3
- **Claim:** Reparameterizable skip connections in depthwise convolutions reduce inference latency without sacrificing training accuracy.
- **Mechanism:** During training, a skip connection is used (X + DW7x7(X)), but during inference, this is converted to a single depthwise convolution (DW7x7(X)), eliminating the addition operation.
- **Core assumption:** The reparameterization preserves the effective transformation learned during training while being more efficient at inference.
- **Evidence anchors:**
  - [section] "A reparameterizable 7 × 7 depthwise convolution is introduced into the MLDC block to also expand the receptive field. The reparameterization follows the method of [58] to eliminate a skip connection at inference time thereby decreasing latency without damaging accuracy"
  - [section] "This is followed by a pointwise convolution and BN, then two dilated convolution blocks expressed as: Z = σ(Dilated2(Y) + Dilated3(Y))"
  - [corpus] Weak evidence - related papers discuss reparameterization but don't specifically validate the latency benefits in mobile settings
- **Break condition:** If the reparameterization leads to significant accuracy degradation or if the latency savings are offset by other operations.

## Foundational Learning

- **Concept:** Dilated convolution and receptive field theory
  - **Why needed here:** Understanding how dilation factors expand receptive fields without increasing parameters is fundamental to grasping why MLDC works
  - **Quick check question:** For a 3x3 convolution with dilation factor 2, what is the theoretical receptive field size, and how does this compare to a standard 5x5 convolution in terms of parameters?

- **Concept:** Mobile architecture design constraints
  - **Why needed here:** RapidNet is designed specifically for mobile devices, requiring understanding of latency, memory, and computational efficiency tradeoffs
  - **Quick check question:** Why would a model with lower GMACs but higher latency still be less suitable for mobile deployment than one with slightly higher GMACs but lower latency?

- **Concept:** Knowledge distillation in model compression
  - **Why needed here:** RapidNet uses knowledge distillation from RegNetY-16GF, making understanding this technique essential for comprehending the training setup
  - **Quick check question:** What is the primary benefit of using knowledge distillation during training rather than just training on ground truth labels?

## Architecture Onboarding

- **Component map:** Conv Stem -> Stage 1: Inverted Residual Blocks -> Downsample (4x → 8x) -> Stage 2: IRBs -> Downsample (8x → 16x) -> Stage 3: IRBs + Dilated Convolution Blocks -> Downsample (16x → 32x) -> Stage 4: IRBs + Dilated Convolution Blocks -> Classification head (global pooling + linear MLP)

- **Critical path:** Input → Conv Stem → Stage 1 → Downsample → Stage 2 → Downsample → Stage 3 → Downsample → Stage 4 → Classification Head

- **Design tradeoffs:**
  - Using dilated convolutions instead of larger kernels for receptive field expansion
  - Placing MLDC blocks only in later stages to balance accuracy and latency
  - Replacing ReLU with GeLU for improved performance
  - Using depthwise convolutions with reparameterizable skip connections

- **Failure signatures:**
  - Accuracy degradation when MLDC blocks are removed
  - Increased latency when reparameterization fails
  - Poor generalization when large kernel FFN is omitted
  - Memory overflow when channel dimensions are scaled improperly

- **First 3 experiments:**
  1. Remove MLDC blocks and replace with standard 3x3 convolutions to measure the contribution of multi-scale receptive field processing
  2. Disable reparameterization and compare inference latency between training and inference modes
  3. Replace GeLU with ReLU activation throughout the network to quantify the impact of this activation choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RapidNet scale when using larger dilation factors in the MLDC blocks?
- Basis in paper: [explicit] The authors conducted an ablation study showing that increasing dilation factors from (2,3) to (3,4) resulted in a 0.4% decrease in accuracy on ImageNet-1K.
- Why unresolved: The ablation study only tested one increase in dilation factors. It's unclear how much larger dilation factors can be used before performance degrades significantly.
- What evidence would resolve it: Systematic testing of RapidNet with various dilation factor combinations (e.g., (2,4), (3,5), (4,6)) and measuring the resulting accuracy and computational cost.

### Open Question 2
- Question: Can the MLDC approach be effectively combined with other architectural innovations like attention mechanisms or graph convolutions?
- Basis in paper: [inferred] The authors positioned MLDC as an alternative to ViT and ViG approaches, but didn't explore hybrid approaches combining MLDC with these other methods.
- Why unresolved: The paper focused on proving the standalone effectiveness of MLDC, but didn't investigate potential synergies with other state-of-the-art techniques.
- What evidence would resolve it: Experiments comparing hybrid architectures that combine MLDC with attention mechanisms or graph convolutions against pure MLDC and pure attention/graph-based models.

### Open Question 3
- Question: How does RapidNet's performance on mobile devices compare to desktop/server-class hardware?
- Basis in paper: [explicit] The authors specifically tested on iPhone 13 mini and emphasized mobile performance, but didn't compare against desktop/server performance.
- Why unresolved: The paper focuses entirely on mobile optimization, but doesn't provide context for how this mobile-focused design might trade off against desktop/server performance.
- What evidence would resolve it: Benchmarking RapidNet on both mobile and desktop/server hardware using the same models to quantify the performance trade-offs of the mobile-optimized design.

### Open Question 4
- Question: What is the impact of MLDC on robustness to adversarial attacks and out-of-distribution data?
- Basis in paper: [inferred] The authors mention that convolutional FFN blocks show greater robustness compared to standard FFN blocks, but don't specifically test MLDC's robustness properties.
- Why unresolved: The paper focuses on accuracy and latency metrics but doesn't explore robustness, which is critical for real-world deployment.
- What evidence would resolve it: Adversarial attack testing and out-of-distribution evaluation comparing RapidNet with competing models across various robustness benchmarks.

## Limitations
- Empirical claims rest on controlled synthetic evaluations rather than field deployment
- Performance improvements over hybrid models may not justify the architectural complexity
- Ablation studies do not isolate individual contributions with statistical significance testing

## Confidence
- **High Confidence:** ImageNet-1K classification results and the fundamental design principle that MLDC can expand receptive fields efficiently
- **Medium Confidence:** Transfer performance to COCO and ADE20K tasks
- **Low Confidence:** Real-world deployment benefits and latency measurements across diverse mobile hardware

## Next Checks
1. Conduct ablation studies with statistical significance testing to isolate the contribution of each architectural innovation (MLDC, LK-FFN, reparameterization) on ImageNet-1K accuracy.
2. Test RapidNet across multiple mobile devices with varying computational capabilities to validate the claimed latency improvements and identify hardware-specific bottlenecks.
3. Evaluate the model's performance under real-world conditions including varying image resolutions, aspect ratios, and input distributions to assess robustness beyond controlled benchmark settings.