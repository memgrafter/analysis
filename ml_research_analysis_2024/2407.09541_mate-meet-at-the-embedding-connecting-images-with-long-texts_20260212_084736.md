---
ver: rpa2
title: 'MATE: Meet At The Embedding -- Connecting Images with Long Texts'
arxiv_id: '2407.09541'
source_url: https://arxiv.org/abs/2407.09541
tags:
- image
- text
- mate
- encoder
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MATE, a novel approach that connects images
  with long texts (lengthy captions and documents) by leveraging the complementary
  strengths of Vision Language Models (VLMs) and Large Language Models (LLMs). Unlike
  existing VLMs that primarily align images with short captions, MATE employs a pretrained
  LLM-based encoder to understand long texts and uses a projection module to align
  VLM image embeddings with LLM text embeddings.
---

# MATE: Meet At The Embedding -- Connecting Images with Long Texts

## Quick Facts
- arXiv ID: 2407.09541
- Source URL: https://arxiv.org/abs/2407.09541
- Authors: Young Kyun Jang; Junmo Kang; Yong Jae Lee; Donghyun Kim
- Reference count: 14
- Primary result: Novel method connecting images with long texts using VLM-LLM alignment without requiring image-long text pairs

## Executive Summary
MATE introduces a novel approach to connect images with long texts by leveraging the complementary strengths of Vision Language Models (VLMs) and Large Language Models (LLMs). Unlike existing VLMs that primarily align images with short captions, MATE employs a pretrained LLM-based encoder to understand long texts and uses a projection module to align VLM image embeddings with LLM text embeddings. The method is trained in a multi-stage manner using existing image-caption and query-document pairs, without requiring additional image-long text pairs. The authors propose two new cross-modal retrieval benchmarks and demonstrate that MATE achieves superior performance in connecting images with long texts, uncovering diverse semantic relationships.

## Method Summary
MATE connects images with long texts through a multi-stage alignment process. It first aligns the VLM text encoder with an LLM encoder using text-only datasets, then transfers this alignment to the image encoder using a projection module. The method leverages the shared contrastive training paradigm of VLMs and LLMs to align their embedding spaces without requiring paired image-long text data. A three-layer projection module with GELU activation maps VLM embeddings to the LLM space, and LoRA adapters are used for efficient fine-tuning. The approach is evaluated on newly introduced benchmarks for image-lengthy caption and image-document retrieval tasks.

## Key Results
- Achieves superior performance on two new cross-modal retrieval benchmarks (DOCCI, CC3M-long, Infoseek, Oven)
- Demonstrates effective multilingual capabilities for cross-lingual retrieval
- Uncovers diverse semantic relationships between images and long texts beyond simple keyword matching
- Shows 10-20% improvement in Recall@K and mAP@K metrics compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MATE aligns VLM and LLM embeddings without requiring image-long text pairs by leveraging the shared contrastive training paradigm of both models.
- Mechanism: Both VLMs and LLMs are trained using contrastive objectives, which creates similar distributional properties in their embedding spaces. MATE exploits this by first aligning text encoders (VLM text → LLM text) using shared text pairs, then transferring this alignment to the image encoder.
- Core assumption: The embedding spaces of VLMs and LLMs share enough common representational structure to enable effective projection through a learned module.
- Evidence anchors:
  - [abstract]: "These models utilize dual-encoder architectures to encode images and text separately, effectively aligning them within a common embedding space"
  - [section 3.1]: "Note that both VLM and LLM embedding spaces are developed in a contrastive manner, and are presumed to share some common representations"
  - [corpus]: Weak - no direct corpus evidence for shared structure; evidence is indirect through training methodology
- Break condition: If the distributional properties of VLM and LLM embeddings diverge significantly due to different pretraining objectives or data distributions, the projection module will fail to find meaningful mappings.

### Mechanism 2
- Claim: The multi-stage training strategy enables progressive knowledge transfer from short-text to long-text domains without requiring paired image-long text data.
- Mechanism: Stage 1 aligns VLM text encoder with LLM using caption-caption pairs. Stage 2 fine-tunes with query-document pairs to capture long-text context. Stage 3 transfers this alignment to image embeddings using minimal image-caption pairs.
- Core assumption: Knowledge gained from aligning text encoders with long-text context can be effectively transferred to image embeddings through the same projection module.
- Evidence anchors:
  - [section 3.2.1]: "We utilize the same number of caption pairs as query-document pairs in a training batch to ensure that ϕ remains robust across diverse captions"
  - [section 3.2.2]: "We seek to align u, where u = ϕ(v) and v = EI (x), with d, where d = ET (t)"
  - [corpus]: Weak - no corpus evidence for effectiveness of this specific transfer strategy
- Break condition: If the semantic relationships learned in text-text alignment don't transfer to image-text relationships, the final alignment will be poor.

### Mechanism 3
- Claim: Using LoRA for efficient fine-tuning preserves the original model capabilities while adapting to new embedding spaces.
- Mechanism: LoRA introduces low-rank trainable matrices that adapt the original weights without modifying them directly, allowing the projection module to retain its understanding of query-document relationships while adapting to image embeddings.
- Core assumption: Low-rank adaptations are sufficient to bridge the gap between VLM and LLM embedding spaces without catastrophic forgetting.
- Evidence anchors:
  - [section 3.2.2]: "Additionally, we apply LoRA (Hu et al., 2021) parameters to both ϕ and EI to keep the original parameters and train the entire model efficiently"
  - [section 4.1]: "We employ additional LoRA (Hu et al., 2021) parameters for the image encoder and ϕ in Section 3.2.2"
  - [corpus]: Moderate - LoRA is well-established in literature but not specifically validated for cross-modal embedding alignment in this context
- Break condition: If the rank of the adaptation is insufficient to capture the complexity of the alignment task, performance will degrade.

## Foundational Learning

- Concept: Contrastive learning objectives
  - Why needed here: Understanding how InfoNCE loss creates embedding spaces where related samples are close and unrelated samples are far apart is crucial for grasping why MATE can align different model embeddings
  - Quick check question: What is the key difference between contrastive loss and standard classification loss in terms of what it optimizes for in embedding space?

- Concept: Cross-modal retrieval evaluation metrics
  - Why needed here: The paper uses recall@k and mAP@k metrics which have specific interpretations for retrieval tasks that differ from standard classification metrics
  - Quick check question: Why might mAP@k be preferred over simple accuracy for evaluating image-document retrieval tasks?

- Concept: Multi-stage training and knowledge transfer
  - Why needed here: MATE's effectiveness relies on understanding how knowledge from one training stage can be transferred to subsequent stages without paired data
  - Quick check question: What is the risk of catastrophic forgetting when fine-tuning a model through multiple stages, and how does LoRA help mitigate this?

## Architecture Onboarding

- Component map:
  - VLM Image Encoder (EI) - processes images into embeddings
  - VLM Text Encoder (ET) - processes short captions into embeddings
  - LLM-based Encoder (E5) - processes long texts into embeddings
  - Projection Module (ϕ) - linear layers with normalization that map VLM embeddings to LLM embedding space
  - LoRA adapters - low-rank matrices applied to EI and ϕ during fine-tuning

- Critical path: Input image → EI → ϕ (with LoRA) → LLM-aligned embedding → similarity search with LLM-encoded documents

- Design tradeoffs:
  - Using projection instead of joint training: More parameter-efficient but may introduce alignment errors
  - Multi-stage training vs. end-to-end: Requires careful orchestration but avoids need for image-long text pairs
  - LoRA vs. full fine-tuning: More efficient but may limit adaptation capacity

- Failure signatures:
  - Poor retrieval performance despite successful training: Indicates misalignment between VLM and LLM spaces
  - Training instability in later stages: Suggests gradient interference between LoRA and original parameters
  - Overfitting to training text pairs: May indicate insufficient regularization or data augmentation

- First 3 experiments:
  1. Verify contrastive alignment works within each model: Compute mutual KNN scores between embeddings from same model on paired data
  2. Test projection module on text-to-text alignment: Measure alignment quality between VLM text and LLM embeddings before and after training ϕ
  3. Validate LoRA preserves original capabilities: Compare retrieval performance on short caption tasks before and after applying LoRA adapters

## Open Questions the Paper Calls Out

- How does the performance of MATE vary when using different projection module architectures beyond the three linear layers with GELU activation used in the paper?
- What is the impact of using different text encoders within the VLM on MATE's performance, particularly encoders with longer context windows?
- How does MATE's performance generalize to other cross-modal retrieval tasks beyond image-lengthy caption and image-document retrieval?

## Limitations

- The entire approach relies on an unverified assumption about shared distributional properties between VLM and LLM embedding spaces
- Claims about efficiency gains may be overstated, as substantial text-only data is still required for training
- Evaluation focuses primarily on retrieval metrics without deeper semantic analysis of relationship quality

## Confidence

**High Confidence**: MATE achieves superior performance on proposed benchmarks compared to baseline approaches; multi-stage training approach is technically sound; LoRA-based fine-tuning is an established technique that works as intended.

**Medium Confidence**: Claims about uncovering diverse semantic relationships are supported by retrieval performance but lack deeper semantic analysis; assertion that method works without image-long text pairs is technically true but requires substantial text-only data; multilingual capabilities are demonstrated but only through cross-lingual retrieval.

**Low Confidence**: Assumption about shared distributional properties between VLM and LLM embeddings is never directly validated; claim that projection module can effectively transfer text-text alignment knowledge to image-text alignment lacks empirical verification; assertion that MATE "connects images with long texts" implies deep semantic understanding that may not extend beyond surface-level correlations.

## Next Checks

1. **Embedding Space Alignment Validation**: Compute direct alignment metrics between VLM and LLM embeddings (such as mutual nearest neighbor accuracy or Procrustes analysis) before and after MATE training to empirically verify the shared structure assumption.

2. **Semantic Relationship Analysis**: Beyond retrieval metrics, conduct qualitative analysis of top retrieved pairs to determine if MATE captures genuine semantic relationships or merely surface-level correlations between images and long texts.

3. **Zero-Shot Cross-Domain Transfer**: Test MATE's performance on completely unseen domains (e.g., medical images with clinical reports) to validate whether the learned alignment generalizes beyond the training distribution or simply memorizes domain-specific patterns.