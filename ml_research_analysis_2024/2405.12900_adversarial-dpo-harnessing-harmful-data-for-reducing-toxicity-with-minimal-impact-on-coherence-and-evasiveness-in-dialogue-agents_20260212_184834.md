---
ver: rpa2
title: 'Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal
  Impact on Coherence and Evasiveness in Dialogue Agents'
arxiv_id: '2405.12900'
source_url: https://arxiv.org/abs/2405.12900
tags:
- toxic
- adpo
- data
- responses
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adversarial DPO (ADPO), an improved training
  algorithm that reduces toxic responses in dialogue agents by incorporating harmful
  data. The method uses a toxic control token to generate unsafe responses and penalizes
  their generation while maintaining performance on preferred responses.
---

# Adversarial DPO: Harnessing Harmful Data for Reducing Toxicity with Minimal Impact on Coherence and Evasiveness in Dialogue Agents

## Quick Facts
- arXiv ID: 2405.12900
- Source URL: https://arxiv.org/abs/2405.12900
- Authors: San Kim; Gary Geunbae Lee
- Reference count: 17
- Key outcome: ADPO reduces toxicity by nearly 10× while increasing evasiveness by only 0.02% compared to DPO's 20% increase

## Executive Summary
This paper introduces Adversarial DPO (ADPO), an improved training algorithm that reduces toxic responses in dialogue agents by incorporating harmful data. The method uses a toxic control token to generate unsafe responses and penalizes their generation while maintaining performance on preferred responses. Experiments show ADPO achieves significantly lower toxicity than standard DPO with minimal impact on coherence and evasiveness, and offers more stable training. On a harmful dialogue dataset, ADPO reduces toxicity by nearly 10× while increasing evasiveness by only 0.02% compared to DPO's 20% increase.

## Method Summary
ADPO fine-tunes Llama2-7B on Anthropic preference data with toxic control token appended to toxic dialogues, then generates preference data using Llama2-13B-chat model to label chosen, rejected, and toxic responses. The method trains with an additional penalty term for toxic responses beyond the standard reward-only approach, using β=0.3 and γ=0.2 for ADPO compared to β=0.9 for DPO.

## Key Results
- ADPO reduces toxicity by nearly 10× compared to standard DPO on harmful dialogue datasets
- Minimal impact on evasiveness (0.02% increase vs DPO's 20% increase)
- Maintains better coherence and offers more stable training with controlled KL divergence
- Demonstrates improved contextual understanding and response diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADPO uses toxic control token to generate unsafe responses that are explicitly penalized during training
- Mechanism: By appending "[TOXIC]" to user prompts, the model generates toxic responses using the same parameters as the clean model. These generated toxic responses are then used as additional negative examples in the loss function, alongside preferred/rejected pairs
- Core assumption: The model can generate meaningful toxic responses when prompted with the control token, and these responses reflect the model's actual toxicity distribution
- Evidence anchors: [abstract] "The ADPO algorithm is designed to train models to assign higher probability distributions to preferred responses and lower distributions to unsafe responses, which are self-generated using the toxic control token." [section 3.1] "This process empowers the model to autonomously generate toxic responses when prompted by the '[TOXIC]' token."

### Mechanism 2
- Claim: ADPO maintains response diversity by providing explicit negative examples beyond just preferred/rejected pairs
- Mechanism: Standard DPO only learns from pairwise preferences, which can lead to mode collapse and evasive responses. ADPO adds a third category (toxic responses) that explicitly defines what the model should avoid, giving it clearer criteria for acceptable responses
- Core assumption: Having explicit negative examples beyond rejected responses helps the model maintain diversity and avoid evasive behavior
- Evidence anchors: [section 5.2] "ADPO not only facilitates the reduction of unwanted responses, specifically unsafe responses in this study, but also aids in maintaining response diversity." [section 5.2] "This strategy aims to secure moderate rewards (or minimize loss) from both selected and non-selected data rather than generating responses that are distinctly aligned or opposed to one particular category."

### Mechanism 3
- Claim: ADPO provides more stable training through explicit penalty terms that complement the reward signal
- Mechanism: The additional penalty term for toxic responses creates a more balanced objective function. This helps prevent the model from over-optimizing on preferred responses at the expense of safety, leading to more stable convergence
- Core assumption: Adding explicit safety penalties to the objective function improves training stability compared to reward-only approaches
- Evidence anchors: [section 5.4] "ADPO maintained chosen KL within the optimal range and showed a steady decrease, while DPO experienced a rapid drop, demonstrating sensitivity to the β." [section 3.2] "We found that incorporating an extra penalty p, interpreted as providing detailed criteria in conjunction with r, enhances training stability."

## Foundational Learning

- Concept: KL divergence as a measure of distribution difference
  - Why needed here: Used to evaluate how much the model's output distribution changes relative to reference models during training
  - Quick check question: What does a high KL divergence between πθ and πref indicate about the model's behavior?

- Concept: Reinforcement learning from human feedback (RLHF) vs direct preference optimization (DPO)
  - Why needed here: ADPO builds on DPO, so understanding the differences between these approaches is crucial for grasping the innovation
  - Quick check question: How does DPO differ from traditional RLHF in terms of stability and computational requirements?

- Concept: Mode collapse in language models
  - Why needed here: ADPO specifically addresses mode collapse by providing additional training signals beyond pairwise preferences
  - Quick check question: What happens to a language model's response diversity when mode collapse occurs during training?

## Architecture Onboarding

- Component map: Base model (Llama2-7B) -> SFT phase with normal and toxic data -> Inner toxic model (same parameters, uses control token) -> Preference data generation pipeline -> ADPO training loop with three loss components

- Critical path: 1. SFT training with toxic control token appended 2. Generate responses at different temperatures 3. LLM annotation to create preference data 4. ADPO training with reward and penalty terms

- Design tradeoffs: Using toxic data for training vs. pure safety filtering, Temperature settings for response generation (1.0 vs 1.5), Balance between β and γ hyperparameters, Single vs. multiple training runs

- Failure signatures: KL divergence spikes indicating instability, Toxicity metrics not improving despite training, Coherence metrics dropping significantly, Model generating nonsensical toxic responses

- First 3 experiments: 1. Compare ADPO vs DPO on same base model with identical hyperparameters 2. Vary the β:γ ratio to find optimal stability-performance balance 3. Test different temperature settings for toxic response generation to ensure meaningful examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ADPO perform when trained on larger datasets (e.g., full 160k Anthropic preference data) compared to the 16k subset used in this study?
- Basis in paper: [explicit] The paper mentions using only 10% of the Anthropic dataset and states that "using full 160k data would lead to better result."
- Why unresolved: The paper conducted experiments with a limited subset of data to demonstrate the effectiveness of ADPO, but did not explore the impact of scaling up the dataset size
- What evidence would resolve it: Training ADPO on the full Anthropic dataset and comparing its performance metrics (toxicity, coherence, evasiveness) against the results obtained with the 16k subset

### Open Question 2
- Question: What is the optimal balance between the β and γ parameters in ADPO's objective function for different types of dialogue datasets?
- Basis in paper: [inferred] The paper found optimal values of β = 0.3 and γ = 0.2 for their experiments, but acknowledges that hyperparameter sensitivity is a challenge in RLHF methods
- Why unresolved: The paper only explored a limited range of hyperparameter values and did not conduct a systematic search for optimal parameter settings across different dataset characteristics
- What evidence would resolve it: Conducting a grid search or Bayesian optimization over a range of β and γ values for various dialogue datasets with different properties (e.g., domain, toxicity levels, length of conversations) to identify patterns in optimal parameter settings

### Open Question 3
- Question: How does ADPO's performance compare to other toxicity mitigation methods when controlling for the same base model and training data?
- Basis in paper: [explicit] The paper compares ADPO to standard DPO and SFT, but acknowledges that "a direct comparison between ADPO and DPO methodologies may not be entirely equitable if based on differently fine-tuned models."
- Why unresolved: While the paper conducts additional experiments to address this concern, it does not provide a comprehensive comparison against the full range of existing toxicity mitigation techniques
- What evidence would resolve it: Implementing a standardized evaluation framework where multiple toxicity mitigation methods (e.g., RLHF, SFT with additional safety fine-tuning, prompt engineering approaches) are trained on the same base model and data, then compared using consistent metrics across various dialogue datasets

## Limitations
- Limited exploration of hyperparameter space for β and γ parameters
- Reliance on LLM-based annotation which may introduce bias and variance
- Tested only on specific datasets and model sizes without broader generalization validation

## Confidence
- Mechanism 1 (toxic control token): Medium - The concept is clearly explained but lacks external validation of the toxicity generation mechanism
- Mechanism 2 (diversity maintenance): Medium - The claim is supported by results but the underlying mechanism is not deeply explored
- Mechanism 3 (training stability): High - Strong empirical evidence with KL divergence metrics and systematic hyperparameter analysis
- Overall performance claims: High - Comprehensive evaluation with multiple metrics and comparison methods

## Next Checks
1. **Mechanism Validation**: Test the toxic control token generation across diverse prompt categories to verify it produces meaningfully toxic responses that reflect real-world toxicity distributions, not just surface-level toxicity
2. **Generalization Test**: Apply ADPO to a different base model (e.g., Mistral-7B) and toxicity dataset (e.g., ToxiChat) to assess whether the stability improvements and toxicity reduction generalize beyond the current experimental setup
3. **Long-term Stability**: Monitor ADPO training across extended iterations (beyond the current evaluation window) to verify that the stability advantages persist and don't lead to unforeseen degradation in other response qualities