---
ver: rpa2
title: Domain Adaptation for Time series Transformers using One-step fine-tuning
arxiv_id: '2401.06524'
source_url: https://arxiv.org/abs/2401.06524
tags:
- data
- target
- source
- domains
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in applying Transformers to time
  series prediction, including limited data, temporal understanding, data shift, generalization,
  and catastrophic forgetting. The proposed One-step fine-tuning approach pre-trains
  a Transformer on a data-rich source domain and fine-tunes it on target domains with
  limited data by adding a percentage of source data to the target domains.
---

# Domain Adaptation for Time series Transformers using One-step fine-tuning

## Quick Facts
- **arXiv ID**: 2401.06524
- **Source URL**: https://arxiv.org/abs/2401.06524
- **Authors**: Subina Khanal; Seshu Tirupathi; Giulio Zizzo; Ambrish Rawat; Torben Bach Pedersen
- **Reference count**: 12
- **One-line primary result**: 4.35% improvement for indoor temperature prediction and 11.54% for wind power prediction over state-of-the-art baselines.

## Executive Summary
This paper tackles the challenge of applying Transformers to time series prediction in data-scarce scenarios. The proposed One-step fine-tuning approach pre-trains a Transformer on a data-rich source domain and fine-tunes it on target domains with limited data. The key innovation is adding a percentage of source data to the target domains during fine-tuning, which mitigates data shift and catastrophic forgetting while improving generalization. Experiments on two real-world datasets demonstrate significant improvements over state-of-the-art baselines.

## Method Summary
The approach involves pre-training a Transformer model on a source domain with sufficient data, then fine-tuning it on target domains with limited data. During fine-tuning, a percentage of source domain data is added to the target domain to address data scarcity, distribution mismatch, and catastrophic forgetting. A gradual unfreezing technique is applied, where each layer of the source model is initially frozen and then gradually unfrozen during training epochs. This allows the model to retain pre-trained temporal representations while adapting to target domain dynamics.

## Key Results
- Achieves 4.35% improvement in indoor temperature prediction accuracy over state-of-the-art baselines
- Achieves 11.54% improvement in wind power prediction accuracy over state-of-the-art baselines
- Demonstrates effectiveness across two real-world datasets: Energy Data (indoor temperature) and MORE Data (wind power)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gradual unfreezing (GU) preserves pre-trained temporal representations while allowing adaptation to target domain dynamics.
- **Mechanism**: By unfreezing layers progressively—starting with the decoder and moving toward the encoder—GU limits catastrophic forgetting during fine-tuning, as early layers (closer to input) maintain the original temporal pattern extraction capabilities learned in the source domain.
- **Core assumption**: Early layers of the Transformer capture domain-agnostic temporal features, while later layers specialize to task-specific patterns.
- **Evidence anchors**:
  - [abstract] "fine-tune the pre-trained model using a gradual unfreezing technique"
  - [section] "apply gradual unfreezing (GU) technique (Howard and Ruder 2018), where each layer of source model is frozen at first and then gradually unfreezed during each training epoch"
  - [corpus] "Domain Adaptation for Industrial Time-series Forecasting via Counterfactual Inference" (related, but does not explicitly validate GU)
- **Break condition**: If the target domain has radically different temporal patterns from the source, unfreezing all layers early may be required, negating GU's benefits.

### Mechanism 2
- **Claim**: Mixing a small percentage of source data into the target domain training set reduces distribution shift and mitigates catastrophic forgetting.
- **Mechanism**: The inclusion of source domain data during fine-tuning acts as a "replay buffer," exposing the model to both domains simultaneously. This dual exposure balances adaptation to the target domain while retaining knowledge of the source domain's temporal patterns.
- **Core assumption**: Both source and target domains share the same underlying learning task (e.g., indoor temperature prediction), so shared features are transferable.
- **Evidence anchors**:
  - [abstract] "adding some percentage of source domain data to the target domains"
  - [section] "we add some percentage of randomly sampled source domain data to the target domains. Including time series instances of the source domain in the target domains jointly addresses three fundamental issues: (i) the problem of data scarcity, (ii) data distribution mismatch, and (iii) catastrophic forgetting"
  - [corpus] "Domain Fusion Controllable Generalization for Cross-Domain Time Series Forecasting" (conceptually related but not directly validating this approach)
- **Break condition**: If the source and target domains are too dissimilar (high MMD), adding source data could bias the model away from the target domain's unique patterns.

### Mechanism 3
- **Claim**: Pre-training on a data-rich source domain provides generalizable temporal representations that can be transferred to target domains with limited data.
- **Mechanism**: Large-scale pre-training on diverse time series from the source domain captures robust, high-level temporal dependencies (e.g., periodicity, trend). These representations serve as a strong initialization for fine-tuning on smaller target datasets, enabling better performance than training from scratch.
- **Core assumption**: Temporal patterns learned in the source domain (e.g., daily/seasonal cycles) are relevant and transferable to the target domain.
- **Evidence anchors**:
  - [abstract] "pre-train the time series Transformer model on a source domain with sufficient data and fine-tune it on the target domain with limited data"
  - [section] "train a time series Transformer model, i.e., the source model, using the data from the source domain until it converges to a certain level of prediction accuracy"
  - [corpus] "Functional Mean Flow in Hilbert Space" (conceptually related but not directly validating transfer learning)
- **Break condition**: If the target domain has unique temporal patterns not present in the source, pre-trained representations may be insufficient, requiring more adaptation.

## Foundational Learning

- **Concept**: Long-range dependency modeling in time series
  - **Why needed here**: Transformers excel at capturing long-range temporal dependencies, which is critical for accurate time series forecasting. Understanding how self-attention mechanisms model these dependencies is essential for grasping why Transformers are chosen and how they can be adapted across domains.
  - **Quick check question**: What role does positional encoding play in enabling Transformers to understand temporal order in time series data?

- **Concept**: Catastrophic forgetting in continual learning
  - **Why needed here**: The paper explicitly addresses catastrophic forgetting as a key challenge. Understanding how neural networks lose previously learned knowledge when fine-tuned on new data is fundamental to appreciating the proposed solutions (GU and source data mixing).
  - **Quick check question**: How does the gradual unfreezing technique help mitigate catastrophic forgetting compared to unfreezing all layers at once?

- **Concept**: Domain adaptation and distribution shift
  - **Why needed here**: The core problem is adapting a model trained on one domain (source) to perform well on another (target) with different data distributions. Understanding metrics like Maximum Mean Discrepancy (MMD) and how they quantify domain differences is crucial for interpreting the experimental setup and results.
  - **Quick check question**: Why is it important to select target domains with varying MMD values relative to the source domain when evaluating domain adaptation performance?

## Architecture Onboarding

- **Component map**:
  - Positional Encoding Layer: Encodes temporal order into input embeddings using sinusoidal functions.
  - Encoder Layers: Stacked self-attention and feed-forward sub-layers for processing sequential input and capturing temporal dependencies.
  - Linear Decoder Layer: Maps encoder outputs to the final prediction space (e.g., future temperature values).

- **Critical path**:
  1. Pre-train Transformer on source domain data until convergence.
  2. For each target domain:
     - Mix a percentage of source domain data into the target training set.
     - Initialize model with pre-trained weights.
     - Apply gradual unfreezing during fine-tuning.
     - Evaluate on target domain test set.

- **Design tradeoffs**:
  - **Data mixing ratio**: Too little source data may not prevent catastrophic forgetting; too much may bias the model away from target domain patterns. The paper uses 5-20% depending on MMD.
  - **Unfreezing schedule**: Gradual vs. full unfreezing balances adaptation speed and stability. The paper uses a 10-20-35 epoch schedule.
  - **Positional encoding**: Sinusoidal encoding is fixed and may not adapt to target domain temporal patterns; learned encodings could be more flexible but risk overfitting with limited data.

- **Failure signatures**:
  - **High test error on source domain after fine-tuning**: Indicates catastrophic forgetting.
  - **Poor performance on target domain**: Suggests insufficient adaptation or mismatch between source and target temporal patterns.
  - **Overfitting on small target datasets**: May require stronger regularization or more source data mixing.

- **First 3 experiments**:
  1. **Pre-training validation**: Train the Transformer on the source domain and evaluate on a held-out source test set to confirm it learns meaningful temporal patterns.
  2. **Fine-tuning without data mixing**: Fine-tune the pre-trained model on a target domain *without* adding source data, using gradual unfreezing, to measure the impact of source data mixing.
  3. **Ablation on unfreezing schedule**: Compare gradual unfreezing (as in the paper) against full unfreezing at the start to quantify the benefit of the GU technique.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the One-step fine-tuning approach vary with different percentages of source domain data added to the target domains?
- Basis in paper: [explicit] The paper mentions that different percentages of source domain data were added to the target domains, and the performance was evaluated. For example, for the target domains S5, S49, and NIST, adding 5% of data from the source domain performed better compared to other percentages of added data, as the MMD values between these target domains with S4 are higher.
- Why unresolved: The paper provides specific examples of the optimal percentage of source domain data for certain target domains but does not explore the performance across a broader range of percentages or provide a general guideline for selecting the optimal percentage.
- What evidence would resolve it: A comprehensive study varying the percentage of source domain data added to the target domains and evaluating the performance for each percentage would provide insights into the optimal range of percentages for different scenarios.

### Open Question 2
- Question: How does the One-step fine-tuning approach compare to other domain adaptation techniques in terms of handling data shift and catastrophic forgetting?
- Basis in paper: [explicit] The paper mentions that the One-step fine-tuning approach addresses the issues of data shift and catastrophic forgetting by adding a percentage of source domain data to the target domains and using a gradual unfreezing technique. It also compares the performance of the One-step fine-tuning approach with other fine-tuning baselines, such as Gradual unfreezing (GU) and Elastic Weight Consolidation (EWC), and shows that it performs better in terms of mitigating catastrophic forgetting.
- Why unresolved: While the paper demonstrates the superiority of the One-step fine-tuning approach over specific baselines, it does not compare it with other domain adaptation techniques or provide a comprehensive evaluation of its effectiveness in handling data shift and catastrophic forgetting compared to other methods.
- What evidence would resolve it: A thorough comparison of the One-step fine-tuning approach with other domain adaptation techniques, such as domain adversarial training, feature disentanglement, or meta-learning approaches, would provide insights into its relative effectiveness in handling data shift and catastrophic forgetting.

### Open Question 3
- Question: How does the performance of the One-step fine-tuning approach vary with different data granularities and temporal patterns in the time series data?
- Basis in paper: [inferred] The paper mentions that the datasets used for evaluation have different data granularities, such as 15-minute intervals for the Energy data and 1-hour intervals for the MORE data. However, it does not explore the impact of data granularity on the performance of the One-step fine-tuning approach or investigate its effectiveness in handling different temporal patterns, such as seasonality or trends.
- Why unresolved: The paper does not provide a detailed analysis of how the One-step fine-tuning approach performs with varying data granularities or how it handles different temporal patterns in the time series data. This information is crucial for understanding the approach's applicability and limitations in real-world scenarios with diverse data characteristics.
- What evidence would resolve it: An extensive evaluation of the One-step fine-tuning approach using datasets with different data granularities and temporal patterns, such as daily, weekly, or monthly data, and datasets with varying degrees of seasonality or trends, would provide insights into its performance and robustness across different scenarios.

## Limitations
- The paper relies on empirical selection of source data mixing percentages (5-20%) without theoretical justification or sensitivity analysis.
- Limited experimental scope to two datasets raises questions about generalizability to other time series domains with different characteristics.
- Absence of detailed architectural specifications and source code presents a significant barrier to independent reproduction.

## Confidence
- **High Confidence**: The core problem formulation (data scarcity, distribution shift, catastrophic forgetting) is well-established in domain adaptation literature. The proposed mechanisms (source data mixing, gradual unfreezing) are grounded in existing techniques and show consistent improvements across both datasets.
- **Medium Confidence**: The quantitative improvements (4.35% for indoor temperature, 11.54% for wind power) are promising but may be dataset-specific. The effectiveness of the GU schedule (10-20-35 epochs) and data mixing ratios could vary with different domain pairs or Transformer architectures.
- **Low Confidence**: The paper does not provide ablation studies on architectural choices (e.g., number of layers, attention heads) or hyperparameter sensitivity analysis, limiting confidence in the robustness of the approach to implementation details.

## Next Checks
1. **Ablation on Unfreezing Schedule**: Systematically compare gradual unfreezing against full unfreezing at the start, and test intermediate schedules (e.g., unfreezing every 5 epochs) to quantify the contribution of the GU technique to performance gains.
2. **Sensitivity Analysis on Data Mixing Ratio**: For each target domain, test a range of source data mixing percentages (e.g., 1%, 5%, 10%, 20%, 30%) to determine if the 5-20% range is optimal or dataset-dependent.
3. **Cross-Dataset Generalization**: Apply the One-step fine-tuning approach to a third, independent time series dataset (e.g., traffic flow or financial data) with different temporal characteristics to assess the method's robustness beyond the Energy and MORE datasets.