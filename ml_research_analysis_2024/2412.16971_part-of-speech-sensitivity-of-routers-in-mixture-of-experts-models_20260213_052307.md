---
ver: rpa2
title: Part-Of-Speech Sensitivity of Routers in Mixture of Experts Models
arxiv_id: '2412.16971'
source_url: https://arxiv.org/abs/2412.16971
tags:
- experts
- layers
- layer
- tokens
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the sensitivity of routers in Mixture of
  Experts (MoE) models to Part-of-Speech (POS) tags. Using six popular MoE models,
  the research analyzes how tokens are routed to experts based on their linguistic
  features.
---

# Part-Of-Speech Sensitivity of Routers in Mixture of Experts Models

## Quick Facts
- arXiv ID: 2412.16971
- Source URL: https://arxiv.org/abs/2412.16971
- Reference count: 40
- Six MoE models show expert specialization for specific POS categories with routing paths achieving 0.79-0.88 POS prediction accuracy

## Executive Summary
This study investigates whether routers in Mixture of Experts (MoE) models exhibit sensitivity to the Part-of-Speech (POS) of tokens. Using six popular MoE models of varying sizes and architectures, the research analyzes routing patterns to determine if tokens are systematically routed to specific experts based on their linguistic features. The findings reveal significant expert specialization for POS categories, with routing paths serving as highly predictive features for POS tags. The study also demonstrates that earlier layers contain more token-characterizing information, and that KL divergence analysis reveals varying degrees of expert specialization across models and layers.

## Method Summary
The study uses OntoNotes 5.0 corpus (5000 random English sentences, 116,379 tokens) converted to Universal Dependencies POS tags. Six MoE models were analyzed including dbrx-base, Mixtral-8x7B-v0.1, Phi-3.5-MoE-instruct, deepseek-moe-16b-base, Qwen1.5-MoE-A2.7B, and OLMoE-1B-7B. Routing paths were extracted by recording top-k experts selected at each layer for all tokens. Layer-wise specialization scores were computed and compared to uniform distributions, while KL divergence measured expert distribution divergence from corpus baselines. MLP classifiers were trained to predict POS from routing paths, and ablation studies progressively removed layer information to assess impact on POS prediction accuracy.

## Key Results
- Expert specialization scores range from +25.21% to +36.32% across models compared to uniform distribution
- POS prediction accuracy from routing paths ranges from 0.79-0.88 across all six models
- KL divergence analysis shows varying specialization degrees across models and layers
- Earlier layers contain more token-characterizing information according to ablation study results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Router specialization arises from positional weighting during softmax routing
- Mechanism: The softmax over top-k logits creates steep gradients favoring tokens with similar feature patterns to route to the same expert. Over training, this amplifies small initial biases into stable routing patterns.
- Core assumption: The input representation contains separable POS-relevant features that the router can learn to discriminate.
- Evidence anchors:
  - [abstract] "Findings from six popular MoE models reveal expert specialization for specific POS categories"
  - [section] "The probability of routing token x to the ith expert among N is given by: pi(x) = e^(h(x)i) / sum_j e^(h(x)j)"
  - [corpus] Weak: Related papers focus on router design but do not discuss POS sensitivity directly.
- Break condition: If POS features are not separable in the token embeddings, the router will distribute tokens uniformly regardless of POS.

### Mechanism 2
- Claim: Earlier layers contain more token-characterizing information
- Mechanism: Initial layers perform broad feature extraction while later layers refine features. POS information is encoded early, making first-layer routing more diagnostic.
- Core assumption: The model's feature hierarchy places coarse syntactic information in early layers.
- Evidence anchors:
  - [abstract] "earlier layers containing more token-characterizing information"
  - [section] "Ablation Study: removing information from the first layers has a greater impact on POS prediction"
  - [corpus] Weak: No direct corpus evidence about layer-wise information content.
- Break condition: If the model architecture reverses typical hierarchy or if POS information is distributed uniformly across layers.

### Mechanism 3
- Claim: Expert specialization is measurable via KL divergence from corpus distribution
- Mechanism: Each expert's POS distribution diverges from the corpus baseline when it processes a non-representative subset of tokens, indicating specialization.
- Core assumption: Specialization manifests as distributional shifts in POS handling across experts.
- Evidence anchors:
  - [abstract] "KL divergence analysis indicates varying degrees of expert specialization across models"
  - [section] "We compute the Kullback-Leibler (KL) divergence between each expert's probability distribution at every layer and the corpus distribution"
  - [corpus] Weak: Related work focuses on router optimization, not distributional analysis.
- Break condition: If all experts maintain corpus-like distributions despite apparent routing patterns.

## Foundational Learning

- Concept: Mixture-of-Experts routing mechanisms
  - Why needed here: Understanding how tokens are assigned to experts is fundamental to interpreting specialization results
  - Quick check question: What mathematical operation determines which experts receive tokens?

- Concept: Part-of-Speech tagging and Universal Dependencies
  - Why needed here: The study uses POS tags as linguistic features to analyze routing patterns
  - Quick check question: How are token-level POS tags derived when tokens don't align with word boundaries?

- Concept: KL divergence and distributional analysis
  - Why needed here: KL divergence measures how expert POS distributions differ from baseline, quantifying specialization
  - Quick check question: What does a high KL divergence between expert and corpus POS distributions indicate?

## Architecture Onboarding

- Component map: Token embedding -> Router (learned linear layer) -> Softmax -> Top-k selection -> Expert processing -> Weighted sum
- Critical path: Token embedding -> Router logits -> Routing probabilities -> Expert selection -> Expert computation -> Output combination
- Design tradeoffs: More experts increase parameter count but not computation (only k active); k controls computation vs. routing flexibility
- Failure signatures: Uniform routing across all experts, low MLP POS prediction accuracy, KL divergence near zero
- First 3 experiments:
  1. Verify routing probabilities sum to 1 and top-k selection works correctly
  2. Check POS distribution uniformity across experts (should show deviation)
  3. Test MLP POS prediction accuracy using routing paths as features

## Open Questions the Paper Calls Out

- Question: Does expert specialization for POS tags generalize across different languages and domains beyond English OntoNotes?
  - Basis in paper: [explicit] The paper explicitly states a limitation that "we only tested English within the domain covered by OntoNotes, which consists of relatively short sentences"
  - Why unresolved: The study only used 5000 random English sentences from OntoNotes 5.0. No multilingual or domain-specific experiments were conducted to test whether the observed POS-based expert specialization patterns hold across different linguistic contexts.
  - What evidence would resolve it: Experiments testing the same POS routing patterns across multiple languages (e.g., morphologically rich languages, languages with different word order) and diverse domains (legal, medical, conversational text) would reveal whether this specialization is a universal property of MoE models or specific to the English OntoNotes corpus.

- Question: How do routers behave differently during inference/generation compared to processing context tokens?
  - Basis in paper: [explicit] The paper states "A limitation of our study is the focus on context tokens, without examining router behavior on generated tokens"
  - Why unresolved: All experiments focused on analyzing routing decisions for input tokens only. The paper explicitly acknowledges not examining how routers behave when generating new tokens during inference.
  - What evidence would resolve it: Analyzing routing patterns for generated tokens across different generation scenarios (different decoding strategies, temperature settings, model sizes) would reveal whether routers maintain consistent POS specialization during generation or exhibit different routing behaviors.

- Question: Which specific architectural or training factors contribute to stronger or weaker POS specialization across models?
  - Basis in paper: [inferred] The paper shows varying degrees of specialization across six different MoE models (Spec ranging from +25.21% to +36.32%) but does not systematically analyze why certain models show stronger specialization than others.
  - Why unresolved: While the paper compares multiple models, it does not investigate the relationship between architectural choices (number of experts, layers, routing mechanisms) or training configurations and the observed specialization patterns.
  - What evidence would resolve it: Controlled experiments varying individual architectural parameters (expert count, routing k value, layer depth) or training conditions (dataset size, curriculum learning) while measuring resulting POS specialization would identify which factors most strongly influence expert specialization development.

## Limitations
- Study focuses exclusively on English text from OntoNotes corpus, limiting generalizability to other languages
- Analysis assumes POS tags directly reflect syntactic information captured by routers, but POS taggers may introduce biases
- Router behavior during generation/inference not examined, only context token processing analyzed

## Confidence
- High Confidence: POS prediction accuracy from routing paths (0.79-0.88) across all six models
- Medium Confidence: Expert specialization for specific POS categories based on routing patterns and KL divergence
- Medium Confidence: Earlier layers contain more token-characterizing information from ablation study results

## Next Checks
1. Cross-linguistic validation: Test POS sensitivity of MoE routers using multiple languages from Universal Dependencies dataset
2. Dynamic routing analysis: Implement sliding window analysis to examine routing pattern changes for tokens in different syntactic contexts
3. Alternative feature ablation: Replace MLP POS prediction with different classifier architectures to verify methodology independence