---
ver: rpa2
title: 'DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation'
arxiv_id: '2404.00264'
source_url: https://arxiv.org/abs/2404.00264
tags:
- dataset
- samples
- dilm
- synthetic
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiLM, a novel text-level dataset distillation
  method that trains a language model to generate informative synthetic training samples
  as text data, rather than optimizing synthetic samples as word embeddings. The key
  innovation is bypassing the discrete nature of text by using a language model as
  a surrogate continuous optimization target and backpropagating the distillation
  loss through differentiable generation probabilities.
---

# DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation

## Quick Facts
- arXiv ID: 2404.00264
- Source URL: https://arxiv.org/abs/2404.00264
- Reference count: 21
- Key outcome: DiLM trains a language model to generate text-level synthetic training samples via gradient matching, outperforming coreset selection methods on GLUE benchmark tasks and enabling model-agnostic distillation

## Executive Summary
This paper introduces DiLM, a novel text-level dataset distillation method that trains a language model to generate informative synthetic training samples as text data, rather than optimizing synthetic samples as word embeddings. The key innovation is bypassing the discrete nature of text by using a language model as a surrogate continuous optimization target and backpropagating the distillation loss through differentiable generation probabilities. Evaluated on three GLUE benchmark text classification tasks, DiLM outperformed coreset selection methods in training BERT, RoBERTa, BERT-Large, and XLNet models, achieving higher accuracy with smaller distilled datasets. DiLM also demonstrated strong generalization to in-context learning of large language models as few-shot prompts. The approach addresses the key limitation of existing embedding-level distillation methods by producing text-level synthetic datasets that can be used to train models independent of word embedding weights, enabling model-agnostic applicability and interpretability.

## Method Summary
DiLM trains a GPT-2 language model to generate synthetic text samples for dataset distillation. The method uses gradient matching loss where the learner's loss (BERT-base) is weighted by generation probabilities to bypass the discrete nature of text sampling. During training, the generator produces N×Iint synthetic samples, which are clustered and a diverse mini-batch is selected for each optimization step. For the final distilled dataset, K-centers clustering selects representative samples from a larger pool of generated samples. The approach enables text-level distillation where synthetic samples are actual text that can train any model, unlike embedding-level methods that produce samples tied to specific word embeddings.

## Key Results
- DiLM outperformed coreset selection methods on SST-2, QQP, and MNLI-m GLUE tasks when training BERT, RoBERTa, BERT-Large, and XLNet models
- Achieved higher accuracy with smaller distilled datasets compared to baselines
- Demonstrated strong generalization to in-context learning of large language models as few-shot prompts
- Produced interpretable text-level synthetic samples versus embedding-level alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a language model as a surrogate continuous optimization target allows gradient-based optimization of discrete text.
- Mechanism: Instead of directly optimizing discrete text tokens, the language model's continuous parameters are optimized. The gradient matching loss is backpropagated through the language model via differentiable generation probabilities that weight the learner's loss.
- Core assumption: The language model's continuous parameters can serve as an effective proxy for the discrete text optimization problem.
- Evidence anchors:
  - [abstract]: "bypassing the discrete nature of text by using a language model as a surrogate continuous optimization target and backpropagating the distillation loss through differentiable generation probabilities."
  - [section 3.3]: "we design an alternative backward pass, inspired by a previous study (Hiraoka et al., 2020), which optimizes a tokenization model for the downstream task's loss through a non-differentiable procedure."
  - [corpus]: Weak evidence - only 1/5 related papers mention similar concepts of using generative models for dataset distillation, but not specifically for text-level distillation.
- Break condition: If the language model's continuous parameters do not adequately capture the information needed to generate informative synthetic text samples.

### Mechanism 2
- Claim: Representative teacher samples improve gradient matching by providing diverse coverage while avoiding decision boundary samples.
- Mechanism: Instead of random samples, K-centers clustering selects representative samples that cover the overall distribution for each class and eliminates samples near decision boundaries which have dominant gradients.
- Core assumption: Representative samples provide better gradient teachers than random samples for dataset distillation.
- Evidence anchors:
  - [section 3.3]: "the representative samples selected by K-centers provide the proper teacher gradient by including diverse samples that cover the overall distribution for each class and eliminating samples near the decision boundaries, which have dominant gradients with large norms."
  - [section 4]: "We used the last hidden state of the [CLS] token in the BERTBASE model as a feature of each training sample."
  - [corpus]: Weak evidence - only 1/5 related papers mention similar concepts of using representative sampling, but not specifically for text dataset distillation.
- Break condition: If the representative samples fail to capture the true distribution of the original dataset or if the clustering becomes unstable with limited data.

### Mechanism 3
- Claim: Diverse mini-batch sampling prevents biased optimization by ensuring the generator explores a broad sample space.
- Mechanism: Instead of generating N synthetic samples for each step, the generator generates N×Iint synthetic samples, clusters them, and randomly selects one sample from each sub-cluster to construct the mini-batch.
- Core assumption: Diversity in generated samples prevents the generator from overfitting to a narrow distribution during training.
- Evidence anchors:
  - [section 3.3]: "Diversity in a mini-batch of generated samples for each step affects the sample space that the generator model explores in training. If the generator model only generates many samples that are similar to each other, this leads to the biased optimization of the generator model."
  - [section 4]: "We set the generation interval to Iint = 200 according to our preliminary experiments."
  - [corpus]: Weak evidence - none of the 5 related papers mention similar concepts of diverse mini-batch sampling for dataset distillation.
- Break condition: If the clustering algorithm fails to properly separate the generated samples or if the diversity requirement becomes too computationally expensive.

## Foundational Learning

- Concept: Gradient matching for dataset distillation
  - Why needed here: This is the core objective function that drives the generation of informative synthetic samples. It ensures that models trained on synthetic data behave similarly to those trained on real data.
  - Quick check question: What is the mathematical formulation of the gradient matching loss and how does it differ from other dataset distillation objectives?

- Concept: Differentiable approximation of non-differentiable operations
  - Why needed here: Text generation involves discrete sampling which is non-differentiable. This concept enables backpropagation through the sampling process.
  - Quick check question: How does the weighted loss approach in DiLM approximate the gradient through discrete sampling, and what are its limitations compared to other approaches like straight-through estimators?

- Concept: Coreset selection methods
  - Why needed here: Coreset selection is used both for selecting representative teacher samples and for selecting the final distilled dataset from generated samples.
  - Quick check question: What are the key differences between K-centers, Herding, and Random selection methods, and how do they affect the quality of the distilled dataset?

## Architecture Onboarding

- Component map: GPT-2 Generator -> Text generation -> BERT Learner loss computation -> Gradient matching loss -> Backpropagation to generator -> Update generator parameters
- Critical path: Generator → Text generation → Learner loss computation → Gradient matching loss → Backpropagation to generator → Update generator parameters
- Design tradeoffs:
  - Using GPT-2 vs larger language models: Smaller model reduces computational cost but may limit sample quality
  - Gradient matching vs other objectives: Simpler to implement but may be less effective than trajectory matching
  - Text-level vs embedding-level distillation: More flexible but potentially lower performance
- Failure signatures:
  - Generated samples are nonsensical or repetitive
  - Performance degrades when using distilled samples for training
  - Training becomes unstable or fails to converge
- First 3 experiments:
  1. Verify that the generator can produce coherent text samples before training
  2. Test gradient matching with a small synthetic dataset to ensure backpropagation works
  3. Compare performance of K-centers vs random selection on a simple dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of generator model (e.g., GPT-2 vs. larger models like GPT-3 or GPT-4) impact the performance of DiLM in dataset distillation?
- Basis in paper: [explicit] The paper mentions using GPT-2 as the generator model and notes potential for improvement by using larger models.
- Why unresolved: The paper only experiments with GPT-2, leaving the impact of using larger models unexplored.
- What evidence would resolve it: Experiments comparing DiLM's performance using different sizes of generator models (e.g., GPT-2, GPT-3, GPT-4) on the same datasets.

### Open Question 2
- Question: Can DiLM be effectively applied to text generation tasks, not just text classification?
- Basis in paper: [explicit] The paper states DiLM can be applied to text generation tasks by treating the entire dataset as a single label, but does not provide experimental results.
- Why unresolved: The paper only evaluates DiLM on text classification datasets, not text generation tasks.
- What evidence would resolve it: Experiments applying DiLM to text generation tasks (e.g., summarization, translation) and comparing the performance of models trained on DiLM-generated data versus the original data.

### Open Question 3
- Question: How does DiLM's performance scale with the size of the original dataset? Is there a point of diminishing returns?
- Basis in paper: [explicit] The paper experiments with different dataset sizes but does not analyze the relationship between original dataset size and DiLM's performance.
- Why unresolved: The paper does not provide a systematic analysis of how DiLM's performance changes as the original dataset size increases.
- What evidence would resolve it: Experiments training DiLM on datasets of varying sizes and analyzing the relationship between original dataset size and the performance of models trained on the distilled data.

### Open Question 4
- Question: How does DiLM compare to other text dataset distillation methods that produce text-level synthetic data?
- Basis in paper: [explicit] The paper mentions other methods (e.g., Sahni and Patel, 2023) but does not provide a direct comparison with DiLM.
- Why unresolved: The paper only compares DiLM to embedding-level distillation methods and coreset selection methods, not other text-level methods.
- What evidence would resolve it: Experiments comparing DiLM's performance to other text-level distillation methods on the same datasets.

## Limitations

- Theoretical foundations for gradient matching through language model surrogates remain incomplete
- Limited evaluation to three GLUE classification tasks without testing on diverse text domains
- Computational overhead of generating 100x more samples than target distilled dataset size

## Confidence

**High Confidence Claims**
- DiLM produces text-level synthetic datasets that can train BERT, RoBERTa, BERT-Large, and XLNet models
- The method outperforms coreset selection baselines on SST-2, QQP, and MNLI-m tasks
- Representative teacher sampling via K-centers improves gradient matching quality

**Medium Confidence Claims**
- The gradient matching loss through generation probabilities effectively approximates discrete text optimization
- Diverse mini-batch sampling prevents generator overfitting during training
- Text-level distillation provides advantages over embedding-level approaches for model-agnostic applicability

**Low Confidence Claims**
- DiLM's performance on few-shot prompting for large language models generalizes beyond the tested examples
- The computational overhead of generating 100x samples is justified by the performance gains
- The method's effectiveness transfers to non-classification tasks or datasets with different characteristics

## Next Checks

1. **Cross-domain validation**: Apply DiLM to datasets from different domains (e.g., biomedical text, legal documents, conversational data) to test generalization beyond the GLUE benchmark. Measure both performance and sample quality across these domains.

2. **Computational efficiency analysis**: Quantify the training time and memory requirements for DiLM versus embedding-level distillation methods. Compare the trade-off between computational cost and accuracy improvements across different dataset sizes.

3. **Ablation study of core components**: Systematically remove or modify key design choices (gradient matching vs alternative objectives, K-centers vs other selection methods, generation probability weighting vs straight-through estimators) to isolate their individual contributions to overall performance.