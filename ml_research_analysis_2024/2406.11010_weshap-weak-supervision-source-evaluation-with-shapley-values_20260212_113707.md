---
ver: rpa2
title: 'WeShap: Weak Supervision Source Evaluation with Shapley Values'
arxiv_id: '2406.11010'
source_url: https://arxiv.org/abs/2406.11010
tags:
- weshap
- value
- accuracy
- shapley
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WeShap, a novel evaluation metric for labeling
  functions (LFs) in programmatic weak supervision (PWS) pipelines. The key innovation
  is using Shapley values to measure each LF's contribution to downstream model accuracy.
---

# WeShap: Weak Supervision Source Evaluation with Shapley Values

## Quick Facts
- arXiv ID: 2406.11010
- Source URL: https://arxiv.org/abs/2406.11010
- Authors: Naiqing Guan; Nick Koudas
- Reference count: 40
- Key outcome: Novel WeShap metric using Shapley values to evaluate LFs in PWS pipelines, achieving 5.0-point average improvement in downstream model accuracy

## Executive Summary
This paper introduces WeShap, a novel evaluation metric for labeling functions (LFs) in programmatic weak supervision (PWS) pipelines. The key innovation is using Shapley values from cooperative game theory to measure each LF's contribution to downstream model accuracy. Unlike existing metrics that focus on accuracy or coverage, WeShap considers the data distribution in feature space and directly quantifies impact on model performance. The authors demonstrate efficient computation of WeShap values using dynamic programming, achieving quadratic complexity relative to the number of LFs. Experiments across nine datasets show WeShap values effectively identify beneficial and harmful LFs, enabling pipeline refinement.

## Method Summary
WeShap treats each LF as a "player" in a cooperative game where the "payoff" is the accuracy gain of a proxy PWS pipeline (majority voting + KNN). The Shapley value formula allocates this payoff fairly among LFs based on their marginal contributions. WeShap values are computed efficiently using dynamic programming to reduce computational complexity from O(2^m) to O(m^2). The method leverages the recursive structure of Shapley value computation to reuse previously computed values, dramatically reducing the number of operations needed.

## Key Results
- WeShap values effectively identify beneficial and harmful LFs across nine diverse datasets
- The method achieves an average 5.0-point improvement in downstream model accuracy compared to state-of-the-art approaches
- WeShap values help understand pipeline behaviors and diagnose mispredictions, offering a comprehensive tool for improving data quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WeShap values directly measure each LF's contribution to downstream model accuracy by using Shapley values from cooperative game theory.
- Mechanism: The WeShap value treats each LF as a "player" in a cooperative game where the "payoff" is the accuracy gain of a proxy PWS pipeline (MV + KNN). The Shapley value formula then allocates this payoff fairly among LFs based on their marginal contributions.
- Core assumption: The proxy pipeline (MV + KNN) is a reasonable approximation of the actual pipeline's behavior.
- Evidence anchors:
  - [abstract]: "This metric quantifies the average contribution of weak supervision sources within a proxy PWS pipeline, leveraging the theoretical underpinnings of Shapley values."
  - [section 3.2]: "Theorem 2. The WeShap value of an LF is equal to its Shapley value in the proxy game using the same set of LFs and holdout dataset."
  - [corpus]: Weak evidence - related papers mention Shapley values but not this specific proxy framework.
- Break condition: If the proxy pipeline (MV + KNN) poorly approximates the actual pipeline, WeShap values may not accurately reflect true LF contributions.

### Mechanism 2
- Claim: WeShap values enable efficient LF evaluation by leveraging dynamic programming to reduce computational complexity from O(2^m) to O(m^2).
- Mechanism: The recursive formula for computing Shapley values exploits overlapping subproblems, allowing reuse of previously computed values. This dramatically reduces the number of operations needed.
- Core assumption: The Shapley value formula has overlapping subproblems that can be exploited for dynamic programming.
- Evidence anchors:
  - [section 3.3]: "Directly computing all ð‘†ð‘‰ +ð‘,ð‘¤ values following Equation 7 requires O (ð‘š4) computations, but this can be optimized further using dynamic programming."
  - [section 3.3]: "The recursive formula, we can compute all ð‘†ð‘‰ +ð‘,ð‘¤ values in O (ð‘š2) computations."
  - [corpus]: Weak evidence - related papers mention Shapley values but not this specific dynamic programming approach.
- Break condition: If the Shapley value formula doesn't have sufficient overlapping subproblems, dynamic programming may not provide significant speedup.

### Mechanism 3
- Claim: WeShap values enable fine-grained revision of the PWS pipeline by providing per-LF contribution scores that can be used to silence or modify specific LF outputs.
- Mechanism: The WeShap contribution score (ð‘¤ð‘– ð‘—) quantifies the impact of a specific weak label (ðœ†ð‘— (ð‘¥ð‘– )) on the accuracy of the proxy PWS pipeline. Low or negative scores indicate harmful labels that can be silenced.
- Core assumption: The proxy pipeline's accuracy is sensitive to individual weak label contributions.
- Evidence anchors:
  - [section 3.2]: "The WeShap contribution scores serve as detailed metrics quantifying the impact of weak labels ðœ†ð‘— (ð‘¥ð‘– ) on the accuracy of the proxy PWS pipeline."
  - [section 4.3]: "WeShap-F distinguishes itself by outshining all other baseline methods in 6 out of 9 datasets, with the MedAbs dataset witnessing the most substantial leap in performance, an 11.7-point increase."
  - [corpus]: Weak evidence - related papers mention Shapley values but not this specific application to LF revision.
- Break condition: If the proxy pipeline's accuracy is not sensitive to individual weak label contributions, WeShap contribution scores may not be useful for fine-grained revision.

## Foundational Learning

- Concept: Cooperative game theory and Shapley values
  - Why needed here: WeShap values are derived from Shapley values, which allocate payoffs in cooperative games. Understanding this theory is crucial for grasping how WeShap values measure LF contributions.
  - Quick check question: What are the key properties of Shapley values that make them suitable for LF evaluation?

- Concept: Dynamic programming
  - Why needed here: WeShap values are computed efficiently using dynamic programming to reduce computational complexity. Understanding this technique is essential for implementing WeShap.
  - Quick check question: How does dynamic programming exploit overlapping subproblems to improve efficiency?

- Concept: K-nearest neighbors (KNN) algorithm
  - Why needed here: The proxy PWS pipeline uses KNN as the downstream model. Understanding KNN is important for comprehending how WeShap values are computed and how they relate to data distribution.
  - Quick check question: How does the choice of K and distance metric affect the performance of the KNN model in the proxy pipeline?

## Architecture Onboarding

- Component map: Unlabeled training data -> Majority voting label model -> KNN downstream model -> WeShap computation -> WeShap values and contribution scores for each LF
- Critical path:
  1. Compute WeShap weights for each LF on each training point
  2. Identify K-nearest neighbors for each validation point
  3. Aggregate WeShap weights to compute WeShap values and contribution scores
- Design tradeoffs:
  - Proxy pipeline choice: Simpler models (MV + KNN) enable efficient computation but may not perfectly approximate the actual pipeline.
  - Dynamic programming: Reduces computational complexity but requires additional memory to store intermediate results.
- Failure signatures:
  - Poor proxy pipeline approximation: WeShap values may not accurately reflect true LF contributions.
  - Insufficient training data: Dynamic programming may not provide significant speedup if there are few overlapping subproblems.
- First 3 experiments:
  1. Implement WeShap computation on a small dataset with known LF contributions to verify correctness.
  2. Compare WeShap values to baseline LF evaluation metrics (accuracy, coverage) on a larger dataset.
  3. Evaluate the impact of WeShap values on PWS pipeline revision by comparing downstream model accuracy before and after revision.

## Open Questions the Paper Calls Out
- How can WeShap values be efficiently computed for label models beyond majority voting, such as Snorkel MeTaL or other probabilistic models?
- What is the impact of dataset smoothness on WeShap's ability to rank labeling functions across different feature spaces and data modalities?
- Can WeShap values be adapted for multi-label classification scenarios where instances can have multiple non-exclusive labels?

## Limitations
- The reliance on a proxy pipeline (MV + KNN) may not generalize well to all PWS scenarios, particularly those using more complex downstream models
- The computational efficiency gains depend heavily on the assumption that Shapley values exhibit sufficient overlapping subproblems
- The method's performance on highly imbalanced datasets or those with complex feature interactions is not fully explored

## Confidence
- WeShap's effectiveness in identifying beneficial/harmful LFs: High
- Computational efficiency of dynamic programming approach: Medium
- Generalizability to non-text/image domains: Low

## Next Checks
1. Test WeShap on a PWS pipeline using a neural network as the downstream model rather than KNN to verify proxy pipeline assumptions
2. Evaluate performance on datasets with known challenging characteristics (high class imbalance, complex feature interactions)
3. Conduct ablation studies to isolate the contribution of dynamic programming efficiency from the core Shapley value methodology