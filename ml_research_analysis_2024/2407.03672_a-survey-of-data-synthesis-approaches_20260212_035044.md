---
ver: rpa2
title: A Survey of Data Synthesis Approaches
arxiv_id: '2407.03672'
source_url: https://arxiv.org/abs/2407.03672
tags:
- data
- synthetic
- augmentation
- learning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper surveys synthetic data techniques, categorizing them
  into four generations based on machine learning advancements: Expert Knowledge,
  Direct Training, Pre-train then Fine-tune, and Foundation Models without Fine-tuning.
  It identifies four key objectives for data augmentation: Improving Diversity, Data
  Balancing, Addressing Domain Shift, and Resolving Edge Cases.'
---

# A Survey of Data Synthesis Approaches

## Quick Facts
- arXiv ID: 2407.03672
- Source URL: https://arxiv.org/abs/2407.03672
- Reference count: 15
- The paper surveys synthetic data techniques, categorizing them into four generations based on machine learning advancements and identifying four key objectives for data augmentation.

## Executive Summary
This survey comprehensively examines synthetic data generation techniques in machine learning, organizing them into four generational categories aligned with ML advancements: Expert Knowledge, Direct Training, Pre-train then Fine-tune, and Foundation Models without Fine-tuning. The paper identifies four primary objectives for data augmentation: improving diversity, data balancing, addressing domain shift, and resolving edge cases. It also discusses post-processing methods to ensure data quality, label consistency, and appropriate distribution. The survey highlights emerging trends toward quality-focused augmentation and identifies key future directions including standardized evaluation benchmarks and multi-modal data augmentation.

## Method Summary
The survey employs a literature review approach, categorizing synthetic data techniques based on their relationship to prevailing machine learning methods across different eras. The authors systematically organize techniques into four generational categories, examine their applications across different data augmentation objectives, and analyze post-processing methods for quality assurance. The methodology involves identifying and synthesizing relevant papers, examining their technical approaches, and organizing findings into a structured framework that maps synthetic data evolution to broader ML trends.

## Key Results
- Synthetic data generation methods evolve in sync with prevailing machine learning techniques, creating distinct generational approaches
- Four key objectives for data augmentation are identified: Improving Diversity, Data Balancing, Addressing Domain Shift, and Resolving Edge Cases
- Post-processing synthetic data for quality, label consistency, and distribution alignment is essential for effective model performance
- The field is shifting from quantity-focused to quality-focused data augmentation as dataset sizes grow

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data generation methods evolve in sync with prevailing machine learning techniques, leading to distinct generational approaches.
- Mechanism: Each generation of synthetic data techniques leverages the dominant ML paradigm of its time—expert knowledge engineering for classical ML, direct training for early deep learning, pre-train then fine-tune for transfer learning, and foundation models without fine-tuning for large pre-trained models.
- Core assumption: The available ML techniques constrain and shape the most effective approaches to synthetic data generation.
- Evidence anchors:
  - [abstract] "Synthesizing data are closely related to the prevailing machine learning techniques at the time, therefore, we summarize the domain of synthetic data techniques into four categories..."
  - [section 3] "we divided the eras of synthetic data techniques into four periods: Expert Knowledge, Direct Training, pre-train then Fine-tune, and Foundation Model without Fine-tuning."
  - [corpus] Weak—related surveys focus on data augmentation broadly but don't explicitly trace evolution alongside ML technique eras.
- Break condition: When new ML paradigms emerge that don't fit neatly into the four categories, or when hybrid approaches blur generational boundaries.

### Mechanism 2
- Claim: Post-processing synthetic data for quality, label consistency, and distribution alignment is essential to ensure synthetic data improves model performance.
- Mechanism: Raw synthetic data generated through various techniques may contain errors, inconsistencies, or distributional mismatches with target tasks. Post-processing filters remove low-quality samples, ensures generated labels match content, and aligns data distribution with training needs.
- Core assumption: Synthetic data generation is imperfect and requires refinement before use.
- Evidence anchors:
  - [abstract] "we categorize the goals of synthetic data filtering into four types for discussion: 1) Basic Quality, 2) Label Consistency, and 3) Data Distribution."
  - [section 4.1] "Basic quality encompasses elements such as fluency, grammatical accuracy, format validation among others."
  - [section 4.2] "When generating synthetic data with labels, there is a possibility of discrepancies between the data and its labels."
- Break condition: When synthetic generation becomes so accurate that post-processing filtering provides negligible performance gains.

### Mechanism 3
- Claim: The shift from quantity-focused to quality-focused data augmentation improves model performance more effectively as dataset sizes grow.
- Mechanism: Early in ML development, simply increasing dataset size through augmentation yielded significant performance gains. As models and datasets mature, adding more data shows diminishing returns while improving data quality yields better results.
- Core assumption: The marginal benefit of additional data decreases as dataset size increases.
- Evidence anchors:
  - [section 5.1] "The emerging trend is toward enabling models to learn effectively from smaller but high quality datasets, achieving performance levels comparable to those obtained from larger but low quality datasets."
  - [section 2.1] "Previous works have found that simply increasing the training data size through data augmentation can often lead to overfitting during subsequent model training."
- Break condition: When data quality improvements plateau and further gains require fundamentally new modeling approaches rather than better data.

## Foundational Learning

- Concept: Machine Learning Technique Evolution
  - Why needed here: Understanding how synthetic data techniques map to ML eras requires knowing what each era's dominant techniques were and their capabilities.
  - Quick check question: Can you explain the key differences between expert systems, direct training neural networks, transfer learning, and foundation models?

- Concept: Data Distribution and Domain Shift
  - Why needed here: Many synthetic data objectives (addressing domain shift, data balancing, edge cases) depend on understanding how data distributions affect model generalization.
  - Quick check question: How would you measure whether a synthetic dataset has successfully addressed a domain shift problem?

- Concept: Evaluation Metrics for Synthetic Data Quality
  - Why needed here: Post-processing and quality-focused augmentation require metrics to assess synthetic data effectiveness beyond just model performance.
  - Quick check question: What metrics would you use to evaluate whether synthetic data maintains diversity while avoiding overfitting?

## Architecture Onboarding

- Component map: Generation stage → Post-processing stage → Integration stage → Model Training
- Critical path: Generation → Post-processing → Integration → Model Training. The post-processing step is critical because poor quality synthetic data can degrade rather than improve model performance.
- Design tradeoffs: Expert knowledge methods are fast but limited in diversity; direct training methods are flexible but require task-specific data; pre-train then fine-tune balances resource use but risks overfitting; foundation models are powerful but may produce less targeted data.
- Failure signatures: Synthetic data that looks realistic but fails to improve model generalization, models that overfit to synthetic patterns, or post-processing that removes too much data leaving insufficient training material.
- First 3 experiments:
  1. Generate synthetic data using expert knowledge techniques on a small imbalanced dataset, apply basic post-processing, and measure class balance improvement.
  2. Use a pre-trained language model to generate synthetic text data for a low-resource NLP task, then fine-tune and compare performance against baseline.
  3. Apply foundation model in-context learning to generate diverse instruction-following data, filter using similarity metrics, and evaluate diversity improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively evaluate the quality of synthetic data generated by foundation models without fine-tuning, especially when compared to fine-tuned models?
- Basis in paper: [explicit] The paper highlights that foundation models applied directly might produce data with biases or inaccuracies that are not immediately apparent, and controlling the nature of the generated data is more challenging compared to fine-tuned models.
- Why unresolved: There is a lack of standardized benchmarks and metrics for evaluating the quality and relevance of synthetic data generated by foundation models, especially in comparison to data from fine-tuned models.
- What evidence would resolve it: Development of comprehensive evaluation frameworks and benchmarks that can assess the quality, diversity, and relevance of synthetic data generated by foundation models, with comparisons to fine-tuned models.

### Open Question 2
- Question: What are the most effective methods for addressing label inconsistency in synthetic data, and how can these methods be integrated into the data augmentation pipeline?
- Basis in paper: [explicit] The paper mentions that discrepancies between generated data and their labels can undermine model performance, and it discusses various post-processing steps to maintain label accuracy.
- Why unresolved: Despite various post-processing techniques, there is no clear consensus on the most effective methods for ensuring label consistency in synthetic data, and integrating these methods into the data augmentation pipeline remains a challenge.
- What evidence would resolve it: Empirical studies comparing different post-processing techniques for label consistency, along with guidelines for integrating these methods into the data augmentation pipeline, would provide clarity.

### Open Question 3
- Question: How can multi-modal data augmentation techniques be developed to handle misaligned or non-straightforward relationships between modalities?
- Basis in paper: [explicit] The paper points out that current multi-modal data augmentation methods assume pre-existing alignment between modalities, overlooking instances where the relationship is not straightforward.
- Why unresolved: There is a gap in research on developing augmentation techniques that consider and enhance non-aligned portions of multi-modal data, which could lead to more robust models.
- What evidence would resolve it: Research demonstrating new multi-modal augmentation techniques that explicitly address misaligned data, along with experiments showing improved model performance on complex tasks, would provide insights.

## Limitations
- The generational categorization may oversimplify the complex evolution of synthetic data techniques, as some modern approaches blend characteristics from multiple generations.
- The survey focuses primarily on textual and vision data, potentially overlooking domain-specific techniques in areas like audio, graph data, or time series.
- The classification system relies heavily on temporal correlation between ML advances and synthetic data methods, which may not capture all relevant techniques or emerging hybrid approaches.

## Confidence

- **High Confidence**: The four core objectives of data augmentation (Improving Diversity, Data Balancing, Addressing Domain Shift, Resolving Edge Cases) are well-established and consistently supported across the ML literature.
- **Medium Confidence**: The generational categorization of synthetic data techniques provides a useful framework but may require refinement as new hybrid approaches emerge that don't fit neatly into defined categories.
- **Medium Confidence**: The emphasis on quality over quantity represents a current trend in the field, though long-term validation of this shift requires more empirical evidence across diverse applications.

## Next Checks

1. **Empirical Validation**: Conduct controlled experiments comparing model performance when trained on synthetic data generated by different generational techniques on identical tasks to verify the claimed performance characteristics.

2. **Framework Extensibility**: Test the four-generation categorization by applying it to synthetic data techniques in emerging domains (e.g., multi-modal, graph-based, or time-series data) to identify gaps or needed extensions.

3. **Quality vs. Quantity Benchmarking**: Design a standardized benchmark that systematically varies synthetic data quality and quantity to empirically validate the diminishing returns of quantity-focused augmentation in mature ML domains.