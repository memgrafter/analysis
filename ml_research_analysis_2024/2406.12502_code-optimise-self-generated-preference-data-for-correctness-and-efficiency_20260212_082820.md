---
ver: rpa2
title: 'Code-Optimise: Self-Generated Preference Data for Correctness and Efficiency'
arxiv_id: '2406.12502'
source_url: https://arxiv.org/abs/2406.12502
tags:
- code
- pass
- solution
- solutions
- mbpp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Code-Optimise introduces a lightweight framework that trains code
  language models using self-generated preference data to optimize both correctness
  and runtime efficiency. The method generates diverse solutions, automatically annotates
  them for functional correctness and runtime, and fine-tunes the model using Direct
  Preference Optimization or supervised fine-tuning with dynamic solution selection
  to reduce overfitting.
---

# Code-Optimise: Self-Generated Preference Data for Correctness and Efficiency

## Quick Facts
- arXiv ID: 2406.12502
- Source URL: https://arxiv.org/abs/2406.12502
- Reference count: 18
- Code-Optimise framework trains code LMs using self-generated preference data to optimize correctness and runtime efficiency

## Executive Summary
Code-Optimise introduces a lightweight framework that trains code language models using self-generated preference data to optimize both correctness and runtime efficiency. The method generates diverse solutions, automatically annotates them for functional correctness and runtime, and fine-tunes the model using Direct Preference Optimization or supervised fine-tuning with dynamic solution selection to reduce overfitting. The framework achieves significant improvements in pass@k, reduces runtimes by up to 6% for in-domain data and 3% for out-of-domain data, and shortens generated code length by up to 48% on MBPP and 23% on HumanEval, resulting in faster and cheaper inference.

## Method Summary
Code-Optimise uses a self-bootstrapping approach where a pretrained code language model generates diverse solutions for programming problems, which are then automatically evaluated for correctness and runtime efficiency. The framework filters problems to retain only those with sufficient contrast between passing and failing solutions, then applies either supervised fine-tuning or Direct Preference Optimization with dynamic solution selection during training. This approach avoids dependency on larger models for learning signals while enabling lightweight training that optimizes both functional correctness and execution efficiency.

## Key Results
- Achieves up to 6% runtime reduction for in-domain data and 3% for out-of-domain data
- Reduces generated code length by up to 48% on MBPP and 23% on HumanEval
- Models optimized via DPO show higher functional correctness compared to baseline and SFT across both datasets

## Why This Works (Mechanism)

### Mechanism 1
Dynamic solution selection during training reduces overfitting by exposing the model to multiple diverse preference pairs per prompt. At the start of each epoch, the model randomly selects a new preference pair (yw, yl) for each problem xi from the self-generated preference data, preventing memorization of static pairs and encouraging generalization across diverse code solutions.

### Mechanism 2
DPO outperforms SFT on self-generated preference data because it can leverage both positive and negative signals (passed vs failed, quick vs slow) rather than only positive examples. DPO uses a pairwise ranking loss that directly optimizes the model to prefer solutions with desired properties (correctness and efficiency) over those without, creating a more nuanced learning signal than SFT's unidirectional optimization.

### Mechanism 3
Self-generated preference data enables lightweight training without requiring large proprietary models for distillation signals. The framework bootstraps the pretrained CLM to generate its own training data by sampling diverse solutions and automatically annotating them for correctness and runtime, creating a closed-loop system that doesn't depend on external data sources.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Why needed - Provides a simple, stable alternative to RL for optimizing models on preference data without requiring a separate reward model or complex training procedures. Quick check - How does DPO's pairwise ranking loss differ from standard cross-entropy loss used in supervised learning?

- **Multinomial sampling with temperature**: Why needed - Temperature-controlled multinomial sampling balances diversity and quality in generated solutions, ensuring the preference data contains both correct and incorrect examples with varying runtimes. Quick check - What effect does lowering the temperature have on the diversity of generated code solutions?

- **Coefficient of variation for runtime stability**: Why needed - Measuring runtime stability using coefficient of variation ensures that efficiency signals are reliable and not distorted by execution noise. Quick check - Why is it important to measure runtime multiple times and only accept measurements with CoV ≤ 0.1?

## Architecture Onboarding

- **Component map**: Problem description → Sampling (100 solutions) → Annotation (correctness + runtime) → Filtering (retain problems with ≥2 passing, ≥1 failing) → Optimization (SFT/DPO with dynamic selection) → Evaluation (pass@k, runtime, code length)

- **Critical path**: The system generates 100 diverse solutions per problem using the base CLM, evaluates each for correctness and runtime stability, filters to retain problems with sufficient contrast between passing and failing solutions, then fine-tunes using SFT or DPO with dynamic solution selection.

- **Design tradeoffs**: The framework trades off data quality for data efficiency by using self-generated preference data rather than curated datasets, reducing cost and dependency on external models but requiring careful quality control through filtering and stability measurements.

- **Failure signatures**: Poor performance indicates either insufficient diversity in sampled solutions (leading to overfitting), unstable runtime measurements (causing noisy efficiency signals), or inadequate filtering criteria (allowing problems without sufficient contrast between good and bad solutions).

- **First 3 experiments**: 
  1. Test sampling diversity by generating solutions for a few problems and manually inspecting the range of correctness and runtime.
  2. Verify runtime stability by running the annotation module multiple times on the same solutions and checking CoV values.
  3. Compare baseline pass@k scores with and without dynamic solution selection to quantify its impact on overfitting.

## Open Questions the Paper Calls Out
- How would the framework perform with larger models (e.g., 70B parameters) compared to the current 13B parameter limit?
- How would the framework perform if extended to other programming languages beyond Python?
- Could the framework be improved by incorporating additional learning signals beyond correctness and runtime efficiency?
- How sensitive is the framework's performance to the choice of sampling temperature and the number of solutions generated per problem?

## Limitations
- Dependence on quality and diversity of self-generated solutions, which may be insufficient if the base CLM lacks diversity
- Runtime measurement assumes stable execution environments that may not reflect real-world computational conditions
- Requires careful parameter tuning, particularly around temperature settings and filtering thresholds, which may need adjustment for different domains

## Confidence
- **High Confidence**: Runtime improvements and code length reduction (well-supported by empirical measurements)
- **Medium Confidence**: DPO outperforming SFT (modest absolute performance differences, sensitive to hyperparameters)
- **Low Confidence**: Dynamic solution selection's effectiveness in reducing overfitting (primarily methodology-based, needs ablation studies)

## Next Checks
1. Implement ablation study comparing static vs dynamic solution selection to quantify impact on overfitting prevention
2. Evaluate runtime measurement stability across different computational platforms to assess robustness against environmental noise
3. Conduct systematic analysis of solution diversity by measuring pairwise code similarity between generated solutions and correlating with performance improvements