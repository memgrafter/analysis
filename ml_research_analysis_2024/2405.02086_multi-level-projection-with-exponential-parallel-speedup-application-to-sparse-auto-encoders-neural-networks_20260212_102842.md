---
ver: rpa2
title: Multi-level projection with exponential parallel speedup; Application to sparse
  auto-encoders neural networks
arxiv_id: '2405.02086'
source_url: https://arxiv.org/abs/2405.02086
tags:
- projection
- bi-level
- complexity
- norm
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new bi-level projection method to address\
  \ the computational complexity issue of the \u21131,\u221E norm projection, which\
  \ is O(nm log(nm)) for a matrix in R^n\xD7m. The proposed bi-level projection method\
  \ reduces the complexity to O(nm) and O(n+m) with full parallel power."
---

# Multi-level projection with exponential parallel speedup; Application to sparse auto-encoders neural networks

## Quick Facts
- arXiv ID: 2405.02086
- Source URL: https://arxiv.org/abs/2405.02086
- Authors: Guillaume Perez; Michel Barlaud
- Reference count: 40
- Key outcome: Introduces bi-level and multi-level projection methods for ℓ1,∞ norm that reduce complexity from O(nm log(nm)) to O(nm) and O(n+m) with full parallel power, achieving 2× speedup in sparse neural networks while improving sparsity and accuracy.

## Executive Summary
This paper addresses the computational complexity of projecting onto the ℓ1,∞ norm ball, which is traditionally O(nm log(nm)) for matrices in R^(n×m). The authors propose a novel bi-level optimization approach that decomposes this projection into simpler sub-projections, reducing the complexity to O(nm). Furthermore, they generalize this to multi-level projections for tensors, achieving exponential parallel speedup with complexity O(n+m). The method is applied to sparse auto-encoders, demonstrating improved sparsity and computational efficiency while maintaining or improving accuracy compared to existing algorithms.

## Method Summary
The bi-level projection method works by first computing the ℓ∞ norm of each column (aggregation step), then projecting the resulting vector onto the ℓ1 ball, and finally projecting each column onto the ℓ∞ ball with the corresponding scalar bound. This decomposition leverages the fact that ℓ1 and ℓ∞ projections are linear-time operations, and each step can be parallelized. The multi-level extension recursively applies this aggregation-projection paradigm across multiple tensor dimensions, replacing product-of-dimensions work with sum-of-dimensions work. The algorithms are implemented in PyTorch C++ with parallel processing using thread pools and applied to sparse neural networks within a supervised autoencoder framework using cross-entropy and Smooth ℓ1 (Huber) loss.

## Key Results
- Bi-level projection reduces ℓ1,∞ projection complexity from O(nm log(nm)) to O(nm)
- Multi-level projection achieves O(n+m) complexity with exponential parallel speedup
- 2× faster than state-of-the-art Euclidean algorithms while providing better sparsity and equal accuracy in neural network applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bi-level formulation splits the ℓ₁,∞ projection into an aggregation step plus a simpler projection, reducing computational complexity from O(nm log(nm)) to O(nm).
- Mechanism: The algorithm first computes the ℓ∞ norm of each column (aggregation), then projects the resulting vector onto the ℓ₁ ball, and finally projects each column onto the ℓ∞ ball with the corresponding scalar bound. Each step is linear or very simple, and the second and third steps are embarrassingly parallel.
- Core assumption: The ℓ₁ ball projection is linear-time, and the ℓ∞ projection of a vector onto a scalar ball is also linear-time.
- Evidence anchors:
  - [abstract]: "the time complexity for the ℓ₁,∞ norm is only O(nm) for a matrix in Rn×m"
  - [section]: "Step 1) complexity is O(nm), step 2) complexity is 0(n), step 3) complexity is 0(n), and step 4 complexity is O(nm)"
  - [corpus]: Weak evidence; no direct citation found in neighbors, but the complexity claim is internally consistent with the described steps.
- Break condition: If ℓ₁ or ℓ∞ projection algorithms become superlinear, or if the aggregation step cannot be computed in O(nm).

### Mechanism 2
- Claim: The tri-level and multi-level extensions further reduce time complexity to O(n+m) with full parallel power.
- Mechanism: By recursively aggregating dimensions (e.g., channels, then rows, then columns) and projecting at each level, the algorithm replaces product-of-dimensions work with sum-of-dimensions work. Each aggregation/projection step operates independently on disjoint index sets, enabling parallel execution.
- Core assumption: Each aggregation step is linear in the size of the input tensor slice, and projections at each level can be parallelized across independent slices.
- Evidence anchors:
  - [abstract]: "O(n+m) with full parallel power" and "exponential parallel speedup"
  - [section]: "the time complexity with a full parallel power is only O(n+m) as steps 1) and 4) can be run in a parallel"
  - [corpus]: No direct support; neighboring papers discuss parallel and sparse methods but not multi-level tensor projection specifically.
- Break condition: If any aggregation step is not linear or if projections cannot be fully parallelized across slices.

### Mechanism 3
- Claim: The proposed method achieves better sparsity and equal accuracy compared to existing ℓ₁,∞ projection algorithms in neural network applications.
- Mechanism: The structured sparsity enforced by ℓ₁,∞ removes entire columns, leading to higher sparsity percentages while maintaining or improving classification accuracy due to regularization effects.
- Core assumption: Removing entire columns does not harm model capacity more than random sparsity, and the projection preserves essential information.
- Evidence anchors:
  - [abstract]: "Experiments show that our projection is 2 times faster than the actual fastest Euclidean algorithms while providing same accuracy and better sparsity in neural networks applications"
  - [section]: "Compared to the baseline the SAE using the ℓ₁,∞ projection improves the accuracy by 7%... The best accuracy is obtained for η = 0.75 for ℓ₁,∞ and for η = 1. for the bilevel ℓ₁,∞ projection, maximum accuracy of both method are similar. However sparsity and computation time are better for bilevel ℓ₁,∞ than for regular ℓ₁,∞."
  - [corpus]: No direct evidence in neighbors; sparsity and accuracy claims are specific to this work's experiments.
- Break condition: If structured sparsity harms model capacity or if the projection loses critical information for reconstruction/classification.

## Foundational Learning

- Concept: ℓₚ,q norm definition and ball projection
  - Why needed here: The entire algorithm is built around computing projections onto ℓₚ,q balls; understanding the norm and ball definitions is essential to follow the decomposition.
  - Quick check question: For a matrix X, how is the ℓ₁,∞ norm computed from its columns?

- Concept: Linear-time projection algorithms for ℓ₁ and ℓ∞
  - Why needed here: The complexity claim depends on knowing that these projections are O(n) and O(n) respectively; without this, the overall O(nm) claim fails.
  - Quick check question: What is the time complexity of projecting a vector onto the ℓ₁ ball?

- Concept: Parallel computation and workload decomposition
  - Why needed here: The speedup claims rely on the algorithm's inherent parallelism; understanding embarrassingly parallel tasks is key to grasping the speedup.
  - Quick check question: Which steps in the bi-level algorithm can be parallelized across columns?

## Architecture Onboarding

- Component map:
  - Aggregation layer: computes norms across specified dimensions (e.g., ℓ∞ of columns).
  - Projection layer: projects aggregated vectors onto ℓ₁ ball (or other norms).
  - Per-column projection layer: projects each column onto ℓ∞ ball with scalar bound.
  - Tensor generalization: recursive application of aggregation and projection across multiple dimensions.

- Critical path:
  1. Aggregation of input tensor using specified norm(s).
  2. Recursive projection of aggregated vector onto ℓ₁ ball (or equivalent).
  3. Parallel projection of each slice (column/channel) using the scalar bounds from step 2.

- Design tradeoffs:
  - Complexity vs. sparsity: ℓ₁,∞ gives structured sparsity but may remove entire columns; ℓ₁,1 preserves all columns but may be less sparse overall.
  - Parallelism vs. memory: Full parallelism reduces time but may require storing intermediate results for all slices simultaneously.
  - Recursion depth vs. overhead: Multi-level projection reduces complexity but adds recursive call overhead.

- Failure signatures:
  - Runtime error if ℓ₁ or ℓ∞ projection implementation is missing or non-linear.
  - Incorrect sparsity if aggregation step is wrong or projection bounds are miscalculated.
  - Memory overflow if tensor slices are too large to hold in memory during parallel processing.

- First 3 experiments:
  1. Verify ℓ₁,∞ projection on a small 3x3 matrix with known result and compare against brute-force projection.
  2. Benchmark the bi-level method against the Chu et al. method on matrices of increasing size, measuring both runtime and sparsity.
  3. Test the tri-level method on a synthetic 3-channel image tensor, ensuring the output satisfies the ℓ₁,∞,∞ constraint and comparing sparsity/accuracy to the bi-level method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bi-level projection method compare to existing projection algorithms in terms of computational complexity for different matrix sizes and sparsity levels?
- Basis in paper: [explicit] The paper claims that the bi-level projection method has a computational complexity of O(nm) for matrices in R^(n×m), compared to O(nm log(nm)) for existing algorithms.
- Why unresolved: While the paper provides theoretical complexity analysis, empirical results comparing the bi-level projection method to existing algorithms for various matrix sizes and sparsity levels are limited.
- What evidence would resolve it: Conducting extensive experiments with matrices of different sizes and sparsity levels, comparing the running times of the bi-level projection method and existing algorithms, would provide empirical evidence to support or refute the claimed computational complexity advantage.

### Open Question 2
- Question: How does the multi-level projection method perform in terms of computational complexity and accuracy compared to the bi-level projection method for tensors of higher orders?
- Basis in paper: [explicit] The paper introduces the multi-level projection method as a generalization of the bi-level projection method for tensors, claiming an exponential parallel speedup. However, the paper only provides theoretical analysis and limited experimental results for the tri-level projection case.
- Why unresolved: The paper lacks comprehensive experimental results comparing the multi-level projection method to the bi-level projection method for tensors of higher orders, making it difficult to assess the practical benefits of the generalization.
- What evidence would resolve it: Conducting experiments with tensors of various orders, comparing the computational complexity and accuracy of the multi-level projection method and the bi-level projection method, would provide insights into the effectiveness of the generalization.

### Open Question 3
- Question: How does the bi-level projection method affect the sparsity and accuracy of neural networks in different application domains?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of the bi-level projection method in improving the sparsity and accuracy of neural networks for synthetic and biological datasets. However, the experiments are limited to specific datasets and network architectures.
- Why unresolved: The paper does not explore the impact of the bi-level projection method on neural networks in different application domains, such as computer vision, natural language processing, or reinforcement learning.
- What evidence would resolve it: Applying the bi-level projection method to neural networks in various application domains and evaluating the resulting sparsity and accuracy would provide insights into the generalizability of the method.

## Limitations

- Limited experimental validation of multi-level projection method beyond theoretical analysis
- Focus on specific datasets and neural network architectures limits generalizability
- Parallel speedup claims rely on theoretical analysis rather than comprehensive empirical measurement across different hardware configurations

## Confidence

- Bi-level projection complexity claims: Medium
- Multi-level extension validation: Low
- Sparsity and accuracy improvements in neural networks: Medium

## Next Checks

1. Verify the linear-time complexity of the ℓ₁ and ℓ∞ projection algorithms used in the bi-level method on matrices of varying sizes.
2. Implement and benchmark the tri-level projection method on synthetic tensors, comparing results against the bi-level method in terms of both accuracy and computational efficiency.
3. Conduct a systematic ablation study on the supervised autoencoder framework, varying hyperparameters and sparsity levels to assess robustness and generalization across multiple datasets.