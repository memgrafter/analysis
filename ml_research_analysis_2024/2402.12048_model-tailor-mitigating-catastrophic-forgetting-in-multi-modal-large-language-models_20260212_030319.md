---
ver: rpa2
title: 'Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language
  Models'
arxiv_id: '2402.12048'
source_url: https://arxiv.org/abs/2402.12048
tags:
- tasks
- parameters
- arxiv
- tailor
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses catastrophic forgetting in multi-modal large\
  \ language models (MLLMs), where fine-tuning for new tasks degrades performance\
  \ on original tasks. The authors propose Model Tailor, a parameter-efficient post-training\
  \ method that preserves pre-trained parameters while selectively replacing a small\
  \ percentage of fine-tuned parameters (\u2264 10%)."
---

# Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models

## Quick Facts
- arXiv ID: 2402.12048
- Source URL: https://arxiv.org/abs/2402.12048
- Authors: Didi Zhu; Zhongyi Sun; Zexi Li; Tao Shen; Ke Yan; Shouhong Ding; Kun Kuang; Chao Wu
- Reference count: 40
- Primary result: Model Tailor maintains ~99% effectiveness on original tasks versus pre-training and achieves ~97% on new tasks compared to standard fine-tuning

## Executive Summary
This paper addresses catastrophic forgetting in multi-modal large language models (MLLMs), where fine-tuning for new tasks degrades performance on original tasks. The authors propose Model Tailor, a parameter-efficient post-training method that preserves pre-trained parameters while selectively replacing a small percentage of fine-tuned parameters (≤ 10%). The method identifies a "model patch" using a fusion of salience and sensitivity analysis, then applies a compensation mechanism to enhance performance on both target and original tasks. Experiments on InstructBLIP and LLaVA-1.5 show that Model Tailor maintains ~99% effectiveness on original tasks versus pre-training and achieves ~97% on new tasks compared to standard fine-tuning, while also demonstrating adaptability to multi-task scenarios.

## Method Summary
Model Tailor is a parameter-efficient post-training method that mitigates catastrophic forgetting in MLLMs. It preserves most pre-trained parameters while selectively updating a small fraction (≤10%) through a three-step process: salience and sensitivity analysis to identify critical parameters, sparse mask creation to select the "model patch," and compensation calculation using inverse Hessian to restore target task performance. The method fuses salience (parameter deviation from pre-trained values) and sensitivity (loss impact via second-order Hessian analysis) scores to prioritize parameters that both changed significantly and matter most for task performance.

## Key Results
- Model Tailor maintains ~99% effectiveness on original tasks versus pre-training
- Achieves ~97% performance on new tasks compared to standard fine-tuning
- Demonstrates adaptability to multi-task scenarios with preserved knowledge across tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model Tailor preserves pre-trained knowledge by maintaining most parameters while selectively updating a small fraction (≤10%).
- Mechanism: The method uses a sparse mask to identify critical parameters ("model patch") and compensates only those, leaving most pre-trained weights unchanged.
- Core assumption: Most parameters in pre-trained MLLMs are task-agnostic and can remain frozen without harming performance.
- Evidence anchors:
  - [abstract]: "Our method primarily preserves the pre-trained parameters while replacing a small number (≤ 10%) of fine-tuned parameters"
  - [section 4.2]: "To mitigate this, we leverage a pruning-inspired strategy...breaking down Model Tailor into more manageable layer-wise sub-tasks"
  - [corpus]: Weak - neighbors focus on LoRA and forgetting mitigation but don't directly validate this specific preservation mechanism
- Break condition: If task-specific parameters are distributed throughout the model rather than concentrated, this mechanism fails.

### Mechanism 2
- Claim: The fusion of salience and sensitivity scores identifies the most impactful parameters for target tasks.
- Mechanism: Salience measures parameter deviation from pre-trained values; sensitivity measures loss impact using second-order Hessian analysis. Their combination prioritizes parameters that both deviated substantially and have high sensitivity.
- Core assumption: Parameters that both deviate substantially and have high sensitivity are the key drivers of task adaptation.
- Evidence anchors:
  - [section 4.3]: "s = ω · ˜s∆ + (1 − ω) · ˜sε" describes the fusion formula
  - [section 4.3]: "This metric underscores the parameters that have shown substantial responsiveness to the task adaptation process"
  - [corpus]: Weak - neighbors mention forgetting mitigation but don't validate this specific dual-score approach
- Break condition: If parameter importance is non-linear or context-dependent, simple score fusion may misidentify critical parameters.

### Mechanism 3
- Claim: The compensation mechanism restores target task performance lost when removing non-essential parameters.
- Mechanism: After identifying the model patch, compensation values are calculated based on the inverse Hessian to adjust the selected parameters, offsetting performance degradation.
- Core assumption: Second-order Hessian information accurately captures how parameter changes affect task loss, enabling precise compensation.
- Evidence anchors:
  - [section 4.4]: "Theorem 4.5 serves as the foundational pillar for determining the patch decorator"
  - [section 4.4]: "By applying ∆Θ∗m to the selected sparse parameters, our method ensures a minimally disruptive integration"
  - [corpus]: Weak - neighbors discuss forgetting but don't validate this specific compensation approach
- Break condition: If the Hessian approximation is poor or the loss landscape is non-convex, compensation may fail to restore performance.

## Foundational Learning

- Concept: Second-order optimization and Hessian matrix
  - Why needed here: The compensation mechanism relies on inverse Hessian calculations to determine optimal parameter adjustments
  - Quick check question: Why does the method use the inverse Hessian rather than just first-order gradients for compensation?

- Concept: Lottery Ticket Hypothesis
  - Why needed here: The method assumes a sparse subset of parameters ("winning tickets") can maintain most task performance
  - Quick check question: How does the sparsity level (≤10%) relate to the lottery ticket hypothesis in this context?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding this phenomenon is crucial for appreciating why parameter preservation matters
  - Quick check question: What distinguishes catastrophic forgetting in MLLMs from traditional catastrophic forgetting in vision models?

## Architecture Onboarding

- Component map: Pre-trained MLLM (InstructBLIP/LLaVA) -> Parameter identification (salience+sensitivity) -> Sparse mask creation -> Compensation calculation (inverse Hessian) -> Parameter adjustment -> Performance evaluation
- Key modules: Mask generator, Compensation calculator, Fusion engine

- Critical path:
  1. Calculate salience scores (parameter deviations)
  2. Calculate sensitivity scores (Hessian-based loss impact)
  3. Fuse scores and create sparse mask
  4. Compute compensation values using inverse Hessian
  5. Apply compensation to selected parameters
  6. Evaluate on both pre-trained and target tasks

- Design tradeoffs:
  - Sparsity vs. performance: Higher sparsity preserves more pre-trained knowledge but may reduce target task performance
  - Salience vs. sensitivity weighting: Different ω values prioritize either target or pre-trained task performance
  - Computational cost: Hessian calculations are expensive but approximated via SparseGPT for scalability

- Failure signatures:
  - Target task performance drops significantly despite high sparsity
  - Pre-trained task performance degrades despite parameter preservation
  - Compensation values become unstable or produce NaNs
  - Mask selection becomes too sparse (near 0%) or too dense (near 100%)

- First 3 experiments:
  1. Single-task fine-tuning baseline: Fine-tune on Flickr30k, measure forgetting on pre-trained tasks
  2. Sparsity sweep: Run Model Tailor with sparsity levels 1%, 5%, 10%, 20% on same task pair
  3. Mask composition analysis: Compare salience-only, sensitivity-only, and fused mask performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Model Tailor's performance scale with different sparsity levels beyond 10% on larger multi-modal models like GPT-4V?
- Basis in paper: [explicit] The paper tests sparsity levels up to 10% and notes a performance plateau beyond this point.
- Why unresolved: The paper only tests up to 10% sparsity, leaving uncertainty about performance at higher sparsity levels on larger models.
- What evidence would resolve it: Experiments testing sparsity levels from 10% to 30% on models like GPT-4V, measuring performance on both original and target tasks.

### Open Question 2
- Question: Does Model Tailor maintain its effectiveness when applied to non-vision modalities like audio or text-only tasks?
- Basis in paper: [inferred] The paper focuses on vision-language models, though catastrophic forgetting is a general ML problem.
- Why unresolved: The paper only evaluates on vision-language tasks, so generalization to other modalities remains unknown.
- What evidence would resolve it: Testing Model Tailor on audio-language models and pure text models with tasks like speech recognition or language translation.

### Open Question 3
- Question: How does Model Tailor's parameter selection strategy compare to other pruning methods like magnitude pruning or movement pruning in terms of final task performance?
- Basis in paper: [explicit] The paper compares to DARE (random parameter selection) and grafting but doesn't compare to other pruning methods.
- Why unresolved: The paper only compares against DARE and grafting, not established pruning methods that could serve as stronger baselines.
- What evidence would resolve it: Head-to-head comparisons of Model Tailor against magnitude pruning and movement pruning on the same datasets with identical sparsity constraints.

### Open Question 4
- Question: What is the computational overhead of calculating the Hessian matrix for very large models, and how does this impact practical deployment?
- Basis in paper: [explicit] The paper mentions using SparseGPT to handle Hessian calculations but doesn't provide detailed complexity analysis.
- Why unresolved: While the paper claims computational feasibility, it lacks specific timing measurements or scaling analysis for production deployment.
- What evidence would resolve it: Detailed benchmarking showing wall-clock time for Hessian calculation on models of increasing size, with comparison to standard fine-tuning time.

## Limitations

- Computational overhead: Hessian-based compensation mechanism may face scalability issues despite SparseGPT approximation claims
- Task generalization scope: Results demonstrated only on specific MLLMs (InstructBLIP and LLaVA-1.5) and task pairs
- Parameter selection validation: Limited ablation studies on why salience+sensitivity fusion outperforms simpler alternatives

## Confidence

- High confidence: The fundamental approach of selective parameter preservation to mitigate catastrophic forgetting is well-grounded in continual learning literature and the experimental results showing maintained original task performance (~99%) are convincing.
- Medium confidence: The compensation mechanism using inverse Hessian is theoretically valid, but the practical effectiveness depends heavily on approximation quality and implementation details not fully specified in the paper.
- Low confidence: Claims about optimal parameter selection through salience+sensitivity fusion lack comprehensive ablation studies, and the specific weighting (ω) appears somewhat arbitrary without sensitivity analysis.

## Next Checks

1. **Ablation study on fusion weighting**: Systematically vary the salience-sensitivity weighting factor ω across the full range [0,1] and measure impact on both original and target task performance to identify optimal settings and validate the fusion approach.

2. **Scalability benchmark**: Implement the compensation mechanism on larger MLLMs (beyond the tested InstructBLIP and LLaVA-1.5) and measure wall-clock time, memory usage, and performance degradation to assess practical deployment feasibility.

3. **Cross-architecture generalization**: Apply Model Tailor to different MLLM architectures (e.g., BLIP-2, LLaVA-2) and diverse task combinations beyond the paper's scope to establish method robustness and identify architectural dependencies.