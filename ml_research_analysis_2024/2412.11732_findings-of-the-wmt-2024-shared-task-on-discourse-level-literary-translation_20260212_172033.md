---
ver: rpa2
title: Findings of the WMT 2024 Shared Task on Discourse-Level Literary Translation
arxiv_id: '2412.11732'
source_url: https://arxiv.org/abs/2412.11732
tags:
- translation
- systems
- wang
- evaluation
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the findings of the WMT 2024 shared task on\
  \ Discourse-Level Literary Translation, which focuses on translating literary texts\
  \ across three language pairs: Chinese\u2192English, Chinese\u2192German, and Chinese\u2192\
  Russian. The task aims to explore the potential of machine translation and large\
  \ language models in overcoming the unique challenges of literary translation, particularly\
  \ at the discourse level."
---

# Findings of the WMT 2024 Shared Task on Discourse-Level Literary Translation

## Quick Facts
- **arXiv ID:** 2412.11732
- **Source URL:** https://arxiv.org/abs/2412.11732
- **Reference count:** 14
- **Key outcome:** Most participating teams' models with literary-domain enhancements outperform baseline systems in document-level BLEU, with significant gaps between human and automatic evaluation results, and constrained track systems achieving performance close to unconstrained systems.

## Executive Summary
This paper presents the findings of the WMT 2024 shared task on Discourse-Level Literary Translation, which focuses on translating Chinese web novels into English, German, and Russian at the document level. Ten submissions from five teams participated, employing both automatic and human evaluations to measure performance. The task aimed to explore the potential of machine translation and large language models in overcoming the unique challenges of literary translation, particularly at the discourse level. Key findings include the superior performance of literary-domain enhanced models, significant discrepancies between human and automatic evaluation results, and the competitive performance of constrained systems. The task highlights the importance of discourse-aware translation in literary texts and provides a platform for evaluating and comparing different methods on a challenging dataset.

## Method Summary
The task focused on translating Chinese web novels into English, German, and Russian using both constrained and unconstrained tracks. Training data included the GuoFeng Webnovel Corpus V2 with sentence-level alignment for Chinese-English and document-level data for other language pairs. Systems employed various approaches including fine-tuning large language models (Llama-2, GPT-4), domain adaptation with literary texts, and chunk-based training. Evaluation used both automatic metrics (BLEU, chrF2, COMET, TER, and document-level BLEU) and human evaluation assessing general quality and discourse awareness on a 0-5 scale.

## Key Results
- Most participating teams' models with literary-domain enhancements outperformed baseline systems in document-level BLEU scores.
- Significant gaps were observed between conclusions drawn from human and automatic evaluations for certain systems.
- The performance of the only constrained track system was close to that of the best unconstrained system in both automatic and human evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enhanced literary-domain knowledge in models improves document-level BLEU (d-BLEU) scores over baseline systems.
- Mechanism: Models fine-tuned or pretrained on literary domain data learn discourse-level patterns, stylistic elements, and contextual coherence that general MT models miss.
- Core assumption: Literary texts contain discourse-level dependencies that are learnable from in-domain data and improve translation quality when modeled explicitly.
- Evidence anchors:
  - [abstract] "most of the participating teams' models (after literary-domain enhancements) outperform Baseline systems (Llama-MT, Google Translate and GPT-4) in terms of d-BLEU"
  - [section] "NLP2CT-UM achieved the highest scores in BLEU (41.6) and COMET (83.56), indicating superior translation quality and contextual alignment with the reference text"
- Break condition: If literary texts lack consistent discourse patterns or if in-domain data is too small to capture domain nuances, improvements in d-BLEU may not materialize.

### Mechanism 2
- Claim: There is a significant gap between conclusions drawn from human and automatic evaluations for certain systems.
- Mechanism: Human evaluation captures discourse-aware quality that automatic metrics cannot fully measure, especially in literary contexts where style and tone matter.
- Core assumption: Automatic metrics focus on surface-level n-gram overlap and fluency, while human evaluators assess deeper discourse coherence and cultural adaptation.
- Evidence anchors:
  - [abstract] "there is a significant gap between the conclusions drawn from human and automatic evaluations for certain systems"
  - [section] "Human evaluation based on a scale from 0-5 encompasses two dimensions: general quality and discourse awareness"
- Break condition: If evaluation guidelines or annotator training are inconsistent, or if human evaluation does not adequately capture discourse-level phenomena, the gap may reflect methodological differences rather than model quality.

### Mechanism 3
- Claim: Constrained track systems can achieve performance close to unconstrained systems in both automatic and human evaluation.
- Mechanism: Constrained systems, limited to permitted data and models, can still learn sufficient discourse-level patterns from high-quality in-domain data, narrowing the gap with unconstrained approaches.
- Core assumption: The provided in-domain data and models are rich enough to capture essential discourse features for literary translation without external augmentation.
- Evidence anchors:
  - [abstract] "the performance of the only Constrained Track system was close to that of the best system in the Unconstrained Track in terms of both automatic and human evaluation"
  - [section] "the in-domain pretrained models as supplementary resources (continuously trained on Chinese/English literary texts)"
- Break condition: If the constrained data lacks coverage of diverse literary genres or discourse phenomena, or if unconstrained systems leverage significantly richer resources, performance gaps may widen.

## Foundational Learning

- Concept: Document-level translation and discourse coherence
  - Why needed here: Literary texts require maintaining consistency in tone, terminology, and narrative flow across sentences and chapters, which sentence-level models often fail to capture.
  - Quick check question: How does document-level BLEU (d-BLEU) differ from standard sentence-level BLEU in evaluating translation quality?

- Concept: Large language models (LLMs) for literary translation
  - Why needed here: LLMs can leverage broader context and literary knowledge to handle idiomatic expressions, style, and coherence better than traditional NMT models.
  - Quick check question: What advantages do LLMs offer over standard NMT models when translating discourse-level literary texts?

- Concept: Human evaluation criteria for translation quality
  - Why needed here: Automatic metrics alone are insufficient for literary translation; human evaluation captures fluency, adequacy, consistency, and discourse awareness.
  - Quick check question: What are the two main dimensions of human evaluation used in this task, and why are both necessary?

## Architecture Onboarding

- Component map: Translation system → Evaluation pipeline (automatic + human) → Ranking mechanism
  - Translation systems: Fine-tuned LLMs, constrained/unconstrained tracks, literary-domain enhancements
  - Evaluation: Sentence-level metrics (BLEU, chrF, COMET, TER), document-level BLEU, human evaluation (general quality, discourse awareness)
  - Ranking: Based on overall human judgments

- Critical path: Model training → Translation generation → Automatic evaluation → Human evaluation → Final ranking
  - Ensure document-level coherence during training
  - Use both automatic and human metrics for comprehensive assessment

- Design tradeoffs: Constrained vs. unconstrained track resources, automatic vs. human evaluation, sentence-level vs. document-level metrics
  - Constrained track limits data/model choice but ensures reproducibility
  - Human evaluation is more accurate for discourse but slower and costlier

- Failure signatures: Low d-BLEU despite high sentence-level BLEU (indicates poor discourse coherence), large gap between human and automatic scores (suggests metrics miss discourse nuances), constrained systems underperforming (may indicate insufficient in-domain data)

- First 3 experiments:
  1. Train a baseline NMT model on the provided literary corpus and evaluate sentence-level vs. document-level BLEU to confirm discourse coherence issues.
  2. Fine-tune an LLM on the same corpus with literary-domain knowledge and compare d-BLEU and human evaluation scores to the baseline.
  3. Test constrained vs. unconstrained track performance by limiting resources to permitted data/models and measuring impact on translation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do document-level BLEU scores correlate with human judgments of discourse-aware translation quality in literary texts?
- Basis in paper: [explicit] The paper notes a significant gap between conclusions drawn from human and automatic evaluations for certain systems, suggesting potential misalignment between d-BLEU scores and human perceptions of discourse quality.
- Why unresolved: While d-BLEU measures document-level coherence, it may not fully capture nuanced aspects of literary translation such as style, tone, and cultural adaptation that human evaluators consider.
- What evidence would resolve it: A detailed correlation analysis between d-BLEU scores and human evaluation results across multiple dimensions for the same systems and test sets.

### Open Question 2
- Question: What specific discourse-level phenomena are most challenging for current machine translation systems in literary contexts?
- Basis in paper: [explicit] The task focuses on discourse-level translation challenges, and human evaluation includes discourse-aware criteria like consistency, word choice, and anaphora.
- Why unresolved: The paper doesn't provide a detailed breakdown of which specific discourse phenomena most differentiate top-performing systems from lower-performing ones.
- What evidence would resolve it: A systematic analysis identifying which types of discourse errors are most prevalent in lower-scoring translations versus higher-scoring ones.

### Open Question 3
- Question: How does the performance of constrained versus unconstrained systems differ when controlling for model size and domain adaptation?
- Basis in paper: [explicit] The paper mentions that the performance of the only constrained track system was close to that of the best unconstrained system, but doesn't provide detailed comparisons controlling for model size or domain adaptation techniques.
- Why unresolved: Without controlling for these variables, it's unclear whether the constrained system's performance reflects the limitations of the constraint itself or other factors like model architecture or training data.
- What evidence would resolve it: Comparative experiments where constrained and unconstrained systems use models of similar size and undergo comparable domain adaptation, isolating the effect of data constraints on translation quality.

## Limitations

- The substantial gap between human and automatic evaluation scores suggests that existing metrics may inadequately capture discourse-level quality for literary texts.
- The constrained track's strong performance raises questions about whether the provided in-domain data sufficiently covers all necessary discourse phenomena for high-quality literary translation.
- Limited information about human evaluation methodology (number of evaluators, inter-annotator agreement, specific guidelines) restricts reproducibility and comparability of results.

## Confidence

- **High Confidence:** The general finding that literary-domain enhanced models outperform baseline systems in document-level BLEU scores is well-supported by the evidence.
- **Medium Confidence:** The claim about significant gaps between human and automatic evaluations is supported, but the specific causes and implications require further investigation.
- **Medium Confidence:** The constrained track's competitive performance is documented, but the long-term sustainability and generalizability of this result need validation with broader testing.

## Next Checks

1. **Metric Validation Study:** Conduct a systematic comparison of automatic metrics against human evaluation specifically for discourse-level phenomena in literary translation, identifying which metrics best correlate with human judgments of discourse quality.

2. **Cross-Genre Evaluation:** Test the best-performing systems on literary texts from different genres (e.g., poetry, drama, prose) to assess whether discourse-aware improvements generalize beyond web novels.

3. **Long-Document Coherence Analysis:** Evaluate systems on extended literary works (novels rather than chapters) to determine if discourse-level improvements scale to longer narrative structures and maintain consistency across entire works.