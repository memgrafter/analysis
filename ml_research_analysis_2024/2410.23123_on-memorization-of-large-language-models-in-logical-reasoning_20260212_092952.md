---
ver: rpa2
title: On Memorization of Large Language Models in Logical Reasoning
arxiv_id: '2410.23123'
source_url: https://arxiv.org/abs/2410.23123
tags:
- reasoning
- training
- knight
- memorization
- direct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) solve
  logical reasoning tasks by memorization or genuine reasoning. To quantify memorization,
  the authors define a Local Inconsistency-based Memorization Score (LiMem) that measures
  the gap between accuracy on training puzzles and consistency under local perturbations
  that preserve the underlying reasoning principles.
---

# On Memorization of Large Language Models in Logical Reasoning

## Quick Facts
- arXiv ID: 2410.23123
- Source URL: https://arxiv.org/abs/2410.23123
- Reference count: 40
- This paper investigates whether LLMs solve logical reasoning tasks by memorization or genuine reasoning using a new Knights and Knaves benchmark and local perturbation analysis.

## Executive Summary
This paper investigates the fundamental question of whether large language models (LLMs) solve logical reasoning tasks through memorization or genuine reasoning capabilities. The authors introduce a new Knights and Knaves (K&K) benchmark with automatic generation capabilities and develop a Local Inconsistency-based Memorization Score (LiMem) to quantify memorization. Through experiments with 17 leading models, they demonstrate that LLMs exhibit varying levels of memorization, particularly under math-level perturbations. The study reveals that while fine-tuning leads to heavy memorization on training data, it also consistently improves generalization performance across different difficulty levels. Notably, the authors show that LLMs can develop reasoning capabilities even when trained on incorrect answers, suggesting that models can extract reasoning principles from partially correct information.

## Method Summary
The authors introduce the Knights and Knaves (K&K) benchmark with an abstract module containing a Generator, Solver, Reasoner, and Perturber to create logical puzzles and their perturbed variants. They define the Local Inconsistency-based Memorization Score (LiMem) to measure the gap between training accuracy and consistency under local perturbations that preserve underlying reasoning principles. The perturbation framework creates variants that maintain the same difficulty and principle while altering surface features. Experiments involve fine-tuning models on K&K puzzles with both correct and incorrect answers, measuring memorization through LiMem scores and generalization through performance on unseen puzzles. The study also includes analysis of model internals and probing techniques to understand the reasoning-memorization tradeoff.

## Key Results
- Fine-tuning on K&K puzzles leads to heavy memorization on training data but also consistently improves generalization performance across difficulty levels
- Models can develop reasoning capabilities from question-answer pairs without explicit reasoning steps, and fine-tuning with detailed reasoning steps further enhances generalization
- High memorization scores under math-level perturbations reveal that models struggle with perturbed versions requiring the same underlying principles
- Even with 50% incorrect training answers, models progressively develop reasoning skills, achieving non-trivial improvements on test sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memorization can coexist with reasoning development in LLMs.
- Mechanism: Models achieve high training accuracy through interpolation (memorizing puzzles) but continue to improve test accuracy over epochs, suggesting underlying reasoning skills develop alongside memorization.
- Core assumption: The model can encode abstract rules from training examples that generalize to unseen puzzles, even if it initially relies on memorization.
- Evidence anchors:
  - [abstract] "we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance."
  - [section] "Fine-tuned model generalizes across different difficulty levels...accuracy gains are larger for N ≤ 6 puzzles, though improvements on harder tasks remain possible."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.388" - moderate similarity suggests related work exists but is not highly focused on this exact phenomenon.
- Break condition: If test accuracy plateaus or decreases despite increased memorization, indicating reasoning skills do not develop.

### Mechanism 2
- Claim: Local perturbations reveal the distinction between memorized and reasoned solutions.
- Mechanism: A high Local Inconsistency-based Memorization Score (LiMem) indicates that the model solved the original puzzle by memorization because it struggles with slightly perturbed versions that require the same underlying reasoning principles.
- Core assumption: Perturbations maintain the same difficulty level and underlying principle but alter the surface form, so consistent performance indicates reasoning while inconsistent performance indicates memorization.
- Evidence anchors:
  - [abstract] "we define a Local Inconsistency-based Memorization Score (LiMem) that measures the gap between accuracy on training puzzles and consistency under local perturbations that preserve the underlying reasoning principles."
  - [section] "High level of memorization occurs when the model shows high accuracy in solving some problems but fails to consistently solve those problems under local perturbations requiring similar mathematical principles."
  - [corpus] Weak - no direct evidence of perturbation methodology in related works.
- Break condition: If perturbations significantly change difficulty or principle, LiMem would not accurately reflect memorization vs reasoning.

### Mechanism 3
- Claim: Fine-tuning on incorrect answers can still improve reasoning capabilities.
- Mechanism: Even when training examples are mislabeled, the model can learn from partially correct role assignments in the answers, gradually developing reasoning skills that transfer to unseen puzzles.
- Core assumption: Each incorrect answer still contains N-1 correct role assignments, providing enough signal for the model to learn the underlying logic.
- Evidence anchors:
  - [section] "Fine-tuning with incorrect answers...Fig. 10 shows that Direct FT with incorrect answers still leads to non-trivial improvements for Llama3-8B...suggesting that the model progressively developed reasoning skills during fine-tuning."
  - [corpus] Weak - no direct evidence of training with incorrect answers in related works.
- Break condition: If the model fails to improve with corrupted training data, indicating it cannot extract reasoning principles from incorrect examples.

## Foundational Learning

- Concept: Boolean satisfiability (SAT) problems
  - Why needed here: Knights and Knaves puzzles reduce to SAT problems, so understanding SAT is crucial for grasping the underlying principles.
  - Quick check question: How does the logical formula (B1 ⇔ S1) ∧ (B2 ⇔ S2) ∧ ... ∧ (BN ⇔ SN) represent a K&K puzzle as a SAT problem?

- Concept: Chain-of-thought (CoT) reasoning
  - Why needed here: The paper explores fine-tuning with and without CoT steps, showing how explicit reasoning steps can guide learning.
  - Quick check question: What is the difference between Direct FT and CoT FT in terms of training data format and loss computation?

- Concept: Local consistency ratio
  - Why needed here: The consistency ratio CR measures whether the model can solve perturbed puzzles consistently, which is key to the memorization score.
  - Quick check question: How is CR(f; D) calculated and what does a low CR value indicate about the model's problem-solving approach?

## Architecture Onboarding

- Component map: Generator -> Solver -> Reasoner -> Perturber (Abstract Module); NL-Generator -> NL-Reasoner -> NL-Perturber (Natural Language Module)
- Critical path: Generator → Solver → Reasoner → Perturber for creating perturbed puzzles with solutions; NL modules convert to natural language for model input
- Design tradeoffs: Perturbation must be local (minimal change) yet maintain same principle/difficulty - too much change breaks the measurement, too little doesn't test generalization
- Failure signatures: High LiMem but no test accuracy improvement indicates memorization without reasoning development; low LiMem despite high training accuracy suggests the model reasons rather than memorizes
- First 3 experiments:
  1. Generate 3-person K&K puzzles with perturbations and verify unique solutions differ between original and perturbed versions.
  2. Fine-tune Llama3-8B on 3-person puzzles with correct answers and measure LiMem on training vs test sets.
  3. Repeat experiment 2 with 50% incorrect answers to test whether reasoning still develops from partially corrupted data.

## Open Questions the Paper Calls Out
None

## Limitations

- Perturbation Design Validity: The central metric (LiMem) depends on perturbations preserving difficulty and reasoning principles, but the paper provides limited empirical validation of this invariance across different perturbation types.
- Generalization Across Reasoning Domains: Results are demonstrated specifically on Knights and Knaves puzzles, and the extent to which findings generalize to other reasoning tasks remains uncertain.
- Role of Model Size and Architecture: Experiments primarily use Llama3-8B and test on a subset of 17 models, with the relationship between model scale and memorization-reasoning tradeoff not thoroughly explored.

## Confidence

- High Confidence: Memorization can coexist with reasoning development in LLMs. The experimental evidence showing consistent generalization improvements across difficulty levels is robust.
- Medium Confidence: Local perturbations effectively distinguish memorization from reasoning. While the theoretical framework is sound, practical implementation challenges may introduce measurement noise.
- Medium Confidence: LLMs can develop reasoning capabilities from question-answer pairs without explicit reasoning steps. The evidence shows this is possible but doesn't fully explain the mechanism.

## Next Checks

1. **Perturbation Robustness Analysis**: Systematically vary perturbation intensity and measure its effect on LiMem scores across different puzzle difficulties to validate whether the perturbation mechanism truly preserves underlying principles while maintaining difficulty.

2. **Cross-Domain Generalization Study**: Apply the same memorization-reasoning framework to at least two other reasoning domains (e.g., arithmetic word problems and commonsense reasoning tasks) to test the generality of the findings beyond logical puzzles.

3. **Architecture Scaling Investigation**: Repeat key experiments across a broader range of model sizes (including much smaller and much larger models) to determine how the memorization-reasoning tradeoff scales with model capacity and whether there are critical thresholds.