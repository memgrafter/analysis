---
ver: rpa2
title: Towards Cross-modal Backward-compatible Representation Learning for Vision-Language
  Models
arxiv_id: '2405.14715'
source_url: https://arxiv.org/abs/2405.14715
tags:
- retrieval
- text
- training
- cross-modal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Cross-modal Backward-compatible Training
  (XBT) to address the incompatibility problem between old and new Vision-Language
  Pretraining (VLP) models in cross-modal retrieval systems. The key innovation is
  a text-only pretrained projection module that aligns new model embeddings with old
  model embeddings using only text data, avoiding the need for expensive image-text
  pairs or backfilling.
---

# Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models

## Quick Facts
- arXiv ID: 2405.14715
- Source URL: https://arxiv.org/abs/2405.14715
- Reference count: 40
- This paper introduces Cross-modal Backward-compatible Training (XBT) to address incompatibility between old and new Vision-Language Pretraining (VLP) models in cross-modal retrieval systems.

## Executive Summary
This paper addresses the critical challenge of maintaining backward compatibility when upgrading Vision-Language Pretraining (VLP) models in cross-modal retrieval systems. The authors introduce Cross-modal Backward-compatible Training (XBT), a method that enables new VLP models to retrieve old gallery embeddings without requiring expensive image-text pairs or backfilling. The key innovation is a text-only pretrained projection module that maps new model embeddings to old model embeddings, combined with parameter-efficient fine-tuning strategies (LoRA and soft prompts) that preserve the new model's knowledge while achieving compatibility.

## Method Summary
XBT works by first pretraining a projection module ϕ using only text data to learn the mapping between new and old VLP model embeddings. This module is then used during XBT training where the new model learns to produce embeddings that, when projected through ϕ, can effectively retrieve old gallery embeddings. The method employs parameter-efficient fine-tuning through LoRA adapters and soft prompts, avoiding full fine-tuning of the original models. The entire process requires only one epoch of training and can be easily reversed by removing the additional parameters, making it an on-off solution for backward compatibility.

## Key Results
- XBT achieves better or comparable cross-modal retrieval performance to old models while maintaining the power of new VLP models
- Text-only pretraining of the projection module significantly reduces the need for image-text pairs compared to traditional approaches
- The method successfully enables backfill-free upgrades when new VLP models emerge
- Performance improvements are demonstrated across multiple datasets including nocaps, Flickr, and COCO

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-only pretraining of the projection module ϕ enables effective mapping from new VLP embeddings to old VLP embeddings without requiring image-text pairs.
- Mechanism: The projection module ϕ is trained solely on text embeddings using contrastive learning to align new text embeddings with old text embeddings. This learned mapping is then applied to both text and image embeddings from the new model.
- Core assumption: The intra-modal distribution of text embeddings in VLP models mirrors that of image embeddings, allowing a projection learned on text to generalize to images.
- Evidence anchors:
  - [abstract] "This module, pretrained solely with text data, significantly reduces the number of image-text pairs required for XBT learning"
  - [section 3.2] "We hypothesize that the distribution of text embeddings in VLP models, which is determined by their semantic similarity, is similarly mirrored in the distribution of their corresponding matched images"
  - [corpus] Weak evidence - only 2 related papers found, neither directly addressing this specific mechanism
- Break condition: If the semantic structure of image and text embeddings in VLP models diverges significantly, the projection learned on text will fail to generalize to images.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning (LoRA and soft prompts) preserves the new model's knowledge while enabling compatibility with the old model.
- Mechanism: Instead of full fine-tuning, small trainable parameters are added to the new model. LoRA modifies low-rank updates to the model weights, while soft prompts add learnable tokens to the input. This allows compatibility learning without altering the original model parameters.
- Core assumption: Adding small trainable parameters can learn the compatibility mapping without degrading the original model's performance.
- Evidence anchors:
  - [abstract] "we utilize parameter-efficient training strategies that improve efficiency and preserve the off-the-shelf new model's knowledge by avoiding any modifications"
  - [section 3.3] "These two factors, in conjunction with ϕ, offer an on-off solution: we can retrieve old samples using additional parameters, and we can easily revert to the original model by removing these parameters for new-to-new retrieval"
  - [corpus] Moderate evidence - related work on LoRA and prompt tuning exists but specific application to backward compatibility is novel
- Break condition: If the compatibility gap is too large, parameter-efficient methods may be insufficient and require full fine-tuning.

### Mechanism 3
- Claim: Cross-modal backward compatibility is achieved by ensuring the new model's embeddings can retrieve old gallery embeddings at least as well as the old model could.
- Mechanism: The XBT loss function trains the new model to minimize the contrastive loss between synthetic old embeddings (produced by ϕ) and the actual old embeddings, ensuring that queries from the new model perform better or equal to queries from the old model when searching the old gallery.
- Core assumption: Improving retrieval performance on the old gallery is sufficient evidence of backward compatibility.
- Evidence anchors:
  - [section 3.1] "The constraints in Eqn. 1 formally express the requirement that the new embedding must perform at least as well as the old embedding in terms of correctly matching image-text pairs"
  - [section 3.3] "Ultimately, ¯vnew and ¯wnew become cross-modal backward-compatible with existing old gallery embeddings, vold and wold, as all embeddings are distributed in the compatible space through ϕ"
  - [corpus] Moderate evidence - backward compatibility is established concept but cross-modal extension is novel
- Break condition: If the old model has unique retrieval capabilities not captured by the contrastive loss, the new model may fail to maintain full backward compatibility.

## Foundational Learning

- Concept: Contrastive learning in embedding spaces
  - Why needed here: The entire XBT approach relies on learning similarities and differences between embeddings using contrastive objectives
  - Quick check question: What is the role of temperature parameter τ in contrastive loss functions?

- Concept: Parameter-efficient fine-tuning methods (LoRA, prompt tuning)
  - Why needed here: These methods enable compatibility learning without modifying the original model parameters, preserving the new model's knowledge
  - Quick check question: How does LoRA achieve parameter efficiency compared to full fine-tuning?

- Concept: Vision-Language Pretraining (VLP) model architecture
  - Why needed here: Understanding how CLIP and similar models create aligned image and text embeddings is crucial for understanding why the text-only projection works
  - Quick check question: Why do VLP models produce aligned image and text embeddings in the same space?

## Architecture Onboarding

- Component map:
  - New VLP encoders (EI_new, ET_new) -> Projection module ϕ -> LoRA adapters -> Soft prompts -> Layer normalization fine-tuning

- Critical path:
  1. Text-only pretraining of ϕ using large text corpus
  2. XBT training with image-text pairs using ϕ, LoRA, and soft prompts
  3. Evaluation on cross-modal retrieval benchmarks

- Design tradeoffs:
  - Text-only pretraining vs. image-text pretraining: Text-only is more efficient but assumes distribution mirroring
  - Parameter-efficient vs. full fine-tuning: Efficient but may be insufficient for large compatibility gaps
  - Single epoch training vs. multi-epoch: Faster but may underfit complex compatibility mappings

- Failure signatures:
  - Poor cross-modal retrieval performance indicates projection module failure
  - Degradation in new-to-new retrieval suggests over-adaptation to old embeddings
  - Memory errors during training suggest incorrect LoRA configuration

- First 3 experiments:
  1. Verify text-only pretraining produces reasonable projections by checking text embedding alignment
  2. Test XBT on a small dataset (like nocaps) to validate the full pipeline
  3. Compare parameter-efficient XBT against full fine-tuning baseline to measure efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scale of the text corpus DT affect the quality of the projection module ϕ and subsequent XBT performance?
- Basis in paper: [explicit] The paper mentions that "By expanding the scale of DT, we can effectively replicate the complete text embedding spaces of both new and old VLP models" and that larger DT improves performance in ablation studies.
- Why unresolved: While the paper shows that larger DT improves performance, the relationship between DT scale and performance gains is not quantified. The optimal scale for different VLP model sizes and domains remains unknown.
- What evidence would resolve it: Systematic experiments varying DT size across multiple orders of magnitude (e.g., 1M, 10M, 100M, 1B text samples) while measuring both ϕ quality and downstream XBT performance would establish the scaling relationship.

### Open Question 2
- Question: Can XBT be extended to handle scenarios where the old and new VLP models have different embedding dimensionalities?
- Basis in paper: [inferred] The paper mentions that "the dimensionality of new and old VLP embeddings may differ" and that ϕ is used to ensure compatibility, suggesting this is a challenge that requires solution.
- Why unresolved: The paper only demonstrates XBT on models with matching dimensionalities (B32→L14, L14→H14). The method's effectiveness when embeddings have different dimensions is not explored.
- What evidence would resolve it: Experiments applying XBT to pairs of models with different embedding dimensions (e.g., CLIP-B32→CLIP-L14) would demonstrate whether the current approach works or if modifications are needed.

### Open Question 3
- Question: What is the relationship between the number of supervised image-text pairs used during XBT training and the zero-shot classification performance of the resulting model?
- Basis in paper: [explicit] The paper shows in Table 7 that increasing the number of supervised pairs (4M→8M→16M) improves zero-shot classification accuracy on ImageNet variants.
- Why unresolved: The paper only tests three specific dataset sizes. The full relationship between training data volume and classification performance, including whether performance plateaus, remains unexplored.
- What evidence would resolve it: A comprehensive study varying the supervised dataset size across multiple scales while measuring zero-shot classification accuracy would reveal the learning curve and potential saturation points.

## Limitations
- The text-only pretraining approach relies on a strong assumption that text and image embedding distributions mirror each other in VLP models
- The method's effectiveness on VLP architectures beyond CLIP (e.g., BLIP, FLAVA) is not established
- The paper does not systematically compare parameter-efficient fine-tuning against full fine-tuning across different compatibility gaps

## Confidence
- **High Confidence**: The core claim that XBT enables backward compatibility without image-text pairs is well-supported by experimental results showing consistent performance improvements across multiple datasets
- **Medium Confidence**: The efficiency claims regarding text-only pretraining reducing data requirements are supported by comparison to full image-text pretraining
- **Medium Confidence**: The parameter-efficient fine-tuning approach is validated through results, but the claim that it preserves the new model's knowledge could be strengthened with more comprehensive ablations

## Next Checks
1. **Distribution Similarity Analysis**: Quantitatively measure the similarity between text and image embedding distributions in VLP models using statistical tests (e.g., Wasserstein distance, maximum mean discrepancy) to validate the core assumption underlying text-only pretraining.

2. **Generalization Study**: Test XBT on a broader range of VLP architectures beyond CLIP, including more recent models like BLIP, FLAVA, or Florence, to establish the method's generalizability across different vision-language pretraining approaches.

3. **Ablation on Parameter Efficiency**: Compare XBT with parameter-efficient fine-tuning against full fine-tuning baselines across different compatibility gaps to determine when parameter-efficient methods are sufficient versus when full fine-tuning becomes necessary.