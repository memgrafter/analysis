---
ver: rpa2
title: Semantic-Guided RL for Interpretable Feature Engineering
arxiv_id: '2410.02519'
source_url: https://arxiv.org/abs/2410.02519
tags:
- features
- feature
- interpretability
- smart
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SMART, a hybrid approach that leverages semantic
  technologies to guide interpretable feature engineering using reinforcement learning.
  The method combines Description Logics reasoning on Knowledge Graphs for feature
  exploitation and Deep Q-Network for guided exploration.
---

# Semantic-Guided RL for Interpretable Feature Engineering

## Quick Facts
- arXiv ID: 2410.02519
- Source URL: https://arxiv.org/abs/2410.02519
- Authors: Mohamed Bouadi; Arta Alavi; Salima Benbernou; Mourad Ouziri
- Reference count: 23
- Primary result: SMART outperforms baselines by 7.24-11.55% while ensuring high feature interpretability

## Executive Summary
This paper introduces SMART, a hybrid approach that leverages semantic technologies to guide interpretable feature engineering using reinforcement learning. The method combines Description Logics reasoning on Knowledge Graphs for feature exploitation and Deep Q-Network for guided exploration. SMART generates interpretable features by inferring domain-specific knowledge and using a novel interpretability metric based on graph decomposition. Experiments on 10 public datasets demonstrate that SMART outperforms baselines like mCAFE, DIFER, and NFS by 7.24%, 11.55%, and 4.86% respectively, while ensuring high feature interpretability.

## Method Summary
SMART is a semantic-guided reinforcement learning framework for interpretable feature engineering that operates in two phases: exploitation and exploration. The exploitation phase uses Description Logics (DL) reasoning on a Knowledge Graph (KG) to infer domain-specific features through logical relationships, while the exploration phase employs Deep Q-Network (DQN) to navigate the feature space guided by a reward function combining model performance and interpretability. The approach introduces a novel interpretability metric based on graph decomposition that quantifies semantic distance and transformation complexity. SMART achieves an average 20.94% improvement over raw data and maintains competitive performance across various domains while ensuring generated features remain interpretable through KG-based semantic reasoning.

## Key Results
- SMART outperforms mCAFE, DIFER, and NFS baselines by 7.24%, 11.55%, and 4.86% respectively
- Achieves average 20.94% improvement over raw data across all tested datasets
- Maintains high feature interpretability scores through KG-based decomposition graph metric
- Shows robust performance across 10 diverse public datasets including classification and regression tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SMART uses semantic technologies (Knowledge Graphs + Description Logics) to guide the generation of interpretable features through logical reasoning.
- Mechanism: The system exploits a Knowledge Graph (KG) to infer domain-specific features using Description Logics (DL) reasoning, then uses Deep Reinforcement Learning (DRL) to explore the search space for additional interpretable features.
- Core assumption: The KG contains sufficient domain knowledge and semantic relationships to infer meaningful, interpretable features.
- Evidence anchors:
  - [abstract] "The former uses Description Logics (DL) to reason on the semantics embedded in Knowledge Graphs (KG) to infer domain-specific features"
  - [section] "SMART exploits the KG to generate new domain-specific features through symbolic reasoning. More specifically, we use HermiT, a reasoner for the DL syntax, to infer new knowledge based on the logical relationships of the KG"
- Break condition: If the KG lacks domain-specific knowledge or semantic relationships, the reasoning mechanism cannot generate interpretable features.

### Mechanism 2
- Claim: SMART employs a novel interpretability metric based on graph decomposition to quantify feature interpretability.
- Mechanism: The system constructs a Decomposition Graph where each node represents a feature and edges represent transformations. Interpretability is calculated as the maximum path score from generated features to known concepts in the KG.
- Core assumption: The interpretability of a feature can be measured by its semantic distance and transformation complexity within the Decomposition Graph.
- Evidence anchors:
  - [abstract] "using a novel interpretability metric based on graph decomposition"
  - [section] "We propose a new metric to assess the interpretability of a feature by calculating the distance between this feature and the concepts of a graph called Decomposition Graph (DecomG)"
- Break condition: If the Decomposition Graph structure doesn't capture meaningful semantic relationships, the interpretability metric becomes unreliable.

### Mechanism 3
- Claim: SMART uses Deep Q-Network (DQN) to explore the feature space while maximizing both model performance and interpretability.
- Mechanism: The DQN agent receives the current dataset state, selects transformations based on Q-values, and receives rewards combining model performance and feature interpretability. This creates a guided exploration that balances both objectives.
- Core assumption: The reward function effectively balances model accuracy and interpretability, guiding the agent toward optimal solutions.
- Evidence anchors:
  - [abstract] "while the latter exploits the KG to conduct a guided exploration of the search space through Deep Reinforcement Learning (DRL)"
  - [section] "Our reward calculation process consists of a linear combination of two metrics. The first one is an evaluation metric P, (e.g. RMSE, AUC), related to a pre-selected ML model, L... The second metric... is used to assess features interpretability"
- Break condition: If the reward function weights are poorly tuned, the agent may prioritize one objective at the expense of the other.

## Foundational Learning

- Concept: Knowledge Graphs and Description Logics
  - Why needed here: SMART relies on reasoning over a KG using DL to infer domain-specific features, which requires understanding semantic relationships and logical inference
  - Quick check question: What is the difference between TBox and ABox in Description Logics, and how does each relate to feature engineering?

- Concept: Deep Reinforcement Learning (DQN)
  - Why needed here: The exploration phase uses DQN to navigate the feature space, requiring understanding of state-action-reward mechanisms and Q-learning
  - Quick check question: How does the ϵ-greedy exploration strategy balance exploration vs exploitation in the context of feature engineering?

- Concept: Feature Interpretability Metrics
  - Why needed here: SMART introduces a novel interpretability metric based on graph decomposition, requiring understanding of how to quantify semantic distance and transformation complexity
  - Quick check question: How would you calculate the interpretability score for a feature that's the result of multiple transformations applied sequentially?

## Architecture Onboarding

- Component map:
  Knowledge Graph (KG) -> Description Logics reasoner (HermiT) -> Semantic vectorization layer -> Deep Q-Network (DQN) -> Decomposition Graph -> Feature transformation engine

- Critical path:
  1. Load and preprocess dataset
  2. Semantic mapping to identify relevant KG entities
  3. DL reasoning to generate initial interpretable features
  4. Semantic vectorization of transformed dataset
  5. DQN agent selects transformations based on Q-values
  6. Apply transformations and evaluate reward (performance + interpretability)
  7. Update DQN and repeat until convergence

- Design tradeoffs:
  - KG complexity vs reasoning efficiency: More complex KGs enable richer features but slower reasoning
  - Reward function weights: Balancing λ between performance and interpretability affects feature quality
  - DQN architecture: Network depth affects learning capability but increases training time

- Failure signatures:
  - Poor performance improvement: May indicate inadequate KG domain knowledge or suboptimal reward function
  - Low interpretability scores: Could suggest the Decomposition Graph isn't capturing semantic relationships well
  - Slow convergence: Might indicate exploration-exploitation balance issues or insufficient state representation

- First 3 experiments:
  1. Test DL reasoning on a simple KG with known relationships to verify feature generation
  2. Validate semantic vectorization by checking if semantically similar features have similar vector representations
  3. Evaluate DQN exploration by running on a small synthetic dataset and visualizing transformation selection patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the interpretability metric scale when applied to large-scale Knowledge Graphs with millions of entities and relationships?
- Basis in paper: [explicit] The paper mentions using graph decomposition and path-based interpretability scoring, but does not address computational complexity for large KGs.
- Why unresolved: The paper only validates on datasets with relatively small KGs, and the computational cost of path-finding in large graphs is not analyzed.
- What evidence would resolve it: Empirical studies showing runtime and memory requirements for varying KG sizes, along with optimization strategies for path computation.

### Open Question 2
- Question: How sensitive is SMART's performance to the choice of λ (the weight balancing interpretability and model accuracy)?
- Basis in paper: [explicit] The paper mentions using scalarization to combine objectives but does not provide sensitivity analysis of λ values.
- Why unresolved: The paper does not explore how different λ values affect the trade-off between accuracy and interpretability across different domains.
- What evidence would resolve it: Systematic experiments varying λ across datasets to show Pareto-optimal trade-offs and domain-specific optimal values.

### Open Question 3
- Question: Can SMART's approach be extended to handle temporal data and evolving Knowledge Graphs?
- Basis in paper: [inferred] The paper uses static KGs and does not address temporal reasoning or dynamic KG updates.
- Why unresolved: The paper focuses on static feature engineering without considering time-dependent features or evolving domain knowledge.
- What evidence would resolve it: Experimental validation on time-series datasets with dynamic KGs and analysis of how temporal reasoning affects interpretability and performance.

## Limitations
- Reliance on comprehensive domain-specific Knowledge Graphs that may not exist for all application areas
- Limited evaluation to structured tabular data, with performance on unstructured data types unexplored
- Computational overhead from KG reasoning and DQN training that may limit scalability to very large datasets

## Confidence
- High Confidence: The semantic-guided RL framework architecture and core mechanism descriptions are well-specified and internally consistent
- Medium Confidence: Performance claims are supported by experimental results on 10 datasets, though absolute baselines and hyperparameter sensitivity are not fully detailed
- Medium Confidence: Interpretability claims are supported by the proposed Decomposition Graph metric, though qualitative interpretability assessments are limited

## Next Checks
1. KG Completeness Analysis: Evaluate SMART's performance sensitivity to KG coverage by systematically removing semantic relationships and measuring impact on feature quality and model performance
2. Cross-Domain Transferability: Test SMART on datasets from domains with no KG overlap to assess how well the method transfers learned semantic relationships
3. Interpretability Benchmarking: Compare SMART's interpretability scores against human expert evaluations of feature comprehensibility to validate the Decomposition Graph metric's effectiveness