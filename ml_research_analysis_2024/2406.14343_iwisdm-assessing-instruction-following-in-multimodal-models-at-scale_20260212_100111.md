---
ver: rpa2
title: 'IWISDM: Assessing instruction following in multimodal models at scale'
arxiv_id: '2406.14343'
source_url: https://arxiv.org/abs/2406.14343
tags:
- task
- object
- tasks
- operators
- iwisdm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces iWISDM, a novel framework for generating and
  evaluating instruction-following tasks in multimodal contexts. iWISDM addresses
  the gap in existing benchmarks by focusing on instruction-following in visual decision-making
  tasks.
---

# IWISDM: Assessing instruction following in multimodal models at scale

## Quick Facts
- arXiv ID: 2406.14343
- Source URL: https://arxiv.org/abs/2406.14343
- Authors: Xiaoxuan Lei; Lucas Gomez; Hao Yuan Bai; Pouya Bashivan
- Reference count: 40
- Key outcome: iWISDM reveals substantial performance gaps between LMMs and humans in instruction-following tasks with visual contexts

## Executive Summary
iWISDM introduces a novel framework for generating and evaluating instruction-following tasks in multimodal contexts. The framework procedurally constructs task graphs through compositional operators, enabling scalable generation of vision-language tasks with varying complexity. When evaluated on large multimodal models (GPT-4V, Gemini-Pro-1.0, Claude-3, InternLM-XComposer2, MMICL) and human subjects, iWISDM reveals a notable performance gap, with LMMs struggling particularly with spatial recognition and increased task complexity.

## Method Summary
iWISDM generates instruction-following tasks through a three-phase process: task graph construction using Boolean and functional operators, node initialization that propagates properties through directed acyclic graphs, and trial instantiation that produces visual frames, natural language instructions, and ground-truth action sequences. The framework creates synthetic visual decision-making tasks using ShapeNet objects with properties like location, category, and object identity. Three benchmarks with increasing complexity levels (Low: 6 frames, Medium: 8 frames, High: 9 frames) were evaluated using specific prompt templates across five LMMs and six human subjects.

## Key Results
- LMMs showed significant performance gaps compared to humans, with GPT-4V achieving only 56.8% accuracy on the most complex benchmark
- Spatial recognition tasks proved particularly challenging for LMMs, indicating limitations in visual reasoning capabilities
- Performance decreased consistently as task complexity increased, suggesting difficulties with task decomposition and sequential reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** iWISDM enables scalable generation of instruction-following tasks by procedurally constructing task graphs through compositional operators.
- **Mechanism:** The 3-phase process (task graph construction → node initialization → trial instantiation) decouples task definition from execution, allowing unlimited task variants from fixed operator sets.
- **Core assumption:** Boolean and functional operators can be combined logically and temporally to represent complex visual decision-making tasks.
- **Evidence anchors:**
  - [abstract] "iWISDM introduces the instructed-Virtual VISual Decision Making (iWISDM) environment engineered to generate a limitless array of vision-language tasks of varying complexity"
  - [section] "iWISDM creates tasks following: f, i, r = iWISDM(G) where, G denotes the task graph, f the sequence of visual frames, i the corresponding language instructions, and r the sequence of ground-truth actions"
  - [corpus] Weak - related works focus on instruction-following evaluation but not procedural generation mechanisms
- **Break condition:** Operator connectivity rules become too restrictive to represent real-world task complexity, or node initialization fails to resolve conflicts in temporal compositions.

### Mechanism 2
- **Claim:** Temporal compositionality in iWISDM captures the sequential nature of real-world tasks better than static benchmarks.
- **Mechanism:** Queue, Overlap, Interleave, and Condition temporal structures allow tasks to be combined across time, creating multi-step decision processes that require memory and planning.
- **Core assumption:** Real-world tasks can be decomposed into sub-tasks that follow these four temporal patterns.
- **Evidence anchors:**
  - [section] "Temporal compositionality is concerned with how different decision rules should be combined together to construct a complex task that extends in time"
  - [section] "In real-world scenarios, individuals often face tasks that require multiple decisions to be made in sequence or in parallel"
  - [corpus] Weak - corpus neighbors focus on instruction-following but not temporal compositionality specifically
- **Break condition:** Temporal structures become too complex for LMMs to track, or the framework cannot represent tasks requiring non-linear temporal dependencies.

### Mechanism 3
- **Claim:** iWISDM's natural language instruction generation enables evaluation of LMMs' instruction-following abilities in multimodal contexts.
- **Mechanism:** During trial instantiation, partial strings are assigned to each operator based on its definition and initialization, creating human-readable instructions that precisely describe the task logic.
- **Core assumption:** Generated instructions accurately capture the task requirements and can be understood by LMMs as well as humans.
- **Evidence anchors:**
  - [section] "Natural language instructions are generated concurrently during task instantiation. A partial string is assigned to each operator in the task graph that depends on its definitions and initialization"
  - [section] "When completing complex tasks, the instruction communicates first the task structure in terms of upcoming observations, then the task rules that determine the relationship between observations and actions"
  - [corpus] Moderate - related works discuss instruction-following evaluation but not automated instruction generation
- **Break condition:** Generated instructions become too complex or ambiguous for LMMs to interpret correctly, or the instruction-generation process fails to capture task nuances.

## Foundational Learning

- **Concept:** Compositional task representation
  - **Why needed here:** iWISDM relies on breaking down complex tasks into simpler sub-tasks that can be combined logically and temporally
  - **Quick check question:** Can you explain how the Switch operator enables conditional task composition?

- **Concept:** Graph traversal and topological ordering
  - **Why needed here:** The backward initialization process requires understanding how to propagate properties through directed acyclic task graphs
  - **Quick check question:** How does the backward recursive approach ensure logical consistency when initializing task graphs?

- **Concept:** Multimodal instruction-following evaluation
  - **Why needed here:** The benchmark's purpose is to assess LMMs' ability to follow instructions that integrate visual and language inputs
  - **Quick check question:** What makes evaluating instruction-following in multimodal contexts more challenging than unimodal evaluation?

## Architecture Onboarding

- **Component map:** AutoTask → Task graph generation → Node initialization → Trial instantiation → Model evaluation
- **Critical path:** AutoTask → Task graph generation → Node initialization → Trial instantiation → Model evaluation
- **Design tradeoffs:**
  - Fixed vs. extensible operator sets (tradeoff between ease of use and flexibility)
  - Synthetic vs. natural stimuli (control vs. realism)
  - Single vs. multiple complexity levels (granularity vs. simplicity)
- **Failure signatures:**
  - Inconsistent instructions that don't match task logic
  - Operator connectivity violations during AutoTask generation
  - Incorrect property propagation in node initialization
  - Missing or malformed frame sequences
- **First 3 experiments:**
  1. Verify basic task generation with simple operator combinations (e.g., single IsSame operator with known ground truth)
  2. Test temporal composition with Queue structure (e.g., two sequential tasks with predictable outcomes)
  3. Evaluate instruction generation accuracy by comparing generated instructions to hand-written descriptions of simple tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LMMs perform on iWISDM tasks with naturalistic stimuli compared to the synthetic ShapeNet dataset used in the study?
- Basis in paper: [inferred] The paper mentions that the benchmark is generated based on a synthetic stimuli dataset while all LMMs are trained with naturalistic stimuli, and aims to confirm whether LMMs can recognize rendered ShapeNet objects.
- Why unresolved: The paper only tests LMMs on the synthetic ShapeNet dataset, so there's no direct comparison to performance on naturalistic stimuli.
- What evidence would resolve it: Re-running the iWISDM benchmarks with LMMs using naturalistic image datasets (e.g., ImageNet) instead of synthetic ShapeNet objects would provide a direct comparison of performance.

### Open Question 2
- Question: Can the compositional nature of iWISDM tasks be leveraged to identify specific failure modes in LMMs and improve their training?
- Basis in paper: [explicit] The paper states that the compositional nature of iWISDM tasks may provide an avenue for exploring failure modes of current LMMs by investigating their specific weaknesses and targeting those during training.
- Why unresolved: The paper does not explore this potential application of iWISDM in depth, focusing instead on using it as a benchmark.
- What evidence would resolve it: Conducting experiments where LMMs are trained on iWISDM tasks with specific compositional structures that target identified weaknesses, and measuring improvements in performance on those tasks.

### Open Question 3
- Question: How does the inclusion of distractors in iWISDM tasks affect LMM performance compared to humans?
- Basis in paper: [inferred] The paper mentions that distractors can be optionally added to iWISDM trials to make them more attention-demanding, but does not report results on how this affects LMM vs human performance.
- Why unresolved: The paper does not include distractors in the main benchmarks, so there's no comparison of how they impact LMM and human performance differently.
- What evidence would resolve it: Re-running the iWISDM benchmarks with distractors included and comparing the performance gap between LMMs and humans with and without distractors.

## Limitations

- The evaluation reveals substantial performance gaps between LMMs and humans, particularly in spatial recognition tasks, suggesting fundamental limitations in how these models process visual information and integrate it with language instructions.
- Current LMMs struggle with task decomposition and sequential reasoning when visual context is involved, as evidenced by decreased performance with increased task complexity.
- While iWISDM successfully generates diverse instruction-following tasks, the actual performance evaluation reveals that current approaches to multimodal instruction following are still in early stages, with even the best-performing models achieving only 56.8% accuracy.

## Confidence

- **High Confidence**: The framework's ability to generate diverse task variants through procedural composition is well-demonstrated. The task generation process and its scalability are clearly established.
- **Medium Confidence**: The performance gap between LMMs and humans is well-documented, but the specific causes (spatial reasoning vs. other factors) require further investigation.
- **Medium Confidence**: The claim about temporal compositionality being more challenging for LMMs is supported, but the exact mechanisms of failure need deeper analysis.

## Next Checks

1. Conduct ablation studies on temporal composition operators to isolate which specific structures (Queue, Overlap, Interleave, Condition) create the most difficulty for LMMs, and test whether simpler sequential models can handle basic temporal tasks.

2. Perform detailed error analysis on spatial recognition failures by comparing LMM performance on tasks with different types of spatial relationships (relative positioning, occlusion, size comparisons) to identify specific visual reasoning weaknesses.

3. Test the hypothesis that instruction complexity (rather than visual complexity) drives performance differences by creating matched pairs of tasks that vary only in instruction length or linguistic complexity while keeping visual content constant.