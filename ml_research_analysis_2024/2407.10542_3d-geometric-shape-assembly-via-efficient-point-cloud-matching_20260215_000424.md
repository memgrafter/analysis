---
ver: rpa2
title: 3D Geometric Shape Assembly via Efficient Point Cloud Matching
arxiv_id: '2407.10542'
source_url: https://arxiv.org/abs/2407.10542
tags:
- assembly
- matching
- proxy
- shape
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of 3D geometric shape assembly,
  which requires precise localization of mating surfaces and correspondence establishment
  between fractured parts. The key innovation is the Proxy Match Transform (PMT),
  a low-complexity high-order feature transform layer that approximates conventional
  quadratic-complexity matching through a shared proxy tensor mechanism.
---

# 3D Geometric Shape Assembly via Efficient Point Cloud Matching

## Quick Facts
- **arXiv ID:** 2407.10542
- **Source URL:** https://arxiv.org/abs/2407.10542
- **Reference count:** 34
- **Primary result:** Achieves 0.39 Correspondence Distance and 0.25 Chamfer Distance on Breaking Bad dataset, outperforming state-of-the-art geometric assembly methods

## Executive Summary
This paper introduces Proxy Match Transform (PMT), a novel efficient feature transform layer for geometric shape assembly. PMT addresses the quadratic complexity bottleneck in conventional high-order convolution by using a shared proxy tensor to exchange information between source and target features. Building on PMT, the authors propose PMTR, a coarse-to-fine matching framework that first identifies potential mating surfaces coarsely and then refines correspondences at fine resolution. Evaluated on the Breaking Bad benchmark, PMTR significantly outperforms existing methods in both accuracy and efficiency.

## Method Summary
The method employs a two-stage pipeline: (1) KPConv-FPN backbone extracts multi-resolution features from point cloud pairs, and (2) coarse-to-fine matching uses PMT layers to establish correspondences. The PMT layer operates independently on source and target features while maintaining effective correspondence matching through a shared proxy tensor mechanism. The coarse-level matcher identifies potential mating surfaces, while fine-level matchers refine the correspondences using higher-resolution features. Relative transformations are computed using optimal transport, and the model is trained with overlap-aware circle loss, point matching loss, and proxy tensor regularization.

## Key Results
- PMTR achieves 0.39 Correspondence Distance and 0.25 Chamfer Distance on Breaking Bad pairwise assembly
- Outperforms state-of-the-art method GeoTransformer by significant margins (0.39 vs 0.51 CD)
- Demonstrates sub-quadratic computational complexity compared to standard high-order convolution
- Shows superior efficiency with lower memory usage during fine-level matching

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Proxy Match Transform (PMT) approximates high-order feature transform with sub-quadratic complexity while maintaining effective correspondence matching
- **Mechanism:** PMT uses a shared proxy tensor P(h) to exchange information between source and target features without computing the full pairwise correlation matrix. Each PMT layer operates independently on source and target features, computing correlations with the proxy tensor instead of each other, then their dot product approximates the high-order convolution output
- **Core assumption:** The orthogonality constraint P(i)ᵀP(j) = I_Demb if i=j and 0 otherwise ensures that the dot product of PMT outputs can approximate high-order convolution
- **Evidence anchors:**
  - [abstract]: "PMT operates independently on source and target features while maintaining effective correspondence matching, achieving sub-quadratic computational complexity"
  - [section 3.1]: "The product of two PMTs (shown on the right) effectively approximates this high-order transform only with sub-quadratic complexity by avoiding direct construction of correlation scores, instead exchanging information through a low-dimensional proxy tensor"
  - [corpus]: Weak evidence - no direct corpus support for this specific mechanism
- **Break condition:** If proxy tensor orthogonality constraint is violated, PMT cannot accurately approximate high-order convolution

### Mechanism 2
- **Claim:** The coarse-to-fine matching framework enables precise geometric shape assembly by first identifying potential mating surfaces coarsely then refining correspondences at fine resolution
- **Mechanism:** PMTR uses coarse-level features to identify potential mating surfaces without computing full pairwise correlations (O(|X|·|Y|) complexity). This provides reliable initial matches that guide fine-level matching, which refines correspondences using high-resolution features. The coarse matches serve as anchors for grouping fine-level features
- **Core assumption:** Coarse-level matching can reliably identify mating surfaces even with reduced resolution, providing a good initialization for fine-level refinement
- **Evidence anchors:**
  - [abstract]: "Building upon PMT, we introduce a new framework, dubbed Proxy Match TransformeR (PMTR), for the geometric assembly task"
  - [section 3.3]: "Our pipeline begins with the point cloud pair embedding... Each of the two subsequent upsampling layers... provide features in a higher resolution... The coarse feature pair {(FX1, FY1)} is used to identify potential mating surfaces to match, while the others {(FXn, FYn)}3n=2 are used to precisely align the identified potential surface matches"
  - [corpus]: Weak evidence - no direct corpus support for this specific coarse-to-fine mechanism
- **Break condition:** If coarse matching fails to identify reliable mating surfaces, fine-level refinement cannot recover accurate correspondences

### Mechanism 3
- **Claim:** Local attention implementation avoids quadratic complexity while maintaining matching quality
- **Mechanism:** Instead of full pairwise attention matrices of size |X|×|X|, PMTR uses local attention that only computes attention scores within neighborhoods of size ϵ≪|X|. This reduces attention size from |X|×|X| to |X|×ϵ, maintaining the benefits of attention while avoiding quadratic complexity
- **Core assumption:** Local attention within neighborhoods captures sufficient context for reliable correspondence matching
- **Evidence anchors:**
  - [section 4.2]: "In our actual implementation, we use local, i.e., sparse, attention for A(h)X by collecting attention scores of the 'neighborhood' of each position, thus reducing attention size to |X|×ϵ instead of |X|×|X| where ϵ∈N+ is the number of neighbors: ϵ≪|X|"
  - [corpus]: Weak evidence - no direct corpus support for this specific local attention implementation
- **Break condition:** If neighborhood size ϵ is too small, local attention cannot capture necessary context for matching

## Foundational Learning

- **Concept:** High-order feature transform and its quadratic complexity
  - **Why needed here:** Understanding why conventional high-order transforms (Rocco et al., 2018; Choy et al., 2020) are computationally prohibitive for geometric assembly, which requires processing high-resolution point clouds
  - **Quick check question:** What is the computational complexity of a standard high-order convolution when matching two point clouds of size N and M?

- **Concept:** Point cloud feature extraction and resolution hierarchy
  - **Why needed here:** PMTR requires understanding how features are extracted at multiple resolutions (coarse, medium, fine) and how they relate to the assembly task
  - **Quick check question:** How does KPConv-FPN backbone generate multi-resolution features, and why is this hierarchy important for coarse-to-fine matching?

- **Concept:** Geometric compatibility and mating surface identification
  - **Why needed here:** The core assembly task requires analyzing geometric compatibility between fractured parts to identify where they should connect
  - **Quick check question:** What geometric properties should mating surfaces exhibit to ensure proper assembly of fractured parts?

## Architecture Onboarding

- **Component map:** Point cloud pair → KPConv-FPN backbone → Coarse matcher (PMT) → Fine matcher (PMT) → Optimal transport → Transformation prediction → Assembly evaluation
- **Critical path:** Point cloud → KPConv-FPN backbone → Coarse matcher (PMT) → Fine matcher (PMT) → Optimal transport → Transformation prediction → Assembly evaluation
- **Design tradeoffs:**
  - Proxy tensor dimension Dproxy vs. matching accuracy: smaller proxy reduces computation but may lose information
  - Number of PMT layers vs. model capacity: more layers increase expressivity but also complexity
  - Local attention neighborhood size ϵ vs. matching quality: larger neighborhoods capture more context but approach quadratic complexity
- **Failure signatures:**
  - Poor assembly quality with large Chamfer distance indicates PMT isn't learning effective correspondences
  - Memory errors during training suggest Dproxy or neighborhood size ϵ is too large
  - Slow inference times indicate attention implementation isn't properly optimized
  - Training instability may result from improper proxy tensor regularization
- **First 3 experiments:**
  1. **Baseline comparison:** Implement PMTR with PMT replaced by standard high-order convolution on low-resolution features to verify the coarse-to-fine approach works
  2. **Ablation study:** Test PMTR with and without proxy tensor sharing to validate the importance of information exchange
  3. **Efficiency test:** Measure FLOPS and memory usage of PMT vs. standard high-order convolution on the same hardware to confirm computational benefits

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several areas remain unexplored:

- How PMT performs on datasets with significantly lower point cloud overlap compared to Breaking Bad
- The theoretical upper bound on proxy tensor dimensionality (Dproxy) beyond which PMT performance plateaus or degrades
- PMT comparison to attention-based methods on registration tasks with non-fractured objects like LiDAR scans

## Limitations

- Performance can be compromised in scenarios with extremely low overlap between point clouds
- Limited ablation studies examining failure cases where coarse matching fails to identify correct mating surfaces
- Theoretical justification for why the proxy tensor orthogonality constraint produces reliable correspondences remains underdeveloped

## Confidence

- **Core claims:** Medium
- **Computational complexity reduction:** Medium
- **Coarse-to-fine matching effectiveness:** Medium
- **Proxy tensor mechanism validity:** Low

## Next Checks

1. **Theoretical analysis:** Derive error bounds for the proxy tensor approximation under different orthogonality constraint violations to understand when PMT fails
2. **Robustness testing:** Evaluate PMTR performance on point clouds with varying sampling densities and noise levels to assess real-world applicability
3. **Scalability benchmark:** Measure computational complexity scaling with point cloud size (N=500, 1000, 2000, 5000) to verify sub-quadratic behavior across the full range of practical inputs