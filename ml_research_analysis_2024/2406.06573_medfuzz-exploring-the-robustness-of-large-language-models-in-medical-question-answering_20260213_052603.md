---
ver: rpa2
title: 'MedFuzz: Exploring the Robustness of Large Language Models in Medical Question
  Answering'
arxiv_id: '2406.06573'
source_url: https://arxiv.org/abs/2406.06573
tags:
- medical
- answer
- question
- target
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedFuzz tests LLM robustness by adversarially modifying medical
  QA benchmark items to violate non-generalizable assumptions, such as adding patient
  characteristics that could lead to biased reasoning. It uses an attacker LLM to
  iteratively modify questions in ways that don't change the correct answer but could
  lead the target LLM to choose distractors based on stereotypes.
---

# MedFuzz: Exploring the Robustness of Large Language Models in Medical Question Answering

## Quick Facts
- arXiv ID: 2406.06573
- Source URL: https://arxiv.org/abs/2406.06573
- Reference count: 40
- One-line primary result: MedFuzz reduced GPT-4 and Claude accuracy by 8-11% after 5 attack iterations by adversarially modifying medical QA items to violate non-generalizable assumptions

## Executive Summary
MedFuzz is a novel approach for evaluating the robustness of large language models in medical question-answering by adversarially modifying benchmark items to violate non-generalizable assumptions. The method uses an attacker LLM to iteratively modify questions by adding patient characteristics that could lead to biased reasoning, while keeping the correct answer unchanged. A permutation test ensures that successful attacks are statistically significant rather than due to chance. When applied to MedQA, MedFuzz reduced GPT-4 and Claude accuracy by 8-11% after 5 attack iterations, demonstrating that LLMs can be vulnerable to non-generalizable assumptions in medical reasoning tasks.

## Method Summary
MedFuzz employs an iterative attack process where an attacker LLM modifies medical QA benchmark questions by adding patient characteristics that violate assumptions underlying the original benchmark. These modifications aim to "trick" the target LLM into choosing distractors based on stereotypes while preserving the correct answer. The approach includes a permutation test that generates control fuzzes by systematically replacing added text while preserving syntactic structure, allowing statistical validation of attack significance. The method also analyzes chain-of-thought explanations to assess whether the LLM's reasoning remains faithful to the modified content, revealing potential biases in clinical reasoning.

## Key Results
- MedFuzz reduced GPT-4 and Claude accuracy by 8-11% after 5 attack iterations on MedQA
- In individual case studies, target LLMs changed to incorrect answers in 10-20% of attacks
- Chain-of-thought explanations often omitted reference to fuzzed patient characteristics when attacks succeeded

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MedFuzz works by adversarially modifying medical QA benchmark items to violate non-generalizable assumptions.
- Mechanism: The attacker LLM iteratively modifies questions by adding patient characteristics that could lead to biased reasoning, while keeping the correct answer unchanged. This exploits the target LLM's vulnerability to non-generalizable assumptions.
- Core assumption: The target LLM's performance on medical benchmarks relies on assumptions that don't hold in real clinical settings.
- Evidence anchors:
  - [abstract] "MedFuzz attempts to modify benchmark questions in ways aimed at confounding the LLM... Successful 'attacks' modify a benchmark item in ways that would be unlikely to fool a medical expert but nonetheless 'trick' the LLM"
  - [section] "MedFuzz attempts to modify items in the benchmark in ways that 'break' a target LLM's ability to answer those items correctly but that would not confound a human medical expert"
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.49" - indicates moderate relatedness in medical QA literature
- Break condition: If the target LLM doesn't change its answer when confronted with modified questions, the attack fails.

### Mechanism 2
- Claim: The permutation test ensures attacks are statistically significant, not due to chance.
- Mechanism: Control fuzzes are generated by systematically replacing text added by the attacker while preserving syntactic structure. The test compares the probability of correct answers between original, attacked, and control versions.
- Core assumption: Random chance can produce apparent attack successes that don't generalize.
- Evidence anchors:
  - [abstract] "we present a permutation test technique that can ensure a successful attack is statistically significant"
  - [section] "we opt to evaluate statistical significance of an individual MedFuzz by permutation test"
  - [corpus] "Uncertainty Estimation of Large Language Models in Medical Question Answering" - indicates ongoing work in this area
- Break condition: If the p-value is not below the significance threshold, the attack is considered statistically insignificant.

### Mechanism 3
- Claim: MedFuzz reveals LLM vulnerability to social biases and stereotypes in medical reasoning.
- Mechanism: By adding patient characteristics that appeal to stereotypes (e.g., about immigrants, substance abuse, or certain ethnicities), the attack can trigger biased reasoning in the target LLM while a human expert would not be affected.
- Core assumption: Medical question-answering benchmarks contain assumptions about patient characteristics that don't generalize to real clinical settings.
- Evidence anchors:
  - [abstract] "Using MedFuzz, we surface the potential for clinical application of an LLM to reflect medical misconceptions and stereotypes"
  - [section] "we focus on assumptions about the types of patient characteristics (PCs) that appear in MedQA... PCs can be used to draw attention to a distractor by providing clinical or medical evidence in favor of the distractor"
  - [corpus] "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering Benchmark Dataset" - suggests diversity considerations in medical QA
- Break condition: If the LLM consistently ignores added patient characteristics and maintains accuracy, it demonstrates robustness to this type of bias.

## Foundational Learning

- Concept: Adversarial machine learning
  - Why needed here: MedFuzz is an adversarial approach that intentionally perturbs inputs to reveal model vulnerabilities
  - Quick check question: What distinguishes MedFuzz from traditional adversarial attacks that add random noise to images?

- Concept: Statistical significance testing
  - Why needed here: The permutation test ensures that successful attacks aren't just due to random chance
  - Quick check question: How does the permutation test distinguish between genuine vulnerabilities and artifacts of the attack process?

- Concept: Chain-of-thought faithfulness
  - Why needed here: Analyzing whether the LLM's explanations mention the fuzzed information reveals if the reasoning is faithful
  - Quick check question: What does it mean if an LLM's CoT explanation omits reference to the patient characteristics added by the attack?

## Architecture Onboarding

- Component map:
  - Attacker LLM: Modifies questions by adding patient characteristics
  - Target LLM: Answers modified questions
  - Evaluation framework: Measures accuracy changes and runs permutation tests
  - Expert reviewers: Validate attack significance and identify interesting case studies

- Critical path: Original question → Attacker modification → Target answer → Accuracy comparison → Permutation test

- Design tradeoffs: The attack must violate assumptions while keeping the correct answer unchanged, limiting the types of modifications possible

- Failure signatures: 
  - Attack fails to change target answer
  - Attack changes correct answer (violates constraint)
  - LLM errors produce incoherent responses
  - Permutation test shows no statistical significance

- First 3 experiments:
  1. Run MedFuzz with 1 attack iteration on a subset of MedQA questions
  2. Analyze faithfulness of CoT responses when attacks succeed
  3. Compare attack success rates between GPT-3.5 and GPT-4 as target models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MedFuzz performance vary when applied to non-medical professional exam benchmarks (e.g., legal, engineering)?
- Basis in paper: [inferred] The paper mentions future work plans to modify the technique for other domains like legal settings where professional exam performance generalizes to practice settings
- Why unresolved: The authors state this as a future work direction but haven't conducted experiments in these domains yet
- What evidence would resolve it: Direct application of MedFuzz to non-medical professional exam benchmarks with comparative analysis of assumption violations and attack effectiveness

### Open Question 2
- Question: Does fine-tuning LLMs on medical data improve their robustness to assumption violations compared to non-fine-tuned models?
- Basis in paper: [inferred] The authors mention future work plans to contrast generalization between fine-tuned and non-fine-tuned models
- Why unresolved: While the paper compares multiple models, it doesn't specifically analyze the impact of medical fine-tuning on assumption violation robustness
- What evidence would resolve it: Systematic comparison of MedFuzz attack effectiveness on fine-tuned vs. non-fine-tuned LLMs using identical medical benchmarks

### Open Question 3
- Question: What is the optimal number of attack iterations needed to reliably identify significant assumption violations without diminishing returns?
- Basis in paper: [explicit] Figure 2 shows accuracy changes across different numbers of attack attempts, suggesting diminishing returns
- Why unresolved: The paper only tests up to 5 attack attempts and doesn't determine the point of statistical convergence or optimal stopping criteria
- What evidence would resolve it: Extended testing with varying attack iteration limits combined with statistical analysis to identify the point where additional attacks no longer significantly improve detection of assumption violations

## Limitations
- The attack methodology relies on the attacker LLM's ability to identify and violate non-generalizable assumptions while preserving the correct answer
- The permutation test may not fully distinguish between attacks that succeed due to subtle linguistic effects versus those that expose fundamental model vulnerabilities
- Case study analysis is limited to expert review of a small number of examples, which may not be representative of overall attack patterns

## Confidence

- **High confidence**: The core finding that MedFuzz can reduce LLM accuracy by 8-11% through targeted modifications that violate benchmark assumptions
- **Medium confidence**: The mechanism by which patient characteristic additions trigger biased reasoning, as this depends on the attacker LLM's success in identifying effective modifications
- **Medium confidence**: The statistical significance of individual attacks, as the permutation test methodology is sound but the choice of control fuzz generation may influence results

## Next Checks

1. **Replication with multiple attacker LLMs**: Run MedFuzz using different attacker models (e.g., GPT-3.5, Claude) to test whether attack patterns are consistent across different attack strategies, helping distinguish between attacker-specific artifacts and genuine model vulnerabilities.

2. **Cross-benchmark validation**: Apply MedFuzz to additional medical QA benchmarks beyond MedQA to determine if the observed vulnerability patterns generalize across different datasets and question styles.

3. **Human expert blind study**: Conduct a blinded study where medical experts answer both original and attacked questions to establish whether the modifications genuinely preserve answerability while remaining clinically plausible, providing ground truth for the "would not confound a human expert" claim.