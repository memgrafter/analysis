---
ver: rpa2
title: 'ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional
  Correctness?'
arxiv_id: '2407.14044'
source_url: https://arxiv.org/abs/2407.14044
tags:
- program
- code
- efficiency
- correctness
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ECCO, a benchmark for evaluating program
  efficiency in large language models while maintaining functional correctness. The
  key innovation is a reproducible evaluation platform using cloud-hosted code execution
  (Judge0) to ensure consistent runtime and memory measurements across varying hardware.
---

# ECCO: Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness?

## Quick Facts
- arXiv ID: 2407.14044
- Source URL: https://arxiv.org/abs/2407.14044
- Reference count: 16
- One-line primary result: No existing method significantly improves both efficiency and functional correctness simultaneously in LLM-generated code optimization

## Executive Summary
This paper introduces ECCO, a benchmark for evaluating program efficiency in large language models while maintaining functional correctness. The key innovation is a reproducible evaluation platform using cloud-hosted code execution (Judge0) to ensure consistent runtime and memory measurements across varying hardware. ECCO supports two optimization paradigms: history-based code editing and NL-instructed generation, with over 50k Python solution pairs across 1.3k competitive programming problems. The paper systematically evaluates three classes of methods—in-context learning, iterative refinement, and fine-tuning—finding that while most methods moderately improve efficiency (speedup 1.44-3.54x, memory reduction 1.01-1.37x), they often degrade functional correctness (pass@1 drops to 8.5-66.6%). Exec-refine consistently maintains the highest correctness (pass@1 39.5-60.8%), while NL feedback methods achieve better efficiency gains. Fine-tuning, especially trajectory-conditioned, shows promise in preserving correctness during history-based editing (pass@1 69.8%). However, no existing method can significantly improve both efficiency and correctness simultaneously, highlighting the challenge of correctness-preserving program optimization.

## Method Summary
ECCO introduces a benchmark for evaluating LLM-generated code efficiency with functional correctness preservation. The method uses cloud-hosted code execution via Judge0 to ensure reproducible runtime and memory measurements across different hardware. Three classes of methods are evaluated: in-context learning (instruction prompting and few-shot), iterative refinement (self-refine with NL feedback, exec-refine with execution feedback, and NL+exec-refine combining both), and fine-tuning (vanilla, execution-conditioned, and trajectory-conditioned). The benchmark supports two optimization paradigms—history-based code editing and NL-instructed generation—across 1.3k competitive programming problems with over 50k Python solution pairs. Models are evaluated on functional correctness (pass@1) and efficiency metrics (speedup, memory reduction) using private test cases, with execution feedback from public test cases guiding the refinement process.

## Key Results
- Iterative refinement with execution feedback (exec-refine) maintains highest functional correctness (pass@1 39.5-60.8%) while providing moderate efficiency gains
- Natural language feedback methods achieve better efficiency improvements but suffer greater correctness degradation
- Fine-tuning, particularly trajectory-conditioned, shows promise in preserving correctness during history-based editing (pass@1 69.8%)
- No existing method significantly improves both efficiency and functional correctness simultaneously
- Speedup ranges from 1.44x to 3.54x and memory reduction from 1.01x to 1.37x across methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Execution information helps maintain functional correctness during code optimization.
- **Mechanism:** When iterative refinement methods include execution feedback (pass/fail status, runtime, memory) from public test cases, models can detect when optimizations introduce bugs and adjust accordingly. This grounding in concrete execution results is more reliable than natural language feedback alone for preserving correctness.
- **Core assumption:** Models can interpret execution feedback correctly and use it to guide bug-fixing rather than just efficiency improvements.
- **Evidence anchors:**
  - [abstract] "we find that adding execution information often helps maintain functional correctness"
  - [section 5.2.2] "exec-refine consistently yields the highest pass@1 for all models, by 3.4–38.8 points more than the other two methods"
  - [section 5.2.2] "exec-refine can fix incorrect programs to pass all public and private test cases, with access to only the PASS/FAIL status of public test cases"
- **Break condition:** If execution feedback becomes noisy or ambiguous (e.g., non-deterministic behavior, flaky tests), models may misinterpret results and fail to preserve correctness.

### Mechanism 2
- **Claim:** Fine-tuning on execution-conditioned data preserves correctness better than prompting alone.
- **Mechanism:** By training models on pairs of programs with their execution results, the model learns the relationship between code changes and their effects on correctness and efficiency. This learned knowledge generalizes better than in-context examples.
- **Core assumption:** The training data captures meaningful patterns between program modifications and their execution outcomes.
- **Evidence anchors:**
  - [section 5.2.3] "fine-tuning is the most effective method in maintaining correctness in the editing paradigm"
  - [section 5.2.3] "vanilla and execution-conditioned tuning improves by 6.9 and 7.8 points, and trajectory-conditioned tuning further gains a 34.6 point increase"
- **Break condition:** If the training data is too small or not representative of the optimization space, fine-tuning may not learn useful patterns.

### Mechanism 3
- **Claim:** Natural language feedback improves efficiency more effectively than execution feedback.
- **Mechanism:** When models receive natural language feedback about inefficiencies (e.g., "this loop is O(n²) and can be optimized to O(n log n)"), they can apply algorithmic knowledge to make substantial efficiency improvements. Execution feedback is limited to incremental improvements based on test results.
- **Core assumption:** Models have sufficient algorithmic knowledge to interpret and act on NL optimization suggestions.
- **Evidence anchors:**
  - [section 5.2.2] "methods that involve NL feedback (self-refine and nl+exec refine) achieve the highest speedup across models"
  - [section 5.2.2] "self-refine and nl+exec refine improve runtime and memory usage for a few cases"
- **Break condition:** If NL feedback is too vague or the model lacks the algorithmic knowledge to interpret it, efficiency improvements may not materialize.

## Foundational Learning

- **Concept:** Execution-based evaluation platform
  - Why needed here: To reliably measure both functional correctness and efficiency metrics (runtime, memory) in a reproducible way across different hardware
  - Quick check question: How does the JUDGE 0 platform ensure consistent execution results across different hardware setups?

- **Concept:** Iterative refinement with feedback loops
  - Why needed here: To progressively improve generated code by incorporating feedback from previous attempts, allowing the model to correct mistakes and optimize performance
  - Quick check question: What are the three types of feedback used in the iterative refinement methods?

- **Concept:** Fine-tuning with trajectory data
  - Why needed here: To help models learn the step-by-step improvement process that leads to optimal solutions by conditioning on intermediate program states
  - Quick check question: How does trajectory-conditioned fine-tuning differ from vanilla fine-tuning in terms of input data?

## Architecture Onboarding

- **Component map:** ECCO dataset (1.3k problems, 50k+ program pairs) -> JUDGE 0 execution engine -> Three method classes (in-context learning, iterative refinement, fine-tuning) -> Two optimization paradigms (history-based editing, NL-instructed generation)

- **Critical path:**
  1. Select problem and input program (for editing) or NL description (for generation)
  2. Generate initial solution using LM
  3. Execute on public test cases to obtain feedback
  4. Refine using iterative methods or generate new solution
  5. Evaluate on private test cases for final correctness and efficiency metrics

- **Design tradeoffs:**
  - Execution feedback vs NL feedback: Execution is more reliable for correctness but NL enables bigger efficiency gains
  - Fine-tuning vs prompting: Fine-tuning better preserves correctness but is resource-intensive; prompting is faster but less reliable
  - Model scale vs performance: Larger models generally perform better but with diminishing returns and higher resource costs

- **Failure signatures:**
  - Drop in pass@1 indicates loss of functional correctness
  - Minimal speedup despite multiple refinement iterations suggests optimization strategies aren't effective
  - High variance across runs indicates instability in the evaluation or generation process

- **First 3 experiments:**
  1. Compare pass@1, speedup, and memory reduction for a single model using in-context learning vs iterative refinement vs fine-tuning
  2. Test exec-refine vs self-refine vs nl+exec-refine to isolate the effect of execution feedback on correctness
  3. Evaluate the impact of model scale by testing the same method across different model sizes (7B, 13B, 70B)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to competitive programming problems with well-defined test cases, may not generalize to real-world software development
- Effectiveness of execution feedback depends on quality and coverage of public test cases
- Fine-tuning approaches require significant computational resources and large amounts of paired data
- Study focuses on decoder-only transformer models, other architectures may yield different results

## Confidence

**High Confidence:** The observation that no method significantly improves both efficiency and correctness simultaneously is well-supported by systematic experiments across multiple models and methods. The benchmark infrastructure appears sound and the evaluation metrics are clearly defined.

**Medium Confidence:** The finding that execution feedback helps maintain correctness is supported by results, but the mechanism's robustness across different problem domains and feedback qualities remains uncertain. The specific effectiveness of trajectory-conditioned fine-tuning is promising but may be sensitive to training data quality and quantity.

**Low Confidence:** The generalizability of efficiency improvements from NL feedback to complex, real-world optimization scenarios is uncertain. The assumption that models can effectively interpret algorithmic improvement suggestions in natural language may not hold for all problem types.

## Next Checks

1. **Cross-Domain Validation:** Test ECCO methods on real-world codebases with different testing paradigms (e.g., unit tests vs integration tests) to assess generalizability beyond competitive programming problems.

2. **Feedback Robustness Analysis:** Systematically vary the quality and coverage of public test cases to measure how execution feedback effectiveness degrades with limited test coverage.

3. **Multi-Language Extension:** Implement ECCO methods for a different programming language (e.g., Java or C++) to validate whether observed tradeoffs between efficiency and correctness persist across language paradigms.