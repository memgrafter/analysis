---
ver: rpa2
title: 'Look at the Text: Instruction-Tuned Language Models are More Robust Multiple
  Choice Selectors than You Think'
arxiv_id: '2404.08382'
source_url: https://arxiv.org/abs/2404.08382
tags:
- answer
- high
- school
- first
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two approaches to evaluating large language
  models (LLMs) on multiple choice questions (MCQs): text-based evaluation, where
  a model''s answer is extracted from its generated text, and first-token probability
  evaluation, where the option with the highest first-token probability is selected.
  The authors find that text answers are more robust to perturbations like typos,
  word swaps, and option order changes than first-token answers, especially when the
  first-token answer mismatches the text answer.'
---

# Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think

## Quick Facts
- arXiv ID: 2404.08382
- Source URL: https://arxiv.org/abs/2404.08382
- Authors: Xinpeng Wang; Chengzhi Hu; Bolei Ma; Paul RÃ¶ttger; Barbara Plank
- Reference count: 24
- Primary result: Text-based MCQ evaluation is more robust to perturbations than first-token probability evaluation for instruction-tuned LLMs, especially when there's a mismatch between the two.

## Executive Summary
This paper investigates the robustness of two methods for evaluating large language models on multiple choice questions: text-based evaluation and first-token probability evaluation. The authors find that text answers are more robust to perturbations like typos, word swaps, and option order changes than first-token answers, particularly when there is a mismatch between the two. The study reveals that selection bias is lower in text answers than in first-token probabilities, and the robustness gap increases as the mismatch rate increases. The authors conclude that text-based evaluation is more reliable for evaluating LLMs on MCQs, especially for models with high mismatch rates.

## Method Summary
The authors evaluate the robustness of MCQ evaluation methods by comparing text-based evaluation (extracting option IDs from generated text) against first-token probability evaluation (selecting the option with highest first-token probability) across instruction-tuned LLMs. They apply five perturbation types to MCQ prompts: letter typos, letter swaps, word swaps, option swaps, and additional options. A Mistral-7b model is fine-tuned using QLoRA to extract text answers from model outputs. The study measures selection bias using standard deviation of recalls (RStD), answer consistency using entropy under perturbations, and accuracy. The models tested include Llama2-7b-Chat, Llama2-13b-Chat, Mistral-7b-Inst, and Gemma-7b-Inst on MMLU and OpinionQA datasets.

## Key Results
- Text answers are more robust to question perturbations than first token probabilities when there is a mismatch between the two.
- Selection bias (RStD) is lower in text answers than in first-token probabilities, especially for models with high mismatch rates.
- The robustness gap between text and first-token answers increases as the mismatch rate increases, with text answers outperforming debiased first-token probabilities when mismatch exceeds 50%.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-based MCQ evaluation is more robust to perturbations than first-token probability evaluation, especially when there is a mismatch between the two.
- Mechanism: Instruction-tuned LLMs generate full text responses that reflect a more holistic understanding of the question and options. The first-token probability, however, is a narrow measure that can be sensitive to superficial patterns like option order or typos. When the model's text response diverges from its first-token prediction, the text is less affected by these perturbations.
- Core assumption: The model's text output is generated by a process that is more stable under linguistic variations than the next-token probability distribution.
- Evidence anchors:
  - [abstract] The authors show that text answers are more robust to question perturbations than first token probabilities when there is a mismatch.
  - [section 3.5] The paper finds that text answers show smaller entropy under perturbations like typos, word swaps, and option order changes compared to first-token answers.
  - [corpus] Related work (Wang et al., 2024) also found that first-token probabilities do not match text answers for instruction-tuned models.
- Break condition: If the model's text generation process itself is brittle to perturbations (e.g., due to sensitivity to input phrasing), the robustness advantage of text answers could diminish.

### Mechanism 2
- Claim: Selection bias is lower in text answers than in first-token probabilities.
- Mechanism: First-token probabilities can be influenced by the frequency of certain tokens in the training data, leading to a bias towards options like 'A'. Text answers, being more semantically driven, are less susceptible to this token-level bias.
- Core assumption: The model's text generation process prioritizes semantic coherence over token frequency.
- Evidence anchors:
  - [section 3.3] The paper shows that text answers have lower standard deviation of recalls (RStD) than first-token probabilities, indicating less selection bias.
  - [section 3.3] Text answers outperform debiased first-token probabilities (using PriDe) in terms of selection bias for models with high mismatch rates.
  - [corpus] Dominguez-Olmedo et al. (2023) also found selection bias in first-token probabilities.
- Break condition: If the model's text generation process is heavily influenced by token frequency or if the model is not well instruction-tuned, the selection bias in text answers could increase.

### Mechanism 3
- Claim: The robustness gap between text and first-token answers increases as the mismatch rate between them increases.
- Mechanism: When the first-token answer frequently mismatches the text answer, it suggests that the first-token probability is a poor proxy for the model's intended answer. In such cases, perturbations that affect the first-token probability are less likely to affect the text answer, leading to a larger robustness gap.
- Core assumption: A high mismatch rate indicates that the first-token probability is not capturing the model's true answer.
- Evidence anchors:
  - [section 3.3] The paper shows that the robustness discrepancy between first-token and text answers increases as the mismatch rate increases.
  - [section 3.3] When the mismatch rate exceeds 50%, text answers are more robust to option order changes than debiased first-token probabilities.
  - [section 3.6] The paper finds a correlation between mismatch rate and the difference in accuracy and selection bias between first-token and text answers.
- Break condition: If the mismatch rate is low (e.g., for models like Mistral-7b with good instruction-following), the robustness gap may be minimal.

## Foundational Learning

- Concept: Instruction-tuning and RLHF
  - Why needed here: The paper's findings are specific to instruction-tuned LLMs. Understanding how instruction-tuning and RLHF affect model behavior is crucial for interpreting the results.
  - Quick check question: How do instruction-tuning and RLHF influence the alignment between a model's text output and its first-token probability?

- Concept: Selection bias in multiple-choice questions
  - Why needed here: The paper discusses how selection bias affects the robustness of MCQ evaluation methods. Understanding the sources and implications of selection bias is essential for evaluating the proposed approach.
  - Quick check question: What factors can contribute to selection bias in LLM MCQ evaluation, and how can it be mitigated?

- Concept: Prompt brittleness and robustness
  - Why needed here: The paper investigates the robustness of MCQ evaluation methods to various perturbations. Understanding the concepts of prompt brittleness and robustness is crucial for designing and interpreting the experiments.
  - Quick check question: How do linguistic perturbations like typos, word swaps, and option order changes affect the performance of LLM MCQ evaluation methods?

## Architecture Onboarding

- Component map: MCQ generation and perturbation module -> LLM inference engine -> Text answer extraction classifier -> First-token probability extraction module -> Evaluation and comparison module

- Critical path:
  1. Generate MCQs and apply perturbations
  2. Run perturbed MCQs through LLMs
  3. Extract text answers using the classifier
  4. Extract first-token probabilities
  5. Evaluate and compare robustness of text and first-token answers

- Design tradeoffs:
  - Using a proprietary model (e.g., GPT-4) for text answer extraction vs. fine-tuning a smaller model: Tradeoff between accuracy and cost/transparency.
  - Using first-token probability vs. full text generation: Tradeoff between simplicity and robustness.
  - Including out-of-distribution data in the classifier training: Tradeoff between generalizability and potential overfitting.

- Failure signatures:
  - High mismatch rate between text and first-token answers: Indicates that first-token evaluation may not be reliable.
  - High selection bias in text answers: Suggests that the text generation process is influenced by superficial patterns.
  - Low robustness of text answers to perturbations: Implies that the text generation process itself is brittle.

- First 3 experiments:
  1. Replicate the main robustness comparison between text and first-token answers on a small set of MCQs and perturbations.
  2. Investigate the relationship between mismatch rate and robustness gap by varying the instruction-following ability of the LLMs.
  3. Evaluate the effectiveness of different text answer extraction methods (e.g., pattern matching, fine-tuned classifier) on a diverse set of LLM outputs.

## Open Questions the Paper Calls Out

- How do different model families (e.g., GPT-4, Gemini) compare to the models studied in terms of mismatch rate between first-token probabilities and text answers, and how does this affect their robustness to perturbations?
- Can the text-based evaluation approach be further improved by incorporating additional linguistic features or context beyond the option IDs, and would this lead to even greater robustness?
- How does the robustness of text answers vary across different MCQ domains or question types, and are there specific domains where the text-based evaluation approach is particularly advantageous or disadvantageous?

## Limitations

- The robustness advantage of text-based evaluation is strongly dependent on the mismatch rate between text and first-token answers.
- The text answer extraction classifier may struggle with model-specific response styles or novel phrasings, leading to incorrect option extraction.
- The findings are based on a specific set of perturbations and may not generalize to other types of linguistic variations or real-world scenarios.

## Confidence

- High: The observation that text answers are more robust to perturbations than first-token probabilities when there is a mismatch between the two.
- Medium: The claim that text-based evaluation is more reliable for evaluating LLMs on MCQs, as this depends on the specific context (e.g., mismatch rate, types of perturbations).
- Low: The assertion that the robustness advantage of text-based evaluation holds across all instruction-tuned LLMs and perturbation types, as the findings are based on a limited set of models and perturbations.

## Next Checks

1. Investigate the relationship between instruction-following ability and mismatch rate by evaluating a wider range of LLMs on the MMLU benchmark.
2. Test the robustness of text-based evaluation to other types of perturbations not considered in the paper, such as paraphrasing, sentence structure changes, or domain-specific jargon.
3. Evaluate the performance of different text answer extraction methods on a diverse set of LLM outputs to identify the most reliable and generalizable approach.