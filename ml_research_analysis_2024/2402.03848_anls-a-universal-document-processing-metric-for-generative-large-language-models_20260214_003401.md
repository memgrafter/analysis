---
ver: rpa2
title: ANLS* -- A Universal Document Processing Metric for Generative Large Language
  Models
arxiv_id: '2402.03848'
source_url: https://arxiv.org/abs/2402.03848
tags:
- anls
- metric
- prompting
- document
- latin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ANLS, a new evaluation metric designed for
  generative large language models (GLLMs) used in document processing tasks such
  as information extraction and classification. The metric extends existing ANLS metrics,
  supporting various data types including strings, lists, dictionaries, and complex
  nested structures, enabling a unified evaluation framework for both generative and
  discriminative models.
---

# ANLS* -- A Universal Document Processing Metric for Generative Large Language Models

## Quick Facts
- **arXiv ID**: 2402.03848
- **Source URL**: https://arxiv.org/abs/2402.03848
- **Reference count**: 34
- **Primary result**: ANLS* extends existing ANLS metrics to support complex structured outputs while maintaining backward compatibility, enabling unified evaluation of generative and discriminative models for document processing tasks.

## Executive Summary
This paper introduces ANLS*, a new evaluation metric designed specifically for generative large language models (GLLMs) used in document processing tasks such as information extraction and classification. The metric extends existing ANLS metrics to support various data types including strings, lists, dictionaries, and complex nested structures, enabling a unified evaluation framework for both generative and discriminative models. Through experiments across seven datasets and more than 20 GLLMs, the metric demonstrated strong versatility and reliability, with Claude-3 and Gemini-1.5-pro emerging as top performers. Additionally, the paper introduces a novel prompting method called SFT that outperforms existing techniques including LATIN by up to 10 percentage points.

## Method Summary
The paper presents ANLS* as an extension of existing ANLS metrics that maintains backward compatibility while adding support for complex structured outputs. The metric recursively decomposes both ground truth and prediction into tree structures, using Hungarian matching for lists and recursive scoring for nested structures. It handles type mismatches by assigning zero score and penalizes missing or hallucinated keys/values appropriately. The evaluation framework was tested across seven document processing datasets using more than 20 different GLLMs, comparing three prompting methods: Simple, LATIN, and the novel SFT approach. The experiments measured performance across various data types and document layouts to validate the metric's universality.

## Key Results
- ANLS* successfully extends ANLS metrics to handle complex nested structures while maintaining compatibility with previous ANLS scores
- Claude-3 and Gemini-1.5-pro demonstrated superior performance among tested GLLMs for document processing tasks
- The SFT prompting method outperformed LATIN by up to 10 percentage points in some cases across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1: Recursive Tree Decomposition
The metric recursively decomposes complex structured outputs into comparable tree structures using Hungarian matching for lists and recursive scoring for nested structures. This allows evaluation of outputs ranging from simple strings to deeply nested dictionaries. The core assumption is that complex structured outputs can be effectively evaluated by decomposing them into comparable tree structures with recursive scoring. Evidence shows the metric maintains backward compatibility with existing ANLS scores while extending functionality. The mechanism fails when output structures are too dissimilar to establish meaningful matching.

### Mechanism 2: SFT Prompting Optimization
The SFT (Specialized Formatting Technique) prompting method outperforms LATIN by up to 10 percentage points by better encoding document properties for text-based GLLMs. This likely incorporates more sophisticated layout-aware instructions that translate 2D document structures into linear text prompts more effectively. The core assumption is that text-based GLLMs achieve better performance when provided with specially formatted prompts that preserve spatial relationships and document structure. The mechanism breaks down when document layouts are simple or when vision-based models that don't require OCR-to-prompt conversion are used.

### Mechanism 3: Type-Aware Scoring
ANLS* implements strict type matching where type mismatches receive zero score, while minor errors in matching content receive proportional penalties. This ensures reliability but may penalize valid semantic matches with different types. The core assumption is that strict type matching ensures reliable evaluation even at the cost of some semantic flexibility. The mechanism has limitations when different representations convey the same meaning but use different types.

## Foundational Learning

- **Concept**: Normalized Levenshtein Similarity (NLS)
  - Why needed here: ANLS* builds on NLS as the fundamental string comparison metric, so understanding how it works is essential for grasping the more complex metric
  - Quick check question: If two strings have a Levenshtein distance of 2 and lengths of 10 and 12 respectively, what would their normalized Levenshtein similarity be?

- **Concept**: Hungarian Matching Algorithm
  - Why needed here: Used to optimally match elements between ground truth and predicted lists when order doesn't matter but content does
  - Quick check question: Why is Hungarian matching preferred over simple greedy matching when comparing two lists of strings for evaluation?

- **Concept**: Recursive Tree Decomposition
  - Why needed here: ANLS* recursively breaks down complex nested structures into comparable subtrees, so understanding this decomposition strategy is crucial
  - Quick check question: How would you handle a dictionary containing lists of dictionaries when decomposing it into a tree structure for comparison?

## Architecture Onboarding

- **Component map**: Input parser -> Tree decomposition -> Type matcher -> Hungarian matcher -> Recursive scorer -> Length calculator -> Output formatter
- **Critical path**: Input → Tree decomposition → Type matching → Recursive scoring → Length normalization → Output
- **Design tradeoffs**: The metric prioritizes strict type matching over semantic similarity, which ensures reliability but may penalize valid semantic matches with different types
- **Failure signatures**: Zero scores due to type mismatches, unexpectedly low scores from recursive penalties, inconsistent results between similar inputs
- **First 3 experiments**:
  1. Test the metric on simple string comparisons to verify basic NLS functionality
  2. Test list comparisons with varying order and content to verify Hungarian matching
  3. Test nested dictionary structures to verify recursive scoring and length calculation

## Open Questions the Paper Calls Out

- **Open Question 1**: How does ANLS* compare in performance and reliability to traditional discriminative models for document processing tasks? The paper mentions ANLS* can also be used for discriminative models but doesn't provide experimental results comparing it with discriminative models.
- **Open Question 2**: How does the performance of ANLS* vary with different OCR systems and their error rates? The paper mentions ANLS* is designed to handle OCR errors but doesn't explore the impact of different OCR systems on the metric's performance.
- **Open Question 3**: How does ANLS* handle more complex nested data structures beyond those tested in the paper? The paper only demonstrates ANLS* on a limited set of complex structures and doesn't explore its performance on more intricate or varied nested data.

## Limitations

- The SFT prompting method's implementation details are not provided, making independent verification challenging
- The evaluation relies heavily on proprietary GLLM APIs, raising concerns about reproducibility and version consistency
- The metric's strict type matching may not adequately capture semantic equivalence when different representations convey the same meaning

## Confidence

- **High confidence**: The ANLS* metric definition and its backward compatibility with existing ANLS metrics are well-specified and theoretically sound
- **Medium confidence**: The experimental results showing Claude-3 and Gemini-1.5-pro as top performers, though these depend on specific model versions and API access that may vary
- **Low confidence**: The SFT prompting method's claimed superiority due to lack of implementation details, though the 10 percentage point improvement is clearly stated

## Next Checks

1. **Implement a minimal SFT prototype** based on the limited description provided and test it against LATIN on a small document dataset to verify the claimed performance improvement
2. **Conduct type sensitivity analysis** by creating test cases where semantically equivalent answers use different data types to quantify how often ANLS* penalizes valid semantic matches
3. **Benchmark ANLS* against existing metrics** on the Kleister Charity dataset, comparing scores with human annotations to validate that ANLS* captures meaningful performance differences