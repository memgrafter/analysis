---
ver: rpa2
title: History-Aware Conversational Dense Retrieval
arxiv_id: '2401.16659'
source_url: https://arxiv.org/abs/2401.16659
tags: []
core_contribution: 'This paper proposes a history-aware conversational dense retrieval
  method (HAConvDR) to address the challenge of noisy conversation history in multi-turn
  interactions. The method incorporates two key ideas: context-denoised query reformulation
  using pseudo relevance judgments for historical turns, and automatic mining of supervision
  signals from historical ground-truth passages.'
---

# History-Aware Conversational Dense Retrieval

## Quick Facts
- arXiv ID: 2401.16659
- Source URL: https://arxiv.org/abs/2401.16659
- Reference count: 15
- Key outcome: HAConvDR achieves up to 10.7% MRR and 8.0% NDCG@3 improvements on TopiOCQA, especially for long conversations with topic shifts

## Executive Summary
This paper addresses the challenge of noisy conversation history in multi-turn conversational search by proposing a history-aware conversational dense retrieval method (HAConvDR). The method combines context-denoised query reformulation using pseudo relevance judgments with automatic mining of supervision signals from historical ground-truth passages. By selectively incorporating relevant historical turns and leveraging historical passages as pseudo-positives and hard negatives, HAConvDR significantly outperforms strong baselines on two conversational search datasets, with particularly notable gains on long conversations with topic shifts.

## Method Summary
HAConvDR improves conversational dense retrieval by addressing two key challenges: noisy conversation history and limited supervision signals. The method employs pseudo relevance judgments (PRJ) to evaluate the relevance of historical turns to the current query, using retrieval performance comparison as the evaluation metric. Based on PRJ results, it performs context-denoised query reformulation by concatenating relevant historical turns (queries and passages) with the current query. For training, HAConvDR automatically mines additional supervision signals by treating historical ground-truth passages from relevant turns as pseudo-positives and passages from irrelevant turns as hard negatives, creating more informative contrastive learning pairs. The approach is built on top of the ANCE dense retrieval framework and uses history-aware contrastive learning for training.

## Key Results
- HAConvDR achieves 10.7% MRR and 8.0% NDCG@3 improvements on TopiOCQA dataset
- Significant gains observed particularly for long conversations with topic shifts
- Outperforms strong baselines including Conv-ANCE with historical supervision signals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-denoised query reformulation using pseudo relevance judgments improves retrieval by filtering irrelevant historical turns
- Mechanism: Historical turns are evaluated for relevance to current query using pseudo-labeling that compares retrieval performance with/without historical context; only relevant turns are included in reformulated query
- Core assumption: Retrieval performance improvement when adding historical turn indicates true relevance to current information need
- Evidence anchors:
  - [abstract] "context-denoised query reformulation using pseudo relevance judgments for historical turns"
  - [section 3.3] Algorithm 1 shows PRJ approach using retrieval performance comparison
  - [corpus] Weak - no corpus evidence found supporting this specific mechanism
- Break condition: Historical turns share topical overlap but differ in information need (topic shift), making relevance judgment fail

### Mechanism 2
- Claim: Mining supervision signals from historical ground-truth passages provides stronger contrastive learning signals than in-batch negatives alone
- Mechanism: Historical ground-truth passages from relevant turns serve as pseudo-positives, while irrelevant turns provide hard negatives, creating more informative contrastive pairs
- Core assumption: Historical passages relevant to current query share semantic similarity despite not directly answering it
- Evidence anchors:
  - [abstract] "automatic mining of supervision signals from historical ground-truth passages"
  - [section 3.5] "P + h contains historical passages from the historical turns that are deemed relevant to qn"
  - [corpus] Weak - no corpus evidence found supporting this specific mechanism
- Break condition: Historical passages from relevant turns are topically similar but semantically distant from current query

### Mechanism 3
- Claim: Historical supervision signals are particularly effective for long conversations with topic shifts
- Mechanism: In topic-shift scenarios, traditional methods over-rely on all history while our method selectively uses relevant history, reducing noise impact
- Core assumption: Topic shifts create situations where most historical turns are irrelevant, making selective history use crucial
- Evidence anchors:
  - [abstract] "especially for long conversations with topic shifts"
  - [section 4.3] "improvements achieved over Conv-ANCE serve as additional validation of the effectiveness of exploiting supplementary supervision signals"
  - [corpus] Weak - no corpus evidence found supporting this specific mechanism
- Break condition: Conversations without topic shifts where most historical turns are relevant to current query

## Foundational Learning

- Concept: Pseudo Relevance Feedback (PRF)
  - Why needed here: Provides theoretical foundation for using top-retrieved documents to improve query representation
  - Quick check question: Why would top-retrieved documents from an initial query often be relevant?

- Concept: Contrastive Learning
  - Why needed here: Core training framework for dense retrieval that requires positive/negative pairs
  - Quick check question: How does contrastive learning distinguish between relevant and irrelevant passages?

- Concept: Query Reformulation
  - Why needed here: Transforms conversational queries into standalone queries suitable for retrieval
  - Quick check question: What makes conversational queries different from ad-hoc queries in terms of information need expression?

## Architecture Onboarding

- Component map:
  Pseudo Relevance Judgment module (Algorithm 1) -> Context-Denoised Query Reformulator (Eq. 2) -> History-Aware Contrastive Learner (Eq. 4) -> Dense Retriever backbone (ANCE)

- Critical path:
  1. Generate PRJs for historical turns
  2. Reformulate query using relevant history
  3. Mine positive/negative supervision signals
  4. Train with history-aware contrastive loss

- Design tradeoffs:
  - Using all history vs. selective history: simplicity vs. denoising
  - Hard negatives from history vs. BM25: relevance vs. diversity
  - Pseudo-positives vs. ground-truth only: supervision quantity vs. quality

- Failure signatures:
  - Poor MRR improvement despite better NDCG@3: reformulator adding noise
  - Degradation on QReCC vs. TopiOCQA: method over-optimized for topic shifts
  - Training instability: hard negative mining creating too difficult optimization

- First 3 experiments:
  1. Ablation: Remove historical pseudo-positives to measure their impact
  2. Ablation: Remove historical hard negatives to measure their impact
  3. Adaptation: Substitute top-k retrieved passages for ground-truth passages to test generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of HAConvDR vary with different amounts of historical ground-truth passages available during training?
- Basis in paper: [explicit] The paper mentions that historical ground-truth passages can be substituted with top-retrieved passages in real-world applications where ground-truth passages are difficult to obtain. It also presents an ablation study on the impact of substituting historical ground-truth passages with top-retrieved passages using different values of k.
- Why unresolved: The paper does not provide a comprehensive analysis of how the effectiveness of HAConvDR changes as the amount of historical ground-truth passages increases or decreases.
- What evidence would resolve it: Experiments comparing the performance of HAConvDR with varying amounts of historical ground-truth passages available during training, including scenarios where all historical ground-truth passages are available, some are available, or none are available.

### Open Question 2
- Question: How does the performance of HAConvDR compare to other state-of-the-art conversational dense retrieval methods on datasets with different characteristics, such as longer conversations or conversations with more frequent topic shifts?
- Basis in paper: [explicit] The paper mentions that HAConvDR achieves better performance on TopiOCQA, which contains more topic-switch phenomena and longer conversations compared to QReCC. It also states that the improvements are more pronounced on TopiOCQA.
- Why unresolved: The paper does not provide a comprehensive comparison of HAConvDR with other state-of-the-art methods on a wider range of datasets with varying characteristics.
- What evidence would resolve it: Experiments comparing the performance of HAConvDR with other state-of-the-art methods on multiple conversational search datasets with different characteristics, such as conversation length, topic shift frequency, and query complexity.

### Open Question 3
- Question: How does the performance of HAConvDR vary with different hyperparameters, such as the number of hard negatives, the batch size, or the learning rate?
- Basis in paper: [explicit] The paper mentions that the number of mined positives and negatives from historical turns can vary across different query turns and that the model uses one historical pseudo positive and one historical hard negative for each training instance to strike a balance between effectiveness and efficiency. It also mentions that the model uses Adam optimizer with a 3e-5 learning rate and a batch size of 32.
- Why unresolved: The paper does not provide a comprehensive analysis of how the performance of HAConvDR changes with different hyperparameter settings.
- What evidence would resolve it: Experiments exploring the impact of different hyperparameter settings on the performance of HAConvDR, including the number of hard negatives, batch size, learning rate, and other relevant hyperparameters.

## Limitations
- Performance on conversations without topic shifts remains unclear
- Impact of noisy pseudo relevance judgments on final retrieval quality is not quantified
- Scalability of mining supervision signals for very long conversations is untested

## Confidence
- Pseudo relevance judgment mechanism: High
- Effectiveness of historical supervision signals: Medium
- Claims about performance on long conversations with topic shifts: Low

## Next Checks
1. **Cross-dataset generalization test**: Apply HAConvDR to additional conversational datasets (e.g., TREC CAsT) to verify robustness beyond TopiOCQA and QReCC
2. **Noise sensitivity analysis**: Systematically vary the quality of pseudo relevance judgments and measure degradation in retrieval performance
3. **Ablation on conversation length**: Compare performance across different conversation lengths to identify at what point historical supervision signals become detrimental rather than beneficial