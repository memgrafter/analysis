---
ver: rpa2
title: Large Scale Knowledge Washing
arxiv_id: '2405.16720'
source_url: https://arxiv.org/abs/2405.16720
tags:
- knowledge
- should
- reasoning
- answer
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of unlearning large amounts of factual
  knowledge from large language models without harming their reasoning abilities.
  Existing unlearning methods often require extensive model updates or downstream
  task awareness, risking performance degradation.
---

# Large Scale Knowledge Washing

## Quick Facts
- arXiv ID: 2405.16720
- Source URL: https://arxiv.org/abs/2405.16720
- Reference count: 40
- Primary result: Method removes unwanted factual knowledge from large language models while preserving reasoning abilities

## Executive Summary
This paper introduces LAW (Large Scale Washing), a method for unlearning large amounts of factual knowledge from large language models without compromising their reasoning capabilities. The approach targets MLP layers in decoder-only models, which are known to store factual knowledge, using a novel optimization objective that removes unwanted knowledge while preserving reasoning skills. Experiments demonstrate that LAW achieves cleaner knowledge removal with less impact on reasoning tasks compared to baseline unlearning methods, while supporting scalable implementation and maintaining model robustness.

## Method Summary
LAW (Large Scale Washing) addresses the challenge of removing unwanted factual knowledge from large language models while preserving reasoning abilities. The method specifically targets the MLP (Multi-Layer Perceptron) layers in decoder-only architectures, which are known to store factual knowledge. LAW employs a novel optimization objective that focuses on updating a subset of parameters to remove unwanted knowledge. This selective parameter optimization allows for cleaner knowledge removal while minimizing disruption to the model's reasoning capabilities. The approach is designed to be scalable and robust, making it suitable for large-scale knowledge washing applications.

## Key Results
- LAW achieves cleaner knowledge removal compared to baseline unlearning methods
- Less impact on reasoning tasks while removing unwanted factual knowledge
- Supports scalable knowledge washing and maintains model robustness

## Why This Works (Mechanism)
The method works by targeting MLP layers in decoder-only models, which are identified as the primary storage locations for factual knowledge. By focusing optimization efforts on these specific layers and using a novel objective function, LAW can selectively remove unwanted knowledge while minimizing disruption to reasoning capabilities. The selective parameter optimization allows for more precise control over what knowledge is removed, avoiding the broad performance degradation often seen in traditional unlearning approaches.

## Foundational Learning
- **MLP layers in decoder-only models**: Why needed - These layers store factual knowledge that needs to be removed. Quick check - Verify that MLP parameters show higher sensitivity to factual knowledge removal.
- **Selective parameter optimization**: Why needed - To minimize impact on reasoning while removing unwanted knowledge. Quick check - Compare performance degradation between selective and full parameter updates.
- **Novel optimization objective**: Why needed - To guide the unlearning process toward cleaner knowledge removal. Quick check - Measure knowledge removal effectiveness across different objective formulations.
- **Knowledge storage mechanisms in LLMs**: Why needed - Understanding where knowledge is stored guides targeted removal. Quick check - Map knowledge distribution across model layers.

## Architecture Onboarding

Component map: Input -> Embedding Layer -> Transformer Blocks (MLP layers targeted) -> Output

Critical path: The MLP layers within transformer blocks are the critical path for knowledge storage and removal.

Design tradeoffs: Selective parameter optimization vs. full model updates; targeted knowledge removal vs. broad performance impact.

Failure signatures: Loss of reasoning abilities when optimization is too aggressive; incomplete knowledge removal when optimization is too conservative.

First experiments:
1. Test knowledge removal effectiveness on controlled datasets with known facts
2. Evaluate reasoning task performance before and after unlearning
3. Measure scalability by applying method to progressively larger models

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on controlled datasets and may not capture real-world complexities
- Limited discussion of potential side effects on model behavior beyond tested reasoning tasks
- Scalability claims require further validation with larger models and more extensive knowledge sets

## Confidence

High confidence:
- MLP layers store factual knowledge and can be targeted for unlearning
- Basic premise of selective parameter optimization is well-supported

Medium confidence:
- Claims of cleaner knowledge removal with less impact on reasoning tasks
- Effectiveness across diverse reasoning benchmarks needs broader validation

Low confidence:
- Scalability claims across different model sizes
- Robustness assertions in real-world scenarios

## Next Checks

1. Test the method across multiple reasoning task types (mathematical reasoning, commonsense reasoning, logical inference) to verify consistent preservation of reasoning abilities.

2. Evaluate the method on larger language models (30B+ parameters) and more extensive knowledge sets to validate scalability claims.

3. Conduct adversarial testing to assess whether unlearned knowledge can be reconstructed through carefully crafted prompts or fine-tuning procedures.