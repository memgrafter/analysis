---
ver: rpa2
title: 'DART: Disentanglement of Accent and Speaker Representation in Multispeaker
  Text-to-Speech'
arxiv_id: '2410.13342'
source_url: https://arxiv.org/abs/2410.13342
tags:
- accent
- speaker
- speech
- dart
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DART, a novel approach to disentangle speaker
  and accent representations in multispeaker text-to-speech (TTS) systems. DART combines
  Multi-Level Variational Autoencoders (ML-VAE) and Vector Quantization (VQ) to improve
  flexibility and personalization in speech synthesis.
---

# DART: Disentanglement of Accent and Speaker Representation in Multispeaker Text-to-Speech

## Quick Facts
- arXiv ID: 2410.13342
- Source URL: https://arxiv.org/abs/2410.13342
- Authors: Jan Melechovsky; Ambuj Mehrish; Berrak Sisman; Dorien Herremans
- Reference count: 40
- One-line primary result: Combines ML-VAE and VQ to disentangle speaker and accent in TTS, achieving superior accent conversion and speaker similarity.

## Executive Summary
This paper proposes DART, a novel approach to disentangle speaker and accent representations in multispeaker text-to-speech systems using multi-level variational autoencoders (ML-VAE) and vector quantization (VQ). The method learns hierarchical latent variables for speaker and accent, then discretizes them via VQ to enforce separation. Evaluated on accented speech data, DART shows improved accent conversion and speaker/accent disentanglement compared to baselines, with the DART512 variant (512-codebook) performing best in accent conversion and speaker similarity.

## Method Summary
DART combines a pre-trained FastSpeech2 TTS backbone with ML-VAE and VQ modules to disentangle speaker and accent representations. The model first pre-trains on LibriTTS to learn robust speaker modeling, then fine-tunes on accented data (L2-ARCTIC) with ML-VAE learning speaker (zs) and accent (zg_a) latents, which are discretized via VQ. The VQ bottleneck promotes separation by mapping continuous latents to discrete codebooks, while KL loss and commitment loss stabilize training.

## Key Results
- DART achieves better objective metrics (MCD, CS, FFE, WER) compared to baselines
- Subjective evaluation shows DART outperforms baselines in naturalness and accent/speaker similarity
- DART512, with codebook size 512, performs best in accent conversion and speaker similarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ML-VAE + VQ combination enables disentanglement by learning hierarchical latents and discretizing them.
- Core assumption: Speaker and accent variations are sufficiently distinct to be encoded into separate latents and separated by VQ.
- Evidence: VQ discretizes continuous latents to discrete codebooks, acting as bottleneck to promote separation (section 2.3).
- Break condition: If accent and speaker are too entangled, VQ may not cleanly separate them.

### Mechanism 2
- Claim: Pre-training TTS backbone on multi-speaker corpus improves accent conversion by providing robust speaker modeling foundation.
- Core assumption: Strong independent speaker representation helps accent conversion modules focus solely on accent changes.
- Evidence: DART 512 leverages pre-trained multi-speaker TTS; DART scratch achieves best overall objective metrics while DART 512 excels in accent conversion (section 3.3).
- Break condition: If pretraining corpus lacks accent diversity, generalization to foreign accents may suffer.

### Mechanism 3
- Claim: VQ bottleneck enforces discrete, separable codebooks that reduce information leakage between speaker and accent embeddings.
- Core assumption: Discretization constrains continuous embeddings enough to promote separation without harming reconstruction.
- Evidence: VQ layers map continuous latents to discrete vectors, acting as bottleneck filtering irrelevant information (section 2.3).
- Break condition: If codebook size too small, lose expressive power; too large, separation benefit diminishes.

## Foundational Learning

- **Concept**: Variational Autoencoders (VAEs) and ELBO
  - Why needed: ML-VAE is core architecture for learning disentangled latents. Understanding ELBO trade-off is essential.
  - Quick check: What does maximizing ELBO achieve in latent space learning, and how does β coefficient influence this trade-off?

- **Concept**: Vector Quantization (VQ) and codebooks
  - Why needed: VQ discretizes continuous latents, enforcing discrete representation space that promotes separation.
  - Quick check: How does commitment loss stabilize VQ training, and why is stop-gradient operation critical?

- **Concept**: Speaker verification embeddings (GE2E)
  - Why needed: Backbone uses pre-trained speaker embeddings for initial speaker modeling.
  - Quick check: How does GE2E loss differ from cross-entropy speaker classification loss in embedding quality for TTS?

## Architecture Onboarding

- **Component map**: Text encoder → Variance Adapter → Mel decoder; ML-VAE encoder branch: phoneme encoder → speaker/accent latent space → VQ bottleneck; Pre-trained speaker embeddings added in variance adapter.

- **Critical path**: 1) Input text → phoneme embeddings; 2) Speaker/accent embeddings (ML-VAE + VQ) merged into variance adapter; 3) Decoder generates mel spectrogram; 4) Reconstruction loss computed against ground truth; 5) ML-VAE KL loss and VQ commitment loss backpropagated.

- **Design tradeoffs**: VQ codebook size (larger → better separation but risk underfitting; smaller → more expressive but risk leakage); Pre-training vs scratch (pre-training helps accent conversion but may reduce speaker identity fidelity); β coefficient in KL loss (higher → stronger disentanglement but may hurt reconstruction).

- **Failure signatures**: High MCD + low CS → poor speaker identity preservation; Low MCD + low accent similarity → model favors speaker over accent; Unstable training → VQ commitment loss oscillating.

- **First 3 experiments**: 1) Train ML-VAE without VQ to observe baseline separation and quantify leakage; 2) Vary β in KL loss to see effect on accent vs speaker similarity trade-off; 3) Compare pre-training on multi-speaker corpus vs training from scratch on accented data to quantify pretraining benefit.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does VQ codebook size affect trade-off between accent conversion quality and speaker similarity in DART?
  - Basis: Paper explores codebook sizes (64, 128, 512) showing different performance patterns but lacks detailed analysis of underlying mechanisms.
  - Why unresolved: No comprehensive study examining relationship between codebook size, accent conversion accuracy, and speaker similarity across various combinations.
  - What evidence would resolve it: Comprehensive study examining codebook size impact on accent conversion accuracy and speaker similarity across various speaker-accent combinations.

- **Open Question 2**: Can DART be extended to handle zero-shot accent conversion where target accent is not seen during training?
  - Basis: DART512's prior knowledge from pre-training aids accent conversion, but zero-shot performance is not explored.
  - Why unresolved: No experiments or discussion of model's performance on unseen accents.
  - What evidence would resolve it: Experiments evaluating DART's performance on converting to accents not present in training data, compared to other zero-shot methods.

- **Open Question 3**: How does DART compare to other disentanglement methods like adversarial learning or information bottleneck approaches?
  - Basis: Paper compares DART to specific baselines but not to other disentanglement techniques like adversarial learning.
  - Why unresolved: Focus on specific baseline models without exploring performance relative to other disentanglement strategies.
  - What evidence would resolve it: Comparative study evaluating DART against other disentanglement methods using same datasets and metrics.

## Limitations

- Assumption that speaker and accent attributes are sufficiently distinct in speech signal to be encoded into separate latent variables is not conclusively proven without ablation studies isolating VQ bottleneck contribution.
- Results are limited to specific accent conversion task (American English to various accents) and dataset (L2-ARCTIC), limiting generalizability to other accent pairs or languages.
- Pre-training strategy shows improved accent conversion but subjective evaluation shows DART scratch performing better in naturalness, suggesting unexplored trade-off between speaker identity preservation and accent conversion quality.

## Confidence

- **High Confidence**: Core methodology combining ML-VAE and VQ is technically sound; objective metrics demonstrate clear improvements over baselines; architecture details and training procedure sufficiently specified for reproduction.
- **Medium Confidence**: Claim that VQ is critical for disentanglement is plausible but not conclusively proven without ablation studies; subjective evaluation results showing DART 512's superiority in accent conversion are supported by data but may be influenced by specific dataset and evaluation methodology.
- **Low Confidence**: Generalizability of results to other accent pairs or languages is not established; paper does not explore potential failure modes when accent and speaker attributes are more entangled than in L2-ARCTIC dataset.

## Next Checks

1. **Ablation Study**: Train version of DART without VQ bottleneck to quantify its contribution to accent/speaker separation. Compare disentanglement quality (CS scores) and reconstruction quality (MCD) between VQ and non-VQ versions.

2. **Cross-Accent Generalization**: Evaluate DART on different accent conversion task (e.g., British English to American English or Spanish-accented English) to test model's generalizability beyond L2-ARCTIC dataset.

3. **VQ Codebook Size Sensitivity**: Systematically vary VQ codebook size (64, 128, 256, 512) and measure trade-off between disentanglement quality (CS scores) and reconstruction quality (MCD). Identify optimal codebook size balancing separation and expressive power.