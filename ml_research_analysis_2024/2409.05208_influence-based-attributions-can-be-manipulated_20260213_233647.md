---
ver: rpa2
title: Influence-based Attributions can be Manipulated
arxiv_id: '2409.05208'
source_url: https://arxiv.org/abs/2409.05208
tags:
- influence
- attack
- data
- ztest
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that influence functions used for data
  attribution in machine learning can be systematically manipulated by an adversary.
  The authors formalize a threat model where an adversary controls the model training
  process and aims to manipulate influence scores while maintaining model accuracy.
---

# Influence-based Attributions can be Manipulated

## Quick Facts
- arXiv ID: 2409.05208
- Source URL: https://arxiv.org/abs/2409.05208
- Authors: Chhavi Yadav; Ruihan Wu; Kamalika Chaudhuri
- Reference count: 40
- An adversary can systematically manipulate influence functions used for data attribution while maintaining model accuracy

## Executive Summary
This paper demonstrates that influence functions used for data attribution in machine learning can be systematically manipulated by an adversary who controls the model training process. The authors formalize a threat model where an adversary aims to manipulate influence scores while maintaining model accuracy. They propose two attack strategies: targeted attacks for data valuation applications that can boost specific training samples' influence scores with up to 94% success rate, and untargeted attacks for fairness applications where scaling model weights reduces fairness of downstream models by up to 16% in demographic parity gap. The paper also provides an impossibility theorem showing certain samples' influence scores cannot be manipulated regardless of the model.

## Method Summary
The authors propose two attack strategies to manipulate influence-based attribution methods. For targeted attacks (data valuation), they train a malicious model that maintains similar test accuracy to the honest model but produces desired influence scores through optimization of a carefully constructed attack objective. They address computational challenges by linearizing the objective and making it backward-friendly to avoid expensive Hessian-Inverse-Vector Products. For untargeted attacks (fairness), they exploit logistic regression properties by scaling model weights, which preserves prediction signs while manipulating influence scores. The attacks are evaluated on three image datasets (CIFAR10, Oxford-IIIT Pet, Caltech-101) and three fairness datasets (Adult Credit, German Credit, Compas) using logistic regression models trained on ResNet features.

## Key Results
- Targeted attacks achieve up to 94% success rate in boosting specific training samples' influence scores with minimal accuracy drop
- Untargeted attacks reduce fairness of downstream models by up to 16% in demographic parity gap through simple weight scaling
- The paper proves an impossibility theorem showing certain training samples cannot be manipulated regardless of model
- Both attack strategies maintain test accuracy close to honest models (within 3% accuracy difference)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Influence functions can be systematically manipulated by an adversary who controls the model training process.
- Mechanism: The adversary trains a malicious model θ′ that maintains similar test accuracy to the honest model θ⋆ but produces desired influence scores by optimizing a carefully constructed attack objective.
- Core assumption: There exist models that behave very similarly in terms of test accuracy but have different influential training samples.
- Evidence anchors:
  - [abstract] "we show that it is indeed possible to systematically train a malicious model very similar to the honest model in test accuracy but has desired influence scores"
  - [section] "The adversarial model training can change model parameters from θ∗ to θ′ while maintaining similar test accuracy"
  - [corpus] "Efficiently Attacking Memorization Scores" - related work on attacking influence-based metrics
- Break condition: If the Rashomon Effect is minimal, meaning few models achieve similar accuracy, manipulation becomes difficult or impossible.

### Mechanism 2
- Claim: The backward pass through influence-based objectives can be made computationally feasible using a rewriting technique.
- Mechanism: The attack objective is linearized and transformed into a backward-friendly form that avoids Hessian-Inverse-Vector Products (HIVPs), enabling gradient computation using standard PyTorch machinery.
- Core assumption: The linearized objective can be rewritten such that gradient computations avoid expensive HIVPs while preserving objective and gradient values.
- Evidence anchors:
  - [abstract] "We address this challenge by proposing a memory-time efficient and backward-friendly algorithm to compute the gradients"
  - [section] "Our idea of rewriting the attack objective involves two essential steps: (1) linearizing the objective (2) making the linearized objective backward-friendly in PyTorch"
  - [corpus] "Scaling up influence functions" - related work on making forward computation efficient
- Break condition: If the linearization approximation introduces too much error, the attack may fail to achieve desired influence scores.

### Mechanism 3
- Claim: For fairness applications, scaling model weights is sufficient to manipulate influence scores while maintaining accuracy.
- Mechanism: By scaling the base model θ⋆ by a constant λ > 0 to create θ′ = λ · θ⋆, the adversary can manipulate influence scores because logistic regression predictions remain unchanged under positive scaling.
- Core assumption: For logistic regression, scaling weights by a positive constant preserves prediction signs, making the scaled and original models indistinguishable in accuracy.
- Evidence anchors:
  - [abstract] "For the fairness application and unlike targeted attacks, manipulate influence scores arbitrarily without targeting specific samples. We find that surprisingly enough scaling model weights is a good enough strategy"
  - [section] "Our attack is deceptively simple – scale the base model θ⋆ by a constant λ > 0. The malicious base model output by the model trainer is now θ′ = λ · θ⋆, instead of θ⋆. Note that for logistic regression the malicious and original base model are indistinguishable since scaling with a positive constant maintains the sign of the predictions"
  - [corpus] Weak - this specific mechanism is not well-covered in related work
- Break condition: If the fairness pipeline uses metrics sensitive to model scaling beyond just prediction signs, the attack may be detectable.

## Foundational Learning

- Concept: Influence Functions as approximations of training sample impact on test loss
  - Why needed here: Understanding the mathematical foundation is essential for grasping how the attack manipulates these functions
  - Quick check question: What does the influence function Iθ⋆(z, ztest) measure in terms of training sample z and test point ztest?

- Concept: Hessian-Inverse-Vector Products (HIVPs) and their computational complexity
  - Why needed here: The attack's efficiency relies on avoiding expensive HIVPs in the backward pass
  - Quick check question: Why are HIVPs computationally expensive and what is the time complexity typically?

- Concept: Logistic regression properties under parameter scaling
  - Why needed here: The fairness attack exploits the fact that logistic regression predictions are invariant to positive scaling of weights
  - Quick check question: If θ is a logistic regression parameter vector, what happens to predictions when we use λθ for λ > 0?

## Architecture Onboarding

- Component map:
  Data Provider -> Model Trainer -> Influence Calculator -> Downstream Application

- Critical path:
  1. Data Provider → Model Trainer (training data)
  2. Model Trainer → Influence Calculator (trained model)
  3. Influence Calculator → Downstream Application (influence scores)
  4. Downstream Application → Fairness Metric or Data Valuation (final outcome)

- Design tradeoffs:
  - Attack strength vs. accuracy preservation: Larger manipulation radius allows stronger attacks but may reduce accuracy
  - Computational efficiency vs. attack success: More sophisticated optimization may yield better results but requires more resources
  - Targeted vs. untargeted attacks: Targeted attacks require more computation but achieve specific goals, while untargeted attacks are simpler but less precise

- Failure signatures:
  - High accuracy difference between honest and malicious models (>3%)
  - Transfer failure: Attack success on original test set doesn't transfer to new test sets
  - Impossibility cases: Certain training samples cannot be manipulated regardless of model

- First 3 experiments:
  1. Implement the Single-Target Attack on CIFAR10 with ranking k=1 and observe success rate vs. accuracy drop
  2. Test the scaling attack on German Credit dataset and measure demographic parity gap changes
  3. Verify the impossibility theorem construction on a simple 2D logistic regression problem

## Open Questions the Paper Calls Out
- Can influence functions be made more robust against adversarial manipulation through cryptographic verification methods?
- How do influence manipulation attacks scale to larger neural network architectures beyond logistic regression?
- Are there inherent properties of certain training samples that make them universally resistant to influence manipulation across all models?
- Can influence-based attribution methods be redesigned to be inherently resistant to the manipulation strategies described in this paper?

## Limitations
- The paper demonstrates attacks only on logistic regression models, not on deeper neural networks
- The impossibility theorem is proven only for specific logistic regression constructions, not general model families
- Practical implications for real-world systems using influence-based attribution are not fully explored

## Confidence
- High confidence: The core claim that influence functions can be manipulated by controlling model training is well-supported by experimental results showing high success rates (>94%) with minimal accuracy degradation
- Medium confidence: The backward-friendly algorithm for efficient gradient computation is theoretically sound, but implementation details and practical efficiency gains are not fully validated
- Medium confidence: The scaling attack for fairness applications is elegant but may not generalize to all fairness metrics or model types beyond logistic regression

## Next Checks
1. Evaluate whether attacks optimized on one test set maintain effectiveness on held-out test sets to assess robustness of the manipulation strategy
2. Test the attack effectiveness on larger models (e.g., deeper neural networks) and datasets to understand practical limitations
3. Develop and test simple statistical tests that could detect the presence of manipulated influence scores to understand the security implications