---
ver: rpa2
title: Learning Mathematical Rules with Large Language Models
arxiv_id: '2410.16973'
source_url: https://arxiv.org/abs/2410.16973
tags:
- mathematical
- distributivity
- equation
- expression
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the ability of large language models to learn
  and apply specific mathematical rules such as distributivity, equation solving,
  and simplification. The authors introduce a rigorous methodology to generate synthetic
  data incorporating these rules, and fine-tune models on such data.
---

# Learning Mathematical Rules with Large Language Models

## Quick Facts
- arXiv ID: 2410.16973
- Source URL: https://arxiv.org/abs/2410.16973
- Reference count: 40
- Models can learn mathematical rules through synthetic data fine-tuning and apply them to solve word problems while maintaining general performance

## Executive Summary
This paper investigates whether large language models can learn specific mathematical rules (distributivity, equation solving, simplification) through synthetic data fine-tuning and then apply these rules to solve word problems. The authors develop a rigorous methodology using abstract syntax trees (ASTs) to generate controlled training data, fine-tune Llama models on these rules, and evaluate both rule application and generalization to word problems. Results show that fine-tuned models can successfully apply learned rules to solve word problems, though limitations exist due to differences in equation formats between training and test data. The study also demonstrates that models can generalize rules like distributivity to unseen variable names when trained on larger vocabularies, with minimal catastrophic forgetting on general benchmarks.

## Method Summary
The methodology involves generating synthetic data using ASTs with controlled simplification rules, fine-tuning Llama-2 7B and Llama-3 8B models on this data, and evaluating performance on word problems and mathematical benchmarks. The synthetic data is constructed to reflect specific mathematical rules without word problems, ensuring controlled training conditions. Fine-tuning uses QLoRA with low-rank adaptation, and evaluation employs SymPy-based correctness checking. The approach addresses bottom-up generalization (applying learned rules to word problems) and top-down generalization (generalizing rules to more complex problems), with special attention to avoiding catastrophic forgetting.

## Key Results
- Fine-tuned Llama-2 model achieves 88.5% accuracy on step-by-step Gaussian elimination
- Models successfully apply learned rules to solve word problems involving resistors, fruit baskets, and quadratic polynomials
- Training on larger vocabulary sizes improves generalization to unseen variable names for distributivity rule
- Performance on general benchmarks remains stable after fine-tuning, indicating minimal catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic data construction using ASTs with controlled simplification rules enables precise targeting of specific mathematical skills.
- Mechanism: By building expressions as ASTs with explicit operator precedence and associativity, the authors control how terms are grouped and simplified. This ensures training examples are presented in a consistent, textbook-like format without word problems.
- Core assumption: The model learns generalizable patterns from well-structured symbolic expressions rather than relying on surface form memorization.
- Evidence anchors:
  - [section B] "We overload our programming language's arithmetic operators...This custom data structure allows us to control precise simplification details such as the order of terms, whether to evaluate numerical expressions, etc."
  - [section 3] "We provide a rigorous methodology to build synthetic data incorporating such rules...perform fine-tuning of large language models on such data."
- Break condition: If the model learns spurious correlations from the synthetic data's format rather than the underlying mathematical rules, performance on word problems with different equation formats would degrade.

### Mechanism 2
- Claim: Fine-tuning on synthetic data for specific rules improves bottom-up generalization to word problems while maintaining performance on general benchmarks.
- Mechanism: The synthetic data teaches the model specific mathematical operations (like solving quadratic equations or simplifying expressions). When applied to word problems, the model first translates the problem into equations using existing knowledge, then applies the newly learned rules to solve them.
- Core assumption: The skills learned from synthetic data transfer to real-world problem-solving contexts when combined with the model's existing reasoning abilities.
- Evidence anchors:
  - [section 3] "Our experiments show that our model can learn and generalize these rules to some extent, as well as suitably reuse them in the context of word problems."
  - [section C.4] "The performance on these benchmarks remains relatively stable after fine-tuning, demonstrating that the newly acquired skills did not disrupt the model's prior knowledge."
- Break condition: If catastrophic forgetting occurs or if the synthetic data format is too dissimilar from word problem formats, the transfer would fail.

### Mechanism 3
- Claim: Training on larger vocabulary sizes improves the model's ability to generalize mathematical rules to unseen variable names and complex expressions.
- Mechanism: When variables are drawn from larger subsets of the tokenizer's vocabulary, the model learns to focus on the structural properties of mathematical operations rather than memorizing specific token patterns. This enables better generalization to new variable names and more complex expressions.
- Core assumption: The model's attention mechanisms can learn abstract mathematical relationships that are invariant to the specific tokens used as variables.
- Evidence anchors:
  - [section D.1] "We find that training on larger vocabulary sizes improves the ability of the model to generalize distributivity to unseen variable names as well as to increasing the number of variables."
  - [section D.1] "The model's performance on the training vocabulary is stable across different vocabulary sizes, and significantly higher than the performance on the complement vocabulary."
- Break condition: If the model still relies on token-level memorization even with large vocabularies, or if the tokenizer's vocabulary structure interferes with learning mathematical abstractions.

## Foundational Learning

- Concept: Abstract Syntax Trees (ASTs) and their role in representing mathematical expressions
  - Why needed here: The paper's methodology relies on constructing mathematical expressions as ASTs to control simplification rules and ensure consistent formatting for training
  - Quick check question: What are the advantages of representing mathematical expressions as ASTs versus plain text strings for training purposes?

- Concept: Catastrophic forgetting in fine-tuning scenarios
  - Why needed here: The paper specifically addresses maintaining performance on general benchmarks while fine-tuning on specialized mathematical tasks, which is a classic catastrophic forgetting concern
  - Quick check question: What are the primary mechanisms by which fine-tuning on new tasks can degrade performance on previously learned tasks?

- Concept: Symbolic mathematics libraries and their limitations for controlled training data generation
  - Why needed here: The paper explicitly chooses to implement custom AST structures instead of using libraries like SymPy to maintain control over simplification rules and expression formatting
  - Quick check question: Why might using a symbolic mathematics library like SymPy be insufficient for generating controlled training data for fine-tuning LLMs?

## Architecture Onboarding

- Component map: Data generation (AST builder with simplification rules) -> Fine-tuning (QLoRA with low-rank adaptation) -> Evaluation (SymPy-based correctness checking) -> Recursive solving mechanism for systems of equations
- Critical path: Data generation → Fine-tuning (with regularizers) → Evaluation on word problems and benchmarks → Analysis of generalization capabilities
- Design tradeoffs: The choice between using synthetic data with controlled formats versus real-world examples, and between vocabulary-restricted versus full-vocabulary training approaches for different generalization goals
- Failure signatures: Models that correctly translate word problems to equations but fail to apply learned rules, models that overfit to synthetic data formats, or models that show catastrophic forgetting on general benchmarks
- First 3 experiments:
  1. Generate synthetic data for a simple rule (like distributivity) and fine-tune a small model to verify the data generation pipeline works
  2. Test the evaluation pipeline on model outputs to ensure the SymPy-based correctness checking functions properly
  3. Run a minimal bottom-up generalization experiment with one word problem type to verify the transfer from synthetic data to real-world applications

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned models on mathematical rules compare when evaluated on tasks requiring multi-step reasoning versus single-step transformations?
- Basis in paper: [explicit] The paper mentions that models can successfully apply learned rules to solve word problems, but performance varies depending on the complexity of the task (e.g., resistors in parallel problems).
- Why unresolved: While the paper demonstrates success in single-step transformations (e.g., simplifying expressions, isolating variables), it does not provide a detailed comparison of performance on multi-step reasoning tasks versus single-step tasks. The paper also notes limitations in generalization due to differences in equation formats between training and test data.
- What evidence would resolve it: Conducting experiments that explicitly compare model performance on multi-step reasoning tasks (e.g., solving systems of equations, combining multiple mathematical rules) versus single-step transformations, with a focus on identifying bottlenecks in reasoning complexity.

### Open Question 2
- Question: What is the impact of varying the size and diversity of the training vocabulary on the model's ability to generalize mathematical rules to unseen variable names and more complex problems?
- Basis in paper: [explicit] The paper discusses the impact of training on larger vocabulary sizes for the distributivity rule, finding that it improves generalization to unseen variable names and increasing the number of variables.
- Why unresolved: While the paper provides evidence for the impact of vocabulary size on distributivity, it does not explore this systematically across all mathematical rules or investigate the optimal vocabulary size for generalization.
- What evidence would resolve it: Conducting experiments that systematically vary the training vocabulary size and diversity for all mathematical rules, and evaluating the impact on generalization performance across different problem complexities.

### Open Question 3
- Question: How does the model's performance on mathematical reasoning tasks change when evaluated on benchmarks with different levels of linguistic complexity (e.g., simple word problems vs. complex word problems with nested clauses)?
- Basis in paper: [inferred] The paper evaluates models on word problems involving resistors, fruit baskets, and quadratic polynomials, but does not explicitly analyze the impact of linguistic complexity on performance.
- Why unresolved: The paper does not provide a detailed analysis of how linguistic complexity in word problems affects the model's ability to translate natural language into equations and apply mathematical rules.
- What evidence would resolve it: Designing experiments that evaluate model performance on word problems with varying levels of linguistic complexity, and analyzing the correlation between linguistic complexity and mathematical reasoning accuracy.

## Limitations

- Synthetic data generation through ASTs may not capture all real-world mathematical expression variations
- Evaluation framework checks final correctness but may not assess intermediate reasoning steps or partial understanding
- Results are limited to small language models (7B and 8B parameters) and specific mathematical rules
- Vocabulary generalization experiments are limited to specific rules like distributivity

## Confidence

*High Confidence:* The paper's methodology for synthetic data generation and the general framework for evaluating mathematical rule learning are well-specified and reproducible. The results showing improved performance on word problems after fine-tuning on synthetic data are robust and clearly demonstrated.

*Medium Confidence:* The claims about vocabulary generalization and the ability to apply learned rules to increasingly complex problems are supported by experiments but may have limited generalizability beyond the specific rules and datasets tested. The assertion that fine-tuning doesn't cause catastrophic forgetting is based on stable benchmark performance but may not capture more subtle forms of knowledge degradation.

*Low Confidence:* The broader implications for mathematical reasoning in LLMs and the potential for applying this methodology to other domains of symbolic reasoning remain speculative and require further validation.

## Next Checks

1. **Intermediate Step Evaluation:** Implement a detailed evaluation framework that checks not just final answer correctness but also intermediate reasoning steps when solving mathematical problems. This would help determine whether models are truly learning mathematical rules or simply memorizing solution patterns.

2. **Cross-Rule Generalization Test:** Design experiments that test whether models fine-tuned on one mathematical rule (e.g., distributivity) can effectively combine this knowledge with other mathematical concepts when solving complex problems, beyond the specific word problems used in the current study.

3. **Real-World Data Validation:** Generate a dataset of mathematical expressions and problems from actual textbooks or educational resources, then test whether models trained on synthetic data can handle the formatting and complexity variations present in real educational materials.