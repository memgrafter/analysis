---
ver: rpa2
title: 'FDC: Fast KV Dimensionality Compression for Efficient LLM Inference'
arxiv_id: '2408.04107'
source_url: https://arxiv.org/abs/2408.04107
tags:
- compression
- time
- average
- matrix
- ratio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of memory constraints in the
  Key-Value Cache (KVC) during large language model (LLM) inference, particularly
  with long prompts. The proposed solution, FDC (Fast KV Dimensionality Compression),
  eliminates the decompression overhead incurred in existing KV dimensionality compression
  systems and reduces attention time.
---

# FDC: Fast KV Dimensionality Compression for Efficient LLM Inference

## Quick Facts
- arXiv ID: 2408.04107
- Source URL: https://arxiv.org/abs/2408.04107
- Authors: Zeyu Zhang; Haiying Shen
- Reference count: 40
- Primary result: Reduces Job Completion Time (JCT) by up to 64% and delivers up to 1.97X throughput under the same latency while maintaining 99% of the accuracy without compression

## Executive Summary
This paper addresses the critical memory constraints in Key-Value Cache (KVC) during large language model (LLM) inference, particularly for long prompts. The proposed solution, FDC (Fast KV Dimensionality Compression), introduces an SVD-based zero-delay compression approach that eliminates the decompression overhead of existing systems while reducing attention computation time. FDC adaptively determines compression ratios across different heads and layers based on their contributions to inference, maximizing compression while maintaining accuracy constraints. Comprehensive experiments demonstrate significant improvements in JCT, throughput, and memory efficiency compared to state-of-the-art methods.

## Method Summary
FDC employs SVD-based zero-delay compression to eliminate decompression overhead by embedding compression operations within model operations. The method computes rotation matrices offline using SVD and directly produces compressed query, key, and value matrices during inference. Adaptive hybrid compression ratio determination classifies tokens as important or unimportant based on attention scores, applying different compression rates accordingly. The system also enhances the attention kernel to balance uneven workloads caused by adaptive compression. FDC is implemented within a communication-efficient sequence parallelism framework, compressing tensors before all-to-all communication to reduce network overhead. The approach was tested on Meta Llama-2 (13B) using "The Pile" and ShareGPT datasets on AWS p4de.24xlarge instances.

## Key Results
- Reduces Job Completion Time (JCT) by up to 64% compared to baselines
- Delivers up to 1.97X throughput under the same latency constraints
- Maintains 99% of the accuracy without compression while achieving significant memory savings
- Outperforms state-of-the-art eviction and quantization methods when combined with FDC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SVD-based compression eliminates the need for explicit compression and decompression operations during inference.
- Mechanism: By pre-multiplying model weight matrices with rotation matrices derived from SVD offline, compressed query, key, and value matrices are directly produced during the model operation. This embeds compression into the computation pipeline itself.
- Core assumption: The rotation matrices found by SVD preserve the relative relationships between vectors and can be reused for online tokens without significant degradation.
- Evidence anchors:
  - [abstract]: "ZDC innovatively embeds compression and decompression operations within model operations and adaptively determines compression ratios at a hybrid layer-token level."
  - [section]: "We can leverage the associativity inherent in matrix multiplication... Consequently, the compressed matrix can be derived directly from input-parameter multiplication."
  - [corpus]: Weak evidence; only one related paper (Palu) uses low-rank projection, but SVD embedding approach is not covered.

### Mechanism 2
- Claim: Adaptive hybrid compression ratio determination maximizes overall compression while maintaining accuracy.
- Mechanism: Tokens are classified into important and unimportant sets based on attention scores. Different compression ratios are applied to each set, and more tokens are marked as important in deeper layers.
- Core assumption: Shallower layers contain more unimportant tokens and closer layers share similar important/unimportant token sets.
- Evidence anchors:
  - [abstract]: "ZDC employs adaptive compression, tailoring KV compression rates across heads and layers based on their contributions to inference to maximize overall compression while maintaining an accuracy loss constraint."
  - [section]: "Based on our observations that closer model layers share more important and unimportant tokens, and shallower layers contain more unimportant tokens (O3 and O4), to improve the trade-off between the effectiveness of QKV compression and accuracy decrease, ZDC adaptively determines the compression ratios in a hybrid layer-token manner."
  - [corpus]: Weak evidence; only one related paper (Palu) mentions low-rank projection but not adaptive compression by token importance.

### Mechanism 3
- Claim: The communication-efficient sequence parallelism framework reduces network communication overhead.
- Mechanism: By compressing Q, K, and V during the projection operation before the all-to-all communication, less data is transferred between GPUs.
- Core assumption: The compression applied before communication does not significantly impact the attention computation results.
- Evidence anchors:
  - [abstract]: "Additionally, FDC enhances the attention kernel to balance uneven workloads caused by the adaptive compression approach to further reduce attention computation latency."
  - [section]: "Leveraging ZDC, Q, K, and V are compressed during the projection operation when they are calculated. Consequently, transmitting compressed Q, K, and V tensors in the first all-to-all communication significantly reduces communication time overhead."
  - [corpus]: Weak evidence; no related papers cover SP frameworks with integrated compression.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) for dimensionality reduction
  - Why needed here: SVD is used to find rotation matrices that preserve vector relationships while allowing dimension reduction, which is the core of the compression method.
  - Quick check question: How does SVD ensure that the most important information is retained when reducing dimensions?

- Concept: Matrix multiplication associativity
  - Why needed here: The associativity of matrix multiplication is leveraged to embed compression into the model operation itself, eliminating the need for separate compression and decompression steps.
  - Quick check question: What is the mathematical property that allows us to pre-multiply the rotation matrix with the weight matrix offline?

- Concept: Attention mechanism in transformers
  - Why needed here: Understanding the attention mechanism is crucial to see how compressing Q, K, and V affects the self-attention computation and how to decompress V during the post-self-attention linear layer.
  - Quick check question: How are the query, key, and value matrices used in the attention computation, and where does decompression need to occur?

## Architecture Onboarding

- Component map: SVD computation offline → QKV compression during model operation → attention computation on compressed tensors → VWL decompression during post-self-attention linear layer → SP communication with compressed tensors
- Critical path: The critical path involves the sequential execution of SVD-based compression, adaptive ratio determination, attention computation on compressed tensors, and communication-efficient SP operations.
- Design tradeoffs:
  - Accuracy vs. compression ratio: Higher compression ratios save more memory and reduce computation but may degrade model accuracy.
  - Compression overhead vs. decompression overhead: Embedding compression into the model operation eliminates decompression overhead but requires more complex pre-processing.
  - Token importance classification accuracy vs. classification overhead: More accurate classification methods may incur higher overhead.
- Failure signatures:
  - Significant accuracy degradation: The rotation matrices may not preserve vector relationships well, or the token importance classification may be inaccurate.
  - Increased inference latency: The compression/decompression operations may be too computationally expensive, or the SP communication may not be efficient.
  - Memory overflow: The compression ratios may be too low, or the model may be too large for the available memory.
- First 3 experiments:
  1. Verify SVD-based compression accuracy: Compress and decompress a set of Q, K, and V matrices using SVD and measure the reconstruction error.
  2. Test adaptive compression ratio determination: Run the model with different compression ratio assignments and measure the accuracy and memory usage.
  3. Evaluate SP communication efficiency: Run the model with and without compression on a multi-GPU setup and measure the communication time and overall inference latency.

## Open Questions the Paper Calls Out

- Can adaptive compression ratio determination methods be further optimized to achieve even lower perplexity and JCT while maintaining accuracy? The paper acknowledges that a regression model is used but suggests reinforcement learning-based methods as potential alternatives for more accurate determinations.

- How effective is ZDC in mitigating communication overhead for prompts with significantly longer lengths and heavier workloads when using more than 8 servers? The paper only tests scalability with up to 8 servers and acknowledges that more servers may be needed for prompts with longer lengths.

- What is the optimal job scheduling strategy among different servers to maximize overall system performance when using ZDC? The paper currently uses a simple round-robin job scheduling strategy and acknowledges that optimal scheduling strategies need to be investigated.

## Limitations

- The SVD-based compression approach relies on theoretical assertions about matrix multiplication associativity without sufficient empirical validation of rotation matrices' effectiveness in preserving semantic relationships.

- The adaptive compression ratio determination depends on a "lightweight regression model" whose design, training data, and performance characteristics are not specified, creating uncertainty about generalization across different domains.

- Communication efficiency claims assume ideal network conditions and do not address potential bottlenecks when scaling to larger GPU clusters beyond the tested 8-server configuration.

## Confidence

- **High confidence**: The fundamental observation that KV cache memory constraints limit LLM inference performance is well-established. The claim that SVD can find rotation matrices for dimensionality reduction is mathematically sound.

- **Medium confidence**: The integration of compression into model operations through matrix multiplication associativity is theoretically valid but requires empirical validation of its practical effectiveness. The memory savings claims are based on controlled experiments but may not translate directly to all deployment scenarios.

- **Low confidence**: The adaptive compression ratio determination method's effectiveness is not fully validated. The claim that token importance classification based on attention scores accurately identifies which tokens can be compressed more aggressively needs more rigorous testing across diverse datasets and model architectures.

## Next Checks

1. **Rotation matrix validation**: Measure reconstruction error and downstream task performance when applying the SVD-derived rotation matrices to a diverse set of query, key, and value matrices from different layers and attention heads. Compare against baseline low-rank projection methods like Palu.

2. **Adaptive compression generalization**: Test the token importance classification and compression ratio assignment across multiple domains (code, scientific text, dialogue) and prompt lengths to verify that the lightweight regression model maintains accuracy while achieving consistent compression ratios.

3. **Multi-GPU scaling validation**: Evaluate the communication efficiency claims by measuring all-to-all communication times and overall inference latency when scaling from 2 to 8+ GPUs, comparing FDC against both uncompressed and other compression baselines under varying network conditions.