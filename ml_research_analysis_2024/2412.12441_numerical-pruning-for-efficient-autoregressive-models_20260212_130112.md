---
ver: rpa2
title: Numerical Pruning for Efficient Autoregressive Models
arxiv_id: '2412.12441'
source_url: https://arxiv.org/abs/2412.12441
tags:
- arxiv
- song
- pruning
- preprint
- flap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a training-free pruning method for compressing
  decoder-only transformer-based autoregressive models, achieving state-of-the-art
  performance on both language and image generation tasks. The method computes a numerical
  score using Newton's method to identify optimal pruning masks for Attention and
  MLP modules, then applies a compensation algorithm to recover pruned models by updating
  remaining weights.
---

# Numerical Pruning for Efficient Autoregressive Models

## Quick Facts
- arXiv ID: 2412.12441
- Source URL: https://arxiv.org/abs/2412.12441
- Authors: Xuan Shen; Zhao Song; Yufa Zhou; Bo Chen; Jing Liu; Ruiyi Zhang; Ryan A. Rossi; Hao Tan; Tong Yu; Xiang Chen; Yufan Zhou; Tong Sun; Pu Zhao; Yanzhi Wang; Jiuxiang Gu
- Reference count: 40
- Primary result: Training-free pruning method achieving state-of-the-art performance on LLaMA and LlamaGen models

## Executive Summary
This paper introduces a training-free pruning method for compressing decoder-only transformer-based autoregressive models, achieving state-of-the-art performance on both language and image generation tasks. The method computes a numerical score using Newton's method to identify optimal pruning masks for Attention and MLP modules, then applies a compensation algorithm to recover pruned models by updating remaining weights. Experiments demonstrate superior perplexity scores across LLaMA model families (7B, 13B, 70B) and LlamaGen models, with reduced GPU memory usage and faster generation speeds.

## Method Summary
The method introduces a numerical score calculation using Newton's method to determine optimal pruning masks for Attention and MLP modules in transformer models. It employs a global pruning strategy that balances parameter reduction between different modules through scaled ranking, followed by a compensation algorithm that updates remaining weights to minimize reconstruction loss. The approach is training-free and achieves state-of-the-art compression performance on both language and image generation tasks while maintaining model accuracy.

## Key Results
- Achieves superior perplexity scores on LLaMA model families (7B, 13B, 70B) across multiple datasets
- Maintains strong performance even at high pruning ratios (up to 70%) where other methods fail
- Demonstrates effective compression for both language generation and image generation tasks
- Reduces GPU memory usage and generation latency while preserving model quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Numerical score computed via Newton's method minimizes pruning error bound for masked weights
- Mechanism: The method formulates pruning as minimizing the ℓ2 error between original and pruned weights, then uses Newton's method to solve the constrained optimization problem efficiently
- Core assumption: The error bound ∥XW∗,i − X(M ◦ W∗,i)∥2 ≤ ρR∥W∗,i∥2 is tight enough to guide effective pruning decisions
- Evidence anchors:
  - [abstract]: "calculates a numerical score with Newton's method for the Attention and MLP modules"
  - [section 3.2]: "we adopt the Newton's method (Bubeck et al. 2015)" and "efficiently compute the optimal numerical z in O(T D3)"
  - [corpus]: Weak evidence - no direct citations found supporting Newton's method for this specific pruning application

### Mechanism 2
- Claim: Global pruning mask balances parameter reduction between Attention and MLP modules through scaled ranking
- Mechanism: The method aggregates numerical scores across all layers, applies scaling factors (α = 4Dh/3) to balance the different weight counts between modules, then applies a global threshold
- Core assumption: The scaling factor properly accounts for the structural differences between Attention heads and MLP channels
- Evidence anchors:
  - [section 3.3]: "we apply scaling factors based on the model design to balance number of pruned parameters between the Attention heads and MLP channels"
  - [section 3.3]: "the scaling factor α is given by 4Dh/3"
  - [corpus]: No direct citations found for this specific scaling approach

### Mechanism 3
- Claim: Compensation algorithm recovers pruned models by updating remaining weights to minimize reconstruction loss
- Mechanism: The method formulates weight perturbation optimization to minimize ℓ2 difference between pre- and post-pruning outputs, then solves analytically for optimal weight updates
- Core assumption: The compensation problem is well-posed and the analytical solution provides effective recovery
- Evidence anchors:
  - [section 3.4]: "we modify the weights with the weight perturbations δW, so that the layer output difference (before and after pruning) measured with ℓ2 norm is minimized"
  - [section 3.4]: "The optimal solution for Eq. (10) can be derived as the following"
  - [corpus]: No direct citations found for this specific compensation formulation

## Foundational Learning

- Concept: Newton's method for constrained optimization
  - Why needed here: Used to efficiently solve the numerical score optimization problem with equality constraint
  - Quick check question: What is the computational complexity of Newton's method per iteration for a D-dimensional problem?

- Concept: Hessian matrix computation and properties
  - Why needed here: Required for Newton's method convergence and for the compensation algorithm's analytical solution
  - Quick check question: Why is the Hessian in this problem guaranteed to be positive semi-definite?

- Concept: Error bounds in matrix approximation
  - Why needed here: Provides theoretical justification for the pruning approach and guides the numerical score formulation
  - Quick check question: What assumptions about the input data are required for the error bound to hold?

## Architecture Onboarding

- Component map: Input layer (B×N×D) -> Attention module (Query/Key/Value projections + output projection) -> MLP module (Up/Gate/Down projections) -> Output layer
- Critical path: Numerical score computation → Global pruning mask generation → Compensation algorithm → Model execution
- Design tradeoffs:
  - Accuracy vs speed: Newton's method provides better scores but increases pre-processing time
  - Global vs local pruning: Global approach achieves better overall compression but requires more coordination
  - Sample size for compensation: More samples improve recovery but increase pre-processing cost
- Failure signatures:
  - Model collapse at high sparsity ratios (>70%)
  - Performance degradation on short sequences
  - Inconsistent results across different datasets
- First 3 experiments:
  1. Verify numerical score computation matches expected error bounds on synthetic data
  2. Test global vs local pruning on a small LLaMA-7B model with varying ratios
  3. Validate compensation effectiveness with different sample sizes (32, 128, 512)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed numerical score perform when applied to attention mechanisms beyond softmax, such as sparse or approximate attention?
- Basis in paper: [inferred] The paper mentions applying the method to attention and MLP modules but does not explore attention variants.
- Why unresolved: The paper focuses on standard attention mechanisms and does not investigate how the pruning strategy generalizes to other attention formulations.
- What evidence would resolve it: Experiments comparing pruning effectiveness across different attention mechanisms, such as sparse attention or approximate attention methods, would clarify the method's applicability.

### Open Question 2
- Question: What is the impact of using different normalization techniques for the input X on the numerical score and pruning performance?
- Basis in paper: [explicit] The paper mentions normalizing the input X to ensure the norm is upper bounded by 1, but does not explore alternative normalization strategies.
- Why unresolved: The choice of normalization could affect the numerical score calculation and subsequent pruning performance, but this is not investigated.
- What evidence would resolve it: Comparative experiments using different normalization techniques (e.g., layer normalization, batch normalization) and their effects on pruning outcomes would provide clarity.

### Open Question 3
- Question: How does the compensation algorithm's performance scale with the size of the model and the number of parameters pruned?
- Basis in paper: [inferred] The paper discusses the compensation algorithm but does not provide detailed analysis of its scalability or performance across different model sizes.
- Why unresolved: The compensation algorithm is crucial for maintaining model performance after pruning, but its effectiveness in large-scale models is not fully explored.
- What evidence would resolve it: Detailed analysis and experiments showing the compensation algorithm's performance across various model sizes and pruning ratios would address this question.

## Limitations

- The compensation algorithm's effectiveness depends heavily on sample representativeness, which may vary across datasets and tasks
- The method shows significant performance drops at high pruning ratios (>70%) for smaller LlamaGen models
- Limited theoretical justification for why the numerical score computation outperforms existing pruning criteria

## Confidence

**High Confidence**: The empirical results showing improved perplexity scores on LLaMA model families and strong FID/IS metrics on LlamaGen models.

**Medium Confidence**: The mechanism explaining how the numerical score computation leads to better pruning decisions.

**Low Confidence**: The generalization claims for image generation tasks, particularly given that the LlamaGen models show significant performance drops at high pruning ratios (>70%).

## Next Checks

1. **Error Bound Validation**: Test the tightness of the pruning error bound across different input distributions and model architectures by computing actual reconstruction errors versus predicted bounds for various pruning ratios.

2. **Scaling Factor Sensitivity**: Conduct an ablation study varying the α scaling factor (4Dh/3) to determine its sensitivity and whether alternative formulations might provide better balance between Attention and MLP modules.

3. **Sample Size Impact**: Systematically evaluate how compensation algorithm performance scales with different numbers of training samples (e.g., 32, 128, 512, 1024) to identify the minimum effective sample size for each model family.