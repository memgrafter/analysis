---
ver: rpa2
title: 'Repairs in a Block World: A New Benchmark for Handling User Corrections with
  Multi-Modal Language Models'
arxiv_id: '2409.14247'
source_url: https://arxiv.org/abs/2409.14247
tags:
- block
- task
- repairs
- instructions
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new dataset, BlockWorld-Repairs, for evaluating
  multi-modal language models on handling Third Position Repairs (TPRs) in instruction-following
  tasks. The dataset contains dialogues in a tabletop manipulation task where users
  give instructions to move blocks and correct the robot's misunderstandings via TPRs.
---

# Repairs in a Block World: A New Benchmark for Handling User Corrections with Multi-Modal Language Models

## Quick Facts
- **arXiv ID**: 2409.14247
- **Source URL**: https://arxiv.org/abs/2409.14247
- **Reference count**: 39
- **Primary result**: All evaluated VLMs significantly underperform humans on TPR tasks in the BlockWorld-Repairs dataset.

## Executive Summary
This paper introduces BlockWorld-Repairs, a new dataset for evaluating multi-modal language models on handling Third Position Repairs (TPRs) in instruction-following tasks. The dataset contains dialogues in a tabletop manipulation task where users give instructions to move blocks and correct robot misunderstandings via TPRs. The authors evaluate several state-of-the-art vision-language models on source block and target position prediction tasks, finding that all models significantly underperform compared to humans. They propose a specialized training regime using completion-only loss, which improves model performance and generalization. The results highlight the need for better training objectives and visual representations to enable VLMs to effectively process TPRs in multi-modal collaborative settings.

## Method Summary
The authors created BlockWorld-Repairs using a Unity engine simulation with 10 blocks per scene. The dataset contains 2059 entries (795 initial instructions, 629 source block TPRs, 635 target position TPRs) split 70/30 for training/testing. They evaluate three VLMs (Idefics2, LLaVA, GPT-4o) on two prediction tasks: source block identification (IoU metric) and target position prediction (block distances). Models are fine-tuned using parameter-efficient methods (LoRA/QLoRA) with a completion-only loss that masks incorrect intermediate predictions during TPR sequences. The evaluation compares model performance against human baselines and tests zero-shot generalization.

## Key Results
- All VLMs significantly underperform humans on TPR tasks, with model accuracies ranging from 25-75% compared to human means of 88-95%
- GPT-4o shows the best overall performance but still falls short of human capabilities, particularly on abstract spatial concepts
- The completion-only loss training regime improves model performance and generalization compared to standard fine-tuning approaches
- Models struggle more with target position prediction than source block identification, indicating challenges with spatial reasoning not anchored to objects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VLMs benefit from completion-only loss during fine-tuning because it prevents learning from incorrect intermediate predictions that occur during TPRs.
- Mechanism: When training on dialogues containing TPRs, the standard cross-entropy loss penalizes all tokens including the incorrect candidate prediction made by the model. By masking these incorrect tokens, the model focuses only on learning the correct repair sequence without being influenced by its own mistakes.
- Core assumption: The intermediate incorrect predictions in TPR sequences are not useful learning signals and actually harm model performance when included in the loss calculation.
- Evidence anchors:
  - [abstract] "We then show that VLMs can benefit from specialised losses targeting relevant tokens during fine-tuning, achieving better performance and generalising better to new scenarios."
  - [section 5.2] "The results of masking these tokens in the loss (user-turns only) suggest that this loss does not help the VLMs as much as completion loss, and has a mixed bag of effects (positive or negative)."
  - [corpus] Weak - the corpus mentions related work on neural code repair but doesn't directly address the completion-only loss mechanism.
- Break condition: If intermediate predictions contain valuable information about the correction process that models need to learn, masking them would prevent learning this important aspect.

### Mechanism 2
- Claim: Models struggle with TPRs because they cannot properly integrate the initial instruction context with the repair information.
- Mechanism: TPRs require models to maintain and update their understanding of the initial instruction while processing the repair. The repair is incomplete on its own and only makes sense when interpreted alongside the robot's pointing position and the original instruction. Models that only process the last turn fail to capture this dependency.
- Core assumption: TPRs fundamentally require maintaining context across multiple turns to be correctly interpreted, unlike simple single-turn instructions.
- Evidence anchors:
  - [abstract] "In these scenarios, referential ambiguities are common and thus being able to process TPRs becomes essential to complete the task."
  - [section 4.1.2] "The repair turns in the BW-R are meaningful only when interpreted alongside the robot's pointing position at the time of the repair, based on the first instruction; thus, they are incomplete on their own."
  - [corpus] Moderate - the corpus includes work on inconsistency handling and error repairs, supporting the general idea that repairs require special processing.
- Break condition: If models can learn to compress all necessary information into the final turn without needing to maintain explicit context from earlier turns.

### Mechanism 3
- Claim: GPT-4o outperforms other VLMs on TPRs because it has better cross-modal grounding capabilities that allow it to understand abstract spatial concepts in repairs.
- Mechanism: The ability to process repairs involving abstract concepts like "rows" and "columns" requires sophisticated cross-modal understanding that maps language to spatial relationships in the visual scene. GPT-4o appears to have learned these mappings during pre-training, while other VLMs struggle with such abstract references.
- Core assumption: Abstract spatial language in repairs requires more sophisticated cross-modal grounding than concrete spatial language.
- Evidence anchors:
  - [section 6.3] "Models are usually able to process simple spatial repairs (i.e., left, right, above), however, they particularly struggle with more abstract concepts (i.e., rows, columns)."
  - [section 6.2] "GPT-4o is the only exception, which shows a more fine-grained understanding of the task yet falls behind on more difficult dialogues."
  - [corpus] Weak - the corpus mentions multimodal evaluation frameworks but doesn't specifically address abstract spatial concept understanding.
- Break condition: If the apparent superiority of GPT-4o is actually due to factors unrelated to cross-modal grounding, such as better instruction following or larger model capacity.

## Foundational Learning

- Concept: Referential ambiguity resolution
  - Why needed here: The BlockWorld-Repairs dataset is specifically designed to be "rife with referential ambiguity" where multiple blocks could match a description. Understanding how models resolve these ambiguities is crucial for interpreting TPR performance.
  - Quick check question: What is the difference between resolving ambiguity in a single-turn instruction versus handling a TPR that corrects an initial misinterpretation?

- Concept: Cross-modal grounding
  - Why needed here: The tasks require mapping language descriptions to specific locations in visual scenes (bounding box prediction). This is the core capability that distinguishes VLMs from text-only models and determines their success on the dataset.
  - Quick check question: How does the model's visual encoder contribute to generating accurate bounding boxes compared to relying solely on language understanding?

- Concept: Communicative grounding
  - Why needed here: TPRs are a specific type of communicative grounding where interlocutors work to establish shared understanding. The dataset evaluates whether models can participate in this collaborative process.
  - Quick check question: What distinguishes a TPR from other types of repairs like clarification requests in terms of the grounding process required?

## Architecture Onboarding

- Component map: Image → Visual encoder (CNN/transformer) → Cross-attention layers → Language model (decoder-only/encoder-decoder) → Bounding box generation head
- Critical path: Image → Visual encoder → Cross-attention → Language model → Bounding box prediction. The critical dependency is the cross-attention mechanism that grounds language in visual features.
- Design tradeoffs: Using parameter-efficient fine-tuning (LoRA/QLoRA) vs full fine-tuning balances adaptation capability with computational cost. The choice of masking criteria (default/user-turn/completion-only) trades off learning from full context versus avoiding negative influence from incorrect predictions.
- Failure signatures: Low source accuracy but reasonable distances indicates models can locate the general area but struggle with precise block identification. Poor performance on target position suggests limitations in understanding spatial relationships not anchored to objects. Failure to improve with completion-only loss indicates the model may not be learning the repair mechanism at all.
- First 3 experiments:
  1. Test baseline zero-shot performance on instructions-only subset to establish VLM capabilities without repair processing.
  2. Fine-tune with default loss on full dataset to see if standard training can learn TPR processing.
  3. Fine-tune with completion-only loss on repairs-only subset to isolate the effect of masking incorrect predictions.

## Open Questions the Paper Calls Out
- How do different visual representations of the block world (e.g., different image resolutions, camera angles, or block textures) affect the performance of VLMs on TPR tasks?
- How do different types of TPRs (e.g., those involving spatial relationships vs. those involving object properties) affect the performance of VLMs?
- How does the size of the training dataset impact the ability of VLMs to generalize to new TPR scenarios?

## Limitations
- The synthetic Unity environment may not capture the full complexity and variability of real-world TPR scenarios.
- The completion-only loss mechanism was only tested on a subset of models and for a single epoch, leaving questions about optimal training duration.
- The evaluation metrics may not fully capture the nuanced understanding required for effective repair processing, particularly for abstract spatial concepts.

## Confidence
- **High Confidence**: VLMs significantly underperform humans on TPR tasks is well-supported by empirical results showing consistent performance gaps across multiple models and metrics.
- **Medium Confidence**: Completion-only loss improves model performance and generalization is supported by experimental results, but the mechanism could benefit from further investigation.
- **Low Confidence**: GPT-4o's superior performance stems from better cross-modal grounding for abstract spatial concepts is speculative without direct evidence linking this to cross-modal grounding capabilities.

## Next Checks
1. **Ablation Study on Loss Components**: Systematically test different masking strategies beyond completion-only loss, including partial masking of intermediate predictions and alternative weighting schemes, to isolate the specific aspects of TPR sequences that most benefit from special handling during training.

2. **Human Variability Assessment**: Conduct a detailed analysis of individual human performance on the TPR tasks, including inter-rater reliability and strategy variation, to establish whether the current human baseline represents typical or exceptional repair handling capabilities.

3. **Real-World Transfer Validation**: Test the fine-tuned models on a small set of real-world images and instructions to assess whether the synthetic BlockWorld-Repairs dataset effectively captures the challenges of TPR processing in naturalistic settings, particularly for abstract spatial concepts.