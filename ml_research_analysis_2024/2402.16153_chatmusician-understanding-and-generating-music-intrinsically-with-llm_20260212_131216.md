---
ver: rpa2
title: 'ChatMusician: Understanding and Generating Music Intrinsically with LLM'
arxiv_id: '2402.16153'
source_url: https://arxiv.org/abs/2402.16153
tags:
- music
- musical
- arxiv
- score
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatMusician introduces an LLM trained on music and language data
  to enable intrinsic music understanding and generation. It uses ABC notation to
  represent music as text, avoiding the need for specialized tokenizers.
---

# ChatMusician: Understanding and Generating Music Intrinsically with LLM

## Quick Facts
- arXiv ID: 2402.16153
- Source URL: https://arxiv.org/abs/2402.16153
- Reference count: 40
- ChatMusician achieves competitive performance on music understanding benchmarks while maintaining or enhancing language capabilities

## Executive Summary
ChatMusician introduces an LLM trained on music and language data to enable intrinsic music understanding and generation. It uses ABC notation to represent music as text, avoiding the need for specialized tokenizers. The model is trained on a 4B token corpus including general language, music knowledge, music scores, and code/math data. It achieves competitive performance on both music understanding benchmarks and general language tasks, with human evaluations showing it produces more musical and structured compositions than GPT-4. The work demonstrates that LLMs can effectively learn musical capabilities while maintaining or enhancing language abilities.

## Method Summary
ChatMusician is built by continual pre-training and fine-tuning of LLaMA2-7B-Base using ABC notation as a text-compatible music representation. The training uses a 4B token music-language corpus (MusicPile) with a data mixture optimized for music generation and understanding. The model is trained with maximum sequence length of 2048 using fp16 precision and LoRA adapters, followed by supervised fine-tuning with a 2:1 music score to music verbal data ratio. Evaluation includes MMLU for language tasks and a novel MusicTheoryBench for music understanding.

## Key Results
- Achieves 46.80% MMLU score, slightly higher than LLaMA2-7B-Base at 46.79%
- Scores 25.6% on music reasoning and 58.2% on music knowledge in MusicTheoryBench
- Human evaluations show ChatMusician produces more musical and structured compositions than GPT-4

## Why This Works (Mechanism)

### Mechanism 1
ABC notation acts as an efficient compression scheme that embeds musical structure and repetition directly into the textual representation, reducing sequence length compared to MIDI and audio codecs. ABC notation uses repeat signs (`|:` and `|:`) and other structural markers that compactly encode repeated phrases, motifs, and sections. This built-in repetition encoding aligns with the sequential modeling strengths of transformers, allowing the model to learn musical patterns without additional tokenization overhead. The compression ratio of ABC notation (288 tokens per song vs. 753 for MIDI-like) preserves enough semantic musical information for the model to generalize across styles and structures.

### Mechanism 2
Integrating music knowledge and music reasoning into a language model does not degrade general language abilities; in fact, it slightly improves MMLU scores. By treating music as a "second language," the model leverages shared linguistic structures (e.g., notation syntax, terminology) and reasoning patterns (e.g., pattern recognition, inference) that reinforce cross-domain learning. The 4B token corpus balances music and language data, preventing catastrophic forgetting. Music knowledge and reasoning tasks engage the same cognitive architectures (pattern matching, sequence modeling) as language tasks, enabling transfer learning.

### Mechanism 3
Zero-shot music reasoning performance is significantly lower than music knowledge, indicating that current LLMs struggle with implicit musical inference (e.g., inferring harmony, key, rhythm from unannotated scores). Music reasoning requires multi-step logical inference beyond pattern matching, engaging higher-order reasoning skills not fully captured by pretraining on symbolic sequences alone. The MusicTheoryBench exposes this gap by testing inference from raw ABC notation. Music reasoning is analogous to mathematical reasoning in requiring explicit chain-of-thought or in-context learning to solve multi-step problems.

## Foundational Learning

- **Concept**: Symbolic music representation (ABC notation)
  - **Why needed here**: ABC notation enables text-based modeling of music without specialized tokenizers, aligning with LLM architecture.
  - **Quick check question**: How does ABC notation encode repetition differently from MIDI, and why is this beneficial for transformer models?

- **Concept**: Music theory fundamentals (intervals, chords, scales, form)
  - **Why needed here**: The MusicTheoryBench tests knowledge of these concepts, requiring the model to understand both explicit definitions and implicit relationships.
  - **Quick check question**: What is the difference between music knowledge (memorization) and music reasoning (inference) in the context of the benchmark?

- **Concept**: Chain-of-thought and in-context learning
  - **Why needed here**: These techniques are used to improve zero-shot music reasoning performance, as shown by the 5-shot ICL experiments on GPT-4.
  - **Quick check question**: How do role-play and 5-shot ICL prompts improve GPT-4's music reasoning scores compared to zero-shot?

## Architecture Onboarding

- **Component map**: LLaMA2-7B-Base -> MusicPile corpus (4B tokens) -> Supervised fine-tuning -> ChatMusician
- **Critical path**: Data curation -> Pretraining -> Supervised fine-tuning -> Evaluation
- **Design tradeoffs**:
  - ABC notation vs. MIDI: Higher compression but less performance nuance
  - Music corpus size vs. language preservation: 4B tokens balanced, but scaling may shift focus
  - Zero-shot vs. few-shot evaluation: Exposes reasoning gaps but may underestimate model capability
- **Failure signatures**:
  - Low MMLU scores: Overfitting to music data
  - Low MusicTheoryBench scores: Insufficient reasoning capability or poor instruction tuning
  - High parsing failure rates: ABC notation generation issues
- **First 3 experiments**:
  1. Data mixture ablation: Test different ratios of music score : music verbal data (e.g., 1:1, 3:1) to find optimal balance for language and music performance.
  2. ABC vs. MIDI encoding: Train two models with ABC and MIDI representations to quantify compression benefits and quality trade-offs.
  3. Reasoning prompt engineering: Apply chain-of-thought, role-play, and 5-shot ICL to the model and measure MusicTheoryBench score improvements.

## Open Questions the Paper Calls Out
- How does the inclusion of math and code data in the training corpus affect the model's performance on music understanding and generation tasks, and what are the specific mechanisms through which these non-music domains contribute to musical capabilities?
- What are the limitations of using ABC notation as the sole representation for music generation and understanding, and how might these limitations impact the model's ability to capture certain musical nuances or styles?
- How can the MusicTheoryBench be expanded and refined to provide a more comprehensive and challenging evaluation of music understanding and reasoning capabilities in LLMs?

## Limitations
- The MusicTheoryBench benchmark lacks established baselines and peer validation
- ABC notation may not capture all musical nuances compared to other representations
- Human evaluation methodology for comparing musical quality lacks transparency

## Confidence
- **High Confidence**: Music generation is possible using text-only LLMs without specialized tokenizers (parsing success rates >99.6%); language abilities are preserved (or slightly improved) during music capability integration (MMLU score comparisons)
- **Medium Confidence**: ABC notation's structural markers provide computational advantages for transformers (plausible but not directly proven); music reasoning vs. knowledge gap observed in MusicTheoryBench (requires external validation)
- **Low Confidence**: ChatMusician's musical compositions are "more musical and structured" than GPT-4 based on human evaluations (lacks methodological transparency)

## Next Checks
1. **Direct ABC vs. MIDI comparison**: Train two identical models with the same corpus but different music encodings (ABC vs. MIDI-like representations) and compare MusicTheoryBench scores and generation quality metrics to isolate the compression benefit.
2. **MusicPile corpus audit**: Reconstruct the data mixture proportions and conduct ablation studies varying the music score : music verbal ratio (e.g., 1:1, 3:1, 4:1) to determine the optimal balance for preserving language while maximizing musical capability.
3. **MusicTheoryBench benchmark validation**: Conduct inter-rater reliability analysis on the benchmark's human evaluations and compare results against an established symbolic music understanding dataset to verify the reasoning gap is not an artifact of the evaluation design.