---
ver: rpa2
title: An Adversarial Perspective on Machine Unlearning for AI Safety
arxiv_id: '2409.18025'
source_url: https://arxiv.org/abs/2409.18025
tags:
- unlearning
- methods
- arxiv
- knowledge
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether machine unlearning methods for
  AI safety effectively remove hazardous knowledge from large language models or merely
  obscure it. The authors challenge the assumption that unlearning offers stronger
  protection than standard safety training by conducting comprehensive white-box evaluations
  on state-of-the-art unlearning methods (RMU, NPO) compared to safety training (DPO).
---

# An Adversarial Perspective on Machine Unlearning for AI Safety

## Quick Facts
- arXiv ID: 2409.18025
- Source URL: https://arxiv.org/abs/2409.18025
- Authors: Jakub Łucki; Boyi Wei; Yangsibo Huang; Peter Henderson; Florian Tramèr; Javier Rando
- Reference count: 40
- Primary result: Unlearning methods primarily obfuscate rather than eliminate hazardous knowledge from LLMs

## Executive Summary
This paper investigates whether machine unlearning methods for AI safety effectively remove hazardous knowledge from large language models or merely obscure it. The authors challenge the assumption that unlearning offers stronger protection than standard safety training by conducting comprehensive white-box evaluations on state-of-the-art unlearning methods (RMU, NPO) compared to safety training (DPO). The core finding is that unlearning methods primarily obfuscate rather than eliminate hazardous knowledge, as demonstrated by the ability to recover most unlearned capabilities through finetuning on unrelated examples and directional ablation techniques.

## Method Summary
The authors evaluate unlearning methods by applying them to language models and then attempting to recover hazardous knowledge using various white-box techniques. They compare RMU and NPO against standard safety training (DPO) on benchmarks like WMDP-Bio and MMLU. Recovery methods include finetuning on unrelated examples, directional ablation of activation directions, enhanced GCG prefixes, and neuron pruning. They also use Logit Lens analysis to examine knowledge persistence in the residual stream.

## Key Results
- Finetuning on just 10 unrelated examples can recover most unlearned capabilities across all methods
- Removing specific activation directions or pruning critical neurons can restore hazardous knowledge
- Enhanced GCG prefixes can recover substantial performance on hazardous knowledge benchmarks
- Logit Lens reveals that unlearning methods remove knowledge more effectively from the residual stream than safety training

## Why This Works (Mechanism)

### Mechanism 1: Shallow Knowledge Obfuscation Rather Than Deep Erasure
- Claim: Unlearning methods primarily obfuscate hazardous knowledge rather than fundamentally remove it from model weights
- Mechanism: The unlearning process modifies shallow features or narrow activation directions that control hazardous knowledge access, but leaves the underlying knowledge embedded in the weights
- Core assumption: Knowledge in LLMs is stored in distributed representations that can be accessed through multiple pathways, and unlearning only blocks some of these pathways
- Evidence anchors:
  - [abstract]: "finestuning on just 10 unrelated examples can recover most unlearned capabilities across all methods, suggesting that unlearning creates shallow rather than deep modifications to model weights"
  - [section]: "Finetuning on just 10 unrelated examples can recover most unlearned capabilities across all methods, suggesting that unlearning creates shallow rather than deep modifications to model weights"
  - [corpus]: "Weak evidence - corpus papers mention related concepts but don't directly address the shallow vs deep distinction"
- Break condition: If knowledge were truly erased rather than obfuscated, finetuning on unrelated examples would not recover capabilities, and ablation of specific directions would not restore performance

### Mechanism 2: Dependence on Narrow Activation Directions
- Claim: Unlearning methods depend on specific directions in the activation space that can be identified and removed
- Mechanism: The unlearning process creates or modifies specific activation directions that suppress hazardous knowledge, but these directions can be identified through representation engineering and ablated
- Core assumption: The suppression of hazardous knowledge is mediated by identifiable, localized changes in the activation space rather than distributed modifications across the entire model
- Evidence anchors:
  - [abstract]: "removing specific activation directions or pruning critical neurons can restore hazardous knowledge, demonstrating that these protections depend on narrow mechanisms"
  - [section]: "We identify and ablate directions responsible for unlearning, successfully recovering hazardous knowledge for most protections"
  - [corpus]: "Weak evidence - corpus papers discuss representation engineering but don't specifically address activation direction dependence"
- Break condition: If unlearning created distributed, fundamental changes, directional ablation would not restore capabilities and would likely damage overall model performance

### Mechanism 3: Universal Adversarial Prefixes Bypass Obfuscation
- Claim: Universal adversarial prefixes can bypass unlearning protections by preventing the model from recognizing hazardous knowledge in the first place
- Mechanism: Unlearning methods add noise or modify representations when hazardous concepts are detected, but adversarial prefixes can prevent this detection by obscuring the context before the unlearning mechanism activates
- Core assumption: Unlearning mechanisms operate reactively (detecting and responding to hazardous content) rather than proactively removing all traces of the knowledge
- Evidence anchors:
  - [abstract]: "Enhanced GCG prefixes and orthogonalization of activation directions can restore substantial performance on hazardous knowledge benchmarks"
  - [section]: "Using enhanced GCG we were able to craft universal adversarial prefixes that increased RMU's accuracy from 29.9% to 53.9%"
  - [corpus]: "Weak evidence - corpus papers mention adversarial attacks but don't specifically address universal prefix attacks on unlearning"
- Break condition: If unlearning truly removed knowledge rather than obscuring it, adversarial prefixes would not be able to restore capabilities since the knowledge would not exist to be recovered

## Foundational Learning

- Concept: Activation Space Manipulation
  - Why needed here: Understanding how unlearning methods modify activation directions and how these can be reversed is central to evaluating unlearning effectiveness
  - Quick check question: How would you identify and ablate a specific direction in the activation space that corresponds to a particular concept?

- Concept: White-Box vs Black-Box Evaluation
  - Why needed here: The paper demonstrates that white-box methods can recover unlearned knowledge while black-box methods fail, highlighting the importance of comprehensive evaluation
  - Quick check question: What are the key differences between white-box and black-box evaluation approaches, and why might white-box methods be more effective at detecting knowledge persistence?

- Concept: Knowledge Storage in LLMs
  - Why needed here: Understanding how knowledge is stored and accessed in LLMs is crucial for understanding why unlearning methods can be bypassed
  - Quick check question: What evidence suggests that LLMs store knowledge in distributed representations that can be accessed through multiple pathways?

## Architecture Onboarding

- Component map: Unlearning methods (RMU, NPO) -> Model weights and activation spaces -> Evaluation methods (WMDP benchmark, MMLU) -> Adversarial recovery methods (finetuning, orthogonalization, enhanced GCG, pruning) -> Performance measurement

- Critical path: 1) Apply unlearning method to create protected model, 2) Measure baseline performance on hazardous knowledge benchmarks, 3) Apply various recovery methods, 4) Measure performance recovery, 5) Compare to baseline unlearned performance

- Design tradeoffs: Unlearning methods must balance knowledge removal with utility preservation, while recovery methods must balance effectiveness with minimal weight modification. The paper shows that current unlearning methods sacrifice robustness for utility

- Failure signatures: Rapid knowledge recovery through finetuning on unrelated examples, successful directional ablation without catastrophic forgetting, existence of universal adversarial prefixes, and Logit Lens revealing recoverable knowledge in the residual stream

- First 3 experiments:
  1. Apply RMU to Zephyr-7B and measure WMDP-Bio accuracy drop
  2. Perform directional ablation on the unlearned model and measure accuracy recovery
  3. Apply enhanced GCG to the unlearned model and measure prefix effectiveness at recovering capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does RMU create persistent noise in the residual stream that degrades subsequent tokens after detecting hazardous concepts?
- Basis in paper: [explicit] The paper demonstrates that RMU introduces persistent noise once hazardous concepts are detected, causing subsequent tokens to be distorted throughout generation
- Why unresolved: The mechanism by which this noise persists and propagates through the transformer layers needs more detailed analysis to understand the exact conditions that trigger it and how it affects generation quality
- What evidence would resolve it: Detailed ablation studies showing at which layers noise injection occurs, how long it persists, and experiments testing different noise injection strategies

### Open Question 2
- Question: Can unlearning methods be designed to remove knowledge from model weights rather than merely obfuscating it?
- Basis in paper: [explicit] The paper finds that current unlearning methods like RMU and NPO primarily obscure knowledge rather than eliminate it, as demonstrated by rapid recovery through finetuning and orthogonalization
- Why unresolved: Current methods show shallow modifications to weights that can be easily reversed, but the fundamental challenge of completely removing knowledge without affecting general capabilities remains unsolved
- What evidence would resolve it: Development and rigorous evaluation of unlearning methods that show no knowledge recovery through any white-box attacks, including finetuning on unrelated data and activation space manipulations

### Open Question 3
- Question: How effective are black-box evaluations compared to white-box evaluations for assessing unlearning robustness?
- Basis in paper: [explicit] The paper argues that black-box evaluations are insufficient for unlearning assessment, as methods that appeared robust in black-box settings were easily bypassed using white-box techniques
- Why unresolved: The paper demonstrates significant discrepancies between black-box and white-box results but doesn't provide a comprehensive framework for evaluating when black-box evaluations might be adequate
- What evidence would resolve it: Systematic comparison of black-box versus white-box attack success rates across multiple unlearning methods, identifying which attack classes are detectable through black-box evaluation

### Open Question 4
- Question: Does NPO show promise for deeper unlearning compared to other methods?
- Basis in paper: [explicit] The paper notes that NPO consistently displays better robustness than DPO or RMU, suggesting gradient ascent might be a promising tool for removing hazardous knowledge
- Why unresolved: While NPO shows initial promise, the paper's implementation still results in greater degradation on MMLU and general capabilities, leaving questions about whether it can achieve both safety and utility
- What evidence would resolve it: Experiments optimizing NPO hyperparameters and architecture to minimize utility loss while maintaining unlearning effectiveness, compared against current methods

### Open Question 5
- Question: Can adversarial prefixes be optimized to both jailbreak unlearned models and produce coherent, correct answers?
- Basis in paper: [explicit] The paper finds that adversarial prefixes can recover hazardous capabilities from RMU but often produce incorrect and overconfident answers, with perplexity analysis showing no correlation between coherence and correctness
- Why unresolved: The relationship between jailbreak quality, answer coherence, and factual accuracy needs investigation to develop methods that can bypass unlearning while maintaining generation quality
- What evidence would resolve it: Experiments correlating jailbreak optimization objectives with answer correctness metrics, potentially developing multi-objective optimization approaches for jailbreak generation

## Limitations

- The evaluation focuses primarily on white-box attacks that require significant access to model internals, which may not generalize to real-world deployment scenarios where such access is restricted
- The finetuning recovery experiments use only 10 unrelated examples, which represents a minimal intervention that may not reflect practical difficulty in deployed systems
- The Logit Lens analysis interpretation relies on assumptions about how attention and residual connections process information and the relationship between residual stream activations and actual knowledge storage

## Confidence

**High Confidence**: The core finding that finetuning on unrelated examples can recover most unlearned capabilities is well-supported by direct experimental evidence across multiple unlearning methods and benchmarks. The quantitative results showing substantial recovery (e.g., RMU recovering from 29.9% to 53.9% WMDP-Bio accuracy) are robust and reproducible.

**Medium Confidence**: The interpretation that unlearning creates "shallow rather than deep modifications" is supported by the finetuning results but requires additional evidence about the nature of weight changes at the level of individual parameters and their impact on model representations.

**Medium Confidence**: The claim that existing jailbreak methods can be adapted to bypass unlearning protections is supported by the enhanced GCG experiments, but the effectiveness may depend on specific implementation details and the particular unlearning method used.

## Next Checks

1. **Black-Box Transferability**: Test whether white-box recovery methods (finetuning, directional ablation, enhanced GCG) can be successfully transferred to black-box scenarios where only model outputs are observable. This would involve training surrogate models or using query-based optimization to approximate the white-box attacks.

2. **Catastrophic Forgetting Analysis**: Measure the impact of recovery methods on model utility by tracking MMLU performance degradation. This would quantify the tradeoff between knowledge recovery and capability preservation, helping to understand whether effective recovery necessarily comes at the cost of general performance.

3. **Activation Space Generalization**: Extend the directional ablation experiments to multiple activation spaces (intermediate layers, attention heads) and different unlearning methods to determine whether the vulnerability to directional manipulation is universal or method-specific. This would help identify whether certain unlearning approaches create more distributed and robust protections.