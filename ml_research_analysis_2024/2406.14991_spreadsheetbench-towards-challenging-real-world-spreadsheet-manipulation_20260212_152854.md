---
ver: rpa2
title: 'SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation'
arxiv_id: '2406.14991'
source_url: https://arxiv.org/abs/2406.14991
tags:
- spreadsheet
- data
- instruction
- solution
- manipulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SpreadsheetBench, a challenging benchmark
  for evaluating large language models on real-world spreadsheet manipulation tasks.
  Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheets,
  SpreadsheetBench is built from 912 real questions gathered from online Excel forums,
  reflecting complex user needs.
---

# SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation

## Quick Facts
- arXiv ID: 2406.14991
- Source URL: https://arxiv.org/abs/2406.14991
- Reference count: 40
- Primary result: State-of-the-art models achieve only 15-20% accuracy on real-world spreadsheet manipulation tasks

## Executive Summary
This paper introduces SpreadsheetBench, a benchmark designed to evaluate large language models on real-world spreadsheet manipulation tasks. Unlike existing benchmarks that rely on synthetic queries and simplified spreadsheets, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, reflecting complex user needs. The associated spreadsheets contain diverse tabular data formats, including multiple tables, non-standard relational tables, and non-textual elements. The paper also proposes an online judge-style evaluation metric with multiple test cases per instruction to ensure robust solutions. Comprehensive evaluations show that state-of-the-art models achieve only 15-20% accuracy, highlighting the benchmark's difficulty and the need for improved coding capabilities in LLMs for spreadsheet manipulation.

## Method Summary
The benchmark is constructed from 912 real questions collected from online Excel forums, with associated spreadsheets containing diverse tabular data formats. Each instruction is paired with multiple test cases (three per instruction) that share similar structure but differ in data. The evaluation uses an Online Judge (OJ)-style metric with soft and hard restrictions, requiring generated solutions to work across all test cases. Various LLMs are evaluated under both single-round and multi-round inference settings, using Python code to modify and interact with spreadsheet files.

## Key Results
- State-of-the-art models achieve only 15-20% accuracy on the benchmark
- GPT-4o shows the highest performance but still struggles with non-standard table structures
- Multiple test cases per instruction effectively catch corner cases and prevent overfitting
- Models perform significantly worse on non-standard relational tables compared to standard tables

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The real-world forum questions create high semantic diversity that exposes model limitations
- Mechanism: By sourcing from actual user posts, the benchmark includes varied phrasing, incomplete context, and domain-specific language that synthetic benchmarks cannot replicate
- Core assumption: Real-world questions contain richer contextual cues and error patterns than synthetically generated ones
- Evidence anchors: [abstract] "built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users"
- Break condition: If forum posts are heavily edited by annotators, semantic diversity is reduced

### Mechanism 2
- Claim: Multiple test cases per instruction prevent overfitting to single spreadsheet instances
- Mechanism: Creating 3 test cases per instruction with perturbed values ensures solutions must generalize beyond the original data
- Core assumption: Solutions that work on one data instance often fail on variations unless explicitly designed for robustness
- Evidence anchors: [abstract] "multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions"
- Break condition: If perturbations are too small to create meaningful variation in edge cases

### Mechanism 3
- Claim: OJ-style evaluation provides more reliable performance measurement than exact match
- Mechanism: By requiring solutions to pass all test cases, the metric eliminates partial credit for solutions that only work on specific inputs
- Core assumption: Partial solutions that work on one instance but fail on others are less valuable than robust solutions
- Evidence anchors: [abstract] "more reliable evaluation metric akin to online judge platforms"
- Break condition: If test cases are not representative of real-world variations

## Foundational Learning

- Concept: Tabular data structure variations (multiple tables, non-standard formats)
  - Why needed here: Spreadsheet manipulation requires understanding complex table layouts beyond simple relational tables
  - Quick check question: What distinguishes a non-standard relational table from a standard one in this benchmark?

- Concept: Spreadsheet programming (VBA, formula logic, Python code generation)
  - Why needed here: The benchmark requires LLMs to generate executable code solutions, not just query answers
  - Quick check question: Why can't TableQA models be directly applied to this benchmark?

- Concept: Online judge evaluation methodology
  - Why needed here: The OJ-style metric is central to the benchmark's reliability claims
  - Quick check question: How does the hard restriction differ from the soft restriction in the OJ metric?

## Architecture Onboarding

- Component map: Data collection → Annotation pipeline → Test case generation → Evaluation framework → Model inference
- Critical path: Data collection → Annotation → Test case generation → Evaluation
- Design tradeoffs: Real data quality vs synthetic control, multiple test cases vs annotation cost, OJ strictness vs partial credit
- Failure signatures: Solutions that work on single test case but fail on others, models that can't handle non-standard tables, performance degradation with row/column size
- First 3 experiments:
  1. Compare performance on single vs multiple test cases for the same instruction
  2. Evaluate model robustness when spreadsheet row count increases
  3. Test whether multi-round prompting improves non-standard table handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be improved to better handle non-standard relational tables with missing or incomplete headers in spreadsheet manipulation tasks?
- Basis in paper: [explicit] The paper identifies that 42.7% of spreadsheets contain non-standard relational tables with nested, incomplete, or missing headers, and notes that LLMs struggle with understanding such structures.
- Why unresolved: The paper shows that even state-of-the-art models like GPT-4o have difficulty interpreting table structures correctly, as evidenced by misalignment issues in Example 2 where the model fails to read the entire lookup table or misidentifies cell positions.
- What evidence would resolve it: A benchmark test showing improved performance on non-standard table structures after implementing specific enhancements to LLMs' table comprehension capabilities, such as training on more diverse table formats or incorporating table structure inference mechanisms.

### Open Question 2
- Question: What is the optimal number of test cases per instruction for balancing evaluation reliability and annotation cost in spreadsheet manipulation benchmarks?
- Basis in paper: [explicit] The paper uses three test cases per instruction in its OJ-style evaluation but acknowledges the high cost of manual annotation and test case generation.
- Why unresolved: While the paper demonstrates that multiple test cases improve evaluation reliability by catching corner cases, it doesn't empirically determine whether three is optimal or if fewer/more test cases would provide better trade-offs between reliability and cost.
- What evidence would resolve it: A systematic study comparing benchmark reliability metrics (false positive/negative rates) across different numbers of test cases per instruction, alongside annotation time and cost analysis.

### Open Question 3
- Question: Why does GPT-4o's performance slightly decline in multi-round settings compared to single-round settings for spreadsheet manipulation tasks?
- Basis in paper: [explicit] The paper observes that GPT-4o's performance is slightly lower in multi-round settings, attributing this to duplicated content when retrieving spreadsheet information that was already provided in the initial prompt.
- Why unresolved: The paper doesn't explore whether this is a fundamental limitation of GPT-4o's instruction-following capabilities or if alternative prompting strategies could mitigate this issue while still benefiting from multi-round interactions.
- What evidence would resolve it: Comparative experiments testing different prompt designs that explicitly instruct GPT-4o to skip already-provided spreadsheet content in subsequent rounds, measuring whether this recovers the performance advantage seen in single-round settings.

## Limitations

- Limited empirical comparison to existing benchmarks for semantic diversity claims
- Unclear whether test case perturbations adequately stress-test model capabilities
- Potential bias in forum-sourced data not fully addressed

## Confidence

**High Confidence (4/5):**
- The benchmark successfully creates challenging real-world tasks through forum question collection
- The dataset contains diverse spreadsheet formats and manipulation requirements
- The evaluation methodology is technically sound and implementable

**Medium Confidence (3/5):**
- The 15-20% accuracy figures accurately reflect model limitations
- Multiple test cases significantly improve evaluation reliability
- Real-world questions provide superior semantic diversity compared to synthetic benchmarks

**Low Confidence (2/5):**
- The benchmark fully represents real-world spreadsheet manipulation complexity
- The OJ-style metric is superior to existing evaluation approaches
- Current model performance will scale predictably with increased model size or fine-tuning

## Next Checks

1. **Synthetic vs Real Benchmark Gap Analysis**: Conduct controlled experiments comparing model performance on SpreadsheetBench versus a carefully matched synthetic benchmark with equivalent complexity. Measure specific performance differences and identify which types of questions/phrases create the largest gaps.

2. **Test Case Perturbation Validation**: Systematically vary test case parameters (data ranges, table structures, formula complexity) to determine the minimum perturbation needed to distinguish robust from fragile solutions. Analyze whether current perturbations adequately stress-test model capabilities.

3. **Evaluation Metric Robustness Testing**: Compare OJ-style evaluation results against alternative metrics (exact match, partial credit systems) across different model types and spreadsheet complexity levels. Determine whether the hard restriction in OJ evaluation creates false negatives for solutions that would be useful in practice.