---
ver: rpa2
title: Large Language Models Portray Socially Subordinate Groups as More Homogeneous,
  Consistent with a Bias Observed in Humans
arxiv_id: '2401.08495'
source_url: https://arxiv.org/abs/2401.08495
tags:
- americans
- gender
- groups
- bias
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models (LLMs) exhibit a homogeneity bias similar\
  \ to human perception, consistently portraying socially subordinate groups\u2014\
  African, Asian, and Hispanic Americans\u2014as more homogeneous than White Americans,\
  \ and women as more homogeneous than men, albeit to a lesser degree. This bias was\
  \ measured using pairwise cosine similarity between sentence embeddings of texts\
  \ generated by ChatGPT for eight intersectional group identities."
---

# Large Language Models Portray Socially Subordinate Groups as More Homogeneous, Consistent with a Bias Observed in Humans

## Quick Facts
- arXiv ID: 2401.08495
- Source URL: https://arxiv.org/abs/2401.08495
- Reference count: 40
- Large language models exhibit homogeneity bias, portraying subordinate groups as more homogeneous than dominant groups

## Executive Summary
This study investigates whether large language models (LLMs) exhibit a homogeneity bias - the tendency to perceive socially subordinate groups as more homogeneous than dominant groups - similar to human perception. Using ChatGPT to generate 30-word texts about eight intersectional group identities (African, Asian, Hispanic, White Americans × men, women), the authors found that ChatGPT consistently portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, and women as more homogeneous than men, though to a lesser degree. The effect of gender on homogeneity varied across racial/ethnic groups, being consistent within African and Hispanic Americans but not within Asian and White Americans. These findings raise concerns about LLMs potentially erasing diverse experiences among subordinate groups and reinforcing social hierarchies through their outputs.

## Method Summary
The study collected 500 ChatGPT-generated 30-word texts for each of eight intersectional group identities. Texts were preprocessed and encoded into sentence embeddings using BERT-base-uncased. Pairwise cosine similarity between embeddings was calculated to measure text homogeneity, with values standardized to mean 0 and standard deviation 1. Linear mixed-effects models were fitted to analyze the effects of race/ethnicity, gender, and their interactions on text homogeneity, with text format included as random intercepts. The authors also examined whether homogeneity bias could be explained by topical alignment using structural topic models.

## Key Results
- ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans across all text formats
- Women were portrayed as more homogeneous than men, though the effect was less pronounced
- Gender effects on homogeneity varied by racial/ethnic group, being consistent within African and Hispanic Americans but not within Asian and White Americans
- Homogeneity bias persisted even after controlling for topical alignment, indicating it cannot be fully explained by shared topics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM outputs are more homogeneous for subordinate groups due to imbalanced training data representation.
- Mechanism: Training corpora contain more frequent and less diverse references to subordinate groups, reducing the model's ability to generate varied content about them.
- Core assumption: Frequency and diversity of group mentions in training data directly influence output diversity.
- Evidence anchors:
  - [abstract] "We consistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans"
  - [section] "It is likely that homogeneous representations of subordinate groups in texts generated by LLMs are also reproductions of bias in the training data."
  - [corpus] "Weak: No direct corpus analysis of training data frequency/diversity provided"
- Break condition: Balanced representation of groups in training data produces equivalent homogeneity across groups.

### Mechanism 2
- Claim: Stereotypical trait associations in training data drive homogeneous representations of subordinate groups.
- Mechanism: Dominant group's worldview in training data leads to stereotypical descriptions of subordinate groups, limiting semantic diversity in model outputs.
- Core assumption: Training data reflects dominant group's perspective and stereotypes.
- Evidence anchors:
  - [abstract] "The presence of this bias in LLMs raises concerns about the potential erasure of diverse experiences among subordinate groups"
  - [section] "Training data of LLMs reflect the dominant group's worldview [4], which, as Fiske [22] suggests, is more prone to stereotyping socially subordinate groups"
  - [corpus] "Weak: Inference based on prior literature, not corpus analysis"
- Break condition: Training data free from stereotypical associations produces diverse outputs for all groups.

### Mechanism 3
- Claim: Homogeneity bias persists even when controlling for topical alignment.
- Mechanism: Semantic and syntactic alignment beyond shared topics contributes to perceived homogeneity in model outputs.
- Core assumption: Sentence embeddings capture semantic/syntactic similarity beyond topical content.
- Evidence anchors:
  - [abstract] "we found that homogeneity bias could not be fully explained by topical alignment alone as homogeneity bias also existed within topics"
  - [section] "These results indicated that homogeneity bias could not be fully explained by topical alignment alone"
  - [corpus] "Weak: Analysis based on structural topic model comparison, not direct corpus evidence"
- Break condition: Removing all forms of semantic/syntactic similarity eliminates homogeneity bias.

## Foundational Learning

- Concept: Social dominance theory and group perception
  - Why needed here: Understanding why subordinate groups are perceived as more homogeneous requires knowledge of social hierarchy effects on group representation.
  - Quick check question: Why would social dominance theory predict different homogeneity patterns for dominant vs subordinate groups?

- Concept: Sentence embeddings and cosine similarity
  - Why needed here: Measuring text homogeneity requires understanding how semantic similarity is quantified using vector representations.
  - Quick check question: What does a high cosine similarity between sentence embeddings indicate about the texts?

- Concept: Linear mixed-effects models
  - Why needed here: Analyzing the interaction between race/ethnicity and gender on homogeneity requires appropriate statistical modeling of nested data.
  - Quick check question: Why would text format be included as a random intercept rather than fixed effect in this analysis?

## Architecture Onboarding

- Component map: Prompt generation → LLM API call → Text collection → Pre-processing → Sentence embedding → Cosine similarity → Statistical analysis → Interpretation
- Critical path: Data collection (500 completions per prompt) → Embedding computation → Similarity calculation → Model fitting → Result interpretation
- Design tradeoffs: Using 30-word texts for consistency vs. real-world usage patterns; pre-registering analysis plan vs. exploratory flexibility
- Failure signatures: Non-compliant completions (AI refusing to generate content); instability in estimates with smaller sample sizes; unexpected interaction patterns
- First 3 experiments:
  1. Test homogeneity bias with longer text formats (e.g., 100 words) to assess scalability
  2. Vary group identity signaling (names, descriptions) to test prompt sensitivity
  3. Compare multiple LLMs with different training data characteristics to isolate data effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the homogeneity bias in LLMs change with longer text generations (e.g., paragraphs or full documents) compared to the 30-word texts used in this study?
- Basis in paper: [inferred] The paper acknowledges that ChatGPT-generated responses are rarely 30-words long and suggests future work exploring the bias in longer forms of text.
- Why unresolved: The current study only used 30-word texts, which may not fully capture the extent of homogeneity bias in more natural, extended text generations.
- What evidence would resolve it: Empirical studies comparing homogeneity bias across different text lengths (e.g., 30-word texts, paragraphs, full documents) using the same methodology as the current study.

### Open Question 2
- Question: Does the homogeneity bias observed in ChatGPT extend to other LLMs or NLP models with different training data or architectures?
- Basis in paper: [inferred] The paper notes that LLMs reproduce biases embedded in their training data and suggests future work exploring how different levels of group representation in training data affect homogeneity of LLM-generated text.
- Why unresolved: The current study only used ChatGPT, and it is unclear whether the observed homogeneity bias is specific to this model or a more general phenomenon across LLMs.
- What evidence would resolve it: Comparative studies examining homogeneity bias across multiple LLMs or NLP models with varying training data, architectures, and group representation.

### Open Question 3
- Question: What are the specific mechanisms by which LLMs perpetuate homogeneity bias, and how can this bias be effectively mitigated?
- Basis in paper: [inferred] The paper discusses potential sources of homogeneity bias, such as selection bias and stereotypical trait associations in training data, and suggests future work exploring how these factors affect homogeneity of LLM-generated text.
- Why unresolved: The current study identifies the presence of homogeneity bias but does not delve into the underlying mechanisms or propose specific mitigation strategies.
- What evidence would resolve it: In-depth analyses of LLM training data to identify the prevalence of homogeneous representations of subordinate groups, and empirical evaluations of various debiasing techniques (e.g., data augmentation, adversarial training, post-processing) to mitigate homogeneity bias in LLM outputs.

## Limitations
- The causal mechanisms remain inferred rather than directly tested through corpus analysis or controlled experiments
- The study is limited to a single LLM (ChatGPT) and sentence embedding model (BERT-base-uncased), raising generalizability questions
- Prompts used to generate texts may have introduced subtle framing effects that could influence homogeneity patterns
- The study measures perceived homogeneity through cosine similarity but cannot distinguish between genuine diversity reduction and stylistic consistency

## Confidence
- **High confidence**: The core finding that ChatGPT exhibits homogeneity bias favoring dominant groups (White Americans, men) over subordinate groups is well-supported by consistent results across multiple analytical approaches and robust to different embedding models.
- **Medium confidence**: The finding that gender effects vary by racial/ethnic group is supported but requires additional validation with alternative prompting strategies to rule out prompt-specific effects.
- **Low confidence**: The proposed causal mechanisms linking training data characteristics to homogeneity bias remain speculative without direct corpus evidence.

## Next Checks
1. Conduct direct analysis of ChatGPT's training corpus (or comparable large corpora) to measure actual frequency and diversity of references to different social groups, testing whether subordinate groups are indeed represented with less diversity.

2. Replicate the homogeneity bias measurements using multiple LLMs (e.g., GPT-4, Claude, LLaMA) with varying training data sources to determine whether the bias is model-specific or reflects broader training data patterns.

3. Systematically vary the prompts used to generate texts about different groups (changing vocabulary, sentence structure, and contextual framing) to test whether homogeneity patterns persist across diverse elicitation strategies or are artifacts of specific prompt design.