---
ver: rpa2
title: Generative Recommender with End-to-End Learnable Item Tokenization
arxiv_id: '2409.05546'
source_url: https://arxiv.org/abs/2409.05546
tags:
- item
- generative
- recommendation
- alignment
- recommender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ETEGRec, an end-to-end generative recommender
  that integrates item tokenization and autoregressive generation. It addresses the
  limitation of existing methods that decouple item tokenization from generative recommendation
  training.
---

# Generative Recommender with End-to-End Learnable Item Tokenization

## Quick Facts
- arXiv ID: 2409.05546
- Source URL: https://arxiv.org/abs/2409.05546
- Reference count: 40
- Outperforms traditional sequential recommendation models and generative recommendation baselines on three recommendation benchmarks

## Executive Summary
This paper introduces ETEGRec, an end-to-end generative recommender that integrates item tokenization and autoregressive generation. Unlike existing methods that decouple these components, ETEGRec jointly optimizes both through recommendation-oriented alignment mechanisms. The dual encoder-decoder architecture enables the model to learn how to tokenize items while simultaneously learning to generate recommendations, addressing a key limitation in current generative recommendation systems.

## Method Summary
ETEGRec uses a dual encoder-decoder architecture where an RQ-VAE-based item tokenizer converts item embeddings into hierarchical token sequences, and a Transformer-based generative recommender predicts these token sequences autoregressively. The key innovation is recommendation-oriented alignment through sequence-item alignment (KL divergence between encoder and item token distributions) and preference-semantic alignment (InfoNCE loss between decoder hidden states and reconstructed item embeddings). An alternating optimization strategy trains the tokenizer and recommender alternately to ensure stable end-to-end learning.

## Key Results
- ETEGRec outperforms traditional sequential recommendation models on three Amazon datasets
- The model achieves better performance than generative recommendation baselines
- Recommendation-oriented alignment significantly improves generative recommendation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimization of tokenizer and recommender through sequence-item alignment improves mutual adaptation.
- Mechanism: The encoder's sequence representation is mapped to the same semantic space as item embeddings, and their token distributions are aligned via KL divergence. This forces the tokenizer to produce tokens that reflect both sequence context and item semantics simultaneously.
- Core assumption: Past sequence states and target item embeddings contain complementary information that should produce similar token distributions when fed through the tokenizer.
- Evidence anchors:
  - [abstract] "sequence-item alignment and preference-semantic alignment" and "two alignment objectives can effectively couple the learning"
  - [section 2.3.1] "we employ the item tokenizer to generate the token distribution for each level... and let P_z^l and P_zE^l denote the token distributions... Our objective is to enforce the two distributions to be similar"
  - [corpus] No direct evidence; the corpus papers mention alignment but don't provide experimental validation of this specific mechanism
- Break condition: If the encoder's sequence representation doesn't capture relevant information about the target item, or if the tokenizer cannot map different inputs to similar distributions, alignment will fail.

### Mechanism 2
- Claim: Preference-semantic alignment via InfoNCE loss connects decoder output to reconstructed item semantics, improving recommendation quality.
- Mechanism: The decoder's first hidden state (representing user preference) is aligned with the reconstructed item embedding from the tokenizer using contrastive learning with in-batch negatives. This creates a feedback loop where the tokenizer learns to produce tokens that enable the recommender to better capture user preference.
- Core assumption: The decoder's first hidden state contains user preference information that should be semantically similar to the reconstructed target item embedding.
- Evidence anchors:
  - [abstract] "preference-semantic alignment employs contrastive learning to align the user preference captured by the Transformer decoder with the target item semantics"
  - [section 2.3.2] "we aim to leverage the connection between the decoder's first hidden state h_D... and the reconstructed semantic embedding ~z... There-fore, we refer to such an association relation as preference-semantic alignment"
  - [corpus] No direct evidence; corpus papers mention contrastive learning but don't validate this specific preference-semantic alignment mechanism
- Break condition: If the reconstructed embedding doesn't preserve semantic information or if the decoder's first state doesn't capture meaningful preference, the alignment will be ineffective.

### Mechanism 3
- Claim: Alternating optimization between tokenizer and recommender prevents training instability and enables stable end-to-end learning.
- Mechanism: Instead of joint optimization, the model alternates between optimizing the tokenizer (keeping recommender fixed) and optimizing the recommender (keeping tokenizer fixed). This staged approach allows each component to adapt to the other without destabilizing the entire system.
- Core assumption: Joint optimization of both components with alignment losses would cause training instability due to conflicting gradients or rapid changes in one component affecting the other.
- Evidence anchors:
  - [abstract] "we further devise an alternating optimization method, to facilitate stable and effective end-to-end learning"
  - [section 2.4] "In order to improve the training stability, we propose an alternating optimization strategy to mutually train the item tokenizer and the generative recommender"
  - [corpus] No direct evidence; corpus papers mention end-to-end optimization but don't discuss alternating training as a solution to instability
- Break condition: If the components can be jointly optimized without instability, or if the alternating schedule doesn't allow sufficient adaptation between components.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) and Residual Quantization
  - Why needed here: The item tokenizer uses RQ-VAE architecture to convert item embeddings into hierarchical token sequences
  - Quick check question: What is the difference between standard VAE and RQ-VAE in terms of quantization approach?

- Concept: Contrastive Learning (InfoNCE loss)
  - Why needed here: Used in preference-semantic alignment to match decoder hidden states with reconstructed item embeddings
  - Quick check question: How does InfoNCE loss differ from standard cross-entropy in learning semantic similarity?

- Concept: Kullback-Leibler (KL) Divergence
  - Why needed here: Used in sequence-item alignment to measure similarity between token distributions
  - Quick check question: What property of KL divergence makes it suitable for comparing probability distributions?

## Architecture Onboarding

- Component map:
  Input sequence -> Item tokenizer (RQ-VAE) -> Token sequence -> Recommender encoder -> Recommender decoder -> Generated tokens -> Alignment losses

- Critical path:
  1. Tokenize input sequence using item tokenizer
  2. Encode sequence with recommender encoder
  3. Decode with recommender decoder to generate target tokens
  4. Apply alignment losses to connect tokenizer and recommender
  5. Alternate optimization between tokenizer and recommender

- Design tradeoffs:
  - Joint vs. alternating optimization: Joint is more theoretically elegant but alternating provides stability
  - Number of codebook levels (L): More levels enable finer granularity but increase complexity
  - Alignment loss weights (μ, λ): Need careful tuning to balance different objectives

- Failure signatures:
  - Poor recommendation performance: May indicate ineffective alignment or suboptimal tokenizer
  - Training instability: Could suggest need for stronger regularization or better alternating schedule
  - Token distribution mismatch: May indicate misalignment between sequence and item representations

- First 3 experiments:
  1. Ablation test: Remove sequence-item alignment and measure performance drop
  2. Hyperparameter sweep: Test different values of μ and λ for alignment losses
  3. Alternating schedule test: Compare different epoch counts for tokenizer vs. recommender optimization phases

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions remain unanswered regarding scalability, cold-start scenarios, and generalization to non-sequential recommendation tasks.

## Limitations
- Limited ablation studies, particularly regarding the necessity of alternating optimization versus joint training
- No rigorous experimental validation of the specific alignment mechanisms' effectiveness
- Assumption that token distributions from sequence representations and item embeddings should be similar is not thoroughly tested

## Confidence

**High Confidence**: The core architectural contribution of integrating item tokenization with generative recommendation in an end-to-end framework is clearly described and technically sound. The alternating optimization strategy is well-motivated and the experimental setup is reproducible.

**Medium Confidence**: The claim that ETEGRec outperforms traditional sequential recommendation models and generative baselines is supported by experimental results on three benchmarks. However, the ablation studies are incomplete, particularly regarding the necessity of alternating optimization versus joint training.

**Low Confidence**: The specific mechanisms of how sequence-item alignment and preference-semantic alignment improve recommendation quality are theoretically justified but lack rigorous experimental validation. The paper asserts these alignments are effective but doesn't provide ablation evidence or alternative alignment strategies for comparison.

## Next Checks

1. **Joint vs. Alternating Optimization Ablation**: Implement and compare joint optimization of tokenizer and recommender with the proposed alternating optimization. Measure training stability metrics (loss curves, gradient norms) and final performance to determine if alternating training is necessary or merely beneficial.

2. **Alignment Mechanism Ablation**: Remove each alignment loss independently (sequence-item alignment and preference-semantic alignment) and measure performance degradation. Additionally, test alternative alignment strategies (e.g., using different divergence measures or contrastive approaches) to isolate the contribution of the specific mechanisms proposed.

3. **Reconstruction Quality Analysis**: Evaluate the quality of item embeddings reconstructed by the RQ-VAE tokenizer using semantic similarity metrics. Compare the similarity between original and reconstructed embeddings for items that should be semantically related versus unrelated. This would validate whether the tokenizer preserves sufficient semantic information for effective preference-semantic alignment.