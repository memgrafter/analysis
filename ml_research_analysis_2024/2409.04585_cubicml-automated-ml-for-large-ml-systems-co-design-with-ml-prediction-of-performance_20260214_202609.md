---
ver: rpa2
title: 'CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of
  Performance'
arxiv_id: '2409.04585'
source_url: https://arxiv.org/abs/2409.04585
tags:
- training
- cubicml
- performance
- search
- jobs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CubicML is an automated machine learning approach for optimizing
  large-scale distributed ML systems. It uses ML models to predict training performance,
  enabling efficient search of optimal system configurations.
---

# CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance

## Quick Facts
- **arXiv ID**: 2409.04585
- **Source URL**: https://arxiv.org/abs/2409.04585
- **Reference count**: 31
- **Key outcome**: 10.3% QPS improvement and Kendall Tau 0.88, Pearson 0.97, Spearman 0.97 correlation metrics

## Executive Summary
CubicML is an automated machine learning framework for optimizing large-scale distributed ML systems by using ML models to predict training performance. The approach enables efficient search of optimal system configurations through relatively small amounts of profiling data (around 150 examples). The framework was evaluated on two key applications: optimizing ZeRO sharding strategy for 73-billion-parameter recommendation models and predicting performance of LLM training across up to 16,384 GPUs. Results demonstrate that the method can achieve significant performance improvements while requiring minimal profiling overhead.

## Method Summary
CubicML employs ML-based prediction models to estimate distributed training performance, enabling automated co-design of system configurations. The framework uses a relatively small dataset of profiling examples (approximately 150) to train predictors that can estimate training speeds and guide optimization of system parameters. The approach was applied to two scenarios: ZeRO sharding optimization for recommendation models and LLM training performance prediction across large GPU clusters. The predictor models achieve high correlation with actual performance metrics while enabling efficient exploration of the configuration space.

## Key Results
- Achieved 10.3% QPS improvement over human-tuned baselines for 73-billion-parameter recommendation models
- Predictor achieved Kendall Tau 0.88, Pearson 0.97, and Spearman 0.97 rank correlation between predicted and actual training speeds for LLM training
- Demonstrated effectiveness with only ~150 profiling examples needed for optimization

## Why This Works (Mechanism)
The framework leverages machine learning to predict performance metrics of distributed training systems, enabling automated optimization of system configurations. By training predictors on relatively small amounts of profiling data, CubicML can efficiently explore the space of possible system configurations and identify optimal settings without requiring exhaustive experimentation. The ML models learn patterns in how different configurations affect performance across various workloads and hardware setups.

## Foundational Learning
- **ZeRO sharding strategies**: Understanding how parameter, gradient, and optimizer state sharding affects distributed training efficiency - needed to optimize memory usage and communication patterns in large models
- **Distributed training performance metrics**: Knowledge of how throughput, latency, and scaling efficiency are measured and related - needed to evaluate and predict system performance
- **Hardware-aware optimization**: Understanding GPU topology, interconnect bandwidth, and memory hierarchy effects - needed to model real-world performance constraints
- **AutoML for systems**: Familiarity with automated configuration search and ML-based performance modeling - needed to implement the prediction and optimization framework

## Architecture Onboarding
**Component Map**: Data Profiler -> Feature Extractor -> Performance Predictor -> Configuration Optimizer -> Training System
**Critical Path**: The performance prediction step is critical as it directly determines the quality of configuration recommendations and overall optimization effectiveness
**Design Tradeoffs**: Balancing profiling data requirements against prediction accuracy; choosing between simpler models with faster inference versus more complex models with potentially better accuracy
**Failure Signatures**: Poor prediction accuracy when extrapolating beyond profiled configuration ranges; reduced effectiveness on heterogeneous hardware configurations not well-represented in training data
**First Experiments**:
1. Profile a small set of configurations to establish baseline performance and feature extraction pipeline
2. Train initial predictor models on the profiling data and validate correlation with actual measurements
3. Run configuration optimization using the predictor and measure performance improvements against baseline configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on specific workloads (recommendation models and LLM training) without exploring broader application domains
- Limited discussion of how prediction accuracy scales with system size beyond 16,384 GPUs or with more complex hybrid parallelism strategies
- No detailed analysis of failure modes or edge cases where the predictor might produce unreliable results

## Confidence
- **High confidence**: Kendall Tau 0.88, Pearson 0.97, Spearman 0.97 correlation metrics for LLM performance prediction
- **High confidence**: 10.3% QPS improvement for recommendation model optimization
- **Medium confidence**: Generalization of the approach to other distributed ML workloads beyond those tested
- **Medium confidence**: Scalability claims for systems larger than 16,384 GPUs

## Next Checks
1. Evaluate the predictor on heterogeneous GPU clusters and different network topologies to assess robustness across diverse hardware configurations
2. Test the approach on additional distributed ML workloads including computer vision and graph neural networks to verify broader applicability
3. Conduct ablation studies to determine the minimum required profiling data and identify which features contribute most to prediction accuracy