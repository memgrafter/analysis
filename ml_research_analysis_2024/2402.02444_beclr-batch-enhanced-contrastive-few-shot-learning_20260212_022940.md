---
ver: rpa2
title: 'BECLR: Batch Enhanced Contrastive Few-Shot Learning'
arxiv_id: '2402.02444'
source_url: https://arxiv.org/abs/2402.02444
tags:
- learning
- beclr
- memory
- conference
- opta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of unsupervised few-shot learning
  (U-FSL), where a model must learn to classify images from novel classes after being
  pretrained on unlabeled images from base classes. The authors propose BECLR, a method
  that combines two key ideas: a Dynamic Clustered Memory (DyCE) module to enhance
  positive sampling during pretraining, and an Optimal Transport-based distribution
  Alignment (OpTA) strategy to mitigate sample bias during inference.'
---

# BECLR: Batch Enhanced Contrastive Few-Shot Learning

## Quick Facts
- arXiv ID: 2402.02444
- Source URL: https://arxiv.org/abs/2402.02444
- Reference count: 38
- One-line primary result: Achieves state-of-the-art results on all established U-FSL benchmarks, outperforming existing baselines by significant margins, especially in low-shot settings.

## Executive Summary
This paper addresses the problem of unsupervised few-shot learning (U-FSL), where a model must learn to classify images from novel classes after being pretrained on unlabeled images from base classes. The authors propose BECLR, a method that combines two key ideas: a Dynamic Clustered Memory (DyCE) module to enhance positive sampling during pretraining, and an Optimal Transport-based distribution Alignment (OpTA) strategy to mitigate sample bias during inference. DyCE infuses class-level insights into the contrastive learning framework, while OpTA aligns the distributions of labeled support and unlabeled query sets. BECLR achieves state-of-the-art results on all established U-FSL benchmarks, outperforming existing baselines by significant margins, especially in low-shot settings.

## Method Summary
BECLR is a two-stage method for unsupervised few-shot learning. During pretraining, it uses a Dynamic Clustered Memory (DyCE) module to enhance positive sampling in contrastive learning by clustering memory into partitions and dynamically updating prototypes via optimal transport. This infuses class-level structure into the unsupervised contrastive framework. During inference, BECLR employs an Optimal Transport-based distribution Alignment (OpTA) strategy to mitigate sample bias by aligning the distributions of labeled support and unlabeled query sets. The method achieves state-of-the-art results on U-FSL benchmarks by improving feature quality during pretraining and reducing distribution shift during inference.

## Key Results
- Achieves state-of-the-art results on all established U-FSL benchmarks.
- Outperforms existing baselines by significant margins, especially in low-shot settings.
- Combines DyCE and OpTA to enhance positive sampling and mitigate sample bias.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DyCE improves positive sampling by clustering memory into P partitions and dynamically updating prototypes via optimal transport.
- **Mechanism**: During pretraining, each batch embedding is assigned to its nearest memory prototype, and k nearest neighbors are drawn from that prototype's partition to form additional positive pairs. This enriches the contrastive loss with class-level structure absent from pure instance-level contrastive learning.
- **Core assumption**: Memory partitions converge to separable class-like clusters even without labels, so nearest-neighbor sampling yields meaningful positives.
- **Evidence anchors**:
  - [abstract]: "a novel Dynamic Clustered mEmory (DyCE) module to promote a highly separable latent representation space for enhancing positive sampling at the pretraining phase and infusing implicit class-level insights into unsupervised contrastive learning."
  - [section]: "Operating under the unsupervised setting, contrastive FSL approaches typically enforce consistency only at the instance level, where each image within the batch and its augmentations correspond to a unique class... We argue that infusing a semblance of class (or membership)-level insights into the unsupervised contrastive paradigm is essential."
  - [corpus]: Weak; no direct neighbor citations, but the embedding quality claim is supported indirectly by improved downstream accuracy.
- **Break condition**: If partitions fail to form separable clusters (e.g., high DBI score, collapsed memory), then nearest-neighbor sampling yields false positives, degrading contrastive learning.

### Mechanism 2
- **Claim**: OpTA mitigates sample bias by aligning the distributions of support and query sets via optimal transport.
- **Mechanism**: During inference, support prototypes are mapped into the query embedding space using an optimal transport plan computed between support prototypes and query features. This reduces the domain shift caused by limited support samples.
- **Core assumption**: Query embeddings are representative of the true class distribution, so aligning support prototypes to the query distribution reduces bias.
- **Evidence anchors**:
  - [abstract]: "We then tackle the, somehow overlooked yet critical, issue of sample bias at the few-shot inference stage. We propose an iterative Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate that it efficiently addresses the problem, especially in low-shot scenarios where FSL approaches suffer the most from sample bias."
  - [section]: "We refer to this phenomenon as sample bias, highlighting that it is overlooked by most (U-)FSL baselines... To address this issue, we introduce an Optimal Transport-based distribution Alignment (OpTA) add-on module within the supervised inference step."
  - [corpus]: Weak; no explicit citations, but distribution alignment is a known technique in domain adaptation literature.
- **Break condition**: If the query set is too small or non-representative, the transport plan will misalign prototypes, increasing bias.

### Mechanism 3
- **Claim**: BECLR's pretraining and inference stages are interdependent; good feature quality from DyCE is necessary for OpTA to be effective.
- **Mechanism**: DyCE produces separable, class-aware embeddings during pretraining; OpTA then exploits this structure to align distributions during inference. The gain from OpTA grows with pretraining performance.
- **Core assumption**: High-quality embeddings enable accurate transport-based alignment; poor embeddings render OpTA ineffective or harmful.
- **Evidence anchors**:
  - [section]: "OpTA operates under the assumption that the query embeddings Z Q are representative of the actual class distributions... its efficiency depends on the quality of the extracted features through the pretrained encoder."
  - [section]: "Fig. 7 assesses this hypothesis by comparing pure pretraining (i.e., BECLR without OpTA) and downstream performance on miniImageNet for the (5-way, 1-shot) setting as training progresses... when the initial pretraining performance is poor, OpTA even leads to performance degradation."
  - [corpus]: Weak; no direct neighbor citations, but the claim aligns with known dependencies in transfer learning.
- **Break condition**: If pretraining collapses or produces entangled embeddings, OpTA cannot meaningfully reduce sample bias.

## Foundational Learning

- **Concept**: Contrastive learning (instance-level vs. cluster-level)
  - Why needed here: BECLR extends standard contrastive learning by introducing cluster-level positive sampling, which requires understanding how instance-level contrastive loss works and its limitations.
  - Quick check question: In instance-level contrastive learning, what defines a positive pair, and why might this be insufficient for U-FSL?

- **Concept**: Optimal transport for distribution alignment
  - Why needed here: OpTA uses Sinkhorn-Knopp to compute a transport plan between support prototypes and query embeddings, a core step for bias mitigation.
  - Quick check question: What is the objective of the optimal transport plan in OpTA, and how does it differ from standard cross-entropy fine-tuning?

- **Concept**: Memory-based clustering (dynamic partitions)
  - Why needed here: DyCE maintains a memory of embeddings partitioned into clusters, whose prototypes are updated via optimal transport, enabling class-aware sampling.
  - Quick check question: How does DyCE update its memory partitions after each batch, and why is equipartitioning enforced?

## Architecture Onboarding

- **Component map**:
  Backbone (ResNet variants) -> Projection head -> DyCE memory -> Contrastive loss (instance + cluster positives) -> EMA teacher
  Inference: Encoder -> OpTA (optimal transport alignment) -> Logistic regression classifier

- **Critical path**:
  1. Augment batch -> Pass through student/teacher -> DyCE enhancement -> Compute contrastive loss -> Update student weights
  2. At inference: Extract support/query embeddings -> Compute OpTA alignment -> Train logistic regression on aligned support -> Evaluate on query

- **Design tradeoffs**:
  - Memory size vs. cluster granularity: Larger memory allows finer clusters but increases computational cost and risk of stale embeddings.
  - k nearest neighbors vs. false positives: More neighbors enrich positives but may introduce noise if clusters are not well separated.
  - OpTA iterations (δ) vs. overfitting: More passes refine alignment but may overfit to the limited support set.

- **Failure signatures**:
  - DyCE: High Davies-Bouldin index, indistinguishable clusters, low downstream accuracy despite pretraining loss decrease.
  - OpTA: Degraded performance on low-shot tasks, especially when pretraining accuracy is poor; unstable transport plans with small query sets.

- **First 3 experiments**:
  1. Verify DyCE cluster separability: Plot UMAP of memory prototypes over epochs; check DBI score trend.
  2. Test OpTA sensitivity: Compare 1-shot accuracy with and without OpTA across different pretraining checkpoints.
  3. Ablation of k in DyCE: Sweep k from 1 to 10; measure impact on pretraining loss and downstream accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of memory size (M) and number of partitions (P) affect the performance of DyCE on different datasets?
- Basis in paper: [explicit] The paper mentions that both M and P are important hyperparameters whose values were selected by evaluating on the validation set of each dataset for model selection. For example, they use M = 8192 and P = 200 for miniImageNet, CIFAR-FS and FC100, but M = 40960 and P = 1000 for tieredImageNet.
- Why unresolved: The paper does not provide a systematic study on how different values of M and P affect the performance of DyCE. It only reports the optimal values for each dataset.
- What evidence would resolve it: A thorough ablation study varying M and P independently for each dataset, showing how performance changes with different values.

### Open Question 2
- Question: Can the OpTA module be extended to work with few-shot tasks where the number of query samples is not larger than the number of support samples?
- Basis in paper: [explicit] The paper states that OpTA operates under the assumption that the query embeddings Z Q are representative of the actual class distributions, which requires the total number of unlabeled query samples to be larger than the total number of labeled support samples. It mentions that OpTA would still perform on imbalanced FSL tasks as long as this condition is met.
- Why unresolved: The paper does not explore the performance of OpTA when this condition is violated, i.e., when |Q| ≤ |S|. It only states that OpTA would still work in such cases without providing any evidence.
- What evidence would resolve it: Experiments evaluating the performance of OpTA on few-shot tasks with |Q| ≤ |S|, comparing it with the performance on tasks with |Q| > |S|.

### Open Question 3
- Question: How does the performance of BECLR compare to supervised FSL methods when the pretraining and testing classes are completely disjoint?
- Basis in paper: [explicit] The paper mentions that the top performing supervised baselines are all transductive methodologies, and such a transductive episodic pretraining cannot be established in a fully unsupervised pretraining strategy as in BECLR. It also argues that self-supervised pretraining helps generalization to the unseen classes, whereas supervised training heavily tailors the model towards the pretraining classes.
- Why unresolved: The paper only compares BECLR with supervised FSL methods on in-domain settings where the target classes still originate from the pretraining dataset. It does not explore how BECLR performs compared to supervised methods when the pretraining and testing classes are completely disjoint.
- What evidence would resolve it: Experiments comparing the performance of BECLR with supervised FSL methods on few-shot tasks where the pretraining and testing classes are completely disjoint, e.g., using datasets with non-overlapping classes.

## Limitations

- Confidence in OpTA's effectiveness is low due to unverified assumptions about query set representativeness.
- Performance heavily depends on DyCE's ability to form separable clusters without labels.
- Limited exploration of OpTA's behavior when the query set is small or non-representative.

## Confidence

- **High**: BECLR achieves state-of-the-art results on U-FSL benchmarks.
- **Medium**: DyCE improves feature quality by infusing class-level structure into contrastive learning.
- **Low**: OpTA consistently mitigates sample bias during inference, especially in low-shot settings.

## Next Checks

1. **DyCE cluster separability**: Plot UMAP of memory prototypes over epochs and compute the Davies-Bouldin index to verify that partitions form separable clusters.
2. **OpTA sensitivity**: Evaluate 1-shot accuracy with and without OpTA across different pretraining checkpoints to assess dependency on feature quality.
3. **Transport plan stability**: Test OpTA on query sets of varying sizes to determine if the alignment degrades with smaller, less representative sets.