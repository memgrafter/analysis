---
ver: rpa2
title: Security Attacks on LLM-based Code Completion Tools
arxiv_id: '2408.11006'
source_url: https://arxiv.org/abs/2408.11006
tags:
- code
- attack
- lccts
- github
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper demonstrates critical security vulnerabilities in LLM-based
  Code Completion Tools (LCCTs), including GitHub Copilot and Amazon Q, by introducing
  two novel attack methodologies: jailbreaking and training data extraction. The researchers
  achieved a 99.4% attack success rate in jailbreaking GitHub Copilot and a 46.3%
  success rate on Amazon Q, significantly exceeding existing methods.'
---

# Security Attacks on LLM-based Code Completion Tools

## Quick Facts
- arXiv ID: 2408.11006
- Source URL: https://arxiv.org/abs/2408.11006
- Authors: Wen Cheng; Ke Sun; Xinyu Zhang; Wei Wang
- Reference count: 10
- One-line primary result: Novel jailbreaking and training data extraction attacks achieve 99.4% success rate on GitHub Copilot and expose privacy vulnerabilities in LCCTs

## Executive Summary
This paper presents critical security vulnerabilities in LLM-based Code Completion Tools (LCCTs) through two novel attack methodologies: jailbreaking and training data extraction. The researchers demonstrate that LCCTs like GitHub Copilot and Amazon Q are vulnerable to attacks that exploit their unique workflow of aggregating contextual information and prioritizing code suggestions. They achieve a 99.4% attack success rate in jailbreaking GitHub Copilot and a 46.3% success rate on Amazon Q, significantly exceeding existing methods. The study also reveals that these code-based attacks are effective against general-purpose LLMs like the GPT series, highlighting broader security issues in how modern LLMs handle code inputs.

## Method Summary
The researchers propose three attack methodologies targeting LCCT security: Contextual Information Aggregation Attack (using filename, current file, and other open files as attack vectors), Hierarchical Code Exploitation Attack (embedding jailbreaking prompts within code snippets using different programming constructs), and Code-Driven Privacy Extraction Attack (extracting sensitive user data from proprietary training datasets). They evaluate these attacks on GitHub Copilot (version 1.211.0) and Amazon Q (version 1.12.0), testing 20 queries per attack category across illegal content, hate speech, pornography, and harmful content. The attacks leverage the specialized workflow of LCCTs, which integrates multiple contextual inputs and prioritizes code suggestions over natural language interaction.

## Key Results
- Achieved 99.4% attack success rate in jailbreaking GitHub Copilot and 46.3% on Amazon Q
- Successfully extracted 54 email addresses and 314 physical addresses from GitHub Copilot's training data
- Demonstrated that code-based attacks are effective against general-purpose LLMs including GPT-3.5, GPT-4, and GPT-4o
- Showed that less sophisticated models (Copilot, Amazon Q, GPT-3.5) are more vulnerable to simpler attacks than advanced models (GPT-4, GPT-4o)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The specialized workflow of LCCTs introduces novel security challenges that can be exploited.
- Mechanism: LCCTs aggregate multiple contextual inputs (filename, current file, other open files) and prioritize code suggestions over natural language, creating unique attack vectors not present in general-purpose LLMs.
- Core assumption: The integration of contextual information and code-focused processing creates vulnerabilities not adequately addressed by existing safety measures.
- Evidence anchors:
  - [abstract]: "Unlike general-purpose LLMs, these tools possess unique workflows, integrating multiple information sources as input and prioritizing code suggestions over natural language interaction, which introduces distinct security challenges."
  - [section 3]: "LCCT's workflow encompasses four key steps: Input Collection, Input Preprocessing, Data Processing, Output Post-Processing."
  - [corpus]: Weak - corpus contains related work on adversarial attacks but lacks specific evidence about LCCT workflow vulnerabilities.
- Break condition: If LCCTs implement comprehensive security checks across all input sources and code contexts, the attack surface would be significantly reduced.

### Mechanism 2
- Claim: Code-based attacks represent a significant threat to both LCCTs and general LLMs.
- Mechanism: Attackers embed malicious jailbreaking prompts within code snippets, exploiting the fact that LCCTs process code-based inputs which are harder to detect than natural language.
- Core assumption: The code-based nature of LCCT inputs makes it more difficult for safety alignment mechanisms to detect embedded malicious content.
- Evidence anchors:
  - [abstract]: "Our study also demonstrates that these code-based attack methods are effective against general-purpose LLMs, such as the GPT series, highlighting a broader security misalignment in the handling of code by modern LLMs."
  - [section 4]: "We then investigate embedding jailbreaking prompts within code snippets, developing two levels of attacks utilizing different programming constructs."
  - [corpus]: Moderate - corpus includes related work on adversarial attacks but lacks specific evidence about code-based attack effectiveness.
- Break condition: If LLMs develop robust mechanisms for analyzing code context and detecting malicious patterns within code structures.

### Mechanism 3
- Claim: The use of proprietary training datasets for LCCTs introduces new privacy risks.
- Mechanism: LCCTs trained on proprietary code datasets may inadvertently memorize and expose sensitive user data (email addresses, physical addresses) during code completion.
- Core assumption: LLMs have inherent data retention capabilities that can be exploited to extract sensitive information from training data.
- Evidence anchors:
  - [abstract]: "We successfully extracted sensitive user data from GitHub Copilot, including 54 real email addresses and 314 physical addresses associated with GitHub usernames."
  - [section 4.3]: "The final attack examines unauthorized access to user privacy through private training datasets via LCCTs, focusing on executing training data extraction attacks."
  - [corpus]: Moderate - corpus includes related work on training data extraction but lacks specific evidence about LCCT privacy vulnerabilities.
- Break condition: If LCCTs implement data sanitization and differential privacy techniques during training to prevent memorization of sensitive information.

## Foundational Learning

- Concept: LLM Safety Alignment
  - Why needed here: Understanding how safety alignment works (or fails) in LCCTs is crucial for identifying attack vectors and designing effective defenses.
  - Quick check question: Why might standard safety alignment techniques be less effective for LCCTs compared to general-purpose LLMs?

- Concept: Code Completion Workflow
  - Why needed here: Understanding the specific workflow of LCCTs (contextual information aggregation, code-focused processing) is essential for identifying unique attack opportunities.
  - Quick check question: How does the contextual information aggregation in LCCTs differ from the input processing in general-purpose LLMs?

- Concept: Training Data Extraction Attacks
  - Why needed here: Understanding how LLMs can be prompted to reveal training data is crucial for assessing the privacy risks of LCCTs.
  - Quick check question: What mechanisms allow LLMs to "memorize" aspects of their training data, and how can this be exploited?

## Architecture Onboarding

- Component map: Input Collection -> Input Preprocessing -> Data Processing -> Output Post-Processing
- Critical path: Input Collection → Input Preprocessing → Data Processing → Output Post-Processing
- Design tradeoffs: Balancing response time constraints with security effectiveness
- Failure signatures: High ASR (Attack Success Rate) in jailbreaking attacks, successful extraction of sensitive user data
- First 3 experiments:
  1. Test jailbreaking attacks on different LCCTs to identify vulnerabilities in contextual information aggregation
  2. Evaluate the effectiveness of code-based attacks across various programming languages
  3. Assess privacy leakage by attempting to extract sensitive user data from different LCCTs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would input preprocessing and output post-processing security measures be in mitigating jailbreaking attacks on LCCTs?
- Basis in paper: [explicit] The paper suggests implementing security measures at both the input preprocessing and output post-processing stages of LCCTs to achieve comprehensive security alignment.
- Why unresolved: The paper only proposes this as a potential solution but does not provide experimental results to validate its effectiveness against the demonstrated jailbreaking attacks.
- What evidence would resolve it: Empirical testing of LCCTs with enhanced input preprocessing and output post-processing security measures, measuring the reduction in attack success rates compared to current methods.

### Open Question 2
- Question: To what extent does the complexity of a model's architecture influence its vulnerability to code-based jailbreaking attacks?
- Basis in paper: [inferred] The paper finds that less sophisticated models (e.g., Copilot, Amazon Q, GPT-3.5) show higher attack success rates for simpler attacks, while more advanced models (e.g., GPT-4 and GPT-4o) resist these attacks, suggesting a correlation between model complexity and vulnerability.
- Why unresolved: The paper observes this trend but does not provide a detailed analysis of the underlying mechanisms or quantify the relationship between model complexity and attack resistance.
- What evidence would resolve it: Systematic testing of various LCCT and LLM models with different levels of complexity using the same attack methodologies, followed by a comparative analysis of their vulnerabilities.

### Open Question 3
- Question: How generalizable are the privacy extraction attacks to other programming languages beyond Python and Go?
- Basis in paper: [explicit] The paper evaluates the attacks using Python and Go, finding that Go increases the attack success rate compared to Python, but does not test other languages.
- Why unresolved: The paper does not explore the effectiveness of these attacks across a broader range of programming languages, leaving the generalizability uncertain.
- What evidence would resolve it: Conducting the same privacy extraction attacks on LCCTs using multiple programming languages, analyzing the attack success rates and identifying any patterns or differences in vulnerability.

## Limitations

- The study cannot fully verify attack mechanisms due to the black-box nature of LCCT evaluation and proprietary training data
- Results may not reflect current security status as the tested LCCT versions (GitHub Copilot v1.211.0, Amazon Q v1.12.0) may have received security updates
- Specific attack success rates and privacy extraction results cannot be independently validated without access to implementation details

## Confidence

- **High Confidence**: The demonstration that code-based attacks are effective against general-purpose LLMs is well-supported by the experimental evidence showing consistent attack success across multiple GPT models (GPT-3.5, GPT-4, GPT-4o).
- **Medium Confidence**: The claim that LCCT-specific workflows introduce novel security challenges is plausible given the documented differences in input processing, but the extent of vulnerability compared to general LLMs requires further investigation.
- **Low Confidence**: The specific attack success rates and privacy extraction results for LCCTs cannot be fully verified without access to the exact implementation details and the ability to replicate the experiments on current versions of the tools.

## Next Checks

1. Reproduce Jailbreaking Attack Success Rates: Implement the described attack methodologies on current versions of GitHub Copilot and Amazon Q to verify if the reported 99.4% and 46.3% attack success rates are still achievable.

2. Cross-Model Code Attack Effectiveness: Test the code-based jailbreaking attacks on a broader range of general-purpose LLMs (including Claude, Gemini, and other open-source models) to validate the claim that these attacks are effective beyond the GPT series.

3. Privacy Extraction Attack Verification: Attempt to extract sensitive user data from GitHub Copilot using the described methodology, focusing on verifying the existence of training data leakage and the specific types of sensitive information that can be extracted.