---
ver: rpa2
title: Designing and Evaluating Dialogue LLMs for Co-Creative Improvised Theatre
arxiv_id: '2405.07111'
source_url: https://arxiv.org/abs/2405.07111
tags:
- show
- about
- human
- lines
- robots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the use of large language models (LLMs) in
  improvised theatre performances at the Edinburgh Festival Fringe, examining how
  conversational AI can co-create with human actors in a live multi-party dialogue
  setting. The research deployed three different LLMs (ChatGPT-3.5, PaLM 2, and Llama
  2) across 26 performances, using a human-in-the-loop curation system where an operator
  provided contextual metadata and a curator selected responses from the AI's generated
  output stream.
---

# Designing and Evaluating Dialogue LLMs for Co-Creative Improvised Theatre

## Quick Facts
- arXiv ID: 2405.07111
- Source URL: https://arxiv.org/abs/2405.07111
- Reference count: 24
- Primary result: Audiences found AI entertaining but not equal to human improvisers in theatrical collaboration

## Executive Summary
This study explored the use of large language models in improvised theatre performances at the Edinburgh Festival Fringe, examining how conversational AI can co-create with human actors in live multi-party dialogue settings. The research deployed three different LLMs (ChatGPT-3.5, PaLM 2, and Llama 2) across 26 performances, using a human-in-the-loop curation system where an operator provided contextual metadata and a curator selected responses from the AI's generated output stream.

The primary findings revealed that while audiences found the AI's contributions entertaining and were curious about its capabilities, they did not perceive it as matching human improvisers' performance quality. Audience surveys showed that 78% felt they were watching improvisers work around the AI's limitations rather than collaborating with it as an equal partner. However, 37% reported feeling more excited about using AI tools for creativity after the show, compared to only 16% feeling more optimistic about AI as storytellers.

## Method Summary
The research deployed three different LLMs (ChatGPT-3.5, PaLM 2, and Llama 2) across 26 performances at the Edinburgh Festival Fringe, using a human-in-the-loop curation system. An operator provided contextual metadata to the AI, and a curator selected responses from the AI's generated output stream for integration into the live performance. The system aimed to create a collaborative environment where AI could contribute to improvised theatre alongside human performers.

## Key Results
- 78% of audiences felt they were watching improvisers work around AI limitations rather than true collaboration
- 37% reported increased excitement about AI tools for creativity versus 16% for AI as storytellers
- LLMs generated contextually relevant responses in multi-party dialogue with sufficient prompt engineering and real-time context

## Why This Works (Mechanism)
The study demonstrated that LLMs can generate contextually relevant responses in multi-party dialogue settings when provided with appropriate prompt engineering and real-time context. The human-in-the-loop curation system allowed for quality control and contextual adaptation, enabling the AI to contribute meaningfully to the creative process despite technical limitations. The participatory design approach engaged both performers and audiences in shaping the AI's role, creating a collaborative framework that leveraged human creativity to complement AI capabilities.

## Foundational Learning
1. **Participatory Design for AI Collaboration**: Why needed - To ensure AI tools meet creative practitioners' needs and expectations; Quick check - Assess whether end-users feel ownership over AI integration in their creative process
2. **Multi-party Dialogue Context Management**: Why needed - To maintain coherent conversations with multiple participants in live settings; Quick check - Measure response relevance across different speaker turn sequences
3. **Real-time AI Response Curation**: Why needed - To filter and select appropriate AI outputs for live performance; Quick check - Compare audience reception of curated versus raw AI responses
4. **Speech Recognition Error Tolerance**: Why needed - To handle imperfect transcription in noisy theatrical environments; Quick check - Evaluate response quality degradation with varying recognition accuracy
5. **Turn-taking Protocol Design**: Why needed - To coordinate natural conversational flow between AI and humans; Quick check - Measure audience perception of conversational naturalness

## Architecture Onboarding

Component map:
Human performers -> Speech recognition -> LLM (ChatGPT-3.5/PaLM 2/Llama 2) -> Response generation -> Human curator -> Performance output

Critical path: Performance dialogue → Speech recognition → LLM processing → Curator selection → Stage integration

Design tradeoffs:
- Human curation vs. automated response selection (quality control vs. latency)
- Multiple LLM models vs. single optimized model (diversity vs. consistency)
- Real-time performance constraints vs. comprehensive context provision

Failure signatures:
- Speech recognition errors causing irrelevant AI responses
- Turn-taking confusion leading to conversational breakdowns
- AI responses lacking theatrical context or character consistency
- Technical delays disrupting performance flow

First experiments:
1. Test single LLM model with automated response selection in controlled environment
2. Implement improved speech recognition optimized for theatrical acoustics
3. Develop AI turn-taking protocol with visual or auditory cues for performers

## Open Questions the Paper Calls Out
None

## Limitations
- Sample size of audience surveys limits generalizability of findings
- Human curation introduces variables that complicate direct AI performance assessment
- Technical limitations in speech recognition and turn-taking may not represent true LLM potential
- Study captured only immediate audience reactions, not long-term impact

## Confidence
- High confidence in audience reception findings (26 performances, direct survey data)
- Medium confidence in technical performance assessments (human curation introduces variables)
- Medium confidence in generalizability (adequate sample for initial insights but limited)

## Next Checks
1. Conduct longitudinal study tracking audience perceptions across multiple performances and cultural contexts
2. Implement automated response selection system to isolate AI performance from human intervention effects
3. Develop and test improved speech recognition and turn-taking protocols for theatrical environments