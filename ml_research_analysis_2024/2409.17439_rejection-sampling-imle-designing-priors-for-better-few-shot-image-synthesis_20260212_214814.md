---
ver: rpa2
title: 'Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis'
arxiv_id: '2409.17439'
source_url: https://arxiv.org/abs/2409.17439
tags:
- data
- imle
- training
- samples
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of high-quality image synthesis
  with limited training data. Existing IMLE-based methods suffer from a misalignment
  issue where the latent codes used during training have a different distribution
  than those encountered at test time, leading to suboptimal performance.
---

# Rejection Sampling IMLE: Designing Priors for Better Few-Shot Image Synthesis

## Quick Facts
- arXiv ID: 2409.17439
- Source URL: https://arxiv.org/abs/2409.17439
- Authors: Chirag Vashist; Shichong Peng; Ke Li
- Reference count: 40
- One-line primary result: Achieves state-of-the-art few-shot image synthesis with 45.9% average FID reduction

## Executive Summary
This paper addresses the challenge of high-quality image synthesis with limited training data by introducing RS-IMLE, a novel approach that modifies the prior distribution used for training through rejection sampling. Existing IMLE-based methods suffer from a misalignment issue where latent codes used during training have a different distribution than those encountered at test time. RS-IMLE achieves state-of-the-art results on nine few-shot image datasets, demonstrating superior precision, recall, and image quality compared to existing GAN and IMLE-based methods.

## Method Summary
RS-IMLE modifies the implicit maximum likelihood estimation framework by applying rejection sampling to the latent codes before training. The method constructs a new prior distribution by rejecting samples that are within ε distance of any data point, ensuring that training latent codes better align with the standard normal distribution used at test time. This addresses the fundamental misalignment between training and inference distributions in IMLE methods. The approach uses a generator network with VDVAE decoder modules and employs fast k-nearest neighbor search for efficient rejection sampling.

## Key Results
- Achieves state-of-the-art performance on nine few-shot image datasets with 45.9% average FID improvement
- Demonstrates superior precision and recall scores compared to existing GAN and IMLE methods
- Produces higher quality, more diverse images with better mode coverage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: RS-IMLE aligns the latent code distribution during training with that during testing by modifying the prior distribution through rejection sampling.
- **Mechanism**: By sampling latent codes from a distribution that ensures distances to data points are at least ε, RS-IMLE avoids selecting latent codes that are too close to the data during training.
- **Core assumption**: The misalignment issue arises because IMLE selects latent codes during training that are closer to the data than random latent codes drawn at test time.
- **Evidence anchors**:
  - [abstract]: "current IMLE-based approaches encounter challenges due to inadequate correspondence between the latent codes selected for training and those drawn during inference."
  - [section]: "we aim to change the prior distribution used for training such that the distribution of latent codes at training time closely match with the distribution of latent codes (drawn from the standard normal distribution) encountered at test time."
- **Break condition**: If ε is set too large, the model may fail to learn meaningful representations due to overly restrictive sampling.

### Mechanism 2
- **Claim**: RS-IMLE ensures that the loss per data point remains sufficiently high during training by rejecting samples within ε distance of any data point.
- **Mechanism**: The algorithm computes the distance between each sample and all data points, rejecting any sample that is within ε distance of any data point before performing the nearest neighbor search for training.
- **Core assumption**: Maintaining a higher loss per data point leads to more meaningful updates to model parameters and prevents the model from overfitting to samples that are already close to the data.
- **Evidence anchors**:
  - [section]: "all samples we obtain by using the prior are guaranteed to be ε-distance away from all data points. This ensures that the loss for each data point is always greater that ε."
  - [section]: "The approach can be interpreted as ignoring the samples that are already close to some data point and instead training on challenging, non-trivial samples."
- **Break condition**: If ε is set too small, the rejection sampling may not effectively change the latent space distribution.

### Mechanism 3
- **Claim**: RS-IMLE improves mode coverage and image quality by ensuring that the latent space is better explored during training.
- **Mechanism**: By changing the prior distribution to one that better matches the ideal distribution, RS-IMLE ensures that the latent codes used during training are more representative of the full latent space.
- **Core assumption**: Better exploration of the latent space during training leads to improved mode coverage and image quality at test time.
- **Evidence anchors**:
  - [abstract]: "This leads to substantially higher quality image generation compared to existing GAN and IMLE-based methods."
  - [section]: "Our method, which we call Rejection Sampling IMLE or RS-IMLE for short, demonstrably improves coverage of the latent space used during training, thereby ensuring better alignment with the prior."
- **Break condition**: If the target prior distribution is not well-designed, the rejection sampling may not effectively improve latent space exploration.

## Foundational Learning

- **Concept**: Implicit Maximum Likelihood Estimation (IMLE)
  - **Why needed here**: RS-IMLE builds upon the IMLE framework to address the misalignment issue in few-shot image synthesis.
  - **Quick check question**: How does IMLE differ from GANs in terms of the objective function?

- **Concept**: Rejection Sampling
  - **Why needed here**: RS-IMLE uses rejection sampling to modify the prior distribution used for training, ensuring better alignment between training and testing distributions.
  - **Quick check question**: What is the key idea behind rejection sampling, and how does it help in RS-IMLE?

- **Concept**: Mode Collapse
  - **Why needed here**: RS-IMLE aims to address the mode collapse issue that is prevalent in GANs and other generative models, especially in few-shot settings.
  - **Quick check question**: What is mode collapse, and why is it a significant challenge in few-shot image synthesis?

## Architecture Onboarding

- **Component map**: Generator network (VDVAE decoder modules) -> Mapping network (1024 latent dimension) -> Upsampling layers -> Residual blocks
- **Critical path**: Sample latent codes → Apply rejection sampling (filter within ε distance) → Perform nearest neighbor search → Compute distances → Update model parameters
- **Design tradeoffs**: Trades computational efficiency (due to rejection sampling) for improved image quality and mode coverage. The choice of ε is crucial for balancing these tradeoffs.
- **Failure signatures**: Poor image quality or mode collapse if ε is not properly tuned per dataset.
- **First 3 experiments**:
  1. Train RS-IMLE on a simple 2D toy dataset and visualize the latent space exploration and image quality compared to IMLE.
  2. Experiment with different values of ε and observe their impact on image quality and mode coverage.
  3. Compare RS-IMLE with other state-of-the-art few-shot image synthesis methods on standard datasets (e.g., Obama, Grumpy Cat, FFHQ-100) using FID, precision, and recall metrics.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the content, several important questions remain:

### Open Question 1
- Question: How does the performance of RS-IMLE scale with dataset size beyond the few-shot regime?
- Basis in paper: [inferred] The paper focuses on few-shot image synthesis and demonstrates superior performance on datasets with 64-389 images.
- Why unresolved: The paper's scope is limited to few-shot scenarios, and no experiments are conducted with larger datasets.
- What evidence would resolve it: Experiments training RS-IMLE on datasets with 10,000+ images and comparing its performance to other methods in the large-scale setting.

### Open Question 2
- Question: What is the optimal value of the hyperparameter ε across different datasets and latent space dimensions?
- Basis in paper: [explicit] The authors perform an ablation study on ε values but only test a small range (0.12-0.22) on three datasets.
- Why unresolved: The paper shows that ε affects performance but doesn't provide guidance on how to choose it for new datasets or architectures.
- What evidence would resolve it: A comprehensive study varying ε across many datasets, latent dimensions, and architectures to identify patterns.

### Open Question 3
- Question: How does RS-IMLE perform on tasks beyond unconditional image synthesis?
- Basis in paper: [inferred] The method is presented for unconditional image synthesis on static images.
- Why unresolved: The theoretical foundation and implementation details are specific to unconditional generation.
- What evidence would resolve it: Experiments applying RS-IMLE to conditional image generation, image-to-image translation, and video synthesis tasks.

## Limitations

- Limited scalability to larger few-shot scenarios beyond the tested 64-389 image range
- Dependence on dataset-specific hyperparameter tuning (ε threshold) without a systematic selection method
- No direct comparison with the most recent few-shot GAN approaches published after the primary baselines

## Confidence

**Confidence Level: Medium** - The claim of 45.9% FID improvement is based on comparisons against existing methods but lacks direct comparison with recent few-shot GAN approaches.

**Confidence Level: Low** - The choice of ε threshold is described as dataset-specific but the sensitivity analysis is limited and no systematic method for determining optimal values is provided.

**Confidence Level: Medium** - While improvements are demonstrated on 9 datasets, the total number of images (64-389 per dataset) represents relatively small-scale problems, and scalability to larger few-shot scenarios remains untested.

## Next Checks

1. **Ablation Study on ε**: Conduct systematic experiments varying ε across multiple orders of magnitude to quantify the sensitivity of FID, precision, and recall scores to this critical hyperparameter.

2. **Comparison with Recent Methods**: Benchmark RS-IMLE against the most recent few-shot GAN approaches (e.g., published after 2022) on standard datasets to validate the claimed state-of-the-art status.

3. **Scalability Test**: Evaluate RS-IMLE on larger few-shot scenarios with 1000-10000 training images to assess performance degradation and computational requirements as dataset size increases.