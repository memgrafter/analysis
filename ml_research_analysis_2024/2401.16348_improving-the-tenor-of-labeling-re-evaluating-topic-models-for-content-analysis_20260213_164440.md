---
ver: rpa2
title: 'Improving the TENOR of Labeling: Re-evaluating Topic Models for Content Analysis'
arxiv_id: '2401.16348'
source_url: https://arxiv.org/abs/2401.16348
tags:
- topic
- user
- slda
- labels
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study is the first to evaluate neural, supervised, and classical
  topic models in an interactive task-based setting. We conduct simulated experiments
  and real user studies to compare LDA, sLDA, and three NTMs (CTM, BERTopic, ETM)
  for content analysis and document annotation.
---

# Improving the TENOR of Labeling: Re-evaluating Topic Models for Content Analysis

## Quick Facts
- arXiv ID: 2401.16348
- Source URL: https://arxiv.org/abs/2401.16348
- Reference count: 30
- Primary result: CTM outperforms other models on cluster evaluation metrics and human evaluations, while LDA remains competitive with other NTMs.

## Executive Summary
This study conducts the first comprehensive evaluation of neural, supervised, and classical topic models in an interactive task-based setting. Through simulated experiments and real user studies, the research compares LDA, sLDA, and three neural topic models (CTM, BERTopic, ETM) for content analysis and document annotation tasks. Results demonstrate that CTM achieves the best performance on both cluster evaluation metrics and human evaluations, though LDA remains competitive with the neural models. The findings reveal that automated coherence metrics do not provide a complete picture of topic modeling capabilities for practical applications, highlighting the importance of task-centered evaluation approaches.

## Method Summary
The study combines topic models with active learning classifiers to help users conduct content analysis and document annotation. The method involves training multiple topic models (LDA, sLDA, CTM, BERTopic, ETM) on two datasets, then concatenating topic probability features with tf-idf representations to train a logistic regression classifier. An active learning preference function selects documents for human labeling based on uncertainty and diversity sampling within topic contexts. The evaluation includes simulated experiments labeling 400 documents and user studies with 15 participants per group using a web interface that displays topic overviews and active learning recommendations.

## Key Results
- CTM performs best on cluster evaluation metrics (Purity, ARI, ANMI) and human evaluations
- LDA remains competitive with other neural topic models despite being a classical approach
- Automated coherence metrics (NPMI) do not reliably predict task-specific performance for content analysis
- Neural topic models combined with active learning improve label induction efficiency

## Why This Works (Mechanism)

### Mechanism 1
CTM outperforms other models on cluster evaluation metrics and human evaluations because its contextualized embeddings capture richer semantic relationships than classical BoW models. CTM concatenates sentence embeddings from a pre-trained language model with BoW representations, enabling the model to leverage both contextual semantic information and traditional topic-word distributions. This dual representation allows CTM to identify more coherent and semantically meaningful topics that align better with human labeling intentions. The core assumption is that pre-trained language model embeddings provide semantically richer representations than traditional BoW approaches for topic modeling tasks.

### Mechanism 2
The combination of topic models with active learning classifiers creates a preference function that guides users to documents most beneficial for label induction. Active learning uses uncertainty and diversity sampling through a preference function that selects documents with highest entropy in their label distribution. When combined with topic models, this function first selects the most confusing topic, then the most confusing document within that topic, reducing context-switching and improving labeling efficiency. The core assumption is that users can resolve classifier confusion more efficiently when documents are presented within their predominant topic context.

### Mechanism 3
Task-based evaluation reveals that automated coherence metrics do not provide a complete picture of topic modeling capabilities for practical applications. While neural topic models generally achieve higher automatic coherence scores, these metrics don't capture how well topics support actual human tasks like content analysis and label set creation. The study shows CTM performs best on practical tasks despite not having the highest coherence scores. The core assumption is that coherence metrics optimized for general topic quality may not align with the specific requirements of human-in-the-loop content analysis tasks.

## Foundational Learning

- Concept: Latent Dirichlet Allocation (LDA) generative process
  - Why needed here: Understanding LDA is crucial as it serves as the baseline model and many concepts (topic distributions, document-topic mixtures) are referenced throughout the study
  - Quick check question: What are the two main distributions LDA samples from, and what do they represent?

- Concept: Active learning and uncertainty sampling
  - Why needed here: The study heavily relies on active learning to guide document selection, and understanding preference functions is essential for grasping the methodology
  - Quick check question: How does the preference function calculate which document a user should label next?

- Concept: Cluster evaluation metrics (Purity, ARI, ANMI)
  - Why needed here: These metrics are the primary quantitative measures used to compare model performance, and understanding their strengths and weaknesses is crucial for interpreting results
  - Quick check question: What is the key difference between ARI and NMI in how they handle chance agreement?

## Architecture Onboarding

- Component map: Topic model generators (LDA, sLDA, CTM, BERTopic, ETM) -> Active learning classifier (logistic regression with SGD) -> Preference function module -> User interface for document labeling. The topic models generate topic distributions, which are concatenated with tf-idf features to train the classifier.
- Critical path: Document selection → User labeling → Classifier update → Topic model inference → Preference function calculation → Next document selection. This loop continues until sufficient labels are created.
- Design tradeoffs: Using pre-trained embeddings (CTM) provides semantic richness but increases computational cost and may introduce domain mismatch. Simpler models (LDA) are faster but may produce less task-relevant topics. The active learning approach optimizes for label efficiency but may miss diverse examples.
- Failure signatures: Poor classifier performance despite high topic coherence scores suggests a mismatch between topic quality metrics and task utility. If users consistently reject recommended documents, the preference function may be poorly calibrated. Low purity scores with high ANMI might indicate label fragmentation.
- First 3 experiments:
  1. Run all topic models on a small subset of the dataset and compare coherence scores to establish baseline performance
  2. Implement the active learning loop with a simple classifier and test document selection preferences
  3. Conduct a small-scale user study with 2-3 participants per group to validate the interface and methodology before full deployment

## Open Questions the Paper Calls Out

### Open Question 1
Do neural topic models consistently outperform classical models across different domains and languages? The study is limited to English datasets (Bills and 20newsgroups), and the authors explicitly acknowledge this limitation. Neural models may behave differently with languages that have different structural properties than English. Comparative studies of neural vs classical topic models across multiple languages with different structural properties (e.g., Chinese, Arabic, German) using the same evaluation framework would resolve this question.

### Open Question 2
How do task-based human evaluations compare to automated coherence metrics in predicting model performance for specific applications? The authors state "our simple simulated experiments serve as a reliable proxy" but acknowledge "relying solely on simulated evaluation metrics has limitations" and that "there is not a strong and direct relationship between coherence and human usability." Systematic comparison of task-based human evaluations versus automated metrics across multiple task types (classification, clustering, information retrieval) with diverse datasets and evaluation criteria would resolve this question.

### Open Question 3
What is the relative performance of human annotators versus LLMs in content analysis tasks using topic models? The authors state "with the rise of LLMs that can complete various tasks close to human level, the use of LLM to help with the process of label set generation...is a more efficient and cost-effective approach" and plan to conduct "further comparative analysis between human created labels and LLM created labels in our future work." Direct comparison studies measuring the quality of labels and content analysis results produced by human annotators versus LLMs, both independently and in combination with topic models, across multiple tasks and datasets would resolve this question.

## Limitations
- Task-based evaluation may not generalize across all content analysis scenarios
- User study sample size (15 participants per group) may limit statistical power
- Computational overhead of neural models could impact scalability in production environments

## Confidence

- **High confidence**: CTM outperforms other models on cluster evaluation metrics (Purity, ARI, ANMI) and demonstrates superior human evaluation performance
- **Medium confidence**: Neural topic models combined with active learning provide better label induction than classical models in task-based settings
- **Medium confidence**: Automated coherence metrics (NPMI) do not fully capture task-specific topic model utility for content analysis

## Next Checks
1. Conduct larger-scale user studies (minimum 30 participants per group) to verify statistical significance of observed performance differences
2. Test model performance across diverse datasets with varying topic granularity, document length, and domain characteristics
3. Evaluate computational efficiency and scalability of neural models versus classical approaches in production-like settings