---
ver: rpa2
title: Learning Discrete Concepts in Latent Hierarchical Models
arxiv_id: '2406.00519'
source_url: https://arxiv.org/abs/2406.00519
tags:
- latent
- variables
- discrete
- theorem
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes learning discrete concepts from high-dimensional
  data as the identification of discrete latent hierarchical causal models. The authors
  establish conditions under which such models are identifiable from continuous observations,
  extending prior work beyond tree and multi-level DAG structures.
---

# Learning Discrete Concepts in Latent Hierarchical Models

## Quick Facts
- arXiv ID: 2406.00519
- Source URL: https://arxiv.org/abs/2406.00519
- Reference count: 40
- This paper establishes conditions for identifying discrete latent hierarchical causal models from continuous observations, extending beyond tree and multi-level DAG structures.

## Executive Summary
This paper formalizes learning discrete concepts from high-dimensional data as the identification of discrete latent hierarchical causal models. The authors establish conditions under which such models are identifiable from continuous observations, extending prior work beyond tree and multi-level DAG structures. They prove that discrete components can be extracted from continuous data under invertibility and connectedness conditions, and that the full hierarchical structure can be identified using non-negative rank tests and t-separation. Experiments on synthetic data show their method outperforms baselines in structure learning (F1 scores of 0.93-0.98 vs 0.63-0.69). The authors also interpret latent diffusion models as hierarchical concept learners, where noise levels correspond to abstraction levels, and demonstrate this empirically through concept injection experiments and attention sparsity analysis.

## Method Summary
The paper proposes a two-stage approach for learning discrete concepts in latent hierarchical models. First, it identifies the bottom-level discrete variables and the bipartite graph connecting them to observed variables. Second, it uses rank-based tests and graph search algorithms to identify the full hierarchical structure. The method relies on conditions of invertibility and connectedness for the generating function, and uses non-negative rank tests to infer graph structures from joint probability tables. The approach is validated on synthetic hierarchical models with binary latent variables following a Gaussian mixture model structure.

## Key Results
- Discrete latent components can be identified from continuous observations under invertibility and connectedness conditions (F1 scores of 0.93-0.98 vs 0.63-0.69 on synthetic data)
- Hierarchical structure among discrete variables can be identified using non-negative rank tests and t-separation
- Latent diffusion models can be interpreted as hierarchical concept learners where noise levels correspond to abstraction levels
- Concept injection experiments show meaningful embeddings of injected concepts in diffusion representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discrete latent variables can be identified from continuous observations when the generating function preserves their information and the continuous support is connected.
- Mechanism: The generating function g maps each discrete state to a connected manifold in the observed space. Because g is invertible and continuous, these manifolds don't intersect, leaving a footprint in the observed data that allows unique identification of the discrete state.
- Core assumption: The generating function g is invertible and continuous, and the continuous support C is connected and closed.
- Evidence anchors:
  - [abstract] "We formulate conditions to facilitate the identification of the proposed causal model, which reveals when learning such concepts from unsupervised data is possible."
  - [section 4.2] "These manifolds do not intersect in the observed variable space X regardless of however close they may be to each other, thanks to the invertibility of the generating function g (Condition 4.3-ii)."
  - [corpus] Weak evidence - no direct mention of invertibility in corpus neighbors.

### Mechanism 2
- Claim: The hierarchical structure among discrete latent variables can be identified using non-negative rank tests and t-separation.
- Mechanism: Non-negative rank of the joint probability table between two variable sets reveals the minimal t-separation set's cardinality in the graph. This provides oracle information about local graph structures, which can be pieced together using a graph search algorithm.
- Core assumption: The discrete model is non-degenerate, faithful, and satisfies conditions on atomic covers and colliders.
- Evidence anchors:
  - [abstract] "Our conditions permit complex causal hierarchical structures beyond latent trees and multi-level directed acyclic graphs in prior work and can handle high-dimensional, continuous observed variables."
  - [section 4.3] "Theorem 4.8 gives a graph structure oracle equivalent to Theorem A2, which we leverage to prove Theorem 4.12."
  - [corpus] Weak evidence - no direct mention of non-negative rank tests in corpus neighbors.

### Mechanism 3
- Claim: Latent diffusion models can be interpreted as hierarchical concept learners where noise levels correspond to abstraction levels.
- Mechanism: The denoising objective at different noise levels can be viewed as performing auto-encoding at different hierarchical levels. Higher noise levels correspond to higher-level concepts in the causal model.
- Core assumption: The diffusion representation can be interpreted as concept embeddings, and the denoising objective performs auto-encoding at different hierarchical levels.
- Evidence anchors:
  - [abstract] "We discuss our theory's implications for understanding the underlying mechanisms of latent diffusion models and provide corresponding empirical evidence for our theoretical insights."
  - [section 6] "We adopt this perspective to interpret the diffusion model's representation through our hierarchical model, which connects the noise level and the hierarchical level of the latent representation in our causal model."
  - [corpus] Weak evidence - no direct mention of diffusion models in corpus neighbors.

## Foundational Learning

- Concept: Causal identification theory
  - Why needed here: The paper formalizes learning discrete concepts as a causal identification problem for discrete latent hierarchical models.
  - Quick check question: What are the conditions under which a causal structure can be identified from observational data?

- Concept: Graph theory (DAGs, t-separation, d-separation)
  - Why needed here: The paper uses graph theory to characterize the hierarchical structure among discrete latent variables and to derive identification conditions.
  - Quick check question: How does t-separation relate to d-separation in DAGs?

- Concept: Non-negative matrix factorization
  - Why needed here: The paper uses non-negative rank tests to infer graph structures from joint probability tables.
  - Quick check question: What is the relationship between non-negative rank and the minimal t-separation set in a discrete model?

## Architecture Onboarding

- Component map:
  - Discrete latent variables d and continuous latent variables c -> Generating function g -> Observed variables x
  - Global discrete state extraction -> Individual discrete component identification -> Bipartite graph discovery
  - Non-negative rank tests + t-separation -> Graph search algorithm -> Hierarchical structure inference

- Critical path:
  1. Verify conditions on the generating function and continuous support
  2. Extract global discrete state from observed data
  3. Identify individual discrete components and the bipartite graph
  4. Infer hierarchical structure using non-negative rank tests and t-separation

- Design tradeoffs:
  - Tradeoff between model complexity and identifiability: More complex hierarchical structures may be harder to identify
  - Tradeoff between computational cost and accuracy: Non-negative rank tests can be expensive, but provide more accurate graph inference

- Failure signatures:
  - If the generating function is not invertible or the continuous support is disconnected, the discrete components may not be identifiable
  - If the model is not faithful or does not satisfy conditions on atomic covers and colliders, the hierarchical structure may not be identifiable

- First 3 experiments:
  1. Verify that the generating function is invertible and the continuous support is connected
  2. Extract the global discrete state from observed data and check if it matches the true discrete state
  3. Infer the hierarchical structure using non-negative rank tests and t-separation, and compare with the true structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the conditions for discrete latent hierarchical model identification be relaxed to handle cases where the generating function g is not invertible?
- Basis in paper: [explicit] The paper states that Condition 4.3-ii requires the generating function g to be invertible and continuous.
- Why unresolved: The invertibility assumption is a strong constraint that may not hold in many practical scenarios, especially with complex data distributions.
- What evidence would resolve it: Developing alternative identification techniques that work under non-invertible generating functions, or proving that invertibility is necessary for certain types of hierarchical structures.

### Open Question 2
- Question: What are the implications of relaxing the "no-twins" condition (Condition 4.1-ii) for the identifiability of discrete latent variables?
- Basis in paper: [explicit] The paper mentions that Condition 4.1-ii ensures distinct latent variables have distinct neighbors.
- Why unresolved: This condition might be too restrictive in real-world applications where some latent variables might have identical neighborhoods.
- What evidence would resolve it: Experimental results showing the impact of relaxing this condition on model performance, or theoretical work proving when it can be safely relaxed.

### Open Question 3
- Question: How can the proposed theory be extended to handle continuous latent variables alongside discrete ones in the hierarchical structure?
- Basis in paper: [inferred] The paper focuses on discrete latent variables but mentions continuous observed variables and the continuous subspace c.
- Why unresolved: Many real-world concepts involve both discrete and continuous attributes, requiring a more general theory.
- What evidence would resolve it: Developing a unified framework that can handle mixed discrete-continuous latent structures, or proving the limitations of such extensions.

## Limitations
- Invertibility and connectedness conditions may be difficult to verify in practice
- Rank-based graph inference requires oracle access to non-negative rank information
- Experimental validation is currently limited to synthetic data, with no real-world datasets tested

## Confidence
- Theoretical claims: High (rigorous mathematical proofs and consistency with prior work)
- Practical applicability: Medium (pending empirical validation on real data and evaluation of computational scalability)
- Diffusion model interpretation: Medium (preliminary empirical support, largely theoretical)

## Next Checks
1. **Real Data Validation**: Apply the method to real-world datasets (e.g., image classification with hierarchical labels) to verify that the identified concepts align with human-interpretable categories and that the hierarchical structure captures meaningful abstraction levels.

2. **Scalability Assessment**: Evaluate computational complexity and performance on high-dimensional data (e.g., CIFAR-10 images) to determine practical limitations of the non-negative rank tests and graph search algorithms.

3. **Robustness Testing**: Systematically vary the generating function properties (e.g., near-singular transformations, partially disconnected supports) to identify breaking points where identification fails, providing practical guidance on method applicability.