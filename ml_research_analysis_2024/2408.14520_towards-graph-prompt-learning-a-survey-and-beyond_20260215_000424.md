---
ver: rpa2
title: 'Towards Graph Prompt Learning: A Survey and Beyond'
arxiv_id: '2408.14520'
source_url: https://arxiv.org/abs/2408.14520
tags:
- graph
- prompt
- learning
- node
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews graph prompt learning, a technique
  that adapts pre-trained models to graph-structured data through task-specific prompts,
  addressing the challenge of aligning pre-training and downstream tasks. The authors
  categorize over 100 works into design principles (tokenization, alignment, tuning),
  homogeneous and heterogeneous graph prompting, and applications like molecular graphs,
  text-attributed graphs, and recommendation systems.
---

# Towards Graph Prompt Learning: A Survey and Beyond

## Quick Facts
- **arXiv ID:** 2408.14520
- **Source URL:** https://arxiv.org/abs/2408.14520
- **Reference count:** 40
- **Key outcome:** This survey systematically reviews graph prompt learning, a technique that adapts pre-trained models to graph-structured data through task-specific prompts, addressing the challenge of aligning pre-training and downstream tasks.

## Executive Summary
Graph prompt learning adapts pre-trained models to graph-structured data by introducing task-specific prompts, offering a flexible and data-efficient alternative to traditional fine-tuning. The authors systematically categorize over 100 works into design principles (tokenization, alignment, tuning), homogeneous and heterogeneous graph prompting, and applications across molecular graphs, text-attributed graphs, and recommendation systems. While demonstrating practical utility in domains like molecular property prediction and streaming recommendations, the field faces challenges including lack of theoretical foundations, robustness evaluation, and standardized datasets. The survey outlines future directions for developing universal prompting mechanisms and improving expressiveness beyond traditional GNN limitations.

## Method Summary
Graph prompt learning modifies pre-trained graph neural networks (GNNs) through task-specific prompts that adapt the model to downstream tasks without full retraining. The approach involves designing prompt tokens (node, structure, or task-level) that are integrated into the pre-trained model, followed by optimization using task-specific loss functions. This method leverages the generalizable knowledge captured during pre-training while minimizing computational costs and data requirements compared to traditional fine-tuning paradigms.

## Key Results
- Graph prompt learning reduces labeled data requirements by adapting pre-trained models through lightweight prompts rather than full fine-tuning
- Task-specific prompts effectively bridge structural gaps between pre-training and downstream graphs
- Applications demonstrated in molecular property prediction (MolCPT) and streaming recommendation systems (GPT4Rec)
- Prompts enable quick adaptation to diverse tasks with minimal retraining compared to traditional methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph prompt learning reduces the need for extensive labeled data by adapting pre-trained models through task-specific prompts rather than full fine-tuning.
- Mechanism: Prompts act as lightweight, task-oriented inputs that guide pre-trained GNNs to focus on relevant features for the downstream task without retraining the entire model.
- Core assumption: Pre-trained GNNs capture generalizable structural knowledge that can be leveraged with minimal task-specific adaptation.
- Evidence anchors:
  - [abstract]: "Graph prompt learning is positioned as more flexible and data-efficient than traditional fine-tuning, enabling quick adaptation to diverse tasks with minimal retraining."
  - [section]: "Graph prompt learning modifies the input graph data or the way information is processed by the model through prompts...requiring minimal changes to the model itself."
  - [corpus]: "Graph prompt learning emerges as a promising alternative, leveraging the knowledge from pre-training without the need for extensive task-specific fine-tuning."
- Break condition: If the pre-trained model lacks generalizable knowledge for the target graph domain (e.g., molecular vs. social graphs), prompts may fail to bridge the gap.

### Mechanism 2
- Claim: Graph prompts address structural discrepancies between pre-training and downstream graphs by introducing task-specific or structure-aware tokens.
- Mechanism: Prompts encode missing structural or contextual information (e.g., motifs, semantic tokens) that align the pre-trained model's understanding with the downstream graph's topology.
- Core assumption: Structural gaps between pre-training and downstream graphs are a primary barrier to effective transfer learning.
- Evidence anchors:
  - [abstract]: "In graph-structured data, not only do the node and edge features often have disparate distributions, but the topological structures also differ significantly."
  - [section]: "Compared with node tokenization, structure tokenization provides a perspective to capture global information of a target node...to capture the expressive neighborhood information."
  - [corpus]: "The incompatibility of data patterns between pre-training graphs and fine-tuning graphs can result in negative effects of pre-training."
- Break condition: If prompts cannot adequately represent the structural differences (e.g., highly dissimilar motifs or graph densities), alignment may fail.

### Mechanism 3
- Claim: Task tokens enable rapid adaptation to diverse downstream tasks by encoding label-specific or task-relevant information.
- Mechanism: Task tokens act as learnable vectors that map pre-trained embeddings to task-specific outputs, reducing the need for retraining the entire model.
- Core assumption: Task-specific information can be effectively encoded as learnable tokens that guide the model's predictions.
- Evidence anchors:
  - [abstract]: "Graph prompt learning...enabling quick adaptation to diverse tasks with minimal retraining."
  - [section]: "Task tokens are designed to suit the various downstream tasks...Task tokens to align the task gaps for better performance."
  - [corpus]: "By providing task-specific prompts, these models can be guided to perform new tasks using their existing knowledge base."
- Break condition: If task tokens are too simplistic to capture complex task requirements, performance may degrade.

## Foundational Learning

- Concept: Tokenization in graph data
  - Why needed here: Graph data lacks natural segmentation like text, so tokenization decomposes graphs into manageable units (nodes, edges, subgraphs) for prompting.
  - Quick check question: How do node-level and structure-level tokens differ in their role within graph prompt learning?
- Concept: Pre-training and fine-tuning paradigms
  - Why needed here: Graph prompt learning builds on these paradigms but replaces full fine-tuning with prompt-based adaptation to reduce computational costs and data requirements.
  - Quick check question: What is the key difference between traditional fine-tuning and graph prompt tuning in terms of parameter updates?
- Concept: Self-supervised learning in graphs
  - Why needed here: Self-supervised pre-training (e.g., contrastive learning) provides the generalizable knowledge that graph prompts leverage for downstream tasks.
  - Quick check question: Why is self-supervised pre-training particularly useful for graph data compared to supervised methods?

## Architecture Onboarding

- Component map: Pre-trained GNN model → Prompt token generation → Task alignment → Prompt tuning → Downstream task prediction
- Critical path: Pre-trained model → Prompt token generation → Task alignment → Prompt tuning → Downstream task prediction
- Design tradeoffs:
  - Flexibility vs. complexity: More complex prompts (e.g., structure-aware) improve performance but increase computational overhead.
  - Generalization vs. specificity: Universal prompts may be less effective than task-specific ones but reduce the need for retraining.
- Failure signatures:
  - Poor performance on downstream tasks: Indicates inadequate prompt design or misalignment between pre-training and downstream data.
  - Overfitting to prompts: Suggests prompts are too task-specific and lack generalizability.
- First 3 experiments:
  1. Test prompt tuning on a small node classification task (e.g., Cora dataset) to validate basic functionality.
  2. Compare structure-aware prompts vs. node-level prompts on a molecular property prediction task to assess structural alignment.
  3. Evaluate task token effectiveness by varying the number of task-specific tokens on a link prediction task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can graph prompts transcend the inherent limitations of traditional GNNs and improve their expressiveness?
- Basis in paper: [inferred] The authors mention that future studies should explore how graph prompts can amplify the expressiveness of GNNs and whether they can transcend the inherent limitations of traditional GNNs.
- Why unresolved: There is currently no systematic investigation into the potential enhancement of expressiveness through graph prompts. The theoretical foundations for the effectiveness of graph prompts in the graph domain remain largely unexplored.
- What evidence would resolve it: Empirical studies comparing the performance of GNNs with and without graph prompts on tasks requiring high expressiveness, along with theoretical analysis of the mechanisms underlying the improvement in expressiveness.

### Open Question 2
- Question: How robust are graph prompt methods to adversarial attacks and optimization stability?
- Basis in paper: [explicit] The authors highlight the need for further investigation into the resilience of graph prompt methods to adversarial attacks, their stability during the optimization process, and rigorous evaluation of their overall robustness compared to traditional approaches across diverse scenarios.
- Why unresolved: Current methodologies lack empirical validation of the robustness of graph prompt methods to adversarial attacks and optimization stability. The overall robustness of graph prompt methods compared to traditional approaches across diverse scenarios is not well understood.
- What evidence would resolve it: Adversarial attack experiments on graph prompt methods, analysis of optimization stability, and comprehensive evaluation of robustness across various graph datasets and tasks.

### Open Question 3
- Question: Can a unified prompting mechanism be developed for graph learning tasks, similar to the universal applicability of prompts in NLP with LLMs?
- Basis in paper: [explicit] The authors note that graph prompts are less universally applicable in graph learning compared to prompts in NLP with LLMs. They mention that prompts must be tailored to the specific characteristics of individual graphs and tasks due to the significant variability in graph structures and attributes.
- Why unresolved: The significant variability in graph structures and attributes poses a challenge in developing a unified prompting mechanism for graph learning tasks. Current efforts focus on specialized graph prompts for domain-specific knowledge, which detracts from the goal of creating a generalized model.
- What evidence would resolve it: Development of a unified prompting mechanism that can be applied to diverse graph learning tasks with a single approach, along with empirical validation of its effectiveness across various graph datasets and tasks.

## Limitations

- Field remains in early stages with limited theoretical foundations for how prompts bridge structural gaps between pre-training and downstream graphs
- Most existing works focus on specific domains (molecular graphs, recommendation systems) with limited validation on diverse graph types
- Standardized datasets and evaluation protocols for graph prompt learning are still lacking, making cross-study comparisons difficult

## Confidence

**High Confidence (4 claims):**
- Graph prompt learning provides a flexible alternative to traditional fine-tuning for graph-structured data
- Prompt-based approaches can reduce data requirements compared to full model retraining
- The survey's taxonomy of design principles (tokenization, alignment, tuning) is comprehensive
- Applications in molecular graphs and recommendation systems demonstrate practical utility

**Medium Confidence (3 claims):**
- Structural discrepancies between pre-training and downstream graphs are a primary barrier to transfer learning
- Task tokens effectively encode label-specific information for diverse downstream tasks
- Graph prompt learning shows promise for real-time applications like streaming recommendations

**Low Confidence (2 claims):**
- The superiority of structure-aware prompts over node-level prompts across all graph types
- Universal prompt designs will be achievable for diverse graph domains without domain-specific adaptation

## Next Checks

1. **Cross-domain transferability validation**: Test the same prompt design on molecular graphs, social networks, and knowledge graphs to quantify generalization across structural domains and identify failure patterns.

2. **Theoretical grounding study**: Develop a formal analysis framework to characterize the alignment between pre-training graph structures and downstream graph distributions, quantifying when prompts can effectively bridge structural gaps.

3. **Standardized benchmark development**: Create a unified evaluation protocol with diverse graph datasets (varying sizes, heterophily levels, and domain types) to enable fair comparison of different graph prompt learning approaches and establish baseline performance metrics.