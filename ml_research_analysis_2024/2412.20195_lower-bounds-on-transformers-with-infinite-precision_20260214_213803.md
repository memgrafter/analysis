---
ver: rpa2
title: Lower bounds on transformers with infinite precision
arxiv_id: '2412.20195'
source_url: https://arxiv.org/abs/2412.20195
tags:
- dimension
- output
- lower
- transformer
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents a new lower bound technique for single-layer\
  \ softmax transformers with infinite precision using VC dimension analysis. The\
  \ method shows that for two tasks - function composition (Compn) and the SUM2 problem\
  \ - any transformer achieving these tasks must have either embedding dimension n^\u03A9\
  (1) or an output MLP of size n^\u03A9(1)."
---

# Lower bounds on transformers with infinite precision

## Quick Facts
- arXiv ID: 2412.20195
- Source URL: https://arxiv.org/abs/2412.20195
- Authors: Alexander Kozachinskiy
- Reference count: 7
- Primary result: New VC dimension technique shows transformers need n^Ω(1) embedding dimension or output MLP size for Comp_n and SUM_2 tasks

## Executive Summary
This paper introduces a novel lower bound technique for single-layer softmax transformers using VC dimension analysis. The method demonstrates that for two specific tasks - function composition (Comp_n) and the SUM_2 problem - any transformer achieving these tasks must have either embedding dimension n^Ω(1) or an output MLP of size n^Ω(1). This contrasts with the palindrome recognition task, which can be solved with constant embedding dimension and constant-size output MLP under infinite precision. The key insight is that while SUM_2 requires high VC dimension due to its connection to the disjointness problem in communication complexity, palindrome recognition can be solved with low VC dimension through a clever encoding scheme.

## Method Summary
The paper employs a VC dimension analysis technique to establish lower bounds on transformer architecture size. The method leverages Theorem 2.3 from [3], which states that VC dimension is polynomial in the number of parameters, arithmetic operations, and conditional jumps needed to compute a hypothesis class. By showing that the output computation of transformers can be expressed using basic arithmetic operations and conditional jumps, the technique bounds the VC dimension to provide lower bounds on required architecture size. The approach is applied to two tasks - Comp_n and SUM_2 - demonstrating that both require either embedding dimension n^Ω(1) or output MLP size n^Ω(1), while palindrome recognition can be solved with constant parameters.

## Key Results
- For Comp_n task, transformers require embedding dimension n^Ω(1) or output MLP size n^Ω(1)
- For SUM_2 task, transformers require embedding dimension n^Ω(1) or output MLP size n^Ω(1)
- Palindrome recognition task can be solved with constant embedding dimension and constant-size output MLP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The VC dimension lower bound technique works by showing that hypothesis classes computed with basic arithmetic operations have polynomial VC dimension in their parameters and computational complexity
- Mechanism: The paper leverages Theorem 2.3 from [3] which states that VC dimension is polynomial in the number of parameters, arithmetic operations, and conditional jumps needed to compute a hypothesis class. Since the transformer's output computation can be expressed as a parametric family of functions using these basic operations, bounding the VC dimension provides a lower bound on the required architecture size
- Core assumption: The output MLP uses ReLU activation and the computation involves only basic arithmetic operations and conditional jumps
- Evidence anchors:
  - [abstract]: "Our technique employs upper bounds on VC dimension of hypothesis classes, computable with a small number of basic arithmetic operations"
  - [section 3]: "it can be computed in no(1) basic arithmetic operations and conditional jumps, based on comparing a real number with 0"
  - [corpus]: Weak - related papers don't discuss VC dimension techniques specifically

### Mechanism 2
- Claim: The SUM_2 task requires high VC dimension (at least n/2) due to its connection to the disjointness problem in communication complexity
- Mechanism: The SUM_2 task is shown to be equivalent to the disjointness problem where two parties must determine if their binary vectors have a position where both have 1. The VC dimension of the communication matrix for disjointness is n, which forces the transformer to have either embedding dimension n^Ω(1) or output MLP size n^Ω(1)
- Core assumption: The transformer architecture with infinite precision can simulate communication protocols between the two halves of the input
- Evidence anchors:
  - [section 3]: "The key factor is that the VC dimension of the communication matrix of the disjointness problem is n for n-bit strings"
  - [abstract]: "For both of these tasks, we show that any 1-layer softmax transformer for them must have either embedding dimension n^Ω(1) or the output MLP of size n^Ω(1)"
  - [corpus]: Missing - related papers don't discuss communication complexity connections

### Mechanism 3
- Claim: The palindrome task can be solved with constant embedding dimension and constant-size output MLP by encoding the difference between symmetric positions into a single numerical value
- Mechanism: The transformer can compute a weighted sum of differences between symmetric positions (a1-b1 + 10^-1(a2-b2) + ... + 10^-(k+1)(ak-bk)), which equals zero if and only if the input is a palindrome. This single number can be checked by a constant-size MLP
- Core assumption: Infinite precision allows representing all necessary differences without overflow or precision loss
- Evidence anchors:
  - [section 3]: "It is easy to construct a softmax layer that computes a number, proportional to: (a1 - b1) + 10^-1(a2 - b2) + ... + 10^-(k+1)(ak - bk)"
  - [abstract]: "It can be solved with constant embedding dimension and constant-size output MLP, assuming infinite precision"
  - [corpus]: Weak - related papers don't discuss palindrome recognition techniques

## Foundational Learning

- Concept: VC dimension theory
  - Why needed here: The entire lower bound argument relies on bounding the VC dimension of hypothesis classes computed by transformers
  - Quick check question: What is the VC dimension of a linear classifier in d dimensions?

- Concept: Communication complexity
  - Why needed here: The lower bound for SUM_2 uses a reduction from the disjointness problem, which is a fundamental problem in communication complexity
  - Quick check question: What is the communication complexity of the equality problem for n-bit strings?

- Concept: Transformer architecture with infinite precision
  - Why needed here: The paper assumes infinite precision arithmetic, which changes what functions can be computed compared to finite precision models
  - Quick check question: How does infinite precision affect the computational power of a single softmax layer?

## Architecture Onboarding

- Component map:
  - Positional encoding p: Maps position-symbol pairs to embedding vectors
  - Key matrix K and query matrix Q: Define attention weights via softmax
  - Value vectors f1...fn: Computed from positional encoding of input tokens
  - Output MLP N: Final classification layer with ReLU activation
  - Convex combination: Weighted sum of value vectors using attention weights

- Critical path:
  1. Compute value vectors from input using positional encoding
  2. Calculate attention weights using key-query product and softmax
  3. Form convex combination of value vectors
  4. Apply output MLP to combined vector
  5. Threshold output to get binary decision

- Design tradeoffs:
  - Embedding dimension vs. output MLP size: Tradeoff exploited in lower bounds
  - Precision requirements: Infinite precision vs. practical finite precision implementations
  - Single-layer vs. multi-layer: This work focuses on single-layer transformers

- Failure signatures:
  - If VC dimension bounds are violated, the lower bound technique fails
  - If infinite precision assumption is broken, palindrome recognition may require larger architecture
  - If attention weights cannot distinguish relevant input patterns, tasks may be unsolvable

- First 3 experiments:
  1. Implement the VC dimension calculation for a simple parametric family to verify the polynomial bound
  2. Test the palindrome recognition algorithm on small examples to verify the weighted sum approach
  3. Verify the SUM_2 reduction by implementing the communication complexity argument on small instances

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between VC dimension and the computational power of transformers with infinite precision?
- Basis in paper: [explicit] The paper shows that VC dimension analysis can prove lower bounds for transformers with infinite precision, contrasting with previous techniques that required bounded precision.
- Why unresolved: The paper demonstrates the technique on two specific problems (Comp_n and SUM_2) but doesn't provide a general characterization of when VC dimension analysis is tight or when it might give overly pessimistic bounds.
- What evidence would resolve it: A systematic study comparing VC dimension bounds against actual transformer capabilities across diverse computational problems would help characterize the relationship.

### Open Question 2
- Question: Can the VC dimension technique be extended to multi-layer transformers with infinite precision?
- Basis in paper: [inferred] The paper focuses exclusively on single-layer transformers, and previous work has shown that multi-layer transformers are significantly more powerful, suggesting the single-layer restriction may be crucial for the VC dimension approach to work.
- Why unresolved: The paper doesn't explore whether the VC dimension technique can be adapted to handle multiple layers, which would be a significant extension of the current results.
- What evidence would resolve it: Either a proof that the VC dimension technique can be extended to multi-layer architectures, or a demonstration that the technique fundamentally breaks down for deeper networks.

### Open Question 3
- Question: What is the exact threshold of precision needed for transformers to compute SUM_2 efficiently?
- Basis in paper: [explicit] The paper shows that SUM_2 requires high VC dimension, contrasting with Palindrome which can be solved with low VC dimension, suggesting that SUM_2 requires more precise computation.
- Why unresolved: The paper doesn't quantify the minimum precision required for SUM_2, only showing that infinite precision allows it to be solved while bounded precision (via communication complexity) makes it hard.
- What evidence would resolve it: An analysis determining the exact precision threshold above which SUM_2 becomes efficiently computable by transformers would clarify this relationship.

### Open Question 4
- Question: How does the VC dimension technique compare to other lower bound techniques for transformers in terms of tightness and generality?
- Basis in paper: [explicit] The paper contrasts the VC dimension technique with previous communication complexity-based approaches, showing advantages for infinite precision transformers.
- Why unresolved: The paper doesn't provide a systematic comparison of VC dimension bounds against other techniques across multiple problems to evaluate their relative strengths and weaknesses.
- What evidence would resolve it: A comprehensive study applying multiple lower bound techniques to a suite of benchmark problems would allow for direct comparison of their effectiveness.

## Limitations
- Assumes infinite precision arithmetic, which may not hold in practical implementations
- Applies only to single-layer transformers and may not extend to deeper architectures
- Provides information-theoretic lower bounds that don't account for computational efficiency or learnability constraints

## Confidence

**High confidence**: The VC dimension technique for computing hypothesis classes with basic arithmetic operations
**Medium confidence**: The specific lower bounds for Comp_n and SUM_2 tasks
**Medium confidence**: The palindrome recognition solution with constant embedding dimension

## Next Checks

1. Verify the VC dimension polynomial bound from Theorem 2.3 on simple parametric families to ensure the theoretical foundation is sound
2. Implement the SUM_2 communication complexity reduction on small examples (n=3,4) to confirm the connection to disjointness problem
3. Test the palindrome recognition algorithm with various non-palindrome inputs to verify the weighted sum approach correctly identifies all negative cases