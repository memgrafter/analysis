---
ver: rpa2
title: On the Power of Randomization in Fair Classification and Representation
arxiv_id: '2406.03142'
source_url: https://arxiv.org/abs/2406.03142
tags:
- u1d45f
- classi
- u1d456
- fair
- u1d453
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the power of randomization in fair classification
  and representation learning to minimize accuracy loss from fairness constraints.
  The authors characterize optimal randomized classifiers for Demographic Parity (DP),
  Equal Opportunity (EO), and Predictive Equality (PE) that can surpass deterministic
  counterparts in accuracy, showing these can be obtained as solutions to convex optimization
  problems.
---

# On the Power of Randomization in Fair Classification and Representation

## Quick Facts
- arXiv ID: 2406.03142
- Source URL: https://arxiv.org/abs/2406.03142
- Reference count: 40
- Key outcome: Randomization in fair classification and representation learning can minimize accuracy loss from fairness constraints, with optimal solutions obtained via convex optimization

## Executive Summary
This paper examines the power of randomization in fair classification and representation learning to minimize accuracy loss from fairness constraints. The authors characterize optimal randomized classifiers for Demographic Parity (DP), Equal Opportunity (EO), and Predictive Equality (PE) that can surpass deterministic counterparts in accuracy. They show these can be obtained as solutions to convex optimization problems. The paper also constructs randomized fair representations for these fairness notions that incur zero cost of fair representation, meaning no accuracy loss compared to optimal fair classifiers on the original data distribution. This improves upon previous work that primarily focused on DP and provided weak or no accuracy guarantees.

## Method Summary
The paper studies optimal randomized fair classifiers and fair representations under DP, EO, and PE constraints in the group-aware setting. The optimal randomized fair classifiers are characterized as mass-threshold or groupwise mass-threshold classifiers, obtainable via convex optimization over selection rates or false positive rates. Fair representations are constructed by mapping cells of the original distribution to distinct points in a new representation space such that the optimal classifier on this representation incurs zero cost of fairness. The approach assumes full knowledge of the joint distribution over features, groups, and labels.

## Key Results
- Randomized fair classifiers can surpass deterministic fair classifiers in accuracy by allowing thresholding by probability mass rather than just scores
- Optimal randomized fair classifiers can be obtained as solutions to convex optimization problems
- Randomized fair representations can achieve zero cost of fair representation compared to optimal fair classifiers on original data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomized fair classifiers can surpass deterministic fair classifiers in accuracy.
- Mechanism: Randomization allows thresholding by probability mass instead of just scores, enabling more fine-grained decision boundaries that satisfy fairness constraints while preserving accuracy.
- Core assumption: The data distribution has non-trivial variation in class probabilities within fairness-constrained groups.
- Evidence anchors:
  - [abstract]: "The key insight is that randomization allows thresholding by probability mass rather than just scores, enabling more accurate fair classifiers and representations."
  - [section 3.1]: "Example 2.8...consider the following randomized classifier...It is easy to see that satisfies DP, and , hence improving the accuracy of the optimal fair deterministic classifiers and ."

### Mechanism 2
- Claim: Optimal randomized fair classifiers can be obtained as solutions to convex optimization problems.
- Mechanism: The loss function for randomized fair classifiers is convex and piecewise linear over the probability mass threshold space, making optimization tractable.
- Core assumption: The fairness constraints (DP, EO, PE) create well-defined boundary conditions that result in convex optimization problems.
- Evidence anchors:
  - [abstract]: "They also show how the optimal randomized fair classifier that they characterize can be obtained as a solution to a convex optimization problem."
  - [section 3.1.1]: "Theorem 2. is a convex, piecewise linear, continuous function over [0, 1]. The optimal classifier as in Theorem 1...is therefore the solution to a convex optimisation problem."

### Mechanism 3
- Claim: Randomized fair representations can achieve zero cost of fair representation compared to optimal fair classifiers on original data.
- Mechanism: The fair representation construction maps cells to distinct representation points, preserving the optimal fairness-accuracy tradeoff by allowing all fair classifiers to be expressible in the representation space.
- Core assumption: The cell-based partition of the feature space captures all relevant information for fair classification.
- Evidence anchors:
  - [abstract]: "They also construct randomized fair representations for these fairness notions that incur zero cost of fair representation, meaning no accuracy loss compared to optimal fair classifiers on the original data distribution."
  - [section 4.1]: "Theorem 5. There exists a DP-fair representation , such that cost of fairness for is 0."

## Foundational Learning

- Concept: Convex optimization
  - Why needed here: The paper relies on solving convex optimization problems to find optimal randomized fair classifiers
  - Quick check question: What property of the loss function makes it amenable to convex optimization methods?

- Concept: Probability mass partitioning
  - Why needed here: Randomization allows thresholding by probability mass rather than scores, requiring understanding of how probability mass partitions work
  - Quick check question: How does probability mass thresholding differ from score-based thresholding in terms of flexibility?

- Concept: Group-wise sorted cells
  - Why needed here: The construction of fair classifiers and representations relies on organizing feature space into cells sorted by group and score
  - Quick check question: What information is preserved when cells are sorted group-wise by score?

## Architecture Onboarding

- Component map:
  - Data distribution P over features, groups, and labels
  - Cell partitioning mechanism that creates disjoint components
  - Score calculation function for each cell
  - Group-wise sorting algorithm for cells
  - Randomized threshold selection mechanism
  - Convex optimization solver for finding optimal thresholds

- Critical path:
  1. Partition feature space into cells
  2. Calculate scores for each cell
  3. Sort cells group-wise by descending score
  4. Apply probability mass thresholding to select cells
  5. Optimize threshold to minimize loss subject to fairness constraints

- Design tradeoffs:
  - Granularity of cell partitioning vs. computational complexity
  - Deterministic vs. randomized thresholding for implementation simplicity
  - Exact vs. approximate fairness constraints for practical deployment

- Failure signatures:
  - Non-convex loss landscape indicating broken optimization assumptions
  - Zero variance in cell scores within groups indicating no benefit from randomization
  - Loss function discontinuities suggesting partitioning issues

- First 3 experiments:
  1. Verify convex optimization works on synthetic data with known optimal solutions
  2. Test randomization advantage on data with clear score variation within fairness-constrained groups
  3. Validate zero-cost representation property by comparing classifier accuracy before and after representation mapping

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we extend the results on randomized fair classifiers to multi-class classification problems?
- Basis in paper: [explicit] The authors conclude by mentioning "extending our results for binary classification to multi-class classification and regression" as a future direction
- Why unresolved: The current characterization only handles binary classification, and multi-class scenarios introduce different complexity in fairness constraints
- What evidence would resolve it: A mathematical framework showing optimal randomized fair classifiers for K-class problems under DP, EO, and PE constraints

### Open Question 2
- Question: Can we characterize optimal randomized fair classifiers under relaxed or approximate fairness notions?
- Basis in paper: [explicit] The authors mention looking at "relaxed or approximate versions of the fairness notions we considered" as a potential direction
- Why unresolved: Current results assume exact fairness constraints, but practical implementations often require approximate fairness due to trade-offs with accuracy
- What evidence would resolve it: Theoretical bounds on the trade-off between approximation in fairness constraints and accuracy loss

### Open Question 3
- Question: What is the empirical performance of the proposed randomized fair representations compared to heuristic approaches?
- Basis in paper: [inferred] The authors construct fair representations with zero cost of fairness theoretically but acknowledge the need for "experimentally validate our theoretical claims"
- Why unresolved: While theoretical guarantees are established, real-world performance may differ due to distribution shifts and implementation constraints
- What evidence would resolve it: Comparative experiments on benchmark datasets showing accuracy and fairness metrics for both proposed and existing fair representation methods

## Limitations

- The theoretical guarantees assume full knowledge of the joint distribution P, which is unrealistic in practice and the paper does not address how finite sample estimation affects performance
- The analysis assumes discrete feature spaces or well-behaved continuous distributions, potentially limiting applicability to real-world data with complex feature interactions
- The construction of groupwise sorted cells and identification of score boundaries requires careful implementation and may become computationally prohibitive for high-dimensional feature spaces

## Confidence

- **High confidence**: The characterization of optimal randomized fair classifiers as mass-threshold classifiers for DP, and the convexity of the corresponding optimization problem
- **Medium confidence**: The extension to EO and PE constraints, which requires more complex groupwise mass-threshold classifiers and FP-boundaries
- **Low confidence**: The practical performance of the proposed methods on real-world datasets with continuous features and finite samples

## Next Checks

1. Implement the convex optimization framework on synthetic datasets with known optimal solutions to verify that the optimization consistently finds the correct thresholds and that the resulting classifiers satisfy the specified fairness constraints

2. Conduct experiments comparing the performance of randomized fair classifiers learned from finite samples versus the theoretical optimal classifiers with full distribution knowledge, quantifying the gap and assessing whether standard estimation techniques can effectively approximate the optimal solutions

3. Evaluate the computational complexity of the cell partitioning and boundary identification process on datasets of increasing dimensionality to determine whether the approach remains tractable for moderate-dimensional feature spaces and identify bottlenecks in the current implementation