---
ver: rpa2
title: Bayesian Adaptive Calibration and Optimal Design
arxiv_id: '2405.14440'
source_url: https://arxiv.org/abs/2405.14440
tags:
- design
- variational
- posterior
- calibration
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for Bayesian calibration of computer
  models that jointly optimizes simulation designs and calibration parameters to maximize
  information gain. The approach uses a variational lower bound on the expected information
  gain, enabling efficient estimation of the posterior distribution over calibration
  parameters.
---

# Bayesian Adaptive Calibration and Optimal Design

## Quick Facts
- arXiv ID: 2405.14440
- Source URL: https://arxiv.org/abs/2405.14440
- Reference count: 40
- Key outcome: BACON achieves better calibration accuracy and information gain compared to baselines through joint optimization of simulation designs and calibration parameters

## Executive Summary
This paper proposes BACON (Bayesian Adaptive Calibration and Optimal Design), a method for Bayesian calibration of computer models that jointly optimizes simulation designs and calibration parameters to maximize information gain. The approach uses a variational lower bound on expected information gain, enabling efficient estimation of the posterior distribution over calibration parameters while simultaneously selecting informative simulation designs. Experiments on synthetic and real-data problems demonstrate that BACON outperforms existing baselines in terms of calibration accuracy and information gain, while also providing an accurate Gaussian process-based emulator for the computer model.

## Method Summary
BACON combines Bayesian experimental design with Gaussian process modeling to jointly optimize simulation inputs (designs and calibration parameters) and estimate the posterior distribution over calibration parameters. The method uses a bi-fidelity Gaussian process to model both real observations and simulator outputs, capturing correlations between these data sources. Instead of directly optimizing expected information gain (which requires intractable posterior integration), BACON maximizes a variational lower bound using a parametric conditional density (implemented as a conditional normalizing flow) to approximate the posterior. This allows efficient sequential updates where each iteration selects the most informative simulation design while simultaneously updating the posterior estimate.

## Key Results
- BACON achieves lower MAP error and RMSE compared to random search and IMSPE baselines
- The method provides better posterior estimates as measured by KL divergence metrics
- BACON's GP emulator accurately captures the simulator behavior while being more sample-efficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimization of simulation inputs (designs and calibration parameters) captures informative correlations across both spaces, improving calibration accuracy.
- Mechanism: The bi-fidelity Gaussian process model jointly models real observations and simulator outputs. By maximizing the variational lower bound of expected information gain with respect to both simulator inputs and the variational posterior, the algorithm selects simulation settings that are maximally informative about the calibration parameters. This joint optimization leverages correlations between the design space and calibration parameter space.
- Core assumption: The simulator and real process can be modeled as different levels of fidelity of the same underlying process.
- Evidence anchors:
  - [abstract]: "the simulator is modelled as a sample from a Gaussian process, which allows us to correlate simulations and observed data with the unknown calibration parameters"
  - [section 2]: "Let ˆ yS := [h(ˆ xi, ˆθi)]S i=1 represent simulated outcomes for a set of designs bXS := {ˆ xi}S i=1 ⊂ X and simulation parametersbΘS := {ˆθi}S i=1 ⊂ Θ"
- Break condition: If the simulator cannot be modeled as a GP, or if the fidelity correlation structure does not hold, the joint optimization loses its advantage.

### Mechanism 2
- Claim: The variational lower bound of expected information gain enables tractable simultaneous estimation of the posterior and optimal designs.
- Mechanism: The EIG objective involves an intractable posterior p(θ*|y, x, θ, Dt-1). The method replaces this with a variational lower bound dEIGt using a parametric conditional density q(θ*|y, x, θ). This allows joint optimization of designs and calibration parameters by maximizing Eq(θ*|y) - p(θ*|Dt-1) under the variational posterior.
- Core assumption: A flexible parametric conditional density q can approximate the true posterior well enough to guide optimal design selection.
- Evidence anchors:
  - [section 5.1]: "Following [13], we replace the EIG by a variational objective which does not directly involve the true posterior over θ*"
  - [section 5.3]: "Any conditional probability density model q(θ*|y) estimating probability densities over the parameter space Θ given an observation y could suit our method"
- Break condition: If the variational family cannot capture the true posterior structure (e.g., multi-modality), the optimization may converge to suboptimal designs.

### Mechanism 3
- Claim: The Gaussian process emulator for the simulator provides a computationally efficient surrogate that correlates simulations and real data.
- Mechanism: By modeling the simulator as a sample from a GP with a bi-fidelity covariance function, the method can compute the posterior predictive distribution p(y*|θ*, x, θ, Dt) in closed form. This enables efficient estimation of the EIG lower bound and subsequent optimization.
- Core assumption: The simulator output can be modeled as a sample from a GP, and the GP covariance function can capture the correlation structure between simulations and real data.
- Evidence anchors:
  - [section 4]: "Since both h and ε are GPs, simulations and real outcomes can be jointly modelled as a single Gaussian process"
  - [section 5.2]: "The simulator is modelled as a sample from a Gaussian process, which allows us to correlate simulations and observed data with the unknown calibration parameters"
- Break condition: If the simulator output violates GP assumptions (e.g., non-stationarity, discontinuities), the GP emulator becomes inaccurate.

## Foundational Learning

- Concept: Bayesian experimental design and information gain
  - Why needed here: The method optimizes simulation designs based on their expected information gain about calibration parameters
  - Quick check question: How does the mutual information I(θ*; y*|y, x, θ) relate to the expected information gain (EIG) in the context of sequential experimental design?

- Concept: Gaussian processes and covariance functions
  - Why needed here: The method relies on modeling the simulator as a sample from a GP with a specific bi-fidelity covariance function
  - Quick check question: How does the bi-fidelity covariance function k(z, z') = kρ(s, s')k̂((x, θ), (x', θ')) + ss'kε(x, x') capture the correlation between real and simulated data?

- Concept: Variational inference and normalizing flows
  - Why needed here: The method uses variational inference to approximate the posterior distribution over calibration parameters
  - Quick check question: How does the conditional normalizing flow qϕ(θ*|y) = log pK(θ*; rϕ(y)) approximate the posterior distribution?

## Architecture Onboarding

- Component map: GP model -> Variational posterior -> EIG optimizer -> Data management -> Hyperparameter adaptation
- Critical path:
  1. Initialize GP model and variational posterior with real data and initial simulations
  2. Optimize EIG lower bound to select next design (x, θ) and update variational posterior
  3. Run simulation with selected design and observe outcome
  4. Update GP model and variational posterior with new data
  5. Repeat steps 2-4 for desired number of iterations
- Design tradeoffs:
  - Fixed vs. adaptive designs: Adaptive designs (joint optimization) vs. fixed designs (separate optimization)
  - Exact vs. variational inference: Exact posterior (MCMC) vs. variational approximation (computational efficiency)
  - Sparse vs. full GP: Sparse GP approximation (scalability) vs. full GP (accuracy)
- Failure signatures:
  - Poor calibration accuracy: Suboptimal designs due to inaccurate GP emulator or variational posterior
  - High computational cost: Expensive EIG optimization or GP inference for large datasets
  - Degenerating posterior: Variational posterior losing accuracy over iterations due to reuse
- First 3 experiments:
  1. Synthetic experiment: Sample function from GP prior, run BACON for T iterations, compare MAP error and RMSE to baselines
  2. Batch optimization: Modify BACON to optimize batches of designs, compare information gain to sequential version
  3. Scalable GP: Implement sparse GP approximation, compare computational cost and accuracy to full GP for large datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of variational posterior distribution (e.g., conditional Gaussian vs. normalising flows) impact the quality of the final posterior estimates and information gain in BACON?
- Basis in paper: [explicit] The paper mentions two types of variational posteriors: conditional Gaussian models and conditional normalising flows. It notes that conditional normalising flows better capture multi-modality in the posterior.
- Why unresolved: The paper does not provide a detailed comparison of the performance of these two types of variational posteriors on the experimental problems.
- What evidence would resolve it: A direct comparison of BACON using both types of variational posteriors on the same set of experimental problems, evaluating their performance in terms of posterior accuracy, information gain, and computational efficiency.

### Open Question 2
- Question: How does the batch size in the batch sequential setting affect the convergence and performance of BACON compared to single-point updates?
- Basis in paper: [explicit] The paper discusses the use of batch parallel evaluations, where a batch of simulation inputs is optimized jointly. It mentions that proposing batches of simulation inputs can be more effective than running single simulations in a sequence.
- Why unresolved: The paper does not provide a detailed analysis of how the batch size impacts the performance of BACON. It only mentions that it allows for parallel evaluations.
- What evidence would resolve it: An experimental study varying the batch size in the batch sequential setting and comparing the convergence speed, information gain, and computational cost of BACON with different batch sizes.

### Open Question 3
- Question: How does the scalability of BACON to large datasets and high-dimensional parameter spaces compare to other Bayesian calibration methods, such as those based on sparse GPs or deep GPs?
- Basis in paper: [explicit] The paper acknowledges that the computational complexity of exact inference with GPs is cubic, which limits scalability to large datasets. It proposes extensions using sparse variational GPs to address this issue.
- Why unresolved: The paper does not provide a comprehensive comparison of BACON's scalability with other methods on large-scale problems.
- What evidence would resolve it: A comparative study of BACON (with and without sparse GP extensions) and other Bayesian calibration methods on problems with large datasets and high-dimensional parameter spaces, evaluating their scalability and performance.

## Limitations

- The bi-fidelity GP assumption may not hold for simulators with complex non-stationary behavior or discontinuities
- Computational complexity scales cubically with the number of data points, limiting applicability to large-scale problems
- The method assumes the ability to run simulations at arbitrary design points, which may not be feasible for expensive simulators

## Confidence

- Mechanism 1 (joint optimization): High - well-supported by theoretical framework and experimental results
- Mechanism 2 (variational inference): Medium - effective in practice but limited by variational family expressiveness
- Mechanism 3 (GP emulator): Medium - relies on strong GP assumptions that may not hold for all simulators

## Next Checks

1. Test BACON on a simulator with known multi-modal posterior to evaluate variational approximation quality and compare with MCMC-based baselines
2. Evaluate computational scaling by running experiments with increasing numbers of simulation points and measuring wall-clock time
3. Apply BACON to a real-world simulator with known non-stationary behavior to assess GP assumption validity