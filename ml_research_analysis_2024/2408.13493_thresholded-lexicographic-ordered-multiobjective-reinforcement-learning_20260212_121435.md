---
ver: rpa2
title: Thresholded Lexicographic Ordered Multiobjective Reinforcement Learning
arxiv_id: '2408.13493'
source_url: https://arxiv.org/abs/2408.13493
tags:
- objective
- will
- algorithm
- function
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a policy gradient method for solving lexicographic
  multi-objective reinforcement learning (MORL) problems. Existing TLQ approaches
  suffer from theoretical limitations and practical issues, including failure to reach
  goal states.
---

# Thresholded Lexicographic Ordered Multiobjective Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.13493
- Source URL: https://arxiv.org/abs/2408.13493
- Authors: Alperen Tercan; Vinayak S. Prabhu
- Reference count: 40
- Primary result: Introduces Lexicographic REINFORCE policy gradient method that successfully solves MORL problems where thresholded lexicographic Q-learning (TLQ) fails

## Executive Summary
This paper addresses the challenge of solving lexicographic multi-objective reinforcement learning (MORL) problems where objectives must be optimized in a strict priority order. Existing thresholded lexicographic Q-learning approaches suffer from theoretical limitations and practical failures, including inability to reach goal states. The authors propose a Lexicographic Projection Algorithm (LPA) that uses hypercone projections to optimize lower-priority objectives while preserving satisfaction of higher-priority ones. They adapt REINFORCE to create Lexicographic REINFORCE, which successfully solves benchmark MORL problems where TLQ fails, including maze navigation tasks with primary goal-reaching and secondary path-quality objectives.

## Method Summary
The method introduces a policy gradient approach for lexicographic MORL that avoids the non-Markovian value function issues plaguing TLQ variants. The core innovation is the Lexicographic Projection Algorithm (LPA), which projects gradients onto hypercones defined by higher-priority objectives' gradients. The algorithm computes gradients for each objective independently and uses FindDirection to project gradients onto hypercones of previous objectives, ensuring that optimization of lower-priority objectives doesn't degrade higher-priority ones. The policy network uses a two-layer neural architecture with one-hot state encoding, ReLU activation, dropout, and softmax output.

## Key Results
- Lexicographic REINFORCE successfully solves maze navigation tasks where TLQ fails to reach goal states
- The method demonstrates superior performance on Fruit Tree Navigation benchmark with endpoint constraints
- Hypercone projection proves more effective than hyperplane projection for maintaining objective satisfaction during optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexicographic projection prevents previously satisfied objectives from degrading during optimization of lower-priority ones.
- Mechanism: When optimizing a lower-priority objective, the gradient is projected onto a hypercone defined by higher-priority objectives' gradients, ensuring that movement directions maintain or improve higher-priority objectives.
- Core assumption: The projection preserves the directional derivative properties needed to maintain higher-priority objective values.
- Evidence anchors: [abstract] "We propose a Lexicographic Projection Algorithm (LPA) that uses hypercone projections to optimize objectives while preserving satisfaction of higher-priority ones." [section 5] "We can use hypercone projection to avoid this..."

### Mechanism 2
- Claim: Policy gradient methods avoid the non-Markovian value function issues that plague value-function approaches in lexicographic settings.
- Mechanism: By directly optimizing policy parameters rather than learning value functions, policy gradient methods bypass the circular dependency between value functions and acceptable policies that causes convergence issues in TLQ.
- Core assumption: The policy parameterization is expressive enough to represent optimal policies for the lexicographic objectives.
- Evidence anchors: [abstract] "The method shows promise for addressing theoretical and practical limitations of value-function-based approaches to MORL." [section 4.1] "However, we discovered that these works on using a Q-learning variant with thresholded ordering perform very poorly..."

### Mechanism 3
- Claim: The conservativeness parameter ∆ allows tuning the trade-off between strict preservation of higher-priority objectives and aggressive optimization of lower-priority ones.
- Mechanism: Larger ∆ values create wider hypercones, allowing more aggressive movement directions that may slightly reduce higher-priority objectives but enable faster progress on lower-priority ones.
- Core assumption: The relationship between ∆ and the acceptable level of degradation for higher-priority objectives is monotonic and predictable.
- Evidence anchors: [section 5] "A hypercone is the set of vectors that make at most a π/2 − ∆ angle with the axis vector..." [section 6.1] "The results also suggest that the hyperplane projection method of [30] is not sufficient..."

## Foundational Learning

- Concept: Hypercone projections in multi-objective optimization
  - Why needed here: The core innovation relies on projecting gradients onto hypercones rather than simpler halfspaces to balance objective satisfaction
  - Quick check question: What is the geometric difference between projecting onto a hypercone versus a halfspace, and why does this matter for lexicographic optimization?

- Concept: Policy gradient methods vs. value-function methods
  - Why needed here: Understanding why policy gradients avoid the circular dependency issues that plague TLQ variants
  - Quick check question: What is the fundamental difference in how policy gradient and value-function methods approach the optimization problem?

- Concept: Thresholded lexicographic ordering (TLO)
  - Why needed here: The entire problem formulation and algorithm design depend on understanding how TLO compares objective vectors
  - Quick check question: How does TLO differ from linear scalarization in terms of user preference specification and the set of Pareto optimal solutions it can represent?

## Architecture Onboarding

- Component map: Environment -> Policy Network -> Action Selection -> Environment -> Gradient Computation -> Lexicographic Projection Algorithm (LPA) -> Optimizer -> Policy Parameters

- Critical path: 1. Environment generates state and reward vector 2. Policy network outputs action probabilities 3. Agent takes action and receives next state 4. Compute gradients for each objective independently 5. Apply LPA to find lexicographic constrained ascent direction 6. Update policy parameters using optimizer

- Design tradeoffs:
  - Hypercone angle ∆ vs. convergence speed: Larger angles allow faster optimization but risk degrading higher-priority objectives
  - Neural network architecture complexity vs. sample efficiency: More complex policies can represent better solutions but require more data
  - Active constraints heuristic vs. strict preservation: Allowing some degradation of "safe" objectives can speed up optimization

- Failure signatures:
  - Objectives not reaching thresholds: Indicates insufficient exploration or inappropriate ∆ value
  - Oscillation in objective values: Suggests buffer hyperparameter b needs adjustment or instability in gradient estimation
  - Poor performance on endpoint objectives: May indicate need for state augmentation or different reward shaping

- First 3 experiments:
  1. Simple 2D optimization benchmark with known Pareto front to validate projection behavior
  2. Maze navigation with primary goal-reaching and secondary path-quality objectives to test TLQ failure cases
  3. Fruit Tree Navigation benchmark to compare against existing methods on endpoint constraints

## Open Questions the Paper Calls Out
1. How sensitive is the performance of Lexicographic REINFORCE to the conservativeness hyperparameter ∆, and can this sensitivity be reduced?
2. Can the Lexicographic Projection Algorithm (LPA) be extended to handle more than two objectives efficiently?
3. How does the performance of Lexicographic REINFORCE compare to other multi-objective reinforcement learning algorithms, such as those based on linear scalarization or Lagrangian relaxation?

## Limitations
- Theoretical convergence guarantees are not fully established, particularly regarding the impact of the conservativeness parameter ∆ on convergence properties
- Performance sensitivity to hyperparameters (∆, temperature, dropout rate) lacks systematic analysis
- Limited comparison to alternative MORL approaches beyond thresholded lexicographic Q-learning

## Confidence
- **High Confidence**: The core algorithmic innovation of using hypercone projections for lexicographic optimization is well-founded and the implementation details are clearly specified.
- **Medium Confidence**: The empirical results showing superiority over TLQ methods are convincing for the specific benchmarks tested, but generalizability to other MORL problems remains to be demonstrated.
- **Low Confidence**: Theoretical convergence guarantees and the relationship between ∆ parameter and optimization behavior lack rigorous analysis.

## Next Checks
1. Conduct an ablation study testing the algorithm with different ∆ values systematically to understand the trade-off between conservativeness and optimization speed.
2. Compare against alternative MORL approaches beyond TLQ, including linear scalarization and other lexicographic methods.
3. Apply the method to a broader range of MORL problems with varying numbers of objectives and different types of threshold constraints.