---
ver: rpa2
title: Aligning CodeLLMs with Direct Preference Optimization
arxiv_id: '2410.18585'
source_url: https://arxiv.org/abs/2410.18585
tags:
- arxiv
- codellms
- reward
- code
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limitation of existing CodeLLMs, which
  primarily focus on pre-training and supervised fine-tuning while neglecting the
  alignment stage. The authors argue that the commonly used Proximal Policy Optimization
  (PPO) algorithm may be suboptimal for CodeLLM alignment due to coarse-grained and
  potentially flawed reward rules.
---

# Aligning CodeLLMs with Direct Preference Optimization

## Quick Facts
- **arXiv ID:** 2410.18585
- **Source URL:** https://arxiv.org/abs/2410.18585
- **Reference count:** 5
- **Primary result:** CodeQwen1.5 7B's scores increased from 0.783 to 0.804 on MBPP and from 0.829 to 0.878 on HumanEval after DPO training

## Executive Summary
This work addresses the limitation of existing CodeLLMs, which primarily focus on pre-training and supervised fine-tuning while neglecting the alignment stage. The authors argue that the commonly used Proximal Policy Optimization (PPO) algorithm may be suboptimal for CodeLLM alignment due to coarse-grained and potentially flawed reward rules. To overcome this, they propose using Direct Preference Optimization (DPO) instead. DPO leverages model likelihood to represent preferences, automatically acquiring fine-grained differentiation between samples from coarse rewarding signals. The authors also introduce a pipeline for collecting preference pairs for DPO on CodeLLMs, utilizing external code executors to provide feedback for ranking code generations.

## Method Summary
The authors propose using Direct Preference Optimization (DPO) to align CodeLLMs instead of the traditional Proximal Policy Optimization (PPO). They collect preference data pairs by generating multiple code solutions for programming problems, evaluating them with an external code executor, and constructing (chosen, rejected) pairs based on which solutions pass unit tests. The DPO training uses these preference pairs to optimize the model's likelihood ratio between preferred and non-preferred responses. They emphasize the importance of on-policy data collection, showing that using data generated by the current policy yields better results than off-policy approaches.

## Key Results
- CodeQwen1.5 7B improved from 0.783 to 0.804 on MBPP benchmark after DPO training
- CodeQwen1.5 7B improved from 0.829 to 0.878 on HumanEval benchmark after DPO training
- On-policy DPO training significantly outperformed off-policy approaches, with off-policy training even degrading model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO automatically learns fine-grained reward differentiation from coarse preference data.
- Mechanism: By optimizing the log-ratio difference between chosen and rejected responses, DPO derives an implicit reward function that captures nuanced distinctions beyond human-crafted rules.
- Core assumption: The preference data pairs (chosen, rejected) contain sufficient signal for the model to learn the correct ordering.
- Evidence anchors:
  - [abstract] "DPO can render the model rank data automatically, giving rise to a fine-grained rewarding pattern more robust than human intervention."
  - [section] "DPO can form a fine-grained characterization of the preference difference after witnessing adequate, various data pairs."
  - [corpus] Weak evidence - most related works focus on DPO variants but don't directly test the fine-grained reward claim.
- Break condition: If preference pairs are noisy or the chosen/rejected distinction is unclear, the learned reward may not generalize.

### Mechanism 2
- Claim: Using execution feedback from a code executor creates high-quality preference pairs for DPO.
- Mechanism: Code executor provides deterministic pass/fail feedback on unit tests, creating objective preference data without human labeling.
- Core assumption: The code executor's feedback accurately reflects the quality of code solutions.
- Evidence anchors:
  - [section] "We introduce external code executors to provide feedback for ranking code generations."
  - [section] "The code and the unit test examples are then submitted to the code executor for evaluation."
  - [corpus] Moderate evidence - execution-based feedback is common in RLHF for code, but DPO-specific validation is limited.
- Break condition: If the code executor has bugs or the unit tests are insufficient, the preference pairs may be incorrect.

### Mechanism 3
- Claim: On-policy DPO training is superior to off-policy DPO for CodeLLMs.
- Mechanism: Training on data generated by the current policy ensures that the preference pairs are relevant to the model's current capabilities.
- Core assumption: Off-policy data may contain patterns the model cannot reproduce, leading to performance degradation.
- Evidence anchors:
  - [section] "We want to emphasize that whether the preference data for DPO training is on-policy or not is very important."
  - [section] "When we use preference data generated by CodeQwen1.5 to train the DeepSeek-Coder, the performance after training is even worse than the original instruct model."
  - [corpus] Weak evidence - related works discuss DPO but don't directly compare on-policy vs off-policy for code tasks.
- Break condition: If the on-policy data collection is too expensive or the model's capabilities don't change significantly during training.

## Foundational Learning

- **Concept: Preference optimization vs. reward modeling**
  - Why needed here: DPO bypasses explicit reward modeling by directly optimizing preference data, which is crucial for avoiding the coarse-grained reward issue in PPO.
  - Quick check question: What's the key difference between DPO and traditional RLHF approaches?

- **Concept: Execution-based feedback for code evaluation**
  - Why needed here: Using a code executor provides deterministic, objective feedback for generating preference pairs without human annotation.
  - Quick check question: How does execution feedback differ from human preference in terms of reliability and scalability?

- **Concept: On-policy vs. off-policy learning**
  - Why needed here: The choice between on-policy and off-policy DPO significantly impacts performance, especially when using model-generated data.
  - Quick check question: Why might off-policy DPO lead to model degradation in some cases?

## Architecture Onboarding

- **Component map:**
  - CodeLLM (policy model) → Code Executor → Preference Dataset → DPO Trainer → Aligned CodeLLM
  - Code Executor: Runs unit tests on generated code
  - Preference Dataset: (query, chosen_response, rejected_response) triples
  - DPO Trainer: Implements the direct preference optimization objective

- **Critical path:**
  1. Generate responses for queries using the current CodeLLM
  2. Evaluate responses with code executor
  3. Construct preference pairs (chosen, rejected)
  4. Train DPO on preference pairs
  5. Evaluate on benchmarks

- **Design tradeoffs:**
  - Execution feedback provides objective data but may miss semantic quality issues
  - On-policy data collection is more expensive but yields better alignment
  - Coarse preference data works for DPO but not for PPO

- **Failure signatures:**
  - CodeLLM performance degrades on benchmarks → Check preference pair quality
  - Training stalls or diverges → Check DPO hyperparameters (beta, learning rate)
  - No improvement over base model → Verify on-policy data collection

- **First 3 experiments:**
  1. Compare PPO vs DPO on the same coarse reward rules to validate the mechanism claim
  2. Test on-policy vs off-policy DPO using different base models to confirm the preference
  3. Vary the number of responses per query (e.g., 4 vs 8) to find the sweet spot for preference pair quality

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the size of the preference dataset impact the performance of DPO training for CodeLLMs?
- **Basis in paper:** [inferred] The paper mentions that the limited number of coding problems available on the internet restricts the size of the preference dataset, but does not investigate the impact of dataset size on model performance.
- **Why unresolved:** The authors did not conduct experiments to explore the relationship between dataset size and model performance, likely due to the scarcity of available coding problems.
- **What evidence would resolve it:** Experiments varying the size of the preference dataset while keeping other factors constant, and measuring the resulting performance of the CodeLLMs on benchmarks like MBPP and HumanEval.

### Open Question 2
- **Question:** How does the choice of reward function in PPO affect the performance of CodeLLMs compared to DPO?
- **Basis in paper:** [explicit] The paper argues that the reward function used in PPO is coarse-grained and potentially flawed, leading to suboptimal alignment of CodeLLMs. It proposes DPO as an alternative that automatically learns fine-grained preferences from data pairs.
- **Why unresolved:** While the paper shows that DPO outperforms PPO in their experiments, it does not directly compare different reward functions in PPO or explore the impact of reward granularity on model performance.
- **What evidence would resolve it:** Experiments comparing the performance of CodeLLMs trained with different reward functions in PPO, including fine-grained rewards, against those trained with DPO.

### Open Question 3
- **Question:** How does the choice of on-policy vs. off-policy data affect the performance of DPO training for CodeLLMs?
- **Basis in paper:** [explicit] The paper demonstrates that using on-policy data (generated by the policy model itself) for DPO training is more beneficial than using off-policy data (generated by another model), as off-policy training can lead to model degradation.
- **Why unresolved:** While the paper shows the superiority of on-policy DPO over off-policy DPO, it does not explore the reasons behind this difference or investigate potential ways to mitigate the negative effects of off-policy training.
- **What evidence would resolve it:** Experiments analyzing the differences in learned preferences and reward models between on-policy and off-policy DPO, and exploring techniques to improve the performance of off-policy DPO.

## Limitations

- The preference pair construction relies heavily on the code executor's accuracy - if the executor has bugs or the unit tests are insufficient, the entire preference dataset may be flawed.
- The sample size of 3,000 preference triples may not be sufficient to capture the full diversity of coding problems.
- The study focuses primarily on pass@1 metrics without deeper analysis of code quality, efficiency, or maintainability.

## Confidence

- **High Confidence:** The mechanism that DPO can learn from coarse preference data is well-supported by both theoretical derivation and empirical results. The claim that on-policy DPO is superior to off-policy DPO is strongly validated through the experiments showing degradation when using off-policy data.
- **Medium Confidence:** The assertion that DPO is superior to PPO for CodeLLM alignment is supported by the empirical results but lacks direct comparison experiments.
- **Low Confidence:** The scalability of the execution-based preference collection method to larger datasets or more complex coding tasks remains unproven.

## Next Checks

1. **Direct PPO vs DPO Comparison:** Implement both algorithms using identical coarse reward rules (pass/fail from code executor) to directly validate whether DPO truly outperforms PPO in learning from coarse signals.

2. **Preference Data Quality Analysis:** Conduct ablation studies by varying the number of responses per query (2, 4, 8, 16) and measuring the impact on final performance to determine optimal preference pair construction strategies.

3. **Cross-Domain Transferability Test:** Evaluate the aligned CodeLLMs on practical coding tasks beyond competitive programming (e.g., real GitHub repositories, documentation generation) to assess real-world applicability of the alignment approach.