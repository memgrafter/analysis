---
ver: rpa2
title: 'Confidence v.s. Critique: A Decomposition of Self-Correction Capability for
  LLMs'
arxiv_id: '2412.19513'
source_url: https://arxiv.org/abs/2412.19513
tags:
- self-correction
- correct
- answer
- acc2
- acc1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models can correct their own responses, but this
  self-correction often reduces accuracy. To understand this behavior, the paper decomposes
  self-correction into two capabilities: confidence (maintaining correct answers)
  and critique (fixing wrong answers).'
---

# Confidence v.s. Critique: A Decomposition of Self-Correction Capability for LLMs

## Quick Facts
- arXiv ID: 2412.19513
- Source URL: https://arxiv.org/abs/2412.19513
- Reference count: 34
- Large language models can correct their own responses, but this self-correction often reduces accuracy.

## Executive Summary
Self-correction in large language models (LLMs) is a double-edged sword: while it aims to improve responses, it often reduces accuracy. This paper introduces a novel decomposition framework that separates self-correction capability into two distinct components: Confidence (maintaining correct answers) and Critique (fixing wrong answers). By quantifying these aspects through Confidence Level and Critique Score metrics from a probabilistic perspective, the research reveals that overall post-correction accuracy is a weighted sum of these two metrics. The study shows that self-correction doesn't always improve performance, with models exhibiting varied behaviors ranging from conservative to liberal correction strategies. The findings suggest that improving both confidence and critique simultaneously is challenging without fine-tuning, and introduce a simple training strategy (CCT) that outperforms standard fine-tuning by transforming data format to enhance both capabilities.

## Method Summary
The paper introduces a probabilistic framework that decomposes self-correction into two independent components: Confidence Level (the probability a model maintains a correct answer) and Critique Score (the probability a model corrects a wrong answer). The framework models self-correction as a probabilistic process where overall accuracy after self-correction equals a weighted sum of these two metrics. The authors define mathematical formulations for both metrics and validate them across various model families including GPT, Gemini, and Llama. They conduct extensive experiments using established benchmarks and introduce a novel training strategy called Confidence-and-Critique Improvement Tuning (CCT) that transforms data format to improve both capabilities simultaneously. The methodology involves systematic evaluation of self-correction behavior across different prompting strategies and model configurations.

## Key Results
- Self-correction often reduces accuracy, with overall post-correction accuracy being a weighted sum of Confidence Level and Critique Score
- Models exhibit distinct self-correction behaviors: some are conservative (high confidence, low critique) while others are liberal (low confidence, high critique)
- Prompting and in-context learning can manipulate these capabilities in predictable directions, but improving both simultaneously is difficult without fine-tuning
- The proposed CCT training strategy outperforms standard fine-tuning by achieving higher accuracy after self-correction through data format transformation

## Why This Works (Mechanism)
The framework works by recognizing that self-correction involves two competing forces: the desire to maintain correct answers (confidence) and the ability to identify and fix incorrect ones (critique). By modeling these as independent probabilistic components, the approach can quantify and optimize each aspect separately. The independence assumption allows for clear measurement of each capability's contribution to overall performance. The CCT training strategy works by explicitly encoding confidence and critique signals in the training data format, forcing the model to learn both aspects simultaneously rather than optimizing for one at the expense of the other.

## Foundational Learning

**Probabilistic Modeling of Decision Making**
*Why needed:* Understanding how self-correction can be modeled as a probabilistic process where different capabilities contribute independently to final outcomes.
*Quick check:* Verify that the weighted sum formula accurately predicts actual self-correction accuracy across different model families.

**Confidence Calibration**
*Why needed:* Recognizing that models need appropriate confidence in their answers to avoid unnecessary corrections of correct responses.
*Quick check:* Measure how often models incorrectly change correct answers versus correctly maintaining them.

**Error Detection and Correction Mechanisms**
*Why needed:* Understanding how models identify and fix their own mistakes versus introducing new errors through over-correction.
*Quick check:* Compare critique scores across models with different instruction-tuning strategies.

**Prompt Engineering for Capability Manipulation**
*Why needed:* Learning how prompt design can influence the balance between confidence and critique behaviors.
*Quick check:* Test whether specific prompt templates consistently shift models toward more conservative or liberal correction patterns.

## Architecture Onboarding

**Component Map:**
Input Prompt → LLM Generation → Self-Correction Module → Output Response
Confidence Module ← ↔ Critique Module → Combined Accuracy Metric

**Critical Path:**
The critical path involves the interaction between the confidence module (which determines whether to maintain current answers) and the critique module (which identifies and attempts to fix errors). The weighted combination of these modules determines final accuracy.

**Design Tradeoffs:**
The framework trades computational simplicity (simple probabilistic decomposition) for potential oversimplification of complex self-correction dynamics. The independence assumption between confidence and critique may not hold perfectly in practice. The binary correctness evaluation may miss nuanced aspects of response quality.

**Failure Signatures:**
Models that over-correct show low confidence scores but potentially high critique scores. Models that under-correct show high confidence but low critique. Extreme values in either direction indicate pathological behavior where the model either never corrects or never maintains correct answers.

**First 3 Experiments:**
1. Measure baseline confidence and critique scores for multiple model families without any prompting intervention
2. Test the effect of different prompt templates on shifting the confidence-critique balance
3. Evaluate the CCT training strategy against standard fine-tuning on a held-out test set

## Open Questions the Paper Calls Out
None

## Limitations
- The decomposition framework assumes independence between confidence and critique components, which may not hold empirically
- Binary correctness judgments in evaluation may not capture the full complexity of real-world self-correction scenarios
- The framework is primarily validated on English benchmarks and may not generalize to other languages or specialized domains
- The proposed CCT training strategy requires labeled data with explicit correctness annotations

## Confidence

**Major Limitations and Uncertainties**
High Confidence: The overall finding that self-correction often reduces accuracy and that the decomposition into confidence and critique components provides a useful analytical framework.
Medium Confidence: The specific quantitative values of Confidence Level and Critique Score metrics across models, as these depend on exact evaluation setup.
Medium Confidence: The claim that prompting can manipulate these capabilities in predictable directions, though magnitude may vary.
Low Confidence: The assertion that improving both confidence and critique simultaneously is inherently difficult without fine-tuning.

## Next Checks
1. Test the decomposition framework on multi-round self-correction scenarios to examine whether the independence assumption between confidence and critique breaks down over multiple iterations.
2. Evaluate the framework on open-ended tasks with subjective or multi-faceted correctness criteria, moving beyond binary judgments to capture more nuanced aspects of self-correction quality.
3. Conduct cross-lingual and cross-domain experiments to assess whether the observed patterns in confidence and critique behaviors generalize beyond English benchmarks and standard NLP tasks.