---
ver: rpa2
title: Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text
  Watermark for Large Language Models
arxiv_id: '2402.14007'
source_url: https://arxiv.org/abs/2402.14007
tags:
- watermark
- text
- positive
- rate
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores cross-lingual consistency in text watermarking
  for large language models (LLMs), a critical aspect for maintaining watermark effectiveness
  across languages. The study reveals that current watermarking methods, including
  KGW, UW, and SIR, exhibit significant deficiencies in cross-lingual consistency,
  with low Pearson Correlation Coefficients (PCCs) and high Relative Errors (REs).
---

# Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models

## Quick Facts
- arXiv ID: 2402.14007
- Source URL: https://arxiv.org/abs/2402.14007
- Authors: Zhiwei He; Binglin Zhou; Hongkun Hao; Aiwei Liu; Xing Wang; Zhaopeng Tu; Zhuosheng Zhang; Rui Wang
- Reference count: 40
- Primary result: Current text watermarking methods show poor cross-lingual consistency, with proposed X-SIR defense substantially improving performance across multiple languages

## Executive Summary
This work explores the critical issue of cross-lingual consistency in text watermarking for large language models (LLMs). The authors reveal that existing watermarking methods, including KGW, UW, and SIR, exhibit significant deficiencies when text is translated across languages, with low Pearson Correlation Coefficients and high Relative Errors. To address this vulnerability, they propose the Cross-lingual Watermark Removal Attack (CWRA), which effectively eliminates watermarks through pivot language translation. The authors also introduce X-SIR, a defense method based on cross-lingual semantic clustering and robust vocabulary partitioning, which substantially improves watermark detection performance and cross-lingual consistency across multiple LLMs and languages.

## Method Summary
The study evaluates three existing watermarking methods (KGW, UW, SIR) across multiple languages by generating watermarked responses to English prompts, translating them into Chinese, Japanese, French, and German, and measuring cross-lingual consistency using PCC and RE metrics. The authors then implement CWRA, a translation-based attack that translates prompts to a pivot language, generates responses, and translates back, effectively removing watermarks. Finally, they propose X-SIR, which improves cross-lingual consistency by treating semantically equivalent token clusters across languages as single watermarking units rather than individual tokens. X-SIR uses cross-lingual semantic clustering and robust vocabulary partitioning to maintain watermark status across translations.

## Key Results
- Current watermarking methods (KGW, UW, SIR) exhibit poor cross-lingual consistency with low PCC values (<0.3) and high RE values (>80%)
- CWRA attack reduces watermark detection AUCs to random-guessing levels (0.5) while maintaining text quality
- X-SIR substantially improves cross-lingual consistency, achieving higher PCC and lower RE values compared to baseline methods
- Cross-lingual semantic clustering and robust vocabulary partitioning are key factors for improving watermark consistency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-lingual watermark removal via pivot language translation (CWRA) effectively eliminates watermark signals while maintaining text quality.
- **Mechanism:** The attack translates the original prompt into a pivot language, generates the response in that pivot language, and then translates it back to the original language. This double translation disrupts the watermarking patterns embedded during initial text generation.
- **Core assumption:** Watermark algorithms are not designed for cross-lingual contexts, making them vulnerable to translation-based disruption.
- **Evidence anchors:**
  - [abstract] "CWRA can effectively remove watermarks, decreasing the AUCs to a random-guessing level without performance loss."
  - [section] "CWRA outperforms re-writing attacks, such as re-translation and paraphrasing...as it decreases the AUCs to a random-guessing level and achieves the highest text quality."
  - [corpus] Weak - no direct evidence of CWRA in related papers, only mentions "Cross-Lingual Summarization as a Black-Box Watermark Removal Attack" without specific mechanism details.
- **Break condition:** If watermarking algorithms incorporate cross-lingual semantic clustering and robust vocabulary partitioning (as proposed in X-SIR), the attack would be less effective.

### Mechanism 2
- **Claim:** Cross-lingual semantic clustering and robust vocabulary partitioning improve watermark consistency across languages.
- **Mechanism:** Instead of treating individual tokens as the smallest unit for watermarking, clusters of semantically equivalent tokens across languages are treated as a single unit. This ensures that when text is translated, tokens remain in the same cluster and retain their watermark status.
- **Core assumption:** Semantic clustering across languages can be effectively implemented using bilingual dictionaries to create connected components of semantically similar tokens.
- **Evidence anchors:**
  - [abstract] "we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose X-SIR as a defense method against CWRA."
  - [section] "Instead of treating each token in the vocabulary as the smallest unit when ironing watermarks, as done by KGW, our method considers a cluster of tokens that share the same semantics across different languages as the smallest unit of processing."
  - [corpus] Weak - no direct evidence of semantic clustering approach in related papers.
- **Break condition:** If the bilingual dictionary is incomplete or if the vocabulary contains word units that cannot be clustered effectively, the approach would fail.

### Mechanism 3
- **Claim:** Semantic invariant watermarking (SIR) shows better cross-lingual consistency than traditional methods (KGW and UW).
- **Mechanism:** SIR assigns similar watermark biases for semantically similar contexts in different languages by training with a multilingual embedding model, making the watermark more resistant to translation.
- **Core assumption:** Using a multilingual embedding model (like S-BERT) can effectively capture semantic similarity across languages for watermarking purposes.
- **Evidence anchors:**
  - [abstract] "we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose X-SIR as a defense method against CWRA."
  - [section] "Based on Eq. 4, SIR has already optimized for Factor 2 when using a multilingual embedding model. For prefixes x and y, the similarity of their vocabulary partitions for the next token should be close to their semantic similarity."
  - [corpus] Weak - no direct evidence of SIR in related papers, only mentions "SimKey: A Semantically Aware Key Module for Watermarking Language Models" without specific mechanism details.
- **Break condition:** If the multilingual embedding model fails to capture semantic similarity across certain language pairs, the method would lose its advantage.

## Foundational Learning

- **Concept: Pearson Correlation Coefficient (PCC)**
  - Why needed here: To measure the linear correlation between watermark strength before and after translation, quantifying cross-lingual consistency.
  - Quick check question: If PCC(S, Ŝ) = 0.9, what does this indicate about watermark strength consistency across languages?

- **Concept: Relative Error (RE)**
  - Why needed here: To assess the magnitude of deviation in watermark strength after translation, complementing PCC by measuring absolute differences rather than just correlation.
  - Quick check question: If RE(S, Ŝ) = 50%, what does this tell us about the watermark strength retention after translation?

- **Concept: Cross-lingual semantic clustering**
  - Why needed here: To understand how grouping semantically equivalent tokens across languages can improve watermark consistency by ensuring translated tokens retain their watermark status.
  - Quick check question: How would you construct semantic clusters from a bilingual dictionary to implement cross-lingual watermarking?

## Architecture Onboarding

- **Component map:** Watermarking engine (KGW, UW, SIR, X-SIR) -> Translation system (CWRA attack) -> Detection system (watermark strength calculation) -> Evaluation framework (PCC, RE, AUC metrics)

- **Critical path:** 1) Generate watermarked text using chosen watermarking method 2) Apply translation (for cross-lingual evaluation) or CWRA attack 3) Detect watermark in translated/attacked text 4) Calculate PCC and RE metrics 5) Evaluate AUC under different attack scenarios

- **Design tradeoffs:** Translation quality vs. watermark removal effectiveness (higher quality translations may preserve more watermark signal) | Vocabulary coverage vs. implementation complexity (more comprehensive bilingual dictionaries improve X-SIR but increase computational overhead) | Multilingual embedding model choice vs. semantic accuracy (better embeddings improve SIR but may be computationally expensive)

- **Failure signatures:** Low PCC values (<0.3) indicate poor cross-lingual consistency | High RE values (>80%) suggest significant watermark strength loss after translation | AUC values approaching random guessing (0.5) indicate successful watermark removal attacks | In X-SIR, inconsistent PCC and RE improvements across different language pairs suggest limitations in dictionary coverage or embedding quality

- **First 3 experiments:** 1) Implement CWRA attack on KGW and measure AUC reduction from baseline 2) Compare cross-lingual consistency (PCC, RE) of SIR vs. KGW across multiple language pairs 3) Implement X-SIR with basic bilingual dictionary and evaluate improvement in CWRA robustness compared to SIR

## Open Questions the Paper Calls Out

- **Question:** How do watermarking methods perform in cross-lingual consistency when using LLMs with extensive multilingual training data?
- **Basis:** The study uses BAICHUAN-7B and LLAMA-2-7B, which may not be fully optimized for multilingual contexts.
- **Why unresolved:** The paper does not explore the impact of model size and multilingual training on watermark consistency.
- **What evidence would resolve it:** Testing cross-lingual consistency with models like GPT-4 or other multilingual LLMs could provide insights into whether larger, more multilingual models perform better in maintaining watermark consistency.

- **Question:** What are the limitations of using semantic clustering for vocabulary partitioning in watermarking methods?
- **Basis:** The paper discusses the limitations of X-SIR, noting that it relies on semantic clustering which only considers tokens shared by the vocabulary and the external dictionary.
- **Why unresolved:** The paper acknowledges limitations but does not explore alternative methods for semantic clustering or vocabulary partitioning.
- **What evidence would resolve it:** Investigating alternative clustering methods or dictionaries that include subword units could provide a more comprehensive understanding of vocabulary partitioning in cross-lingual contexts.

- **Question:** How does the choice of pivot language affect the effectiveness of the Cross-lingual Watermark Removal Attack (CWRA)?
- **Basis:** The paper suggests that an attacker could choose a pivot language at which the LLM excels to perform CWRA effectively.
- **Why unresolved:** The paper does not systematically explore the impact of different pivot languages on the effectiveness of CWRA.
- **What evidence would resolve it:** Conducting experiments with various pivot languages and analyzing their impact on CWRA effectiveness would provide insights into optimal strategies for attackers.

## Limitations

- The evaluation relies on a relatively small dataset (500 English prompts from mc4) and three language pairs, which may not capture the full complexity of cross-lingual scenarios.
- The effectiveness of X-SIR depends heavily on the quality and coverage of the bilingual dictionary used for semantic clustering, but the paper does not provide details on how this dictionary is constructed or validated.
- The CWRA attack assumes that double translation sufficiently disrupts watermark patterns, but this may not hold for languages with closer linguistic relationships or for watermarking methods specifically designed with cross-lingual robustness in mind.

## Confidence

- **High confidence:** The core finding that current watermarking methods (KGW, UW, SIR) exhibit poor cross-lingual consistency with low PCC values and high RE values is well-supported by the experimental results.
- **Medium confidence:** The proposed X-SIR defense method shows promising improvements in cross-lingual consistency, but the implementation details of semantic clustering and vocabulary partitioning are not fully specified.
- **Low confidence:** The assumption that semantic clustering using bilingual dictionaries is the optimal approach for improving cross-lingual consistency lacks strong theoretical justification.

## Next Checks

1. **Dictionary Coverage Analysis:** Conduct a systematic evaluation of how bilingual dictionary coverage affects the performance of X-SIR across different language pairs. Measure the relationship between dictionary completeness and improvements in PCC and RE values, and identify the minimum coverage threshold required for effective cross-lingual watermarking.

2. **Cross-lingual Embedding Validation:** Implement a comparison between the bilingual dictionary approach and multilingual embedding-based clustering for semantic grouping. Evaluate which method provides better cross-lingual consistency while maintaining watermark robustness against CWRA attacks, particularly for language pairs with varying degrees of linguistic similarity.

3. **Pivot Language Ablation Study:** Test the sensitivity of CWRA to different pivot language choices by systematically varying the intermediate language in the translation chain. Analyze how linguistic distance between source, pivot, and target languages affects watermark removal effectiveness and identify optimal pivot language selection strategies for maximum attack success.