---
ver: rpa2
title: Automated Machine Learning for Multi-Label Classification
arxiv_id: '2402.18198'
source_url: https://arxiv.org/abs/2402.18198
tags:
- learning
- machine
- automl
- multi-label
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis addresses the challenge of automated machine learning
  (AutoML) for multi-label classification (MLC), where each data point can be associated
  with multiple class labels. The author develops ML-Plan, a novel AutoML approach
  based on hierarchical task network planning, which can naturally represent hierarchical
  dependencies in machine learning pipelines.
---

# Automated Machine Learning for Multi-Label Classification

## Quick Facts
- arXiv ID: 2402.18198
- Source URL: https://arxiv.org/abs/2402.18198
- Reference count: 40
- Key outcome: ML-Plan outperforms state-of-the-art AutoML approaches for multi-label classification

## Executive Summary
This thesis presents ML-Plan, a novel automated machine learning (AutoML) approach based on hierarchical task network planning, specifically designed to handle the complexities of multi-label classification (MLC). The work extends ML-Plan from single-label classification to support unlimited-length machine learning pipelines and complex MLC configurations. The approach demonstrates superior performance compared to existing AutoML methods while maintaining good scalability with increased MLC complexity. Additional contributions include label-wise base learner selection (LiBRe), evolved nested dichotomies for classification, and runtime prediction for ML pipelines.

## Method Summary
The thesis introduces ML-Plan as a hierarchical task network planning approach for AutoML that can naturally represent dependencies in machine learning pipelines. The method is first validated for single-label classification, then extended to handle multi-label classification scenarios with unlimited pipeline lengths. The approach incorporates runtime prediction mechanisms to improve search efficiency and includes ensemble methods specifically designed for MLC tasks. The work also explores label-wise base learner selection and evolved nested dichotomies as complementary techniques for improving MLC performance.

## Key Results
- ML-Plan outperforms state-of-the-art AutoML approaches for multi-label classification tasks
- The hierarchical task network planning approach scales well with increased MLC complexity
- Runtime prediction models significantly improve AutoML search efficiency
- Ensemble methods combining evolved nested dichotomies show competitive performance

## Why This Works (Mechanism)
ML-Plan's effectiveness stems from its ability to represent hierarchical dependencies in machine learning pipelines through hierarchical task network planning. This approach naturally captures the complex relationships between preprocessing steps, model selection, and post-processing in MLC scenarios. The runtime prediction component enables more efficient search by estimating the computational cost of different pipeline configurations, allowing the system to explore promising regions of the search space more thoroughly. The label-wise base learner selection strategy optimizes performance by treating each label independently while maintaining overall coherence.

## Foundational Learning

**Hierarchical Task Network Planning**: A planning paradigm that represents complex tasks as hierarchies of simpler subtasks with dependencies.
*Why needed*: Enables natural representation of machine learning pipeline dependencies and complex MLC workflows.
*Quick check*: Can represent preprocessing, model selection, and post-processing as dependent subtasks in a coherent framework.

**Multi-Label Classification**: Classification where each instance can be associated with multiple class labels simultaneously.
*Why needed*: Real-world scenarios often involve multiple relevant labels per data point, requiring specialized algorithms.
*Quick check*: Handle label cardinality (average number of labels per instance) and label correlation patterns.

**Binary Relevance**: A transformation approach for MLC that trains one binary classifier per label.
*Why needed*: Simplifies MLC to multiple binary classification problems while preserving label independence.
*Quick check*: Ensure base learners can handle imbalanced label distributions and label correlations.

## Architecture Onboarding

Component map: Data preprocessing -> Feature engineering -> Model selection -> Post-processing -> Evaluation

Critical path: Data preprocessing → Feature engineering → Model selection → Post-processing

Design tradeoffs:
- Search space complexity vs. computational efficiency
- Pipeline length flexibility vs. optimization time
- Generalization vs. task-specific optimization

Failure signatures:
- Poor performance on high-cardinality labels
- Inefficient search in high-dimensional feature spaces
- Suboptimal handling of label correlations

First experiments:
1. Compare ML-Plan performance against baseline AutoML approaches on benchmark MLC datasets
2. Evaluate runtime prediction accuracy across different pipeline configurations
3. Test label-wise base learner selection on datasets with varying label distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements may not generalize across all MLC dataset types
- Scalability claims limited by computational resources and may not hold for extremely large problems
- Practical deployment implications of On-The-Fly Computing remain conceptual
- Sensitivity to pipeline search space configurations requires further investigation

## Confidence

High:
- ML-Plan's technical innovation and performance advantages over existing AutoML approaches for MLC

Medium:
- Scalability assertions and runtime prediction enhancements based on specific experimental conditions

Low:
- Practical deployment implications of On-The-Fly Computing without implementation details

## Next Checks

1. Test ML-Plan across a broader range of MLC datasets, including those with extreme label cardinality and different feature distributions
2. Conduct head-to-head comparisons with emerging AutoML frameworks specifically designed for MLC tasks
3. Implement and evaluate the runtime prediction model on real-world MLC problems with varying computational constraints and pipeline complexities