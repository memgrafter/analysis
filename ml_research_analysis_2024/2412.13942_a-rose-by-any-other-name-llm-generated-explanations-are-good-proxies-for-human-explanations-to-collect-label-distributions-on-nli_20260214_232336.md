---
ver: rpa2
title: 'A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies for
  Human Explanations to Collect Label Distributions on NLI'
arxiv_id: '2412.13942'
source_url: https://arxiv.org/abs/2412.13942
tags:
- explanations
- human
- entailment
- contradiction
- neutral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  generate high-quality explanations to approximate human judgment distributions (HJDs)
  in natural language inference (NLI), reducing the need for costly human-annotated
  explanations. The authors prompt LLMs to generate explanations for NLI instances
  and labels, then evaluate how well these model explanations help approximate HJD
  compared to human explanations.
---

# A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies for Human Explanations to Collect Label Distributions on NLI

## Quick Facts
- arXiv ID: 2412.13942
- Source URL: https://arxiv.org/abs/2412.13942
- Reference count: 40
- Key outcome: LLM-generated explanations perform comparably to human explanations in approximating human judgment distributions for NLI when combined with a few human labels

## Executive Summary
This paper investigates whether large language models can generate high-quality explanations to approximate human judgment distributions in natural language inference tasks, potentially reducing the need for costly human-annotated explanations. The authors find that model explanations perform comparably to human explanations when combined with a few human labels, achieving similar results on distribution similarity metrics and downstream task performance. The approach generalizes to datasets without human-provided explanations and out-of-domain test sets, with ablation studies confirming that explanation content matters and human preference experiments revealing that explanation variability can serve as a useful quality indicator.

## Method Summary
The method involves generating model explanations for NLI instances and labels using LLMs (Llama3-70b, Mixtral-8x7b, GPT-4o), then evaluating how well these explanations help approximate human judgment distributions compared to human explanations. The process uses human labels to guide explanation selection, converts explanations into probability distributions via MCQA prompting with first-token probability method, and evaluates performance using KL divergence, Jensen-Shannon distance, Total Variation Distance, and downstream weighted F1 scores. The approach is tested on datasets with human labels and explanations (VariErr, MNLI) as well as out-of-domain test sets (ANLI).

## Key Results
- Model explanations yield comparable results to human explanations in approximating human judgment distributions when provided with human labels
- The approach generalizes to datasets without human-provided explanations and out-of-domain test sets
- Explanation variability serves as an indicator for explanation quality, with more diverse model explanations sometimes achieving better results than preferred ones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated explanations can approximate human judgment distributions when combined with a few human labels.
- Mechanism: Model explanations provide relevant information about NLI label reasoning, allowing better estimation of label distributions by capturing human label variation.
- Core assumption: Explanation content contains meaningful information about premise-hypothesis relationships.
- Evidence anchors: [abstract] comparable results to human explanations; [section 3.2] label-free methods perform poorly; [corpus] average neighbor FMR=0.471 (weak evidence).
- Break condition: If explanations become generic or hallucinate without capturing reasoning patterns.

### Mechanism 2
- Claim: Model explanations generalize to datasets without human explanations and out-of-domain test sets.
- Mechanism: Using human labels to guide explanation selection creates a transferable framework for datasets lacking explanations.
- Core assumption: NLI reasoning patterns remain consistent across different datasets.
- Evidence anchors: [abstract] generalizes to datasets without explanations; [section 4.2] improved scores after fine-tuning on MNLI; [corpus] evaluation methods exist.
- Break condition: If out-of-domain data has fundamentally different characteristics or reasoning patterns.

### Mechanism 3
- Claim: Explanation variability serves as an indicator for explanation quality and HLV modeling.
- Mechanism: More diverse explanations provide better coverage of the reasoning space, leading to improved HJD approximation.
- Core assumption: Human annotators naturally vary in reasoning approaches, and capturing this variation improves model performance.
- Evidence anchors: [section 6] unpreferred explanations achieved best results due to higher diversity; [section 6] Table 5 shows human explanations have greatest variability.
- Break condition: If explanations become too diverse and lose coherence or relevance to the NLI task.

## Foundational Learning

- Concept: Kullback-Leibler (KL) Divergence
  - Why needed here: Measures difference between human judgment distributions and model-generated distributions at instance level.
  - Quick check question: If KL divergence between two distributions is 0, what does this tell you about their similarity?

- Concept: Jensen-Shannon Distance (JSD)
  - Why needed here: Provides symmetric measure of distribution similarity complementing KL divergence.
  - Quick check question: How does JSD differ from KL divergence in terms of symmetry and boundedness?

- Concept: Multi-choice question answering (MCQA) prompting
  - Why needed here: Method to obtain model judgment distributions from LLMs by presenting NLI instances with label options.
  - Quick check question: What is the advantage of using MCQA prompting over direct classification for capturing distribution information?

## Architecture Onboarding

- Component map: NLI instance → LLM explanation generation → Label-guided selection → MJD estimation → Distribution evaluation → Fine-tuning
- Critical path: NLI instance → LLM explanation generation → Label-guided selection → MJD estimation → Distribution evaluation → Fine-tuning
- Design tradeoffs:
  - Label-free vs label-guided explanation selection: Label-free is more scalable but less accurate; label-guided is more accurate but requires human labels
  - First vs longest explanation selection: First captures LLM preferences; longest captures more detailed reasoning
  - Number of explanations per label: More explanations provide more diversity but increase computational cost
- Failure signatures:
  - Poor distribution similarity metrics (high KL/JSD/TVD)
  - Low downstream task performance (low F1 scores)
  - MJD distributions that don't match human judgment patterns
  - Explanations that are generic or hallucinate without capturing reasoning
- First 3 experiments:
  1. Generate model explanations for a small set of NLI instances and manually evaluate their relevance and quality
  2. Test label-free explanation selection on a validation set to establish baseline performance
  3. Implement label-guided selection using a few human labels and compare performance to label-free approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-generated explanations effectively capture and model human label variation (HLV) in domains beyond natural language inference (NLI)?
- Basis in paper: [explicit] The paper states "Our findings are: • Model explanations are comparable to human explanations in approximating HJD on NLI, and can be scaled up from a few annotations of datasets without explanations."
- Why unresolved: Experiments were conducted exclusively on NLI datasets. While results are promising, the paper does not test whether this approach generalizes to other tasks with label variation like sentiment analysis, fact-checking, or question answering.
- What evidence would resolve it: Testing the LLM explanation generation approach on multiple diverse NLP tasks and comparing performance to human explanations on approximating their respective human judgment distributions.

### Open Question 2
- Question: How does the variability of LLM-generated explanations correlate with their effectiveness in approximating human judgment distributions across different LLMs and tasks?
- Basis in paper: [explicit] The paper states "Results indicate that LLM-generated explanations can significantly reduce annotation costs, making it a scalable and efficient proxy for capturing HLV" and discusses variability as a potential indicator in human preference experiments.
- Why unresolved: While the paper explores variability as a potential indicator through human preference experiments, it does not establish systematic correlation between explanation variability and approximation effectiveness across different LLMs, tasks, or explanation selection strategies.
- What evidence would resolve it: Comprehensive study measuring explanation variability using multiple metrics and correlating it with HJD approximation performance across different LLMs, tasks, and selection strategies.

### Open Question 3
- Question: What is the optimal combination of human labels and LLM-generated explanations for approximating human judgment distributions in different annotation scenarios?
- Basis in paper: [explicit] The paper tests different configurations including label-free explanations, label-guided explanations with varying numbers of human labels, and gradual replacement experiments.
- Why unresolved: Experiments show that a few human labels combined with model explanations perform well, but don't systematically explore optimal ratio of human labels to model explanations or how this might vary based on dataset characteristics, annotation costs, or task complexity.
- What evidence would resolve it: Systematic exploration of different human label-to-model explanation ratios across multiple datasets and tasks, measuring both approximation quality and cost-effectiveness to determine optimal configurations for different scenarios.

## Limitations
- Label-guided approach still requires some human annotations, limiting full automation
- Results primarily validated on English NLI datasets, with uncertain generalizability to other languages or tasks
- Explanation generation process depends on quality and diversity of LLM outputs, which may vary across different model versions

## Confidence

- High confidence: Core finding that LLM explanations can approximate HJD when combined with human labels (supported by multiple metrics and ablation studies)
- Medium confidence: Generalizability to out-of-domain datasets (limited by single OOD test set ANLI)
- Medium confidence: Role of explanation variability as quality indicator (interesting finding but needs further validation)

## Next Checks

1. Test the approach on non-English NLI datasets to assess language generalizability
2. Conduct ablation studies with varying numbers of human labels to determine minimum annotation requirements
3. Evaluate performance on different types of explanation generation tasks beyond NLI to assess broader applicability