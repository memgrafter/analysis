---
ver: rpa2
title: Exploring Dark Knowledge under Various Teacher Capacities and Addressing Capacity
  Mismatch
arxiv_id: '2405.13078'
source_url: https://arxiv.org/abs/2405.13078
tags:
- knowledge
- teacher
- class
- classes
- teachers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how teacher network capacity affects knowledge
  distillation (KD) performance, addressing the "capacity mismatch" phenomenon where
  larger, more accurate teachers can underperform smaller ones in KD. The authors
  make two key observations: (1) larger teachers tend to produce probability vectors
  with lower distinction among non-ground-truth classes, and (2) teachers with different
  capacities are consistent in their cognition of relative class affinity.'
---

# Exploring Dark Knowledge under Various Teacher Capacities and Addressing Capacity Mismatch

## Quick Facts
- **arXiv ID**: 2405.13078
- **Source URL**: https://arxiv.org/abs/2405.13078
- **Reference count**: 40
- **Primary result**: Proposes ISATS method that outperforms previous KD methods by addressing capacity mismatch through instance-specific temperature scaling

## Executive Summary
This paper investigates the capacity mismatch phenomenon in knowledge distillation where larger, more accurate teachers can underperform smaller ones. The authors observe that larger teachers produce less varied probabilities among non-ground-truth classes due to increased feature compactness, yet maintain consistent relative class affinities. They propose three methods to address this issue: FGCR (fusing global class relations), RegT (regularizing teachers), and ISATS (instance-specific asymmetric temperature scaling). Experimental results demonstrate that ISATS achieves state-of-the-art performance across multiple datasets including CIFAR-100, TinyImageNet, CUB, and Stanford Dogs.

## Method Summary
The paper proposes three methods to address capacity mismatch in knowledge distillation. FGCR fuses global class relations by adjusting temperature based on inter-class distances. RegT regularizes teacher outputs to maintain probability variance. ISATS dynamically adjusts temperature per training instance to maximize variance among non-ground-truth class probabilities. The methods are evaluated against baseline KD approaches on various teacher-student pairs across multiple image classification datasets, with experiments following a 240-epoch training procedure using SGD optimizer.

## Key Results
- Larger teachers produce probability vectors with lower distinction among non-ground-truth classes
- Teachers with different capacities maintain consistent relative class affinity rankings
- ISATS outperforms previous methods (ESKD, TAKD, SCKD, ATS) on CIFAR-100, TinyImageNet, CUB, and Stanford Dogs
- Instance-specific temperature scaling effectively recovers discriminative power lost in larger teachers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger teacher networks produce less varied probabilities among non-ground-truth classes due to increased feature compactness within classes and increased dispersion between classes.
- Mechanism: As teacher network capacity increases, the extracted features of the same class become more compact while features between different classes become more dispersed. This leads to a larger logit for the ground-truth class and less varied logits for non-ground-truth classes after softmax normalization.
- Core assumption: The relationship between feature compactness and classification weight direction holds in high-dimensional space as it does in low-dimensional visualizations.
- Evidence anchors:
  - [section] "If the intra-class feature angle becomes larger, the feature of a specific sample will be far away from the classification weights of the other classes. Similarly, the feature of a specific sample will be closer to the corresponding class's classification weight."
  - [section] "If the intra-class features are nearly orthogonal, the logits for non-ground-truth classes will be less varied."
  - [corpus] Weak - corpus papers don't directly address the feature compactness-probabilities relationship
- Break condition: If the feature extraction pattern reverses (more compact inter-class features with dispersed intra-class features), the mechanism would fail.

### Mechanism 2
- Claim: Teachers with different capacities show consistent relative class affinities despite differences in absolute probability values.
- Mechanism: Different capacity teachers maintain similar rank orderings of class probabilities for each sample, as measured by rank set overlap, Spearman correlation, and Kendall's τ.
- Core assumption: The relative ranking of class affinities captures the essential "dark knowledge" that is valuable for student learning.
- Evidence anchors:
  - [abstract] "teachers with different capacities are basically consistent in their cognition of relative class affinity"
  - [section] "These metrics only depend on classes' relative magnitudes and are irrelevant to the absolute values."
  - [section] "the Spearman correlations are almost all larger than 0.85"
- Break condition: If the relative rankings diverge significantly between teacher capacities, the mechanism would fail.

### Mechanism 3
- Claim: Instance-specific temperature scaling can recover the discriminative power lost in larger teachers by making non-ground-truth class probabilities more distinct.
- Mechanism: By finding the optimal temperature for each training instance that maximizes variance among non-ground-truth class probabilities, the method restores the distinctness needed for effective knowledge transfer.
- Core assumption: The optimal temperature that maximizes variance among non-ground-truth classes also improves the quality of dark knowledge transfer.
- Evidence anchors:
  - [abstract] "The proposed ISATS method dynamically adjusts temperatures per training sample to enhance variance among non-ground-truth class probabilities."
  - [section] "τ⋆(x) = arg max τ v(q(x; τ))" where v(·) calculates the variance of the elements in a vector
  - [section] "ISATS saves the effort of finding the proper value of τ2 and τ1"
- Break condition: If the variance-maximizing temperature does not correlate with better distillation performance, the mechanism would fail.

## Foundational Learning

- Concept: Feature compactness and dispersion metrics
  - Why needed here: Understanding how feature geometry changes with model capacity is crucial for explaining why larger teachers produce less varied non-ground-truth probabilities
  - Quick check question: How would you measure whether intra-class features are becoming more compact in a high-dimensional space?

- Concept: Temperature scaling in softmax
  - Why needed here: Temperature scaling controls the sharpness of probability distributions, which is central to understanding how to adjust teacher outputs for better distillation
  - Quick check question: What happens to the probability distribution when temperature approaches infinity versus when it approaches zero?

- Concept: Knowledge distillation loss formulation
  - Why needed here: Understanding the standard KL divergence loss and how it relates to the teacher's output probabilities is essential for implementing the proposed methods
  - Quick check question: How does the standard knowledge distillation loss weight the contribution of the teacher's softened probabilities versus the student's own predictions?

## Architecture Onboarding

- Component map: Teacher networks (various capacities) → Feature extraction layer → Classification layer → Softmax with temperature scaling → Student network learning objective
- Critical path: Training teachers → Computing instance-specific temperatures → Applying temperature scaling → Student training with distillation loss
- Design tradeoffs: ISATS provides instance-specific optimization but requires additional computation for temperature search; simpler methods like RegT are faster but may be less effective
- Failure signatures: Student performance degrades when using larger teachers without capacity mismatch solutions; minimal variance in non-ground-truth class probabilities; high correlation between teacher capacities and student degradation
- First 3 experiments:
  1. Visualize feature compactness metrics (intra-class vs inter-class angles) across teacher capacities on a simple dataset
  2. Compute rank set overlap and Spearman correlation between teacher outputs of different capacities on CIFAR-10
  3. Implement ISATS on CIFAR-100 and compare student performance against standard KD and ATS baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed ISATS method perform on other types of neural networks beyond convolutional architectures, such as transformers or graph neural networks?
- Basis in paper: [inferred] The paper focuses on evaluating ISATS on CNNs for image classification tasks (CIFAR-100, TinyImageNet, CUB, Stanford Dogs) but does not explore its effectiveness on other network architectures
- Why unresolved: The paper only demonstrates ISATS on convolutional neural networks for image classification tasks, leaving its performance on other architectures and domains unexplored
- What evidence would resolve it: Empirical results comparing ISATS performance on transformers, GNNs, or other non-CNN architectures across various domains (NLP, graph-based tasks, etc.)

### Open Question 2
- Question: What is the theoretical foundation for why relative class affinity remains consistent across teacher capacities while absolute probability values become less discriminative in larger teachers?
- Basis in paper: [explicit] The paper observes this phenomenon and provides empirical explanations through feature analysis and visualization, but does not provide a formal theoretical framework
- Why unresolved: The paper offers intuitive explanations based on feature compactness and inter-class distance changes, but lacks a rigorous mathematical proof or theoretical model explaining this consistency
- What evidence would resolve it: A formal theoretical framework or mathematical proof demonstrating why the rank ordering of class affinities remains stable while absolute probability values diverge as network capacity increases

### Open Question 3
- Question: How does the proposed ISATS method scale with extremely large teacher models (e.g., 100M+ parameters) or with models trained on datasets with significantly more classes (e.g., ImageNet with 1000 classes)?
- Basis in paper: [inferred] The paper evaluates on relatively small-scale problems (CIFAR-100 with 100 classes, TinyImageNet with 200 classes) and does not test on very large models or datasets with hundreds of classes
- Why unresolved: The experiments focus on moderate-sized models and datasets, leaving questions about ISATS's effectiveness and computational efficiency on larger-scale problems unanswered
- What evidence would resolve it: Empirical results showing ISATS performance on extremely large models (GPT-scale, ViT-Huge) and high-cardinality classification tasks (ImageNet, JFT-300M) with runtime analysis and accuracy comparisons

## Limitations

- The theoretical justification for why variance-maximizing temperatures improve distillation quality lacks rigorous mathematical proof
- The paper's core claims about feature compactness rely heavily on low-dimensional visualizations that may not capture high-dimensional dynamics
- Experiments focus on moderate-sized models and datasets, leaving questions about scalability to extremely large models and datasets unanswered

## Confidence

- **High confidence**: The empirical observation that larger teachers often underperform smaller ones in knowledge distillation is well-established and consistently reproduced across experiments
- **Medium confidence**: The proposed mechanisms explaining capacity mismatch (feature compactness/dispersion, rank consistency) are plausible but require more direct validation through ablation studies
- **Low confidence**: The theoretical justification for why variance-maximizing temperatures improve distillation quality lacks rigorous mathematical proof and could benefit from additional theoretical analysis

## Next Checks

1. Conduct ablation studies removing the feature compactness/dispersion mechanism to isolate its contribution to the capacity mismatch phenomenon
2. Perform controlled experiments varying only the temperature parameter while keeping teacher capacity constant to validate the ISATS mechanism independently
3. Extend experiments to larger-scale datasets (ImageNet, JFT) to verify whether the capacity mismatch phenomenon and proposed solutions generalize beyond academic benchmarks