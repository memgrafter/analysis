---
ver: rpa2
title: 'HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos'
arxiv_id: '2411.19167'
source_url: https://arxiv.org/abs/2411.19167
tags:
- object
- objects
- hand
- dataset
- pose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HOT3D is a large-scale egocentric dataset for 3D hand and object
  tracking that provides synchronized multi-view RGB/monochrome images, 3D poses,
  and object models. It contains over 833 minutes of recordings from 19 subjects interacting
  with 33 diverse objects using Project Aria glasses and Quest 3 headsets.
---

# HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos

## Quick Facts
- arXiv ID: 2411.19167
- Source URL: https://arxiv.org/abs/2411.19167
- Authors: Prithviraj Banerjee, Sindi Shkodrani, Pierre Moulon, Shreyas Hampali, Shangchen Han, Fan Zhang, Linguang Zhang, Jade Fountain, Edward Miller, Selen Basol, Richard Newcombe, Robert Wang, Jakob Julian Engel, Tomas Hodan
- Reference count: 40
- Primary result: HOT3D dataset with 833+ minutes of multi-view egocentric videos for 3D hand/object tracking; multi-view methods outperform single-view baselines by 41% for hand tracking, 8-12% for object pose estimation

## Executive Summary
HOT3D is a large-scale egocentric dataset enabling 3D hand and object tracking research with synchronized multi-view RGB/monochrome images, 3D poses, and object models. The dataset contains over 833 minutes of recordings from 19 subjects interacting with 33 diverse objects using Project Aria glasses and Quest 3 headsets. The authors demonstrate that multi-view methods significantly outperform single-view baselines across all tasks: UmeTrack hand tracking improves by 41%, FoundPose object pose estimation achieves 8-12% higher recall, and stereo matching for in-hand object localization substantially outperforms monocular approaches.

## Method Summary
The HOT3D dataset provides synchronized multi-view RGB/monochrome images from Project Aria glasses and Quest 3 headsets, along with 3D poses and object models for 19 subjects interacting with 33 objects. The authors evaluate three tasks: 3D hand tracking using a multi-view extension of UmeTrack, 6DoF object pose estimation using a multi-view extension of FoundPose, and 3D lifting of in-hand objects using stereo matching. The multi-view methods leverage geometric constraints from multiple viewpoints to improve accuracy compared to single-view baselines. The dataset uniquely enables evaluation of multi-view methods for these tasks, which were previously limited to single-view approaches.

## Key Results
- Multi-view UmeTrack hand tracking improves accuracy by 41% compared to single-view baseline
- Multi-view FoundPose object pose estimation achieves 8-12% higher recall rates (13-34% relative improvement)
- Stereo matching for 3D lifting of in-hand objects substantially outperforms monocular depth estimation
- HOT3D provides the first dataset enabling evaluation of multi-view methods for these three tasks simultaneously

## Why This Works (Mechanism)

### Mechanism 1
Multi-view input significantly improves accuracy in 3D hand tracking by providing additional constraints that help resolve depth ambiguities present in single-view images. When using multiple synchronized cameras, the system can triangulate 3D points from 2D observations across views, reducing uncertainty in depth estimation. UmeTrack achieves 41% improvement in accuracy when moving from single-view to two-view input.

**Core assumption:** Cameras are properly calibrated and synchronized, allowing accurate correspondence between views and reliable triangulation.

**Evidence anchors:**
- [abstract] "Experiments show that multi-view methods significantly outperform single-view baselines across all tasks: UmeTrack hand tracking improves by 41%"
- [section 4.1] "The accuracy drop is even larger when the tracker, still trained only on one of the datasets, is evaluated in the two-view mode."

**Break condition:** Camera synchronization fails, calibration drifts over time, or the hand moves too rapidly between frames causing temporal misalignment.

### Mechanism 2
FoundPose's multi-view extension achieves 8-12% higher recall rates by establishing 2D-3D correspondences across multiple views rather than a single view. The multi-view FoundPose retrieves templates based on the sum of per-view template scores, then establishes multi-view 2D-3D correspondences between each template and all views before solving the generalized PnP problem.

**Core assumption:** Objects maintain consistent appearance across views and the 2D-3D correspondences can be reliably established using DINOv2 features.

**Evidence anchors:**
- [abstract] "multi-view methods significantly outperform single-view baselines... FoundPose object pose estimation achieves 8-12% higher recall"
- [section 4.2] "Our straightforward multi-view extension of FoundPose significantly outperforms the original single-view version, achieving 8–12% higher recall rates."

**Break condition:** Poor feature matching between views due to lighting differences, object texture changes, or extreme occlusions.

### Mechanism 3
Stereo matching for 3D lifting of in-hand objects substantially outperforms monocular approaches by leveraging geometric constraints from two viewpoints. The StereoMatch method constructs stereo crop pairs that surround object masks in both views, extracts DINOv2 patch features, establishes 2D-2D correspondences between the crops, triangulates these correspondences to obtain 3D points, and estimates the object location using the robust mean of the 3D point set.

**Core assumption:** The stereo baseline is sufficient to create meaningful disparity for depth estimation, and the object segmentation masks in both views are accurate.

**Evidence anchors:**
- [abstract] "stereo matching for in-hand object localization substantially outperforms monocular approaches"
- [section 4.4] "The StereoMatch method outperforms MonoDepth regardless of whether the ground-truth segmentation masks or masks predicted by MRCNN-DA are used as input."

**Break condition:** Insufficient stereo baseline, inaccurate segmentation masks, or objects moving too rapidly between views causing temporal misalignment.

## Foundational Learning

- **Concept: Epipolar geometry and triangulation**
  - Why needed here: Multi-view tracking fundamentally relies on establishing correspondences between views and triangulating 3D points from 2D observations
  - Quick check question: Given two calibrated cameras observing the same point, what geometric constraint must the corresponding image points satisfy?

- **Concept: Camera calibration and synchronization**
  - Why needed here: Accurate multi-view tracking requires precise knowledge of camera parameters and temporal alignment of image streams
  - Quick check question: What happens to triangulation accuracy if camera intrinsics are misestimated by 1%?

- **Concept: Feature matching and correspondence**
  - Why needed here: Methods like FoundPose and StereoMatch depend on establishing reliable correspondences between views using learned features
  - Quick check question: How does the choice of feature descriptor affect the robustness of multi-view correspondence establishment?

## Architecture Onboarding

- **Component map:** Aria/Quest3 sensors → synchronization → calibration → storage → Image preprocessing → feature extraction → correspondence → triangulation → pose estimation → evaluation

- **Critical path:** Sensor data capture → multi-view synchronization → calibration → feature extraction → correspondence establishment → 3D reconstruction → pose estimation → evaluation

- **Design tradeoffs:**
  - Sensor resolution vs. processing speed (higher resolution improves accuracy but increases computational load)
  - Number of views vs. calibration complexity (more views provide better constraints but require more complex calibration)
  - Feature descriptor complexity vs. matching robustness (complex features may be more discriminative but slower to compute)
  - Temporal resolution vs. storage requirements (higher frame rates capture more detail but require more storage)

- **Failure signatures:**
  - Large triangulation errors indicating calibration issues
  - Poor correspondence matching suggesting feature descriptor problems
  - Inconsistent results across views indicating synchronization issues
  - Sudden accuracy drops during rapid hand movements suggesting temporal aliasing

- **First 3 experiments:**
  1. Verify multi-view synchronization by checking timestamp alignment across all camera streams
  2. Test calibration accuracy by triangulating known points and measuring reconstruction error
  3. Evaluate single-view vs. multi-view performance on a simple tracking task to validate the core mechanism claim

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several important questions emerge from the work:

1. How does the performance of multi-view egocentric tracking methods compare when using different headset configurations (e.g., Aria's RGB + monochrome setup vs Quest 3's dual monochrome)?
2. What is the impact of temporal information on the performance of 3D lifting methods for in-hand objects?
3. How does the quality of 3D object models (geometry and PBR materials) affect the performance of model-based 6DoF object pose estimation?
4. How does the eye gaze signal from Aria correlate with hand-object interaction patterns and can it improve tracking performance?

## Limitations

- Domain gap between UmeTrack and HOT3D datasets may cause transfer learning issues when applying models trained on one dataset to the other
- Multi-view methods assume ideal conditions (proper calibration, sufficient baseline, accurate segmentation) that may not hold in real-world scenarios
- The dataset is limited to specific headset configurations and may not generalize to other camera setups or interaction scenarios

## Confidence

- **High Confidence:** The fundamental mechanism of multi-view triangulation improving depth estimation accuracy is well-established and directly supported by the experimental results.
- **Medium Confidence:** The specific performance improvements (41% for hand tracking, 8-12% for object pose) are demonstrated on HOT3D but may not generalize to other datasets or real-world conditions due to domain shift issues.
- **Low Confidence:** The substantial superiority of stereo matching over monocular approaches assumes ideal conditions (sufficient baseline, accurate masks) that may not hold in practice.

## Next Checks

1. Conduct ablation studies on HOT3D to quantify the impact of calibration accuracy and temporal synchronization on multi-view performance improvements.
2. Test model transfer from HOT3D to real-world egocentric video sequences to evaluate domain generalization capabilities.
3. Evaluate performance degradation under challenging conditions (low light, extreme occlusions, insufficient stereo baseline) to identify failure modes.