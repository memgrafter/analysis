---
ver: rpa2
title: 'SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian Languages'
arxiv_id: '2403.18933'
source_url: https://arxiv.org/abs/2403.18933
tags:
- semantic
- they
- task
- mexico
- semeval-2024
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This shared task introduced the first large-scale evaluation of
  semantic textual relatedness (STR) across 14 African and Asian languages. Unlike
  earlier tasks focusing on similarity, this work tackled the broader phenomenon of
  semantic relatedness using comparative annotations and Best-Worst Scaling.
---

# SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian Languages

## Quick Facts
- **arXiv ID**: 2403.18933
- **Source URL**: https://arxiv.org/abs/2403.18933
- **Reference count**: 40
- **Primary result**: First large-scale evaluation of semantic textual relatedness across 14 African and Asian languages, with top methods achieving Spearman correlations up to 0.819

## Executive Summary
This paper presents the first large-scale evaluation of semantic textual relatedness (STR) across 14 African and Asian languages. The task introduced Best-Worst Scaling (BWS) annotation methodology to address limitations of traditional rating scales, collecting comparative annotations from native speakers. Three competition tracks (supervised, unsupervised, and crosslingual) attracted 163 participants and 70 submissions from 51 teams. Top-performing approaches combined data augmentation, multilingual transformers, and adapter-based fine-tuning, demonstrating that STR remains challenging across languages with no single approach dominating universally.

## Method Summary
The task involved predicting semantic textual relatedness scores for sentence pairs across 14 languages using three tracks: supervised (with labeled data), unsupervised (without semantic relatedness data), and crosslingual (using labeled data from other languages). Teams employed various approaches including multilingual transformers, adapter-based fine-tuning, data augmentation through translation, and character n-gram similarity measures. The evaluation metric was Spearman rank correlation between predicted and human-annotated rankings. BWS annotation methodology was used to collect comparative judgments from native speakers, avoiding biases of traditional rating scales.

## Key Results
- 163 participants submitted 70 systems across 51 teams
- Top supervised track performance reached Spearman correlation of 0.819
- No single approach dominated universally across all languages and tracks
- Adapter-based fine-tuning combined with data augmentation showed strong performance
- Character n-gram methods proved effective in unsupervised settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Best-Worst Scaling (BWS) produces more reliable and discriminating scores than traditional rating scales for semantic relatedness annotation.
- Mechanism: BWS asks annotators to select both best and worst items from a tuple, revealing order relationships between all pairs within the tuple and reducing biases inherent in rating scales.
- Core assumption: Annotators can reliably identify relative relationships between sentence pairs rather than absolute scores.
- Evidence anchors:
  - [abstract]: "we use comparative annotations and rely on the intuitions of fluent speakers for each language to choose between sentence pairs"
  - [section]: "We used Best–Worst Scaling (BWS) (Louviere and Woodworth, 1991; Kiritchenko and Mohammad, 2017a), a form of comparative annotation that avoids various biases of traditional rating scales"
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.528, average citations=0.0" - Weak evidence, no direct citations to BWS methodology
- Break condition: If annotators cannot reliably identify relative relationships or if language-specific cultural differences affect relative judgments.

### Mechanism 2
- Claim: Translation-based data augmentation combined with adapter-based fine-tuning outperforms standard fine-tuning for low-resource languages.
- Mechanism: Translation of English training data to target languages increases training data size, while adapter-based tuning preserves the pre-trained model's knowledge while adapting to the specific task.
- Core assumption: The semantic relationships captured in English data transfer well to other languages when properly translated.
- Evidence anchors:
  - [section]: "The team explored both fine-tuning and adapter-based tuning. Given a target language, they first fine-tuned the cross-encoder-based AfroXLMR model... on the augmented data as a warm-up or TAPT"
  - [section]: "They experimented with full fine-tuning, adapter fine-tuning using MAD (Pfeiffer et al., 2020), and data augmentation using different language combinations"
  - [corpus]: Weak evidence - no direct corpus support for adapter-based tuning effectiveness
- Break condition: If translation quality is poor or if semantic relationships don't transfer well across languages.

### Mechanism 3
- Claim: Character n-gram based similarity measures can effectively capture semantic relatedness without requiring large training datasets.
- Mechanism: Character n-grams capture surface-level similarities and patterns that correlate with semantic relatedness, especially useful for unsupervised settings.
- Core assumption: Surface-level textual patterns correlate with semantic relationships across languages.
- Evidence anchors:
  - [section]: "The system processed each pair of utterances independently, generating a distance between them without relying on additional information. Their pre-processing involved lower-casing of texts and making use of character n-grams ranging from 1 to 5 characters"
  - [section]: "The HW–TSC team used innovative techniques such as the N-gram chars method with XLM-R and m-BERT tokenizers"
  - [corpus]: Weak evidence - no direct corpus support for character n-gram effectiveness
- Break condition: If character n-gram patterns don't correlate with semantic relationships or if languages have vastly different character systems.

## Foundational Learning

- Concept: Best-Worst Scaling methodology
  - Why needed here: Understanding why BWS was chosen over traditional rating scales and how it affects annotation reliability
  - Quick check question: How does BWS reduce annotation biases compared to traditional rating scales?

- Concept: Adapter-based fine-tuning
  - Why needed here: Understanding the trade-offs between full fine-tuning and adapter-based approaches for multilingual models
  - Quick check question: What are the advantages of adapter-based tuning over full fine-tuning in terms of model capacity and training efficiency?

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding how models can leverage data from high-resource languages to improve performance on low-resource languages
  - Quick check question: What factors determine the success of cross-lingual transfer for semantic relatedness tasks?

## Architecture Onboarding

- Component map: Data pipeline (collection → preprocessing → augmentation) -> Model selection (monolingual vs multilingual vs cross-lingual) -> Training pipeline (fine-tuning vs adapter-based vs unsupervised) -> Evaluation pipeline (Spearman correlation calculation)

- Critical path: Data collection → Annotation (BWS) → Model training → Evaluation → System submission

- Design tradeoffs:
  - Translation quality vs data quantity in augmentation
  - Model size vs computational efficiency
  - Monolingual specialization vs multilingual generalization

- Failure signatures:
  - Low Spearman correlation despite high training accuracy
  - Performance degradation on specific language families
  - High variance in results across different language pairs

- First 3 experiments:
  1. Implement and test BWS annotation methodology on a small dataset to verify reliability
  2. Compare full fine-tuning vs adapter-based tuning on a multilingual model with augmented data
  3. Evaluate character n-gram based similarity against transformer-based approaches in unsupervised setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the use of language family information as a feature affect semantic textual relatedness prediction performance across different language families in the SemEval-2024 Task 1?
- Basis in paper: [explicit] The paper mentions that team MaiNLP leveraged information about language families in Track C, which led to good and interpretable results.
- Why unresolved: The paper does not provide detailed quantitative results or analysis of the impact of language family features on performance across different language families.
- What evidence would resolve it: A detailed analysis comparing the performance of models with and without language family features across different language families would resolve this question.

### Open Question 2
- Question: What is the impact of data augmentation techniques, such as translation and paraphrasing, on the performance of semantic textual relatedness prediction models across low-resource languages?
- Basis in paper: [explicit] The paper mentions that teams like AAdaM used translation-based data augmentation to increase training data size for better performance.
- Why unresolved: The paper does not provide a comprehensive analysis of the effectiveness of different data augmentation techniques across all languages or a comparison of their impact on low-resource versus high-resource languages.
- What evidence would resolve it: A systematic study comparing the performance of models trained with different data augmentation techniques across low-resource and high-resource languages would resolve this question.

### Open Question 3
- Question: How do different embedding methods, such as character n-grams and language-specific BERT models, compare in their effectiveness for semantic textual relatedness prediction in unsupervised settings?
- Basis in paper: [explicit] The paper mentions that teams like SATLab used character n-grams and MasonTigers used various embedding methods including language-specific BERT models.
- Why unresolved: The paper does not provide a detailed comparison of the effectiveness of these embedding methods in unsupervised settings across different languages.
- What evidence would resolve it: A comparative study evaluating the performance of different embedding methods in unsupervised settings across various languages would resolve this question.

## Limitations

- BWS annotation methodology lacks direct empirical validation within this work, with only weak evidence for annotation reliability
- Success of adapter-based fine-tuning over full fine-tuning is demonstrated anecdotally rather than systematically compared
- Character n-gram approach effectiveness across diverse language families remains underexplored, particularly for languages with vastly different character systems
- Potential cultural biases in semantic relatedness judgments across different language communities were not addressed

## Confidence

- **High confidence**: The task successfully established a new benchmark for STR across 14 African and Asian languages, with clear participation metrics (163 participants, 70 submissions from 51 teams) and measurable performance outcomes.
- **Medium confidence**: The superiority of BWS over traditional rating scales is theoretically justified but lacks direct empirical validation within this work.
- **Medium confidence**: Translation-based augmentation combined with adapter fine-tuning shows strong results, but the specific conditions under which this outperforms alternatives need more systematic testing.
- **Low confidence**: Character n-gram effectiveness across diverse language families remains underexplored, particularly for languages with vastly different character systems.

## Next Checks

1. **Annotation Reliability Validation**: Conduct a follow-up study comparing BWS annotations against traditional rating scales on the same sentence pairs to quantify bias reduction and reliability improvements across all 14 languages.

2. **Systematic Ablation Study**: Design controlled experiments comparing full fine-tuning vs. adapter-based tuning across different language families and data augmentation strategies to identify optimal conditions for each approach.

3. **Cross-Cultural Semantic Analysis**: Analyze STR performance variance across language families to identify whether semantic relatedness judgments are culturally influenced, and whether this affects model generalization across African vs. Asian language groups.