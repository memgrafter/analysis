---
ver: rpa2
title: Data-Driven Subsampling in the Presence of an Adversarial Actor
arxiv_id: '2401.03488'
source_url: https://arxiv.org/abs/2401.03488
tags:
- subsampling
- adversarial
- data
- modulation
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effects of adversarial attacks on a
  data-driven subsampling-based automatic modulation classification (AMC) system.
  The system uses deep learning models for both classification and subsampling.
---

# Data-Driven Subsampling in the Presence of an Adversarial Actor

## Quick Facts
- arXiv ID: 2401.03488
- Source URL: https://arxiv.org/abs/2401.03488
- Authors: Abu Shafin Mohammad Mahdee Jameel; Ahmed P. Mohamed; Jinho Yi; Aly El Gamal; Akshay Malhotra
- Reference count: 22
- Primary result: Data-driven subsampling improves adversarial robustness in automatic modulation classification systems

## Executive Summary
This paper investigates how data-driven subsampling affects adversarial robustness in automatic modulation classification (AMC) systems. The authors demonstrate that subsampling itself acts as an effective defense mechanism against adversarial attacks, with data-driven subsamplers showing particular resilience. Through experiments with various subsampling schemes and attack scenarios, the study finds that protecting the choice of subsampler from adversaries significantly enhances system robustness, with the ResNet subsampler performing best overall.

## Method Summary
The research employs a deep learning-based AMC system where both classification and subsampling are performed using neural networks. The authors evaluate multiple subsampling strategies including fixed methods (uniform) and data-driven approaches (Complex-CNN, ResNet, CLDNN, and holistic ensemble). Adversarial attacks are simulated using the Carlini-Wagner L∞ attack on both the classifier alone (Mod Attack) and both classifier and subsampler (Mod+SubSamp Attack). Performance is measured across different signal-to-noise ratios (SNR) to assess robustness under varying channel conditions.

## Key Results
- Subsampling itself is an effective deterrent to adversarial attacks, reducing accuracy drops compared to no subsampling
- The ResNet subsampler demonstrates the highest robustness against both Mod Attack and Mod+SubSamp Attack scenarios
- Protecting the choice of subsampler from adversaries significantly improves adversarial robustness

## Why This Works (Mechanism)

### Mechanism 1
- Subsampling introduces adversarial robustness by reducing the effective attack surface, acting as a compression filter that limits the number of coordinates an adversary can manipulate
- Core assumption: The subsampler selects informative points that the classifier depends on, minimizing the impact of altering other points
- Evidence: Accuracy drops are consistently higher when no subsampling is employed compared to when subsampling is used

### Mechanism 2
- Protecting the choice of subsampler from the adversary significantly improves adversarial robustness by forcing attackers to optimize against a generic worst-case
- Core assumption: The subsampler selection is unpredictable and the adversary cannot adaptively choose which subsampler to attack
- Evidence: In the absence of information about the specific subsampler, the adversary's ability to impact classification accuracy becomes significantly limited

### Mechanism 3
- Data-driven subsamplers outperform fixed subsamplers in adversarial scenarios because they adapt to dataset structure and learn which samples are most informative for classification
- Core assumption: Training data is representative of the adversarial environment and the subsampler's learned pattern remains effective under attack
- Evidence: The ResNet subsampler provides the best classification accuracies across all attack scenarios

## Foundational Learning

- Concept: Automatic Modulation Classification (AMC) and its role in adaptive wireless systems
  - Why needed: Understanding AMC is essential to interpret results in the context of wireless communication systems
  - Quick check: What is the difference between likelihood-based and feature-based AMC, and why do deep learning approaches improve upon them?

- Concept: Adversarial attacks in deep learning, especially the Carlini-Wagner (CW) L∞ attack
  - Why needed: The threat model relies on CW L∞ perturbations to understand why subsampling helps
  - Quick check: How does the CW L∞ attack constrain perturbation magnitude, and why is it effective against modulation classifiers?

- Concept: Subsampling strategies (fixed vs data-driven) and ensemble methods
  - Why needed: The paper compares multiple subsampling schemes, making their differences key to understanding experimental results
  - Quick check: What distinguishes a data-driven subsampler from a fixed subsampler, and how does the ensemble approach combine their strengths?

## Architecture Onboarding

- Component map: Base station -> I/Q samples transmission -> Subsampler -> Classifier -> Modulation decision
- Critical path:
  1. Base station encodes data with modulation scheme
  2. I/Q samples transmitted over-the-air
  3. If no attack, UE applies subsampler → classifier → decoded modulation
  4. If Mod Attack, adversary modifies I/Q → UE subsampler + classifier
  5. If Mod+SubSamp Attack, adversary predicts subsampling → modifies → UE classifier

- Design tradeoffs:
  - Subsampling rate vs. classification accuracy: Higher subsampling speeds computation but risks accuracy loss
  - Data-driven vs. fixed subsampling: Data-driven adapts to dataset but requires training; fixed is simple but less robust
  - Ensemble subsampling vs. single model: Holistic combines multiple subsamplers for robustness but adds complexity

- Failure signatures:
  - Sudden drop in classification accuracy under Mod Attack indicates classifier vulnerability
  - Greater accuracy drop under Mod+SubSamp Attack indicates subsampler vulnerability
  - Uniform performance degradation across SNR suggests systemic issues rather than attack-specific problems

- First 3 experiments:
  1. Run AMC classifier with no subsampling across SNR range 0–20 dB; record baseline accuracy
  2. Introduce ResNet subsampler (N=64, α=2) and repeat classification; compare accuracy and speed
  3. Simulate Mod Attack using CW L∞; measure accuracy drop with and without subsampling; analyze subsampler resilience

## Open Questions the Paper Calls Out

### Open Question 1
- How does the performance of data-driven subsampling compare to traditional fixed subsampling methods in terms of adversarial robustness?
- Basis: The paper discusses both fixed and data-driven subsampling schemes and compares their performance
- Why unresolved: The paper provides comparison but doesn't explain why data-driven subsampling might be more effective
- What evidence would resolve it: Detailed analysis of mechanisms by which data-driven subsampling enhances adversarial robustness

### Open Question 2
- What is the impact of the choice of ranker model (RM) on the adversarial robustness of the subsampling scheme?
- Basis: The paper mentions different ranker models (Complex-CNN, ResNet, CLDNN) are used
- Why unresolved: The paper doesn't analyze how RM choice affects subsampling robustness
- What evidence would resolve it: Comparative study of adversarial robustness of different ranker models

### Open Question 3
- How does the ensemble method (Holistic Subsampler) perform in terms of adversarial robustness compared to individual data-driven subsamplers?
- Basis: The paper introduces a holistic subsampler that combines multiple methods
- Why unresolved: The paper doesn't provide detailed comparison against individual subsamplers in adversarial scenarios
- What evidence would resolve it: Detailed analysis of holistic subsampler's performance in adversarial attacks

### Open Question 4
- What is the effect of varying the subsampling rate on the adversarial robustness of the AMC system?
- Basis: The paper mentions different subsampling rates but doesn't explore their impact
- Why unresolved: The paper lacks comprehensive analysis of how different rates affect robustness
- What evidence would resolve it: Experimental results showing impact of different subsampling rates

### Open Question 5
- How does the adversarial robustness of the AMC system change when the adversary has partial knowledge about the subsampling scheme?
- Basis: The paper discusses scenarios with partial knowledge
- Why unresolved: The paper doesn't analyze how partial knowledge affects robustness
- What evidence would resolve it: Experimental results showing impact of partial knowledge

## Limitations

- Experimental validation is limited to the RML2016.10a dataset and specific deep learning architectures (ResNet-based)
- Study focuses on CW L∞ attacks with fixed perturbation budget, leaving open questions about other attack types
- Assumption that protecting subsampler choice is feasible in practice may not hold in all deployment scenarios

## Confidence

- **High confidence**: The core finding that subsampling reduces accuracy drops under adversarial attack is well-supported by experimental results
- **Medium confidence**: The claim that ResNet subsampler provides best robustness requires more extensive validation across different datasets
- **Medium confidence**: The recommendation to protect subsampler choice from adversaries is logically sound but lacks empirical validation of implementation feasibility

## Next Checks

1. Validate the adversarial robustness findings using RML2016.10b and other modulation classification datasets to assess whether ResNet subsampling remains superior across different signal conditions

2. Evaluate system performance against multiple attack types (PGD, FGSM, boundary attacks) beyond CW L∞ to determine if subsampling robustness generalizes across attack methodologies

3. Design experiments where the adversary can probe the subsampler's selection pattern over time, testing whether the proposed protection mechanism remains effective under adaptive attack strategies