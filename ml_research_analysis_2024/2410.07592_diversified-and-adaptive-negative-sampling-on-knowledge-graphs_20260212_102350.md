---
ver: rpa2
title: Diversified and Adaptive Negative Sampling on Knowledge Graphs
arxiv_id: '2410.07592'
source_url: https://arxiv.org/abs/2410.07592
tags:
- negative
- triplets
- knowledge
- sampling
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating high-quality negative
  triplets for knowledge graph embedding. Existing methods often ignore the diversity
  and adaptiveness of the sampling process, which harms the informativeness of negative
  triplets.
---

# Diversified and Adaptive Negative Sampling on Knowledge Graphs

## Quick Facts
- arXiv ID: 2410.07592
- Source URL: https://arxiv.org/abs/2410.07592
- Reference count: 40
- One-line primary result: DANS achieves state-of-the-art performance with MRR of 0.404, 0.257, and 0.920 on WN18RR, NELL-995, and UMLS using DistMult decoder

## Executive Summary
This paper addresses the problem of generating high-quality negative triplets for knowledge graph embedding by proposing a generative adversarial approach called Diversified and Adaptive Negative Sampling (DANS). The method introduces a two-way generator that produces diverse negative triplets associated with both entities and relations, along with an adaptive mechanism using FiLM layers to localize the global generator for different entities and relations. Evaluated on three benchmark knowledge graphs (WN18RR, NELL-995, UMLS) using DistMult, RotatE, and ComplEx decoders, DANS consistently outperforms state-of-the-art baselines and demonstrates the effectiveness of both the two-way generator and adaptive FiLM layer through ablation studies.

## Method Summary
DANS operates on top of existing KG embedding models using a GAN architecture where a two-way generator creates negative samples and a discriminator evaluates them. The generator has two pathways: G_E conditioned on entity embeddings and G_R conditioned on entity-relation product embeddings, both enhanced with FiLM layers for entity/relation-specific adaptation. The discriminator uses both adversarial and auxiliary losses to improve sample quality. Training alternates between updating the generator, discriminator, and base model. The method generates Ns=20 negative triplets per positive triplet, with 10 from each generator pathway and 10 via random sampling, and evaluates using filtered ranking metrics (MRR, H@1, NDCG@5).

## Key Results
- DANS achieves state-of-the-art MRR of 0.404 on WN18RR, 0.257 on NELL-995, and 0.920 on UMLS using DistMult decoder
- The two-way generator design significantly improves performance compared to single-pathway alternatives
- FiLM layer adaptation provides consistent gains across all three benchmark datasets
- DANS outperforms all baseline methods across different decoder models (DistMult, RotatE, ComplEx)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Two-way generator increases overall informativeness by producing negative triplets associated with both entities and relations.
- **Mechanism:** The generator has two pathways: one conditioned on entity embeddings (N(e_h, σ²I)) and another conditioned on entity-relation product embeddings (N(e_h ⊗ e_r, σ²I)). This diversification leads to negative samples drawn from two distinct prior distributions, increasing coverage.
- **Core assumption:** Negative triplets that are related to either the head entity or the relation (or both) are more informative than random negative triplets.
- **Evidence anchors:**
  - [abstract] "DANS is equipped with a two-way generator that generates more diverse negative triplets through two pathways"
  - [section] "we propose a two-way generator that consists of two pathways, namely G_E and G_R, to generate negative triplets associated with a given entity and entity-relation, respectively"
  - [corpus] Weak - no direct corpus mention of the two-way generator design
- **Break condition:** If the diversity gain is outweighed by noise from unrelated negative triplets, or if the prior distributions overlap too much with positive samples.

### Mechanism 2
- **Claim:** FiLM layer increases individual informativeness by adapting global generator to local entity/relation contexts.
- **Mechanism:** Feature-wise Linear Modulation applies learnable scaling (α) and shifting (β) to each hidden layer activation based on entity/relation input, effectively creating entity-specific or relation-specific generators without training separate models.
- **Core assumption:** Different entities and relations have distinct sampling needs, and a single global generator cannot optimally adapt to these differences.
- **Evidence anchors:**
  - [abstract] "an adaptive mechanism that produces more fine-grained examples by localizing the global generator for different entities and relations"
  - [section] "we employ a Feature-wise Linear Modulation (FiLM) layer [23] that conditions the generator on a given entity or entity-relation input"
  - [corpus] Weak - no corpus evidence directly supporting FiLM adaptation for negative sampling
- **Break condition:** If FiLM parameters overfit to training entities, leading to poor generalization on unseen entities.

### Mechanism 3
- **Claim:** Two-way discriminator improves generator quality by distinguishing real vs fake entities and fake entities from different generator pathways.
- **Mechanism:** Discriminator has two pathways: D_Adv for real/fake classification and D_Aux for distinguishing between G_E and G_R outputs. This dual-task learning provides richer feedback to the generator.
- **Core assumption:** Making the discriminator more sensitive to differences between generator pathways will force the generator to produce more distinct and informative negative samples.
- **Evidence anchors:**
  - [abstract] "the discriminator which utilize both adversarial and auxiliary losses to improve the quality of produced samples"
  - [section] "we further equip the discriminator with the ability to distinguish the fake entities generated by the two pathways"
  - [corpus] Weak - no corpus evidence directly supporting two-way discriminator for negative sampling
- **Break condition:** If the auxiliary task (distinguishing G_E vs G_R outputs) becomes too easy or too hard, it may not provide useful gradient signals to the generator.

## Foundational Learning

- **Concept:** Knowledge Graph Embeddings
  - **Why needed here:** DANS operates on top of existing KG embedding models (RGCN + DistMult/RotatE/ComplEx), so understanding how entities and relations are represented as vectors is essential.
  - **Quick check question:** What is the scoring function for DistMult, and how does it determine if a triplet is positive or negative?

- **Concept:** Generative Adversarial Networks
  - **Why needed here:** DANS uses GAN architecture where the generator creates negative samples and the discriminator evaluates them. Understanding the minimax game and training dynamics is crucial.
  - **Quick check question:** How does the generator's objective function encourage it to produce "harder" negative samples that fool the discriminator?

- **Concept:** Graph Convolutional Networks
  - **Why needed here:** RGCN is used as the base embedding model, and understanding how GCNs aggregate neighborhood information for node embeddings is important for grasping how entity representations are computed.
  - **Quick check question:** How does the RGCN equation aggregate information from neighbors under different relations?

## Architecture Onboarding

- **Component map:** Base model (RGCN + DistMult/RotatE/ComplEx decoder) -> Generator (G_E and G_R pathways with FiLM layers) -> Discriminator (D_Adv and D_Aux pathways) -> Alternating training updates

- **Critical path:** Generator → Discriminator → Base model updates in alternating fashion. The quality of negative samples directly impacts base model training.

- **Design tradeoffs:**
  - Two pathways vs single pathway: More diversity but increased complexity and training time
  - FiLM adaptation vs separate models per entity: More efficient but risk of overfitting
  - Random sampling for diversity vs only generator outputs: Balances informativeness and coverage

- **Failure signatures:**
  - Generator collapse: Discriminator becomes too strong, generator produces low-quality samples
  - Mode collapse: Generator focuses on one type of negative sample, reducing diversity
  - Overfitting: FiLM parameters become too entity-specific, hurting generalization
  - Unstable training: Discriminator and generator losses oscillate without convergence

- **First 3 experiments:**
  1. Ablation study: Compare DANS with single-pathway generator (G_E only, G_R only, both without FiLM) on WN18RR using DistMult decoder
  2. Parameter sensitivity: Test different numbers of negative triplets per positive (Ns = 5, 10, 20, 30) and regularization coefficients (λ = 1e-6, 1e-5, 1e-4, 1e-3)
  3. Diversity visualization: Use t-SNE to visualize positive vs negative entity embeddings for a specific relation, comparing DANS vs random sampling

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions arise from the work:

## Limitations
- The paper lacks detailed architectural specifications for FiLM layers and MLP depths in both generator and discriminator pathways
- Exact interaction between adversarial and auxiliary losses during training is not fully clarified
- Implementation details for handling entity/relation-specific adaptation without overfitting are sparse
- Scalability to larger knowledge graphs with millions of entities and relations is not tested

## Confidence
- High confidence in the overall GAN-based framework and its theoretical motivation
- Medium confidence in the effectiveness of two-way generator design due to ablation study results
- Low confidence in FiLM layer implementation details and their actual impact on diversity

## Next Checks
1. Implement the FiLM layer ablation to test whether entity-specific adaptation actually improves negative sample quality or if it's overfitting
2. Conduct diversity analysis using t-SNE to visualize whether two-way generator outputs cover more diverse regions of the embedding space than single-pathway approaches
3. Test the stability of adversarial training by varying the ratio of generator to discriminator updates and monitoring convergence behavior across multiple runs