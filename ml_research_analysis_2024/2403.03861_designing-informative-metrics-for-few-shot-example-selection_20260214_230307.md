---
ver: rpa2
title: Designing Informative Metrics for Few-Shot Example Selection
arxiv_id: '2403.03861'
source_url: https://arxiv.org/abs/2403.03861
tags:
- examples
- sentence
- prompt
- tagging
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a complexity-based prompt selection method
  for few-shot sequence tagging tasks. The core idea is to select training examples
  whose syntactic and semantic complexity aligns with the test sample using sentence-level
  and word-level metrics: normalized sentence similarity, normalized smoothed length
  similarity, and normalized label entropy.'
---

# Designing Informative Metrics for Few-Shot Example Selection

## Quick Facts
- arXiv ID: 2403.03861
- Source URL: https://arxiv.org/abs/2403.03861
- Reference count: 3
- Primary result: Complexity-based prompt selection improves few-shot sequence tagging performance by up to 28.85 F1/accuracy points

## Executive Summary
This paper introduces a complexity-based approach for selecting informative examples in few-shot sequence tagging tasks. Rather than relying on semantic similarity alone, the method uses three metrics—normalized sentence similarity, normalized smoothed length similarity, and normalized label entropy—to align the syntactic and semantic complexity of training examples with test samples. The approach significantly improves few-shot performance across multiple models and tasks, achieving state-of-the-art results on CoNLL2003 NER with a 5% absolute improvement for GPT-4.

## Method Summary
The method selects training examples whose complexity profiles match the test sample using a combination of sentence-level and word-level metrics. For each test sentence, the approach calculates three similarity scores with each candidate training example: semantic similarity using sentence embeddings (all-MiniLM-L6-v2), syntactic similarity using smoothed length matching, and label diversity using entropy. These are combined into a weighted complexity score, with task-specific weights optimized via grid search. The top-k examples by complexity score are selected to form the few-shot prompt, which is then used with PLMs like GPT-Neo, GPT-j, or Llama2 for inference.

## Key Results
- Complexity-based retrieval improves few-shot accuracy by up to 28.85 points over baselines across NER, POS tagging, chunking, and privacy policy datasets
- Achieves state-of-the-art few-shot NER results on CoNLL2003 with a 5% absolute improvement for GPT-4
- Shows consistent improvements across multiple model sizes, from smaller models like GPT-j-6B to larger ones like GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Complexity-based prompt retrieval aligns the syntactic and semantic complexity of training examples with the test sample.
- Mechanism: The approach calculates similarity scores between the test sentence and each candidate training example using normalized sentence similarity, normalized smoothed length similarity, and normalized label entropy. Examples with the highest combined complexity score are selected as the few-shot prompt.
- Core assumption: Matching the syntactic and semantic complexity of examples to the test sample improves the model's ability to generalize.
- Evidence anchors:
  - [abstract] "This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples."
  - [section 3] "Our core idea is simple: to select examples that closely resemble the test sample in both semantics and length while ensuring a diverse range of labels for the task."
  - [corpus] Weak. Only 5 related papers found, average FMR=0.538. Limited direct evidence of complexity matching improving few-shot performance.
- Break condition: If the training data lacks examples with sufficiently diverse complexity profiles, the method may fail to find good matches for certain test samples.

### Mechanism 2
- Claim: Sentence-level and word-level metrics capture complementary aspects of complexity.
- Mechanism: Sentence similarity measures semantic alignment, smoothed length similarity ensures syntactic similarity, and label entropy encourages selection of examples with diverse label distributions. The combination provides a more holistic measure of example informativeness.
- Core assumption: Different complexity metrics capture different facets of example quality for few-shot learning.
- Evidence anchors:
  - [section 3] "We use both sentence- and word-level complexity measures to match prompt examples to evaluation sentences."
  - [section 3.4] "We propose a complexity score to align the syntactico-semantic complexity of prompts and examples. The three component metrics are weighted and summed to produce the final complexity score."
  - [corpus] Weak. No direct evidence that combining multiple complexity metrics is better than using a single metric.
- Break condition: If one metric dominates the complexity score due to poor weight tuning, the complementary benefits may be lost.

### Mechanism 3
- Claim: Complexity-based retrieval is more effective than random selection or embedding-based retrieval.
- Mechanism: By explicitly measuring and matching complexity, the method can select examples that are more informative for the specific test sample, rather than relying on semantic similarity alone.
- Core assumption: Complexity alignment is a better proxy for example informativeness than semantic similarity.
- Evidence anchors:
  - [abstract] "We also see large gains of upto 28.85 points (F1/Acc.) in smaller models likeGPT-j-6B."
  - [section 5] "Our results show that CP retrieval significantly improves few-shot accuracy over the baseline across all models and considered tasks."
  - [corpus] Moderate. Related work on example selection exists, but specific evidence for complexity-based methods is limited.
- Break condition: If the complexity metrics are poorly correlated with actual example informativeness for the task, the method may underperform simpler baselines.

## Foundational Learning

- Concept: Sentence embeddings and cosine similarity
  - Why needed here: To measure semantic similarity between the test sentence and candidate examples.
  - Quick check question: How would you calculate the cosine similarity between two sentence embeddings of dimension 384?

- Concept: Entropy and label distribution
  - Why needed here: To encourage selection of examples with diverse label distributions, which may be more informative for the model.
  - Quick check question: If a sentence has labels [B-PER, I-PER, O, O, O], what is the entropy of this label distribution?

- Concept: Sigmoid function and smoothing
  - Why needed here: To create a smooth, tapered similarity curve for length matching, rather than a hard threshold.
  - Quick check question: What is the output of the sigmoid function when the input is 0? When the input is very large?

## Architecture Onboarding

- Component map:
  Sentence embedding model -> Similarity score calculator -> Label entropy calculator -> Complexity score calculator -> Example selector -> PLM

- Critical path: Embed test sentence → Calculate similarity scores with all training examples → Calculate label entropy for each example → Combine into complexity score → Select top-k examples → Format as few-shot prompt → Run inference

- Design tradeoffs:
  - Using a larger sentence embedding model may improve similarity measurement but increase computation time.
  - Tuning the weights for the complexity score components can significantly impact performance but requires a development set.
  - The method assumes the training data has sufficient diversity in complexity; if not, performance may suffer.

- Failure signatures:
  - If the method consistently underperforms random selection, the complexity metrics may be poorly correlated with example quality.
  - If performance is highly sensitive to the weight tuning, the method may be overfitting to the development set.
  - If the method shows large variance across different test samples, the training data may lack examples with the right complexity profiles.

- First 3 experiments:
  1. Run the method with k=1 on a small subset of the data and manually inspect the selected examples and their similarity to the test sample.
  2. Compare the performance of the method using only sentence similarity vs. only length similarity vs. the full complexity score.
  3. Test the sensitivity of the method to the weight tuning by running with several different weight combinations and plotting the performance curve.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored based on the limitations discussed.

## Limitations
- The method requires training data with sufficient diversity in complexity profiles, which may not hold for all domains
- Weight tuning for the complexity score components requires a development set, adding a hyperparameter optimization step
- Evaluation is limited to English sequence tagging tasks and specific PLM models, limiting generalizability

## Confidence
**High Confidence**: The core empirical claim that complexity-based prompt retrieval improves few-shot performance across multiple tasks and models. The experimental results are well-documented with clear baselines and significant improvements reported.

**Medium Confidence**: The claim that complexity alignment is the primary mechanism for improvement. While the results support this, alternative explanations (such as the method simply increasing label diversity) aren't fully ruled out.

**Medium Confidence**: The state-of-the-art claim for few-shot NER on CoNLL2003. The 5% absolute improvement is substantial, but the comparison is limited to a few recent methods and doesn't include all competitive approaches.

## Next Checks
1. **Ablation Study**: Run the full pipeline with only sentence similarity, only length similarity, and only label entropy to quantify the contribution of each component to the final performance.

2. **Training Data Diversity Analysis**: Systematically evaluate the method's performance as a function of training data diversity by creating datasets with varying levels of complexity diversity.

3. **Cross-Domain Generalization**: Test the method on sequence tagging tasks in different domains (e.g., medical text, legal documents) or different languages to assess how well the complexity metrics generalize.