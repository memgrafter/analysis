---
ver: rpa2
title: Inference Time Alignment with Reward-Guided Tree Search
arxiv_id: '2406.15193'
source_url: https://arxiv.org/abs/2406.15193
tags:
- instruction
- replacement
- mutation
- reward
- darwin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DARWIN, an inference-time alignment method
  that uses a reward-guided tree search to generate more aligned responses from LLMs.
  The method combines instruction mutation for exploration and reward-guided beam
  replacement for exploitation, outperforming other inference-time alignment methods
  like ARGS on benchmarks AlpacaEval 2 and MT-Bench.
---

# Inference Time Alignment with Reward-Guided Tree Search

## Quick Facts
- arXiv ID: 2406.15193
- Source URL: https://arxiv.org/abs/2406.15193
- Authors: Chia-Yu Hung; Navonil Majumder; Ambuj Mehrish; Soujanya Poria
- Reference count: 22
- Primary result: DARWIN outperforms ARGS on AlpacaEval 2 and MT-Bench using reward-guided tree search

## Executive Summary
This paper introduces DARWIN, an inference-time alignment method that leverages reward-guided tree search to improve the alignment of LLM responses. The approach combines instruction mutation for exploration and reward-guided beam replacement for exploitation, effectively balancing exploration and exploitation during inference. DARWIN demonstrates performance comparable to preference-tuned models while achieving better results than other inference-time alignment methods on established benchmarks.

## Method Summary
DARWIN frames alignment as a tree search problem where each node represents a partially decoded token sequence. The method maintains an archive of instructions and iteratively samples from this archive to generate mutated instructions. For each mutated instruction, multiple beams are decoded, and their rewards are evaluated by a reward model. The top-k rewarded states undergo beam replacement, where low-value states are replaced with high-value ones. This process repeats for multiple mutation cycles, with the archive being updated based on improved rewards. The final output is the highest-reward sequence from all mutation cycles.

## Key Results
- DARWIN outperforms ARGS on AlpacaEval 2 and MT-Bench benchmarks
- Performance comparable to preference-tuned models despite being an inference-time method
- Effective at trading inference-time compute for enhanced alignment performance
- Shows consistent improvement with multiple mutation cycles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward-guided tree search improves alignment by balancing exploration (instruction mutation) and exploitation (reward-guided beam replacement)
- Mechanism: The method frames alignment as a tree search where each node is a partially decoded token sequence. Exploration is achieved by mutating the original instruction into multiple variants, guiding independent search processes. Exploitation uses reward-guided beam replacement to replace low-value states with high-value ones, effectively pruning the search space toward better-aligned responses
- Core assumption: High-value states discovered during search are probabilistically closer to other high-value states, making replacement-based exploitation effective
- Evidence anchors: Empirical evidence shows DARWIN outperforms other inference-time alignment methods on AlpacaEval 2 and MT-Bench

### Mechanism 2
- Claim: Iterative instruction mutation accumulates diverse search paths that improve final alignment quality
- Mechanism: DARWIN maintains an archive of instructions. In each cycle, a candidate instruction is sampled, mutated into multiple variants, and each variant guides a separate decoding run. The top-k rewarded states from all runs are used to update the archive, ensuring instructions that consistently yield high rewards persist
- Core assumption: Mutated instructions that deviate significantly from the original will yield low reward scores, allowing the system to avoid off-topic exploration
- Evidence anchors: The key assumption is that search guided by significantly mutated instructions will reach low-value states

### Mechanism 3
- Claim: Multiple beams per mutation improve coverage and robustness of the search
- Mechanism: DARWIN can generate multiple beams per mutated instruction (controlled by parameter nb). All beams are combined before applying reward-guided replacement, increasing the diversity of candidate responses explored per mutation cycle
- Core assumption: Generating more beams per mutation increases the chance of discovering high-reward states without significantly increasing computational cost per cycle
- Evidence anchors: When employing Mutation as an exploration technique, general improvement in WR performance is observed

## Foundational Learning

- Concept: Reward models as alignment proxies
  - Why needed here: DARWIN relies on a reward model to evaluate how aligned a partially generated response is with the original instruction. Understanding how reward models are trained and what they capture is essential to predict their effectiveness and limitations
  - Quick check question: What is the difference between an outcome reward model and a process reward model, and which does DARWIN use?

- Concept: Tree search algorithms and exploration-exploitation trade-offs
  - Why needed here: DARWIN explicitly frames alignment as a tree search problem. Knowledge of how exploration (trying new paths) and exploitation (refining known good paths) balance is critical to tuning hyperparameters like replacement period and number of mutations
  - Quick check question: In a fixed-width tree search, how does increasing the replacement period affect the balance between exploration and exploitation?

- Concept: Instruction mutation and prompt engineering
  - Why needed here: The mutation step uses the LLM itself to rephrase or elaborate the instruction. Understanding prompt engineering techniques helps in designing effective mutation strategies that stay semantically close to the original instruction
  - Quick check question: How can you design a mutation prompt that encourages elaboration without changing the core intent of the instruction?

## Architecture Onboarding

- Component map:
  - Input instruction → Archive of instructions (initially contains seed)
  - Archive → Sampled candidate instruction → Mutator (LLM) → n mutated instructions
  - Each mutated instruction → Decoder (LLM) → Generated response sequence
  - Generated sequences → Reward model evaluator → Reward scores
  - Reward scores → Top-k selection → Beam replacement operation
  - Top-k beams → Archive update (if reward improves)
  - Final output: Highest-reward sequence from all mutation cycles

- Critical path:
  1. Archive sampling → Mutation generation
  2. Multi-beam decoding per mutation
  3. Reward evaluation per beam
  4. Replacement and top-k selection
  5. Archive update
  6. Repeat for N mutation cycles

- Design tradeoffs:
  - Exploration depth (number of mutation cycles) vs. computational cost
  - Replacement period length (m) vs. alignment quality (longer m favors exploitation)
  - Number of mutations per cycle (n) vs. diversity of search
  - Number of beams per mutation (nb) vs. robustness vs. cost

- Failure signatures:
  - Poor alignment: Archive fills with irrelevant instructions; reward scores are noisy or inconsistent
  - High variance in outputs: Insufficient replacement period; too few beams per mutation
  - Slow convergence: Excessive mutation cycles with low reward improvement
  - Overfitting to reward model: Overly aggressive replacement leading to repetitive outputs

- First 3 experiments:
  1. Baseline: Run Sample N & Best-of-N with N=5 and N=15; record AlpacaEval2 LC and WR
  2. Single mutation cycle: Run DARWIN with #MC=1, m=40, nb=1; compare to baseline
  3. Multiple mutation cycles: Run DARWIN with #MC=2, m=40, nb=1; measure improvement in WR and LC

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DARWIN vary with different sizes of language models (e.g., Llama-3-70B-Instruct vs. Llama-3-8B-Instruct)?
- Basis in paper: Inferred
- Why unresolved: The paper only evaluates DARWIN on smaller-scale models (8B and 7B parameters) due to computational limitations, leaving the performance on larger models untested
- What evidence would resolve it: Conducting experiments with larger models and comparing their performance to smaller models would provide insights into the scalability and effectiveness of DARWIN

### Open Question 2
- Question: What is the impact of using a weaker reward model (e.g., Gemma-2B) on the performance of DARWIN compared to a stronger reward model (e.g., Llama3-8B)?
- Basis in paper: Explicit
- Why unresolved: The paper shows that DARWIN requires a strong reward model to outperform baselines, but the specific performance differences with weaker reward models are not fully explored
- What evidence would resolve it: Running DARWIN with both strong and weak reward models and comparing the results would clarify the dependency on reward model quality

### Open Question 3
- Question: How does the choice of the replacement period (m) affect the balance between exploration and exploitation in DARWIN?
- Basis in paper: Inferred
- Why unresolved: The paper suggests that tuning the replacement period is important, but the optimal balance between exploration and exploitation is not clearly defined
- What evidence would resolve it: Conducting experiments with varying replacement periods and analyzing their impact on exploration and exploitation would provide insights into finding the optimal balance

## Limitations
- Computational cost increases significantly with multiple mutation cycles and beams per mutation
- Performance heavily depends on the quality of the reward model, which is not fully specified
- Limited evaluation to specific benchmarks (AlpacaEval 2 and MT-Bench) may not generalize to all alignment tasks

## Confidence

- High: The method combines instruction mutation for exploration and reward-guided beam replacement for exploitation
- Medium: DARWIN outperforms other inference-time alignment methods like ARGS on benchmarks AlpacaEval 2 and MT-Bench
- Low: The method achieves performance comparable to preference-tuned models

## Next Checks

1. Implement a simple version of DARWIN with a publicly available reward model (e.g., from Hugging Face) and test it on a small subset of AlpacaEval 2 to verify the basic mechanics of instruction mutation and reward-guided replacement

2. Conduct an ablation study by disabling the mutation step or the replacement step individually to measure their individual contributions to the overall performance

3. Test DARWIN on a different alignment benchmark (e.g., a subset of HumanEval or BBH) to assess generalizability beyond the evaluated tasks