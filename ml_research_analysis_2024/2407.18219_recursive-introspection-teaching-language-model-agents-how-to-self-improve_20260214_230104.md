---
ver: rpa2
title: 'Recursive Introspection: Teaching Language Model Agents How to Self-Improve'
arxiv_id: '2407.18219'
source_url: https://arxiv.org/abs/2407.18219
tags:
- rise
- performance
- arxiv
- step
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RISE fine-tunes LLMs to recursively improve their responses across
  multiple turns. It converts single-turn problems into multi-turn Markov decision
  processes and uses on-policy data collection with reward-weighted regression to
  train the model to self-correct.
---

# Recursive Introspection: Teaching Language Model Agents How to Self-Improve

## Quick Facts
- **arXiv ID**: 2407.18219
- **Source URL**: https://arxiv.org/abs/2407.18219
- **Reference count**: 40
- **Primary result**: RISE enables Llama2-7B, Llama3-8B, and Mistral-7B to improve themselves over turns on math reasoning tasks, achieving 17.7-23.9% gains on GSM8K and 4.6-11.1% on MATH over five turns.

## Executive Summary
RISE introduces a novel approach to teaching language models to recursively improve their own responses through iterative fine-tuning. The method converts single-turn problems into multi-turn Markov decision processes, enabling models to learn self-correction strategies. By collecting on-policy rollouts and using reward-weighted regression, RISE trains models to refine their responses across multiple turns, achieving significant performance gains on mathematical reasoning tasks without disrupting one-turn abilities.

## Method Summary
RISE transforms single-turn language modeling into a multi-turn Markov decision process where states include prompts, attempt histories, and optional feedback. The model generates responses (actions) that transition between states, receiving sparse rewards for correct answers. On-policy rollouts generate training data, which is relabeled with improved responses via best-of-N sampling or oracle distillation. Reward-weighted regression with advantage centering trains the model to improve responses over turns, iterating until performance plateaus. The approach scales well with model size and generalizes to out-of-distribution prompts.

## Key Results
- RISE achieves 17.7% improvement for Llama2-7B and 23.9% for Mistral-7B on GSM8K over five turns
- On MATH, RISE delivers 4.6% improvement for Llama2-7B and 11.1% for Mistral-7B
- The method preserves single-turn performance while enabling multi-turn self-improvement
- RISE generalizes to out-of-distribution prompts and scales effectively with model size

## Why This Works (Mechanism)

### Mechanism 1: Multi-turn MDP Conversion
RISE enables self-improvement by converting single-turn problems into multi-turn MDPs where the model learns to correct its own errors through iterative fine-tuning. The approach constructs an induced MDP from single-turn data where states include the prompt, history of attempts, and optional feedback. The model generates actions (responses) that transition to new states, receiving sparse rewards for correct answers. On-policy rollouts generate trajectories, and reward-weighted regression trains the model to improve responses over turns. This mechanism assumes the model contains latent knowledge needed to solve problems but requires iterative refinement to access and apply it correctly.

### Mechanism 2: Sequential Conditional Modeling
RISE works because sequential conditioning on intermediate responses provides more flexible capacity to model complex distributions than single-turn prediction. Instead of directly modeling p(ð‘¦*|ð‘¥), RISE learns a sequence of conditionals ðœ‹ðœƒ(ð‘¦ð‘–+1|ð‘¥, ð‘¦0:ð‘–) that when marginalized produce a more expressive distribution. This is analogous to diffusion models vs VAEs in image generation. The mechanism assumes the target distribution requires more capacity than the model affords by conditioning only on input prompt tokens.

### Mechanism 3: On-policy Learning with Advantage Weighting
RISE's on-policy data collection with reward-weighted regression enables learning from both successful and unsuccessful rollouts, avoiding the "rich-get-richer" phenomenon. RISE collects on-policy rollouts from the current model, then relabels subsequent steps with improved responses obtained via best-of-N sampling or oracle queries. Reward-weighted regression with advantage-centering trains on all data, weighting updates by exponential reward transformations. This mechanism assumes training on on-policy data that reflects the learner's own error distribution is more effective than training on expert data alone.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - Why needed here: RISE converts single-turn problems into multi-turn MDPs to enable sequential decision-making and self-correction
  - Quick check question: What are the four components of an MDP (states, actions, transition function, reward function) and how does RISE define each for math reasoning tasks?

- **Concept: On-policy vs Off-policy Learning**
  - Why needed here: RISE uses on-policy rollouts to collect data that reflects the learner's actual error distribution, which is crucial for effective self-correction
  - Quick check question: Why does training on on-policy data generated by the learner's own distribution outperform training on expert data alone for self-correction tasks?

- **Concept: Reward-weighted Regression and Advantage Weighting**
  - Why needed here: RISE uses reward-weighted regression with advantage centering to train on both successful and unsuccessful rollouts without bias toward easy problems
  - Quick check question: How does centering exponentiated rewards around the mean (advantage weighting) prevent the "rich-get-richer" phenomenon in RISE's training?

## Architecture Onboarding

- **Component map**: Data collection module -> Training module -> Inference module -> Environment interface
- **Critical path**: 1. Generate on-policy rollouts from current model, 2. Relabel with improved responses (best-of-N or oracle), 3. Train using reward-weighted regression, 4. Iterate until performance plateaus
- **Design tradeoffs**: Oracle vs self-distillation (better supervision vs computational cost), early termination vs full rollouts (efficiency vs generality), number of turns (refinement vs error propagation)
- **Failure signatures**: Performance degrades over turns (model not learning), single-turn performance drops (base capabilities disrupted), no improvement over iterations (poor on-policy distribution), out-of-distribution failure (overfitting to training prompts)
- **First 3 experiments**: 1. Verify MDP construction on small dataset, 2. Test reward-weighted regression on synthetic data, 3. Run RISE with oracle supervision on GSM8K

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RISE's performance compare to more recent approaches like direct preference optimization (DPO) or proximal policy optimization (PPO) for fine-tuning language models on mathematical reasoning tasks?
- Basis in paper: The paper discusses RISE's performance compared to various baselines including Self-Refine and GLoRE, but does not compare against DPO or PPO which have gained popularity since the paper's publication
- Why unresolved: The paper predates the widespread adoption of DPO and PPO for LLM fine-tuning, and did not include these methods in their experimental comparisons
- What evidence would resolve it: Running RISE against DPO and PPO on the same datasets (GSM8K, MATH) with comparable model sizes would provide a direct comparison of their effectiveness for inducing self-improvement capabilities in LLMs

### Open Question 2
- Question: What is the optimal number of turns for RISE to maximize performance without causing degradation in earlier turns or excessive computational cost?
- Basis in paper: The paper evaluates RISE up to 5 turns but notes that the optimal number of turns was not determined and that more turns could potentially lead to degradation
- Why unresolved: The paper only reports results for 5 turns, leaving open the question of whether more or fewer turns would be optimal for different tasks or model sizes
- What evidence would resolve it: Conducting experiments varying the number of turns (e.g., 1, 3, 5, 7, 10) and analyzing the trade-off between performance gains and computational cost would identify the optimal turn count for different scenarios

### Open Question 3
- Question: How does RISE's self-improvement capability generalize to other domains beyond mathematical reasoning, such as code generation, text summarization, or question answering?
- Basis in paper: The paper demonstrates RISE's effectiveness on math reasoning tasks (GSM8K, MATH) and mentions potential generalization to other domains, but does not provide empirical evidence for other task types
- Why unresolved: The experiments are limited to mathematical reasoning tasks, and it's unclear whether the self-improvement mechanism would be equally effective for tasks with different characteristics (e.g., open-ended generation vs. structured problem-solving)
- What evidence would resolve it: Applying RISE to other task domains like code generation (e.g., HumanEval), summarization (e.g., CNN/DailyMail), or question answering (e.g., SQuAD) and measuring performance improvements over turns would demonstrate its broader applicability

## Limitations

- Limited evidence of generalization beyond mathematical reasoning tasks to other domains like code generation or text summarization
- High computational cost of multiple fine-tuning iterations with on-policy data collection
- Cannot create knowledge where none exists - only refines existing capabilities

## Confidence

- **High confidence**: The core mechanism of converting single-turn problems to multi-turn MDPs and using reward-weighted regression is well-specified and theoretically sound
- **Medium confidence**: The claim about RISE's ability to generalize beyond math reasoning tasks has limited empirical support
- **Low confidence**: The paper's assertion that RISE avoids the "rich-get-richer" phenomenon through advantage weighting is supported by ablation studies but lacks theoretical justification

## Next Checks

1. **Cross-domain generalization test**: Evaluate RISE on a diverse set of tasks beyond math reasoning, including text summarization, code generation, and question answering, to assess whether the self-improvement mechanism transfers across domains

2. **Knowledge gap analysis**: Systematically test RISE on problems where the base model has limited capability, measuring whether iterative refinement can compensate for missing fundamental knowledge versus cases where the knowledge gap is too large

3. **Cost-effectiveness benchmarking**: Compare RISE's performance improvements against alternative approaches (e.g., scaling model size, chain-of-thought prompting) while accounting for computational costs of multiple fine-tuning iterations and on-policy data collection