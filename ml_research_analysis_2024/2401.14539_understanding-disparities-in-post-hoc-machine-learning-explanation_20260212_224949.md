---
ver: rpa2
title: Understanding Disparities in Post Hoc Machine Learning Explanation
arxiv_id: '2401.14539'
source_url: https://arxiv.org/abs/2401.14539
tags:
- explanation
- disparities
- shift
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically examines how data and model properties
  affect explanation disparities in post-hoc machine learning explanations. The authors
  investigate four key data characteristics - sample size imbalance, covariate shift,
  concept shift, and omitted variable bias - and two model properties - inclusion
  of sensitive attributes and model complexity - across both synthetic and real-world
  datasets.
---

# Understanding Disparities in Post Hoc Machine Learning Explanation

## Quick Facts
- arXiv ID: 2401.14539
- Source URL: https://arxiv.org/abs/2401.14539
- Reference count: 40
- One-line primary result: Explanation disparities in post-hoc ML explanations are most pronounced under concept shift and covariate shift, particularly for neural network models

## Executive Summary
This study systematically examines how data and model properties affect explanation disparities in post-hoc machine learning explanations. The authors investigate four key data characteristics (sample size imbalance, covariate shift, concept shift, and omitted variable bias) and two model properties (inclusion of sensitive attributes and model complexity) across both synthetic and real-world datasets. Using LIME explanations on logistic regression and neural network models, they find that explanation disparities are most pronounced under conditions of concept shift (up to 27.63% mean fidelity gap) and covariate shift (up to 4.52% mean fidelity gap), particularly for neural network models. The results show that including sensitive attributes in model training can either increase or decrease disparities depending on the data-generating process, while omitting relevant variables consistently increases disparities, especially for complex models.

## Method Summary
The study uses a synthetic data generation approach to systematically vary four data properties (sample size imbalance, covariate shift, concept shift, and omitted variable bias) and two model properties (inclusion of sensitive attributes and model complexity). The authors train both logistic regression and neural network models with/without sensitive attributes on these datasets, then apply LIME to generate explanations. They compute fidelity metrics comparing model predictions to explanations, specifically Maximum Fidelity Gap (ŒîAcc) and Mean Fidelity Gap (Œîgroup_Acc) between advantaged and disadvantaged groups. The experimental framework is validated on both synthetic data and the Adult income dataset.

## Key Results
- Explanation disparities are most pronounced under concept shift conditions (up to 27.63% mean fidelity gap) and covariate shift (up to 4.52% mean fidelity gap), particularly for neural network models
- Including sensitive attributes in model training can either increase or decrease disparities depending on whether the inclusion aligns with the true causal structure
- Omitting relevant variables with direct effects on the outcome consistently increases disparities, especially for complex models like neural networks
- Neural networks show larger fidelity gaps than linear models under concept shift conditions due to their ability to capture non-linear relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanation disparities increase when neural networks encounter concept shift while linear models show relatively stable fidelity.
- Mechanism: When concept shift exists (P(Y|L,A=0) ‚â† P(Y|L,A=1)), the relationship between features and outcome varies by group. Neural networks can capture this non-linear relationship but require accurate training data distribution to learn it. If the training data doesn't reflect the true distribution shift, the network's learned function diverges from the actual decision boundary, causing larger fidelity gaps. Linear models, being unable to capture non-linear relationships, maintain consistent (though potentially inaccurate) explanations across groups.
- Core assumption: The neural network can approximate the true underlying function if given sufficient training data, but concept shift creates a mismatch between training and test distributions that disproportionately affects complex models.
- Evidence anchors:
  - [abstract] "explanation disparities are most pronounced under conditions of concept shift (up to 27.63% mean fidelity gap) and covariate shift (up to 4.52% mean fidelity gap), particularly for neural network models"
  - [section] "as concept shift increases, Œîgroup_Acc and ŒîAcc increase for NNÃ∏A considerably in comparison to NNùê¥, LRÃ∏A, and LRùê¥"
  - [corpus] Weak - no direct corpus evidence about concept shift and neural network fidelity relationships
- Break condition: If the training data distribution perfectly matches the test distribution or if the concept shift is linear rather than non-linear, neural networks may not show disproportionate fidelity gaps.

### Mechanism 2
- Claim: Omitting variables with direct effects on the outcome increases explanation disparities, especially for complex models.
- Mechanism: When a variable C has a direct effect on Y that is not mediated by other variables, excluding it from model training means the model cannot learn this direct relationship. For complex models like neural networks that can potentially capture complex interactions, this omission creates a larger gap between the true decision boundary and the learned model. The explanation method (LIME) attempts to approximate this imperfect model, resulting in higher fidelity gaps. Simpler linear models show smaller increases because they cannot capture the complex direct effects anyway.
- Core assumption: The omitted variable has a significant direct effect on the outcome that cannot be adequately captured through other variables in the model.
- Evidence anchors:
  - [abstract] "omitting relevant variables consistently increases disparities, especially for complex models"
  - [section] "Œîgroup_Acc for NNÃ∏ùê∂ increases as the direct effect of C on Y increases. This characteristic is also observed across ŒîAcc"
  - [corpus] Weak - no direct corpus evidence about omitted variable effects on explanation fidelity
- Break condition: If the omitted variable's effect is fully mediated through other variables or if the direct effect is negligible, excluding it may not significantly impact explanation disparities.

### Mechanism 3
- Claim: Including sensitive attributes in model training can either increase or decrease disparities depending on the data-generating process.
- Mechanism: When the sensitive attribute A has a legitimate causal relationship with the outcome Y (Y ‚´´ A|L,C), including it in model training allows the model to learn this relationship accurately, potentially reducing disparities. However, when A is not causally related to Y given other variables (Y ‚ä• A|L,C), including it introduces spurious correlations that the model learns, leading to higher disparities. The effect depends on whether including A aligns with the true causal structure.
- Core assumption: The causal relationship between the sensitive attribute and outcome determines whether including it improves or worsens model performance and explanation fidelity.
- Evidence anchors:
  - [abstract] "including sensitive attributes in model training can either increase or decrease disparities depending on the data-generating process"
  - [section] "if the inclusion of the sensitive attribute (A) in training the black box model aligns with the causal structure (Y ‚´´ A|C,L) then A needs to be included in model training, and explanation disparities are smaller"
  - [corpus] Weak - no direct corpus evidence about sensitive attribute inclusion effects on explanation fidelity
- Break condition: If the causal relationship between sensitive attribute and outcome is ambiguous or if the data-generating process changes across subgroups, the effect of including sensitive attributes may be unpredictable.

## Foundational Learning

- Concept: Causal directed acyclic graphs (DAGs) and their use in understanding fairness in machine learning
  - Why needed here: The paper uses DAGs to model the data-generating process and understand how different variables affect outcomes and explanations. Understanding these relationships is crucial for interpreting why certain model properties affect explanation disparities.
  - Quick check question: In a DAG where A ‚Üí L ‚Üí Y and A ‚Üí Y, what does it mean if we observe that including A in model training increases disparities?

- Concept: Post-hoc explanation methods and their fidelity metrics
  - Why needed here: The paper focuses on LIME as a post-hoc explanation method and evaluates its fidelity using metrics like maximum fidelity gap and mean fidelity gap. Understanding how these methods work and what fidelity means is essential for grasping the paper's contributions.
  - Quick check question: If a post-hoc explanation method has high fidelity, what does this tell us about the relationship between the explanation and the black box model?

- Concept: Covariate shift, concept shift, and omitted variable bias as sources of algorithmic unfairness
  - Why needed here: The paper systematically examines how these three data properties affect explanation disparities. Understanding these concepts is crucial for following the experimental design and interpreting the results.
  - Quick check question: How does covariate shift differ from concept shift in terms of their effects on model performance and explanation disparities?

## Architecture Onboarding

- Component map:
  - Data generation module: Creates synthetic datasets with controlled properties (sample size, covariate shift, concept shift, omitted variables)
  - Model training pipeline: Trains both logistic regression and neural network models with/without sensitive attributes
  - LIME explanation module: Generates local explanations for individual predictions
  - Evaluation framework: Computes fidelity metrics (maximum fidelity gap, mean fidelity gap) across groups
  - Analysis dashboard: Visualizes disparities across different experimental conditions

- Critical path:
  1. Generate synthetic/real data with specified properties
  2. Train black box models (LR/NN) with/without sensitive attributes
  3. Apply LIME to generate explanations
  4. Compute fidelity metrics comparing model predictions to explanations
  5. Analyze disparities across groups and experimental conditions

- Design tradeoffs:
  - Synthetic vs. real data: Synthetic data allows controlled experiments but may miss real-world complexities; real data captures actual patterns but introduces confounding factors
  - Model complexity: Neural networks can capture complex relationships but are more sensitive to data distribution shifts; linear models are more stable but may miss important patterns
  - Explanation method choice: LIME is widely used but has limitations in perturbation processes and hyperparameter sensitivity

- Failure signatures:
  - High fidelity gaps across all conditions: May indicate issues with the explanation method implementation or evaluation metrics
  - Inconsistent results between synthetic and real data: Suggests missing factors in the synthetic data generation or unaccounted real-world complexities
  - Disparities only appearing for complex models: Could indicate that simpler models are masking important relationships or that the data distribution shifts are subtle

- First 3 experiments:
  1. Test the effect of sample size imbalance by varying the proportion of disadvantaged group in training data from 5% to 50%
  2. Introduce covariate shift by limiting the range of features available for the disadvantaged group in training data
  3. Create concept shift by varying the interaction strength between sensitive attribute and features in the data-generating process

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results of explanation disparities change when using different post-hoc explanation methods (e.g., SHAP) compared to LIME, and do these methods exhibit similar dependencies on data and model properties?
- Basis in paper: [explicit] The authors state that while their study focuses on LIME, other explanation methods like SHAP may also exhibit disparity challenges that depend on the properties of the data and the black box model. They suggest that future work should extend the investigation to other explanation methods.
- Why unresolved: The study only examined LIME, leaving the behavior of other explanation methods unexplored.
- What evidence would resolve it: Conducting a systematic comparison of explanation disparities across multiple post-hoc explanation methods (e.g., LIME, SHAP, counterfactuals) under varying data and model properties.

### Open Question 2
- Question: How do explanation disparities change when considering more complex causal graphs with unmeasured confounding between the sensitive attribute and the outcome, and how would this affect the recommendations for mitigating disparities?
- Basis in paper: [explicit] The authors mention that their study considers simple causal graphs without unmeasured confounding between the sensitive attribute and the outcome. They suggest that further efforts to include unmeasured confounding can provide additional insights about the relevance of data properties.
- Why unresolved: The study only considered simple causal graphs, limiting the generalizability of the findings to real-world scenarios with potential unmeasured confounding.
- What evidence would resolve it: Conducting simulations and experiments with more complex causal graphs that include unmeasured confounding between the sensitive attribute and the outcome.

### Open Question 3
- Question: How do the results of explanation disparities change when considering different sensitive attributes (e.g., race, age) compared to gender, and do the findings hold across different sensitive attributes?
- Basis in paper: [explicit] The study focuses on gender as the sensitive attribute, but the authors acknowledge that other sensitive attributes like race or age could be considered. They suggest that future work should examine the impact of different sensitive attributes on explanation disparities.
- Why unresolved: The study only examined gender as the sensitive attribute, limiting the generalizability of the findings to other sensitive attributes.
- What evidence would resolve it: Conducting experiments with different sensitive attributes (e.g., race, age) and comparing the results to the findings for gender.

## Limitations
- The study relies heavily on synthetic data generation, which may not capture all complexities of real-world datasets
- The use of LIME as the sole explanation method limits generalizability to other post-hoc explanation approaches
- The focus on binary classification tasks restricts applicability to multi-class or regression problems

## Confidence
- **High confidence**: The observed effects of concept shift and omitted variable bias on explanation disparities are well-supported by the experimental results and mechanistic reasoning
- **Medium confidence**: The findings regarding covariate shift and the impact of including sensitive attributes in model training, while supported by results, may be more context-dependent
- **Low confidence**: The specific numerical thresholds for fidelity gaps (e.g., 27.63% for concept shift) may vary significantly with different datasets, model architectures, or explanation parameters

## Next Checks
1. **Replication with alternative explanation methods**: Validate the findings using SHAP or other post-hoc explanation methods to ensure the observed disparities are not specific to LIME's mechanics
2. **Real-world dataset validation**: Apply the experimental framework to diverse real-world datasets (e.g., healthcare, criminal justice) to test generalizability of the findings beyond synthetic data
3. **Sensitivity analysis of causal assumptions**: Systematically vary the assumed causal structure in the data-generating process to understand how robust the findings are to different underlying causal relationships