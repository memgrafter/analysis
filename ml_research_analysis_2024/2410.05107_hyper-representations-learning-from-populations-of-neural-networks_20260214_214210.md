---
ver: rpa2
title: 'Hyper-Representations: Learning from Populations of Neural Networks'
arxiv_id: '2410.05107'
source_url: https://arxiv.org/abs/2410.05107
tags:
- weights
- learning
- weight
- zoos
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis explores Neural Network weight spaces as a foundation
  for model analysis and generation. It proposes hyper-representations, a self-supervised
  method that learns meaningful structures from populations of trained models, encoding
  model properties like accuracy and hyperparameters.
---

# Hyper-Representations: Learning from Populations of Neural Networks

## Quick Facts
- **arXiv ID:** 2410.05107
- **Source URL:** https://arxiv.org/abs/2410.05107
- **Reference count:** 0
- **Primary result:** Self-supervised hyper-representations learn meaningful structures from populations of neural networks, enabling prediction of model properties and generation of new models for fine-tuning and transfer learning.

## Executive Summary
This thesis introduces hyper-representations as a self-supervised method for learning meaningful structures from populations of trained neural network models. The approach learns lower-dimensional representations that encode model properties like accuracy, hyperparameters, and training state by processing neural network weights through a transformer-based encoder-decoder architecture with reconstruction and contrastive losses. The work addresses challenges of high dimensionality, architectural incompatibility, and weight space symmetries through techniques like layer-wise loss normalization and sequential decomposition (SANE). Experiments demonstrate that hyper-representations can predict model characteristics, generalize to unseen architectures and tasks, and generate competitive models for fine-tuning and transfer learning, scaling to large models like ResNet-18.

## Method Summary
Hyper-representations are learned using an autoencoder architecture with transformer-based encoder and decoder that processes tokenized neural network weights. The model is trained with reconstruction loss (MSE) to compress and reconstruct weight sequences, combined with contrastive guidance loss (NT_Xent) to preserve meaningful structure. Weight preprocessing includes standardization per layer, alignment to a reference model, and tokenization into manageable subsequences. Layer-wise loss normalization balances reconstruction error across layers with uneven weight distributions. Data augmentations like permutation, noise, and masking help the model handle weight space symmetries. The trained encoder produces embeddings that can predict model properties via linear probing or serve as initialization for sampling new models through methods like kernel density estimation, neighbor sampling, or GANs.

## Key Results
- Hyper-representations successfully predict model characteristics (accuracy, epoch, generalization gap) with R² scores of 0.8-0.9 using linear probing
- Generated models from hyper-representations achieve competitive performance on fine-tuning and transfer learning tasks compared to random initialization
- The approach generalizes to unseen architectures and tasks, demonstrated through experiments with CNNs, MLPs, and transfer between different datasets
- Sequential decomposition (SANE) enables scaling to larger models like ResNet-18 while maintaining embedding quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyper-representations learn task-agnostic, lower-dimensional representations of neural network weights that encode model properties like accuracy, hyperparameters, and training state.
- Mechanism: The self-supervised learning task minimizes reconstruction loss (MSE) and contrastive loss (NT_Xent), forcing the encoder to compress weight information into a bottleneck while maintaining structure via augmentation (permutation, noise, masking). This preserves information about generating factors (D, A, λ) that determine model behavior.
- Core assumption: Trained neural network models populate meaningful structures in weight space, and these structures encode latent information about the models' generating factors and properties.
- Evidence anchors:
  - [abstract]: "trained NN models indeed occupy meaningful structures in the weight space, that can be learned and used."
  - [section]: "Through extensive experiments, this thesis demonstrates that hyper-representations uncover model properties, such as their performance, state of training, or hyperparameters."
  - [corpus]: Weak - corpus neighbors are on different topics, no direct evidence for weight space structure encoding.
- Break condition: If weight space symmetries (permutations, sign changes, continuous equivariances) are not properly handled, the learned representations will not be meaningful or stable.

### Mechanism 2
- Claim: Layer-wise loss normalization is critical for generating functional models from hyper-representations.
- Mechanism: Without normalization, layers with narrow weight distributions become weak links in reconstructed models, causing performance to drop to random guessing. Normalization balances reconstruction error across layers, ensuring all layers are accurately reconstructed and information flows through the model.
- Core assumption: Weight magnitudes are unevenly distributed across different layers in many model zoos, leading to uneven reconstruction if loss is not normalized.
- Evidence anchors:
  - [abstract]: "introduces methods that allow hyper-representations to generalize beyond model sizes, architectures, and tasks."
  - [section]: "We introduce layer-wise loss normalization (LWLN), which we motivate and detail in the following."
  - [corpus]: Missing - no corpus evidence for layer-wise loss normalization specifically.
- Break condition: If weight distributions are already balanced across layers (e.g., through careful initialization), layer-wise normalization may provide minimal benefit.

### Mechanism 3
- Claim: Sequential decomposition of weight vectors into token sequences enables scaling hyper-representation learning to larger models and varying architectures.
- Mechanism: Instead of processing the entire flattened weight vector at once, SANE tokenizes weights into manageable subsequences (windows) and processes them sequentially. This decouples memory requirements from model size and allows embedding models with varying architectures as long as token size is consistent.
- Core assumption: Global model information is preserved in the layer-wise components of neural networks, allowing meaningful embeddings from subsequences.
- Evidence anchors:
  - [abstract]: "it presents methods that allow hyper-representations to generalize beyond model sizes, architectures, and tasks."
  - [section]: "SANEaddresses these limitations by decomposing the entire weight vectorwi into layers or smaller subsets, and then sequentially processes them."
  - [corpus]: Weak - corpus neighbors are on different topics, no direct evidence for sequential decomposition approach.
- Break condition: If global information is not preserved in layer-wise components or if subsequences lack sufficient context, embeddings will lose important information.

## Foundational Learning

- Concept: Neural Network Weight Spaces
  - Why needed here: Understanding the domain is crucial for grasping the challenges of operating in weight space and the need for robust representation learning methods.
  - Quick check question: What are the main challenges in neural network weight spaces that make direct operations problematic?

- Concept: Self-Supervised Learning
  - Why needed here: Hyper-representations are learned using self-supervised learning techniques (reconstruction and contrastive losses) without requiring labeled data about model properties.
  - Quick check question: How do reconstruction and contrastive losses work together to learn meaningful representations of neural network weights?

- Concept: Transformer Architectures
  - Why needed here: The encoder and decoder in hyper-representations are based on transformer architectures with multi-head self-attention, which are used to process the weight sequences.
  - Quick check question: How does the transformer architecture handle the permutation symmetries inherent in neural network weight spaces?

## Architecture Onboarding

- Component map:
  Model zoos -> Weight preprocessing (standardize, align, tokenize) -> Hyper-representation model (encoder-decoder transformer) -> Sampling methods (KDE, neighbor sampling, GANs) -> Downstream tasks (property prediction, model generation)

- Critical path:
  1. Generate or obtain diverse model zoos
  2. Preprocess weights (standardize, align, tokenize)
  3. Train hyper-representation model on weight sequences
  4. Compute embeddings for analysis or generate new models via sampling
  5. Evaluate on downstream tasks

- Design tradeoffs:
  - Window size vs. context: Larger windows capture more context but increase memory requirements
  - Tokenization granularity: Tokenizing per weight vs. per neuron/kernel affects computational load and information preservation
  - Augmentation strength: Too much augmentation may destroy useful information, too little may lead to overfitting

- Failure signatures:
  - Poor reconstruction quality: Indicates issues with the autoencoder architecture or training process
  - Unstable embeddings: Suggests problems with handling weight space symmetries or insufficient regularization
  - Degraded performance in downstream tasks: May indicate loss of important information during preprocessing or representation learning

- First 3 experiments:
  1. Train hyper-representation on a small, homogeneous model zoo (e.g., MNIST-CNNs with varying seeds only)
  2. Evaluate embedding quality by predicting model accuracy from embeddings using linear probing
  3. Generate new models via sampling and evaluate their performance on fine-tuning tasks compared to random initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do hyper-representations perform on transfer learning tasks when the source and target tasks have very different data distributions (e.g., natural images vs. medical images)?
- Basis in paper: [explicit] The paper mentions that hyper-representations can generalize to new tasks and architectures, and shows some transfer learning experiments (e.g., SVHN to MNIST, STL-10 to CIFAR-10). However, it doesn't explore the case of highly dissimilar distributions.
- Why unresolved: The paper's transfer learning experiments focus on datasets within the same domain (image classification), but don't test the limits of hyper-representations' ability to bridge large domain gaps.
- What evidence would resolve it: Experiments comparing hyper-representations' performance on transfer learning tasks with varying degrees of domain shift, including tasks from entirely different domains like medical imaging or natural language processing.

### Open Question 2
- Question: Can hyper-representations be used to identify and mitigate biases in neural networks, such as gender or racial biases in facial recognition models?
- Basis in paper: [inferred] The paper discusses the potential of hyper-representations for model analysis, including identifying model properties and weaknesses. It also mentions the possibility of using hyper-representations for model governance and certification.
- Why unresolved: While the paper suggests the potential for model analysis, it doesn't specifically explore the use of hyper-representations for bias detection and mitigation.
- What evidence would resolve it: Experiments demonstrating the ability of hyper-representations to identify biased models and generate less biased models through targeted sampling or fine-tuning.

### Open Question 3
- Question: How do the computational costs of hyper-representations compare to traditional model analysis and generation methods, especially for large-scale models like GPT-3 or PaLM?
- Basis in paper: [explicit] The paper mentions the challenges of high dimensionality and computational expense associated with neural network weight spaces. It also discusses the scalability of SANE to larger models like ResNet-18.
- Why unresolved: While the paper acknowledges the computational challenges, it doesn't provide a detailed comparison of the computational costs of hyper-representations versus traditional methods for large-scale models.
- What evidence would resolve it: A comprehensive analysis of the computational costs (e.g., time, memory) of hyper-representations compared to traditional methods for various model sizes and tasks, including large-scale models like GPT-3 or PaLM.

## Limitations
- Limited external validation of core claims about weight space structure encoding model properties
- Sequential decomposition may lose global context when processing large weight vectors in subsequences
- Layer-wise loss normalization necessity depends on specific weight distributions in different model zoos

## Confidence
- High confidence: The feasibility of learning representations from neural network populations, basic autoencoder architecture, and most experimental results
- Medium confidence: Claims about generalizing across architectures and tasks, the effectiveness of layer-wise normalization for all scenarios, and the robustness of sequential decomposition
- Low confidence: The assertion that weight space structures are universally meaningful across all neural network types and training paradigms, and that the learned representations will scale seamlessly to extremely large models (e.g., >1B parameters)

## Next Checks
1. Test hyper-representation generalization by training on one architecture family (e.g., CNNs) and evaluating on a completely different family (e.g., Transformers) for both prediction and generation tasks
2. Validate the necessity of layer-wise loss normalization by comparing model generation performance with and without normalization across multiple model zoos with different weight distributions
3. Assess the impact of subsequence context by systematically varying window sizes in SANE and measuring the degradation in reconstruction quality and downstream task performance