---
ver: rpa2
title: 'Building a Taiwanese Mandarin Spoken Language Model: A First Attempt'
arxiv_id: '2411.07111'
source_url: https://arxiv.org/abs/2411.07111
tags:
- speech
- text
- user
- spoken
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report presents an initial attempt to build a spoken
  large language model (LLM) for Taiwanese Mandarin, aiming for real-time, speech-to-speech
  interaction in multi-turn conversations. The end-to-end model uses a decoder-only
  transformer architecture initialized from LLaMA-3.1 8B, enabling seamless interaction
  without architectural modifications.
---

# Building a Taiwanese Mandarin Spoken Language Model: A First Attempt

## Quick Facts
- arXiv ID: 2411.07111
- Source URL: https://arxiv.org/abs/2411.07111
- Reference count: 19
- Primary result: Initial Taiwanese Mandarin spoken LLM achieves acceptable intelligibility (CER 28.3-32.8%) and naturalness (MOS 3.2-3.5) in speech-to-speech conversations

## Executive Summary
This technical report presents an initial attempt to build a spoken large language model for Taiwanese Mandarin, aiming for real-time, speech-to-speech interaction in multi-turn conversations. The end-to-end model uses a decoder-only transformer architecture initialized from LLaMA-3.1 8B, enabling seamless interaction without architectural modifications. The system supports real-time turn-taking via probabilistic end-of-turn detection and processes speech through streaming ASR and HuBERT-based encoders, generating responses in hybrid text-speech units decoded via diffusion models.

## Method Summary
The method uses LLaMA-3.1 8B as backbone, fine-tuned with speech capabilities through a two-phase approach. First, pre-training on ASR/TTS tasks with 44k hours of data learns to align text tokens with speech units. Second, SFT on 100k+ hours of dialogue data with various input/output modalities trains the model for conversational tasks. The system processes speech through streaming Whisper ASR and HuBERT encoders, generating responses in hybrid text-speech units decoded via diffusion models. Data preparation involved synthesizing dialogues with Taiwanese Mandarin TTS and inserting interruptions for full-duplex training.

## Key Results
- Intelligibility measured by CER ranges from 28.3% to 32.8% across different input-output modality combinations
- Speech naturalness (MOS prediction) scores range from 3.2 to 3.5, indicating acceptable quality
- Contextual coherence in speech-to-speech settings achieves LLM Score of 3.4 out of 5
- The model can achieve satisfactory performance without ASR, offering reduced latency potential

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real-time turn-taking without architectural modifications using EOT token probability detection
- Mechanism: Monitors next-token probability distributions for EOT token exceeding threshold to trigger response generation and interruption detection
- Core assumption: EOT token probability reliably indicates conversation turn completion
- Evidence anchors: [abstract], [section 2.5], Weak corpus evidence
- Break condition: EOT token becomes unreliable due to domain shift or insufficient training data

### Mechanism 2
- Claim: Text LLM initialization preserves language understanding capabilities
- Mechanism: Pre-training aligns text tokens with speech units while maintaining text-based training data
- Core assumption: Text-LLM capabilities transfer to speech through alignment training
- Evidence anchors: [section 4.1.1], [section 3.2.1], Moderate corpus evidence
- Break condition: Catastrophic forgetting occurs despite alignment training

### Mechanism 3
- Claim: Synthesized dialogue data prevents catastrophic forgetting
- Mechanism: Text-based LLMs generate conversations converted to speech using TTS models
- Core assumption: Synthesized data simulates real dialogue while preserving text-LLM capabilities
- Evidence anchors: [section 3.2.1], [section 3.1], Weak corpus evidence
- Break condition: Synthesized dialogue fails to capture essential real-world conversational patterns

## Foundational Learning

- **Concept**: End-of-turn detection using probabilistic thresholds
  - Why needed here: Enables real-time turn-taking without explicit user signals
  - Quick check question: How would you implement EOT detection if probability threshold fails?

- **Concept**: Catastrophic forgetting in fine-tuning large language models
  - Why needed here: Explains why training only on dialogue data degrades text understanding
  - Quick check question: What would happen if you fine-tuned directly on 100k hours of dialogue?

- **Concept**: Modality control through system prompts
  - Why needed here: Allows flexible switching between text, speech, and hybrid outputs
  - Quick check question: How would you modify system prompts for video modality support?

## Architecture Onboarding

- **Component map**: User speech → Streaming ASR → HuBERT encoder → token stream → LLaMA-3.1 8B transformer → speech units → diffusion decoder → audio output
- **Critical path**: User speech → ASR → token stream → transformer → speech units → diffusion decoder → audio output
- **Design tradeoffs**: Hybrid vs pure speech units (alignment vs latency), streaming vs offline components (real-time vs quality), pre-training data composition (capability preservation vs domain adaptation)
- **Failure signatures**: High CER (>35%) indicates ASR/speech unit extraction failure, low MOS (<3.0) indicates diffusion decoder quality issues, poor LLM Score (<2.5) indicates system prompt parsing failure
- **First 3 experiments**: Test EOT detection reliability with controlled interruptions, evaluate catastrophic forgetting by comparing text comprehension before/after fine-tuning, measure streaming latency components to identify bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the end-to-end model achieve human-level conversational latency without architectural modifications?
- Basis: Current latency is 1900ms vs human-level under 500ms, with all components needing inherent streaming support
- Why unresolved: Streaming Whisper and decoder chunking identified as major contributors but no evidence of achieving <500ms
- What evidence would resolve it: Demonstration of <500ms latency in conversational tasks while maintaining metrics

### Open Question 2
- Question: Does synthetic dialogue training impact handling of unexpected conversational dynamics?
- Basis: Switched from real-world to synthetic data due to catastrophic forgetting, but may limit handling of natural conversation dynamics
- Why unresolved: Evaluation uses pre-designed dialogues without testing spontaneous conversational abilities
- What evidence would resolve it: Comparative evaluation showing performance differences between synthetic vs real dialogue training on unscripted conversations

### Open Question 3
- Question: Can the model maintain performance when switching between input modalities during ongoing conversations?
- Basis: Model trained on various input/output modality combinations but evaluation only tests static settings
- Why unresolved: Evaluates fixed modality settings but doesn't test dynamic switching during conversation flow
- What evidence would resolve it: Testing showing consistent performance when users switch input modalities mid-conversation

## Limitations

- Evaluation lacks comprehensive testing of real-world conversational dynamics, speaker adaptation across diverse accents, and robustness to noisy environments
- Reliance on synthesized dialogue data may not fully capture complexity and variability of natural human conversation
- EOT token reliability is uncertain given potential for domain shift and code-switching in Taiwanese Mandarin contexts

## Confidence

- EOT detection reliability: **Medium** - Limited validation across diverse conversational scenarios
- Catastrophic forgetting prevention: **Medium** - Synthetic data approach shows promise but effectiveness varies
- Human-level latency achievement: **Low** - Current latency measurements acknowledge further optimizations needed

## Next Checks

1. **EOT Detection Reliability Testing**: Conduct controlled experiments with varied speech patterns, interruptions, and overlapping dialogue to measure EOT detection accuracy across diverse conversational scenarios. Compare against ground truth turn boundaries to establish reliability thresholds.

2. **Catastrophic Forgetting Validation**: Systematically evaluate the model's text comprehension capabilities on standard benchmarks before and after speech fine-tuning, measuring performance degradation across multiple domains to quantify effectiveness of synthetic dialogue approach.

3. **End-to-End Latency Profiling**: Measure and characterize latency contributions of each system component under realistic load conditions to identify bottlenecks and verify claim that ASR can be eliminated without sacrificing real-time performance.