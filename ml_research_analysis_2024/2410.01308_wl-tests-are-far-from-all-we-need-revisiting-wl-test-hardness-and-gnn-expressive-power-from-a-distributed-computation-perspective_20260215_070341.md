---
ver: rpa2
title: 'WL Tests Are Far from All We Need: Revisiting WL-Test Hardness and GNN Expressive
  Power from a Distributed Computation Perspective'
arxiv_id: '2410.01308'
source_url: https://arxiv.org/abs/2410.01308
tags:
- node
- graph
- nodes
- each
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of understanding graph neural
  network (GNN) expressive power through the lens of computational complexity. The
  authors identify three key issues in existing analyses: underestimated preprocessing
  time complexity, mismatch between anonymous WL tests and non-anonymous features,
  and unrealistic assumptions in the CONGEST model.'
---

# WL Tests Are Far from All We Need: Revisiting WL-Test Hardness and GNN Expressive Power from a Distributed Computation Perspective

## Quick Facts
- arXiv ID: 2410.01308
- Source URL: https://arxiv.org/abs/2410.01308
- Reference count: 40
- This paper addresses the challenge of understanding graph neural network (GNN) expressive power through the lens of computational complexity.

## Executive Summary
This paper addresses fundamental limitations in analyzing GNN expressive power through the Weisfeiler-Lehman (WL) test. The authors identify three key issues in existing analyses: underestimated preprocessing time complexity, mismatch between anonymous WL tests and non-anonymous features, and unrealistic assumptions about computational resources. To resolve these issues, they propose the Resource-Limited CONGEST (RL-CONGEST) model, which incorporates explicit constraints on computational resources and includes optional preprocessing and postprocessing phases. Using this framework, the authors prove several novel results about WL test hardness, the role of virtual nodes in reducing network capacity requirements, and the natural alignment of high-order GNNs with model checking problems.

## Method Summary
The paper introduces the Resource-Limited CONGEST (RL-CONGEST) model by extending the standard CONGEST model with explicit resource constraints on computational power and message sizes. The model incorporates optional preprocessing and postprocessing phases to handle graph transformations and final aggregation operations. Using this framework, the authors analyze the computational complexity of the WL test, demonstrate how virtual nodes can reduce network capacity requirements, and establish connections between high-order GNNs and first-order model checking problems. The approach leverages communication complexity theory and distributed computing principles to provide rigorous bounds on GNN expressiveness.

## Key Results
- The HASH function in the WL test requires linear network capacity (depth × width) in relation to graph size
- Virtual nodes can reduce the network capacity needed for WL computation from O(n) to O(Δ)
- High-order GNNs naturally align with first-order model checking problems rather than WL equivalence classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RL-CONGEST model exposes computational bottlenecks in WL tests that were previously hidden
- Mechanism: By imposing explicit resource constraints (bounded message size, limited computation per node), the model reveals that computing the HASH function in WL requires linear network capacity
- Core assumption: Network resources are the limiting factor, not just expressivity theory
- Evidence anchors:
  - [abstract] "we prove that the HASH function in the WL test typically requires the network capacity (depth multiplied by width) to be linear in relation to the graph's size"
  - [section 4.1] "either deterministically or randomly with zero error, the model's width w and depth d must satisfy d = Ω(D + m/w log n)"
- Break condition: If we allow unlimited computation per node (standard CONGEST), the hardness disappears

### Mechanism 2
- Claim: Virtual nodes reduce network size requirements for WL computation
- Mechanism: Adding a virtual node connected to all other nodes enables centralized aggregation of neighborhood information, reducing required depth×width product from O(n) to O(Δ)
- Core assumption: Virtual nodes can be added without affecting graph semantics for the WL computation
- Evidence anchors:
  - [section 4.2] "introducing a virtual node can reduce the network capacity required to compute one iteration of the WL problem"
  - [section 4.2] "There exists a deterministic RL-CONGEST algorithm... with width w and depth d satisfying dw = O(Δ)"
- Break condition: If virtual nodes significantly change graph structure or require expensive preprocessing

### Mechanism 3
- Claim: High-order GNNs naturally align with model checking problems rather than WL equivalence
- Mechanism: The k-WL graph construction + message passing corresponds to evaluating Ck model checking formulas on graphs
- Core assumption: Model checking problems are more practically meaningful than WL equivalence classes
- Evidence anchors:
  - [abstract] "we suggest that high-order GNNs correspond to first-order model-checking problems"
  - [section 4.3] "it is more meaningful to discuss the expressiveness of GNNs in terms of solving problems, such as model checking"
- Break condition: If practical GNN applications don't map to logical formulas

## Foundational Learning

- Concept: Distributed computing models (LOCAL vs CONGEST)
  - Why needed here: The paper's entire framework is built on distributed computing theory
  - Quick check question: What's the key difference between LOCAL and CONGEST models in terms of message size?

- Concept: Weisfeiler-Lehman graph isomorphism test
  - Why needed here: The paper uses WL test as the baseline for GNN expressiveness
  - Quick check question: What is the updating formula for the standard WL test?

- Concept: Communication complexity
  - Why needed here: Used to prove lower bounds on WL computation hardness
  - Quick check question: What is the communication complexity of the EQn function?

## Architecture Onboarding

- Component map: Preprocessing (graph transformations, feature computation) -> Message-passing (RL-CONGEST with bounded resources) -> Postprocessing (READOUT operations, final aggregation)

- Critical path: Preprocessing → Message-passing (O(k²) rounds) → Postprocessing

- Design tradeoffs:
  - More preprocessing vs fewer message-passing rounds
  - Higher-order GNNs vs computational resource constraints
  - Expressiveness vs practical implementability

- Failure signatures:
  - Preprocessing time complexity exceeds algorithmic task complexity
  - Mismatch between anonymous WL analysis and non-anonymous features
  - Unrealistic assumptions about node computational resources

- First 3 experiments:
  1. Implement WL test simulation with and without virtual nodes, measure depth×width product
  2. Compare RL-CONGEST vs standard CONGEST on NP-complete problems
  3. Test high-order GNN alignment with model checking on synthetic graph problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can higher-order GNNs solve the PNF Ck+1 model checking problem or general Ck model checking problems beyond the PNF Ck case?
- Basis in paper: [explicit] The paper conjectures that higher-order GNNs, with k-WL graph construction as preprocessing, may have the potential to solve PNF Ck+1 or general Ck model checking problems.
- Why unresolved: The authors only proved that RL-CONGEST can solve PNF Ck model checking problems in O(k²) rounds, but did not establish results for Ck+1 or general Ck cases.
- What evidence would resolve it: A formal proof showing that higher-order GNNs can or cannot solve PNF Ck+1 or general Ck model checking problems, either through construction of an algorithm or complexity-theoretic lower bounds.

### Open Question 2
- Question: What is the optimal trade-off between computational resources and round complexity in the RL-CONGEST model?
- Basis in paper: [inferred] The paper introduces the RL-CONGEST model with flexible configurations for computational resources but does not establish non-trivial trade-offs between resources and round complexity.
- Why unresolved: The authors presented near-optimal algorithms for specific problems (like WL test simulation) but did not systematically explore how varying computational resources affects round complexity across different problems.
- What evidence would resolve it: A comprehensive characterization of the relationship between allowed computational resources (e.g., TC0, P, NP) and achievable round complexity for various graph problems, potentially through matching upper and lower bounds.

### Open Question 3
- Question: Is there a corresponding model equivalence problem or other logic-related problems for the GD-WL framework?
- Basis in paper: [explicit] The paper notes that while GD-WL was designed to enhance GNN expressiveness, it is unclear whether there exists a corresponding model equivalence problem or logic-related characterization for this framework.
- Why unresolved: The authors align various WL test variants with model equivalence problems (e.g., C2 ME, C3 ME), but GD-WL's relationship to logic remains unexplored.
- What evidence would resolve it: Identification of a logic-based problem (such as a model checking or equivalence problem) that captures the expressive power of GD-WL, or a proof that no such natural correspondence exists.

## Limitations
- The RL-CONGEST model adds complexity that may make practical implementation challenging
- The assumption that virtual nodes can be added without affecting graph semantics requires further validation
- The alignment between high-order GNNs and model checking problems needs empirical verification

## Confidence
**High Confidence**: The computational hardness results for the WL test under RL-CONGEST constraints are well-supported by established communication complexity theory and the model's formal definitions.

**Medium Confidence**: The role of virtual nodes in reducing network capacity is theoretically proven but may have practical limitations depending on graph structure and the nature of the features being computed.

**Medium Confidence**: The connection between high-order GNNs and model checking problems provides valuable theoretical insight but requires more empirical validation to confirm practical relevance.

## Next Checks
1. **Implementation Verification**: Implement the RL-CONGEST model and verify that the theoretical bounds on depth×width product hold across diverse graph classes.

2. **Virtual Node Impact**: Test the virtual node approach on graphs with varying structures (scale-free, random, community-based) to assess whether the reduced network capacity requirements generalize beyond theoretical proofs.

3. **Model Checking Alignment**: Conduct empirical studies comparing high-order GNN performance on problems that map to first-order model checking versus traditional graph tasks to validate the theoretical alignment.