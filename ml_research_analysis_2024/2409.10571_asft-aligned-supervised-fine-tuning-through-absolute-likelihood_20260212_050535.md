---
ver: rpa2
title: 'ASFT: Aligned Supervised Fine-Tuning through Absolute Likelihood'
arxiv_id: '2409.10571'
source_url: https://arxiv.org/abs/2409.10571
tags:
- asft
- loss
- optimization
- llms
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ASFT, a new preference optimization method
  that avoids the limitations of Bradley-Terry model-based approaches like DPO. ASFT
  directly optimizes absolute likelihoods for chosen and rejected responses instead
  of using relative ranking scores, eliminating the need for a reference model.
---

# ASFT: Aligned Supervised Fine-Tuning through Absolute Likelihood

## Quick Facts
- arXiv ID: 2409.10571
- Source URL: https://arxiv.org/abs/2409.10571
- Reference count: 16
- Llama3-8B achieves 48% improvement over SFT on Arena-Hard benchmark

## Executive Summary
This paper introduces ASFT (Aligned Supervised Fine-Tuning), a novel preference optimization method that directly optimizes absolute likelihoods for chosen and rejected responses rather than using relative ranking scores from Bradley-Terry models. By eliminating the need for a reference model and addressing gradient imbalances in existing methods like DPO, ASFT achieves superior performance on human preference benchmarks while reducing training time by 13% and GPU memory usage by 16.7%. The approach demonstrates consistent improvements across Arena-Hard, MT-Bench, and text generation tasks using Llama3-8B-Instruct.

## Method Summary
ASFT fine-tunes language models by optimizing absolute likelihoods for preferred and dispreferred responses in pairwise preference data. The method combines standard supervised fine-tuning loss with an alignment loss that directly maximizes the probability of generating preferred responses and minimizes the probability of generating dispreferred responses. This approach avoids the Bradley-Terry model's gradient imbalances and eliminates dependency on reference models. The loss function uses a β parameter (typically 0.1) to balance the SFT and alignment components, enabling more stable training across different initialization conditions.

## Key Results
- 48% improvement over SFT on Arena-Hard benchmark (500 technical problem-solving queries)
- Superior performance on MT-Bench with better ratings on 80 instruction-following questions
- Improved BLEU-4 and ROUGE scores on text generation tasks while reducing training time by 13% and GPU memory by 16.7%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASFT optimizes absolute likelihoods rather than relative ranking scores, eliminating the need for a reference model
- Mechanism: Instead of using Bradley-Terry model-based relative comparisons, ASFT directly maximizes the probability of generating preferred responses and minimizes the probability of generating dispreferred responses
- Core assumption: Direct optimization of absolute likelihoods provides more effective gradient updates than relative ranking approaches
- Evidence anchors:
  - [abstract] "optimizing absolute likelihood for each response, rather than using the Bradley-Terry model, and eliminates the need for a reference model"
  - [section 2] "empirical findings suggest that non-reinforcement learning (non-RL) alignment methods are sensitive to the effectiveness of reference model"
- Break condition: If the assumption that absolute likelihoods provide better gradients than relative comparisons proves false, or if the simplified optimization becomes unstable in practice

### Mechanism 2
- Claim: ASFT mitigates the issue where DPO loss decreases probability of generating dispreferred data faster than increasing probability of producing preferred data
- Mechanism: By balancing the gradient updates for both preferred and dispreferred responses, ASFT ensures more proportional learning from human preferences
- Core assumption: The BT model's gradient imbalance creates suboptimal learning behavior that can be corrected through absolute likelihood optimization
- Evidence anchors:
  - [abstract] "ASFT mitigates the issue where the DPO loss function decreases the probability of generating human-dispreferred data at a faster rate than it increases the probability of producing preferred data"
  - [section 4.1] "LR (x1; x2) has a more significant impact on x2 than on x1, because the gradient with respect to x2 is larger than that for x1"
- Break condition: If the gradient analysis proves incorrect or if other factors dominate the learning dynamics beyond the BT model's limitations

### Mechanism 3
- Claim: ASFT is less sensitive to initialization and reference model effectiveness
- Mechanism: The absolute likelihood optimization approach provides more stable gradients regardless of the initial model state, reducing dependence on effective SFT as a prerequisite
- Core assumption: Removing reference model dependency while maintaining stable optimization is achievable through the proposed loss function
- Evidence anchors:
  - [abstract] "ASFT is less sensitive to initialization" and "eliminates the need for a reference model"
  - [section 4.3] "ASFT does not suffer from the limitations of BT model, such as an excessive focus on dispreferred responses"
- Break condition: If the independence from reference model proves to create other stability issues or if the initialization sensitivity manifests in ways not captured by current analysis

## Foundational Learning

- Concept: Maximum Likelihood Estimation (MLE)
  - Why needed here: ASFT uses MLE principles to maximize likelihood of preferred responses and minimize likelihood of dispreferred ones
  - Quick check question: How does maximizing the log-likelihood of preferred responses while minimizing it for dispreferred responses differ from standard supervised learning objectives?

- Concept: Bradley-Terry Model and its limitations
  - Why needed here: Understanding why BT-based approaches like DPO have gradient imbalances is crucial for appreciating ASFT's improvements
  - Quick check question: What specific property of the BT model's gradient causes it to penalize dispreferred responses more heavily than it rewards preferred ones?

- Concept: Reference-free alignment methods
  - Why needed here: ASFT's innovation relies on understanding how alignment can work without a separate reference model
- Quick check question: What are the computational and practical advantages of removing reference model dependency in preference optimization?

## Architecture Onboarding

- Component map:
  Input (x, yw, yl) -> ASFT Loss Function -> Fine-tuned Model

- Critical path:
  1. Prepare pairwise preference dataset
  2. Initialize base model (e.g., Llama3-8B-Instruct)
  3. Apply ASFT loss during training
  4. Monitor BLEU/ROUGE and benchmark performance

- Design tradeoffs:
  - No reference model: Reduced computational overhead but potentially less stable gradients
  - Absolute vs relative likelihoods: More direct optimization but requires careful handling of probability space
  - Single-stage alignment: Faster training but may be less robust than multi-stage approaches

- Failure signatures:
  - Degraded text generation quality on standard metrics (BLEU, ROUGE)
  - Poor performance on benchmarks like MT-Bench or Arena-Hard
  - Unstable training dynamics or failure to converge

- First 3 experiments:
  1. Implement ASFT loss function and verify it produces sensible gradients on synthetic preference data
  2. Compare ASFT vs DPO training dynamics on a small preference dataset to observe gradient behavior
  3. Evaluate fine-tuned model on a simple instruction-following task to confirm alignment effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical gradient analysis relies on specific assumptions about response distributions that may not hold across all preference datasets
- Scalability analysis limited to single 8B parameter model configuration
- Claims about initialization sensitivity reduction based on limited validation

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Addressing BT model gradient imbalance | High |
| Reduced initialization sensitivity | Medium |
| Computational efficiency improvements | High |

## Next Checks

1. Gradient behavior analysis across multiple preference datasets to verify ASFT consistently produces more balanced updates than BT-based methods under varying preference distributions

2. Systematic sensitivity analysis varying β across its suggested range (0.1-0.5) and testing different initialization strategies to better establish robustness claims

3. Ablation studies isolating the effects of reference model removal versus absolute likelihood optimization to clarify which mechanism drives primary performance improvements