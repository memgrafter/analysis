---
ver: rpa2
title: Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus
  Fine-Tuned Vision Transformers in Image-Based Security Applications
arxiv_id: '2403.17787'
source_url: https://arxiv.org/abs/2403.17787
tags:
- lmms
- malware
- fine-tuned
- prompt
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compared prompt-engineered large multimodal models
  (LMMs) and fine-tuned vision transformers (ViTs) in two cybersecurity tasks: trigger
  detection and malware classification. For trigger detection, ViTs achieved perfect
  performance (100% accuracy), while GPT-4o attained the highest LMM performance (91.9%
  accuracy, 91% F1-score).'
---

# Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models Versus Fine-Tuned Vision Transformers in Image-Based Security Applications

## Quick Facts
- arXiv ID: 2403.17787
- Source URL: https://arxiv.org/abs/2403.17787
- Reference count: 40
- Primary result: Fine-tuned Vision Transformers significantly outperform prompt-engineered LMMs in both trigger detection (100% vs 91.9% accuracy) and malware classification (97.11% vs 34.6% F1-score) tasks.

## Executive Summary
This study compares prompt-engineered large multimodal models (LMMs) and fine-tuned vision transformers (ViTs) in two cybersecurity tasks: trigger detection and malware classification. The results show that ViTs achieve superior performance in both tasks, with perfect accuracy in trigger detection and F1-scores above 97% in malware classification. LMMs, despite iterative prompt improvements, consistently underperform, achieving maximum F1-scores of only 34.6% for malware classification. The findings suggest that fine-tuned ViTs are more effective for precise and dependable image-based security tasks, particularly those requiring complex visual pattern analysis.

## Method Summary
The study evaluates five LMMs (LLaVA, BakLLaVA, Moondream2, Gemini-pro-vision, GPT-4o) using prompt engineering techniques against a fine-tuned ViT model on two cybersecurity tasks. For trigger detection, MNIST images with artificially added 4x4 pixel triggers are classified as "triggered" or "unaltered." For malware classification, the MaleVis dataset containing 25 malware classes across 5 families is used. The ViT model (google/vit-base-patch16-224) is fine-tuned on 90% of each dataset, while LMMs are evaluated using three different prompt templates per task. Performance is measured using accuracy, precision, recall, F1-score, and confusion matrices.

## Key Results
- ViTs achieved 100% accuracy in trigger detection, while the best LMM (GPT-4o) reached 91.9% accuracy
- ViTs achieved F1-scores of 97.11% for 25 malware classes and 97.61% for 5 malware families
- LMMs performed poorly on malware classification, with GPT-4o at 34.6% F1-score and Gemini-pro-vision at 12.7% F1-score
- Fine-tuned ViTs demonstrated superior capability in visual pattern analysis and classification compared to prompt-engineered LMMs

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuned ViTs achieve near-perfect performance on visually evident tasks like trigger detection due to their architectural ability to focus precisely on relevant visual features. ViTs process images as sequences of patches, allowing them to dynamically allocate attention across different image segments. This enables precise localization and recognition of small, specific visual patterns (e.g., 4x4 pixel triggers in MNIST images). The fine-tuning process results in optimal performance, achieving 100% in accuracy, precision, recall, and F1-Score.

### Mechanism 2
Prompt-engineered LMMs struggle with complex visual pattern analysis tasks like malware classification due to limitations in extracting sufficient context from prompts alone. LMMs rely on textual prompts to guide visual analysis, but in complex tasks requiring deep pattern recognition, the prompts cannot provide enough contextual information for the model to distinguish between subtle visual differences. This limitation is evident in the significantly lower F1-scores (34.6% for GPT-4o) compared to fine-tuned ViTs.

### Mechanism 3
Fine-tuned ViTs demonstrate superior capability in visual pattern analysis and classification compared to prompt-engineered LMMs due to their ability to learn task-specific visual features through fine-tuning. Fine-tuning adjusts the model's parameters on specific datasets, allowing it to optimize for the nuances of the task. This specialization enables better recognition of intricate visual patterns compared to general-purpose LMMs, as evidenced by the high F1-scores (97.11% and 97.61%) achieved in malware classification.

## Foundational Learning

- Concept: Transformer architecture fundamentals
  - Why needed here: Understanding how ViTs and LMMs process visual and multimodal data
  - Quick check question: How do transformers handle sequential data differently from convolutional networks?

- Concept: Fine-tuning vs. prompt engineering
  - Why needed here: The paper directly compares these two approaches for adapting models to specific tasks
  - Quick check question: What are the key differences in how fine-tuning and prompt engineering modify model behavior?

- Concept: Cybersecurity visual analysis tasks
  - Why needed here: The study focuses on trigger detection and malware classification, which require specific understanding of visual patterns in security contexts
  - Quick check question: What makes trigger detection a "visually evident" task while malware classification is "visually non-evident"?

## Architecture Onboarding

- Component map:
  - Image preprocessing and normalization for both ViTs and LMMs
  - Model selection between prompt-engineered LMMs (LLaVA, BakLLaVA, Moondream, Gemini-pro-vision, GPT-4o) and fine-tuned ViTs
  - Task-specific adaptation: Fine-tuning for ViTs vs. prompt engineering for LMMs
  - Evaluation using classification metrics (accuracy, precision, recall, F1-score) and confusion matrices

- Critical path:
  1. Prepare datasets (MNIST with triggers, MaleVis malware images)
  2. Implement fine-tuning pipeline for ViTs
  3. Develop prompt engineering strategies for LMMs
  4. Run experiments and collect metrics
  5. Compare performance across models and tasks

- Design tradeoffs:
  - ViTs: Higher performance but require fine-tuning on task-specific data
  - LMMs: Easier to use via API but limited by prompt engineering effectiveness
  - Computational resources: Fine-tuning ViTs requires more resources than prompt engineering

- Failure signatures:
  - ViTs: Overfitting during fine-tuning (indicated by poor generalization to test set)
  - LMMs: Consistent bias toward specific classes despite prompt variations
  - Both: Inability to detect subtle visual patterns or distinguish between similar classes

- First 3 experiments:
  1. Fine-tune ViT on MNIST trigger detection task and evaluate on test set
  2. Test LLaVA with baseline prompt on MNIST trigger detection
  3. Compare Gemini-pro-vision performance with different prompt variations on malware classification task

## Open Questions the Paper Calls Out

### Open Question 1
How do prompt-engineered LMMs and fine-tuned ViTs compare in detecting more complex or multi-faceted backdoor triggers beyond simple pixel variations? The paper tested trigger detection using simple 4x4 pixel variations and found ViTs achieved perfect performance while LMMs reached 91.9% accuracy with GPT-4o. The authors note the need to explore more complex trigger patterns.

### Open Question 2
What specific architectural or training characteristics of ViTs enable their superior performance in visually non-evident tasks like malware classification compared to LMMs? The paper demonstrates ViTs achieving 97.11% F1-score for malware classification while LMMs only reached 34.6% F1-score, yet does not explain the underlying reasons for this performance gap.

### Open Question 3
Can prompt engineering techniques be systematically developed and optimized to bridge the performance gap between LMMs and fine-tuned ViTs for complex visual tasks? While the study tested multiple prompts, it did not explore comprehensive prompt engineering methodologies or determine if there are fundamental limits to what can be achieved through prompting alone.

## Limitations

- The MNIST trigger detection task uses artificially added triggers that may not represent real-world adversarial examples
- The MaleVis malware dataset, while comprehensive, may not capture the full diversity of malware visualization patterns encountered in practice
- The evaluation focuses on classification accuracy without assessing model robustness to adversarial attacks or out-of-distribution samples

## Confidence

- **High Confidence**: ViTs outperform LMMs in malware classification tasks (97.11% and 97.61% F1-scores vs 34.6% and 12.7% F1-scores)
- **Medium Confidence**: ViTs achieve perfect performance on trigger detection while LMMs perform significantly worse
- **Low Confidence**: The generalizability of these findings to other visually evident tasks beyond trigger detection

## Next Checks

1. **Real-world Trigger Detection**: Test the same model comparison on naturally occurring adversarial triggers in real malware samples rather than artificially added patterns to validate the practical utility of ViT performance advantages.

2. **Adversarial Robustness Testing**: Evaluate both model types against adversarial attacks designed to fool trigger detection and malware classification systems to understand security implications beyond baseline accuracy.

3. **Cross-dataset Generalization**: Test model performance on malware visualization datasets from different sources or domains to assess whether the observed performance patterns hold across varied threat landscapes.