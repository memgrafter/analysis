---
ver: rpa2
title: 'GW-MoE: Resolving Uncertainty in MoE Router with Global Workspace Theory'
arxiv_id: '2406.12375'
source_url: https://arxiv.org/abs/2406.12375
tags:
- tokens
- experts
- uncertain
- arxiv
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of uncertainty in Mixture-of-Experts
  (MoE) models, where some tokens have nearly equal scores for selecting each expert,
  leading to incorrect selections. Inspired by the Global Workspace Theory (GWT),
  the authors propose GW-MoE, a fine-tuning method that broadcasts uncertain tokens
  to all experts during training.
---

# GW-MoE: Resolving Uncertainty in MoE Router with Global Workspace Theory

## Quick Facts
- arXiv ID: 2406.12375
- Source URL: https://arxiv.org/abs/2406.12375
- Authors: Haoze Wu; Zihan Qiu; Zili Wang; Hang Zhao; Jie Fu
- Reference count: 13
- One-line primary result: GW-MoE achieves an average 0.53 increase in performance on the GLUE benchmark and shows consistent improvements across various tasks and model sizes

## Executive Summary
This paper addresses the problem of uncertainty in Mixture-of-Experts (MoE) models, where some tokens have nearly equal scores for selecting each expert, leading to incorrect selections. Inspired by the Global Workspace Theory (GWT), the authors propose GW-MoE, a fine-tuning method that broadcasts uncertain tokens to all experts during training. This allows all experts to learn the knowledge of uncertain tokens, making them less sensitive to the choice during inference without introducing additional overhead. The method consistently improves performance across various tasks and model sizes.

## Method Summary
GW-MoE is a fine-tuning method for MoE models that addresses uncertainty in expert selection by broadcasting uncertain tokens to all experts during training. The method computes router scores and entropy for each token, identifies uncertain tokens (those with entropy above a threshold H*), and routes certain tokens to top-K experts while broadcasting uncertain tokens to all experts. This ensures all experts learn the relevant knowledge for uncertain tokens, making them less sensitive to expert selection during inference. The approach maintains the same inference efficiency as standard MoE while improving performance across multiple tasks and model sizes.

## Key Results
- Achieves an average 0.53 increase in performance on the GLUE benchmark
- Shows better results on question-answering (SQuAD, Quoref), summarization (DialogSum), and code generation (HumanEval) tasks
- Demonstrates consistent improvements across various model sizes and task types
- Maintains inference efficiency while improving training robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Broadcasting uncertain tokens to all experts during fine-tuning reduces the impact of incorrect expert selection during inference.
- Mechanism: During fine-tuning, uncertain tokens are routed to all experts instead of just the top-K. This ensures all experts learn the relevant knowledge for these tokens, making them less sensitive to the choice during inference.
- Core assumption: All experts can effectively learn from uncertain tokens when exposed to them during fine-tuning.
- Evidence anchors:
  - [abstract]: "The core idea is to broadcast the uncertain tokens across experts during fine-tuning. Therefore, these tokens can acquire the necessary knowledge from any expert during inference and become less sensitive to the choice."
  - [section]: "GW-MoE enables the expert updates caused by uncertain tokens to be fully-differentiable. During fine-tuning, the input tokens are divided into certain and uncertain parts based on H*."

### Mechanism 2
- Claim: Randomly selecting experts for uncertain tokens during inference can sometimes outperform top-K selection.
- Mechanism: When tokens have nearly equal scores for selecting each expert (high uncertainty), the top-K operator may make suboptimal choices. Random selection among these uncertain tokens can sometimes yield better results because it avoids consistently poor deterministic choices.
- Core assumption: The top-K operator is not always optimal for tokens with highly similar expert scores.
- Evidence anchors:
  - [abstract]: "we demonstrate that this uncertainty can lead to incorrect selections" and "we let the uncertain tokens (entropy greater than 2.0) in the last layer of JetMoE randomly select experts, and the average results (blue) from multiple experiments on three tasks are better than those obtained by using the Top-K operator to select experts (dashed line)."

### Mechanism 3
- Claim: GW-MoE reduces the sensitivity of uncertain tokens to expert selection by storing their knowledge in all experts.
- Mechanism: By broadcasting uncertain tokens to all experts during fine-tuning, the knowledge required by these tokens is stored across all experts. During inference, uncertain tokens can obtain necessary information from any expert, reducing the impact of choosing the "wrong" expert.
- Core assumption: Storing knowledge across all experts for uncertain tokens is beneficial and does not create harmful interference.
- Evidence anchors:
  - [abstract]: "Therefore, these tokens can acquire the necessary knowledge from any expert during inference and become less sensitive to the choice."
  - [section]: "This ensures that during fine-tuning, all experts can learn the knowledge from the uncertain tokens. In other words, if we consider the experts as memory blocks, we store the information needed by uncertain tokens in all the blocks."

## Foundational Learning

- Concept: Entropy as a measure of uncertainty in router scores
  - Why needed here: The paper uses normalized entropy to identify uncertain tokens that have nearly equal scores for choosing each expert.
  - Quick check question: If a token has scores [0.25, 0.25, 0.25, 0.25] for 4 experts, what is its normalized entropy?

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: GW-MoE is a fine-tuning method specifically designed for MoE models, which use a router to dynamically select experts for each token.
  - Quick check question: In a standard MoE setup with 8 experts and top-2 routing, how many experts are activated for each token during inference?

- Concept: Global Workspace Theory (GWT)
  - Why needed here: The paper draws inspiration from GWT, which suggests that complex signals can be processed by broadcasting information through a global workspace, similar to how uncertain tokens are broadcast to all experts.
  - Quick check question: How does the concept of broadcasting uncertain tokens to all experts in GW-MoE relate to the idea of a global workspace in GWT?

## Architecture Onboarding

- Component map: Input tokens -> Router (produces expert scores) -> Entropy calculation -> Token classification (certain/uncertain) -> Top-K routing (certain tokens) / Broadcasting (uncertain tokens) -> Experts -> Output

- Critical path:
  1. Compute router scores for all tokens
  2. Calculate entropy of router scores
  3. Identify uncertain tokens (entropy > H*)
  4. Route certain tokens to top-K experts (standard MoE)
  5. Broadcast uncertain tokens to all experts
  6. Compute loss and backpropagate gradients
  7. Update expert parameters based on both certain and uncertain tokens

- Design tradeoffs:
  - Accuracy vs. computational overhead: GW-MoE introduces additional computation during fine-tuning but maintains the same inference efficiency as standard MoE
  - Broad knowledge vs. specialization: Broadcasting uncertain tokens to all experts may reduce individual expert specialization but increases overall robustness
  - Threshold sensitivity: The choice of H* affects which tokens are considered uncertain and can impact performance

- Failure signatures:
  - Performance degradation if H* is set too low (certain tokens are incorrectly broadcast)
  - Increased training time and memory usage if max num slots is set too high
  - Ineffective fine-tuning if uncertain tokens do not contain meaningful information for experts to learn

- First 3 experiments:
  1. Validate that uncertain tokens exist in pre-trained MoE models by computing entropy distributions and identifying tokens with high entropy
  2. Demonstrate that random expert selection for uncertain tokens can outperform top-K selection on a specific task
  3. Compare GW-MoE performance against standard fine-tuning on a diverse set of tasks and model sizes to verify consistent improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GW-MoE perform during pre-training of MoE models compared to standard fine-tuning?
- Basis in paper: [inferred] The paper focuses on fine-tuning pre-trained MoE models but does not explore using GW-MoE during pre-training.
- Why unresolved: The paper explicitly states this as a limitation, noting that they did not explore the possibility of using GW-MoE during pre-training.
- What evidence would resolve it: Experiments comparing GW-MoE to standard fine-tuning during pre-training of MoE models on various tasks and model sizes.

### Open Question 2
- Question: Can GW-MoE be effectively applied to larger-scale models like Mixtral 8x7B and Mixtral 8x22B?
- Basis in paper: [inferred] The paper mentions this as a limitation, stating that due to experimental conditions, they did not validate GW-MoE on larger-scale models like Mixtral 8x7B and Mixtral 8x22B.
- Why unresolved: The paper explicitly states this as a limitation, noting that they did not validate their method on larger-scale models due to experimental conditions.
- What evidence would resolve it: Experiments applying GW-MoE to larger-scale models like Mixtral 8x7B and Mixtral 8x22B on various tasks and comparing performance to standard fine-tuning.

### Open Question 3
- Question: How can GW-MoE be improved to enhance performance on decoder-only models in understanding tasks like MMLU?
- Basis in paper: [inferred] The paper mentions that due to the lack of semantics in broadcast tokens, GW-MoE is unable to provide enhancement to decoder-only models in understanding tasks like MMLU.
- Why unresolved: The paper explicitly states this as a limitation, noting that they did not improve GW-MoE to enhance performance on decoder-only models in understanding tasks.
- What evidence would resolve it: Modifications to GW-MoE that incorporate semantic information in the broadcast tokens or alternative approaches to enhance performance on decoder-only models in understanding tasks.

## Limitations

- Weak empirical validation of core mechanisms: The paper provides performance improvements across benchmarks but lacks ablation studies that would definitively prove the broadcasting mechanism is responsible for these gains.
- Limited scope of uncertainty analysis: The paper focuses on entropy-based uncertainty but does not explore other forms of uncertainty that might affect MoE performance.
- Threshold sensitivity concerns: The method relies on setting a threshold H* to identify uncertain tokens, but the paper does not provide comprehensive analysis of how sensitive results are to this hyperparameter.

## Confidence

**High confidence claims**: The empirical results showing consistent performance improvements across multiple tasks and model sizes (average 0.53 increase on GLUE benchmark) appear well-supported by the data presented.

**Medium confidence claims**: The assertion that random expert selection for uncertain tokens can outperform top-K selection is supported by experimental results but lacks strong theoretical justification.

**Low confidence claims**: The fundamental claim that broadcasting uncertain tokens to all experts during fine-tuning is the primary mechanism driving performance improvements has the lowest confidence due to limited ablation studies.

## Next Checks

1. **Ablation study on broadcasting mechanism**: Conduct controlled experiments that isolate the effect of broadcasting uncertain tokens to all experts by comparing GW-MoE against variants where uncertain tokens are handled differently (e.g., routed to top-2 experts instead of all experts, or excluded from training entirely).

2. **Threshold sensitivity analysis**: Systematically vary the H* threshold across a wide range of values and measure the impact on performance, training stability, and the distribution of uncertain tokens.

3. **Expert specialization analysis**: Measure the degree of specialization before and after GW-MoE fine-tuning by analyzing expert output distributions, correlation between expert activations, and task-specific performance of individual experts.