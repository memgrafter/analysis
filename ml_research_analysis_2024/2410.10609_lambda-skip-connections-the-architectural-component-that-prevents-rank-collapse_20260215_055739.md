---
ver: rpa2
title: 'Lambda-Skip Connections: the architectural component that prevents Rank Collapse'
arxiv_id: '2410.10609'
source_url: https://arxiv.org/abs/2410.10609
tags:
- rank
- collapse
- skip
- have
- layernorm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates rank collapse in sequence models, where
  embedding vectors converge to a uniform token, reducing model expressivity and causing
  training instabilities. While well-studied in transformers, rank collapse has not
  been thoroughly examined in State Space Models (SSMs).
---

# Lambda-Skip Connections: the architectural component that prevents Rank Collapse

## Quick Facts
- arXiv ID: 2410.10609
- Source URL: https://arxiv.org/abs/2410.10609
- Authors: Federico Arangath Joseph; Jerome Sieber; Melanie N. Zeilinger; Carmen Amo Alonso
- Reference count: 40
- Primary result: Lambda-skip connections with appropriate parameter λ prevent rank collapse in sequence models by counteracting exponential decay of embedding rank

## Executive Summary
This paper addresses the problem of rank collapse in sequence models, where embedding vectors converge to a uniform token, reducing model expressivity and causing training instabilities. While well-studied in transformers, this phenomenon has not been thoroughly examined in State Space Models (SSMs). The authors propose lambda-skip connections, a parameterized version of skip connections, and develop a general theoretical framework applicable to both transformers and SSMs. They derive sufficient conditions on the lambda parameter to prevent rank collapse and validate their findings through experiments on Mamba and S4 architectures.

## Method Summary
The method involves analyzing rank collapse through a mathematical framework that expresses the model architecture as Y(k) = D(k)(M(k-1) + λI)Y(k-1)C(k-1)ᵥ, where λI represents the lambda-skip connection. The authors derive sufficient conditions for rank collapse prevention based on the relationship between λ and architectural constants. Experiments validate the theoretical claims by implementing lambda-skip connections in pre-trained Mamba-2 and S4 models, measuring the rank collapse metric μ(Y(k)) across layers with varying λ values, and comparing performance on LRA image and MQAR tasks.

## Key Results
- Lambda-skip connections with appropriate λ values prevent rank collapse, while λ=0 leads to exponential decay of the rank collapse metric
- LayerNorm ensures the rank collapse bound is input-independent, strengthening the theoretical guarantee
- Gating mechanisms, originally designed for memory improvement, play an important role in preventing rank collapse
- Tuning λ is crucial for preventing rank collapse, with the ideal choice being a = 1 for the collapse rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lambda-skip connections prevent rank collapse by introducing a controlled skip strength parameter λ that counteracts the exponential decay of the rank collapse metric.
- Mechanism: The lambda-skip connection modifies the standard skip connection to include a parameter λ, allowing control over the skip connection strength. When λ is chosen appropriately, it ensures that the rank collapse metric μ(Y(K)) remains bounded below by aKμ(Y(0))², preventing the exponential decay to zero.
- Core assumption: The model architecture can be expressed in the form Y(k) = D(k)(M(k-1) + λI)Y(k-1)C(k-1)ᵥ, where M(k) represents the main mechanism (attention or recurrent block), D(k) is LayerNorm, and λI is the lambda-skip connection.
- Evidence anchors:
  - [abstract]: "We study how a parametrized version of the classic skip connection component, which we call lambda-skip connections, provides guarantees for rank collapse prevention."
  - [section]: "We provide a mathematical analysis of this metric in terms of the different architectural components... The goal of this paper is to show how the introduction of λ(k) in the lambda-skip connection influences rank collapse."
  - [corpus]: Weak - The corpus neighbors discuss skip connections and rank collapse but do not specifically address the lambda-skip connection mechanism or its theoretical guarantees.
- Break condition: If λ does not satisfy the condition λ²c² - aS²(CM + |λ|)² > 0, rank collapse can still occur despite the presence of lambda-skip connections.

### Mechanism 2
- Claim: LayerNorm, when combined with lambda-skip connections, ensures that the rank collapse bound is independent of the input sequence, making the prevention more robust.
- Mechanism: LayerNorm normalizes the output after the lambda-skip connection, which means the constant CM (supremum of the Frobenius norm of M(k)) depends only on the weights and sequence length, not on the input. This input independence strengthens the theoretical guarantee.
- Core assumption: LayerNorm is applied as D(k)Y(k) where D(k) = diag(1/||Ỹ(k)ᵢ,:||₂), normalizing each row of the output matrix.
- Evidence anchors:
  - [abstract]: "Both empirical observations and theoretical results support that the presence of skip connections... as well as the LayerNorm component... in the architecture mitigate the issue of rank collapse."
  - [section]: "The presence of LayerNorm implies that the bound provided in equation 7 does not depend on the input even when the matrix M is input-dependent."
  - [corpus]: Weak - While LayerNorm is mentioned in related works, the specific role of LayerNorm in conjunction with lambda-skip connections for input-independent rank collapse prevention is not addressed in the corpus.
- Break condition: If LayerNorm is ablated, the rank collapse prevention becomes dependent on the input sequence, potentially weakening the guarantee.

### Mechanism 3
- Claim: The choice of λ is critical; too small |λ| leads to rank collapse while appropriate |λ| values prevent it, with the ideal being a = 1 for the collapse rate.
- Mechanism: The parameter λ controls the strength of the skip connection. If |λ| is too small, the model suffers from rank collapse. However, when |λ| is sufficiently large (satisfying the condition |λ| > (aCM S² + √(ac²C²MS²))/(c² - aS²)), rank collapse is prevented. The ideal choice is a = 1, but this is mediated by the values of c and S in different architectures.
- Core assumption: The relationship between λ and the collapse rate a is governed by the condition λ²c² - aS²(CM + |λ|)² > 0, where c, S, and CM are architecture-specific constants.
- Evidence anchors:
  - [abstract]: "We present a sufficient condition to guarantee prevention of rank collapse across all the aforementioned architectures."
  - [section]: "In particular, we note that to guarantee prevention of rank collapse, the ideal choice is a = 1. However, this choice will be mediated by the values of c and S in the different architectures."
  - [corpus]: Weak - The corpus neighbors discuss skip connections and their impact on training but do not specifically address the critical role of λ in preventing rank collapse or the mathematical conditions required.
- Break condition: If |λ| is not chosen to satisfy the condition λ²c² - aS²(CM + |λ|)² > 0, rank collapse will occur regardless of other architectural components.

## Foundational Learning

- Concept: Matrix norms and singular values (σmin, σmax)
  - Why needed here: The analysis relies heavily on bounds involving matrix norms (Frobenius norm) and singular values to establish the relationship between architectural components and rank collapse prevention.
  - Quick check question: If A is a matrix, what is the relationship between ||AB||F and σmin(B)||A||F?

- Concept: Skip connections and their role in deep networks
  - Why needed here: Understanding the standard skip connection mechanism and its purpose in stabilizing training is crucial for grasping why lambda-skip connections are an effective modification.
  - Quick check question: What is the primary motivation behind introducing skip connections in deep residual networks?

- Concept: Layer normalization and its effect on training stability
  - Why needed here: LayerNorm plays a critical role in the theoretical analysis by making the rank collapse bound input-independent, which is a key aspect of the prevention mechanism.
  - Quick check question: How does LayerNorm differ from Batch Normalization in terms of normalization scope?

## Architecture Onboarding

- Component map: Input → Main mechanism (M(k)) → Lambda-skip connection (λI) → LayerNorm (D(k)) → MLP → Output
- Critical path: The critical components for rank collapse prevention are the lambda-skip connection and LayerNorm.
- Design tradeoffs:
  - λ too small: Risk of rank collapse
  - λ too large: Potential loss of expressivity or optimization difficulties
  - Without LayerNorm: Rank collapse prevention becomes input-dependent
  - With LayerNorm: Input-independent guarantee but computational overhead
- Failure signatures:
  - Rank collapse metric μ(Y(k)) approaching zero exponentially
  - Vanishing gradients during training
  - Reduced model expressivity due to uniform token embeddings
  - Training instability in deeper layers
- First 3 experiments:
  1. Implement a transformer with lambda-skip connections and vary λ across layers to observe the effect on rank collapse metric.
  2. Ablate the skip connection (set λ=0) in a selective SSM and measure rank collapse to confirm theoretical predictions.
  3. Compare rank collapse in architectures with and without LayerNorm while keeping lambda-skip connections constant.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of gating mechanisms in the theoretical analysis affect the bounds on rank collapse prevention?
- Basis in paper: [inferred] The paper notes that gating mechanisms are excluded from the theoretical analysis for simplicity, but they empirically show these mechanisms play a crucial role in preventing rank collapse.
- Why unresolved: The theoretical framework developed focuses on lambda-skip connections and LayerNorm, but does not account for gating mechanisms which are present in many modern SSM architectures like Mamba.
- What evidence would resolve it: A theoretical extension of the current framework to include gating mechanisms, showing how they modify the sufficient conditions for preventing rank collapse and potentially tightening the bounds.

### Open Question 2
- Question: What is the optimal strategy for learning the lambda parameter across layers in deep sequence models?
- Basis in paper: [explicit] The paper shows that treating lambda as a learnable parameter does not affect performance and may improve stability, but does not explore optimal learning strategies.
- Why unresolved: The paper only considers fixed lambda values across layers in theory and initializes lambda=-1 for learning in experiments, without exploring different learning rates, schedules, or layer-specific values.
- What evidence would resolve it: Systematic experiments comparing different lambda learning strategies (fixed, layer-specific, adaptive) across various architectures and tasks, measuring both rank collapse prevention and model performance.

### Open Question 3
- Question: How do MLP layers influence the rank collapse phenomenon in sequence models?
- Basis in paper: [explicit] The paper acknowledges that MLPs increase the Lipschitz constant of the network and may help prevent rank collapse, but does not include them in the theoretical analysis.
- Why unresolved: The theoretical analysis focuses on skip connections and LayerNorm, explicitly stating that including MLPs would further improve the bounds but not pursuing this direction.
- What evidence would resolve it: A theoretical extension incorporating MLPs into the rank collapse analysis, showing how they modify the sufficient conditions and potentially providing tighter bounds on the rank collapse measure.

## Limitations

- Theoretical framework may not capture all architectural variations beyond transformers and SSMs
- Limited experimental validation scope with only Mamba and S4 architectures tested
- No practical guidelines provided for selecting optimal λ values across different tasks and datasets

## Confidence

**High Confidence**: The theoretical analysis of rank collapse prevention mechanism is well-established, with clear mathematical derivations showing how lambda-skip connections with appropriate λ values prevent exponential rank decay.

**Medium Confidence**: The experimental validation showing lambda-skip connections prevent rank collapse in Mamba and S4 architectures is convincing, but the limited scope of tested architectures and datasets means broader applicability claims should be treated with caution.

**Low Confidence**: The practical implications for model performance and training dynamics when tuning λ across different tasks are not thoroughly explored, and the paper doesn't provide clear guidelines for practitioners.

## Next Checks

**Validation Check 1**: Test the lambda-skip connection framework on additional sequence model architectures beyond transformers and SSMs, such as convolutional sequence models or hybrid architectures, to assess generalizability of the theoretical guarantees.

**Validation Check 2**: Conduct a comprehensive ablation study varying λ across a wider range of values and measuring not just rank collapse metrics but also downstream task performance, training stability, and convergence speed to establish practical guidelines for λ selection.

**Validation Check 3**: Investigate the interaction between lambda-skip connections and other architectural components like activation functions, attention mechanisms, and different normalization schemes to determine whether the rank collapse prevention mechanism is robust to architectural variations.