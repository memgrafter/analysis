---
ver: rpa2
title: Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks
  and Model Ensembles
arxiv_id: '2410.09303'
source_url: https://arxiv.org/abs/2410.09303
tags:
- token
- tokens
- cover
- tokenization
- byte-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how tokenization in language models introduces
  "tokenization bias," where the predictive distribution over the next byte can differ
  substantially from that of a byte-level model, even when both are statistically
  equivalent. The authors propose the Byte-Token Representation Lemma, which establishes
  a mapping between token-level and byte-level distributions, and develop an efficient
  algorithm to sample exact byte-level predictions from any tokenized model without
  retraining.
---

# Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks and Model Ensembles

## Quick Facts
- arXiv ID: 2410.09303
- Source URL: https://arxiv.org/abs/2410.09303
- Reference count: 40
- One-line primary result: Eliminates tokenization bias to achieve 18% improvement on FIM coding benchmarks and up to 3.7% improvement in model ensemble performance.

## Executive Summary
This paper addresses tokenization bias in language models, where the predictive distribution over the next byte differs substantially between tokenized and byte-level models even when statistically equivalent. The authors introduce the Byte-Token Representation Lemma to map token-level distributions to exact byte-level distributions, and develop an efficient algorithm to sample byte-level predictions without retraining. Their method eliminates tokenization bias and enables seamless model ensembling by aggregating predictions in a universal byte space. Experiments demonstrate significant improvements on FIM coding benchmarks and reasoning tasks.

## Method Summary
The authors propose the Byte-Token Representation Lemma to establish a mapping between token-level and byte-level distributions. This allows them to develop an efficient algorithm that samples exact byte-level predictions from any tokenized model without retraining. The method involves searching for all valid token sequences (cover encodings) that could follow the prompt prefix, then mapping these to exact byte probabilities. This eliminates tokenization bias and enables model ensembling by mapping predictions to a universal byte space where they can be aggregated.

## Key Results
- 18% improvement on FIM coding benchmarks (HumanEval, MBPP)
- Up to 3.7% improvement in model ensemble performance over individual models across reasoning, knowledge, and coding tasks
- Exact byte-level predictions without retraining or model modification
- Efficient O(nℓ) algorithm for cover encoding search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokenization bias arises because the probability of the next byte under a tokenized model differs from the true byte-level probability due to incomplete token context.
- Mechanism: The Byte-Token Representation Lemma maps the tokenized distribution over next tokens to an exact byte-level distribution by considering all valid token sequences covering the prompt prefix.
- Core assumption: The tokenizer is deterministic and every string maps to a unique encoding; the ground-truth byte-level distribution is accessible.
- Evidence anchors:
  - [abstract] "predictive distributions over the next byte can be substantially different"
  - [section] "Tokenization bias emerges, i.e. the probability P (x2=“A”|t1=“A”)=0.0 is not equal to P (x2=“A”|x1=“A”) = 1 −α"
  - [corpus] weak - only mentions tokenization bias, no direct anchor
- Break condition: If the tokenizer is non-deterministic or the model assigns non-zero probability to invalid encodings, the exact mapping fails.

### Mechanism 2
- Claim: The cover encodings search algorithm efficiently enumerates all valid token sequences that could follow the prompt without exhaustive search.
- Mechanism: By reversing the encoding process and exploiting invalid encoding constraints, the algorithm finds all token sequences covering the prompt prefix in O(nℓ) time.
- Core assumption: BPE/MPE tokenization schemes allow reverse search to recover valid encodings without missing possibilities.
- Evidence anchors:
  - [section] "searching from the reverse order, for any suffix xn i+1, we can find all tokens that start with xn i+1"
  - [section] "the number of model runs is at most nℓ"
  - [corpus] weak - corpus does not detail the search algorithm
- Break condition: If tokenization scheme changes (e.g., BPE dropout) or vocabulary size grows unpredictably, search complexity may exceed O(nℓ).

### Mechanism 3
- Claim: Ensembling tokenized models is enabled by mapping each model's predictions to a universal byte space, preserving statistical equivalence while enabling aggregation.
- Mechanism: Byte-level predictions from each model are averaged directly, since all models now operate in the same byte domain.
- Core assumption: Each model's byte-level predictions are statistically equivalent to its original token-level predictions.
- Evidence anchors:
  - [abstract] "mapping predictions and conditioning domains to byte-space, one can easily aggregate predictions from multiple models"
  - [section] "By mapping their predictions and conditioning domains to byte-space, one can easily aggregate predictions"
  - [corpus] moderate - mentions model ensembles and aggregation but not the byte-space mapping explicitly
- Break condition: If models have conflicting tokenization schemes that break byte-level equivalence, aggregation introduces bias.

## Foundational Learning

- Concept: Markov chains and autoregressive processes
  - Why needed here: The paper uses Markov chain examples to illustrate tokenization bias and derive token-level distributions from byte-level ones.
  - Quick check question: If P(x2="A"|x1="A") = 1-α in a 1st-order Markov chain, what is P(t2="A"|t1="AA") under the given vocabulary?
- Concept: Byte-Pair Encoding (BPE) and Maximum Prefix Encoding (MPE)
  - Why needed here: These tokenization schemes are used throughout the paper to define the cover encodings and derive the mapping lemma.
  - Quick check question: Why does an encoding become invalid under BPE/MPE, and how does this affect probability assignment?
- Concept: Probability marginalization and conditional probability factorization
  - Why needed here: The Byte-Token Representation Lemma requires summing over cover encodings and computing conditional probabilities to recover exact byte-level predictions.
  - Quick check question: Given cover encodings t1 and t2 with probabilities 0.6 and 0.4, what is P(x1) under the lemma?

## Architecture Onboarding

- Component map: Input -> Cover encodings search -> Byte-token mapping -> Next-byte sampling
- Critical path: Extract cover encodings → Map to byte probabilities → Sample next byte
- Design tradeoffs:
  - Memory: Must store cover encodings and their probabilities for each step
  - Speed: O(nℓ) search per byte, but amortized over generation
  - Accuracy: Exact byte-level predictions vs approximate token-level ones
- Failure signatures:
  - Invalid encodings with non-zero probability (model violation)
  - Missing cover encodings (search bug)
  - Non-convergent probabilities (numerical instability)
- First 3 experiments:
  1. Reproduce the 1st-order Markov chain tokenization bias example with (α,β)=(0.4,0.3)
  2. Apply cover encoding search to the 3rd-order Markov chain and compare recovered probabilities
  3. Implement next-byte sampling on a small BPE-trained GPT-2 and test on a FIM coding benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between tokenization bias and the cumulative impact on long sequences, and how does this bias compound over multiple generations?
- Basis in paper: [inferred] The paper analyzes tokenization bias in single-token predictions but does not examine its cumulative impact over hundreds of tokens in real-world applications.
- Why unresolved: The theoretical framework and experiments focus on single-step predictions, leaving the long-term effects of bias on generation quality unexplored.
- What evidence would resolve it: Systematic experiments measuring performance degradation over sequences of varying lengths, comparing tokenized and byte-level models.

### Open Question 2
- Question: How does tokenization bias manifest differently across various tokenization algorithms (e.g., BPE vs. MPE vs. token-free models), and are there algorithmic differences in their susceptibility to bias?
- Basis in paper: [explicit] The paper uses BPE and MPE as examples but does not provide a comprehensive comparison of bias across different tokenization schemes or token-free models.
- Why unresolved: The analysis is limited to specific tokenizers, and the paper does not explore how algorithmic differences in tokenization affect bias.
- What evidence would resolve it: Empirical studies comparing tokenization bias across multiple tokenization algorithms and token-free models, with quantitative metrics.

### Open Question 3
- Question: Can the Byte-Token Representation Lemma be extended to handle stochastic tokenizers like BPE-dropout, and what are the theoretical and practical implications of such an extension?
- Basis in paper: [explicit] The paper briefly mentions BPE-dropout and suggests the BTR Lemma holds but does not provide a detailed analysis or practical implementation.
- Why unresolved: The paper acknowledges the theoretical possibility but does not explore the practical challenges or benefits of extending the method to stochastic tokenizers.
- What evidence would resolve it: A rigorous theoretical extension of the BTR Lemma for stochastic tokenizers and experimental validation of its effectiveness.

## Limitations

- The method relies on deterministic tokenizers and unique string encodings; non-deterministic tokenizers break the exact mapping
- O(nℓ) complexity claims depend on tokenizer properties and may not hold for all tokenization schemes
- Byte-level model ensembling benefits may not generalize to arbitrary model combinations with different tokenization schemes
- Long-term cumulative effects of tokenization bias on generation quality remain unexplored

## Confidence

**High Confidence:** The existence of tokenization bias and its mathematical characterization through the Byte-Token Representation Lemma. The empirical results showing 18% improvement on FIM coding benchmarks are well-supported by the experimental setup and multiple model evaluations.

**Medium Confidence:** The efficiency claims of the cover encoding search algorithm (O(nℓ) complexity) and the exact elimination of tokenization bias through the proposed sampling method. While the theoretical analysis appears sound, the practical runtime characteristics depend on implementation details and tokenizer behavior.

**Low Confidence:** The universality of byte-level model ensembling benefits across all model combinations and task types. The 3.7% improvement figure comes from specific model combinations and may not generalize to arbitrary model ensembles.

## Next Checks

1. **Algorithm Correctness Verification:** Implement the cover encoding search algorithm on the simple 1st-order Markov chain example provided in the paper with (α,β)=(0.4,0.3) and verify that the recovered byte-level probabilities match the theoretical predictions exactly.

2. **Tokenizer Robustness Testing:** Test the method on tokenizers with known non-deterministic behavior (such as BPE dropout) or those with large vocabularies to measure how tokenization bias changes and whether the O(nℓ) complexity claim holds under these conditions.

3. **Cross-Model Ensemble Generalization:** Create ensembles combining models with different tokenization schemes (e.g., GPT-2 with BPE and a model using SentencePiece) and measure whether byte-level mapping preserves statistical equivalence and whether ensemble benefits still materialize.