---
ver: rpa2
title: 'AI Oversight and Human Mistakes: Evidence from Centre Court'
arxiv_id: '2401.16754'
source_url: https://arxiv.org/abs/2401.16754
tags:
- oversight
- hawk-eye
- ball
- line
- when
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of AI oversight on human decision-making
  using data from professional tennis, where Hawk-Eye review was introduced to overrule
  umpire calls. The researchers find that umpires lowered their overall mistake rate
  by 8% after Hawk-Eye review was implemented.
---

# AI Oversight and Human Mistakes: Evidence from Centre Court

## Quick Facts
- arXiv ID: 2401.16754
- Source URL: https://arxiv.org/abs/2401.16754
- Reference count: 25
- Umpires lowered their overall mistake rate by 8% after Hawk-Eye review was implemented

## Executive Summary
This study examines the impact of AI oversight on human decision-making using data from professional tennis, where Hawk-Eye review was introduced to overrule umpire calls. The researchers find that umpires lowered their overall mistake rate by 8% after Hawk-Eye review was implemented. However, they also discovered that umpires increased the rate at which they called balls in, leading to a shift from Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). The study structurally estimates the psychological costs of being overruled by AI and finds that umpires care 37% more about Type II errors under AI oversight. These findings suggest that while AI oversight can improve overall performance, it can also lead to unintended behavioral distortions in human decision-making.

## Method Summary
The study analyzes data from professional tennis tournaments before and after the introduction of Hawk-Eye review. The researchers used three data sources: Hawk-Eye base data (ball bounce locations, serving player, ongoing score, point winner), Challenge data (challenges during initial years of Hawk-Eye), and video auditing data for validation. They employed OLS regressions to estimate the effect of AI oversight on umpires' performance and structurally estimated psychological costs of being overruled by AI using a model of rational inattentive umpires. The analysis focused on identifying incorrect calls using four specific criteria and examining the types and rates of errors before and after AI implementation.

## Key Results
- Umpires lowered their overall mistake rate by 8% after Hawk-Eye review implementation
- Umpires increased the rate at which they called balls in, shifting from Type II to Type I errors
- Structurally estimated psychological costs show umpires care 37% more about Type II errors under AI oversight

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI oversight reduces overall error rates through rational inattention.
- Mechanism: Umpires reduce attention to close calls because AI oversight creates psychological costs for being overruled.
- Core assumption: Umpires are rationally inattentive and respond to the introduction of Hawk-Eye by lowering effort where AI oversight is most likely.
- Evidence anchors:
  - [abstract]: "umpires lowered their overall mistake rate by 8% after the introduction of Hawk-Eye review, in line with rational inattention given psychological costs to being overruled by AI."
  - [section]: "We structurally estimate the psychological costs of being overruled by AI using a model of rational inattentive umpires."
- Break condition: If umpires do not respond rationally to the introduction of AI oversight, or if the psychological costs of being overruled are negligible.

### Mechanism 2
- Claim: AI oversight creates a shift in error types due to asymmetric psychological costs.
- Mechanism: Umpires avoid Type II errors (calling a ball out when in) more than Type I errors (calling a ball in when out) due to the higher psychological cost of the former.
- Core assumption: Umpires care more about Type II errors under AI oversight, leading to a distortion in their decision-making.
- Evidence anchors:
  - [abstract]: "umpires increased the rate at which they called balls in, leading to a shift from Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out)."
  - [section]: "We structurally estimate a lower bound on the psychological costs being overruled by AI... and our results suggest that because of these costs, umpires cared twice as much about Type II errors (calling a ball out when in) after the introduction of Hawk-Eye review."
- Break condition: If umpires do not perceive a difference in the psychological costs of Type I and Type II errors under AI oversight.

### Mechanism 3
- Claim: AI oversight has heterogeneous effects on different groups of players.
- Mechanism: Umpires reduce controversial mistakes for top players while increasing less costly mistakes for other players.
- Core assumption: Umpires are influenced by the social status of players and adjust their error rates accordingly.
- Evidence anchors:
  - [abstract]: "umpires increased the rate at which they called balls in, leading to a shift from Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out)."
  - [section]: "We do not observe strong evidence that the introduction of Hawk-Eye benefited one group over the other in aggregate terms. However, what we see is that umpires reduce controversial mistakes for Superstar players while seemingly compensating the rest of the players by increasing less costly mistakes in their favor."
- Break condition: If umpires do not differentiate between top and other players in their error rates.

## Foundational Learning

- Concept: Rational inattention
  - Why needed here: To understand how umpires reduce effort in response to AI oversight.
  - Quick check question: What is the key assumption of the rational inattention model used in this paper?

- Concept: Type I and Type II errors
  - Why needed here: To understand the shift in error types observed in the data.
  - Quick check question: What is the difference between a Type I error and a Type II error in the context of tennis umpiring?

- Concept: Asymmetric costs
  - Why needed here: To understand why umpires respond differently to Type I and Type II errors under AI oversight.
  - Quick check question: What is the key assumption about the costs of Type I and Type II errors under AI oversight?

## Architecture Onboarding

- Component map: Hawk-Eye Base -> Challenge -> Video Auditing -> OLS Regression -> Structural Estimation
- Critical path: 1. Merge data sources, 2. Estimate effects of AI oversight on overall mistake rate, 3. Estimate effects on Type I and Type II errors, 4. Structurally estimate AI oversight penalty
- Design tradeoffs: Use of observed challenge rates vs. true challenge rates, assumption of linear cost of attention vs. other cost functions
- Failure signatures: Inconsistent merging of data sources, non-robust estimates of AI oversight effects, non-robust structural estimates of AI oversight penalty
- First 3 experiments: 1. Check robustness of merging algorithm by comparing to video auditing, 2. Estimate effects of AI oversight on overall mistake rate using different specifications, 3. Estimate effects on Type I and Type II errors using different distance thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the psychological costs of being overruled by AI vary across different high-stakes domains (e.g., law, medicine, air traffic control) compared to professional tennis?
- Basis in paper: [explicit] The paper suggests that the psychological costs of being overruled by AI can vary across settings, but does not empirically compare these costs across different domains.
- Why unresolved: The paper only provides evidence from professional tennis and does not examine other domains where AI oversight is used.
- What evidence would resolve it: Comparative studies measuring psychological costs of AI oversight in multiple high-stakes domains, using consistent methodologies to quantify these costs.

### Open Question 2
- Question: What are the long-term effects of AI oversight on human decision-making quality and psychological well-being in professional settings?
- Basis in paper: [inferred] The paper provides short-term evidence of AI oversight effects but does not examine long-term impacts on decision-making quality or psychological well-being.
- Why unresolved: The study's data covers only a few years after AI implementation, and does not track long-term changes in performance or psychological costs.
- What evidence would resolve it: Longitudinal studies tracking decision-making quality and psychological well-being of professionals over extended periods (5+ years) after AI oversight implementation.

### Open Question 3
- Question: How do different AI oversight implementation guidelines affect human decision-making distortions and overall performance?
- Basis in paper: [explicit] The paper suggests that the particular implementation guidelines of AI oversight can shift incentives for decision-makers, but does not empirically test different implementation approaches.
- Why unresolved: The study examines a single AI oversight implementation (Hawk-Eye in tennis) and does not explore alternative implementation strategies.
- What evidence would resolve it: Controlled experiments or field studies comparing decision-making quality and distortions under different AI oversight implementation guidelines across various settings.

## Limitations
- The findings rely on the assumption that umpires are rationally inattentive, which may not fully capture the complexity of human decision-making under AI oversight
- The study focuses on professional tennis, limiting generalizability to other domains where AI oversight is applied
- The structural estimation of psychological costs is based on a model that may not capture all relevant factors influencing umpire behavior

## Confidence
- High Confidence: The finding that AI oversight reduces overall error rates by 8% is supported by robust data and analysis. The shift from Type II to Type I errors is also clearly demonstrated in the data.
- Medium Confidence: The structural estimation of psychological costs and the interpretation of these costs as driving the observed behavioral changes are more speculative. While the model is plausible, there may be other factors influencing umpire behavior that are not captured.
- Low Confidence: The claim that umpires reduce controversial mistakes for top players while increasing less costly mistakes for others is based on limited evidence and may be influenced by unobserved factors.

## Next Checks
1. Replicate the structural estimation using alternative models to estimate the psychological costs of being overruled by AI and compare results
2. Apply the analysis to other sports or domains where AI oversight is used to test generalizability
3. Investigate whether other factors, such as umpire fatigue or pressure, may influence the observed patterns of error rates and error types