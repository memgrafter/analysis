---
ver: rpa2
title: BERTs are Generative In-Context Learners
arxiv_id: '2406.04823'
source_url: https://arxiv.org/abs/2406.04823
tags:
- language
- deberta
- gpt-3
- learning
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that masked language models like DeBERTa can perform
  in-context learning, a capability previously thought to be exclusive to causal language
  models like GPT. The authors introduce a simple inference technique that reformats
  inputs to enable DeBERTa to generate text and rank completions without additional
  training or architectural changes.
---

# BERTs are Generative In-Context Learners

## Quick Facts
- arXiv ID: 2406.04823
- Source URL: https://arxiv.org/abs/2406.04823
- Reference count: 40
- Key outcome: DeBERTa can perform in-context learning without additional training or architectural changes, outperforming GPT-3 on average on SuperGLUE tasks

## Executive Summary
This paper demonstrates that masked language models like DeBERTa can perform generative in-context learning, a capability previously thought exclusive to causal language models like GPT-3. The authors introduce a simple inference technique that reformats inputs with [MASK] tokens to enable DeBERTa to generate text and rank completions without additional training or architectural changes. Evaluation shows DeBERTa performs competitively on language understanding tasks, outperforming GPT-3 on average (68.4 vs 68.9 accuracy on SuperGLUE with 1.5B parameters), but trails on translation and closed-book question answering. DeBERTa also demonstrates length generalization beyond its training length due to relative positional embeddings.

## Method Summary
The paper introduces a simple inference technique to enable masked language models like DeBERTa to perform generative in-context learning. This involves reformulating prompts with [MASK] tokens and [SEP] tokens to allow autoregressive generation. The method also includes a modified pseudo-log-likelihood scoring approach for ranking text sequences by masking additional context tokens. No additional training or architectural changes are required - the approach works directly with pretrained DeBERTa checkpoints.

## Key Results
- DeBERTa performs in-context learning without additional training, reformulating inputs with [MASK] tokens
- On SuperGLUE tasks, DeBERTa (1.5B) achieves 68.4 accuracy, outperforming GPT-3 (68.9) on average
- DeBERTa demonstrates length generalization beyond 512 tokens due to relative positional embeddings
- MLMs and causal LMs show complementary strengths - MLMs excel at language understanding while causal LMs lead in translation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masked language models can perform generative in-context learning by reformulating prompts with [MASK] tokens and [SEP] tokens
- Mechanism: The MLM is adapted for generation by appending [MASK] tokens to prompts, predicting each sequentially, and adding extra [MASK] tokens to avoid premature sequence termination
- Core assumption: The MLM's pretraining on fill-in-the-blank tasks provides sufficient generative capability when prompts are properly formatted
- Evidence anchors: Abstract states the simple inference technique enables DeBERTa to perform generative tasks without additional training or architectural changes

### Mechanism 2
- Claim: Masked language models can rank text sequences using a modified pseudo-log-likelihood score
- Mechanism: The ranking method masks additional context tokens to reduce local dependencies while maintaining global bidirectional context, improving accuracy over standard PLL
- Core assumption: Masking additional tokens reduces the overestimation of multi-token word likelihoods while preserving enough context for accurate scoring
- Evidence anchors: Abstract mentions a similar approach is used to retrieve a pseudo-log-likelihood score for ranking sequences

### Mechanism 3
- Claim: Masked language models with relative positional embeddings can generalize to sequences longer than their training length
- Mechanism: DeBERTa's use of relative positional embeddings with logarithmic buckets allows it to handle longer sequences without the performance collapse seen in absolute positional encoding
- Core assumption: Relative positional embeddings are inherently more flexible for length generalization than absolute positional encodings
- Evidence anchors: Abstract states DeBERTa demonstrates length generalization beyond its training length due to relative positional embeddings

## Foundational Learning

- Concept: Autoregressive vs. Masked Language Modeling
  - Why needed here: Understanding the fundamental difference between these training objectives is crucial for grasping why MLMs were thought incapable of generation
  - Quick check question: What is the key difference between how causal LMs and masked LMs process context during training?

- Concept: In-context learning
  - Why needed here: This paper demonstrates that in-context learning is not exclusive to causal LMs, so understanding what it is helps appreciate the significance
  - Quick check question: How does in-context learning differ from traditional finetuning approaches?

- Concept: Positional encoding
  - Why needed here: The paper highlights how relative positional encodings enable length generalization, which is key to understanding DeBERTa's advantages
  - Quick check question: Why might relative positional encodings be more flexible than absolute positional encodings for handling varying sequence lengths?

## Architecture Onboarding

- Component map: Input formatting layer -> Masked language model (DeBERTa) -> Generation layer -> Ranking layer -> Evaluation layer
- Critical path: Prompt → Input formatting → MLM prediction → Output generation/ranking → Evaluation
- Design tradeoffs:
  - Speed vs. capability: MLM generation is slower than causal LM generation due to inability to cache attention vectors
  - Accuracy vs. simplicity: Masking additional tokens improves PLL scoring but adds computational overhead
  - Generalization vs. training: Relative positional embeddings enable length generalization but may be less precise for shorter sequences
- Failure signatures:
  - Generation produces repetitive or incoherent text → Check [MASK] token formatting and number of additional masks
  - Ranking fails to distinguish correct answers → Verify additional context masking is properly implemented
  - Performance degrades on long sequences → Confirm relative positional embedding functionality
- First 3 experiments:
  1. Test basic text generation with a simple prompt and verify the output follows the prompt logically
  2. Compare PLL scores for a set of completions to ensure ranking capability works
  3. Evaluate performance on a short sequence length task versus a task requiring longer context to test length generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of masked to causal training objectives for hybrid language models to maximize in-context learning performance across different task categories?
- Basis in paper: The authors note that masked and causal language models have "complementary strengths" and suggest "promising hybrid approaches that combine the strengths of both objectives"
- Why unresolved: The paper only demonstrates that both architectures can perform in-context learning with distinct advantages, but does not explore hybrid training approaches or determine optimal mixing ratios
- What evidence would resolve it: Empirical comparison of various hybrid training approaches across the same task categories evaluated in the paper

### Open Question 2
- Question: How does the training data efficiency of masked language models compare to causal models when developing in-context learning capabilities under controlled conditions?
- Basis in paper: The authors note that DeBERTa was "pretrained on a relatively small and clean text corpus... almost an order of magnitude less data than what was used to pretrain GPT-3" yet achieved comparable in-context learning performance
- Why unresolved: While the authors speculate that MLMs might be "more data-efficient," they acknowledge this "would however need to be evaluated with a comprehensive study"
- What evidence would resolve it: Controlled experiments training causally and masked models on datasets of varying sizes

## Limitations

- Architecture-specific findings: Results are based solely on DeBERTa with relative positional embeddings, which may not generalize to other MLMs
- Limited scope of tasks: Evaluation covers SuperGLUE, translation, and closed-book QA, but not broader in-context learning applications
- Inference efficiency: Paper acknowledges MLM generation is slower than causal LM generation but doesn't quantify this performance difference

## Confidence

- High Confidence: Core claim that MLMs can perform in-context learning through simple inference modifications is well-supported by empirical results
- Medium Confidence: Claim that MLMs and causal LMs have complementary strengths is reasonable but based on limited task set
- Low Confidence: Assertion that relative positional embeddings are primary reason for length generalization is plausible but not definitively proven

## Next Checks

1. Test the inference method with other MLMs (e.g., BERT, RoBERTa, ELECTRA) that use different positional encoding schemes to verify findings are not DeBERTa-specific

2. Measure wall-clock time difference between MLM-based and causal LM-based generation for equivalent outputs, and explore optimizations like partial caching or adaptive masking strategies

3. Evaluate the method on a broader range of in-context learning tasks including code generation, mathematical reasoning, and multimodal tasks to assess true scope of MLMs' generative capabilities