---
ver: rpa2
title: 'Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone
  Language Models'
arxiv_id: '2410.17389'
source_url: https://arxiv.org/abs/2410.17389
tags:
- reward
- state
- rewards
- ranking
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to incorporate noisy LLM feedback
  into reinforcement learning by learning a potential-based scoring function over
  repeated LLM-generated preference rankings. The key idea is to issue uninformative
  rewards for states in which the LLM is uncertain, which naturally emerges when the
  LLM's rankings are inconsistent.
---

# Navigating Noisy Feedback: Enhancing Reinforcement Learning with Error-Prone Language Models

## Quick Facts
- arXiv ID: 2410.17389
- Source URL: https://arxiv.org/abs/2410.17389
- Reference count: 19
- Primary result: Potential difference rewards from noisy LLM rankings improve RL performance over direct reward methods

## Executive Summary
This paper addresses the challenge of incorporating noisy language model (LLM) feedback into reinforcement learning by proposing a potential-based scoring function approach. The key insight is that inconsistent LLM rankings naturally produce uninformative rewards, which helps filter out noise during training. The method involves querying LLMs multiple times for state pair rankings to measure confidence, then training a scoring model that converts these rankings into potential-based rewards for RL agents. Empirical results show improved convergence speed and policy returns compared to baselines, particularly in Grid World environments where the method achieves up to 98% of ground truth performance.

## Method Summary
The method trains a scoring model to convert LLM-generated state rankings into potential-based rewards for RL agents. It uses multi-query LLM feedback to measure ranking confidence, then applies a confidence-weighted loss to train the scoring model. The resulting rewards are computed as differences between successive state potentials, which naturally become uninformative when LLM rankings are inconsistent. This approach is tested across Grid World and MuJoCo environments using various LLMs including GPT-4, Llama-3, and Mixtral, with performance measured against direct reward and default reward baselines.

## Key Results
- Potential difference rewards improve convergence speed and policy returns over direct reward methods
- The method achieves up to 98% of ground truth ranking performance in Grid World environments
- Multi-query LLM feedback (1-10 queries) shows improved performance with increased queries, though at computational cost
- Method shows robustness to reward scaling parameters compared to direct reward approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inconsistent LLM rankings naturally produce uninformative rewards via potential-based scoring
- Mechanism: When the LLM is uncertain between two states, the confidence-weighted loss pushes their scores toward equality, making their potential difference near zero
- Core assumption: LLM certainty is expressed through output consistency (multiple queries yield same ranking)
- Evidence anchors:
  - [abstract] "We theoretically show that inconsistent rankings lead to uninformative rewards with our approach"
  - [section 4.1] "Based on the definition of feedback confidence, we derive an equivalent form of the RLHF loss based on ranking confidence and consistency"
  - [corpus] No direct corpus evidence for this specific mechanism; requires understanding of LLM confidence calibration literature
- Break condition: If LLM rankings remain consistent even when uncertain, this mechanism fails to produce uninformative rewards

### Mechanism 2
- Claim: Potential difference rewards are less sensitive to reward scale than direct reward methods
- Mechanism: Using σψ(st) - σψ(st-1) as reward automatically normalizes scale and incorporates uncertainty through score convergence
- Core assumption: Score differences between successive states capture meaningful action information while filtering noise
- Evidence anchors:
  - [section 4.2] "Our potential difference formulation helps alleviate this issue as 1) uncertain states converge to the same score value so the impact of noisy rankings no longer needs to be mitigated through post-processing"
  - [section 5.3.1] "Our method shows robustness to the choice of the flat step penalty, as the curves of penalty variances are less divergent"
  - [corpus] Weak corpus support; needs empirical validation in diverse domains
- Break condition: If score function is too noisy or poorly trained, potential differences may not capture useful action information

### Mechanism 3
- Claim: Multi-query ranking reduces noise by measuring output consistency
- Mechanism: Querying the same state pair multiple times and averaging results increases confidence measurement accuracy, especially for lower-performing LLMs
- Core assumption: LLM outputs become more consistent with repeated queries on the same input
- Evidence anchors:
  - [section 5.4.1] "The result demonstrates that with more and more queries, the potential-difference reward gradually improves the training performance"
  - [section 4.1] "It has been shown that the certainty of LLM predictions can be quantified by issuing the same query multiple times and measuring the consistency of the predictions"
  - [corpus] No direct corpus evidence for this specific mechanism; requires understanding of LLM calibration literature
- Break condition: If LLM exhibits high variance even with repeated queries, this mechanism fails to improve confidence measurement

## Foundational Learning

- Concept: Preference-based reinforcement learning with Bradley-Terry model
  - Why needed here: This work builds on RLHF framework but replaces human with LLM feedback, requiring understanding of how preference rankings translate to reward functions
  - Quick check question: How does the Bradley-Terry model convert pairwise preferences into a probability distribution for learning?

- Concept: Potential-based reward shaping
  - Why needed here: The core innovation uses state scores as potentials, with rewards derived from differences between successive states
  - Quick check question: What are the mathematical conditions for a potential function to preserve optimal policy in MDPs?

- Concept: LLM confidence calibration through output consistency
  - Why needed here: The method relies on measuring LLM certainty by querying the same state pair multiple times
  - Quick check question: How does repeated querying help distinguish between high-confidence and low-confidence LLM predictions?

## Architecture Onboarding

- Component map: State pair sampler -> LLM feedback module -> Scoring model trainer -> RL agent -> Multi-query handler

- Critical path:
  1. Sample state pairs from environment
  2. Query LLM multiple times for each pair
  3. Calculate confidence scores from query consistency
  4. Train scoring model with confidence-weighted loss
  5. Generate rewards as potential differences
  6. Train RL agent with these rewards

- Design tradeoffs:
  - Single vs multi-query: Multi-query increases confidence accuracy but costs more LLM calls
  - Scoring model complexity: More complex models may overfit noisy rankings but capture richer state representations
  - Reward normalization: Potential difference naturally scales rewards but may reduce sensitivity to subtle state differences

- Failure signatures:
  - RL agent fails to converge: Likely scoring model poorly trained or LLM rankings too noisy
  - High variance in learning curves: May indicate insufficient multi-query sampling or unstable scoring model training
  - Policy exploits scoring artifacts: Scoring model may have learned spurious correlations in training data

- First 3 experiments:
  1. Grid World NoLock with GT rankings: Verify basic mechanism works with perfect feedback
  2. Grid World Lock with Mixtral (single query): Test performance degradation with noisy LLM feedback
  3. Grid World Lock with Mixtral (multi-query): Validate multi-query improvement mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the potential difference reward method scale to more complex, realistic environments compared to the simpler Grid World and MuJoCo environments tested in this paper?
- Basis in paper: [explicit] The paper states "Our current analysis is limited to relatively simple discrete and continuous environments" and discusses exploring "more complex, realistic environments" in future work.
- Why unresolved: The paper only tested on relatively simple environments, leaving uncertainty about performance in more complex settings with richer state and action spaces.
- What evidence would resolve it: Empirical results showing performance of the method on complex, realistic environments with visual inputs and more intricate dynamics.

### Open Question 2
- Question: What is the optimal number of queries to make to an LLM for ranking state pairs to balance between accuracy and computational cost?
- Basis in paper: [explicit] The paper discusses multi-query approaches and shows that more queries generally improve performance, but notes this comes with computational cost. They test 1-10 queries synthetically.
- Why unresolved: The paper shows improvement with more queries but doesn't identify an optimal point balancing accuracy gains against computational expense.
- What evidence would resolve it: Empirical analysis identifying the point of diminishing returns where additional queries provide minimal accuracy improvement relative to computational cost.

### Open Question 3
- Question: How does the method perform in non-fully observable environments where state information is incomplete or noisy?
- Basis in paper: [explicit] The paper assumes "the environment is fully observable" and acknowledges this assumption may be violated in real-world scenarios.
- Why unresolved: The method relies on accurate state information for ranking, but real-world environments often have partial observability or noisy observations.
- What evidence would resolve it: Empirical results showing performance when applied to partially observable environments or environments with noisy state information.

## Limitations
- Limited to relatively simple environments (Grid World, MuJoCo) with discrete and continuous state spaces
- Assumes fully observable environments, which may not hold in real-world scenarios
- Computational overhead of multiple LLM queries not thoroughly analyzed for practical deployment

## Confidence
- Handling noisy LLM feedback through potential-based scoring: High confidence
- Robustness to reward scaling parameters: Medium confidence
- Multi-query mechanism effectiveness: Medium confidence

## Next Checks
1. **Scale robustness test**: Systematically vary step penalties across multiple orders of magnitude in both Grid World and MuJoCo environments to better characterize the method's robustness to reward scaling.

2. **Extreme noise scenario**: Test the method with intentionally corrupted rankings (e.g., randomizing a percentage of LLM outputs) to establish performance bounds and identify failure modes.

3. **Query efficiency analysis**: Conduct an ablation study varying the number of queries per state pair to determine the optimal tradeoff between confidence accuracy and computational cost.