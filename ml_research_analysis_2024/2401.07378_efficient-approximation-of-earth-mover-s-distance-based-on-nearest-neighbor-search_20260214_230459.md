---
ver: rpa2
title: Efficient approximation of Earth Mover's Distance Based on Nearest Neighbor
  Search
arxiv_id: '2401.07378'
source_url: https://arxiv.org/abs/2401.07378
tags:
- nns-emd
- image
- images
- distance
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NNS-EMD, a novel approach for approximating
  the Earth Mover's Distance (EMD) using Nearest Neighbor Search (NNS). The method
  aims to achieve high accuracy, low time complexity, and high memory efficiency.
---

# Efficient approximation of Earth Mover's Distance Based on Nearest Neighbor Search

## Quick Facts
- arXiv ID: 2401.07378
- Source URL: https://arxiv.org/abs/2401.07378
- Reference count: 40
- Primary result: NNS-EMD achieves 44x to 135x speedup over exact EMD while maintaining high accuracy

## Executive Summary
This paper introduces NNS-EMD, a novel approach for approximating the Earth Mover's Distance (EMD) using Nearest Neighbor Search (NNS). The method aims to achieve high accuracy, low time complexity, and high memory efficiency. NNS-EMD significantly reduces the number of data points compared in each iteration, offering opportunities for parallel processing. The algorithm is accelerated via vectorization on GPU, particularly beneficial for large datasets. Experimental results show that NNS-EMD can be 44x to 135x faster than the exact EMD implementation and achieves superior accuracy, speedup, and memory efficiency over existing approximate EMD methods.

## Method Summary
NNS-EMD is an iterative algorithm that approximates EMD by replacing exhaustive pairwise distance calculations with nearest neighbor searches. The method uses GPU vectorization to parallelize the nearest neighbor operations and batch processing to handle multiple queries simultaneously. In each iteration, NNS-EMD finds the nearest neighbor for each consumer point, calculates the flow and cost, updates the weights, and eliminates zero-weight points. The algorithm converges when all points are consumed. Two protocols are proposed: Greedy Protocol (GP) and Random Protocol (RP), with GP generally providing better accuracy.

## Key Results
- NNS-EMD achieves 44x to 135x speedup over exact EMD implementation
- The method demonstrates up to 5% higher accuracy compared to state-of-the-art EMD approximations on image classification and retrieval tasks
- NNS-EMD offers superior memory efficiency compared to existing approximate EMD methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NNS-EMD reduces computational complexity by replacing exhaustive pairwise distance calculations with nearest neighbor searches.
- Mechanism: Instead of computing distances between all supplier-consumer pairs (O(n²)), NNS-EMD only compares each consumer point to its nearest supplier neighbor, drastically reducing the number of comparisons.
- Core assumption: Nearest neighbor distances are close approximations of the global optimal transport distances.
- Evidence anchors:
  - [abstract] "The NNS operation reduces the number of data points compared in each NNS iteration"
  - [section] "Our experimental results indicate that NNS-EMD significantly outperforms the exact EMD implementation, achieving 44×to 135×speedup"
  - [corpus] Weak evidence - corpus papers focus on other ANN/EMD approximations, not this specific nearest-neighbor reduction mechanism
- Break condition: If nearest neighbor distances diverge significantly from optimal transport distances, the approximation error becomes unacceptable and accuracy drops below acceptable thresholds.

### Mechanism 2
- Claim: GPU vectorization and batch processing enable parallel computation of multiple NNS operations simultaneously.
- Mechanism: The algorithm allocates each consumer's NNS operation to separate GPU threads, transforming O(n²) complexity to O(n) per iteration, with batch processing handling multiple queries simultaneously.
- Core assumption: Distance computations between different supplier-consumer pairs are independent and can be computed in parallel.
- Evidence anchors:
  - [section] "We enhance NNS-EMD with both parallelized NNS operation and batch processing on GPU, leveraging parallelism at both the data point level and the dataset level"
  - [section] "As shown in Tab. 5, vectorization leads to a significant speedup compared to the non-vectorized implementation"
  - [corpus] Weak evidence - corpus papers mention ANN search but don't specifically address this GPU vectorization approach for EMD
- Break condition: When GPU memory becomes insufficient for batch processing, or when the overhead of parallelization exceeds the benefits for small datasets.

### Mechanism 3
- Claim: Weight update and elimination of zero-weight points reduces the active dataset size in subsequent iterations.
- Mechanism: After each iteration, points with zero remaining weight are eliminated from consideration, continuously shrinking the active dataset and reducing computation in later iterations.
- Core assumption: The elimination of zero-weight points maintains the correctness of the approximation while reducing computational load.
- Evidence anchors:
  - [section] "When a point's weight is fully exhausted or consumed, it is eliminated, and only the points with positive weights will participate in the next iteration"
  - [section] "The weight update operation not only ensures the convergence of the NNS-EMD algorithm until all the points are consumed, but also contributes to efficient computation"
  - [corpus] No direct evidence - this specific weight elimination mechanism is unique to NNS-EMD
- Break condition: If the weight elimination process removes points too aggressively, potentially skipping important transport paths and degrading approximation quality.

## Foundational Learning

- Concept: Earth Mover's Distance (EMD) and Optimal Transport
  - Why needed here: NNS-EMD is fundamentally an approximation algorithm for EMD, so understanding the exact formulation is crucial
  - Quick check question: What are the constraints (2) and (3) that define the feasible flow in the EMD optimization problem?

- Concept: GPU vectorization and parallel computing
  - Why needed here: The performance gains in NNS-EMD heavily rely on GPU parallelism and batch processing
  - Quick check question: How does the independence of distance computations between supplier-consumer pairs enable GPU parallelization?

- Concept: Nearest Neighbor Search algorithms
  - Why needed here: NNS-EMD's core efficiency comes from replacing exhaustive comparisons with NN searches
  - Quick check question: What distance metric does NNS-EMD use for NN searches, and why was this choice made over alternatives?

## Architecture Onboarding

- Component map: NNS operation (GPU-parallelized) -> Flow and cost calculation (CPU/GPU) -> Weight update/elimination (CPU)
- Critical path: The critical path is the NNS operation in each iteration. Since each iteration depends on the previous one's weight updates, the GPU must complete the NNS before the CPU can update weights and prepare for the next iteration.
- Design tradeoffs: The algorithm trades some approximation accuracy for significant speedup. Using L2 distance instead of L1 provides better accuracy but slightly higher computational cost. The greedy protocol improves accuracy over random protocol but adds ranking overhead.
- Failure signatures: If the algorithm converges too quickly (few iterations), it may indicate overly aggressive weight elimination or poor NN approximations. If speedup is minimal, the dataset may be too small for GPU parallelism to be beneficial, or the batch size may be improperly configured.
- First 3 experiments:
  1. Test NNS-EMD on a small synthetic dataset (100 points) to verify correctness against exact EMD and observe the convergence behavior.
  2. Profile the GPU utilization during NNS operations to identify bottlenecks and optimize batch sizes.
  3. Compare L1 vs L2 distance metrics on a medium dataset to quantify the accuracy tradeoff and determine the optimal choice for different data distributions.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions. However, it raises some implicit questions about the generalizability of NNS-EMD to other data types beyond images and the impact of varying batch sizes on performance.

## Limitations
- The experimental evaluation focuses primarily on image classification and retrieval tasks, with limited exploration of the method's performance on other data types or distributions.
- The claims of superior accuracy compared to state-of-the-art methods are supported by experimental results, but the paper lacks a detailed theoretical analysis of the approximation error bounds.
- The method's performance on imbalanced datasets, where the number of suppliers and consumers differ significantly, is not explored.

## Confidence
- **Speedup and Memory Efficiency Claims: High** - Well-supported by experimental results and theoretical complexity analysis
- **Accuracy Claims: Medium** - Supported by experiments but lack theoretical error bounds
- **Generalizability Claims: Low** - Limited to image tasks, unclear how well it transfers to other domains

## Next Checks
1. **Theoretical Analysis**: Derive and validate approximation error bounds for NNS-EMD compared to exact EMD, quantifying the tradeoff between accuracy and efficiency under different data distributions.

2. **Ablation Study**: Conduct a systematic ablation study to isolate the contributions of each mechanism (nearest neighbor reduction, GPU vectorization, and weight elimination) to the overall performance improvements.

3. **Domain Generalization**: Test NNS-EMD on non-image datasets (e.g., text, graphs, time series) to assess its generalizability and identify potential limitations or necessary adaptations for different data types.