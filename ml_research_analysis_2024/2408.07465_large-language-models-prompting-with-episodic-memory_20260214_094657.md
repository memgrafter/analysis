---
ver: rpa2
title: Large Language Models Prompting With Episodic Memory
arxiv_id: '2408.07465'
source_url: https://arxiv.org/abs/2408.07465
tags:
- examples
- prompt
- in-context
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces POEM, a prompt optimization method that leverages
  episodic memory to improve in-context learning (ICL) performance. The core idea
  is to use episodic memory to store the performance of different permutations of
  ICL examples for training inputs, and then during testing, retrieve the best permutation
  for each test query based on the most similar training examples in memory.
---

# Large Language Models Prompting With Episodic Memory

## Quick Facts
- arXiv ID: 2408.07465
- Source URL: https://arxiv.org/abs/2408.07465
- Authors: Dai Do; Quan Tran; Svetha Venkatesh; Hung Le
- Reference count: 40
- Key outcome: POEM improves in-context learning performance by 5.3% over recent techniques through episodic memory-based prompt optimization

## Executive Summary
This paper introduces POEM, a novel approach for optimizing in-context learning (ICL) in large language models. The method uses episodic memory to store the performance of different permutations of demonstration examples for training inputs, then retrieves the best permutation for each test query based on similarity to training examples. By avoiding complex RL optimization and focusing on similarity-ranked example orderings, POEM achieves significant improvements across multiple text classification and language understanding tasks.

## Method Summary
POEM formulates prompt optimization as a reinforcement learning problem where the state is the test query, the action is the ordering of in-context examples, and the reward is the performance of the resulting prompt. During training, the method stores state-action-reward tuples in episodic memory. At test time, it uses a nearest-neighbor estimator to find the top-k most similar training examples and selects the action with the highest estimated value. The key innovation is encoding actions as permutations of similarity ranks rather than specific example sequences, reducing the action space from M!/(M-m)! to m! and encouraging generalization.

## Key Results
- POEM outperforms TEMPERA and RLPrompt by over 5.3% on average across multiple text classification tasks
- The method consistently improves ICL performance compared to conventional heuristic ordering methods
- POEM adapts well to broader language understanding tasks including commonsense reasoning and question answering
- The approach eliminates the need for complex RL optimization while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: POEM improves ICL by optimizing demonstration example ordering based on similarity to test queries
- Mechanism: Stores performance of different example permutations for training inputs, retrieves best permutation for test queries based on similar training examples
- Core assumption: Similarity between test queries and training examples predicts optimal example ordering
- Evidence anchors: [abstract] selecting sequence yielding highest total rewards from top-k similar training examples; [section] memory serves as non-parametric nearest-neighbors model
- Break condition: If similarity metric doesn't correlate with actual ordering performance, or memory lacks similar training examples

### Mechanism 2
- Claim: POEM achieves efficient optimization by using similarity-ranked action encoding
- Mechanism: Encodes actions as permutations of similarity ranks rather than specific examples, reducing action space from M!/(M-m)! to m!
- Core assumption: Relative similarity of examples to test query matters more than specific identities
- Evidence anchors: [abstract] permutation focuses on arrangement of example rank rather than specific content; [section] encoding operates within similarity rank space
- Break condition: If similarity-ranked encoding fails to capture ordering nuances, or m! remains too large for efficient search

### Mechanism 3
- Claim: POEM provides stable performance using non-parametric nearest-neighbor estimation for novel states
- Mechanism: Uses weighted sum of rewards from k most similar states in memory to estimate values for novel test states
- Core assumption: Performance of orderings for similar training examples predicts performance for novel test examples
- Evidence anchors: [abstract] nearest-neighbor estimator used for novel states; [section] employing nearest-neighbor estimator for unseen states
- Break condition: If nearest-neighbor estimator provides inaccurate value estimates, or similarity metric fails to correlate with ordering performance

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: POEM optimizes ICL by finding best ordering of demonstration examples for given test queries
  - Quick check question: What is the main idea behind in-context learning, and how does it differ from traditional fine-tuning approaches?

- Concept: Reinforcement learning (RL) and episodic memory
  - Why needed here: POEM formulates prompt optimization as RL problem using episodic memory to store and retrieve performance of different orderings
  - Quick check question: How does episodic memory usage in POEM differ from traditional RL approaches, and what advantages does it offer for prompt optimization?

- Concept: Nearest-neighbor estimation and similarity metrics
  - Why needed here: POEM uses nearest-neighbor estimator with cosine similarity to handle novel test states and generalize ordering performance
  - Quick check question: Why is similarity metric choice important for nearest-neighbor estimator in POEM, and how might different metrics affect method performance?

## Architecture Onboarding

- Component map: State encoder -> Example selector -> Action encoder -> Episodic memory -> Memory reader -> Prompt constructor
- Critical path:
  1. Encode test query using state encoder
  2. Retrieve m most similar in-context examples using example selector
  3. Find top-k most similar states in episodic memory using memory reader
  4. Estimate value of each action using weighted sum of rewards from k similar states
  5. Select action with highest estimated value and reorder examples
  6. Construct final prompt using prompt constructor and query downstream LLM

- Design tradeoffs:
  - Memory size: Larger memory improves generalization but increases storage/retrieval costs
  - Number of nearest neighbors (k): Larger k provides robust estimates but increases computation
  - Number of in-context examples (m): Larger m provides more context but increases action space complexity

- Failure signatures:
  - Poor performance on novel states: Indicates inaccurate value estimates from nearest-neighbor estimator
  - Inconsistent performance across runs: Suggests unreliable state-action-reward storage/retrieval or poor similarity metric
  - Slow convergence during training: May indicate too large action space or insufficient reward signal

- First 3 experiments:
  1. Verify state encoder produces meaningful, consistent vector representations for input text
  2. Test example selector and action encoder on small example set to ensure similarity-based ordering works
  3. Implement and test episodic memory and memory reader components using small training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does POEM performance change with different similarity metrics (cosine vs. Euclidean distance) for encoding actions?
- Basis in paper: [inferred] Uses cosine similarity but doesn't explore alternatives
- Why unresolved: Choice of similarity metric could significantly impact encoded action quality and overall performance
- What evidence would resolve it: Experiments with different similarity metrics compared against cosine baseline

### Open Question 2
- Question: How does episodic memory size affect POEM performance with large training datasets?
- Basis in paper: [explicit] Memory size limited to Dtrain size, acknowledges fixed smaller size may be needed for large Dtrain
- Why unresolved: Impact of memory size on performance not thoroughly investigated for large training datasets
- What evidence would resolve it: Experiments with varying memory sizes (10%, 25%, 50%, 100% of Dtrain) analyzing performance trade-offs

### Open Question 3
- Question: How does POEM perform on other NLP tasks like NER or machine translation?
- Basis in paper: [inferred] Focuses on classification and language understanding but doesn't explore NER or MT
- Why unresolved: Effectiveness for other NLP tasks remains unknown; limited to specific task types
- What evidence would resolve it: Applying POEM to NER and MT tasks and comparing against existing methods

## Limitations

- The method's effectiveness relies heavily on the assumption that similarity between training examples and test queries predicts optimal prompt ordering
- Generalizability to tasks beyond text classification and language understanding (e.g., summarization, dialogue generation) remains unverified
- Implementation requires careful tuning of multiple hyperparameters (k, m, memory size) without systematic ablation studies

## Confidence

**High Confidence**: The core mechanism of using episodic memory to store and retrieve optimal prompt orderings based on similarity is well-specified and theoretically sound

**Medium Confidence**: Empirical results showing 5.3% average improvement are convincing for tested datasets, but evaluation on only 10 datasets limits generalizability claims

**Low Confidence**: Claims about avoiding "complex RL optimization" should be qualified as method still involves sequential decision-making and requires careful reward design

## Next Checks

1. **Cross-Domain Generalization Test**: Apply POEM to a dataset from a significantly different domain (e.g., medical text or legal documents) to assess whether similarity-based ordering assumption holds outside tested domains

2. **Ablation Study on k Parameter**: Systematically vary number of nearest neighbors (k) used in memory reader to determine optimal value and understand sensitivity across different task types

3. **Comparison with Domain-Specific Baselines**: Compare POEM against task-specific prompt optimization methods (e.g., designed specifically for question answering or text classification) rather than only general baselines