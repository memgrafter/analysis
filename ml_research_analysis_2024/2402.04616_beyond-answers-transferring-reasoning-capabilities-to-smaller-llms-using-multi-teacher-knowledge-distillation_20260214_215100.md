---
ver: rpa2
title: 'Beyond Answers: Transferring Reasoning Capabilities to Smaller LLMs Using
  Multi-Teacher Knowledge Distillation'
arxiv_id: '2402.04616'
source_url: https://arxiv.org/abs/2402.04616
tags:
- reasoning
- tinyllm
- teacher
- rationales
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TinyLLM transfers reasoning capabilities from large language models
  (LLMs) to smaller ones via multi-teacher knowledge distillation, enabling the student
  model to learn both correct answers and underlying rationales. It leverages multiple
  teacher LLMs to capture diverse reasoning skills and employs an in-context example
  generator with a teacher-forcing Chain-of-Thought strategy to produce accurate,
  contextually grounded rationales.
---

# Beyond Answers: Transferring Reasoning Capabilities to Smaller LLMs Using Multi-Teacher Knowledge Distillation

## Quick Facts
- arXiv ID: 2402.04616
- Source URL: https://arxiv.org/abs/2402.04616
- Authors: Yijun Tian; Yikun Han; Xiusi Chen; Wei Wang; Nitesh V. Chawla
- Reference count: 28
- One-line primary result: TinyLLM transfers reasoning capabilities from large LLMs to smaller ones via multi-teacher knowledge distillation, achieving up to +12.57% performance improvement over full fine-tuning while being 98.9% smaller.

## Executive Summary
TinyLLM introduces a novel approach to knowledge distillation that transfers reasoning capabilities from large language models to smaller ones by learning both correct answers and underlying rationales. The method leverages multiple teacher LLMs to capture diverse reasoning skills and employs an in-context example generator with a teacher-forcing Chain-of-Thought strategy to produce accurate, contextually grounded rationales. Through multi-task instruction tuning, the student model is trained to predict answers and generate rationales, mimicking teacher reasoning while achieving significant performance improvements over existing methods.

## Method Summary
TinyLLM uses multi-teacher knowledge distillation to transfer reasoning capabilities from large LLMs to smaller ones. The approach involves generating rationales from multiple teacher models using in-context examples and teacher-forcing Chain-of-Thought prompting, then training a student model through multi-task instruction tuning to predict answers and generate rationales. The student learns to integrate diverse reasoning perspectives from different teachers while being only 1.1% the size of the teacher models, achieving performance improvements of up to 12.57% over full fine-tuning on six datasets spanning commonsense and biomedical reasoning tasks.

## Key Results
- TinyLLM achieves up to +12.57% performance improvement over full fine-tuning across six datasets
- The model outperforms both teacher models despite being only 1.1% their size
- Multi-teacher setup with rationale learning shows significant advantages over single-teacher approaches
- Performance improvements are consistent across both commonsense and biomedical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-teacher knowledge distillation improves reasoning diversity by combining rationales from different LLM architectures.
- Mechanism: Multiple teacher models generate distinct rationales for the same input using in-context examples and teacher-forcing Chain-of-Thought. The student learns to integrate these diverse perspectives during multi-task instruction tuning.
- Core assumption: Different LLMs possess diverse reasoning skills that complement each other, and the student can effectively synthesize this knowledge.
- Evidence anchors:
  - [abstract]: "Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs."
  - [section 2.2]: "We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios."

### Mechanism 2
- Claim: In-context example generator improves rationale quality by providing task-specific context to teacher models.
- Mechanism: The generator produces relevant examples from the same dataset, which are included in the input to teachers along with questions, options, and correct answers. This helps teachers generate more contextually grounded rationales.
- Core assumption: Providing concrete examples helps LLMs better understand the task structure and reasoning requirements.
- Evidence anchors:
  - [section 2.2]: "To enable the rationales that are generated by teachers to be grounded in contextually appropriate scenarios, we introduce an optional in-context example generator."
  - [ablation study]: "w/o in-context rules out the in-context examples during rationale generation" and shows performance degradation.

### Mechanism 3
- Claim: Teacher-forcing Chain-of-Thought strategy improves rationale accuracy by including correct answers in the prompt.
- Mechanism: Unlike standard CoT prompting, the teacher-forcing approach includes the correct answer alongside the question and options, helping teachers generate rationales that logically lead to the correct conclusion.
- Core assumption: Knowing the correct answer helps teachers construct more coherent and accurate reasoning paths.
- Evidence anchors:
  - [section 2.2]: "We hypothesize that the inclusion of ùê¥ùëñ alongside ùëÑùëñ and ùëÇùëñ facilitates a more nuanced understanding of the input context and the correct logical rationales leading to the answer."
  - [section 2.2]: "TinyLLM posits a distinct advantage in integrating the correct answer ùê¥ùëñ into the input."

## Foundational Learning

- Concept: Chain-of-Thought reasoning
  - Why needed here: The paper relies on CoT to generate rationales, and teacher-forcing CoT is a key innovation that improves rationale quality.
  - Quick check question: How does standard CoT prompting differ from teacher-forcing CoT in terms of input formulation?

- Concept: Knowledge distillation principles
  - Why needed here: TinyLLM builds on knowledge distillation but extends it with multi-teacher setup and rationale learning, requiring understanding of both standard KD and its limitations.
  - Quick check question: What are the two main drawbacks of existing KD methods that TinyLLM addresses?

- Concept: Multi-task learning and instruction tuning
  - Why needed here: The student model is trained using multi-task instruction tuning to simultaneously predict answers and generate rationales, requiring knowledge of how to balance multiple learning objectives.
  - Quick check question: How does the loss function in TinyLLM combine answer prediction and rationale generation tasks?

## Architecture Onboarding

- Component map: Input ‚Üí In-context Example Generator ‚Üí Multiple Teacher LLMs (with teacher-forcing CoT) ‚Üí Rationale Collection ‚Üí Student LLM (multi-task instruction tuning) ‚Üí Output
- Critical path: Question/Options ‚Üí Teacher rationale generation ‚Üí Student training ‚Üí Inference without pre-generated rationales
- Design tradeoffs: Multiple teachers increase diversity but also computational cost during training; in-context examples improve quality but add complexity; teacher-forcing CoT may bias teachers toward the given answer
- Failure signatures: If student underperforms full fine-tuning, check teacher rationale quality; if student performs poorly on certain datasets, check teacher diversity and in-context example relevance
- First 3 experiments:
  1. Test single teacher vs. multi-teacher performance to validate diversity benefit
  2. Test with and without in-context examples to measure context impact
  3. Test different teacher-forcing CoT variants (with/without correct answer) to validate the teacher-forcing hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance gains from TinyLLM compare when using different types of in-context examples (e.g., random vs. relevance-based selection) in the in-context example generator?
- Basis in paper: [explicit] The paper states that in-context examples are selected randomly within the same dataset, but does not explore alternative selection strategies or their impact on performance.
- Why unresolved: The paper only mentions random selection without investigating whether relevance-based or other strategies could yield better results.
- What evidence would resolve it: Experiments comparing TinyLLM performance using different in-context example selection strategies (e.g., random, relevance-based, difficulty-based) across the same datasets.

### Open Question 2
- Question: What is the optimal number of teacher models to use in TinyLLM, and does this number vary based on the reasoning task or dataset complexity?
- Basis in paper: [inferred] The paper uses multiple teachers but does not systematically study the effect of varying the number of teachers on performance or explore whether certain tasks benefit more from additional teachers.
- Why unresolved: The paper demonstrates benefits of multiple teachers but doesn't investigate the diminishing returns or optimal number for different scenarios.
- What evidence would resolve it: Experiments varying the number of teacher models (e.g., 2, 3, 4, 5) and analyzing performance across different datasets and reasoning tasks.

### Open Question 3
- Question: How does TinyLLM's performance scale with student model size beyond the 80M, 250M, and 780M parameters tested, and is there a point where full fine-tuning becomes more effective?
- Basis in paper: [explicit] The paper tests three student sizes but doesn't explore performance trends across a broader range of model sizes or identify potential crossover points with full fine-tuning.
- Why unresolved: The paper shows TinyLLM outperforming full fine-tuning for tested sizes but doesn't investigate performance at different scale ranges.
- What evidence would resolve it: Experiments testing TinyLLM with student models ranging from very small (e.g., 50M) to very large (e.g., 3B) parameters, comparing performance against full fine-tuning across this range.

## Limitations

- The paper doesn't provide quantitative evidence comparing single-teacher vs multi-teacher performance on the same datasets, making the diversity benefit assumption rather than empirically validated
- The in-context example generator's effectiveness is supported by ablation studies, but the selection mechanism for examples is not fully specified, making replication challenging
- While TinyLLM outperforms full fine-tuning by up to +12.57%, the absolute performance on some datasets remains lower than the teacher models, raising questions about practical utility

## Confidence

- **High Confidence:** The core mechanism of multi-teacher knowledge distillation with rationale learning is technically sound and builds on established principles. The ablation studies provide reasonable evidence for individual component contributions.
- **Medium Confidence:** The performance claims are supported by experimental results, but the lack of detailed implementation specifications and the limited dataset diversity reduce confidence in generalizability.
- **Low Confidence:** The paper makes claims about TinyLLM's ability to outperform teacher models despite being smaller, but doesn't provide sufficient analysis of when and why this occurs or what factors influence the performance gap.

## Next Checks

1. **Single vs. Multi-Teacher Comparison:** Conduct controlled experiments comparing TinyLLM's performance using single teacher models versus the multi-teacher setup to quantitatively validate the diversity benefit claimed in the paper.

2. **Rationale Quality Analysis:** Perform qualitative and quantitative analysis of the generated rationales, including coherence scores, logical consistency, and human evaluation, to verify that the teacher-forcing CoT strategy actually improves reasoning quality rather than just leading to correct answers.

3. **Cross-Domain Generalization Test:** Evaluate TinyLLM's performance on additional reasoning datasets outside the six tested domains to assess the approach's generalizability and identify potential limitations in domain adaptation.