---
ver: rpa2
title: Making Better Use of Unlabelled Data in Bayesian Active Learning
arxiv_id: '2404.17249'
source_url: https://arxiv.org/abs/2404.17249
tags:
- learning
- active
- data
- bayesian
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that conventional fully supervised models in
  Bayesian active learning neglect information in unlabelled data, harming both predictive
  performance and data acquisition decisions. The authors propose a simple semi-supervised
  framework: pretrain a deterministic encoder on unlabelled data, then train a stochastic
  prediction head via Bayesian active learning.'
---

# Making Better Use of Unlabelled Data in Bayesian Active Learning

## Quick Facts
- arXiv ID: 2404.17249
- Source URL: https://arxiv.org/abs/2404.17249
- Authors: Freddie Bickford Smith; Adam Foster; Tom Rainforth
- Reference count: 29
- Key outcome: Semi-supervised Bayesian active learning with pretrained encoders outperforms fully supervised models and random acquisition baselines

## Executive Summary
This paper addresses a fundamental limitation in Bayesian active learning: conventional approaches ignore the rich information available in unlabelled data, leading to suboptimal predictive performance and inefficient data acquisition. The authors propose a simple yet effective solution that pretrains a deterministic encoder on unlabelled data, then trains a stochastic prediction head using Bayesian active learning. This semi-supervised framework produces significantly better results than fully supervised alternatives while being computationally more efficient.

## Method Summary
The proposed approach involves pretraining a deterministic encoder on unlabelled data using contrastive learning methods (SimCLR or MoCo v2), then fixing this encoder during active learning. A lightweight prediction head is trained on top of the frozen encoder using Bayesian methods. The EPIG acquisition function is used to select the most informative data points by directly targeting information gain in predictions rather than parameters. This design allows the model to leverage unlabelled data for better representation learning while maintaining computational efficiency through updates only to the prediction head.

## Key Results
- Semi-supervised models with pretrained encoders significantly outperform fully supervised Bayesian active learning baselines
- EPIG acquisition function provides substantial improvements over random acquisition, especially with small label budgets
- The semi-supervised approach is computationally faster than fully supervised alternatives, with speed advantages increasing as datasets become harder
- Speedups are particularly pronounced when combined with efficient Bayesian inference methods like Bayesian neural tangent kernels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-supervised models outperform fully supervised models because they can leverage information from unlabelled data to learn better representations of the input manifold.
- Mechanism: Pretraining a deterministic encoder on unlabelled data allows the model to capture rich information about the underlying input structure, which improves similarity judgments between inputs and thus produces more appropriate predictive correlations.
- Core assumption: Unlabelled data contains useful information about the input manifold that can improve downstream predictive performance.
- Evidence anchors:
  - [abstract]: "Unlabelled data can provide a significant amount of information helpful for downstream predictive tasks, as demonstrated by cases like few-shot learning in vision models (Chen et al., 2020c) and in-context learning in language models (Brown et al., 2020)."
  - [section]: "The encoder is pretrained on unlabelled data and then treated as a fixed function with no stochastic parameters... Because the encoder is deterministic and only needs to be trained once, it can be big and have a lot of processing power while still avoiding the issues discussed in Section 3."
  - [corpus]: Weak evidence - corpus does not provide direct evidence for this mechanism.
- Break Condition: If the unlabelled data does not contain useful information about the input manifold, or if the pretraining method is not effective at capturing this information.

### Mechanism 2
- Claim: EPIG acquisition function outperforms BALD because it targets information gain in predictions rather than parameters, avoiding pathologies associated with parameter uncertainty.
- Mechanism: EPIG directly targets reductions in predictive uncertainty on a random target input, filtering out parameter uncertainty that doesn't correspond to reduced predictive uncertainty. This allows the model to focus on acquiring data that is informative for the task of interest.
- Core assumption: Information about downstream predictions is more valuable than global reductions in parameter uncertainty for active learning.
- Evidence anchors:
  - [abstract]: "Crucially these models do not account for all the information at hand: they neglect the rich information often conveyed by unlabelled data, which is usually assumed to be cheaply available... This neglected information can be critical to both making predictions and estimating the reducible uncertainty present in the model."
  - [section]: "EPIG targets only reductions in parameter uncertainty that correspond to reduced predictive uncertainty on x*, avoiding BALD's pathology of acquiring data that is informative with respect to the model's parameters but not its downstream predictions."
  - [corpus]: Weak evidence - corpus does not provide direct evidence for this mechanism.
- Break Condition: If the model's predictions are highly correlated with parameter uncertainty, or if the target input distribution is not representative of the task of interest.

### Mechanism 3
- Claim: Fixing the encoder significantly reduces computational cost and improves update consistency compared to fine-tuning the encoder.
- Mechanism: By keeping the encoder fixed, updates only need to be made to the lightweight prediction head, avoiding the computational cost of updating all model parameters. This also ensures update consistency, as the exact Bayesian update assumed by formal notions of reducible uncertainty only needs to be applied to the prediction head parameters.
- Core assumption: The encoder can be effectively pretrained on unlabelled data and does not need to be updated during active learning.
- Evidence anchors:
  - [abstract]: "The (implicit) priors used in practice can poorly reflect our beliefs about the reducible uncertainty present in the model, and it is usually not possible to accurately approximate the Bayesian updates that formal notions of reducible uncertainty assume."
  - [section]: "One reason for this is that we only need to make updates to pϕ(θh) at each active-learning step, with any associated inference much easier for this small subset of the model parameters than for all of them."
  - [corpus]: Weak evidence - corpus does not provide direct evidence for this mechanism.
- Break Condition: If the encoder is not effectively pretrained, or if the prediction head requires significant updates to the encoder for good performance.

## Foundational Learning

- Concept: Bayesian experimental design
  - Why needed here: Active learning is a form of Bayesian experimental design, where the goal is to identify what data will yield the most information gain in a quantity of interest.
  - Quick check question: What is the difference between information gain in parameters vs predictions, and why does this matter for active learning?
- Concept: Semi-supervised learning
  - Why needed here: The proposed approach uses a semi-supervised model, where an encoder is pretrained on unlabelled data and a prediction head is trained using labelled data.
  - Quick check question: How does incorporating unlabelled data into the model improve predictive performance and uncertainty estimation?
- Concept: Mutual information
  - Why needed here: The EPIG acquisition function is based on mutual information between predictions and labels, which measures the reduction in predictive uncertainty due to new data.
  - Quick check question: How is mutual information related to information gain, and why is it a useful quantity for active learning?

## Architecture Onboarding

- Component map: Pretrained encoder -> Prediction head -> EPIG acquisition function
- Critical path: Pretrain encoder -> Initialize prediction head -> Active learning loop (acquire data -> Update prediction head -> Evaluate)
- Design tradeoffs:
  - Encoder size vs computational cost: Larger encoders capture more information but increase pretraining cost
  - Prediction head complexity vs update consistency: Simpler prediction heads are easier to update but may limit performance
  - Acquisition function choice: EPIG targets predictive uncertainty but may be more computationally expensive than BALD
- Failure signatures:
  - Poor pretraining: Encoder does not capture useful information from unlabelled data, leading to subpar performance
  - Overfitting: Prediction head overfits to small labelled dataset, requiring regularization or early stopping
  - Inefficient acquisition: Acquisition function does not effectively identify informative data, wasting labelling budget
- First 3 experiments:
  1. Pretrain encoder on unlabelled data using SimCLR or MoCo v2, depending on dataset
  2. Initialize prediction head and evaluate performance on randomly acquired data
  3. Run active learning with EPIG acquisition and compare to random acquisition baseline

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The approach assumes unlabelled data is available and representative of the task distribution, which may not hold in all scenarios
- Effectiveness depends heavily on the quality and quantity of unlabelled data as well as the choice of pretraining method
- Fixed encoder assumption may limit the model's ability to adapt to domain shifts between unlabelled and labelled data

## Confidence
- Semi-supervised advantage (High): Supported by empirical results showing improved performance over fully supervised models
- EPIG superiority (Medium): Theoretical justification is sound, but limited empirical comparison to BALD
- Computational benefits (High): Clear from the reduced parameter space and simpler update requirements

## Next Checks
1. Test performance when unlabelled data distribution differs significantly from labelled data distribution
2. Compare EPIG to BALD on larger-scale datasets with more complex models
3. Evaluate the sensitivity of results to different pretraining methods and unlabelled data quantities