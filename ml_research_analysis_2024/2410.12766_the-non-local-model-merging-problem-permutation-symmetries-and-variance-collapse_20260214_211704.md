---
ver: rpa2
title: 'The Non-Local Model Merging Problem: Permutation Symmetries and Variance Collapse'
arxiv_id: '2410.12766'
source_url: https://arxiv.org/abs/2410.12766
tags:
- merging
- expert
- task
- merged
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors propose a method for merging models trained on different\
  \ tasks when the models are not fine-tuned from the same pretrained weights. This\
  \ non-local merging scenario poses challenges, as standard merging techniques often\
  \ fail due to variance collapse\u2014a phenomenon where the activations of the merged\
  \ model differ significantly from those of the expert models."
---

# The Non-Local Model Merging Problem: Permutation Symmetries and Variance Collapse

## Quick Facts
- arXiv ID: 2410.12766
- Source URL: https://arxiv.org/abs/2410.12766
- Reference count: 25
- Key outcome: The authors propose Task-specific Activation Correction (TACT) to enable effective model merging when experts are fine-tuned from different foundation models, addressing variance collapse through per-task BatchNorm layers.

## Executive Summary
The paper addresses the challenge of merging deep learning models trained on different tasks when those models are not fine-tuned from the same pretrained weights. Standard merging techniques fail in this "non-local" setting due to variance collapse - where merged activations differ significantly from expert models. The authors introduce TACT, a technique that applies layer-wise rescaling and shifting using BatchNorm layers to align activation statistics of the merged model with those of the expert models. This approach significantly outperforms standard merging techniques in non-local settings while maintaining performance in local settings.

## Method Summary
The method involves training two independent foundation models, fine-tuning each to get experts per task, aligning foundations via weight matching to enable permutation, and applying TACT by computing activation statistics for each expert model. BatchNorm parameters in the merged model are set to match those statistics. The approach combines permutation alignment (from prior work) with task-specific activation correction through BatchNorm layers inserted after each module. This allows the merged model to maintain expert activation statistics for each task during inference.

## Key Results
- TACT significantly improves non-local merging performance, outperforming standard techniques by large margins
- The method maintains strong performance in local merging scenarios where experts share the same foundation model
- TACT adds computational overhead and memory requirements proportional to the number of tasks, but provides substantial accuracy gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-local merging fails because merged activations collapse variance compared to expert models.
- Mechanism: Different foundation models produce experts whose activation distributions differ in scale and shape. Averaging their weights yields a model whose activations have reduced variance, hurting generalization.
- Core assumption: Activation statistics (mean, variance) encode task-specific information critical for downstream performance.
- Evidence anchors:
  - [abstract] "standard merging techniques often fail to generalize effectively in this non-local setting... due to 'variance collapse'..."
  - [section 3.1] "we observe that standard merging techniques often fail... even when accounting for permutation symmetries using standard techniques"
- Break condition: If expert models share similar internal representations (e.g., from same architecture + similar pretraining), variance collapse is minimal and correction unnecessary.

### Mechanism 2
- Claim: Aligning foundation models via permutation restores linear connectivity in the loss landscape.
- Mechanism: Networks from different basins become linearly connected when permuted so their weight spaces align; averaging then stays in a low-loss region.
- Core assumption: Linear connectivity modulo permutation holds for networks from different basins if properly aligned.
- Evidence anchors:
  - [section 2.1] "permutations were the only source of error barriers... all SGD-trained networks converge to a single basin modulo permutations"
  - [section 3.1] "localize foundation models using permutations... Elocalized ← {θ : θ ∈ E finetuned from θ0} ∪ {P ⋆[θ] : θ ∈ E finetuned from θ1}"
- Break condition: If permutation alignment algorithm cannot find a good alignment (e.g., dissimilar architectures or training regimes), merging remains in high-loss regions.

### Mechanism 3
- Claim: Task-specific BatchNorm layers restore expert activation statistics for each task.
- Mechanism: For each task t, BatchNorm layers are initialized with the mean and variance of expert activations on task t. During inference, activations are rescaled and shifted to match expert statistics, mitigating mismatch.
- Core assumption: BatchNorm layers can adapt activations per task without interfering with cross-task knowledge.
- Evidence anchors:
  - [section 4] "We propose to correct the channel-wise activations of the merged model to satisfy: E[A(k)_θmerged(X)] = E[A(k)_θt(X)], Var[A(k)_θmerged(X)] = Var[A(k)_θt(X)]"
- Break condition: If task datasets are too small or diverse, BatchNorm statistics become unreliable or too task-specific, harming generalization.

## Foundational Learning

- Concept: Linear mode connectivity (LMC)
  - Why needed here: Merging assumes that convex combinations of weights remain in a low-loss region; LMC justifies this for models from the same basin.
  - Quick check question: If two models have zero loss barrier along their convex combination, are they linearly connected?

- Concept: Permutation invariance of neural networks
  - Why needed here: Different weight orderings can produce identical functions; aligning permutations enables meaningful merging across basins.
  - Quick check question: Does swapping two neurons in a layer always preserve network output?

- Concept: Batch Normalization and internal covariate shift
  - Why needed here: BatchNorm standardizes activations per channel, allowing per-task correction to match expert statistics.
  - Quick check question: What statistics does BatchNorm track during training?

## Architecture Onboarding

- Component map:
  Foundation models (F) → independently pretrained
  Expert models (E) → fine-tuned from F on tasks
  Permutation alignment module → aligns F to enable merging
  TACT correction module → BatchNorm layers per task after each module
  Merged model → output of LocalMerging(F, E) + TACT

- Critical path:
  1. Train two independent foundation models.
  2. Fine-tune each to get experts per task.
  3. Align foundations via weight matching (Ainsworth et al.).
  4. Permute experts to match aligned foundations.
  5. Merge using standard method (Task Arithmetic, TIES, TALL-mask).
  6. Compute task-specific activation statistics.
  7. Insert and initialize BatchNorm layers per task.
  8. Deploy merged model with per-task BatchNorm switching.

- Design tradeoffs:
  - Permutation alignment adds computational cost but is essential for non-local merging.
  - TACT adds memory overhead proportional to number of tasks but improves accuracy.
  - Choice of merging method (Task Arithmetic vs TIES vs TALL-mask) trades off simplicity vs compression.

- Failure signatures:
  - Large ℓ₂ distance between merged and expert activations indicates misalignment.
  - High variance ratio discrepancy indicates collapse.
  - Poor performance on worst task suggests permutation alignment insufficient.

- First 3 experiments:
  1. Merge 2 tasks from same foundation (local) → verify baseline.
  2. Merge 2 tasks from different foundations without permutation → confirm failure.
  3. Merge same tasks with permutation but without TACT → measure improvement gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance of non-local merging improve with deeper or wider networks, or does the variance collapse phenomenon become more pronounced?
- Basis in paper: [inferred] The paper discusses variance collapse as a key challenge in non-local merging, and the experiments are conducted on VGG16 and ViT-B16 architectures. The authors suggest that deeper layers deviate more from expert models, indicating a potential relationship between network depth and merging difficulty.
- Why unresolved: The paper does not explore how network architecture (depth or width) affects the non-local merging performance or the severity of variance collapse.
- What evidence would resolve it: Experiments comparing non-local merging performance across different network architectures with varying depths and widths, measuring activation statistics and final accuracy.

### Open Question 2
- Question: Can a single permutation alignment work for merging more than two foundation models, or is strong linear connectivity modulo permutation necessary?
- Basis in paper: [explicit] The paper states that current permutation algorithms only work for weak linear connectivity (aligning two networks at a time) and that experiments are restricted to two foundation models due to this limitation.
- Why unresolved: The paper does not test whether a single permutation can align multiple foundation models simultaneously, which would be required for merging experts from more than two foundation models.
- What evidence would resolve it: Experiments attempting to merge experts from three or more foundation models using a single permutation alignment, comparing results with pairwise merging approaches.

### Open Question 3
- Question: Are there data-agnostic correction methods that could replace task-specific activation correction (TACT) and reduce computational overhead?
- Basis in paper: [explicit] The authors mention that TACT requires computing activation statistics for each task, and suggest exploring data-agnostic correction methods as future work.
- Why unresolved: The paper only proposes TACT as a task-specific solution and does not investigate whether general correction methods could achieve similar results without task-specific computation.
- What evidence would resolve it: Experiments comparing TACT with data-agnostic correction methods (e.g., using average statistics across all tasks) in terms of merging performance and computational efficiency.

### Open Question 4
- Question: How does the choice of foundation model pretraining dataset size affect non-local merging performance compared to local merging?
- Basis in paper: [explicit] The paper shows that weaker foundation models (pretrained on ImageNet-1k vs ImageNet-21k) lead to significant deterioration in local merging results, but does not explore this effect in non-local merging.
- Why unresolved: The experiments focus on comparing local vs non-local merging with the same foundation models, without investigating how foundation model quality affects the relative performance of each approach.
- What evidence would resolve it: Experiments comparing local and non-local merging performance across foundation models with varying pretraining dataset sizes, measuring the performance gap between approaches.

## Limitations

- The empirical validation is limited to image classification tasks with specific architectures (VGG16, ViT-B16), leaving generalizability to other domains uncertain.
- TACT adds computational overhead and memory requirements proportional to the number of tasks, which may limit scalability to many-task scenarios.
- The effectiveness of permutation alignment depends on finding good weight matchings, which may be challenging for more complex architectures or training regimes.

## Confidence

- **High confidence**: The theoretical framework for non-local merging failure due to variance collapse and the mechanism by which TACT addresses this through BatchNorm layers. The permutation alignment approach for localizing foundation models is also well-established from prior work.
- **Medium confidence**: The empirical results showing TACT's effectiveness across different merging methods and tasks, given the limited scope of tested architectures and datasets. The computational overhead claims are reasonable but not exhaustively quantified.
- **Low confidence**: The generalizability of these findings to non-image domains (NLP, audio) and more complex architectures where BatchNorm may behave differently or where permutation alignment becomes significantly harder.

## Next Checks

1. **Cross-domain validation**: Apply TACT to model merging in NLP tasks (e.g., merging LLMs fine-tuned on different reasoning tasks) to test generalizability beyond image classification.

2. **Architectural robustness test**: Evaluate TACT's effectiveness on architectures without BatchNorm layers (e.g., ResNets without BN, or architectures using LayerNorm) to determine if the approach requires architectural modifications or if alternative normalization strategies exist.

3. **Scaling analysis**: Systematically measure the computational overhead and memory requirements of TACT as the number of tasks increases from 2 to 10+ tasks, quantifying the trade-off between accuracy gains and resource costs.