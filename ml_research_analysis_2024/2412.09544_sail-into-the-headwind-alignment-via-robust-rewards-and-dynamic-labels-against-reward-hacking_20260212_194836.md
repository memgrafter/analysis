---
ver: rpa2
title: 'Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against
  Reward Hacking'
arxiv_id: '2412.09544'
source_url: https://arxiv.org/abs/2412.09544
tags:
- reward
- preference
- hacking
- arxiv
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses reward hacking in offline preference optimization,
  where optimization of an imperfect reward model leads to undesired behaviors. The
  authors identify two types of reward hacking stemming from statistical fluctuations
  in preference datasets: Type I (subpar choices appearing favorable) and Type II
  (decent choices appearing less favorable).'
---

# Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking

## Quick Facts
- arXiv ID: 2412.09544
- Source URL: https://arxiv.org/abs/2412.09544
- Authors: Paria Rashidinejad; Yuandong Tian
- Reference count: 40
- This paper addresses reward hacking in offline preference optimization, where optimization of an imperfect reward model leads to undesired behaviors.

## Executive Summary
This paper addresses reward hacking in offline preference optimization, where optimization of an imperfect reward model leads to undesired behaviors. The authors identify two types of reward hacking stemming from statistical fluctuations in preference datasets: Type I (subpar choices appearing favorable) and Type II (decent choices appearing less favorable). To mitigate Type I reward hacking, they propose POWER, a new preference optimization method that combines Guiasu's weighted entropy with a robust reward maximization objective, providing finite-sample guarantees under general function approximation. To address Type II reward hacking, they develop dynamic labels that update preference labels toward "stationary labels," diminishing gradients for untrustworthy samples. Empirically, POWER with dynamic labels (POWER-DL) consistently outperforms state-of-the-art methods on alignment benchmarks, achieving up to 13.0 points improvement on AlpacaEval 2.0 and 11.5 points on Arena-Hard over DPO, while maintaining or improving performance on downstream tasks like mathematical reasoning.

## Method Summary
The authors propose POWER-DL, a preference optimization method that combines weighted entropy robust rewards with dynamic labels to mitigate reward hacking. POWER uses weighted entropy to down-weight poorly covered choices in the dataset, while dynamic labels gradually update preference labels toward stationary values based on learned preferences. This approach provides two separate mechanisms to address Type I and Type II reward hacking, with hyperparameters η and γ controlling the level of conservatism for each mechanism.

## Key Results
- POWER-DL achieves up to 13.0 points improvement on AlpacaEval 2.0 over DPO
- POWER-DL achieves up to 11.5 points improvement on Arena-Hard over DPO
- POWER-DL maintains or improves performance on downstream tasks like mathematical reasoning while improving alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted entropy robust rewards (POWER) mitigate Type I reward hacking by down-weighting poorly covered, subpar choices through the use of response-length-based weights (w(y) = 1/|y|).
- Mechanism: The weighted entropy term Hw(π) in POWER's objective assigns lower weights to longer responses, which are more likely to be poorly covered in the dataset. This discourages learning from untrustworthy preference labels in regions of sparse data coverage, while still learning effectively from well-covered choices.
- Core assumption: Response length correlates with coverage quality in preference datasets, such that longer responses are more likely to be poorly covered and contain higher statistical fluctuations.
- Evidence anchors:
  - [abstract]: "Due to the weighted entropy, POWER effectively learns from well-covered choices in the dataset, even those with a large divergence against the initial model, countering potential underoptimization in divergence-based methods."
  - [section 4.3]: "Theorem 1 suggests that a particularly appealing choice for weights is the inverse response length w(y) = 1/|y|, which intuitively discourages learning policies that generate lengthy responses. Theoretically, while using Shannon entropy (w(y) = 1) results in a convergence rate that grows linearly with response length, with weights w(y) = 1/|y| convergence rate only depends on the logarithm of the vocabulary (token) size and logarithm of response length."
- Break condition: If response length does not correlate with coverage quality, or if the dataset is uniformly well-covered across all response lengths, the effectiveness of this mechanism would be reduced.

### Mechanism 2
- Claim: Dynamic labels (DL) mitigate Type II reward hacking by gradually updating preference labels toward "stationary labels" that reflect learned preferences, diminishing gradients for untrustworthy samples in low-coverage regions.
- Mechanism: The dynamic label update rule lt+1 = (1 - γ)lt + γlt, where lt is updated toward a stationary label based on the learned parameter gap and empirical preferences, causes the learned preferences in low-coverage regions to remain close to initialization. This prevents the model from deviating too far from the initial model in untrustworthy areas while still learning from high-coverage data.
- Core assumption: The learned parameter gap dθt(y1, y0) provides a reliable signal for updating preference labels, and the empirical preference gap provides a reasonable target for label updates in high-coverage regions.
- Evidence anchors:
  - [abstract]: "To mitigate Type II Reward Hacking, we analyze the learning dynamics of preference optimization and develop a novel technique that dynamically updates preference labels toward certain 'stationary labels', resulting in diminishing gradients for untrustworthy samples."
  - [section 5]: "Theorem 2 shows that for poorly covered pairs (small μ0,1), learned preferences σ(dT) remain close to initialization, while for high coverage pairs (large μ0,1), learned preferences converge to empirical preferences."
- Break condition: If the empirical preference gap is highly unreliable or if the learned parameter gap does not correlate well with true preferences, the dynamic label updates may not effectively mitigate Type II reward hacking.

### Mechanism 3
- Claim: POWER-DL achieves a favorable bias-variance tradeoff by interpolating between robust rewards (POWER) and maintaining closeness to the initial model (through dynamic labels), allowing for adjustable conservatism.
- Mechanism: POWER-DL combines the weighted entropy robust reward framework with dynamic labels, providing two separate knobs (η for robust rewards, γ for dynamic labels) to adjust the level of conservatism. This allows trading off between Type I and Type II reward hacking based on the relative quality of the initial model compared to preference data.
- Core assumption: The two sources of distribution shift (between final model and data, and between initial model and data) require separate mechanisms for effective mitigation of reward hacking.
- Evidence anchors:
  - [abstract]: "POWER-DL consistently outperforms state-of-the-art methods on alignment benchmarks, achieving improvements of up to 13.0 points on AlpacaEval 2.0 and 11.5 points on Arena-Hard over DPO, while also improving or maintaining performance on downstream tasks such as mathematical reasoning."
  - [section 7]: "POWER-DL interpolates between the initial model and robust rewards, allowing to trade off the two types of reward hacking through adjusting conservatism parameters η and γ, reflecting relative quality of the initial model compared to preference data and up to removing conservatism completely by setting η = γ = 0."
- Break condition: If the initial model is uniformly poor or uniformly excellent across all regions, the need for interpolation between the initial model and robust rewards would be reduced.

## Foundational Learning

- Concept: Bradley-Terry model of human preferences
  - Why needed here: The Bradley-Terry model is the underlying probabilistic model for pairwise preferences used throughout the paper. It characterizes the probability of preferring one response over another based on their reward difference, which is fundamental to understanding reward hacking and the design of preference optimization methods.
  - Quick check question: What is the mathematical expression for the probability of preferring response y1 over y0 according to the Bradley-Terry model, and what does σ(z) represent?

- Concept: Offline reinforcement learning and concentrability coefficients
  - Why needed here: The paper draws parallels between reward hacking in preference optimization and challenges in offline RL, particularly the concept of concentrability coefficients which measure how well a target policy is covered by the offline dataset. Understanding these concepts is crucial for appreciating the theoretical guarantees provided by POWER.
  - Quick check question: How does the single-policy concentrability coefficient Cμπ(R, π') defined in Definition 2 measure coverage of a competing policy π in the offline preference dataset?

- Concept: Weighted entropy and its properties
  - Why needed here: Weighted entropy is a key component of POWER's objective function. Understanding its definition, properties (such as non-negativity and upper bounds), and how it differs from Shannon entropy is essential for grasping why POWER effectively mitigates Type I reward hacking.
  - Quick check question: What is the definition of weighted entropy Hw(p) for a discrete distribution p with non-negative weights w(y), and how does it differ from Shannon entropy?

## Architecture Onboarding

- Component map: Preference dataset -> POWER-DL objective with dynamic labels -> Final aligned model
- Critical path:
  1. Initialize dynamic labels l0i = 0 for all samples
  2. For each iteration t:
     - Compute POWER-DL objective with current labels
     - Update policy parameters with stop gradient on labels
     - Update dynamic labels toward stationary values based on learned preferences
  3. Return final aligned model
- Design tradeoffs:
  - POWER vs POWER-DL: POWER alone provides robustness against Type I hacking but is vulnerable to Type II; adding dynamic labels mitigates Type II at the cost of additional complexity
  - Response length normalization: Using w(y) = 1/|y| prevents sample complexity from depending linearly on response length but may not be optimal for all applications
  - Conservatism parameters: η and γ control the tradeoff between reward hacking types but require careful tuning
- Failure signatures:
  - Over-conservatism: Model performs well on alignment benchmarks but degrades on downstream tasks (too much reliance on initial model)
  - Under-conservatism: Model overfits to preference data and suffers from reward hacking (labels not sufficiently distrusted in low-coverage regions)
  - Label update instability: Dynamic labels oscillate or converge to incorrect values (learning dynamics not properly balanced)
- First 3 experiments:
  1. Ablation study: Compare POWER-DL vs POWER vs DPO on AlpacaEval 2.0 and GSM8K to isolate contributions of dynamic labels
  2. Hyperparameter sensitivity: Test POWER-DL performance across a grid of η and γ values to identify robust ranges
  3. Coverage analysis: Measure correlation between response length and empirical preference reliability to validate the response length normalization assumption

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do dynamic labels generalize to out-of-distribution robustness in preference optimization?
- Basis in paper: Inferred from the discussion section, which suggests that future research could apply dynamic labels to out-of-distribution robustness.
- Why unresolved: The paper focuses on mitigating reward hacking within the scope of offline preference optimization with static datasets. It does not explore how dynamic labels perform when the model encounters data or preferences that are significantly different from the training distribution.
- What evidence would resolve it: Experiments comparing the performance of POWER-DL against other methods on preference datasets that include out-of-distribution examples, measuring robustness metrics like accuracy and win rates in such scenarios.

### Open Question 2
- Question: What is the impact of the choice of weight function w(y) on the performance of POWER-DL?
- Basis in paper: Inferred from the theoretical section, which discusses the use of inverse response length as weights and mentions other possible choices like response preference scores and per-sample importance weights. However, the paper only implements w(y) = 1/|y| in practice.
- Why unresolved: The paper provides theoretical analysis on the benefits of using inverse response length but does not empirically compare the performance of POWER-DL with different weight functions.
- What evidence would resolve it: Experiments comparing the performance of POWER-DL with different weight functions (e.g., response preference scores, per-sample importance weights) on various alignment benchmarks and downstream tasks.

### Open Question 3
- Question: How does the interplay between statistical errors and reward misspecification affect reward hacking?
- Basis in paper: Inferred from the discussion section, which mentions investigating the interplay between statistical errors and reward misspecification as a future research direction.
- Why unresolved: The paper focuses on reward hacking due to statistical fluctuations in preference data. It does not explore how reward hacking is affected when the underlying reward function is misspecified or biased.
- What evidence would resolve it: Experiments comparing the performance of POWER-DL against other methods when the preference data is generated using a misspecified reward function, measuring the extent of reward hacking and the effectiveness of POWER-DL in mitigating it.

## Limitations

- The theoretical guarantees rely on strong assumptions about the preference dataset structure and may not hold in practice.
- The empirical validation primarily focuses on single-turn preference data, leaving questions about performance on multi-turn conversations.
- The dynamic label mechanism introduces additional hyperparameters (γ) that require careful tuning, with the sensitivity of performance to these choices not fully explored.

## Confidence

- High confidence: The empirical superiority of POWER-DL over baseline methods (DPO) on alignment benchmarks (AlpacaEval 2.0, Arena-Hard) is well-supported by the results presented.
- Medium confidence: The theoretical analysis of POWER's finite-sample guarantees and the characterization of Type I/II reward hacking are sound, but rely on assumptions that may not hold in practice.
- Medium confidence: The mechanism by which dynamic labels mitigate Type II reward hacking is plausible and supported by theory, but the empirical validation could be more comprehensive.

## Next Checks

1. **Ablation on multi-turn datasets**: Test POWER-DL performance on multi-turn conversation datasets (e.g., UltraChat-200K) to verify that the method generalizes beyond single-turn preferences and to identify any degradation in performance with increased conversation complexity.

2. **Dynamic label stability analysis**: Conduct a systematic study of how the choice of γ affects the convergence and stability of dynamic labels across different dataset sizes and quality levels, particularly focusing on the conditions under which labels might oscillate or converge to incorrect values.

3. **Downstream task sensitivity**: Perform a more granular analysis of how POWER-DL's conservatism parameters (η and γ) affect performance on specific downstream tasks (mathematical reasoning, instruction-following) to better understand the tradeoff between alignment quality and task preservation.