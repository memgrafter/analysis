---
ver: rpa2
title: Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization
arxiv_id: '2401.15914'
source_url: https://arxiv.org/abs/2401.15914
tags:
- class
- feature
- classes
- features
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Existing vision-language models, such as CLIP, perform well on
  a variety of tasks but struggle with open-domain concepts due to their closed-set
  evaluation. Finetuning these models often leads to overfitting the known classes
  in the dataset, resulting in degraded performance on unknown classes.
---

# Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization

## Quick Facts
- arXiv ID: 2401.15914
- Source URL: https://arxiv.org/abs/2401.15914
- Authors: Yuhang Zang; Hanlin Goh; Josh Susskind; Chen Huang
- Reference count: 20
- Key outcome: OGEN improves OOD generalization with up to 18.77% absolute gains in accuracy

## Executive Summary
Existing vision-language models like CLIP struggle with open-domain concepts due to closed-set evaluation during finetuning, leading to overfitting known classes and degraded performance on unknown classes. The authors propose OGEN, a method that enhances out-of-distribution (OOD) generalization by synthesizing features for unknown classes and applying adaptive self-distillation. This approach addresses the fundamental limitation of vision-language models that perform well on known classes but fail to generalize to unseen concepts.

## Method Summary
OGEN introduces a class-conditional feature generator that synthesizes OOD features based on class names, helping regularize decision boundaries between known and unknown classes. The method also employs adaptive self-distillation to transfer knowledge between model checkpoints, reducing overfitting. The feature generator creates synthetic representations of unknown classes using text descriptions, while the self-distillation component ensures consistent knowledge transfer across training iterations.

## Key Results
- OGEN achieves up to 18.77% absolute gains in accuracy across various datasets
- Consistent improvements in OOD generalization across different evaluation settings
- Demonstrates effectiveness in addressing overfitting to known classes during finetuning

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge of closed-set evaluation in vision-language models. By synthesizing features for unknown classes, OGEN creates a more robust decision boundary that accounts for potential out-of-distribution samples. The adaptive self-distillation component further stabilizes training by transferring knowledge across checkpoints, preventing catastrophic forgetting of previously learned concepts.

## Foundational Learning
- **Feature Synthesis**: Why needed - To create representations for unknown classes; Quick check - Verify generated features align with semantic descriptions
- **Self-Distillation**: Why needed - To prevent overfitting and knowledge loss; Quick check - Monitor consistency between checkpoints
- **Class-Conditional Generation**: Why needed - To produce class-specific synthetic features; Quick check - Validate generated features against ground truth semantics

## Architecture Onboarding
**Component Map**: Text Encoder -> Feature Generator -> Classifier -> Self-Distillation Module
**Critical Path**: Input text description → Feature synthesis → Classification → Knowledge transfer via self-distillation
**Design Tradeoffs**: Balances between synthetic feature quality and computational overhead; manages the tradeoff between model complexity and generalization
**Failure Signatures**: Poor feature synthesis quality, inconsistent knowledge transfer between checkpoints, overfitting to synthetic examples
**First Experiments**: 1) Evaluate feature generator output quality, 2) Test self-distillation stability across checkpoints, 3) Measure impact of synthetic features on decision boundaries

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Feature generator relies on text descriptions that may not capture full visual semantic complexity
- Adaptive self-distillation hyperparameter sensitivity not thoroughly explored
- Evaluation focused primarily on classification tasks, leaving other vision-language applications untested

## Confidence
- High: Effectiveness of OGEN in improving OOD generalization on benchmark datasets
- Medium: Generalizability to real-world scenarios with diverse data distributions
- Low: Scalability to extremely large vision-language models and computational efficiency

## Next Checks
1. Evaluate OGEN's performance on non-classification tasks such as image-text retrieval or open-vocabulary segmentation
2. Conduct ablation studies on feature generator and self-distillation components
3. Test robustness to noisy or ambiguous text descriptions of unknown classes