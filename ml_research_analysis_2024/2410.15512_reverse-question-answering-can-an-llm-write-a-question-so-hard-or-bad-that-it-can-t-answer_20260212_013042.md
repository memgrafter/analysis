---
ver: rpa2
title: 'Reverse Question Answering: Can an LLM Write a Question so Hard (or Bad) that
  it Can''t Answer?'
arxiv_id: '2410.15512'
source_url: https://arxiv.org/abs/2410.15512
tags:
- question
- answer
- questions
- llms
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates reverse question answering (RQA), where
  a model generates a question given an answer, and compares its performance to standard
  QA. Testing 16 LLMs on trivia datasets, the study reveals that LLMs are significantly
  weaker at RQA for numerical answers but slightly better for textual answers.
---

# Reverse Question Answering: Can an LLM Write a Question so Hard (or Bad) that it Can't Answer?

## Quick Facts
- arXiv ID: 2410.15512
- Source URL: https://arxiv.org/abs/2410.15512
- Reference count: 40
- LLMs perform significantly worse at RQA for numerical answers compared to textual answers, and often answer their own invalid questions correctly

## Executive Summary
This paper investigates reverse question answering (RQA), where language models generate questions from given answers rather than answering questions. The study compares RQA performance against standard QA across 16 LLMs using trivia datasets, revealing that LLMs are notably weaker at generating valid questions for numerical answers while showing slight improvement for textual answers. A striking finding is that LLMs frequently answer their own invalidly generated questions correctly, suggesting that RQA errors stem from more than just knowledge gaps. The research demonstrates that RQA performance correlates with question difficulty and answer rarity in pretraining data, particularly highlighting challenges with multi-step reasoning for numerical domains.

## Method Summary
The researchers tested 16 large language models on reverse question answering tasks using trivia datasets, comparing their performance to standard QA. In RQA, models were given answers and asked to generate corresponding questions, which were then evaluated for validity and difficulty. The study analyzed performance differences between numerical and textual answers, examining how often models could answer their own generated questions. They also investigated correlations between RQA performance and factors like answer rarity in pretraining data and question complexity, providing insights into the limitations of LLMs in generating valid questions.

## Key Results
- LLMs show significantly worse performance on RQA tasks for numerical answers compared to textual answers
- Models often answer their own invalidly generated RQA questions correctly, indicating errors aren't solely knowledge gaps
- RQA performance correlates with answer rarity in pretraining data and question difficulty

## Why This Works (Mechanism)
RQA tasks expose fundamental limitations in how LLMs process information generation versus information retrieval. When generating questions from answers, models must engage in abductive reasoning - working backward from known facts to construct valid queries. This process reveals gaps in multi-step reasoning capabilities, particularly for numerical domains where logical constraints are more rigid. The finding that models answer their own invalid questions correctly suggests that RQA failures stem from generation constraints rather than pure knowledge deficits, highlighting the task's potential for evaluating deeper reasoning abilities.

## Foundational Learning
- **Abductive reasoning**: Working backward from observations to generate plausible explanations - needed to construct valid questions from answers, quick check: test models on backward reasoning tasks
- **Question generation evaluation**: Methods for assessing question quality and validity - needed to determine if RQA outputs are answerable, quick check: human evaluation of generated questions
- **Numerical reasoning constraints**: Understanding how mathematical relationships affect question generation - needed for RQA with numerical answers, quick check: compare performance on numerical vs textual RQA
- **Pretraining data characteristics**: Knowledge of corpus composition and answer frequency - needed to explain RQA performance correlations, quick check: analyze answer distribution in training data

## Architecture Onboarding
Component map: Input Answer -> Question Generator -> Generated Question -> QA Model -> Answer Prediction
Critical path: Question generation directly impacts QA model performance, with errors propagating through the system
Design tradeoffs: Focus on question validity versus question difficulty - valid questions may be too easy, difficult questions may be invalid
Failure signatures: Invalid questions often lack sufficient context, have ambiguous constraints, or require impossible multi-step reasoning
First experiments:
1. Generate questions from single numerical answers and evaluate answerability
2. Compare RQA performance across models of varying sizes and architectures
3. Test RQA with synthetic answers of controlled complexity

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Evaluation methodology doesn't strictly validate question-answer validity, potentially missing complex RQA scenarios
- Reliance on trivia datasets may introduce domain-specific biases limiting generalizability
- Generated questions sometimes lack sufficient context to be uniquely answerable

## Confidence
- High confidence: Comparative performance differences between RQA and standard QA tasks
- Medium confidence: Correlation between RQA performance and answer rarity in pretraining data
- Medium confidence: Claim that errors aren't solely due to knowledge gaps

## Next Checks
1. Conduct human evaluation studies to assess quality and validity of generated RQA questions
2. Test RQA performance across diverse domain-specific datasets beyond trivia knowledge
3. Implement controlled experiments with synthetic answers of varying complexity to isolate performance factors