---
ver: rpa2
title: Using Similarity to Evaluate Factual Consistency in Summaries
arxiv_id: '2409.15090'
source_url: https://arxiv.org/abs/2409.15090
tags:
- metrics
- sbertscore
- metric
- factuality
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of factual consistency evaluation
  for abstractive text summaries, which is a key challenge in natural language generation.
  The authors propose SBERTScore, a zero-shot metric that uses sentence-level similarity
  based on pre-trained embeddings to compare summaries with source documents.
---

# Using Similarity to Evaluate Factual Consistency in Summaries

## Quick Facts
- arXiv ID: 2409.15090
- Source URL: https://arxiv.org/abs/2409.15090
- Reference count: 14
- Primary result: SBERTScore is a zero-shot metric using sentence-level semantic similarity for factual consistency evaluation in text summaries

## Executive Summary
This paper addresses the challenge of evaluating factual consistency in abstractive text summaries using similarity-based approaches. The authors propose SBERTScore, which computes cosine similarity between sentence embeddings rather than relying on word-level matching like existing metrics. This sentence-level approach captures more complete semantic meaning and requires no additional training, making it a practical zero-shot solution. Experiments show SBERTScore outperforms widely-used word-level metrics like BERTScore and is competitive with more complex NLI and QA-based factuality metrics.

## Method Summary
SBERTScore uses pre-trained sentence embeddings (specifically SBERT) to compute cosine similarity between summary sentences and source document sentences. The metric identifies the most similar source sentence for each summary sentence and aggregates these similarities to produce a final consistency score. Unlike previous approaches that work at the token or word level, SBERTScore operates at the sentence level, capturing broader semantic meaning. The method is zero-shot, requiring no additional training data or fine-tuning. The authors also explore combining SBERTScore with other metrics using logical AND operations to improve balanced accuracy.

## Key Results
- SBERTScore outperforms BERTScore and achieves competitive performance with NLI and QA-based factuality metrics
- The metric excels at identifying correct summaries while maintaining reasonable false positive rates
- Combining SBERTScore with other metrics using logical AND can improve balanced accuracy by reducing false positives
- SBERTScore demonstrates particular effectiveness in zero-shot settings without requiring additional training data

## Why This Works (Mechanism)
SBERTScore works by leveraging sentence-level semantic embeddings to capture the full meaning of text units rather than relying on word-level matching. The cosine similarity between SBERT embeddings provides a robust measure of semantic equivalence that is less sensitive to paraphrasing and lexical variation. By comparing complete sentences rather than individual words or tokens, the metric can better handle the semantic drift common in abstractive summarization while still maintaining sensitivity to factual discrepancies.

## Foundational Learning
- Sentence embeddings: Why needed - Capture semantic meaning of entire sentences rather than individual words; Quick check - Verify embeddings preserve semantic similarity for paraphrased sentences
- Cosine similarity: Why needed - Provides normalized measure of semantic equivalence between embeddings; Quick check - Ensure scores are bounded between -1 and 1 with meaningful interpretation
- Zero-shot learning: Why needed - Eliminates need for labeled training data and domain-specific fine-tuning; Quick check - Test metric performance across multiple domains without adaptation
- Factual consistency evaluation: Why needed - Critical quality measure for generated summaries beyond fluency and relevance; Quick check - Compare metric scores against human factuality judgments
- Logical AND combination: Why needed - Can improve precision by requiring multiple metrics to agree; Quick check - Measure impact on both precision and recall when combining metrics

## Architecture Onboarding
Component map: Summary sentences -> SBERT encoder -> Sentence embeddings -> Cosine similarity -> Source sentences -> Aggregation -> Consistency score

Critical path: The core computation involves encoding both summary and source sentences using SBERT, computing pairwise cosine similarities, selecting maximum similarity for each summary sentence, and aggregating these scores.

Design tradeoffs: The sentence-level approach sacrifices some granularity compared to token-level methods but gains robustness to paraphrasing. The zero-shot nature trades potential performance gains from training against practical deployment benefits.

Failure signatures: The metric may fail on negation cases where semantically similar sentences have opposite truth values, and on summaries containing hallucinations not present in source documents.

First experiments: 1) Compare SBERTScore against human factuality judgments on a sample of summaries; 2) Test sensitivity to sentence order and structure variations; 3) Evaluate performance on summaries containing known factual errors versus correct summaries.

## Open Questions the Paper Calls Out
None

## Limitations
- The metric may struggle with negation and contradiction cases where semantically similar sentences have opposite truth values
- Performance evaluation is primarily based on CNN/DailyMail data, potentially limiting generalization to other domains
- The logical AND combination approach significantly reduces recall while improving balanced accuracy
- Semantic similarity does not guarantee factual correctness, as semantically similar statements can still be factually incorrect

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| SBERTScore achieves competitive performance with zero-shot sentence-level similarity computation | High |
| SBERTScore is "competitive with more complex NLI and QA-based factuality metrics" | Medium |
| Zero-shot similarity-based metrics are "a promising approach for factual consistency evaluation" | Low |

## Next Checks
1. Conduct error analysis specifically on negation and contradiction cases to quantify SBERTScore's failure modes and determine if these represent edge cases or systematic weaknesses.

2. Test SBERTScore on summary data from diverse domains (scientific articles, legal documents, dialogues) to assess generalization beyond news articles.

3. Implement a targeted study comparing SBERTScore against human factuality judgments on summaries containing subtle factual errors to better understand the practical gap between semantic similarity and factual correctness.