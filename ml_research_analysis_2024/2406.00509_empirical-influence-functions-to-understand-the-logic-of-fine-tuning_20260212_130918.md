---
ver: rpa2
title: Empirical influence functions to understand the logic of fine-tuning
arxiv_id: '2406.00509'
source_url: https://arxiv.org/abs/2406.00509
tags:
- influence
- fine-tuning
- training
- belongs
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces empirical influence functions (EIFs) to study
  how individual training samples affect neural network outputs during fine-tuning.
  The authors propose six desiderata for desirable influence function behavior, including
  semantic distance decay, sparsity, transitivity, and logical consistency.
---

# Empirical influence functions to understand the logic of fine-tuning

## Quick Facts
- arXiv ID: 2406.00509
- Source URL: https://arxiv.org/abs/2406.00509
- Reference count: 34
- Primary result: Empirical influence functions reveal fundamental differences between CNN and LLM fine-tuning, with LLMs violating all proposed desiderata for desirable influence function behavior

## Executive Summary
This paper introduces empirical influence functions (EIFs) as a tool to study how individual training samples affect neural network outputs during fine-tuning. The authors propose six desiderata for desirable influence function behavior, including semantic distance decay, sparsity, transitivity, and logical consistency. They evaluate these properties on both simple CNN models trained on FashionMNIST and MNIST, and a modern LLM (Phi-3) with synthetic knowledge domains. The results reveal stark differences between the two architectures: CNNs show symmetric influence functions, while LLMs exhibit asymmetry and violate all proposed desiderata, suggesting fundamental limitations in how LLMs learn new knowledge through fine-tuning.

## Method Summary
The authors develop a framework for quantifying how individual training examples influence model outputs during fine-tuning, creating empirical influence functions that measure the change in model predictions when specific samples are included or excluded from training. They propose six desiderata for desirable influence function behavior: semantic distance decay (influence decreases with semantic distance), sparsity (few influential samples), transitivity (consistent influence chains), ontology respect (coherent knowledge organization), causality (clear cause-effect relationships), and logical implication (consistent logical relationships). The framework is tested on simple CNNs trained on FashionMNIST and MNIST, then extended to LLMs (Phi-3) using synthetic knowledge domains created for controlled experimentation.

## Key Results
- CNN fine-tuning shows symmetric influence functions, with no distinction between learning from noisy vs clean samples
- LLM fine-tuning exhibits asymmetry but violates all six desiderata: no transitivity, ontology respect, causality, compositionality, logical implication, or meaningful sparsity
- Prompting with relevant information partially rescues model performance, satisfying more desiderata than fine-tuning

## Why This Works (Mechanism)
The empirical influence functions work by measuring the change in model outputs when individual training samples are included or excluded from the fine-tuning process. This perturbation-based approach captures how each training example uniquely affects the model's learned representations and predictions. For CNNs, this mechanism reveals symmetric influence patterns because the convolutional architecture processes spatial features in a relatively uniform manner. For LLMs, the mechanism exposes the complex, distributed nature of transformer-based knowledge representation, where individual tokens may have non-linear and context-dependent effects on downstream predictions.

## Foundational Learning
- Influence functions in machine learning: Used to understand how training data points affect model predictions, providing insights into model behavior and data quality
- Fine-tuning vs prompting: Fine-tuning modifies model weights for new tasks, while prompting leverages existing knowledge through clever input formatting
- Semantic distance in embeddings: Measures how far apart concepts are in the model's learned representation space
- Ontology in knowledge systems: The hierarchical organization of concepts and their relationships
- Transitivity in learning: The principle that if A influences B and B influences C, then A should indirectly influence C
- Logical implication in neural networks: How models should preserve logical relationships between concepts during learning

## Architecture Onboarding

**Component map:** CNN fine-tuning -> Influence function measurement -> Desiderata evaluation
LLM fine-tuning -> Influence function measurement -> Desiderata evaluation -> Prompting comparison

**Critical path:** Data preparation → Model training/fine-tuning → Influence function computation → Desiderata evaluation → Results analysis

**Design tradeoffs:** The paper uses synthetic knowledge domains for LLMs to ensure controlled experiments, but this limits generalizability to real-world scenarios. The binary success/failure classification simplifies evaluation but may miss nuanced model behaviors.

**Failure signatures:** If influence functions show no semantic decay, this indicates the model treats all knowledge equally regardless of relevance. Lack of sparsity suggests the model cannot identify truly important training examples. Asymmetric influences in CNNs would indicate architectural issues.

**3 first experiments:**
1. Verify symmetric influence functions in CNN fine-tuning by testing multiple image classes and noise levels
2. Test LLM influence function asymmetry using different prompt templates on the same synthetic knowledge domains
3. Compare influence function properties across different fine-tuning methods (full fine-tuning vs adapters) on the same tasks

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Synthetic knowledge domains may not capture the complexity of real-world fine-tuning scenarios
- Binary classification of task success vs failure may oversimplify nuanced model behavior
- The six proposed desiderata lack formal theoretical grounding and may reflect preferences rather than universal requirements

## Confidence
High: CNN experimental procedures and consistent findings across FashionMNIST and MNIST datasets
Medium: The conceptual framework of influence functions and proposed desiderata
Low: LLM experiments with Phi-3 on synthetic domains, binary success metrics, and limited prompting strategies

## Next Checks
1. Replication of the LLM experiments using multiple real-world datasets and established fine-tuning benchmarks to verify whether the observed violations persist outside synthetic domains
2. Testing additional fine-tuning methods (adapter-based approaches, LoRA, full fine-tuning) to determine if the influence function limitations are method-specific or inherent to LLMs
3. Exploring alternative metrics for influence function evaluation that might better capture the nuances of LLM learning behavior, particularly for knowledge-intensive tasks where binary success/failure may be inadequate