---
ver: rpa2
title: Deductive Closure Training of Language Models for Coherence, Accuracy, and
  Updatability
arxiv_id: '2401.08574'
source_url: https://arxiv.org/abs/2401.08574
tags:
- seed
- language
- documents
- training
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called Deductive Closure Training
  (DCT) that leverages language models' reasoning capabilities during inference to
  improve their reliability at training time. The core idea is to use language models
  to generate additional text implied by or contradicting seed documents, reason globally
  about the correctness of this generated text, and fine-tune on text inferred to
  be correct.
---

# Deductive Closure Training of Language Models for Coherence, Accuracy, and Updatability

## Quick Facts
- arXiv ID: 2401.08574
- Source URL: https://arxiv.org/abs/2401.08574
- Authors: Afra Feyza Akyürek; Ekin Akyürek; Leshem Choshen; Derry Wijaya; Jacob Andreas
- Reference count: 40
- Primary result: DCT improves fact verification accuracy by 26% (supervised) and 12% (unsupervised) on CREAK dataset

## Executive Summary
This paper introduces Deductive Closure Training (DCT), a method that leverages language models' reasoning capabilities during inference to improve their reliability at training time. The approach generates additional text implied by or contradicting seed documents, reasons globally about the correctness of this generated text, and fine-tunes on text inferred to be correct. The authors demonstrate significant improvements in language model fact verification, text generation accuracy, and updatability across three datasets.

## Method Summary
DCT operates through a pipeline of document generation, consistency evaluation, and fine-tuning. Given seed text, an LM generates implications and contradictions, creating a set of related documents. The LM then evaluates the logical consistency between these documents and assigns truth values. Finally, the LM is fine-tuned on the subset of documents identified as most likely to be true and logically consistent. This process can be applied in both supervised settings (using provided seed claims) and unsupervised settings (generating seeds from LM's own knowledge).

## Key Results
- On CREAK fact verification dataset, supervised DCT improves accuracy by 26% and unsupervised DCT improves accuracy by 12%
- On MQUAKE question answering dataset, DCT outperforms state-of-the-art baselines by 3-9% accuracy
- On Reversal Curse dataset, DCT improves accuracy by 18-46% on reversed statements without significantly hurting performance on original questions

## Why This Works (Mechanism)

### Mechanism 1
- Language models can generate logical implications and contradictions of seed documents with sufficient accuracy to be useful for training
- The LM samples text conditioned on prompts asking for implications and contradictions, creating a set of related documents that can be evaluated for consistency
- Core assumption: The LM has learned sufficient world knowledge during pretraining to generate meaningful implications and contradictions when prompted

### Mechanism 2
- The LM can accurately evaluate the logical consistency between generated documents and assign truth values
- The LM is prompted to estimate the probability that each document is true, and these probabilities are combined with logical consistency checks to identify the most likely consistent set
- Core assumption: The LM's probability estimates for truth values are reliable enough to support consistency evaluation

### Mechanism 3
- Fine-tuning on documents inferred to be true improves the LM's factual accuracy and coherence
- The LM is fine-tuned on the subset of documents identified as most likely to be true and logically consistent, reinforcing these patterns
- Core assumption: The subset of documents identified as true and consistent represents high-quality training signal

## Foundational Learning

- **Logical consistency and deductive closure**: DCT explicitly relies on identifying logical implications and contradictions, and ensuring the final set of documents is consistent
  - Why needed here: The entire method is built on the framework of deductive reasoning
  - Quick check question: If document A states "All birds can fly" and document B states "Penguins are birds", what must be true for logical consistency?

- **Language model prompting and generation**: DCT heavily relies on generating documents through prompting and evaluating them through prompting
  - Why needed here: The method uses the LM itself to generate and evaluate content
  - Quick check question: What prompt template would you use to ask an LM to generate logical implications of a given statement?

- **Fine-tuning and parameter-efficient adaptation**: The final step of DCT is fine-tuning the LM on the selected documents
  - Why needed here: The method needs to modify the LM's parameters to incorporate the learned consistency patterns
  - Quick check question: Why might LoRA be preferred over full fine-tuning for DCT?

## Architecture Onboarding

- **Component map**: Input seed documents → Document generation (implications/contradictions) → Consistency evaluation (truth value assignment) → Fine-tuning on inferred-true documents
- **Critical path**: Seed document → Generation → Evaluation → Fine-tuning
- **Design tradeoffs**: Tradeoff between generation breadth (more implications/contradictions) and evaluation quality (time/compute for consistency checking)
- **Failure signatures**: Inconsistent truth value assignments, poor generation quality, failure to propagate edits
- **First 3 experiments**:
  1. Run unsupervised DCT on a small set of seed documents and evaluate accuracy on a held-out test set
  2. Compare different generation strategies (implications only vs implications+contradictions) for effect on performance
  3. Test the effect of the double-checking step on generation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DCT scale with the size and complexity of the generated document graphs beyond a single layer of implications?
- Basis in paper: The paper mentions that DCT could in principle be applied to arbitrary graphs of relations between statements, but experiments only use a single layer of implications from seed data
- Why unresolved: The authors did not experiment with multi-hop DCT or more complex graph structures
- What evidence would resolve it: Experiments comparing DCT performance with varying graph depths and structures

### Open Question 2
- Question: Does DCT exhibit systematic biases or differences in accuracy for certain types of factual content, particularly across different languages or domains?
- Basis in paper: The authors note that all experiments use English text and acknowledge the possibility that DCT may behave differently in different languages
- Why unresolved: The paper only evaluates on English-language datasets and does not analyze performance across different content domains or languages
- What evidence would resolve it: Cross-linguistic experiments and systematic analysis of DCT performance across diverse factual domains

### Open Question 3
- Question: What is the optimal balance between supervised and unsupervised seed document generation for maximizing DCT performance across different tasks?
- Basis in paper: The authors show that semi-supervised learning combining both supervised and self-supervised data improves performance
- Why unresolved: The experiments use fixed ratios of supervised to unsupervised data without exploring the full parameter space
- What evidence would resolve it: Systematic ablation studies varying the ratio of supervised to unsupervised data across multiple tasks

## Limitations

- The method's reliance on the language model's ability to generate meaningful implications and contradictions may not hold for all domains or model sizes
- The computational cost of generating and evaluating the full deductive closure may limit scalability to larger models or more complex domains
- The unsupervised variant's performance heavily depends on the quality of model-generated seed documents, which introduces potential error propagation

## Confidence

- **High Confidence**: The core mechanism of using language models to generate implications and contradictions, followed by consistency evaluation and fine-tuning, is technically sound and well-explained
- **Medium Confidence**: The claim that DCT improves updatability through "edit propagation" is supported by the Reversal Curse results, but the mechanism could benefit from more detailed analysis
- **Low Confidence**: The scalability of DCT to larger models and its effectiveness on open-ended generation tasks remain unclear from the current results

## Next Checks

1. **Scalability Test**: Evaluate DCT on larger language models (e.g., Llama-2-13B or 70B) to assess computational requirements and performance scaling

2. **Domain Generalization**: Test DCT on domains outside the current datasets, particularly in specialized fields like medicine or law where logical consistency is critical

3. **Robustness Analysis**: Systematically evaluate DCT's performance when exposed to noisy, contradictory, or adversarial seed documents to identify failure modes in the consistency evaluation process