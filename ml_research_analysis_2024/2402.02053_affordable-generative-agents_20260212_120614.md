---
ver: rpa2
title: Affordable Generative Agents
arxiv_id: '2402.02053'
source_url: https://arxiv.org/abs/2402.02053
tags:
- agents
- agent
- arxiv
- events
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Affordable Generative Agents (AGA), a framework
  that significantly reduces the cost of simulating believable LLM-based agents while
  maintaining performance. AGA optimizes agent-environment interactions through a
  Lifestyle Policy that reuses learned plans instead of repeated LLM inferences, and
  improves inter-agent interactions using a Social Memory module that compresses dialogue
  information and models social relationships.
---

# Affordable Generative Agents

## Quick Facts
- arXiv ID: 2402.02053
- Source URL: https://arxiv.org/abs/2402.02053
- Authors: Yangbin Yu; Qin Zhang; Junyou Li; Qiang Fu; Deheng Ye
- Reference count: 33
- Key outcome: Reduces token consumption by up to 68.9% compared to baseline Generative Agents while maintaining performance

## Executive Summary
This paper introduces Affordable Generative Agents (AGA), a framework that significantly reduces the cost of simulating believable LLM-based agents while maintaining performance. AGA achieves this through two main optimizations: a Lifestyle Policy that reuses learned action sequences instead of repeated LLM inferences, and a Social Memory module that compresses dialogue information and models social relationships. Experiments demonstrate that AGA achieves comparable performance in social interaction and task completion while substantially reducing token consumption.

## Method Summary
AGA optimizes agent-environment interactions through a Lifestyle Policy that stores and reuses learned action sequences, eliminating repetitive LLM inferences. The framework also improves inter-agent interactions by compressing dialogue information into concise summary events and explicitly modeling social relationships. When encountering a new plan, AGA checks for similar stored plans using embedding similarity and execution conditions; if found, it reuses the stored actions, otherwise it decomposes the plan through LLM inference and stores the resulting actions for future reuse.

## Key Results
- Reduces token consumption by up to 68.9% compared to baseline Generative Agents
- Maintains comparable performance in social interaction and task completion
- Demonstrates that agents have a finite behavioral range in fixed environments, validated through cumulative activity generation experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing repeated LLM inferences with learned policies significantly reduces token consumption while maintaining agent performance.
- Mechanism: The Lifestyle Policy stores action sequences from previous plan executions along with their execution conditions. When a similar plan is encountered, the policy retrieves the stored action sequence if environment conditions match, eliminating LLM inference.
- Core assumption: Agent behaviors in fixed environments follow finite patterns that can be captured and reused through policies.
- Evidence anchors: [abstract] "we substitute repetitive LLM inferences with learned policies"; [section 3.1] "The determination of execution condition is highly correlated with the environment"
- Break condition: If environment changes dynamically or agent encounters truly novel situations that don't match stored policies, framework must fall back to LLM inference.

### Mechanism 2
- Claim: Compressing dialogue information and modeling social relationships reduces inter-agent interaction costs while improving response quality.
- Mechanism: Instead of passing all retrieved memory events to LLM, framework summarizes them into concise "Summary Events" and explicitly models relationships/feelings between agents, reducing prompt length while providing targeted social context.
- Core assumption: Agents can generate believable responses with compressed social information rather than full memory streams.
- Evidence anchors: [abstract] "compress auxiliary dialogue information" and "model social relationships between agents"; [section 3.2] "Social Memory reduces costs of conversation by compressing auxiliary information"
- Break condition: If agents require detailed contextual information for specific interactions, compressed summaries may lack sufficient detail, potentially degrading response quality.

### Mechanism 3
- Claim: Agents have finite behavioral range in fixed environments, making complete behavior coverage possible through policy storage.
- Mechanism: By analyzing cumulative activity generation over multiple simulation runs, framework demonstrates agents eventually stop generating new activities, indicating bounded behavior space that can be fully captured by stored policies.
- Core assumption: LLM agents behave as role-playing characters constrained by predefined profiles and environment, limiting behavioral diversity.
- Evidence anchors: [section 5.3] "after numerous trials, the agents cease to generate new activities" and "agents can only generate believable behaviors within a certain range"
- Break condition: If agents incorporate external knowledge or environment becomes dynamic, behavioral range may expand beyond what can be captured through static policies.

## Foundational Learning

- Concept: Case-Based Reasoning (CBR)
  - Why needed here: Lifestyle Policy mechanism is directly inspired by CBR, where new problems are solved by adapting solutions to previously solved, similar issues
  - Quick check question: How does Lifestyle Policy determine whether a stored plan can be reused in current environment?

- Concept: Text embedding and similarity matching
  - Why needed here: Framework uses embedding models to extract features from plan descriptions and compare them with stored plans using cosine similarity to determine plan matches
  - Quick check question: What threshold value is used to determine whether two plans are similar enough to reuse?

- Concept: Memory compression techniques
  - Why needed here: Social Memory module applies text compression principles to reduce length of dialogue prompts while maintaining information density
  - Quick check question: How does framework compress 30-45 relevant events into approximately 100 tokens?

## Architecture Onboarding

- Component map: Perception -> Lifestyle Policy (plan match check) -> Social Memory (dialogue handling) -> Execution -> Policy/Memory update
- Critical path: Perception module gathers environmental data and agent information, Lifestyle Policy checks for plan matches, Social Memory handles dialogue compression and relationship modeling, Execution module carries out actions and updates policy/memory based on outcomes
- Design tradeoffs:
  - High similarity threshold (0.97) ensures policy accuracy but may miss some reusable plans
  - Compressed dialogue summaries reduce costs but may lose some contextual nuance
  - Finite behavior assumption enables cost reduction but limits adaptability to dynamic environments
- Failure signatures:
  - High similarity threshold causes excessive LLM invocations when plans don't match exactly
  - Compressed dialogue summaries lead to contextually inappropriate responses
  - Stored policies become outdated when environment conditions change
- First 3 experiments:
  1. Test plan matching with varying similarity thresholds (0.9, 0.95, 0.97) to find optimal balance between reuse and accuracy
  2. Compare response quality when using full memory streams versus compressed Summary Events in dialogue generation
  3. Measure cumulative activity generation over increasing simulation runs to verify finite behavioral range claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper limit of believable behaviors an LLM agent can generate in a fixed environment, and how does this limit scale with environment complexity?
- Basis in paper: [explicit] The paper demonstrates that agents can only generate finite behaviors in fixed environments and introduces mind wandering to increase behavioral diversity.
- Why unresolved: The paper shows empirical evidence of finite behaviors but does not quantify the theoretical maximum or establish how environment complexity affects this limit.
- What evidence would resolve it: Systematic experiments varying environment complexity (number of items, possible interactions, social relationships) while measuring rate of new behavior generation across many simulation runs.

### Open Question 2
- Question: How does the quality of the Lifestyle Policy change over time as more plans are stored, and what is the optimal policy size for balancing cost reduction and behavior diversity?
- Basis in paper: [explicit] The paper mentions that Lifestyle Policy becomes more effective as it stores more policies, reducing costs by reusing existing plans.
- Why unresolved: The paper doesn't explore the diminishing returns of adding more policies or the point at which the policy becomes too large and inefficient to search.
- What evidence would resolve it: Longitudinal studies tracking policy effectiveness (cost reduction, behavior coverage) as number of stored plans increases, identifying point of diminishing returns.

### Open Question 3
- Question: How do different implementations of mind wandering (e.g., different sampling strategies, timing of random event injection) affect the diversity and believability of agent behaviors?
- Basis in paper: [explicit] The paper introduces mind wandering as a technique to increase behavioral diversity but only describes one implementation using weighted sampling based on repetition rate.
- Why unresolved: The paper presents only one mind wandering approach without comparing alternative implementations or analyzing their effects on behavior diversity.
- What evidence would resolve it: Comparative experiments testing different mind wandering strategies (different sampling distributions, timing of injection, types of random events) while measuring behavioral diversity metrics and human-likeness scores.

## Limitations
- Behavioral range claim rests on a single fixed environment (Stanford Town), not validated in larger or more dynamic environments
- 68.9% cost reduction figure comes from comparing against a baseline that isn't extensively characterized
- Framework assumes all environments have predictable, finite behavior patterns that can be captured through policies

## Confidence
- High confidence in token reduction mechanism (Lifestyle Policy) - follows well-established case-based reasoning principles
- Medium confidence in Social Memory compression effectiveness - concept is sound but limited empirical validation
- Low confidence in behavioral range claim - evidence based on single environment with limited agent profiles, no statistical analysis provided

## Next Checks
1. Test the behavioral saturation hypothesis in at least two additional environments with different complexity levels and agent profiles to determine if finite behavior pattern holds across diverse scenarios.

2. Conduct a systematic ablation study measuring response quality degradation as dialogue compression ratios increase, to identify point where information loss begins affecting interaction believability.

3. Implement dynamic environment scenarios where conditions change during simulation to measure how frequently framework falls back to LLM inference, validating whether cost savings are maintained in realistic usage patterns.