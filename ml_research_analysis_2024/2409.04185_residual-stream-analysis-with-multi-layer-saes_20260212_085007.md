---
ver: rpa2
title: Residual Stream Analysis with Multi-Layer SAEs
arxiv_id: '2409.04185'
source_url: https://arxiv.org/abs/2409.04185
tags:
- layer
- layers
- latent
- figure
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces multi-layer sparse autoencoders (MLSAEs) to
  study how information flows across layers in transformer language models. Unlike
  standard SAEs trained on single layers, MLSAEs are trained on residual stream activation
  vectors from all layers, enabling analysis of cross-layer representations.
---

# Residual Stream Analysis with Multi-Layer SAEs

## Quick Facts
- arXiv ID: 2409.04185
- Source URL: https://arxiv.org/abs/2409.04185
- Reference count: 40
- One-line primary result: Multi-layer sparse autoencoders reveal that latents are often active at single layers for individual tokens but active across multiple layers when aggregated over many tokens.

## Executive Summary
This paper introduces multi-layer sparse autoencoders (MLSAEs) to study how information flows across layers in transformer language models. Unlike standard SAEs trained on single layers, MLSAEs are trained on residual stream activation vectors from all layers, enabling analysis of cross-layer representations. The authors find that while individual latents are often active at only one layer for a given token, different tokens activate the same latents at different layers. Quantitatively, the variance of latent activation distributions over layers is two orders of magnitude greater when aggregating over many tokens compared to a single token. For larger models, latents are increasingly active across multiple layers, consistent with greater similarity between adjacent layer activations. These findings were preserved even when applying tuned-lens transformations to account for basis differences across layers.

## Method Summary
The authors train a single k-sparse autoencoder on residual stream activations from all transformer layers, treating activations from different layers as separate training examples with shared encoder/decoder parameters. They use Adam optimizer with learning rate 1e-4, expansion factors R = n/d (powers of 2 from 1 to 256), and sparsity k (powers of 2 from 16 to 512). The model is trained on 1 billion tokens from the Pile (excluding Books3), and evaluated on reconstruction error (FVU, MSE), sparsity (L1 norm), and downstream loss (delta CE, KL divergence). Tuned-lens transformations are optionally applied to account for basis differences across layers.

## Key Results
- MLSAEs achieve comparable reconstruction error to single-layer SAEs while enabling cross-layer analysis
- Individual latents are often active at a single layer for a given token, but different tokens activate the same latents at different layers
- Variance of latent activation distributions over layers is two orders of magnitude greater when aggregating over many tokens versus a single token
- Tuned-lens transformations do not substantially increase multi-layer latent activations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-layer SAEs capture cross-layer representation dynamics by training on activation vectors from every transformer layer.
- Mechanism: By treating activations from each layer as separate training examples, MLSAEs enforce parameter tying across layers, enabling direct comparison of feature activity patterns across the residual stream.
- Core assumption: Residual stream activations preserve information across layers and can be modeled by a single SAE.
- Evidence anchors:
  - [abstract] "multi-layer SAE (MLSAE): a single SAE trained on the residual stream activation vectors from every transformer layer"
  - [section] "we consider the activation vectors from each layer as separate training examples, which is equivalent to training a single SAE at each layer individually but with the parameters tied across layers"
  - [corpus] Weak evidence - corpus doesn't directly address this mechanism
- Break condition: If residual stream basis differs substantially between layers, a single SAE cannot capture shared structure effectively.

### Mechanism 2
- Claim: MLSAE latents can be active at different layers for different tokens, revealing how representations evolve through the network.
- Mechanism: For individual tokens, latents are often active at only one layer, but aggregating over many tokens shows latents are active at multiple layers, indicating layer-specific processing of different semantic concepts.
- Core assumption: The layer at which a latent activates varies systematically with the semantic content of tokens.
- Evidence anchors:
  - [abstract] "individual latents are often active at a single layer for a given token or prompt, but the layer at which an individual latent is active may differ for different tokens or prompts"
  - [section] "when aggregating over a large sample of 10 million tokens from the test set, we observed that most latents were active at multiple layers, but for a single prompt, most latent activations were isolated to a single layer"
  - [corpus] No direct evidence found
- Break condition: If all tokens activate the same latents at the same layers regardless of semantic content.

### Mechanism 3
- Claim: Tuned-lens transformations don't substantially change cross-layer activation patterns because they preserve the overall structure of residual stream dynamics.
- Mechanism: Pre-trained tuned-lens transformations map each layer's residual stream to a basis more similar to the output layer, but this doesn't fundamentally alter which latents are active at which layers.
- Core assumption: The tuned-lens transformation preserves the relative relationships between layers in the residual stream.
- Evidence anchors:
  - [abstract] "we relax the assumption that the residual stream basis is the same at every layer by applying pre-trained tuned-lens transformations to activation vectors before passing them to the encoder. Surprisingly, this does not obviously increase the extent of multi-layer latent activations"
  - [section] "the tuned-lens approach decreases the degree to which latents are active at multiple layers when aggregating over tokens"
  - [corpus] No direct evidence found
- Break condition: If tuned-lens transformations fundamentally altered the semantic content or temporal flow of information in the residual stream.

## Foundational Learning

- Concept: Sparse Autoencoders (SAEs)
  - Why needed here: Understanding how SAEs decompose dense activations into interpretable features is fundamental to grasping why MLSAEs work differently than single-layer SAEs.
  - Quick check question: What is the key difference between standard SAEs and the multi-layer SAEs proposed in this paper?

- Concept: Residual Stream Perspective
  - Why needed here: The paper's core insight relies on understanding that transformers preserve information across layers in the residual stream, which motivates the multi-layer approach.
  - Quick check question: Why would we expect activation vectors at adjacent layers to be relatively similar?

- Concept: Variance Analysis
  - Why needed here: The paper uses variance of latent activation distributions over layers as a quantitative measure to compare cross-layer activity patterns.
  - Quick check question: What does it mean if the variance of latent activations over layers is much larger when aggregating over many tokens versus a single token?

## Architecture Onboarding

- Component map: Residual stream activation extraction -> Multi-layer SAE training with parameter tying -> Latent activation analysis across layers -> Variance metric computation
- Critical path: 1) Collect activation vectors from all layers for training examples 2) Train single SAE on all layer activations with parameter tying 3) Analyze latent activation distributions across layers 4) Compute variance metrics to quantify cross-layer activity
- Design tradeoffs: MLSAEs sacrifice some layer-specific optimization for the ability to directly compare features across layers, but achieve comparable reconstruction error to single-layer SAEs.
- Failure signatures: Dead latents (rarely activated features), poor reconstruction error compared to single-layer SAEs, or latents that activate at the same layers for all tokens regardless of semantic content.
- First 3 experiments:
  1. Train MLSAE on Pythia-70m with default hyperparameters and verify reconstruction error matches single-layer SAEs
  2. Compute cosine similarities between adjacent layer activations to confirm residual stream preservation
  3. Plot heatmaps of latent activation distributions over layers for both aggregate and single-prompt cases to visualize cross-layer patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the difference between aggregate and single-token latent activation patterns across layers?
- Basis in paper: [explicit] The paper observes that while individual latents are often active at a single layer for a given token or prompt, different tokens activate the same latents at different layers, and quantifies this by showing that variance of latent activation distributions over layers is much greater when aggregating over many tokens compared to a single token.
- Why unresolved: The paper identifies this phenomenon but does not investigate the underlying mechanisms that cause latents to be active at different layers for different tokens.
- What evidence would resolve it: Experimental results showing whether this pattern arises from token-specific semantic content, positional embeddings, or other factors would clarify the mechanism.

### Open Question 2
- Question: How do tuned-lens transformations affect the interpretability and circuit analysis utility of MLSAE features?
- Basis in paper: [explicit] The paper applies tuned-lens transformations to account for basis differences across layers but finds this does not obviously increase multi-layer latent activations, with variance ratios actually decreasing.
- Why unresolved: The paper does not explore whether tuned-lens MLSAE features provide better or worse interpretability for circuit analysis compared to standard MLSAE features.
- What evidence would resolve it: Systematic comparison of circuit identification success rates using tuned-lens vs standard MLSAE features would determine their relative utility.

### Open Question 3
- Question: What is the optimal feature-stacked vs data-stacked approach for capturing cross-layer superposition in transformers?
- Basis in paper: [inferred] The paper briefly discusses this as an alternative approach but discards it, stating that a single set of sparse features describing activations at every layer makes it difficult to understand information flow.
- Why unresolved: The paper does not empirically test whether feature-stacked MLSAEs could capture cross-layer superposition phenomena that data-stacked approaches miss.
- What evidence would resolve it: Comparative analysis of data-stacked vs feature-stacked MLSAE performance on tasks requiring cross-layer superposition understanding would reveal the optimal approach.

## Limitations

- The analysis relies on variance metrics that assume specific relationships between latent activation patterns and semantic processing without direct validation
- The paper does not investigate causal mechanisms underlying why latents are active at different layers for different tokens
- The interpretation of tuned-lens transformation results as "surprising" lacks exploration of alternative explanations

## Confidence

- **High Confidence**: The technical implementation of MLSAEs and the basic finding that individual latents are often active at single layers for single tokens. The variance ratio calculation methodology is sound.
- **Medium Confidence**: The interpretation that variance differences between aggregate and single-token cases reveal meaningful cross-layer representation dynamics. The claim about tuned-lens transformations not increasing multi-layer activations.
- **Low Confidence**: The broader mechanistic claims about what cross-layer activation patterns reveal about information flow and representation evolution in transformers. The connection between observed patterns and specific semantic processing mechanisms.

## Next Checks

1. Conduct ablation studies varying the sparsity level k and expansion factor R to determine how sensitive the observed cross-layer patterns are to architectural hyperparameters of the MLSAE.

2. Perform controlled experiments with synthetic data where the ground truth layer-specific processing is known, to validate that MLSAE latent activation patterns correctly recover the expected cross-layer dynamics.

3. Design experiments using feature intervention or activation patching to test whether latents active at different layers for different tokens have causal effects on model outputs that match the claimed semantic processing roles.