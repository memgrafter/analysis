---
ver: rpa2
title: Examining the Role of Relationship Alignment in Large Language Models
arxiv_id: '2410.01708'
source_url: https://arxiv.org/abs/2410.01708
tags:
- social
- comments
- context
- human
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLM-generated comments can be aligned
  with human ones by incorporating social relationship information. The study analyzes
  Facebook post-comment data and compares LLM-generated comments with human comments
  across eight semantic tones (e.g., emotional support, humor, self-disclosure) while
  varying age, gender, and friendship closeness.
---

# Examining the Role of Relationship Alignment in Large Language Models

## Quick Facts
- arXiv ID: 2410.01708
- Source URL: https://arxiv.org/abs/2410.01708
- Reference count: 14
- One-line primary result: LLM-generated comments can align with human comments without social context prompts, but explicit social relationship information in prompts doesn't consistently improve similarity and sometimes decreases it.

## Executive Summary
This paper investigates whether LLM-generated comments can be aligned with human ones by incorporating social relationship information. The study analyzes Facebook post-comment data and compares LLM-generated comments with human comments across eight semantic tones (e.g., emotional support, humor, self-disclosure) while varying age, gender, and friendship closeness. Results show that while LLMs can generate comments similar to humans without social context prompts, explicitly including social relationship information in prompts does not consistently improve similarity and sometimes decreases it. The findings suggest that LLMs struggle to generalize personalized comments through prompting alone, likely because social context wasn't included in their training data.

## Method Summary
The study uses 665k Facebook post-comment pairs from U.S. users ≥18, with features including commenter/poster gender, age, and friendship closeness (stranger, non-close friend, close friend). Eight semantic tones (Surprised, Funny, Insult, Emotional Support, Self-Disclosure, Worried, Thankful, Relaxed) are labeled via classifier. LLM comments are generated using Llama 3.0 (70B) under three prompt conditions: no context, subset of context, and all context. The similarity between human and LLM-generated comments is measured through regression coefficients predicting semantic tone from social context variables, comparing the association strength between the two groups.

## Key Results
- LLM-generated comments and human comments are equally sensitive to social context, suggesting LLMs can comprehend semantics from the original post alone.
- When no context information is included in the prompt, the tones of the generated comments are moderately similar to the tone of the original human comments.
- Adding social context to prompts strengthens similarity for some tones (surprised, insult) but reduces it for others, with overall similarity marginally reduced.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Social relationship context is implicitly encoded in human language, enabling LLMs to infer it from post text alone.
- Mechanism: The LLM learns to associate linguistic cues (e.g., emotional tone, vocabulary choice) with social relationships during training, allowing it to generate socially appropriate comments without explicit context.
- Core assumption: Training data contains sufficient social relationship signals embedded in human-human interactions.
- Evidence anchors:
  - [abstract] "LLM-generated comments and human comments are equally sensitive to social context, suggesting that LLMs can comprehend semantics from the original post alone."
  - [section] "When no context information is included in the prompt, the tones of the generated comments are moderately similar to the tone of the original human comments, with the similarity coefficients ranging from .309 for surprised to .585 for worried."
  - [corpus] Weak - corpus shows related work on social media analysis but lacks direct evidence of social signal encoding in training data.
- Break condition: If training data lacks sufficient social relationship examples, the implicit encoding mechanism fails and explicit context becomes necessary.

### Mechanism 2
- Claim: LLMs overreact to explicit social context prompts, overweighting this information relative to implicit social cues in the post.
- Mechanism: When given explicit social information, the model prioritizes this over subtle linguistic signals, leading to less accurate social alignment than when inferring context from post content alone.
- Core assumption: LLMs are sensitive to prompt engineering and can be biased by explicit instructions.
- Evidence anchors:
  - [abstract] "When we include all social relationship information in the prompt, the similarity between human comments and LLM-generated comments decreases."
  - [section] "Adding context information to the prompt strengthens the similarity for surprised and insult but reduces it for four others... Overall, the similarity between the human and generated comments was marginally reduced when the prompt includes the social context information."
  - [corpus] Weak - corpus contains related work on persona alignment but lacks direct evidence of prompt sensitivity for social context.
- Break condition: If the model architecture better balances explicit and implicit social signals, the overreaction problem diminishes.

### Mechanism 3
- Claim: Selection bias in who comments on posts creates correlation between post content and commenter demographics that LLMs learn during training.
- Mechanism: Posts with certain emotional tones or topics attract commenters with specific demographic profiles, creating a statistical association that LLMs learn and replicate without needing explicit social context.
- Core assumption: Commenter selection is not random and correlates with both post content and commenter demographics.
- Evidence anchors:
  - [abstract] "even without including social context information in the prompt, LLM-generated comments and human comments are equally sensitive to social context, suggesting that LLMs can comprehend semantics from the original post alone."
  - [section] "One explanation is that the LLM is capturing the tone of the original post and generates a comment that is similar in tone to the original post (e.g., the LLM generates a funny comment in response to a funny post), much as people do."
  - [corpus] Weak - corpus contains related work on social media engagement but lacks direct evidence of commenter selection bias.
- Break condition: If commenter selection becomes more random or diverse, the learned correlation between post content and commenter demographics weakens.

## Foundational Learning

- Concept: Linear mixed effects regression
  - Why needed here: To model the relationship between social context variables and semantic tone while accounting for nested data structure (multiple comments per post)
  - Quick check question: What does the random effect for post ID capture in the regression models?

- Concept: Log transformation for skewed distributions
  - Why needed here: Comment semantic tone scores are often right-skewed, and log transformation normalizes the distribution for regression analysis
  - Quick check question: Why do we interpret regression coefficients differently when both independent and dependent variables are log-transformed?

- Concept: Inverse propensity weighting
  - Why needed here: To correct for selection bias in who comments on which posts, ensuring the analysis accounts for differential commenting probabilities
  - Quick check question: How does inverse propensity weighting address the issue that certain users are more likely to comment on certain types of posts?

## Architecture Onboarding

- Component map: Facebook public posts → preprocessing → semantic tone classification → LLM generation → similarity analysis
- Critical path: 1) Collect and preprocess Facebook post-comment pairs 2) Apply semantic tone classifier to human comments 3) Generate LLM comments with different prompt conditions 4) Apply semantic tone classifier to LLM comments 5) Run regression analyses comparing human and LLM comments 6) Analyze results and draw conclusions
- Design tradeoffs: Using pre-trained LLM vs fine-tuning on social context data; prompt engineering vs architectural changes for social context incorporation; broad semantic tone categories vs more granular emotional analysis
- Failure signatures: LLM comments show no correlation with human comment tones; explicit social context prompts significantly improve similarity; regression models fail to converge or show nonsensical coefficients; semantic tone classifier accuracy drops significantly on LLM comments
- First 3 experiments: 1) Compare semantic tone similarity between human-human and LLM-human comment pairs for a small sample to validate methodology 2) Test different prompt formulations (order of social context, zero-shot CoT) on a subset of posts to optimize prompt engineering 3) Analyze regression coefficient differences between human and LLM comments by semantic tone category to identify specific social context effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating social relationship information during LLM training (rather than just prompting) reduce the discrepancy between generated and human comments?
- Basis in paper: [inferred] The authors note that LLMs' training data lacks explicit social context information, and suggest this might explain why prompting with social context doesn't improve alignment. They explicitly state future work should "consider improving LLMs' ability to generate socially-realistic comments, such as incorporating social context information at the training stage."
- Why unresolved: The paper only tested prompt engineering approaches and found they didn't consistently improve similarity. They didn't experiment with training-time incorporation of social context.
- What evidence would resolve it: Training an LLM with social relationship metadata included in the training corpus and comparing its generated comments to both baseline LLMs and human comments across the same eight semantic tones.

### Open Question 2
- Question: What specific aspects of social relationships (beyond basic age, gender, and friendship strength) are most important for generating contextually appropriate comments?
- Basis in paper: [explicit] The authors note that human commenters may use "additional social context or relationship information that is invisible to the LLM" and provide an example where a commenter alludes to a shared golf history. They also mention that social factors "extend beyond the stereotypical associations with age, gender, and relationship strength used in our LLM prompts."
- Why unresolved: The study only tested basic demographic and friendship strength variables, but observed that these were insufficient for capturing the full complexity of human social communication.
- What evidence would resolve it: Analyzing human-human comment exchanges to identify specific linguistic markers or contextual clues people use to signal relationship nuances, then testing whether incorporating these richer social signals into LLM prompts improves comment alignment.

### Open Question 3
- Question: Does the public nature of Facebook posts systematically bias the observed relationship between social context and comment tone compared to private communications?
- Basis in paper: [explicit] The authors note an inconsistency where "people self-disclose less when commenting on a close friend's post than when responding to strangers" and suggest this "may result from the public nature of post and comment pairs used in the current study; people may not want to reveal intimate personal information even when responding to close friends because their responses are open to the world to see."
- Why unresolved: The study only analyzed public Facebook posts, so it cannot distinguish whether the observed social context effects reflect authentic communication patterns or are artifacts of public visibility.
- What evidence would resolve it: Comparing the same social context effects on comment tone between public posts and private messages or small-group communications on the same platform, controlling for other variables.

## Limitations
- The study only tested one LLM (Llama 3.0 70B), making it unclear whether results generalize across different model architectures.
- The semantic tone classifier's accuracy on LLM-generated comments versus human comments is unknown, potentially affecting similarity measurements.
- The Facebook dataset's demographic representativeness and temporal specificity (May 2024, U.S. users) may limit external validity.

## Confidence

- **High confidence**: The finding that LLMs can generate socially appropriate comments without explicit context (Mechanism 1) is well-supported by consistent similarity coefficients across multiple semantic tones.
- **Medium confidence**: The observation that explicit social context prompts sometimes decrease similarity (Mechanism 2) is supported but requires further investigation into prompt engineering effects.
- **Low confidence**: The claim about selection bias creating correlations between post content and commenter demographics (Mechanism 3) lacks direct empirical support in the corpus and needs additional validation.

## Next Checks

1. **Cross-model validation**: Test whether the observed prompt sensitivity effects replicate across different LLM architectures (e.g., GPT-4, Claude) to determine if the findings are model-specific or generalizable.

2. **Classifier performance audit**: Compare semantic tone classifier accuracy on human versus LLM-generated comments using human validation to ensure measurement consistency and identify potential bias in tone classification.

3. **Fine-tuning experiment**: Compare prompting-based social context incorporation with fine-tuning approaches on the same dataset to determine if architectural changes outperform prompt engineering for social relationship alignment.