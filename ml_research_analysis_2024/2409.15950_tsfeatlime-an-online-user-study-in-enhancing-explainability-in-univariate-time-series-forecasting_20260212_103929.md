---
ver: rpa2
title: 'TSFeatLIME: An Online User Study in Enhancing Explainability in Univariate
  Time Series Forecasting'
arxiv_id: '2409.15950'
source_url: https://arxiv.org/abs/2409.15950
tags:
- time
- series
- participants
- data
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents TSFeatLIME, a framework for explaining univariate
  time series forecasting models by extending TSLIME. The approach integrates auxiliary
  features (lag, rolling window, and expanding window) and considers pairwise Euclidean
  distances between queried and perturbed time series to improve surrogate model fidelity.
---

# TSFeatLIME: An Online User Study in Enhancing Explainability in Univariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2409.15950
- Source URL: https://arxiv.org/abs/2409.15950
- Reference count: 17
- Key outcome: Online user study shows TSFeatLIME explanations significantly more effective for non-CS participants in predicting model output changes

## Executive Summary
This paper introduces TSFeatLIME, a framework that extends TSLIME to explain univariate time series forecasting models. The approach integrates auxiliary features (lag, rolling window, and expanding window) and incorporates pairwise Euclidean distances between queried and perturbed time series to improve surrogate model fidelity. Experiments on furniture sales and electricity consumption datasets demonstrate that distance weighting significantly enhances surrogate model fidelity without sacrificing black-box accuracy. A user study with 160 participants reveals that explanations are more effective for non-computer science participants in predicting model output changes, while both explanations are found helpful regardless of background.

## Method Summary
TSFeatLIME extends TSLIME by incorporating pairwise Euclidean distances between queried and perturbed time series into surrogate model training, using distance-weighted linear regression to improve local fidelity. The framework also integrates auxiliary features including rolling and expanding window statistics alongside lag features. Block bootstrap perturbation preserves temporal dependencies while generating samples near the queried series. The approach is evaluated on furniture sales and electricity consumption datasets, comparing fidelity with and without distance weighting across different feature combinations.

## Key Results
- Distance weighting significantly improves surrogate model fidelity without sacrificing black-box accuracy
- Auxiliary features provide additional explanatory information without major fidelity impact
- Explanations are significantly more effective for non-computer science participants in predicting model output changes
- Both explanations found helpful by participants regardless of background

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating pairwise Euclidean distance between queried time series and perturbed samples improves surrogate model fidelity without sacrificing black-box accuracy.
- Mechanism: The distance weighting prioritizes samples closer to the queried time series during surrogate model training, ensuring local explanations are more faithful to the black-box model's behavior in the region of interest.
- Core assumption: Samples closer to the queried time series are more representative of the local behavior the explanation should capture.
- Evidence anchors:
  - [abstract] "incorporates the pairwise Euclidean distances between the queried time series and the generated samples to improve the fidelity of the surrogate models"
  - [section] "Our results demonstrate that considering this can dramatically improve the model's ability to mimic without sacrificing black-box accuracy"
- Break condition: If the distance weighting overemphasizes noise in the perturbations or if the black-box model's behavior is not locally smooth, fidelity improvements may not materialize.

### Mechanism 2
- Claim: Adding auxiliary features (rolling window and expanding window) provides additional explanatory information without significantly affecting surrogate model fidelity.
- Mechanism: These features capture different temporal patterns (short-term trends via rolling windows, cumulative effects via expanding windows) that complement lag features, giving users a richer understanding of feature importance.
- Core assumption: Auxiliary features provide meaningful temporal information that users can interpret alongside lag features.
- Evidence anchors:
  - [abstract] "integrating an auxiliary feature into the surrogate model"
  - [section] "Our experimental findings reveal that incorporating auxiliary features neither significantly increases nor decreases the behaviour of surrogate models when mimicking black-box models"
- Break condition: If auxiliary features introduce multicollinearity or if the surrogate model cannot effectively utilize them, fidelity may degrade.

### Mechanism 3
- Claim: Explanations are more effective for participants without computer science backgrounds in predicting model output changes.
- Mechanism: Non-CS participants may approach the explanation interface with less preconceived notions about model behavior, allowing them to engage more openly with the visual and textual explanations provided.
- Core assumption: CS participants have different expectations or analytical approaches that affect their interaction with the explanation interface.
- Evidence anchors:
  - [abstract] "the user study suggests that the explanations were significantly more effective for participants without a computer science background"
  - [section] "Among the non-computer science group, a significant difference was observed...leading to the rejection of the null hypothesis"
- Break condition: If the explanation interface is not designed with non-CS users in mind, or if CS participants have specific training that makes them better at counterfactual reasoning, the observed effect may reverse.

## Foundational Learning

- Concept: Block Bootstrap for time series perturbation
  - Why needed here: Standard LIME perturbation disrupts temporal dependencies in time series data, so block bootstrap preserves autocorrelation while generating perturbed samples.
  - Quick check question: How does block bootstrap differ from standard bootstrap when applied to time series data?

- Concept: Local surrogate model training
  - Why needed here: The surrogate model approximates the black-box model's behavior locally, allowing interpretable explanations while maintaining fidelity to the original predictions.
  - Quick check question: Why is it important that the surrogate model is trained only on perturbed samples near the queried time series?

- Concept: Euclidean distance weighting in local explanations
  - Why needed here: Weighting perturbed samples by their distance from the queried series ensures the explanation focuses on the most relevant local region.
  - Quick check question: What would happen to explanation fidelity if all perturbed samples were weighted equally regardless of distance?

## Architecture Onboarding

- Component map:
  - Data preprocessing -> Block Bootstrap perturbation -> Feature extraction (lag, rolling, expanding) -> Black-box prediction -> Distance calculation -> Surrogate model training -> Explanation generation -> User interface
  - External dependencies: Prolific for user study recruitment, block bootstrap implementation, linear regression model

- Critical path:
  1. Generate perturbed samples using block bootstrap
  2. Extract features from perturbed samples
  3. Get black-box predictions for perturbed samples
  4. Calculate pairwise distances
  5. Train weighted linear regression surrogate model
  6. Extract feature importance weights for explanation

- Design tradeoffs:
  - Block length vs. number of perturbed samples: Longer blocks preserve more temporal structure but generate fewer unique samples
  - Feature selection: More auxiliary features provide richer explanations but may increase model complexity and reduce interpretability
  - Distance weighting strength: Stronger weighting focuses on local behavior but may reduce robustness to perturbations

- Failure signatures:
  - Low fidelity scores: May indicate poor perturbation strategy, inadequate feature selection, or issues with black-box model predictions
  - High variance in explanations: Could suggest insufficient number of perturbed samples or inappropriate block length
  - User confusion in study: Might indicate explanations are too complex or interface design issues

- First 3 experiments:
  1. Test different block lengths (3, 4, 5) and swap counts (2, 3, 4) to find optimal perturbation parameters
  2. Compare fidelity with and without distance weighting across different auxiliary feature combinations
  3. Run user study with a smaller sample size to validate interface design before full deployment

## Open Questions the Paper Calls Out
None

## Limitations
- User study findings based on single online study with 160 participants; generalizability across domains and models not established
- Specific mechanisms behind background differences in explanation effectiveness remain unclear
- No assessment of surrogate model stability across multiple perturbations of same queried time series

## Confidence
**High Confidence**: Technical implementation of TSFeatLIME follows established methods; experimental results showing improved fidelity with distance weighting are robust
**Medium Confidence**: Claims about auxiliary features providing meaningful information supported by experiments but practical value for end-users not empirically validated
**Medium Confidence**: User study results showing differential effectiveness across CS and non-CS backgrounds are statistically significant but may be influenced by unmeasured confounding factors

## Next Checks
1. **Stability Analysis**: Conduct multiple perturbations of the same queried time series to measure variance in explanations and assess whether TSFeatLIME produces consistent results across different random seeds.

2. **Cross-Domain Validation**: Apply TSFeatLIME to time series datasets from different domains (finance, healthcare, sensor data) and with different black-box models (neural networks, gradient boosting, ARIMA) to test generalizability of the approach.

3. **Controlled User Study**: Design a follow-up user study that systematically varies the complexity of explanations and measures how CS versus non-CS participants perform across different task types (prediction, counterfactual reasoning, model debugging) to isolate the mechanisms behind the observed background differences.