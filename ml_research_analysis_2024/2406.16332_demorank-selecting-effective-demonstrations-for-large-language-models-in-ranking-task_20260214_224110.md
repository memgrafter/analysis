---
ver: rpa2
title: 'DemoRank: Selecting Effective Demonstrations for Large Language Models in
  Ranking Task'
arxiv_id: '2406.16332'
source_url: https://arxiv.org/abs/2406.16332
tags:
- demonstration
- passage
- training
- demonstrations
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DemoRank, a framework for selecting effective
  demonstrations for large language models in ranking tasks. It addresses the challenge
  of selecting appropriate in-context demonstrations by formulating the process as
  a retrieve-then-rerank approach.
---

# DemoRank: Selecting Effective Demonstrations for Large Language Models in Ranking Task

## Quick Facts
- arXiv ID: 2406.16332
- Source URL: https://arxiv.org/abs/2406.16332
- Reference count: 26
- Primary result: DemoRank achieves 3 points higher than E5 on HotpotQA and 7 points higher than BM25 on FEVER in 3-shot settings

## Executive Summary
This paper introduces DemoRank, a framework for selecting effective demonstrations for large language models in ranking tasks. It addresses the challenge of selecting appropriate in-context demonstrations by formulating the process as a retrieve-then-rerank approach. DemoRank first trains a demonstration retriever using LLM feedback and then introduces a dependency-aware demonstration reranker. To train the reranker efficiently, the paper proposes a method to construct dependency-aware training samples by iteratively selecting demonstrations that maximize LLM feedback when combined with previously selected ones.

## Method Summary
DemoRank introduces a novel framework for selecting demonstrations in ranking tasks with Large Language Models. The approach consists of two main stages: a retrieval stage where a demonstration retriever is trained using LLM feedback, and a reranking stage where a dependency-aware demonstration reranker is introduced. The reranker is trained using a novel method that constructs dependency-aware training samples by iteratively selecting demonstrations that maximize LLM feedback when combined with previously selected ones. This retrieve-then-rerank approach aims to improve the quality of in-context demonstrations for few-shot learning scenarios in ranking tasks.

## Key Results
- DemoRank demonstrates strong performance across multiple ranking datasets in both in-domain and out-of-domain scenarios
- Achieves 3 points higher than E5 on HotpotQA and 7 points higher than BM25 on FEVER in 3-shot settings
- Shows advantages over supervised models when training data is limited and performs well with different demonstration numbers

## Why This Works (Mechanism)
DemoRank works by addressing the challenge of selecting appropriate in-context demonstrations for Large Language Models in ranking tasks. The framework formulates demonstration selection as a retrieve-then-rerank problem, which allows for more sophisticated selection beyond simple similarity metrics. The dependency-aware reranker captures interactions between demonstrations, ensuring that selected demonstrations complement each other rather than being redundant. By training the retriever using LLM feedback, the system learns to identify demonstrations that are not just topically relevant but also instructionally effective. The iterative construction of training samples ensures that the reranker learns to select demonstrations that maximize overall performance when used together.

## Foundational Learning
- **In-context learning**: Why needed - enables LLMs to perform tasks with few examples; Quick check - observe performance on few-shot tasks
- **Ranking tasks**: Why needed - evaluates model's ability to order items by relevance; Quick check - measure ranking accuracy metrics
- **Retrieval and reranking**: Why needed - allows multi-stage selection of relevant content; Quick check - compare retrieval-only vs. reranking performance
- **LLM feedback**: Why needed - provides more nuanced evaluation than static metrics; Quick check - verify feedback correlates with downstream performance
- **Dependency-aware selection**: Why needed - prevents redundancy and captures complementary demonstrations; Quick check - analyze diversity and complementarity of selected demonstrations
- **Few-shot learning**: Why needed - addresses scenarios with limited labeled data; Quick check - test performance across different shot settings

## Architecture Onboarding

**Component Map:** Retriever -> Dependency-Aware Reranker -> Demonstration Selection

**Critical Path:** Input query → Retriever → Initial demonstration set → Reranker → Final demonstration selection → LLM prompt

**Design Tradeoffs:** The retrieve-then-rerank approach adds computational overhead compared to single-stage retrieval but captures dependencies between demonstrations. The dependency-aware reranker requires more complex training data construction but results in more complementary demonstrations.

**Failure Signatures:** Poor performance may occur if the retriever fails to identify relevant demonstrations initially, if the reranker overfits to training data dependencies, or if the LLM feedback used for training is noisy or inconsistent.

**First Experiments:**
1. Compare performance of DemoRank against baseline retrievers (BM25, E5) on a held-out test set
2. Conduct ablation study removing the dependency-aware component to quantify its contribution
3. Evaluate robustness by testing on out-of-domain ranking tasks not seen during training

## Open Questions the Paper Calls Out
None

## Limitations
- The dependency-aware reranking mechanism introduces complexity that may limit applicability in time-sensitive scenarios
- Iterative sample construction for training requires significant computational overhead during the training phase
- The paper does not extensively discuss scalability with increasing numbers of demonstrations

## Confidence

**High confidence**: The experimental results showing improved performance over established baselines (BM25, E5, K-means) are well-supported by the data presented.

**Medium confidence**: The claim about strong generalization to out-of-domain scenarios, while demonstrated, would benefit from more diverse domain shifts to fully validate the robustness claim.

**Medium confidence**: The advantage over supervised models when training data is limited is convincing but the comparison is constrained to specific few-shot settings.

## Next Checks
1. Conduct ablation studies specifically isolating the impact of the dependency-aware component versus the retrieval component to quantify their respective contributions to performance gains
2. Evaluate the framework's performance with significantly larger demonstration pools (e.g., 50+ demonstrations) to test scalability claims and identify potential performance plateaus or degradation points
3. Measure and report wall-clock time for the iterative training sample construction process to assess practical deployment considerations in time-sensitive applications