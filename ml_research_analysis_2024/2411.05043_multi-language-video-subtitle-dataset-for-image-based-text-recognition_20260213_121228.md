---
ver: rpa2
title: Multi-language Video Subtitle Dataset for Image-based Text Recognition
arxiv_id: '2411.05043'
source_url: https://arxiv.org/abs/2411.05043
tags:
- subtitle
- images
- text
- recognition
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Multi-language Video Subtitle Dataset was developed to support
  text recognition research across Thai and English languages, containing 4,224 subtitle
  images extracted from 24 videos. The dataset features 157 unique characters including
  Thai consonants, vowels, tone marks, punctuation, numerals, Roman characters, and
  special symbols, with text lengths ranging from 1 to 80 characters.
---

# Multi-language Video Subtitle Dataset for Image-based Text Recognition

## Quick Facts
- arXiv ID: 2411.05043
- Source URL: https://arxiv.org/abs/2411.05043
- Reference count: 7
- Primary result: Multi-language video subtitle dataset with 4,224 images achieving 5.29% CER using CNN-LSTM with WBS decoding

## Executive Summary
The Multi-language Video Subtitle Dataset addresses the challenge of text recognition across Thai and English languages in video content. The dataset contains 4,224 subtitle images extracted from 24 videos, featuring 157 unique characters including Thai consonants, vowels, tone marks, punctuation, numerals, Roman characters, and special symbols. Text lengths range from 1 to 80 characters, with most containing 10-40 characters. The dataset enables development of robust recognition systems by providing diverse multilingual text data with complex background variations, achieving character error rates of 9.36% and 5.29% using different deep learning approaches.

## Method Summary
The dataset was created by extracting subtitle frames from 24 YouTube and Facebook videos featuring both Thai and English text. Images were preprocessed to 1,280×720 resolution and organized with embedded labels in filenames. Two deep learning approaches were evaluated: a CNN-LSTM architecture using modified VGG19 backbone with CTC loss achieving 9.36% CER, and a FusionCNNs-LSTM architecture combining VGG-s1 and VGG-s2 features with Word Beam Search decoding achieving 5.29% CER. The methods focus on extracting spatial and temporal features through CNN and LSTM architectures respectively.

## Key Results
- Dataset contains 4,224 subtitle images with 157 unique characters across Thai and English languages
- Text recognition models achieved character error rates of 9.36% (CTC) and 5.29% (WBS) on the dataset
- Text lengths vary from 1 to 80 characters, with most images containing 10-40 characters
- Fusion of VGG-s1 and VGG-s2 architectures with WBS decoding outperformed standard CTC approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-language video subtitle datasets enable robust training of text recognition models by exposing them to diverse character sets and complex backgrounds.
- Mechanism: By providing 4,224 subtitle images with 157 unique characters (Thai, English, numerals, and special symbols), the dataset allows deep learning models to learn to recognize text across different languages and visual conditions, improving generalization.
- Core assumption: Models trained on diverse multilingual text data will perform better on unseen text recognition tasks than those trained on single-language datasets.
- Evidence anchors:
  - [abstract]: "The Multi-language Video Subtitle Dataset is a comprehensive collection designed to support research in text recognition across multiple languages."
  - [section]: "The dataset provides a resource for addressing challenges in text recognition within complex backgrounds."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.467, average citations=0.0. Weak corpus evidence for direct validation.
- Break condition: If the model cannot handle significant variations in font, size, and background complexity, or if character confusion rates remain high for certain scripts.

### Mechanism 2
- Claim: Connectionist Temporal Classification (CTC) decoding enables end-to-end text recognition without character-level alignment during training.
- Mechanism: CTC allows the model to predict character sequences directly from image features without requiring precise character position annotations, making training more efficient and scalable.
- Core assumption: The temporal dependencies in subtitle text can be effectively captured by sequence models (LSTM) even without explicit character alignment.
- Evidence anchors:
  - [section]: "The modified VGG19 was employed as the CNN backbone, resulting in a character error rate (CER) of 9.36%."
  - [section]: "Their approach integrated the fusion of VGG-s1 and VGG-s2 architectures... Their experimental results indicated that WBS outperformed the CTC algorithm, with the FusionCNNs-LSTM achieving a CER of 5.29%."
  - [corpus]: Weak corpus evidence for direct validation.
- Break condition: If the text recognition task requires very long sequences or if the background complexity makes temporal feature extraction unreliable.

### Mechanism 3
- Claim: Fusion of spatial and temporal features improves text recognition accuracy compared to using only spatial features.
- Mechanism: Combining CNN-extracted spatial features with LSTM-extracted temporal features allows the model to capture both character shapes and their sequential relationships, leading to better recognition performance.
- Core assumption: Text recognition benefits from both spatial understanding (character appearance) and temporal understanding (character sequence).
- Evidence anchors:
  - [section]: "Both of these methods focus on the fusion of deep learning algorithms, particularly CNN and LSTM architectures, to extract both spatial and temporal features."
  - [section]: "Additionally, to enhance feature robustness, a multi-layer adaptive spatial-temporal feature fusion network (ASTFF) can be employed to extract spatial-temporal features."
  - [corpus]: Weak corpus evidence for direct validation.
- Break condition: If the temporal component adds unnecessary complexity without improving accuracy, or if the spatial features alone are sufficient for the text lengths in the dataset.

## Foundational Learning

- Concept: Character Error Rate (CER)
  - Why needed here: CER is the primary evaluation metric for text recognition systems, measuring the accuracy of character-level predictions.
  - Quick check question: If a system has a CER of 9.36%, what percentage of characters are correctly recognized on average?

- Concept: Connectionist Temporal Classification (CTC)
  - Why needed here: CTC is the loss function used for training sequence models in text recognition without requiring character-level alignment.
  - Quick check question: How does CTC handle blank predictions and repeated characters during decoding?

- Concept: Convolutional Neural Networks (CNNs) for feature extraction
  - Why needed here: CNNs are used to extract spatial features from subtitle images before sequence modeling.
  - Quick check question: What advantages do CNNs provide for image-based text recognition compared to fully connected networks?

## Architecture Onboarding

- Component map: Image input → CNN backbone (VGG19 or VGG-s1/s2 fusion) → Spatial feature map → LSTM layers → CTC/WBS decoder → Character sequence output
- Critical path: CNN feature extraction → LSTM sequence modeling → CTC/WBS decoding
- Design tradeoffs: CTC vs WBS decoding (speed vs accuracy), single CNN vs fused CNN architectures (simplicity vs robustness)
- Failure signatures: High CER indicating poor feature extraction or sequence modeling, slow inference suggesting inefficient decoding
- First 3 experiments:
  1. Train a baseline CNN-LSTM model with CTC decoding on the full dataset and measure CER
  2. Replace CTC with WBS decoding and compare CER and inference speed
  3. Implement VGG-s1/s2 fusion architecture and evaluate performance improvement over single CNN baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of text length distribution on model performance for recognition tasks, particularly for subtitle images with 1-80 characters?
- Basis in paper: [explicit] The dataset contains text lengths ranging from 1 to 80 characters, with most images containing 10-40 characters. The paper notes this variability adds complexity but doesn't analyze how text length affects recognition accuracy.
- Why unresolved: The paper doesn't provide detailed analysis of how recognition performance varies with text length, which would be important for understanding model limitations and optimization strategies.
- What evidence would resolve it: Performance metrics (CER, accuracy) broken down by text length ranges, showing recognition accuracy trends as text length increases.

### Open Question 2
- Question: How does the performance of CNN-LSTM architectures compare to other deep learning approaches like Transformer-based models on this multilingual dataset?
- Basis in paper: [explicit] The paper mentions that CNN-LSTM architectures achieved 9.36% CER using CTC and 5.29% using WBS, but doesn't compare these results to other modern architectures.
- Why unresolved: The paper focuses on CNN-LSTM methods without exploring whether newer architectures like Transformers could provide better performance on this multilingual dataset.
- What evidence would resolve it: Comparative experiments showing CER and other metrics for Transformer-based models versus CNN-LSTM approaches on the same dataset.

### Open Question 3
- Question: What is the computational efficiency of different recognition models on this dataset in terms of processing time per image and memory usage?
- Basis in paper: [inferred] The paper mentions the dataset "provides a foundation for improving computational efficiency in text recognition systems" but doesn't provide any efficiency metrics for the tested models.
- Why unresolved: While the paper reports accuracy metrics, it doesn't provide any information about the computational costs of the recognition models, which is crucial for real-world deployment in video processing applications.
- What evidence would resolve it: Detailed timing measurements for inference, memory consumption statistics, and comparisons of efficiency metrics across different model architectures.

## Limitations

- Corpus analysis reveals weak external validation with only 25 related papers and zero average citations, suggesting limited peer review and community validation
- Dataset composition lacks detailed statistics on character frequency distribution across Thai and English scripts, potentially leading to imbalanced training
- Exact train/validation/test splits are unspecified, making faithful reproduction difficult and limiting reproducibility
- Evaluation focuses solely on character error rate without considering inference speed or computational efficiency for real-world deployment

## Confidence

- **High confidence**: The dataset contains 4,224 subtitle images with specified resolution and character set diversity, and the overall methodology of using CNN-LSTM architectures with CTC decoding is well-established in text recognition literature.
- **Medium confidence**: The reported CER values of 9.36% and 5.29% are plausible given the architecture descriptions, though the lack of hyperparameter details and validation splits creates uncertainty about reproducibility.
- **Low confidence**: Claims about the dataset's effectiveness for real-world applications are not well-supported due to the absence of ablation studies, comparison with existing datasets, or evaluation on external test sets.

## Next Checks

1. Conduct a character frequency analysis to identify potential imbalances in the training data distribution across Thai and English characters, and retrain models with class-balanced sampling to verify if performance on underrepresented characters improves.

2. Implement ablation studies comparing the CNN-LSTM architecture with and without temporal features (LSTM layers) on the same dataset split to quantify the actual contribution of spatial-temporal fusion to the reported CER improvements.

3. Evaluate model performance on an external test set of video subtitles from different sources than those used in training to assess generalization beyond the specific video collection used for dataset creation.