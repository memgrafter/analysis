---
ver: rpa2
title: Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens
arxiv_id: '2406.08477'
source_url: https://arxiv.org/abs/2406.08477
tags:
- tokens
- item
- recommendation
- items
- meta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of representing users and items
  in large language models (LLMs) for recommender systems. Traditional methods using
  in-vocabulary tokens for user/item IDs lead to poor performance due to semantic
  conflicts and limited expressiveness.
---

# Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens

## Quick Facts
- arXiv ID: 2406.08477
- Source URL: https://arxiv.org/abs/2406.08477
- Reference count: 40
- Primary result: Outperforms state-of-the-art methods across various recommendation tasks with significant improvements in metrics like HR@10 and NDCG@10

## Executive Summary
This paper addresses the challenge of representing users and items in large language models (LLMs) for recommender systems. Traditional methods using in-vocabulary tokens for user/item IDs lead to poor performance due to semantic conflicts and limited expressiveness. The proposed META ID framework constructs out-of-vocabulary (OOV) tokens based on meta-path-based embeddings learned from user-item interaction history. By clustering these embeddings, META ID creates hierarchical OOV tokens that capture user/item correlations while ensuring diversity. Experiments show META ID outperforms state-of-the-art methods across various recommendation tasks, achieving significant improvements in metrics like HR@10 and NDCG@10.

## Method Summary
META ID constructs out-of-vocabulary tokens for users and items by first sampling meta-paths from user-item interaction graphs, then training a skip-gram model to learn user/item embeddings from these meta-paths. K-means clustering is applied to these embeddings to generate hierarchical OOV tokens, creating both coarse-grained cluster tokens and fine-grained in-cluster tokens sorted by distance to cluster centroids. The OOV tokens are integrated into the LLM's vocabulary and undergo a linear transformation initialized with cluster centroid embeddings before being processed by the LLM's embedding layer. The framework is evaluated on recommendation tasks using metrics like Hit Ratio and NDCG.

## Key Results
- On the Toys dataset, META ID achieves HR@10 of 0.0803 compared to 0.0088 for Sequential ID
- META ID consistently outperforms state-of-the-art methods across multiple recommendation datasets
- The framework demonstrates improved diversity scores while maintaining strong recommendation accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: META ID creates hierarchical OOV tokens that capture user/item correlations while ensuring diversity
- Mechanism: Clustering meta-path-based embeddings into centroids (coarse-grained tokens) and sorting by distance to centroids (fine-grained tokens) creates hierarchical token IDs
- Core assumption: User-item interaction patterns captured in meta-paths can be effectively clustered to reveal meaningful user/item groupings
- Evidence anchors:
  - [abstract]: "By clustering the learned representations from historical user-item interactions, we make the representations of user/item combinations share the same OOV tokens if they have similar properties"
  - [section 4.2]: "Within each cluster, assign a token based on the distance to the centroid... We assign fine-grained tokens "<yi>" in ascending order according to their distance from the cluster centroid"
  - [corpus]: Weak - neighboring papers discuss OOV handling but don't directly address clustering-based token generation

### Mechanism 2
- Claim: META ID's representation augmentation through linear transformation of OOV tokens improves LLM performance
- Mechanism: OOV tokens undergo a linear transformation initialized with category embeddings from cluster centroids before being processed by the LLM's embedding layer
- Core assumption: Initializing OOV token embeddings with learned cluster centroid representations provides better starting points than random initialization
- Evidence anchors:
  - [section 4.3]: "the OOV tokens undergo a linear layer F(·) initialized with the category embeddings μg from Equation 7"
  - [section D]: "Our findings show that the latter method substantially improves META ID's performance"
  - [corpus]: Weak - neighboring papers mention initialization but don't specifically test centroid-based initialization for recommendation tokens

### Mechanism 3
- Claim: META ID's combined user-item indexing captures richer collaborative information than item-only indexing
- Mechanism: By clustering both user and item representations together, the resulting OOV tokens encode relationships between users and items in the same semantic space
- Core assumption: Users and items can be meaningfully represented in the same embedding space where their interactions create patterns
- Evidence anchors:
  - [section 4.2]: "We then create the between-cluster tokens ⟨CT i⟩ for cluster i, and sort the in-cluster users/items based on the cosine distance to the cluster centroids"
  - [section 5.4]: "reveals that the combined user-item indexing (User&Item) outperforms either user-only or item-only indexing"
  - [corpus]: Weak - neighboring papers focus on item indexing but don't explore combined user-item token generation

## Foundational Learning

- Concept: Meta-paths in heterogeneous networks
  - Why needed here: Meta-paths capture the sequence of user-item interactions that form the basis for learning user/item representations
  - Quick check question: What does a U-I-U meta-path represent in a user-item interaction graph?

- Concept: Skip-gram models for representation learning
  - Why needed here: Skip-gram models learn embeddings from meta-path sequences that capture the co-occurrence patterns of users and items
  - Quick check question: How does the skip-gram objective function (Equation 10) encourage items that co-occur with users to have similar representations?

- Concept: K-means clustering for token generation
  - Why needed here: K-means clustering groups similar users/items together so they can share the same OOV tokens, creating the hierarchical structure
  - Quick check question: What property of K-means clustering makes it suitable for creating the coarse-grained OOV tokens in META ID?

## Architecture Onboarding

- Component map: Meta-path sampler -> Skip-gram model -> User/item embeddings -> K-means clustering -> OOV token generation -> LLM integration
- Critical path:
  1. Sample meta-paths from user-item interaction graph
  2. Train skip-gram model to learn user/item embeddings
  3. Apply K-means clustering to create hierarchical token structure
  4. Integrate OOV tokens with LLM through extended vocabulary and linear transformation
  5. Fine-tune LLM on recommendation tasks

- Design tradeoffs:
  - Clustering granularity (number of clusters) vs vocabulary size: More clusters create more specific tokens but increase vocabulary size
  - Random initialization vs centroid-based initialization: Random is simpler but centroid-based may capture more meaningful patterns
  - User-only vs combined user-item indexing: Combined captures more information but increases complexity

- Failure signatures:
  - Poor performance on sparse datasets: May indicate clustering is creating meaningless groupings
  - Degradation when increasing OOV token size: May indicate noise is being introduced into the token representations
  - Performance drops with centroid-based initialization: May indicate cluster centroids don't capture meaningful patterns

- First 3 experiments:
  1. Compare random initialization vs centroid-based initialization of OOV tokens on a small dataset
  2. Test different clustering granularities (varying number of clusters) on a single dataset to find optimal token size
  3. Compare user-only vs combined user-item indexing on a dataset with both sparse and dense regions

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored:
- How META ID scales to extremely large recommender systems with trillions of users and items
- How the framework handles cold-start scenarios for new users or items
- The impact of different clustering methods beyond K-means on META ID's performance

## Limitations
- Evaluation limited to three relatively small Amazon datasets, raising concerns about generalizability to larger-scale production systems
- Clustering-based token generation assumes user-item interaction patterns form meaningful groupings without validating semantic meaningfulness of clusters
- Computational overhead of meta-path sampling and skip-gram training is not thoroughly analyzed

## Confidence

**High Confidence**: The empirical results showing META ID outperforming traditional in-vocabulary token methods (Sequential ID, Hashing ID) across all tested datasets and metrics.

**Medium Confidence**: The claim that centroid-based initialization of OOV tokens provides significant performance improvements over random initialization, though the ablation study is limited to a single dataset.

**Low Confidence**: The assertion that combined user-item indexing consistently outperforms user-only or item-only indexing across diverse recommendation scenarios, as the underlying reasons for this benefit are not thoroughly analyzed.

## Next Checks

1. **Scalability Validation**: Test META ID on a large-scale dataset with millions of users and items (e.g., MovieLens-25M or a production-scale dataset) to verify whether the clustering-based OOV generation remains effective and computationally feasible at scale.

2. **Cluster Quality Analysis**: Perform qualitative analysis of the K-means clusters to verify they capture meaningful user/item segments. This could include examining cluster centroids to see if they represent coherent user types or item categories, and comparing cluster quality metrics against random groupings.

3. **Cross-Domain Generalization**: Evaluate META ID's performance when trained on one domain (e.g., Beauty) and tested on a different domain (e.g., Toys) to assess whether the learned OOV tokens capture generalizable user/item patterns or are too domain-specific.