---
ver: rpa2
title: 'Let''s Be Self-generated via Step by Step: A Curriculum Learning Approach
  to Automated Reasoning with Large Language Models'
arxiv_id: '2410.21728'
source_url: https://arxiv.org/abs/2410.21728
tags:
- problem
- example
- many
- hours
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel automatic reasoning prompt approach
  named LBS3, which draws inspiration from curriculum learning. Specifically, LBS3
  initially guides large language models (LLMs) to recall easy-to-hard proxy queries
  that are pertinent to the target query.
---

# Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2410.21728
- Source URL: https://arxiv.org/abs/2410.21728
- Reference count: 40
- Key outcome: Achieves 2.83% average accuracy improvement over state-of-the-art baselines

## Executive Summary
This paper introduces LBS3, a novel automatic reasoning prompt approach that leverages curriculum learning principles for large language models. The method guides LLMs to generate progressively challenging proxy queries related to the target problem, then uses exemplary prompts from easier queries to solve harder ones. The approach demonstrates strongly competitive performance across various reasoning-intensive tasks with both open- and closed-source LLMs, achieving an average accuracy improvement of 2.83% compared to state-of-the-art baselines.

## Method Summary
LBS3 employs a three-stage curriculum learning approach for automated reasoning. First, it prompts LLMs to generate easy-to-hard proxy queries that are semantically related to the target query. Second, it uses a progressive strategy where exemplary prompts from successfully solved easier proxy queries guide the LLM in tackling more difficult proxy queries. Finally, this progressive solving of increasingly complex proxy queries enhances the quality of solutions for the original target problem. The method is designed to work with various LLMs and reasoning tasks without requiring additional training or fine-tuning.

## Key Results
- Achieves 2.83% average accuracy improvement over state-of-the-art baselines
- Demonstrates strong performance across both open- and closed-source LLMs
- Shows effectiveness in various reasoning-intensive tasks

## Why This Works (Mechanism)
The approach works by leveraging curriculum learning principles where the LLM progressively builds reasoning capabilities through a carefully structured sequence of tasks. By first generating and solving easier proxy queries, the model establishes foundational reasoning patterns and exemplar solutions. These exemplar prompts then serve as scaffolds for tackling more complex queries, creating a bootstrapping effect where each solved problem contributes to the model's ability to solve subsequent, more difficult problems. The self-generated nature of the curriculum allows the approach to adapt to different reasoning domains and query types.

## Foundational Learning
- Curriculum learning principles: why needed - to structure learning progression from simple to complex; quick check - verify the proxy difficulty scaling is appropriate
- Proxy query generation: why needed - to create intermediate reasoning steps; quick check - assess semantic relevance between proxy and target queries
- Exemplary prompt methodology: why needed - to transfer learned reasoning patterns; quick check - measure consistency between easy and hard query solutions
- Progressive difficulty scaling: why needed - to prevent cognitive overload; quick check - track solution success rates across difficulty levels
- Self-supervised reasoning: why needed - to enable adaptation without external supervision; quick check - evaluate performance across different LLMs

## Architecture Onboarding

Component Map:
Proxy Query Generation -> Difficulty Progression -> Exemplary Prompt Application -> Target Query Solution

Critical Path:
Target Query → Proxy Generation → Easy Proxy Solving → Exemplar Creation → Hard Proxy Solving → Target Solution

Design Tradeoffs:
The approach balances between proxy query quality and generation efficiency, with self-generation potentially introducing noise but enabling domain adaptation. The progressive difficulty scaling trades computational overhead for improved solution quality. The reliance on exemplar prompts from previous solutions creates a dependency chain that could propagate errors but also enables knowledge transfer.

Failure Signatures:
Poor proxy query relevance leading to ineffective intermediate steps; incorrect difficulty scaling causing either trivial or overwhelming intermediate problems; exemplar prompts that fail to capture essential reasoning patterns; accumulation of errors through the progressive solving chain.

First Experiments:
1. Test proxy query generation quality and semantic relevance to target queries
2. Evaluate the effectiveness of difficulty scaling by measuring success rates across proxy levels
3. Assess exemplar prompt quality by comparing solutions with and without exemplar guidance

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks detailed ablation studies to isolate individual component contributions
- Does not fully characterize task diversity and complexity distributions
- Limited analysis of failure modes when self-generated proxies are of low quality

## Confidence
- High confidence: The core methodology and general experimental setup are clearly described
- Medium confidence: The reported accuracy improvements are reliable within the tested conditions
- Medium confidence: The comparison with state-of-the-art baselines appears valid but could benefit from more extensive benchmarking

## Next Checks
1. Conduct systematic ablation studies to quantify the individual contributions of proxy generation, difficulty progression, and exemplary prompts to overall performance
2. Test the approach on a more diverse set of reasoning tasks with explicit difficulty gradients to better evaluate the curriculum learning effectiveness
3. Analyze failure cases where self-generated proxy queries lead to degraded performance to understand the approach's limitations