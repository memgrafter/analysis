---
ver: rpa2
title: 'STAT: Shrinking Transformers After Training'
arxiv_id: '2406.00061'
source_url: https://arxiv.org/abs/2406.00061
tags:
- pruning
- layer
- network
- heads
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STAT is a pruning method for transformer models that eliminates
  attention heads and neurons without fine-tuning. It uses matrix factorizations to
  select structures to remove and computes corrections to preserve accuracy.
---

# STAT: Shrinking Transformers After Training

## Quick Facts
- arXiv ID: 2406.00061
- Source URL: https://arxiv.org/abs/2406.00061
- Reference count: 34
- Primary result: Matrix factorization pruning method achieving 90% FLOPs reduction without fine-tuning

## Executive Summary
STAT is a post-training pruning method that eliminates attention heads and neurons from transformer models while preserving accuracy through matrix factorization and correction computations. The method uses pivoted QR factorizations to select important structures and computes interpolation matrices to compensate for removed elements, all without requiring any fine-tuning. STAT achieves competitive accuracy with methods that include fine-tuning, compressing models up to 90% of their original FLOPs while maintaining performance on downstream tasks.

## Method Summary
STAT prunes transformer models by collecting intermediate activations from unlabeled data examples, then using pivoted QR factorizations to select which attention heads and neurons to keep. For each pruned structure, the method computes a correction/interpolation matrix via least squares that reconstructs the removed outputs from retained ones. These corrections are then fused into subsequent layer weights, preserving network accuracy without retraining. The method scales to large models using randomized sketching techniques and handles different transformer architectures by adjusting for residual connections and layer normalization placement.

## Key Results
- Achieves 90% FLOPs reduction while maintaining accuracy on BERT, DistilBERT, and Llama-2
- Outperforms existing gradient-free pruning methods across all tested models
- Prunes Llama-2 7B in under 3 hours on a single GPU using only 256 data examples
- Matches or exceeds accuracy of methods requiring fine-tuning for compression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pivoted QR factorizations of attention head outputs select heads to keep by measuring how much each head contributes to reconstructing the overall layer output.
- Mechanism: Flatten the concatenated attention head outputs into a matrix where each column is a vectorized head. Perform column-pivoted QR to identify the most linearly independent columns (heads). Those columns correspond to heads that preserve maximal information with minimal redundancy.
- Core assumption: Attention heads produce outputs that are approximately linearly independent, and this independence correlates with functional importance.
- Evidence anchors:
  - [abstract]: "Each layer block in the network is compressed using a series of principled matrix factorizations that preserve the network structure."
  - [section]: "Because subselection and element-wise activation functions commute, this ID yields the compressed network..."
  - [corpus]: Weak—no neighbor papers mention pivoted QR or interpolative decomposition.
- Break condition: If attention heads are highly correlated or redundant in a way that does not reflect importance, pivoted QR may retain less useful heads or discard critical ones.

### Mechanism 2
- Claim: After pruning heads, a dense correction matrix computed via least squares compensates for removed heads and preserves layer output.
- Mechanism: Recompute the output of the pruned heads as a linear combination of retained heads. The least squares solve finds the optimal interpolation matrix to minimize reconstruction error on the pruning data.
- Core assumption: The outputs of pruned heads can be well approximated as a linear combination of retained heads for the data distribution used.
- Evidence anchors:
  - [abstract]: "...while preserving accuracy by calculating a correction to the weights of the next layer."
  - [section]: "We then solve a least squares problem to compute the correction/interpolation matrix."
  - [corpus]: Weak—neighbors discuss pruning but not correction-matrix computation.
- Break condition: If the approximation error from linear reconstruction is large relative to the target accuracy, the correction fails and accuracy degrades.

### Mechanism 3
- Claim: Structured matrix factorizations avoid retraining by embedding correction into the next layer's weights.
- Mechanism: The correction matrix from pruning is fused into the subsequent fully connected or attention block, preserving forward pass structure without needing to update gradients over many iterations.
- Core assumption: Errors from pruning can be fully compensated by modifying only the next layer's parameters, without propagating changes deeper into the network.
- Evidence anchors:
  - [abstract]: "...STAT eliminates both attention heads and neurons from the network, while preserving accuracy by calculating a correction to the weights of the next layer."
  - [section]: "Because in deeper networks T may be folded into the next layer and does not need to be maintained explicitly as a linear layer."
  - [corpus]: Weak—neighbors focus on training-time pruning or post-training but not layer-level correction fusion.
- Break condition: If the layer interaction is too complex for a single-layer correction to capture, errors will accumulate and accuracy will drop.

## Foundational Learning

- Concept: Rank-revealing QR factorizations and interpolative decompositions
  - Why needed here: Provide principled selection of structurally important neurons/heads while giving a computable correction matrix.
  - Quick check question: Why does a pivoted QR factorization yield a column subset that approximates the original matrix with controlled error?

- Concept: Randomized sketching for large matrices
  - Why needed here: Enable scalable computation of IDs on matrices with millions of rows (e.g., flattened attention outputs for long sequences).
  - Quick check question: How does multiplying by a sparse ±1 matrix reduce dimensionality while preserving column selection quality for IDs?

- Concept: Residual connections and layer normalization effects on error propagation
  - Why needed here: Determine how pruning-induced errors in one layer affect downstream layers and how to weight per-layer pruning decisions.
  - Quick check question: In a residual block, if you add an error term ε after a layer, how does the subsequent layer norm amplify or attenuate that error?

## Architecture Onboarding

- Component map: Unlabeled pruning data -> Forward pass to collect activations -> Pivoted QR for head/neuron selection -> Least squares for correction matrix -> Fuse correction into next layer weights -> Repeat per layer

- Critical path:
  1. Forward pass on pruning data to collect intermediate activations
  2. Pivoted QR on concatenated attention outputs → head selection
  3. Least squares solve → dense correction matrix
  4. Fuse correction into next layer
  5. Repeat per layer

- Design tradeoffs:
  - Speed vs accuracy: Non-pivoted QR + single shot vs pivoted QR + iterative refinement
  - Memory vs parallelism: Full batch intermediate states vs per-example streaming with sketching
  - Compression vs fidelity: Aggressive pruning ratios increase FLOPs savings but raise reconstruction error

- Failure signatures:
  - Sharp accuracy drop after pruning → likely over-pruned critical heads or neurons
  - Noisy intermediate activations → insufficient pruning data or poor sketching
  - Out-of-memory errors → blocked QR not tuned for model size or sequence length

- First 3 experiments:
  1. Run STAT on BERT base with 256 examples, compare accuracy vs Kwon et al. at 50% FLOPs.
  2. Vary pruning dataset size (128→1024) and plot accuracy curve to find saturation point.
  3. Replace pivoted QR with random head selection, measure degradation to confirm importance of principled selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the placement of layer normalization in different transformer architectures affect the sensitivity of the network to pruning-induced errors?
- Basis in paper: [inferred] The paper notes that BERT applies layernorm after adding residuals while Llama applies it before attention and fully connected layers, and uses different error weighting functions for each architecture.
- Why unresolved: The paper mentions this observation and uses empirical weighting functions but does not provide a systematic theoretical analysis of how layernorm placement affects error propagation and pruning sensitivity.
- What evidence would resolve it: A comprehensive study comparing pruning results across multiple architectures with different layernorm placements, combined with theoretical analysis of error propagation in each configuration.

### Open Question 2
- Question: What is the optimal strategy for selecting pruning ratios per layer when considering both accuracy preservation and computational efficiency?
- Basis in paper: [explicit] The paper mentions that different layers may have different sensitivities and importance, and uses a one-shot technique to determine per-layer sizes based on estimated per-layer errors weighted by FLOPs.
- Why unresolved: The paper uses a heuristic weighting function and one-shot approach, but does not explore whether iterative methods or more sophisticated optimization techniques could yield better results.
- What evidence would resolve it: Comparative experiments between one-shot and iterative pruning methods across various model architectures and datasets, measuring both accuracy and computational overhead.

### Open Question 3
- Question: How does the size of the pruning dataset affect the quality of compression in large decoder models like Llama?
- Basis in paper: [explicit] The paper uses 256 examples for Llama pruning and notes that STAT sees improvements in accuracy until roughly 512 data examples for BERT, but does not fully explore the relationship for larger models.
- Why unresolved: While the paper provides results for a specific dataset size, it acknowledges that larger pruning sets might improve performance but would increase compression time, without systematically studying this trade-off.
- What evidence would resolve it: Systematic experiments varying the pruning dataset size for Llama and other large models, measuring both accuracy retention and computational cost across different compression levels.

### Open Question 4
- Question: Can the computational efficiency of STAT be further improved for extremely large models with billions of parameters?
- Basis in paper: [explicit] The paper mentions using randomized sketching techniques and blocking neurons into groups to scale to large models, but notes that these approaches introduce trade-offs between accuracy and computational resources.
- Why unresolved: The paper presents scaling solutions but does not explore whether alternative randomized algorithms, distributed computing approaches, or hardware-specific optimizations could further improve efficiency.
- What evidence would resolve it: Performance comparisons of STAT with various scaling techniques (different sketching methods, parallelization strategies, hardware accelerators) on models of increasing size, measuring both accuracy and wall-clock time.

## Limitations
- The core assumption that attention heads are linearly independent needs further validation across diverse model architectures
- Limited theoretical grounding for why correction matrices fully compensate for removed structures in deeper networks
- The method's effectiveness for all transformer architectures beyond BERT/DistilBERT/Llama-2 family remains uncertain

## Confidence
- **High confidence**: STAT achieves significant FLOPs reduction without fine-tuning on tested models and datasets
- **Medium confidence**: The pivoted QR selection method reliably identifies important heads/neurons
- **Medium confidence**: Correction matrices computed via least squares adequately preserve accuracy
- **Low confidence**: The method generalizes equally well to all transformer architectures and tasks
- **Low confidence**: The 256-example pruning dataset is universally sufficient across all scenarios

## Next Checks
1. **Layer-wise Error Propagation Analysis**: Systematically measure how pruning-induced errors in each layer propagate through subsequent layers, particularly examining whether single-layer corrections adequately compensate for removed structures in deeper networks.

2. **Architecture-Agnostic Validation**: Test STAT on a broader range of transformer architectures including encoder-decoder models and vision transformers to assess whether the method's effectiveness extends beyond the BERT/DistilBERT/Llama-2 family.

3. **Minimum Data Requirement Study**: Conduct a systematic study varying pruning dataset sizes from 64 to 4096 examples across different model sizes to determine the actual minimum data requirements and identify when accuracy plateaus.