---
ver: rpa2
title: 'Hydragen: High-Throughput LLM Inference with Shared Prefixes'
arxiv_id: '2402.05099'
source_url: https://arxiv.org/abs/2402.05099
tags:
- attention
- hydragen
- prefix
- batch
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Hydragen, a hardware-aware exact implementation
  of attention for transformer-based large language models (LLMs) with shared prefixes.
  The key insight is that when processing batches of sequences with common prefixes,
  attention can be decomposed into separate computations over the shared prefix and
  unique suffixes.
---

# Hydragen: High-Throughput LLM Inference with Shared Prefixes

## Quick Facts
- **arXiv ID**: 2402.05099
- **Source URL**: https://arxiv.org/abs/2402.05099
- **Reference count**: 40
- **Primary result**: Hydragen achieves up to 32x speedup over vLLM for LLM inference with shared prefixes

## Executive Summary
Hydragen is a hardware-aware exact implementation of attention for transformer-based LLMs that exploits shared prefixes across input sequences. The key insight is that when processing batches of sequences with common prefixes, attention can be decomposed into separate computations over the shared prefix and unique suffixes. This decomposition enables efficient computation of prefix attention by batching queries across sequences, replacing many memory-bound matrix-vector products with hardware-friendly matrix-matrix products and reducing redundant memory reads. Hydragen achieves significant improvements in end-to-end decoding throughput for CodeLlama-13b, with up to 32x speedup over competitive baselines like vLLM. The method generalizes to tree-based sharing patterns and enables efficient processing of very long shared contexts (up to 16K tokens) with minimal throughput degradation.

## Method Summary
Hydragen decomposes transformer attention computation by splitting it into prefix and suffix components when processing batches of sequences with shared prefixes. For the prefix portion, Hydragen batches queries across all sequences, converting multiple independent attention operations into a single matrix-matrix product that leverages GPU tensor cores. The suffix portion is computed separately for each sequence using standard attention mechanisms. The final output is obtained by combining the prefix and suffix attention results through a log-sum-exp decomposition of the softmax operation. Hydragen generalizes beyond simple prefix-suffix decomposition to handle hierarchical tree-based sharing patterns by applying the same decomposition recursively at each node of the sharing tree.

## Key Results
- Up to 32x speedup over vLLM for end-to-end decoding with CodeLlama-13b on 8xA100-40GB GPUs
- Maintains high throughput with shared prefixes up to 16K tokens with minimal degradation
- 55% reduction in inference time for competitive programming tasks with tree-based sharing patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hydragen eliminates redundant prefix reads by decomposing attention into prefix and suffix computations
- **Mechanism**: Shared prefixes create identical keys/values across sequences, so attention over prefix can be batched across all sequences, replacing many memory-bound matrix-vector products with fewer matrix-matrix products
- **Core assumption**: The softmax operation can be decomposed across prefix and suffix by combining log-sum-exp values
- **Evidence anchors**: [abstract] "Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences"

### Mechanism 2
- **Claim**: Inter-sequence batching enables tensor core usage and reduces memory transfers
- **Mechanism**: By batching prefix queries across sequences, Hydragen converts multiple independent attention operations into a single matrix-matrix product, utilizing GPU tensor cores and amortizing memory reads
- **Core assumption**: Matrix-matrix products are more efficient than matrix-vector products on modern GPUs due to tensor cores
- **Evidence anchors**: [abstract] "This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications"

### Mechanism 3
- **Claim**: Hydragen generalizes to hierarchical sharing patterns beyond simple prefix-suffix decomposition
- **Mechanism**: The attention decomposition can be applied recursively at each node of a sharing tree, enabling batching at multiple levels of the hierarchy
- **Core assumption**: The softmax combination formula extends naturally to tree structures
- **Evidence anchors**: [abstract] "Hydragen generalizes beyond simple prefix-suffix decomposition and can be applied to tree-based prompt sharing patterns"

## Foundational Learning

- **Matrix multiplication vs matrix-vector product arithmetic intensity**
  - *Why needed here*: Understanding why batching matrix-vector products into matrix-matrix products improves hardware utilization
  - *Quick check question*: What is the arithmetic intensity ratio between matrix-matrix and matrix-vector products for the same matrix dimensions?

- **GPU tensor cores and their optimization requirements**
  - *Why needed here*: Hydragen's performance gains depend on using tensor cores, which require matrix-matrix products
  - *Quick check question*: What is the minimum matrix dimension typically required to utilize GPU tensor cores effectively?

- **Softmax decomposition and log-sum-exp stability**
  - *Why needed here*: Hydragen's correctness depends on being able to combine prefix and suffix attention results through their log-sum-exp values
  - *Quick check question*: How does the log-sum-exp trick prevent numerical overflow when computing softmax over large values?

## Architecture Onboarding

- **Component map**: Input queries → Prefix KV cache + Suffix KV cache → Hydragen attention decomposition → Matrix-matrix product on prefix + Matrix-vector products on suffixes → Softmax combination → Output
- **Critical path**: The prefix attention computation via matrix-matrix product is the primary performance bottleneck that Hydragen optimizes
- **Design tradeoffs**: Hydragen adds complexity by decomposing attention but gains significant performance improvements; requires tracking prefix/suffix boundaries
- **Failure signatures**: Poor performance when suffix length dominates, incorrect softmax combination leading to wrong outputs, memory issues when batch size exceeds hardware limits
- **First 3 experiments**:
  1. Microbenchmark comparing Hydragen prefix attention speed vs FlashAttention with varying batch sizes
  2. End-to-end throughput comparison with fixed batch size and varying prefix lengths
  3. Hierarchical sharing experiment with tree-structured prompt sharing patterns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does Hydragen's performance scale with tree-based sharing patterns beyond the two-level hierarchy demonstrated in the competitive programming experiments?
- **Basis in paper**: [explicit] Section 3.3 discusses generalization to tree-based sharing patterns, and Section 4.4 shows results for two-level sharing in competitive programming.
- **Why unresolved**: The paper only demonstrates a two-level hierarchy (few-shot prompt shared globally, problem descriptions shared within groups). Real-world applications might have deeper or more complex sharing structures that could benefit from Hydragen's hierarchical approach.
- **What evidence would resolve it**: Experiments showing Hydragen's performance on datasets with multi-level tree sharing patterns (e.g., document retrieval with multiple sections, code generation with nested function calls) would demonstrate its scalability to more complex sharing structures.

### Open Question 2
- **Question**: What is the impact of varying attention head configurations (multi-head vs multi-query vs grouped-query attention) on Hydragen's performance gains?
- **Basis in paper**: [explicit] Section 3.4 discusses that multi-headed attention models benefit more from Hydragen than multi-query or grouped-query attention models, but doesn't provide quantitative comparisons.
- **Why unresolved**: The paper mentions the theoretical impact but doesn't provide empirical measurements comparing Hydragen's performance across different attention configurations on the same task.
- **What evidence would resolve it**: Controlled experiments running the same inference tasks on models with different attention head configurations (multi-head, multi-query, grouped-query) while measuring Hydragen's relative performance gains would quantify this relationship.

### Open Question 3
- **Question**: How does Hydragen's memory usage compare to vLLM's PagedAttention when processing extremely long shared contexts (e.g., 32K tokens or more)?
- **Basis in paper**: [explicit] Section 4.1 shows Hydragen maintains performance with 16K token prefixes, but the paper doesn't explore longer contexts or provide detailed memory usage comparisons.
- **Why unresolved**: While the paper demonstrates Hydragen's ability to handle 16K token prefixes with minimal throughput degradation, it doesn't explore the memory efficiency limits or compare memory usage to vLLM at these extreme lengths.
- **What evidence would resolve it**: Detailed memory profiling of Hydragen vs vLLM when processing shared contexts of 32K, 64K, and 128K tokens, including both peak memory usage and memory access patterns, would reveal Hydragen's memory efficiency advantages at extreme lengths.

## Limitations
- Performance gains highly dependent on substantial shared prefixes across sequences
- Potential numerical instability from softmax decomposition via log-sum-exp combination
- Implementation assumes static batch patterns rather than handling dynamic user inputs

## Confidence
**High Confidence**: The core mechanism of decomposing attention into prefix and suffix computations is mathematically sound and leverages well-established GPU optimization principles. The empirical throughput improvements on CodeLlama-13b with shared prefixes are well-supported by the experimental results.

**Medium Confidence**: The claim that Hydragen generalizes effectively to tree-based sharing patterns has some theoretical support but limited empirical validation. The 55% reduction in inference time for competitive programming tasks is impressive but based on a single application domain.

**Low Confidence**: The paper's claims about Hydragen being "hardware-agnostic" are not well-supported. While the authors mention it can be implemented using standard PyTorch libraries, they don't demonstrate implementations on different hardware platforms.

## Next Checks
1. **Numerical Stability Analysis**: Implement Hydragen with comprehensive logging of intermediate values (LSE components, prefix/suffix attention scores) and run tests with inputs spanning the full range of typical transformer values, including extreme cases with very large/small logits. Compare final outputs against exact attention computation to quantify numerical drift.

2. **Generalization Benchmark Suite**: Create a diverse benchmark with multiple sharing patterns including random sequences (no sharing), prefix-only sharing, suffix-only sharing, tree-structured sharing, and mixed patterns. Measure Hydragen's performance relative to FlashAttention and vLLM across all scenarios to identify where the approach succeeds and fails.

3. **Cross-Platform Porting Exercise**: Implement Hydragen using different backends (PyTorch CPU, PyTorch GPU, JAX, potentially ONNX Runtime) and measure whether the core performance benefits persist across platforms. Document any architectural assumptions that limit hardware portability and identify required modifications for different hardware targets.