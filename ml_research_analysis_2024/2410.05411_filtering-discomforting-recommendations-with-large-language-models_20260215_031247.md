---
ver: rpa2
title: Filtering Discomforting Recommendations with Large Language Models
arxiv_id: '2410.05411'
source_url: https://arxiv.org/abs/2410.05411
tags:
- user
- filtering
- content
- users
- discomfortfilter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiscomfortFilter, a tool designed to filter
  discomforting recommendations on personalized platforms using LLMs. The tool constructs
  an editable preference profile for users and helps them express filtering needs
  through conversation, masking discomforting preferences within the profile.
---

# Filtering Discomforting Recommendations with Large Language Models

## Quick Facts
- arXiv ID: 2410.05411
- Source URL: https://arxiv.org/abs/2410.05411
- Reference count: 40
- DiscomfortFilter tool successfully filters discomforting recommendations using LLM-based preference profiles

## Executive Summary
This paper introduces DiscomfortFilter, a tool designed to filter discomforting recommendations on personalized platforms using Large Language Models. The tool constructs an editable preference profile for users and helps them express filtering needs through conversation, masking discomforting preferences within the profile. A one-week user study with 24 participants demonstrated the effectiveness of DiscomfortFilter in filtering out discomforting recommendations while maintaining user satisfaction and perceived control over their recommendation experience.

## Method Summary
DiscomfortFilter employs a multi-agent LLM pipeline that constructs user preference profiles from interaction data, engages users in conversational discovery of filtering needs, and applies generated filtering rules to recommendations in a plug-and-play manner. The system operates independently of underlying recommendation algorithms, intercepting content before presentation to users. A one-week user study was conducted with 24 participants on a Q&A platform (Zhihu), collecting interaction logs, filtering outcomes, and post-study questionnaires measuring acceptance through the Technology Acceptance Model.

## Key Results
- User study showed 83% acceptance rate with participants agreeing the tool effectively filtered discomforting content
- Preference profile construction improved LLM reasoning, enabling a 3.8B open-source model to rival top commercial models in offline proxy tasks
- Users found the conversational interface helpful for articulating filtering needs they couldn't express proactively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constructing an editable preference profile improves LLM reasoning and simplifies user alignment.
- Mechanism: The Preference Profile Construction Module creates a personalized feature-based profile by analyzing user behavior, which acts as a refined input for LLMs, reducing task complexity and enabling better predictions.
- Core assumption: A well-constructed preference profile provides essential user-specific information that aligns LLM outputs with user expectations.
- Evidence anchors:
  - [abstract]: "The constructed preference profile improves LLM reasoning and simplifies user alignment, enabling a 3.8B open-source LLM to rival top commercial models in an offline proxy task."
  - [section]: "Our construction method reduces task complexity by streamlining the reasoning process through a well-constructed preference profile. This enhancement enables open-source models to match or even surpass the performance of certain commercial models."
  - [corpus]: Weak evidence - corpus papers discuss user profile management but don't directly address the specific mechanism of preference profile construction for discomfort filtering.
- Break condition: If the preference profile fails to capture nuanced user preferences or becomes outdated, LLM reasoning will degrade and user alignment will become difficult.

### Mechanism 2
- Claim: Conversational agents help users articulate filtering needs that are otherwise difficult to express.
- Mechanism: The Filtering Needs Discovery Module uses two strategies - preference profile explanation and filtering record explanation - to engage users in conversation, helping them identify and articulate discomforting content preferences.
- Core assumption: Users have difficulty proactively expressing filtering needs but can recognize and communicate them when prompted with relevant information about their preferences or past filtering decisions.
- Evidence anchors:
  - [abstract]: "Through a guided conversation, DiscomfortFilter assists the user in expressing personalized filtering needs, and then masks the discomforting preferences within the profile."
  - [section]: "All participants agreed that understanding the preferences reflected in platform recommendations and their own behavior would encourage them to express these needs."
  - [corpus]: Weak evidence - corpus papers discuss LLM-based user profile management but don't specifically address conversational mechanisms for filtering discomforting content.
- Break condition: If conversational agents fail to provide relevant context or if users remain unable to articulate their needs despite prompting, the filtering process will be ineffective.

### Mechanism 3
- Claim: The plug-and-play approach maintains flexibility while filtering discomforting recommendations without requiring changes to platform algorithms.
- Mechanism: DiscomfortFilter operates as a third-party tool that intercepts and filters recommendations based on user-configured rules before they reach the user, working independently of the underlying recommendation algorithms.
- Core assumption: Users prefer controlling what they see without understanding or modifying the recommendation algorithms themselves, and filtering can be effectively done at the presentation layer.
- Evidence anchors:
  - [abstract]: "Based on the edited profile, DiscomfortFilter facilitates the discomforting recommendations filtering in a plug-and-play manner, maintaining flexibility and transparency."
  - [section]: "The plug-and-play approach means that the tool operates independently of specific personalized algorithms and directly affects the outputs of these algorithms."
  - [corpus]: Weak evidence - corpus papers discuss recommendation systems but don't specifically address plug-and-play filtering approaches for discomforting content.
- Break condition: If platform changes break the filtering interface or if users need algorithmic-level control rather than content-level filtering, the plug-and-play approach will fail.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their capabilities
  - Why needed here: Understanding LLM capabilities is essential for grasping how DiscomfortFilter uses them for reasoning, conversation, and content filtering.
  - Quick check question: What are the key differences between using LLMs for content generation versus content analysis and decision-making?

- Concept: Recommender systems and personalization algorithms
  - Why needed here: Understanding how recommender systems work is crucial for comprehending why discomforting recommendations occur and how DiscomfortFilter intervenes.
  - Quick check question: How do collaborative filtering and content-based filtering approaches differ in their potential to generate discomforting recommendations?

- Concept: Preference modeling and user profiling
  - Why needed here: The core of DiscomfortFilter relies on constructing and maintaining accurate user preference profiles to guide filtering decisions.
  - Quick check question: What are the trade-offs between implicit and explicit preference collection methods in user profiling?

## Architecture Onboarding

- Component map: User behavior → Preference Profile Construction → Filtering Needs Discovery → Candidate Rule Generation → Content Filter Module → Filtered recommendations

- Critical path: User behavior → Preference Profile Construction → Filtering Needs Discovery → Candidate Rule Generation → Content Filter Module → Filtered recommendations

- Design tradeoffs:
  - Local processing vs. cloud processing: Privacy benefits of local processing vs. computational limitations
  - Rule granularity vs. rule management complexity: More specific rules vs. harder to maintain
  - Real-time filtering vs. batch processing: Immediate protection vs. computational efficiency

- Failure signatures:
  - High false positive rate: Filtering non-discomforting content
  - High false negative rate: Missing discomforting content
  - System lag: Delayed filtering causing discomfort
  - Rule conflicts: Inconsistent filtering behavior

- First 3 experiments:
  1. Test filtering accuracy on a controlled dataset with known discomforting content
  2. Measure LLM performance with vs. without preference profiles on recommendation prediction task
  3. User study measuring comfort levels before and after using DiscomfortFilter

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do users' folk theories about algorithmic systems influence their interactions with DiscomfortFilter and their expectations for algorithmic transparency?
- Basis in paper: [explicit] The paper discusses how users develop "folk theories" about algorithmic systems and their attempts to influence recommendations through behavior, leading to "algorithmic irritation" when expectations are not met. It also mentions that DiscomfortFilter aims to enhance user-perceived contestability in interactions with personalized algorithms.
- Why unresolved: The study does not explicitly explore how users' pre-existing beliefs about algorithms affect their adoption and use of DiscomfortFilter, or how these beliefs influence their expectations for transparency and control.
- What evidence would resolve it: Qualitative analysis of user interviews focusing on their pre-existing beliefs about algorithms, their expectations for transparency, and how these beliefs influenced their interactions with DiscomfortFilter. Quantitative data on user engagement patterns based on their stated algorithmic beliefs.

### Open Question 2
- Question: What is the long-term impact of DiscomfortFilter on users' emotional well-being and their relationship with recommendation platforms?
- Basis in paper: [inferred] The paper discusses how discomforting recommendations can lead to negative emotional consequences like anxiety, unease, or distress. It also mentions that DiscomfortFilter aims to prevent data collection from users' interactions with discomforting recommendations, enabling more accurate user modeling.
- Why unresolved: The user study only covered a one-week period, which is insufficient to assess the long-term emotional impact of using DiscomfortFilter. The study also does not explore how the tool affects users' overall relationship with recommendation platforms over time.
- What evidence would resolve it: Longitudinal studies tracking users' emotional states and platform usage patterns over several months of using DiscomfortFilter, including surveys measuring trust, satisfaction, and perceived control over recommendations.

### Open Question 3
- Question: How does the use of DiscomfortFilter affect the diversity of content users are exposed to over time?
- Basis in paper: [inferred] The paper discusses how DiscomfortFilter filters out discomforting recommendations, which could potentially create a "filter bubble" effect. It also mentions that the platform's recommender systems dynamically adjust to users' evolving preferences based on their interactions.
- Why unresolved: The study does not analyze how the use of DiscomfortFilter affects the variety of content users encounter over time. It also does not explore whether the tool inadvertently limits users' exposure to diverse perspectives or novel content.
- What evidence would resolve it: Analysis of users' content consumption patterns over time, measuring the diversity of topics, viewpoints, and content types they engage with. Comparison of diversity metrics between users who use DiscomfortFilter and those who do not.

## Limitations

- User study sample size (24 participants over one week) may not capture long-term usage patterns or edge cases in filtering preferences
- System performance depends heavily on preference profile construction quality, but lacks detailed error analysis of failure modes
- Study focused on a single Q&A platform, limiting generalizability across different recommendation contexts

## Confidence

- **High Confidence**: The core finding that users can effectively express and filter discomforting preferences through conversational interfaces is well-supported by the user study data
- **Medium Confidence**: The claim that the preference profile construction improves LLM reasoning is supported by offline proxy tasks but would benefit from more rigorous cross-validation across different recommendation domains
- **Medium Confidence**: The plug-and-play approach's effectiveness is demonstrated in practice but lacks comparison to alternative filtering architectures

## Next Checks

1. **Cross-platform validation**: Deploy DiscomfortFilter on multiple recommendation platforms (news, e-commerce, social media) to test generalizability and identify platform-specific failure modes
2. **Long-term usage study**: Conduct a multi-month study to assess whether users maintain engagement with the filtering system and whether preference profiles remain accurate over time
3. **Privacy impact assessment**: Perform a detailed analysis of the privacy implications of storing comprehensive user preference profiles locally, including potential risks and mitigation strategies