---
ver: rpa2
title: Analyzing Correlations Between Intrinsic and Extrinsic Bias Metrics of Static
  Word Embeddings With Their Measuring Biases Aligned
arxiv_id: '2409.09260'
source_url: https://arxiv.org/abs/2409.09260
tags:
- bias
- word
- words
- weat
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study re-analyzed correlations between intrinsic and extrinsic
  bias metrics of static word embeddings by ensuring both metrics measured the same
  bias. The authors extracted characteristic words from datasets of extrinsic bias
  metrics (WinoBias and hate speech detection) and measured intrinsic bias metrics
  (WEAT and RNSB) using these words.
---

# Analyzing Correlations Between Intrinsic and Extrinsic Bias Metrics of Static Word Embeddings With Their Measuring Biases Aligned

## Quick Facts
- **arXiv ID**: 2409.09260
- **Source URL**: https://arxiv.org/abs/2409.09260
- **Authors**: Taisei KatÃ´; Yusuke Miyao
- **Reference count**: 39
- **Primary result**: Intrinsic bias metrics correlate with WinoBias but not hate speech detection when measuring the same bias type

## Executive Summary
This study investigates correlations between intrinsic and extrinsic bias metrics in static word embeddings by ensuring both metrics measure the same type of bias. The researchers extracted characteristic words from extrinsic bias datasets (WinoBias and hate speech detection) and measured intrinsic bias metrics (WEAT and RNSB) using these words. They generated 45 word embeddings with varying bias levels through dataset balancing and ATTRACT-REPEL algorithms. The study found moderate to high correlations between aligned intrinsic metrics and WinoBias (Spearman's r up to 0.91), but little to no correlation with hate speech detection metrics. This demonstrates that intrinsic bias metrics can predict biased behavior in coreference resolution tasks but not in hate speech detection, emphasizing the importance of matching measuring biases when analyzing metric correlations.

## Method Summary
The researchers created 45 word embeddings with different bias levels using dataset balancing and ATTRACT-REPEL algorithms. They extracted characteristic words from two extrinsic bias datasets: WinoBias (for coreference resolution) and hate speech detection datasets. These characteristic words were then used to measure intrinsic bias metrics (WEAT and RNSB) on the generated embeddings. By ensuring both intrinsic and extrinsic metrics measured the same type of bias, the researchers could accurately assess correlations between them. The analysis used Spearman's rank correlation coefficient to quantify relationships between the aligned metrics.

## Key Results
- Moderate to high correlations (Spearman's r up to 0.91) between aligned intrinsic metrics and WinoBias
- Little to no correlation between aligned intrinsic metrics and hate speech detection metrics
- Intrinsic bias metrics can predict biased behavior in coreference resolution tasks but not in hate speech detection
- Importance of measuring the same bias type when analyzing metric correlations

## Why This Works (Mechanism)
The study works by aligning the measurement of bias across intrinsic and extrinsic metrics. When both metrics evaluate the same bias characteristics using the same words, correlations become apparent. The ATTRACT-REPEL algorithm allows controlled manipulation of bias levels in embeddings, while dataset balancing ensures systematic variation in bias. By extracting characteristic words from extrinsic datasets, the researchers created a common vocabulary for measuring bias across both metric types, eliminating the confounding factor of measuring different biases.

## Foundational Learning

**Word Embeddings**: Vector representations of words that capture semantic relationships, needed to understand how bias manifests in NLP systems. Quick check: Can you explain how "king - man + woman = queen" demonstrates semantic relationships?

**Intrinsic Bias Metrics (WEAT, RNSB)**: Tests that measure bias within word embeddings without downstream applications, required to quantify bias in the embedding space itself. Quick check: How do WEAT and RNSB differ in their approach to measuring bias?

**Extrinsic Bias Metrics**: Measures of bias in downstream applications like coreference resolution or hate speech detection, essential for understanding real-world impact of biased embeddings. Quick check: What makes WinoBias specifically designed for measuring gender bias in coreference resolution?

**ATTRACT-REPEL Algorithm**: A method for generating embeddings with controlled bias levels by attracting relevant words and repelling irrelevant ones, crucial for creating embeddings with systematic bias variation. Quick check: How does ATTRACT-REPEL differ from simple dataset balancing in controlling embedding properties?

**Spearman's Rank Correlation**: A non-parametric measure of statistical dependence between two variables, necessary for quantifying relationships between bias metrics. Quick check: Why might Spearman's correlation be preferred over Pearson's correlation for this analysis?

## Architecture Onboarding

**Component Map**: Dataset preparation -> Embedding generation (ATTRACT-REPEL) -> Characteristic word extraction -> Intrinsic metric measurement -> Extrinsic metric measurement -> Correlation analysis

**Critical Path**: The key sequence is characteristic word extraction from extrinsic datasets, followed by measuring intrinsic metrics on these words across different embeddings, then computing correlations. This path directly tests whether aligned metrics correlate.

**Design Tradeoffs**: The study trades comprehensiveness (testing many downstream tasks) for precision (perfect alignment of measured biases). This ensures valid correlation analysis but limits generalizability to other bias types and tasks.

**Failure Signatures**: Low correlations might indicate: (1) fundamental differences between intrinsic and extrinsic bias manifestations, (2) insufficient word selection from characteristic words, (3) improper alignment of bias types, or (4) limitations of the correlation measure itself.

**First Experiments**:
1. Replicate correlation analysis with additional extrinsic datasets from different NLP tasks
2. Apply the methodology to contextualized embeddings to test generalizability
3. Vary the word selection threshold for characteristic words to test robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to two extrinsic bias datasets (WinoBias and hate speech detection), constraining generalizability
- Focus on static word embeddings excludes contextualized embeddings increasingly used in modern NLP
- Dataset balancing and ATTRACT-REPEL approaches may not capture full spectrum of real-world embedding biases

## Confidence
- Correlations between aligned metrics in coreference resolution: **High**
- Lack of correlation with hate speech detection: **Medium**
- Importance of measuring the same bias type: **Medium**

## Next Checks
1. Replicate the correlation analysis using additional extrinsic bias datasets from diverse downstream tasks (e.g., sentiment analysis, named entity recognition) to test generalizability.
2. Apply the same methodology to contextualized embeddings (BERT, RoBERTa) to assess whether findings extend beyond static embeddings.
3. Conduct a sensitivity analysis by varying the word selection criteria for measuring intrinsic bias metrics to determine robustness of correlation findings.