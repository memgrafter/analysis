---
ver: rpa2
title: Interactive Mars Image Content-Based Search with Interpretable Machine Learning
arxiv_id: '2402.16860'
source_url: https://arxiv.org/abs/2402.16860
tags:
- prototypes
- image
- class
- evidence
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the need for interpretable content-based image
  classification in NASA's Planetary Data System, where millions of planetary images
  must be searched and categorized. The authors develop an interpretable deep learning
  classifier based on ProtoPNet, adding a diversity-enhancing loss term to improve
  the variety of prototypes learned for each class.
---

# Interactive Mars Image Content-Based Search with Interpretable Machine Learning

## Quick Facts
- arXiv ID: 2402.16860
- Source URL: https://arxiv.org/abs/2402.16860
- Reference count: 6
- Primary result: Interpretable ProtoPNet classifier achieves 82.5% accuracy on Mars surface images at 90% confidence threshold, comparable to non-interpretable baseline

## Executive Summary
This paper addresses the challenge of interpretable content-based image classification for NASA's Planetary Data System, where millions of planetary images require automated categorization and search capabilities. The authors develop an interpretable deep learning classifier based on ProtoPNet, enhanced with a diversity-increasing loss term to improve the variety of prototypes learned for each class. The system is designed for deployment on the PDS Image Atlas, enabling scientists to explore Mars image content with visual explanations for classifications.

## Method Summary
The approach uses a ProtoPNet architecture with VGG19 or ResNet18 backbone to extract features from Mars surface images, then learns 10 prototypes per class in the feature space. A diversity-enhancing loss term encourages prototypes to come from different training images, preventing redundancy. The model is trained on the MSL surface dataset with sol-based temporal splitting and outputs classifications only when confidence exceeds 90%. The system provides visual explanations by showing which prototype regions in test images contributed to each classification decision.

## Key Results
- Interpretable ProtoPNet achieves 82.5% top-1 accuracy at 90% confidence threshold on Mars surface images
- The model abstains from classifying approximately 20% of images below the confidence threshold
- Prototype diversity analysis reveals significant variation across classes, with Night Sky showing high diversity and Float Rock showing low diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProtoPNet with diversity loss learns interpretable prototypes that both classify accurately and provide visual explanations for why classifications are made
- Mechanism: The model extracts features from images using a backbone CNN, then learns prototypes for each class in the feature space. During training, the model minimizes cross-entropy loss while also enforcing clustering of similar features and separation of different classes. The diversity loss ensures prototypes are distinct, preventing redundancy
- Core assumption: Visual similarity in feature space correlates with human interpretability of the prototypes as meaningful image regions
- Evidence anchors:
  - [abstract] "The authors develop an interpretable deep learning classifier based on ProtoPNet, adding a diversity-enhancing loss term to improve the variety of prototypes learned for each class"
  - [section] "The similarity score represents the strength of the prototype match, while the weights of the fully connected layer represent its contribution to a class during training"
  - [corpus] Weak - no direct comparison to similar interpretability approaches found in the corpus
- Break condition: If the diversity loss is too strong, it may prevent the model from learning accurate prototypes for classes with inherently similar features, reducing classification accuracy

### Mechanism 2
- Claim: The diversity loss term specifically addresses the problem of prototype redundancy, improving both interpretability and performance
- Mechanism: The diversity loss encourages each prototype to come from a different training image by penalizing prototypes that are too close together in feature space. This forces the model to learn a more varied set of exemplars for each class
- Core assumption: Classes with more diverse visual characteristics require more diverse prototypes to be accurately represented
- Evidence anchors:
  - [section] "Based on observations we made during the evaluation, our contribution was the incorporation of a diversity-enhancing term to the original work by Chen et al. (2019), which notably amplified the diversity of evidence utilized and subsequently enhanced performance"
  - [section] "We noticed a lack of diversity among prototypes, i.e., prototypes often look visually similar or come from the same training image"
  - [corpus] Weak - no direct evidence about diversity loss effects found in corpus
- Break condition: If the margin parameter in the diversity loss is set too high, it may force prototypes to be too dissimilar, preventing the model from learning accurate representations of classes with subtle variations

### Mechanism 3
- Claim: The abstention mechanism at 90% confidence threshold balances accuracy and coverage for deployment
- Mechanism: The model only outputs classifications when confidence exceeds 90%, reducing false positives at the cost of not classifying some images. This is particularly important for scientific applications where incorrect classifications could mislead research
- Core assumption: Users prefer no answer to a wrong answer when dealing with scientific data analysis
- Evidence anchors:
  - [section] "This is crucial as only classification results with a confidence level of 0.9 or greater are delivered to the Atlas"
  - [section] "At an abstention rate less than 20%, the ResNet18 architecture fine-tuned on the MSL surface dataset yields the highest 'Acc(0.9)' of 86.39% with an abstention rate of 15.5%"
  - [corpus] Weak - no similar confidence-based filtering mechanisms found in corpus
- Break condition: If the confidence threshold is set too high, the system may abstain from classifying too many images, reducing its utility for users

## Foundational Learning

- Concept: ProtoPNet architecture and training process
  - Why needed here: Understanding how the model learns prototypes and uses them for classification is essential for debugging and improving the system
  - Quick check question: How does the model compute similarity between a test image and a prototype?

- Concept: Diversity loss and its mathematical formulation
  - Why needed here: The diversity loss is the key contribution of this work and requires understanding to tune and potentially improve
  - Quick check question: What is the purpose of the margin parameter in the diversity loss?

- Concept: Confidence-based classification and abstention
  - Why needed here: The deployment strategy relies on filtering classifications by confidence, which affects both accuracy and coverage
  - Quick check question: Why might the authors choose a 90% confidence threshold rather than 80% or 95%?

## Architecture Onboarding

- Component map:
  Backbone CNN (VGG19/ResNet18) -> ProtoPNet layer -> Diversity loss -> Confidence threshold -> Deployment interface

- Critical path: Image → Backbone features → Prototype similarity scores → Weighted sum → Classification → Confidence check → Output/Abstain

- Design tradeoffs:
  - Interpretability vs. accuracy: ProtoPNet sacrifices some accuracy for explainability
  - Diversity vs. specificity: Too much diversity may harm classes with subtle variations
  - Coverage vs. reliability: Lower confidence threshold increases coverage but may introduce errors

- Failure signatures:
  - Low diversity in prototypes despite diversity loss (check margin and λ3)
  - High abstention rate (check confidence threshold and model calibration)
  - Misclassifications with confident wrong prototypes (check prototype correctness analysis)

- First 3 experiments:
  1. Vary the diversity loss weight λ3 to find optimal balance between diversity and accuracy
  2. Test different confidence thresholds (80%, 85%, 90%, 95%) to optimize coverage vs. reliability
  3. Compare prototype diversity and correctness across different backbone architectures (VGG19 vs. ResNet18)

## Open Questions the Paper Calls Out

- Question: How does the diversity-enhancing loss term specifically affect the classification accuracy and abstention rate across different classes of Mars images?
  - Basis in paper: [explicit] The authors introduced a diversity-enhancing loss term to improve the variety of prototypes learned for each class and observed a 2 percentage point rise in test accuracy with no significant drop in train accuracy
  - Why unresolved: The paper mentions the overall impact of the diversity loss but does not provide a detailed analysis of its effects on individual classes, particularly the most and least diverse classes
  - What evidence would resolve it: A detailed class-wise analysis showing the classification accuracy and abstention rate before and after the inclusion of the diversity loss term for each class

- Question: What is the impact of user feedback on the model refinement protocol, and how does it affect the overall system improvement?
  - Basis in paper: [explicit] The authors plan to investigate the effect of user reporting mechanisms on overall system improvement and to incorporate user feedback into model development
  - Why unresolved: The paper outlines the intention to explore user feedback mechanisms but does not provide any results or evidence of their impact on model performance
  - What evidence would resolve it: Empirical data showing the improvement in model accuracy and reliability after incorporating user feedback, along with user engagement metrics

- Question: How does the interpretability of the ProtoPNet model influence the trust and understanding of non-expert users in the context of the PDS Image Atlas?
  - Basis in paper: [explicit] The authors aim to bridge the gap between the mental model of Atlas users and planetary image classifiers by providing visual explanations for classifications
  - Why unresolved: The paper does not include any user studies or feedback to assess whether the interpretability features actually improve user trust and understanding
  - What evidence would resolve it: Results from user studies evaluating the effectiveness of the interpretability features in enhancing user trust and understanding, including qualitative feedback and quantitative metrics

## Limitations

- The prototype-based interpretability may not scale well to datasets with many classes or highly diverse visual characteristics
- The performance gap between interpretable (82.5%) and non-interpretable (86.39%) models suggests some accuracy tradeoff remains
- Prototype diversity analysis reveals significant variation across classes, with some classes (Night Sky) showing high diversity while others (Float Rock) do not

## Confidence

- High confidence: The interpretable model achieves comparable accuracy to non-interpretable baselines at 90% confidence threshold
- Medium confidence: The diversity loss effectively increases prototype variety, though with class-specific variation
- Medium confidence: The abstention mechanism appropriately balances accuracy and coverage for scientific deployment

## Next Checks

1. Test the model on a held-out temporal split (e.g., sols 2500-2600) to verify generalization to unseen mission periods
2. Compare prototype correctness across different backbone architectures (VGG19 vs. ResNet18) to determine optimal feature extraction
3. Evaluate the diversity loss hyperparameters (λ3 and margin) systematically to identify optimal settings for different class types