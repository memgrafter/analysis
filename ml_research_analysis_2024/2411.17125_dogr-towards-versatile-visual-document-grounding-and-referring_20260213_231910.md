---
ver: rpa2
title: 'DOGR: Towards Versatile Visual Document Grounding and Referring'
arxiv_id: '2411.17125'
source_url: https://arxiv.org/abs/2411.17125
tags:
- data
- text
- document
- grounding
- dogr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DOGR, a new approach for document grounding
  and referring tasks in visual document understanding. The key contribution is the
  DOGR-Engine, which generates high-quality multi-granular parsing data and diverse
  instruction-tuning data to enhance grounding and referring capabilities in multimodal
  large language models (MLLMs).
---

# DOGR: Towards Versatile Visual Document Grounding and Referring

## Quick Facts
- arXiv ID: 2411.17125
- Source URL: https://arxiv.org/abs/2411.17125
- Authors: Yinan Zhou; Yuxin Chen; Haokun Lin; Yichen Wu; Shuyu Yang; Zhongang Qi; Chen Ma; Li Zhu; Ying Shan
- Reference count: 40
- One-line primary result: DOGR achieves 83.2% accuracy on grounded answering tasks and 76.3% F1 score on grounding performance, outperforming state-of-the-art MLLMs.

## Executive Summary
This paper introduces DOGR, a new approach for document grounding and referring tasks in visual document understanding. The key contribution is the DOGR-Engine, which generates high-quality multi-granular parsing data and diverse instruction-tuning data to enhance grounding and referring capabilities in multimodal large language models (MLLMs). The authors construct DOGR-Bench, a comprehensive benchmark with 3.6K test samples across seven tasks on three document types, to evaluate MLLMs' grounding and referring performance. The proposed DOGR model, trained on the generated data, demonstrates superior performance on both grounding and referring tasks, outperforming existing state-of-the-art MLLMs while maintaining competitive results on traditional document understanding benchmarks.

## Method Summary
DOGR-Engine generates multi-granular parsing data through re-rendering strategies for posters/charts and merge strategies for PDF documents, creating comprehensive text box annotations at word, phrase, line, paragraph, and full-page levels. The benchmark categorizes tasks by input format (Grounded Question vs Plain-Text Question) and output format (Grounded Answer, Grounded Reasoning, Grounded Open-ended Answer, Plain Text Answer), creating 7 task combinations. The DOGR model uses a three-stage training pipeline: pre-aligning with LLaVA-558K, pre-training with DocStruct4M plus 2.1M parsing data, and fine-tuning with 2M instruction-tuning samples. GPT-4o generates instruction-tuning data with response format prompts while Post-annotating Strategy ensures accurate grounding annotations for PDF documents.

## Key Results
- DOGR achieves 83.2% accuracy on grounded answering tasks and 76.3% F1 score on grounding performance
- Outperforms state-of-the-art MLLMs including InternVL2, IXC2.5, and LLaVA-Next on DOGR-Bench
- Demonstrates strong generalization capabilities beyond training domains, handling screenshots of paper content and specially shaped fan charts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DOGR-Engine generates high-quality multi-granular parsing data that significantly improves text localization and recognition in visual documents.
- Mechanism: The engine employs a Re-rendering Strategy for posters and charts to obtain precise bounding boxes, and a Merge Strategy for PDF documents to combine ordered and unordered annotations, creating comprehensive parsing data across word, phrase, line, paragraph, and full-page levels.
- Core assumption: Precise bounding box annotations are essential for fine-grained document understanding and can be systematically generated through automated rendering and merging strategies.
- Evidence anchors:
  - [abstract] "DOGR-Engine generates: (1) 2.1M multi-granular document parsing data, which provides text box annotations at the word, phrase, line, paragraph and full-page level across posters, charts, and PDF documents."
  - [section 3.1.1] "We propose the Re-rendering Strategy to obtain precise bounding boxes... The poster data features sparse textual content, making it straightforward to grasp the dependencies between text blocks."
  - [corpus] "weak evidence - related work focuses on visual prompting but lacks specific bounding box generation mechanisms"
- Break condition: If rendering fails to capture text boundaries accurately or if merging strategies cannot resolve conflicts between ordered and unordered annotations.

### Mechanism 2
- Claim: DOGR-Bench provides comprehensive evaluation of grounding and referring capabilities across seven distinct tasks, revealing performance gaps in existing MLLMs.
- Mechanism: The benchmark categorizes tasks by input format (Grounded Question vs Plain-Text Question) and output format (Grounded Answer, Grounded Reasoning, Grounded Open-ended Answer, Plain Text Answer), creating 7 task combinations that test different aspects of grounding and referring.
- Core assumption: Clear task definition and comprehensive evaluation are necessary to identify specific limitations in MLLMs' grounding and referring capabilities.
- Evidence anchors:
  - [abstract] "we construct DOGR-Bench, a benchmark covering seven grounding and referring tasks across three document types (chart, poster, and PDF document), offering a comprehensive evaluation of fine-grained document understanding."
  - [section 4] "By combining two input forms and four output forms, we derive 7 document grounding and referring tasks."
  - [corpus] "related work mentions benchmarks but lacks the specific task categorization and comprehensive evaluation framework"
- Break condition: If task definitions become ambiguous or if evaluation metrics fail to capture meaningful performance differences.

### Mechanism 3
- Claim: Instruction-tuning data construction using GPT-4o with response format prompts enables MLLMs to seamlessly integrate grounding and referring capabilities into dialogue and reasoning.
- Mechanism: GPT-4o generates queries and responses in grounded format with text wrapped in <ocr></ocr> and coordinates in <bbox></bbox>, while Post-annotating Strategy ensures accurate grounding annotations for PDF documents.
- Core assumption: High-quality instruction-tuning data with accurate grounded text in both query and response is crucial for activating MLLMs' grounding and referring capabilities.
- Evidence anchors:
  - [abstract] "instruction-tuning data to activate MLLMs' grounding and referring capabilities in dialogue and reasoning."
  - [section 3.2] "we leverage GPT-4o [16] to generate diverse-formatted instruction-tuning data tailored to various text granularities... incorporate a response format prompt into each query."
  - [corpus] "weak evidence - related surveys mention visual prompting but lack specific instruction-tuning mechanisms"
- Break condition: If GPT-4o fails to follow grounding format requirements or if Post-annotating Strategy cannot accurately retrieve bounding boxes.

## Foundational Learning

- Concept: Optical Character Recognition (OCR) limitations in complex document layouts
  - Why needed here: Understanding why existing OCR tools are insufficient for fine-grained document tasks
  - Quick check question: What are the primary limitations of OCR tools when processing documents with complex layouts?

- Concept: Multi-granular text parsing and its importance
  - Why needed here: Understanding how different text granularities (word, phrase, line, paragraph) contribute to document understanding
  - Quick check question: How does parsing text at different granularities improve grounding and referring capabilities?

- Concept: Grounding vs referring in multimodal contexts
  - Why needed here: Distinguishing between grounding (text-in location-out) and referring (location-in text-out) tasks
  - Quick check question: What is the fundamental difference between grounding and referring tasks in document understanding?

## Architecture Onboarding

- Component map:
  - DOGR-Engine: Data generation pipeline (multi-granular parsing + instruction-tuning)
  - DOGR-Bench: Evaluation framework (7 tasks × 3 document types)
  - DOGR: Baseline model (vision encoder + projector + LLM)
  - Training pipeline: Three-stage approach (pre-aligning → pre-training → fine-tuning)

- Critical path: Data generation → Model training → Evaluation → Iteration
  1. Generate multi-granular parsing data using DOGR-Engine
  2. Construct instruction-tuning data with GPT-4o assistance
  3. Train DOGR model through three-stage pipeline
  4. Evaluate on DOGR-Bench
  5. Analyze performance gaps and iterate

- Design tradeoffs:
  - High-resolution processing vs computational efficiency (pixel shuffle usage)
  - Comprehensive evaluation vs benchmark complexity (7 task categories)
  - Automated data generation vs annotation quality (re-rendering vs manual annotation)

- Failure signatures:
  - Poor text localization/recognition → Check multi-granular parsing data quality
  - Inaccurate grounding coordinates → Verify bounding box annotation accuracy
  - Weak instruction-following → Examine GPT-4o generation prompts and format requirements

- First 3 experiments:
  1. Test Re-rendering Strategy on sample poster data to verify bounding box accuracy
  2. Evaluate Post-annotating Strategy on PDF documents to ensure coordinate retrieval
  3. Validate instruction-tuning data format compliance with GPT-4o output parsing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between pre-training data volume and instruction-tuning data quality for maximizing grounding and referring performance in MLLMs?
- Basis in paper: [inferred] The paper demonstrates that DOGR achieves superior performance on grounding and referring tasks compared to existing models, but notes that extensive pre-training and fine-tuning on large datasets were not conducted as in InternVL2 and IXC2.5.
- Why unresolved: While the paper shows the effectiveness of their DOGR-Engine in generating high-quality data, it does not explore the trade-offs between data volume and quality or determine the optimal ratio for achieving the best performance.
- What evidence would resolve it: Systematic ablation studies varying the proportion of pre-training vs. instruction-tuning data while measuring grounding and referring performance across different MLLM architectures.

### Open Question 2
- Question: How does the grounding and referring performance of MLLMs degrade when applied to documents with significantly different layouts or cultural contexts than those in the training data?
- Basis in paper: [explicit] The paper mentions that DOGR demonstrates strong generalization capabilities beyond some training domains, handling screenshots of paper content and specially shaped fan charts, but also notes failure cases with unfamiliar structural documents.
- Why unresolved: The paper provides qualitative examples of both successful generalization and failure cases, but lacks systematic evaluation of performance degradation across diverse document types and cultural contexts not represented in the training data.
- What evidence would resolve it: Comprehensive testing of DOGR and other MLLMs on document datasets from diverse cultural contexts, languages, and layout structures, measuring performance metrics across these variations.

### Open Question 3
- Question: What are the fundamental limitations of current grounding and referring approaches in handling documents with overlapping text elements or complex hierarchical structures?
- Basis in paper: [explicit] The paper discusses failure cases where DOGR struggles with incomplete grounding content when interrupted or wrapped text is present, and shows examples where referring capabilities are limited by complex document structures.
- Why unresolved: While the paper identifies specific failure modes, it does not systematically analyze the underlying architectural or algorithmic limitations that cause these failures, particularly in handling overlapping text or deeply nested hierarchical document structures.
- What evidence would resolve it: Detailed analysis of grounding errors on documents with overlapping elements, comparison of different architectural approaches (token-based vs. region-based), and evaluation of hierarchical parsing capabilities across multiple MLLM models.

## Limitations

- The DOGR framework relies heavily on automated data generation through rendering and merging strategies, which may introduce systematic biases or errors in bounding box annotations that are difficult to detect.
- The quality of GPT-4o-generated instruction-tuning data depends on the model's ability to consistently follow complex grounding format requirements across diverse document types.
- The three-stage training pipeline involves multiple components (vision encoder, projector, LLM) whose individual contributions to performance gains are not clearly isolated.

## Confidence

- **High confidence**: The task definition and benchmark construction methodology (7 tasks × 3 document types) are well-specified and reproducible.
- **Medium confidence**: The multi-granular parsing data generation strategies (re-rendering and merge strategies) are theoretically sound but may face practical implementation challenges.
- **Medium confidence**: The instruction-tuning data construction using GPT-4o is a reasonable approach but depends on the quality and consistency of AI-generated data.

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of multi-granular parsing data versus instruction-tuning data to overall performance improvements.
2. Perform cross-validation on the DOGR-Bench tasks to verify that the task categorization (GQ/PQ × GA/GR/GO/PA) captures meaningful differences in model capabilities.
3. Test the robustness of DOGR-Engine's rendering and merging strategies on documents with complex layouts that deviate from the training distribution.