---
ver: rpa2
title: Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning
arxiv_id: '2404.00213'
source_url: https://arxiv.org/abs/2404.00213
tags:
- knowledge
- facts
- datasets
- training
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using Supervised Fine-Tuning (SFT) to inject
  new knowledge into Large Language Models (LLMs) by comparing token-based and fact-based
  dataset generation methods. Using sporting events as examples, they find that token-based
  scaling can improve Q&A accuracy but may lack uniform coverage of new knowledge,
  while fact-based scaling ensures more systematic coverage.
---

# Injecting New Knowledge into Large Language Models via Supervised Fine-Tuning

## Quick Facts
- **arXiv ID**: 2404.00213
- **Source URL**: https://arxiv.org/abs/2404.00213
- **Reference count**: 19
- **Key outcome**: Fact-based scaling through SFT provides more systematic knowledge coverage than token-based scaling and achieves near-parity with RAG baselines for domain adaptation

## Executive Summary
This paper investigates Supervised Fine-Tuning (SFT) as a method for injecting new knowledge into Large Language Models, specifically focusing on facts occurring after the model's training cutoff date. The authors compare two dataset scaling approaches: token-based scaling (generating questions based on token counts) and fact-based scaling (generating questions for each atomic fact). Using sporting events as examples, they demonstrate that while token-based scaling can improve Q&A accuracy, it may lack uniform coverage of new knowledge. Their fact-based method leads to more effective knowledge ingestion through SFT, showing considerable performance improvements in out-of-domain Q&A tasks and achieving near parity with RAG baselines.

## Method Summary
The paper employs a systematic approach to knowledge injection using SFT on GPT-4. Documents are first preprocessed to extract plain text sections, then GPT-4 is used to distill atomic facts from these sections. Two parallel dataset generation pipelines are created: token-based scaling generates Q&A pairs based on token counts, while fact-based scaling generates 10 unique Q&A pairs for each atomic fact. The models are fine-tuned using LORA technique with rank 16, batch size 1, and 3 epochs. Evaluation is performed using both token-based and fact-based evaluation sets with GPT-4-based binary correctness assessment.

## Key Results
- Fact-based scaling provides more systematic coverage of knowledge than token-based scaling
- Simple SFT with fact-based datasets achieves near-parity with RAG baselines for knowledge injection
- Token-based scaling shows diminishing returns at higher scales, particularly for pre-cutoff knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fact-based scaling ensures systematic coverage of new knowledge by generating questions for each atomic fact, preventing the bias toward more prominent facts seen in token-based scaling.
- Mechanism: The dataset generation process first extracts atomic facts from source documents, then generates 10 unique Q&A pairs for each fact. This ensures that even less prominent facts receive attention during training, unlike token-based approaches which rely on GPT-4's question generation distribution that may favor certain facts over others.
- Core assumption: GPT-4's question generation distribution over source text sections is nonuniform and may not cover all atomic facts equally, especially when scaling purely by token count.
- Evidence anchors:
  - [abstract] "Fact-based scaling, on the other hand, offers a more systematic approach to ensure even coverage across all facts."
  - [section] "Clearly GPT-4 generated the questions for each section according to some distribution that is nonuniform; it is further unreasonable to assume this distribution will match that of a user querying about these events"
  - [corpus] Weak - no direct corpus evidence for nonuniformity; inferred from experimental results
- Break condition: If the fact extraction process misses important facts or if GPT-4 cannot generate meaningful questions for certain atomic facts, coverage will be incomplete despite the systematic approach.

### Mechanism 2
- Claim: Scaling by fact count rather than token count provides more consistent performance improvements across different knowledge domains, as demonstrated by the more uniform gains from 1x to 5x to 10x scaling in fact-based experiments.
- Mechanism: By ensuring each fact receives multiple Q&A representations during training, the model develops better understanding of the full knowledge space. This contrasts with token-based scaling where some facts may receive many questions while others receive none, leading to diminishing returns as scale increases.
- Core assumption: The number of questions generated for each fact in fact-based scaling is sufficient to capture the knowledge without overfitting to specific phrasings.
- Evidence anchors:
  - [abstract] "Our experiments on GPT-4 demonstrate that while token-based scaling can lead to improvements in Q&A accuracy, it may not provide uniform coverage of new knowledge."
  - [section] "Unlike with the token-based scaling, we see no negative drops in performance as we scale up, even for FIFA 2018."
  - [corpus] Weak - corpus evidence only shows fact coverage analysis, not direct evidence for scaling consistency
- Break condition: If the fact extraction process generates too many facts or if the question generation for each fact becomes repetitive, the scaling benefits may diminish or training may become inefficient.

### Mechanism 3
- Claim: Simple SFT with fact-based datasets can achieve near-parity with RAG baselines for knowledge injection, validating the effectiveness of direct training approaches for domain adaptation.
- Mechanism: The fact-based SFT approach allows the model to internalize new knowledge directly into its parameters, creating a persistent knowledge base that doesn't require external retrieval at inference time. This internalization is achieved through systematic coverage of facts combined with varied question representations.
- Core assumption: The model's pre-training provides sufficient foundation for learning new facts through fine-tuning, and catastrophic forgetting of existing knowledge is not a significant concern for this application.
- Evidence anchors:
  - [abstract] "Our results show considerable performance improvements in Q&A tasks related to out-of-domain knowledge. This study contributes to the understanding of domain adaptation for LLMs"
  - [section] "We demonstrate that even straightforward SFT can lead to substantial improvements in model performance when handling out-of-domain, post-cutoff knowledge"
  - [corpus] Moderate - corpus contains related work on knowledge injection and adapter-based approaches that support the premise
- Break condition: If the model experiences significant catastrophic forgetting of pre-existing knowledge, or if the new facts are too complex to be captured through simple SFT without more sophisticated techniques.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT) as a domain adaptation technique
  - Why needed here: The paper investigates SFT as a method for injecting new knowledge into LLMs, specifically for facts occurring after the model's knowledge cutoff date. Understanding SFT mechanics is essential for interpreting the experimental results and methodology.
  - Quick check question: What is the key difference between SFT and other fine-tuning approaches like RLHF or continued pretraining, and why might SFT be preferred for knowledge injection tasks?

- Concept: Token-based vs fact-based dataset scaling
  - Why needed here: The paper directly compares these two scaling approaches and demonstrates that fact-based scaling provides more systematic coverage. Understanding the distinction is crucial for interpreting the results and implications.
  - Quick check question: How does the token-based scaling approach potentially fail to cover all facts in a document, and what mechanism does fact-based scaling use to address this limitation?

- Concept: Catastrophic forgetting and its mitigation
  - Why needed here: The paper mentions catastrophic forgetting as a concern for knowledge injection methods, though they use direct training approaches. Understanding this concept helps explain why certain techniques (like RAG or adapter-based methods) are considered alternatives.
  - Quick check question: What is catastrophic forgetting in the context of LLM fine-tuning, and how might the fact-based scaling approach help mitigate this risk compared to token-based scaling?

## Architecture Onboarding

- Component map: Document → Fact extraction → Dataset generation → SFT training → Evaluation
- Critical path: Document → Fact extraction → Dataset generation → SFT training → Evaluation. The fact extraction and dataset generation steps are the most critical differentiators from standard approaches.
- Design tradeoffs: Fact-based scaling provides systematic coverage but requires additional computation for fact extraction and may generate larger datasets (2x token count for 1x fact scaling vs 1x token scaling). Token-based scaling is simpler but may miss facts and show diminishing returns at higher scales.
- Failure signatures: Performance degradation at high scaling factors (especially for token-based), significant accuracy gaps between token-on-token and token-on-fact evaluations indicating uncovered facts, and overfitting symptoms when training for too many epochs with repetitive data.
- First 3 experiments:
  1. Replicate the token-based scaling experiment on a single document to establish baseline performance and observe the diminishing returns phenomenon at 10x scale.
  2. Implement fact extraction on the same document and generate the fact-based 1x dataset to compare coverage and initial performance against token-based 1x.
  3. Run cross-validation by evaluating token-based trained models on fact-based evaluation sets to quantify the coverage gap and validate the need for fact-based scaling.

## Open Questions the Paper Calls Out

- Question: How does the performance of fact-based SFT compare to RAG when using the same underlying document knowledge?
- Basis in paper: [explicit] The paper mentions that "in none of the document/dataset-scaling configurations do we outperform RAG with fine-tuning" and discusses benchmarking against RAG baselines.
- Why unresolved: The paper only compares SFT performance to RAG as a contextual benchmark, not as a direct comparison using identical knowledge sources. The exact performance gap between fact-based SFT and RAG using the same documents is not quantified.
- What evidence would resolve it: An experiment directly comparing fact-based SFT and RAG performance on identical document knowledge sources, measuring the exact accuracy difference.

## Limitations

- Coverage Gap Quantification: The paper demonstrates that token-based scaling leads to inconsistent fact coverage, but the exact magnitude of the coverage gap is not precisely quantified.
- Fact Extraction Quality: The paper relies on GPT-4 to extract atomic facts from source documents, but there's no validation of this extraction process.
- Generalizability to Non-Sporting Domains: All evaluation datasets come from sporting events, limiting generalizability to other knowledge domains.

## Confidence

**High Confidence (Corpus supports, multiple evidence anchors, clear mechanism)**:
- Fact-based scaling provides more systematic coverage of knowledge than token-based scaling
- Simple SFT with fact-based datasets can achieve near-parity with RAG baselines
- Token-based scaling shows diminishing returns at higher scales, particularly for pre-cutoff knowledge

**Medium Confidence (Evidence present but some gaps or assumptions)**:
- Fact-based scaling provides more consistent performance improvements across different knowledge domains
- The systematic coverage advantage of fact-based scaling is primarily due to preventing bias toward prominent facts
- Fact-based SFT internalizes knowledge effectively without requiring external retrieval

**Low Confidence (Limited evidence, strong assumptions, or speculative claims)**:
- Fact-based scaling will maintain its advantages when applied to more complex, less structured knowledge domains
- The fact extraction process using GPT-4 is reliable and complete for the intended application
- The 2x token count overhead for fact-based scaling is justified by the performance gains in all scenarios

## Next Checks

1. **Coverage Gap Quantification**: Run a detailed analysis comparing the exact coverage statistics between token-based and fact-based scaling approaches. For each fact in the source documents, determine whether it received at least one question in each scaling method, and calculate the coverage gap as a percentage.

2. **Cross-Domain Validation**: Apply the same methodology to a non-sporting domain (e.g., scientific literature, news articles, or technical documentation) and compare the relative performance of token-based versus fact-based scaling.

3. **Catastrophic Forgetting Analysis**: Implement a pre- and post-fine-tuning evaluation of the model's performance on general knowledge tasks (e.g., MMLU or similar benchmarks) to quantify the extent of catastrophic forgetting. Compare this against the gains in the new knowledge domain to determine the true cost-benefit tradeoff of the SFT approach.