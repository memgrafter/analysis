---
ver: rpa2
title: 'INT-FlashAttention: Enabling Flash Attention for INT8 Quantization'
arxiv_id: '2409.16997'
source_url: https://arxiv.org/abs/2409.16997
tags:
- quantization
- int-flashattention
- flashattention
- int8
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INT-FlashAttention, the first INT8 quantization
  architecture compatible with the forward workflow of FlashAttention, which significantly
  improves the inference speed of FlashAttention on Ampere GPUs. The proposed method
  implements the Q, K, and V matrices in self-attention module with fully INT8 format
  and uses INT8 general matrix multiplication (GEMM) kernels to replace all matrix
  multiplications during inference.
---

# INT-FlashAttention: Enabling Flash Attention for INT8 Quantization

## Quick Facts
- arXiv ID: 2409.16997
- Source URL: https://arxiv.org/abs/2409.16997
- Authors: Shimao Chen, Zirui Liu, Zhiying Wu, Ce Zheng, Peizhuang Cong, Zihan Jiang, Yuhan Wu, Lei Su, Tong Yang
- Reference count: 6
- One-line primary result: Achieves 72% faster inference speed compared to FlashAttention with FP16 while reducing quantization error by 82% compared to FP8 methods

## Executive Summary
INT-FlashAttention introduces the first INT8 quantization architecture fully compatible with FlashAttention's forward workflow, enabling significant speedups on Ampere GPUs while maintaining high accuracy. The method implements Q, K, and V matrices in self-attention using fully INT8 format with token-level scaling, replacing all matrix multiplications with INT8 GEMM kernels during inference. This approach achieves substantial performance gains while preserving token-level information that previous tensor-level quantization methods lose, resulting in both faster inference and improved accuracy over existing quantization strategies.

## Method Summary
INT-FlashAttention quantizes the Q, K, and V matrices from self-attention modules using INT8 format with token-level scaling factors, then leverages Ampere GPU's INT8 computational support through specialized GEMM kernels. The method maintains mathematical equivalence with standard FlashAttention by implementing online softmax adaptation that scales intermediate results and implicitly handles normalization factors. While Q and K use per-token quantization with separate scaling vectors SQ and SK, V matrix currently uses tensor-level quantization, with per-block quantization identified as future work. The architecture preserves the core FlashAttention tiling strategy while replacing all FP16 operations with INT8 equivalents.

## Key Results
- 72% faster inference speed compared to FlashAttention with FP16 on Ampere GPUs
- 82% smaller quantization error compared to FlashAttention with FP8 data format
- Maintains token-level information preservation, offering substantial accuracy improvement over existing tensor-level FP8 methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: INT-FlashAttention uses INT8 quantization with token-level scaling to achieve high accuracy while maintaining speed
- Mechanism: Token-level quantization applies separate scaling factors SQ and SK for each row of Q and K matrices, reducing quantization error from outlier features compared to tensor-level approaches
- Core assumption: Row-wise feature distributions in Q and K matrices are relatively homogeneous within each row but vary across different rows
- Evidence anchors: [abstract] "By preserving token-level information, INT-FlashAttention offers a substantial accuracy improvement over existing tensor-level FP8 methods"; [section 3.2] "We maintain two vector scalars SQ, SK ∈ RN, for token-level quantization of the Q and K matrices"
- Break condition: If row-wise distributions become highly heterogeneous within a single row, token-level quantization loses its advantage

### Mechanism 2
- Claim: INT8 GEMM kernels provide significant speedup over FP16 equivalents on Ampere GPUs
- Mechanism: Ampere GPUs have dedicated hardware units optimized for INT8 operations, reducing both computation time and memory bandwidth requirements
- Core assumption: INT8 operations on Ampere GPUs are significantly faster than FP16 operations due to dedicated hardware support
- Evidence anchors: [abstract] "significantly improves the inference speed of FlashAttention on Ampere GPUs"; [section 1] "Given that the Ampere architecture provides computational support for INT8 data format"
- Break condition: If INT8 GEMM kernels are not properly optimized or if memory bandwidth becomes the bottleneck instead of computation

### Mechanism 3
- Claim: Online softmax adaptation to INT8 maintains mathematical equivalence while enabling quantization
- Mechanism: The algorithm scales intermediate results by R (INT8 range) and rounds to integers, then implicitly divides by R through the normalization factor to maintain softmax equivalence
- Core assumption: The small rounding errors introduced by INT8 quantization are negligible compared to the benefits of reduced computation and memory usage
- Evidence anchors: [section 3.2] "We directly use the quantized weight matrix P(j) to update the sum of exponentials l(j), thereby implicitly dividing the scaling factor SP into l(j)"; [section 3.1] "Online softmax computes local softmax within each block...and finally rescales the results to get the same output as standard softmax"
- Break condition: If quantization error accumulates significantly across multiple attention layers, degrading model accuracy

## Foundational Learning

- Concept: FlashAttention tiling strategy
  - Why needed here: INT-FlashAttention builds directly on FlashAttention's memory-efficient tiling approach, so understanding the tiling mechanism is essential
  - Quick check question: How does FlashAttention reduce memory complexity from O(N²) to O(N)?

- Concept: Tensor Core operations on NVIDIA GPUs
  - Why needed here: INT-FlashAttention leverages Ampere GPU Tensor Cores for INT8 operations, requiring understanding of hardware capabilities
  - Quick check question: What are the key differences between Tensor Core operations for INT8 vs FP16 on Ampere GPUs?

- Concept: Symmetric linear quantization
  - Why needed here: INT-FlashAttention uses linear symmetric quantization for Q, K, and V matrices with per-token scaling
  - Quick check question: How does linear symmetric quantization with per-token scaling differ from per-tensor scaling in terms of quantization error?

## Architecture Onboarding

- Component map: Input Q, K, V matrices in INT8 format → Token-level scaling with SQ, SK vectors → INT8 GEMM kernels replacing FP16 operations → Online softmax computation → Attention matrix output with dequantization
- Critical path: Memory read → INT8 GEMM → Online softmax computation → Memory write
- Design tradeoffs:
  - Speed vs accuracy: INT8 provides speedup but introduces quantization error
  - Memory vs computation: Tiling reduces memory but increases kernel launches
  - Complexity vs generality: Token-level quantization is more complex but more accurate than tensor-level
- Failure signatures:
  - Accuracy degradation: Check quantization error bounds and scaling factor calculation
  - Performance regression: Verify INT8 kernel optimization and memory coalescing
  - Memory overflow: Examine tiling parameters and block sizes
- First 3 experiments:
  1. Verify mathematical equivalence: Compare outputs of standard FlashAttention (FP16) vs INT-FlashAttention with synthetic inputs
  2. Quantization error analysis: Measure Mean Relative Error across different activation distributions
  3. Performance benchmarking: Compare inference time vs standard FlashAttention across sequence lengths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal quantization strategy for the V matrix in INT-FlashAttention to further improve inference speed and accuracy?
- Basis in paper: [explicit] The paper states that the V matrix currently is only implemented with tensor-level quantization and mentions it remains a significant challenge to quantize V on a per-token basis
- Why unresolved: The paper acknowledges this as a limitation and future work item, suggesting that per-token quantization for the V matrix is non-trivial and requires further investigation
- What evidence would resolve it: Comparative experiments showing the performance (inference speed and accuracy) of INT-FlashAttention with different V matrix quantization strategies (tensor-level, per-block, and per-token) on various LLM architectures and datasets

### Open Question 2
- Question: How does the INT-FlashAttention quantization error scale with sequence length and model size?
- Basis in paper: [inferred] The paper shows quantization error for sequence lengths up to 16k, but doesn't explore the relationship between quantization error, sequence length, and model size in detail
- Why unresolved: The paper provides limited data on how quantization error varies with sequence length and model size, which is crucial for understanding the method's scalability
- What evidence would resolve it: Extensive experiments measuring quantization error across a wide range of sequence lengths (e.g., 1k to 128k) and model sizes (e.g., from 7B to 70B parameters) using various quantization metrics

### Open Question 3
- Question: How does INT-FlashAttention perform on non-A100 GPUs with INT8 support (e.g., H100)?
- Basis in paper: [explicit] The paper focuses on Ampere GPUs (A100) and doesn't report performance on newer GPU architectures with INT8 support
- Why unresolved: The paper's performance evaluation is limited to a specific GPU architecture, leaving uncertainty about the method's effectiveness on other hardware platforms
- What evidence would resolve it: Comparative benchmarking of INT-FlashAttention against baseline methods on multiple GPU architectures (A100, H100, and potentially others) with INT8 support, measuring inference speed and quantization accuracy

## Limitations

- The V matrix currently uses only tensor-level quantization rather than per-token or per-block approaches, representing a significant accuracy and performance limitation
- Performance claims are limited to Ampere A100 GPUs without validation on other GPU architectures with INT8 support
- The paper lacks comprehensive analysis of quantization error accumulation across multiple attention layers in deep networks

## Confidence

**High Confidence**: The fundamental approach of combining INT8 quantization with FlashAttention's tiling strategy is technically sound. The mathematical framework for token-level quantization and online softmax adaptation follows established principles in low-precision computing.

**Medium Confidence**: The performance speedup claims are reasonable given Ampere GPU architecture specifications, but actual gains will vary based on implementation details, memory bandwidth, and specific model characteristics. The 82% error reduction claim over FP8 methods requires validation across diverse workloads.

**Low Confidence**: The assertion that token-level quantization preserves "substantial accuracy improvement" lacks comprehensive empirical validation across multiple model families and downstream tasks. The impact of quantization error accumulation across deep attention networks remains underexplored.

## Next Checks

1. **Cross-Architecture Validation**: Test INT-FlashAttention on different GPU architectures (Hopper, Ada Lovelace) to verify that the claimed 72% speedup is architecture-dependent and to identify performance bottlenecks specific to each generation.

2. **Error Propagation Analysis**: Conduct multi-layer attention experiments to measure how quantization errors accumulate across depth, particularly focusing on the impact on model convergence during fine-tuning and inference accuracy on benchmark tasks.

3. **Scalability Benchmarking**: Evaluate performance and accuracy degradation with sequence lengths exceeding 8K tokens, measuring both memory efficiency gains and any accuracy trade-offs from increased token-level scaling factor overhead.