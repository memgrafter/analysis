---
ver: rpa2
title: Learning Differentiable Surrogate Losses for Structured Prediction
arxiv_id: '2411.11682'
source_url: https://arxiv.org/abs/2411.11682
tags:
- learning
- output
- structured
- space
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a novel framework for structured prediction,
  termed Explicit Loss Embedding (ELE), which learns a differentiable surrogate loss
  through contrastive learning on the output space. The method consists of three main
  steps: learning an output embedding via contrastive learning, solving a surrogate
  regression problem with the learned loss, and decoding predictions using projected
  gradient descent.'
---

# Learning Differentiable Surrogate Losses for Structured Prediction

## Quick Facts
- arXiv ID: 2411.11682
- Source URL: https://arxiv.org/abs/2411.11682
- Reference count: 19
- Proposes a framework (ELE) that learns differentiable surrogate losses for structured prediction through contrastive learning on output space

## Executive Summary
This paper introduces Explicit Loss Embedding (ELE), a novel framework for structured prediction that learns differentiable surrogate losses through contrastive learning on the output space. The method addresses a key limitation in structured prediction: the need for hand-crafted loss functions by learning a loss function directly from output training data. Applied to molecular graph prediction from SMILES strings, ELE achieves competitive or better performance compared to existing methods using pre-defined losses. The framework's key innovation is its ability to predict novel graph structures not present in the training set through a gradient-based decoding strategy.

## Method Summary
ELE consists of three main steps: (1) learning an output embedding via contrastive learning that captures similarity in the original output space, (2) solving a surrogate regression problem with the learned loss to train an input neural network, and (3) decoding predictions using projected gradient descent. The framework is applied to molecular graph prediction where SMILES strings are inputs and molecular graphs are outputs. The output embedding is learned using Relational Graph Convolutional Networks (R-GCNs) with node dropping augmentation, while the input network uses a Transformer encoder. Predictions are made by optimizing the decoding objective through gradient descent on a continuous relaxation of the output space.

## Key Results
- ELE achieves competitive or better performance compared to existing methods using pre-defined losses on molecular graph prediction
- The framework successfully predicts novel graph structures not present in the training set
- The flexible design allows for the use of neural networks to handle complex input data types like SMILES strings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The contrastive learning step learns an explicit differentiable embedding of structured outputs that captures similarity in the original output space.
- Mechanism: By pulling together positive pairs (similar structures like molecules with same scaffold) and pushing apart negative pairs in the learned embedding space, the network learns a representation where Euclidean distance corresponds to meaningful structural similarity.
- Core assumption: The contrastive objective (e.g., InfoNCE) effectively captures the relevant structure similarity when positive samples are created through meaningful transformations (node dropping) and negative samples are from different structures.
- Evidence anchors:
  - [abstract] "a structured loss function, parameterized by neural networks, is learned directly from output training data through Contrastive Learning"
  - [section 2.2.1] "CRL aims at finding a NN-parameterized mapping ψˆθ : Y → Sd−1 such that the loss ℓϕ,φ(...) is minimized"
  - [corpus] Strong evidence - related papers on contrastive learning for structured data (node2vec, GraphCL, SimCSE) show effectiveness of this approach
- Break condition: If positive/negative sampling doesn't capture true structural similarity, the learned embedding won't reflect meaningful distances for prediction.

### Mechanism 2
- Claim: The finite-dimensional explicit embedding enables differentiable surrogate regression with neural networks.
- Mechanism: Unlike kernel methods with implicit infinite-dimensional features, the explicit d-dimensional embedding allows direct parameterization of the hypothesis space H with a neural network uW, making the surrogate regression problem computationally tractable and differentiable.
- Core assumption: The embedding dimension d is sufficiently large to capture structure-relevant information while being small enough for efficient neural network training.
- Evidence anchors:
  - [abstract] "the differentiable loss not only enables the learning of neural networks due to the finite dimension of the surrogate space"
  - [section 2.2.2] "the fact that Sd−1 ⊂ Rd is of finite dimension d enables us to parameterize the hypothesis space H with a deep neural network"
  - [corpus] Moderate evidence - DSOKR paper shows benefits of dimensionality reduction for structured prediction, supporting this mechanism
- Break condition: If d is too small, the embedding loses information; if too large, computational efficiency is lost.

### Mechanism 3
- Claim: Differentiable loss enables gradient-based decoding to predict novel structures not in training set.
- Mechanism: The learned embedding ψˆθ is differentiable with respect to the structured variable y, allowing projected gradient descent to optimize the decoding objective ∥ψˆθ(y) − h ˆW (x)∥2 over the continuous relaxation of the output space, then mapping back to discrete structures.
- Core assumption: The continuous relaxation of the structured output space is meaningful and the reverse operator R−1 can recover valid discrete structures from optimized continuous ones.
- Evidence anchors:
  - [abstract] "allows for the prediction of new structures via a decoding strategy based on gradient descent"
  - [section 2.2.3] "we propose to solve the decoding problem described by Equation 10 by Gradient Descent"
  - [section 3] "The following proposition details how to conduct the projection operation PG(·) during gradient descent" with specific formulas for graph projection
  - [corpus] Limited evidence - SPEN uses gradient-based decoding but only for multi-label spaces, not general graphs
- Break condition: If the relaxation/projection doesn't preserve structural validity or if the optimization landscape is too non-convex.

## Foundational Learning

- Concept: Contrastive Learning and InfoNCE loss
  - Why needed here: To learn the output embedding that captures meaningful similarity between structured objects
  - Quick check question: How does the InfoNCE loss encourage similar structures to be close in embedding space while dissimilar ones are far apart?

- Concept: Implicit Loss Embedding (ILE) property
  - Why needed here: To understand why the learned squared distance loss is valid for structured prediction (corresponds to a proper loss function)
  - Quick check question: What mathematical condition must a loss function satisfy to have the ILE property, and why is this important for Fisher consistency?

- Concept: Graph Neural Networks (GNNs) and Relational GNNs
  - Why needed here: To implement the output embedding ψθ that can process graph-structured data and be differentiable with respect to graph features
  - Quick check question: How does the layer operation in R-GCNs (summing over edge types) ensure differentiability with respect to both node and edge features?

## Architecture Onboarding

- Component map:
  - Output NN (ψθ) -> Contrastive learning module -> Input NN (hW) -> Decoding module (PGD)
- Critical path: Output NN training (contrastive) → Input NN training (surrogate regression) → Decoding (PGD)
- Design tradeoffs:
  - Embedding dimension d vs. expressiveness vs. computational cost
  - Node dropping rate vs. positive sample quality vs. training stability
  - PGD steps vs. prediction quality vs. inference time
  - Using pre-training vs. training from scratch
- Failure signatures:
  - Poor contrastive learning: Embeddings show no meaningful clustering by structure type
  - Bad surrogate regression: Training loss decreases but test GED remains high
  - Decoding failure: PGD doesn't converge or produces invalid structures
- First 3 experiments:
  1. Train output NN with contrastive learning only, visualize embeddings with t-SNE to check if similar molecules cluster together
  2. Fix output NN, train input NN with surrogate regression, check if hW(xi) ≈ ψˆθ(yi) for training pairs
  3. Implement candidate selection decoding, measure GED vs. random baseline to verify learned loss is useful

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided.

## Limitations
- The method's scalability to larger graphs remains untested, as experiments are limited to molecules with at most 9 nodes
- The contrastive learning approach requires careful sampling of positive and negative pairs, which may not generalize well to all structured output domains
- The method's performance relative to state-of-the-art structured prediction methods on non-graph tasks is not evaluated

## Confidence
- High confidence: The theoretical framework connecting contrastive learning to explicit loss embedding is sound and well-grounded in existing literature
- Medium confidence: The empirical results on molecular graph prediction are convincing but limited in scope
- Low confidence: The generalizability of the method to other structured prediction domains (e.g., sequences, trees) beyond molecular graphs

## Next Checks
1. **Scalability test:** Apply ELE to larger molecular graphs (e.g., from ZINC database) or protein structures to evaluate performance degradation with increased graph size
2. **Domain