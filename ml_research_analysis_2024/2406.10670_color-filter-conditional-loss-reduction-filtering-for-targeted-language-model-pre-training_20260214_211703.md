---
ver: rpa2
title: 'CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model
  Pre-training'
arxiv_id: '2406.10670'
source_url: https://arxiv.org/abs/2406.10670
tags:
- data
- color-filter
- prior
- selection
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoLoR-Filter, a simple and efficient data
  selection method for language model pre-training that selects high-quality subsets
  from large datasets. The method scores each sequence by the difference in likelihood
  between a model trained on downstream data and a prior model, effectively selecting
  data that is more likely under the downstream task.
---

# CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training

## Quick Facts
- **arXiv ID**: 2406.10670
- **Source URL**: https://arxiv.org/abs/2406.10670
- **Reference count**: 40
- **Key result**: CoLoR-Filter selects high-quality subsets from large datasets by scoring sequences based on likelihood differences between downstream-trained and prior models, achieving strong scaling performance.

## Executive Summary
CoLoR-Filter introduces a simple and efficient data selection method for language model pre-training that identifies high-quality data subsets from large datasets. The method works by scoring each sequence based on the difference in likelihood between a model trained on downstream data and a prior model, effectively selecting data that aligns with the downstream task distribution. This approach demonstrates strong empirical scaling: a 1.2b parameter model trained on data selected by a 150m parameter model matches performance of models trained on 25x more randomly selected tokens for Books domain adaptation and 11x more tokens for downstream multiple-choice tasks.

## Method Summary
CoLoR-Filter scores sequences by computing the difference in likelihood between an auxiliary model (trained on downstream data) and a prior model. This conditional loss reduction approach identifies data points where the auxiliary model assigns higher probability than the prior, indicating better alignment with the target domain. The method is computationally efficient because it only requires model inference rather than training on the full dataset. Selection aggressiveness is controlled by a hyperparameter τ, with results showing favorable scaling that has not yet saturated even at extreme compression ratios like 1/64 of data points.

## Key Results
- 1.2b parameter model matches performance of models trained on 25x more tokens for Books domain adaptation
- 1.2b parameter model matches performance of models trained on 11x more tokens for downstream multiple-choice tasks
- Strong scaling with selection aggressiveness hyperparameter τ, with no saturation observed even at 1/64 data selection ratio

## Why This Works (Mechanism)
CoLoR-Filter works by leveraging the likelihood difference between two models to identify data that is more representative of the downstream task. The auxiliary model, trained on downstream data, captures the target domain distribution, while the prior model represents general language knowledge. By selecting sequences where the auxiliary model assigns significantly higher likelihood than the prior, the method effectively filters for data that reduces conditional loss on the target task. This approach is more efficient than traditional methods because it requires only inference rather than full model training on the selection dataset.

## Foundational Learning

**Likelihood scoring**: Measuring probability assigned to sequences by language models
- *Why needed*: Forms the core metric for evaluating data quality in CoLoR-Filter
- *Quick check*: Verify that likelihood differences correlate with downstream task performance

**Domain adaptation**: Adjusting models to perform well on specific target domains
- *Why needed*: The method's effectiveness depends on capturing domain-specific patterns
- *Quick check*: Test performance across diverse domains beyond the demonstrated Books and multiple-choice tasks

**Model scaling**: Understanding how model performance changes with parameter count
- *Why needed*: CoLoR-Filter shows favorable scaling from 150m to 1.2b parameters
- *Quick check*: Validate scaling behavior with even larger models beyond those tested

## Architecture Onboarding

**Component map**: Data → Model inference → Likelihood scoring → Selection filtering → Target model training
- *Critical path*: Data scoring via auxiliary model → selection based on τ threshold → training target model on filtered data
- *Design tradeoffs*: Computational efficiency (inference only) vs. potential information loss from aggressive filtering
- *Failure signatures*: Poor downstream performance when auxiliary model underfits or when τ is set too aggressively
- *3 first experiments*: 1) Verify likelihood scoring produces meaningful rankings, 2) Test performance with varying τ values, 3) Compare against random selection baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness relies heavily on availability of sufficient downstream data for pre-training the auxiliary model, with potential degradation at extremely small data regimes
- Performance across diverse domains remains untested beyond Books and multiple-choice tasks, raising questions about generalizability
- Computational efficiency claims depend on parallel inference capabilities and infrastructure specifics not detailed in the paper

## Confidence
- **High confidence**: The core algorithmic contribution and computational efficiency are well-established
- **Medium confidence**: Empirical scaling results show convincing performance within tested ranges but may not generalize to all configurations
- **Medium confidence**: Claims about selection data generalizing to larger target models are supported but could benefit from testing on substantially larger model sizes

## Next Checks
1. Test CoLoR-Filter's performance when downstream data is extremely limited (<1,000 examples) to establish minimum viable dataset size
2. Evaluate the method across diverse domains including code, scientific literature, and multilingual datasets
3. Conduct experiments with target models significantly larger than 1.2b parameters (e.g., 10b+ parameters) to verify scaling behavior at extreme compression ratios