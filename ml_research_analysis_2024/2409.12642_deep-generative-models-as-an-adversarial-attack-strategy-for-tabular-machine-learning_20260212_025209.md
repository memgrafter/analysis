---
ver: rpa2
title: Deep generative models as an adversarial attack strategy for tabular machine
  learning
arxiv_id: '2409.12642'
source_url: https://arxiv.org/abs/2409.12642
tags:
- adversarial
- data
- tabular
- examples
- dgms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper adapts four popular deep generative models for tabular
  data into adversarial deep generative models (AdvDGMs) to generate adversarial examples
  that satisfy domain constraints. The key innovation is integrating a constraint
  repair layer with the AdvDGMs to ensure generated examples comply with linear inequalities
  representing domain knowledge.
---

# Deep generative models as an adversarial attack strategy for tabular machine learning

## Quick Facts
- arXiv ID: 2409.12642
- Source URL: https://arxiv.org/abs/2409.12642
- Authors: Salijona Dyrmishi; Mihaela Cătălina Stoian; Eleonora Giunchiglia; Maxime Cordy
- Reference count: 25
- Key outcome: C-AdvWGAN achieves 95% attack success rates with minimal 0.12s runtime overhead across four real-world datasets

## Executive Summary
This paper proposes adversarial deep generative models (AdvDGMs) for generating adversarial examples in tabular machine learning. The key innovation is integrating a constraint repair layer that ensures generated examples satisfy domain constraints while maintaining high attack success rates. Experiments show that WGAN-based AdvDGMs outperform other architectures, achieving up to 95% attack success rates while adding only 0.12 seconds of runtime overhead.

## Method Summary
The method adapts four tabular deep generative models (WGAN, TableGAN, CTGAN, TVAE) into adversarial generators by modifying their training objectives to include adversarial loss (causing prediction changes) and perturbation loss (limiting magnitude). A constraint repair layer is integrated to ensure generated examples satisfy linear inequalities representing domain knowledge. The approach is evaluated across four real-world datasets and three target models, comparing unconstrained vs. constrained generation during training vs. sampling.

## Key Results
- AdvWGAN achieves the highest attack success rates (up to 95%) across all target models
- Constraint repair layer improves attack success rates for most models while ensuring domain compliance
- Runtime overhead is minimal (0.02-0.12 seconds) for constrained generation
- Compared to state-of-the-art attacks, best-performing AdvWGAN ranks second-best for two datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraint repair layer ensures domain compliance while preserving adversarial effectiveness
- Core assumption: Linear constraints can be minimally repaired without destroying adversarial perturbations
- Evidence anchors: Abstract, section 4 methodology, corpus assumptions
- Break condition: Non-linear constraints or complex feature interactions

### Mechanism 2
- Claim: WGAN architecture provides superior adversarial attack performance
- Core assumption: WGAN's training stability translates to better adversarial example generation
- Evidence anchors: Section 6.1 results, corpus assumptions
- Break condition: Other DGMs achieving similar gradient stability

### Mechanism 3
- Claim: Minimal runtime overhead justifies the constraint layer benefits
- Core assumption: Constraint repair computation is negligible compared to benefits
- Evidence anchors: Section 6.2 runtime measurements, section 6.1 ASR improvements
- Break condition: Complex constraints requiring significant computation

## Foundational Learning

- Adversarial machine learning and adversarial examples
  - Why needed: Understanding how to generate inputs that cause ML models to make incorrect predictions while maintaining minimal distance from legitimate examples
  - Quick check: What distinguishes an adversarial example from random noise in terms of its relationship to the original input and its effect on model predictions?

- Deep Generative Models (DGMs) and their training objectives
  - Why needed: Understanding how DGMs learn data distributions and how their objectives can be modified for adversarial purposes
  - Quick check: How does the generator-discriminator game in GANs differ from standard supervised learning, and why is this useful for generating adversarial examples?

- Linear constraint satisfaction and constraint repair
  - Why needed: Understanding how to enforce domain knowledge expressed as linear inequalities on generated examples
  - Quick check: Given a set of linear inequalities, how can you determine if a point satisfies all constraints, and what minimal modifications might enforce compliance?

## Architecture Onboarding

- Component map: Original example -> Generator -> Constraint Repair -> Target Classifier evaluation -> Adversarial loss computation -> Model update
- Critical path: Original example → Generator → Constraint Repair → Target Classifier evaluation → Adversarial loss computation → Model update
- Design tradeoffs: Runtime vs. attack success rate, model complexity vs. training stability, constraint strictness vs. adversarial effectiveness
- Failure signatures: Low attack success rates, constraint violations, unrealistic feature values
- First 3 experiments:
  1. Generate unconstrained adversarial examples with AdvWGAN and measure attack success rate and constraint violations
  2. Add constraint repair layer and compare runtime overhead vs. constraint satisfaction improvement
  3. Compare AdvWGAN performance against other DGM-based approaches on the same datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does WGAN consistently outperform other tabular DGMs as an adversarial attack method?
- Basis: WGAN achieves high success rates despite being older and not always most performant in augmentation literature
- Why unresolved: Paper doesn't explain why WGAN's architecture is more suitable for adversarial attacks
- Evidence needed: Further experiments comparing AdvWGAN with other AdvDGMs on wider range of datasets and target models

### Open Question 2
- Question: Why is the constraint layer more effective when included during training versus only during sampling?
- Basis: Paper states further investigation is needed to understand this difference
- Why unresolved: No detailed analysis of how constraint layer affects training process versus sampling process
- Evidence needed: Controlled experiments comparing C-AdvDGMs with and without constraint layer during training

### Open Question 3
- Question: How can the constrained layer be extended to handle more complex constraints beyond linear inequalities?
- Basis: Current implementation limited to linear inequalities and "if-else" conjunctions
- Why unresolved: Paper doesn't explore extending to non-linear inequalities or complex logical combinations
- Evidence needed: Implementation and evaluation with more complex constraint types

## Limitations

- Performance advantage of WGAN may be specific to tested datasets and target models
- Linear constraint repair may not generalize to domains with non-linear relationships or complex feature interactions
- High attack success rates depend on specific hyperparameter settings and dataset characteristics

## Confidence

- High confidence: Runtime overhead measurements (0.02-0.12 seconds) with clear methodology
- Medium confidence: Attack success rate comparisons between DGM architectures
- Medium confidence: Constraint repair effectiveness for linear constraints

## Next Checks

1. Test constrained generation approach on datasets with non-linear domain constraints to evaluate generalizability
2. Conduct ablation studies isolating impact of WGAN's training stability versus other architectural choices
3. Evaluate constraint repair layer's robustness with larger perturbations (L2 norm > 0.05) to understand performance limits