---
ver: rpa2
title: 'CELL your Model: Contrastive Explanations for Large Language Models'
arxiv_id: '2406.11785'
source_url: https://arxiv.org/abs/2406.11785
tags:
- prompt
- response
- contrastive
- cell
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CELL, a novel method for generating contrastive
  explanations for large language models (LLMs). The core idea is to explain why an
  LLM outputs a particular response by showing how slightly modifying the input prompt
  would lead to a different, less preferable, or contradictory response.
---

# CELL your Model: Contrastive Explanations for Large Language Models

## Quick Facts
- arXiv ID: 2406.11785
- Source URL: https://arxiv.org/abs/2406.11785
- Authors: Ronny Luss; Erik Miehling; Amit Dhurandhar
- Reference count: 40
- Key outcome: CELL generates contrastive explanations by showing how slightly modifying a prompt leads to different LLM responses, achieving 58-60% favorability vs 36-44% for baselines

## Executive Summary
This paper introduces CELL, a novel method for generating contrastive explanations for large language models. The core idea is to explain why an LLM outputs a particular response by showing how slightly modifying the input prompt would lead to a different, less preferable, or contradictory response. The key contribution is a budgeted algorithm that efficiently finds these contrastive examples while minimizing the number of expensive LLM queries, which is crucial for long prompts or documents.

CELL outperforms baselines in generating favorable contrasts, achieving 58-60% favorability compared to 36-44% for baselines. It can successfully find contrastive examples 56-95% of the time while maintaining low edit distances of 0.12-0.32 words. The method is also applied to explain conversational agents by identifying dimensions where responses can be improved.

## Method Summary
CELL generates contrastive explanations by perturbing substrings of the input prompt and using a user-defined scoring function to evaluate the quality of the modified prompt-response pair. The algorithm iteratively masks and replaces substrings, queries the LLM to generate responses, and scores these responses against the original using the scoring function. The process continues until a sufficiently different (contrasting) response is found or the budget is exhausted. CELL employs a seed-driven approach where some seeds are generated from the initial prompt and others from previously perturbed prompts, allowing efficient exploration of the perturbation space under budget constraints.

## Key Results
- CELL achieves 58-60% favorability for contrastive explanations compared to 36-44% for baselines
- Flip rates of 56-95% with edit distances of only 0.12-0.32 words
- Efficient scaling for longer documents where the number of model calls plateaus
- Successfully applied to explain conversational agents along multiple dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CELL generates contrastive explanations by perturbing substrings of the input prompt and using a user-defined scoring function to evaluate the quality of the modified prompt-response pair.
- Mechanism: The algorithm iteratively masks and replaces substrings, queries the LLM to generate responses, and scores these responses against the original using the scoring function. The process continues until a sufficiently different (contrasting) response is found.
- Core assumption: A user-defined scoring function can meaningfully quantify the "difference" between the original and contrastive responses in a way that aligns with the user's goals.
- Evidence anchors:
  - [abstract]: "Our explanations suggest that an LLM outputs a reply to a given prompt because if the prompt was slightly modified, the LLM would have given a different response that is either less preferable or contradicts the original response."
  - [section]: "Denote by x0 an input prompt... Define g(x0, y0, yc) as a scoring function... We formulate the contrastive explanation problem... minimize f (x0, x) subject to g (x0, LLM(x0), LLM(x)) ≥ δ"
- Break condition: The scoring function may not capture the desired notion of "contrast" if it is poorly defined or misaligned with user expectations, leading to irrelevant or misleading explanations.

### Mechanism 2
- Claim: CELL's budgeted search strategy efficiently finds contrastive examples by adaptively sampling from a large space of possible perturbations while respecting a query budget.
- Mechanism: CELL maintains a set of "seed" prompts and iteratively samples perturbations around these seeds, prioritizing areas likely to yield high-scoring contrastive examples. The number of seeds and samples per seed are adjusted based on the remaining budget and search progress.
- Core assumption: The search space of prompt perturbations can be effectively explored using a budgeted, adaptive sampling strategy without exhaustive enumeration.
- Evidence anchors:
  - [section]: "the second algorithm, CELL, is our main algorithmic contribution and involves an adaptive search constrained by a budget on the number of calls to the LLM being explained."
  - [section]: "CELL employs a seed-driven approach where some seeds are generated from the initial prompt and others from previously perturbed prompts."
- Break condition: If the budget is too small relative to the search space, CELL may fail to find any contrastive examples, even if they exist.

### Mechanism 3
- Claim: The choice of scoring function (preference, contradiction, BLEU SUMM) directly influences the type and quality of contrastive explanations generated.
- Mechanism: Each scoring function encodes a different notion of "contrast": preference measures relative helpfulness, contradiction measures logical inconsistency, and BLEU SUMM measures summary similarity. CELL optimizes for the specific type of contrast defined by the chosen function.
- Core assumption: Different user needs can be addressed by swapping in different scoring functions without changing the underlying search algorithm.
- Evidence anchors:
  - [section]: "We formalize scoring functions used in Section 5... Preference: This scoring function outputs a score defining which of two responses is preferable..."
  - [section]: "Contradiction: This scoring function inputs two responses y1 and y2... We define the contradiction score as the difference p2 − p1."
- Break condition: The scoring function may be computationally expensive or noisy, leading to unreliable optimization and poor-quality explanations.

## Foundational Learning

- Concept: Scoring functions for contrastive explanations
  - Why needed here: CELL requires a user-defined scoring function to evaluate the quality of contrastive examples. Understanding how to design such functions is critical for effective use of the method.
  - Quick check question: If you want to explain why an LLM gave a particular summary, which scoring function would you use and why?

- Concept: Adaptive search under budget constraints
  - Why needed here: CELL's efficiency relies on its ability to explore the space of prompt perturbations without exceeding a query budget. Understanding adaptive search strategies is key to tuning and extending the algorithm.
  - Quick check question: How does CELL decide how many seeds to generate and how many perturbations to sample around each seed in each iteration?

- Concept: Text infilling for prompt modification
  - Why needed here: CELL modifies prompts by masking and replacing substrings using an infiller model. Understanding how infilling works is important for controlling the nature of the perturbations.
  - Quick check question: What are the trade-offs between using a BERT-based infiller (single word replacement) versus a T5-based infiller (multi-word replacement)?

## Architecture Onboarding

- Component map: Input prompt -> Split into substrings -> Mask & infill (using infiller model) -> Query LLM -> Score responses (using user-defined function) -> Select best perturbations -> Repeat until contrastive example found or budget exhausted -> Output contrastive prompt and response

- Critical path:
  1. Split input prompt into substrings
  2. Generate initial perturbations (masking & infilling)
  3. Query LLM to get responses for perturbations
  4. Score responses using user-defined function
  5. Select best perturbations and repeat until contrastive example found or budget exhausted

- Design tradeoffs:
  - Split k parameter: Higher k reduces search space but may miss subtle contrasts; lower k increases search space and computational cost.
  - Budget size: Larger budget increases chance of finding good contrasts but also increases cost; smaller budget may fail to find any contrasts.
  - Scoring function choice: Different functions yield different types of explanations; some may be noisier or more expensive to compute.

- Failure signatures:
  - No contrastive example found: Budget too small or scoring function not aligned with user goals.
  - Explanations too similar to original: Split k too high or scoring function not sensitive enough.
  - Explanations too dissimilar: Split k too low or scoring function too strict.
  - High computational cost: Budget too large or infiller too slow.

- First 3 experiments:
  1. Run CELL with split k=1 and a simple scoring function (e.g., contradiction) on a short prompt from the MIC dataset. Verify that it finds a contrastive example and measure the edit distance.
  2. Compare CELL vs m-CELL on a medium-length prompt, measuring both flip rate and number of model calls. Confirm that CELL scales better for longer prompts.
  3. Test CELL with the BLEU SUMM scoring function on a document from the XSum dataset. Evaluate whether it successfully finds contrasts between the original and summary responses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would CELL perform on even longer documents (e.g., book-length texts) and what would be the scaling behavior in terms of model calls and edit distance?
- Basis in paper: [inferred] The paper mentions CELL's efficiency on longer documents from the XSum dataset and shows the number of model calls plateaus, but doesn't explore scaling to much longer documents.
- Why unresolved: The experiments only go up to XSum document lengths. Book-length documents would require understanding if the budgeted approach maintains efficiency or if new algorithmic modifications would be needed.
- What evidence would resolve it: Running CELL on datasets with progressively longer documents (news articles → research papers → books) while measuring model calls, edit distance, and computation time to establish scaling laws.

### Open Question 2
- Question: How sensitive is CELL's performance to the choice of infiller model, and would fine-tuning the infiller specifically for contrastive explanation tasks improve results?
- Basis in paper: [explicit] The paper states "Various options exist for the infiller model... While finetuning an infiller could further help performance... this is expensive and out of the scope of the current work."
- Why unresolved: The experiments use T5-large without fine-tuning, leaving open the question of whether specialized infillers could substantially improve contrastive explanation quality or reduce the number of model calls needed.
- What evidence would resolve it: Comparing CELL's performance across multiple infillers (BERT, BART, T5) both with and without fine-tuning on contrastive explanation tasks, measuring preference scores, flip rates, and edit distances.

### Open Question 3
- Question: Can CELL be extended to generate multi-step contrastive explanations that modify multiple aspects of the input (e.g., both helpfulness and relevance simultaneously)?
- Basis in paper: [inferred] The conversation application section shows explaining along single dimensions, but the algorithmic framework could potentially be extended to handle multiple constraints or objectives.
- Why unresolved: The current CELL formulation optimizes for a single scoring function, but real-world explanations might need to satisfy multiple criteria (e.g., maintain relevance while reducing helpfulness).
- What evidence would resolve it: Modifying CELL to handle multi-objective optimization or sequential explanations, then evaluating whether it can generate contrastive examples that satisfy multiple rubric dimensions simultaneously on the conversation dataset.

## Limitations

- Algorithm Specification Gaps: The CELL and m-CELL algorithms rely on several key functions (NUM_SEEDS, GENERATE_SEEDS, SAMPLE_SEEDS) that are only described conceptually but not fully specified, creating uncertainty about faithful implementation.
- Evaluation Methodology Concerns: Using LLM-as-a-Judge (Prometheus2) for evaluating explanation quality introduces potential circularity and bias, with reported human-LLM agreement of only 56% on preference judgments.
- Generalization Boundaries: While experiments cover two datasets, the evaluation focuses primarily on binary yes/no and summarization tasks, limiting claims about explaining "any LLM output" across arbitrary tasks and domains.

## Confidence

**High Confidence (8/10)**: The core algorithmic contribution of using budgeted adaptive search to find contrastive examples efficiently is well-supported. The experimental results showing CELL outperforming baselines in favorability (58-60% vs 36-44%) and achieving reasonable flip rates (56-95%) while maintaining low edit distances (0.12-0.32 words) are internally consistent.

**Medium Confidence (6/10)**: The claim that CELL "successfully explains conversational agents by identifying dimensions where responses can be improved" is supported by qualitative examples but lacks quantitative validation on conversational datasets.

**Low Confidence (4/10)**: The assertion that CELL can explain "any LLM output" across arbitrary tasks and domains is aspirational rather than demonstrated. The evaluation is limited to two specific datasets and task types.

## Next Checks

1. **Algorithm Fidelity Test**: Implement CELL and m-CELL exactly as specified, then run ablation studies varying the NUM_SEEDS, GENERATE_SEEDS, and SAMPLE_SEEDS parameters. Measure how these changes affect flip rates and favorability scores to understand which components are critical to performance.

2. **Human Evaluation Benchmark**: Conduct a small-scale human study where participants rate CELL-generated explanations versus baseline explanations on the same prompts. Compare human preferences directly against the LLM-as-a-Judge scores to assess alignment and identify systematic biases in automated evaluation.

3. **Cross-Domain Generalization Test**: Apply CELL to three additional task types not covered in the original evaluation: code generation (from CodeXGLUE), multi-turn dialogue (from MultiWOZ), and creative writing (from StoryCommonsense). Measure whether CELL maintains reasonable flip rates and favorability scores across these diverse domains, or if performance degrades significantly.