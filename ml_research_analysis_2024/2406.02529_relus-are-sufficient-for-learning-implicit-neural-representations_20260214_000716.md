---
ver: rpa2
title: ReLUs Are Sufficient for Learning Implicit Neural Representations
arxiv_id: '2406.02529'
source_url: https://arxiv.org/abs/2406.02529
tags:
- neural
- relu
- function
- networks
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of spectral bias in ReLU neural
  networks when learning implicit neural representations (INRs) for imaging tasks.
  The authors propose a novel approach to overcome this issue by constraining groups
  of ReLU neurons within each hidden layer of a deep neural network (DNN) to emulate
  second-order B-spline wavelets.
---

# ReLUs Are Sufficient for Learning Implicit Neural Representations

## Quick Facts
- arXiv ID: 2406.02529
- Source URL: https://arxiv.org/abs/2406.02529
- Authors: Joseph Shenouda; Yamin Zhou; Robert D. Nowak
- Reference count: 27
- Primary result: BW-ReLU networks achieve state-of-the-art performance on INR tasks by constraining ReLU neurons to emulate B-spline wavelets

## Executive Summary
This paper addresses the spectral bias problem in ReLU neural networks when learning implicit neural representations (INRs) for imaging tasks. The authors propose BW-ReLU, a method that constrains groups of ReLU neurons to emulate second-order B-spline wavelets, resulting in well-conditioned feature matrices throughout training. This approach leads to faster convergence and improved performance across various INR tasks including signal representation, super-resolution, and computed tomography reconstruction. The method achieves state-of-the-art results while maintaining the simplicity of ReLU activations, and the authors provide theoretical analysis using the variation norm to quantify function regularity.

## Method Summary
The BW-ReLU method constrains groups of 7 ReLU neurons within each hidden layer to emulate second-order B-spline wavelets, creating a localized and semi-orthogonal feature representation. Each group shares input and output weights, with fixed orientations for the ReLUs within each group. The approach maintains a well-conditioned Gram matrix throughout training, overcoming the ill-conditioning that typically plagues standard ReLU networks. The method is evaluated using a three-hidden-layer DNN trained with Adam optimizer, with hyperparameters tuned for optimal performance across various INR tasks.

## Key Results
- BW-ReLU networks achieve state-of-the-art performance on INR tasks including signal representation, super-resolution, and CT reconstruction
- The method performs comparably to or better than INR architectures using unconventional activation functions
- Variation norm analysis provides a principled way to tune scaling parameters and quantify learned function regularity
- Gram matrix remains well-conditioned throughout training, leading to faster convergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Spectral bias in ReLU networks stems from ill-conditioning of the feature embedding matrix ΦΦ^T
- **Mechanism:** When ReLU neurons are added, their biases can become nearly colinear, causing the Gram matrix to have a very high condition number, making gradient descent inefficient
- **Core assumption:** Network trains via gradient descent on fixed features, and ill-conditioning slows convergence
- **Evidence anchors:** Theorem 3.1 shows condition number grows as Ω(K³) for ReLU neurons; abstract states ReLU networks have "inherent bias" causing high-frequency approximation struggles

### Mechanism 2
- **Claim:** BW-ReLU neurons form a localized, semi-orthogonal set keeping Gram matrix well-conditioned
- **Mechanism:** Each group of 7 ReLU neurons constrained to emulate B-spline wavelet is compact with limited overlap, preventing colinearity and ensuring bounded condition number (O(1) per Theorem 3.3)
- **Core assumption:** B-spline wavelets are semi-orthogonal; wavelets of different scales are orthogonal to each other
- **Evidence anchors:** Theorem 3.3 proves κ(G_ψ) = O(1) for BW-ReLU networks; abstract states constraints "remedy the spectral bias"

### Mechanism 3
- **Claim:** Variation norm provides principled measure of learned function regularity, guiding hyperparameter selection
- **Mechanism:** For ReLU networks, weight decay regularizes variation norm (L1 norm of second derivative); BW-ReLU neurons contribute 16||v||₂||w||₂, so scaling c affects regularity predictably
- **Core assumption:** Variation norm equals sum over neurons of ||v||₂||w||₂ and correlates with generalization
- **Evidence anchors:** Equation (30) shows ∥gθ∥_V = 16c Σ||w_k||₂||v_k||₂ linking scaling to regularity; abstract mentions quantifying regularity "suggests a principled way to tune INRs"

## Foundational Learning

- **Concept:** Gram matrix conditioning
  - **Why needed here:** Explains why ReLU networks converge slowly; directly tied to spectral bias
  - **Quick check question:** What happens to eigenvalues of G_σ when ReLU biases are nearly equal?

- **Concept:** Semi-orthogonality of B-spline wavelets
  - **Why needed here:** Underpins well-conditioning of BW-ReLU networks
  - **Quick check question:** Why are wavelets of different scales orthogonal in L²?

- **Concept:** Variation norm and its connection to smoothness
  - **Why needed here:** Provides principled metric for selecting scaling parameter c
  - **Quick check question:** How does increasing c affect variation norm and thus smoothness of learned function?

## Architecture Onboarding

- **Component map:** Input layer → Shared weights per group → Constrained ReLU group (7 neurons) → BW-ReLU activation → Output layer
- **Critical path:** 1) Initialize shared weights for each group, 2) Enforce fixed orientation of ReLUs within each group, 3) Train with gradient descent; monitor condition number of Gram matrix
- **Design tradeoffs:** Fixed group structure limits representational flexibility but guarantees conditioning; scaling parameter c trades off frequency capture (high c) vs. smoothness (low c)
- **Failure signatures:** Training loss plateaus early (likely Gram matrix ill-conditioning); high variance across runs (possible improper initialization or scaling); low PSNR in inverse problems (variation norm too high; reduce c)
- **First 3 experiments:** 1) Fit simple 1D function with BW-ReLU vs. ReLU; compare convergence speed, 2) Vary c in BW-ReLU; plot variation norm vs. PSNR on CT reconstruction, 3) Replace BW-ReLU with Gabor wavelet; compare Gram matrix condition numbers

## Open Questions the Paper Calls Out
- How do BW-ReLU networks perform on tasks beyond imaging, such as neural radiance fields or physics-informed neural networks?
- How does the scaling parameter c affect variation norm and generalization performance in high-dimensional settings?
- Can the concept of BW-ReLU be extended to other types of wavelets beyond second-order B-spline wavelets?
- How does choice of wavelet scale and shift affect performance of BW-ReLU networks in different INR tasks?

## Limitations
- Performance claims rely on specific imaging tasks; generalization to other domains remains unproven
- Theoretical analysis assumes particular initialization and training dynamics that may not hold in all scenarios
- Variation norm correlation with generalization requires further validation across diverse tasks

## Confidence
- High: Theoretical results on Gram matrix conditioning for BW-ReLU networks
- Medium: Experimental performance claims and spectral bias mitigation
- Medium: Variation norm as principled hyperparameter selection tool

## Next Checks
1. **Gram matrix analysis across training**: Track condition number of feature matrix throughout training for both standard ReLU and BW-ReLU networks on multiple INR tasks to verify proposed conditioning improvement persists during optimization

2. **Ablation of wavelet constraints**: Systematically remove semi-orthogonality constraints while maintaining 7-neuron group structure to isolate effect of localization versus orthogonality on conditioning and performance

3. **Variation norm generalization**: Test correlation between variation norm and generalization error across multiple INR tasks with varying levels of complexity and noise to validate proposed hyperparameter selection heuristic