---
ver: rpa2
title: 'FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food
  Culture'
arxiv_id: '2406.11030'
source_url: https://arxiv.org/abs/2406.11030
tags:
- questions
- question
- food
- dish
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FoodieQA, a fine-grained multimodal dataset
  designed to evaluate models'' understanding of regional Chinese food culture through
  image and text-based multiple-choice questions. The dataset consists of 389 images
  from 14 Chinese cuisine types and corresponding questions across three tasks: multi-image
  visual question answering, single-image visual question answering, and text-only
  question answering.'
---

# FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture

## Quick Facts
- **arXiv ID**: 2406.11030
- **Source URL**: https://arxiv.org/abs/2406.11030
- **Reference count**: 15
- **Primary result**: State-of-the-art VLMs underperform humans by 21-41% on Chinese food culture understanding tasks

## Executive Summary
FoodieQA is a fine-grained multimodal dataset designed to evaluate models' understanding of regional Chinese food culture through image and text-based multiple-choice questions. The dataset consists of 389 images from 14 Chinese cuisine types and corresponding questions across three tasks: multi-image visual question answering, single-image visual question answering, and text-only question answering. Human annotators created challenging questions requiring cultural knowledge and visual understanding. When evaluated on this dataset, state-of-the-art vision-language models show significant performance gaps compared to humans, with open-weight models underperforming by 41% on multi-image tasks and 21% on single-image tasks.

## Method Summary
The dataset was created through human annotation of food images from 14 Chinese cuisine types, with questions designed to test fine-grained cultural understanding. The three task types were evaluated using zero-shot prompting across multiple open-weight and API-based VLMs and LLMs. Human accuracy benchmarks were established through multiple annotator evaluations. The dataset includes comprehensive metadata about dishes including flavor profiles, ingredients, cooking methods, and regional information.

## Key Results
- Open-weight VLMs underperform humans by 41% on multi-image and 21% on single-image VQA tasks
- LLMs outperform humans on text-only questions (56.2% human vs higher model accuracy)
- Performance varies significantly based on question language, with models performing better on Chinese-language questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FoodieQA challenges VLMs by requiring fine-grained visual understanding of culturally-specific food features
- Mechanism: The dataset contains questions that probe subtle visual attributes like dishware, ingredient arrangement, and presentation styles that are culturally specific
- Core assumption: Current VLMs lack sufficient exposure to culturally-diverse food imagery during pretraining
- Evidence anchors:
  - [abstract] "VLMs still fall short by 41% on multi-image and 21% on single-image VQA tasks"
  - [section] "Current fine-grained VL datasets... are often framed as binary classification tasks, which limits their difficulty"
  - [corpus] FMR scores show related work exists but no direct equivalent to FoodieQA's regional Chinese food focus
- Break condition: If models are explicitly trained on diverse regional Chinese food imagery with fine-grained cultural annotations

### Mechanism 2
- Claim: Multi-image VQA tasks are significantly harder than single-image tasks because they require comparative reasoning
- Mechanism: Models must identify subtle differences across multiple images while considering cultural context
- Core assumption: Models can handle single-image classification but struggle with cross-image comparison and cultural reasoning
- Evidence anchors:
  - [abstract] "the open-weights VLMs still fall short by 41% on multi-image and 21% on single-image VQA tasks"
  - [section] "Multi-image VQA requires the ability to compare detailed visual features from multiple images, similar to how humans browse a restaurant menu"
  - [corpus] Related benchmarks focus on single-image tasks or binary classification
- Break condition: If models develop explicit mechanisms for cross-image comparison or cultural knowledge grounding

### Mechanism 3
- Claim: Text-only LLMs outperform VLMs on food culture questions because they leverage textual cultural knowledge
- Mechanism: LLMs have better access to written cultural information about food while VLMs must extract this from visual features
- Core assumption: Cultural food knowledge is more readily available in text form than visual form
- Evidence anchors:
  - [abstract] "While LLMs excel at text-based question answering, surpassing human accuracy"
  - [section] "Our analysis of language and prompt templates indicates that models can be sensitive to the language in which questions are asked"
  - [corpus] Multiple related works show LLMs excel at cultural understanding tasks when given textual context
- Break condition: If VLMs develop better mechanisms for extracting cultural knowledge from visual features

## Foundational Learning

- Concept: Cultural specificity in visual features
  - Why needed here: Understanding that food presentation, dishware, and ingredient arrangements vary significantly by region and culture
  - Quick check question: Can you identify three visual features that distinguish northern from southern Chinese hotpot presentations?

- Concept: Multi-hop visual reasoning
  - Why needed here: Many questions require identifying a dish first, then applying cultural knowledge to answer
  - Quick check question: Given an image of mapo tofu, what cultural knowledge would you need to determine if it's authentic Sichuan cuisine?

- Concept: Visual vs textual cultural knowledge encoding
  - Why needed here: Understanding how cultural information is represented differently in visual vs textual domains
  - Quick check question: How might the phrase "sweet and sour flavor" be represented visually vs textually in a Chinese dish?

## Architecture Onboarding

- Component map:
  Image preprocessing pipeline -> VLM backbone -> Cross-attention layers -> Answer generation
  Separate text-only LLM path for text QA
  Prompt engineering module for different task formats

- Critical path:
  Image encoding → cultural feature extraction → cross-modal reasoning → answer selection
  (Most failure points occur in the cultural feature extraction stage)

- Design tradeoffs:
  - Image resolution vs computational cost: Higher resolution needed for fine-grained features
  - Prompt specificity vs generalization: Very specific prompts work better but don't transfer
  - Bilingual training vs language-specific optimization: Bilingual models perform better on Chinese tasks

- Failure signatures:
  - High accuracy on obvious visual features but poor on subtle cultural ones
  - Strong performance on ingredients but weak on regional associations
  - Better English performance for multilingual models on Chinese-specific tasks

- First 3 experiments:
  1. Test single-image VQA with dish names revealed vs hidden to isolate recognition vs reasoning
  2. Evaluate multi-image VQA with and without cultural context in prompts
  3. Compare performance on culturally-specific vs generic food images within the same cuisine type

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would performance change if FoodieQA expanded to include dishes from multiple countries beyond China, and would cross-cultural food understanding models perform better than single-culture models?
- Basis in paper: Explicit - The paper states "Looking ahead, we aim to expand the dataset to include dishes from other countries and regions" in the conclusion.
- Why unresolved: The current dataset only covers Chinese regional cuisines, limiting the ability to test cross-cultural food understanding. The authors hypothesize that this expansion could advance the direction but haven't tested it.
- What evidence would resolve it: Performance comparisons of the same VLMs/LLMs on an expanded multi-country FoodieQA dataset versus the current Chinese-only version, measuring changes in accuracy across different cultural contexts.

### Open Question 2
- Question: Would providing additional cultural context metadata (beyond what's already collected) to models significantly improve their performance on FoodieQA tasks, and what type of metadata would be most beneficial?
- Basis in paper: Inferred - The paper discusses how "understanding food and its cultural implications remains a complex and under-explored task" and mentions collecting meta-information about dishes, but doesn't test whether additional cultural context helps.
- Why unresolved: The current meta-information collection is limited to basic attributes, and the paper doesn't explore whether enriching this with more detailed cultural context (e.g., historical significance, eating occasions, cultural symbolism) would help models perform better.
- What evidence would resolve it: Controlled experiments adding different types of cultural metadata to the same questions and measuring performance improvements, identifying which metadata types yield the greatest accuracy gains.

### Open Question 3
- Question: How would training VLMs specifically on culturally diverse food datasets (rather than general vision-language data) impact their performance on fine-grained cultural food understanding tasks like those in FoodieQA?
- Basis in paper: Explicit - The paper notes that "understanding food and its cultural context remains a complex and under-explored task" and shows current models struggle significantly with these tasks.
- Why unresolved: All evaluated models were pretrained on general data rather than food-specific culturally diverse data, so the impact of specialized training is unknown.
- What evidence would resolve it: Training experiments where VLMs are pretrained or fine-tuned on large culturally diverse food datasets, then evaluated on FoodieQA to measure performance improvements compared to general-purpose VLMs.

## Limitations

- The dataset is limited to Chinese regional cuisines, which restricts generalizability to other cultural food understanding tasks
- Zero-shot evaluation methodology may not fully capture model capabilities, as prompt engineering could significantly impact results
- The performance gap between models and humans may be influenced by dataset size limitations and specific prompt template choices

## Confidence

- **High**: Dataset creation methodology and human benchmark establishment are directly measured and reproducible
- **Medium**: VLM performance comparisons due to prompt sensitivity and model variation factors
- **Medium**: Claims about specific failure modes, as they require further systematic investigation

## Next Checks

1. **Cross-dataset validation**: Test the same models on multiple cultural food understanding datasets (e.g., CVLUE for Chinese food, and international equivalents) to isolate whether performance gaps are specific to regional Chinese cuisine or indicative of broader VLM limitations.

2. **Prompt sensitivity analysis**: Systematically vary prompt templates and formats across all three task types to establish the robustness of performance differences between models, particularly focusing on the large gap between multi-image and single-image performance.

3. **Cultural knowledge ablation**: Create controlled subsets of questions that vary in their reliance on cultural vs visual knowledge (e.g., questions about universally recognizable food features vs culturally specific preparation methods) to better understand where VLMs specifically fail.