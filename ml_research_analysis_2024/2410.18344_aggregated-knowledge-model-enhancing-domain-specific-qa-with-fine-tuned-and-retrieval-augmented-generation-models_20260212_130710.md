---
ver: rpa2
title: 'Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and
  Retrieval-Augmented Generation Models'
arxiv_id: '2410.18344'
source_url: https://arxiv.org/abs/2410.18344
tags:
- knowledge
- question
- answers
- answer
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses enhancing closed-domain Question Answering
  (QA) systems for the Lawrence Berkeley National Laboratory (LBL) ScienceIT domain.
  The authors fine-tune two large language models and develop five retrieval-augmented
  generation (RAG) models using AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4,
  and Google Gemini-Pro.
---

# Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and Retrieval-Augmented Generation Models

## Quick Facts
- arXiv ID: 2410.18344
- Source URL: https://arxiv.org/abs/2410.18344
- Reference count: 28
- Fine-tuned and RAG models achieve 8%+ performance improvement with AKM approach

## Executive Summary
This paper presents the Aggregated Knowledge Model (AKM), a novel approach for enhancing closed-domain Question Answering systems in the Lawrence Berkeley National Laboratory (LBL) ScienceIT domain. The authors fine-tune two large language models and develop five retrieval-augmented generation (RAG) models using various cloud platforms. By synthesizing responses from these seven models using K-means clustering, AKM demonstrates significant performance improvements, achieving over 8% better overall performance compared to the best individual model (OpenAI GPT-4 with RAG). The study provides valuable insights into developing specialized QA systems tailored to specific domains, with AKM consistently achieving higher scores across multiple evaluation metrics.

## Method Summary
The research focuses on enhancing closed-domain QA systems for the LBL ScienceIT domain by comparing fine-tuned and retrieval-augmented generation (RAG) models. The authors fine-tune two large language models and develop five RAG models using AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, and Google Gemini-Pro. The Aggregated Knowledge Model (AKM) synthesizes responses from these seven models using K-means clustering to select the most representative answers. This approach combines the strengths of multiple models while addressing individual weaknesses, resulting in improved performance across various metrics including BLEU, ROUGE, and Semantic Textual Similarity.

## Key Results
- AKM achieves over 8% performance improvement compared to the best individual model (OpenAI GPT-4 with RAG)
- Consistent performance gains across multiple evaluation metrics (BLEU, ROUGE, Semantic Textual Similarity)
- K-means clustering effectively synthesizes responses from multiple models to select the most representative answers

## Why This Works (Mechanism)
The Aggregated Knowledge Model works by leveraging the complementary strengths of multiple specialized models through a K-means clustering approach. Each model brings unique capabilities and knowledge representations to the task, with fine-tuned models capturing domain-specific nuances and RAG models providing access to up-to-date information. The K-means clustering algorithm identifies clusters of similar responses and selects the most representative answer, effectively filtering out noise and inconsistencies from individual models. This aggregation approach reduces the impact of individual model weaknesses while amplifying their collective strengths, resulting in more accurate and comprehensive answers for domain-specific questions.

## Foundational Learning
- **Fine-tuning LLMs for domain adaptation**: Required to adapt general-purpose models to specialized domains like ScienceIT; quick check: compare domain-specific performance against base models
- **Retrieval-augmented generation**: Needed to incorporate up-to-date information and domain-specific knowledge bases; quick check: measure retrieval accuracy and relevance scores
- **K-means clustering for response aggregation**: Essential for synthesizing multiple model outputs into coherent answers; quick check: analyze cluster quality metrics and silhouette scores
- **Multi-metric evaluation**: Critical for comprehensive assessment of QA system performance; quick check: compare BLEU, ROUGE, and STS scores across models
- **Domain-specific dataset curation**: Fundamental for training and evaluating specialized QA systems; quick check: validate dataset coverage and question-answer quality
- **Cloud platform integration**: Necessary for accessing and managing various LLM APIs and RAG implementations; quick check: measure API response times and reliability

## Architecture Onboarding

**Component Map:**
Data Preparation -> Model Training (Fine-tuning) -> RAG Pipeline Setup -> Individual Model Evaluation -> K-means Clustering -> AKM Response Selection

**Critical Path:**
Data preparation and curation → Model fine-tuning and RAG setup → Individual model evaluation → K-means clustering implementation → AKM response synthesis → Multi-metric evaluation

**Design Tradeoffs:**
- Computational cost vs. performance gain (7 models vs. single best model)
- Model diversity vs. consistency in responses
- Fine-tuning depth vs. generalization capability
- RAG retrieval quality vs. generation quality
- Manual evaluation subjectivity vs. automated metrics

**Failure Signatures:**
- Poor K-means clustering quality (low silhouette scores)
- Inconsistent performance across evaluation metrics
- High variance in individual model outputs
- Computational resource bottlenecks
- Manual evaluation bias or low inter-annotator agreement

**3 First Experiments:**
1. Compare individual model performance on a held-out validation set
2. Test K-means clustering with different cluster numbers (k=3,5,7)
3. Evaluate response consistency across multiple runs of the same question

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important questions emerge from the research:

### Open Question 1
How would the Aggregated Knowledge Model (AKM) perform when applied to domains outside of the ScienceIT field?
The paper focuses specifically on the LBL ScienceIT domain, but the AKM is presented as a generalizable approach for enhancing domain-specific QA systems. The study only evaluates AKM within the ScienceIT domain, leaving its performance in other domains untested. Testing AKM across multiple distinct domains (e.g., medical, legal, financial) with domain-specific datasets and comparing its performance against baseline models in each domain would resolve this question.

### Open Question 2
What is the optimal number of models to aggregate in the AKM for maximizing performance while minimizing computational overhead?
The paper uses 7 models in the AKM, but doesn't explore whether this is the optimal number or how performance scales with different numbers of models. The study doesn't systematically vary the number of models in the AKM to determine the point of diminishing returns or optimal configuration. Experiments testing AKM performance with different numbers of models (e.g., 3, 5, 7, 9, 11) while measuring both performance gains and computational costs would resolve this question.

### Open Question 3
How does the AKM handle contradictory or highly divergent responses from the individual models?
The paper mentions that K-means clustering is used to select the most representative answer, but doesn't detail how the model handles cases where individual models provide significantly different answers. The clustering approach is described, but the paper doesn't provide analysis of how AKM performs when models disagree substantially or provide contradictory information. Case studies analyzing specific questions where individual models provided divergent answers, examining how AKM selected the final answer and the quality of that selection would resolve this question.

## Limitations
- Evaluation confined to single domain (LBL ScienceIT), limiting generalizability
- Manual evaluation involves only two annotators, potentially introducing subjectivity
- K-means clustering approach lacks detailed explanation of parameter choices
- Computational resource requirements not discussed for practical deployment
- Comparison focuses on accuracy metrics without addressing efficiency or cost

## Confidence
- **High confidence**: AKM's superior performance over individual models is well-supported by experimental results
- **Medium confidence**: Specific performance gains (8% improvement) are accurate for ScienceIT domain but may not generalize
- **Medium confidence**: Assertion that fine-tuning is more effective than RAG for domain-specific QA needs additional testing

## Next Checks
1. **Cross-domain validation**: Test AKM approach on at least two different specialized domains (e.g., healthcare, legal) to assess generalizability
2. **Scalability analysis**: Measure computational costs, inference times, and resource requirements for each component to evaluate practical deployment feasibility
3. **Multi-annotator evaluation**: Expand manual evaluation to at least 5 annotators with established inter-annotator agreement metrics to strengthen reliability