---
ver: rpa2
title: 'Get rich quick: exact solutions reveal how unbalanced initializations promote
  rapid feature learning'
arxiv_id: '2406.06158'
source_url: https://arxiv.org/abs/2406.06158
tags:
- dynamics
- learning
- networks
- linear
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper derives exact solutions for a minimal model that transitions
  between lazy and rich learning regimes in neural networks. The key insight is that
  unbalanced layer-specific initialization variances and learning rates conspire to
  influence learning regimes through conserved quantities that constrain parameter
  and function space trajectories.
---

# Get rich quick: exact solutions reveal how unbalanced initializations promote rapid feature learning

## Quick Facts
- arXiv ID: 2406.06158
- Source URL: https://arxiv.org/abs/2406.06158
- Authors: Daniel Kunin; Allan Raventós; Clémentine Dominé; Feng Chen; David Klindt; Andrew Saxe; Surya Ganguli
- Reference count: 40
- Primary result: Unbalanced layer-specific initialization variances and learning rates drive rapid feature learning through conserved quantities that constrain learning trajectories.

## Executive Summary
This paper provides exact solutions for how unbalanced initializations affect learning regimes in neural networks. The key insight is that initialization variances and learning rates conspire to create conserved quantities that constrain parameter and function space trajectories, fundamentally altering how networks learn features. The analysis reveals that while balanced initializations promote rapid feature learning in linear networks, upstream initializations (faster learning in earlier layers) accelerate rich learning in nonlinear networks.

## Method Summary
The paper derives exact solutions for minimal neural network models, analyzing how conserved quantities constrain learning trajectories in parameter and function space. It studies linear and nonlinear networks, deriving relationships between initialization scales, learning rates, and learning regimes. The authors validate their theoretical predictions through experiments on teacher-student setups, CNNs, hierarchical data, and transformers, showing that upstream initializations drive feature learning in finite-width networks and reduce sample complexity and grokking time.

## Key Results
- In linear networks, rapid feature learning only occurs from balanced initializations
- In nonlinear networks, upstream initializations accelerate rich learning by promoting faster learning in earlier layers
- Unbalanced initializations drive feature learning in deep finite-width networks and improve interpretability in CNNs
- Upstream initializations reduce sample complexity in hierarchical data and decrease time to grokking in modular arithmetic tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unbalanced layer-specific initialization variances and learning rates drive rapid feature learning by creating conserved quantities that constrain learning trajectories.
- Mechanism: The conserved quantity δ = ηwa² - ηa||w||² regulates how much the first layer (weights) versus second layer (activations) updates during training. Upstream initializations (δ > 0) cause early layers to learn faster, downstream initializations (δ < 0) cause later layers to learn faster, and balanced initializations (δ = 0) lead to synchronized learning.
- Core assumption: The network maintains this conserved quantity throughout gradient flow, and the learning regime depends on its sign and magnitude.
- Evidence anchors:
  - [abstract] "Our analysis reveals that they conspire to influence the learning regime through a set of conserved quantities that constrain and modify the geometry of learning trajectories in parameter and function space."
  - [section] "This confines the parameter dynamics to the surface of a hyperboloid where the magnitude and sign of the conserved quantity determines the geometry."
  - [corpus] Weak evidence for exact mathematical formulation of conserved quantities in finite-width networks; most prior work focuses on infinite-width analysis.
- Break condition: If the conserved quantity changes during training (e.g., due to discretization effects from SGD), the trajectory constraint would no longer hold.

### Mechanism 2
- Claim: In linear networks, balanced initializations lead to rapid rich learning, while in nonlinear networks, upstream initializations accelerate feature learning.
- Mechanism: For linear networks, balanced initializations allow both layers to update proportionally, creating rich feature learning dynamics. For nonlinear networks, upstream initializations enable rapid changes in activation patterns (partition alignment phase) while maintaining minimal parameter movement.
- Core assumption: The relationship between layer learning speeds and feature learning depends on whether the network is linear or nonlinear.
- Evidence anchors:
  - [abstract] "In linear networks, rapid feature learning only occurs from balanced initializations, while in nonlinear networks, upstream initializations that promote faster learning in earlier layers can accelerate rich learning."
  - [section] "In linear networks, rapid rich learning can only occur from balanced initializations, while in nonlinear networks, upstream initializations can actually accelerate rich learning."
  - [corpus] Limited evidence comparing balanced vs upstream initializations in nonlinear networks; most studies focus on infinite-width limits.
- Break condition: If the network architecture deviates significantly from the studied models (e.g., deeper networks, different activation functions), the relationship between initialization balance and learning regime may change.

### Mechanism 3
- Claim: The directionality of the relative scale (upstream vs downstream) critically influences the learning trajectory and inductive bias.
- Mechanism: Upstream initializations cause rapid alignment of early layers to data structure while downstream initializations create delayed rich regimes where kernel evolution happens late in training.
- Core assumption: The sign of δ determines not just learning speed but the order of learning phases (alignment vs fitting).
- Evidence anchors:
  - [abstract] "While in nonlinear networks, unbalanced initializations that promote faster learning in earlier layers can accelerate rich learning."
  - [section] "Due to the depth-dependent expressivity of layers in a network, upstream and downstream initializations often exhibit fundamentally distinct learning trajectories."
  - [corpus] Strong evidence for this mechanism from experiments showing upstream initializations reduce grokking time and improve sample complexity.
- Break condition: If the dataset structure doesn't have hierarchical features or if normalization layers mask the initialization effects, the directionality benefits may disappear.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK) evolution distinguishes lazy from rich learning regimes.
  - Why needed here: The paper uses NTK distance as a primary metric to track the transition between lazy and rich regimes.
  - Quick check question: What happens to the NTK matrix during lazy learning vs rich learning?

- Concept: Mirror flow analysis for determining inductive bias at interpolation.
  - Why needed here: The paper extends time-warped mirror flow analysis to understand how initialization affects the interpolating solution.
  - Quick check question: How does the potential function change when δ approaches zero vs infinity?

- Concept: Conservation laws in gradient flow dynamics.
  - Why needed here: The paper relies on conserved quantities (like δ) to derive exact solutions and constrain learning trajectories.
  - Quick check question: What mathematical property ensures a quantity remains constant during gradient flow?

## Architecture Onboarding

- Component map:
  - Single-neuron linear network: Parameters (a, w) with conserved quantity δ
  - Two-layer linear network: Weight matrices (W, A) with conserved matrix Δ
  - Two-layer ReLU network: Same parameters but with activation pattern matrix C
  - Deep linear network: Chain of weight matrices with isotropic initialization
  - Experiments: Teacher-student setup, CNNs, RHM, transformers

- Critical path: Derive conserved quantities → Transform to spherical/hyperbolic coordinates → Solve ODEs exactly → Analyze NTK evolution → Validate experimentally

- Design tradeoffs: 
  - Linear vs nonlinear analysis: Linear allows exact solutions but may miss activation pattern effects
  - Single-neuron vs multi-neuron: Single-neuron gives clean theory but multi-neuron shows real-world complexity
  - Finite-width vs infinite-width: Finite-width captures finite-size effects but infinite-width enables mean-field analysis

- Failure signatures:
  - NTK remains constant throughout training → lazy regime
  - Large parameter movement with small kernel change → delayed rich regime
  - No improvement in test accuracy despite kernel evolution → alignment without fitting

- First 3 experiments:
  1. Teacher-student setup with two-layer ReLU networks varying τ and δ to observe test loss and kernel distance
  2. CNN training on MNIST with scaled first layer weights to measure Gabor filter smoothness
  3. Transformer grokking experiment with scaled embedding weights to measure time to generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the distribution of per-neuron conserved quantities (δk) influence feature learning in finite-width networks, and how does this compare to infinite-width predictions?
- Basis in paper: [explicit] The paper discusses connections to infinite-width analysis where the random initialization induces a distribution over per-neuron conserved quantities, but states this is left for future work.
- Why unresolved: The paper focuses on finite-width networks and only briefly mentions the infinite-width connection, without providing empirical or theoretical analysis of how the distribution of δk affects feature learning in practice.
- What evidence would resolve it: Empirical studies comparing feature learning dynamics in finite-width networks with varying distributions of δk, and theoretical analysis connecting these distributions to infinite-width predictions.

### Open Question 2
- Question: How do discretization and stochastic effects of SGD disrupt the conservation laws central to this study, and what simplicity biases do they introduce?
- Basis in paper: [explicit] The paper explicitly mentions this as a limitation, stating that SGD disrupts conservation laws and introduces additional simplicity biases.
- Why unresolved: The paper's theoretical analysis assumes gradient flow, which doesn't account for the discrete updates and noise inherent in SGD. Understanding how these effects modify the learning dynamics and biases is crucial for practical applications.
- What evidence would resolve it: Experiments comparing feature learning dynamics under gradient flow versus SGD with various learning rates and batch sizes, and theoretical analysis of how SGD modifies the conserved quantities and learning trajectories.

### Open Question 3
- Question: How does the learning speed profile across layers impact feature learning, inductive biases, and generalization in deep networks?
- Basis in paper: [explicit] The conclusion section mentions this as an important direction for future work, noting that the paper encourages further investigation into unbalanced initializations to optimize efficient feature learning.
- Why unresolved: The paper provides evidence that upstream initializations (faster learning in earlier layers) can accelerate rich learning, but doesn't systematically explore how different learning speed profiles across layers affect the learning process and final model performance.
- What evidence would resolve it: Experiments training deep networks with various layer-specific learning rate schedules, analyzing feature learning dynamics, and measuring generalization performance across different tasks.

## Limitations
- Theoretical analysis relies on idealized gradient flow dynamics that may not hold in practical settings with SGD noise
- Extension to nonlinear networks beyond the minimal two-layer case lacks theoretical guarantees
- Practical significance in very deep networks (>10 layers) and with modern architectural components remains untested

## Confidence

**High confidence**: The conserved quantity mechanism (δ = ηwa² - ηa||w||²) and its effect on learning trajectories is mathematically rigorous for the minimal models studied. The relationship between initialization balance and learning regime in linear networks is well-established.

**Medium confidence**: The extension to nonlinear networks and the claim that upstream initializations accelerate feature learning is supported by experiments but lacks theoretical guarantees beyond the minimal two-layer case.

**Low confidence**: The practical significance of initialization effects in very deep networks (>10 layers) and with modern architectural components like batch normalization remains untested.

## Next Checks
1. Test whether the conserved quantity δ remains approximately constant during SGD training with different batch sizes and learning rate schedules.
2. Experimentally verify that upstream initialization benefits persist when adding batch normalization layers that could mask initialization effects.
3. Extend the analysis to three-layer networks to test whether the initialization directionality principle generalizes beyond two-layer architectures.