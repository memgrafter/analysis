---
ver: rpa2
title: A Bayesian Optimization Approach to Machine Translation Reranking
arxiv_id: '2411.09694'
source_url: https://arxiv.org/abs/2411.09694
tags:
- bayesopt
- scoring
- proxy
- candidates
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational cost of reranking machine
  translation candidates with large evaluation models by posing reranking as a Bayesian
  optimization problem. The core method uses Gaussian processes to strategically select
  which candidates to score, balancing exploration and exploitation to find top-scoring
  candidates while evaluating only a fraction of the full candidate list.
---

# A Bayesian Optimization Approach to Machine Translation Reranking

## Quick Facts
- arXiv ID: 2411.09694
- Source URL: https://arxiv.org/abs/2411.09694
- Authors: Julius Cheng; Maike Züfle; Vilém Zouhar; Andreas Vlachos
- Reference count: 25
- Key outcome: Reduces computational cost of MT reranking by 61% while maintaining translation quality

## Executive Summary
This paper proposes using Bayesian optimization with Gaussian processes to reduce the computational cost of reranking machine translation candidates. Instead of scoring all candidates with expensive quality estimation models, the method strategically selects which candidates to score based on a balance of exploration and exploitation. The approach achieves the same CometKiwi score using only 70 scoring evaluations compared to a baseline using 180, representing a 61% reduction in computation. A multi-fidelity extension further improves performance by incorporating cheaper proxy scoring models, with distilled smaller models showing particularly strong results.

## Method Summary
The method frames MT reranking as a Bayesian optimization problem where the goal is to find the highest-scoring translation candidate while minimizing the number of expensive scoring function evaluations. Candidates are embedded using mean-pooled decoder activations, and a Gaussian process with RBF kernel models the score function over this embedding space. An acquisition function (Expected Improvement) selects promising candidates to evaluate next. A multi-fidelity extension incorporates cheaper proxy scoring models to further improve search efficiency. The method is evaluated on the WMT23 Metrics dataset using CometKiwi as the main scoring model and smaller distilled Comet variants as proxy scorers.

## Key Results
- Achieves same CometKiwi score with 70 scoring evaluations vs 180 for baseline (61% reduction)
- Multi-fidelity extension improves performance by using proxy scoring models
- Distilled smaller Comet models work particularly well as proxy scorers
- Performance degrades with larger batch sizes but effect diminishes for larger n

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian optimization with Gaussian processes reduces computational cost by strategically selecting which translation candidates to score
- Mechanism: The algorithm uses a Gaussian process to model the unknown score function over the candidate space, then uses an acquisition function (Expected Improvement) to select the most promising candidates to evaluate next. This balances exploration of uncertain candidates with exploitation of known high-scoring candidates.
- Core assumption: The translation candidates can be embedded into a space where similar candidates (by embedding distance) have similar scores
- Evidence anchors:
  - [abstract] "By strategically selecting candidates to score based on a balance of exploration and exploitation, we show that it is possible to find top-scoring candidates when scoring only a fraction of the candidate list"
  - [section 3.1] "We compute the GP posterior of all y ∈ ¯Cobs with Equation 2 and 3 given the scores of Cobs, which is then used to compute EI with Equation 5"
- Break condition: If the embedding space fails to capture score similarity (e.g., candidates that are lexically similar but semantically different have very different scores), the GP prior becomes uninformative and BayesOpt degenerates to random search

### Mechanism 2
- Claim: Multi-fidelity extension improves search efficiency by using cheaper proxy scoring models
- Mechanism: A multi-fidelity kernel combines the candidate embedding kernel with a kernel over score functions, allowing observations from proxy scorers to inform the GP posterior for the main scorer. This provides additional signal about promising candidates before expensive main scoring is performed.
- Core assumption: Proxy scoring models are correlated with the main scoring model, so observations from proxy scores provide useful information about likely main scores
- Evidence anchors:
  - [abstract] "where the candidates are first scored with a cheaper but noisier proxy scoring model, which further improves the cost-performance tradeoff"
  - [section 3.2] "Our kernel for BayesOpt+GP+Proxy is the product of the RBF kernel from Section 3.1 and a kernel over score functions"
- Break condition: If proxy and main scoring models are poorly correlated (low covariance), proxy observations provide little useful information and may even mislead the search

### Mechanism 3
- Claim: Candidate embedding based on mean-pooled decoder activations provides an efficient similarity measure
- Mechanism: Instead of using full string comparison or expensive semantic embeddings, the method uses the mean-pooled token-level outputs from the final decoder layer when generating each candidate. This embedding is fast to compute and captures translation-specific features.
- Core assumption: The decoder activations contain sufficient information to determine candidate similarity for the purposes of score prediction
- Evidence anchors:
  - [section 3.1] "Our kernel is KMT(yi, yj) = KRBF(emb(yi), emb(yj)), where emb returns the mean-pooled token-level outputs of the final decoder layer when generating y"
  - [section 3.1] "emb exploits meaning representations produced automatically during candidate list generation and thus requires negligible overhead"
- Break condition: If decoder activations don't capture the features that determine translation quality scores, the embedding-based similarity will be uninformative for the GP model

## Foundational Learning

- Concept: Gaussian Processes and Bayesian Optimization
  - Why needed here: The core method uses GPs to model the score function over candidates and BayesOpt to iteratively select candidates to evaluate
  - Quick check question: What is the role of the kernel function in a Gaussian process, and how does it affect the posterior distribution?

- Concept: Multi-fidelity optimization
  - Why needed here: The multi-fidelity extension uses observations from cheaper proxy models to improve search efficiency
  - Quick check question: How does the multi-fidelity kernel combine information from different scoring models, and why is the empirical covariance between models important?

- Concept: Expected Improvement acquisition function
  - Why needed here: EI balances exploration and exploitation by considering both the predicted mean and uncertainty of candidate scores
  - Quick check question: How does the Expected Improvement formula encourage both exploration of uncertain candidates and exploitation of known good candidates?

## Architecture Onboarding

- Component map:
  NLLB model with ϵ-sampling -> Candidate generation -> Mean-pooled decoder embeddings -> GP with RBF kernel -> Expected Improvement acquisition -> Candidate selection -> Scoring model (CometKiwi) -> GP posterior update

- Critical path:
  1. Generate candidates with NLLB
  2. Compute initial embeddings and proxy scores (if using multi-fidelity)
  3. Initialize GP with random subset of candidates
  4. Iteratively select batch of candidates using EI
  5. Score selected candidates with main model
  6. Update GP posterior and repeat until budget exhausted
  7. Return highest-scoring candidate

- Design tradeoffs:
  - Embedding quality vs. computation time (mean-pooled decoder activations chosen for speed)
  - Batch size vs. search efficiency (larger batches faster but may miss optimal candidates)
  - Proxy model quality vs. cost (better proxies help more but may be expensive)
  - RBF bandwidth selection (affects similarity scaling, needs tuning)

- Failure signatures:
  - GP posterior variance remains high across all candidates (poor embedding space)
  - Selected candidates show no score improvement over iterations (acquisition function failing)
  - Multi-fidelity extension performs worse than single-fidelity (proxy models poorly correlated)
  - Performance degrades significantly with larger batch sizes (greedy selection missing optimal candidates)

- First 3 experiments:
  1. Run baseline comparison: random candidate selection vs. BayesOpt+GP with n=30, measure CometKiwi score improvement
  2. Test multi-fidelity extension: compare BayesOpt+GP with and without proxy scores (m=50) at n=50
  3. Evaluate batch size impact: run BayesOpt+GP with k=1,5,10 at n=70, measure score degradation and runtime improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal batch size k for BayesOpt+GP that balances computational efficiency and reranking performance?
- Basis in paper: [explicit] The paper experiments with k = 1, 2, 5, 10 and observes performance degradation with larger k, but notes this effect diminishes for larger n
- Why unresolved: The paper shows that larger k reduces performance but doesn't systematically explore the tradeoff curve or determine the optimal k for different budget sizes
- What evidence would resolve it: A comprehensive study mapping performance vs. k for different n values would identify optimal batch sizes for various computational budgets

### Open Question 2
- Question: How do different kernel choices affect BayesOpt+GP performance for MT reranking?
- Basis in paper: [explicit] The paper uses a simple RBF kernel based on embedding similarity but notes this as a potential future direction
- Why unresolved: Only one kernel type was explored despite kernel choice being a principal design decision in Bayesian optimization that could significantly impact performance
- What evidence would resolve it: Experiments comparing multiple kernel types (e.g., Matérn, spectral mixture) and learned kernels would reveal which best capture MT quality score relationships

### Open Question 3
- Question: Can BayesOpt+GP be integrated with candidate generation rather than treating it as a post-hoc reranking step?
- Basis in paper: [explicit] The paper mentions this as a future direction and notes that current approach treats generation and reranking separately
- Why unresolved: The paper doesn't explore whether incorporating reranking signals during generation could improve efficiency or quality compared to the current two-stage approach
- What evidence would resolve it: Comparing integrated vs. separate generation-reranking approaches on the same computational budget would reveal potential synergies or drawbacks

### Open Question 4
- Question: How does the quality-cost tradeoff of BayesOpt+GP compare to directly optimizing for human judgment rather than automated metrics?
- Basis in paper: [inferred] The paper focuses on optimizing CometKiwi scores but acknowledges limitations of automated metrics and references human judgment correlation studies
- Why unresolved: The paper validates effectiveness only on automated metrics without testing whether improvements transfer to human-perceived translation quality
- What evidence would resolve it: Human evaluation studies comparing systems optimized with BayesOpt+GP versus traditional methods would reveal if computational savings come at the cost of human preference

## Limitations

- Method assumes embedding space captures score similarity, which may not hold for all translation domains
- Empirical evaluation limited to English-German translation on WMT23 Metrics dataset
- Kernel choice and hyperparameters may require tuning for different translation tasks

## Confidence

- Claim Cluster 1: 61% reduction in scoring evaluations (High confidence)
- Claim Cluster 2: Multi-fidelity extension improves performance (Medium confidence)
- Claim Cluster 3: GP model generalizes across translation tasks (Low confidence)

## Next Checks

1. Cross-domain generalization test: Evaluate BayesOpt+GP on a different translation dataset (e.g., WMT22 instead of WMT23) and measure degradation in CometKiwi score vs. computational savings ratio.

2. Proxy model sensitivity analysis: Systematically vary the quality and cost of proxy models (e.g., different distillation ratios, different base architectures) and measure the impact on overall search efficiency and final translation quality.

3. Scaling study: Test the method with larger candidate lists (500-1000 candidates) and different language pairs to assess how the computational savings and performance characteristics change with problem scale.