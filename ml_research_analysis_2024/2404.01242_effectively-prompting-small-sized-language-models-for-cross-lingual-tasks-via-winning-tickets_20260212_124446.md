---
ver: rpa2
title: Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via
  Winning Tickets
arxiv_id: '2404.01242'
source_url: https://arxiv.org/abs/2404.01242
tags:
- parameters
- language
- performance
- languages
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current soft prompt methods are less effective on small-sized models
  due to their limited capacity to capture task-specific knowledge. To address this,
  we propose Lottery Ticket Prompt-learning (LTP), which combines winning tickets
  with soft prompts by selecting a subset of parameters that change the most during
  pre-training and fine-tuning them along with prompt-related parameters.
---

# Effectively Prompting Small-sized Language Models for Cross-lingual Tasks via Winning Tickets

## Quick Facts
- **arXiv ID**: 2404.01242
- **Source URL**: https://arxiv.org/abs/2404.01242
- **Reference count**: 17
- **Primary result**: LTP achieves 37.22% average accuracy on XNLI vs 33.24% for deep prompt-tuning using only 20% of parameters

## Executive Summary
This paper introduces Lottery Ticket Prompt-learning (LTP), a method that combines winning tickets with soft prompts to improve cross-lingual transfer for small-sized language models. The approach addresses the challenge that current soft prompt methods are less effective on small models due to their limited capacity to capture task-specific knowledge. By selecting parameters that change most during MLM fine-tuning and combining them with soft prompts, LTP achieves better cross-lingual transfer performance while using only 20% of the original parameters.

## Method Summary
The method involves two key phases: parameter selection and selective fine-tuning. First, the model is fine-tuned on English Wikipedia using MLM, and parameters that change most during this process are selected as "winning tickets." The input/output embeddings are decoupled and the embedding layer is frozen. Second, soft prompts (4 continuous tokens per example) are prepended to the input, and only the selected parameters plus prompt-related parameters are fine-tuned on cross-lingual NLI tasks using few-shot data (1-32 samples per class per language).

## Key Results
- LTP achieves 37.22% average accuracy on XNLI compared to 33.24% for deep prompt-tuning
- LTP achieves 37.04% on AmericasNLI compared to 35.55% for soft prompting
- Uses only 20% of original parameters while outperforming baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selecting parameters that changed the most during MLM fine-tuning creates a task-adapted sparse subnetwork
- Core assumption: Parameters that change significantly during English MLM fine-tuning will generalize across languages for NLI tasks
- Evidence anchors: Abstract and section text about selecting top K parameters based on change magnitude
- Break condition: If English MLM changes don't capture task-relevant patterns, or if selected parameters overfit to English

### Mechanism 2
- Claim: Reducing trainable parameters prevents overfitting in low-resource scenarios
- Core assumption: Smaller parameter space leads to better generalization when training data is scarce
- Evidence anchors: Abstract about blending winning tickets with prompting, section on data scarcity
- Break condition: If frozen parameters contain task-critical information, or if prompt parameters alone cannot capture task patterns

### Mechanism 3
- Claim: Middle-layer parameters are more expressive for cross-lingual transfer
- Core assumption: Middle layers contain more cross-lingual generalizable information than other layers
- Evidence anchors: Section on layer expressiveness, citations supporting middle layer effectiveness
- Break condition: If middle layers don't contain task-relevant information, or if language-specific patterns are primarily in other layers

## Foundational Learning

- **Concept: Lottery Ticket Hypothesis**
  - Why needed here: Provides theoretical foundation for parameter selection strategy and sparse subnetwork training
  - Quick check question: Why does training a sparse subnetwork from initial values often match full network performance?

- **Concept: Cross-lingual transfer learning**
  - Why needed here: The method relies on knowledge transfer from English to other languages, requiring understanding of transfer mechanisms
  - Quick check question: What makes some parameters more transferable across languages than others?

- **Concept: Masked Language Modeling objective**
  - Why needed here: The parameter selection process uses MLM fine-tuning as the basis for identifying active parameters
  - Quick check question: How does MLM training affect different parameter groups differently?

## Architecture Onboarding

- **Component map**: XLM-Roberta-base backbone → Parameter selection module → Soft prompt module → Fine-tuning controller
- **Critical path**: Parameter selection → Soft prompt initialization → Selective fine-tuning → Evaluation
- **Design tradeoffs**: Parameter efficiency vs performance, selection strategy complexity vs effectiveness, frozen vs trainable parameter ratio
- **Failure signatures**: Poor performance despite high parameter efficiency (selection failure), overfitting despite reduced parameters (prompt inadequacy), performance degradation across languages (knowledge loss)
- **First 3 experiments**:
  1. Verify parameter selection produces meaningful change patterns by comparing pre-trained vs MLM-finetuned parameter distributions
  2. Test different active ratios (20%, 50%, 75%) to find optimal tradeoff between efficiency and performance
  3. Compare parameter selection from different layer groups (all layers vs middle layers only) to validate layer-specific effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LTP perform on languages with very limited or no training data?
- Basis in paper: Paper mentions effectiveness for low-resource languages but lacks results for zero-resource languages
- Why unresolved: Focuses on languages with some training data, not languages with none
- What evidence would resolve it: Experimental results comparing LTP performance on languages with no training data vs those with some

### Open Question 2
- Question: How does LTP compare to other prompting methods in terms of efficiency and effectiveness?
- Basis in paper: Mentions LTP is simpler and more efficient than deep prompt tuning
- Why unresolved: Lacks comprehensive comparison across various tasks and model sizes
- What evidence would resolve it: Experimental results comparing LTP to other methods across range of tasks and model sizes

### Open Question 3
- Question: How does parameter selection strategy affect model performance on cross-lingual tasks?
- Basis in paper: Discusses importance of selecting right parameters and explores different strategies
- Why unresolved: Doesn't provide definitive answer on optimal parameter selection strategy
- What evidence would resolve it: Experimental results comparing LTP with different parameter selection strategies

## Limitations
- The effectiveness depends on untested assumption that MLM fine-tuning changes capture task-relevant information
- Cross-lingual transfer untested for language families vastly different from Indo-European
- Few-shot evaluation doesn't test scalability to larger training sets
- Soft prompt architecture details remain underspecified

## Confidence

- **High confidence**: Experimental methodology (XNLI/AmericasNLI evaluation, few-shot learning setup) is clearly specified and reproducible
- **Medium confidence**: Cross-lingual transfer improvements demonstrated but generalizability to other language families unproven
- **Low confidence**: Theoretical justification for MLM changes capturing language-agnostic information is weakly supported

## Next Checks

1. Implement control experiment with random parameter selection vs LTP's MLM-change-based selection to validate the selection strategy's effectiveness
2. Evaluate LTP on a language family distinctly different from Indo-European (e.g., Sino-Tibetan) to test cross-lingual generalization
3. Systematically test active ratios from 5% to 80% to identify optimal efficiency-performance tradeoff across different language families and task complexities