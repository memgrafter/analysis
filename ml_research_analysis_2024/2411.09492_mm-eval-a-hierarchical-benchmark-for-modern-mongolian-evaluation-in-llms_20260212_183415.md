---
ver: rpa2
title: 'MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in LLMs'
arxiv_id: '2411.09492'
source_url: https://arxiv.org/abs/2411.09492
tags:
- language
- mongolian
- knowledge
- evaluation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating large language
  models (LLMs) on low-resource languages, specifically Mongolian. It introduces MM-Eval,
  a hierarchical benchmark dataset based on the Modern Mongolian Language Textbook
  I and enriched with WebQSP and MGSM datasets.
---

# MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in LLMs

## Quick Facts
- arXiv ID: 2411.09492
- Source URL: https://arxiv.org/abs/2411.09492
- Reference count: 7
- Primary result: Introduces MM-Eval, a hierarchical benchmark showing LLMs perform significantly better on syntax (90.69%) than reasoning tasks (29.6%) in Mongolian

## Executive Summary
This paper introduces MM-Eval, a hierarchical benchmark dataset designed to evaluate large language models (LLMs) on Modern Mongolian, a low-resource language. The dataset is structured into four levels—syntax, semantics, knowledge, and reasoning—based on the Modern Mongolian Language Textbook I and supplemented with WebQSP and MGSM datasets. Experiments with multiple models reveal significant performance gaps across these levels, with all models struggling most on reasoning tasks. The benchmark provides valuable insights for advancing NLP and LLM capabilities in low-resource language contexts.

## Method Summary
The MM-Eval benchmark was constructed using Modern Mongolian Language Textbook I as the primary source, supplemented with WebQSP for knowledge tasks and MGSM for reasoning tasks. The dataset comprises 1,840 total tasks organized into four hierarchical levels: 569 syntax, 677 semantics, 344 knowledge, and 250 reasoning tasks. Evaluation was conducted using multiple models including Qwen2-7B-Instruct, GLM4-9b-chat, Llama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 with temperature=0, top-p=0.1, frequency penalty=1, and a system prompt indicating Mongolian proficiency.

## Key Results
- Models achieve highest accuracy on syntax tasks (up to 90.69%), significantly outperforming other levels
- Semantic task performance shows moderate success (up to 72.53%), indicating partial language understanding
- Knowledge tasks demonstrate moderate decline (up to 80.52%), suggesting successful transfer from high-resource to low-resource contexts
- All models struggle with reasoning tasks, with highest accuracy reaching only 29.6%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The hierarchical structure (syntax → semantics → knowledge → reasoning) captures incremental complexity in language proficiency and aligns with cognitive load theory.
- **Mechanism:** By organizing tasks from surface-level syntactic manipulation to abstract reasoning, the benchmark isolates model capabilities at each level, preventing high performance in one domain from masking failures in another.
- **Core assumption:** Performance degradation across levels reflects genuine capability gaps rather than dataset artifacts or translation noise.
- **Evidence anchors:**
  - [abstract] Models perform 90.69% on syntax vs 29.6% on reasoning, suggesting a clear capability gap.
  - [section 4.3] Results reveal that while models perform well in basic linguistic tasks, they struggle with semantic understanding and complex reasoning.
- **Break condition:** If reasoning failures are due to ambiguous translations or culturally opaque content rather than reasoning ability, the hierarchy would misattribute the cause.

### Mechanism 2
- **Claim:** Using a standardized textbook (Modern Mongolian Language Textbook I) ensures linguistic consistency and representativeness across syntactic and semantic tasks.
- **Mechanism:** Textbook sentences are curated for language learning progression, so their reuse in evaluation ensures tasks map to established proficiency benchmarks.
- **Core assumption:** Textbook content reflects natural language use and is not artificially simplified to the point of being unrepresentative of real-world Mongolian.
- **Evidence anchors:**
  - [section 3.2] The primary source of data for language abilities is Modern Mongolian Language Textbook I, selected for its structured progression of language skills.
  - [section 3.4] Sentences from textbook dialogues are used for syntax evaluation, leveraging their pedagogical design.
- **Break condition:** If textbook sentences lack diversity in style or domain, the benchmark may overfit to textbook-style language and fail to generalize to other Mongolian text types.

### Mechanism 3
- **Claim:** Supplementing textbook data with WebQSP and MGSM datasets introduces external knowledge and reasoning challenges that the textbook alone cannot provide.
- **Mechanism:** WebQSP adds factual knowledge tasks, while MGSM introduces math reasoning; both extend the benchmark beyond textbook language proficiency into cognitive domains influenced by general training data.
- **Core assumption:** The external datasets, when translated to Mongolian, retain their difficulty and validity in the target language.
- **Evidence anchors:**
  - [section 3.2] Knowledge data is derived from WebQSP and heuristic-generated content; reasoning data comes from MGSM.
  - [section 3.6] WebQSP is filtered for common knowledge to ensure relevance.
- **Break condition:** If translations of WebQSP or MGSM content introduce errors or ambiguities, the tasks may no longer measure knowledge or reasoning accurately.

## Foundational Learning

- **Concept:** Hierarchical task design
  - **Why needed here:** To separate and measure distinct cognitive and linguistic abilities without conflating them, ensuring interpretable performance diagnostics.
  - **Quick check question:** Why does separating syntax from semantics matter for diagnosing model weaknesses?

- **Concept:** Dataset provenance and translation fidelity
  - **Why needed here:** Since much of the data is translated from English/Chinese, ensuring translation accuracy is critical to avoid measuring translation quality instead of language understanding.
  - **Quick check question:** What verification steps are taken to ensure translated questions preserve their original meaning?

- **Concept:** Low-resource language evaluation principles
  - **Why needed here:** Standard high-resource benchmarks don't apply; evaluation must account for limited training data, script differences, and domain coverage.
  - **Quick check question:** How does the benchmark account for the morphological complexity of Mongolian when designing semantic tasks?

## Architecture Onboarding

- **Component map:** Data Collection (Textbook OCR → sentence extraction → deduplication) → Task Generation (Syntax/semantics/knowledge/reasoning) → Model Inference (Local/API) → Evaluation (Accuracy aggregation)
- **Critical path:** Data → Translation → Task Generation → Model Inference → Accuracy Aggregation
- **Design tradeoffs:** Translation-based data introduces potential noise but enables reuse of high-quality English/Chinese datasets; limited to multiple-choice format for consistency but loses nuance of open-ended tasks.
- **Failure signatures:** Large accuracy drops at semantic or reasoning levels may indicate translation issues rather than model weaknesses; inconsistent performance across similar models may suggest dataset bias.
- **First 3 experiments:**
  1. Run syntax tasks only on all models to establish baseline performance and confirm models can handle Mongolian orthography.
  2. Run knowledge tasks only to test transfer learning from high-resource to low-resource contexts.
  3. Run semantic tasks only to isolate understanding of word-level meaning versus syntactic form.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of LLMs on Mongolian syntax tasks compare to their performance on syntax tasks in high-resource languages, and what factors contribute to any observed differences?
- **Basis in paper:** [inferred]
- **Why unresolved:** The paper only evaluates LLM performance on Mongolian syntax tasks and does not provide a comparative analysis with high-resource languages. The factors contributing to performance differences are not explored.
- **What evidence would resolve it:** Comparative studies evaluating LLM performance on syntax tasks across multiple languages, including both high-resource and low-resource languages like Mongolian, would help identify performance differences and contributing factors.

### Open Question 2
- **Question:** What specific mechanisms enable LLMs to transfer knowledge from high-resource languages to low-resource languages like Mongolian, and how can these mechanisms be optimized?
- **Basis in paper:** [explicit]
- **Why unresolved:** While the paper suggests that models can transfer general knowledge from high-resource to low-resource contexts, it does not delve into the specific mechanisms behind this transfer or how they can be optimized.
- **What evidence would resolve it:** Research investigating the underlying mechanisms of knowledge transfer in LLMs, along with experiments aimed at optimizing these mechanisms for low-resource languages, would provide insights into improving performance.

### Open Question 3
- **Question:** How does the inclusion of more diverse and extensive reasoning tasks in the MM-Eval dataset affect the evaluation of LLMs' reasoning capabilities in Mongolian?
- **Basis in paper:** [inferred]
- **Why unresolved:** The paper notes that the current MM-Eval dataset is limited by a narrow scope of logical reasoning tasks. Expanding the dataset could potentially lead to different evaluations of LLM reasoning capabilities.
- **What evidence would resolve it:** Expanding the MM-Eval dataset to include a wider variety of reasoning tasks and re-evaluating LLM performance would reveal the impact of task diversity on reasoning capability assessments.

## Limitations

- Benchmark relies heavily on translated data (WebQSP and MGSM), potentially introducing semantic drift or ambiguity in Mongolian without explicit translation quality validation
- Dataset size (1,840 total tasks) may be insufficient for robust statistical analysis, especially with only 250 reasoning examples
- Evaluation only considers multiple-choice formats, potentially missing nuances of open-ended language understanding
- Does not address potential script encoding issues specific to Mongolian Cyrillic or tokenization impact on model performance

## Confidence

- **High Confidence**: Models perform significantly better on syntax (90.69%) than reasoning tasks (29.6%)—this pattern is clearly demonstrated and mechanistically explained
- **Medium Confidence**: Knowledge task performance (80.52%) indicates transfer learning from high-resource to low-resource languages—plausible but requires further validation with controlled training data analysis
- **Low Confidence**: Semantic task performance (72.53%) accurately reflects true semantic understanding—potentially confounded by translation quality and task design limitations

## Next Checks

1. Conduct a blind human evaluation of a subset of translated reasoning and knowledge tasks to verify semantic fidelity and task difficulty preservation
2. Perform ablation studies by testing models on Mongolian subsets of MGSM and WebQSP (if available) versus the translated versions to quantify translation impact
3. Expand the benchmark with additional task types (e.g., cloze tests, summarization) and increase sample sizes, particularly for reasoning tasks, to improve statistical reliability