---
ver: rpa2
title: A Comparative Study of Pruning Methods in Transformer-based Time Series Forecasting
arxiv_id: '2412.12883'
source_url: https://arxiv.org/abs/2412.12883
tags:
- pruning
- time
- series
- arxiv
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comparative benchmark study of pruning methods
  for Transformer-based models in time series forecasting. The authors evaluate unstructured
  and structured pruning on five state-of-the-art models (Transformer, Informer, Autoformer,
  FEDformer, Crossformer) across multiple datasets.
---

# A Comparative Study of Pruning Methods in Transformer-based Time Series Forecasting

## Quick Facts
- arXiv ID: 2412.12883
- Source URL: https://arxiv.org/abs/2412.12883
- Reference count: 40
- Models can be pruned to 90% sparsity while maintaining or improving performance, especially for models with Fourier decomposition

## Executive Summary
This paper provides a comprehensive benchmark study comparing unstructured and structured pruning methods for Transformer-based models in time series forecasting. The authors evaluate five state-of-the-art models (Transformer, Informer, Autoformer, FEDformer, Crossformer) across multiple datasets, finding that certain models can be pruned to high sparsity levels while maintaining or even improving performance. The study reveals that unstructured pruning can achieve up to 90% sparsity with minimal performance loss, while structured pruning using DepGraph shows inconsistent results. Notably, the research demonstrates that fine-tuning pruned models is essential for performance recovery and that model size requirements scale with dataset size.

## Method Summary
The study employs unstructured weight magnitude pruning and structured node pruning using DepGraph on five Transformer-based models trained for multivariate time series forecasting. Models are trained with Adam optimizer, batch size 32, learning rate 5e-4 for 10 epochs with early stopping. Unstructured pruning removes weights closest to zero based on magnitude, creating sparsity levels from 0.8^i for i=0,...,10. Structured pruning removes entire rows/columns in weight matrices to reduce computational load. The study evaluates performance using Mean Squared Error (MSE) on test data, parameter density, FLOPs reduction, and inference time speedup across multiple datasets with forecasting horizons of 96, 192, 336, and 720 steps.

## Key Results
- Unstructured pruning achieves up to 90% sparsity while maintaining or improving performance, particularly for Autoformer and FEDformer models
- Structured pruning using DepGraph fails to achieve consistent results across different model-dataset combinations
- Structured pruning reduces FLOPs by up to 7x but does not provide significant inference time speedup
- Fine-tuning pruned models is necessary to recover performance loss from pruning
- Model size requirements scale with dataset size, with larger datasets requiring larger models even after pruning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning reduces model size and computational demand without significantly harming predictive performance.
- Mechanism: Removing weights close to zero after training reduces the number of parameters and operations while maintaining essential learned features.
- Core assumption: Most weights in a trained network are close to zero and do not contribute significantly to output.
- Evidence anchors:
  - [abstract] "Pruning is an established approach to reduce neural network parameter count and save compute."
  - [section] "The unstructured weight pruning sets the smallest parameters to zero, resulting in a sparsity ð‘ , a fractional number denoting the number of zeros in relation to full parameter count."
  - [corpus] Weak evidence - related papers focus on alternative compression methods like vector quantization rather than pruning.

### Mechanism 2
- Claim: Structured pruning can achieve actual inference speedup by removing groups of parameters.
- Mechanism: Removing entire rows/columns in weight matrices reduces computational load in matrix-vector multiplication.
- Core assumption: Removing structured groups of parameters will lead to actual speed improvements when supported by hardware/software.
- Evidence anchors:
  - [abstract] "While structured pruning using DepGraph fails to achieve consistent results across different model-dataset combinations."
  - [section] "On the other hand,structured weight pruning is able to yield performance gains by removing rows of the weight matrix, which reduces the computational load in the matrix-vector-multiplication."
  - [corpus] Weak evidence - related papers focus on alternative architectures rather than pruning methods.

## Foundational Learning

### Pruning
- Why needed: To reduce model size and computational requirements while maintaining predictive performance
- Quick check: Measure parameter count reduction and performance change after applying magnitude-based weight removal

### Structured vs Unstructured Pruning
- Why needed: Different approaches to parameter removal with varying impacts on hardware acceleration
- Quick check: Compare inference speedup and FLOPs reduction between row/column removal versus individual weight removal

### Fine-tuning After Pruning
- Why needed: To recover performance loss that occurs when removing parameters
- Quick check: Evaluate model performance before and after retraining pruned models with the same objective function

## Architecture Onboarding

### Component Map
Input Data -> Transformer-based Model (5 variants) -> Pruning Module -> Fine-tuning Module -> Performance Evaluation

### Critical Path
Model Training â†’ Unstructured Pruning â†’ Performance Evaluation â†’ Structured Pruning â†’ Performance Evaluation â†’ Fine-tuning â†’ Final Performance Evaluation

### Design Tradeoffs
- Unstructured pruning: Higher sparsity achievable but no actual speedup without sparse kernel support
- Structured pruning: Potential for actual speedup but inconsistent performance across models/datasets
- Fine-tuning: Essential for performance recovery but adds computational cost

### Failure Signatures
- Model divergence during training or out-of-memory errors for longer forecast horizons
- Structured pruning failing to achieve target sparsity due to model architecture dependencies
- Inconsistent performance across different model-dataset combinations with structured pruning

### First Experiments
1. Train all five models on ETTm1 dataset with forecast length 192 using specified hyperparameters
2. Apply unstructured pruning at multiple sparsity levels and evaluate performance impact
3. Implement structured pruning using DepGraph and measure resulting density, FLOPs reduction, and inference time speedup

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does structured pruning fail to achieve consistent results across different model-dataset combinations?
- Basis in paper: [explicit] The paper shows that structured pruning using DepGraph fails to achieve consistent results across different model-dataset combinations.
- Why unresolved: While the paper demonstrates the failure of structured pruning in specific cases, it does not identify the specific conditions or characteristics of models/datasets that lead to this inconsistency.
- What evidence would resolve it: Comparative analysis identifying specific model architectures, dataset characteristics (e.g., feature dimensions, sampling frequency), or pruning configurations that consistently lead to successful or failed structured pruning outcomes.

### Open Question 2
- Question: What is the relationship between dataset size, model complexity, and optimal pruning ratios for achieving maximum performance?
- Basis in paper: [explicit] The paper shows that larger models with more parameters yield better performance on larger datasets even after pruning, while smaller models work well on small datasets.
- Why unresolved: The study provides evidence of this relationship but does not establish quantitative guidelines for determining optimal pruning ratios based on dataset size and model complexity.
- What evidence would resolve it: Systematic experiments mapping dataset size, model complexity, and pruning ratios to optimal performance metrics across a range of configurations.

### Open Question 3
- Question: Why does unstructured pruning amount to an academic exercise without actual speedup despite achieving high sparsity?
- Basis in paper: [explicit] The paper states that current frameworks only mask pruned elements and do not support unstructured sparsity further, for example by employing specialized kernels for sparse matrix-vector-multiplication.
- Why unresolved: The paper identifies the limitation but does not explore potential solutions or alternative approaches that could enable actual speedup from unstructured pruning.
- What evidence would resolve it: Development and evaluation of hardware/software solutions (e.g., specialized kernels, optimized libraries) that enable actual speedup from unstructured pruning, or alternative sparse matrix operations that maintain theoretical benefits.

## Limitations

- Structured pruning using DepGraph shows inconsistent performance across different model-dataset combinations
- Unstructured pruning achieves high sparsity but does not provide actual inference speedup due to lack of sparse kernel support
- Limited exploration of the relationship between dataset size, model complexity, and optimal pruning ratios

## Confidence

- **High Confidence**: Unstructured pruning maintaining performance at high sparsity levels (up to 90%)
- **Medium Confidence**: Structured pruning effectiveness claims - results show significant variability across model-dataset combinations with DepGraph
- **Low Confidence**: Model size reduction conclusions - limited experimental validation with only one model type and dataset combination

## Next Checks

1. Reproduce structured pruning experiments: Implement and validate the DepGraph structured pruning approach on all five models across multiple datasets to confirm the inconsistent performance findings

2. Extended dataset size study: Conduct additional experiments varying dataset sizes systematically (e.g., 10%, 25%, 50%, 75%, 100% of full dataset) for all models to clarify the relationship between dataset size, model complexity, and pruning effectiveness

3. Hardware-specific inference evaluation: Measure actual inference speedup on different hardware platforms (CPU, GPU, specialized accelerators) to verify the gap between theoretical FLOPs reduction and practical performance gains from structured pruning