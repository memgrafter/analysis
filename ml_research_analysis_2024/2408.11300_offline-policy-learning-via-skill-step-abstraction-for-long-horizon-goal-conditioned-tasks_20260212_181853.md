---
ver: rpa2
title: Offline Policy Learning via Skill-step Abstraction for Long-horizon Goal-Conditioned
  Tasks
arxiv_id: '2408.11300'
source_url: https://arxiv.org/abs/2408.11300
tags:
- policy
- learning
- skill-step
- goal
- skill
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel offline goal-conditioned (GC) policy
  learning framework that leverages skill-step abstraction to address long-horizon
  tasks with sparse rewards. The core idea is to iteratively learn a skill-step model
  and GC policy in tandem, using model-guided rollouts to generate diverse trajectories
  beyond the initial dataset.
---

# Offline Policy Learning via Skill-step Abstraction for Long-horizon Goal-Conditioned Tasks

## Quick Facts
- arXiv ID: 2408.11300
- Source URL: https://arxiv.org/abs/2408.11300
- Reference count: 11
- This paper presents a novel offline goal-conditioned (GC) policy learning framework that leverages skill-step abstraction to address long-horizon tasks with sparse rewards.

## Executive Summary
This paper introduces a framework for offline goal-conditioned reinforcement learning that addresses long-horizon tasks with sparse rewards through skill-step abstraction. The core innovation is an iterative learning process that jointly trains a skill-step model and goal-conditioned policy, using model-guided rollouts to generate diverse trajectories beyond the initial dataset. This approach enables the policy to adapt to a wide range of goals and handle goal distribution shifts effectively. The framework incorporates a modular GC policy hierarchy with a skill-step goal generator for efficient fine-tuning, demonstrated through experiments on maze navigation and Franka kitchen manipulation tasks.

## Method Summary
The method learns temporally abstracted skills from offline data using conditional VAEs, then decomposes long-horizon goals into sequences of near-term skill-step goals. A skill-step model captures both flat and temporally abstracted dynamics, enabling model-guided rollouts that expand the dataset with diverse trajectories. The goal-conditioned policy is organized hierarchically, with a skill-step goal generator producing near-term goals and inverse dynamics mapping these to specific skills. During few-shot adaptation, only the skill-step goal generator is fine-tuned while other components remain frozen, enabling parameter-efficient adaptation to new goal distributions.

## Key Results
- Outperforms state-of-the-art GC and skill-based RL methods in both zero-shot and few-shot adaptation scenarios
- Achieves normalized scores of up to 98.7% in zero-shot evaluation and 92.5% in few-shot adaptation
- Demonstrates effective handling of goal distribution shifts through iterative model-guided rollouts
- Shows successful application on maze navigation and Franka kitchen manipulation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Skill-step model enables goal distribution shift adaptation by iteratively expanding achievable goals beyond the initial dataset.
- Mechanism: The framework uses model-guided rollouts to generate new trajectories that branch from existing ones, using learned skill embeddings and latent dynamics. These imaginary trajectories are added back to the dataset, allowing the model and policy to progressively cover a wider range of goals.
- Core assumption: The environment dynamics are consistent between the training dataset and the target tasks, allowing the learned latent dynamics to generalize.
- Evidence anchors:
  - [abstract] "This allows the policy to adapt to a wide range of goals and handle goal distribution shifts effectively."
  - [section 3.1] "To broaden the range of skills and goals beyond the scope of merely imitating individual trajectories in the offline dataset, we employ the iteratively refined skill-step model..."
  - [corpus] Weak evidence: No directly comparable methods using iterative model-guided rollouts for offline goal-conditioned RL found in corpus.
- Break condition: If environment dynamics differ significantly between training and target tasks, the latent dynamics model will fail to generate meaningful rollouts.

### Mechanism 2
- Claim: Modular GC policy hierarchy enables parameter-efficient fine-tuning for few-shot adaptation.
- Mechanism: The policy is decomposed into three modules: skill decoder, skill policy, and skill-step goal generator. Only the skill-step goal generator is updated during fine-tuning, while other modules remain frozen, preserving learned low-level skills and dynamics understanding.
- Core assumption: The inverse skill-step dynamics learned from offline data remain valid for the target task, making it safe to freeze those components.
- Evidence anchors:
  - [abstract] "The framework incorporates a modular GC policy hierarchy with a skill-step goal generator for efficient fine-tuning."
  - [section 3.2] "In this hierarchy, the inverse skill-step dynamics P −1 θ is trained as part of the skill-step model... For downstream tasks with goal distribution shifts... we leverage this hierarchy in a way that only the skill-step goal generator needs to be updated."
  - [corpus] No directly comparable modular approaches found in corpus for offline GC RL.
- Break condition: If the goal distribution shift is so large that the frozen modules become incompatible with the new goals, even fine-tuning the goal generator will fail.

### Mechanism 3
- Claim: Skill-step abstraction reduces long-horizon goal complexity by decomposing goals into near-term subgoals aligned with temporally abstracted skills.
- Mechanism: Long-horizon goals are broken down into sequences of skill-step goals, where each skill-step corresponds to a temporally abstracted H-step action sequence. The policy first predicts a skill-step goal, then uses inverse dynamics to find the skill that achieves it.
- Core assumption: Skills learned from offline data capture useful behavior patterns that can be chained together to achieve long-horizon goals.
- Evidence anchors:
  - [abstract] "where skills are acquired from existing data and long-horizon goals are decomposed into sequences of near-term goals that align with these skills."
  - [section 3.2] "By this decomposition, for a pair of current state st and long-horizon goal g, the GC policy first infers a skill-step goal ˆht+H and then obtains its corresponding skill z via the inverse skill-step dynamics."
  - [corpus] Weak evidence: Some skill-based RL methods exist but not specifically for goal-conditioned decomposition in offline settings.
- Break condition: If the skill horizon H is not well-matched to the task structure, either too short (insufficient abstraction) or too long (difficult to learn dynamics), the decomposition will fail.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) for skill embedding learning
  - Why needed here: Skills are learned as latent embeddings from H-step sub-trajectories using conditional VAEs, providing a compressed representation of temporally abstracted behaviors
  - Quick check question: What is the role of the KL divergence term in the skill loss function?

- Concept: Latent state space alignment with skills
  - Why needed here: The state encoder and dynamics models operate in a latent space that is aligned with skill embeddings, making it easier to stitch sub-trajectories and predict skill outcomes
  - Quick check question: How does the model loss ensure that the latent state space becomes aligned with skills?

- Concept: Hierarchical reinforcement learning structure
  - Why needed here: The policy is decomposed into high-level skill selection and low-level skill execution, enabling temporal abstraction for long-horizon tasks
  - Quick check question: What is the advantage of decomposing the skill policy into skill-step goal generation and inverse dynamics?

## Architecture Onboarding

- Component map:
  - Skill encoder (qϕ) -> Skill decoder (πL θ) -> State encoder (Eθ) -> State decoder (Dϕ) -> Skill prior (pθ) -> Skill-step dynamics (Pθ) -> Flat dynamics (Pϕ) -> Inverse skill-step dynamics (P −1 θ) -> Skill-step goal generator (fψ) -> High-level policy (πZ ψ) -> Low-level policy (πL θ)

- Critical path:
  1. Offline training: Jointly optimize all modules using iterative model-guided rollouts
  2. Zero-shot evaluation: Deploy learned policy directly
  3. Few-shot adaptation: Fine-tune only skill-step goal generator

- Design tradeoffs:
  - Skill horizon H: Longer horizons provide better temporal abstraction but make dynamics learning harder
  - Latent space dimensionality: Higher dimensions may capture more complexity but risk overfitting
  - Rollout branching frequency: More branches generate more diversity but increase computational cost

- Failure signatures:
  - Poor zero-shot performance: Likely issues with skill learning or model-guided rollout generation
  - Degraded few-shot adaptation: May indicate frozen modules are not compatible with new goals
  - Unstable training: Could suggest incorrect hyperparameter choices for VAE or dynamics learning

- First 3 experiments:
  1. Validate skill learning: Check reconstruction quality and KL divergence values on validation set
  2. Test model-guided rollouts: Visualize generated trajectories and measure diversity compared to original dataset
  3. Evaluate skill-step goal prediction: Measure accuracy of fψ predictions against actual skill execution outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed framework handle scenarios where both environment dynamics and goal distributions shift simultaneously during adaptation?
- Basis in paper: [inferred] The paper mentions extending the skill-step model for more complex circumstances where both dynamics and goals shift, but does not elaborate on specific mechanisms.
- Why unresolved: The framework currently assumes either goal distribution shifts or dynamics shifts, not both concurrently. Handling both simultaneously presents additional challenges in maintaining skill alignment and policy adaptability.
- What evidence would resolve it: Experimental results showing GLvSA's performance on tasks with concurrent shifts in both environment dynamics and goal distributions, compared to baselines that address each type of shift separately.

### Open Question 2
- Question: What is the impact of different skill horizon lengths (H) on the computational efficiency of the iterative learning process?
- Basis in paper: [explicit] The paper discusses how different skill horizons affect performance (Table 5), but does not address computational efficiency.
- Why unresolved: While performance is evaluated for different skill horizons, the trade-off between computational cost and learning effectiveness remains unexplored.
- What evidence would resolve it: Analysis of training time and resource usage across different skill horizon settings, demonstrating the computational trade-offs.

### Open Question 3
- Question: How does the framework scale to high-dimensional state spaces and complex manipulation tasks beyond the kitchen environment?
- Basis in paper: [inferred] The experiments focus on maze navigation and kitchen manipulation, suggesting potential limitations in scalability.
- Why unresolved: The framework's effectiveness in more complex, high-dimensional environments with longer-horizon tasks is not demonstrated.
- What evidence would resolve it: Successful application of GLvSA to benchmark robotic manipulation tasks with high-dimensional state spaces (e.g., dexterous manipulation, assembly tasks) with comparative performance analysis.

## Limitations

- Relies heavily on the quality and diversity of the offline dataset - poor initial data will limit performance regardless of training procedure
- Assumes consistent environment dynamics between training and target tasks, which may not hold in practice
- Limited scalability evidence - experiments are confined to maze navigation and kitchen manipulation tasks

## Confidence

- Confidence is **High** for the core methodology of using skill-step abstraction and iterative model-guided rollouts
- Confidence is **Medium** for the claimed advantages over state-of-the-art methods, as experiments are limited to two specific environments
- Confidence is **Low** for the scalability and generalization claims, as the paper doesn't provide experiments on more complex or diverse environments

## Next Checks

1. **Dataset Dependency Test**: Evaluate the method on datasets of varying quality and diversity to assess how sensitive performance is to the initial offline data. Compare against baselines on both high-quality and low-quality datasets.

2. **Goal Distribution Shift Stress Test**: Design experiments with increasingly large goal distribution shifts (e.g., changing from navigating to one corner of the maze to navigating to a completely different maze structure) to identify the breaking point of the adaptation mechanism.

3. **Component Ablation Study**: Systematically disable or modify individual components (e.g., skill-step goal generator, model-guided rollouts) to isolate their contributions and verify that the reported improvements are not due to other factors.