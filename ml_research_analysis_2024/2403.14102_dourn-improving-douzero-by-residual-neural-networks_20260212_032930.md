---
ver: rpa2
title: 'DouRN: Improving DouZero by Residual Neural Networks'
arxiv_id: '2403.14102'
source_url: https://arxiv.org/abs/2403.14102
tags:
- residual
- douzero
- doudizhu
- learning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DouRN, a deep reinforcement learning model for
  the complex card game DouDiZhu, addressing challenges of large action space and
  three-player dynamics. The authors enhance the existing DouZero model by incorporating
  residual neural networks (ResNet) to improve feature learning and convergence speed,
  and introduce a bidding system to optimize landlord selection.
---

# DouRN: Improving DouZero by Residual Neural Networks

## Quick Facts
- arXiv ID: 2403.14102
- Source URL: https://arxiv.org/abs/2403.14102
- Authors: Yiquan Chen; Yingchao Lyu; Di Zhang
- Reference count: 10
- Key outcome: DouRN achieves 55.8% winning rate as landlord and 56.4% as peasant, outperforming DouZero and human players

## Executive Summary
This paper addresses the challenge of applying deep reinforcement learning to DouDiZhu, a complex three-player card game with large action spaces. The authors enhance the existing DouZero model by incorporating residual neural networks (ResNet) to improve feature learning and convergence speed, and introduce a bidding system to optimize landlord selection. Their proposed DouRN model consistently outperforms both DouZero and experienced human players in comprehensive experiments.

## Method Summary
The paper proposes DouRN, which builds upon DouZero by integrating residual neural networks and a bidding system. Two ResNet architectures are tested: stacking residual blocks on the original MLP and replacing the MLP entirely. The bidding system uses a neural network to evaluate hand strength and opponent bids for landlord selection. Training is conducted using self-play with Monte Carlo methods, and winning rates are measured against both DouZero and human players.

## Key Results
- DouRN with 4 residual blocks achieves higher winning rates than DouZero within the same training time
- As landlord, DouRN wins 55.8% of games versus DouZero's lower rate
- As peasant, DouRN wins 56.4% of games versus DouZero's lower rate
- The bidding system reduces experienced human players' winning rates from 52% to 46%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Residual blocks improve learning efficiency and performance in large action spaces
- Mechanism: Skip connections allow gradients to flow directly through the network, mitigating vanishing gradient problems in deep architectures
- Core assumption: The complex card game DouDiZhu benefits from deeper networks that can capture intricate state-action relationships
- Evidence anchors: [abstract] "Building on this work, our study incorporates residual networks into the model... Our findings demonstrate that this model significantly improves the winning rate within the same training time."

### Mechanism 2
- Claim: The bidding system improves decision-making for landlord selection
- Mechanism: The bidding system uses a neural network to evaluate hand strength and opponent bids, making more informed decisions about whether to become landlord
- Core assumption: Landlord selection significantly impacts game outcomes, and AI can make better decisions than random or rule-based approaches
- Evidence anchors: [abstract] "we introduce a call scoring system to assist the agent in deciding whether to become a landlord... With these enhancements, our model consistently outperforms the existing version of DouZero and even experienced human players."

### Mechanism 3
- Claim: Combining residual networks with the bidding system creates synergistic improvements
- Mechanism: Residual networks provide better feature learning for game state evaluation, while the bidding system optimizes initial game setup, leading to overall performance gains
- Core assumption: Improvements in both mid-game decision making (via ResNet) and initial setup (via bidding) compound to create superior overall performance
- Evidence anchors: [abstract] "By combining these two enhancements, the newly trained model, DouRN, outperforms the original DouZero model and exhibits superior capability against experienced DouDiZhu players."

## Foundational Learning

- Concept: Deep Reinforcement Learning (DRL) fundamentals
  - Why needed here: Understanding DRL is crucial for grasping how DouRN learns to play DouDiZhu through self-play and reward optimization
  - Quick check question: What are the key components of a DRL algorithm, and how do they interact in the context of card games?

- Concept: Residual Networks (ResNet) architecture
  - Why needed here: ResNet's skip connections are the core innovation that enables deeper networks and better performance in this application
  - Quick check question: How do residual connections help mitigate the vanishing gradient problem in deep neural networks?

- Concept: Monte Carlo Tree Search (MCTS) methods
  - Why needed here: MCTS is used in DouZero and likely in DouRN for action selection and game tree exploration
  - Quick check question: How does MCTS balance exploration and exploitation in the context of card games with large action spaces?

## Architecture Onboarding

- Component map: Game state input -> ResNet feature extraction -> MCTS action selection -> Bidding system evaluation -> Training loop (self-play -> reward calculation -> network updates)

- Critical path: 1. Game state input → ResNet feature extraction 2. MCTS uses ResNet outputs for action selection 3. Bidding system evaluates initial hand strength 4. Training loop: self-play → reward calculation → network updates

- Design tradeoffs: ResNet depth vs. computational cost; Bidding system complexity vs. decision accuracy; MCTS search depth vs. real-time performance

- Failure signatures: Overfitting (model performs well in training but poorly against new opponents); Instability (training loss fluctuates wildly or diverges); Poor landlord selection (consistently losing as landlord despite strong hands)

- First 3 experiments: 1. Compare DouRN with 2, 4, and 6 residual blocks against DouZero baseline 2. Test DouRN with and without the bidding system against human players 3. Analyze the impact of different ResNet architectures (stacked vs. replacement) on training time and performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the discussion and limitations, several important questions remain unanswered about the model's performance and potential improvements.

## Limitations
- Limited human player testing with only 100 games against a small number of opponents
- No comparison to other advanced Doudizhu AI models like DouZero+ or DeltaDou
- Computational efficiency concerns with residual networks increasing training time

## Confidence
- DouRN performance vs. DouZero: High
- ResNet contribution to convergence: Medium
- Bidding system effectiveness: Medium
- Human player comparison: Low

## Next Checks
1. Conduct ablation studies to isolate the contribution of residual blocks versus other architectural modifications in DouRN
2. Expand human player testing with larger sample sizes and stratified skill levels to better validate bidding system performance
3. Implement and test alternative deep learning architectures (e.g., transformer-based models) for comparison with the ResNet approach