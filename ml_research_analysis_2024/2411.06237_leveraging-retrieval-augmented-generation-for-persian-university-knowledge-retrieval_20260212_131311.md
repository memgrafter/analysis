---
ver: rpa2
title: Leveraging Retrieval-Augmented Generation for Persian University Knowledge
  Retrieval
arxiv_id: '2411.06237'
source_url: https://arxiv.org/abs/2411.06237
tags:
- generation
- questions
- data
- university
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Retrieval-Augmented Generation (RAG) pipeline
  using Persian language models to improve university knowledge retrieval. The authors
  developed a two-stage RAG approach that extracts data from the University of Isfahan's
  official website and employs advanced prompt engineering to generate accurate, contextually
  relevant responses to user queries.
---

# Leveraging Retrieval-Augmented Generation for Persian University Knowledge Retrieval

## Quick Facts
- arXiv ID: 2411.06237
- Source URL: https://arxiv.org/abs/2411.06237
- Authors: Arshia Hemmat; Kianoosh Vadaei; Mohammad Hassan Heydari; Afsaneh Fatemi
- Reference count: 32
- One-line primary result: A two-stage RAG pipeline using Persian language models achieves Faithfulness score of 0.839, Answer Relevance of 0.823, and Context Relevance of 0.216 for university knowledge retrieval

## Executive Summary
This paper presents a Retrieval-Augmented Generation (RAG) pipeline specifically designed for Persian university knowledge retrieval. The authors developed a two-stage RAG approach that extracts data from the University of Isfahan's official website and employs advanced prompt engineering to generate accurate, contextually relevant responses to user queries. They created the UniversityQuestionBench (UQB) dataset, specifically tailored for Persian-language academic queries, and evaluated their system using RAGAS metrics: Faithfulness, Answer Relevance, and Context Relevance.

The system demonstrates significant improvements in response quality compared to baseline approaches, with the Dorna model achieving strong performance across all three metrics. The two-stage architecture, which first classifies queries by department and then performs targeted retrieval, shows promise for improving precision in domain-specific knowledge retrieval tasks. The paper also highlights the importance of Persian-specific embeddings and provides insights into the challenges of building RAG systems for non-English languages.

## Method Summary
The authors developed a two-stage RAG pipeline for Persian university knowledge retrieval. First, they extracted data from the University of Isfahan's official website, focusing on the most frequently accessed information by students. They then created the UniversityQuestionBench (UQB) dataset by surveying students about common questions and scraping relevant information from the university website. The RAG system uses Dorna (a Persian Llama-3 model) for both query classification and answer generation, with Persian-specific embeddings for document retrieval using FAISS. The system was evaluated using RAGAS metrics including Faithfulness, Answer Relevance, and Context Relevance.

## Key Results
- The Dorna model achieved Faithfulness score of 0.839, Answer Relevance of 0.823, and Context Relevance of 0.216
- The two-stage RAG approach with department classification significantly improved query-document matching precision
- Persian-specific embeddings provided better semantic matching for Persian academic queries compared to general-purpose embeddings
- The system demonstrated enhanced precision and relevance in generated responses, improving user experience and reducing time required to obtain relevant answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage RAG approach improves query-document matching by first classifying the query department and then retrieving relevant paragraphs.
- Mechanism: The pipeline uses Dorna to classify queries into departments, then uses FAISS similarity search on department-specific paragraphs to retrieve the top 3 relevant documents.
- Core assumption: Query classification into departments significantly narrows the search space, improving retrieval precision.
- Evidence anchors: [abstract] "We developed the UniversityQuestionBench (UQB) dataset, specifically tailored for Persian-language academic queries"; [section] "Step 2: Identify the type of question and determine the relevant department using the DORNA Model"
- Break condition: If query classification fails or departments are not well-defined, the retrieval precision will degrade.

### Mechanism 2
- Claim: Persian-specific embeddings improve retrieval quality for Persian academic queries.
- Mechanism: The system uses "persian-sentence-transformer-news-wiki-pairs-v3" for embedding paragraphs, which is specifically trained on Persian text, leading to better semantic matching for Persian queries.
- Core assumption: Domain-specific embeddings capture semantic nuances better than general-purpose embeddings for Persian text.
- Evidence anchors: [abstract] "experimental results showed significant improvements in response quality"; [section] "We utilize the Persian embedding model named persian-sentence-transformer-news-wiki-pairs-v3 for embedding the paragraphs"
- Break condition: If the Persian embeddings are not well-trained on the specific academic domain, retrieval quality will suffer.

### Mechanism 3
- Claim: The RAGAS evaluation metrics provide a comprehensive assessment of RAG pipeline performance.
- Mechanism: The system evaluates using Faithfulness (how accurately answers reflect retrieved documents), Answer Relevance (how well answers address queries), and Context Relevance (how well retrieved documents match queries).
- Core assumption: These three metrics together capture the essential dimensions of RAG system quality.
- Evidence anchors: [abstract] "evaluated our system using RAGAS metrics: Faithfulness, Answer Relevance, and Context Relevance"; [section] "To comprehensively assess the effectiveness of our RAG pipeline and LLMs, we utilize three key metrics as defined in the RAGAS paper"
- Break condition: If the metrics don't align with actual user needs, the evaluation may be misleading.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) pipeline architecture
  - Why needed here: Understanding how retrieval and generation components work together is essential for modifying or debugging the system
  - Quick check question: What are the two main stages in a RAG pipeline and what happens in each?

- Concept: Persian language processing challenges
  - Why needed here: The system is specifically designed for Persian academic queries, requiring understanding of Persian morphology and syntax
  - Quick check question: What specific challenges does Persian morphology present for text processing systems?

- Concept: FAISS similarity search
  - Why needed here: FAISS is used for efficient retrieval of similar documents, understanding its parameters and limitations is crucial for optimization
  - Quick check question: How does FAISS perform similarity search on vector embeddings?

## Architecture Onboarding

- Component map: Query input → Department classification (Dorna) → FAISS retrieval (Persian embeddings) → Prompt template generation → Answer generation (Dorna) → RAGAS evaluation
- Critical path: Query → Department classification → FAISS retrieval → Answer generation
- Design tradeoffs: Department classification vs. direct retrieval (classification improves precision but adds complexity); Number of retrieved documents (3) balances relevance with context richness; Embedding model choice (Persian-specific vs. general) affects accuracy
- Failure signatures: Low Faithfulness (model hallucinations or poor document relevance); Low Answer Relevance (poor query understanding or prompt engineering issues); Low Context Relevance (poor embedding quality or retrieval parameters)
- First 3 experiments: 1) Vary the number of retrieved documents (1, 3, 5) and measure impact on Faithfulness and Answer Relevance; 2) Compare Persian-specific embeddings vs. general embeddings on the same queries; 3) Test department classification accuracy on a sample of queries to identify misclassifications

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Dorna with Dorna Embeddings compare to other models in terms of faithfulness, answer relevance, and context relevance?
- Basis in paper: [explicit] The paper explicitly compares the performance of Dorna with Dorna Embeddings to other models using these metrics.
- Why unresolved: While the paper provides specific scores for each metric, it does not discuss the statistical significance of these differences or how they might impact real-world applications.
- What evidence would resolve it: A detailed statistical analysis comparing the performance of Dorna with other models, including significance tests and real-world application scenarios, would provide a clearer understanding of its relative effectiveness.

### Open Question 2
- Question: What are the potential challenges and limitations of using Persian-Sentence-Embedding-V3 in RAG systems, particularly in terms of faithfulness and answer relevance?
- Basis in paper: [inferred] The paper mentions that Persian-Sentence-Embedding-V3 performs differently across metrics, suggesting potential challenges.
- Why unresolved: The paper does not delve into the specific reasons for the performance differences of Persian-Sentence-Embedding-V3 or how these might be addressed.
- What evidence would resolve it: An in-depth analysis of the challenges and limitations of Persian-Sentence-Embedding-V3, including technical reasons for performance differences and potential solutions, would clarify its applicability in RAG systems.

### Open Question 3
- Question: How can the dataset diversity be effectively expanded to improve the model's performance across a wider array of academic subjects and contexts?
- Basis in paper: [explicit] The paper suggests expanding dataset diversity as a future direction to enhance model performance.
- Why unresolved: The paper does not provide specific strategies or methodologies for effectively expanding the dataset diversity.
- What evidence would resolve it: A comprehensive study on effective strategies for dataset expansion, including case studies and experimental results, would offer practical insights into improving model performance.

### Open Question 4
- Question: What are the implications of integrating real-time updates from course selection departments on the accuracy and relevance of the model's responses?
- Basis in paper: [inferred] The paper proposes integrating real-time updates as a future contribution, implying potential benefits.
- Why unresolved: The paper does not explore the technical or practical implications of integrating real-time updates on model performance.
- What evidence would resolve it: An experimental study evaluating the impact of real-time updates on model accuracy and response relevance would provide empirical evidence of its benefits and challenges.

### Open Question 5
- Question: How can the trade-offs between different models and embeddings be optimized to achieve the best overall performance in RAG systems?
- Basis in paper: [explicit] The paper discusses trade-offs between models and embeddings but does not provide optimization strategies.
- Why unresolved: The paper highlights trade-offs but lacks a framework for optimizing these to achieve balanced performance.
- What evidence would resolve it: A theoretical framework or experimental study on optimizing trade-offs between models and embeddings, including performance benchmarks and optimization algorithms, would guide the development of more effective RAG systems.

## Limitations

- The evaluation relies solely on automated RAGAS metrics without human evaluation, which may not fully capture real-world performance
- The system's dependence on the University of Isfahan's website data creates a narrow knowledge base that may not generalize to other Persian universities or domains
- The study lacks comparative analysis against baseline systems or alternative approaches, making it difficult to assess the true improvement achieved

## Confidence

**High Confidence Claims:**
- The two-stage RAG architecture is technically sound and implementable
- Persian-specific embeddings provide better semantic matching for Persian text than general-purpose embeddings
- The RAGAS evaluation framework is appropriate for assessing RAG system quality

**Medium Confidence Claims:**
- The Dorna model achieves the reported metric scores on the UQB dataset
- The department classification step meaningfully improves retrieval precision
- The prompt engineering techniques contribute to improved answer quality

**Low Confidence Claims:**
- The system's performance generalizes beyond the University of Isfahan context
- The automated metrics accurately reflect user experience and practical utility
- The two-stage approach provides significant advantages over simpler RAG implementations

## Next Checks

1. **Human Evaluation Study**: Conduct a user study with university students to evaluate the system's answers against the automated RAGAS metrics, particularly focusing on the low Context Relevance score and assessing practical usability.

2. **Generalization Testing**: Test the system architecture with data from other Persian universities or different knowledge domains to assess whether the performance gains are specific to the University of Isfahan context or represent more general improvements.

3. **Baseline Comparison**: Implement and compare the system against a simpler RAG pipeline without department classification to quantify the actual contribution of the two-stage approach to the reported performance improvements.