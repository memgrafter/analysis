---
ver: rpa2
title: 'PHOENIX: Open-Source Language Adaption for Direct Preference Optimization'
arxiv_id: '2401.10580'
source_url: https://arxiv.org/abs/2401.10580
tags:
- arxiv
- language
- available
- online
- visited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PHOENIX, a German-language large language model
  adapted from Mistral using Direct Preference Optimization (DPO). The key outcome
  is the development of a 7B-parameter model that performs on par with models of its
  size and competes with models 10 times larger.
---

# PHOENIX: Open-Source Language Adaption for Direct Preference Optimization

## Quick Facts
- arXiv ID: 2401.10580
- Source URL: https://arxiv.org/abs/2401.10580
- Authors: Matthias Uhlig; Sigurd Schacht; Sudarshan Kamath Barkur
- Reference count: 40
- One-line primary result: 7B-parameter German LLM achieving performance on par with models 10x larger through translated datasets and DPO

## Executive Summary
PHOENIX is a German-language large language model adapted from Mistral using Direct Preference Optimization (DPO). The authors translated 1.5 billion characters of English datasets to German using the open-source ALMA model, reducing translation costs from over €30,000 to about €30. The resulting 7B-parameter model achieves an average score of 5.78 on the German mt-bench, outperforming Llama-2-70B-chat in Reasoning and Roleplay while also showing improved results on German versions of HellaSwag, ARC, and MMLU benchmarks.

## Method Summary
The method involves translating English instruction datasets (Ultrachat and Ultrafeedback) to German using the ALMA model with vLLM library for inference. The translated data is then used for Supervised Fine-Tuning (SFT) on a German-adapted Mistral base model, followed by Direct Preference Optimization (DPO) training using the translated Ultrafeedback dataset. This approach combines cost-effective translation with alignment techniques to create a high-performing German language model.

## Key Results
- Achieves average score of 5.78 on German mt-bench
- Outperforms Llama-2-70B-chat in Reasoning and Roleplay
- Shows improved results on German versions of HellaSwag, ARC, and MMLU benchmarks compared to LeoLM-Mistral-7B-Chat
- Reduces translation costs from over €30,000 to about €30 for 1.5 billion characters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PHOENIX achieves high performance in German language tasks by using translated datasets combined with Direct Preference Optimization (DPO)
- Mechanism: The model leverages translated instruction data and DPO to align with human preferences, improving task performance beyond what standard fine-tuning achieves
- Core assumption: Translated datasets maintain semantic fidelity and are suitable for training language models to perform well in German
- Evidence anchors:
  - [abstract] "The model achieves an average score of 5.78 on the German mt-bench, outperforming the Llama-2-70B-chat in Reasoning and Roleplay"
  - [section] "To improve the work of the LAION team [25] and to further promote the use of LLMs for the German language, we extend the approach pursued so far and optimize the Mistral [3] adaptation of the LAION team[25] by extended Supervised Finetuning and the use of DPO"
  - [corpus] Weak. Only mentions the existence of similar works but no direct evidence of translation quality or DPO effectiveness for German
- Break condition: If translated data introduces significant semantic drift or if DPO fails to properly align the model with German human preferences, performance will degrade

### Mechanism 2
- Claim: Using open-source translation tools significantly reduces the cost of creating multilingual datasets
- Mechanism: By employing the ALMA model and the vLLM library for translation, the cost of translating 1.5 billion characters is reduced from over €30,000 to about €30
- Core assumption: Open-source translation models can provide sufficient quality for training purposes at a fraction of the cost of commercial APIs
- Evidence anchors:
  - [section] "Due to advances in the quality of open-source translation models, such costly APIs are not the only option. That's why, for this project, the ALMA model from Microsoft[38], which has shown outstanding quality in translation tasks, was used to translate the data"
  - [section] "With this approach, the translation cost of these two datasets could be reduced to approximately 30 Euros, which is 100 times cheaper"
  - [corpus] Weak. While related papers exist, none provide direct evidence of cost savings or translation quality for LLM training datasets
- Break condition: If the translation quality is insufficient, the resulting model will not perform well, negating the cost benefits

### Mechanism 3
- Claim: PHOENIX's architecture allows it to compete with models 10 times larger by optimizing for efficiency and performance
- Mechanism: The model utilizes Grouped-Query Attention and Flash-Attention to reduce calculation time and memory overhead, enabling a 7B-parameter model to perform on par with much larger models
- Core assumption: Architectural improvements like Grouped-Query Attention and Flash-Attention provide significant performance gains without increasing model size
- Evidence anchors:
  - [abstract] "The model achieves an average score of 5.78 on the German mt-bench, outperforming the Llama-2-70B-chat in Reasoning and Roleplay"
  - [section] "Structural improvements such as Grouped-Query Attention [6] or Flash-Attention [7], [8] for example were able to drastically reduce both the calculation time and the memory overhead of the attention block"
  - [corpus] Weak. The corpus mentions related works but does not provide direct evidence of the specific architectural improvements used in PHOENIX
- Break condition: If the architectural optimizations do not provide the expected performance gains, the model will not be able to compete with larger models

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is used to align the model with human preferences without the need for a separate reward model, simplifying the training process and improving performance
  - Quick check question: What is the main advantage of using DPO over traditional RLHF methods?

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is used to adapt the base model to follow instructions and generate human-like responses before applying DPO for further alignment
  - Quick check question: How does SFT differ from unsupervised pre-training in terms of objectives and data requirements?

- Concept: Multilingual Model Adaptation
  - Why needed here: Adapting models to other languages is crucial for expanding their utility beyond English, and PHOENIX demonstrates this by achieving high performance in German tasks
  - Quick check question: What are the main challenges in adapting large language models to languages with different grammatical structures and vocabularies?

## Architecture Onboarding

- Component map: Base model (German-adapted Mistral 7B) -> SFT -> DPO -> Evaluation on German benchmarks
- Critical path: Translate datasets → SFT → DPO → Evaluate on German benchmarks
- Design tradeoffs: Balancing translation quality and cost, choosing between model size and performance, selecting appropriate hyperparameters for fine-tuning
- Failure signatures: Poor performance on German benchmarks, high translation costs, overfitting during fine-tuning, misalignment with human preferences
- First 3 experiments:
  1. Translate a small subset of the dataset using ALMA and evaluate the translation quality
  2. Perform SFT on the translated data and assess the model's ability to follow instructions in German
  3. Apply DPO to the SFT model and evaluate improvements in alignment with human preferences on a validation set

## Open Questions the Paper Calls Out
None

## Limitations

- The translation approach relies heavily on the quality of the open-source ALMA model, which is not independently verified in this work
- Performance comparison with larger models may be influenced by differences in base model architectures and training procedures
- The paper assumes semantic fidelity is maintained during translation without systematic evaluation of translation quality

## Confidence

**High Confidence**: The cost reduction claim from €30,000 to €30 for translating 1.5 billion characters using ALMA is supported by concrete calculations and is verifiable through the described methodology.

**Medium Confidence**: The claim that PHOENIX achieves "on par" performance with 7B-parameter models and competes with models 10 times larger is supported by benchmark results, but the comparison methodology and baseline conditions could be more rigorously documented.

**Low Confidence**: The assertion that the model "outperforms Llama-2-70B-chat in Reasoning and Roleplay" specifically requires careful interpretation, as the benchmarks used (mt-bench) may not fully capture the complexity of these tasks, and the comparison is based on a single aggregate metric rather than task-specific analyses.

## Next Checks

1. **Translation Quality Assessment**: Conduct human evaluation of a random sample of translated instruction-response pairs to verify semantic fidelity and cultural appropriateness for German language contexts.

2. **Cross-Lingual Benchmark Validation**: Test PHOENIX on English versions of the same benchmarks (when available) to isolate whether performance gains are due to genuine capability improvements versus language-specific optimization.

3. **Ablation Study on Training Components**: Remove either the SFT or DPO component and evaluate the impact on final performance to quantify the contribution of each training phase to the model's capabilities.