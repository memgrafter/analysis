---
ver: rpa2
title: Outlier-weighed Layerwise Sampling for LLM Fine-tuning
arxiv_id: '2405.18380'
source_url: https://arxiv.org/abs/2405.18380
tags:
- fine-tuning
- layers
- arxiv
- lisa
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Outlier-weighted Layerwise Sampling (OWS),
  a novel memory-efficient fine-tuning method for large language models (LLMs). OWS
  addresses two limitations in existing approaches: uniform layer sampling and full-rank
  updates of sampled layers.'
---

# Outlier-weighed Layerwise Sampling for LLM Fine-tuning

## Quick Facts
- arXiv ID: 2405.18380
- Source URL: https://arxiv.org/abs/2405.18380
- Authors: Pengxiang Li; Lu Yin; Xiaowei Gao; Shiwei Liu
- Reference count: 10
- Primary result: Up to 1.1% average accuracy gain on Commonsense Reasoning, 3.0% improvement on MMLU, and 10% boost on MT-Bench with only 21GB memory for 7B LLM fine-tuning

## Executive Summary
This paper introduces Outlier-weighted Layerwise Sampling (OWS), a novel memory-efficient fine-tuning method for large language models (LLMs). OWS addresses two limitations in existing approaches: uniform layer sampling and full-rank updates of sampled layers. The method assigns higher sampling probabilities to layers with more outliers (features/weights with significantly larger magnitudes), then updates these sampled layers using gradient low-rank projection. This design allows OWS to fine-tune more layers without proportional memory cost increases.

Extensive experiments on LLaMa2 and Mistral models show OWS consistently outperforms baseline methods including full fine-tuning. Key results include up to 1.1% average accuracy gain on Commonsense Reasoning, 3.0% improvement on MMLU, and 10% boost on MT-Bench. OWS also achieves superior memory efficiency, enabling 7B LLM fine-tuning with only 21GB of memory. The approach demonstrates strong generalization across various architectures and benchmarks while maintaining significant performance advantages over existing parameter-efficient fine-tuning methods.

## Method Summary
OWS is a memory-efficient fine-tuning method that combines outlier-weighted layer sampling with low-rank gradient updates. The method calculates outlier ratios for each layer using a threshold-based approach, maps these ratios to sampling probabilities, and then samples a subset of layers for fine-tuning at each iteration. For the sampled layers, gradients are projected into a low-rank subspace using SVD, reducing memory requirements while maintaining performance. The approach strategically prioritizes layers with more outliers, which are assumed to contain more task-relevant information, while the low-rank projection enables efficient updates without proportional memory increases.

## Key Results
- Up to 1.1% average accuracy gain on Commonsense Reasoning benchmark compared to baselines
- 3.0% improvement on MMLU benchmark and 10% boost on MT-Bench
- Achieves 7B LLM fine-tuning with only 21GB memory, demonstrating superior memory efficiency
- Consistently outperforms LISA, GaLore, LoRA, and full fine-tuning across multiple tasks and model architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Assigning higher sampling probabilities to layers with more outliers enables OWS to prioritize fine-tuning of layers containing more task-relevant information
- Mechanism: Layers with more outliers have received larger gradients during pretraining, indicating they encode more essential information and adapt more effectively to downstream tasks
- Core assumption: Outlier concentration correlates with layer importance for downstream task adaptation
- Evidence anchors:
  - [abstract] "OWS strategically assigns higher sampling probabilities to layers with more outliers"
  - [section] "Our motivation stems from the crucial role outliers play in preserving LLM performance"
  - [corpus] Weak evidence - no direct corpus support for outlier-importance correlation
- Break condition: If outlier distribution becomes uniform across layers or doesn't correlate with task-specific information content

### Mechanism 2
- Claim: Low-rank gradient projection enables memory-efficient fine-tuning of sampled layers while maintaining performance
- Mechanism: By projecting gradients into a low-rank subspace, OWS reduces optimizer state memory requirements while still capturing the most important gradient directions
- Core assumption: The most significant gradient directions can be effectively captured in a low-rank subspace
- Evidence anchors:
  - [abstract] "To further increase the number of fine-tuned layers without a proportional rise in memory costs, we incorporate gradient low-rank projection"
  - [section] "the gradient matrix is projected into a low-rank subspace using Singular Value Decomposition (SVD)"
  - [corpus] Weak evidence - GaLore reference exists but no specific OWS low-rank projection evidence
- Break condition: If important gradient information cannot be effectively captured in the chosen rank subspace

### Mechanism 3
- Claim: Non-uniform layer sampling outperforms uniform sampling by focusing resources on more important layers
- Mechanism: Instead of sampling layers uniformly (like LISA), OWS uses outlier-weighted probabilities to focus fine-tuning on layers that matter most
- Core assumption: Different layers have varying importance for fine-tuning performance
- Evidence anchors:
  - [section] "random sampling underperforms compared to a simple baseline—monotonic decreasing sampling probabilities"
  - [section] "LISA employs random sampling of layers for fine-tuning, which results in suboptimal performance"
  - [corpus] Moderate evidence - LISA comparison provides some support
- Break condition: If layer importance is uniform across all layers or if the outlier metric doesn't capture importance effectively

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: Used for projecting gradients into low-rank subspaces in OWS
  - Quick check question: What does SVD decompose a matrix into, and why is this useful for low-rank approximation?

- Concept: Importance sampling
  - Why needed here: The theoretical foundation for non-uniform layer sampling in OWS
  - Quick check question: How does importance sampling differ from uniform sampling in terms of variance reduction?

- Concept: Layerwise outlier detection
  - Why needed here: Core mechanism for determining which layers to sample more frequently
  - Quick check question: How is the outlier score Aij = ||Xj||2 · |Wij| calculated, and what does it represent?

## Architecture Onboarding

- Component map:
  Outlier ratio calculation module -> Sampling probability assignment module -> Training loop with layer selection -> Low-rank projection module -> Optimizer state updates

- Critical path:
  1. Calculate layerwise outlier distribution using Equation 2
  2. Map outlier ratios to sampling probabilities
  3. At each iteration, sample γ layers based on these probabilities
  4. For sampled layers, project gradients into r-dimensional subspace
  5. Update optimizer states in low-rank subspace
  6. Project updates back to original space

- Design tradeoffs:
  - Rank level (r) vs memory efficiency: Higher rank captures more information but uses more memory
  - Number of sampled layers (γ) vs performance: More layers improves performance but increases memory usage
  - Outlier threshold (τ) vs sampling accuracy: Different thresholds may capture different outlier patterns

- Failure signatures:
  - Poor performance despite high memory usage: Likely indicates rank level too low
  - Memory usage matches LISA despite sampling fewer layers: Possible issue with low-rank projection implementation
  - No improvement over LISA: May indicate outlier metric not capturing layer importance effectively

- First 3 experiments:
  1. Compare OWS vs LISA on a single task with varying γ (1, 2, 4) while keeping r=128 fixed
  2. Test different rank levels (32, 64, 128) with fixed γ=2 to find optimal memory-performance tradeoff
  3. Verify outlier-weighted sampling outperforms uniform sampling by implementing OWS with uniform probabilities as a baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical justification for the choice of τ = 13 in outlier detection, and how sensitive is OWS performance to this hyperparameter across different model architectures and datasets?
- Basis in paper: [explicit] The paper states "We empirically find τ = 13 consistently works well and choose it for all experiments in this paper" but does not provide theoretical justification for this choice.
- Why unresolved: The paper only provides empirical validation on specific models (LLaMa2, Mistral) and tasks, without exploring the sensitivity of τ across different architectures, dataset characteristics, or model scales.
- What evidence would resolve it: Systematic ablation studies varying τ across diverse model families, dataset types, and model sizes, coupled with theoretical analysis of how τ relates to outlier distributions in pre-trained weights.

### Open Question 2
- Question: How does OWS perform when applied to extremely large models (e.g., 70B+ parameters) where layerwise outlier distributions may differ significantly from smaller models?
- Basis in paper: [inferred] The paper only evaluates OWS on 7B and 13B models, with no experiments on larger-scale models despite LLMs rapidly increasing in size.
- Why unresolved: The outlier detection mechanism and layerwise sampling strategy may behave differently in extremely large models where the distribution of weights and gradients could have fundamentally different characteristics.
- What evidence would resolve it: Experiments applying OWS to models with 30B+ parameters, analysis of how outlier distributions scale with model size, and comparison of memory-performance trade-offs at different scales.

### Open Question 3
- Question: What is the optimal frequency for updating the low-rank gradient subspace, and how does this choice interact with different training dynamics and convergence properties?
- Basis in paper: [explicit] The paper states "We update the gradient subspace every 200 iterations to better capture the dynamic trajectory of fine-tuning" but does not explore alternative update frequencies or provide theoretical guidance.
- Why unresolved: The choice of subspace update frequency could significantly impact both training efficiency and final model performance, yet only one frequency is tested despite the dynamic nature of fine-tuning.
- What evidence would resolve it: Systematic experiments varying subspace update frequencies across different tasks and models, analysis of training dynamics with different update schedules, and theoretical characterization of how subspace drift affects convergence.

## Limitations
- The correlation between outlier concentration and layer importance for downstream tasks remains primarily theoretical with limited empirical validation
- The optimal choice of rank level (r=128) and outlier threshold (τ=13) appears somewhat arbitrary with no sensitivity analysis
- Memory efficiency comparisons focus on memory usage rather than training time, which could be affected by additional computational overhead

## Confidence

- High confidence: OWS consistently outperforms baseline methods on tested benchmarks (MMLU, MT-Bench, Commonsense Reasoning)
- Medium confidence: The memory efficiency claims and comparison with LISA are well-supported by the ablation studies
- Low confidence: The theoretical justification for outlier-based sampling as a proxy for layer importance lacks independent validation

## Next Checks

1. Conduct ablation studies varying the outlier threshold τ and rank level r to determine sensitivity and identify optimal hyperparameter ranges
2. Implement a synthetic benchmark where ground truth layer importance is known to validate whether outlier concentration correlates with actual importance
3. Compare training throughput (steps/second) between OWS and baselines to quantify computational overhead from outlier detection and SVD operations