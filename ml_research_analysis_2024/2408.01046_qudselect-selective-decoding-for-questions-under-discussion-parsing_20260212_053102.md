---
ver: rpa2
title: 'QUDSELECT: Selective Decoding for Questions Under Discussion Parsing'
arxiv_id: '2408.01046'
source_url: https://arxiv.org/abs/2408.01046
tags:
- answer
- question
- anchor
- sentence
- quds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of Question Under Discussion (QUD)
  parsing, a task that aims to reveal discourse relationships between sentences using
  implicit questions. The main problem is that previous QUD parsers, which use a pipeline
  approach (anchor detection followed by question generation), struggle to satisfy
  all theoretical criteria like answer compatibility, givenness, and anchor relevance.
---

# QUDSELECT: Selective Decoding for Questions Under Discussion Parsing

## Quick Facts
- arXiv ID: 2408.01046
- Source URL: https://arxiv.org/abs/2408.01046
- Authors: Ashima Suvarna; Xiao Liu; Tanmay Parekh; Kai-Wei Chang; Nanyun Peng
- Reference count: 14
- Primary result: 9% improvement in human evaluation and 4% improvement in automatic evaluation over state-of-the-art QUD parsers

## Executive Summary
This paper addresses the challenge of Question Under Discussion (QUD) parsing, which aims to reveal discourse relationships between sentences using implicit questions. Previous pipeline approaches that separately predict anchors and generate questions struggle to satisfy theoretical criteria like answer compatibility, givenness, and anchor relevance. The authors propose QUDSELECT, a joint-training framework that instruction-tunes models to simultaneously predict anchor sentences and generate associated questions. During inference, they sample multiple QUD candidates and select the best one using criteria scorers, achieving state-of-the-art performance on the DCQA dataset.

## Method Summary
QUDSELECT uses instruction-tuning to jointly train models for anchor sentence prediction and question generation. The approach leverages LLaMA2-7B, Mistral-7B, or GPT-4 as base models with LORA low-rank adaptation for parameter efficiency. During inference, the model employs selective decoding by sampling multiple anchor-question candidates using beam search, then scores each candidate using reference-free criteria scorers for answer compatibility (NLI-based), givenness (content word overlap), and anchor relevance (content word overlap with question focus). The highest-scoring candidate is selected as the final prediction.

## Key Results
- 9% improvement in human evaluation (67% directly answered questions, 78% questions with no unseen concepts, 68% fully grounded questions)
- 4% improvement in automatic evaluation (F1 scores for answer compatibility, givenness, anchor relevance)
- Performance improves with increased number of candidates, suggesting potential for further gains
- Outperforms state-of-the-art baselines on DCQA dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of anchor prediction and question generation via instruction-tuning improves performance by providing a holistic view of the task.
- Mechanism: Instead of using separate models for anchor selection and question generation, instruction-tuning allows the model to jointly predict both components given an answer sentence and context. This joint inference enables the model to better satisfy theoretical criteria like answer compatibility, givenness, and anchor relevance.
- Core assumption: Models with instruction-following capabilities can effectively learn to jointly predict anchors and questions when trained on appropriate data.
- Evidence anchors:
  - [abstract] "Using instruction-tuning, we train models to simultaneously predict the anchor sentence and generate the associated question."
  - [section] "Unlike previous works that use separate models for anchor prediction and question generation, we exploit the instruction following ability of LLMs to perform these two steps jointly"
- Break condition: If the instruction-tuning fails to capture the dependencies between anchor selection and question generation, or if the model architecture cannot effectively represent both tasks simultaneously.

### Mechanism 2
- Claim: Selective decoding with multiple candidate generation and criteria-based scoring improves QUD quality by explicitly incorporating theoretical criteria.
- Mechanism: During inference, the model samples multiple anchor-question pairs using beam search, then scores each candidate using criteria scorers for answer compatibility, givenness, and anchor relevance. The highest-scoring candidate is selected as the final prediction.
- Core assumption: Generating multiple candidates and selecting the best one based on criteria scores will yield better results than greedy decoding or single-pass generation.
- Evidence anchors:
  - [abstract] "To explicitly incorporate the criteria, we adopt a selective decoding strategy of sampling multiple QUD candidates during inference, followed by selecting the best one with criteria scorers."
  - [section] "We sample multiple anchor sentences and question candidates by selectively utilizing beam-search with a wide beam while decoding."
- Break condition: If the criteria scorers are poorly calibrated or if the sampling strategy fails to generate diverse enough candidates, the selection process may not improve results.

### Mechanism 3
- Claim: Using reference-free, training-free criteria scorers enables effective evaluation without requiring additional annotated data or reference QUDs.
- Mechanism: The paper implements criteria scorers using off-the-shelf models and simple heuristics. Answer compatibility is scored using an NLI model, givenness uses content word overlap, and anchor relevance also uses content word overlap with the question focus.
- Core assumption: Simple heuristics and off-the-shelf models can effectively approximate the theoretical criteria without requiring task-specific training.
- Evidence anchors:
  - [section] "We consider the three key principles of QUD as our criteria: answer-compatibility, givenness, and anchor relevance. We implement reference-free and training-free scorers for each of them."
  - [section] "Answer Compatibility: This criterion indicates that the question q should be answerable by the answer sentence si. We regard this as a natural language inference (NLI) task..."
- Break condition: If the heuristics fail to capture the nuances of the theoretical criteria, or if the off-the-shelf models perform poorly on the task.

## Foundational Learning

- Concept: Question Under Discussion (QUD) framework
  - Why needed here: The entire paper is built around this discourse framework that uses implicit questions to reveal relationships between sentences. Understanding QUD is essential for grasping the problem being solved.
  - Quick check question: What are the three theoretical criteria that QUD structures must satisfy according to the paper?

- Concept: Discourse parsing and coherence
  - Why needed here: QUD parsing is a specific type of discourse parsing task. Understanding how discourse structures work is crucial for understanding why QUD parsing matters and how it differs from other discourse analysis tasks.
  - Quick check question: How does the QUD framework differ from traditional discourse parsing approaches like RST (Rhetorical Structure Theory)?

- Concept: Instruction-tuning and few-shot learning
  - Why needed here: The paper's approach relies on instruction-tuning LLMs to jointly predict anchors and questions. Understanding how instruction-tuning works is essential for understanding the proposed method.
  - Quick check question: What is the key difference between instruction-tuning and traditional fine-tuning approaches?

## Architecture Onboarding

- Component map: Base LLM -> Instruction-tuning layer (with LORA) -> Selective decoding module -> Criteria scoring module -> Selection logic

- Critical path:
  1. Input: Context sentences + answer sentence
  2. Instruction-tuned model generates anchor and question
  3. Selective decoding generates multiple candidates
  4. Criteria scorers evaluate each candidate
  5. Best candidate is selected and output

- Design tradeoffs:
  - Joint vs. pipeline approach: Joint training provides holistic view but may be harder to train
  - Number of candidates (k): More candidates may improve quality but increase computation
  - Criteria scoring: Reference-free scoring avoids need for reference QUDs but may be less accurate
  - Model size: Larger models may perform better but increase inference cost

- Failure signatures:
  - Poor answer compatibility: Questions not answerable by given answer sentence
  - Low givenness: Questions contain concepts not present in context
  - Weak anchor relevance: Questions not well-grounded in anchor sentence
  - Hallucinations: Generated questions contain false or fabricated information

- First 3 experiments:
  1. Ablation study: Compare joint training vs. pipeline approach with same base model and decoding strategy
  2. Hyperparameter sweep: Vary number of candidates (k) to find optimal value for performance/quality tradeoff
  3. Criteria scorer analysis: Compare different scoring approaches (e.g., different NLI models, alternative givenness metrics) to identify most effective combination

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several unresolved issues emerge from the methodology and results.

## Limitations
- The instruction-tuning approach lacks detailed specification of prompt format and training methodology, making it difficult to assess whether improvements are due to joint architecture or better prompting
- The selective decoding mechanism relies on reference-free, training-free criteria scorers that may not fully capture theoretical criteria nuances
- Evaluation methodology has limitations including limited inter-annotator agreement statistics and potential crowdworker expertise variations

## Confidence
- High confidence: The core finding that joint training of anchor prediction and question generation outperforms pipeline approaches is well-supported by experimental results
- Medium confidence: The effectiveness of selective decoding with multiple candidate generation and criteria-based scoring is supported but depends on specific implementation details
- Low confidence: The claim that reference-free, training-free criteria scorers can effectively evaluate QUD quality without requiring additional annotated data is least well-supported

## Next Checks
1. **Prompt and Training Protocol Validation**: Conduct controlled experiments comparing joint training with carefully designed pipeline approaches using the same instruction-tuning methodology to isolate architectural benefits from prompting strategies.

2. **Criteria Scorer Ablation Study**: Systematically evaluate each criteria scorer's contribution by comparing full QUDSELECT against variants using different scorer combinations, alternative scoring methods, and varying numbers of candidates.

3. **Long-Form Context Evaluation**: Test QUDSELECT on longer documents or conversations to assess whether the method scales effectively beyond the news article domain used in the DCQA dataset, validating generalization to more complex discourse structures.