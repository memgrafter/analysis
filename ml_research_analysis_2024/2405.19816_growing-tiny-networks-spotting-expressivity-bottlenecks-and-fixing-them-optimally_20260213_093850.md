---
ver: rpa2
title: 'Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally'
arxiv_id: '2405.19816'
source_url: https://arxiv.org/abs/2405.19816
tags:
- neurons
- loss
- learning
- gradient
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TINY, a method for dynamically growing neural
  network architectures during training by detecting and resolving expressivity bottlenecks
  in real time. The core idea is to monitor the gap between the desired function updates
  (from backpropagation) and what can be achieved with the current architecture, then
  optimally add neurons to close this gap.
---

# Growing Tiny Networks: Spotting Expressivity Bottlenecks and Fixing Them Optimally

## Quick Facts
- arXiv ID: 2405.19816
- Source URL: https://arxiv.org/abs/2405.19816
- Reference count: 40
- Tiny networks match ResNet18 accuracy on CIFAR-100 while starting from very small architectures

## Executive Summary
This paper introduces TINY, a method for dynamically growing neural networks during training by detecting and resolving expressivity bottlenecks in real time. TINY monitors the gap between desired function updates (from backpropagation) and what can be achieved with the current architecture, then optimally adds neurons to close this gap. Starting from very small networks, TINY grows them layer-by-layer using first-order derivatives only, avoiding costly manual architecture search while matching the accuracy of large fixed architectures.

## Method Summary
TINY detects expressivity bottlenecks by computing the distance between the desired pre-activation update (vgoal) and the closest achievable update given the current architecture. It uses SVD-based low-rank approximation to optimally add neurons that reduce this gap, projecting vgoal onto the tangent space of achievable variations before adding neurons. This projection-based approach prevents redundancy and improves convergence compared to methods like GradMax that maximize gradient norm without considering existing expressivity.

## Key Results
- TINY matches ResNet18 accuracy on CIFAR-100 starting from 1/64 width architectures
- TINY significantly outperforms GradMax in final accuracy (65.8% vs 45.0% for s=1/64, ∆t=0.25)
- TINY achieves competitive training time while avoiding manual architecture search

## Why This Works (Mechanism)

### Mechanism 1
TINY improves expressivity by adding neurons that close the gap between the desired functional gradient and what the current architecture can achieve. During training, TINY monitors the expressivity bottleneck (Ψl) at each layer, which quantifies how far the desired pre-activation update (vgoal) is from the closest achievable one (v). It then optimally adds neurons via SVD-based low-rank approximation to reduce this gap.

### Mechanism 2
TINY achieves better final accuracy than GradMax by projecting the desired update onto the reachable function space before adding neurons. GradMax maximizes ||∇ΩL||² without considering existing expressivity, potentially adding redundant neurons. TINY first projects vgoal onto the tangent space Tal A to avoid redundancy, then finds optimal new neurons to fill the residual gap.

### Mechanism 3
TINY converges to zero training error in at most n neuron additions by greedily adding neurons that reduce the loss. Using full-batch protocol and updating existing weights optimally when adding new neurons, TINY ensures each added neuron decreases the loss. Under mild assumptions (polynomial activation of order ≥ n²), completed-TINY reaches zero training error.

## Foundational Learning

- Concept: Functional gradient descent vs. parametric gradient descent
  - Why needed here: TINY operates in the functional space, projecting desired gradients onto achievable function variations; understanding this distinction is key to grasping bottleneck detection.
  - Quick check question: What is the difference between ∇fL(f) and ∂f/∂θ E[∂f/∂θ^T vgoal] in terms of what space they operate in?

- Concept: Expressivity bottleneck and tangent space
  - Why needed here: TINY quantifies how far the desired update is from what the current architecture can express; this requires understanding the tangent space of achievable variations.
  - Quick check question: How is the expressivity bottleneck Ψl defined mathematically in terms of vgoal and the tangent space?

- Concept: Low-rank matrix approximation and SVD
  - Why needed here: TINY uses SVD to find the optimal neurons to add by approximating the residual gap in the pre-activation space.
  - Quick check question: Why does the SVD of S^{-1/2}N give the optimal neurons, and what do the singular values represent?

## Architecture Onboarding

- Component map: Forward pass -> compute activations al, bl -> Backward pass -> compute vgoal -> Bottleneck detection -> compute Ψl -> Neuron addition -> SVD of S^{-1/2}N -> Weight update -> Apply δW* and add neurons

- Critical path:
  1. Forward pass → compute al, bl
  2. Backward pass → compute vgoal
  3. For each layer l: compute S, N matrices from Bl-2 and Vgoal
  4. SVD of S^{-1/2}N → get K*, αk, ωk
  5. Update Wl with δW* and add neurons
  6. Repeat until convergence or max neurons

- Design tradeoffs:
  - Full batch vs. minibatch: Full batch ensures zero training error but is expensive; minibatch is faster but may not converge to zero
  - SVD rank vs. accuracy: Higher rank (more neurons) reduces bottleneck but increases parameters and cost
  - Projection onto Tal A vs. direct addition: Projection avoids redundancy but adds computational overhead

- Failure signatures:
  - Divergence: Loss increases after neuron addition → check SVD stability or line search
  - Stagnation: Ψl remains high → check if Tal A is too small or if vgoal is noisy
  - Overfitting: Training loss → 0 but test loss increases → consider regularization or early stopping

- First 3 experiments:
  1. Single-layer regression: Fit f(x) = 2 sin(x) + x with TINY starting from a linear model; verify loss decreases with each neuron
  2. CIFAR-100 tiny ResNet: Compare TINY vs. GradMax starting from ResNet1/64; measure final accuracy and training time
  3. Expressivity bottleneck ablation: Disable projection step in TINY and observe redundancy and accuracy drop vs. full TINY

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of activation function affect the invertibility of the activation matrix in TINY and its convergence properties? The paper mentions that the invertibility of the activation matrix A is crucial for TINY's convergence, and it depends on the activation function. It states that with probability 1 over function and neuron parameters, the activity matrix A is full rank for polynomial activation functions of order at least n². However, it also notes that piecewise-linear activation functions such as ReLU are not covered by this proposition. A comprehensive analysis of the invertibility of the activation matrix for various activation functions is needed.

### Open Question 2
What is the impact of the minibatch size on the statistical reliability and performance of TINY? The paper discusses the variance of the matrices M* and S^(-1/2)N computed using a minibatch of n samples, but does not provide a detailed analysis of the impact of the minibatch size on the statistical reliability and performance of TINY. A systematic study of how the minibatch size affects the convergence speed and final performance of TINY on various datasets and architectures is required.

### Open Question 3
How does TINY compare to other neural architecture search (NAS) methods in terms of computational efficiency and final performance? The paper only compares TINY to GradMax and mentions that it outperforms other NAS search methods in its category. A thorough comparison of TINY with a diverse set of NAS methods on multiple datasets and architectures, evaluating both computational efficiency and final performance, is needed.

## Limitations
- Theoretical convergence guarantees rely heavily on full-batch training and polynomial activation functions of order ≥ n², which are not realistic in practical deep learning
- Expressivity bottleneck quantification depends on accurate estimation of tangent spaces, which may be sensitive to noise in minibatch estimates
- Comparison uses specific hyperparameter settings that may not generalize across different architectures or datasets

## Confidence

- High confidence: The mechanism of detecting expressivity bottlenecks through tangent space projections and the basic formulation of TINY's optimization problem
- Medium confidence: The empirical results showing TINY outperforming GradMax, as these depend on specific implementation details not fully specified
- Low confidence: The theoretical guarantees of zero training error convergence under stated assumptions, given their disconnect from practical training scenarios

## Next Checks

1. Implement TINY and GradMax on CIFAR-10 with ResNet18 to verify if the accuracy advantage holds on a different dataset with the same architecture
2. Test TINY with ReLU activations instead of polynomial activations to assess practical convergence behavior and identify any failure modes
3. Analyze the sensitivity of TINY's performance to different minibatch sizes to quantify the gap between theoretical full-batch guarantees and practical minibatch training