---
ver: rpa2
title: Predicting drug-gene relations via analogy tasks with word embeddings
arxiv_id: '2406.00984'
source_url: https://arxiv.org/abs/2406.00984
tags:
- genes
- setting
- analogy
- embeddings
- relations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores using word embeddings trained on biomedical
  text to predict drug-gene relations through analogy computations. The core idea
  is to calculate vector differences between drug and gene embeddings to define relation
  vectors, which are then added to query drug embeddings to predict target genes.
---

# Predicting drug-gene relations via analogy tasks with word embeddings

## Quick Facts
- arXiv ID: 2406.00984
- Source URL: https://arxiv.org/abs/2406.00984
- Reference count: 40
- Primary result: Word embeddings trained on biomedical text can predict drug-gene relations through analogy computations with top-10 accuracy over 0.6 in global settings

## Executive Summary
This study explores using word embeddings trained on biomedical text to predict drug-gene relations through analogy computations. The core idea is to calculate vector differences between drug and gene embeddings to define relation vectors, which are then added to query drug embeddings to predict target genes. Experiments using BioConceptVec and custom-trained embeddings show top-10 accuracy of over 0.6 in global settings and over 0.8 in pathway-specific settings. The approach performs comparably to large language models like GPT-4 and outperforms random baselines. Year-split experiments demonstrate the ability to predict future unknown relations. Biological analysis reveals that predicted genes often share structural or functional similarities with known targets, suggesting the embeddings capture higher-order biological knowledge.

## Method Summary
The method trains skip-gram word embeddings on biomedical text (BioConceptVec or custom-trained on PubMed abstracts), then uses vector arithmetic to predict drug-gene relations. Relation vectors are computed as mean differences between drug and gene embeddings from known pairs. For a query drug, the relation vector is added to its embedding and genes are ranked by cosine similarity. The approach is evaluated in global and pathway-specific settings using KEGG pathway categorization, with performance measured by top-k accuracy and MRR on held-out data from AsuratDB.

## Key Results
- Top-10 accuracy exceeds 0.6 in global settings and 0.8 in pathway-specific settings
- Performance comparable to GPT-4 on drug-gene prediction tasks
- Outperforms random baseline and TransE methods
- Successfully predicts drug-gene relations from future years using temporal split experiments
- Predicted genes show structural/functional similarities to known targets

## Why This Works (Mechanism)

### Mechanism 1
Word embeddings trained on biomedical text contain latent structural and functional relationships between drugs and genes. Skip-gram models learn distributed representations by predicting surrounding words in biomedical abstracts. During this process, drug and gene terms that co-occur in similar biological contexts are mapped to nearby points in high-dimensional space. The model thus encodes higher-order biological knowledge—such as shared pathways, molecular functions, and structural similarities—into the embedding geometry without explicit labeling. Core assumption: Co-occurrence patterns in biomedical literature reflect underlying biological relationships strong enough to be captured by distributional semantics.

### Mechanism 2
Simple vector arithmetic on embeddings can solve analogy tasks for drug-gene relations by implicitly modeling relational semantics. The model computes relation vectors as differences between drug and gene embeddings (e.g., `ug - ud` for a known pair). These vectors encode the transformation from drug space to gene space. Adding this relation vector to a query drug embedding produces a point in embedding space that aligns closely with its target genes due to preserved relational geometry. Core assumption: Relational semantics are linearly representable in embedding space and can be approximated by averaging over multiple instances.

### Mechanism 3
Pathway-based categorization improves analogy task performance by reducing search space noise and increasing contextual specificity. By grouping drugs and genes into biological pathways, the model computes pathway-specific relation vectors (`vp`) rather than global ones. This confines analogy computation to a relevant subspace, reducing interference from unrelated drug-gene pairs and increasing precision. Core assumption: Biological pathways create coherent subspaces where drug-gene interactions are more predictable and less noisy than in the global embedding space.

## Foundational Learning

- Concept: Skip-gram word embeddings and distributional semantics
  - Why needed here: The entire method relies on vector representations learned by predicting surrounding words in text. Understanding how these embeddings capture semantic relationships is essential to grasp why analogy tasks work.
  - Quick check question: What is the key training objective of skip-gram models, and how does it lead to semantic clustering in embedding space?

- Concept: Cosine similarity and vector arithmetic in high-dimensional spaces
  - Why needed here: Analogy tasks are solved by computing cosine similarities after adding relation vectors. Understanding the geometric interpretation of these operations is crucial for interpreting results.
  - Quick check question: How does centering embeddings before computing cosine similarity affect the outcome, and why is it done here?

- Concept: Biological pathways and their role in organizing drug-gene interactions
  - Why needed here: The pathway-wise setting leverages predefined biological groupings to improve prediction. Knowing how pathways are defined and used in bioinformatics is necessary to understand the experimental design.
  - Quick check question: What is a KEGG pathway, and how are drugs and genes associated with it in this study?

## Architecture Onboarding

- Component map:
  Embedding Trainer -> Relation Vector Calculator -> Pathway Mapper -> Analogy Engine -> Evaluator

- Critical path:
  1. Load pre-trained or custom embeddings
  2. Map drug-gene relations from AsuratDB
  3. Compute relation vectors (v for global, vp for pathways)
  4. For each query drug, add relation vector and rank genes by similarity
  5. Evaluate predictions against ground truth

- Design tradeoffs:
  - Dimensionality vs. search space size: BioConceptVec uses 100D with ~28K genes; custom uses 300D with ~51K genes. Higher dimensions may capture more nuance but increase computation
  - Global vs. pathway vectors: Global vectors are simpler but noisier; pathway vectors are more precise but require pathway mapping
  - Static vs. contextual embeddings: Static embeddings are faster and easier to train but cannot handle polysemy or temporal shifts

- Failure signatures:
  - Low top-k accuracy across all settings → embeddings may not capture drug-gene semantics
  - High variance between pathway settings → pathway definitions may be noisy or incomplete
  - Performance drops sharply in year-split settings → embeddings may not generalize temporally

- First 3 experiments:
  1. Sanity check: Run analogy tasks on a small, manually curated drug-gene set to verify vector arithmetic works as expected
  2. Embedding comparison: Compare BioConceptVec vs. custom embeddings on a fixed set of queries to quantify performance differences
  3. Ablation study: Test global vs. pathway vectors on the same queries to measure impact of pathway categorization

## Open Questions the Paper Calls Out

### Open Question 1
Can analogy computation of word embeddings be extended to predict other types of biological relationships beyond drug-gene pairs, such as drug-drug interactions or protein-protein interactions? The authors note that BioConceptVec includes concepts such as diseases and mutations, and suggest that analogy computations using relation vectors could potentially be applied to relationships beyond drugs and genes. This remains untested as the study focuses exclusively on drug-gene relations.

### Open Question 2
How do the properties of drugs or genes (e.g., research history, number of known interactions) affect the performance of analogy tasks in predicting drug-gene relations? The authors mention that performance might be influenced by drug/gene properties and conduct a simple experiment examining correlation between answer set size and search result rank. While they observe a slight negative correlation in global settings, little to no correlation was observed in pathway settings, and deeper understanding of how specific properties affect performance is lacking.

### Open Question 3
How would the performance of analogy computation compare to TransE when using limited training data, and what does this imply about the scalability of each method? The authors compare analogy computation with TransE under varying proportions of training data and find that analogy computation maintains consistent performance regardless of training data size, while TransE's performance degrades with less data. While the study demonstrates that analogy computation outperforms TransE with limited data, the full implications for scalability and applicability to large-scale biological datasets are not explored.

## Limitations

- The assumption that co-occurrence patterns in biomedical literature reliably reflect true drug-gene interactions is not empirically validated
- Performance gains from pathway-specific settings could be influenced by pathway definition quality and completeness
- Temporal generalization results are promising but limited to a single year-split experiment

## Confidence

- High Confidence: Vector arithmetic on embeddings can predict drug-gene relations (supported by consistent performance across settings)
- Medium Confidence: Pathway categorization improves performance (supported by relative gains but lacks ablation baselines)
- Low Confidence: Embeddings capture higher-order biological knowledge (inferred from structural/functional similarities but not directly measured)

## Next Checks

1. Conduct controlled experiments comparing BioConceptVec to random embeddings to quantify the contribution of learned semantics vs. embedding space geometry
2. Perform ablation studies removing pathway categorization to isolate its contribution to performance gains
3. Validate predicted gene-drug pairs through external biological databases to confirm functional/structural similarity claims beyond statistical performance metrics