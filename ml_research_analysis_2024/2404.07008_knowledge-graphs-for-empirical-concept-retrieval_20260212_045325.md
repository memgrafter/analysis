---
ver: rpa2
title: Knowledge graphs for empirical concept retrieval
arxiv_id: '2404.07008'
source_url: https://arxiv.org/abs/2404.07008
tags:
- concept
- concepts
- knowledge
- data
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach to define empirical concepts for
  explainable AI using knowledge graphs (KGs) like WordNet, Wikidata, and ConceptNet.
  The authors develop an interactive workflow for users to define concepts based on
  KGs and automatically retrieve relevant data from Wikimedia Commons and Wikipedia.
---

# Knowledge graphs for empirical concept retrieval

## Quick Facts
- arXiv ID: 2404.07008
- Source URL: https://arxiv.org/abs/2404.07008
- Reference count: 40
- Key outcome: This paper presents an approach to define empirical concepts for explainable AI using knowledge graphs (KGs) like WordNet, Wikidata, and ConceptNet.

## Executive Summary
This paper introduces a method to define empirical concepts for explainable AI using knowledge graphs like WordNet, Wikidata, and ConceptNet. The authors develop an interactive workflow allowing users to define concepts based on KGs and automatically retrieve relevant data from Wikimedia Commons and Wikipedia. They evaluate their concept datasets on concept activation vectors (CAVs) and concept activation regions (CARs), demonstrating that these provide robust and accurate explanations. The study finds that model representations of concepts align well with KG structures, suggesting KGs are relevant for XAI.

## Method Summary
The authors propose an interactive workflow for defining empirical concepts using knowledge graphs. Users can select main concepts and navigate hierarchical relationships to find sub-concepts. The system automatically retrieves relevant data from Wikimedia Commons and Wikipedia based on KG definitions. CAVs and CARs are then trained on these concept datasets to evaluate accuracy, robustness, and alignment with human-defined concepts. The approach leverages multiple knowledge graphs (WordNet, Wikidata, ConceptNet) to ensure comprehensive concept coverage.

## Key Results
- CAVs and CARs trained on Wikimedia Commons data achieve comparable or better accuracy than those trained on Pascal VOC dataset
- High agreement between CAVs and CARs indicates robust explanations across different concept separability methods
- Sub-concepts show higher alignment in model representations than non-related concepts, suggesting human-machine conceptual alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graphs provide a structured, human-curated source for defining empirical concepts in explainable AI.
- Mechanism: By leveraging the hierarchical and semantic relationships in knowledge graphs like WordNet, Wikidata, and ConceptNet, concepts can be interactively defined and disambiguated, ensuring alignment with user intentions.
- Core assumption: Knowledge graphs contain accurate, comprehensive, and reliable semantic relationships that reflect human conceptual understanding.
- Evidence anchors:
  - [abstract] "knowledge graph-based concepts are relevant for XAI"
  - [section 3.1] "WordNet is a lexical database, which captures linguistic relations between words...curated by experts"
  - [corpus] Weak evidence - corpus shows related papers but no direct validation of KG accuracy
- Break condition: If knowledge graphs contain significant errors, outdated information, or fail to capture domain-specific concepts, the approach breaks down.

### Mechanism 2
- Claim: Concept activation vectors/regions (CAVs/CARs) trained on KG-derived datasets are robust and accurate.
- Mechanism: The diverse and comprehensive data retrieved from Wikimedia Commons and Wikipedia for KG-defined concepts leads to well-defined CAVs/CARs that generalize well across different models and distributions.
- Core assumption: Wikimedia and Wikipedia contain sufficient, diverse examples for each concept to enable effective CAV/CAR training.
- Evidence anchors:
  - [section 4.1] "CAVs and CARs trained on Wikimedia have a comparable accuracy on the test set, and even outperforming the CAVs and CARs trained on the Pascal VOC dataset"
  - [section 4.2] "high agreement between CAVs and CARs...in later layers...evidence of high robustness to varying negative sets"
  - [corpus] Weak evidence - corpus shows related papers but no direct validation of CAV/CAR performance
- Break condition: If the retrieved concept data is insufficient, noisy, or not representative, CAVs/CARs will be poorly defined and inaccurate.

### Mechanism 3
- Claim: Internal model representations align with human-defined knowledge graph structures, enabling human-machine alignment.
- Mechanism: By measuring cosine similarity between CAVs/CARs of related concepts and sub-concepts in the KG structure, we find that semantically related concepts have more similar representations in model internal layers.
- Core assumption: Deep learning models develop internal representations that reflect semantic relationships between concepts similar to human categorization.
- Evidence anchors:
  - [section 4.3] "sub-concepts are more aligned than non-related concepts...suggesting some inherent alignment"
  - [section 3.6] "concepts and sub-concepts defined by humans are aligned in the internal representations of the models"
  - [corpus] Weak evidence - corpus shows related papers but no direct validation of model-human alignment
- Break condition: If models learn representations that don't reflect semantic relationships, or if KG structures don't match human conceptual understanding, alignment breaks down.

## Foundational Learning

- Concept: Concept-based explainable AI
  - Why needed here: This paper builds on concept-based XAI methods like CAVs and CARs, extending them with KG-derived concepts
  - Quick check question: What is the key difference between CAVs and CARs in terms of concept separability assumptions?

- Concept: Knowledge graphs and their structure
  - Why needed here: Understanding how KGs like WordNet, Wikidata, and ConceptNet represent semantic relationships is crucial for the proposed workflow
  - Quick check question: How do hypernym-hyponym relations in WordNet differ from "subclass of" relations in Wikidata?

- Concept: Wikimedia Commons and Wikipedia as data sources
  - Why needed here: The paper relies on these resources for automated concept data retrieval based on KG definitions
  - Quick check question: What advantages does Wikimedia Commons offer over labeled datasets like Pascal VOC for concept data retrieval?

## Architecture Onboarding

- Component map:
  KG extraction module (WordNet/Wikidata/ConceptNet APIs) -> Interactive concept definition interface -> Wikimedia Commons/Wikipedia data retrieval module -> CAV/CAR training and evaluation module -> Model alignment analysis module

- Critical path:
  1. Define main concept using KG extraction
  2. Navigate hierarchy to find sub-concepts
  3. Retrieve concept data from Wikimedia/Wikipedia
  4. Train CAVs/CARs on retrieved data
  5. Evaluate accuracy, robustness, and alignment

- Design tradeoffs:
  - Using multiple KGs provides broader coverage but increases complexity
  - Interactive concept definition ensures alignment but requires user input
  - Wikimedia data retrieval is automated but may introduce noise

- Failure signatures:
  - Low CAV/CAR accuracy indicates poorly defined concepts or insufficient data
  - Poor alignment between related concepts suggests model representations don't capture semantic relationships
  - High sensitivity to negative set variations indicates concept data issues

- First 3 experiments:
  1. Implement KG extraction for a simple concept (e.g., "dog") and visualize the hierarchy
  2. Retrieve concept data for the extracted KG and train basic CAVs
  3. Compare CAVs trained on KG-derived data vs. labeled data for a simple classification task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the characteristics of different knowledge graphs (e.g., Wikidata vs. WordNet) impact the quality and interpretability of concept activation vectors/regions in XAI?
- Basis in paper: [explicit] The paper compares WordNet and Wikidata for concept definition and mentions ConceptNet as an alternative, but does not provide a detailed comparative analysis of how different KG characteristics affect CAV/CAR performance.
- Why unresolved: The paper focuses on demonstrating the feasibility of using KGs for concept definition, but does not systematically investigate how KG structure, coverage, or update frequency impact the resulting explanations.
- What evidence would resolve it: A controlled study comparing CAV/CAR performance across multiple KGs with varying characteristics (e.g., semantic richness, update frequency, domain specificity) would clarify their relative strengths and weaknesses for XAI applications.

### Open Question 2
- Question: What is the optimal balance between concept granularity and explanation accuracy in concept-based XAI methods?
- Basis in paper: [inferred] The paper demonstrates that concepts and sub-concepts defined by KGs share similar CAV/CAR representations, suggesting hierarchical concept structures could be beneficial. However, it does not investigate how varying levels of concept granularity affect explanation quality.
- Why unresolved: The paper shows alignment between human and machine representations but does not explore the trade-offs between more specific vs. more general concepts in terms of explanation accuracy and interpretability.
- What evidence would resolve it: Systematic experiments varying concept granularity (e.g., comparing CAV/CAR performance for broad vs. specific concepts) would reveal the optimal balance for different XAI applications.

### Open Question 3
- Question: How can concept-based XAI methods be made more robust to distribution shifts while maintaining interpretability?
- Basis in paper: [explicit] The paper demonstrates that CAVs/CARs trained on Wikimedia data maintain reasonable accuracy on Pascal VOC data, but notes that different datasets can lead to varying explanations.
- Why unresolved: While the paper shows some robustness to distribution shifts, it does not provide a systematic framework for ensuring consistent explanations across different data distributions while preserving the interpretability benefits of concept-based approaches.
- What evidence would resolve it: Developing and validating techniques that explicitly account for domain shift in CAV/CAR training (e.g., domain adaptation methods) while maintaining concept interpretability would address this challenge.

## Limitations
- The approach relies heavily on the quality and coverage of knowledge graphs, which may not capture all domain-specific concepts accurately
- Automated data retrieval from Wikimedia Commons and Wikipedia introduces potential noise that could affect CAV/CAR quality
- The alignment analysis between model representations and KG structures requires further validation across diverse model architectures and datasets

## Confidence
- High confidence: CAV/CAR accuracy when trained on KG-derived datasets, particularly for broad, well-represented concepts
- Medium confidence: Robustness across different negative sets and model architectures, given limited testing scope
- Low confidence: Claims about human-machine alignment based on cosine similarity, requiring more rigorous statistical validation

## Next Checks
1. Test CAV/CAR performance on domain-specific concepts not well-represented in general knowledge graphs to assess KG limitations
2. Conduct ablation studies removing either Wikimedia Commons or Wikipedia data to quantify their individual contributions
3. Apply the approach to diverse model architectures (transformers, RNNs) and datasets beyond ImageNet to verify generalizability of human-machine alignment findings