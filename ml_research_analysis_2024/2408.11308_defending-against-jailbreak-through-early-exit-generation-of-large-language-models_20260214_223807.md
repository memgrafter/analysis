---
ver: rpa2
title: Defending against Jailbreak through Early Exit Generation of Large Language
  Models
arxiv_id: '2408.11308'
source_url: https://arxiv.org/abs/2408.11308
tags: []
core_contribution: EEG-Defender defends against jailbreak attacks by leveraging the
  observation that jailbreak prompts are more similar to harmful prompts in the early
  and middle layers of large language models, but shift toward benign prompts in later
  layers. It trains prototype classifiers using benign and rejected harmful prompts
  at each layer, then computes a harmfulness score from early layers to decide whether
  to refuse generation before producing the first token.
---

# Defending against Jailbreak through Early Exit Generation of Large Language Models

## Quick Facts
- arXiv ID: 2408.11308
- Source URL: https://arxiv.org/abs/2408.11308
- Authors: Chongwen Zhao; Zhihao Dou; Kaizhu Huang
- Reference count: 30
- Primary result: Reduces jailbreak attack success rates by ~85% while maintaining high utility for benign prompts

## Executive Summary
EEG-Defender is a novel approach for defending large language models against jailbreak attacks by leveraging early transformer layer embeddings. The method exploits the observation that jailbreak prompts are more similar to harmful prompts in early and middle layers, but shift toward benign prompts in later layers. By training prototype classifiers on embeddings from each layer and computing harmfulness scores before the first token is generated, EEG-Defender can refuse harmful responses with minimal computational overhead and no fine-tuning required. The approach achieves state-of-the-art performance, reducing attack success rates by roughly 85% compared to ~50% for existing decoding-based defenses, while maintaining benign answering rates around 90%.

## Method Summary
EEG-Defender trains prototype classifiers using benign and rejected harmful prompts at each transformer layer. For a given user prompt, it extracts embeddings from early layers (up to α×n, where α=0.75 and n is total layers), computes cosine distances to prototype centers for benign and harmful classes, and accumulates a harmfulness score. If this score exceeds a threshold t, the system refuses to generate a response before the first token is produced. The method requires no fine-tuning of the original LLM and adds near-zero computational overhead by leveraging existing model computations. Prototypes are trained on a small number of rejected harmful prompts (384 from toxic-chat dataset) and benign prompts (60 from Alpaca Eval), then tested across three models and ten jailbreak methods.

## Key Results
- Reduces Attack Success Rate by approximately 85% compared to 50% for existing decoding-based defenses
- Maintains Benign Answering Rate around 90% across all tested models
- Achieves near-zero computational overhead with Additional Operation Ratio (AOR) - Rejection Rate (RR) ≈ 0.83
- Prototype classifiers maintain over 80% accuracy in distinguishing jailbreak prompts up to the twelfth layer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early transformer outputs contain more discriminative features for detecting jailbreak prompts compared to later outputs.
- Mechanism: Jailbreak prompts have embeddings in early and middle layers that are more similar to harmful prompts than benign prompts. As layers progress, jailbreak embeddings shift toward benign embeddings, making detection harder in later layers.
- Core assumption: The internal representation of jailbreak prompts is inherently more similar to harmful prompts in shallow layers due to shared semantic patterns.
- Evidence anchors:
  - [abstract]: "We identify that while jailbreaking prompts may yield output logits similar to benign prompts, their initial embeddings within the model's latent space tend to be more analogous to those of malicious prompts."
  - [section]: "We found that in the early and middle layer latent space, jailbreak prompts (black dots) are more closer to harmful prompts (red dots) than to benign prompts (blue dots)."
- Break condition: If jailbreak prompts are engineered to specifically mimic benign embeddings in early layers, the discriminative power would be reduced.

### Mechanism 2
- Claim: Using early exit generation based on prototype classifiers achieves high accuracy with minimal computational overhead.
- Mechanism: By training prototype classifiers on rejected harmful and benign prompts at each layer, EEG-Defender can classify jailbreak prompts with high accuracy (over 80%) in early layers, allowing for early termination before generating harmful content.
- Core assumption: Prototype classifiers trained on a small number of rejected harmful and benign prompts can generalize well to detect jailbreak prompts across different models.
- Evidence anchors:
  - [abstract]: "EEG-Defender is capable of reducing the Attack Success Rate (ASR) by a significant margin, roughly 85% in comparison with 50% for the present SOTAs, with minimal impact on the utility and effectiveness of LLMs."
  - [section]: "classifiers collected from the early layers perform much better than those from the later layers. The accuracy in distinguishing jailbreak prompts exceed 80% for both models up to the twelfth layer."
- Break condition: If the distribution of jailbreak prompts shifts significantly from the training distribution, classifier accuracy would degrade.

### Mechanism 3
- Claim: EEG-Defender achieves near-zero computational overhead by leveraging existing model computations.
- Mechanism: The framework calculates harmfulness scores using embeddings from early layers before generating the first token, allowing immediate termination without additional model inference.
- Core assumption: The computational cost of extracting embeddings from early layers is negligible compared to full generation.
- Evidence anchors:
  - [abstract]: "EEG-Defender requires no fine-tuning of the original LLM and incurs minimal additional computational cost compared to existing defense methods, making it seamlessly integrable into current workflows."
  - [section]: "We estimate that our method introduces only a near-zero additional computational burden to the original LLM."
- Break condition: If the harmfulness score calculation becomes a bottleneck due to extremely high-dimensional embeddings or frequent early exits, computational benefits diminish.

## Foundational Learning

- Concept: Prototype-based classification
  - Why needed here: EEG-Defender uses prototype classifiers to distinguish between benign and harmful prompts based on their embeddings at each layer.
  - Quick check question: What is the main advantage of using prototype classifiers over traditional classifiers in this context?

- Concept: Layer-wise analysis of LLM representations
  - Why needed here: Understanding how jailbreak prompts are represented differently across transformer layers is crucial for the defense mechanism.
  - Quick check question: Why do jailbreak prompts shift from being similar to harmful prompts to being similar to benign prompts as layers progress?

- Concept: Early exit mechanisms in LLMs
  - Why needed here: EEG-Defender implements early exit generation to terminate harmful responses before they are fully generated.
  - Quick check question: How does early exit generation improve both safety and computational efficiency compared to post-generation filtering?

## Architecture Onboarding

- Component map:
  Input layer -> Layer embedding extractor -> Prototype classifier -> Harmfulness score accumulator -> Decision module -> Original LLM

- Critical path:
  1. User prompt enters EEG-Defender
  2. Embeddings extracted from early layers (up to α×n)
  3. Prototype classifiers compute distances to benign and harmful centers
  4. Harmfulness score accumulates positive classifications
  5. If score exceeds threshold t, refuse response; otherwise, pass to LLM
  6. LLM generates response

- Design tradeoffs:
  - Early layer usage (α) vs. accuracy: Higher α improves detection but increases computational cost
  - Harmfulness score threshold (t) vs. false positives: Lower t increases safety but may reduce utility
  - Prototype selection vs. generalization: Including more diverse rejected prompts improves robustness but increases training complexity

- Failure signatures:
  - High false positive rate: Indicates threshold t is too low or prototype centers are not well-separated
  - Low detection rate: Suggests early layers lack discriminative features or α is too small
  - Computational overhead: Implies excessive early layer usage or inefficient embedding extraction

- First 3 experiments:
  1. Vary α parameter to find optimal balance between detection accuracy and computational cost
  2. Test transferability of prototypes across different LLM architectures
  3. Evaluate robustness against unseen jailbreak attack methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EEG-Defender perform against multi-turn jailbreak attacks compared to single-turn attacks?
- Basis in paper: [explicit] The authors note that their work primarily focuses on existing single-turn jailbreak attack methods and mention that multi-turn jailbreak attacks may become more prevalent in the future, which they have not yet evaluated.
- Why unresolved: The paper does not provide experimental results or analysis on multi-turn jailbreak attacks, leaving a gap in understanding how the defense mechanism would perform in more complex conversational scenarios.
- What evidence would resolve it: Conducting experiments with multi-turn jailbreak attacks on EEG-Defender and comparing the results with single-turn attacks would provide insights into its effectiveness in more dynamic and extended conversations.

### Open Question 2
- Question: What is the impact of EEG-Defender on the performance of Multi-Modal LLMs (MLLMs)?
- Basis in paper: [explicit] The authors acknowledge that existing defending methods for MLLMs are inadequate and suggest that future work could focus on developing defense mechanisms for these models.
- Why unresolved: The paper does not explore or test EEG-Defender on MLLMs, leaving uncertainty about its applicability and effectiveness in handling multimodal inputs and outputs.
- What evidence would resolve it: Testing EEG-Defender on various MLLMs and evaluating its performance in detecting and preventing jailbreak attacks in multimodal contexts would clarify its potential and limitations.

### Open Question 3
- Question: How does EEG-Defender's performance vary with different hyperparameters, such as the harmfulness score threshold (t) and the early layer ratio (α)?
- Basis in paper: [explicit] The authors analyze the sensitivity of these hyperparameters in Section 5.3, but the analysis is limited to a specific model (Vicuna) and does not explore the full range of possible values or their impact across different models.
- Why unresolved: The paper provides a partial understanding of hyperparameter sensitivity but does not offer a comprehensive analysis across various models and settings, which could affect the generalizability of the results.
- What evidence would resolve it: Conducting a systematic hyperparameter study across multiple models and attack methods would provide a clearer picture of how these parameters influence EEG-Defender's performance and robustness.

## Limitations

- Layer-wise embedding stability remains empirically unverified beyond three tested architectures
- Prototype classifier generalization to evolved jailbreak prompts not rigorously tested
- Threshold selection appears somewhat arbitrary without systematic sensitivity analysis

## Confidence

**High confidence**: The layer-wise analysis showing jailbreak prompts are more similar to harmful prompts in early layers is well-supported by evidence. Computational efficiency claims are straightforward to verify.

**Medium confidence**: The 85% ASR reduction claim is based on comprehensive testing but absolute numbers vary significantly by model and attack type. Utility maintenance (BAR ≈ 90%) is encouraging but would benefit from broader testing.

**Low confidence**: Claims about transferability to unseen attack methods and robustness against adaptive adversaries are not empirically validated. The paper doesn't address potential evasion strategies.

## Next Checks

1. **Cross-architecture transferability test**: Evaluate EEG-Defender's prototypes trained on Vicuna embeddings against jailbreak detection on completely different model families (e.g., GPT-based models, Claude, or open-source models with different architectures).

2. **Adaptive adversary evaluation**: Design a jailbreak attack specifically engineered to maximize cosine similarity to benign prototypes in early layers while maintaining harmful semantics. Test whether attackers can systematically reduce detection accuracy by 20% or more.

3. **Threshold optimization study**: Systematically vary the harmfulness score threshold t across a wide range (t=5 to t=20) and measure the resulting precision-recall tradeoff. Identify the optimal operating point for different deployment scenarios.