---
ver: rpa2
title: Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)
arxiv_id: '2403.09680'
source_url: https://arxiv.org/abs/2403.09680
tags:
- tsetlin
- machine
- class
- training
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a pre-sorting method for Tsetlin Machines using
  genetic algorithms and K-Medoid clustering to improve accuracy and drastically reduce
  training and inference times. The method identifies K maximally dispersed datapoints
  per class, clusters remaining data, and aligns clusters to maximize inter-class
  Hamming distance before distributing to independent Tsetlin Machines.
---

# Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)

## Quick Facts
- arXiv ID: 2403.09680
- Source URL: https://arxiv.org/abs/2403.09680
- Reference count: 6
- Up to 10% accuracy improvement and 383X training time reduction on MNIST-style datasets

## Executive Summary
This paper presents a pre-sorting method for Tsetlin Machines that uses genetic algorithms and K-Medoid clustering to significantly improve accuracy and reduce training and inference times. The method identifies K maximally dispersed datapoints per class, clusters remaining data around them, and aligns clusters to maximize inter-class Hamming distance before distributing to independent Tsetlin Machines. Tested on MNIST-style datasets, the approach achieves up to 10% accuracy improvement and reduces training time by up to 383X and inference time by up to 99X compared to baseline Tsetlin Machines.

## Method Summary
The pre-sorted Tsetlin Machine uses a three-stage pipeline: (1) genetic algorithm for maximum dispersion to find K maximally dispersed datapoints per class, (2) K-Medoid clustering to form clusters around these initial medoids, and (3) genetic algorithm for class alignment to maximize inter-class Hamming distance. The resulting clusters are then distributed to K independent Tsetlin Machines, each learning a simpler, more homogeneous subset of the data. This approach reduces the effective problem complexity for each machine while maintaining or improving overall accuracy.

## Key Results
- Up to 10% accuracy improvement compared to baseline Tsetlin Machines
- Up to 383X reduction in training time
- Up to 99X reduction in inference time
- Particularly effective for datasets with class overlap or translation variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-sorting datasets into clusters reduces the effective problem complexity for each Tsetlin Machine.
- Mechanism: The pre-sort architecture identifies K maximally dispersed datapoints per class and clusters the remaining data around them. Each cluster is then assigned to a separate Tsetlin Machine. This means each machine only needs to learn patterns for a simpler, more homogeneous subset of the data.
- Core assumption: Tsetlin Machines are more effective when trained on less complex, more homogeneous datasets.
- Evidence anchors:
  - [abstract] "Tested on MNIST-style datasets, the approach achieves up to 10% accuracy improvement and reduces training time by up to 383X and inference time by up to 99X compared to baseline Tsetlin Machines."
  - [section] "Another reason is that having pre-simplified the problem, the test accuracy reached 100% for some pre-sorted Tsetlin Machines after only a few epochs."
  - [corpus] Weak - no direct supporting evidence found in corpus neighbors.
- Break condition: If clustering does not significantly reduce intra-class variance or if inter-class variance remains high within clusters, the simplification benefit diminishes.

### Mechanism 2
- Claim: Aligning clusters to maximize inter-class Hamming distance improves classification accuracy.
- Mechanism: After clustering, an expedited genetic algorithm is used to align clusters such that each vertical stack in the alignment matrix has the maximum possible Hamming distance between classes. This increases the distinctiveness of the sub-patterns each Tsetlin Machine needs to learn.
- Core assumption: Tsetlin Machines perform better when the sub-patterns to be learned are more distinct between classes.
- Evidence anchors:
  - [section] "The hamming distances between the vertically aligned medoids have now significantly increased. This leads to more unique sub-patterns that can be learned to differentiate between Medoids A and D and Medoids B and C, making the task of the Testlin Machine easier."
  - [abstract] "Finally, an expedited genetic algorithm is used to align K independent Tsetlin Machines by maximising hamming distance."
  - [corpus] Weak - no direct supporting evidence found in corpus neighbors.
- Break condition: If the genetic algorithm fails to find a good alignment or if the Hamming distances do not significantly increase, the accuracy improvement may not materialize.

### Mechanism 3
- Claim: Reducing the number of clauses per Tsetlin Machine by a factor of K leads to significant speedups.
- Mechanism: In the baseline Tsetlin Machine, 4000 clauses are updated for every datapoint passed into a class. In the pre-sorted approach, only 1/K * 4000 clauses are updated on average per datapoint, as each datapoint is distributed to one of K smaller Tsetlin Machines.
- Core assumption: The computational complexity of Tsetlin Machines scales linearly with the number of clauses.
- Evidence anchors:
  - [section] "Principally, fewer clauses are run per training cycle. This is because in the baseline Tsetlin Machine, 4000 clauses are updated for every datapoint passed into a class. In pre-sorted Tsetlin Machine, only 1/K * 4000 clauses are updated on average per datapoint."
  - [abstract] "results demonstrate up to 10% improvement in accuracy, ~383X reduction in training time and ~99X reduction in inference time."
  - [corpus] Weak - no direct supporting evidence found in corpus neighbors.
- Break condition: If the distribution of datapoints to Tsetlin Machines is highly uneven, some machines may still process a large number of clauses, reducing the overall speedup.

## Foundational Learning

- Concept: Tsetlin Machine Architecture
  - Why needed here: Understanding the baseline Tsetlin Machine is crucial to grasp how the pre-sort architecture improves upon it.
  - Quick check question: What are the two main components of a Tsetlin Machine and how do they interact?

- Concept: Hamming Distance
  - Why needed here: Hamming distance is the core metric used for clustering and alignment in the pre-sort architecture.
  - Quick check question: How is Hamming distance calculated between two binary strings?

- Concept: Genetic Algorithms
  - Why needed here: Genetic algorithms are used to solve the maximum dispersion problem and align clusters in the pre-sort architecture.
  - Quick check question: What are the three main operators used in a genetic algorithm?

## Architecture Onboarding

- Component map:
  Binary Maximum Dispersion -> Binary K-Medoid Clustering -> Genetic Class Alignment -> Distributed Tsetlin Machines

- Critical path:
  1. Binarize the input dataset.
  2. Run Binary Maximum Dispersion to find initial medoids.
  3. Run Binary K-Medoid Clustering to form clusters.
  4. Run Genetic Class Alignment to align clusters.
  5. Distribute clusters to K independent Tsetlin Machines.
  6. Train and infer using the distributed Tsetlin Machines.

- Design tradeoffs:
  - Choosing K: Larger K leads to more speedup but may reduce accuracy if clusters become too small.
  - Expedited genetic algorithms: Faster than exhaustive search but may not find the global optimum.
  - Clause distribution: Equal distribution vs. proportionate to cluster size.

- Failure signatures:
  - Accuracy drops significantly compared to baseline: Possible issues with clustering or alignment.
  - Training/inference times do not improve: Possible issues with clause distribution or parallelization.
  - Memory usage increases: Possible issues with storing intermediate results or managing multiple Tsetlin Machines.

- First 3 experiments:
  1. Test the Binary Maximum Dispersion stage on a small, simple dataset to verify it correctly identifies K maximally dispersed datapoints.
  2. Test the Binary K-Medoid Clustering stage to verify it forms meaningful clusters around the initial medoids.
  3. Test the Genetic Class Alignment stage on a small set of clusters to verify it maximizes inter-class Hamming distance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of accuracy improvement when using pre-sorting with Tsetlin Machines for datasets with high class overlap?
- Basis in paper: [explicit] The paper notes pre-sorted Tsetlin Machines handle class-overlap by separating dissimilar symbols into independent machines, achieving up to 10% accuracy improvement on datasets like KMNIST
- Why unresolved: The paper demonstrates empirical improvements but doesn't establish theoretical bounds or analyze how much class overlap can be mitigated through pre-sorting alone
- What evidence would resolve it: Systematic experiments varying degrees of class overlap across multiple datasets with known ground truth symbols, comparing baseline vs pre-sorted accuracy to establish performance ceilings

### Open Question 2
- Question: How does the pre-sorting method scale to larger datasets (e.g., ImageNet-level) in terms of computational overhead and memory requirements?
- Basis in paper: [inferred] The paper tests on MNIST-style datasets and mentions parallelization potential, but doesn't explore scaling beyond small image datasets or analyze memory footprint reduction in detail
- Why unresolved: Current experiments are limited to 28x28 pixel datasets, and while hardware implications are discussed, the computational complexity of pre-sorting for high-resolution images or large-scale datasets remains unexplored
- What evidence would resolve it: Implementation and benchmarking of the complete pre-sorting pipeline on larger datasets, measuring total computation time including pre-sorting overhead and peak memory usage

### Open Question 3
- Question: What is the optimal number of medoids (K) for a given dataset, and how sensitive is performance to this parameter?
- Basis in paper: [explicit] The paper tests K values of 2, 4, 8, 16, 32 but doesn't provide guidance on selecting K or analyze performance sensitivity to this parameter
- Why unresolved: The experiments show performance varies with K, but there's no systematic analysis of the trade-offs between accuracy gains, training time reduction, and computational overhead as K changes
- What evidence would resolve it: Comprehensive parameter sweep across a wide range of K values for multiple datasets, including statistical analysis of performance variance and identification of diminishing returns thresholds

### Open Question 4
- Question: How does the pre-sorting methodology perform with non-binarized data or different binarization thresholds?
- Basis in paper: [inferred] The paper uses simple thresholding for binarization but doesn't explore alternative methods or evaluate pre-sorting on raw continuous-valued data
- Why unresolved: Current results are based on a specific binarization approach, but the robustness of the pre-sorting methodology to different feature representations remains untested
- What evidence would resolve it: Experiments comparing pre-sorted Tsetlin Machines using different binarization methods (adaptive thresholding, histogram-based, etc.) or directly on continuous features with appropriate distance metrics

## Limitations

- The "expedited" genetic algorithms lack specific parameter definitions, making exact reproduction challenging
- Claims about 383X training speedup and 99X inference speedup depend on implementation details not fully specified
- Methodology effectiveness may vary significantly depending on dataset characteristics and choice of K parameter

## Confidence

- Accuracy improvement claims: **Medium** - supported by experimental results on MNIST-style datasets but limited to specific dataset types
- Training/inference time reduction claims: **Medium** - theoretically sound but dependent on implementation details not fully specified
- Mechanism explanations: **High** - the three proposed mechanisms (problem simplification, Hamming distance alignment, clause reduction) are logically coherent and supported by the described methodology

## Next Checks

1. Implement the pre-sort pipeline with varying K values (2, 4, 8, 16) on the four datasets and measure accuracy, training time, and inference time at each K to identify the optimal tradeoff point.
2. Conduct ablation studies removing each stage of the pre-sort pipeline (maximum dispersion, K-Medoid clustering, genetic alignment) to quantify the contribution of each component to the final performance.
3. Test the methodology on non-MNIST style datasets (e.g., CIFAR-10 binarized) to evaluate generalizability beyond the reported dataset family.