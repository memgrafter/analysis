---
ver: rpa2
title: Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning
  Tasks
arxiv_id: '2410.11005'
source_url: https://arxiv.org/abs/2410.11005
tags:
- arxiv
- language
- reasoning
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first systematic evaluation of Large Language
  Models'' (LLMs) fairness and robustness in reasoning tasks when queries are expressed
  in African American Vernacular English (AAVE) rather than Standardized English (SE).
  The authors introduce ReDial, a high-quality human-annotated dataset containing
  over 1,200 parallel SE-AAVE query pairs across four reasoning categories: algorithm,
  math, logic, and integrated reasoning.'
---

# Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks

## Quick Facts
- arXiv ID: 2410.11005
- Source URL: https://arxiv.org/abs/2410.11005
- Authors: Fangru Lin; Shaoguang Mao; Emanuele La Malfa; Valentin Hofmann; Adrian de Wynter; Xun Wang; Si-Qing Chen; Michael Wooldridge; Janet B. Pierrehumbert; Furu Wei
- Reference count: 37
- Key outcome: LLMs show 5-11.5% performance drops on AAVE vs SE reasoning prompts

## Executive Summary
This paper presents the first systematic evaluation of Large Language Models' fairness and robustness when processing African American Vernacular English (AAVE) versus Standardized English (SE) in reasoning tasks. The authors introduce ReDial, a human-annotated dataset containing over 1,200 parallel SE-AAVE query pairs across four reasoning categories. Experiments with 13 widely used LLMs reveal statistically significant performance drops of 5-11.5% on AAVE prompts compared to SE counterparts, even with advanced prompting techniques like Chain-of-Thought.

## Method Summary
The study employs human AAVE speakers to rewrite prompts from seven popular benchmarks (HumanEval, GSM8K, etc.) while preserving semantic meaning, creating the ReDial dataset. The evaluation framework tests 13 widely used LLMs using both direct and Chain-of-Thought prompting, measuring pass rates and computing performance gaps with McNemar's test for statistical significance. The authors also conduct synthetic perturbation experiments and feature importance analysis to understand the mechanisms behind performance degradation.

## Key Results
- LLMs show statistically significant performance drops of 5-11.5% on AAVE prompts compared to SE counterparts
- Performance gaps persist across all four reasoning categories: algorithm, math, logic, and integrated reasoning
- Model scaling and Chain-of-Thought prompting cannot fully close the dialect performance gap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs show statistically significant performance drops (5-11.5%) when processing AAVE compared to SE in reasoning tasks.
- Mechanism: Dialect-specific morphosyntactic features and conversational norms in AAVE create parsing difficulties that exceed what synthetic perturbations can capture.
- Core assumption: The performance gap is not solely due to unfamiliarity with dialect features but involves deeper semantic processing challenges.
- Evidence anchors:
  - [abstract] "Experiments with synthetic perturbations and AAVE-specific feature injections show that while these factors contribute to performance degradation, they fail to replicate the severity observed with human-annotated data"
  - [section 4.1] "even under the most extreme synthetic perturbations, performance drops are notably less severe than those observed with human-rewritten prompts"
  - [corpus] Weak evidence - corpus only shows related work on dialect bias but not specific to synthetic vs human perturbation comparison
- Break condition: If synthetic perturbations with equivalent perplexity levels produce similar performance drops as human-rewritten AAVE prompts, the mechanism fails.

### Mechanism 2
- Claim: Performance drops persist across all four reasoning categories (algorithm, math, logic, integrated) and are not task-specific.
- Mechanism: LLMs lack robustness to natural language variation within a single language, affecting their ability to maintain semantic understanding across dialectal variants.
- Core assumption: The reasoning capabilities of LLMs are fundamentally brittle to within-language variation, not just cross-lingual differences.
- Evidence anchors:
  - [section 3.2] "When aggregated by task type, AAVE queries cause a statistically significant performance drop across all these categories"
  - [section 3.2] "integrated reasoning tasks...show some of the largest relative drops (about 30%)"
  - [corpus] Moderate evidence - corpus shows related work on dialect brittleness but not specifically across all reasoning categories
- Break condition: If performance drops are isolated to specific task types or reasoning categories, the mechanism fails.

### Mechanism 3
- Claim: Model scaling and advanced prompting techniques (including Chain-of-Thought and o1-style reasoning) do not close the dialect performance gap.
- Mechanism: Larger model size and enhanced reasoning scaffolding cannot compensate for fundamental dialect understanding deficits.
- Core assumption: The dialect gap stems from training data and architectural limitations that are not addressed by parameter scaling or inference-time techniques.
- Evidence anchors:
  - [abstract] "This discrepancy persists even with advanced prompting techniques like Chain-of-Thought"
  - [section 3.2] "GPT-o1's performance gap decreases from about 0.116 (zero-shot) to 0.043 (CoT)" but remains significant
  - [section 3.2] "even Llama-3.1-70B-Instruct, among the largest and most capable tested models, suffers from significant performance drops"
  - [corpus] Moderate evidence - corpus shows related work on LLM scaling but not specifically dialect robustness
- Break condition: If larger models or specific prompting techniques completely eliminate the performance gap, the mechanism fails.

## Foundational Learning

- Concept: Statistical significance testing (McNemar's test) for paired binary data
  - Why needed here: To determine whether performance differences between SE and AAVE prompts are statistically meaningful across multiple model evaluations
  - Quick check question: When comparing two sets of binary outcomes from the same subjects (or prompts), which test accounts for the paired nature of the data?

- Concept: Perplexity as a measure of language model understanding
  - Why needed here: To distinguish between simple comprehension difficulties and deeper semantic processing issues when LLMs handle dialectal text
  - Quick check question: If a language model shows higher perplexity on AAVE text compared to SE text with similar complexity, what does this indicate about its understanding of the dialect?

- Concept: Mutual information for feature importance analysis
  - Why needed here: To identify which dialect-specific lexical and morphosyntactic features most strongly predict performance degradation in reasoning tasks
  - Quick check question: When computing mutual information between token distributions of SE and AAVE prompts, what does a high mutual information value indicate about that token's role in distinguishing the dialects?

## Architecture Onboarding

- Component map: Benchmark sampling -> Human annotation (AAVE speakers) -> Quality validation (cross-validation, correctness checks) -> Prompt formatting -> Model inference (temperature=0) -> Pass rate calculation -> Statistical significance testing -> Perplexity calculation -> Synthetic perturbation generation -> Feature importance analysis

- Critical path: High-quality human-annotated dialect data collection -> Systematic evaluation across multiple models and tasks -> Analysis of failure modes -> Identification of contributing factors

- Design tradeoffs:
  - Human annotation vs. synthetic generation: Higher quality but more expensive vs. scalable but less representative
  - Temperature settings: Temperature=0 for reproducibility vs. higher temperatures for more natural responses
  - Model selection: Representative coverage vs. exhaustive testing of all available models

- Failure signatures:
  - Models passing on SE but failing on AAVE with similar semantic content
  - Performance drops exceeding what synthetic perturbations with equivalent perplexity would predict
  - Consistent patterns of errors across different reasoning task types

- First 3 experiments:
  1. Compare perplexity scores between SE and AAVE prompts to establish baseline comprehension differences
  2. Apply synthetic perturbation with increasing feature density to determine if performance drops scale linearly with dialect feature introduction
  3. Test model families with varying parameter counts to determine if scaling alone can close the performance gap

## Open Questions the Paper Calls Out
None

## Limitations
- Human annotation bottleneck: Reliance on human AAVE speakers creates scalability limitations with only 1,200+ pairs, potentially insufficient for detecting subtle model differences
- Synthetic perturbation incompleteness: Underspecified perturbation methodology doesn't fully characterize which dialect features remain unreplicable through synthetic means
- Temperature sensitivity: Using temperature=0 may not reflect real-world deployment conditions where higher temperatures are common

## Confidence
- High confidence: Core finding of statistically significant 5-11.5% performance drops on AAVE vs SE is well-supported by McNemar's test across multiple model families
- Medium confidence: Performance drops persisting across all four reasoning categories is moderately supported, though sample sizes within categories may be insufficient
- Medium confidence: Model scaling and Chain-of-Thought prompting cannot fully close dialect gap is supported but could benefit from testing additional techniques

## Next Checks
1. Calculate and compare perplexity scores for SE and AAVE prompts across all reasoning categories to establish whether comprehension difficulties alone can explain the performance gaps
2. Systematically vary synthetic perturbation intensity across different dialect feature types to determine which specific features contribute most to the gap between synthetic and human-rewritten performance
3. Test whether observed dialect performance gaps generalize to other dialect pairs (e.g., British English vs. Indian English) to determine if findings represent general language model limitations or something specific to AAVE