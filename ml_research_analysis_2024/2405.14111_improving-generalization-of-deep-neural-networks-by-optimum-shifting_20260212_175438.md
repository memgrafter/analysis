---
ver: rpa2
title: Improving Generalization of Deep Neural Networks by Optimum Shifting
arxiv_id: '2405.14111'
source_url: https://arxiv.org/abs/2405.14111
tags:
- training
- trace
- uni00000048
- loss
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces optimum shifting (OS), a method to improve
  neural network generalization by moving model parameters from sharp minima to flatter
  minima while keeping training loss unchanged. The approach exploits the under-determined
  nature of matrix multiplications within neural networks, allowing parameters to
  be adjusted within the solution space to minimize Hessian trace (a measure of flatness).
---

# Improving Generalization of Deep Neural Networks by Optimum Shifting

## Quick Facts
- **arXiv ID:** 2405.14111
- **Source URL:** https://arxiv.org/abs/2405.14111
- **Reference count:** 40
- **One-line primary result:** Stochastic optimum shifting improves generalization by moving parameters from sharp to flat minima while maintaining training loss.

## Executive Summary
This paper introduces Optimum Shifting (OS), a method to improve neural network generalization by moving model parameters from sharp minima to flatter minima while keeping training loss unchanged. The approach exploits the under-determined nature of matrix multiplications within neural networks, allowing parameters to be adjusted within the solution space to minimize Hessian trace (a measure of flatness). To reduce computational cost and increase flexibility, the authors propose stochastic optimum shifting, which operates on small batches of data and leverages Neural Collapse theory to ensure training loss remains stable. Experiments on CIFAR-10/100, ImageNet, and PASCAL VOC demonstrate that OS consistently improves test accuracy across various architectures (VGG, ResNet, DenseNet, YOLO) by reducing Hessian trace and improving generalization.

## Method Summary
The method is based on the observation that when the input and output of a neural network are fixed, the matrix multiplications within the network can be treated as systems of under-determined linear equations. This enables adjustment of parameters in the solution space while maintaining training loss. The authors propose two variants: Optimum Shifting (OS) which operates on the entire dataset, and Stochastic Optimum Shifting (SOS) which operates on small batches to reduce computational cost. SOS leverages Neural Collapse theory to ensure training loss remains stable when adjusting parameters. The method minimizes the Frobenius norm of the final linear layer's weight matrix, which is shown to minimize the trace of the Hessian, leading to flatter minima and better generalization.

## Key Results
- OS improves test accuracy by 0.43% on CIFAR-100 and 0.65% on ImageNet across multiple architectures
- When applied to pre-trained models, OS improves CIFAR-100 accuracy from 76.06% to 77.59% and ImageNet from 76.73% to 77.48%
- SOS reduces Hessian trace significantly (e.g., from 100.3 to 84.2 on CIFAR-100) while maintaining training loss
- The method is compatible with standard regularization techniques and improves YOLO performance on PASCAL VOC object detection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Moving parameters from sharp minima to flatter minima improves generalization.
- **Mechanism:** The method exploits the under-determined nature of matrix multiplications in neural networks, allowing parameters to be adjusted within the solution space while maintaining training loss. This is achieved by solving a constrained optimization problem that minimizes the trace of the Hessian (a measure of flatness).
- **Core assumption:** The input-output relationship of the neural network can be treated as a system of under-determined linear equations.
- **Evidence anchors:**
  - [abstract]: "Our method is based on the observation that when the input and output of a neural network are fixed, the matrix multiplications within the network can be treated as systems of under-determined linear equations, enabling adjustment of parameters in the solution space."
  - [section 3.2]: "The training loss value of a neural network will not change no matter how the parameter V is adjusted in the solution space if the input matrix A and output matrix Z are both fixed."
  - [corpus]: Weak. The corpus neighbors do not directly discuss under-determined systems or flatness in the context of this mechanism.
- **Break condition:** If the input matrix A is not under-determined (i.e., rank(A) >= m), the method cannot find alternative solutions within the solution space.

### Mechanism 2
- **Claim:** Stochastic optimum shifting reduces computational cost and increases degrees of freedom.
- **Mechanism:** By operating on small batches of data, the method reduces computational cost and ensures the input matrix is more likely to be under-determined, providing more degrees of freedom for parameter adjustment.
- **Core assumption:** Neural Collapse theory ensures that if the loss is unchanged in a small batch, it remains unchanged across the entire training set.
- **Evidence anchors:**
  - [abstract]: "To reduce computational cost and increase flexibility, the authors propose stochastic optimum shifting, which operates on small batches of data and leverages Neural Collapse theory to ensure training loss remains stable."
  - [section 3.2]: "According to the theory of the Neural Collapse [32, 23, 31], if the loss is unchanged in a small batch of training data (typically, 'batch ≥ n' where n is the class number), it is expected to be unchanged across all training data."
  - [corpus]: Weak. The corpus neighbors do not discuss Neural Collapse or stochastic optimization in the context of this mechanism.
- **Break condition:** If the batch size is too small, it may not keep the training loss value unchanged, violating the Neural Collapse assumption.

### Mechanism 3
- **Claim:** Minimizing the Frobenius norm of the final linear layer's weight matrix minimizes the trace of the Hessian.
- **Mechanism:** The method leverages a theorem that shows the trace of the Hessian is linearly dependent on the Frobenius norm of the final linear layer's weight matrix. By minimizing this norm, both the upper and lower bounds of the Hessian trace are minimized, leading to better generalization.
- **Core assumption:** The relationship between the Frobenius norm of the final linear layer's weight and the Hessian trace holds for various neural network architectures (CNN, ResNet, DenseNet, MLP).
- **Evidence anchors:**
  - [section 3.1]: "Theorem 3.2 For an l-layer MLP or convolutional (CNN, ResNet and Densenet) neural network, given the loss function L, the trace of Hessian can be bounded by: C0 + C1||V||2 ≤ tr(HL) ≤ C0 + C2||V||2, where V is the weight matrix of the last linear layer and C0, C1, C2 are constants and independent of the last hidden layer's weight V."
  - [corpus]: Weak. The corpus neighbors do not discuss the relationship between Frobenius norm and Hessian trace in the context of this mechanism.
- **Break condition:** If the relationship between the Frobenius norm and Hessian trace does not hold for a specific architecture, the method may not effectively minimize the Hessian trace.

## Foundational Learning

- **Concept:** Under-determined linear systems
  - **Why needed here:** The method relies on the property that under-determined linear systems have infinite solutions, allowing parameters to be adjusted within the solution space.
  - **Quick check question:** If a system of linear equations has more variables than equations, what is the relationship between the rank of the coefficient matrix and the number of variables?

- **Concept:** Hessian matrix and its trace
  - **Why needed here:** The method aims to minimize the trace of the Hessian, which is a measure of the flatness of the loss landscape. Understanding the Hessian and its properties is crucial for grasping the mechanism.
  - **Quick check question:** What does the trace of the Hessian matrix represent in the context of loss landscapes?

- **Concept:** Neural Collapse
  - **Why needed here:** The method leverages Neural Collapse theory to ensure that if the loss is unchanged in a small batch, it remains unchanged across the entire training set, enabling stochastic optimum shifting.
  - **Quick check question:** What is the key observation of Neural Collapse theory regarding the behavior of within-class variations during training?

## Architecture Onboarding

- **Component map:** Input matrix (A) -> Output matrix (Z) -> Weight matrix (V) -> Loss function (L) -> Hessian matrix

- **Critical path:**
  1. Compute the input and output matrices (A and Z) for the linear layer.
  2. Perform Gaussian elimination to ensure A is row-independent.
  3. Solve the constrained optimization problem to find the new weight matrix V* that minimizes the Frobenius norm while satisfying AV* = Z.
  4. Update the parameters with the new weight matrix V*.

- **Design tradeoffs:**
  - Batch size for stochastic optimum shifting: Larger batches reduce computational cost but may limit degrees of freedom, while smaller batches increase degrees of freedom but may not keep the training loss unchanged.
  - Frequency of applying optimum shifting: Applying it too frequently may slow down training, while applying it too infrequently may not effectively minimize the Hessian trace.

- **Failure signatures:**
  - Training loss increases after applying optimum shifting: Indicates that the method may not have kept the loss unchanged, violating the Neural Collapse assumption.
  - No improvement in test accuracy: Suggests that the method may not have effectively moved the parameters to a flatter minimum or that other factors are limiting generalization.

- **First 3 experiments:**
  1. Apply optimum shifting to a pre-trained model on a small dataset (e.g., CIFAR-10) and compare the test accuracy before and after.
  2. Vary the batch size for stochastic optimum shifting and observe its impact on test accuracy and computational cost.
  3. Integrate optimum shifting with a standard training scheme (e.g., SGD with weight decay) and compare the results to the baseline.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important ones arise from the methodology:

- How does stochastic optimum shifting perform on non-image tasks like NLP or tabular data where Neural Collapse may not apply?
- What is the theoretical guarantee that keeping loss unchanged on a small batch (e.g., batch=300 for CIFAR-100) will maintain loss stability across the full dataset?
- How does the computational complexity of SOS scale with network depth and width, and what are the practical limits for very large architectures?

## Limitations

- The method's effectiveness depends on the Neural Collapse assumption, which may not hold for all architectures and datasets
- Computational complexity increases significantly for large models, particularly when applying OS to the entire dataset
- The method is currently limited to image classification and object detection tasks, with unclear applicability to other domains

## Confidence

- **High Confidence:** The general principle of moving from sharp to flat minima for improved generalization is well-established in literature.
- **Medium Confidence:** The specific mathematical formulation and theorem relating Frobenius norm to Hessian trace appears sound but requires verification across diverse architectures.
- **Low Confidence:** The practical effectiveness of Stochastic Optimum Shifting in real-world scenarios, particularly with large-scale models and complex datasets.

## Next Checks

1. **Theoretical Verification:** Rigorously test the relationship between Frobenius norm and Hessian trace across diverse architectures (including transformers and recurrent networks) to confirm the proposed bounds.
2. **Robustness Testing:** Evaluate Stochastic Optimum Shifting's performance across various batch sizes and architectures to determine the limits of the Neural Collapse assumption.
3. **Implementation Validation:** Develop a reference implementation with detailed logging of matrix operations to verify that training loss remains unchanged during parameter adjustment.