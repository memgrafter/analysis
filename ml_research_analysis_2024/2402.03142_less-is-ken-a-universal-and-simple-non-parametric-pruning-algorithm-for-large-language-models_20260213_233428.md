---
ver: rpa2
title: 'Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large
  Language Models'
arxiv_id: '2402.03142'
source_url: https://arxiv.org/abs/2402.03142
tags:
- pruning
- parameters
- each
- matrix
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KEN is a universal, unstructured pruning algorithm for LLMs based
  on Kernel Density Estimation (KDE). It selects the most influential parameters while
  restoring others to their pre-trained values, enabling substantial memory savings
  without compromising performance.
---

# Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for Large Language Models

## Quick Facts
- arXiv ID: 2402.03142
- Source URL: https://arxiv.org/abs/2402.03142
- Authors: Michele Mastromattei; Fabio Massimo Zanzotto
- Reference count: 10
- Primary result: KEN achieves at least 25% parameter reduction while maintaining or improving F1-weighted scores compared to unpruned models.

## Executive Summary
KEN is a universal, unstructured pruning algorithm for LLMs based on Kernel Density Estimation (KDE). It selects the most influential parameters while restoring others to their pre-trained values, enabling substantial memory savings without compromising performance. Extensive evaluations across seven transformer models show KEN achieves parameter reductions of at least 25% while maintaining or improving F1-weighted scores compared to unpruned models. KEN outperforms established pruning algorithms and PEFT methods like LoRA. The algorithm generates a compact subnetwork that can be stored separately and injected into the pre-trained model as needed. KENviz, an accompanying visualization tool, reveals uniform parameter distribution without clustering, demonstrating KEN's effective selection strategy. The approach offers a simple, non-parametric alternative to complex pruning techniques while delivering excellent compression and performance results.

## Method Summary
KEN is a non-parametric pruning algorithm that operates on fine-tuned transformer models by selecting influential parameters using Kernel Density Estimation (KDE). For each row in the model's weight matrices, KEN computes the KDE distribution using Scott's rule to determine bandwidth, then identifies the k points with highest likelihood while resetting all other parameters to their pre-trained values. This creates a compact subnetwork that can be stored separately and injected into the pre-trained model as needed. The algorithm is universal and works across different transformer architectures without requiring task-specific tuning. KENviz is an accompanying visualization tool that reveals the uniform distribution of selected parameters across matrices, demonstrating the effectiveness of the selection strategy.

## Key Results
- KEN achieves parameter reductions of at least 25% while maintaining or improving F1-weighted scores compared to unpruned models.
- KEN outperforms established pruning algorithms and PEFT methods like LoRA across multiple transformer models and tasks.
- KENviz reveals uniform parameter distribution without clustering, demonstrating KEN's effective selection strategy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kernel Density Estimation (KDE) identifies the most influential parameters by capturing the smoothed distribution of each matrix row.
- Mechanism: KEN computes KDE for each row of a fine-tuned matrix using Scott's rule to set the bandwidth, then retains the k points with highest likelihood while resetting others to pre-trained values.
- Core assumption: The distribution of fine-tuned parameters contains meaningful structure that KDE can reveal, and that retaining high-density points preserves model performance.
- Evidence anchors:
  - [abstract]: "KEN - inspired by the winning ticket pruning hypothesis (Frankle and Carbin, 2018) - identifies and retains the most influential parameters using KDEs while resetting the others to their original pre-trained values."
  - [section]: "KEN calculates the KDE distribution of the row rt_i using a bandwidth parameter h determined following Scott’s rule of thumb... The k points that best fit the rt_i row distribution are identified using the KDE likelihood, while the others are reset to their pre-trained values."
  - [corpus]: Weak. No direct corpus mention of KDE-based parameter selection; related work focuses on structured pruning or low-rank methods, not density-based selection.
- Break condition: If the fine-tuned parameter distribution is too noisy or multimodal, KDE may select suboptimal points; if k is too small, performance collapses.

### Mechanism 2
- Claim: Resetting non-selected parameters to pre-trained values preserves model performance while reducing size.
- Mechanism: After KDE selects k points per row, all unselected parameters are restored to their values from the pre-trained checkpoint, forming a compact subnetwork.
- Core assumption: The pre-trained model provides a stable baseline, and the selected k points carry sufficient task-specific adaptation.
- Evidence anchors:
  - [abstract]: "KEN aims to construct optimized transformers by selectively preserving the most significant parameters while restoring others to their pre-training state."
  - [section]: "The k points that best fit the rt_i row distribution are identified using the KDE likelihood, while the others are reset to their pre-trained values."
  - [corpus]: Weak. Related pruning work often removes or zeros parameters; restoring to pre-trained values is distinct and not well represented in cited literature.
- Break condition: If the task requires large parameter shifts from pre-training, resetting non-selected parameters will hurt performance; if k is too low, subnetwork becomes too sparse.

### Mechanism 3
- Claim: Uniform parameter selection without clustering maintains generalization and avoids overfitting to specific parameter groups.
- Mechanism: KENviz shows that selected parameters are evenly distributed across matrix rows and layers, with consistent neighbor counts regardless of k, indicating balanced retention.
- Core assumption: Uniform distribution of retained parameters is more effective than clustered selection for maintaining model behavior.
- Evidence anchors:
  - [abstract]: "KENviz... reveals uniform parameter distribution without clustering, demonstrating KEN's effective selection strategy."
  - [section]: "KENviz reveals that KEN uniformly selects parameters across matrices, preventing parameter clusters."
  - [corpus]: Weak. No corpus evidence directly supports uniform selection; most pruning literature focuses on magnitude or impact-based selection, which can be clustered.
- Break condition: If certain parameter groups are critical for task performance, uniform selection may miss them; if data distribution changes, uniform strategy may become suboptimal.

## Foundational Learning

- Concept: Kernel Density Estimation (KDE) and bandwidth selection via Scott's rule.
  - Why needed here: KDE is the core mechanism for identifying influential parameters; understanding bandwidth choice is critical for tuning k.
  - Quick check question: How does Scott's rule compute the bandwidth, and why is it appropriate for KDE-based pruning?
- Concept: Winning ticket hypothesis and its relation to parameter selection.
  - Why needed here: KEN is inspired by the winning ticket idea; understanding this connection clarifies why resetting non-selected parameters can work.
  - Quick check question: What is the key insight of the winning ticket hypothesis, and how does KEN adapt it using KDE instead of iterative pruning?
- Concept: Transformer architecture and attention matrix structure.
  - Why needed here: KEN operates on transformer matrices (QKV projections, FFNs); knowing matrix roles helps interpret pruning impact.
  - Quick check question: Which transformer matrices are most sensitive to pruning, and how does KEN handle them uniformly?

## Architecture Onboarding

- Component map: Pre-trained model → Fine-tuning → KEN processing (row-wise KDE → parameter selection → reset to pre-trained) → Compressed model + support dictionary → Model injection
- Critical path: KDE computation → parameter selection → matrix replacement → saving compressed model + support dict → loading into pre-trained model
- Design tradeoffs: High k → better performance but less compression; low k → high compression but risk of performance drop. Uniform selection vs. targeted pruning. Restoring to pre-trained vs. zeroing.
- Failure signatures: Performance collapse when k too low; slow processing on large models; mismatched support dictionary during loading; uneven parameter retention indicating KDE bandwidth issues.
- First 3 experiments:
  1. Run KEN on a small BERT model with k values spanning low to high; measure F1 change and compression ratio.
  2. Compare KEN against random parameter selection on the same model and dataset; quantify performance gap.
  3. Use KENviz to inspect parameter distribution in key matrices; verify uniform selection and neighbor consistency.

## Open Questions the Paper Calls Out
The paper explicitly states that while KEN shows promising results on sequence classification tasks, future work will explore its effectiveness on other NLP tasks such as question answering, summarization, and code generation. The authors also note that KEN's computational efficiency becomes challenging for extremely large models (10B+ parameters), suggesting this as an area for future investigation. Additionally, the paper calls for systematic studies to determine optimal strategies for selecting the k parameter across different architectures and tasks, as current results rely on empirical testing rather than principled guidelines.

## Limitations
- Computational efficiency challenges when scaling to extremely large models (10B+ parameters) due to linear processing time with model size, layers, and k value.
- Limited evaluation scope focused primarily on sequence classification tasks, with unclear generalizability to other NLP tasks like question answering or summarization.
- Lack of systematic methodology for determining optimal k values across different architectures and tasks, relying instead on empirical testing.

## Confidence
- **Mechanism 1 (KDE-based parameter selection)**: Low confidence. The KDE mechanism is described but lacks empirical validation of its superiority over simpler selection methods (e.g., magnitude pruning) or sensitivity to bandwidth choice.
- **Mechanism 2 (Resetting to pre-trained values)**: Medium confidence. The claim is supported by experimental results showing maintained or improved performance, but no ablation is provided to isolate the effect of resetting versus other strategies (e.g., zeroing).
- **Mechanism 3 (Uniform parameter distribution)**: Low confidence. KENviz visualizations are presented, but no statistical analysis or comparison to clustered selection baselines is provided to substantiate the claim of superiority.

## Next Checks
1. **Hyperparameter Sensitivity**: Systematically vary Scott's bandwidth and k across a range of transformer models and datasets to identify stable operating regions and failure modes.
2. **Runtime Efficiency Comparison**: Measure wall-clock inference time and memory usage of KEN-compressed models versus LoRA and other PEFT baselines on identical hardware.
3. **Distribution Analysis**: Quantitatively compare parameter retention distributions (e.g., entropy, clustering metrics) between KEN and magnitude-based pruning to validate the claimed uniformity and its impact on generalization.