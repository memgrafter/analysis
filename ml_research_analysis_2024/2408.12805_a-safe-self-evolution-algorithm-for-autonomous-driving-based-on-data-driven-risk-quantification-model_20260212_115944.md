---
ver: rpa2
title: A Safe Self-evolution Algorithm for Autonomous Driving Based on Data-Driven
  Risk Quantification Model
arxiv_id: '2408.12805'
source_url: https://arxiv.org/abs/2408.12805
tags:
- safety
- vehicle
- driving
- risk
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a safe self-evolution algorithm for autonomous
  driving based on a data-driven risk quantification model. The method aims to enable
  autonomous driving systems to independently evolve in complex and open environments
  while ensuring safety.
---

# A Safe Self-evolution Algorithm for Autonomous Driving Based on Data-Driven Risk Quantification Model

## Quick Facts
- arXiv ID: 2408.12805
- Source URL: https://arxiv.org/abs/2408.12805
- Authors: Shuo Yang; Shizhen Li; Yanjun Huang; Hong Chen
- Reference count: 40
- Primary result: Safe self-evolution algorithm for autonomous driving using data-driven risk quantification model to enable safe exploration in complex traffic scenarios without sacrificing learning performance.

## Executive Summary
This paper presents a safe self-evolution algorithm for autonomous driving that combines a data-driven risk quantification (RQ) model with an adjustable safety guard mechanism. The approach uses a transformer-based RQ model to assess environmental safety situations, which then informs adjustable safety limits for an RL-based driving policy. The method aims to enable autonomous driving systems to independently evolve in complex and open environments while ensuring safety through a risk-aware safety guard that prevents collisions while allowing exploration.

## Method Summary
The method integrates a data-driven risk quantification model based on attention mechanisms with a safety-evolution decision-control algorithm. The RQ model uses a transformer encoder to process state information (ego vehicle and surrounding traffic) and outputs a quantitative risk value (0-100%) along with importance rankings. This RQ output informs adjustable safety limits through a Safe Critical Acceleration mechanism. The overall system uses soft actor-critic (SAC) reinforcement learning for policy learning, with a safety guard that checks proposed actions against safety limits and overrides when necessary. The approach was validated in CARLA simulation and on real vehicles across various traffic scenarios.

## Key Results
- The proposed algorithm improved average speed while ensuring no collisions, reducing safety guard intervention ratio from over 5.3% to 2.7%
- Simulation results showed superior performance compared to baseline methods (SAC, CPO, FOCOPS, TRPO-lag, Safety Guard) across dense traffic and mixed traffic scenarios
- Real vehicle tests demonstrated the algorithm's ability to control actual autonomous vehicles to output safe, smooth, and reasonable lane-changing overtaking actions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Risk quantification model outputs values that reflect human risk perception.
- Mechanism: Transformer-based RQ model uses attention to weight state features and produce a scalar risk value and importance ranking.
- Core assumption: State input contains sufficient contextual information for the model to distinguish high/low risk.
- Evidence anchors:
  - [abstract] "risk quantification model based on the attention mechanism... modeling the way humans perceive risks during driving"
  - [section] "RQ model outputs a quantitative value of risk RQ... outputs a risk quantitative value given by the agent... ranging from 0 to 100%"
  - [corpus] Weak: neighbors do not describe transformer-based risk quantification with attention
- Break condition: If attention weights become uniform or model cannot distinguish between safe/unsafe states.

### Mechanism 2
- Claim: Adjustable safety limits improve exploration without sacrificing safety.
- Mechanism: Safe Critical Acceleration (SCA) adjusts based on RQ output; low RQ → relaxed limits, high RQ → tighter limits.
- Core assumption: RQ correlates with actual collision risk in the environment.
- Evidence anchors:
  - [abstract] "safety-evolution decision-control integration algorithm with adjustable safety limits is designed, and the proposed risk quantization model is integrated into it"
  - [section] "The proposed algorithm improves the average speed while ensuring no collision at all, and reduces the safety guard intervention ratio from more than 5.3% to 2.7%"
  - [corpus] Weak: neighbors mention safety RL but not adjustable SCA based on risk quantification
- Break condition: If RQ value fails to predict actual safety, causing overly aggressive or overly conservative policy adjustments.

### Mechanism 3
- Claim: Cross-domain migration from simulation to real world preserves safety and performance.
- Mechanism: Similarity of operational domains (Φs ≃ ⌢ Φa) allows safe transfer of trained policy.
- Core assumption: Sim2real gap is mitigated by the safety guard that uses real-time state input.
- Evidence anchors:
  - [abstract] "Simulation and real-vehicle experiments results illustrate the effectiveness of the proposed method"
  - [section] "Real vehicle test results show that the proposed algorithm can control the real autonomous vehicle to output safe, smooth and reasonable lane-changing overtaking actions"
  - [corpus] Weak: neighbors do not explicitly discuss sim2real transfer with safety guards
- Break condition: If real-world dynamics deviate significantly from simulation, causing unsafe behavior despite safety guard.

## Foundational Learning

- Concept: Transformer encoder with multi-head self-attention
  - Why needed here: To model complex interactions between ego vehicle and multiple traffic participants for risk assessment
  - Quick check question: What is the role of the Q, K, V matrices in the attention computation?

- Concept: Reinforcement learning with soft actor-critic (SAC)
  - Why needed here: To learn safe and efficient driving policies that balance exploration and exploitation
  - Quick check question: How does the entropy term in SAC encourage exploration?

- Concept: Frenet coordinate system for trajectory planning
  - Why needed here: To represent road-relative positions and plan smooth longitudinal/lateral trajectories
  - Quick check question: What are the s and d axes in Frenet coordinates?

## Architecture Onboarding

- Component map:
  Data-driven Risk Quantification (DD-RQ) Model -> Safety-Evolution Decision-Control Integration -> Safety Guard Policy

- Critical path:
  1. Sensor data → state vector
  2. State vector → DD-RQ model → RQ + IR
  3. RQ → SCA adjustment → safety limits
  4. RL policy → action proposal
  5. Safety guard checks action vs safety limits
  6. If safe, execute; else, override with safe action

- Design tradeoffs:
  - Model complexity vs. inference speed: Transformer adds computation but improves accuracy
  - Safety vs. performance: Adjustable limits balance safety and exploration
  - Sim2real gap: Safety guard mitigates but cannot eliminate mismatch

- Failure signatures:
  - High collision rate despite safety guard
  - Excessive safety guard interventions (>5%)
  - RQ values not correlating with actual risk
  - Policy convergence failure in training

- First 3 experiments:
  1. Single-lane following with static obstacles; test RQ model accuracy
  2. Multi-lane scenario with dynamic traffic; test SCA adjustment and guard
  3. Mixed traffic with pedestrians; test generalization of DD-RQ model

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, based on the limitations and scope of the work, several important questions emerge regarding the algorithm's performance in non-stationary traffic patterns, computational efficiency, and robustness to extreme weather conditions or sensor failures.

## Limitations

- Network architectures for the transformer RQ model, value network, and policy network lack complete details (layer sizes, activation functions), creating ambiguity in reproduction
- The correlation between RQ values and actual collision risk remains unverified in the provided results
- Sim2real transfer claims rely on qualitative real-vehicle tests rather than quantitative comparison of performance metrics across domains

## Confidence

- **Medium**: RQ model effectiveness and safety guard performance (limited ablation studies, no RQ-accuracy correlation analysis)
- **Medium**: Sim2real transfer validity (qualitative results only, no quantitative domain adaptation metrics)
- **High**: Basic algorithm functionality and safety guard preventing collisions (demonstrated in simulation with controlled scenarios)

## Next Checks

1. **RQ Model Calibration**: Systematically vary environmental risk levels in simulation and measure whether RQ outputs correlate with actual collision probability. Generate scatter plots of RQ vs. empirical risk and compute correlation coefficients.

2. **Ablation of Safety Limits**: Run experiments with fixed safety limits (no RQ-based adjustment) across multiple traffic densities to quantify the performance safety tradeoff. Compare collision rates, average speeds, and learning convergence rates.

3. **Domain Gap Analysis**: Implement a quantitative metric for sim2real gap (e.g., distribution distance between simulated and real-world state-action pairs) and measure how safety guard intervention rates change when transferring between domains.