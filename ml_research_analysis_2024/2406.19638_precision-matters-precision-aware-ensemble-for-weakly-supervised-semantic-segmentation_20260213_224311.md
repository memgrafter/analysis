---
ver: rpa2
title: 'Precision matters: Precision-aware ensemble for weakly supervised semantic
  segmentation'
arxiv_id: '2406.19638'
source_url: https://arxiv.org/abs/2406.19638
tags:
- segmentation
- orandnet
- wsss
- precision
- miou
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving precision in weakly
  supervised semantic segmentation (WSSS) where image-level labels are used. The authors
  identify that pseudo-masks with high mean Intersection over Union (mIoU) do not
  always lead to high segmentation performance due to noise and low precision.
---

# Precision matters: Precision-aware ensemble for weakly supervised semantic segmentation
## Quick Facts
- arXiv ID: 2406.19638
- Source URL: https://arxiv.org/abs/2406.19638
- Reference count: 2
- Primary result: ORANDNet improves WSSS precision by +6.8 mIoU on PASCAL VOC 2012 compared to baseline

## Executive Summary
This paper addresses the challenge of improving precision in weakly supervised semantic segmentation (WSSS) where image-level labels are used. The authors identify that pseudo-masks with high mean Intersection over Union (mIoU) do not always lead to high segmentation performance due to noise and low precision. To tackle this, they propose ORANDNet, an ensemble approach that combines Class Activation Maps (CAMs) from two distinct classifiers (ResNet-50 and ViT) using a probabilistic OR and AND operation. This method enhances the precision of pseudo-masks by focusing on consistent activations and reducing noise. Additionally, curriculum learning with scale scheduling is incorporated to further mitigate small noise in the pseudo-masks.

## Method Summary
ORANDNet improves precision in WSSS by combining CAMs from two distinct classifiers (ResNet-50 and ViT) through a probabilistic OR operation for input and AND operation for prediction. The method trains an FCN to predict AND CAMs from OR CAMs, effectively filtering noise while preserving object coverage. Curriculum learning with scale scheduling is applied during segmentation training, progressively downsampling images to remove small noise early in training. The refined pseudo-masks are then used to train a Deeplab-v1 segmentation model.

## Key Results
- Achieves +6.8 mIoU improvement on PASCAL VOC 2012 validation set compared to baseline IRN method
- Improves precision of pseudo-masks while maintaining reasonable recall through OR-then-AND architecture
- Demonstrates effectiveness as an add-on module that can enhance various WSSS models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ORANDNet improves precision by focusing on consistent activations between two distinct classifiers
- Mechanism: The method uses probabilistic AND operation on CAMs from ResNet and ViT to retain only activations present in both models, effectively removing inconsistent noise
- Core assumption: Activations that are consistent across two structurally different classifiers (ResNet vs ViT) are more likely to be true object regions rather than noise
- Evidence anchors:
  - [abstract] "ORANDNet combines Class Activation Maps (CAMs) from two different classifiers to increase the precision of pseudo-masks (PMs)"
  - [section] "We design ORANDNet to take a probabilistic OR of two CAMs as an input and to produce a probabilistic AND of two CAMs as an output"
  - [corpus] Weak evidence - related papers focus on CAM refinement but don't specifically address ensemble-based precision improvement
- Break condition: If the two classifiers have highly correlated error patterns, the AND operation may remove true positives along with noise

### Mechanism 2
- Claim: OR operation followed by AND prediction balances recall and precision
- Mechanism: The OR operation between CAMs ensures sufficient recall coverage, while the AND prediction network learns to filter noise while preserving true object regions
- Core assumption: A neural network can learn the mapping from OR-combined noisy activations to cleaner AND-combined pseudo-masks
- Evidence anchors:
  - [section] "ORANDNet is then trained with the AND CAM as the pseudo-mask and the OR CAM as the input"
  - [section] "ORANDNet learns to reduce noise in the pseudo-masks and enhance precision while preserving the object coverage of activations"
  - [corpus] No direct evidence found in corpus papers about this specific OR-then-AND architecture
- Break condition: If the OR CAM contains too much noise, the network may struggle to learn the mapping to clean pseudo-masks

### Mechanism 3
- Claim: Scale scheduling removes small false activations during early training
- Mechanism: Training with progressively larger image sizes allows the model to focus on larger, more reliable object regions first, ignoring small noisy activations
- Core assumption: Small activations are more likely to be noise than true object regions, especially in early training stages
- Evidence anchors:
  - [section] "We adopt a curriculum learning strategy in second stage, progressively downsampling training data... This method effectively discards smaller noise early in training"
  - [abstract] "To further mitigate small noise in the PMs, we incorporate curriculum learning"
  - [corpus] No direct evidence in corpus papers about scale scheduling for noise removal
- Break condition: If objects of interest are genuinely small, this approach may remove valid small object regions

## Foundational Learning

- Concept: Class Activation Maps (CAMs)
  - Why needed here: CAMs form the basis of pseudo-mask generation in WSSS; understanding their properties is essential for grasping ORANDNet's approach
  - Quick check question: What is the fundamental difference between CAMs generated by ResNet versus ViT architectures?

- Concept: Weakly Supervised Learning
  - Why needed here: The paper operates in a setting where only image-level labels are available, not pixel-level masks
  - Quick check question: Why is it challenging to train semantic segmentation models with only image-level supervision?

- Concept: Precision vs Recall tradeoff
  - Why needed here: The paper specifically targets precision improvement while maintaining reasonable recall for effective segmentation training
  - Quick check question: How does focusing on precision rather than just mIoU improve the quality of pseudo-masks for segmentation training?

## Architecture Onboarding

- Component map:
  Two classifiers (ResNet-50 and ViT) -> CAM generation -> OR operation -> ORANDNet (FCN) -> AND CAM prediction -> IRN refinement -> Segmentation model

- Critical path:
  1. Train ResNet-50 and ViT classifiers on image-level labels
  2. Generate CAMs from both classifiers
  3. Apply OR operation to create OR CAM input
  4. Train ORANDNet to predict AND CAM from OR CAM
  5. Apply IRN refinement to ORANDNet outputs
  6. Use refined pseudo-masks to train segmentation model with scale scheduling

- Design tradeoffs:
  - Using two different architectures increases computational cost but provides complementary views
  - The AND operation improves precision but could reduce recall if classifiers disagree too much
  - Scale scheduling helps with noise but may miss small objects

- Failure signatures:
  - Poor performance if classifiers have correlated failure modes
  - Segmentation results degrade if scale scheduling removes too many small but valid object regions
  - Training instability if OR CAM is too noisy for the AND prediction network

- First 3 experiments:
  1. Compare pseudo-mask quality (precision, recall, mIoU) between single CAMs and ORANDNet outputs
  2. Ablation study: ORANDNet vs naive ensemble (simple averaging) of CAMs
  3. Scale scheduling impact: Compare segmentation performance with and without progressive image sizing during training

## Open Questions the Paper Calls Out
- Open Question 1: How does the ensemble method generalize to more than two classifiers?
  - Basis in paper: [explicit] The authors mention using CAMs from two distinct classifiers (ResNet-50 and ViT) and later extend the method to AMN and MCTformer. However, they do not explore the use of more than two classifiers.
  - Why unresolved: The paper does not provide evidence on the performance impact of using more than two classifiers.
  - What evidence would resolve it: Experimental results comparing the performance of ORANDNet with different numbers of classifiers would provide insights into its scalability.

## Limitations
- The method's performance gains rely heavily on the complementary nature of ResNet and ViT CAMs, which may not generalize across all architectures or datasets
- Scale scheduling could harm performance on datasets with many small objects by removing valid small object regions
- The paper doesn't adequately address how ORANDNet performs when the two classifiers have correlated error patterns

## Confidence
- Performance improvement claim: Medium - While the 6.8 mIoU improvement on PASCAL VOC 2012 is significant, the paper doesn't adequately address how ORANDNet performs when the two classifiers have correlated error patterns
- Generalization to other datasets: Low - Limited testing beyond PASCAL VOC 2012
- Architecture robustness: Medium - Relies on specific ResNet-ViT combination without testing other classifier pairs

## Next Checks
1. Test ORANDNet with different classifier pairs (e.g., ResNet-50 + EfficientNet) to verify that precision gains aren't architecture-specific to ResNet-ViT combinations
2. Conduct detailed analysis of how scale scheduling affects small object segmentation performance, potentially adjusting the scheduling strategy for datasets with smaller objects
3. Measure the correlation between ResNet and ViT CAM errors to quantify how much ORANDNet benefits from complementary rather than redundant information, and test failure cases where error patterns are highly correlated