---
ver: rpa2
title: 'Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction'
arxiv_id: '2407.13943'
source_url: https://arxiv.org/abs/2407.13943
tags:
- werewolf
- game
- seer
- each
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Werewolf Arena, a framework for evaluating
  LLMs using the social deduction game Werewolf. The framework incorporates a dynamic
  turn-taking system where agents bid to speak, mirroring real-world group discussions.
---

# Werewolf Arena: A Case Study in LLM Evaluation via Social Deduction

## Quick Facts
- arXiv ID: 2407.13943
- Source URL: https://arxiv.org/abs/2407.13943
- Authors: Suma Bailis; Jane Friedhoff; Feiyang Chen
- Reference count: 40
- Key outcome: Introduces Werewolf Arena framework for LLM evaluation using social deduction games with dynamic bidding system

## Executive Summary
Werewolf Arena presents a novel framework for evaluating large language models through the social deduction game Werewolf. The framework introduces a dynamic bidding system where agents compete to speak, creating more natural group discussions than fixed turn orders. Through tournaments featuring Gemini and GPT models, the authors demonstrate the framework's ability to reveal distinct strengths and weaknesses in strategic reasoning and communication across different LLM architectures.

## Method Summary
The authors implement Werewolf Arena using a dynamic bidding system where agents choose urgency levels (0-4) to compete for speaking turns, rather than using fixed or random orders. The framework supports 8-player games with standard Werewolf roles (Seer, Doctor, Werewolves, Villagers) and includes memory systems for agents to track game state. Evaluation consists of intra-family round-robin tournaments between different model variants, with performance measured through win rates, debate dynamics (bidding behavior, synthetic votes), and specific Seer role metrics including reveal timing and persuasiveness.

## Key Results
- Gemini 1.5 Pro emerged as the strongest overall player, particularly when playing as Villager
- GPT-4 models exhibited higher verbosity that was sometimes perceived as suspicious by other agents
- Seer performance varied significantly across models, highlighting the importance of strategic information disclosure
- Self-play tournaments achieved relatively balanced win rates (40-60%) across most models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bidding system allows agents to express urgency levels that reflect their strategic needs in real time.
- Mechanism: Agents choose from discrete urgency levels (0-4) where higher values indicate stronger desire to speak, enabling dynamic turn-taking that mirrors natural group discussions.
- Core assumption: Agents can accurately assess the strategic value of their utterances and bid accordingly.
- Evidence anchors:
  - [abstract] "we introduce a dynamic turn-taking system where players bid to speak, rather than relying on predefined or random speaking orders"
  - [section] "agents choose from four distinct levels of interest in speaking" and "The highest bidder speaks next"
  - [corpus] Weak evidence - no direct corpus support for the bidding mechanism's effectiveness
- Break condition: If agents consistently misjudge utterance value, bidding will not reflect actual strategic importance.

### Mechanism 2
- Claim: Information asymmetry in Werewolf creates a challenging environment that requires sophisticated social reasoning from LLMs.
- Mechanism: The game's structure ensures some players (like the Seer) have privileged information while others must deduce roles through conversation, creating a natural testbed for reasoning about hidden information.
- Core assumption: The information asymmetry is sufficient to create meaningful evaluation challenges without being artificially constrained.
- Evidence anchors:
  - [abstract] "the inherent information asymmetry of Werewolf, where only some players possess incomplete knowledge of others' roles, mirrors dynamics of real-world social interactions"
  - [section] "Werewolf players engage in a battle of wits, leveraging deception and persuasion to achieve their respective goals"
  - [corpus] Weak evidence - corpus neighbors discuss social deduction but don't directly address information asymmetry's role in evaluation
- Break condition: If information becomes too easily discoverable through other means, the asymmetry loses its evaluative power.

### Mechanism 3
- Claim: Self-play tournaments with balanced win rates provide a fair comparison framework for evaluating different LLM capabilities.
- Mechanism: By having each model play both sides of the game, inherent advantages are minimized and performance differences reflect actual reasoning and communication abilities rather than role-specific advantages.
- Core assumption: The game design allows for balanced win rates when models are equally skilled, making performance differences meaningful.
- Evidence anchors:
  - [section] "we design a balanced framework where a single model, playing both Villager and Werewolf roles, results in a relatively even win rate for both sides"
  - [section] "all models, except GPT-3.5 achieved relatively balanced win rates (40-60%) in self-play"
  - [corpus] Weak evidence - corpus neighbors don't discuss tournament design or balance considerations
- Break condition: If one role consistently has an inherent advantage regardless of model skill, the comparison framework breaks down.

## Foundational Learning

- Concept: Game theory and strategic reasoning
  - Why needed here: Understanding how agents make decisions under uncertainty and against opponents is crucial for interpreting LLM behavior in Werewolf
  - Quick check question: What is the difference between a Nash equilibrium and a dominant strategy in the context of Werewolf?

- Concept: Information theory and entropy
  - Why needed here: The paper uses voting entropy to measure consensus formation during debates, requiring understanding of how information spreads and uncertainty decreases
  - Quick check question: How does Shannon entropy apply to measuring disagreement in voting patterns?

- Concept: Natural language generation evaluation
  - Why needed here: Assessing communication quality and strategic language use requires understanding different evaluation metrics and their limitations
  - Quick check question: What are the key differences between automatic and human evaluation of LLM-generated dialogue?

## Architecture Onboarding

- Component map: Game Master (rules enforcement) -> Agents (with memory and role-specific actions) -> Bidding system (turn-taking) -> Tournament framework (evaluation)
- Critical path: Agent reasoning -> Bid selection -> Turn order determination -> Action execution -> Memory update -> Next turn
- Design tradeoffs: Bidding adds strategic depth but increases computational complexity; simplified Werewolf rules make the game tractable but may miss some social nuances
- Failure signatures: Agents consistently bid 0 (not participating), agents bid 4 every turn (no nuance), voting entropy remains high throughout debates (no consensus)
- First 3 experiments:
  1. Test bidding system with random bid values to verify turn order works as expected
  2. Run self-play with a single model to verify balanced win rates
  3. Test Seer role with perfect information disclosure to verify information asymmetry mechanics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic bidding system affect the game's outcome compared to a fixed turn order?
- Basis in paper: [explicit] The paper states that the bidding mechanic allows agents to strategically choose when to speak, mirroring real-world discussions. However, it does not provide a direct comparison between bidding and fixed turn order.
- Why unresolved: The paper does not present data comparing the effectiveness of the bidding system to a fixed turn order in terms of win rates or other relevant metrics.
- What evidence would resolve it: A controlled experiment comparing the performance of agents using bidding versus fixed turn order in a statistically significant number of games.

### Open Question 2
- Question: To what extent does the verbosity of GPT-4 models contribute to their lower win rate compared to Gemini 1.5 Pro?
- Basis in paper: [inferred] The paper notes that GPT-4 models tend to be more verbose and that this verbosity was sometimes perceived as suspicious by other agents, potentially contributing to Gemini 1.5 Pro's stronger performance.
- Why unresolved: While the paper observes a correlation between verbosity and win rate, it does not establish a causal relationship or quantify the impact of verbosity on the game's outcome.
- What evidence would resolve it: A controlled experiment manipulating the verbosity of GPT-4 models while keeping other factors constant to isolate the effect of verbosity on win rate.

### Open Question 3
- Question: How does the performance of the Seer role vary across different models and game stages?
- Basis in paper: [explicit] The paper analyzes Seer performance, including the timing of reveals, the accuracy of identifying Werewolves, and the persuasiveness of the Seer's arguments.
- Why unresolved: While the paper provides descriptive statistics on Seer performance, it does not delve into the reasons behind the observed variations or explore how the Seer's performance changes as the game progresses.
- What evidence would resolve it: A detailed analysis of the Seer's decision-making process, including the factors influencing the timing of reveals and the strategies used to persuade other players, across different models and game stages.

## Limitations
- Limited empirical validation of bidding system effectiveness across different model families
- Absence of human comparison baselines to validate evaluation framework metrics
- Uncertainty about framework's generalizability beyond self-play to mixed-model scenarios

## Confidence
- Werewolf Arena framework design and implementation: **High**
- Gemini 1.5 Pro performance as strongest player: **Medium**
- Bidding system's effectiveness in creating natural dialogue: **Low**
- Framework's generalizability to mixed-model and human comparisons: **Low**

## Next Checks
1. **Cross-Model Tournament Analysis**: Run tournaments mixing different model families (e.g., Gemini vs GPT) to assess whether the framework can distinguish between models with different strengths and whether role-specific advantages emerge in mixed settings.

2. **Human Baseline Comparison**: Compare LLM performance against human players in controlled experiments to validate whether the framework's metrics align with human perceptions of strategic reasoning and communication quality.

3. **Bidding Strategy Sensitivity Analysis**: Systematically test how different bidding strategies affect game outcomes by implementing alternative bidding mechanisms (e.g., continuous vs discrete bids, different urgency distributions) and analyzing their impact on turn-taking quality and overall performance.