---
ver: rpa2
title: 'Towards Understanding the Word Sensitivity of Attention Layers: A Study via
  Random Features'
arxiv_id: '2402.02969'
source_url: https://arxiv.org/abs/2402.02969
tags:
- random
- have
- where
- features
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the word sensitivity of attention layers in
  transformers using random features. The authors define word sensitivity as how much
  a single word change affects the output of a feature map.
---

# Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features

## Quick Facts
- **arXiv ID**: 2402.02969
- **Source URL**: https://arxiv.org/abs/2402.02969
- **Reference count**: 40
- **Primary result**: RAF models can distinguish between sentences differing by a single word, while RF models cannot, leading to better generalization for RAF

## Executive Summary
This paper provides a formal characterization of the fundamental difference between fully connected and attention layers in transformers through the lens of word sensitivity. The authors define word sensitivity as how much a single word change affects the output of a feature map. They demonstrate that random attention features (RAF) models exhibit high word sensitivity that remains constant regardless of context length, while random features (RF) models have low word sensitivity that decreases with longer contexts. This theoretical distinction explains why RAF models can better generalize by distinguishing between similar sentences, a capability that RF models lack.

## Method Summary
The authors study word sensitivity through a theoretical framework comparing random features (RF) models and random attention features (RAF) models. They analyze how single word substitutions propagate through these architectures using mathematical proofs and theoretical bounds. The analysis examines the variance of output changes when individual words in the input sequence are modified. They also validate their theoretical findings through experiments on BERT embeddings, comparing the behavior of actual attention mechanisms with their random feature counterparts.

## Key Results
- RAF models maintain high word sensitivity regardless of context length, while RF models show decreasing sensitivity as context grows
- RAF models can distinguish between sentences differing by a single word, enabling better generalization
- Experiments on BERT embeddings confirm the theoretical predictions about word sensitivity differences

## Why This Works (Mechanism)
The paper's core insight is that attention mechanisms fundamentally differ from fully connected layers in how they process and propagate information about individual input tokens. In attention layers, each word's contribution is explicitly weighted through the attention mechanism, creating a pathway for word-level changes to affect the output regardless of context length. In contrast, random feature models process inputs through fixed, word-agnostic transformations where individual word changes become increasingly diluted in longer sequences. This architectural difference manifests as the persistent high word sensitivity in RAF models versus the diminishing sensitivity in RF models.

## Foundational Learning
1. **Word sensitivity** - The sensitivity of a model's output to changes in individual input words; needed to quantify how attention differs from fully connected layers; quick check: measure output variance when substituting single words
2. **Random features (RF) models** - Models using fixed random projections instead of learned weights; needed as a baseline to compare against attention; quick check: implement simple RF layer and measure sensitivity
3. **Random attention features (RAF) models** - Models using random attention weights; needed to isolate attention's unique properties; quick check: create RAF layer and compare sensitivity to RF
4. **Context length effects** - How model behavior changes with input sequence length; needed to understand scalability of attention mechanisms; quick check: vary input length and measure sensitivity changes
5. **Orthogonal inputs assumption** - Theoretical assumption that input words are orthogonal; needed for mathematical tractability in proofs; quick check: verify how results change with correlated inputs
6. **Feature map output variance** - Statistical measure of output stability; needed to quantify word sensitivity formally; quick check: compute variance of outputs under word substitutions

## Architecture Onboarding

**Component Map**: Input words -> Embedding layer -> RF or RAF layer -> Output feature map

**Critical Path**: The attention mechanism in RAF models creates a direct pathway from individual word embeddings to the output through learned attention weights, while RF models use fixed transformations that average over all words.

**Design Tradeoffs**: RAF models gain better generalization through word sensitivity but at higher computational cost; RF models are more efficient but lose the ability to distinguish fine-grained differences between similar inputs.

**Failure Signatures**: RF models fail to distinguish between semantically different sentences that differ by only a few words; attention models may overfit to word-level patterns at the expense of broader context understanding.

**First Experiments**:
1. Measure word sensitivity of a simple RF layer as context length varies
2. Compare RAF layer sensitivity to RF layer sensitivity on identical inputs
3. Test model performance on a sentence similarity task where single-word differences matter

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical analysis focuses on single-layer models, potentially missing complexities in deep transformer architectures
- The orthogonal input words assumption may not reflect real-world text data with semantic correlations
- Limited quantitative empirical validation beyond qualitative comparisons on BERT embeddings

## Confidence
- Theoretical claims about word sensitivity differences: High
- Empirical validation on BERT embeddings: Medium
- Generalizability to multi-layer transformers: Medium

## Next Checks
1. Extend the theoretical analysis to multi-layer transformer architectures and examine how word sensitivity accumulates or changes across layers
2. Conduct systematic experiments measuring word sensitivity on BERT embeddings across multiple tasks, input lengths, and layer depths
3. Evaluate model performance when word sensitivity is explicitly controlled or modified through architectural changes or training procedures