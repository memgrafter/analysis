---
ver: rpa2
title: Off-policy Evaluation with Deeply-abstracted States
arxiv_id: '2406.19531'
source_url: https://arxiv.org/abs/2406.19531
tags:
- state
- policy
- irrelevance
- learning
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurate off-policy evaluation
  (OPE) in large state spaces by introducing a novel iterative state abstraction procedure
  called DSA (Deep State Abstraction). The method leverages Markov state abstractions
  (MSAs) to progressively compress the state space while preserving the Markov property,
  which is crucial for maintaining the accuracy of OPE estimators.
---

# Off-policy Evaluation with Deeply-abstracted States

## Quick Facts
- **arXiv ID**: 2406.19531
- **Source URL**: https://arxiv.org/abs/2406.19531
- **Reference count**: 40
- **Primary result**: Iterative state abstraction (DSA) reduces RMSE in off-policy evaluation by 15-60% on benchmark tasks

## Executive Summary
This paper addresses the challenge of accurate off-policy evaluation (OPE) in large state spaces by introducing a novel iterative state abstraction procedure called DSA (Deep State Abstraction). The method leverages Markov state abstractions (MSAs) to progressively compress the state space while preserving the Markov property, which is crucial for maintaining the accuracy of OPE estimators. By alternating between model-irrelevance and backward-model-irrelevance abstractions at each iteration, DSA achieves substantial state space reduction while maintaining OPE accuracy, outperforming existing methods on environments with large state spaces.

## Method Summary
DSA alternates between two types of Markov State Abstractions (MSAs) at each iteration: model-irrelevance and backward-model-irrelevance. The procedure starts with the original state space and progressively maps it to smaller abstract spaces while preserving the Markov property. At each iteration, the abstraction is combined with the target policy to ensure π-irrelevance. The final deeply-abstracted state space is then used for off-policy evaluation using Fitted Q-Evaluation (FQE). The iterative nature allows for more refined abstractions compared to single-iteration approaches, reducing both variance (through smaller state space) and bias (through better estimation of nuisance functions).

## Key Results
- DSA achieves the smallest RMSE in most tested environments, outperforming baselines by 15-60%
- State abstraction is beneficial for OPE, with post-abstraction estimators consistently outperforming baseline FQE on ground state space
- The iterative procedure is more effective than single-iteration abstractions, as shown by comparisons against single-iteration state abstractions
- DSA demonstrates consistent performance across environments, achieving best or second-best results in all tested cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iteratively alternating between model-irrelevance and backward-model-irrelevance MSAs progressively compresses the state space while preserving the Markov property, leading to more accurate OPE estimators.
- Mechanism: Each iteration reduces state space cardinality and estimation error in nuisance functions (Q-function and MIS ratio). The Markov property is maintained by construction, ensuring identifiability of these functions in the abstract space.
- Core assumption: Both abstraction types are π-irrelevant MSAs that preserve the Markov property.
- Evidence anchors:
  - [abstract] "The core idea is to alternate between two types of MSAs at each iteration: one based on model-irrelevance and another based on backward-model-irrelevance."
  - [section 4.2] "This motivates us to alternate between these two abstractions at each iteration of the procedure."
  - [corpus] Weak evidence - related works focus on single-iteration abstractions, not iterative procedures.
- Break condition: If an abstraction fails to preserve the Markov property or becomes non-π-irrelevant, the OPE estimator's bias increases substantially.

### Mechanism 2
- Claim: The iterative procedure improves MSE by reducing both variance (through smaller state space) and bias (through better estimation of nuisance functions).
- Mechanism: Finer abstractions reduce asymptotic variance of DRL, MIS, and Q-function-based estimators. The bias term decreases as estimation errors κq(ϕ) and κw(ϕ) shrink with state space size.
- Core assumption: The bias-variance decomposition from Theorem 1 holds and the estimation errors decrease with state space cardinality.
- Evidence anchors:
  - [section 4.1] "the finer the abstract state space, the lower the MSE of these estimators"
  - [section 4.1] "Equation (3) provides a bias-variance decomposition for the MSE of DRL"
  - [corpus] Weak evidence - no direct corpus support for iterative improvement of bias/variance tradeoff.
- Break condition: If estimation errors don't decrease with state space size (e.g., due to poor function approximation), the iterative procedure may not improve MSE.

### Mechanism 3
- Claim: The backward-model-irrelevance abstraction extends Allen et al. [2021]'s work to handle history-dependent behavior policies, enabling iterative application.
- Mechanism: By using the time-reversed MDP perspective and relaxing the Markovian behavior policy assumption, the backward-model-irrelevance abstraction remains valid even after state space reduction in earlier iterations.
- Core assumption: The history-dependent behavior policy can be accommodated through the modified backward-model-irrelevance condition (8).
- Evidence anchors:
  - [section 4.2] "we modify the proposed backward-model-irrelevance by employing a history-dependent behavior policy"
  - [section 4.2] "Lemma 2 guarantees that the resulting abstraction ϕ, obtained by applying the refined backward-model-irrelevance condition...remains an MSA"
  - [corpus] Weak evidence - Allen et al. [2021] assumes Markovian behavior policy, no corpus support for history-dependent extension.
- Break condition: If the history-dependent modification fails to preserve the Markov property, the abstraction becomes invalid.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire framework relies on MDP structure - state transitions, rewards, and policies must satisfy the Markov property for abstractions to work.
  - Quick check question: Can you explain why the Markov property is crucial for both the original MDP and the abstract MDPs?

- Concept: Importance Sampling (IS) ratios
  - Why needed here: IS ratios (both sequential and marginalized) are fundamental to many OPE methods, and their behavior changes under state abstraction.
  - Quick check question: How does the marginalization of IS ratios help reduce variance compared to sequential IS?

- Concept: State abstraction types (model-irrelevance vs backward-model-irrelevance)
  - Why needed here: The paper alternates between these two types, each preserving different aspects needed for OPE.
  - Quick check question: What's the key difference between what model-irrelevance and backward-model-irrelevance preserve in the MDP structure?

## Architecture Onboarding

- Component map: Raw trajectories -> MSA training (alternating Type-I and Type-II) -> Deeply-abstracted state space -> FQE application -> OPE evaluation
- Critical path: 1) Train Type-I MSA on ground state space, 2) Apply abstraction and train Type-II MSA on result, 3) Repeat alternating pattern K times, 4) Apply FQE to final abstracted space, 5) Evaluate OPE performance
- Design tradeoffs: More iterations (larger K) → smaller state space but potentially higher abstraction estimation error; Abstraction complexity vs. estimation accuracy in downstream FQE; Memory vs. performance when storing intermediate abstract spaces
- Failure signatures: MSE increases with more iterations (indicates estimation error accumulation); Abstract state space collapses to few states (indicates poor abstraction learning); Training instability in abstraction neural networks (indicates hyperparameter issues)
- First 3 experiments: 1) Single-iteration comparison: Type-I vs Type-II MSA on Cartpole-v0 with 300D state, 2) Iteration sensitivity: DSA with K=1,2,3,4 on LunarLander-v2, 3) Baseline comparison: DSA vs STAR vs MSA vs Auto-Encoder on Atlantis-ram-v5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of iterations K in the DSA procedure for different environments?
- Basis in paper: Explicit - "determining the optimal number of iterations, K, remains an open question. This choice involves a trade-off: a larger K yields a smaller abstracted state space, but the estimation error for learning the abstraction may accumulate over iterations."
- Why unresolved: The paper notes that while empirically K=4 works well for complex environments like Atlantis-ram-v5 and SpaceInvaders-ram-v5, there's no theoretical guidance on how to choose K optimally, and the trade-off between abstraction quality and estimation error accumulation is not well understood.
- What evidence would resolve it: A theoretical framework for determining optimal K based on environment characteristics and data properties, or empirical results showing how performance varies with K across different environments and data regimes.

### Open Question 2
- Question: How can we account for the estimation error incurred during the construction of abstractions in the theoretical analysis?
- Basis in paper: Explicit - "our theoretical analysis does not account for the estimation error incurred during the construction of the abstractions."
- Why unresolved: The current theoretical framework only analyzes the bias-variance decomposition of OPE estimators applied to abstract spaces, but doesn't consider errors from the abstraction learning process itself, which could be significant.
- What evidence would resolve it: A unified theoretical framework that incorporates both abstraction learning errors and OPE estimation errors, potentially using techniques from statistical learning theory or empirical process theory.

### Open Question 3
- Question: Are there more efficient ways to construct abstractions that satisfy the proposed irrelevance conditions?
- Basis in paper: Inferred - While the paper proposes two types of MSAs (model-irrelevance and backward-model-irrelevance), it acknowledges these are adaptations of existing methods and doesn't explore whether more efficient construction methods exist.
- Why unresolved: The current implementation relies on adapting existing algorithms for model-irrelevance and backward-model-irrelevance, but there's no systematic exploration of alternative construction methods or optimization of the existing approaches.
- What evidence would resolve it: Empirical comparisons of different abstraction construction methods, or theoretical results on the optimality of the proposed methods under various assumptions.

### Open Question 4
- Question: How does the performance of DSA vary with different target policy types beyond state-agnostic policies?
- Basis in paper: Inferred - The experiments focus on state-agnostic target policies, but the paper doesn't explore how DSA performs with more complex policy types or policies that depend heavily on the irrelevant features.
- Why unresolved: The current analysis is limited to policies that don't depend on state information, which is a restrictive assumption that doesn't cover many practical scenarios where policies are state-dependent.
- What evidence would resolve it: Experiments with different policy types (e.g., policies that depend on irrelevant features), or theoretical analysis of DSA's performance under various policy complexity assumptions.

## Limitations

- Empirical validation limited to only 4 MuJoCo tasks, which is a relatively small and homogeneous set of environments
- Theoretical analysis assumes access to "reasonably accurate" transition and reward models without providing clear guidance on verification
- Performance claims based on a narrow set of policy types (state-agnostic), limiting generalizability to real-world scenarios

## Confidence

- **High confidence**: The core mechanism of iterative state abstraction (Mechanism 1) is well-supported by the paper's theoretical framework and the fundamental properties of MSAs
- **Medium confidence**: The bias-variance tradeoff improvement (Mechanism 2) is theoretically sound but relies on assumptions about estimation error behavior that need more empirical validation
- **Low confidence**: The extension to history-dependent behavior policies (Mechanism 3) is the most speculative claim, with limited empirical evidence and weak corpus support

## Next Checks

1. **Robustness testing**: Evaluate DSA performance on environments with varying levels of stochasticity and reward structures to test the method's robustness beyond the current MuJoCo tasks

2. **Iteration sensitivity analysis**: Systematically vary the number of iterations K and measure the tradeoff between abstraction quality and estimation error to identify optimal iteration counts for different environment types

3. **Assumption verification protocol**: Develop concrete metrics and procedures to verify the "reasonable accuracy" assumption for transition and reward models before applying DSA, ensuring reliable deployment in practice