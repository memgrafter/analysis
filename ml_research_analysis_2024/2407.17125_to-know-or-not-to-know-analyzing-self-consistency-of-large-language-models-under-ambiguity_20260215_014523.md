---
ver: rpa2
title: To Know or Not To Know? Analyzing Self-Consistency of Large Language Models
  under Ambiguity
arxiv_id: '2407.17125'
source_url: https://arxiv.org/abs/2407.17125
tags:
- entity
- entities
- provide
- llms
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how large language models (LLMs) handle\
  \ entity type ambiguity and self-consistency. The authors propose an evaluation\
  \ protocol that disentangles a model\u2019s knowledge of ambiguous entities from\
  \ its ability to apply that knowledge correctly."
---

# To Know or Not To Know? Analyzing Self-Consistency of Large Language Models under Ambiguity

## Quick Facts
- arXiv ID: 2407.17125
- Source URL: https://arxiv.org/abs/2407.17125
- Authors: Anastasiia Sedova; Robert Litschko; Diego Frassinelli; Benjamin Roth; Barbara Plank
- Reference count: 15
- One-line primary result: LLMs achieve only 80% average accuracy on ambiguous entity prompts and struggle with self-consistency, especially for non-preferred readings

## Executive Summary
This study investigates how large language models handle entity type ambiguity and self-consistency. The authors propose an evaluation protocol that disentangles a model's knowledge of ambiguous entities from its ability to apply that knowledge correctly. They test six state-of-the-art LLMs on 49 ambiguous entities across seven types, using four studies: verifying knowledge, eliciting entity reading preferences, testing knowledge application, and checking self-consistency. Results show LLMs perform poorly with ambiguous prompts (80% accuracy on average), struggle to consistently apply knowledge (especially for non-preferred entity readings), and cannot self-verify their answers. Performance is significantly biased toward preferred readings, often correlated with entity popularity.

## Method Summary
The evaluation protocol consists of four studies: (1) Knowledge verification - confirms models know different entity readings, (2) Eliciting preferences - identifies preferred vs. alternative readings, (3) Knowledge to application - tests ability to apply knowledge under ambiguity, (4) Applying to knowing - tests self-consistency. The core architecture is a prompt-response evaluation loop with manual annotation. The study uses 49 ambiguous entities across 7 types (animal, fruit, myth, person, location, abstract, company) and tests six state-of-the-art LLMs. Manual annotation determines whether responses correctly disambiguate entities and apply appropriate knowledge.

## Key Results
- LLMs achieve only 80% average accuracy on ambiguous entity prompts
- Performance drops to 74.5% for non-preferred entity readings compared to 85.5% for preferred readings
- No tested model could confirm all knowledge provided in the previous study, indicating poor self-consistency
- Entity popularity in training data correlates with model preference bias toward certain readings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle to apply factual knowledge correctly when entity type ambiguity is present, even if they possess that knowledge.
- Mechanism: Entity ambiguity introduces multiple valid interpretations for the same term, and LLMs rely on contextual cues to disambiguate. Without sufficient cues, they default to preferred readings (often the most frequent or popular interpretation in training data), leading to incorrect answers for non-preferred readings.
- Core assumption: The model's internal knowledge includes both readings, but application requires correct disambiguation which fails under ambiguity.
- Evidence anchors:
  - [abstract]: "LLMs perform poorly with ambiguous prompts (80% accuracy on average)"
  - [section]: "Despite seemingly simple task, LLMs fail to disambiguate and handle entities consistently"
- Break condition: When prompts provide explicit disambiguation cues (e.g., "Provide the founding year for company Apple"), performance improves significantly.

### Mechanism 2
- Claim: LLM performance is systematically biased toward preferred entity readings, which correlates with entity popularity in training data.
- Mechanism: During pretraining, entities appear in different contexts with varying frequencies. The most frequent interpretation becomes the "preferred reading" in the model's representation space. When prompted ambiguously, the model defaults to this dominant interpretation rather than considering alternative meanings.
- Core assumption: Training data frequency directly influences the model's preference hierarchy for entity interpretations.
- Evidence anchors:
  - [abstract]: "Performance is significantly biased toward preferred readings, often correlated with entity popularity"
  - [section]: "A model's preferred reading of an entity is influenced by its frequency in the pre-training corpus"
- Break condition: When alternative readings have higher training data frequency than preferred readings.

### Mechanism 3
- Claim: LLMs cannot self-verify their answers, even when they successfully disambiguate entities.
- Mechanism: Self-consistency requires the model to maintain coherence between its knowledge state and generated responses. Under ambiguity, even successful disambiguation doesn't guarantee the model can verify that its response aligns with its internal knowledge, particularly for non-preferred readings.
- Core assumption: The model's self-consistency mechanism is weaker than its initial knowledge application, especially under ambiguous conditions.
- Evidence anchors:
  - [abstract]: "Cannot self-verify their answers"
  - [section]: "None of the tested models confirmed all the knowledge provided in the previous study"
- Break condition: When models are prompted with unambiguous references or when verification is separated from generation.

## Foundational Learning

- Concept: Entity disambiguation and polysemy
  - Why needed here: The entire study hinges on understanding how models handle entities with multiple meanings. Without grasping polysemy, one cannot understand why ambiguity creates problems.
  - Quick check question: What is the difference between "Apple" the fruit and "Apple" the company, and why would this distinction matter for a language model?

- Concept: Knowledge application vs. knowledge possession
  - Why needed here: The study explicitly disentangles "knowing" from "applying" knowledge. Understanding this distinction is crucial for interpreting why models might know multiple readings but fail to apply them correctly.
  - Quick check question: Can you explain why a model might correctly identify that "Jaguar" can mean both an animal and a car company, but still answer incorrectly when asked about its "speed"?

- Concept: Self-consistency and internal knowledge coherence
  - Why needed here: The study investigates whether models can verify their own answers against their internal knowledge. This requires understanding what self-consistency means in the context of LLMs.
  - Quick check question: If a model states that "Tesla was founded in 2003," what would it need to do to demonstrate self-consistency when asked to verify this claim?

## Architecture Onboarding

- Component map: Study 1 (Knowledge verification) -> Study 2 (Eliciting preferences) -> Study 3 (Knowledge to application) -> Study 4 (Applying to knowing) -> Analysis of accuracy, preference bias, and self-consistency
- Critical path: Prompt generation → LLM response → Manual annotation/evaluation → Analysis of accuracy, preference bias, and self-consistency. The most critical component is the manual annotation process, which determines whether responses correctly disambiguate entities and apply appropriate knowledge.
- Design tradeoffs: The study uses ambiguous entities with at least two readings (company vs. non-company) to create a controlled environment for testing disambiguation. Tradeoff: this simplification may not capture all real-world ambiguity scenarios but provides clear experimental conditions.
- Failure signatures: Models consistently default to preferred readings under ambiguity (80% accuracy overall, but only 74.5% for non-preferred readings). Self-consistency failures occur when models cannot verify their own responses, especially for non-preferred readings. High correlation between entity popularity and performance indicates frequency bias.
- First 3 experiments:
  1. Implement Study 1: Verify knowledge possession by prompting models with "Tell me about <entity-type> <entity>" and checking if they generate meaningful, factually correct output for all entity types.
  2. Implement Study 2: Test entity reading preferences by grouping entities and analyzing which reading the model consistently selects as the common attribute.
  3. Implement Study 3: Test knowledge application by prompting with ambiguous questions like "Provide the <entity-property> for <entity>" and comparing performance against non-ambiguous prompts with explicit entity type hints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does entity ambiguity affect the reliability of LLM-generated factual knowledge in downstream applications?
- Basis in paper: [explicit] The paper discusses how LLMs struggle with entity ambiguity, achieving only 80% accuracy on average, and how this affects their self-consistency and trustworthiness.
- Why unresolved: The paper focuses on the direct impact of ambiguity on model performance but does not explore how this translates to real-world applications where factual accuracy is critical.
- What evidence would resolve it: Empirical studies testing LLM outputs in practical scenarios (e.g., question-answering systems, information retrieval) where entity ambiguity is common, measuring the impact on task performance and user trust.

### Open Question 2
- Question: Can training strategies or architectural modifications improve LLM performance on ambiguous entities?
- Basis in paper: [inferred] The paper identifies entity ambiguity as a critical challenge for LLM trustworthiness but does not propose or evaluate potential solutions to mitigate this issue.
- Why unresolved: The study focuses on diagnosing the problem rather than exploring methods to address it, such as fine-tuning on disambiguated datasets or incorporating entity resolution mechanisms.
- What evidence would resolve it: Experiments comparing LLM performance on ambiguous entities before and after applying targeted training techniques or architectural changes designed to handle ambiguity.

### Open Question 3
- Question: How does the degree of polysemy in entity types influence LLM performance and self-consistency?
- Basis in paper: [inferred] The paper notes that the properties of entities might contain ambiguity but does not thoroughly address how varying degrees of polysemy affect model behavior.
- Why unresolved: The study adopts a binary distinction between company and non-company readings, but does not explore how more nuanced levels of ambiguity might impact performance.
- What evidence would resolve it: Analysis of LLM performance across entities with varying degrees of polysemy, correlating the number of interpretations with accuracy and self-consistency metrics.

## Limitations
- Evaluation focuses exclusively on entity type ambiguity with exactly two readings (company vs. non-company)
- Manual annotation process for determining correct responses introduces potential subjectivity
- Corpus-based popularity analysis correlates with performance but cannot establish causation for the frequency-preference relationship

## Confidence
- **High**: LLMs perform significantly worse on ambiguous entity prompts compared to disambiguated prompts (80% vs 90.2% accuracy)
- **Medium**: Performance bias toward preferred readings correlates with entity popularity in training data
- **Medium**: LLMs cannot consistently self-verify their own responses under ambiguity

## Next Checks
1. **Multi-reading validation**: Test the same methodology with entities having three or more distinct readings to verify if the two-reading framework captures the full scope of ambiguity challenges.
2. **Cross-corpus frequency analysis**: Conduct controlled experiments manipulating training data frequency of alternative readings to establish causal relationships between corpus statistics and model preferences.
3. **Self-consistency stress test**: Design prompts that explicitly ask models to compare their current answer against previously stated knowledge to measure the gap between knowledge possession and self-verification capabilities.