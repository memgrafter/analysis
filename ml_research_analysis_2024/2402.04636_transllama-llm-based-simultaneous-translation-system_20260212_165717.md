---
ver: rpa2
title: 'TransLLaMa: LLM-based Simultaneous Translation System'
arxiv_id: '2402.04636'
source_url: https://arxiv.org/abs/2402.04636
tags:
- translation
- source
- wait
- target
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TransLLaMa, a simultaneous translation system
  that fine-tunes decoder-only LLMs to perform both translation and segmentation without
  a separate policy. The key idea is to train the LLM on causally aligned source-target
  pairs with special "wait" tokens, enabling it to decide when to translate or wait
  for more input.
---

# TransLLaMa: LLM-based Simultaneous Translation System

## Quick Facts
- arXiv ID: 2402.04636
- Source URL: https://arxiv.org/abs/2402.04636
- Authors: Roman Koshkin; Katsuhito Sudoh; Satoshi Nakamura
- Reference count: 28
- Primary result: LLM fine-tuning achieves BLEU scores approaching or exceeding state-of-the-art baselines for simultaneous translation

## Executive Summary
This paper introduces TransLLaMa, a simultaneous translation system that fine-tunes decoder-only LLMs (Llama-2 13B and 70B) to perform both translation and segmentation without a separate policy. The key innovation is training the LLM on causally aligned source-target pairs with special "wait" tokens, enabling it to decide when to translate or wait for more input. Results show that fine-tuned LLMs achieve BLEU scores comparable to or exceeding state-of-the-art baselines on English-German and English-Russian translation tasks, while zero-shot GPT-4 evaluation also shows promising results.

## Method Summary
The approach involves fine-tuning decoder-only LLMs on causally aligned source-target pairs where <WAIT> tokens are inserted into target sentences to ensure causal alignment. The LLM learns to generate either translation tokens or <WAIT> tokens based on partial source context. During inference, a modified wait-k strategy controls the latency-quality tradeoff. For speech-to-text translation, Whisper ASR processes audio input before feeding to the LLM. The system uses LoRA fine-tuning with 4-bit quantization for efficiency.

## Key Results
- Llama-2 13B and 70B models achieve BLEU scores approaching or exceeding state-of-the-art baselines on en-de and en-ru tasks
- Fine-tuned LLMs demonstrate strong performance with limited training data (4000 sentences)
- Zero-shot GPT-4 evaluation shows promising results, suggesting potential for future improvements
- Causal alignment through <WAIT> tokens enables effective policy-free simultaneous translation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal alignment of source and target tokens through <WAIT> tokens enables the LLM to learn when to translate vs. wait.
- Mechanism: By inserting <WAIT> tokens into the target sequence such that no target content word appears before its corresponding source word, the fine-tuning dataset becomes causally aligned. The LLM learns to generate <WAIT> tokens when more source context is needed, otherwise it outputs target tokens.
- Core assumption: The LLM can learn to recognize causal dependencies in the fine-tuning data and generalize this to decide when to generate <WAIT> tokens during inference.
- Evidence anchors:
  - [abstract] "The causality of the source is guaranteed by inserting one or more <WAIT> tokens into the target sentence to ensure that target content words never appear earlier than their closest equivalents in the source."
  - [section] "In such a dataset, a target word never appears before its corresponding (when such correspondence can be established) source word in time, which is defined as the number of words from the sentence start."

### Mechanism 2
- Claim: Supervised fine-tuning on causally aligned data enables the LLM to learn a "policy-free" approach to simultaneous translation.
- Mechanism: The LLM is fine-tuned on the causal alignment task, learning to generate either the next target token or a <WAIT> token based on the partial source context. This eliminates the need for a separate policy model to decide when to read or write.
- Core assumption: The LLM can learn to make optimal read/write decisions based solely on the fine-tuning data without requiring a separate policy component.
- Evidence anchors:
  - [abstract] "This obviates the need for a separate policy and enables the LLM to perform English-German and English-Russian SiMT tasks with BLEU scores that are comparable to those of specific state-of-the-art baselines."
  - [section] "We call our model policy-free, because as a result of fine-tuning on a causally aligned dataset the LLM becomes capable of deciding when to output translation and when to read in more of the source, without requiring a separate policy."

### Mechanism 3
- Claim: Fine-tuning on a small dataset of causally aligned pairs is sufficient to achieve state-of-the-art performance.
- Mechanism: The LLM is fine-tuned on a relatively small dataset (4000 sentences for training, 100 for validation) of causally aligned source-target pairs. Despite the small size, the model achieves BLEU scores comparable to or exceeding state-of-the-art baselines.
- Core assumption: The LLM's pre-training provides a strong foundation that allows it to learn the simultaneous translation task effectively even with limited fine-tuning data.
- Evidence anchors:
  - [section] "We randomly selected 4000 sentences for training and 100 sentences for validation."
  - [section] "Results show that fine-tuned LLMs (Llama-2 13B and 70B) achieve BLEU scores approaching or exceeding state-of-the-art baselines on English-German and English-Russian translation tasks."

## Foundational Learning

- Concept: Causal alignment and the role of <WAIT> tokens
  - Why needed here: Understanding how <WAIT> tokens create a causally aligned dataset is crucial for grasping the core mechanism of the approach.
  - Quick check question: Why is it important that no target content word appears before its corresponding source word in the aligned dataset?

- Concept: Fine-tuning decoder-only LLMs for sequence-to-sequence tasks
  - Why needed here: The approach relies on fine-tuning a decoder-only LLM (like Llama-2) to perform simultaneous translation, which is typically done by encoder-decoder models.
  - Quick check question: How does the fine-tuning objective (Eq. 1) differ from the typical language modeling objective?

- Concept: Evaluation metrics for simultaneous translation (BLEU, AL, LAAL)
  - Why needed here: Understanding the metrics used to evaluate the model's performance is essential for interpreting the results and comparing with baselines.
  - Quick check question: What is the difference between Average Lagging (AL) and Length Adaptive Average Lagging (LAAL), and why are both used?

## Architecture Onboarding

- Component map: ASR (Whisper) -> LLM (Llama-2) -> Output Generator

- Critical path:
  1. Prepare causally aligned fine-tuning data
  2. Fine-tune the LLM on the aligned data
  3. At inference, process audio with ASR (for S2TT)
  4. Feed partial source and target into LLM
  5. LLM generates next token or <WAIT>
  6. Update prompt and repeat until <EOS>

- Design tradeoffs:
  - Using a decoder-only LLM vs. encoder-decoder model
  - Tradeoff between translation quality and latency controlled by k
  - Impact of system message length on inference speed
  - 4-bit vs. 16-bit quantization for memory/speed tradeoff

- Failure signatures:
  - Poor translation quality: LLM not generating appropriate <WAIT> tokens or outputting incorrect translations
  - Excessive latency: LLM generating too many <WAIT> tokens or k set too high
  - Slow inference: Large system message, 4-bit quantization, or ASR delays

- First 3 experiments:
  1. Fine-tune Llama-2 13B on the causally aligned dataset and evaluate T2TT performance on the MuST-C v2.0 test set for en-de and en-ru.
  2. Implement the S2TT pipeline with Whisper ASR and evaluate the same model on the TED-TST-2023 test set.
  3. Compare the performance of the fine-tuned model with zero-shot GPT-4 on the same test sets to assess the impact of fine-tuning.

## Open Questions the Paper Calls Out

- Question: What is the impact of system message length on SiMT performance and latency?
  - Basis in paper: [explicit] The paper notes that the system message is often longer than the source sentence itself and identifies it as a performance bottleneck. Removing the system message speeds up inference with no noticeable quality drop for supervised fine-tuned models.

## Limitations

- The approach relies heavily on SimAlign for establishing word-level correspondences, with limited validation of alignment quality
- Performance claims are based on limited test sets (102 sentences per language pair) without extensive ablation studies
- Zero-shot GPT-4 results lack sufficient detail about prompt engineering and evaluation methodology

## Confidence

**High Confidence**: The core mechanism of using <WAIT> tokens to create causally aligned training data is technically sound and well-grounded in sequence-to-sequence learning principles.

**Medium Confidence**: The reported BLEU scores and latency metrics are plausible given the model architecture and training setup, but the limited test set size reduces confidence in the magnitude of improvements claimed.

**Low Confidence**: The zero-shot GPT-4 results are presented without sufficient detail about prompt engineering or evaluation methodology.

## Next Checks

1. **Alignment Quality Analysis**: Conduct a systematic evaluation of SimAlign's output quality by manually verifying word-level correspondences for a sample of aligned sentence pairs. Calculate alignment accuracy and analyze failure cases to understand how alignment errors might impact model performance.

2. **Training Data Scaling Study**: Replicate the fine-tuning experiments with varying dataset sizes (e.g., 500, 1000, 2000, 4000, 8000 sentences) to determine whether the claimed data efficiency is genuine or an artifact of the specific 4000-sentence subset used in the paper.

3. **ASR Error Propagation Evaluation**: Implement a controlled experiment that injects synthetic ASR errors into the input stream at varying rates (0%, 5%, 10%, 20%) to measure how translation quality degrades with ASR accuracy. Compare this degradation pattern against a baseline system that receives perfect transcriptions.