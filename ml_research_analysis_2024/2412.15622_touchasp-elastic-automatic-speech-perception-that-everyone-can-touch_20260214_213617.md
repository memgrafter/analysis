---
ver: rpa2
title: 'TouchASP: Elastic Automatic Speech Perception that Everyone Can Touch'
arxiv_id: '2412.15622'
source_url: https://arxiv.org/abs/2412.15622
tags:
- data
- speech
- arxiv
- recognition
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report introduces TouchASP, an elastic automatic speech perception
  system combining elastic MoE (eMoE) training with large-scale data collection and
  multi-task learning. eMoE enables dynamic scaling of expert routing during inference,
  allowing a single model to be deployed across devices with varying computational
  capabilities.
---

# TouchASP: Elastic Automatic Speech Perception that Everyone Can Touch
## Quick Facts
- arXiv ID: 2412.15622
- Source URL: https://arxiv.org/abs/2412.15622
- Reference count: 40
- Key outcome: eMoE with large-scale data achieves CER reduction from 4.98% to 2.45% on SpeechIO testsets

## Executive Summary
TouchASP is an elastic automatic speech perception system that combines elastic Mixture of Experts (eMoE) training with large-scale data collection and multi-task learning. The system enables dynamic scaling of expert routing during inference, allowing deployment across devices with varying computational capabilities. By gathering and processing 1 million hours of diverse speech data from the internet, TouchASP achieves significant performance improvements while extending beyond speech recognition to support multilingual transcription, language identification, age/gender recognition, emotion detection, and sound event detection.

## Method Summary
TouchASP employs an elastic MoE (eMoE) architecture with dynamic expert routing during training and inference, enabling a single model to adapt to different computational constraints. The system includes a data pipeline that collects diverse audio from the internet, processes it through dual ASR transcription (whisperX and Paraformer), and filters low-quality data using Rover consistency checks. The multi-task learning framework trains on multiple speech perception tasks simultaneously using a unified token-based output format, with specialized tokens for different tasks including speech recognition, language identification, age/gender/emotion recognition, and sound event detection.

## Key Results
- Achieves CER reduction from 4.98% to 2.45% on SpeechIO testsets
- Demonstrates dynamic scaling capability across devices with different computational constraints
- Successfully implements multi-task learning across six speech perception tasks
- Processes and filters 1 million hours of internet speech data for training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Elastic Mixture of Experts (eMoE) enables dynamic model scaling without retraining
- Mechanism: eMoE uses dynamic expert routing during training and prunes experts during inference based on runtime resource constraints, allowing the same model to adapt to devices with varying computational capabilities
- Core assumption: Expert grouping strategy (power configuration) allows for meaningful performance vs efficiency tradeoffs
- Evidence anchors:
  - [abstract]: "eMoE enables dynamic scaling of expert routing during inference, allowing a single model to be deployed across devices with varying computational capabilities"
  - [section]: "eMoE employs the dynamic expert strategy, and during inference, the number of experts is pruned to adapt to devices with different capabilities"
  - [corpus]: Weak - no direct corpus evidence, but related work shows MoE effectiveness in scaling
- Break condition: If expert routing becomes too sparse during pruning, model performance degrades significantly due to loss of important information

### Mechanism 2
- Claim: Large-scale weak supervision data collection improves model performance
- Mechanism: The data pipeline gathers diverse audio from the internet, transcribes it using multiple ASR models, and filters low-quality data based on consistency between transcriptions
- Core assumption: Multiple independent ASR models are unlikely to make the same errors, so consistent transcriptions indicate high-quality data
- Evidence anchors:
  - [abstract]: "A data pipeline is developed to gather and process 1 million hours of diverse speech data from the internet, achieving a significant reduction in Character Error Rate (CER) from 4.98% to 2.45% on SpeechIO testsets"
  - [section]: "We found that a single ASR model may make mistakes, but the probability of both ASR models making mistakes simultaneously is significantly lower"
  - [corpus]: Weak - no direct corpus evidence, but related papers on data scaling support this approach
- Break condition: If the data diversity is too low or filtering thresholds are too strict, the data pipeline may fail to provide sufficient training examples

### Mechanism 3
- Claim: Multi-task learning framework extends model capabilities beyond speech recognition
- Mechanism: The model is trained on multiple speech perception tasks (language identification, age/gender recognition, emotion detection, sound event detection) using a unified token-based output format
- Core assumption: Different speech perception tasks can share representations and mutually benefit from joint training
- Evidence anchors:
  - [abstract]: "The system extends beyond speech recognition to support multilingual and multi-dialect transcription, language identification, age and gender recognition, emotion detection, and sound event detection"
  - [section]: "By training on a multi-task format, Whisper[12] could perform transcription, translation, voice activity detection, alignment, and language identification"
  - [corpus]: Weak - no direct corpus evidence, but multi-task learning is well-established in NLP
- Break condition: If tasks have conflicting objectives or require incompatible representations, multi-task learning may harm performance on individual tasks

## Foundational Learning

- Concept: Mixture of Experts (MoE)
  - Why needed here: MoE enables model scaling by activating only a subset of parameters per input, reducing computational cost while maintaining capacity
  - Quick check question: How does MoE differ from traditional dense models in terms of parameter utilization?

- Concept: Weak supervision and data filtering
  - Why needed here: Large-scale data collection from the internet requires automated quality control to ensure useful training examples
  - Quick check question: What are the risks of using weak supervision compared to human-labeled data?

- Concept: Multi-task learning and shared representations
  - Why needed here: Multiple speech perception tasks can benefit from shared feature extraction and joint optimization
  - Quick check question: How might multi-task learning affect the model's ability to specialize in individual tasks?

## Architecture Onboarding

- Component map: Data collection -> Preprocessing (VAD, ASR, Rover) -> eMoE training -> Expert pruning -> Multi-task fine-tuning -> Inference
- Critical path: Data collection → Preprocessing → eMoE training → Expert pruning → Multi-task fine-tuning → Inference
- Design tradeoffs:
  - Model size vs. inference efficiency (more experts = better performance but higher cost)
  - Data diversity vs. quality (more data = better generalization but potential noise)
  - Task complexity vs. joint optimization (more tasks = broader capabilities but potential interference)
- Failure signatures:
  - High CER on test sets indicates data quality or model architecture issues
  - Inconsistent task performance suggests multi-task learning conflicts
  - Slow inference times may indicate inefficient expert pruning
- First 3 experiments:
  1. Verify eMoE routing works by testing different expert configurations on a validation set
  2. Test data pipeline quality by comparing filtered vs. unfiltered data performance
  3. Validate multi-task learning by training on subsets of tasks and measuring cross-task transfer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dynamic expert sampling strategy in eMoE affect model convergence compared to static expert selection methods?
- Basis in paper: [explicit] The paper mentions using different sampling probabilities for different groups to avoid unbalanced training, but does not provide detailed convergence analysis.
- Why unresolved: The paper only mentions the sampling strategy but does not provide empirical evidence comparing convergence rates with static methods.
- What evidence would resolve it: A controlled experiment comparing training loss curves and convergence speed between eMoE's dynamic sampling and traditional static expert selection.

### Open Question 2
- Question: What is the optimal balance between shared and non-shared experts in the DeepSeekMoE architecture for speech recognition tasks?
- Basis in paper: [explicit] The paper explores different configurations (S=0,1,2,4; N=15; K=3) but does not provide a systematic analysis of the trade-offs.
- Why unresolved: While multiple configurations are tested, the paper does not provide a clear framework for determining the optimal balance.
- What evidence would resolve it: A comprehensive ablation study varying the number of shared and non-shared experts across multiple tasks and datasets.

### Open Question 3
- Question: How does the data retention rate vary across different languages and domains, and what factors influence this variation?
- Basis in paper: [explicit] The paper mentions that audiobook data has higher retention rates than outdoor live broadcast data, but does not provide detailed analysis across languages.
- Why unresolved: The paper only provides a general observation about domain differences but does not explore language-specific patterns or contributing factors.
- What evidence would resolve it: A detailed analysis of retention rates across multiple languages and domains, including acoustic characteristics and data quality metrics.

### Open Question 4
- Question: What is the impact of task interference in the multi-task learning framework, particularly between speech recognition and other perception tasks?
- Basis in paper: [inferred] The paper mentions that tasks can benefit from each other but does not provide evidence of potential negative interference.
- Why unresolved: While positive transfer is mentioned, the paper does not investigate whether some tasks might negatively impact others.
- What evidence would resolve it: A systematic study isolating each task and measuring performance changes when adding or removing other tasks from the training setup.

## Limitations
- Lack of detailed technical specifications and empirical validation data
- No specific dataset names, test conditions, or statistical significance measures provided
- Multi-task learning claims remain theoretical without performance metrics for individual tasks
- Implementation details for eMoE routing strategies remain underspecified

## Confidence
- High confidence: The core concept of eMoE enabling dynamic scaling is well-established in literature
- Medium confidence: The claimed CER improvement is plausible given the scale of data and architecture
- Low confidence: Multi-task learning performance claims and specific implementation details remain underspecified

## Next Checks
1. Implement controlled experiments comparing TouchASP performance against baseline ASR models using standardized benchmark datasets
2. Conduct ablation studies on the eMoE architecture to verify that expert pruning configurations maintain claimed performance-efficiency tradeoffs
3. Validate the multi-task learning framework by measuring task-specific performance degradation when adding/removing individual tasks from joint training