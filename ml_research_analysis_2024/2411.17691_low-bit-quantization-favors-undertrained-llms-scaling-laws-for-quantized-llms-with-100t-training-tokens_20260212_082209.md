---
ver: rpa2
title: 'Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized
  LLMs with 100T Training Tokens'
arxiv_id: '2411.17691'
source_url: https://arxiv.org/abs/2411.17691
tags:
- training
- quantization
- tokens
- low-bit
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work reveals that low-bit quantization performs better on\
  \ undertrained large language models (LLMs) than fully trained ones, showing that\
  \ smaller models or those with more training tokens experience greater quantization-induced\
  \ degradation (QiD). By analyzing over 1500 quantized checkpoints of Pythia models\
  \ (160M to 12B parameters, trained on up to 206B tokens), the authors derive scaling\
  \ laws modeling QiD as a function of model size, training tokens, and bit width:\
  \ \u0394qLoss(N,D,P) = k\xB7D^\u03B2/(N^\u03B1\xB7P^\u03B3), where \u03B2=0.5316,\
  \ \u03B1=0.2276, \u03B3=5.4812."
---

# Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens

## Quick Facts
- **arXiv ID**: 2411.17691
- **Source URL**: https://arxiv.org/abs/2411.17691
- **Reference count**: 9
- **Primary result**: Low-bit quantization performs better on undertrained LLMs than fully trained ones, with scaling laws predicting severe degradation for 2-3 bit quantization at 100 trillion training tokens

## Executive Summary
This paper reveals a counterintuitive finding: low-bit quantization performs better on undertrained large language models (LLMs) than on fully trained ones. Through extensive experiments on over 1,500 quantized Pythia checkpoints ranging from 160M to 12B parameters and trained up to 206B tokens, the authors derive scaling laws that model quantization-induced degradation (QiD) as a function of model size, training tokens, and bit width. The key insight is that undertrained models with significant weight fluctuations are more robust to quantization-induced precision loss, while fully trained models that have stabilized weights are more sensitive to quantization errors. The authors propose using QiD as a metric to assess training levels and predict that 2-3 bit quantization will cause severe performance degradation for models trained with 100 trillion tokens, posing significant challenges for future low-bit LLM deployment.

## Method Summary
The authors use GPTQ quantization to create 2-bit, 3-bit, and 4-bit versions of Pythia LLM checkpoints at various training stages. They measure quantization-induced degradation (QiD) as the difference in cross-entropy loss between pre- and post-quantization models on 1,000 randomly sampled texts from RefinedWeb dataset. Using this extensive dataset of quantized checkpoints, they fit scaling laws of the form ΔqLoss(N,D,P) = k·D^β/(N^α·P^γ) to model how QiD varies with model size (N), training tokens (D), and bit width (P). The authors also compare their findings with native low-bit training approaches like BitNet and release all quantized checkpoints for future research.

## Key Results
- Undertrained LLMs show significantly less quantization-induced degradation than fully trained models
- Scaling laws predict that 2-3 bit quantization will cause severe degradation for models trained with 100 trillion tokens
- Native low-bit training (BitNet) shows similar undertrained-favoring trends, though the performance gap manifests later
- Larger models and higher bit widths both reduce quantization-induced degradation
- All quantized checkpoints are released for community use and further research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-bit quantization performs better on undertrained LLMs because early training stages exhibit high weight fluctuation, making the model robust to small perturbations introduced by quantization.
- Mechanism: During early training, weights undergo significant changes across checkpoints, so quantization-induced precision loss has limited impact. In later training, weights stabilize and rely more on precision for fine-tuning, making quantization-induced degradation more pronounced.
- Core assumption: The training dynamics follow the information bottleneck theory, where early training has large mean/small variance gradients, and late training has small mean/large variance gradients.
- Evidence anchors:
  - [abstract] "models with larger sizes or fewer training tokens experience less quantization-induced degradation (QiD) when applying low-bit quantization"
  - [section 4.1] "Early (undertrained) checkpoints exhibit significant weight fluctuations during training, making the model relatively robust to weight variations"
  - [corpus] Weak - corpus lacks direct experimental validation of this specific mechanism
- Break condition: If weight fluctuation patterns differ significantly from assumed patterns, or if training dynamics don't follow information bottleneck theory predictions.

### Mechanism 2
- Claim: The scaling law ∆qLoss(N,D,P) = k·D^β/(N^α·P^γ) accurately models quantization-induced degradation as a function of model size, training tokens, and bit width.
- Mechanism: QiD increases with more training tokens (D^β) but decreases with larger model size (N^α) and higher bit width (P^γ), following power-law relationships derived from empirical data.
- Core assumption: The relationship between QiD and these factors follows predictable power-law scaling, similar to conventional LLM scaling laws.
- Evidence anchors:
  - [section 3.5] "We propose the unified scaling law for low-bit quantization as follows: ∆qLoss(N, D, P) = k · D^β/(N^α·P^γ)"
  - [section 3.2] "we propose the relationship between training tokens and QiD as follows: ∆qLoss(D) ≈ b · D^β"
  - [corpus] Weak - corpus neighbors discuss scaling laws but don't directly validate this specific formulation
- Break condition: If empirical data deviates significantly from predicted power-law relationships, or if interactions between factors are more complex than modeled.

### Mechanism 3
- Claim: Native low-bit training (e.g., BitNet) shows similar undertrained-favoring trends as post-quantization, though the gap manifests later in training.
- Mechanism: Native low-bit training maintains the model's ability to operate under low precision throughout training, delaying but not eliminating the performance gap that appears with post-quantization.
- Core assumption: The fundamental relationship between training stage and quantization robustness applies to both post-quantization and native low-bit training.
- Evidence anchors:
  - [section 4.4] "we suspect that native low-bit LLMs are also likely to favor undertrained LLMs...the gap manifests later compared to post-quantization"
  - [section 4.4] "In the early stages of training, the training loss curves of BitNet closely match (and even outperform) those of bf16"
  - [corpus] Weak - corpus doesn't contain direct evidence about native low-bit training behavior
- Break condition: If native low-bit training fundamentally alters training dynamics in ways that don't preserve the undertrained-favoring pattern.

## Foundational Learning

- Concept: Scaling laws for LLM performance (Kaplan et al., 2020; Hoffmann et al., 2022)
  - Why needed here: The paper builds on conventional scaling laws to derive new scaling laws specifically for quantization-induced degradation
  - Quick check question: What is the functional form of conventional scaling laws relating model size to loss?

- Concept: Quantization-induced degradation (QiD) measurement
  - Why needed here: QiD is the core metric used throughout the paper to quantify how quantization affects model performance
  - Quick check question: How is QiD formally defined in terms of pre- and post-quantization losses?

- Concept: Information bottleneck theory in neural network training
  - Why needed here: The paper uses this theory to explain why undertrained models are more robust to quantization
  - Quick check question: According to information bottleneck theory, how do gradient statistics change during training?

## Architecture Onboarding

- Component map: Pythia LLMs (160M-12B parameters) -> GPTQ quantization -> RefinedWeb evaluation -> QiD measurement -> Scaling law fitting
- Critical path: Train Pythia checkpoints → Apply quantization → Measure QiD → Fit scaling laws → Validate predictions → Release quantized checkpoints
- Design tradeoffs: Using Pythia provides controlled training levels but limits generalizability; releasing checkpoints aids reproducibility but requires significant storage
- Failure signatures: Poor scaling law fit indicates model-data mismatch; inconsistent QiD across test sets suggests tokenizer issues; training instability suggests hardware limitations
- First 3 experiments:
  1. Replicate QiD measurement on 12B Pythia checkpoint at different training stages with 4-bit GPTQ quantization
  2. Validate scaling law predictions by testing on out-of-distribution model sizes (e.g., Llama models)
  3. Compare QiD patterns across different quantization methods (GPTQ vs AWQ vs bitandbytes) on same checkpoints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multi-stage training strategies (including supervised fine-tuning and preference optimization) affect quantization-induced degradation in large language models?
- Basis in paper: [explicit] The authors note that advanced LLMs today often employ multi-stage training strategies including supervised fine-tuning and preference optimization, which may cause the behavior of the model after quantization to be significantly different.
- Why unresolved: The scaling laws derived in this work are primarily focused on single-stage pre-trained language models, and the authors explicitly state that the impact of multi-stage training strategies on quantization performance remains unexplored.
- What evidence would resolve it: Experimental results comparing quantization-induced degradation across models trained with different multi-stage strategies (e.g., supervised fine-tuning, preference optimization) versus single-stage pre-training, using the same scaling law framework.

### Open Question 2
- Question: Does native low-bit training (e.g., BitNet) maintain its performance advantage over post-quantization as models scale to 100 trillion training tokens?
- Basis in paper: [explicit] The authors observe that BitNet models perform well when undertrained but struggle to match fully trained counterparts, and express concerns about their performance at larger training scales.
- Why unresolved: The current BitNet results only show performance up to 100 billion training tokens, while the authors predict severe degradation for low-bit quantization at 100 trillion tokens, creating uncertainty about whether native low-bit training would face similar challenges.
- What evidence would resolve it: Training and quantization experiments with BitNet and similar native low-bit models at training scales approaching 100 trillion tokens, comparing their performance degradation against traditional post-quantization approaches.

### Open Question 3
- Question: What is the relationship between quantization precision requirements and the training dynamics of different model architectures beyond decoder-only Transformers?
- Basis in paper: [inferred] The authors observe that as models become more fully trained, they increasingly rely on precision to continue optimizing the training objective, suggesting a fundamental relationship between training dynamics and precision requirements.
- Why unresolved: The analysis focuses on decoder-only models from the Pythia suite, and while the authors mention BitNet as a contrasting architecture, they don't explore how different architectural choices (e.g., encoder-decoder, hybrid architectures) might affect quantization precision requirements during training.
- What evidence would resolve it: Comparative studies of quantization-induced degradation across multiple model architectures (encoder-decoder, hybrid, sparse models) trained to similar levels, measuring how architectural differences influence precision requirements throughout the training process.

### Open Question 4
- Question: How does the composition of training data (e.g., synthetic vs. natural data) affect the relationship between training tokens and quantization-induced degradation?
- Basis in paper: [inferred] The authors mention synthetic data creation innovations and predict models will reach 100 trillion training tokens, suggesting data composition might be relevant to understanding quantization behavior at extreme scales.
- Why unresolved: The experiments use RefinedWeb dataset for evaluation, and while the authors acknowledge the trend toward synthetic data, they don't investigate how different data compositions might influence the scaling laws for quantization-induced degradation.
- What evidence would resolve it: Experiments training identical model architectures on different data compositions (pure natural, mixed natural-synthetic, pure synthetic) to equivalent token counts, measuring how data composition affects quantization-induced degradation and whether the scaling laws remain consistent across data types.

## Limitations
- The study focuses exclusively on Pythia models, limiting generalizability to other LLM architectures and training methodologies
- Analysis primarily considers GPTQ quantization with limited exploration of other quantization methods (AWQ, bitandbytes)
- Experiments are limited to English text data from RefinedWeb, potentially not extending to other languages or specialized domains
- Predictions about 100 trillion token training scenarios are highly speculative with no actual models tested in this regime

## Confidence
- **High Confidence**: The empirical observation that low-bit quantization performs better on undertrained LLMs is well-supported by the extensive dataset of 1,500+ quantized checkpoints. The scaling law formulation (ΔqLoss(N,D,P) = k·D^β/(N^α·P^γ)) is mathematically consistent and provides a reasonable fit to the data.
- **Medium Confidence**: The mechanistic explanation linking weight fluctuation patterns during training to quantization robustness is plausible but not directly validated. The connection to information bottleneck theory provides theoretical grounding, but the specific relationship between gradient statistics and quantization performance requires further experimental verification.
- **Low Confidence**: Predictions about 100 trillion token training scenarios and their quantization behavior are highly speculative, as no models in this regime were actually tested. The extrapolation of scaling laws to this regime carries significant uncertainty.

## Next Checks
1. **Cross-architecture validation**: Test the scaling laws on non-Pythia models (e.g., Llama, Mistral) to assess generalizability beyond the Pythia training methodology.
2. **Mechanism validation**: Conduct controlled experiments measuring weight fluctuation patterns across training stages and correlate these with quantization-induced degradation to directly validate the proposed mechanism.
3. **Extreme regime testing**: Evaluate quantization performance on models trained with 50-100 trillion tokens (when available) to validate or refine the predictions about large-scale training scenarios.