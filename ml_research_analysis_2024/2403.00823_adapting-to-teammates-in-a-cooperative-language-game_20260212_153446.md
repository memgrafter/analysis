---
ver: rpa2
title: Adapting to Teammates in a Cooperative Language Game
arxiv_id: '2403.00823'
source_url: https://arxiv.org/abs/2403.00823
tags:
- agent
- each
- codenames
- team
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first adaptive agent for Codenames that
  can learn to coordinate with different teammates during gameplay. The Adaptive Codenames
  Ensemble (ACE) uses a multi-armed bandit approach to select among a set of expert
  agents, each using different language models, in order to maximize a novel Codenames
  Linear Team (CoLT) rating function.
---

# Adapting to Teammates in a Cooperative Language Game

## Quick Facts
- arXiv ID: 2403.00823
- Source URL: https://arxiv.org/abs/2403.00823
- Reference count: 27
- Primary result: First adaptive Codenames agent that learns to coordinate with different teammates using multi-armed bandit and language models

## Executive Summary
This paper introduces ACE (Adaptive Codenames Ensemble), the first adaptive agent for the Codenames game that can learn to coordinate with different teammates during gameplay. ACE uses a multi-armed bandit approach to select among a set of expert agents, each using different language models, in order to maximize a novel Codenames Linear Team (CoLT) rating function. The system can adapt to individual teammates without prior knowledge about their language patterns or compatibility, achieving performance close to the best possible expert for each partner.

## Method Summary
ACE employs a multi-armed bandit framework with UCB selection to dynamically choose among 7 expert agents, each using different language models (word2vec, GloVe variants, ConceptNet NumberBatch). The CoLT rating function maps outcome distributions to scalar ratings and is trained via supervised learning on synthetic data using Monte Carlo simulations. During gameplay, ACE observes outcomes after each turn, updates expert counts, and selects the next expert based on UCB scores that balance exploitation and exploration.

## Key Results
- ACE adapts quickly to individual teammates, reaching 66% of final performance after only two games with a partner
- The system achieves performance close to the best expert for each teammate without requiring prior knowledge
- CoLT rating correlates with win rate and win time, providing a single metric to evaluate Codenames teams

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ACE learns to match internal language models to a teammate's communication style by maximizing CoLT ratings via UCB selection.
- Mechanism: Each turn, the agent evaluates all experts using their observed outcome distributions normalized into probabilities, then feeds these to the CoLT function. The UCB score combines this rating with an exploration bonus. The expert with highest UCB is chosen, and the outcome distribution for that expert is updated.
- Core assumption: The CoLT rating correlates with team success in competitive play and reflects linguistic compatibility.
- Evidence anchors: Experimental analysis shows adaptation to individual teammates and performance close to best expert; CoLT designed as reward signal for UCB.

### Mechanism 2
- Claim: CoLT weights are trained so that rating differences predict competitive win probability.
- Mechanism: Random feature vectors simulate outcome distributions. Monte Carlo simulations estimate win rates between simulated teams. The difference of feature vectors is input to CoLT, passed through a sigmoid to produce a target win probability. Gradient descent trains weights to minimize error.
- Core assumption: Synthetic outcome distributions and simulated games adequately represent real Codenames play.
- Evidence anchors: 18000 training samples generated; final training loss 0.02699 with R2 score of 0.885.

### Mechanism 3
- Claim: ACE adapts quickly when a single expert vastly outperforms others with a given teammate.
- Mechanism: When one expert has consistently high CoLT ratings, its UCB score dominates after a few turns due to low exploration bonus. The agent locks onto that expert, achieving near-optimal performance.
- Core assumption: High performance is signaled early and clearly by CoLT ratings.
- Evidence anchors: With partner cases show strong signals enabling best partner identification quickly; 66% of final performance reached after two games.

## Foundational Learning

- Concept: Multi-armed bandit (MAB) framework
  - Why needed here: Provides principled way to balance exploration of experts with exploitation of best one using online feedback.
  - Quick check question: In UCB, what term encourages exploration of less-tried experts?

- Concept: Language model embedding and cosine similarity
  - Why needed here: Experts use embeddings to measure semantic similarity between clue words and board words; strategy depends on these distances.
  - Quick check question: How does the spymaster strategy decide the number of words to associate with a clue?

- Concept: Supervised learning with synthetic data
  - Why needed here: CoLT weights trained using simulated games between random outcome distributions, not real human data.
  - Quick check question: What function is applied to the CoLT score difference to produce a target probability?

## Architecture Onboarding

- Component map: Base Codenames agents (experts) -> CoLT rating module -> UCB selector -> Game state updater
- Critical path: Spymaster/Guesser -> Select Expert -> Generate Action -> Observe Outcome -> Update Counts -> Repeat
- Design tradeoffs:
  - UCB gives exploration but requires careful tuning of constant c; too high leads to slow convergence, too low to premature lock-in.
  - CoLT training uses synthetic data for speed, but may not generalize to human play styles.
  - Precomputing 300 nearest neighbors per word speeds inference but fixes vocabulary size.
- Failure signatures:
  - Slow or no improvement in CoLT rating over many games -> likely poor expert diversity or CoLT misalignment.
  - Oscillating expert choice -> UCB exploration term too large or reward signal too noisy.
  - Consistently low win rates -> mismatch between CoLT weights and actual competitive success.
- First 3 experiments:
  1. Replace CoLT with random scorer, keep everything else same; measure if ACE still outperforms static baselines.
  2. Use fixed UCB constant c and sweep over its value; identify setting yielding fastest convergence on with-partner games.
  3. Add new expert using different language model (e.g., BERT); verify ACE can discover and prefer it when beneficial.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine the optimal size and composition of the expert ensemble for the ACE agent to maximize performance while minimizing computational overhead?
- Basis in paper: The paper mentions future work will explore how size and composition of ensemble impacts performance, and how to construct effective ensemble of reduced size from larger set of candidates.
- Why unresolved: Current study used fixed set of 7 expert agents without systematic investigation of ensemble size/composition effects.
- What evidence would resolve it: Experiments comparing ACE performance with ensembles of varying sizes and compositions, using metrics such as CoLT rating, win rate, and computational efficiency.

### Open Question 2
- Question: Can the ACE agent's approach be extended to handle continuous language spaces or more complex language games beyond Codenames?
- Basis in paper: ACE designed for specific language game Codenames, but underlying concept of adapting to individual teammates using multi-armed bandit could potentially apply to other language-based tasks.
- Why unresolved: Current study only evaluated ACE in context of Codenames, did not explore applicability to other language games or general language understanding tasks.
- What evidence would resolve it: Applying ACE to other language games or language understanding tasks, evaluating performance compared to non-adaptive approaches.

### Open Question 3
- Question: How can the ACE agent's performance be improved when the ensemble contains no expert that is a good match for the current teammate?
- Basis in paper: Paper mentions that when all experts do poorly or performance is more mixed with no standout, performance and adaptation times are slower for ACE agent.
- Why unresolved: Current study did not investigate strategies to improve ACE performance in scenarios where ensemble lacks good expert match for teammate.
- What evidence would resolve it: Developing and evaluating techniques such as ensemble pruning, expert creation on the fly, or dynamic expert selection to improve ACE performance in challenging teammate matching scenarios.

## Limitations
- CoLT rating function trained on synthetic outcome distributions rather than observed human play, raising questions about external validity
- Paper does not report variability across multiple runs or statistical significance testing for performance differences
- Unknown degree to which CoLT correlates with competitive success when playing with real human teammates

## Confidence

- **High confidence**: ACE can adapt to teammates using same language model; mechanism of UCB selection with CoLT rewards is sound.
- **Medium confidence**: ACE achieves near-best-expert performance for most teammates; CoLT correlates with competitive win probability.
- **Low confidence**: CoLT generalizes to human players beyond tested language models; ACE will perform equally well with arbitrary new teammates.

## Next Checks

1. Evaluate ACE with actual human partners using within-subjects design (same humans play with ACE and static baseline) to measure real-world CoLT correlation.

2. Conduct ablation studies removing exploration term from UCB to quantify its contribution to adaptation speed and final performance.

3. Test ACE against wider range of expert language models (e.g., BERT, RoBERTa) to assess whether ensemble can still identify best expert when models differ substantially in architecture.