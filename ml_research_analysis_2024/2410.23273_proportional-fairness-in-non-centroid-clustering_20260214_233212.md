---
ver: rpa2
title: Proportional Fairness in Non-Centroid Clustering
arxiv_id: '2410.23273'
source_url: https://arxiv.org/abs/2410.23273
tags:
- loss
- agents
- clustering
- cluster
- core
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies proportional fairness in non-centroid clustering,\
  \ extending the fairness framework from centroid clustering to cases where agent\
  \ loss depends on the other agents in their cluster rather than distance to a cluster\
  \ center. The authors adapt two fairness criteria\u2014the core and fully justified\
  \ representation (FJR)\u2014to this setting and investigate their satisfiability\
  \ under different loss functions (arbitrary, average, and maximum)."
---

# Proportional Fairness in Non-Centroid Clustering

## Quick Facts
- arXiv ID: 2410.23273
- Source URL: https://arxiv.org/abs/2410.23273
- Reference count: 38
- This paper studies proportional fairness in non-centroid clustering, extending fairness frameworks from centroid clustering to cases where agent loss depends on other agents in their cluster rather than distance to a cluster center.

## Executive Summary
This paper studies proportional fairness in non-centroid clustering, extending the fairness framework from centroid clustering to cases where agent loss depends on the other agents in their cluster rather than distance to a cluster center. The authors adapt two fairness criteria—the core and fully justified representation (FJR)—to this setting and investigate their satisfiability under different loss functions (arbitrary, average, and maximum). They show that while the core can be empty for structured loss functions, FJR is always satisfiable. The paper introduces GreedyCohesiveClustering, which achieves exact FJR under arbitrary losses, and proves that the efficient GreedyCapture algorithm achieves constant-factor FJR approximations for average and maximum losses. They also design an efficient auditing algorithm to estimate FJR approximation. Experiments on real data show that GreedyCapture provides significantly better fairness guarantees than traditional algorithms (k-means++ and k-medoids) with only modest increases in standard clustering objectives.

## Method Summary
The paper studies proportional fairness in non-centroid clustering where agent loss depends on other agents in their cluster. Three loss functions are considered: arbitrary, average, and maximum. The authors adapt core and FJR fairness criteria to this setting, showing that while the core can be empty for structured loss functions, FJR is always satisfiable. They introduce GreedyCohesiveClustering for exact FJR and GreedyCapture for efficient constant-factor approximations. An auditing algorithm is designed to estimate FJR approximation of any clustering. Experiments use three UCI datasets (Census Income, Diabetes, Iris) comparing GreedyCapture with k-means++ and k-medoids on fairness and accuracy metrics.

## Key Results
- The core can be empty for structured loss functions (average and maximum), while FJR is always satisfiable
- GreedyCohesiveClustering achieves exact FJR under arbitrary losses
- GreedyCapture achieves constant-factor FJR approximations (4-FJR for average, 2-FJR for maximum)
- AuditFJR algorithm estimates FJR approximation of any clustering up to a constant factor
- GreedyCapture provides significantly better fairness guarantees than k-means++ and k-medoids with only modest increases in standard clustering objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The GreedyCapture algorithm achieves a constant-factor approximation to FJR for both average and maximum loss functions by iteratively finding cohesive clusters using the SmallestAgentBall subroutine.
- Mechanism: GreedyCapture grows balls centered at agents rather than cluster centers, capturing at least ⌈n/k⌉ agents each time. The SmallestAgentBall subroutine returns a cluster of ⌈n/k⌉ agents closest to some agent i*, ensuring that any deviating coalition S of size at least n/k will contain agents with losses bounded relative to those in the captured cluster.
- Core assumption: The triangle inequality holds for the distance metric, allowing us to bound distances within captured clusters and between coalition members.
- Evidence anchors:
  - [abstract] "the efficient GreedyCapture algorithm achieves a constant approximation of FJR"
  - [section] "We show that the FJR approximation achieved by GreedyCapture stems from the fact that its key subroutine achieves a constant approximation of that of the GreedyCohesiveClustering algorithm."
  - [corpus] Weak evidence - corpus contains related work but no direct evidence for this specific mechanism
- Break condition: If the triangle inequality fails for the distance metric, the bounding arguments in the proof of Theorem 3 no longer hold, and the constant-factor approximation guarantee may be lost.

### Mechanism 2
- Claim: The AuditFJR algorithm can estimate the FJR approximation of any given clustering up to a constant factor by iteratively finding cohesive clusters and removing the agent with minimum loss.
- Mechanism: AuditFJR calls a λ-approximation algorithm for the Most Cohesive Cluster problem n times, each time removing only the agent with smallest loss under the given clustering. It returns the maximum FJR violation witnessed across these n possible deviating coalitions.
- Core assumption: A λ-approximation algorithm for the Most Cohesive Cluster problem exists and can be called efficiently.
- Evidence anchors:
  - [abstract] "We also design an efficient auditing algorithm, which estimates the FJR approximation of any given clustering solution up to a constant factor."
  - [section] "we show that the same technique that we use to algorithmically achieve a constant approximation of FJR can be used to also estimate the FJR approximation of any given clustering, up to a constant factor"
  - [corpus] Weak evidence - corpus contains related work but no direct evidence for this specific mechanism
- Break condition: If no λ-approximation algorithm exists for the Most Cohesive Cluster problem (e.g., if it's NP-hard to approximate), then AuditFJR cannot be implemented efficiently, and the constant-factor auditing guarantee is lost.

### Mechanism 3
- Claim: The core can be empty for structured loss functions (average and maximum) even though FJR is always satisfiable.
- Mechanism: For average loss, the construction in Theorem 2 creates an instance where any clustering has a deviating coalition that improves by a factor approaching 1+√3/2 ≈ 1.366. For maximum loss, the construction in Theorem 1 creates an instance where any clustering has a deviating coalition that improves by an infinite factor.
- Core assumption: The loss functions are structured (average or maximum) rather than arbitrary, allowing for the specific constructions that demonstrate core emptiness.
- Evidence anchors:
  - [abstract] "We show that the core can be approximated only under structured loss functions, and even then, the best approximation we are able to establish, using an adaptation of the GreedyCapture algorithm developed for centroid clustering, is unappealing for a natural loss function."
  - [section] "we prove that the core can still be empty, albeit there is now room for a finite approximation" and "we show that the core can be empty for structured loss functions"
  - [corpus] Weak evidence - corpus contains related work but no direct evidence for this specific mechanism
- Break condition: If the loss functions are arbitrary rather than structured, then the core can be empty for any finite approximation (Theorem 1), which is a stronger negative result.

## Foundational Learning

- Concept: Proportional fairness guarantees (core and FJR) in clustering
  - Why needed here: The paper extends proportional fairness from centroid clustering to non-centroid clustering, requiring understanding of these fairness concepts
  - Quick check question: What is the key difference between the core and FJR fairness guarantees?

- Concept: Non-centroid clustering and loss functions
  - Why needed here: The paper studies fairness in non-centroid clustering where agent loss depends on other agents in their cluster, not on distance to a cluster center
  - Quick check question: How does the loss function differ between centroid and non-centroid clustering?

- Concept: Approximation algorithms and constant-factor guarantees
  - Why needed here: The paper designs algorithms that achieve constant-factor approximations to fairness guarantees, requiring understanding of approximation algorithm techniques
  - Quick check question: What is the significance of achieving a constant-factor approximation rather than an exact solution?

## Architecture Onboarding

- Component map:
  - GreedyCohesiveClustering -> A (SmallestAgentBall) -> cohesive clusters
  - GreedyCapture (efficient implementation of GreedyCohesiveClustering)
  - AuditFJR -> A (λ-approximation for Most Cohesive Cluster) -> FJR estimation
  - Most Cohesive Cluster problem (optimization problem solved by subroutines)

- Critical path:
  1. GreedyCohesiveClustering calls A (SmallestAgentBall) to find cohesive clusters
  2. GreedyCapture is an efficient implementation of GreedyCohesiveClustering
  3. AuditFJR iteratively calls A to estimate FJR approximation of given clustering

- Design tradeoffs:
  - Exact FJR vs. efficient approximation: GreedyCohesiveClustering achieves exact FJR but is inefficient; GreedyCapture is efficient but only achieves constant-factor approximation
  - Core vs. FJR: Core is stronger but can be empty; FJR is weaker but always satisfiable
  - Average vs. maximum loss: Different approximation guarantees for each loss function

- Failure signatures:
  - If triangle inequality fails: Constant-factor approximation guarantees may be lost
  - If Most Cohesive Cluster problem is hard to approximate: AuditFJR cannot be implemented efficiently
  - If loss functions are arbitrary: Core can be empty for any finite approximation

- First 3 experiments:
  1. Verify that GreedyCapture achieves constant-factor FJR approximation on synthetic data with known optimal solution
  2. Test AuditFJR on known clustering solutions to verify constant-factor estimation accuracy
  3. Compare core and FJR guarantees on instances where core is known to be empty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: For the maximum loss, does there always exist a clustering in the core?
- Basis in paper: [explicit] The paper states "We easily see that the core can still be empty, albeit there is now room for a finite approximation" and mentions this as an open question.
- Why unresolved: The paper only shows a constant approximation for the core (2-core) but does not prove whether the core is always non-empty.
- What evidence would resolve it: A proof that either always shows the core is non-empty for maximum loss, or provides a counterexample where the core is empty.

### Open Question 2
- Question: For the average loss, does there always exist a clustering in the α-core for some constant α?
- Basis in paper: [explicit] The paper states "Despite significant effort, we are unable to determine whether the core is always non-empty for the maximum loss, or whether a constant approximation of the core can be guaranteed for the average loss."
- Why unresolved: The paper only establishes an O(n/k) approximation for the core with average loss, which is not a constant.
- What evidence would resolve it: A proof that either always shows the core is non-empty for average loss with some constant α, or provides a counterexample where the core is empty for any constant α.

### Open Question 3
- Question: For the average (or maximum) loss, what is the smallest α for which an α-FJR clustering can be computed in polynomial time, assuming P≠NP?
- Basis in paper: [explicit] The paper states "Determining the best FJR approximation achievable in polynomial time remains an open question."
- Why unresolved: The paper shows that GreedyCapture achieves a constant approximation (4-FJR for average, 2-FJR for maximum) in polynomial time, but doesn't determine if this is optimal.
- What evidence would resolve it: A proof that either establishes a lower bound on the approximation ratio that cannot be achieved in polynomial time, or provides an algorithm that achieves a better constant approximation in polynomial time.

## Limitations

- The constant-factor approximation guarantees for GreedyCapture depend critically on the triangle inequality holding for the distance metric
- The constructions showing core emptiness represent worst-case scenarios that may not manifest in practical clustering applications
- The GreedyCapture algorithm's efficiency comes at the cost of exact fairness guarantees, with uncharacterized practical impact

## Confidence

- Core FJR approximation claims: High confidence in theoretical framework, Medium confidence in practical approximation quality
- GreedyCapture algorithm implementation: High confidence in correctness, Medium confidence in practical performance
- Auditing algorithm guarantees: High confidence in theoretical bounds, Medium confidence in computational efficiency for large-scale problems

## Next Checks

1. Empirical validation of the GreedyCapture approximation factor on synthetic datasets where optimal solutions are known, to verify the claimed constant-factor bounds hold in practice
2. Testing the sensitivity of the approximation guarantees to violations of the triangle inequality using non-metric distance functions
3. Comparative analysis of core vs FJR fairness on real-world clustering problems to understand the practical significance of core emptiness results