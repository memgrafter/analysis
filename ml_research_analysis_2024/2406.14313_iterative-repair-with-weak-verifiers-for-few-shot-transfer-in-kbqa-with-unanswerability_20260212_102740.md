---
ver: rpa2
title: Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability
arxiv_id: '2406.14313'
source_url: https://arxiv.org/abs/2406.14313
tags:
- logical
- question
- questions
- kbqa
- form
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of few-shot transfer learning for
  knowledge base question answering (KBQA) with unanswerability, where target examples
  include both answerable and unanswerable questions. The proposed solution, FUn-FuSIC,
  extends FuSIC-KBQA with iterative repair using feedback from a diverse suite of
  strong and weak verifiers, including a novel back-translation-based verifier.
---

# Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability

## Quick Facts
- arXiv ID: 2406.14313
- Source URL: https://arxiv.org/abs/2406.14313
- Reference count: 22
- Achieves 76.6 F1 overall and 70.2 EM-s on WebQSP→GrailQAbility for KBQA with unanswerability

## Executive Summary
This paper addresses the challenge of few-shot transfer learning for knowledge base question answering (KBQA) where target examples include both answerable and unanswerable questions. The authors propose FUn-FuSIC, an extension of FuSIC-KBQA that uses iterative repair with feedback from strong and weak verifiers, including a novel back-translation-based verifier. FUn-FuSIC employs self-consistency adapted for unanswerability (scUn) to select consensus logical forms. Experiments on two new datasets demonstrate significant improvements over state-of-the-art models, achieving 76.6 F1 overall and 70.2 EM-s on WebQSP→GrailQAbility while establishing new state-of-the-art performance for answerable-only KBQA transfer.

## Method Summary
FUn-FuSIC extends FuSIC-KBQA by incorporating iterative repair with feedback from a suite of strong and weak verifiers. The method uses a retriever to fetch schema elements and paths, followed by LLM-based generation of initial logical forms. The iterative repair loop (FUn) employs strong verifiers for syntax checking and KB inconsistency detection, while weak verifiers identify potential errors without rejecting candidates outright. A novel back-translation-based verifier detects question-logical form mismatches. Self-consistency adapted for unanswerability (scUn) selects the consensus logical form by assessing likelihood of majority answers rather than simple majority voting. The approach is evaluated on two new datasets (WebQSP→GrailQAbility and WebQSP→GraphQAbility) with 100 few-shot training examples each.

## Key Results
- Achieves 76.6 F1 overall and 70.2 EM-s on WebQSP→GrailQAbility
- Outperforms state-of-the-art adaptations by significant margins on both datasets
- Establishes new state-of-the-art performance for answerable-only KBQA transfer
- Demonstrates strong performance on both schema-level and data-level unanswerability detection

## Why This Works (Mechanism)

### Mechanism 1
Iterative repair with weak verifiers enables detection and handling of unanswerability by allowing candidate logical forms to survive partial failures. Weak verifiers flag potential issues without rejecting the logical form outright, letting FUn collect multiple candidates for later consensus selection via scUn. Evidence shows weak verifier accuracy at 75% (V4b) and 88% (V3 overall), supporting partial reliability. Break condition: If weak verifiers are too inaccurate, they produce false positives that pollute the candidate set.

### Mechanism 2
Self-consistency adapted for unanswerability (scUn) replaces majority-answer selection with likelihood assessment, enabling proper handling of unanswerable questions. scUn first checks if the most popular non-empty answer has enough supporters (≥ t = ⌊|L|/2⌋), then selects among logical forms agreeing on NA or returns NK. Ablation shows scUn vs sc drops EM-s from 59.2 to 37.2 on WebQSP→GrailQAbility, confirming importance. Break condition: If candidate set L is too small or answers are too evenly distributed, scUn defaults to NK/NA incorrectly.

### Mechanism 3
Question-logical form disagreement verifier (V3) using back-translation and equivalence check improves schema-level unanswerability detection. V3 naturalizes variable names, back-translates to a natural language question, and checks semantic equivalence with the original question using few-shot exemplars. Evidence shows V3 accuracy 88-90% on datasets; ablation shows 42→64 EM-s improvement for schema-level questions. Break condition: If back-translation or equivalence classifier fails often, V3 rejects valid logical forms or misses real mismatches.

## Foundational Learning

- **Knowledge Base Question Answering (KBQA) as semantic parsing** - Why needed: The task requires mapping natural language to executable logical forms over a KB; understanding this framing clarifies why iterative repair and verification matter. Quick check: What distinguishes semantic parsing KBQA from retrieval-based QA?

- **Strong vs weak verification in program synthesis** - Why needed: FUn relies on mixing certain checks (syntax, KB inconsistency) with probabilistic ones (answer incompatibility, question-logical form disagreement) to handle ambiguity in unanswerability. Quick check: When would you classify a verifier as "weak" versus "strong" in a repair loop?

- **Self-consistency for aggregating reasoning paths** - Why needed: scUn builds on self-consistency but adapts it to handle NA/empty answers instead of assuming all paths produce valid answers. Quick check: How does self-consistency differ when applied to answerable-only vs unanswerable KBQA?

## Architecture Onboarding

- **Component map**: Retrieval (RetinaQA) → Generation (LLM via PUn) → Iterative Repair (FUn with verifiers) → Consensus Selection (scUn) → Output
- **Critical path**: Retrieval → Generation → FUn iterations (≤ n=4) → scUn → Output
- **Design tradeoffs**: Weak verifiers improve recall but add noise; strong verifiers ensure precision but may over-reject. scUn handles unanswerability but adds latency vs standard self-consistency. Using gpt-4 vs open models affects cost/reproducibility trade-off.
- **Failure signatures**: High candidate set L size but low EM-s → weak verifiers too permissive. scUn defaulting to NK often → candidate set too sparse or answers too split. Syntax errors persist across iterations → retriever or generation prompt issue.
- **First 3 experiments**:
  1. Run FUn with only strong verifiers; measure drop in schema-level EM-s.
  2. Replace scUn with vanilla self-consistency; observe unanswerable performance collapse.
  3. Swap gpt-4 with Mistral; compare accuracy and iteration count.

## Open Questions the Paper Calls Out

### Open Question 1
How would FUn-FuSIC perform with open-source LLMs instead of GPT-4? The paper notes experiments were limited to GPT-4 due to cost constraints and suggests performance might drop with less powerful open-source models. This remains unresolved as the authors didn't test FUn-FuSIC with open-source LLMs.

### Open Question 2
Can the weak verifier accuracy be improved for the Empty Answer Verifier (V4b) across different datasets? The paper reports V4b accuracies of 75% and 68% for WebQSP→GraphQAbility and WebQSP→GrailQAbility respectively, noting accuracy depends on the nature and fraction of unanswerable questions. The paper doesn't explore strategies to improve V4b accuracy.

### Open Question 3
How does FUn-FuSIC handle questions with multiple valid logical forms that are semantically equivalent but syntactically different? The error analysis mentions some incorrect logical forms are semantically equivalent to the gold standard but fail the EM-s check, indicating a limitation in the evaluation metric. The paper doesn't propose solutions for this issue.

### Open Question 4
What is the impact of increasing the number of FUn iterations beyond 4 on model performance? The paper uses n=4 FUn iterations and mentions some questions require multiple iterations to generate correct logical forms, but doesn't explore the performance trade-off with more iterations.

## Limitations
- Limited verifier specification - Exact prompt templates and thresholds for weak verifiers are not fully specified in the paper
- Dataset accessibility - The two new datasets are not publicly available at the time of writing
- GPT-4 dependency - Approach relies heavily on GPT-4-0613, creating cost barriers and reproducibility issues
- Weak verifier accuracy - Paper acknowledges verifiers make "occasional mistakes" but doesn't fully characterize error patterns

## Confidence

**High confidence** - Core mechanism of iterative repair with mixed strong/weak verifiers and scUn consensus selection are well-supported by ablation studies and experimental results.

**Medium confidence** - Specific implementation details of the back-translation verifier and its impact on schema-level unanswerability detection lack full methodological transparency for independent verification.

**Low confidence** - Claims about general applicability beyond tested datasets and scalability to other domains remain speculative without additional experiments.

## Next Checks

1. **Verifier ablation study** - Systematically disable each weak verifier (V3, V4) in isolation to quantify their individual contributions to performance, particularly on schema-level unanswerability questions.

2. **Alternative model comparison** - Replicate the FUn-FuSIC pipeline using an open-source model like GPT-3.5 or Mistral to assess the impact of model choice on performance and iteration efficiency.

3. **Cross-domain transferability** - Test the approach on a different KBQA dataset (e.g., ComplexWebQuestions or MetaQA) to evaluate whether the iterative repair mechanism generalizes beyond the WebQSP-derived datasets.