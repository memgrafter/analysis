---
ver: rpa2
title: 'DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels'
arxiv_id: '2409.02465'
source_url: https://arxiv.org/abs/2409.02465
tags:
- reasoning
- context
- evidence
- long-context
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DetectiveQA, a dataset for evaluating long-context
  reasoning in large language models using detective novels. The dataset contains
  1200 human-annotated questions with reference reasoning steps from novels averaging
  over 100k tokens.
---

# DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels

## Quick Facts
- **arXiv ID**: 2409.02465
- **Source URL**: https://arxiv.org/abs/2409.02465
- **Reference count**: 22
- **Key outcome**: Introduces DetectiveQA dataset with 1200 human-annotated questions from detective novels averaging 100k+ tokens, revealing persistent challenges in long-context reasoning for mainstream LLMs

## Executive Summary
This paper introduces DetectiveQA, a dataset designed to evaluate long-context reasoning capabilities in large language models using detective novels. The dataset contains 1200 human-annotated questions with reference reasoning steps from novels averaging over 100k tokens, providing a challenging testbed for assessing how well models can track evidence across extended narratives and perform multi-hop reasoning. The authors introduce a step-wise reasoning metric that uses GPT-4 to evaluate how well models' reasoning aligns with detective reference steps, enabling more granular assessment than traditional accuracy metrics.

When evaluating mainstream LLMs including GPT-4, Claude, and LLaMA, the study reveals persistent challenges in long-context reasoning, particularly in evidence retrieval. The dataset enables in-depth analysis of models' reasoning abilities and provides insights into their performance bottlenecks. The paper demonstrates that even state-of-the-art models struggle with the combination of long-context processing and complex reasoning required by detective narratives, highlighting the need for more rigorous evaluations of long-context reasoning capabilities.

## Method Summary
DetectiveQA is constructed by collecting detective novels in English and Chinese, then using agent-assisted annotation to create multiple-choice questions with corresponding reasoning steps and evidence positions. The evaluation framework employs three context settings: Context+Question (full novel context), Question-Only (novel title/author only), and Evidence+Question (only golden evidence). Model responses are evaluated using both multiple-choice accuracy and a step-wise reasoning metric where GPT-4 judges check if reference reasoning steps are contained in the model's output. This approach enables analysis of whether failures stem from evidence retrieval limitations or reasoning capability gaps.

## Key Results
- DetectiveQA reveals persistent challenges in long-context reasoning for mainstream LLMs, particularly in evidence retrieval from contexts averaging 100k+ tokens
- Step-wise reasoning metric shows significant performance gaps between models, with GPT-4 achieving 78.9% accuracy while open-source models like LLaMA3.1 score below 40%
- Evidence+Question setting consistently outperforms Context+Question, indicating evidence retrieval as a major bottleneck in long-context reasoning
- Question-Only setting reveals minimal data contamination, suggesting the dataset's questions require genuine reasoning rather than memorization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DetectiveQA effectively evaluates long-context reasoning by combining narrative complexity with multi-hop reasoning requirements.
- **Mechanism**: The dataset leverages detective novels averaging 100k+ tokens to create questions requiring models to track evidence across long contexts, synthesize clues, and infer implicit information.
- **Core assumption**: Detective novels provide sufficiently complex narratives where reasoning chains involve both explicit evidence and implicit detective inferences.
- **Evidence anchors**:
  - [abstract] "The dataset contains 1200 human-annotated questions with reference reasoning steps from novels averaging over 100k tokens."
  - [section] "detective novels are widely regarded as the most distinctive in terms of reasoning features" and provide "a sufficiently realistic and challenging setting"
  - [corpus] Weak evidence - related work focuses on narrative reasoning but not specifically detective novel reasoning evaluation
- **Break condition**: If detective novels don't actually require complex multi-hop reasoning or if evidence distribution is too sparse for meaningful evaluation.

### Mechanism 2
- **Claim**: The step-wise reasoning metric provides more granular evaluation than traditional accuracy metrics by assessing reasoning process quality.
- **Mechanism**: Uses GPT-4 to compare model-generated reasoning chains against reference steps, evaluating both explicit evidence retrieval and implicit inference capabilities.
- **Core assumption**: LLM judges can reliably evaluate reasoning process quality and distinguish between correct reasoning and lucky guessing.
- **Evidence anchors**:
  - [abstract] "we introduce a step-wise reasoning metric, which enhances the evaluation of LLMs' reasoning processes"
  - [section] "the automated evaluation of long outputs from LLMs remains challenging" but "thanks to the annotation, where each question is linked to corresponding reference steps, we can assess the logical coherence"
  - [corpus] Weak evidence - G-Eval and ReasonEval mentioned as similar approaches but not specifically for narrative reasoning
- **Break condition**: If GPT-4 judge shows poor agreement with human judges or cannot distinguish between superficially similar reasoning chains.

### Mechanism 3
- **Claim**: Different context settings (Context+Question, Question-Only, Evidence+Question) enable disentangling long-context processing from reasoning ability.
- **Mechanism**: By comparing performance across settings, researchers can identify whether failures stem from evidence retrieval limitations or reasoning capability gaps.
- **Core assumption**: Question-Only setting effectively tests for data contamination while Evidence+Question isolates reasoning from long-context processing.
- **Evidence anchors**:
  - [section] "We provide three context settings...to analyze different issues in LLMs' long-context reasoning"
  - [section] "the question-only setting merely wins on a small proportion of questions" indicating data contamination is not severe
  - [corpus] Weak evidence - similar ablation approaches mentioned in related work but not specifically for detective novel reasoning
- **Break condition**: If setting comparisons don't reveal clear performance patterns or if one setting consistently dominates others.

## Foundational Learning

- **Concept**: Multi-hop reasoning
  - Why needed here: DetectiveQA questions require synthesizing evidence from multiple locations in long contexts, not just single-hop retrieval
  - Quick check question: Can a model answer questions requiring evidence from three separate paragraphs combined with an inference?

- **Concept**: Implicit reasoning
  - Why needed here: Detective novels often require readers to infer connections between explicit evidence, testing deeper comprehension
  - Quick check question: Does the model recognize that "no luggage" + "dinner date" + "expected return time" implies foul play?

- **Concept**: Long-context evidence retrieval
  - Why needed here: With contexts averaging 118k tokens, models must efficiently locate relevant evidence without exhaustive scanning
  - Quick check question: Can a model find relevant clues in a 200k token novel when the evidence appears 150k tokens from the start?

## Architecture Onboarding

- **Component map**: Novel ingestion -> Agent-assisted annotation -> Human refinement -> Dataset compilation -> Model evaluation across three context settings -> GPT-4 judging -> Metric calculation -> Performance analysis

- **Critical path**: 
  1. Annotate questions with reference steps and evidence positions
  2. Evaluate models across all three context settings
  3. Analyze results using step-wise reasoning metric
  4. Identify performance bottlenecks through ablation studies

- **Design tradeoffs**:
  - Dataset size vs. annotation quality: 1200 questions provides good coverage but required extensive human annotation
  - GPT-4 judging vs. human evaluation: Automated evaluation enables scalability but may miss nuanced reasoning errors
  - Context length vs. model compatibility: 100k+ tokens challenges current models but excludes smaller-context systems

- **Failure signatures**:
  - Low accuracy but high reasoning scores: Models guess correctly without proper reasoning chains
  - High accuracy but low reasoning scores: Models retrieve evidence but fail to synthesize into coherent conclusions
  - Poor performance only on long contexts: Evidence retrieval limitations rather than reasoning capability

- **First 3 experiments**:
  1. Run baseline evaluation on GPT-4 across all three context settings to establish performance bounds
  2. Compare open-source models (LLaMA3.1, GLM4, Qwen2.5) to identify which have strongest long-context reasoning
  3. Perform ablation study focusing on evidence depth to determine at what point retrieval fails

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the step-wise reasoning metric's correlation with human judgments compare across different types of reasoning (explicit vs implicit evidence)?
- **Basis in paper**: [explicit] The paper mentions that GPT-4 shows high agreement with human judges for the step-wise reasoning metric, but doesn't break down this correlation by evidence type.
- **Why unresolved**: The paper validates the metric's reliability overall but doesn't analyze whether it performs differently for explicit evidence versus implicit inference steps.
- **What evidence would resolve it**: Detailed correlation analysis showing kappa scores separately for explicit evidence steps and implicit reasoning steps, potentially revealing systematic biases in the metric.

### Open Question 2
- **Question**: What is the optimal balance between long-context processing and reasoning ability in detective novel understanding?
- **Basis in paper**: [inferred] The paper disentangles long-context processing and reasoning through Evidence+Question and Question-Only settings, but doesn't optimize for the trade-off between these capabilities.
- **Why unresolved**: The ablation studies show that both capabilities matter, but don't determine how much long-context ability can be sacrificed for better reasoning (or vice versa) without harming overall performance.
- **What evidence would resolve it**: Systematic experiments varying the amount of context provided (partial evidence, multiple evidence snippets, etc.) to find the minimum context needed for optimal reasoning performance.

### Open Question 3
- **Question**: How do different instruction tuning approaches affect long-context reasoning performance on DetectiveQA?
- **Basis in paper**: [explicit] The paper evaluates several models with different architectures and training approaches but doesn't systematically compare instruction tuning methods.
- **Why unresolved**: While the paper shows performance gaps between models, it doesn't isolate how different instruction tuning strategies for long-context reasoning impact results.
- **What evidence would resolve it**: Controlled experiments comparing models with identical architectures but different instruction tuning approaches (standard vs long-context-specific vs reasoning-focused) on DetectiveQA.

## Limitations
- The paper assumes without empirical validation that detective novels provide uniquely challenging reasoning scenarios compared to other narrative forms
- GPT-4 judging introduces both cost barriers and potential bias, with no reported inter-annotator agreement or human validation of the automated metric
- 1200 questions may not adequately represent the full spectrum of reasoning difficulties present in 100k+ token documents

## Confidence
- **Low**: Confidence in the claim that detective novels provide uniquely challenging reasoning scenarios, as the comparison to existing long-context benchmarks is superficial
- **Medium**: Confidence in the step-wise reasoning metric's effectiveness, given the lack of human validation and potential GPT-4 judging biases
- **Medium**: Confidence in the dataset's coverage of long-context reasoning challenges, as the paper doesn't analyze question distribution across different reasoning types

## Next Checks
1. **Human validation of GPT-4 judging**: Conduct a small-scale human evaluation (10-20 questions) where human judges score the same reasoning chains that GPT-4 evaluates, measuring inter-annotator agreement to validate the automated metric's reliability.

2. **Cross-genre comparison study**: Create a parallel dataset using non-detective narrative texts (e.g., historical fiction, literary fiction) with identical question formats and evaluate whether detective novels truly present unique reasoning challenges or if the complexity stems from general narrative length and complexity.

3. **Evidence retrieval efficiency analysis**: For each model evaluated, measure evidence retrieval performance as a function of distance from the query location (e.g., 10k, 50k, 100k tokens away), identifying at what context depths retrieval breaks down and whether this correlates with overall performance drops.