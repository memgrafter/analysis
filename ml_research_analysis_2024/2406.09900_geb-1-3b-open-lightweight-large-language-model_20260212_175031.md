---
ver: rpa2
title: 'GEB-1.3B: Open Lightweight Large Language Model'
arxiv_id: '2406.09900'
source_url: https://arxiv.org/abs/2406.09900
tags:
- arxiv
- geb-1
- language
- dataset
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GEB-1.3B, a lightweight large language model
  trained on 550 billion tokens in both Chinese and English. The model uses advanced
  techniques such as ROPE, Group-Query-Attention, and FlashAttention-2 to accelerate
  training, and is fine-tuned with 10 million instruction-based samples for better
  alignment.
---

# GEB-1.3B: Open Lightweight Large Language Model

## Quick Facts
- arXiv ID: 2406.09900
- Source URL: https://arxiv.org/abs/2406.09900
- Authors: Jie Wu; Yufeng Zhu; Lei Shen; Xuqing Lu
- Reference count: 8
- Primary result: State-of-the-art performance among 1.3B models on MMLU, C-Eval, and CMMLU benchmarks

## Executive Summary
This paper introduces GEB-1.3B, a lightweight large language model trained on 550 billion tokens in both Chinese and English. The model employs advanced techniques such as ROPE, Group-Query-Attention, and FlashAttention-2 to accelerate training while maintaining performance. Through fine-tuning with 10 million instruction-based samples, GEB-1.3B achieves state-of-the-art results on general benchmarks like MMLU, C-Eval, and CMMLU, outperforming similar-sized models such as MindLLM-1.3B and TinyLLaMA-1.1B. The model demonstrates efficient CPU inference at 12 tokens/second in FP32 and is released as open-source.

## Method Summary
GEB-1.3B is a 1.3B parameter transformer model trained on 550 billion tokens from Chinese and English corpora using a two-stage process. The pre-training phase uses AdamW optimizer with cosine decay learning rate schedule on 64 NVIDIA RTX3090ti GPUs with BFloat16 precision. Key architectural innovations include Group-Query-Attention for faster inference, FlashAttention-2 for memory-efficient training, and RoPE for position encoding. The model is fine-tuned using 16 million samples of SFT data followed by 10,000 samples of DPO data. The training process includes specific loss spike mitigation techniques such as batch sample replacement and early gradient scaling.

## Key Results
- Achieves state-of-the-art performance on MMLU, C-Eval, and CMMLU benchmarks among 1.3B models
- Outperforms MindLLM-1.3B and TinyLLaMA-1.1B on general benchmark evaluations
- Demonstrates efficient CPU inference at 12 tokens/second in FP32 precision
- Maintains strong performance on Chinese language benchmarks despite smaller vocabulary size

## Why This Works (Mechanism)

### Mechanism 1: Group-Query-Attention (GQA)
GQA significantly accelerates inference by sharing the same query (Q) across multiple heads for keys (K) and values (V). This reduces redundant computation compared to standard multi-head attention where each head has independent Q, K, and V matrices. The model can still learn diverse attention patterns while benefiting from the speedup.

### Mechanism 2: FlashAttention-2
FlashAttention-2 optimizes memory access patterns during attention computation by reorganizing operations to be more memory-efficient. This reduces the number of memory reads and writes through operation fusion and memory-aware batching, addressing the memory bandwidth bottleneck in attention computation.

### Mechanism 3: Instruction Fine-tuning
Fine-tuning with 10 million instruction-response pairs improves model alignment by teaching the model to follow instructions and generate helpful responses. This aligns the model's behavior with human expectations through exposure to diverse, high-quality instruction data spanning various subjects.

## Foundational Learning

- **Transformer architecture and attention mechanism**: Understanding transformer architecture is crucial for comprehending design choices like GQA and FlashAttention-2. *Quick check*: What is the role of the attention mechanism in transformers, and how does it enable long-range dependency capture?

- **Pre-training and fine-tuning**: The paper describes a two-stage training process. *Quick check*: What is the difference between pre-training and fine-tuning, and why is it beneficial to use both in developing LLMs?

- **Benchmarks and evaluation metrics**: The model is evaluated on MMLU, C-Eval, and CMMLU benchmarks. *Quick check*: What do these benchmarks measure, and how do they differ in their focus on language proficiency?

## Architecture Onboarding

- **Component map**: Tokenizer (64,896 vocab) -> Word Embedding Layer (untied, RoPE) -> Transformer Blocks (24 layers, 16 heads, SwiGLU, 8/3 FFN, Post-RMSNorm) -> Training Infrastructure (AdamW, BFloat16, 64 RTX3090ti GPUs)

- **Critical path**: Pre-training (650B tokens → 64 GPUs training → loss stabilization) → Fine-tuning (SFT 16M → DPO 10k → Evaluation on MMLU/C-Eval/CMMLU/ToxiGen)

- **Design tradeoffs**: Smaller vocabulary (64,896) reduces parameters but may limit expressiveness; untied embeddings increase memory but improve performance; GQA accelerates inference with potential attention diversity reduction; FP32 position IDs prevent collisions but increase memory usage

- **Failure signatures**: Loss spikes during pre-training indicate batch size or learning rate issues; poor benchmark performance suggests inadequate pre-training or fine-tuning; slow inference speed results from inefficient attention or hardware utilization

- **First 3 experiments**: 1) Verify GQA speedup by comparing inference time with MHA baseline; 2) Test FlashAttention-2 efficiency by measuring training throughput with/without it; 3) Evaluate fine-tuning impact by comparing performance before/after SFT and DPO

## Open Questions the Paper Calls Out

1. **Dataset scaling impact**: How does GEB-1.3B's performance scale with increasing dataset size beyond 550 billion tokens, and is there a point of diminishing returns? This remains unresolved as the paper doesn't test scaling beyond the stated dataset size.

2. **Quantization trade-offs**: What is the impact of different quantization techniques on GEB-1.3B's inference speed and accuracy? The paper mentions plans to explore quantization but doesn't provide experimental results.

3. **Specialized domain performance**: How does GEB-1.3B compare to larger models in specialized domains not covered by general benchmarks? The paper focuses on general benchmarks without exploring domain-specific capabilities.

## Limitations

- Training details of competitor models are not fully disclosed, making direct performance attribution difficult
- Inference speed claims (12 tokens/second) are based on FP32 precision, with quantized performance yet to be validated
- Instruction fine-tuning dataset composition and quality are not fully specified
- Evaluation focuses primarily on general benchmarks without extensive testing in specialized domains

## Confidence

**High Confidence**: Architectural innovations (GQA, FlashAttention-2, RoPE) and their implementation details are well-documented with specific technical specifications and reasonable justifications.

**Medium Confidence**: Performance claims on benchmarks are supported by standard evaluation metrics, but comparisons with other models lack full transparency in training conditions.

**Low Confidence**: Inference speed claims and actual instruction data quality remain to be independently verified through reproduction or third-party testing.

## Next Checks

1. **Benchmark Reproducibility Test**: Re-run MMLU, C-Eval, and CMMLU evaluations using publicly released model weights under identical conditions to verify reported performance metrics.

2. **Inference Speed Validation**: Measure actual CPU inference speed of GEB-1.3B with FP32 and quantized versions (INT8/INT4) on standard hardware to confirm the 12 tokens/second baseline.

3. **Instruction Data Quality Analysis**: Sample and analyze the instruction fine-tuning dataset to verify its diversity, quality, and representativeness, and test the model's instruction-following capabilities on held-out instruction sets.