---
ver: rpa2
title: 'KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models'
arxiv_id: '2402.02801'
source_url: https://arxiv.org/abs/2402.02801
tags:
- tuning
- tickets
- winning
- parameters
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of finding small, efficient parameter
  subsets for multilingual fine-tuning of large language models (LLMs). The proposed
  KS-Lottery method identifies "winning tickets" within the embedding layer by using
  the Kolmogorov-Smirnov Test to detect significant parameter distribution shifts
  before and after fine-tuning.
---

# KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language Models

## Quick Facts
- arXiv ID: 2402.02801
- Source URL: https://arxiv.org/abs/2402.02801
- Authors: Fei Yuan; Chang Ma; Shuai Yuan; Qiushi Sun; Lei Li
- Reference count: 32
- Primary result: Finding certified lottery tickets using KS test on embedding layer achieves translation performance comparable to full fine-tuning with only 18 token embeddings of LLaMA-7B

## Executive Summary
This paper introduces KS-Lottery, a method for finding certified winning tickets in multilingual language models by identifying significant parameter distribution shifts in the embedding layer. The approach uses the Kolmogorov-Smirnov Test to detect which parameters change meaningfully during fine-tuning, allowing selective fine-tuning of only these "winning tickets" while keeping other parameters frozen. The method is theoretically proven to guarantee performance comparable to full fine-tuning while using significantly fewer parameters, achieving competitive translation results with as few as 18 token embeddings.

## Method Summary
KS-Lottery applies the Kolmogorov-Smirnov Test to analyze parameter distribution shifts between pre-trained and fine-tuned embedding parameters. The method first fine-tunes the embedding layer on a source task (Alpaca-En), then identifies winning tickets as parameters showing statistically significant distribution changes. These winning tickets are then selectively fine-tuned on downstream multilingual tasks (e.g., bilingual translation on Flores-101) while other parameters remain frozen. The theoretical framework provides certified accuracy guarantees, ensuring that performance will match full fine-tuning when winning tickets are properly selected based on significance thresholds.

## Key Results
- Achieves translation performance comparable to full fine-tuning using only 18 token embeddings of LLaMA-7B
- Outperforms other parameter-efficient methods like LoRA and Prefix-Tuning while using fewer parameters
- Provides theoretical certification guarantees that winning tickets will match full fine-tuning performance
- Demonstrates effectiveness across multiple language pairs (en→ca, en→ro, en→es, en→de) in Flores-101

## Why This Works (Mechanism)
The method exploits the observation that parameter distribution shifts during fine-tuning reveal which parameters are actually important for task adaptation. By focusing only on parameters that change significantly (as measured by KS test), the approach identifies the minimal subset needed for effective fine-tuning. The theoretical guarantees stem from statistical bounds on distribution differences, ensuring that significant shifts correspond to performance-critical parameters. This selective approach reduces computational overhead while maintaining performance through certified parameter importance identification.

## Foundational Learning

**Kolmogorov-Smirnov Test**
- Why needed: Provides statistical measure to detect significant distribution shifts in parameters
- Quick check: Verify understanding of null hypothesis and p-value interpretation in two-sample KS test

**Lottery Ticket Hypothesis**
- Why needed: Framework for identifying sparse subnetworks that can be trained in isolation
- Quick check: Understand how winning tickets relate to original model performance

**Multilingual Fine-tuning**
- Why needed: Context for applying parameter-efficient methods to cross-lingual tasks
- Quick check: Recognize challenges of fine-tuning single model for multiple languages

## Architecture Onboarding

**Component Map**
Embedding Layer -> KS Test Analysis -> Winning Ticket Selection -> Selective Fine-tuning

**Critical Path**
Fine-tune embedding -> Compute KS statistics -> Identify significant parameters (α threshold) -> Fine-tune winning tickets on target task -> Evaluate spBLEU

**Design Tradeoffs**
- Parameter efficiency vs. computational overhead of KS test
- Significance threshold α vs. coverage of important parameters
- Embedding layer focus vs. potential benefits of other layers

**Failure Signatures**
- Poor performance indicates incorrect α selection or insufficient parameter coverage
- Training instability suggests improper parameter isolation during selective fine-tuning

**3 First Experiments**
1. Apply KS test to identify distribution shifts in embedding parameters after fine-tuning
2. Vary α threshold to observe impact on winning ticket size and performance
3. Compare spBLEU performance between full fine-tuning and KS-Lottery with 18 token embeddings

## Open Questions the Paper Calls Out

**Open Question 1**
Can KS-Lottery be applied to other layers beyond the embedding layer, and what would be the performance implications? The paper mentions this is possible but focuses on embedding layer for convenience and stability. Experimental validation on other layers like transformer layers could reveal different patterns of parameter importance and potentially improve efficiency.

**Open Question 2**
How does the choice of α (significance level) affect the balance between certified accuracy and prediction accuracy in practical applications? While theoretical insights exist about α's impact on certification bounds, practical implications across diverse tasks and datasets need empirical exploration to optimize method performance.

**Open Question 3**
What are the computational trade-offs between KS-Lottery and other parameter-efficient methods like LoRA and Prefix-Tuning in terms of training time and resource usage? The paper highlights parameter efficiency but doesn't address computational costs of running KS tests or overall training efficiency compared to alternatives.

## Limitations
- Evaluation limited to specific bilingual translation tasks (Flores-101) with narrow language pairs
- Theoretical guarantees rely on assumptions about parameter distribution shifts that may not generalize
- Scalability to diverse multilingual scenarios and different model architectures remains unproven

## Confidence
- **High confidence**: Core methodology of using KS test for identifying significant parameter shifts is clearly specified and theoretically justified
- **Medium confidence**: Empirical results showing competitive performance with minimal parameter tuning are convincing but limited to specific model sizes and tasks
- **Low confidence**: Claims about broader applicability to different model families and task types are not sufficiently validated

## Next Checks
1. Test KS-Lottery's performance across diverse multilingual tasks beyond translation, including cross-lingual retrieval and zero-shot classification
2. Validate the method's effectiveness on smaller language models (1B-3B parameters) to assess scalability
3. Conduct ablation studies to determine the optimal significance level α and its impact on winning ticket selection across different language pairs