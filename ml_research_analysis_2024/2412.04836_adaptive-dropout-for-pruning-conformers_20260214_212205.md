---
ver: rpa2
title: Adaptive Dropout for Pruning Conformers
arxiv_id: '2412.04836'
source_url: https://arxiv.org/abs/2412.04836
tags:
- latexit
- dropout
- pruning
- proposed
- regularization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to effectively perform joint training-and-pruning
  based on adaptive dropout layers with unit-wise retention probabilities. The proposed
  method is based on the estimation of a unit-wise retention probability in a dropout
  layer.
---

# Adaptive Dropout for Pruning Conformers

## Quick Facts
- arXiv ID: 2412.04836
- Source URL: https://arxiv.org/abs/2412.04836
- Reference count: 0
- Proposed method achieves 54% parameter reduction while improving WER by ~1% on LibriSpeech

## Executive Summary
This paper introduces an adaptive dropout-based pruning method for Conformers that simultaneously reduces parameters and improves speech recognition accuracy. The approach estimates unit-wise retention probabilities during training using back-propagation and Gumbel-Softmax, allowing identification of prunable units. By strategically placing adaptive dropout layers in three key locations within each Conformer block, the method achieves significant parameter reduction without sacrificing (and in fact slightly improving) model performance on the LibriSpeech task.

## Method Summary
The proposed method introduces adaptive dropout layers with unit-wise retention probabilities at three critical locations within each Conformer block: the hidden layer of the feed-forward network component, the query and value vectors of the self-attention component, and the input vectors of the LConv component. Retention probabilities are estimated using back-propagation combined with the Gumbel-Softmax technique, enabling differentiable approximation of discrete dropout decisions. This joint training-and-pruning approach allows the model to learn which units can be effectively pruned while maintaining or improving performance.

## Key Results
- Achieved 54% reduction in parameters while improving WER by approximately 1% on LibriSpeech
- Demonstrated effective parameter reduction without accuracy degradation (in fact, with slight improvement)
- Showed that adaptive dropout can be successfully integrated at multiple points within Conformer architecture

## Why This Works (Mechanism)
The method works by learning which units to retain or prune through differentiable approximation of discrete dropout decisions. By estimating retention probabilities for each unit, the model can identify and effectively remove redundant or less important components. The Gumbel-Softmax technique enables back-propagation through the discrete dropout decision, allowing the model to jointly optimize for both performance and sparsity during training rather than requiring separate pruning and fine-tuning phases.

## Foundational Learning

**Conformer architecture**: Hybrid of transformer self-attention and convolution modules - needed for understanding where adaptive dropout layers are placed; quick check: verify the standard Conformer block structure

**Gumbel-Softmax technique**: Differentiable approximation of discrete sampling - needed for enabling gradient-based learning of retention probabilities; quick check: confirm the temperature parameter affects approximation quality

**Unit-wise dropout**: Per-unit rather than per-layer dropout rates - needed for fine-grained pruning decisions; quick check: verify that individual unit retention can vary significantly

**Speech recognition evaluation**: WER as primary metric - needed for understanding the performance measurement; quick check: confirm baseline WER on LibriSpeech

## Architecture Onboarding

**Component map**: Input -> Adaptive Dropout (LConv) -> Self-Attention (with adaptive dropout on Q/V) -> Feed-Forward (with adaptive dropout hidden layer) -> Output

**Critical path**: The attention mechanism and feed-forward network are the primary computational bottlenecks, with adaptive dropout providing pruning opportunities in both

**Design tradeoffs**: Joint training-and-pruning vs separate phases - the proposed method trades some training complexity for end-to-end optimization, potentially achieving better final performance

**Failure signatures**: Over-aggressive pruning leading to accuracy degradation, or under-pruning resulting in minimal parameter reduction

**First experiments**: 1) Baseline Conformer training without pruning, 2) Single adaptive dropout location (e.g., only FFN), 3) Varying dropout temperature parameter to study its effect on pruning aggressiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single speech recognition task (LibriSpeech), raising generalizability concerns
- Lack of ablation studies to isolate contribution of each dropout location
- No analysis of inference latency or memory usage patterns during deployment
- Absence of comparison against established pruning methods like magnitude pruning

## Confidence
- Core claims: Medium - technically coherent but limited evaluation scope
- 54% parameter reduction: Medium - plausible given multi-location dropout application
- 1% WER improvement: Medium - modest gain suggests incremental rather than transformative improvement

## Next Checks
1. Evaluate the method across multiple ASR datasets (multi-lingual, noisy conditions) to test robustness
2. Conduct ablation studies to quantify the individual impact of dropout placement in each component
3. Compare against established pruning methods using identical experimental conditions and compute budgets