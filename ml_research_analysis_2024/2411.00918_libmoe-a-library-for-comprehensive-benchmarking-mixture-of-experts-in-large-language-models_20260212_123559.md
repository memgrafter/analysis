---
ver: rpa2
title: 'LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large
  Language Models'
arxiv_id: '2411.00918'
source_url: https://arxiv.org/abs/2411.00918
tags:
- expert
- arxiv
- experts
- training
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LibMoE is a unified framework for reproducible research on Mixture-of-Experts
  (MoE) models, designed to lower computational barriers and standardize evaluation
  across both pretraining and sparse-upcycling regimes. It supports diverse model
  sizes and seven state-of-the-art MoE algorithms, providing modular tools for routing
  dynamics, expert utilization, and load balancing analysis.
---

# LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models

## Quick Facts
- arXiv ID: 2411.00918
- Source URL: https://arxiv.org/abs/2411.00918
- Reference count: 40
- One-line primary result: Comprehensive benchmarking framework revealing marginal performance differences between MoE variants but distinct routing behaviors across tasks and training regimes.

## Executive Summary
LibMoE is a unified framework designed to enable reproducible research on Mixture-of-Experts models for large language models and vision-language models. The library lowers computational barriers by supporting diverse model sizes and providing modular tools for analyzing routing dynamics, expert utilization, and load balancing across seven state-of-the-art MoE algorithms. Through comprehensive experiments, the authors reveal that current MoE variants show only marginal performance differences under matched conditions, with distinct routing behaviors emerging across tasks and training regimes.

The framework supports both pretraining and sparse-upcycling regimes, enabling systematic comparison of routing strategies and specialization patterns. Key findings include that initialization scale strongly affects early load balancing, routing stability and optimality differ between pretraining and upcycling, and expert representations remain diverse without explicit regularization. LibMoE provides actionable insights into routing strategies, specialization, and training dynamics while establishing standardized evaluation protocols for the MoE research community.

## Method Summary
LibMoE is a PyTorch-based framework that supports training and evaluation of MoE models across both LLM and VLM domains. The framework implements seven state-of-the-art MoE algorithms (SMoE, σ-MoE, XMoE, SharedE-V2/V3, MoE++, TC-MoE) and provides modular components for routing, training, and evaluation. Training uses 4×H100 GPUs with AdamW optimizer, cosine learning rate schedule, gradient clipping, and mixed precision. The framework supports both pretraining (from scratch) and sparse-upcycling (initializing MoE layers from dense pretrained checkpoints) regimes. Evaluation metrics include task-specific performance, expert routing dynamics (change rate, allocation entropy, weight allocation entropy, router margin), load balancing efficiency, and cross-task specialization patterns across 11 VLM benchmarks and 9 LLM benchmarks.

## Key Results
- Current MoE variants show only marginal performance differences under matched conditions
- Initialization scale (router weight std) strongly affects early load balancing without changing the loss function
- Sparse upcycling exposes algorithmic differences in routing optimality more than from-scratch pretraining
- Expert representations remain diverse without explicit regularization
- Shared experts in sparse upcycling act as stable anchors, improving routing stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initialization scale (std of router weights) directly influences early load balancing without changing the loss function.
- Mechanism: Smaller initialization std reduces the initial variance in routing logits, leading to more uniform expert selection probabilities and thus better early load balance.
- Core assumption: The router logits are directly influenced by the initialization distribution and this effect is strong enough to shape routing before adaptation.
- Evidence anchors:
  - [abstract]: "subtle changes in router initialization shape early expert utilization"
  - [section]: Figure 10 shows performance improvement with lower std values (0.02 vs 0.04 vs 0.06) under the same softmax routing.
- Break condition: If the router logits are normalized or re-scaled early in training, or if expert specialization overrides initial uniformity.

### Mechanism 2
- Claim: Shared experts act as stable anchors in sparse upcycling, improving routing stability.
- Mechanism: When shared experts are initialized from a pretrained dense model, they provide consistent, general-purpose representations, allowing the router to focus on task-specific expert allocation rather than co-adapting with unstable experts.
- Core assumption: Pretrained experts encode useful general features and their stability reduces routing volatility.
- Evidence anchors:
  - [abstract]: "initialization scale strongly affects early load balancing, routing stability and optimality differ between pretraining and upcycling"
  - [section]: "In the VLM sparse upcycling setup, SMoE layers are initialized from a pretrained dense model, where shared experts already encode general purpose visual linguistic representations."
- Break condition: If the pretrained experts are not well-aligned with the task or if the task requires very different representations.

### Mechanism 3
- Claim: Sparse upcycling exposes algorithmic differences in routing optimality more than from-scratch pretraining.
- Mechanism: In upcycling, models start from a fixed dense backbone, making routing decisions more critical and revealing differences in how well algorithms select experts. In pretraining, all components adapt together, masking these differences.
- Core assumption: Routing optimality becomes a bottleneck when the underlying representations are fixed.
- Evidence anchors:
  - [abstract]: "routing stability and optimality differ between pretraining and upcycling"
  - [section]: "In terms of expert optimality, we analyze performance sensitivity via DropTop-1 and DropTop1&2 perturbation... In sparse upcycling, the performance drop patterns differ substantially between algorithms."
- Break condition: If the dense backbone is too far from optimal for the task, or if routing algorithms converge to similar suboptimal patterns.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) routing and gating
  - Why needed here: The entire paper analyzes routing behaviors, stability, and optimization across multiple MoE variants.
  - Quick check question: What is the role of the router in an MoE layer, and how does it select experts?

- Concept: Sparse upcycling vs. full pretraining
  - Why needed here: The paper explicitly compares these two regimes to reveal differences in routing behavior and expert utilization.
  - Quick check question: How does sparse upcycling differ from full pretraining in terms of initialization and training objectives?

- Concept: Load balancing and expert specialization
  - Why needed here: The paper evaluates how different MoE designs balance load across experts and how this relates to task specialization.
  - Quick check question: Why is load balancing important in MoE models, and how can it conflict with expert specialization?

## Architecture Onboarding

- Component map:
  LibMoE framework -> Seven MoE variants (SMoE, σ-MoE, XMoE, SharedE-V2/V3, MoE++, TC-MoE) -> Datasets (LLaVA-665K, OneVision, SlimPajama) -> Evaluation (11 VLM benchmarks + 9 LLM benchmarks)

- Critical path:
  1. Initialize router and experts (with appropriate std for initialization scale)
  2. Forward pass: Router computes scores, top-K experts selected
  3. Compute MoE output and loss
  4. Apply load balancing loss (optional) and backpropagate
  5. Monitor expert utilization and routing stability

- Design tradeoffs:
  - Initialization std: Smaller std improves early load balance but may slow convergence
  - Shared vs. non-shared experts: Shared experts improve stability in upcycling but may reduce specialization
  - Load balancing loss: Helps balance but may hurt performance if too strong

- Failure signatures:
  - High imbalance in expert utilization (some experts rarely selected)
  - Large variance in expert selection across tokens or layers
  - Router saturation remains low throughout training (unstable routing)

- First 3 experiments:
  1. Train a small MoE model with different router initialization std values and compare early load balance
  2. Compare routing stability (expert change rate) between sparse upcycling and full pretraining on the same model
  3. Evaluate the impact of removing the load balancing loss on expert utilization and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does SMoE exhibit a counterintuitive performance gain under the DropTop1 perturbation in VLM tasks, and what does this imply about its routing efficiency?
- Basis in paper: [explicit] Section 5b states that SMoE shows a "+2.3 gain in ∆Total Score under DropTop1 perturbation" in the VLM setting, suggesting suboptimal routing decisions in its original configuration.
- Why unresolved: The paper does not explain the mechanism behind this gain or whether it reflects inefficiency in expert utilization or an unintended optimization artifact.
- What evidence would resolve it: Controlled ablation studies comparing expert contribution weights and routing entropy before and after perturbation, along with analysis of task-specific routing patterns.

### Open Question 2
- Question: How do the initialization strategies of shared experts in sparse upcycling contribute to the observed differences in routing stability between pretraining and upcycling regimes?
- Basis in paper: [explicit] Section 5a discusses how shared experts, when initialized from a pretrained dense checkpoint in VLMs, act as stable anchors, whereas in pretraining they must co-adapt from scratch, leading to higher expert change rates.
- Why unresolved: The paper identifies the phenomenon but does not quantify the contribution of shared expert initialization stability versus other factors like task complexity or dataset size.
- What evidence would resolve it: Comparative experiments isolating shared expert initialization effects by controlling for task type and dataset scale, measuring routing volatility over time.

### Open Question 3
- Question: To what extent does expert similarity persist or evolve during sparse upcycling, and how does this impact long-term model performance and robustness?
- Basis in paper: [explicit] Section 5f reports that expert output weights exhibit low cosine similarity over training, indicating specialization, but does not analyze the temporal dynamics or downstream effects on generalization.
- Why unresolved: The paper provides a static snapshot of expert similarity but lacks longitudinal analysis linking similarity trends to performance stability or robustness to distribution shifts.
- What evidence would resolve it: Longitudinal tracking of expert similarity coupled with performance metrics across training epochs and out-of-distribution evaluation tasks.

## Limitations
- Initialization scale experiments show clear trends but don't control for other routing hyperparameters that could influence early load balancing
- Sparse upcycling comparisons reveal performance differences between algorithms, but the fixed dense backbone may introduce confounding factors not fully controlled
- Expert diversity findings rely on qualitative interpretation of routing patterns rather than quantitative measures of representation similarity

## Confidence
- **High Confidence**: Initialization scale effects on early load balancing (Mechanism 1) - supported by direct experimental evidence and clear before/after comparisons
- **Medium Confidence**: Shared experts improving routing stability in upcycling (Mechanism 2) - plausible mechanism but relies on assumed benefits of pretrained representations
- **Low Confidence**: Sparse upcycling revealing algorithmic differences more than pretraining (Mechanism 3) - results show differences but alternative explanations exist

## Next Checks
1. Replicate the initialization scale experiments with controlled temperature and routing hyperparameters to isolate the initialization effect
2. Test sparse upcycling with both well-matched and poorly-matched dense backbones to verify if routing differences persist independent of representation quality
3. Measure expert representation similarity using centered kernel alignment (CKA) or similar metrics to quantify diversity claims quantitatively