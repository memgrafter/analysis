---
ver: rpa2
title: Uncertainty-aware self-training with expectation maximization basis transformation
arxiv_id: '2405.01175'
source_url: https://arxiv.org/abs/2405.01175
tags:
- basis
- data
- self-training
- pages
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an uncertainty-aware self-training framework
  that addresses the overconfidence issue in pseudo-label generation by combining
  model and dataset uncertainty information. The method uses Expectation-Maximization
  (EM) with Gaussian Mixture Models to smooth labels and estimate uncertainty, while
  a basis extraction network initializes the EM process with orthogonal bases derived
  from dataset features.
---

# Uncertainty-aware self-training with expectation maximization basis transformation
## Quick Facts
- arXiv ID: 2405.01175
- Source URL: https://arxiv.org/abs/2405.01175
- Reference count: 10
- Introduces uncertainty-aware self-training framework addressing overconfidence in pseudo-label generation

## Executive Summary
This paper presents an uncertainty-aware self-training framework that addresses the overconfidence issue in pseudo-label generation for unsupervised domain adaptation. The method combines model and dataset uncertainty information using Expectation-Maximization with Gaussian Mixture Models to smooth labels and estimate uncertainty. A basis extraction network initializes the EM process with orthogonal bases derived from dataset features. The approach iteratively updates pseudo-labels and model parameters through uncertainty-aware retraining, where samples with higher label variance receive lower training weights.

## Method Summary
The proposed method integrates uncertainty estimation into the self-training loop through an EM-based basis transformation approach. It initializes the EM process using a basis extraction network that creates orthogonal bases from dataset features, then iteratively refines pseudo-labels while simultaneously estimating both model and dataset uncertainties. The framework assigns lower weights to samples with higher label variance during retraining, creating a feedback loop that progressively improves pseudo-label quality. The method demonstrates effectiveness across multiple domain adaptation tasks including image classification (Office-31, VisDA17) and semantic segmentation (GTA5→Cityscapes, SYNTHIA→Cityscapes).

## Key Results
- Achieves 1-3 percentage point improvements over confidence-aware self-training algorithms
- Demonstrates state-of-the-art performance on synthetic-to-real domain adaptation tasks
- Shows consistent gains across various benchmarks including Office-31 and VisDA17
- Effectively captures uncertainty information to alleviate overconfident pseudo-label issues

## Why This Works (Mechanism)
The method works by explicitly modeling both model uncertainty (from prediction confidence) and dataset uncertainty (from feature distribution) through EM-based basis transformation. By initializing the EM process with orthogonal bases derived from dataset features, the framework captures the intrinsic structure of the target domain. The iterative refinement of pseudo-labels while simultaneously estimating uncertainties creates a self-correcting mechanism that prevents the propagation of overconfident errors. The weighting scheme that assigns lower importance to samples with higher label variance ensures that uncertain predictions have minimal impact on model updates, leading to more robust adaptation.

## Foundational Learning
- **Expectation-Maximization (EM) algorithm**: A statistical method for finding maximum likelihood estimates in probabilistic models with latent variables. Why needed: Provides the mathematical foundation for iteratively refining pseudo-labels while estimating uncertainty distributions. Quick check: Verify understanding of E-step (expectation) and M-step (maximization) in the context of pseudo-label refinement.

- **Gaussian Mixture Models (GMM)**: Probabilistic models that represent data as a mixture of multiple Gaussian distributions. Why needed: Used to model the uncertainty distribution of pseudo-labels, allowing for smooth label representations rather than hard assignments. Quick check: Understand how GMM components represent different classes and how responsibilities are calculated.

- **Orthogonal basis transformation**: A mathematical technique that creates a set of linearly independent vectors spanning the feature space. Why needed: Enables the initialization of the EM process with a well-conditioned representation that captures dataset structure. Quick check: Verify that basis vectors are indeed orthogonal and span the relevant feature dimensions.

- **Self-training in domain adaptation**: A semi-supervised learning approach where a model trained on labeled source data generates pseudo-labels for unlabeled target data. Why needed: Forms the core framework within which uncertainty modeling is integrated. Quick check: Understand the standard self-training loop and where uncertainty estimation is inserted.

- **Uncertainty quantification in deep learning**: Methods for estimating the confidence or uncertainty of model predictions. Why needed: Essential for distinguishing reliable from unreliable pseudo-labels. Quick check: Differentiate between aleatoric (data) and epistemic (model) uncertainty in the context of domain adaptation.

- **Sample weighting in training**: Assigning different importance weights to training samples based on their reliability or difficulty. Why needed: Allows the model to focus on more reliable pseudo-labels while down-weighting uncertain predictions. Quick check: Verify that the weighting scheme effectively implements the uncertainty-aware retraining strategy.

## Architecture Onboarding
- **Component map**: Input features -> Basis Extraction Network -> Orthogonal Basis Initialization -> EM Algorithm (GMM) -> Pseudo-label Smoothing -> Uncertainty Estimation -> Sample Weighting -> Model Retraining -> Updated Pseudo-labels (loop)
- **Critical path**: The iterative loop from EM-based pseudo-label smoothing through uncertainty estimation to sample-weighted retraining forms the core of the method's effectiveness.
- **Design tradeoffs**: The method trades computational overhead for improved pseudo-label quality through iterative EM refinement and uncertainty estimation.
- **Failure signatures**: Overconfident pseudo-labels in early iterations, sensitivity to initialization quality, and potential convergence issues in the EM process.
- **3 first experiments**: (1) Verify EM convergence with synthetic data and known ground truth; (2) Test basis extraction network initialization quality on benchmark datasets; (3) Validate uncertainty-aware weighting by comparing training dynamics with and without uncertainty-based sample weights.

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability of the method to larger datasets and its performance on highly imbalanced target domains. It also raises questions about the computational overhead introduced by the iterative EM process and basis extraction network, particularly for real-time applications. Additionally, the authors suggest further investigation into the method's behavior across different domain adaptation scenarios and its robustness to varying degrees of domain shift.

## Limitations
- Reliance on dataset-driven initialization may not generalize across domains with substantially different feature distributions
- Performance on highly imbalanced target domains remains unclear
- Computational overhead from iterative EM process and basis extraction network could limit scalability

## Confidence
- High: Synthetic-to-real domain adaptation tasks (GTA5→Cityscapes, SYNTHIA→Cityscapes) showing consistent 1-3 percentage point gains
- Medium: Office and VisDA benchmarks where results depend on specific source-target domain characteristics
- Low: Scalability and real-time application scenarios due to computational overhead concerns

## Next Checks
1. Evaluate performance on highly imbalanced target domains to assess robustness to class distribution shifts
2. Conduct ablation studies isolating contributions of basis extraction network versus EM uncertainty modeling
3. Test scalability by measuring computational overhead on larger datasets (COCO or ImageNet-scale) and analyzing trade-off between performance gains and training time