---
ver: rpa2
title: 'RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF for
  Conversational QA over KGs with RAG'
arxiv_id: '2412.17690'
source_url: https://arxiv.org/abs/2412.17690
tags:
- question
- retrieval
- knowledge
- ragonite
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAGONITE, a novel system for conversational
  question answering over RDF knowledge graphs that addresses the brittleness of SPARQL
  and the inability to handle abstract intents. RAGONITE employs a two-pronged approach
  combining SQL queries over a database derived from the KG and text retrieval over
  KG verbalizations, with iterative retrieval to refine results when initial outputs
  are unsatisfactory.
---

# RAGONITE: Iterative Retrieval on Induced Databases and Verbalized RDF for Conversational QA over KGs with RAG

## Quick Facts
- arXiv ID: 2412.17690
- Source URL: https://arxiv.org/abs/2412.17690
- Reference count: 0
- 28/30 correct answers on BMW KG benchmark, outperforming SQL-only (18), verbalization-only (24), and SPARQL-only (4) baselines

## Executive Summary
RAGONITE is a conversational QA system over RDF knowledge graphs that addresses the brittleness of SPARQL and the inability to handle abstract intents. The system combines SQL queries over a database derived from the KG with text retrieval over KG verbalizations, using iterative retrieval to refine results when initial outputs are unsatisfactory. An LLM integrates results from both branches to generate coherent answers. Evaluated on a BMW KG, RAGONITE achieved 28/30 correct answers compared to significantly lower scores for specialized baselines, demonstrating the effectiveness of its hybrid approach.

## Method Summary
RAGONITE employs a two-pronged retrieval pipeline that converts RDF triples into a relational database for SQL querying and verbalizes KG facts into natural language passages for text retrieval. The system uses an LLM to generate intent-explicit SQL and NL search queries, executes both branches, and iteratively refines results when quality is unsatisfactory. A final LLM integration step combines SQL results and text passages into the final answer. The approach supports heterogeneous QA by incorporating supplementary text sources and achieves interactive response times of ~6.3 seconds per question.

## Key Results
- Achieved 28/30 correct answers on BMW KG benchmark
- Outperformed SQL-only (18/30), verbalization-only (24/30), and SPARQL-only (4/30) baselines
- Maintained interactive response times (~6.3 seconds per question) with iterative retrieval support
- Successfully handled heterogeneous QA across lookup, complex, and abstract intent categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SQL queries over a database derived from the KG are more reliable than SPARQL queries generated by LLMs for complex intents.
- Mechanism: The system automatically converts RDF triples into a relational database schema, where SQL generation is simpler and more accurate for LLMs due to training data abundance and clearer structure.
- Core assumption: Relational databases with explicit schemas are easier for LLMs to reason about than graph-based SPARQL.
- Evidence anchors: [abstract] "it is brittle for complex intents and conversational questions, and (ii) it is not suitable for more abstract needs"; [section] "a KG often contains information that can satisfy more abstract intents: this cannot be harnessed via SPARQL"

### Mechanism 2
- Claim: Verbalizing KG facts into natural language passages enables handling of abstract intents that cannot be captured by structured queries.
- Mechanism: RDF triples are converted into NL passages using simple concatenation rules, allowing LLMs to reason over KG content like regular text.
- Core assumption: LLMs can effectively reason over KG verbalizations when structured query approaches fail.
- Evidence anchors: [abstract] "text-search results over verbalizations of KG facts"; [section] "Abstract questions like ‘Innovative highlights in x7? ’ require NL understanding and common sense reasoning to map the intent to equipments and accessories"

### Mechanism 3
- Claim: Iterative retrieval with error feedback improves both SQL and text search results when initial outputs are unsatisfactory.
- Mechanism: The LLM monitors results quality and requests additional rounds of retrieval, providing error messages from previous attempts to guide corrections.
- Core assumption: Error feedback helps retrievers correct mistakes and improve subsequent results.
- Evidence anchors: [abstract] "Our pipeline supports iterative retrieval: when the results of any branch are found to be unsatisfactory, the system can automatically opt for further rounds"; [section] "Sending such errors back to the retrievers gives them the opportunity to correct small mistakes, like resolving an ambiguous column name"

## Foundational Learning

- Concept: RDF triple structure (subject-predicate-object)
  - Why needed here: Understanding how KGs store facts is essential for grasping database induction and verbalization processes
  - Quick check question: What are the three components of an RDF triple and how do they map to database columns?

- Concept: Vector similarity search for text retrieval
  - Why needed here: The system uses dense retrievers to find relevant verbalized passages based on semantic similarity
  - Quick check question: How does vector search differ from keyword search in finding relevant passages?

- Concept: Intent-explicit question formulation
  - Why needed here: The system requires questions that contain all necessary context without relying on conversation history
  - Quick check question: What information must be included in an intent-explicit question that might be implicit in a conversational question?

## Architecture Onboarding

- Component map: Database induction -> KG verbalization -> Intent-explicit SQL generation -> Intent-explicit NL question generation -> Iterative retrieval controller -> Branch integration LLM -> Answer

- Critical path: Question → Intent-explicit generation → SQL execution + Text retrieval → Iterative refinement → Branch integration → Answer

- Design tradeoffs:
  - Database vs graph: Trade-offs between query simplicity and semantic richness
  - Verbalization vs structured: Trade-offs between handling abstract intents and maintaining precision
  - Iteration count: Trade-offs between answer quality and response time

- Failure signatures:
  - SQL generation failures: Entity linking errors, schema comprehension issues
  - Text retrieval failures: Irrelevant passages, missing critical information
  - Integration failures: Conflicting information between branches, LLM confusion

- First 3 experiments:
  1. Test database induction with a small KG to verify schema creation and data insertion
  2. Verify verbalization quality by checking if converted passages preserve semantic meaning
  3. Test iterative retrieval by intentionally providing poor initial results and checking if subsequent rounds improve quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for automating database schema simplification when inducing a database from a knowledge graph, beyond the current approach of identifying entity types and relationships?
- Basis in paper: [inferred] The paper mentions that manually creating an equivalent database from the KG could result in a simpler schema, and that finding the DB with least complexity is a topic for future work.
- Why unresolved: The paper does not explore alternative methods for schema simplification or provide a systematic approach to determining the optimal schema complexity.
- What evidence would resolve it: Comparative studies evaluating different schema induction strategies on various KGs, measuring query performance, answer accuracy, and computational efficiency.

### Open Question 2
- Question: How can RAGONITE's performance be improved for handling entity linking errors, particularly for ad hoc mentions in conversational questions that don't exactly match entity labels in the KG?
- Basis in paper: [explicit] The evaluation section notes that incorrect entity linking for ad hoc mentions was a particular vulnerability of SQL and SPARQL, where verbalizations were helpful.
- Why unresolved: The paper identifies the problem but doesn't propose or evaluate specific solutions for improving entity linking robustness.
- What evidence would resolve it: Experiments comparing different entity linking approaches (fuzzy matching, contextual embeddings, or hybrid methods) integrated into RAGONITE's pipeline.

### Open Question 3
- Question: What is the impact of incorporating reflection mechanisms that enable each module to critique and improve its output on RAGONITE's overall performance and efficiency?
- Basis in paper: [explicit] The conclusions mention that key future work would enhance the agentic workflow by incorporating reflection mechanisms that enable each module to critique and improve its output, without compromising efficiency.
- Why unresolved: The paper proposes this as future work but doesn't implement or evaluate such reflection mechanisms.
- What evidence would resolve it: A modified version of RAGONITE with implemented reflection mechanisms compared against the baseline in terms of accuracy, response time, and computational overhead.

## Limitations
- Performance based on 30 curated questions may not reflect real-world conversational complexity with extensive context referencing
- Reliance on LLM-based SQL generation carries risks of hallucination or incorrect schema interpretation for complex KG structures
- Iterative retrieval introduces additional latency (up to 14.6s for 3-round queries) that may be prohibitive for interactive applications

## Confidence

**High**: SQL generation over relational databases is more reliable than SPARQL generation for complex intents (supported by the 18→28 improvement)

**Medium**: Verbalization enables handling of abstract intents (mechanism appears sound but lacks comparative ablation)

**Low**: Iterative retrieval consistently improves results (mechanism described but effectiveness depends on LLM quality and threshold criteria)

## Next Checks
1. Test RAGONITE on a larger, more diverse KG benchmark (e.g., DBpedia or Wikidata) with 100+ questions to assess scalability and generalization
2. Implement an ablation study comparing RAGONITE variants: SQL-only, verbalization-only, and different iteration limits to quantify each component's contribution
3. Measure entity linking accuracy by comparing LLM-generated SQL queries against ground truth SPARQL queries for the same questions