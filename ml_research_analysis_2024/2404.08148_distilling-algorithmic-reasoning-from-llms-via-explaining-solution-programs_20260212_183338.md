---
ver: rpa2
title: Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs
arxiv_id: '2404.08148'
source_url: https://arxiv.org/abs/2404.08148
tags:
- problem
- reasoning
- reasoner
- solve
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel method to distill algorithmic reasoning
  from large language models (LLMs) by leveraging their ability to explain solutions
  rather than solve problems directly. Instead of having LLMs solve programming problems,
  the method uses an LLM to generate editorial-style explanations for correct solutions,
  then fine-tunes a smaller "Reasoner" model on these explanations to learn problem-solving
  strategies.
---

# Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs

## Quick Facts
- arXiv ID: 2404.08148
- Source URL: https://arxiv.org/abs/2404.08148
- Authors: Jierui Li; Raymond Mooney
- Reference count: 18
- Primary result: Achieves 6.1% solve@10 rate vs 3.3% for direct prompting on competitive programming problems

## Executive Summary
This paper introduces a novel approach to distill algorithmic reasoning from large language models by leveraging their ability to explain solutions rather than solve problems directly. Instead of having LLMs solve programming problems, the method uses an LLM to generate editorial-style explanations for correct solutions, then fine-tunes a smaller "Reasoner" model on these explanations to learn problem-solving strategies. This Reasoner generates reasoning hints that guide a separate "Coder" model to implement solutions. Experiments on competitive programming problems show this approach outperforms strong zero-shot baselines and models fine-tuned directly on code.

## Method Summary
The method employs a two-step framework where an Explainer LLM generates editorial-style explanations for <problem, solution-program> pairs. These explanations are then used to fine-tune a smaller Reasoner model, which learns to generate reasoning hints for new problems. A separate Coder model (kept as zero-shot) uses these hints to implement solutions. This decomposition allows the Reasoner to focus on strategic reasoning while the Coder specializes in implementation. The approach is particularly effective at avoiding brute-force solutions and handling more difficult problems compared to learning directly from code.

## Key Results
- Achieves solve@10 rate of 6.1% versus 3.3% for direct prompting on competitive programming problems
- Outperforms models fine-tuned directly on code by learning from natural language explanations instead
- Effectively avoids brute-force solutions and handles more difficult problems than baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
The Explainer LLM can reliably generate high-quality editorial-style explanations for competitive programming solutions because LLMs excel at code comprehension and explanation tasks. The Explainer reads both the problem statement and a correct solution program, then generates a natural language explanation following an editorial template. This explanation captures the problem-solving strategy, key insights, and step-by-step reasoning in a way that preserves semantic meaning beyond just the code.

### Mechanism 2
Learning from natural language explanations is more effective than learning from code alone because explanations capture semantic reasoning rather than just implementation details. The Reasoner fine-tunes on <problem, explanation> pairs rather than <problem, solution-program> pairs. Natural language explanations contain high-level algorithmic concepts, problem analysis, and strategic thinking that code alone may not convey clearly.

### Mechanism 3
Decomposing the problem-solving process into separate Reasoner and Coder modules improves performance compared to end-to-end code generation. The Reasoner focuses solely on generating reasoning processes/hints, while the Coder focuses on implementation. This separation allows each component to specialize in its respective task rather than trying to handle both reasoning and implementation simultaneously.

## Foundational Learning

- **Dynamic programming and algorithmic complexity analysis**
  - Why needed here: Competitive programming problems often require understanding of DP approaches and efficiency constraints. The method needs to distinguish between brute-force and optimal solutions.
  - Quick check question: What is the time complexity difference between a naive O(2^n) approach and a DP O(n^2) solution for a typical competitive programming problem?

- **Chain-of-thought reasoning and its limitations**
  - Why needed here: The paper contrasts explain-based and solve-based CoT approaches, requiring understanding of when CoT fails and why explanation might succeed.
  - Quick check question: Why might standard CoT prompting fail on complex competitive programming problems while explanation-based approaches succeed?

- **Knowledge distillation and fine-tuning strategies**
  - Why needed here: The core method involves distilling reasoning abilities from LLMs through fine-tuning on explanations, requiring understanding of how knowledge transfer works.
  - Quick check question: How does fine-tuning on <problem, explanation> pairs differ from fine-tuning on <problem, solution-code> pairs in terms of what the model learns?

## Architecture Onboarding

- **Component map**: Explainer (LLM that generates explanations) → Fine-tuning dataset (problem, explanation pairs) → Reasoner (fine-tuned model that generates hints) → Coder (zero-shot model that implements solutions) → Online judge (evaluates correctness and efficiency)

- **Critical path**: Explainer generates explanations → Reasoner fine-tunes on explanations → Reasoner generates hints for new problems → Coder implements solutions using hints → Online judge evaluates solutions

- **Design tradeoffs**: Using a separate Explainer adds complexity but enables higher-quality explanations; fine-tuning Reasoner on explanations vs. code involves balancing data quality vs. quantity; sampling strategies (more from Reasoner vs. more from Coder) affect diversity of approaches

- **Failure signatures**: Low solve rates despite good explanations (Reasoner/Coder not learning effectively); high public test pass rates but low solve rates (brute-force solutions being generated); inconsistent performance across difficulty levels (model not generalizing well)

- **First 3 experiments**:
  1. Compare solve@10 rates between direct prompting, naive CoT, editorial CoT, and 0-Reasoner+Coder baselines
  2. Test different fine-tuning strategies (uniform vs. weighted by difficulty) on the Reasoner
  3. Evaluate different sampling strategies (M=1,T=10 vs M=5,T=2 vs M=10,T=1) for combining Reasoner and Coder outputs

## Open Questions the Paper Calls Out

### Open Question 1
How does the explain-based approach perform on programming problems that require entirely different algorithmic paradigms (e.g., graph theory vs. dynamic programming)? The paper mentions testing on competitive programming problems but doesn't explore performance across different algorithmic paradigms. Systematic evaluation showing solve rates for different problem categories would reveal whether the approach is equally effective across paradigms.

### Open Question 2
What is the relationship between explanation quality (as measured by human evaluation) and downstream code generation performance? The paper relies on LLM-generated "silver" explanations without evaluating their quality or establishing correlation with performance. Human evaluation of explanation quality paired with correlation analysis against solve rates would establish this relationship.

### Open Question 3
How does the proposed approach scale with problem complexity beyond the 800-3600 rating range tested? The paper only mentions that "harder problems might lead to low-quality noisy explanations" but doesn't test this empirically or explore the upper limits of the approach. Evaluation on problems with ratings above 3600 would reveal scalability limits and whether the approach can handle truly difficult algorithmic reasoning tasks.

## Limitations
- The method's effectiveness critically depends on the Explainer LLM's ability to generate high-quality editorial-style explanations
- The approach may struggle with problems requiring mathematical insight or those outside typical competitive programming patterns
- The separation into Reasoner and Coder modules introduces complexity and potential communication overhead

## Confidence

**High Confidence**: The experimental results showing solve@10 improvements over baseline methods are well-supported by the data. The claim that natural language explanations capture semantic reasoning better than code alone is consistent with the observed performance gains.

**Medium Confidence**: The mechanism explaining why explanation-based approaches work better than standard CoT is plausible but relies on the assumption that LLMs can reliably translate code to explanations.

**Low Confidence**: The scalability of the approach to problems outside the competitive programming domain is uncertain. The robustness of the method to different Explainer LLM qualities is not thoroughly tested.

## Next Checks

1. **Ablation Study on Explanation Quality**: Systematically evaluate how variations in explanation quality (from the Explainer) affect Reasoner performance, including cases with poor, moderate, and excellent explanations to establish the sensitivity of the approach to this component.

2. **Cross-Domain Generalization Test**: Apply the trained Reasoner and Coder to algorithmic problems from different domains (e.g., mathematical puzzles, data analysis tasks, or real-world coding challenges) to assess whether the approach generalizes beyond competitive programming.

3. **Efficiency vs. Accuracy Tradeoff Analysis**: Conduct controlled experiments varying the sampling strategy parameters (M and T) to quantify the precise relationship between the number of reasoning hints and implementation attempts, and how this affects both solve rates and computational cost.