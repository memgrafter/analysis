---
ver: rpa2
title: Multimodal Physiological Signals Representation Learning via Multiscale Contrasting
  for Depression Recognition
arxiv_id: '2406.16968'
source_url: https://arxiv.org/abs/2406.16968
tags:
- data
- depression
- fnirs
- recognition
- signals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses depression recognition using multimodal physiological
  signals (fNIRS and EEG). The authors propose a Multimodal physiological signals
  Representation Learning framework via Multiscale Contrasting (MRLMC) that uses Siamese
  network architecture to learn complementary features from fNIRS and EEG signals.
---

# Multimodal Physiological Signals Representation Learning via Multiscale Contrasting for Depression Recognition

## Quick Facts
- arXiv ID: 2406.16968
- Source URL: https://arxiv.org/abs/2406.16968
- Authors: Kai Shao; Rui Wang; Yixue Hao; Long Hu; Min Chen; Hans Arno Jacobsen
- Reference count: 40
- Achieves 0.917 accuracy, 0.850 precision, 0.881 recall, and 0.831 F1-score on multimodal depression recognition

## Executive Summary
This paper proposes a Multimodal physiological signals Representation Learning framework via Multiscale Contrasting (MRLMC) for depression recognition using fNIRS and EEG signals. The framework employs a Siamese network architecture with weight-sharing encoders to learn complementary features from both modalities. Key innovations include a spatio-temporal contrasting module for dynamic feature extraction and a semantic consistency module for maximizing semantic similarity. Extensive experiments on publicly available and self-collected datasets demonstrate superior performance compared to state-of-the-art models.

## Method Summary
MRLMC uses a Siamese network architecture to process augmented fNIRS and EEG pairs, extracting complementary features through multiscale spatio-temporal convolution and semantic consistency modules. The framework includes time-domain data augmentation, MSC module for spatio-temporal feature extraction, and transformer-based semantic feature learning with contrastive and focal loss objectives. The method processes raw multimodal physiological signals to produce depression classification with demonstrated transferability to other multimodal time series tasks.

## Key Results
- Achieves 0.917 accuracy, 0.850 precision, 0.881 recall, and 0.831 F1-score on multimodal depression recognition
- Outperforms state-of-the-art models on both MODMA dataset (53 participants) and self-collected fNIRS-EEG dataset (96 participants with fNIRS, 64 with both)
- Demonstrates transferability to other multimodal time series downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal contrasting via Siamese network architecture enables complementary feature learning between fNIRS and EEG.
- Mechanism: Siamese network with weight-sharing encoders processes augmented fNIRS and EEG pairs, minimizing representation differences while maximizing complementarity through contrastive loss.
- Core assumption: fNIRS and EEG capture complementary information about brain activity under the same stimulation task.
- Evidence anchors:
  - [abstract] "Siamese network architecture to learn complementary features from fNIRS and EEG signals"
  - [section] "Siamese network architecture to learn the feature representations of fNIRS and EEG signals"
  - [corpus] Weak evidence - no direct corpus citations about Siamese multimodal contrasting for depression
- Break condition: If fNIRS and EEG capture redundant rather than complementary information, the contrastive learning objective fails.

### Mechanism 2
- Claim: Multiscale spatio-temporal convolution captures dynamic brain activation patterns across multiple temporal and spatial scales.
- Mechanism: Multiple parallel convolution layers with different kernel sizes extract spatio-temporal features at varying scales, then concatenate to form comprehensive representation.
- Core assumption: Brain activation patterns have multi-scale spatio-temporal characteristics that require different receptive fields.
- Evidence anchors:
  - [abstract] "design a spatio-temporal contrasting module to learn the representation of fNIRS and EEG through weight-sharing multiscale spatio-temporal convolution"
  - [section] "design a multiscale spatio-temporal convolution (MSC) module to learn the spatio-temporal representation and dynamic characteristics"
  - [corpus] No direct corpus evidence about multiscale convolution for multimodal physiological signals
- Break condition: If brain activation patterns are not inherently multi-scale, additional complexity provides no benefit.

### Mechanism 3
- Claim: Semantic consistency module maximizes semantic similarity between modalities to capture deeper brain activation state information.
- Mechanism: Transformer-based semantic feature extraction followed by cosine similarity maximization ensures both modalities capture consistent semantic representations of brain activation.
- Core assumption: fNIRS and EEG signals contain consistent semantic information about brain activation states despite different measurement modalities.
- Evidence anchors:
  - [abstract] "semantic consistency module to maximize semantic similarity under stimulation tasks"
  - [section] "semantic consistency module to further learn the semantic consistency representation under stimulation tasks"
  - [corpus] Weak evidence - no direct corpus citations about transformer-based semantic consistency for multimodal physiological signals
- Break condition: If modalities capture fundamentally different aspects of brain activity, forcing semantic consistency degrades performance.

## Foundational Learning

- Concept: Siamese network architecture with weight-sharing encoders
  - Why needed here: Enables direct comparison of feature representations from different modalities while ensuring fair comparison through shared weights
  - Quick check question: How does weight-sharing in Siamese networks prevent modality-specific bias in feature learning?

- Concept: Contrastive learning and temperature-scaled similarity
  - Why needed here: Maximizes feature similarity for positive pairs (same sample, different modalities) while minimizing similarity for negative pairs
  - Quick check question: What role does the temperature parameter τ play in controlling the sharpness of similarity distributions?

- Concept: Transformer-based semantic feature extraction
  - Why needed here: Captures contextual relationships and deep semantic information across temporal sequences in physiological signals
  - Quick check question: How does multi-head attention in transformers help capture different aspects of semantic relationships?

## Architecture Onboarding

- Component map: Input augmentation → Spatio-temporal contrasting (MSC + contrastive loss) → Semantic consistency (Transformer + semantic loss) → Classification (focal loss)
- Critical path: Raw signals → Time-domain augmentation → MSC feature extraction → Spatio-temporal contrastive loss → Transformer semantic extraction → Semantic consistency loss → Focal loss classification
- Design tradeoffs: Siamese architecture ensures fair modality comparison but limits modality-specific feature specialization; transformer adds semantic depth but increases computational cost
- Failure signatures: Poor performance on single modalities suggests spatio-temporal module issues; inconsistent semantic features suggest transformer configuration problems
- First 3 experiments:
  1. Test single modality performance with and without augmentation to verify spatio-temporal module effectiveness
  2. Validate semantic consistency by comparing similarity scores between modalities before and after semantic module
  3. Ablation study removing focal loss to assess impact on class imbalance handling

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity of multiscale spatio-temporal convolution and transformer-based semantic module may limit real-time deployment
- Lack of ablation studies on relative contribution of each module to overall performance
- Dataset sizes are reasonable but not large enough to conclusively establish generalizability across diverse populations

## Confidence

- **High confidence**: The core mechanism of multimodal contrasting via Siamese architecture is well-established in the literature and the experimental results demonstrate clear performance improvements over baseline models.
- **Medium confidence**: The effectiveness of multiscale spatio-temporal convolution for capturing dynamic brain activation patterns is supported by the results, but the specific implementation details and hyperparameter choices significantly impact performance.
- **Low confidence**: The semantic consistency module's contribution to overall performance is difficult to assess without detailed ablation studies and quantitative analysis of semantic similarity improvements.

## Next Checks

1. Conduct detailed ablation studies to quantify the individual and combined contributions of the spatio-temporal contrasting module and semantic consistency module to overall performance.

2. Perform cross-dataset validation using independent depression recognition datasets to assess generalizability and robustness of the framework across different populations and recording conditions.

3. Implement computational efficiency analysis to evaluate real-time deployment feasibility and identify potential bottlenecks in the multiscale convolution and transformer components.