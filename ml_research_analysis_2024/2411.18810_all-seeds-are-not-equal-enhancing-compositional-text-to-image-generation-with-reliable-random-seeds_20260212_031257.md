---
ver: rpa2
title: 'All Seeds Are Not Equal: Enhancing Compositional Text-to-Image Generation
  with Reliable Random Seeds'
arxiv_id: '2411.18810'
source_url: https://arxiv.org/abs/2411.18810
tags:
- seeds
- reliable
- diffusion
- train
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of compositional accuracy in text-to-image
  diffusion models, particularly for prompts involving numerical quantities and spatial
  relations. The core method idea is to leverage reliable random seeds that consistently
  produce accurate object counts and placements, and then fine-tune models on images
  generated using these seeds.
---

# All Seeds Are Not Equal: Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds

## Quick Facts
- arXiv ID: 2411.18810
- Source URL: https://arxiv.org/abs/2411.18810
- Authors: Shuangqi Li; Hieu Le; Jingyi Xu; Mathieu Salzmann
- Reference count: 40
- One-line primary result: Seed mining and selective fine-tuning improves compositional accuracy by 19.5-60.7% over baselines

## Executive Summary
This paper addresses the challenge of compositional accuracy in text-to-image diffusion models, particularly for prompts involving numerical quantities and spatial relations. The authors propose leveraging reliable random seeds that consistently produce accurate object counts and placements, then fine-tuning models on images generated using these seeds. By using an off-the-shelf vision-language model (CogVLM2) for automatic curation, the approach avoids manual annotation while achieving significant improvements in compositional accuracy over both pretrained models and inference-time methods.

## Method Summary
The method involves mining reliable seeds using CogVLM2 evaluation, generating training data with these seeds, and fine-tuning the model by adjusting only the query and key projection layers in attention modules. This selective fine-tuning preserves the model's general capabilities while improving compositional accuracy. The approach is evaluated on two models (Stable Diffusion 2.1 and PixArt-α) using the Comp90 dataset, showing substantial improvements in numerical and spatial composition tasks compared to baselines and inference-time methods.

## Key Results
- Stable Diffusion 2.1: 29.3% improvement in numerical composition accuracy, 60.7% in spatial composition
- PixArt-α: 19.5% improvement in numerical composition, 21.1% in spatial composition
- Outperforms inference-time methods (LMD, MultiDiffusion, Ranni) while maintaining better output diversity and image quality
- Reliable seeds identified through automatic curation generalize across different object categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initial random seeds determine object placement patterns that correlate with compositional accuracy
- Mechanism: Different seeds bias the diffusion model's cross-attention maps toward distinct spatial regions, creating "layout templates" that are more or less conducive to accurate object counts and spatial relations
- Core assumption: The initial noise influences the entire sampling trajectory in a consistent way, not just the final image
- Evidence anchors: [abstract] "certain noise patterns are more reliable for compositional prompts than others"; [section 3.1] "each initial seed leads to distinct areas where objects are more likely to be placed"
- Break condition: If seed effects are purely random or if fine-tuning eliminates seed-specific patterns

### Mechanism 2
- Claim: Fine-tuning on self-generated data with reliable seeds generalizes seed-specific compositional behaviors
- Mechanism: By training only Q and K projection layers in attention modules on images from reliable seeds, the model learns to reproduce these favorable spatial arrangements without requiring specific seeds
- Core assumption: Compositional accuracy is primarily determined by attention mechanisms rather than other model parameters
- Evidence anchors: [section 4.3] "we opt for only fine-tuning the query and key projection layers in the attention modules"; [section 5.1.2] "Our fine-tuning strategy effectively addresses these issues and leads to more feasible image arrangements"
- Break condition: If other model parameters are equally or more important for compositional accuracy

### Mechanism 3
- Claim: CogVLM2 provides reliable automatic curation of compositional accuracy without manual annotation
- Mechanism: The vision-language model can accurately assess object counts and spatial relations in generated images, enabling large-scale data mining
- Core assumption: CogVLM2's assessment correlates strongly with human judgment and actual compositional correctness
- Evidence anchors: [section 4.2] "we employ the off-the-shelf vision language model CogVLM2 (Hong et al., 2024) for finding reliable seeds"; [section 5.1.1] "To validate the reliability of GPT-4o for this task, we conducted a human evaluation study"
- Break condition: If CogVLM2 assessments become unreliable for complex scenes or edge cases

## Foundational Learning

- Concept: Diffusion sampling process and cross-attention mechanisms
  - Why needed here: Understanding how seeds influence the sampling trajectory and how attention maps determine object placement
  - Quick check question: How does changing the initial noise vector affect the subsequent denoising steps in a diffusion model?

- Concept: Vision-language model capabilities and limitations
  - Why needed here: CogVLM2 is used for both seed mining and data curation, requiring understanding of its strengths and weaknesses
  - Quick check question: What are the limitations of using vision-language models for counting objects in complex scenes?

- Concept: Fine-tuning strategies and parameter selection
  - Why needed here: The method selectively fine-tunes only attention projection layers to preserve other capabilities
  - Quick check question: Why might fine-tuning all parameters on self-generated data lead to model collapse?

## Architecture Onboarding

- Component map: Stable Diffusion 2.1/PixArt-α → CogVLM2 → Seed Mining → Fine-tuning (Q,K projections only) → Enhanced model
- Critical path: Seed mining → Data generation → Fine-tuning → Evaluation
- Design tradeoffs: Selective fine-tuning preserves general capabilities but may limit compositional improvements; using reliable seeds reduces diversity but improves accuracy
- Failure signatures: Visual artifacts, loss of general image quality, overfitting to specific layouts, reduced output diversity
- First 3 experiments:
  1. Verify seed effects by generating images with different seeds for the same prompt and comparing attention maps
  2. Test CogVLM2 accuracy by comparing its assessments against human judgments on a sample of generated images
  3. Evaluate fine-tuning impact by comparing Q,K-only fine-tuning against full-model fine-tuning on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of reliable seeds generalize across different object categories and prompt contexts?
- Basis in paper: [explicit] The paper demonstrates that reliable seeds identified for numerical composition generalize to unseen object categories, with top-performing seeds maintaining accuracy across different contexts.
- Why unresolved: While the paper shows generalization across categories, it does not systematically test whether the same reliable seeds perform consistently across vastly different prompt contexts (e.g., artistic vs. realistic, complex vs. simple scenes).
- What evidence would resolve it: Systematic testing of reliable seeds across diverse prompt contexts and object categories, with statistical analysis of seed performance consistency.

### Open Question 2
- Question: What is the impact of fine-tuning with reliable seeds on the model's ability to handle novel or rare compositional prompts not seen during training?
- Basis in paper: [inferred] The paper evaluates fine-tuning on self-generated data but does not explicitly test the model's performance on novel compositional prompts outside the training distribution.
- Why unresolved: The paper focuses on improving compositional accuracy within the scope of the training data but does not address how well the fine-tuned model generalizes to entirely new compositional scenarios.
- What evidence would resolve it: Experiments evaluating the fine-tuned model on a diverse set of novel compositional prompts, measuring accuracy and robustness.

### Open Question 3
- Question: How do the proposed methods compare in terms of computational efficiency and scalability to other state-of-the-art approaches for compositional text-to-image generation?
- Basis in paper: [explicit] The paper mentions that sampling with reliable seeds is computationally efficient but does not provide a detailed comparison of computational costs across different methods.
- Why unresolved: While the paper highlights the efficiency of sampling with reliable seeds, it lacks a comprehensive analysis of the computational trade-offs between their approach and other methods like LMD and MultiDiffusion.
- What evidence would resolve it: Detailed benchmarking of computational resources (e.g., GPU time, memory usage) required for each method across various tasks and model sizes.

## Limitations

- Limited generalizability to other model architectures beyond Stable Diffusion 2.1 and PixArt-α
- Potential reliability issues with CogVLM2 for complex scenes and edge cases
- Theoretical mechanism connecting seeds to compositional accuracy remains partially unexplained

## Confidence

- **High Confidence**: The empirical improvements in compositional accuracy (29.3% numerical, 60.7% spatial for Stable Diffusion 2.1) and the superiority over inference-time methods are well-supported by the evaluation results.
- **Medium Confidence**: The claim that selective fine-tuning of Q,K projection layers is sufficient to capture compositional improvements is reasonable but could benefit from more extensive ablation studies.
- **Low Confidence**: The theoretical mechanism explaining why seeds have consistent compositional effects across different prompts remains underdeveloped.

## Next Checks

1. **Cross-model validation**: Test the seed mining and fine-tuning approach on a different architecture (e.g., SDXL or FLUX) to assess generalizability beyond the two models studied.

2. **Prompt template robustness**: Evaluate whether reliable seeds maintain their effectiveness across variations in prompt phrasing and template structure, particularly for prompts not included in the original training data.

3. **Long-term stability assessment**: Conduct extended sampling from the fine-tuned models to verify that the compositional improvements don't degrade over time or lead to emergent artifacts not captured in the initial evaluation.