---
ver: rpa2
title: 'PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks'
arxiv_id: '2402.04284'
source_url: https://arxiv.org/abs/2402.04284
tags:
- temporal
- training
- batch
- uni00000013
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training Memory-based Dynamic
  Graph Neural Networks (MDGNNs) with large temporal batch sizes. MDGNNs leverage
  a memory module to capture long-term temporal dependencies, but training them faces
  the issue of temporal discontinuity when using large batches, which neglects the
  chronological order of events.
---

# PRES: Toward Scalable Memory-Based Dynamic Graph Neural Networks

## Quick Facts
- arXiv ID: 2402.04284
- Source URL: https://arxiv.org/abs/2402.04284
- Reference count: 40
- Primary result: Enables up to 4x larger temporal batch sizes (3.4x speed-up) without sacrificing model generalization performance

## Executive Summary
This paper addresses the challenge of training Memory-based Dynamic Graph Neural Networks (MDGNNs) with large temporal batch sizes. MDGNNs leverage a memory module to capture long-term temporal dependencies, but training them faces the issue of temporal discontinuity when using large batches, which neglects the chronological order of events. The authors propose PRES, an iterative prediction-correction scheme combined with a memory coherence learning objective. PRES mitigates the variance induced by temporal discontinuity and improves the convergence rate of MDGNN training. Experiments demonstrate that PRES enables up to 4x larger temporal batch sizes (3.4x speed-up) without sacrificing model generalization performance on tasks like temporal link prediction and node classification across multiple benchmark datasets.

## Method Summary
The PRES framework addresses temporal discontinuity in MDGNN training through two key components: an iterative prediction-correction scheme and memory coherence smoothing. The prediction-correction component estimates the true memory states affected by temporal discontinuity using a Gaussian Mixture Model (GMM), while the memory coherence objective regularizes training to improve gradient alignment. The method partitions events into temporal batches, processes them in parallel through MESSAGE, MEMORY, and EMBEDDING modules, applies prediction-correction to estimate true memory states, computes loss with memory coherence regularization, and backpropagates to update model parameters.

## Key Results
- Enables 3.4x speed-up in MDGNN training
- Allows up to 4x larger temporal batch sizes without performance degradation
- Achieves competitive performance on temporal link prediction and node classification tasks across five benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
Temporal discontinuity causes information loss when pending events within the same batch are processed in parallel rather than sequentially. When events involving the same node coexist within a batch, batch processing only triggers one memory update instead of the multiple updates that would occur if events were processed chronologically. This loss of temporal granularity reduces the model's ability to capture accurate temporal patterns.

### Mechanism 2
Large temporal batch sizes introduce variance in the gradient estimation due to negative sampling, but this variance is offset by reduced variance from fewer pending events. Theorem 1 shows that while smaller temporal batch sizes reduce pending events, they increase variance in the gradient estimate due to negative sampling. Larger batch sizes reduce this sampling variance but increase pending event effects. The tradeoff favors larger batch sizes for robustness to noise.

### Mechanism 3
Memory coherence measures how well the gradients computed with fresh memory align with gradients computed with past memory from pending events, and improving coherence accelerates convergence. Memory coherence quantifies the alignment between gradients computed with the current memory state versus gradients computed with memory states from pending events. Higher coherence means the memory updates are more consistent, leading to faster convergence as shown in Theorem 2.

## Foundational Learning

- **Temporal Graph Neural Networks (TGNNs)**: Understanding how TGNNs handle time-varying graph structures differently from static GNNs is crucial for grasping why batch processing creates temporal discontinuity. Quick check: How do TGNNs handle time-varying graph structures differently from static GNNs, and why does this make batch processing problematic?

- **Stochastic Gradient Descent (SGD)**: The paper draws parallels between mini-batch SGD and temporal batch processing in MDGNNs. Understanding how mini-batch size affects gradient variance in SGD helps contextualize the theoretical analysis of temporal batch effects. Quick check: What is the relationship between mini-batch size and gradient variance in SGD, and how does this analogy apply to temporal batches in MDGNNs?

- **Memory modules (GRU, LSTM)**: MDGNNs use memory modules to capture temporal dependencies. Understanding how these modules work and why they're effective for sequential data is key to understanding why temporal discontinuity is problematic. Quick check: How do memory modules like GRU or LSTM maintain state across time steps, and why would disrupting this sequential processing affect their performance?

## Architecture Onboarding

- **Component map**: MESSAGE module -> MEMORY module -> EMBEDDING module -> Prediction-correction scheme -> Memory coherence objective
- **Critical path**: 1) Partition events into temporal batches, 2) Process each batch through MESSAGE, MEMORY, and EMBEDDING modules, 3) Apply prediction-correction to estimate true memory states, 4) Compute loss with memory coherence regularization, 5) Backpropagate and update model parameters
- **Design tradeoffs**: Larger batch sizes offer better data parallelism and reduced negative sampling variance but increase temporal discontinuity effects; smaller batch sizes reduce temporal discontinuity but increase negative sampling variance and reduce data parallelism; memory coherence weight (β) higher values improve convergence but may over-regularize; prediction model complexity more complex models may better estimate memory states but increase computational overhead
- **Failure signatures**: Training instability or divergence likely due to incorrect prediction model parameters or overly aggressive memory coherence regularization; poor generalization despite good training performance may indicate overfitting to the memory coherence objective; no speedup with larger batch sizes could mean the prediction-correction scheme isn't effectively mitigating temporal discontinuity; excessive GPU memory usage might suggest inefficient memory state tracking or storage
- **First 3 experiments**: 1) Verify temporal discontinuity effects by training with varying batch sizes (1, 10, 100, 1000) and measuring performance degradation as batch size increases, 2) Test prediction-correction effectiveness by comparing training with and without the prediction-correction scheme at large batch sizes to quantify variance reduction, 3) Validate memory coherence impact by training with different β values (0, 0.01, 0.1, 1.0) to find the optimal balance between convergence speed and final performance

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the impact of different prediction models (beyond GMM) on the effectiveness of the PRES framework in mitigating temporal discontinuity? The paper uses a GMM for the prediction model but does not explore other prediction model architectures.

- **Open Question 2**: How does the choice of the memory coherence regularization strength (β) affect the trade-off between convergence speed and final accuracy in MDGNN training? The paper shows that increasing β leads to faster convergence but can negatively impact accuracy, but does not explore the full range of its impact.

- **Open Question 3**: Can the PRES framework be extended to other dynamic graph neural network architectures beyond MDGNNs? The concepts of temporal discontinuity and memory coherence may be applicable to other dynamic GNN architectures, but the paper does not explore this applicability.

## Limitations
- Theoretical claims about gradient variance and convergence rates rely on assumptions about the smoothness of memory update functions that may not hold for all MDGNN architectures
- Results are primarily demonstrated on academic benchmark datasets with limited validation on industrial-scale graphs where temporal patterns may differ substantially
- The prediction-correction scheme's computational overhead, while claimed to be minimal, could become significant at extreme scales

## Confidence

- **High Confidence**: The empirical demonstration that PRES enables 3.4x speed-up with up to 4x larger batch sizes on multiple benchmark datasets
- **Medium Confidence**: The theoretical analysis of gradient variance tradeoffs and the proposed memory coherence metric as a measure of temporal discontinuity effects
- **Low Confidence**: The generalization of results to industrial-scale graphs with different temporal characteristics and event distributions

## Next Checks
1. **Architectural Robustness Test**: Evaluate PRES across diverse MDGNN architectures beyond the three models tested (TGN, JODIE, APAN) to verify the generality of the convergence guarantees
2. **Industrial Scale Validation**: Apply PRES to real-world industrial graphs with varying temporal characteristics (bursty vs. steady event rates) to assess performance in production scenarios
3. **Overhead Quantification**: Systematically measure the computational and memory overhead introduced by the prediction-correction scheme and memory coherence regularization across different graph sizes and densities