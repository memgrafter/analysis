---
ver: rpa2
title: 'DeDe: Detecting Backdoor Samples for SSL Encoders via Decoders'
arxiv_id: '2411.16154'
source_url: https://arxiv.org/abs/2411.16154
tags:
- backdoor
- attacks
- dataset
- learning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeDe addresses the problem of detecting backdoor samples in SSL
  encoders, which can maliciously map triggered inputs to target embeddings. The method
  trains a decoder to invert the encoder mapping, using global and local embeddings
  to reconstruct images.
---

# DeDe: Detecting Backdoor Samples for SSL Encoders via Decoders

## Quick Facts
- **arXiv ID**: 2411.16154
- **Source URL**: https://arxiv.org/abs/2411.16154
- **Authors**: Sizai Hou; Songze Li; Duanyi Yao
- **Reference count**: 40
- **Primary result**: DeDe achieves high upstream detection accuracy (AUC scores >0.9) and effectively mitigates backdoor attacks in downstream tasks without requiring clean datasets or trigger knowledge.

## Executive Summary
DeDe addresses the critical problem of detecting backdoor samples in self-supervised learning (SSL) encoders, which can maliciously map triggered inputs to target embeddings. The method trains a decoder to invert the encoder mapping, using global and local embeddings to reconstruct images. For triggered inputs, the decoder produces outputs significantly different from the input, enabling detection via reconstruction error. Evaluated on contrastive learning and CLIP models, DeDe achieves high upstream detection accuracy (AUC scores >0.9) and effectively mitigates backdoor attacks in downstream tasks, outperforming state-of-the-art methods like DECREE and ASSET. Notably, DeDe maintains consistent performance across various attack types, including stealthy backdoors, without requiring clean datasets or trigger knowledge.

## Method Summary
DeDe trains a decoder for any given SSL encoder using an auxiliary dataset, which can be out-of-distribution or slightly poisoned (poison rate <1%). The method leverages the discrepancy between the input and the decoded output to identify potential backdoor misbehavior during inference. DeDe divides the reconstruction dependency into two parts: the global embedding from the victim encoder and the local patch embeddings from masked patches of the input image. By training a lightweight MAE decoder to reconstruct images from both global and local embeddings, DeDe can identify backdoored samples through reconstruction error analysis, even when backdoor attacks are designed to be stealthy or use invisible triggers.

## Key Results
- Achieves high upstream detection accuracy with AUC scores consistently above 0.9 across multiple attack types
- Effectively mitigates backdoor attacks in downstream tasks by filtering detected backdoor samples
- Outperforms state-of-the-art methods like DECREE and ASSET while requiring no clean datasets or trigger knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backdoor behavior in SSL encoders creates a distinct mapping pattern where triggered inputs are mapped to target embeddings that differ significantly from the normal intra-class clustering.
- Mechanism: When an encoder is backdoored, triggered inputs are mapped to embeddings that belong to a different class region in the embedding space. This creates a cross-class mapping that deviates from the normal intra-class mapping where similar images converge to the same embedding cluster. The decoder trained by DeDe attempts to invert this mapping, and for backdoored inputs, the reconstruction error is large because the decoder expects the embedding to correspond to the target class's visual features rather than the actual input features.
- Core assumption: Backdoor mappings introduce significant reconstruction errors that can be distinguished from normal mappings, and these errors are consistent enough to serve as detection criteria.
- Evidence anchors:
  - [abstract]: "DeDe trains a decoder for any given SSL encoder using an auxiliary dataset... so that for any triggered input that misleads the encoder into the target embedding, the decoder generates an output image significantly different from the input."
  - [section]: "DeDe aims to establish a region-to-region mapping to invert the normal mapping of an encoder, thereby enabling the identification of malicious behaviors that deviate from the normal mapping."
  - [corpus]: Weak - The corpus mentions related backdoor detection papers but doesn't specifically discuss the reconstruction error mechanism used by DeDe.

### Mechanism 2
- Claim: The combination of global and local embeddings in the reconstruction process allows DeDe to capture sufficient information to detect backdoor behavior even when backdoor embeddings are designed to be stealthy.
- Mechanism: DeDe leverages both global embeddings (from the victim encoder) and local embeddings (from masked patches of the input image) to train the decoder. The local embeddings provide texture and color information that helps constrain the reconstruction, while the global embeddings capture the overall semantic content. For backdoored inputs, the global embedding points to a different semantic region, creating a mismatch that results in high reconstruction error when combined with the local patch information from the original input.
- Core assumption: The combination of global and local information is sufficient to reconstruct normal inputs accurately while producing distinct errors for backdoored inputs.
- Evidence anchors:
  - [abstract]: "DeDe leverages the discrepancy between the input and the decoded output to identify potential backdoor misbehavior during inference."
  - [section]: "DeDe divides the reconstruction dependency into two parts, one is the global embedding and the other is the local patch embeddings."
  - [corpus]: Weak - The corpus doesn't discuss the specific mechanism of combining global and local embeddings for backdoor detection.

### Mechanism 3
- Claim: DeDe's effectiveness doesn't depend on knowing the trigger pattern or having clean training data, making it robust against various attack scenarios.
- Mechanism: By training the decoder on an auxiliary dataset that can be out-of-distribution or even slightly poisoned, DeDe learns the normal mapping relationship between embeddings and images without requiring knowledge of the backdoor trigger. The method works by detecting anomalies in the reconstruction process rather than trying to identify specific trigger patterns, which makes it effective against both visible and invisible triggers.
- Core assumption: The normal embedding-to-image mapping relationship can be learned from auxiliary data regardless of its distribution relative to the training data, and this learned relationship is sufficiently different from backdoor mappings to enable detection.
- Evidence anchors:
  - [abstract]: "DeDe maintains consistent performance across various attack types, including stealthy backdoors, without requiring clean datasets or trigger knowledge."
  - [section]: "DeDe trains a decoder for the SSL encoder on an auxiliary dataset (can be out-of-distribution or even slightly poisoned)."
  - [corpus]: Weak - The corpus mentions backdoor detection papers but doesn't specifically address the aspect of not requiring clean data or trigger knowledge.

## Foundational Learning

- Concept: Self-Supervised Learning (SSL) and contrastive learning
  - Why needed here: Understanding how SSL works is crucial because DeDe is specifically designed to detect backdoors in SSL pre-trained encoders. The mechanism relies on the properties of SSL embedding spaces and how backdoor attacks manipulate them.
  - Quick check question: What is the fundamental difference between how SSL encoders and supervised encoders learn representations, and why does this difference make backdoor detection more challenging in SSL?

- Concept: Backdoor attacks in machine learning
  - Why needed here: DeDe is a defense mechanism against backdoor attacks, so understanding how these attacks work (trigger patterns, target embeddings, stealthiness) is essential for grasping why DeDe's approach is effective.
  - Quick check question: How do backdoor attacks in SSL differ from those in supervised learning, and what makes them particularly challenging to defend against?

- Concept: Autoencoders and reconstruction-based detection
  - Why needed here: DeDe uses a decoder trained to reconstruct images from embeddings as its detection mechanism. Understanding how autoencoders work and how reconstruction error can be used for anomaly detection is key to understanding DeDe's approach.
  - Quick check question: How can reconstruction error be used as a detection mechanism for anomalies, and what are the limitations of this approach?

## Architecture Onboarding

- Component map:
  - Victim encoder (f) -> Patch encoder (he) -> Decoder (hd) -> Detection threshold

- Critical path:
  1. Train patch encoder he on masked versions of auxiliary dataset images
  2. Train decoder hd to minimize reconstruction loss using both global embeddings from victim encoder and local embeddings from patch encoder
  3. During inference, compute reconstruction error for each input
  4. Compare reconstruction error to threshold to determine if input is backdoored

- Design tradeoffs:
  - Masking ratio: Higher ratios provide more challenge for reconstruction but may lose too much information; lower ratios make reconstruction easier but may not effectively detect backdoors
  - Auxiliary dataset choice: Out-of-distribution datasets may provide better generalization but could introduce distribution shift issues; in-distribution datasets are more relevant but may be unavailable
  - Threshold selection: Too low causes false positives; too high causes false negatives

- Failure signatures:
  - Consistently high reconstruction errors for both clean and backdoored inputs suggests the decoder hasn't learned the normal mapping well
  - Consistently low reconstruction errors for both clean and backdoored inputs suggests the backdoor is sophisticated enough to maintain normal reconstruction patterns
  - High false positive rate indicates threshold is too low or decoder is too sensitive
  - High false negative rate indicates threshold is too high or decoder isn't sensitive enough to backdoor patterns

- First 3 experiments:
  1. Train DeDe on a clean auxiliary dataset and test on a dataset with known backdoor samples to verify basic functionality
  2. Test DeDe with different masking ratios (e.g., 0.75, 0.9, 0.99) to find optimal balance between reconstruction difficulty and detection accuracy
  3. Evaluate DeDe's performance when the auxiliary dataset is out-of-distribution compared to the training data to assess robustness to dataset choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DeDe perform on more diverse attack types beyond the current evaluation, such as those with adaptive triggers or multiple triggers?
- Basis in paper: [explicit] The paper discusses the effectiveness of DeDe against various attack types, including stealthy backdoors and attacks with imperceptible triggers, but does not explore adaptive triggers or multiple triggers.
- Why unresolved: The evaluation focuses on existing attack types, and the robustness of DeDe against adaptive or multi-trigger scenarios remains untested.
- What evidence would resolve it: Empirical results demonstrating DeDe's performance on datasets with adaptive triggers or multiple triggers, showing its ability to maintain high detection accuracy.

### Open Question 2
- Question: Can DeDe be extended to detect backdoors in other SSL paradigms beyond contrastive learning and CLIP, such as masked autoencoders (MAE)?
- Basis in paper: [explicit] The paper evaluates DeDe on contrastive learning and CLIP models, but does not test its applicability to other SSL paradigms like MAE.
- Why unresolved: The detection mechanism relies on the encoder-decoder relationship, which may vary across different SSL paradigms, and its effectiveness in these settings is not explored.
- What evidence would resolve it: Experimental results showing DeDe's detection accuracy and robustness when applied to MAE and other SSL paradigms, comparing its performance with existing methods.

### Open Question 3
- Question: How does the choice of auxiliary dataset (in-distribution vs. out-of-distribution) affect DeDe's detection performance, and what is the optimal strategy for dataset selection?
- Basis in paper: [explicit] The paper mentions that DeDe can use an auxiliary dataset that is out-of-distribution or slightly poisoned, but does not provide a detailed analysis of how dataset choice impacts performance.
- Why unresolved: The impact of dataset selection on detection accuracy is not quantified, and the trade-offs between in-distribution and out-of-distribution datasets are not explored.
- What evidence would resolve it: Comparative studies showing detection performance metrics (e.g., AUC, TPR, FPR) for different auxiliary dataset choices, along with guidelines for optimal dataset selection based on the characteristics of the target encoder and attack.

## Limitations

- The core reconstruction-based detection mechanism may break down for sophisticated backdoor attacks that maintain reconstruction consistency across both triggered and clean samples
- The method's performance on larger-scale SSL models (e.g., CLIP with ViT-L/14) remains untested, creating uncertainty about scalability
- Reliance on an auxiliary dataset, even if out-of-distribution, introduces potential failure modes if the distribution shift is too large or if the dataset contains significant contamination

## Confidence

- **High confidence**: The detection mechanism's basic premise (that backdoored inputs produce higher reconstruction errors) is well-supported by experimental results across multiple attack types and SSL architectures. The method's effectiveness in improving downstream task performance after filtering detected backdoors is demonstrated consistently.
- **Medium confidence**: Claims about robustness to various attack types, including invisible backdoors, are supported but may not generalize to all possible attack variants. The assumption that auxiliary dataset choice has minimal impact on detection performance needs more rigorous validation.
- **Low confidence**: Claims about performance on extremely large-scale SSL models (beyond ViT-B/16) are extrapolated without direct experimental evidence. The method's behavior in continuous deployment scenarios with evolving attack patterns is not addressed.

## Next Checks

1. **Cross-architecture robustness test**: Evaluate DeDe on a diverse set of SSL architectures including DINO, MAE, and BYOL models to verify detection effectiveness is not limited to contrastive learning frameworks like SimCLR.

2. **Transfer learning evaluation**: Test DeDe's detection performance when the SSL encoder is fine-tuned on downstream tasks, as the encoder's embedding space may shift during fine-tuning and affect reconstruction patterns.

3. **Adaptive attack analysis**: Design and evaluate an adaptive backdoor attack specifically targeting DeDe's reconstruction-based detection mechanism to identify potential vulnerabilities and establish robustness bounds.