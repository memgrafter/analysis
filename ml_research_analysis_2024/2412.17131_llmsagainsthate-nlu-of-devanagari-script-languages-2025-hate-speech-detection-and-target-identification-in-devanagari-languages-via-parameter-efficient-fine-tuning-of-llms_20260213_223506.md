---
ver: rpa2
title: 'LLMsAgainstHate @ NLU of Devanagari Script Languages 2025: Hate Speech Detection
  and Target Identification in Devanagari Languages via Parameter Efficient Fine-Tuning
  of LLMs'
arxiv_id: '2412.17131'
source_url: https://arxiv.org/abs/2412.17131
tags:
- hate
- speech
- detection
- languages
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a parameter-efficient fine-tuning (PEFT) approach
  using Low-Rank Adaptation (LoRA) to detect hate speech and identify its targets
  in Hindi and Nepali texts. The method fine-tunes large language models (LLMs) with
  a small subset of parameters, addressing computational and resource constraints
  while maintaining effectiveness.
---

# LLMsAgainstHate @ NLU of Devanagari Script Languages 2025: Hate Speech Detection and Target Identification in Devanagari Languages via Parameter Efficient Fine-Tuning of LLMs

## Quick Facts
- **arXiv ID:** 2412.17131
- **Source URL:** https://arxiv.org/abs/2412.17131
- **Reference count:** 14
- **Primary result:** Nemo-Instruct-2407 achieved 90.05% F1 for hate speech detection and 71.47% F1 for target identification in Hindi/Nepali texts

## Executive Summary
This paper presents a parameter-efficient fine-tuning (PEFT) approach using Low-Rank Adaptation (LoRA) to detect hate speech and identify its targets in Hindi and Nepali texts. The method fine-tunes large language models (LLMs) with a small subset of parameters, addressing computational and resource constraints while maintaining effectiveness. Experiments on the CHiPSAL dataset show that Nemo-Instruct-2407 achieved the highest performance with 90.05% F1 score for hate speech detection and 71.47% F1 score for target identification. Class imbalance in the dataset resulted in lower F1 scores for underrepresented classes, particularly for community-targeted hate speech.

## Method Summary
The paper proposes a parameter-efficient fine-tuning approach using LoRA to adapt LLMs for hate speech detection and target identification in Devanagari-scripted languages. The method involves inserting low-rank matrices into the attention layers of pretrained models while freezing original weights. Four LLMs were fine-tuned: Nemo-Instruct-2407, Llama-3.1-8B, Qwen2.5-7B-Instruct, and Phi3-medium-4k-Instruct. Models were quantized to 4-bit precision to reduce memory consumption and trained on 16GB NVIDIA T4 GPU for 2-4 epochs. The approach addresses computational constraints while maintaining performance comparable to full fine-tuning.

## Key Results
- Nemo-Instruct-2407 achieved highest performance with 90.05% F1 for hate speech detection
- Nemo-Instruct-2407 achieved 71.47% F1 for target identification (individual/organizational/community)
- Nemo-Instruct-2407 outperformed larger models like Llama-3.1-8B despite having fewer parameters
- Class imbalance resulted in lower F1 scores for underrepresented classes (69.07% for hate speech, 62.84% for community targets)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA significantly reduces trainable parameters while maintaining detection performance comparable to full fine-tuning
- Mechanism: LoRA decomposes weight update matrix into two low-rank matrices (A and B) inserted into attention layers, freezing original model weights
- Core assumption: Weight update for fine-tuning can be approximated by low-rank matrix without significant performance loss
- Evidence anchors:
  - [abstract] "Parameter Efficient Fine-Tuning (PEFT) has emerged as a more adaptable and cost-effective solution"
  - [section] "LoRA reduces the number of trainable parameters by decomposing weight updates into low-rank matrices"
  - [corpus] Weak evidence - related papers focus on multilingual hate speech detection but don't discuss LoRA's parameter efficiency

### Mechanism 2
- Claim: LoRA allows efficient fine-tuning without increasing inference latency
- Mechanism: After training, LoRA weight update (AB^T) is merged with original model weights, keeping total parameters unchanged
- Core assumption: Computational overhead of low-rank matrices during training is offset by merging before inference
- Evidence anchors:
  - [abstract] "Parameter Efficient Fine-Tuning (PEFT) has emerged as a more adaptable and cost-effective solution"
  - [section] "LoRA does not add to inference latency, as after training, weight update AB^T is added to model weights"
  - [corpus] Weak evidence - related papers discuss multilingual detection but don't analyze inference efficiency

### Mechanism 3
- Claim: Nemo-Instruct-2407 achieves highest performance despite fewer parameters than competing models
- Mechanism: Model's architecture and training methodology provide better generalization for Devanagari-scripted languages
- Core assumption: Model architecture and pretraining data quality are more important than raw parameter count for this task
- Evidence anchors:
  - [section] "Nemo performs better than Llama despite having smaller size"
  - [section] "Nemo-Instruct-2407 achieved highest performance with 90.05% F1 score for hate speech detection"
  - [corpus] Weak evidence - related papers discuss multilingual hate speech detection but don't compare model sizes and performance

## Foundational Learning

- Concept: Transformer architecture and multi-head self-attention
  - Why needed here: Understanding how LoRA modifies attention layers requires knowledge of transformer internals
  - Quick check question: How does multi-head self-attention allow transformers to capture both local and global dependencies?

- Concept: Parameter-efficient fine-tuning techniques
  - Why needed here: LoRA is one of several PEFT methods; understanding alternatives helps evaluate its suitability
  - Quick check question: What are the key differences between LoRA, prefix tuning, and adapter-based methods?

- Concept: Class imbalance and its impact on model performance
  - Why needed here: The dataset shows significant class imbalance, which affects model training and evaluation
  - Quick check question: How does class imbalance typically affect F1 scores for minority classes?

## Architecture Onboarding

- Component map:
  Input: Devanagari-scripted text (Hindi/Nepali) -> Model: Pretrained LLM (Nemo-Instruct-2407, Llama-3.1, etc.) -> LoRA adapters: Low-rank matrices in attention layers -> Training pipeline: Quantization (4-bit), Unsloth acceleration -> Output: Hate speech classification and target identification

- Critical path:
  1. Load pretrained model with quantization
  2. Insert LoRA adapters into attention layers
  3. Train with task-specific dataset
  4. Merge LoRA weights with base model
  5. Evaluate on test set

- Design tradeoffs:
  - LoRA rank vs. performance: Higher rank captures more complex patterns but increases computational cost
  - Quantization level: 4-bit reduces memory usage but may impact numerical precision
  - Training epochs: More epochs improve performance but risk overfitting on imbalanced data

- Failure signatures:
  - Poor performance on minority classes: Indicates class imbalance issues
  - High variance across runs: Suggests instability in LoRA training
  - Memory errors: Model/gradient checkpointing may be needed for larger models

- First 3 experiments:
  1. Baseline: Evaluate zero-shot performance of pretrained models on the dataset
  2. LoRA ablation: Test different rank values (8, 16, 32) to find optimal tradeoff
  3. Class balancing: Apply weighted loss or oversampling to address class imbalance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LoRA compare to other parameter-efficient fine-tuning methods (like prefix tuning or prompt tuning) for hate speech detection in low-resource languages?
- Basis in paper: [explicit] The paper mentions that LoRA was chosen for fine-tuning but does not compare its performance against other PEFT methods
- Why unresolved: The paper only evaluates LoRA without benchmarking against alternative PEFT techniques
- What evidence would resolve it: Comparative experiments testing LoRA against other PEFT methods (e.g., prefix tuning, prompt tuning) on the same datasets with identical model sizes and hyperparameters

### Open Question 2
- Question: Can synthetic data generation effectively address the class imbalance issues observed in hate speech detection and target identification tasks?
- Basis in paper: [inferred] The authors note that class imbalance affects performance, particularly for underrepresented classes like community-targeted hate speech, and suggest future work on data generation techniques
- Why unresolved: While the authors identify class imbalance as a limitation, they do not empirically test whether synthetic data generation can mitigate this issue
- What evidence would resolve it: Experiments showing performance improvements on imbalanced classes after applying synthetic data generation techniques (e.g., data augmentation, back-translation, or generative models)

### Open Question 3
- Question: How do the fine-tuned models generalize to languages outside the Devanagari script family?
- Basis in paper: [explicit] The authors state their system "can be potentially applied to other languages as well" but do not test this claim
- Why unresolved: The paper focuses exclusively on Hindi and Nepali without evaluating cross-linguistic generalization to non-Devanagari languages
- What evidence would resolve it: Performance evaluation of the fine-tuned models on hate speech detection tasks in non-Devanagari languages (e.g., Tamil, Telugu, or European languages) with comparable datasets

## Limitations
- Lack of detailed hyperparameter specifications beyond LoRA rank and alpha values
- No ablation studies on different LoRA ranks or comparison with other PEFT methods
- Class imbalance not fully addressed through techniques like weighted loss or oversampling

## Confidence
**High Confidence Claims:**
- Nemo-Instruct-2407 achieved 90.05% F1 for hate speech detection and 71.47% F1 for target identification
- LoRA reduces trainable parameters while maintaining performance
- Nemo-Instruct-2407 outperforms larger models like Llama-3.1-8B

**Medium Confidence Claims:**
- LoRA does not increase inference latency after weight merging
- 4-bit quantization effectively reduces memory consumption without significant performance degradation
- Class imbalance explains lower F1 scores for minority classes

**Low Confidence Claims:**
- The specific mechanisms by which Nemo-Instruct-2407 generalizes better for Devanagari languages
- The optimal LoRA rank value for this specific task
- The long-term stability of LoRA-adapted models on out-of-distribution data

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Conduct experiments varying LoRA rank (8, 16, 32) and learning rates to establish the sensitivity of performance to these critical parameters and identify optimal configurations.

2. **Class Imbalance Mitigation**: Implement weighted loss functions or oversampling techniques to evaluate whether minority class performance can be improved beyond the reported F1 scores of 69.07% (hate speech) and 62.84% (community targets).

3. **Cross-Lingual Generalization Test**: Evaluate the fine-tuned models on a held-out test set containing mixed Hindi-Nepali texts or out-of-domain examples to assess the generalization capabilities claimed for Nemo-Instruct-2407.