---
ver: rpa2
title: 'AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient
  Large Language Models Fine-Tuning'
arxiv_id: '2406.18060'
source_url: https://arxiv.org/abs/2406.18060
tags:
- fine-tuning
- adazeta
- methods
- gradient
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces AdaZeta, an adaptive zeroth-order fine-tuning\
  \ framework that improves the performance and convergence of LLM fine-tuning while\
  \ reducing memory usage. The key innovations are: (1) Fast-forward tensorized adapters\
  \ that compress adapter weights using Tensor-Train decomposition, reducing trainable\
  \ parameters by over 80\xD7 compared to LoRA while maintaining high representational\
  \ ability, and (2) An adaptive query number schedule that sublinearly increases\
  \ the number of gradient queries per epoch to reduce variance and prevent divergence\
  \ in large-scale fine-tuning."
---

# AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning

## Quick Facts
- **arXiv ID**: 2406.18060
- **Source URL**: https://arxiv.org/abs/2406.18060
- **Reference count**: 40
- **Key outcome**: AdaZeta achieves better accuracy than existing zeroth-order methods while using 5-8× less memory and can outperform first-order AdamW methods on several tasks.

## Executive Summary
This paper introduces AdaZeta, an adaptive zeroth-order fine-tuning framework for large language models that combines tensorized adapters with an adaptive query number schedule. The method reduces memory usage by over 80× compared to LoRA while maintaining high representational ability, and achieves faster convergence by sublinearly increasing the number of gradient queries per epoch to reduce variance. Extensive experiments on Roberta-Large and Llama-2-7B demonstrate that AdaZeta outperforms existing zeroth-order methods and can even surpass first-order optimization baselines on certain tasks.

## Method Summary
AdaZeta uses Tensor-Train decomposition to compress adapter weights into a sequence of small tensor factors, dramatically reducing trainable parameters while maintaining representational capacity. The framework implements an adaptive query number schedule that increases the number of gradient queries per epoch sublinearly (Qk = min(αeβk, Qmax)) to reduce variance in zeroth-order gradient estimation. Fast-forward tensorized adapters accelerate inference through parallel contraction of tensor factors instead of sequential processing. The method eliminates backpropagation by estimating gradients through forward passes with perturbations, making it highly memory-efficient for large-scale LLM fine-tuning.

## Key Results
- Achieves 5-8× memory reduction compared to first-order methods
- Reaches same evaluation loss 6× faster than MeZO-LoRA on Llama-2-7B
- Outperforms first-order AdamW methods on several NLU tasks
- Reduces trainable parameters by over 80× compared to LoRA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Tensorized adapters reduce trainable parameters while maintaining representational ability
- **Mechanism**: Tensor-Train (TT) decomposition compresses weight matrices into a sequence of small tensor factors, reducing parameter count by over 80× compared to LoRA
- **Core assumption**: The TT format can maintain sufficient representational capacity while dramatically reducing parameters
- **Evidence anchors**:
  - [abstract] "compress adapter weights using Tensor-Train decomposition, reducing trainable parameters by over 80× compared to LoRA while maintaining high representational ability"
  - [section 2.2] "The tensorized adapters reduce trainable parameters by over 80×, making them a better fit for ZO fine-tuning"
- **Break condition**: If the tensor ranks become too low, the compressed representation loses necessary capacity to capture task-specific patterns

### Mechanism 2
- **Claim**: Adaptive query number schedule prevents divergence in large-scale ZO fine-tuning
- **Mechanism**: Sublinearly increasing the number of gradient queries per epoch reduces variance in ZO gradient estimation, with Qk = min(αeβk, Qmax)
- **Core assumption**: Variance in ZO gradient estimation is the primary cause of divergence in large-scale fine-tuning
- **Evidence anchors**:
  - [abstract] "adaptive query number schedule that sublinearly increases the number of gradient queries per epoch to reduce variance and prevent divergence"
  - [section 3.2] "we consider a simple but effective sublinear increasing query number adjustment schedule"
- **Break condition**: If the learning rate is too high or the model is too sensitive, even reduced variance may not prevent divergence

### Mechanism 3
- **Claim**: Fast-forward tensorized adapters accelerate inference by enabling parallel contraction
- **Mechanism**: Instead of sequential contraction of tensor factors, the method groups factors to enable parallel processing, avoiding high-dimensional tensors
- **Core assumption**: Sequential contraction is the bottleneck in tensorized adapter inference speed
- **Evidence anchors**:
  - [section 2.2] "we propose a new parallel contraction method to speed up the forward passes"
  - [section 3.2] "This method divides the sequence of tensor factors into several groups to enable parallel processing"
- **Break condition**: If the tensor structure doesn't allow effective grouping, parallel contraction gains may be minimal

## Foundational Learning

- **Concept: Zeroth-order optimization**
  - **Why needed here**: AdaZeta eliminates backpropagation by estimating gradients through forward passes with perturbations
  - **Quick check question**: How does zeroth-order gradient estimation differ from first-order methods in terms of memory usage and gradient accuracy?

- **Concept: Tensor-Train decomposition**
  - **Why needed here**: The TT format compresses high-dimensional weight matrices into sequences of lower-rank tensors, enabling parameter-efficient fine-tuning
  - **Quick check question**: What is the relationship between tensor rank and the compression ratio in TT decomposition?

- **Concept: Variance reduction in stochastic optimization**
  - **Why needed here**: Both the adaptive query schedule and tensorized adapters work by reducing variance in gradient estimates to improve convergence
  - **Quick check question**: How does increasing the number of queries affect the variance of ZO gradient estimates?

## Architecture Onboarding

- **Component map**: Forward pass → perturbation → loss evaluation → gradient estimation → parameter update
- **Critical path**: Forward pass → perturbation → loss evaluation → gradient estimation → parameter update
- **Design tradeoffs**: Memory vs. accuracy (fewer parameters = less memory but potential capacity loss); query count vs. speed (more queries = better estimates but slower training)
- **Failure signatures**: Training divergence (insufficient variance reduction), poor accuracy (inadequate representational capacity), slow convergence (suboptimal query scheduling)
- **First 3 experiments**:
  1. Implement tensorized adapter forward pass with parallel contraction and benchmark vs sequential
  2. Test adaptive query scheduling on small model with fixed learning rate to verify variance reduction
  3. Combine both components on medium-sized model to verify end-to-end improvement over MeZO-LoRA

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the adaptive query number schedule interact with different learning rate schedules in AdaZeta?
- **Basis in paper**: [inferred] The paper discusses the adaptive query schedule but doesn't explore interactions with learning rate scheduling.
- **Why unresolved**: The theoretical analysis focuses on the query schedule's impact, but empirical results don't vary learning rate schedules to test their interaction.
- **What evidence would resolve it**: Experiments comparing AdaZeta with different learning rate schedules (cosine decay, step decay, etc.) would show how query adaptation interacts with other hyperparameters.

### Open Question 2
- **Question**: What is the optimal tensor rank and shape for tensorized adapters across different model architectures and tasks?
- **Basis in paper**: [explicit] The paper mentions that "the optimal factors shape and tensor rank for the tensor-train method can only be determined by the experiments' trail" and references adaptive rank exploration.
- **Why unresolved**: The experiments use fixed tensor ranks (5, 8, 16) without systematic exploration of the design space or comparison to adaptive methods.
- **What evidence would resolve it**: A comprehensive ablation study varying tensor ranks and shapes across multiple model architectures and tasks would identify optimal configurations.

### Open Question 3
- **Question**: How does AdaZeta scale to much larger models (e.g., Llama-2-70B or beyond) and what are the memory/time bottlenecks at extreme scales?
- **Basis in paper**: [inferred] The experiments only go up to Llama-2-7B, and the paper mentions distributed optimization as future work.
- **Why unresolved**: The theoretical analysis assumes a fixed model dimension d, but doesn't address how the algorithm's complexity scales with increasingly large models.
- **What evidence would resolve it**: Scaling experiments with models larger than 7B parameters, measuring memory usage, training time, and performance degradation would reveal practical limitations.

### Open Question 4
- **Question**: Can the adaptive query schedule be combined with other variance reduction techniques (e.g., momentum, SVRG) to further improve convergence?
- **Basis in paper**: [explicit] The paper mentions that "applying the adaptive query schedule to other PEFT methods may yield significantly better performance" and discusses MeZO-SVRG as a related approach.
- **Why unresolved**: The paper only implements the adaptive query schedule with tensorized adapters, not exploring combinations with other variance reduction methods.
- **What evidence would resolve it**: Experiments implementing the adaptive query schedule with other PEFT methods (LoRA, prefix-tuning) and combining it with momentum-based techniques would show potential performance gains.

## Limitations
- The effectiveness of the adaptive query schedule on extremely large models (>7B parameters) remains unproven
- Optimal tensor rank selection requires task-specific tuning that isn't fully automated
- The theoretical convergence guarantees may not fully capture practical training dynamics and hyperparameter interactions

## Confidence
**High Confidence**: Memory efficiency claims (5-8× reduction) are well-supported by parameter reduction calculations and consistent across multiple experiments. Convergence speed improvements (6× faster on Llama-2-7B) are demonstrated through clear empirical evidence.

**Medium Confidence**: Accuracy claims relative to first-order methods need more scrutiny. While AdaZeta outperforms AdamW on several tasks, comparison conditions and hyperparameter tuning for baseline methods aren't fully detailed.

**Low Confidence**: Claims about preventing divergence in large-scale fine-tuning through variance reduction are theoretical rather than empirically proven across diverse model sizes and tasks.

## Next Checks
1. **Scalability testing**: Evaluate AdaZeta on models larger than Llama-2-7B (e.g., 13B or 70B parameters) to verify that the adaptive query schedule continues to prevent divergence and that memory savings scale proportionally.

2. **Hyperparameter sensitivity analysis**: Systematically vary tensor ranks, learning rates, and query scheduling parameters to determine the robustness of AdaZeta's performance and identify optimal configurations for different task types.

3. **Cross-task generalization**: Test AdaZeta across a broader range of NLP tasks including long-context scenarios and multilingual datasets to validate whether the observed improvements generalize beyond the reported NLU benchmarks.