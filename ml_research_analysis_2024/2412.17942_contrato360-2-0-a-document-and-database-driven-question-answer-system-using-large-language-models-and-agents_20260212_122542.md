---
ver: rpa2
title: 'Contrato360 2.0: A Document and Database-Driven Question-Answer System using
  Large Language Models and Agents'
arxiv_id: '2412.17942'
source_url: https://arxiv.org/abs/2412.17942
tags:
- contract
- data
- information
- language
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Contrato360 2.0, a Q&A system for contract
  management that integrates information from contract PDFs and Contract Management
  Systems (CMS) using Large Language Models (LLMs) and agents. The system employs
  Retrieval-Augmented Generation (RAG) with metadata filtering, a context-aware text-to-SQL
  agent for database queries, and Prompt Engineering for response standardization.
---

# Contrato360 2.0: A Document and Database-Driven Question-Answer System using Large Language Models and Agents

## Quick Facts
- arXiv ID: 2412.17942
- Source URL: https://arxiv.org/abs/2412.17942
- Reference count: 4
- System provides correct answers for all 10 direct questions and 9 out of 10 indirect questions in evaluation by IT contract specialists

## Executive Summary
Contrato360 2.0 is an advanced Q&A system for contract management that integrates information from contract PDFs and Contract Management Systems (CMS) using Large Language Models (LLMs) and agents. The system employs Retrieval-Augmented Generation (RAG) with metadata filtering, a context-aware text-to-SQL agent for database queries, and Prompt Engineering for response standardization. Agents orchestrate the workflow by dynamically routing queries to the appropriate processing modules. Evaluation with two IT contract specialists demonstrated significant improvements in response relevance and accuracy compared to previous approaches, providing correct answers for all 10 direct questions and 9 out of 10 indirect questions.

## Method Summary
The system uses a multi-agent architecture with RAG and text-to-SQL capabilities. PDF contracts are converted into chunks with metadata, then embedded using OpenAI's text-davinci-002 model and stored in a ChromaDb vector store. A RAG agent retrieves semantically relevant contract sections using metadata filtering, while a text-to-SQL agent queries the CMS database using LangChain. Router agents dynamically direct queries to the appropriate processing module based on query type. The system processes user queries through this orchestrated workflow, combining structured database information with unstructured contract text to provide comprehensive answers.

## Key Results
- Achieved 100% accuracy on 10 direct questions about contract information
- Achieved 90% accuracy on 10 indirect questions requiring reasoning across multiple sources
- Demonstrated significant improvements in response relevance and accuracy compared to previous approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG with metadata filtering improves retrieval accuracy for structured documents like contracts
- Mechanism: By adding document metadata (source, contract number, clause) as filters during similarity search, the system retrieves semantically correct chunks rather than just similar text
- Core assumption: Contract documents have standardized structures where metadata reliably identifies relevant sections
- Evidence anchors: "To overcome this issue, it is necessary to add semantics to the chunks, by including document metadata. And when accessing the vectorstore, use this metadata to filter the information returned."

### Mechanism 2
- Claim: Context-aware text-to-SQL agent enables precise database queries for contract management data
- Mechanism: The agent performs entity recognition, maps entities to database schema, validates SQL safety, and executes queries to retrieve structured contract data
- Core assumption: Contract management systems have consistent database schemas that can be mapped to natural language queries
- Evidence anchors: "The LangChain SQL Agent (Langchain, 2024) has proven to be a highly flexible tool for interacting with the CMS database"

### Mechanism 3
- Claim: Multi-agent orchestration dynamically routes queries to appropriate processing modules
- Mechanism: Router agent analyzes queries and directs them to RAG, SQL, or other specialized agents based on query type and context
- Core assumption: Query types can be reliably classified and routed to appropriate processing modules
- Evidence anchors: "In Contrato360, Agents play a pivotal role in orchestrating the flow of execution"

## Foundational Learning

- **Vector embeddings and semantic similarity**: Why needed - Understanding how text is converted to vectors for similarity search in RAG systems. Quick check - How does cosine similarity measure the relationship between two document vectors?
- **Text-to-SQL translation and entity recognition**: Why needed - Required to understand how natural language queries are converted to database queries. Quick check - What is the difference between syntactic and semantic parsing in text-to-SQL systems?
- **Prompt engineering and context windows**: Why needed - Essential for understanding how instructions are provided to LLMs for consistent responses. Quick check - How does context window size affect the performance of LLM-based question answering?

## Architecture Onboarding

- **Component map**: User Interface → Router Agent → (RAG Agent/SQL Agent) → LLM → Response
- **Critical path**: Query → Router → RAG/SQL → Vectorstore/Database → LLM → Response
- **Design tradeoffs**: Accuracy vs. response time (more complex routing increases accuracy but adds latency)
- **Failure signatures**: Incorrect answers may stem from poor chunking, metadata errors, or agent routing mistakes
- **First 3 experiments**:
  1. Test RAG retrieval accuracy with and without metadata filtering using controlled contract queries
  2. Validate text-to-SQL accuracy by comparing generated SQL against expected results
  3. Test agent routing by creating queries that should be directed to different specialized agents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the chunking strategy based on contract sections affect the relevance of retrieved information compared to other chunking strategies (e.g., token-based chunking)?
- Basis in paper: The paper discusses using contract sections as chunking limits and mentions this improves relevance within single contracts
- Why unresolved: The paper doesn't provide comparative analysis between section-based chunking and other strategies
- What evidence would resolve it: A controlled experiment comparing retrieval accuracy using different chunking strategies on the same set of contracts

### Open Question 2
- Question: What specific adjustments to queries and/or prompts would be needed to improve semantic understanding of complex concepts like "Waiver of Bidding"?
- Basis in paper: The paper mentions that the concept of "Waiver of Bidding" was not well captured in evaluation, suggesting room for improvement
- Why unresolved: The paper doesn't provide details on what specific prompt engineering or query modifications would address this limitation

## Limitations

- Limited evaluation sample size with only two IT contract specialists testing 20 questions total
- System performance with non-technical users or different contract domains remains unestablished
- Dependency on consistent metadata extraction from PDF contracts creates vulnerabilities with poorly formatted documents

## Confidence

- **High confidence** in the RAG with metadata filtering mechanism: Supported by clear technical descriptions and established literature on semantic search improvements
- **Medium confidence** in the text-to-SQL agent's accuracy: While the approach is well-documented, performance may vary significantly based on database schema complexity
- **Medium confidence** in the multi-agent orchestration: The concept is sound, but real-world performance with edge cases and hybrid queries requires further validation

## Next Checks

1. Conduct a larger-scale evaluation with diverse user types (non-technical users, legal professionals) and contract domains to assess generalizability
2. Test system performance with contracts containing inconsistent metadata or varying structural formats to identify RAG system robustness limits
3. Evaluate text-to-SQL agent performance across different CMS database schemas and complex query types to establish reliability boundaries