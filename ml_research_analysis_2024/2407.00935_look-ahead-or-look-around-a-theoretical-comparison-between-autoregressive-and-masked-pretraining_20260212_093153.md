---
ver: rpa2
title: Look Ahead or Look Around? A Theoretical Comparison Between Autoregressive
  and Masked Pretraining
arxiv_id: '2407.00935'
source_url: https://arxiv.org/abs/2407.00935
tags:
- masked
- autoregressive
- tasks
- objective
- theoretical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first theoretical comparison between autoregressive
  and masked self-supervised learning (SSL) objectives, focusing on their performance
  in classification and content generation tasks. Through establishing a unified framework
  connecting SSL objectives to matrix decomposition, the authors theoretically demonstrate
  that masked SSL achieves superior downstream classification performance due to its
  ability to create more inter-sample connections, while autoregressive SSL shows
  better generation performance due to alignment between pretraining and downstream
  objectives.
---

# Look Ahead or Look Around? A Theoretical Comparison Between Autoregressive and Masked Pretraining

## Quick Facts
- **arXiv ID**: 2407.00935
- **Source URL**: https://arxiv.org/abs/2407.00935
- **Reference count**: 40
- **Primary result**: First theoretical comparison showing masked SSL achieves superior classification performance while autoregressive SSL excels at content generation, with proposed objectives improving both

## Executive Summary
This paper provides the first theoretical comparison between autoregressive and masked self-supervised learning objectives, establishing a unified framework that connects SSL objectives to matrix decomposition. The authors theoretically demonstrate that masked SSL creates more inter-sample connections, leading to superior downstream classification performance, while autoregressive SSL shows better generation performance due to alignment between pretraining and downstream objectives. Based on these insights, they propose diversity-enhanced autoregressive and variable-length masked objectives that significantly improve the respective weaknesses of each approach.

## Method Summary
The authors establish a unified theoretical framework connecting SSL objectives to matrix decomposition, specifically analyzing the co-occurrence matrices formed during pretraining. They compare standard autoregressive (GPT-style) and masked (BERT-style) SSL objectives on text data, then propose two new objectives: diversity-enhanced autoregressive SSL (which predicts multiple subsequent tokens) and variable-length masked SSL (which uses random mask lengths). Models are pretrained on the Pile dataset using Transformer architecture with 16 layers and hidden size 768, then evaluated on GLUE benchmark for classification and perplexity on WikiText-2 for generation tasks.

## Key Results
- Masked SSL achieves superior downstream classification performance due to more inter-sample connections
- Autoregressive SSL shows better generation performance, especially for short prompts
- Diversity-enhanced autoregressive objective improves classification accuracy by ~3%
- Variable-length masked objective improves generation quality by ~30%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Masked SSL fosters more inter-sample connections in classification tasks due to flexible target token positions, leading to stronger graph connectivity and superior downstream classification performance.
- **Mechanism**: Randomly masking different tokens across samples creates a bipartite graph where samples connect through shared unmasked tokens, resulting in a more connected graph structure with smaller singular values.
- **Core assumption**: Co-occurrence matrix as adjacency matrix of bipartite graph, with smaller singular values indicating stronger connectivity.
- **Evidence anchors**:
  - [abstract]: "randomly selected target tokens in masked SSL could lead to more connections between different semantically similar samples, leading to superior downstream classification performance compared with autoregressive SSL."
  - [section 4.1.2]: "Theorem 4.2, the singular values of the masked SSL co-occurrence matrix are smaller than that of the autoregressive SSL, which verifies the intuition that the masked SSL can foster more connections by the multiple-place predictions."

### Mechanism 2
- **Claim**: Masked SSL underperforms in content generation for short texts due to misalignment between fixed-length unmasked texts during pretraining and variable-length test samples.
- **Mechanism**: During pretraining, masked SSL always uses fixed-length context to predict masked tokens. When generating from short prompts, the model lacks the full context it was trained on, leading to poor predictions.
- **Core assumption**: Pretraining length should match downstream generation length for optimal performance.
- **Evidence anchors**:
  - [abstract]: "the misalignment between the flexible lengths of test samples and the fixed length of unmasked texts in masked SSL (vs. flexible lengths of conditional texts in autoregressive SSL) hinders its generation performance."
  - [section 4.2]: "When predicting the masked words in the pretraining process, the lengths of inputs are fixed... Consequently, the masked SSL model may struggle to accurately infer the complete texts due to limited information in the downstream generation tasks."

### Mechanism 3
- **Claim**: The proposed diversity-enhanced autoregressive objective improves classification performance by introducing more diverse prediction targets, strengthening inter-sample connections.
- **Mechanism**: By predicting multiple subsequent tokens instead of just the next one, the model creates more opportunities for different samples to connect through shared prediction targets, enhancing overall graph connectivity.
- **Core assumption**: Increasing diversity of prediction targets in autoregressive SSL will lead to a more connected co-occurrence matrix.
- **Evidence anchors**:
  - [abstract]: "we propose diversity-enhanced autoregressive and variable-length masked objectives, which substantially improve the classification performance of autoregressive SSL and the generation performance of masked SSL."
  - [section 4.1.2]: "Naturally, we propose the following diversity-enhanced autoregressive SSL objective: Ldar,t(Θ) = −Exi Xk log P (xi,[k+1,k+t]|xi,1, · · · , xi,k; Θ), where xi,[k+1,k+t] is a token randomly selected from {xi,k+1, · · · , xi,k+t}, t ≥ 1."

## Foundational Learning

- **Concept**: Singular Value Decomposition (SVD) and its relationship to graph connectivity
  - **Why needed here**: Used to analyze co-occurrence matrix and establish connections between matrix properties and graph connectivity
  - **Quick check question**: Given a co-occurrence matrix A, how do the singular values of A relate to the connectivity of the corresponding bipartite graph?

- **Concept**: Self-supervised learning (SSL) objectives and their impact on representation learning
  - **Why needed here**: Compares two types of SSL objectives and their effects on downstream tasks
  - **Quick check question**: What is the key difference between autoregressive and masked SSL objectives, and how might this difference affect the learned representations?

- **Concept**: Linear evaluation protocol in self-supervised learning
  - **Why needed here**: Evaluates downstream classification performance using a linear classifier on frozen pretrained features
  - **Quick check question**: In the linear evaluation protocol, what is the role of the pretrained encoder, and why is it kept frozen during the downstream evaluation?

## Architecture Onboarding

- **Component map**: Input data -> Encoder (Transformer) -> SSL Objective (autoregressive/masked/diversity-enhanced/variable-length) -> Learned representations -> Downstream tasks (classification/generation)

- **Critical path**:
  1. Pretrain encoder using chosen SSL objective on unlabeled dataset
  2. Extract features from pretrained encoder for downstream samples
  3. For classification: Train linear classifier on top of frozen features
  4. For generation: Use pretrained encoder to generate text based on conditional inputs

- **Design tradeoffs**:
  - Choice of SSL objective impacts downstream performance differently for classification vs. generation
  - Mask ratio in masked SSL affects balance between reconstruction quality and representation learning
  - Model architecture (layers, hidden size) influences capacity to learn complex representations

- **Failure signatures**:
  - Poor classification performance: Insufficient inter-sample connections in learned representations
  - Poor generation for short prompts: Length misalignment between pretraining and downstream inputs
  - Overfitting to pretraining objective: Representations that don't generalize to downstream tasks

- **First 3 experiments**:
  1. Pretrain autoregressive and masked SSL models on small text corpus, evaluate classification performance, compare representations using SVD and analyze graph connectivity
  2. Implement diversity-enhanced autoregressive objective, evaluate impact on classification performance, compare representations with standard autoregressive model
  3. Implement variable-length masked objective, evaluate impact on generation performance for short prompts, compare generated outputs with standard masked model

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the analysis, several questions emerge:

### Open Question 1
- **Question**: How does the proposed diversity-enhanced autoregressive objective affect performance on tasks requiring long-term dependencies?
- **Basis in paper**: [inferred] The paper proposes a diversity-enhanced autoregressive objective and demonstrates improvements on classification tasks, but does not specifically analyze performance on tasks requiring long-term dependencies
- **Why unresolved**: The paper focuses on classification and content generation tasks without evaluating on tasks with long-term dependencies like machine translation or summarization
- **What evidence would resolve it**: Experimental results comparing the proposed objective with standard autoregressive SSL on tasks requiring long-term dependencies

### Open Question 2
- **Question**: How does the variable-length masked objective affect performance on tasks with variable-length input sequences?
- **Basis in paper**: [explicit] The paper proposes a variable-length masked objective to mitigate length misalignment, but does not provide specific experimental results on tasks with variable-length input sequences
- **Why unresolved**: The paper focuses on classification and content generation tasks with fixed-length sequences without evaluating on tasks with variable-length inputs like text classification with documents of different lengths
- **What evidence would resolve it**: Experimental results comparing the proposed objective with standard masked SSL on tasks with variable-length input sequences

### Open Question 3
- **Question**: How does the choice of mask ratio in the variable-length masked objective affect performance on different downstream tasks?
- **Basis in paper**: [explicit] The paper proposes a variable-length masked objective with random mask ratio but does not provide detailed analysis of how mask ratio affects performance
- **Why unresolved**: The paper only considers a fixed range for mask ratio without exploring the impact of different ranges or strategies on performance
- **What evidence would resolve it**: Experimental results comparing performance of the proposed objective with different mask ratio ranges or strategies

## Limitations

- Theoretical framework relies heavily on singular value decomposition properties of co-occurrence matrices, which may not universally map to downstream task performance
- Empirical validation primarily conducted on text data with limited results on image datasets, raising questions about generalizability across modalities
- Proposed objectives tested with limited hyperparameter exploration and on a restricted set of downstream tasks
- Theoretical analysis focuses on linear evaluation protocols, which may not capture full potential when fine-tuning is allowed

## Confidence

- **High confidence**: Mathematical relationship between co-occurrence matrices and bipartite graph connectivity, and basic theoretical framework connecting SSL objectives to matrix decomposition
- **Medium confidence**: Specific claim that masked SSL inherently creates more inter-sample connections than autoregressive SSL, and proposed mechanisms for why this leads to better classification performance
- **Medium confidence**: Empirical improvements from proposed diversity-enhanced autoregressive and variable-length masked objectives, though magnitude may vary with different experimental conditions
- **Low confidence**: Generalizability of findings across different data modalities and downstream tasks beyond those specifically tested

## Next Checks

1. **Cross-modal validation**: Replicate the theoretical analysis and empirical validation on image datasets (e.g., ImageNet) to assess whether the same relationships between SSL objectives, co-occurrence matrices, and downstream performance hold for visual data.

2. **Ablation study on proposed objectives**: Conduct a comprehensive ablation study varying the diversity parameter t in the diversity-enhanced autoregressive objective and the mask length distribution in the variable-length masked objective to understand the sensitivity of performance improvements to these hyperparameters.

3. **Fine-tuning vs. linear evaluation comparison**: Compare the theoretical predictions and empirical performance when using the pretrained models for full fine-tuning rather than just linear evaluation, to assess whether the claimed advantages persist under different evaluation protocols.