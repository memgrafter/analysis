---
ver: rpa2
title: 'MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model
  Training on EU Languages'
arxiv_id: '2410.01036'
source_url: https://arxiv.org/abs/2410.01036
tags:
- speech
- data
- language
- languages
- license
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOSEL, a collection of 950,000 hours of open-source
  compliant speech data for training foundation models in 24 EU languages. It surveys
  labeled and unlabeled datasets under permissive licenses, then automatically generates
  transcripts for 441,000 hours of unlabeled data, releasing them under CC-BY 4.0.
---

# MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages

## Quick Facts
- arXiv ID: 2410.01036
- Source URL: https://arxiv.org/abs/2410.01036
- Reference count: 33
- 950,000 hours of open-source compliant speech data for 24 EU languages

## Executive Summary
This paper introduces MOSEL, a comprehensive collection of 950,000 hours of open-source compliant speech data for training foundation models across 24 EU languages. The work addresses a critical gap in the field: while many speech foundation models have emerged recently, none fully meet open-source criteria that require not just model weights but also training code and data under permissive licenses. The authors surveyed existing labeled and unlabeled datasets, automatically generated transcripts for 441,000 hours of unlabeled data, and released everything under CC-BY 4.0. A proof-of-concept experiment on Maltese demonstrates that combining labeled and pseudo-labeled data significantly improves ASR performance, validating the dataset's utility for developing truly open-source speech models.

## Method Summary
The authors conducted a comprehensive survey of ASR datasets and unlabeled speech corpora under open-source compliant licenses, collecting 505,725 hours of labeled data and 444,467 hours of unlabeled data across 24 EU languages. They automatically generated transcripts for 441,206 hours of unlabeled data using Whisper large v3, applying filtering techniques to remove unreliable transcriptions and releasing the dataset under CC-BY 4.0 license. For validation, they trained a sequence-to-sequence ASR model with a 12-layer Conformer encoder and 6-layer Transformer decoder on Maltese, initializing with English ASR weights, and evaluated performance improvements from combining labeled and pseudo-labeled data.

## Key Results
- Collected 950,000 hours of open-source compliant speech data for 24 EU languages
- Generated automatic transcripts for 441,206 hours of unlabeled data under CC-BY 4.0
- Demonstrated significant ASR performance improvement for Maltese using combined labeled and pseudo-labeled data (WER dropped from ~80 to ~24)
- Proved that no existing speech foundation model fully meets open-source criteria requiring weights, code, and training data

## Why This Works (Mechanism)

### Mechanism 1
Open-source compliance requires both code availability and training data under open licenses, not just model weights. The paper defines OS AI as requiring the system, training code, and training data to be available under open-source terms, rejecting models that only open-source weights. Core assumption: No existing SFM meets all three OS requirements (weights, code, data). Evidence: [abstract] "no existing SFM has model weights, code, and training data publicly available under open-source terms." Break condition: If a model emerges with all three components under open licenses, the OS definition here would no longer exclude existing SFMs.

### Mechanism 2
Pseudo-labeling unlabelled speech data can significantly improve ASR performance for low-resource languages. Automatic transcripts generated via Whisper are used as weak supervision to augment scarce labelled data, especially for languages like Maltese with minimal resources. Core assumption: Whisper's ASR outputs, even if imperfect, still add useful signal to supervised training. Evidence: [abstract] "combining labeled and pseudo-labeled data significantly improves ASR performance"; [section 4] WER drops from ~80 to ~39 with pseudo-labels, further to ~24 with filtering. Break condition: If Whisper's ASR quality is too low (e.g., >90% WER) for a language, the pseudo-labels may harm rather than help training.

### Mechanism 3
Excluding NC/SA/ND licensed data is essential to maintain open-source compliance. Data with non-commercial, share-alike, or no-derivatives restrictions violates the four freedoms required for OS AI. Core assumption: Including such data would taint the resulting model's OS status. Evidence: [section 1] Lists NC, ND, SA as incompatible with OS-compliant licenses; [section 2] Explicitly excludes datasets with these restrictions. Break condition: If a new license is introduced that is OS-compliant yet allows certain protections (e.g., attribution-only derivatives), this mechanism would need revision.

## Foundational Learning

- **Open Source AI Definition**: Why needed: The paper hinges on a strict interpretation of what counts as open-source in AI, distinguishing from mere open weights. Quick check: Does the model have all three: weights, training code, and training data under open licenses?

- **Weak Supervision / Pseudo-labeling**: Why needed: Enables training on massive unlabelled datasets by generating approximate labels, crucial for low-resource EU languages. Quick check: What is the quality gap between manual and auto-generated transcripts for a given language?

- **Data Filtering for ASR Quality**: Why needed: Improves downstream model performance by removing mislabeled segments, hallucinations, and non-target language speech. Quick check: How does filtering affect WER when training on pseudo-labeled data?

## Architecture Onboarding

- **Component map**: Data collection layer -> Pseudo-labeling layer -> Training layer -> Evaluation layer
- **Critical path**: 1. Identify OS-compliant datasets per EU language 2. Download and preprocess audio/text 3. Run Whisper inference on unlabelled data 4. Apply filtering (LID + hallucination detection) 5. Train Conformer-Transformer ASR model 6. Evaluate on held-out test sets
- **Design tradeoffs**: Open-source compliance vs. data quantity: Excluding NC/SA/ND data reduces total hours but ensures OS status; Whisper quality vs. compute cost: Higher quality models improve pseudo-labels but cost more GPU hours; Filtering strictness vs. data retention: Aggressive filtering removes noise but may discard useful data
- **Failure signatures**: High WER despite large training set → likely noisy pseudo-labels or filtering issues; GPU memory overflow during Whisper inference → batch size too large for audio chunks; Model fails to converge on low-resource language → insufficient clean labelled + pseudo-labeled data
- **First 3 experiments**: 1. Train baseline ASR on only labeled data for a mid-resource EU language; record WER 2. Add pseudo-labeled data without filtering; compare WER improvement 3. Add filtering steps; measure WER change and data retention rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum amount of labeled data required to effectively train an OSSFM for low-resource EU languages?
- Basis: [inferred] The proof-of-concept experiment on Maltese showed that training an ASR model using only supervised data (16 hours) failed to converge, necessitating the use of pseudo-labeled data.
- Why unresolved: The paper does not provide a clear threshold for the minimum amount of labeled data required.
- What evidence would resolve it: Experiments varying the amount of labeled data used for training OSSFMs on low-resource languages, with clear convergence criteria.

### Open Question 2
- Question: How effective are advanced filtering techniques compared to simple filtering methods in improving the quality of pseudo-labeled data for low-resource languages?
- Basis: [explicit] The paper mentions that simple filtering techniques were applied and proved effective, but more advanced filtering techniques can provide further benefits.
- Why unresolved: The paper does not provide a comparison between simple and advanced filtering techniques.
- What evidence would resolve it: A comparative study of ASR performance using pseudo-labeled data with simple filtering techniques versus advanced filtering techniques.

### Open Question 3
- Question: What are the challenges and potential solutions for collecting and releasing open-source compliant data for Irish, the least-resourced language in the EU?
- Basis: [explicit] The paper highlights the lack of unlabeled OS-compliant data for Irish, with only 17 hours of labeled speech collected.
- Why unresolved: The paper identifies the need for more open-source compliant data for Irish but does not explore specific challenges or solutions.
- What evidence would resolve it: Research on the specific challenges of data collection for Irish, including legal, logistical, and linguistic barriers.

## Limitations
- The paper does not provide detailed filtering criteria for pseudo-labeled data, making it difficult to assess the reliability of the 441,206 hours of automatically generated transcripts
- The proof-of-concept experiment focuses solely on Maltese, a very low-resource language, limiting generalizability to other EU languages with varying resource levels
- No comparison is made against existing semi-open SFMs that might have different definitions of open-source compliance

## Confidence

- **High confidence**: Dataset collection methodology and open-source compliance framework are well-documented and follow established licensing principles
- **Medium confidence**: Effectiveness of pseudo-labeling for low-resource languages based on Maltese results, but requiring validation across more languages
- **Low confidence**: Absolute WER improvements without knowing the exact filtering parameters and their impact on data quality

## Next Checks

1. **Cross-language validation**: Test the combined labeled + pseudo-labeled training approach on at least 3 additional EU languages spanning different resource levels (high, medium, low) to assess generalizability beyond Maltese

2. **Filtering methodology transparency**: Replicate the filtering pipeline with different strictness thresholds to quantify the trade-off between data retention rate and WER improvement, and document the exact criteria used

3. **Comparison with existing approaches**: Benchmark MOSEL's pseudo-labeled data quality against other semi-open SFMs' approaches to data collection and licensing to validate the claim that no existing SFM meets all open-source criteria