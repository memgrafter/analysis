---
ver: rpa2
title: Verifying Properties of Binary Neural Networks Using Sparse Polynomial Optimization
arxiv_id: '2405.17049'
source_url: https://arxiv.org/abs/2405.17049
tags:
- infeasible
- verification
- neural
- networks
- tighter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses robustness verification of Binary Neural Networks
  (BNNs) against adversarial attacks. It introduces a novel SDP-based approach using
  sparse Polynomial Optimization to encode and verify BNN properties.
---

# Verifying Properties of Binary Neural Networks Using Sparse Polynomial Optimization

## Quick Facts
- arXiv ID: 2405.17049
- Source URL: https://arxiv.org/abs/2405.17049
- Reference count: 40
- Introduces SDP-based approach using sparse Polynomial Optimization for BNN robustness verification

## Executive Summary
This paper addresses the challenge of verifying Binary Neural Networks (BNNs) against adversarial attacks through a novel optimization-based approach. The authors develop a sparse polynomial optimization framework that constructs first-order SDP relaxations with tautological constraints to improve verification accuracy. The method demonstrates superior performance compared to existing LP-based approaches, achieving significantly less conservative bounds while maintaining computational efficiency. The approach is notable for its compatibility with continuous input spaces and its effectiveness against both ℓ∞ and ℓ₂-based adversarial attacks.

## Method Summary
The proposed method encodes BNN verification as a polynomial optimization problem, then applies sparse sum-of-squares (SOS) relaxations to solve it via semidefinite programming. The key innovation lies in incorporating tautological constraints that tighten the relaxation bounds while maintaining tractability. The approach leverages the specific structure of BNNs—with binary weights and activations—to create efficient polynomial representations. By using sparse optimization techniques, the method avoids the computational complexity typically associated with dense polynomial formulations, enabling practical verification of medium-sized BNNs within reasonable timeframes.

## Key Results
- Achieves 15-26% less conservative bounds compared to LP-based methods
- Demonstrates 20x faster verification for larger perturbations versus LP approaches
- Successfully verifies properties under both ℓ∞ and ℓ₂ adversarial attacks

## Why This Works (Mechanism)
The method works by exploiting the mathematical structure of BNNs through polynomial optimization. By representing the BNN's forward pass as a set of polynomial constraints and applying SOS relaxations, the approach can systematically bound the network's output range under input perturbations. The sparse formulation ensures computational tractability while maintaining solution quality.

## Foundational Learning
- **Polynomial Optimization**: Mathematical framework for optimizing polynomials subject to polynomial constraints. Needed to model BNN verification as an optimization problem. Quick check: Can represent ReLU networks as polynomial constraints through auxiliary variables.
- **Semidefinite Programming (SDP)**: Convex optimization technique for solving certain classes of polynomial optimization problems. Required for solving the SOS relaxations. Quick check: SDP solvers can handle matrices of size up to ~10,000 variables efficiently.
- **Sum-of-Squares (SOS) Relaxations**: Method to transform non-convex polynomial optimization into convex semidefinite programs. Essential for making the verification problem tractable. Quick check: SOS conditions are equivalent to non-negativity for polynomials of degree ≤ 2.
- **Tautological Constraints**: Constraints that are always true but help tighten relaxations. Used to improve bound quality without increasing computational complexity. Quick check: Can be derived from network architecture properties.
- **Sparse Optimization**: Techniques to exploit problem structure and reduce computational burden. Critical for scaling to larger networks. Quick check: Can reduce variable count by orders of magnitude for structured problems.

## Architecture Onboarding

**Component Map**: Input Space -> Polynomial Encoding -> Sparse SOS Relaxation -> SDP Solver -> Output Bounds

**Critical Path**: The verification pipeline flows from input perturbation specification through polynomial constraint generation, sparse relaxation construction, SDP solution, and final bound extraction. The most computationally intensive step is the SDP solve, which scales with the number of polynomial variables and degree of relaxation.

**Design Tradeoffs**: The method balances relaxation tightness against computational cost. Higher-order relaxations provide tighter bounds but require more computational resources. Sparse formulations sacrifice some bound quality for significant speed improvements. The choice of tautological constraints involves a similar tradeoff between tightening bounds and increasing problem size.

**Failure Signatures**: Verification may fail when (1) the SDP solver cannot find a feasible solution within time limits, (2) relaxation bounds become too loose to be meaningful, or (3) the sparse approximation loses critical structural information. These typically manifest as timeout errors or bounds that exceed practical thresholds.

**First Experiments**: 1) Verify a small 2-layer BNN on MNIST with ℓ∞ perturbations of magnitude 0.1, 2) Compare LP vs SDP bounds on a 3-layer network with ℓ₂ perturbations, 3) Test scalability by verifying a 4-layer network with increasing input dimensions.

## Open Questions the Paper Calls Out
None

## Limitations
- Restricted to binary networks with binarized weights and activations only
- Scalability bounded by SDP solver capabilities, with verification times reaching several minutes for larger networks
- Comparative analysis focuses primarily on LP-based approaches with limited benchmarking against modern MILP solvers

## Confidence
- **High Confidence**: SDP relaxation accuracy improvements over LP methods, verification speed comparisons, continuous input space compatibility
- **Medium Confidence**: 15-26% less conservative bounds (architecture and attack scenario dependent)
- **Medium Confidence**: ℓ₂-based attack verification performance (limited ablation studies)

## Next Checks
1. Test scalability on deeper BNNs (8+ layers) and larger input dimensions to evaluate practical deployment limits
2. Compare performance against modern MILP solvers on identical benchmark sets with standardized metrics
3. Evaluate robustness verification under adaptive attacks specifically designed to exploit relaxation gaps in the SDP formulation