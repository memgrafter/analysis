---
ver: rpa2
title: 'The Qiyas Benchmark: Measuring ChatGPT Mathematical and Language Understanding
  in Arabic'
arxiv_id: '2407.00146'
source_url: https://arxiv.org/abs/2407.00146
tags:
- language
- arabic
- chatgpt
- questions
- verbal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Qiyas benchmark, a novel evaluation framework
  designed to assess the mathematical reasoning and language understanding capabilities
  of large language models (LLMs) in Arabic. The benchmark is derived from the Qiyas
  exam, a standardized test used for university admissions in Saudi Arabia, ensuring
  its quality and relevance.
---

# The Qiyas Benchmark: Measuring ChatGPT Mathematical and Language Understanding in Arabic

## Quick Facts
- arXiv ID: 2407.00146
- Source URL: https://arxiv.org/abs/2407.00146
- Authors: Shahad Al-Khalifa; Hend Al-Khalifa
- Reference count: 0
- ChatGPT-4 achieved 64% accuracy vs ChatGPT-3.5-turbo at 49% on Arabic language and math tasks

## Executive Summary
This paper introduces the Qiyas benchmark, a novel evaluation framework designed to assess the mathematical reasoning and language understanding capabilities of large language models (LLMs) in Arabic. The benchmark is derived from the Qiyas exam, a standardized test used for university admissions in Saudi Arabia, ensuring its quality and relevance. The authors evaluated ChatGPT-4 and ChatGPT-3.5-turbo across zero-shot, one-shot, and three-shot prompt settings, finding that ChatGPT-4 consistently outperformed its counterpart. The benchmark covers 2,407 questions spanning quantitative and verbal sections, providing a standardized resource for measuring Arabic LLM capabilities.

## Method Summary
The authors created a benchmark from the Qiyas standardized exam used for Saudi university admissions, consisting of 2,407 Arabic questions across quantitative (math) and verbal (language) sections. They evaluated ChatGPT-3.5-turbo and ChatGPT-4 using zero-shot, one-shot, and three-shot prompting with the instruction "Write the answer only" to ensure concise responses. Performance was measured by comparing model answers to correct answers in the dataset, with detailed error analysis to identify specific weaknesses in mathematical reasoning and language understanding.

## Key Results
- ChatGPT-4 achieved 64% overall accuracy compared to ChatGPT-3.5-turbo's 49% on the Qiyas benchmark
- Both models struggled with complex algebraic equations, probability questions, and tasks requiring deeper contextual understanding
- Performance varied significantly depending on prompt setting and question type, with few-shot prompting generally improving results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using zero-shot, one-shot, and three-shot prompting allows evaluation of models' reasoning capabilities across varying context availability.
- Mechanism: By presenting the same questions under different prompt settings, the study measures how much contextual examples improve performance, isolating the effect of training data and prompting strategy.
- Core assumption: Models' performance differences across prompt settings reflect underlying differences in reasoning ability rather than just familiarity with test format.
- Evidence anchors:
  - [abstract] "assessed the performance of ChatGPT-3.5-turbo and ChatGPT-4 on our benchmarks... across zero-shot, one-shot, and few-shot settings"
  - [section] "we employed zero-shot prompts, but we subsequently extended it by incorporating one-shot and 3-shot prompts"
  - [corpus] Weak evidence - neighboring papers focus on Arabic LLM benchmarks but don't specifically address prompting strategies.
- Break condition: If performance differences are driven primarily by test format familiarity rather than reasoning ability, the mechanism fails.

### Mechanism 2
- Claim: Using a real standardized test (Qiyas) ensures the benchmark reflects actual educational assessment standards.
- Mechanism: Questions are written by domain experts experienced in designing and grading Qiyas exams, providing validated quality and relevance.
- Core assumption: Expert-designed questions accurately measure mathematical reasoning and language understanding abilities relevant to university admissions.
- Evidence anchors:
  - [abstract] "derived from a standardized test widely used for university admissions in Saudi Arabia, ensuring their quality has been validated by educational experts"
  - [section] "The questions used in the evaluation were written by domain experts experienced in designing and grading Qiyas exams"
  - [corpus] Weak evidence - while neighboring papers address Arabic benchmarks, none specifically validate against standardized educational tests.
- Break condition: If expert validation doesn't translate to actual measure of reasoning abilities, the mechanism fails.

### Mechanism 3
- Claim: Evaluating both ChatGPT-3.5-turbo and ChatGPT-4 establishes baseline performance for Arabic language models.
- Mechanism: Comparing results across different model versions shows progress in Arabic language understanding and mathematical reasoning capabilities.
- Core assumption: Performance differences between model versions reflect genuine improvements rather than differences in training data or architecture.
- Evidence anchors:
  - [abstract] "ChatGPT-4 achieved an overall average accuracy of 64%, while ChatGPT-3.5-turbo achieved an overall accuracy of 49%"
  - [section] "The overall results show that ChatGPT-4 outperforms ChatGPT-3.5-turbo in a wide variety of linguistic and mathematical domains"
  - [corpus] Weak evidence - neighboring papers benchmark Arabic capabilities but don't specifically compare across model versions.
- Break condition: If performance differences are due to factors other than genuine capability improvements, the mechanism fails.

## Foundational Learning

- Concept: Standardized test design and validation
  - Why needed here: Understanding how Qiyas exam questions are validated by educational experts ensures the benchmark's quality
  - Quick check question: What makes a standardized test question valid for measuring reasoning abilities?

- Concept: Prompt engineering strategies
  - Why needed here: Different prompt settings (zero-shot, one-shot, three-shot) reveal how context affects model performance
  - Quick check question: How do you design prompts that effectively test a model's reasoning without providing too much context?

- Concept: Error analysis methodology
  - Why needed here: Categorizing errors helps identify specific weaknesses in models' Arabic language understanding
  - Quick check question: What types of errors indicate fundamental limitations in language model reasoning versus surface-level issues?

## Architecture Onboarding

- Component map: Dataset creation -> Prompt formulation -> Model evaluation -> Error analysis -> Benchmark release
- Critical path: Dataset creation -> Prompt formulation -> Model evaluation -> Error analysis
- Design tradeoffs: Expert-validated questions vs. larger dataset size; controlled prompting vs. natural usage
- Failure signatures: Low accuracy across all models; inconsistent performance between prompt settings; poor error analysis categorization
- First 3 experiments:
  1. Test prompt formulations on a small subset to ensure models respond as expected
  2. Compare zero-shot performance between ChatGPT-4 and ChatGPT-3.5-turbo on simple questions
  3. Run error analysis on incorrectly answered questions to identify common failure patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ChatGPT's performance vary when visual elements are included in the Qiyas exam questions?
- Basis in paper: [explicit] The authors note that they focused on questions without visual representations due to ChatGPT-4's difficulty in processing visual information, but the Qiyas exam includes image-based questions.
- Why unresolved: The authors intentionally excluded image-based questions from their evaluation to mirror the examination process and due to ChatGPT-4's limitations with visual information.
- What evidence would resolve it: Evaluating ChatGPT models on the image-based questions from the Qiyas exam would provide insights into their multimodal capabilities and limitations with Arabic language tasks.

### Open Question 2
- Question: What are the specific linguistic challenges that current Arabic LLMs face, and how can these be addressed?
- Basis in paper: [inferred] The authors highlight the need for more Arabic-focused training data and model development efforts to enhance the mathematical and linguistic capabilities of future Arabic LLMs, given their current limitations with the complexities of the Arabic language.
- Why unresolved: The authors acknowledge the current limitations of Arabic LLMs but do not provide specific details on the linguistic challenges or potential solutions.
- What evidence would resolve it: Conducting a detailed linguistic analysis of the errors made by Arabic LLMs on the Qiyas benchmark and exploring targeted approaches to address these challenges would provide valuable insights.

### Open Question 3
- Question: How do other state-of-the-art LLMs perform on the Qiyas benchmark compared to ChatGPT-4 and ChatGPT-3.5-turbo?
- Basis in paper: [explicit] The authors evaluated Gemini-pro on a subset of questions incorrectly answered by ChatGPT models and found it to be promising, particularly in reading comprehension, but did not provide a comprehensive comparison.
- Why unresolved: The authors only evaluated Gemini-pro on a limited set of questions and did not compare its performance to ChatGPT models across all question types and prompt settings.
- What evidence would resolve it: Evaluating a wider range of state-of-the-art LLMs on the entire Qiyas benchmark, including all question types and prompt settings, would provide a more comprehensive understanding of their capabilities and limitations compared to ChatGPT models.

## Limitations

- The Qiyas benchmark dataset is not publicly available, making independent verification difficult
- The paper does not specify the exact prompt examples used in one-shot and three-shot settings
- Evaluation is limited to ChatGPT models, leaving uncertainty about other Arabic language models' performance

## Confidence

**High Confidence Claims:**
- The Qiyas benchmark dataset consists of 2,407 questions derived from a standardized Saudi Arabian university admissions test
- ChatGPT-4 outperforms ChatGPT-3.5-turbo on Arabic mathematical and language understanding tasks
- Both models show consistent performance patterns across different question types

**Medium Confidence Claims:**
- The performance differences between models reflect genuine improvements in Arabic language capabilities
- Zero-shot, one-shot, and three-shot prompting effectively isolates reasoning ability from test familiarity
- The error analysis accurately identifies fundamental limitations in Arabic language model reasoning

**Low Confidence Claims:**
- The benchmark represents a comprehensive evaluation of Arabic language model capabilities
- The performance gap between ChatGPT-4 and ChatGPT-3.5-turbo indicates substantial progress in Arabic NLP
- The identified error patterns will generalize to other Arabic language models

## Next Checks

1. **Dataset Accessibility Verification**: Contact the authors and Saudi Arabian educational authorities to determine the feasibility of obtaining the Qiyas benchmark dataset for independent evaluation, documenting the access process and any restrictions.

2. **Prompt Reproducibility Test**: Recreate the prompt settings using the methodology described, then systematically test with alternative prompt examples to measure sensitivity of results to prompt formulation and ensure findings are robust to prompt variations.

3. **Cross-Model Generalization Study**: Evaluate additional Arabic language models (including open-source models like Jais or Phoenix) on the Qiyas benchmark to determine whether the performance patterns and limitations identified for ChatGPT models generalize across the broader landscape of Arabic language models.