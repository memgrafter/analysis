---
ver: rpa2
title: Increasing faithfulness in human-human dialog summarization with Spoken Language
  Understanding tasks
arxiv_id: '2409.10070'
source_url: https://arxiv.org/abs/2409.10070
tags:
- dialog
- summary
- summarization
- call
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study aims to improve faithfulness in human-human dialog summarization
  by incorporating task-specific semantic information. The authors propose using Spoken
  Language Understanding (SLU) elements such as call type and named entities to enhance
  summary accuracy.
---

# Increasing faithfulness in human-human dialog summarization with Spoken Language Understanding tasks

## Quick Facts
- arXiv ID: 2409.10070
- Source URL: https://arxiv.org/abs/2409.10070
- Authors: Eunice Akani; Benoit Favre; Frederic Bechet; Romain Gemignani
- Reference count: 5
- Primary result: Incorporating task-specific semantic information from Spoken Language Understanding improves faithfulness in human-human dialog summarization

## Executive Summary
This paper addresses the challenge of hallucination in dialog summarization by incorporating task-specific semantic information from Spoken Language Understanding (SLU) systems. The authors propose using call type classification and named entity hallucination risk (NEHR) as selection criteria to identify the most faithful summaries from multiple candidates. They demonstrate that their approach significantly improves summary accuracy while reducing hallucinations, even when using automatic transcriptions instead of manual ones.

## Method Summary
The proposed method involves three main components: automatic transcription using WhisperX, semantic information prediction through SLU classifiers, and summary generation with selection. The system generates multiple summary candidates using different decoding parameters, then selects the most faithful one based on KL divergence between call type distributions and NEHR metrics. The approach is evaluated on the DECODA corpus with three dataset versions (manual transcripts, augmented data, automatic transcriptions) using the BARThez model.

## Key Results
- The proposed semantic selection method (Pipeline C) achieves the highest NE-F1 score of 77.67% and NE-P of 76.31%
- Incorporating semantic information improves summary faithfulness, with the best results using combined NEHR and KL divergence metrics
- Automatic transcription using WhisperX is viable but shows approximately 10% decrease in NE-F1 compared to manual transcripts
- Data augmentation improves ROUGE scores while maintaining semantic accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific semantic information reduces hallucinations in dialog summarization.
- Mechanism: By conditioning summary generation on predicted call types and entities, the model stays grounded to task-relevant facts.
- Core assumption: Semantic information from SLU systems can be effectively transferred to dialog summarization.
- Evidence anchors:
  - [abstract] "We suggest using the semantic information proposed for performing Spoken Language Understanding (SLU) in human-machine dialogue systems for goal-oriented human-human dialogues to obtain a more semantically faithful summary regarding the task."
  - [section 2.1] "We hypothesize that by incorporating task-specific semantic information into the summarization process, we can effectively address inconsistencies related to this information in the data."
- Break condition: If semantic predictions are noisy or misaligned with dialog content, conditioning may introduce more errors than it prevents.

### Mechanism 2
- Claim: Selection criteria based on semantic divergence and named entity hallucination risk improves summary reliability.
- Mechanism: Multiple candidate summaries are generated and filtered using KL divergence on call types and NEHR metric to select the most faithful one.
- Core assumption: Summaries with lower semantic divergence from the dialog are more faithful.
- Evidence anchors:
  - [section 2.2] "we propose using semantic information to select the most reliable summary from different summaries obtained by tweaking decoding parameters such as top-p."
  - [section 2.2] "we suggest using call type as a summary selection criterion... calculating the divergence between the probability distribution on all call types for both summary and dialog classifiers using the KL divergence."
- Break condition: If the classification models themselves hallucinate or are poorly calibrated, the selection metric may choose incorrect summaries.

### Mechanism 3
- Claim: Automatic speech transcription can substitute for manual transcripts without major loss in summary quality.
- Mechanism: Using WhisperX to transcribe audio and fine-tuning summarization models on augmented data compensates for transcription errors.
- Core assumption: ASR quality is sufficient to preserve semantic information needed for summarization.
- Evidence anchors:
  - [section 3.3] "The findings indicate that data augmentation enhances summary scores while semantic scores remain constant, with a slight increase in NE-F1. In terms of automatic transcription, several metrics declined. However, the scores of the large model continue to be competitive, suggesting that the ASR system could be a viable option."
  - [section 3.1] "We used WhisperX [Bain et al., 2023], a state-of-the-art model for ASR based on Whisper [Radford et al., 2022]."
- Break condition: If WER exceeds critical thresholds, semantic information may be lost, degrading summary faithfulness.

## Foundational Learning

- Concept: Spoken Language Understanding (SLU) task hierarchy
  - Why needed here: The method repurposes SLU semantic elements (domain, intent, entities) for summarization.
  - Quick check question: Can you list the three semantic levels typically defined in SLU and explain their role in dialog understanding?

- Concept: Named Entity Hallucination Risk (NEHR) metric
  - Why needed here: NEHR measures the proportion of named entities in a summary that don't appear in the source, used to detect hallucinations.
  - Quick check question: How does NEHR differ from standard named entity recognition metrics like precision and recall?

- Concept: KL divergence for probability distribution comparison
  - Why needed here: Used to measure semantic consistency between predicted call types for dialog and summary.
  - Quick check question: In what way does KL divergence capture the "distance" between two probability distributions, and why is it appropriate for comparing call type predictions?

## Architecture Onboarding

- Component map:
  ASR component (WhisperX) → Automatic transcription
  SLU classifiers (intent and entity extraction) → Semantic information prediction
  Summarization model (BARThez) → Summary generation
  Selection module → NEHR and KL divergence based filtering
  Data pipeline → Data augmentation with LLM for training

- Critical path:
  ASR → Semantic prediction → Summary generation → Selection filtering → Final summary output

- Design tradeoffs:
  - Using automatic vs. manual transcription: tradeoff between scalability and accuracy
  - Selection based on multiple criteria: balances call type consistency with entity fidelity
  - Data augmentation strategy: increases training data but may introduce stylistic inconsistencies

- Failure signatures:
  - High WER leading to poor semantic predictions
  - NEHR-based selection picking summaries with correct entities but missing key information
  - KL divergence not capturing semantic nuances beyond call types

- First 3 experiments:
  1. Train summarization model with and without semantic conditioning on a small validation set to measure impact on NE-F1.
  2. Test selection criteria separately: use only DKL vs only NEHR vs combined to observe differences in CT-Acc and NE-F1.
  3. Compare performance using manual transcripts vs WhisperX (Tiny, Base, Large) to find WER threshold where quality degrades significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed approach for selecting the most reliable summary based on semantic information (call type and named entities) compare to other existing methods for summary selection in dialog summarization?
- Basis in paper: [explicit] The paper introduces a new method for selecting the most reliable summary based on call type prediction and named entity hallucination risk (NEHR).
- Why unresolved: The paper only compares the proposed method to a baseline model without selection, and does not compare it to other existing methods for summary selection in dialog summarization.
- What evidence would resolve it: Experiments comparing the proposed method to other existing methods for summary selection in dialog summarization.

### Open Question 2
- Question: How does the performance of the proposed approach change when using different types of semantic information, such as semantic frames, instead of call type and named entities?
- Basis in paper: [explicit] The paper mentions that other semantic information, such as semantic frames, can be used, but does not explore this possibility.
- Why unresolved: The paper only uses call type and named entities as semantic information, and does not explore the use of other types of semantic information.
- What evidence would resolve it: Experiments using different types of semantic information, such as semantic frames, instead of call type and named entities.

### Open Question 3
- Question: How does the proposed approach perform on other types of goal-oriented human-human dialogues, such as those in the customer service domain?
- Basis in paper: [explicit] The paper only evaluates the proposed approach on the DECODA corpus, which contains goal-oriented human-human dialogues from a call center.
- Why unresolved: The paper only evaluates the proposed approach on one type of goal-oriented human-human dialogues, and does not explore its performance on other types of dialogues.
- What evidence would resolve it: Experiments evaluating the proposed approach on other types of goal-oriented human-human dialogues, such as those in the customer service domain.

## Limitations
- The method only incorporates call type and named entities from SLU, leaving domain and intent information unused
- Performance decreases by approximately 10% when using automatic transcriptions instead of manual transcripts
- The approach is only evaluated on one specific domain (call center dialogues) limiting generalizability claims

## Confidence

### Confidence Levels
- **High confidence**: The core mechanism of using semantic information for summary selection is well-supported by experimental results, particularly the effectiveness of combining KL divergence and NEHR metrics
- **Medium confidence**: The data augmentation strategy shows promise but its impact is somewhat mixed, with improved ROUGE scores but only marginal gains in semantic metrics
- **Medium confidence**: The automatic transcription results are promising but the 10% performance drop suggests careful consideration is needed before deploying in production environments

## Next Checks

1. Test the method with additional SLU elements (domain and intent) to evaluate whether incorporating the complete semantic hierarchy improves summary faithfulness beyond the current approach

2. Conduct a comprehensive analysis of the trade-off between different WER thresholds and summary quality to establish clear guidelines for when automatic transcription is viable

3. Evaluate the method's performance on dialogue datasets from different domains (beyond call centers) to assess generalizability of the semantic conditioning approach