---
ver: rpa2
title: The Death of Feature Engineering? BERT with Linguistic Features on SQuAD 2.0
arxiv_id: '2404.03184'
source_url: https://arxiv.org/abs/2404.03184
tags:
- bert
- answer
- features
- linguistic
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a method to improve BERT-based models for\
  \ the SQuAD 2.0 question answering task by incorporating linguistic features. The\
  \ approach uses SpaCy to extract four token-level features\u2014NER, POS, DEP, and\
  \ STOP\u2014from both the context and the question."
---

# The Death of Feature Engineering? BERT with Linguistic Features on SQuAD 2.0

## Quick Facts
- arXiv ID: 2404.03184
- Source URL: https://arxiv.org/abs/2404.03184
- Reference count: 2
- Primary result: BERT-base with linguistic features improves SQuAD 2.0 EM/F1 by 2.17/2.14 points

## Executive Summary
This work introduces a method to improve BERT-based models for the SQuAD 2.0 question answering task by incorporating linguistic features. The approach uses SpaCy to extract four token-level features—NER, POS, DEP, and STOP—from both the context and the question. These features are embedded and concatenated with BERT's output before being fed into a final linear layer for span prediction. Experiments show that the proposed method improves Exact Match (EM) and F1 scores by 2.17 and 2.14 points, respectively, over BERT-base on the dev set. On the hidden test set, the best single model (BERT-large with linguistic features) achieves 76.55 EM and 79.97 F1. Error analysis reveals that the linguistic features help the model locate answers in complex linguistic contexts where vanilla BERT predicts "No Answer." However, both models still struggle with distinguishing answerable from unanswerable questions. The study demonstrates that feature engineering remains valuable, especially when computational resources for large models are constrained.

## Method Summary
The authors propose a BERT-based model for SQuAD 2.0 that incorporates linguistic features extracted using SpaCy. Four token-level features (NER, POS, DEP, STOP) are extracted for both context and question, embedded, and concatenated with BERT's output before the final linear layer for span prediction. The model is trained on SQuAD 2.0 and evaluated using EM and F1 metrics.

## Key Results
- BERT-base with linguistic features achieves 2.17 EM and 2.14 F1 improvement over vanilla BERT-base on dev set
- BERT-large with linguistic features achieves 76.55 EM and 79.97 F1 on hidden test set
- Error analysis shows linguistic features help locate answers in complex contexts where BERT alone predicts "No Answer"
- Both models struggle with distinguishing answerable from unanswerable questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linguistic features improve BERT's ability to locate answers in complex linguistic contexts
- Mechanism: The four token-level features (NER, POS, DEP, STOP) provide explicit syntactic and semantic cues that help the model resolve ambiguity and identify answer spans more accurately, especially when BERT alone struggles with complex linguistic structures
- Core assumption: BERT's learned representations benefit from complementary linguistic features that provide explicit structural information not fully captured by self-attention
- Evidence anchors:
  - [abstract] "Our error analysis also shows that the linguistic architecture can help model understand the context better in that it can locate answers that BERT only model predicted 'No Answer' wrongly."
  - [section] "Analyzing these questions, we can see that they are usually very complex in logic and clause structure, thus the baseline BERT model has difficulties analyzing such sentences with highly complex linguistic structures. However, when we add the linguistic features... our model can predict the correct answers."
- Break condition: If the linguistic features are noisy or incorrectly extracted, they could mislead the model and degrade performance

### Mechanism 2
- Claim: Feature engineering remains valuable when computational resources are constrained
- Mechanism: Smaller models like BERT-base benefit more from linguistic features because they have less capacity to learn complex linguistic patterns from raw text alone
- Core assumption: The marginal gain from feature engineering is inversely proportional to the model's pre-training scale and capacity
- Evidence anchors:
  - [section] "We also experimented our new architecture on BERT large model, the EM and F1 results are pretty similar, though both results increased a lot. One of the postulated reason is that BERT base is relatively less powerful in comparison with BERT large and thus adding additional features may see more significant improvement."
- Break condition: If computational resources are abundant, the cost-benefit ratio of feature engineering diminishes as larger models can learn equivalent patterns

### Mechanism 3
- Claim: The concatenation approach effectively combines BERT representations with linguistic features
- Mechanism: By embedding the linguistic features and concatenating them with BERT's output before the final linear layer, the model can learn to weight and integrate both sources of information appropriately
- Core assumption: The final linear layer can effectively learn to combine the complementary information from BERT and linguistic features
- Evidence anchors:
  - [section] "We concatenate the output from the BERT model and the output from the linguistic features linear layer and treat this as the input for the final linear layer."
- Break condition: If the feature dimensions or scales are mismatched, the linear layer may struggle to balance the two information sources effectively

## Foundational Learning

- Concept: Token-level feature extraction using SpaCy
  - Why needed here: The model requires consistent, token-aligned features that match BERT's tokenization for effective concatenation
  - Quick check question: How does SpaCy's tokenization align with BERT's WordPiece tokenization, and what preprocessing is needed to ensure feature alignment?

- Concept: Pre-trained contextual embeddings (BERT)
  - Why needed here: BERT provides rich contextual representations that form the foundation of the QA model
  - Quick check question: What are the key architectural differences between BERT-base and BERT-large that explain their performance gap?

- Concept: Span prediction in extractive QA
  - Why needed here: The task requires predicting answer boundaries within the context, not generating free-form answers
  - Quick check question: How does the model handle the case where no answer exists in the context (SQuAD 2.0)?

## Architecture Onboarding

- Component map: Context/Question → BERT → Feature Extraction → Embedding → Concatenation → Final Linear → Answer Span
- Critical path: Context/Question → BERT → Feature Extraction → Embedding → Concatenation → Final Linear → Answer Span
- Design tradeoffs:
  - Feature engineering vs. model scaling (BERT-large vs. BERT-base with features)
  - Computational cost of feature extraction vs. performance gain
  - Model complexity vs. interpretability
- Failure signatures:
  - Performance degrades when SpaCy fails to extract features correctly
  - Model struggles when linguistic features are not aligned with BERT tokenization
  - No significant improvement over BERT-base indicates feature engineering may be unnecessary
- First 3 experiments:
  1. Baseline: Train BERT-base without linguistic features to establish performance floor
  2. Feature ablation: Train with only one linguistic feature type (e.g., POS) to assess individual contributions
  3. Scale comparison: Compare BERT-base with features vs. BERT-large without features to determine cost-effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different combinations of linguistic features (NER, POS, DEP, STOP) individually impact the performance of BERT-based models on SQuAD 2.0?
- Basis in paper: [explicit] The paper incorporates NER, POS, DEP, and STOP features and reports overall performance improvement but does not isolate the contribution of each feature.
- Why unresolved: The study combines all features and does not perform ablation studies to determine the individual impact of each linguistic feature.
- What evidence would resolve it: Ablation studies showing performance changes when each feature is removed or added individually.

### Open Question 2
- Question: Why does BERT-large with linguistic features perform similarly to BERT-large alone, unlike BERT-base?
- Basis in paper: [explicit] The authors note that BERT-large performs well on its own, so additional features don't significantly improve results.
- Why unresolved: The paper does not provide a detailed analysis or hypothesis explaining the differential impact of linguistic features on BERT-base vs BERT-large.
- What evidence would resolve it: A detailed comparative analysis of the internal representations and attention patterns in BERT-base vs BERT-large when linguistic features are added.

### Open Question 3
- Question: What specific aspects of linguistic structure are most challenging for BERT models, and how do linguistic features address these challenges?
- Basis in paper: [inferred] The error analysis shows that BERT struggles with complex linguistic contexts, and linguistic features help in these cases.
- Why unresolved: The paper does not specify which linguistic structures are most problematic or how each feature type (NER, POS, DEP, STOP) contributes to resolving these issues.
- What evidence would resolve it: A detailed error analysis categorizing errors by linguistic complexity and mapping them to the specific features that help.

## Limitations
- Limited ablation analysis: The paper demonstrates performance improvements but does not provide comprehensive ablation studies to isolate the contribution of each linguistic feature.
- Feature alignment ambiguity: The paper doesn't specify how SpaCy's tokenization aligns with BERT's WordPiece tokenization, which is critical for correct feature integration.
- Resource constraint generalization: The claim about feature engineering value under resource constraints is based on a single comparison rather than systematic exploration.

## Confidence
- High Confidence: The claim that incorporating linguistic features improves performance on SQuAD 2.0 is well-supported by the reported metrics and sound experimental methodology.
- Medium Confidence: The claim that linguistic features help in complex linguistic contexts where BERT struggles is supported by error analysis examples, though sample size is not specified.
- Low Confidence: The claim that feature engineering remains valuable when computational resources are constrained is based on limited comparisons and would benefit from more systematic analysis.

## Next Checks
1. **Feature ablation study**: Systematically remove each linguistic feature (NER, POS, DEP, STOP) individually and in combinations to quantify their individual and collective contributions.
2. **Tokenization alignment verification**: Implement and validate a rigorous method for aligning SpaCy's tokenization with BERT's WordPiece tokenization, including handling of out-of-vocabulary words and special tokens.
3. **Resource-accuracy tradeoff analysis**: Systematically compare different model sizes (BERT-base, BERT-medium, BERT-large) with and without linguistic features across multiple computational budgets.