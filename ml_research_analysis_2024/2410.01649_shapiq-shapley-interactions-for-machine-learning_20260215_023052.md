---
ver: rpa2
title: 'shapiq: Shapley Interactions for Machine Learning'
arxiv_id: '2410.01649'
source_url: https://arxiv.org/abs/2410.01649
tags:
- games
- explanation
- shapiq
- feature
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: shapiq is an open-source Python package for computing Shapley Values
  and Shapley Interactions (SIs) in machine learning. It unifies state-of-the-art
  algorithms to efficiently approximate SIs for any explanation order, addressing
  the exponential complexity of exact computation.
---

# shapiq: Shapley Interactions for Machine Learning

## Quick Facts
- arXiv ID: 2410.01649
- Source URL: https://arxiv.org/abs/2410.01649
- Reference count: 40
- shapiq is an open-source Python package for computing Shapley Values and Shapley Interactions (SIs) in machine learning.

## Executive Summary
shapiq is an open-source Python package that unifies state-of-the-art algorithms to efficiently approximate Shapley Values and any-order Shapley Interactions in machine learning. By addressing the exponential complexity of exact computation, the package provides a comprehensive toolkit for explaining feature interactions beyond traditional attributions. It includes a benchmarking suite with 11 real-world ML applications and pre-computed ground-truth values for systematic evaluation. Experimental results demonstrate that Shapley Interactions significantly improve faithfulness over standard Shapley Values, with pairwise interactions yielding the most substantial gains.

## Method Summary
The shapiq package implements a unified interface for 7 algorithms approximating Shapley Interactions across 4 interaction indices, along with 7 algorithms for approximating Shapley Values. It provides exact computation for smaller games (n ≤ 16) through the ExactComputer class, which computes 18 interaction indices and serves as ground truth for benchmarking. The package includes pre-computed games from 11 machine learning applications across tabular, text, and image domains, enabling systematic evaluation of approximation methods. Key metrics include faithfulness (R²) for varying explanation orders, mean squared error against ground truth, and precision@5 for top interactions.

## Key Results
- Shapley Interactions (SIs) significantly improve faithfulness over Shapley Values (SHAP), with pairwise interactions yielding the largest gains
- Performance of approximation methods varies strongly across application domains, with kernel-based approaches excelling in local explanations and stratified-sampling methods performing best in data valuation
- The shapiq package extends SHAP beyond feature attributions, consolidating SI applications in ML and facilitating future research

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The package enables accurate approximation of Shapley Interactions (SIs) by unifying state-of-the-art algorithms in a general framework.
- Mechanism: The package implements 7 algorithms for approximating SIs across 4 different interaction indices, and another 7 algorithms for approximating SVs, providing a comprehensive toolset for practitioners.
- Core assumption: Different approximation methods excel in different application domains, allowing for optimal selection based on the specific use case.
- Evidence anchors:
  - [abstract]: "Due to the exponential complexity of computing SVs and SIs, various methods have been proposed that exploit structural assumptions or yield probabilistic estimates given limited resources."
  - [section 4.2]: "Most notably, the ranking of approximators varies strongly between the different applications domains, which is depicted in Figure 5 (a) and (b)."
- Break condition: If the application domain is not well-represented in the benchmark suite, the optimal approximation method may not be identified.

### Mechanism 2
- Claim: The package provides efficient computation of exact SIs for smaller games, enabling ground truth comparisons.
- Mechanism: The shapiq.ExactComputer class provides an interface for computing 18 interaction indices and game-theoretic concepts, including the MIs, which serve as ground truth for benchmarking.
- Core assumption: For games with n ≤ 16 players, exact computation is feasible and provides a reliable benchmark for approximation methods.
- Evidence anchors:
  - [section 3.1]: "The shapiq.ExactComputer class provides an interface for computing 18 interaction indices and game-theoretic concepts, including the MIs."
  - [section 4]: "For all games that include n ≤ 16 players, the value functions have been pre-computed by evaluating all coalitions and storing the games to file."
- Break condition: For games with n > 16 players, exact computation becomes computationally prohibitive, limiting the benchmarking capability.

### Mechanism 3
- Claim: The package facilitates research on SIs by providing a comprehensive benchmarking suite across multiple domains.
- Mechanism: The package includes a benchmarking suite containing 11 machine learning applications of SIs with pre-computed games and ground-truth values, enabling systematic evaluation of approximation methods.
- Core assumption: A diverse set of benchmark games across different domains allows for comprehensive evaluation of approximation methods and identification of domain-specific strengths.
- Evidence anchors:
  - [abstract]: "Moreover, it includes a benchmarking suite containing 11 machine learning applications of SIs with pre-computed games and ground-truth values to systematically assess computational performance across domains."
  - [section 4]: "To illustrate its versatility, we conduct benchmarks across a wide variety of traditional ML-based SV application scenarios."
- Break condition: If the benchmark suite does not cover a relevant domain, the performance of approximation methods in that domain may not be accurately assessed.

## Foundational Learning

- Concept: Shapley Values and Shapley Interactions
  - Why needed here: These are the core concepts that the package implements and enables computation for, forming the basis for understanding feature interactions and attributions in machine learning models.
  - Quick check question: Can you explain the difference between a Shapley Value and a Shapley Interaction in the context of feature attribution?

- Concept: Cooperative Game Theory
  - Why needed here: The package is rooted in cooperative game theory, using concepts like coalitions and value functions to quantify the contributions of entities in machine learning tasks.
  - Quick check question: How does the concept of a coalition in cooperative game theory relate to a group of features in a machine learning model?

- Concept: Approximation Methods for SVs and SIs
  - Why needed here: The package implements various approximation methods to efficiently compute SVs and SIs, which is crucial for handling the exponential complexity of exact computation.
  - Quick check question: What are the main types of approximation methods implemented in the package, and how do they differ in their approach?

## Architecture Onboarding

- Component map: Load pre-computed game -> Select appropriate approximator -> Run approximation -> Visualize results
- Critical path: Load pre-computed game -> Select appropriate approximator -> Run approximation -> Visualize results
- Design tradeoffs:
  - Exact vs. approximate computation: Exact computation provides ground truth but is limited to smaller games; approximation methods are efficient but introduce error.
  - Domain-specific vs. general methods: Domain-specific methods may be more accurate but less flexible; general methods are more widely applicable but may be less optimal for specific tasks.
- Failure signatures:
  - High approximation error: Indicates that the chosen method may not be suitable for the specific game or domain.
  - Slow computation: May suggest that the game size is too large for exact computation or that the approximation method is inefficient for the given task.
- First 3 experiments:
  1. Run a simple benchmark game (e.g., Sum of Unanimity Model) with a few different approximation methods to compare their performance.
  2. Use the TreeSHAP-IQ explainer to explain feature interactions in a small decision tree model and visualize the results.
  3. Implement a new cooperative game and use the ExactComputer to compute exact SIs for benchmarking approximation methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do higher-order Shapley Interactions (SIs) affect the interpretability of complex machine learning models in practice?
- Basis in paper: [explicit] The paper discusses the potential of SIs to improve interpretability by capturing joint contributions of feature groups, with results showing that pairwise interactions significantly enhance faithfulness over standard Shapley Values.
- Why unresolved: While the paper demonstrates improved faithfulness metrics, it does not explore how these interactions are perceived by end-users or how they impact decision-making in real-world scenarios.
- What evidence would resolve it: User studies or case studies where SIs are applied to real-world models, measuring interpretability gains and user comprehension.

### Open Question 2
- Question: Which approximation methods for Shapley Interactions are most effective across different machine learning domains?
- Basis in paper: [explicit] The paper benchmarks various approximation methods, showing that kernel-based approaches excel in local explanations, while stratified-sampling methods perform best in data valuation tasks.
- Why unresolved: The paper provides domain-specific performance insights but does not establish a universal method or framework for selecting the best approximation technique across diverse applications.
- What evidence would resolve it: A comprehensive study comparing approximation methods across a broader range of domains and model types, with clear guidelines for method selection.

### Open Question 3
- Question: How can Shapley Interactions be visualized effectively for higher-order feature interactions?
- Basis in paper: [inferred] The paper introduces visualization tools for SIs but acknowledges the challenge of visualizing higher-order interactions, suggesting it as a potential research direction.
- Why unresolved: The paper does not provide detailed solutions or evaluations for visualizing complex interactions, leaving this as an open area for exploration.
- What evidence would resolve it: Development and testing of innovative visualization techniques specifically designed for higher-order interactions, with user feedback on their effectiveness.

## Limitations

- Exact computation of Shapley Interactions is limited to games with n ≤ 16 players due to exponential complexity
- The effectiveness of approximation methods varies significantly across different machine learning domains
- The benchmarking suite, while comprehensive, may not cover all relevant application domains

## Confidence

High: The package's ability to efficiently approximate SIs and its value as a research tool
Medium: The claim that SIs consistently improve faithfulness over SHAP across all domains
Medium: The assertion that kernel-based approaches excel in local explanations while stratified-sampling methods perform best in data valuation

## Next Checks

1. Evaluate the performance of approximation methods on a new domain not represented in the benchmark suite to assess generalizability beyond the provided applications.

2. Conduct ablation studies on the TreeSHAP-IQ explainer to quantify the specific contribution of the interaction component to overall explanation quality.

3. Test the scalability limits of the ExactComputer by attempting to compute exact SIs for games with n > 16 players to determine practical size constraints.