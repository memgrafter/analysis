---
ver: rpa2
title: Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
arxiv_id: '2402.08277'
source_url: https://arxiv.org/abs/2402.08277
tags:
- data
- quality
- source
- sources
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a synthetic data generation pipeline with
  automated quality filters to create high-quality datasets for fine-tuning LLMs in
  Evidence-Based Question Answering (QA). The authors propose four test sets to benchmark
  in- and out-of-distribution performance and show that data quality, significantly
  improved by the proposed quality filters, matters more than quantity for Evidence-Based
  QA fine-tuning.
---

# Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering

## Quick Facts
- arXiv ID: 2402.08277
- Source URL: https://arxiv.org/abs/2402.08277
- Reference count: 29
- This paper introduces a synthetic data generation pipeline with automated quality filters to create high-quality datasets for fine-tuning LLMs in Evidence-Based Question Answering (QA).

## Executive Summary
This paper addresses the challenge of fine-tuning large language models for Evidence-Based Question Answering by introducing a synthetic data generation pipeline with automated quality filters. The authors demonstrate that data quality, significantly improved by their proposed quality filters, matters more than quantity for Evidence-Based QA fine-tuning. They create synthetic datasets (SYNSCIQA, SYNSCIQA+, SYNSCIQA++) and propose four test sets to benchmark in- and out-of-distribution performance, showing that fine-tuning on synthetic data improves performance on both types of test sets.

## Method Summary
The authors generate synthetic datasets using OpenAI LLMs (GPT-3.5, GPT-4) to create instruction-answer pairs for Evidence-Based QA. They apply quality filters including source quality and answer attributability to create progressively higher quality datasets (SYNSCIQA, SYNSCIQA+, SYNSCIQA++). They fine-tune Llama-2-13b-chat and Zephyr-7b-β using QLoRA on these synthetic datasets for 5 epochs, evaluating performance on in-domain test sets and three out-of-distribution test sets (GENSEARCH, CHATREPORT, CLIMATEQA) using source quality and attributability scores.

## Key Results
- Data quality improvements from automated filters matter more than quantity for Evidence-Based QA fine-tuning
- Fine-tuning on synthetic data improves performance on both in- and out-of-distribution test sets
- In-domain synthetic data performance substantially correlates with out-of-distribution performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quality filtering significantly improves fine-tuning performance for Evidence-Based QA
- Mechanism: Automated filters remove low-quality data points by enforcing source quality and answer attributability criteria, leaving only data where citations are correct and answers are fully entailed by sources
- Core assumption: NLI models can accurately judge sentence-source entailment for Evidence-Based QA
- Evidence anchors:
  - [abstract]: "we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA"
  - [section 3.2]: "we apply a source quality filter to the original SYNSCIQA" and "we also apply the answer attributability quality dimension as a filter"
  - [corpus]: Weak - no direct corpus evidence of filter effectiveness
- Break condition: NLI models fail to accurately judge entailment, causing incorrect data to pass filters

### Mechanism 2
- Claim: Fine-tuning on synthetic data transfers positively to real-world RAG applications
- Mechanism: Synthetic data generation pipeline creates diverse, task-specific instruction-answer pairs that capture the core Evidence-Based QA task distribution, allowing models to learn the required quality dimensions
- Core assumption: Synthetic data distribution approximates real-world RAG data distribution sufficiently for transfer learning
- Evidence anchors:
  - [abstract]: "fine-tuning on synthetic data improves performance on both in- and out-of-distribution"
  - [section 5.2]: "fine-tuning always lead to better sourcing and attribution performance than original LLMs on in-domain and out-of-distribution test sets"
  - [corpus]: Moderate - synthetic data shows transfer to three different real-world test sets
- Break condition: Real-world data distribution differs too much from synthetic data for effective transfer

### Mechanism 3
- Claim: In-domain synthetic data performance correlates strongly with out-of-distribution performance
- Mechanism: The synthetic data captures the core Evidence-Based QA task structure, making in-domain performance a reliable indicator of generalization ability
- Core assumption: Performance correlation between in-domain and out-of-distribution data indicates valid development set utility
- Evidence anchors:
  - [abstract]: "performance scores on in-domain test set substantially indicate out-of-distribution performance"
  - [section 5.2]: "the performance on synthetic data has a strong correlation with OOD performance"
  - [corpus]: Moderate - correlation observed across multiple test sets with varying distribution distances
- Break condition: Correlation breaks down for test sets with very different distributions from synthetic data

## Foundational Learning

- Concept: Quality dimensions in Evidence-Based QA
  - Why needed here: The entire fine-tuning approach depends on correctly identifying and enforcing quality dimensions
  - Quick check question: What are the three quality dimensions defined for Evidence-Based QA in this paper?

- Concept: Synthetic data generation pipeline
  - Why needed here: Understanding how the pipeline creates diverse, high-quality training data is crucial for implementation
  - Quick check question: What are the key steps in the synthetic data generation pipeline described in section 3.1?

- Concept: NLI models for attributability scoring
  - Why needed here: The attributability score relies on NLI model predictions to judge sentence-source entailment
  - Quick check question: Which NLI models are used for attributability scoring and how are their predictions aggregated?

## Architecture Onboarding

- Component map: Synthetic data generation → Quality filters → Fine-tuning → Evaluation on multiple test sets (in-domain + OOD)
- Critical path: Generate synthetic data → Apply quality filters → Fine-tune model → Evaluate on in-domain test → Select checkpoint → Evaluate on OOD test sets
- Design tradeoffs: Higher quality data vs. larger quantity of lower quality data; more epochs vs. overfitting risk
- Failure signatures: Poor source quality scores despite high attributability; strong in-domain performance but weak OOD performance; overfitting indicated by negative correlation between performance and epoch number
- First 3 experiments:
  1. Fine-tune on SYNSCIQA vs. SYNSCIQA++ and compare source quality scores
  2. Compare performance of different epoch checkpoints on in-domain vs. OOD test sets
  3. Validate attributability score by comparing against human/GPT-4 annotations on sampled data points

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of synthetic data impact the generalizability of fine-tuned models on out-of-distribution tasks?
- Basis in paper: [explicit] The paper discusses the importance of data quality over quantity in fine-tuning models for Evidence-Based QA and explores the impact of different quality datasets on model performance.
- Why unresolved: While the paper shows that higher quality synthetic data leads to better performance, it does not fully explore how these improvements translate to out-of-distribution tasks.
- What evidence would resolve it: Comparative studies of model performance on a wide range of out-of-distribution tasks using datasets of varying quality would provide clearer insights.

### Open Question 2
- Question: Can the proposed data generation pipeline be adapted to other domains beyond scientific question answering?
- Basis in paper: [inferred] The paper introduces a data generation pipeline that synthesizes high-quality datasets for Evidence-Based QA, suggesting potential adaptability to other domains.
- Why unresolved: The paper focuses on scientific question answering, and there is no direct evidence or experimentation with other domains.
- What evidence would resolve it: Implementing the pipeline in different domains and evaluating the quality and performance of the generated datasets would demonstrate its adaptability.

### Open Question 3
- Question: What are the long-term effects of fine-tuning on synthetic data on the model's ability to handle real-world, dynamic information?
- Basis in paper: [inferred] The paper explores the effects of fine-tuning on synthetic data and its impact on model performance, but does not address long-term effects or dynamic information handling.
- Why unresolved: The study focuses on immediate performance improvements and does not consider the sustainability of these improvements over time or with changing information.
- What evidence would resolve it: Longitudinal studies tracking model performance over time and across evolving datasets would provide insights into long-term effects.

## Limitations

- The effectiveness of automated quality filters depends on the reliability of NLI models for judging sentence-source entailment
- Limited direct validation of the attributability scoring mechanism against human annotations
- Correlation between in-domain and out-of-distribution performance needs further validation across a broader range of test sets

## Confidence

- **High Confidence**: Quality filtering significantly improves fine-tuning performance - Supported by direct comparisons showing SYNSCIQA++ outperforms SYNSCIQA in source quality scores
- **Medium Confidence**: Synthetic data fine-tuning transfers to real-world RAG applications - Supported by positive results on three OOD test sets, but transfer effectiveness varies by test set
- **Medium Confidence**: In-domain performance correlates with out-of-distribution performance - Observed correlation is strong but based on limited test sets

## Next Checks

1. **Validate attributability scoring mechanism**: Manually annotate 100 random data points from SYNSCIQA++ using human annotators to verify the accuracy of NLI-based attributability scores and assess filter effectiveness

2. **Expand OOD test set coverage**: Evaluate fine-tuned models on additional RAG datasets with different domains and question types to verify the generalizability of the in-domain performance correlation

3. **Ablation study on filter combinations**: Systematically test different combinations of quality filters (source quality, attributability, answer relevance) to determine which dimensions contribute most to performance improvements