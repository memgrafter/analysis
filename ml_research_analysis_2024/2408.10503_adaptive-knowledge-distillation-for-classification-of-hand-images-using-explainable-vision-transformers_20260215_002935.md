---
ver: rpa2
title: Adaptive Knowledge Distillation for Classification of Hand Images using Explainable
  Vision Transformers
arxiv_id: '2408.10503'
source_url: https://arxiv.org/abs/2408.10503
tags:
- images
- hand
- domain
- methods
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of vision transformers (ViTs) for
  classification of hand images. We use explainability tools to explore the internal
  representations of ViTs and assess their impact on the model outputs.
---

# Adaptive Knowledge Distillation for Classification of Hand Images using Explainable Vision Transformers

## Quick Facts
- arXiv ID: 2408.10503
- Source URL: https://arxiv.org/abs/2408.10503
- Authors: Thanh Thi Nguyen; Campbell Wilson; Janis Dalins
- Reference count: 39
- Primary result: Vision transformers significantly outperform traditional methods for hand image classification, and adaptive knowledge distillation prevents catastrophic forgetting during domain adaptation

## Executive Summary
This paper investigates vision transformers (ViTs) for hand image classification and introduces adaptive knowledge distillation methods to prevent catastrophic forgetting during domain adaptation. The authors use explainability tools like Grad-CAM and deep feature factorization to analyze ViT internal representations and their impact on outputs. Two adaptive distillation methods are proposed that allow a student model to retain source domain knowledge while adapting to new domains. The experimental results demonstrate that ViTs outperform traditional machine learning methods and that the proposed approaches achieve excellent performance on both source and target domains, particularly when domains are dissimilar.

## Method Summary
The paper fine-tunes six ViT variants on two public hand image datasets (IIT Delhi palm prints and 11k hands dataset) using standard hyperparameters. Two adaptive knowledge distillation methods are introduced: method 1 uses a weighted loss function that emphasizes teacher imitation early in training and shifts to learning the new domain later, while method 2 adds imitation of specific teacher internal states (Norm and final linear layer of 11th encoder) using cosine embedding loss. An ensemble of ViT models serves as the teacher. The methods are evaluated through domain adaptation experiments comparing palm to dorsal hand classification performance.

## Key Results
- ViT models significantly outperform traditional machine learning methods on hand image classification tasks
- Adaptive distillation methods prevent catastrophic forgetting, maintaining source domain performance while adapting to new domains
- The proposed approaches achieve excellent performance particularly when source and target domains exhibit significant dissimilarity

## Why This Works (Mechanism)

### Mechanism 1
Adaptive knowledge distillation allows a student model to retain source domain knowledge while adapting to a new domain without access to source domain data. The distillation process uses a weighted loss function that emphasizes imitating the teacher model's outputs (soft loss) at the beginning of training and gradually shifts focus to learning the new domain (hard loss) as training progresses. The weight on the soft loss is scaled by the epoch number over total epochs, allowing the student to gradually deviate from the teacher's behavior.

### Mechanism 2
Imitating the teacher's internal states, specifically the Norm component and final linear component of the 11th encoder layer, helps the student model retain source domain knowledge more effectively than just imitating outputs. The student model learns to replicate the teacher's internal representations using cosine embedding loss between the teacher's and student's internal states, in addition to the standard distillation loss.

### Mechanism 3
Using an ensemble of multiple ViT models as the teacher provides more stable and sometimes better distillation than using a single model. The soft voting ensemble aggregates the pseudo-probabilities from multiple ViT variants, creating a more robust teacher that the student can learn from. This ensemble approach helps mitigate individual model biases and errors.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: The paper addresses the problem of catastrophic forgetting when adapting a model to a new domain without access to source domain data
  - Quick check question: What happens to a neural network's performance on the original task when it's fine-tuned on a new task without any mechanism to preserve old knowledge?

- Concept: Vision transformers (ViTs)
  - Why needed here: The paper uses ViTs as both the teacher and student models for the classification task
  - Quick check question: How do vision transformers differ from convolutional neural networks in terms of their architecture and how they process image data?

- Concept: Knowledge distillation
  - Why needed here: The paper proposes adaptive knowledge distillation methods to transfer knowledge from a teacher model to a student model during domain adaptation
  - Quick check question: What is the primary goal of knowledge distillation, and how does it differ from traditional supervised learning?

## Architecture Onboarding

- Component map: Teacher models (ensemble of 6 ViT variants or student copy) -> Distillation module (soft loss, hard loss, cosine embedding loss) -> Student model (Google ViT) -> Classification output
- Critical path: 1) Train teacher models on source domain data, 2) Initialize student model with source domain knowledge, 3) Compute distillation losses (soft, hard, and cosine embedding), 4) Update student model parameters using the weighted loss function, 5) Evaluate student performance on both source and target domains
- Design tradeoffs: Using an ensemble teacher provides more stable guidance but loses the ability to imitate internal states (method 2). The adaptive weighting scheme (method 1) helps prevent catastrophic forgetting but requires careful tuning of the temperature parameter and epoch scaling
- Failure signatures: 1) Poor performance on source domain after adaptation (catastrophic forgetting), 2) Poor performance on target domain (inadequate adaptation), 3) Unstable training (improper loss weighting or temperature parameter)
- First 3 experiments:
  1. Train the student model on the source domain (left-palm images) and evaluate its performance on the same domain to establish a baseline
  2. Transfer the student model to the target domain (right-palm images) without any distillation and measure the accuracy drop on the source domain to assess catastrophic forgetting
  3. Apply method 1 (adaptive distillation with ensemble teacher) and compare the student's performance on both source and target domains to the baseline without distillation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance characteristics of the proposed adaptive distillation methods compare when applied to other biometric modalities such as fingerprint and iris recognition?
- Basis in paper: [explicit] The paper mentions this as a potential future work direction
- Why unresolved: The current study only evaluated the methods on hand image datasets
- What evidence would resolve it: Experimental results comparing the adaptive distillation methods on fingerprint and iris recognition datasets

### Open Question 2
- Question: What are the long-term effects of using the proposed distillation methods on model performance and stability when continuously adapting to new domains over extended periods?
- Basis in paper: [inferred] The paper focuses on domain adaptation for two specific datasets but does not address the long-term implications of continuous adaptation
- Why unresolved: The study's scope is limited to a few adaptation scenarios
- What evidence would resolve it: Longitudinal studies tracking model performance across multiple domain adaptation cycles

### Open Question 3
- Question: How do the explainability insights gained from Grad-CAM and DFF tools influence the design and optimization of future vision transformer architectures for biometric applications?
- Basis in paper: [explicit] The paper utilizes Grad-CAM and DFF for explainability
- Why unresolved: While the paper demonstrates the utility of these tools, it does not explore how these insights could be systematically incorporated into the design of more effective or interpretable ViT architectures
- What evidence would resolve it: Comparative studies of ViT architectures with and without design modifications informed by explainability insights

## Limitations

- The explainability analysis is primarily qualitative without quantitative metrics for explainability
- The choice of internal components (Norm and final linear layer of 11th encoder) for imitation appears somewhat arbitrary without clear justification
- Experimental setup focuses heavily on palm vs dorsal hand classification, potentially limiting generalizability to other domain adaptation scenarios

## Confidence

- High confidence: ViT models significantly outperform traditional machine learning methods on hand image classification tasks
- Medium confidence: The proposed adaptive distillation methods effectively prevent catastrophic forgetting during domain adaptation
- Low confidence: The specific internal components chosen for imitation in method 2 are optimal for knowledge transfer

## Next Checks

1. Conduct ablation studies to determine whether the specific choice of Norm and final linear layer of the 11th encoder is optimal for imitation, or if other internal components could yield better results

2. Extend experiments to include other types of domain shifts beyond palm vs dorsal hand classification to test the generalizability of the adaptive distillation methods

3. Implement quantitative metrics for explainability to complement the qualitative Grad-CAM and deep feature factorization visualizations, providing more objective measures of model interpretability