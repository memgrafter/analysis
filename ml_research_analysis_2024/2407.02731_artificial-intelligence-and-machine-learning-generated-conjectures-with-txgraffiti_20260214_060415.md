---
ver: rpa2
title: Artificial intelligence and machine learning generated conjectures with TxGraffiti
arxiv_id: '2407.02731'
source_url: https://arxiv.org/abs/2407.02731
tags:
- conjectures
- conjecture
- txgraffiti
- graphs
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents TxGraffiti, a machine learning-based artificial
  intelligence designed to automate the generation of mathematical conjectures, particularly
  in graph theory. The system uses a combination of linear optimization and heuristic
  filtering to produce novel conjectures.
---

# Artificial intelligence and machine learning generated conjectures with TxGraffiti

## Quick Facts
- arXiv ID: 2407.02731
- Source URL: https://arxiv.org/abs/2407.02731
- Reference count: 35
- Primary result: Machine learning system that automatically generates novel mathematical conjectures in graph theory using linear optimization and heuristic filtering

## Executive Summary
TxGraffiti is an AI system that uses machine learning and linear optimization to automatically generate mathematical conjectures, particularly in graph theory. The system works by creating a database of precomputed numerical and Boolean properties of mathematical objects, then applying supervised learning techniques to find optimal bounding functions relating these properties. Two main heuristics, Theo and Dalmation-static, filter the generated conjectures to ensure they are both strong and novel. The system has successfully generated conjectures that have led to publications in reputable mathematical journals.

## Method Summary
TxGraffiti generates mathematical conjectures by first creating a database of mathematical objects (such as graphs) and computing numerical and Boolean properties for each object. It then uses linear optimization to find optimal bounding functions relating these properties, generating candidate conjectures. The Theo heuristic filters conjectures by keeping only the most general version when redundant inequalities appear, while the Dalmation-static heuristic removes conjectures that don't add new equality instances to the knowledge base. This two-stage filtering process ensures the final conjectures are both strong (hold with equality on many instances) and novel (not previously known).

## Key Results
- Successfully generated numerous conjectures that led to publications in mathematical journals
- Demonstrated ability to find non-trivial relationships between graph invariants using linear optimization
- Proved Theorem 5 relating independence number and matching number for regular graphs through generated conjectures
- Validated approach works for generating conjectures in graph theory with database sizes ranging from hundreds to thousands of objects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TxGraffiti uses a machine learning-based linear optimization to discover optimal bounding functions relating graph invariants.
- Mechanism: The system maps each graph to a set of precomputed numerical and Boolean properties, then applies supervised learning (linear programming) to find functions f(m,b) that maximize equality cases between properties.
- Core assumption: Linear relationships between graph invariants are sufficient to capture meaningful mathematical conjectures.
- Evidence anchors:
  - [abstract]: "uses a combination of linear optimization and heuristic filtering to produce novel conjectures"
  - [section 3.2]: "TxGraffiti employs a machine learning, data-driven approach using linear optimization methods... to find optimal parameters m, b ∈ R"
  - [corpus]: Weak - corpus doesn't discuss the linear optimization mechanism specifically
- Break condition: If graph invariants require non-linear relationships or higher-order polynomial constraints, the linear optimization approach would miss these conjectures.

### Mechanism 2
- Claim: The Theo heuristic ensures conjecture generality by filtering out redundant inequalities.
- Mechanism: When the same inequality appears in multiple conjectures, Theo keeps only the most general hypothesis (removing more specific versions).
- Core assumption: More general conjectures are more valuable than specific ones, even if the specific ones have more equality cases.
- Evidence anchors:
  - [section 3.3]: "The Theo heuristic checks if any proposed inequality relation appears more than once in the list of conjectures, and then only selects the proposed conjectures with this inequality that have the most general hypothesis statement"
  - [abstract]: "Theo and Dalmation heuristics... to filter and refine the conjectures, ensuring they are both strong and novel"
  - [corpus]: Weak - corpus doesn't discuss the Theo heuristic specifically
- Break condition: If specific conjectures with strong equality cases are actually more valuable than general ones, Theo would incorrectly filter them out.

### Mechanism 3
- Claim: The Dalmation-static heuristic ensures conjectures provide "new information" by checking for unique equality instances.
- Mechanism: For each conjecture, if no graph achieves equality that hasn't already been covered by previous conjectures, the conjecture is removed.
- Core assumption: Conjectures that don't add new equality instances to the knowledge base are less valuable than those that do.
- Evidence anchors:
  - [section 3.3]: "Dalmation-static heuristic... if the set of graphs attaining equality in Conjecture i does not contain a which is also graph not contained in G, then remove Conjecture i"
  - [abstract]: "The system employs two main heuristics, Theo and Dalmation-static, to filter and refine the conjectures"
  - [corpus]: Weak - corpus doesn't discuss Dalmation-static specifically
- Break condition: If conjectures with overlapping equality instances are still mathematically valuable, Dalmation-static would incorrectly filter them out.

## Foundational Learning

- Concept: Linear optimization and linear programming
  - Why needed here: TxGraffiti uses linear programming to find optimal bounding functions between graph invariants
  - Quick check question: What is the difference between linear programming and linear optimization?

- Concept: Graph theory fundamentals (independence number, matching number, regular graphs)
  - Why needed here: The system generates conjectures about graph invariants like α(G) and µ(G), and Theorem 5 relates these for regular graphs
  - Quick check question: What is the relationship between independence number and matching number in a bipartite graph?

- Concept: Heuristics and filtering in AI systems
  - Why needed here: Theo and Dalmation-static are heuristics that filter the conjecture list based on generality and uniqueness
  - Quick check question: What is the difference between a heuristic and an algorithm in AI?

## Architecture Onboarding

- Component map: Database of graphs → Feature generation (numerical/Boolean properties) → Linear optimization (find bounding functions) → Sorting by touch number → Theo filtering → Dalmation-static filtering → Output conjectures
- Critical path: Feature generation → Linear optimization → Theo filtering → Dalmation-static filtering
- Design tradeoffs: Linear optimization is computationally efficient but may miss non-linear relationships; heuristics reduce output but may filter valuable conjectures
- Failure signatures: If conjectures are too obvious (e.g., α(G) ≤ n(G)), check feature generation quality; if no conjectures are produced, check linear optimization constraints; if conjectures are redundant, check Theo filtering
- First 3 experiments:
  1. Run TxGraffiti on a small database of 10-20 well-known graphs with obvious relationships to verify basic functionality
  2. Modify feature generation to include a known non-linear relationship and observe if TxGraffiti captures it
  3. Disable Theo filtering temporarily to see if more specific conjectures with high equality cases are valuable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can TxGraffiti's machine learning approach be extended to generate conjectures in mathematical domains beyond graph theory?
- Basis in paper: [explicit] The paper states that TxGraffiti's methodology could be applied to different types of data, provided a table of numerical and Boolean properties is generated.
- Why unresolved: The paper only demonstrates TxGraffiti's application to graph theory. Extending the approach to other mathematical domains requires further development and validation.
- What evidence would resolve it: Successful generation of novel and meaningful conjectures in at least two other mathematical domains (e.g., number theory, algebra) using TxGraffiti's methodology.

### Open Question 2
- Question: How does the quality of the database (e.g., number of objects, selection of special cases) impact the strength and novelty of conjectures generated by TxGraffiti?
- Basis in paper: [explicit] The paper emphasizes the importance of data quality and mentions using databases of several hundred objects, but also experimenting with thousands.
- Why unresolved: The paper does not provide a detailed analysis of how database size and composition affect conjecture quality. Optimal database design for maximum conjecture generation remains unclear.
- What evidence would resolve it: A systematic study comparing conjecture quality across databases of varying sizes and compositions, identifying the optimal balance between database size and conjecture strength.

### Open Question 3
- Question: Can TxGraffiti's heuristics (Theo and Dalmation-static) be further refined or combined with other methods to improve the filtering of conjectures?
- Basis in paper: [inferred] The paper describes the current heuristics but does not explore potential improvements or alternative approaches to conjecture filtering.
- Why unresolved: The effectiveness of the current heuristics is demonstrated, but their limitations and potential for enhancement are not fully explored. Other filtering methods or combinations thereof may yield better results.
- What evidence would resolve it: Comparative studies evaluating the performance of refined or alternative filtering methods against the current heuristics, measuring improvements in conjecture quality and reduction of false positives.

### Open Question 4
- Question: How can TxGraffiti's conjecture generation process be made more transparent and interpretable to mathematicians?
- Basis in paper: [inferred] The paper describes the technical aspects of TxGraffiti but does not address how mathematicians can understand and trust the generated conjectures.
- Why unresolved: The inner workings of TxGraffiti, particularly its machine learning components, may be opaque to users. Enhancing interpretability is crucial for adoption and trust in the system's outputs.
- What evidence would resolve it: Development of visualization tools or explanatory models that clearly illustrate how TxGraffiti arrives at its conjectures, enabling mathematicians to understand and verify the reasoning process.

## Limitations

- Linear optimization may miss important non-linear relationships between graph invariants
- Heuristic filtering may systematically bias conjecture selection based on subjective criteria
- Conjecture novelty claims are difficult to verify without comprehensive literature review

## Confidence

- **High confidence**: Basic functionality of TxGraffiti as a conjecture generation system using linear optimization and heuristic filtering
- **Medium confidence**: Effectiveness of Theo and Dalmation-static heuristics in producing high-quality conjectures
- **Low confidence**: Claims about the novelty of all generated conjectures

## Next Checks

1. **Database composition analysis**: Systematically vary the composition of the underlying graph database (e.g., include only bipartite graphs, only regular graphs, only trees) and measure how this affects the types and quality of generated conjectures.

2. **Heuristic ablation study**: Run TxGraffiti with individual heuristics disabled (Theo only, Dalmation-static only, no heuristics) and compare the resulting conjecture sets.

3. **Non-linear extension experiment**: Modify the optimization framework to include simple non-linear terms (quadratic, logarithmic) and compare the resulting conjectures with those from the linear-only version.