---
ver: rpa2
title: Deep deterministic policy gradient with symmetric data augmentation for lateral
  attitude tracking control of a fixed-wing aircraft
arxiv_id: '2407.11077'
source_url: https://arxiv.org/abs/2407.11077
tags:
- policy
- target
- symmetric
- symmetry
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops two symmetry-integrated Deep Deterministic
  Policy Gradient (DDPG) algorithms for aircraft lateral attitude tracking control.
  The first algorithm, DDPG with Symmetric Data Augmentation (DDPG-SDA), leverages
  system symmetry to augment state transition samples, effectively doubling the dataset
  size.
---

# Deep deterministic policy gradient with symmetric data augmentation for lateral attitude tracking control of a fixed-wing aircraft

## Quick Facts
- arXiv ID: 2407.11077
- Source URL: https://arxiv.org/abs/2407.11077
- Reference count: 40
- Primary result: DDPG-SCA achieves fastest convergence (14.212 avg gradient in first 500 episodes) and best tracking (IAEM 1.044 for roll, 0.439 for yaw rate)

## Executive Summary
This paper addresses aircraft lateral attitude tracking control using reinforcement learning by exploiting system symmetry. Two novel DDPG algorithms are developed: DDPG-SDA leverages symmetric data augmentation to double the effective dataset size, while DDPG-SCA introduces a two-critic structure with two-step approximate policy iteration. The approach is validated on a linearized aircraft lateral model, demonstrating improved convergence speed and tracking performance compared to standard DDPG.

## Method Summary
The method involves two symmetry-integrated DDPG algorithms for aircraft lateral attitude tracking. DDPG-SDA augments state transition samples using system symmetry to double dataset size without extra exploration. DDPG-SCA further improves sample utilization by employing a two-critic structure with separate training on original and augmented samples, using a two-step approximate policy iteration method. Both algorithms use a two-hidden-layer actor network with tanh-ReLU activation functions.

## Key Results
- DDPG-SCA achieves fastest convergence with average gradient of 14.212 in first 500 episodes
- Best tracking performance with IAEM of 1.044 for roll angle and 0.439 for yaw rate
- Symmetric data augmentation effectively doubles dataset size without additional exploration
- Two-critic structure provides more efficient sample utilization than single-critic approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Symmetric data augmentation doubles the effective dataset size without extra exploration.
- **Mechanism:** For each explored transition (x_t, a_t, x_{t+1}), a symmetric counterpart (x'_t, a'_t, x'_{t+1}) is computed by reflecting state and negating action, then exploiting the dynamical system symmetry to derive the next state.
- **Core assumption:** The underlying system satisfies the symmetry conditions in Theorem 1 (F and G are identical for symmetric states, and either the reference point is zero or the dynamics matrix F is identity).
- **Evidence anchors:**
  - [abstract]: "leverages environment symmetry to augment explored transition samples"
  - [section]: Theorem 1 and equations (19)-(21) derive the symmetry conditions
  - [corpus]: Weak - related work discusses symmetry in ILPs and robotics but not the exact augmentation formula used here
- **Break condition:** If the system's F or G functions differ between symmetric states, the augmentation will generate invalid next states, corrupting the dataset.

### Mechanism 2
- **Claim:** Two-critic structure improves sample utilization efficiency.
- **Mechanism:** Critic 1 is trained on original explored samples, Critic 2 on symmetric augmented samples; both are used in alternating two-step policy iteration to update the actor, so each critic specializes on a complementary data partition.
- **Core assumption:** Separate training on augmented vs. original data yields more accurate Q-value estimates for each region, improving policy updates.
- **Evidence anchors:**
  - [abstract]: "introducing a second critic trained on the augmented samples, resulting in a dual-critic structure"
  - [section]: "two-step approximate policy iteration method" section describes the alternation
  - [corpus]: Weak - no direct corpus evidence of dual-critic training with symmetric data augmentation
- **Break condition:** If the augmented data distribution is too different from explored data, the two critics may learn conflicting Q-value estimates, destabilizing training.

### Mechanism 3
- **Claim:** "tanh-ReLU" activation improves actor stability in symmetric control.
- **Mechanism:** By combining tanh and ReLU in hidden layers, the actor network can model both bounded symmetric control outputs and sparse activations, aligning with the symmetric action space.
- **Core assumption:** The symmetric action space (a_t = -a'_t) benefits from activation functions that respect sign symmetry.
- **Evidence anchors:**
  - [section]: "To reduce training complexity, we propose ’tanh-ReLU’ activation functions for two-hidden-layer actor network in DDPG-SCA algorithm"
  - [corpus]: Weak - no corpus evidence or analysis of this specific activation choice
- **Break condition:** If the tanh-ReLU combination causes gradient vanishing or exploding in deeper layers, actor training could stall or diverge.

## Foundational Learning

- **Concept:** Symmetry in dynamical systems (Theorem 1)
  - Why needed here: The entire augmentation strategy hinges on mathematically valid symmetry conditions for state transitions.
  - Quick check question: For a symmetric system with reference x* = 0, what must be true about F(xt) and G(xt) for symmetric states?
- **Concept:** Two-step approximate policy iteration
  - Why needed here: Enables separate training of two critics without mixing their data, avoiding the slowdown seen in single-critic mixed batches.
  - Quick check question: In the two-step update, which critic is used to evaluate the actor in the second step?
- **Concept:** Replay buffer management and batch sampling
  - Why needed here: Augmented samples must be stored separately to preserve the dual-critic training structure; sampling strategy affects convergence speed.
  - Quick check question: What sampling strategy ensures each critic gets only its designated data type during training?

## Architecture Onboarding

- **Component map:** Environment (aircraft lateral dynamics) -> Agent (DDPG with two critics ψ1, ψ2 and actor ϑ) -> Buffers (D1 for explored samples, D2 for symmetric augmented samples) -> Networks (target critics and actor for delayed updates)
- **Critical path:**
  1. Collect (x_t, a_t, r_t, x_{t+1}) from environment
  2. Compute symmetric sample (x'_t, -a_t, r_t, x'_{t+1}) using symmetry
  3. Store in D1 and D2 respectively
  4. Sample batches from D1 → update ψ1 and ϑ
  5. Sample batches from D2 → update ψ2 and ϑ
  6. Update target networks with τ
- **Design tradeoffs:**
  - Using two separate buffers doubles memory but keeps training efficient
  - Symmetric augmentation assumes exact symmetry; any model mismatch degrades performance
  - tanh-ReLU activation is a heuristic choice; may need tuning per system
- **Failure signatures:**
  - Critic loss spikes indicate invalid augmented samples
  - Actor gradient instability suggests activation mismatch or bad symmetry assumption
  - Slow convergence despite augmentation hints at poor replay buffer balance
- **First 3 experiments:**
  1. Run with DDPG-SDA (single buffer, mixed samples) to establish baseline
  2. Enable dual-buffer DDPG-SCA, monitor critic losses separately for D1 vs D2
  3. Test tanh-ReLU activation vs pure ReLU, compare actor stability and convergence speed

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Symmetry assumptions validity: The entire augmentation strategy depends on Theorem 1's symmetry conditions being exactly satisfied. While the aircraft lateral model may exhibit approximate symmetry, the paper doesn't validate this empirically or analyze sensitivity to symmetry violations.
- Activation function efficacy: The "tanh-ReLU" activation is presented as a novel contribution, but no ablation studies compare it against standard activations, and no theoretical justification is provided.
- Dual-critic benefit quantification: While DDPG-SCA shows faster convergence, the paper doesn't isolate the contribution of symmetric data augmentation versus the two-critic structure, making it unclear which mechanism drives the improvement.

## Confidence
- Symmetry assumptions validity: Medium
- Activation function efficacy: Low
- Dual-critic benefit quantification: Medium

## Next Checks
1. **Symmetry validation:** Verify the aircraft lateral dynamics model satisfies the symmetry conditions in Theorem 1 by computing F(x_t) and G(x_t) for symmetric state-action pairs across the operational envelope.
2. **Ablation study:** Implement a single-critic variant of DDPG-SCA that uses mixed batches (explored + augmented samples) to isolate the benefit of symmetric data augmentation from the dual-critic structure.
3. **Activation analysis:** Replace "tanh-ReLU" with standard ReLU and pure tanh in the actor network, comparing convergence speed and final tracking performance to quantify the activation choice's impact.