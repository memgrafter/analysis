---
ver: rpa2
title: 'Jill Watson: A Virtual Teaching Assistant powered by ChatGPT'
arxiv_id: '2405.11070'
source_url: https://arxiv.org/abs/2405.11070
tags:
- jill
- watson
- context
- chatgpt
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Jill Watson, a Virtual Teaching Assistant
  powered by ChatGPT that requires no prior training and uses a modular skill-based
  architecture to answer student questions based on course documents. The system addresses
  hallucination and safety concerns through document citation, textual entailment
  verification, and moderation filters.
---

# Jill Watson: A Virtual Teaching Assistant powered by ChatGPT

## Quick Facts
- arXiv ID: 2405.11070
- Source URL: https://arxiv.org/abs/2405.11070
- Authors: Karan Taneja; Pratyusha Maiti; Sandeep Kakar; Pranav Guruprasad; Sanjeev Rao; Ashok K. Goel
- Reference count: 31
- Key outcome: Jill Watson outperforms legacy knowledge-based Jill Watson and OpenAI Assistants in response quality while generating fewer potentially harmful or confusing answers

## Executive Summary
Jill Watson is a Virtual Teaching Assistant powered by ChatGPT that requires no prior training and uses a modular skill-based architecture to answer student questions based on course documents. The system addresses hallucination and safety concerns through document citation, textual entailment verification, and moderation filters. Jill Watson can process and converse using multiple large documents, making it well-suited for intelligent textbooks. The system achieves high response quality with minimal hallucinations and generates significantly fewer harmful or confusing answers compared to baseline approaches.

## Method Summary
Jill Watson uses a modular skill-based architecture inspired by XiaoIce, where incoming queries are routed to appropriate skills via a skill classifier. The system preprocesses course documents into passages with various representations and uses Dense Passage Retrieval (DPR) to find relevant content. ChatGPT generates responses based on retrieved passages, with prompts requiring document citations. The system employs coreference resolution, textual entailment verification for hallucination detection, and moderation filters to prevent unsafe content. No training is required as the system relies on prompting strategies and existing APIs.

## Key Results
- Jill Watson achieves significantly higher response quality than legacy knowledge-based Jill Watson and OpenAI Assistants
- The system generates fewer potentially harmful or confusing answers while maintaining strong performance on question answering
- Jill Watson successfully processes and answers questions based on multiple large documents, demonstrating suitability for intelligent textbooks

## Why This Works (Mechanism)

### Mechanism 1
The modular skill-based architecture allows easy integration of new capabilities and makes the system well-suited for intelligent textbooks. The system uses a skill classifier to route incoming queries to the appropriate skill (e.g., Contextual Answering, Greetings, Self-awareness) based on the query content and context. This modular design allows for independent development and testing of each skill.

### Mechanism 2
The system addresses hallucination and safety concerns through document citation, textual entailment verification, and moderation filters. The system prompts ChatGPT to cite the document and page number when using content from a text chunk. It also uses textual entailment to check if the context completely entails the generated answer, and applies moderation filters to detect and prevent unsafe content.

### Mechanism 3
The system can answer questions based on multiple large documents which makes it well-suited for intelligent textbooks. The system pre-processes course documents into passages with various representations (original text, heading, clean text, summary text, and context embeddings). It uses dense passage retrieval (DPR) to find the most relevant passages for a given query, and then prompts ChatGPT to generate an answer based on those passages.

## Foundational Learning

- Concept: Coreference resolution
  - Why needed here: To resolve references to entities or events in user queries based on the conversation context, allowing the system to construct complete queries for retrieval.
  - Quick check question: Given the context "John started reading a book" and the query "When did he start?", what is the resolved query after coreference resolution?

- Concept: Dense passage retrieval (DPR)
  - Why needed here: To find the most relevant passages in the pre-processed documents for a given query, based on the similarity between query embeddings and context embeddings.
  - Quick check question: What are the two main components of the DPR architecture, and how are they aligned during training?

- Concept: Textual entailment
  - Why needed here: To check if the context completely entails the generated answer, helping to detect and reduce hallucinations.
  - Quick check question: What is the entailment relation between text T and hypothesis H if H is a logical consequence of T?

## Architecture Onboarding

- Component map: Coreference Resolution -> Skill Classifier -> (Relevant Skill) -> Moderation Filter
- Critical path: The critical path for processing a user query is: Coreference Resolution -> Skill Classifier -> (Relevant Skill) -> Moderation Filter. The Coreference Resolution component resolves any references in the query, the Skill Classifier component categorizes the query into the appropriate skill, the relevant skill component generates the response, and the Moderation Filter component checks for unsafe content.
- Design tradeoffs: The system trades off response time for accuracy by using a modular architecture with independent skills. This allows for more accurate responses but may increase the response time. The system also trades off the ability to answer all questions for safety by using moderation filters to prevent unsafe content.
- Failure signatures: Common failure modes include: misclassifying queries into the wrong skill, failing to find relevant information in the documents, generating hallucinations or unsafe content, and timing out due to slow response generation.
- First 3 experiments:
  1. Test the Coreference Resolution component with a set of queries containing explicit and implicit coreferences.
  2. Test the Skill Classifier component with a set of queries belonging to different skill categories.
  3. Test the Contextual Answering Skill component with a set of queries that can be answered using the pre-processed documents.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Jill Watson compare to other AI teaching assistants in real-world classroom deployments beyond the initial study?
- Basis in paper: [inferred] The paper mentions a deployment in a diverse range of courses, but does not provide detailed comparative data.
- Why unresolved: The paper only provides a snapshot of the initial deployment and does not offer a longitudinal study or comparative analysis with other AI teaching assistants.
- What evidence would resolve it: Data from extended deployments in various courses and direct comparisons with other AI teaching assistants in terms of student engagement, learning outcomes, and user satisfaction.

### Open Question 2
What are the long-term effects of using AI teaching assistants like Jill Watson on student learning and engagement?
- Basis in paper: [explicit] The paper mentions the potential for AI assistants to boost student and teacher productivity but does not explore long-term impacts.
- Why unresolved: The paper focuses on the technical aspects and initial deployment of Jill Watson but does not investigate the sustained impact on student learning and engagement over time.
- What evidence would resolve it: Longitudinal studies tracking student performance, engagement, and feedback over multiple semesters or academic years.

### Open Question 3
How can the skill-based architecture of Jill Watson be further expanded to incorporate more diverse and specialized capabilities?
- Basis in paper: [explicit] The paper mentions the potential for expanding the skill-based architecture with software tools and API services.
- Why unresolved: The paper provides a framework for the skill-based architecture but does not explore the full potential or specific examples of how it can be expanded.
- What evidence would resolve it: Case studies or prototypes demonstrating the integration of new skills and capabilities into the Jill Watson system, along with their impact on performance and user experience.

## Limitations

- The evaluation methodology relies on human judgment for response quality assessment without reporting inter-rater reliability or specific evaluation criteria
- The comparison against OpenAI Assistants is limited to 150 questions, which may not capture edge cases or domain-specific failure modes
- The system's performance on complex, multi-hop reasoning questions or those requiring integration across multiple documents is not evaluated

## Confidence

- High Confidence: The modular architecture design and its benefits for extensibility are well-established concepts borrowed from XiaoIce and other multi-skill systems.
- Medium Confidence: The claim that Jill Watson outperforms legacy knowledge-based Jill Watson and OpenAI Assistants is supported by the reported metrics, but the evaluation set size (150 questions) and lack of statistical significance testing reduce confidence.
- Low Confidence: The assertion that the system is "well-suited for intelligent textbooks" is primarily based on its ability to process multiple documents, without empirical evidence of performance in actual textbook scenarios.

## Next Checks

1. **Statistical validation of comparative performance**: Conduct a larger-scale evaluation (minimum 500 questions) with multiple human raters and report inter-rater reliability scores and statistical significance testing for the performance differences between Jill Watson, legacy Jill Watson, and OpenAI Assistants.

2. **Hallucination detection effectiveness**: Implement an automated evaluation where known factual errors are injected into source documents, then measure whether the textual entailment verification and citation mechanisms successfully detect these hallucinations at rates above 90%.

3. **Multi-document reasoning capability**: Design a test set of 50 questions requiring integration of information across at least three different source documents, then measure both retrieval accuracy (percentage of relevant passages retrieved) and answer correctness to validate the system's suitability for complex textbook scenarios.