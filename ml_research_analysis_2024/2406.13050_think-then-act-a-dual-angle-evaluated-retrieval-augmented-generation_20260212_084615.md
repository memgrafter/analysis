---
ver: rpa2
title: 'Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation'
arxiv_id: '2406.13050'
source_url: https://arxiv.org/abs/2406.13050
tags:
- retrieval
- query
- answer
- framework
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Think-then-Act, a framework for retrieval-augmented
  generation that addresses the limitations of existing approaches by incorporating
  a dual-angle evaluation process. Instead of performing retrieval indiscriminately
  or generating responses before deciding on additional retrieval, the framework first
  assesses the input query for clarity and completeness, rewriting it if necessary,
  and then evaluates the model's capability to answer the query before deciding if
  additional retrieval is needed.
---

# Think-then-Act: A Dual-Angle Evaluated Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID**: 2406.13050
- **Source URL**: https://arxiv.org/abs/2406.13050
- **Authors**: Yige Shen; Hao Jiang; Hua Qu; Jihong Zhao
- **Reference count**: 13
- **Primary result**: Think-then-Act framework improves retrieval-augmented generation performance with dual-angle evaluation, achieving notable accuracy and efficiency gains across five diverse datasets.

## Executive Summary
This paper introduces Think-then-Act, a framework for retrieval-augmented generation that addresses key limitations of existing approaches by incorporating a dual-angle evaluation process. The framework first assesses input queries for clarity and completeness, rewriting them if necessary, then evaluates the model's capability to answer before deciding on additional retrieval. Experimental results on five diverse datasets demonstrate significant performance improvements compared to existing baselines, with the framework showing versatility across both English and non-English contexts.

## Method Summary
The Think-then-Act framework employs a two-phase process to optimize retrieval-augmented generation. First, it assesses input queries for clarity and completeness, rewriting ambiguous or incomplete queries into multiple clear sub-queries when needed. Second, it evaluates the model's capability to answer the query by comparing a confidence score (β) against a threshold (β′ = 0.5), determining whether additional retrieval is necessary. The framework uses prompting methods for query rewriting and model evaluation, comparing results against baselines including Standard Prediction, Original Chain-of-Thought, and Retrieve-then-Read approaches across five datasets using gpt-3.5-turbo as the underlying model.

## Key Results
- Think-then-Act framework significantly improves performance on five diverse datasets including HotPotQA, 2WikiMultihopQA, StrategyQA, FEVER, and a custom Chinese Poetry dataset.
- The framework demonstrates notable improvements in accuracy and efficiency compared to existing baselines, with ablation studies validating the optimal model confidence threshold.
- The approach performs well in both English and non-English contexts, showcasing its versatility and potential for broader applications.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-phase evaluation reduces unnecessary retrieval calls by first ensuring the query is clear and complete, then checking if the model can answer without retrieval.
- Mechanism: Query assessment + capability check + conditional retrieval based on a confidence threshold (β′ = 0.5).
- Core assumption: Large language models can accurately self-assess their ability to answer a given query.
- Evidence anchors:
  - [abstract] "Our framework employs a two-phase process: (i) assessing the input query for clarity and completeness to determine if rewriting is necessary; and (ii) evaluating the model’s capability to answer the query and deciding if additional retrieval is needed."
  - [section] "We propose an approach where the model’s capabilities are evaluated prior to generation, which achieves a balance between precision and efficiency in situations where absolute accuracy is not required."
  - [corpus] Weak - corpus neighbors do not directly support self-assessment accuracy.
- Break condition: If the model cannot accurately judge its own capability, it may retrieve unnecessarily or skip retrieval when it's needed, hurting performance.

### Mechanism 2
- Claim: Rewriting ambiguous or incomplete queries into multiple sub-queries improves retrieval precision and final answer quality.
- Mechanism: When a query is labeled AMBIGUOUS, the model decomposes it into clear sub-queries, each targeted to a specific aspect of the original query.
- Core assumption: Decomposition into sub-queries captures all necessary information without losing context.
- Evidence anchors:
  - [abstract] "Our framework employs a two-phase process: (i) assessing the input query for clarity and completeness to determine if rewriting is necessary."
  - [section] "The model resolves ambiguity by breaking down the query into multiple, straightforward sub-queries, each addressing a specific aspect of the original query."
  - [corpus] Weak - corpus neighbors do not directly address query decomposition.
- Break condition: If decomposition fragments context or loses semantic coherence, final answers may be incomplete or incorrect.

### Mechanism 3
- Claim: Selective retrieval based on model confidence threshold optimizes resource usage without sacrificing accuracy.
- Mechanism: Confidence score β compared to threshold β′ determines whether to retrieve; retrieval only when β < β′.
- Core assumption: Confidence scores reliably predict the need for external information.
- Evidence anchors:
  - [abstract] "Experimental results on five datasets show that the Think-then-Act framework significantly improves performance. Our framework demonstrates notable improvements in accuracy and efficiency compared to existing baselines."
  - [section] "β < β′, indicating that the model lacks sufficient confidence to answer the query on its own, so retrieval is required. β ≥ β′, suggesting that the model is confident in its ability to provide an accurate response without additional information, so retrieval is not needed."
  - [corpus] Weak - corpus neighbors do not provide evidence for confidence-based retrieval decisions.
- Break condition: If confidence estimation is miscalibrated, retrieval may be omitted when needed or performed when unnecessary.

## Foundational Learning

- Concept: Self-assessment by language models
  - Why needed here: Enables the framework to decide if retrieval is necessary before generating a response.
  - Quick check question: Can a language model reliably estimate whether it knows the answer to a given question?

- Concept: Query decomposition for ambiguity resolution
  - Why needed here: Transforms vague or multi-aspect queries into multiple clear sub-queries for targeted retrieval.
  - Quick check question: Does breaking an ambiguous query into sub-queries improve retrieval accuracy without losing semantic coherence?

- Concept: Confidence thresholding in retrieval decisions
  - Why needed here: Balances retrieval frequency with computational cost and accuracy.
  - Quick check question: What threshold value for model confidence best balances retrieval necessity and resource efficiency?

## Architecture Onboarding

- Component map: Query assessment module → Query rewriting module → Capability check module → Retrieval module (conditional) → Response generation module.
- Critical path: Query assessment → Capability check → (optional) Retrieval → Final answer generation.
- Design tradeoffs:
  - Higher β′ → fewer retrievals, faster but potentially less accurate.
  - Lower β′ → more retrievals, more accurate but slower and costlier.
  - Quality of query rewriting ↔ decomposition accuracy and final answer correctness.
- Failure signatures:
  - High retrieval ratio despite clear queries → query assessment miscalibration.
  - Low accuracy on ambiguous queries → rewriting module failure.
  - Over-retrieval → confidence threshold too low.
  - Under-retrieval → confidence threshold too high.
- First 3 experiments:
  1. Vary β′ from 0.0 to 1.0 on a small dataset to identify optimal threshold for retrieval efficiency vs accuracy.
  2. Test query rewriting accuracy by comparing rewritten queries against human-annotated clarifications.
  3. Compare retrieval counts and final accuracy when using direct decision vs confidence score methods for capability check.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal model confidence threshold (β′) across different task domains and languages?
- Basis in paper: [explicit] The paper mentions ablation studies validate the optimal model confidence threshold (β′ = 0.5) but does not explore whether this threshold generalizes across domains or languages.
- Why unresolved: The experiments only report results using a single threshold value (β′ = 0.5) without testing different thresholds across various datasets or language contexts.
- What evidence would resolve it: Systematic experiments varying β′ across multiple domains (multihop QA, commonsense reasoning, fact checking, domain-specific QA) and languages, showing how optimal thresholds differ based on task characteristics.

### Open Question 2
- Question: How does the Think-then-Act framework perform when integrated with fine-tuned white-box models instead of black-box LLMs?
- Basis in paper: [explicit] The limitations section explicitly states the study exclusively used black-box models like GPT-3.5 and suggests future work could focus on fine-tuning a white-box model based on their framework.
- Why unresolved: All experiments were conducted using black-box models requiring API calls, which limits practical deployment due to cost and security concerns.
- What evidence would resolve it: Comparative experiments showing performance, efficiency, and cost differences between black-box and white-box implementations of the framework.

### Open Question 3
- Question: How does the framework handle mixed-type question scenarios where a single dataset contains various question types?
- Basis in paper: [inferred] The limitations section mentions that experiments were isolated to single dataset types and suggests future research should investigate performance on diverse datasets incorporating various types of questions within a single dataset.
- Why unresolved: The current evaluation approach tests the framework on separate datasets for each task type, which doesn't reflect real-world scenarios where users ask diverse questions in sequence.
- What evidence would resolve it: Experiments using multi-task datasets or concatenated datasets containing mixed question types, measuring performance consistency and adaptability across different question categories.

## Limitations
- The framework relies on language models' ability to accurately self-assess their knowledge, a capability that remains contentious in the literature and lacks direct empirical validation in the paper.
- The optimal confidence threshold (β′) is presented as empirically determined but without sufficient sensitivity analysis across diverse query types or domains.
- The absence of detailed prompt specifications and ablation studies on query decomposition quality represents a significant limitation for reproducibility and understanding component contributions.

## Confidence
- **High confidence**: The experimental results showing performance improvements over baselines are well-documented and verifiable through the reported metrics on standard datasets.
- **Medium confidence**: The dual-angle evaluation framework's conceptual design appears sound, but its practical effectiveness depends on unverified assumptions about model self-assessment capabilities.
- **Low confidence**: Claims about resource optimization benefits and the precise mechanisms by which query rewriting improves retrieval precision lack sufficient supporting evidence.

## Next Checks
1. Conduct controlled experiments varying the confidence threshold β′ across multiple orders of magnitude to empirically determine the optimal balance between retrieval efficiency and answer accuracy.
2. Design and implement a validation study comparing model confidence scores against human-annotated retrieval necessity judgments to test the reliability of the self-assessment mechanism.
3. Perform systematic ablation studies isolating the contribution of query rewriting from other framework components to quantify its specific impact on retrieval precision and final answer quality.