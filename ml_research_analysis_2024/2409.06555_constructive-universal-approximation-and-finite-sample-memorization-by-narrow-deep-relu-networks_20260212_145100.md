---
ver: rpa2
title: Constructive Universal Approximation and Finite Sample Memorization by Narrow
  Deep ReLU Networks
arxiv_id: '2409.06555'
source_url: https://arxiv.org/abs/2409.06555
tags:
- neural
- network
- theorem
- have
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a fully constructive approach to analyzing
  the expressivity of deep ReLU neural networks, connecting universal approximation
  and finite sample memorization to discrete nonlinear dynamics and controllability.
  The authors prove that any dataset with N distinct points in R^d and M output classes
  can be exactly classified using a multilayer perceptron (MLP) of width 2 and depth
  at most 2N + 4M - 1, with all network parameters constructed explicitly.
---

# Constructive Universal Approximation and Finite Sample Memorization by Narrow Deep ReLU Networks

## Quick Facts
- arXiv ID: 2409.06555
- Source URL: https://arxiv.org/abs/2409.06555
- Reference count: 40
- Primary result: Any dataset with N distinct points and M classes can be exactly classified using a width-2 MLP of depth at most 2N + 4M - 1 with explicit parameters

## Executive Summary
This paper presents a fully constructive approach to analyzing the expressivity of deep ReLU neural networks, connecting universal approximation and finite sample memorization to discrete nonlinear dynamics and controllability. The authors prove that narrow neural networks can achieve both exact classification of any dataset and universal function approximation with explicit parameter constructions. The work bridges approximation theory and control theory by interpreting neural network layers as transformations in discrete nonlinear dynamical systems, where neurons act as hyperplanes partitioning the state space.

## Method Summary
The paper develops a constructive approach to neural network expressivity using geometric and dynamical considerations. For memorization, it builds a width-2 network that partitions space using hyperplanes and drives data points through successive transformations: dimension reduction, class compression, sorting, and label mapping. For universal approximation, it constructs a width-d+1 network that approximates functions by first building simple functions on hyperrectangular partitions, then using a two-step neural network to map these partitions to function values. All parameters are explicitly constructed rather than learned through optimization.

## Key Results
- Any dataset with N distinct points in R^d and M output classes can be exactly classified using a width-2 MLP of depth at most 2N + 4M - 1
- The explicit interpolating parameters provide upper bounds on regularized empirical loss, with trained networks converging to minimum-norm interpolants as regularization vanishes
- Universal approximation theorem in L^p(Ω; R^+) for bounded domains using MLPs of fixed width d+1, with explicit depth bounds scaling as ‖f‖ᵈ_{W^{1,p}}ε⁻ᵈ

## Why This Works (Mechanism)

### Mechanism 1
A ReLU multilayer perceptron with width 2 and depth 2N + 4M - 1 can exactly classify any dataset with N distinct points and M output classes. The network parameters are constructed explicitly using geometric and dynamical considerations. Each neuron represents a hyperplane that partitions the state space into regions with distinct activation behaviors. By strategically selecting parameters across layers, the network drives data points through successive transformations: dimension reduction, compression of classes into representative points, sorting of these points along a line, and finally mapping to their respective labels. The construction fails if the ReLU function does not satisfy the required monotonicity and collapse properties, or if the dataset contains duplicate points.

### Mechanism 2
Regularized training with small λ produces parameters converging to a minimum-norm interpolant. The explicit interpolating parameters from the width-2 construction provide an upper bound on the optimal value of the regularized loss. As λ → 0, the training objective forces the network to fit the data exactly while keeping parameter norms bounded. Any accumulation point of minimizers corresponds to a minimum-norm interpolant. The convergence argument may fail if the loss function is not continuous or the activation function is not Lipschitz.

### Mechanism 3
The universal approximation result for Lp(Ω; R+) uses a fixed width d + 1 neural network with explicit depth bounds. The proof proceeds by approximating the target function with a simple function supported on a family of hyperrectangles. A two-step neural network construction then maps these hyperrectangles to the function values: first compressing each hyperrectangle to a point without mixing others, then applying the width-2 memorization result to assign the correct values. The depth scales as ‖f‖ᵈ_{W^{1,p}}ε⁻ᵈ for f ∈ W^{1,p}. If the function f has very rough behavior (e.g., not in W^{1,p}), the depth bounds may become impractical or the approximation may fail.

## Foundational Learning

- **Concept: ReLU activation function and its geometric interpretation**
  - Why needed here: The entire construction relies on understanding how ReLU partitions space via hyperplanes and how the activation regions behave under parameter changes.
  - Quick check question: Given w = (1, -1) and b = 0, what is the output of σ(Wx + b) for x = (2, 1) and x = (0, 1)?

- **Concept: Hyperplane separation and convex kernels**
  - Why needed here: The compression process depends on using multiple hyperplanes to create convex regions that can collapse specific data points while preserving others.
  - Quick check question: How many hyperplanes are needed in R² to create a bounded convex kernel (polyhedron)?

- **Concept: Simple function approximation and Lebesgue differentiation**
  - Why needed here: The universal approximation proof builds on approximating functions by simple functions on hyperrectangular partitions.
  - Quick check question: What is the error bound when approximating an Lᵖ function by a simple function using a grid of size h?

## Architecture Onboarding

- **Component map:** Input (d-dim) -> Dimension reduction (1 neuron) -> Class compression (2 neurons) -> Sorting (2 neurons) -> Label mapping (2 neurons) -> Output (1-dim)

- **Critical path:**
  1. Project d-dimensional data to 1D while preserving distinctness
  2. Compress each class into a single point using alternating data structuring and compression layers
  3. Sort the compressed points along a line
  4. Map sorted points to their target labels/values
  For universal approximation: Add hyperrectangle compression step before classification

- **Design tradeoffs:**
  - Width vs. Depth: Width 2 is minimal for memorization but requires O(N) layers; width d+1 reduces depth complexity for approximation
  - Exactness vs. Generalization: The construction guarantees exact fit but may not generalize well to unseen data
  - Simplicity vs. Efficiency: The geometric approach is interpretable but may be less efficient than learned parameters

- **Failure signatures:**
  - Duplicate data points (violates distinctness assumption)
  - Non-separable classes requiring more complex decision boundaries than hyperplane partitions
  - Functions with high oscillation (W^{1,p} norm too large, leading to impractical depth)
  - Loss of geometric structure during dimension reduction

- **First 3 experiments:**
  1. **Binary classification test:** Implement the width-2 construction for a simple 2D dataset with two classes. Verify exact classification and count the required layers.
  2. **Label scaling test:** Use the construction for M > 2 classes with 1D labels. Check if the sorting and mapping steps correctly order and assign labels.
  3. **Function approximation test:** Apply the width d+1 universal approximation construction to approximate a smooth function (e.g., Gaussian) on a 2D domain. Measure L² error vs. depth and compare with theoretical bounds.

## Open Questions the Paper Calls Out

### Open Question 1
Can the universal approximation results for Lp(Ω; R+) be extended to more general function spaces like Sobolev or Besov spaces? The authors note that while some results exist for Sobolev spaces, they don't provide the geometric interpretability and explicit parameter construction of their approach. A constructive proof showing how to approximate functions in Sobolev/Besov spaces using neural networks of width d+1, with explicit parameter construction and geometric interpretation, would resolve this.

### Open Question 2
What is the optimal trade-off between width and depth for memorization in deep neural networks? The authors note that existing results show a trade-off (e.g., width 12 allows memorization with O(N^(1/2) + log(N)) neurons, while width 2 requires O(N) neurons). A systematic analysis proving lower bounds on width for memorization with bounded depth, or demonstrating specific width-depth combinations that achieve optimal parameter efficiency, would resolve this.

### Open Question 3
How does the topology of the dataset affect the number of layers needed for memorization? The authors mention that their current approach projects data into one dimension, potentially losing the original data distribution and clustering structure. Empirical or theoretical results showing how preserving the intrinsic dimensionality of datasets through better projection methods reduces the required depth for memorization compared to the worst-case bounds would resolve this.

## Limitations

- The width-2 construction requires O(N) layers for N data points, making it impractical for large datasets
- The explicit parameter construction assumes distinct data points and may fail with duplicates
- The universal approximation depth bound scales exponentially with dimension (‖f‖ᵈ_{W^{1,p}}ε⁻ᵈ), making it computationally prohibitive for high-dimensional functions with high Sobolev regularity

## Confidence

- **High confidence**: The width-2 memorization construction - the geometric approach is fully specified with explicit parameters and proven correct through step-by-step induction
- **Medium confidence**: The training convergence analysis - relies on standard optimization arguments but depends on the correctness of the explicit interpolant parameters
- **Low confidence**: The universal approximation bounds - while the construction is explicit, the exponential depth scaling and its dependence on Sobolev norms make practical verification challenging

## Next Checks

1. **Empirical width-2 construction test**: Implement the full memorization algorithm for datasets of varying size (N=5, 10, 20) and verify exact classification while measuring the actual layer depth and parameter norms.

2. **Sobolev regularity dependence study**: Approximate functions with different W^{1,p} norms (e.g., smooth Gaussian vs. piecewise linear) using the width d+1 construction and empirically verify the predicted depth scaling.

3. **Robustness to data structure test**: Evaluate the memorization construction on datasets with different geometric properties (uniformly distributed, clustered, adversarial) to identify failure modes and parameter sensitivity.