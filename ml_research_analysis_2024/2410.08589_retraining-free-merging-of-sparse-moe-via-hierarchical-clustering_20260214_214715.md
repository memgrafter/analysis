---
ver: rpa2
title: Retraining-Free Merging of Sparse MoE via Hierarchical Clustering
arxiv_id: '2410.08589'
source_url: https://arxiv.org/abs/2410.08589
tags:
- experts
- merging
- expert
- clustering
- hc-smoe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HC-SMoE, a retraining-free, task-agnostic
  expert merging framework for sparse mixture-of-experts models that reduces parameter
  count through hierarchical clustering based on expert outputs. Unlike prior methods
  that rely on router logits or frequency-based grouping, HC-SMoE uses averaged expert
  outputs as a similarity metric to capture functional relationships between experts,
  combined with hierarchical clustering to ensure robust groupings.
---

# Retraining-Free Merging of Sparse MoE via Hierarchical Clustering

## Quick Facts
- arXiv ID: 2410.08589
- Source URL: https://arxiv.org/abs/2410.08589
- Reference count: 14
- Key outcome: HC-SMoE reduces MoE parameters by 25-50% without retraining, outperforming baselines across eight zero-shot tasks

## Executive Summary
This paper introduces HC-SMoE, a retraining-free, task-agnostic expert merging framework for sparse mixture-of-experts models. The method uses hierarchical clustering based on averaged expert outputs to identify functionally similar experts, then merges them to reduce parameter count without requiring retraining. Unlike prior approaches that rely on router logits or frequency-based grouping, HC-SMoE captures functional relationships between experts through output similarity metrics. Extensive experiments on Qwen and Mixtral models demonstrate consistent performance improvements over existing pruning and merging baselines across multiple zero-shot language tasks.

## Method Summary
HC-SMoE operates through a hierarchical clustering approach that uses averaged expert outputs as similarity metrics to identify functionally related experts. The framework first computes the mean output vectors for each expert across training samples, then applies hierarchical clustering to group similar experts based on these representations. Within each cluster, experts are merged using either frequency-weighted or average merging strategies to preserve important routing information while reducing parameters. The method is task-agnostic and requires no retraining, making it suitable for deployment across different downstream applications. The approach is specifically designed for sparse MoE architectures where only a subset of experts are active per token.

## Key Results
- Achieves 25-50% parameter reduction while maintaining or improving performance over original models
- Consistently outperforms existing pruning and merging baselines across eight zero-shot language tasks
- Demonstrates effectiveness on both Qwen and Mixtral MoE architectures
- Provides comparable or better performance than original models despite significant parameter reduction

## Why This Works (Mechanism)
HC-SMoE works by capturing functional relationships between experts through their output patterns rather than routing behavior. By using averaged expert outputs as similarity metrics, the method identifies experts that produce similar transformations on input data, indicating redundant functionality. Hierarchical clustering then creates robust groupings that preserve essential expert diversity while eliminating redundancy. The merging strategy maintains routing information through frequency-weighted or average approaches, ensuring that the reduced model can still effectively route tokens to appropriate functional units. This output-based similarity captures more meaningful relationships between experts than router logit-based methods, which may be influenced by noise or training artifacts.

## Foundational Learning

Sparse MoE Architecture
- Why needed: Understanding how mixture-of-experts models distribute computation across multiple specialized networks
- Quick check: Verify that only top-k experts are activated per token and how gating mechanisms work

Hierarchical Clustering
- Why needed: The method relies on agglomerative clustering to group similar experts without predefined cluster numbers
- Quick check: Understand dendrogram construction and linkage criteria used for expert grouping

Expert Output Similarity Metrics
- Why needed: The core innovation uses averaged expert outputs rather than router logits for clustering
- Quick check: Confirm how mean outputs are computed and normalized across training samples

Parameter Reduction Tradeoffs
- Why needed: Balancing compression benefits against potential performance degradation
- Quick check: Understand how frequency-weighted vs average merging affects retained routing information

Zero-shot Evaluation
- Why needed: The primary evaluation metric measures performance without task-specific fine-tuning
- Quick check: Verify task diversity and evaluation methodology across different benchmarks

## Architecture Onboarding

Component Map
Router/Gating Network -> Expert Pool -> Output Aggregation -> HC-SMoE Clustering -> Merged Expert Pool

Critical Path
Input tokens → Router determines top-k experts → Selected experts process tokens → Outputs aggregated → Hierarchical clustering on averaged outputs → Experts merged within clusters → Reduced model with preserved functionality

Design Tradeoffs
- Output-based vs logit-based similarity: Captures functional relationships but requires full forward passes
- Hierarchical vs flat clustering: Provides more robust groupings but increases computational complexity
- Frequency-weighted vs average merging: Preserves routing importance vs equal contribution

Failure Signatures
- Performance degradation when clustering creates overly broad groups that eliminate necessary expert diversity
- Increased computational overhead for very large expert pools due to hierarchical clustering
- Suboptimal merging when expert outputs are too similar, providing minimal parameter reduction

3 First Experiments
1. Test clustering on a small MoE with 8 experts to visualize grouping patterns and verify similarity metrics
2. Compare frequency-weighted vs average merging on a validation set to determine optimal strategy
3. Evaluate parameter reduction impact on a single downstream task before scaling to multiple benchmarks

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the text provided.

## Limitations

- Computational overhead may become prohibitive for extremely large expert pools where hierarchical clustering is expensive
- Theoretical justification for using averaged expert outputs as similarity metrics is lacking
- Evaluation focuses primarily on zero-shot settings, leaving fine-tuning and domain-specific performance uncertain
- Modest parameter reductions (25-50%) compared to more aggressive pruning strategies
- Task-agnostic approach may miss opportunities for task-specific optimization

## Confidence

High confidence in technical implementation and experimental methodology (tested across multiple models and tasks)
Medium confidence in superiority claims relative to baselines (limited baseline selection)
Low confidence in theoretical justifications for similarity metric choice and scalability claims for very large expert pools

## Next Checks

1. Test HC-SMoE on fine-tuning scenarios and domain-specific tasks to evaluate generalization beyond zero-shot settings
2. Evaluate the impact on routing efficiency and inference latency to quantify practical deployment trade-offs
3. Scale experiments to models with 1000+ experts to validate performance claims for very large expert pools