---
ver: rpa2
title: Can Language Models Explain Their Own Classification Behavior?
arxiv_id: '2405.07436'
source_url: https://arxiv.org/abs/2405.07436
tags:
- word
- input
- contains
- rule
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether language models can explain their
  own classification behavior. The authors introduce ArticulateRules, a dataset of
  simple text classification tasks generated by rule functions, each with a natural
  language explanation.
---

# Can Language Models Explain Their Own Classification Behavior?

## Quick Facts
- arXiv ID: 2405.07436
- Source URL: https://arxiv.org/abs/2405.07436
- Reference count: 40
- Models vary widely in articulation accuracy, with GPT-4 showing nascent self-explanation capabilities while GPT-3 fails to articulate even after finetuning.

## Executive Summary
This paper investigates whether language models can explain their own classification behavior through self-explanation. The authors introduce ArticulateRules, a dataset of simple text classification tasks with natural language explanations, and evaluate models' ability to articulate the rules they use for classification. While models like GPT-3 achieve high classification accuracy (≥95%), they struggle to articulate these rules accurately, with finetuning providing only marginal improvements. In contrast, GPT-4 demonstrates significantly better articulation accuracy (72% vs 7% for GPT-3), suggesting some nascent self-explanation capabilities in larger models.

## Method Summary
The authors created ArticulateRules, a dataset of 20 rule functions generating text classification tasks, each paired with natural language explanations. They evaluated models using in-context learning and after finetuning on classification tasks. Models were tested on binary classification accuracy and their ability to articulate rules through multiple-choice and freeform responses. The evaluation included both in-distribution and out-of-distribution examples, with adversarial inputs designed to test robustness. Articulation accuracy was measured using both programmatic and LLM-based evaluation methods.

## Key Results
- Articulation accuracy varies considerably between models, with a sharp increase from GPT-3 (7%) to GPT-4 (72%)
- GPT-3 fails to articulate most rules even after finetuning, with 7/10 rules at 0% accuracy
- Finetuning on ground-truth explanations provides only marginal improvements in articulation accuracy
- Models can achieve high classification accuracy (≥95%) while struggling to articulate the rules they use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Articulation accuracy improves with model scale because larger models have richer representations of rule-based logic.
- Mechanism: As model size increases, internal representations capture more nuanced patterns and the model's ability to reflect on its own behavior improves, leading to better articulation.
- Core assumption: Model scale correlates with capacity for self-explanation.
- Evidence anchors:
  - [abstract] "articulacy accuracy varies considerably between models, with a particularly sharp increase from GPT-3 to GPT-4"
  - [section 3.1] "Applying our evaluation in-context, we find articulation accuracy improves with scale; Curie (13B), GPT-3 (175B) and GPT-4 (unknown) achieved 1%, 7% and 72% average accuracy respectively on the in-context freeform articulation task"
  - [corpus] Weak - only 5 related papers, none directly address scaling of self-explanation
- Break condition: If articulation does not improve with scale, or if a smaller model matches a larger one on articulation accuracy.

### Mechanism 2
- Claim: Finetuning on correct explanations can improve a model's ability to articulate rules it uses.
- Mechanism: By training on ground-truth explanations, the model learns to map its internal decision process to natural language descriptions.
- Core assumption: The model's classification behavior can be captured by simple rules, and finetuning on these rules will teach the model to articulate them.
- Evidence anchors:
  - [abstract] "Further finetuning on ground-truth explanations of rules R marginally improved performance. However, articulation accuracy remained at 0% for 7/10 rules and exceeded 15% for only 1/10 rules"
  - [section 3.2.3] "GPT-3-c was trained on the freeform articulation training sets for all rule functions except one held-out rule function. It was then evaluated on the held-out rule function's test set for freeform articulation...which achieved > 0% test accuracy for only 3 out of 10 rules"
  - [corpus] Weak - no related papers directly address finetuning on self-explanations
- Break condition: If finetuning on correct explanations does not improve articulation accuracy, or if accuracy remains very low even after finetuning.

### Mechanism 3
- Claim: The articulation task is inherently harder than the classification task, so models that classify well may still struggle to articulate their reasoning.
- Mechanism: Generating a natural language explanation requires the model to introspect and verbalize its decision process, which is a higher-level cognitive task than simply making a classification decision.
- Core assumption: Articulation is a more complex task than classification.
- Evidence anchors:
  - [abstract] "GPT-3 can be finetuned to achieve high classification accuracy (≥ 95% both in- and out-of-distribution) on a subset of 10 tasks in ArticulateRules...However, this finetuned GPT-3 failed to articulate rules R that closely approximated its behavior (0% accuracy for n = 200 instances of rules)"
  - [section 3.2.3] "GPT-3-c achieved exactly 0% test accuracy for 10 out of 10 rule functions, suggesting it entirely fails to articulate its reasoning in natural language even for simple classification rules"
  - [corpus] Weak - no related papers directly compare difficulty of classification vs. articulation tasks
- Break condition: If models can articulate with high accuracy after finetuning, or if articulation accuracy matches classification accuracy.

## Foundational Learning

- Concept: Binary text classification
  - Why needed here: The models must first learn to classify inputs according to simple rules before they can articulate those rules.
  - Quick check question: What is the difference between a rule-based classifier and a statistical classifier?

- Concept: Few-shot learning
  - Why needed here: The models are given a small number of examples to learn the classification rule, rather than being trained on a large dataset.
  - Quick check question: How does few-shot learning differ from traditional supervised learning?

- Concept: Adversarial attacks
  - Why needed here: Adversarial inputs are used to test the robustness of the model's classification behavior and ensure it has learned the intended rule.
  - Quick check question: What is the purpose of using adversarial examples in model evaluation?

## Architecture Onboarding

- Component map: Rule functions -> Classification tasks -> Adversarial attacks -> Classification evaluation -> Articulation tasks -> Articulation evaluation
- Critical path: Generate rule functions -> Create classification tasks -> Apply adversarial attacks -> Evaluate classification accuracy -> Generate articulation tasks -> Evaluate articulation accuracy
- Design tradeoffs: Larger models may have better articulation accuracy but are more expensive to use. Finetuning can improve accuracy but requires additional compute and data. Including adversarial attacks makes the evaluation more robust but also more complex.
- Failure signatures: Low articulation accuracy despite high classification accuracy suggests the model is not introspecting on its own behavior. Zero articulation accuracy even after finetuning suggests the model is not learning to map its internal representations to natural language.
- First 3 experiments:
  1. Evaluate a range of models on the in-context classification and articulation tasks to establish a baseline.
  2. Finetune the best performing model on the classification task and evaluate its accuracy.
  3. Finetune the finetuned model on the articulation task and evaluate its accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can language models provide faithful explanations of their own classification behavior, and if so, under what conditions?
- Basis in paper: [explicit] The paper directly investigates whether LLMs can give faithful high-level explanations of their own internal processes, and defines faithful explanations as those that accurately describe the model's behavior on a wide range of held out in-distribution and out-of-distribution examples.
- Why unresolved: While the paper finds that GPT-4 shows some capability for self-explanation, it still fails on around 30% of rules. Additionally, the paper acknowledges that high performance on their articulation benchmark is a necessary but insufficient condition for faithful model self-explanation.
- What evidence would resolve it: Further research could explore whether specific prompting techniques, fine-tuning approaches, or architectural modifications could improve LLM self-explanation capabilities. Additionally, developing more rigorous benchmarks for evaluating the faithfulness of model explanations could help resolve this question.

### Open Question 2
- Question: What are the key differences between GPT-3 and GPT-4 that lead to GPT-4's superior self-explanation capabilities?
- Basis in paper: [explicit] The paper notes a sharp increase in articulation accuracy from GPT-3 to GPT-4, suggesting GPT-4 has nascent self-explanation capabilities. However, the paper acknowledges that instruction fine-tuning alone does not fully explain GPT-4's capabilities.
- Why unresolved: The paper does not deeply investigate the specific architectural or training differences between GPT-3 and GPT-4 that contribute to the self-explanation gap.
- What evidence would resolve it: Analyzing the differences in model architecture, training data, and fine-tuning techniques between GPT-3 and GPT-4 could shed light on what enables GPT-4's self-explanation capabilities. Comparing the internal representations and reasoning processes of the two models on the same tasks could also provide insights.

### Open Question 3
- Question: How can we distinguish between faithful self-explanations and explanations that are simply the most likely continuation based on the model's pre-training data?
- Basis in paper: [explicit] The paper acknowledges that high accuracy on the freeform articulation task is a necessary but insufficient condition for faithful model self-explanation, as the model might be outputting the most probable next token based on its context rather than introspecting on its reasoning.
- Why unresolved: The paper does not propose a definitive method for disentangling faithful introspection from statistical completions in model explanations.
- What evidence would resolve it: Developing techniques to probe the internal representations and reasoning processes of models during classification and articulation tasks could help distinguish between genuine self-explanation and statistical completion. Additionally, creating more challenging articulation tasks that require deeper reasoning and are less likely to be solved by simple pattern completion could help tease apart these two possibilities.

## Limitations

- The scaling relationship between model size and self-explanation capability is based on limited data points (only three models tested)
- Finetuning on ground-truth explanations provides only marginal improvements, with unclear mechanisms for why it fails
- The evaluation methodology relies on automated metrics that may not fully capture the quality and faithfulness of explanations

## Confidence

- High Confidence: GPT-3 fails to articulate rules even after finetuning, while GPT-4 shows significantly better performance
- Medium Confidence: Articulation is inherently harder than classification based on the results, though this is not directly proven
- Low Confidence: Model scale directly correlates with self-explanation capacity due to limited data and unaccounted factors

## Next Checks

1. Test articulation accuracy across a broader range of model sizes and architectures to determine if the scaling relationship holds more generally
2. Experiment with different finetuning strategies, such as curriculum learning or multi-task training combining classification and articulation
3. Conduct a detailed qualitative analysis of cases where models fail to articulate correctly, categorizing error types to understand underlying mechanisms