---
ver: rpa2
title: Distilling LLMs' Decomposition Abilities into Compact Language Models
arxiv_id: '2402.01812'
source_url: https://arxiv.org/abs/2402.01812
tags:
- arxiv
- reasoning
- sub-questions
- problem
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores distilling the decomposition abilities of large
  language models (LLMs) into smaller, more efficient models using offline reinforcement
  learning. The authors generate a task-specific dataset by leveraging an LLM to decompose
  mathematical problems into sub-questions and provide feedback on their usefulness.
---

# Distilling LLMs' Decomposition Abilities into Compact Language Models

## Quick Facts
- arXiv ID: 2402.01812
- Source URL: https://arxiv.org/abs/2402.01812
- Authors: Denis Tarasov; Kumar Shridhar
- Reference count: 19
- One-line primary result: AI-generated feedback enables training compact models that achieve 51.2% accuracy on GSM8K, significantly lower than ChatGPT's 71.2%

## Executive Summary
This study explores distilling reasoning capabilities from large language models (LLMs) into smaller, more efficient models through offline reinforcement learning. The authors generate a task-specific dataset by leveraging ChatGPT to decompose mathematical problems into sub-questions and provide feedback on their usefulness. They then train smaller language models using fine-tuning and offline RL techniques to replicate the decomposition process. Experimental results demonstrate that while the approach shows promise, there remains a significant performance gap between the best-performing approach and ChatGPT, indicating room for improvement in distilling reasoning capabilities.

## Method Summary
The study employs a two-phase data collection process where ChatGPT first generates sub-questions for mathematical problems, then provides answers and feedback on sub-question usefulness. The authors then train smaller language models using three approaches: standard behavioral cloning, filtered behavioral cloning (which considers only high-quality examples), and offline reinforcement learning. The RL formulation treats text generation as a token-level Partially Observable Markov Decision Process (POMDP). The GSM8K dataset serves as the evaluation benchmark, with models trained to decompose problems and arrive at correct solutions.

## Key Results
- Best-performing approach (filtered behavioral cloning) achieves 51.2% accuracy on GSM8K, compared to ChatGPT's 71.2%
- Standard behavioral cloning shows inferior performance to filtered approaches
- Offline RL demonstrates potential but hasn't matched the performance of filtered behavioral cloning
- AI-generated feedback proves effective as a training signal despite dataset imperfections

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI-generated feedback can serve as a viable signal for fine-tuning models, even when the dataset is not ideal.
- Mechanism: The study leverages AI-generated feedback on sub-questions to provide a nuanced signal at the individual question level, mitigating sparsity concerns associated with relying solely on the correctness of final answers.
- Core assumption: AI-generated feedback is comparable in quality to human-annotated feedback and can effectively guide model training.
- Evidence anchors: [abstract]: "Recent studies have intriguingly revealed that AI feedback closely resembles human feedback (Lee et al., 2023)"; [section 4.1]: "In the subsequent phase of our data collection, we focused on generating responses to the previously obtained sub-questions."

### Mechanism 2
- Claim: Offline reinforcement learning can effectively distill reasoning capabilities from large language models into smaller, more efficient models.
- Mechanism: The study employs offline RL techniques to train smaller language models using the AI-generated dataset, aiming to replicate the decomposition process and reasoning abilities of the larger models.
- Core assumption: Offline RL algorithms can effectively learn from the AI-generated dataset and transfer the reasoning capabilities to smaller models.
- Evidence anchors: [abstract]: "This study focuses on distilling the LLMs' decomposition skills into compact models using offline reinforcement learning"; [section 5.2]: "Building upon the formulation proposed by Snell et al. (2022), we cast the text generation problem as a token-level Partially Observable Markov Decision Process (POMDP)."

### Mechanism 3
- Claim: Filtered behavioral cloning can improve performance by considering only high-quality examples in the dataset.
- Mechanism: The study introduces filtered behavioral cloning (Filtered BC) as a modification of the standard behavioral cloning approach, considering only a fraction of the best trajectories in the dataset.
- Core assumption: A substantial number of high-quality examples are present in the dataset, and focusing on these examples during training can lead to improved performance.
- Evidence anchors: [section 5.2]: "Filtered BC (Chen et al., 2021) introduces a modification of BC by considering only a fraction of the best trajectories in the dataset"; [section 5.3]: "Filtered BC demonstrates improved performance over standard BC in most scenarios."

## Foundational Learning

- Concept: Reinforcement Learning
  - Why needed here: Reinforcement learning is employed to train the smaller language models using the AI-generated dataset, allowing them to learn from the feedback and replicate the decomposition process and reasoning abilities of the larger models.
  - Quick check question: What is the key difference between supervised learning and reinforcement learning in the context of this study?

- Concept: Data Distillation
  - Why needed here: Data distillation is used to transfer the knowledge and reasoning capabilities from the large language models to the smaller, more efficient models. It involves leveraging the AI-generated dataset and feedback to train the smaller models.
  - Quick check question: How does the concept of data distillation differ from traditional model distillation techniques?

- Concept: Feedback Loop
  - Why needed here: The feedback loop is crucial in this study as it allows the AI models to generate feedback on their own sub-questions, which is then used to guide the training process of the smaller models. This iterative process helps refine the reasoning capabilities of the smaller models.
  - Quick check question: What is the role of the feedback loop in the context of this study, and how does it contribute to the overall learning process?

## Architecture Onboarding

- Component map: ChatGPT (LLM) -> AI-generated dataset and feedback -> Smaller language models -> Offline RL algorithms -> Distilled reasoning capabilities

- Critical path:
  1. Large Language Models generate the AI-generated dataset and feedback.
  2. Smaller Language Models are initialized and prepared for training.
  3. Offline Reinforcement Learning algorithms are applied to train the smaller models using the AI-generated dataset and feedback.
  4. The trained smaller models are evaluated for their reasoning capabilities.

- Design tradeoffs:
  - Model size vs. reasoning performance: Larger models tend to have better reasoning abilities but are computationally expensive, while smaller models are more efficient but may have limited reasoning capabilities.
  - Dataset quality vs. quantity: A higher-quality dataset with fewer examples may be more effective than a larger dataset with lower quality examples.
  - Feedback granularity: Providing more granular feedback at the individual question level may lead to better performance but requires more computational resources.

- Failure signatures:
  - Significant performance gap between the trained smaller models and the original large language models.
  - Inability of the smaller models to effectively replicate the reasoning process and decompose complex problems into sub-questions.
  - Instability or poor convergence of the offline reinforcement learning algorithms during training.

- First 3 experiments:
  1. Train a smaller language model using the standard behavioral cloning approach and evaluate its performance on the reasoning task.
  2. Apply the filtered behavioral cloning technique and compare its performance against the standard behavioral cloning approach.
  3. Implement the offline reinforcement learning approach using the AI-generated dataset and feedback, and assess its effectiveness in distilling the reasoning capabilities into the smaller models.

## Open Questions the Paper Calls Out
None

## Limitations
- Significant performance gap between distilled models (51.2% accuracy) and ChatGPT (71.2% accuracy)
- Limited evaluation to a single mathematical reasoning domain (GSM8K)
- Uncertainty about the quality and consistency of AI-generated feedback

## Confidence

**High Confidence**: The experimental methodology and implementation details are well-documented, including clear descriptions of the dataset generation process, training procedures, and evaluation metrics. The study's technical approach to using offline reinforcement learning for distillation is methodologically sound.

**Medium Confidence**: The core claim that AI-generated feedback can serve as a viable training signal is supported by experimental results, but the performance gap with the original model suggests this mechanism may have limitations that warrant further investigation.

**Low Confidence**: The scalability of the approach to more complex reasoning tasks and different domains remains uncertain, as does the long-term stability of the distilled models when applied to real-world scenarios.

## Next Checks
1. **Cross-domain generalization test**: Apply the same distillation methodology to a different reasoning task (e.g., logical reasoning or scientific problem-solving) to evaluate the approach's generalizability beyond mathematical word problems.

2. **Human feedback comparison**: Conduct a controlled study comparing the effectiveness of AI-generated feedback versus human-annotated feedback in the distillation process, measuring both performance and training efficiency.

3. **Progressive scaling experiment**: Systematically vary the size of the distilled models and measure the relationship between model size, training data quality, and reasoning performance to identify optimal scaling parameters.