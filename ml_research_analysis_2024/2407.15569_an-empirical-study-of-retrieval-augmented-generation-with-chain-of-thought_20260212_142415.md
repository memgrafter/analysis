---
ver: rpa2
title: An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought
arxiv_id: '2407.15569'
source_url: https://arxiv.org/abs/2407.15569
tags:
- raft
- documents
- performance
- question
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the RAFT (Retrieval Augmented Fine-Tuning)
  method to improve the performance of small-scale generative dialogue models on complex
  reasoning tasks. RAFT integrates chain-of-thought reasoning with supervised fine-tuning
  and retrieval augmented generation, training models to reason from noisy external
  documents.
---

# An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought

## Quick Facts
- arXiv ID: 2407.15569
- Source URL: https://arxiv.org/abs/2407.15569
- Authors: Yuetong Zhao; Hongyu Cao; Xianyu Zhao; Zhijian Ou
- Reference count: 0
- Primary result: RAFT method improves small-scale generative dialogue models on complex reasoning tasks by combining chain-of-thought reasoning with supervised fine-tuning and retrieval augmented generation.

## Executive Summary
This paper introduces RAFT (Retrieval Augmented Fine-Tuning), a method that integrates chain-of-thought reasoning with supervised fine-tuning and retrieval augmented generation to improve small-scale generative dialogue models on complex reasoning tasks. The approach trains models to reason from noisy external documents by including both relevant and distractor documents during fine-tuning. RAFT is evaluated across diverse datasets including HotpotQA, PubMedQA, and DuReader robust, achieving significant performance improvements over strong baselines.

The method demonstrates particular effectiveness for comparison-type reasoning tasks and shows robust gains even with distractor documents present. By teaching models to extract relevant information and perform step-by-step reasoning, RAFT addresses key limitations of standard RAG approaches, especially when dealing with noisy or irrelevant retrieval results. The approach bridges the gap between general pre-training and task-specific performance through domain-specific fine-tuning with retrieval context.

## Method Summary
RAFT fine-tunes small-scale language models using a dataset constructed from target QA datasets combined with both oracle documents (containing relevant information) and randomly selected distractor documents. The fine-tuning uses chain-of-thought style answers as targets, teaching models to perform step-by-step reasoning from the provided documents. The method involves two dataset construction approaches depending on whether questions have multiple reference documents or single documents. Models are fine-tuned on this augmented dataset and evaluated on their ability to answer questions while handling noise and irrelevant information in the retrieval results.

## Key Results
- RAFT consistently outperforms strong baselines, achieving up to 42.78% gains in F1 score and 42.13% in EM score over plain RAG on HotpotQA.
- The method shows robust gains even with distractor documents present, significantly improving model robustness against irrelevant retrieval results.
- Chain-of-thought reasoning is shown to be critical for handling complex inputs and noise, with RAFT + CoT achieving more significant performance gains in the presence of distractor documents compared to RAFT without CoT.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAFT improves model robustness against irrelevant retrieval results by including distractor documents in training.
- Mechanism: During RAFT fine-tuning, each question is paired with oracle documents and randomly selected distractor documents. The model learns to identify and use only the relevant information while ignoring distractors, building resilience to noisy retrieval outputs.
- Core assumption: The model can learn to distinguish relevant from irrelevant documents when both are present during training.
- Evidence anchors:
  - [abstract] "RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities."
  - [section] "First, in addition to the oracle documents, irrelevant distractor documents are also included in the reference documents to improve model's robustness against irrelevant information retrieved during the retrieval process."
- Break condition: If distractor documents overwhelm oracle documents in number or quality, the model may fail to learn the distinction.

### Mechanism 2
- Claim: Chain-of-thought reasoning enables small-scale models to perform complex reasoning tasks that typically require large models.
- Mechanism: By fine-tuning on chain-of-thought style responses that show step-by-step reasoning, the model learns to decompose complex problems into intermediate steps, mimicking the reasoning process that large models perform through prompting.
- Core assumption: Chain-of-thought reasoning can be effectively learned through supervised fine-tuning rather than just prompting.
- Evidence anchors:
  - [abstract] "RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG)"
  - [section] "Second, chain-of-thought style responses are used as the target text in the fine-tuning dataset rather than plain short answers to improve model's reasoning capability."
- Break condition: If the chain-of-thought format is too rigid or doesn't match the reasoning style needed for certain tasks, performance gains may diminish.

### Mechanism 3
- Claim: RAFT's domain-specific fine-tuning bridges the gap between general pre-training and task-specific performance.
- Mechanism: By fine-tuning on domain-specific datasets with retrieval context, the model learns both the reasoning patterns and the domain-specific language and knowledge structures needed for accurate responses.
- Core assumption: Domain-specific fine-tuning with retrieval context provides more relevant learning signals than general fine-tuning.
- Evidence anchors:
  - [abstract] "RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG)"
  - [section] "This is akin to training the model to compute results from relevant information before taking an exam."
- Break condition: If the fine-tuning dataset is too small or not representative of the target domain, the model may overfit or fail to generalize.

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: Enables models to break down complex reasoning tasks into manageable intermediate steps
  - Quick check question: How does chain-of-thought differ from direct answer generation in terms of model output structure?

- Concept: Retrieval augmented generation
  - Why needed here: Provides external knowledge sources to supplement model's internal knowledge
  - Quick check question: What are the main challenges RAG faces that RAFT addresses?

- Concept: Supervised fine-tuning with context
  - Why needed here: Allows models to learn task-specific patterns while maintaining the ability to use retrieval context
  - Quick check question: How does RAFT's fine-tuning differ from standard supervised fine-tuning?

## Architecture Onboarding

- Component map:
  - Retriever: Fetches relevant and distractor documents based on query
  - RAFT model: Fine-tuned LLM that processes query + documents → chain-of-thought reasoning → answer
  - Dataset processor: Constructs RAFT fine-tuning dataset with oracle and distractor documents
  - GPT-3.5 generator: Creates chain-of-thought responses for training data

- Critical path:
  1. Query → Retriever fetches documents
  2. Documents + query → RAFT model
  3. RAFT model generates chain-of-thought reasoning
  4. Reasoning leads to final answer

- Design tradeoffs:
  - Including distractor documents improves robustness but increases computational cost
  - Chain-of-thought responses provide better reasoning but are more complex to generate
  - Domain-specific fine-tuning improves performance but requires labeled data

- Failure signatures:
  - Model ignores relevant documents and relies on distractors
  - Chain-of-thought reasoning becomes circular or illogical
  - Performance degrades significantly when distractors are added

- First 3 experiments:
  1. Test RAFT model on HotpotQA with only oracle documents to establish baseline
  2. Add distractor documents to test robustness improvements
  3. Compare RAFT with and without chain-of-thought responses to measure CoT impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RAFT method perform on long-form QA tasks compared to short-form QA tasks, and what are the underlying reasons for any differences in performance?
- Basis in paper: [explicit] The paper mentions that RAFT achieved a 13% performance improvement for long-answer questions over zero-shot prompting baseline, but the gain was less prominent than for short-form QA. The authors note that long answers focus more on induction and summarization rather than definitive results derived from reasoning.
- Why unresolved: The paper only provides a preliminary comparison and suggests that the study of long-form QA with chain-of-thought needs further exploration. The specific factors contributing to the performance gap between long and short-form QA are not fully investigated.
- What evidence would resolve it: Detailed experiments comparing RAFT's performance on long-form and short-form QA tasks across various datasets, along with an analysis of the types of reasoning and information processing required for each task.

### Open Question 2
- Question: How does the inclusion of distractor documents in the RAFT method affect the model's performance on different types of reasoning tasks, such as bridge-type and comparison-type QA?
- Basis in paper: [explicit] The paper shows that RAFT performs better on comparison-type questions than bridge-type questions. It also demonstrates that RAFT achieves more significant performance gains in the presence of distractor documents compared to the DSF+RAG baseline.
- Why unresolved: While the paper provides some insights into the performance differences between task types and the impact of distractor documents, it does not fully explore the interaction between these factors. The specific reasons why RAFT handles comparison-type questions better with distractor documents are not explicitly stated.
- What evidence would resolve it: Experiments isolating the effects of distractor documents on different task types, along with an analysis of how the model processes and integrates information from multiple documents in each case.

### Open Question 3
- Question: How does the chain-of-thought (CoT) component of the RAFT method contribute to the model's ability to handle noise and irrelevant information in the retrieved documents?
- Basis in paper: [explicit] The paper states that the CoT method achieved more significant performance gains in the presence of distractor documents compared to the RAFT method without CoT. It also mentions that adding CoT effectively guides the model to extract correct information from complex input and enhances the model's logical rigor and accuracy.
- Why unresolved: While the paper demonstrates the benefits of CoT in handling noise and irrelevant information, it does not provide a detailed explanation of the underlying mechanisms. The specific ways in which CoT improves the model's robustness and accuracy in the presence of noise are not fully explored.
- What evidence would resolve it: A thorough analysis of the reasoning process generated by the model with and without CoT, focusing on how the model identifies and filters out irrelevant information, as well as how it integrates the relevant information into a coherent and accurate answer.

## Limitations

- The paper relies on GPT-3.5 for generating chain-of-thought training data, which introduces potential reproducibility issues due to unspecified prompt templates and variability in generated reasoning.
- The selection criteria for distractor documents remain underspecified, with only mention of random selection without quality controls to ensure truly irrelevant documents are chosen.
- The evaluation with distractor documents is limited to specific datasets, lacking analysis of failure cases or conditions under which the robustness breaks down.

## Confidence

**High Confidence**: The claim that RAFT improves performance over plain RAG is well-supported by the empirical results, with substantial gains reported across multiple datasets and metrics (F1 and EM scores). The ablation study clearly demonstrates the contribution of each component.

**Medium Confidence**: The assertion that chain-of-thought reasoning is critical for handling complex inputs and noise is supported by the ablation results, but the paper does not provide detailed analysis of why CoT specifically enables this robustness. The mechanism could be more thoroughly explored.

**Medium Confidence**: The claim about robustness to distractor documents is supported by experimental results, but the paper lacks analysis of failure cases or conditions under which the robustness breaks down. The evaluation with distractors is limited to specific datasets.

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary the GPT-3.5 prompt templates used to generate chain-of-thought training data and measure the impact on final RAFT model performance to assess reproducibility and sensitivity to training data generation.

2. **Distractor Quality Impact**: Conduct experiments with varying qualities and relevance levels of distractor documents (e.g., partially relevant vs. completely irrelevant) to determine the boundaries of RAFT's robustness and identify failure conditions.

3. **Cross-Dataset Generalization**: Evaluate RAFT models fine-tuned on one dataset (e.g., HotpotQA) on completely different reasoning tasks to assess whether the chain-of-thought reasoning capabilities generalize beyond the training domain.