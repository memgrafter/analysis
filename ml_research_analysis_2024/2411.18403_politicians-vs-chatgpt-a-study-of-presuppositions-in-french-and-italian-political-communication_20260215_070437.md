---
ver: rpa2
title: Politicians vs ChatGPT. A study of presuppositions in French and Italian political
  communication
arxiv_id: '2411.18403'
source_url: https://arxiv.org/abs/2411.18403
tags:
- chatgpt
- texts
- politicians
- discourse
- presupposition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares presuppositions in French and Italian political
  texts with those generated by ChatGPT. Presuppositions, often used for implicit
  persuasion, were analyzed in speeches by politicians (Macron, Le Pen, Meloni, Zingaretti)
  and their AI-generated counterparts.
---

# Politicians vs ChatGPT. A study of presuppositions in French and Italian political communication

## Quick Facts
- arXiv ID: 2411.18403
- Source URL: https://arxiv.org/abs/2411.18403
- Reference count: 8
- Key result: ChatGPT generates more potentially manipulative presuppositions than human politicians

## Executive Summary
This study compares presuppositions in French and Italian political texts with those generated by ChatGPT. Presuppositions, often used for implicit persuasion, were analyzed in speeches by politicians (Macron, Le Pen, Meloni, Zingaretti) and their AI-generated counterparts. Results show ChatGPT texts contain more potentially manipulative presuppositions (PMPs) than politicians' speeches. PMPs in ChatGPT texts are mainly conveyed through change-of-state verbs and stance-taking functions, while politicians use a more diverse range of triggers and functions, including criticism. This suggests ChatGPT tends to be more verbose, repetitive, and vague in its use of presuppositions.

## Method Summary
The study collected politician speeches and generated ChatGPT texts using politician prompts. Researchers manually annotated potentially manipulative presuppositions (PMPs), focusing on presupposition triggers and discourse functions. They calculated interrater agreement and performed statistical analysis to compare PMP frequency, form, and function between text types. The analysis examined different presupposition triggers (change-of-state verbs, factive verbs, comparative clauses, etc.) and their discourse functions (criticism, praise, stance-taking).

## Key Results
- ChatGPT-generated texts contain more potentially manipulative presuppositions than politicians' speeches
- ChatGPT uses narrower range of triggers (mainly change-of-state verbs and stance-taking)
- Politicians employ more diverse triggers including criticism and praise
- ChatGPT tends to be more verbose, repetitive, and vague in its use of presuppositions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT generates more potentially manipulative presuppositions (PMPs) than human politicians.
- Mechanism: ChatGPT tends to be verbose and repetitive, using change-of-state verbs and stance-taking functions as primary triggers for PMPs.
- Core assumption: Longer texts generated by ChatGPT inherently lead to repetition and vagueness, increasing PMP frequency.
- Evidence anchors:
  - [abstract] "Results show ChatGPT texts contain more potentially manipulative presuppositions (PMPs) than politicians' speeches."
  - [section] "ChatGPT tends to be more verbose, repetitive, and vague in its use of presuppositions."
  - [corpus] Found 25 related papers, average neighbor FMR=0.397, indicating moderate relevance but no direct citation evidence.
- Break condition: If the verbosity and repetition patterns in ChatGPT's output are reduced or if the model is fine-tuned to avoid vague language, the frequency of PMPs would decrease.

### Mechanism 2
- Claim: ChatGPT uses a narrower range of presupposition triggers compared to human politicians.
- Mechanism: ChatGPT predominantly uses change-of-state verbs and stance-taking functions, while politicians employ a more diverse set of triggers including criticism and praise.
- Core assumption: The model's training data and architecture predispose it to favor certain linguistic patterns over others.
- Evidence anchors:
  - [abstract] "PMPs in ChatGPT texts are mainly conveyed through change-of-state verbs and stance-taking functions, while politicians use a more diverse range of triggers and functions."
  - [section] "Other types of triggers are much less frequent (such as (semi-)factives, FCT, and relative clauses, REL) or almost unattested in the corpus."
  - [corpus] No direct evidence, but the moderate neighbor FMR suggests some related research exists.
- Break condition: If the model's training data is diversified or if the model is fine-tuned to recognize and utilize a broader range of presupposition triggers, the diversity of triggers would increase.

### Mechanism 3
- Claim: ChatGPT's generated texts are less explicit in criticism compared to human politicians.
- Mechanism: ChatGPT tends to produce less direct criticism, focusing more on stance-taking and self-praise, leading to vaguer and more general language.
- Core assumption: The model's architecture and training data influence its tendency to avoid explicit criticism.
- Evidence anchors:
  - [abstract] "PMPs in ChatGPT texts are mainly conveyed through change-of-state verbs and stance-taking functions, while politicians use a more diverse range of triggers and functions, including criticism."
  - [section] "ChatGPT tends to produce less explicit criticism associated with PMPs, unlike politicians."
  - [corpus] No direct evidence, but the moderate neighbor FMR suggests some related research exists.
- Break condition: If the model is fine-tuned to recognize and incorporate explicit criticism or if the training data includes more examples of direct criticism, the explicitness of criticism would increase.

## Foundational Learning

- Concept: Presupposition triggers
  - Why needed here: Understanding the different types of presupposition triggers is crucial for analyzing the use of PMPs in political texts.
  - Quick check question: Can you list at least three types of presupposition triggers and provide an example for each?

- Concept: Implicit communication
  - Why needed here: Implicit communication, including presuppositions, plays a significant role in political discourse and manipulation.
  - Quick check question: How does implicit communication differ from explicit communication in political texts?

- Concept: Large Language Models (LLMs)
  - Why needed here: Understanding the architecture and functioning of LLMs like ChatGPT is essential for analyzing their output and identifying potential biases.
  - Quick check question: What are the key components of an LLM's architecture that influence its text generation capabilities?

## Architecture Onboarding

- Component map: Training data -> Model architecture (transformer-based) -> Fine-tuning process -> Text generation output
- Critical path: Input processing -> Context understanding -> Text generation -> Output refinement
- Design tradeoffs: Creativity vs coherence, verbosity vs conciseness, diverse vs repetitive language patterns
- Failure signatures: Over-reliance on certain presupposition triggers, excessive verbosity, vague language lacking specificity
- First 3 experiments:
  1. Compare the frequency and types of presupposition triggers used by ChatGPT and human politicians in a controlled set of political texts
  2. Analyze the impact of adjusting model parameters (e.g., temperature, diversity penalty) on the use of presuppositions in generated texts
  3. Fine-tune the model on a dataset with diverse presupposition triggers and evaluate the changes in its output

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of ChatGPT-generated presuppositions compare to human-generated ones in terms of actual persuasion and manipulation?
- Basis in paper: [explicit] The paper discusses the potential for ChatGPT to generate persuasive and manipulative content, but does not test the actual persuasive impact on human readers.
- Why unresolved: The study focuses on the frequency and distribution of presuppositions but does not conduct an experiment to measure their persuasive effect on human subjects.
- What evidence would resolve it: An experimental study comparing the persuasiveness of ChatGPT-generated texts versus human-written texts on a sample of human participants, measuring agreement with the content and behavioral intentions.

### Open Question 2
- Question: What are the specific linguistic and contextual factors that make certain presuppositions more effective or manipulative than others?
- Basis in paper: [inferred] The paper identifies various presupposition triggers and discourse functions, but does not explore the relative effectiveness of these factors in achieving persuasive or manipulative goals.
- Why unresolved: The study provides a descriptive analysis of presuppositions but does not delve into the psychological or contextual mechanisms that make some presuppositions more impactful than others.
- What evidence would resolve it: A comprehensive analysis of linguistic and contextual factors, such as trigger type, discourse function, and situational context, combined with empirical data on the effectiveness of different presupposition strategies in various communication contexts.

### Open Question 3
- Question: How do the linguistic patterns of ChatGPT-generated presuppositions differ across languages, and what factors contribute to these differences?
- Basis in paper: [explicit] The study analyzes presuppositions in French and Italian texts generated by ChatGPT, but does not provide a detailed comparison of the linguistic patterns across languages.
- Why unresolved: The paper mentions the use of English as the primary language for ChatGPT training, which may influence its output in other languages, but does not investigate the specific linguistic differences between French and Italian texts.
- What evidence would resolve it: A detailed comparative analysis of ChatGPT-generated presuppositions in multiple languages, examining the influence of training data, language-specific linguistic features, and cultural context on the generated content.

## Limitations
- Only 20 ChatGPT-generated texts were analyzed, limiting generalizability
- Interrater agreement was lower for discourse function annotations due to subjective interpretation
- Unknown training data and potential bias from prompt excerpt selection

## Confidence

**Major Claim Confidence:**
- **High confidence**: ChatGPT generates more PMPs than politicians' speeches
- **Medium confidence**: ChatGPT uses narrower range of triggers (change-of-state verbs and stance-taking)
- **Medium confidence**: Politicians employ more diverse triggers including criticism

## Next Checks
1. Replicate analysis with larger sample size (100+ ChatGPT texts) to improve statistical power and generalizability
2. Conduct cross-validation with multiple annotators and calculate inter-rater reliability for all annotation categories
3. Compare presupposition patterns across different ChatGPT versions/temperature settings to identify model-specific biases