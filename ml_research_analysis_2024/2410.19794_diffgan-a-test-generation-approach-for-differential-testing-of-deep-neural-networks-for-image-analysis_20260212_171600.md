---
ver: rpa2
title: 'DiffGAN: A Test Generation Approach for Differential Testing of Deep Neural
  Networks for Image Analysis'
arxiv_id: '2410.19794'
source_url: https://arxiv.org/abs/2410.19794
tags:
- inputs
- triggering
- testing
- test
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DiffGAN, a black-box test generation approach
  for differential testing of Deep Neural Networks (DNNs) used in image classification
  tasks. The method leverages Generative Adversarial Networks (GANs) and the Non-dominated
  Sorting Genetic Algorithm II (NSGA-II) to generate diverse and valid triggering
  inputs that reveal behavioral discrepancies between DNN models with similar accuracy
  levels.
---

# DiffGAN: A Test Generation Approach for Differential Testing of Deep Neural Networks for Image Analysis

## Quick Facts
- arXiv ID: 2410.19794
- Source URL: https://arxiv.org/abs/2410.19794
- Reference count: 40
- Key outcome: Generates four times more diverse and valid triggering inputs than baseline DRfuzz for differential testing of DNN image classifiers

## Executive Summary
This paper presents DiffGAN, a black-box test generation approach for differential testing of Deep Neural Networks (DNNs) used in image classification tasks. The method leverages Generative Adversarial Networks (GANs) and the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to generate diverse and valid triggering inputs that reveal behavioral discrepancies between DNN models with similar accuracy levels. DiffGAN employs two custom fitness functions focused on diversity and divergence to guide the exploration of the GAN input space and identify discrepancies between model outputs. The approach is black-box, making it applicable to a wider range of situations without requiring access to model internals.

## Method Summary
DiffGAN combines a pre-trained GAN with NSGA-II optimization to generate diverse triggering inputs that expose behavioral discrepancies between DNN models. The method trains a GAN on augmented test datasets, then uses NSGA-II to explore the GAN's latent space with two fitness functions: one maximizing divergence between model outputs (Euclidean distance plus label disagreement) and another maximizing diversity among generated inputs. A two-stage filtering process (discriminator-based followed by SSIM-based) ensures generated inputs are valid. The approach is evaluated on eight pairs of DNN models trained on MNIST and CIFAR-10 datasets.

## Key Results
- DiffGAN generates four times more triggering inputs than baseline DRfuzz within the same testing budget
- Generated inputs achieve higher diversity and validity ratios compared to baseline
- DiffGAN improves machine learning-based model selection accuracy by 8-10% compared to DRfuzz inputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The combination of GAN latent space exploration with NSGA-II multi-objective optimization effectively generates diverse triggering inputs that expose behavioral discrepancies between DNN models.
- **Mechanism**: GANs produce novel, realistic images from a high-dimensional latent space. NSGA-II evolves latent vectors using two fitness functions: one maximizes divergence (Euclidean distance between output probability vectors), and the other maximizes diversity (minimum cosine distance to previously generated triggering inputs). This dual-objective search ensures both behavioral disagreement and input diversity.
- **Core assumption**: The latent space of a well-trained GAN adequately spans the domain of valid, realistic inputs for the DNN models under test.
- **Evidence anchors**:
  - [abstract]: "leverages a Generative Adversarial Network (GAN) and the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to generate diverse and valid triggering inputs that effectively reveal behavioral discrepancies between models."
  - [section]: "Since the models under test often exhibit similar high accuracy levels, GANs alone might not be effective in generating diverse triggering inputs that reveal behavioural disagreements between highly accurate models. To overcome these limitations, we leverage NSGA-II to effectively explore the large GAN’s latent space..."
  - [corpus]: Corpus contains related works on GAN-based test generation and multi-objective optimization in DNN testing; no direct contradiction.
- **Break condition**: If the GAN's latent space is too constrained or poorly aligned with the target domain, NSGA-II cannot generate valid inputs even with good optimization.

### Mechanism 2
- **Claim**: The modified divergence fitness function (Euclidean distance plus label disagreement check) more effectively targets inputs that cause the DNN models to produce different final predictions.
- **Mechanism**: Standard Euclidean distance measures only the magnitude of probability vector differences, not whether the models predict different classes. By adding +1 to the distance score when models output different labels, the search explicitly prioritizes inputs that lead to classification disagreements.
- **Core assumption**: Final predicted labels are the most relevant indicator of behavioral discrepancy for classification tasks.
- **Evidence anchors**:
  - [section]: "To address this limitation, we modified the fitness function to reflect differences in model predictions... if the model’s output different labels for a GAN-generated image, the fitness function increments the Euclidean distance by one..."
  - [section]: "This adjusted fitness function more accurately characterizes inputs that are likely to reveal significant behavioral differences between models, prioritizing inputs that not only produce different probability vectors but also result in distinct classifications by the models."
  - [corpus]: Weak - corpus contains works on test input generation and GANs but no explicit evidence about label-based fitness modification in differential testing context.
- **Break condition**: If models frequently disagree on probability vectors but still agree on labels, this fitness function may overemphasize certain types of disagreements.

### Mechanism 3
- **Claim**: The two-stage filtering process (discriminator-based followed by SSIM-based) significantly improves the validity ratio of generated triggering inputs.
- **Mechanism**: The GAN discriminator, trained to distinguish real from generated images, filters out unrealistic outputs. SSIM then compares each remaining image to its most similar real counterpart; low SSIM scores indicate invalid inputs. This layered approach reduces false positives from either filter alone.
- **Core assumption**: Both the discriminator and SSIM thresholds are well-calibrated to the dataset characteristics.
- **Evidence anchors**:
  - [section]: "To address this issue, we developed a two-stage filtration process for invalid triggering inputs... This process incorporates two state-of-the-art filtration techniques to ensure the quality and validity of the generated images..."
  - [section]: "By integrating discriminator-based filtering with SSIM-based invalidity detection, we developed a two-stage filtration system for enhancing the quality and validity of GAN-generated images."
  - [corpus]: Weak - corpus mentions GANs and validity filtering in general but lacks direct evidence for the specific two-stage approach or SSIM threshold selection method.
- **Break condition**: If either filter is too strict or too lenient relative to dataset characteristics, the validity ratio will drop or valid inputs will be incorrectly discarded.

## Foundational Learning

- **Concept**: Generative Adversarial Networks (GANs) and their latent space structure
  - Why needed here: GANs are the primary mechanism for generating realistic test inputs; understanding latent space exploration is essential for the NSGA-II search process.
  - Quick check question: What is the typical dimensionality of a GAN latent vector used in image generation tasks, and why is this dimensionality chosen?

- **Concept**: Non-dominated Sorting Genetic Algorithm II (NSGA-II) and multi-objective optimization
  - Why needed here: NSGA-II drives the search through the GAN latent space, balancing diversity and divergence objectives to find triggering inputs.
  - Quick check question: How does NSGA-II handle multiple conflicting objectives during the selection process?

- **Concept**: Differential testing and behavioral discrepancy detection
  - Why needed here: The core goal is to find inputs where two DNN models behave differently; understanding what constitutes a "triggering input" is fundamental.
  - Quick check question: What is the difference between a triggering input and an adversarial input in the context of DNN testing?

## Architecture Onboarding

- **Component map**:
  GAN model (generator + discriminator) -> NSGA-II search algorithm -> Fitness functions (divergence-based, diversity-based) -> Genetic operators (crossover, mutation) -> Filtering pipeline (discriminator + SSIM) -> Evaluation pipeline (diversity metrics, validity checking)

- **Critical path**:
  1. Train GAN on augmented test dataset
  2. Initialize NSGA-II population with random latent vectors
  3. Evaluate fitness (divergence + diversity)
  4. Apply genetic operators to evolve population
  5. Filter generated images for validity
  6. Return triggering inputs

- **Design tradeoffs**:
  - GAN training vs. search efficiency: Pre-trained GANs could reduce upfront cost but may be less aligned with specific test datasets
  - Filter strictness vs. validity ratio: Stricter filters improve validity but may discard valid inputs
  - Population size vs. diversity: Larger populations explore more but increase computation time

- **Failure signatures**:
  - Low validity ratio: Discriminator or SSIM thresholds may be mis-calibrated
  - Low diversity: Diversity fitness function or clustering approach may be ineffective
  - No triggering inputs: GAN latent space may not span the domain of valid inputs, or models may be too similar

- **First 3 experiments**:
  1. Run DiffGAN on a simple MNIST model pair with a pre-trained GAN to verify the end-to-end pipeline works
  2. Compare DiffGAN outputs with and without the label disagreement modification in the divergence fitness function
  3. Test different SSIM threshold values to find the optimal balance between validity and retention of valid inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can DiffGAN be adapted to support other machine learning tasks beyond image classification, such as natural language processing or time series analysis?
- Basis in paper: [explicit] The authors discuss the potential generalizability of DiffGAN to other domains and mention that it could be extended to tasks like image regression and segmentation by adapting the divergence fitness function.
- Why unresolved: While the authors provide a theoretical framework for extending DiffGAN, they do not provide concrete examples or experimental results demonstrating its effectiveness on other ML tasks.
- What evidence would resolve it: Experimental results showcasing DiffGAN's performance on tasks like text generation, sentiment analysis, or time series forecasting would provide strong evidence for its generalizability.

### Open Question 2
- Question: What is the optimal balance between diversity and divergence in the fitness functions for DiffGAN to achieve the best results in terms of generating triggering inputs?
- Basis in paper: [explicit] The authors discuss the importance of both diversity and divergence in their fitness functions but do not provide a definitive answer on the optimal balance between them.
- Why unresolved: The optimal balance likely depends on the specific models being tested and the desired outcomes. Finding a general rule for this balance requires further research.
- What evidence would resolve it: Empirical studies comparing DiffGAN's performance with different weightings of the diversity and divergence fitness functions on various model pairs would help determine the optimal balance.

### Open Question 3
- Question: How does the performance of DiffGAN scale with increasingly large and complex datasets, such as ImageNet?
- Basis in paper: [explicit] The authors discuss the scalability of DiffGAN and mention that it can handle larger datasets by leveraging pretrained GANs, but they do not provide concrete evidence of its performance on extremely large datasets.
- Why unresolved: The authors' experiments focus on relatively smaller datasets like MNIST and CIFAR-10. Testing DiffGAN on massive datasets like ImageNet would provide insights into its limitations and potential bottlenecks.
- What evidence would resolve it: Experimental results comparing DiffGAN's performance on ImageNet with its performance on smaller datasets, in terms of triggering input generation and computational efficiency, would address this question.

## Limitations
- The validity filtering thresholds (SSIM ≤ 0.75 and discriminator probability ≤ 0.5) are heuristic choices without systematic justification for their optimality across different datasets.
- The evaluation focuses exclusively on MNIST and CIFAR-10 datasets, limiting generalizability to more complex image domains or other application areas like NLP or tabular data.
- The paper assumes that behavioral discrepancies between DNN models are meaningful indicators of model quality, but does not establish what constitutes a "significant" disagreement or its practical implications for model deployment.

## Confidence

- **High confidence**: The core mechanism of combining GAN latent space exploration with NSGA-II multi-objective optimization for differential testing is well-supported by the evidence and technical implementation details.
- **Medium confidence**: The effectiveness of the modified divergence fitness function incorporating label disagreement is plausible but lacks direct comparative evidence against the standard Euclidean distance approach.
- **Medium confidence**: The two-stage filtering process improves validity ratios, but the optimal threshold values and their dataset dependence are not thoroughly explored.

## Next Checks
1. **Cross-dataset validation**: Test DiffGAN on a more complex dataset (e.g., ImageNet subset) to evaluate scalability and performance beyond MNIST/CIFAR-10.
2. **Threshold sensitivity analysis**: Systematically vary the SSIM and discriminator thresholds to determine their impact on validity ratios and identify optimal values for different dataset characteristics.
3. **Label disagreement impact**: Conduct ablation studies comparing the modified divergence fitness function with and without the label disagreement component to quantify its contribution to triggering input quality.