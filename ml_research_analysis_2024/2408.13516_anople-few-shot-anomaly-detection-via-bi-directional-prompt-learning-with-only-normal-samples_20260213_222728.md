---
ver: rpa2
title: 'AnoPLe: Few-Shot Anomaly Detection via Bi-directional Prompt Learning with
  Only Normal Samples'
arxiv_id: '2408.13516'
source_url: https://arxiv.org/abs/2408.13516
tags:
- anomaly
- detection
- anomalies
- visual
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of few-shot anomaly detection
  (FAD) without access to true anomalous samples, a difficult real-world problem.
  The proposed method, AnoPLe, uses bidirectional prompt learning with multi-modal
  interactions between textual and visual prompts in a CLIP-based framework.
---

# AnoPLe: Few-Shot Anomaly Detection via Bi-directional Prompt Learning with Only Normal Samples

## Quick Facts
- arXiv ID: 2408.13516
- Source URL: https://arxiv.org/abs/2408.13516
- Authors: Yujin Lee; Seoyoon Jang; Hyunsoo Yoon
- Reference count: 40
- Achieves 94.1% and 96.3% Image AUROC on MVTec-AD in 1- and 4-shot settings without real anomaly samples

## Executive Summary
This paper addresses few-shot anomaly detection (FAD) without access to true anomalous samples, a challenging real-world problem. The proposed method, AnoPLe, uses bidirectional prompt learning with multi-modal interactions between textual and visual prompts in a CLIP-based framework. Anomalies are simulated using Perlin noise and Gaussian noise to train the model, and a lightweight decoder with a multi-view signal is introduced to improve localization by learning local semantics from multiple image scales. The approach aligns global and local semantic representations to enhance image-level detection. AnoPLe achieves strong performance on MVTec-AD (94.1% and 96.3% Image AUROC in 1- and 4-shot settings) and VisA (86.2% Image AUROC in 1-shot), closely matching state-of-the-art methods despite not being exposed to real anomalies.

## Method Summary
AnoPLe is a CLIP-based framework for few-shot anomaly detection that uses bidirectional prompt learning between textual and visual prompts. The method simulates anomalies in both pixel space (using Perlin noise) and latent space (using Gaussian noise) to train the model without requiring real anomalous samples. A lightweight decoder with a learnable multi-view signal processes multi-scale images to enhance local semantic comprehension. The model employs an alignment loss to connect global encoder semantics with local decoder semantics, improving image-level anomaly detection. During training, the model sees both original images and non-overlapping sub-images conditioned on a multi-view signal, while inference uses only the original image.

## Key Results
- Achieves 94.1% and 96.3% Image AUROC on MVTec-AD in 1- and 4-shot settings
- Reaches 86.2% Image AUROC on VisA dataset in 1-shot setting
- Closes performance gap with state-of-the-art methods that use real anomalies
- Demonstrates strong few-shot anomaly detection capability without exposure to true anomalous samples

## Why This Works (Mechanism)

### Mechanism 1
Multi-modal bidirectional prompt coupling allows the model to extract richer anomaly-relevant features than single-modal prompts, especially when true anomalies are absent. The model projects prompts from one modality into the other's hidden space and concatenates them at each layer, allowing cross-modal communication. This creates a more flexible representation space that can learn anomaly-related semantics from simulated anomalies in both pixel and latent space.

### Mechanism 2
The multi-view signal with sub-images improves local semantic comprehension and mitigates distributional shift during inference. During training, the model receives both the original image and N non-overlapping sub-images, each conditioned on a distinct position in the multi-view signal. This forces the model to learn both global and local semantics. At inference, only the original image is used, but the model has learned to distinguish global from local views via the multi-view signal.

### Mechanism 3
Aligning global encoder semantics with local decoder semantics via the alignment loss improves image-level anomaly detection performance. The model computes a weighted sum of pixel-level logits and aligns it with the global [CLS] token representation via cosine similarity. This distills localized anomaly information into the global representation used for image-level scoring.

## Foundational Learning

- Concept: Prompt learning in vision-language models (VLMs)
  - Why needed here: AnoPLe replaces manually engineered prompts with learnable continuous vectors to adapt CLIP to anomaly detection without prior anomaly knowledge.
  - Quick check question: What is the difference between hard prompts and learnable (soft) prompts in VLMs?

- Concept: Multi-scale feature learning
  - Why needed here: The model uses sub-images during training to capture local anomalies while maintaining global context, improving both localization and detection.
  - Quick check question: How does training on multiple image scales affect a model's ability to detect local vs. global anomalies?

- Concept: Cross-modal representation alignment
  - Why needed here: Bidirectional coupling aligns textual and visual prompts to enrich the representation space for anomaly detection in the absence of true anomalies.
  - Quick check question: Why might unidirectional prompt communication be insufficient for anomaly detection tasks?

## Architecture Onboarding

- Component map: CLIP backbone (ViT-B/16+) -> Learnable textual prompts (Pt) -> Learnable visual prompts (Pv) -> Multi-view signal (c) -> Lightweight decoder (D) -> Visual memory bank -> Loss functions (pixel-level, image-level, alignment)

- Critical path: 1) Simulate anomalies (pixel + latent) 2) Encode images and text with bidirectional prompts 3) Decode patch features for localization 4) Compute multi-level losses 5) Update prompts and decoder 6) Use visual memory and alignment for inference

- Design tradeoffs: Multi-scale training improves local detection but increases training complexity; Bidirectional coupling enriches features but adds computational overhead; Visual memory aids localization but may introduce stale representations if not updated

- Failure signatures: Poor localization: decoder fails to refine patch features; Weak image-level detection: alignment loss not effective; Overfitting to sub-images: multi-view signal not distinguishing views

- First 3 experiments: 1) Test with only uni-modal prompts (text or image only) to confirm benefit of bidirectional coupling 2) Remove multi-view signal and sub-images to assess impact on local detection 3) Disable alignment loss to measure contribution to image-level performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of AnoPLe change when trained with real anomalous samples instead of simulated ones? The paper states AnoPLe achieves strong performance without exposure to true anomalies and has a small performance gap compared to state-of-the-art methods that do use real anomalies. This remains unresolved as the paper does not provide experimental results comparing AnoPLe's performance with and without real anomalies.

### Open Question 2
What is the impact of the multi-view signal on the model's ability to generalize to images with different aspect ratios or scales than those used during training? The paper introduces a multi-view signal to mitigate distributional shift caused by using only the original image during inference, after training with sub-images. This remains unresolved as the paper does not test the model's robustness to images with varying aspect ratios or scales beyond those used in training.

### Open Question 3
How does the choice of state word in the textual prompt affect the model's ability to detect different types of anomalies (e.g., structural vs. logical anomalies)? The paper shows that the choice of state word (e.g., "abnormal," "damaged," "defective") affects performance, but does not analyze its impact on different anomaly types. This remains unresolved as the paper does not provide a detailed analysis of how different state words influence the detection of specific anomaly categories.

### Open Question 4
What is the computational overhead introduced by the multi-view signal and how does it compare to the benefits gained in terms of accuracy? The paper introduces a multi-view signal to mitigate distributional shift, but does not provide a detailed analysis of its computational cost. This remains unresolved as the paper does not report the additional computational time or resources required for the multi-view signal.

## Limitations

- The effectiveness of bidirectional prompt coupling is assumed rather than empirically validated through ablation studies
- The multi-view signal's impact on mitigating distributional shift lacks empirical analysis of inference performance
- Reliance on simulated anomalies raises questions about robustness to real-world anomalies that differ substantially from simulated patterns

## Confidence

**High Confidence (8/10):**
- The overall framework of using CLIP with prompt learning for few-shot anomaly detection is technically sound and well-implemented
- The performance improvements on MVTec-AD and VisA datasets are verifiable through standard metrics

**Medium Confidence (6/10):**
- The bidirectional prompt coupling provides meaningful advantages over single-modal prompting for anomaly detection
- The multi-view signal effectively mitigates distributional shift and improves local semantic comprehension

**Low Confidence (4/10):**
- The alignment loss between global and local semantics is essential for achieving state-of-the-art image-level detection performance
- The method generalizes effectively to real-world anomalies that differ substantially from simulated patterns

## Next Checks

1. **Ablation Study for Bidirectional Coupling:** Implement and evaluate versions with only textual prompts, only visual prompts, unidirectional coupling (text→image or image→text only), and compare performance to the bidirectional approach to definitively establish whether the cross-modal communication is providing the claimed benefits.

2. **Distributional Shift Analysis:** During inference, systematically compare model performance when using only the original image versus using the multi-view signal. Additionally, evaluate the model on datasets with different image characteristics than MVTec-AD and VisA to assess robustness to distributional shift.

3. **Real Anomaly Transfer Test:** Evaluate the pre-trained model on a dataset containing real anomalies without any fine-tuning, comparing performance to models trained with real anomaly examples to test whether the simulated anomaly training strategy effectively generalizes to genuine anomaly detection tasks.