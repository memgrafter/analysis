---
ver: rpa2
title: 'EmpathicStories++: A Multimodal Dataset for Empathy towards Personal Experiences'
arxiv_id: '2405.15708'
source_url: https://arxiv.org/abs/2405.15708
tags:
- empathy
- dataset
- story
- data
- stories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMPATHIC STORIES++, the first longitudinal
  multimodal dataset capturing empathy in real-world settings. Collected over a month-long
  deployment of social robots in 41 participants' homes, the dataset includes 53 hours
  of video, audio, and text data from natural storytelling interactions, along with
  self-reported empathy ratings and psychometric surveys.
---

# EmpathicStories++

## Quick Facts
- **arXiv ID**: 2405.15708
- **Source URL**: https://arxiv.org/abs/2405.15708
- **Reference count**: 17
- **Primary result**: Introduces EMPATHIC STORIES++, a longitudinal multimodal dataset capturing empathy in real-world settings, and benchmarks empathy prediction models using personal experiences.

## Executive Summary
This paper introduces EMPATHIC STORIES++, the first longitudinal multimodal dataset capturing empathy in real-world settings. Collected over a month-long deployment of social robots in 41 participants' homes, the dataset includes 53 hours of video, audio, and text data from natural storytelling interactions, along with self-reported empathy ratings and psychometric surveys. The dataset addresses limitations of existing empathy datasets by providing in-the-wild, longitudinal, and self-annotated data. The authors also introduce a novel task of predicting empathy based on personal experiences, evaluating it in two contexts: participants' own stories and reflections on others' stories. Using state-of-the-art models, they benchmark this task, showing that model performance varies by context, with GPT-4 excelling in text-only settings and AMER performing well with multimodal inputs in reflective scenarios. This work advances empathy modeling in AI and provides a valuable resource for future research.

## Method Summary
The EMPATHIC STORIES++ dataset was collected through a month-long deployment of social robots in participants' homes, capturing 53 hours of video, audio, and text data from natural storytelling interactions. Participants engaged with the robot, sharing personal stories and reflecting on others' experiences. Self-reported empathy ratings and psychometric surveys were collected alongside the interaction data. The dataset is used to introduce a novel task of predicting empathy towards others' stories based on personal experiences, evaluated in two contexts: participants' own stories (Story Share) and reflections on stories they read (Reflection). State-of-the-art multimodal models (AMER, TFN, LF-LSTM, EF-LSTM, EmpathicStoriesBART, GPT-4) are benchmarked on this task using features extracted from the multimodal data.

## Key Results
- EMPATHIC STORIES++ is the first longitudinal dataset capturing empathy in real-world settings, with 53 hours of video, audio, and text data from 41 participants.
- The dataset enables a novel task of predicting empathy towards others' stories based on personal experiences, evaluated in two contexts: participants' own stories and reflections on others' stories.
- Model performance varies by context, with GPT-4 excelling in text-only settings and AMER performing well with multimodal inputs in reflective scenarios.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The dataset captures authentic empathy responses by collecting data in natural, longitudinal, and self-annotated settings.
- **Mechanism:** Real-world deployment in participants' homes over a month-long period, combined with self-reported empathy ratings and psychometric surveys, enables modeling of genuine, context-dependent empathy responses.
- **Core assumption:** Participants are more likely to express genuine empathy when interacting with a social robot in a familiar, comfortable environment over an extended period, and self-annotation captures more authentic subjective experiences