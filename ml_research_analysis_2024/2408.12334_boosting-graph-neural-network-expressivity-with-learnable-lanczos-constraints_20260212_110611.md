---
ver: rpa2
title: Boosting Graph Neural Network Expressivity with Learnable Lanczos Constraints
arxiv_id: '2408.12334'
source_url: https://arxiv.org/abs/2408.12334
tags:
- graph
- constraints
- matrix
- lanczos
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLwLC, a method to boost Graph Neural Network
  (GNN) expressivity for link prediction tasks. The core idea is to embed induced
  subgraphs into the graph Laplacian's eigenbasis using a Learnable Lanczos algorithm
  with Linear Constraints.
---

# Boosting Graph Neural Network Expressivity with Learnable Lanczos Constraints

## Quick Facts
- arXiv ID: 2408.12334
- Source URL: https://arxiv.org/abs/2408.12334
- Authors: Niloofar Azizi; Nils Kriege; Horst Bischof
- Reference count: 22
- One-line primary result: LLwLC achieves 20x and 10x speedup by requiring only 5% and 10% of training data on PubMed and OGBL-Vessel datasets respectively

## Executive Summary
This paper introduces LLwLC, a method to boost Graph Neural Network (GNN) expressivity for link prediction tasks by embedding induced subgraphs into the graph Laplacian's eigenbasis using a Learnable Lanczos algorithm with Linear Constraints. The approach addresses fundamental limitations of standard GNNs, particularly their inability to distinguish graphs that are indistinguishable by the 2-Weisfeiler-Lehman test and their struggles with node automorphism problems. Two novel subgraph extraction strategies are proposed: encoding vertex-deleted subgraphs and applying Neumann eigenvalue constraints, enabling the method to capture link-specific representations while maintaining computational efficiency.

## Method Summary
LLwLCNet combines a Learnable Lanczos algorithm with Linear Constraints (LLwLC) block and two MLP layers to perform link prediction. The method operates by constructing constraint matrices that encode specific subgraph structures (vertex-deleted subgraphs and Neumann eigenvalue constraints) and projecting the Lanczos process into the null space of these constraints. This low-rank approximation achieves O(κE + k²n) complexity instead of O(n³) for full eigendecomposition. The architecture takes a graph and link (u,v) as input, extracts induced subgraphs, applies the constrained Lanczos algorithm to obtain eigenbasis V and eigenvalues R, processes R through MLP layers, transforms signals using V, and produces link predictions through global pooling.

## Key Results
- LLwLC achieves state-of-the-art performance on link prediction benchmarks, distinguishing graphs indistinguishable by 2-WL test
- The method requires only 5% and 10% of training data from PubMed and OGBL-Vessel datasets respectively, achieving 20x and 10x speedup
- LLwLC enables differentiation between k-regular graphs and solves node automorphism problems for link prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLwLC can distinguish graphs that are indistinguishable by the 2-Weisfeiler-Lehman test by encoding vertex-deleted subgraphs into the eigenbasis.
- Mechanism: The algorithm constructs a constraint matrix C where each column corresponds to a vertex-deleted subgraph, encoding node degrees and zero entries for deleted nodes. This projects the eigenbasis into a space that preserves reconstruction constraints, enabling differentiation of graphs with identical WL features.
- Core assumption: Vertex-deleted subgraphs uniquely characterize most graphs (reconstruction conjecture holds).
- Evidence anchors:
  - [abstract]: "For the former, we demonstrate the ability to distinguish graphs that are indistinguishable by 2-WL"
  - [section]: "Utilizing vertex-deleted subgraphs allows us to distinguish between graphs that are not distinguishable with the 2-WL method"
  - [corpus]: No direct evidence found for reconstruction conjecture's practical applicability to LLwLC.

### Mechanism 2
- Claim: Neumann eigenvalue constraints enable differentiation of k-regular graphs and solve node automorphism problems for link prediction.
- Mechanism: The constraint matrix encodes boundary conditions between k-hop and (k+1)-hop neighbors, forcing eigenvectors to satisfy linear constraints that capture link-specific representations. This creates distinct feature spaces for automorphic nodes based on their relationships to specific neighbors.
- Core assumption: Neumann eigenvalue constraints preserve the graph's structural information while adding discriminative power.
- Evidence anchors:
  - [abstract]: "The latter focuses on link representations enabling differentiation between k-regular graphs and node automorphism"
  - [section]: "Neumann eigenvalue constraints, which encodes induced subgraphs and link representations"
  - [corpus]: No direct evidence found for Neumann constraints' effectiveness in link prediction.

### Mechanism 3
- Claim: The low-rank Lanczos approximation with linear constraints provides computational efficiency while maintaining expressiveness.
- Mechanism: By projecting the Lanczos process into the null space of constraints and using QR factorization for sparse matrices, LLwLC achieves O(κE + k²n) complexity instead of O(n³) for full eigendecomposition, enabling scalability to large graphs.
- Core assumption: The number of constraints k remains small relative to graph size, and QR factorization remains numerically stable.
- Evidence anchors:
  - [section]: "The time complexity involved in extracting subgraphs depends on the product of the maximum degree of nodes and the count of nodes in the boundary"
  - [section]: "Table 1a compares the time complexities of LLwLCNet and other link prediction methods"
  - [corpus]: No direct evidence found for QR factorization stability in this specific context.

## Foundational Learning

- Concept: Spectral graph theory and graph Laplacian eigenbasis
  - Why needed here: LLwLC operates in the spectral domain, requiring understanding of how graph structure maps to eigenvalues/vectors
  - Quick check question: What property of the graph Laplacian ensures its eigenvectors form an orthonormal basis?

- Concept: Lanczos algorithm for eigenvalue approximation
  - Why needed here: LLwLC extends the Lanczos algorithm to handle linear constraints, requiring knowledge of the basic algorithm's mechanics
  - Quick check question: How does the Lanczos algorithm reduce the complexity of finding extreme eigenvalues compared to full eigendecomposition?

- Concept: Weisfeiler-Lehman graph isomorphism test
  - Why needed here: LLwLC's expressivity is measured against WL test limitations, requiring understanding of what WL can and cannot distinguish
  - Quick check question: What specific graph structures can the 1-WL test not distinguish that motivates LLwLC's design?

## Architecture Onboarding

- Component map: Input graph → Constraint matrix construction → Lanczos algorithm with constraints → Eigenbasis V and eigenvalues R → MLP layers on R → Signal transformation → Global pooling → Output
- Critical path: Constraint extraction → Eigenbasis computation → MLP filtering → Final prediction
- Design tradeoffs: More constraints increase expressivity but computational cost; fewer constraints improve speed but may miss discriminative information
- Failure signatures: Training instability suggests constraint matrix issues; poor validation performance indicates insufficient constraints or learning rate problems
- First 3 experiments:
  1. Run LLwLC on Cora dataset with only Neumann constraints to verify basic functionality
  2. Add 5 vertex-deleted subgraph constraints to test expressivity improvement
  3. Measure training time vs baseline GNNs to validate computational efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the number of constraints (k) affect the trade-off between model expressivity and computational efficiency?
- Basis in paper: [explicit] The paper mentions that using just 10 constraints from vertex-deleted subgraphs yields state-of-the-art enhancements in benchmark datasets.
- Why unresolved: The paper does not provide a systematic study of how varying the number of constraints impacts both expressivity and computational cost.
- What evidence would resolve it: A comprehensive ablation study showing performance metrics (e.g., accuracy, F1-score) and computational metrics (e.g., training time, memory usage) for different numbers of constraints would provide insights into this trade-off.

### Open Question 2
- Question: Can the Learnable Lanczos algorithm with Linear Constraints (LLwLC) be effectively applied to other graph-related tasks beyond link prediction, such as node classification or graph classification?
- Basis in paper: [inferred] The paper focuses on link prediction tasks and demonstrates the effectiveness of LLwLC in this domain. However, it does not explore its applicability to other graph-related tasks.
- Why unresolved: The paper's experiments are limited to link prediction, and there is no theoretical analysis or empirical evidence to support the extension of LLwLC to other tasks.
- What evidence would resolve it: Experiments on node classification and graph classification tasks, along with theoretical analysis of the algorithm's behavior in these contexts, would provide insights into its broader applicability.

### Open Question 3
- Question: What is the impact of the graph's density on the performance of LLwLC, and how does it affect the time complexity of the algorithm?
- Basis in paper: [explicit] The paper mentions that the time complexity of extracting vertex-deleted subgraphs from a dense graph is O(n^3), while for sparse graphs, it is reduced by a factor of O(n).
- Why unresolved: The paper does not provide a detailed analysis of how graph density affects the performance and computational efficiency of LLwLC.
- What evidence would resolve it: A study comparing the performance and time complexity of LLwLC on graphs with varying densities would provide insights into its scalability and efficiency across different graph structures.

## Limitations

- The method's effectiveness relies on the reconstruction conjecture, which remains unproven mathematically
- The paper lacks comprehensive validation of Neumann eigenvalue constraints' effectiveness across diverse graph families
- Computational complexity claims are theoretically justified but not empirically validated across varying graph sizes and constraint counts

## Confidence

**High Confidence**: Computational efficiency improvements (20x-10x speedup with reduced training data) are well-supported by empirical results on PubMed and OGBL-Vessel datasets.

**Medium Confidence**: State-of-the-art performance claims are supported by benchmark results, though comparisons could be more comprehensive against recent GNN architectures.

**Low Confidence**: Theoretical claims about breaking 2-WL equivalence and solving node automorphism problems rely heavily on unproven mathematical conjectures without extensive empirical validation.

## Next Checks

1. **Constraint Sensitivity Analysis**: Systematically vary the number of vertex-deleted subgraph and Neumann constraints (k=1, 5, 10, 20) on Cora dataset to quantify the trade-off between expressivity and computational cost.

2. **Graph Family Robustness**: Test LLwLC on known hard cases for WL tests (e.g., regular graphs, strongly regular graphs, and cospectral graphs) to empirically verify 2-WL equivalence breaking claims.

3. **Scalability Benchmark**: Measure training and inference time on graphs ranging from 10⁴ to 10⁶ edges while varying constraint counts to validate the stated O(κE + k²n) complexity empirically.