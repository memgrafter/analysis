---
ver: rpa2
title: Can Large Language Models Replicate ITS Feedback on Open-Ended Math Questions?
arxiv_id: '2405.06414'
source_url: https://arxiv.org/abs/2405.06414
tags:
- feedback
- llms
- student
- error
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  replicate intelligent tutoring system (ITS) feedback for open-ended math questions.
  The authors fine-tune both open-source (Mistral-7B) and proprietary (GPT-3.5) LLMs
  on real student responses paired with corresponding ITS feedback.
---

# Can Large Language Models Replicate ITS Feedback on Open-Ended Math Questions?

## Quick Facts
- arXiv ID: 2405.06414
- Source URL: https://arxiv.org/abs/2405.06414
- Reference count: 0
- Large language models can replicate template-based ITS feedback when trained on similar errors but struggle with novel error types

## Executive Summary
This study investigates whether large language models can replicate intelligent tutoring system (ITS) feedback for open-ended math questions. The authors fine-tune both open-source (Mistral-7B) and proprietary (GPT-3.5) LLMs on real student responses paired with corresponding ITS feedback. They evaluate performance using BLEU and ROUGE metrics across three data splits: response-level, question-level, and error-level. Results show that LLMs perform well when trained on similar errors but struggle with previously unseen student errors. GPT-3.5 outperforms Mistral-7B in all settings. The study concludes that while LLMs can learn feedback formatting, they lack deep understanding of mathematical errors.

## Method Summary
The study fine-tunes Mistral-7B and GPT-3.5 on 26,845 student responses to linear equation word problems, each paired with ITS feedback and error labels. Models are evaluated using BLEU-4 and ROUGE-L metrics across three experimental splits: response-level (similar responses), question-level (new questions), and error-level (new error types). The authors compare fine-tuning with in-context learning approaches, testing models with and without error label information. Performance is measured through 5-fold cross-validation with 100 training examples per fold.

## Key Results
- GPT-3.5 consistently outperforms Mistral-7B across all experimental settings
- LLMs perform well on feedback generation for known error types but struggle with novel errors
- Error label information improves GPT-3.5 performance but degrades Mistral-7B performance
- Models learn to replicate feedback formatting rather than understanding mathematical concepts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can replicate ITS feedback formatting when provided with similar training examples
- Mechanism: The LLM learns the structural template of feedback messages through fine-tuning on question-response-feedback tuples. When presented with a new response that shares an error type with training data, the model generates feedback by filling in the learned template with context-specific terms
- Core assumption: Feedback messages for the same error class share highly similar structure and only vary in question-specific terminology
- Evidence anchors:
  - [abstract] "LLMs can replicate highly structured feedback given appropriate training data"
  - [section] "Since feedback messages in this dataset are template-based, which means that ones corresponding to the same error are very similar and only vary in question-specific terminology"
  - [corpus] No direct corpus evidence for this mechanism, but 5 related papers suggest similar findings about LLMs and feedback generation
- Break condition: When presented with errors that differ substantially from training data, the model fails to generate meaningful feedback and instead produces generic or incorrect messages

### Mechanism 2
- Claim: GPT-3.5 outperforms Mistral-7B across all experimental settings
- Mechanism: Larger models have greater capacity to learn and generalize from training data, including better understanding of mathematical concepts and error patterns
- Core assumption: Model size and architecture directly correlate with performance on mathematical reasoning and feedback generation tasks
- Evidence anchors:
  - [section] "We see that GPT-3.5 outperforms Mistral-7B in all settings. This result is somewhat expected since GPT-3.5 is orders of magnitude larger than Mistral-7B"
  - [section] "GPT-3.5 works relatively well even without fine-tuning as long as it has access to ICL examples via the prompt"
  - [corpus] No direct corpus evidence, but related papers on LLM tutoring systems suggest similar performance differences between model sizes
- Break condition: When both models are given in-context examples from the same error class, the performance gap narrows significantly, suggesting the advantage is partly due to better few-shot learning capabilities

### Mechanism 3
- Claim: Error label information improves GPT-3.5 performance but degrades Mistral-7B performance
- Mechanism: Larger models can effectively use explicit error labels as additional guidance for feedback generation, while smaller models become confused by the additional information
- Core assumption: The ability to process and utilize explicit error labels depends on the model's capacity to understand the relationship between labels and feedback structure
- Evidence anchors:
  - [section] "We see that the inclusion of the error label significantly improves the performance of feedback generation for GPT-3.5, but decreases performance for Mistral-7B"
  - [section] "This observation can likely be explained by the difference in scale and, perhaps by extension, intrinsic mathematical reasoning capabilities between these two models"
  - [corpus] No direct corpus evidence, but this aligns with findings in related papers about LLMs and error detection
- Break condition: When error labels are removed, both models revert to performance levels based primarily on template learning rather than error understanding

## Foundational Learning

- Concept: Template-based feedback generation
  - Why needed here: The ITS uses predefined templates for different error types, so understanding this structure is essential for replicating the feedback mechanism
  - Quick check question: What are the key components of a template-based feedback system, and how do they differ from free-form feedback?

- Concept: Fine-tuning vs In-Context Learning (ICL)
  - Why needed here: The study compares both approaches for adapting LLMs to feedback generation, with different resource requirements and performance characteristics
  - Quick check question: What are the main differences between fine-tuning and ICL in terms of computational cost, data requirements, and performance on structured tasks?

- Concept: Text similarity metrics (BLEU and ROUGE)
  - Why needed here: These metrics are used to quantitatively evaluate how closely generated feedback matches reference ITS feedback
  - Quick check question: How do BLEU and ROUGE metrics differ in what they measure, and what are their limitations for evaluating feedback quality?

## Architecture Onboarding

- Component map: Data preparation -> Model adaptation (fine-tuning or ICL) -> Feedback generation -> Evaluation -> Analysis
- Critical path: Data pipeline: Collects student responses, correct answers, error labels, and ITS feedback -> LLM model: Either fine-tuned Mistral-7B or GPT-3.5 with or without in-context examples -> Evaluation module: Computes BLEU and ROUGE scores comparing generated vs reference feedback -> Split generator: Creates train/test splits at response, question, or error level
- Design tradeoffs: Fine-tuning provides better performance but requires more resources and risks overfitting; ICL is more efficient but may not capture all nuances of feedback generation
- Failure signatures: Low BLEU/ROUGE scores on error-level splits indicate inability to generalize to new error types; performance degradation when error labels are added suggests model confusion
- First 3 experiments:
  1. Response-level split: Test model's ability to replicate feedback for similar responses
  2. Question-level split: Test model's ability to generalize to new questions within known error types
  3. Error-level split: Test model's ability to handle previously unseen error types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger open-source models like Llama-2 70b compare to Mistral-7B and GPT-3.5 in generating feedback for open-ended math questions?
- Basis in paper: [explicit] The paper states "experiments with larger open-source models, such as Llama-2 70b, remain to be performed, although setting them up can be challenging."
- Why unresolved: The paper only experimented with Mistral-7B (an open-source model) and GPT-3.5 (a proprietary model), leaving a gap in understanding how larger open-source models would perform
- What evidence would resolve it: Conducting experiments with Llama-2 70b or similar larger open-source models, comparing their performance metrics (BLEU, ROUGE scores) to those of Mistral-7B and GPT-3.5 across the same experimental settings

### Open Question 2
- Question: What alternative representations of student errors could improve LLM performance in generating feedback?
- Basis in paper: [explicit] The paper mentions "better representations, perhaps a latent one [16], is worth further investigation" regarding error representation
- Why unresolved: The study used error labels as the representation, which did not consistently improve performance and sometimes confused models, suggesting better representations are needed
- What evidence would resolve it: Testing different error representations (e.g., latent embeddings, semantic representations) in the feedback generation task and measuring their impact on LLM performance across various experimental settings

### Open Question 3
- Question: How do LLMs perform on feedback generation for more diverse open-ended math questions across different topics and formats?
- Basis in paper: [explicit] The paper notes "our findings suggest that error label text is not always an effective representation of student errors" and suggests testing "a set of more diverse open-ended math questions, both in terms of topics and question formats."
- Why unresolved: The current dataset consists of similar questions focused on linear equations, limiting generalizability to other mathematical domains and question types
- What evidence would resolve it: Conducting experiments with datasets containing diverse math topics (geometry, algebra, calculus) and different question formats (proofs, word problems, numerical problems), then comparing LLM performance across these varied contexts

## Limitations
- Results are constrained by the dataset's narrow focus on linear equation word problems with 11 predefined error types
- Template-based feedback means models learn pattern replication rather than deep mathematical understanding
- Evaluation uses text similarity metrics rather than measuring pedagogical effectiveness or learning outcomes

## Confidence
- **High confidence**: LLMs can replicate highly structured feedback when provided with appropriate training data showing similar error patterns
- **Medium confidence**: GPT-3.5 outperforms Mistral-7B across all settings due to its larger size and better few-shot learning capabilities
- **Medium confidence**: Error labels improve GPT-3.5 performance but degrade Mistral-7B performance

## Next Checks
1. Test on diverse error types: Evaluate model performance on a dataset with significantly more error types (e.g., 50+ error categories) and more complex mathematical domains to determine if template-based feedback generation scales to richer mathematical understanding
2. Pedagogical validation: Conduct a study measuring whether LLM-generated feedback leads to improved student learning outcomes compared to original ITS feedback, rather than just comparing text similarity metrics
3. Cross-domain generalization: Test whether models trained on linear equation feedback can generate meaningful feedback for completely different mathematical domains (e.g., geometry proofs or calculus problems) when provided with minimal adaptation data