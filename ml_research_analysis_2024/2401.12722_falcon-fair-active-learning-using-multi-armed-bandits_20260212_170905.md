---
ver: rpa2
title: 'Falcon: Fair Active Learning using Multi-armed Bandits'
arxiv_id: '2401.12722'
source_url: https://arxiv.org/abs/2401.12722
tags:
- fairness
- falcon
- samples
- group
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Falcon is a fair active learning framework that uses multi-armed
  bandits to automatically select samples that improve fairness and accuracy. It addresses
  the challenge of improving fairness when ground truth labels are unavailable during
  sample selection by using a trial-and-error labeling strategy and adversarial MABs
  to balance informativeness and postpone rate.
---

# Falcon: Fair Active Learning using Multi-armed Bandits

## Quick Facts
- arXiv ID: 2401.12722
- Source URL: https://arxiv.org/abs/2401.12722
- Reference count: 40
- Primary result: Falcon achieves 1.8-4.5x higher fairness scores while being more efficient than existing fair active learning approaches

## Executive Summary
Falcon is a novel fair active learning framework that addresses the challenge of improving fairness when ground truth labels are unavailable during sample selection. It uses a trial-and-error labeling strategy combined with adversarial multi-armed bandits to automatically select samples that improve both fairness and accuracy. By postponing samples with undesired labels and probabilistically alternating between fair and accurate labeling, Falcon achieves significant improvements in fairness metrics while maintaining competitive accuracy levels.

## Method Summary
Falcon uses a trial-and-error labeling strategy where samples are selected from sensitive groups and postponed if they have undesired labels for the target group. It models the trade-off between informativeness and postpone rate as policies, using adversarial multi-armed bandits like EXP3 to dynamically select the best policy based on fairness improvement rewards. The framework also probabilistically alternates between fair labeling and traditional active learning to control the accuracy-fairness trade-off without modifying the underlying active learning method.

## Key Results
- Falcon achieves 1.8-4.5x higher fairness scores compared to existing fair active learning approaches
- Maintains competitive accuracy while significantly improving fairness metrics (DP, EO, ED, PP, EER)
- More efficient than baselines, with reduced running time while achieving better fairness-accuracy trade-off

## Why This Works (Mechanism)

### Mechanism 1
Falcon improves fairness by dynamically selecting samples from target groups defined by sensitive attributes and labels. It uses a trial-and-error labeling strategy where samples from sensitive groups are selected and postponed if they have undesired labels for the target group. This avoids negatively impacting fairness while still allowing informative samples to be used later if needed.

### Mechanism 2
Falcon automatically selects the best sampling policy using adversarial multi-armed bandits (MABs). It models the trade-off between sample informativeness and postpone rate as policies, where each policy selects samples based on a risk-taking parameter. Adversarial MABs like EXP3 dynamically choose the best policy based on rewards in terms of fairness improvement.

### Mechanism 3
Falcon balances fairness and accuracy by probabilistically alternating between fair and accurate labeling. It interleaves fair labeling (with probability λ) and traditional active learning for accuracy (with probability 1-λ). This allows controlling the accuracy-fairness trade-off without modifying the active learning method.

## Foundational Learning

- **Multi-armed bandits (MABs) for policy selection**
  - Why needed: To dynamically choose the best sampling policy balancing informativeness and postpone rate
  - Quick check: How does the EXP3 algorithm update policy selection probabilities based on rewards?

- **Group fairness measures and subgroup decomposition**
  - Why needed: To identify target groups for labeling that improve specific fairness metrics
  - Quick check: How can demographic parity be decomposed into subgroup accuracies?

- **Active learning and sample selection strategies**
  - Why needed: To efficiently label samples that improve both fairness and accuracy
  - Quick check: What is the difference between uncertainty-based and diversity-based active learning?

## Architecture Onboarding

- **Component map**: Input → Target group identification → Policy selection → Sample selection → Model training → Output
- **Critical path**: User input → Target group identification → Policy selection → Sample selection → Model training → Output
- **Design tradeoffs**: Policy space size vs. computational efficiency, batch size vs. reward signal quality, λ value vs. fairness-accuracy trade-off
- **Failure signatures**: Fairness not improving (check target group identification), accuracy dropping (check λ value), running time too long (check MAB updates)
- **First 3 experiments**:
  1. Run Falcon with λ=0 (pure fairness) on a small biased dataset to verify fairness improvement
  2. Compare Falcon with and without trial-and-error on a dataset where postponing labels is crucial
  3. Test different policy sets (e.g., [0.3, 0.5, 0.7] vs. [0.4, 0.6]) on a dataset to see impact on fairness

## Open Questions the Paper Calls Out

- **Open Question 1**: How does Falcon's policy selection strategy generalize to fairness metrics beyond the ones evaluated in the paper?
  - Basis: The paper states Falcon can support any group fairness measure but only evaluates DP, EO, ED, PP, and EER
  - Why unresolved: Theoretical guarantees and empirical performance on other metrics remain unknown
  - What evidence would resolve it: Experiments applying Falcon to additional fairness metrics

- **Open Question 2**: What is the theoretical convergence behavior of Falcon when combined with traditional active learning methods?
  - Basis: The paper combines Falcon with AL for accuracy-fairness trade-off but acknowledges full analysis is future work
  - Why unresolved: No theoretical bounds on convergence or regret in the combined setting
  - What evidence would resolve it: Theoretical analysis proving regret bounds for Falcon in the blended setting

- **Open Question 3**: How sensitive is Falcon's performance to the choice of initial model and dataset bias?
  - Basis: The paper mentions initial data distributions are not heavily biased but doesn't systematically analyze sensitivity
  - Why unresolved: Performance variation with different initial model qualities or degrees of dataset bias is unexplored
  - What evidence would resolve it: Systematic experiments varying initial model performance and dataset bias levels

## Limitations
- The effectiveness depends heavily on the postpone rate remaining within acceptable bounds (30-50% observed)
- Does not provide sufficient detail on handling cases where postponing becomes excessive
- The fairness improvement mechanism assumes postponing samples with undesired labels is always beneficial

## Confidence
- **High confidence**: Experimental results showing Falcon outperforming baselines on fairness metrics are well-supported
- **Medium confidence**: MAB-based policy selection is theoretically sound but specific reward propagation details are incomplete
- **Medium confidence**: Claim that Falcon controls accuracy-fairness trade-off through λ is supported but optimal setting may be dataset-dependent

## Next Checks
1. Test Falcon's behavior when the postpone rate exceeds 50% to determine if the method degrades gracefully or fails catastrophically
2. Implement a variant of Falcon that uses a different reward signal to verify MAB-based policy selection robustness
3. Conduct ablation studies on the trial-and-error component by comparing Falcon with a variant that labels all samples immediately