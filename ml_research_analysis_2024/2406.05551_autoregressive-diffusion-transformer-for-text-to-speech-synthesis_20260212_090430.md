---
ver: rpa2
title: Autoregressive Diffusion Transformer for Text-to-Speech Synthesis
arxiv_id: '2406.05551'
source_url: https://arxiv.org/abs/2406.05551
tags:
- speech
- arxiv
- ardit
- tokens
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes autoregressive diffusion transformers (ARDiT)
  for text-to-speech synthesis. Instead of using discrete audio tokens, ARDiT directly
  generates continuous audio tokens in vector space.
---

# Autoregressive Diffusion Transformer for Text-to-Speech Synthesis

## Quick Facts
- arXiv ID: 2406.05551
- Source URL: https://arxiv.org/abs/2406.05551
- Reference count: 40
- Key outcome: ARDiT achieves near-perfect speech editing and high-quality zero-shot TTS by generating continuous audio tokens directly, with DMD enabling single-step generation and 170ms latency.

## Executive Summary
This paper proposes autoregressive diffusion transformers (ARDiT) for text-to-speech synthesis, which directly generate continuous audio tokens in vector space rather than using discrete audio tokens. The authors demonstrate that ARDiT excels in zero-shot TTS and speech editing, achieving near-perfect speech editing capabilities. By distilling ARDiT with distribution matching distillation, the model can generate speech with high quality using only one network evaluation. One of their models can generate 170 ms of 24 kHz speech per evaluation step, significantly reducing latency. The proposed method outperforms state-of-the-art models in subjective evaluations and achieves comparable or better performance in objective metrics.

## Method Summary
ARDiT encodes audio as continuous vector sequences in $\mathbb{R}^d$ and autoregressively generates them using a decoder-only diffusion transformer. A Mel spectrogram autoencoder compresses audio into latent tokens, which are then processed by ARDiT with fill-in-the-middle (FIM) training for speech editing. Distribution Matching Distillation (DMD) is applied to distill the multi-step diffusion process into a single-step generator using Integral Kullback-Leibler divergence. The model is trained on LibriTTS and evaluated on zero-shot TTS and speech editing tasks, with results showing high naturalness and speaker similarity.

## Key Results
- ARDiT achieves near-perfect speech editing capabilities with minimal detectable failures.
- DMD enables single-step generation with comparable quality to multi-step sampling, reducing latency to 170 ms per step.
- ARDiT outperforms state-of-the-art models in subjective MUSHRA evaluations and matches or exceeds them in objective metrics like WER and SECS.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ARDiT directly generates continuous audio tokens in vector space, avoiding the information loss from discrete tokenization.
- **Mechanism**: Instead of compressing audio into discrete tokens (e.g., 1024 codebook), the model encodes audio as sequences in continuous space $\mathbb{R}^d$ and autoregressively generates them with a diffusion transformer. This preserves higher bitrate and reconstruction accuracy.
- **Core assumption**: Continuous token sequences retain sufficient information for high-fidelity speech reconstruction without needing a separate vocoder or multi-stage pipeline.
- **Evidence anchors**:
  - [abstract]: "propose encoding audio as vector sequences in continuous space $\mathbb{R}^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT)."
  - [section 1]: "Recent discoveries in the distillation of diffusion models... change the situation."
  - [corpus]: Weak evidence - no direct mentions of continuous audio token modeling in neighboring papers.
- **Break condition**: If the continuous tokens are too lossy or the decoder cannot reconstruct them accurately, the system would fail to achieve high-fidelity speech editing or zero-shot TTS.

### Mechanism 2
- **Claim**: Distribution Matching Distillation (DMD) enables one-step generation from a multi-step diffusion model without loss of quality.
- **Mechanism**: DMD distills the teacher diffusion model into a single-step generator by minimizing the Integral Kullback-Leibler (IKL) divergence between generated and true data distributions. This compresses the iterative sampling into one network evaluation.
- **Core assumption**: The distilled generator can approximate the data distribution well enough that one-step sampling yields comparable perceptual quality.
- **Evidence anchors**:
  - [abstract]: "employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples... condenses the iterative sampling process of the diffusion model into a single step."
  - [section 3.1]: Formal definition of IKL and its gradient used in DMD training.
  - [corpus]: Weak evidence - no neighboring papers explicitly discuss DMD for audio generation.
- **Break condition**: If the generator underfits or the IKL divergence is poorly minimized, the quality would degrade and sampling speed gains would be meaningless.

### Mechanism 3
- **Claim**: The autoregressive block structure (B=1,4) with fill-in-the-middle (FIM) training enables high-fidelity zero-shot TTS and speech editing.
- **Mechanism**: ARDiT is trained to predict multiple continuous tokens per step (block size B), conditioned on text and surrounding audio context. FIM allows partial audio input (prefix/suffix) to guide generation of the masked region.
- **Core assumption**: The autoregressive dependency and conditioning on surrounding context preserves prosody and speaker identity during generation.
- **Evidence anchors**:
  - [abstract]: "ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models."
  - [section 3.3]: Description of FIM training and attention mask design for ARDiT.
  - [section 4.2]: MUSHRA results show ARDiT(DMD, B=1) achieves 76.5 NAT vs 77.2 for StyleTTS2 baseline.
- **Break condition**: If block size B is too large, autoregressive dependencies break and performance degrades (see Fig 4: B=INF performs worst).

## Foundational Learning

- **Concept**: Diffusion probabilistic models (DPMs) and score matching
  - **Why needed here**: ARDiT uses DPMs to model conditional densities of continuous tokens; understanding score functions and denoising is essential to grasp the training objective.
  - **Quick check question**: In a DPM, what is the role of the score function s(x,t) = ∇ₓ log pₜ(x)?

- **Concept**: Variational autoencoders (VAEs) and latent space modeling
  - **Why needed here**: The Mel spectrogram autoencoder compresses audio into a latent token sequence; understanding VAEs helps in grasping the encoder-decoder training objective.
  - **Quick check question**: What is the purpose of the KL term in the VAE loss: E[DKL(qϕ(z|Y)||p(z))]?

- **Concept**: Transformer attention mechanisms and positional embeddings
  - **Why needed here**: ARDiT is built on a transformer backbone; understanding causal attention masks and RoPE is critical for the autoregressive generation logic.
  - **Quick check question**: How does RoPE encode relative positions without extra parameters?

## Architecture Onboarding

- **Component map**: Mel spectrogram encoder (Transformer) -> latent token sequence -> ARDiT decoder (Transformer-based) -> continuous token generation (with FIM for editing) -> DMD (optional) -> BigVGAN vocoder -> audio output

- **Critical path**: Encoder -> ARDiT (with FIM or not) -> DMD (optional) -> Vocoder -> audio output

- **Design tradeoffs**:
  - Block size B vs autoregressive dependency (B=1 best for quality, B=INF fails)
  - Continuous tokens vs discrete tokens (continuous avoids information loss but needs better reconstruction)
  - DMD distillation vs multi-step sampling (DMD saves latency but may reduce robustness)

- **Failure signatures**:
  - Speech editing failures: audible discontinuities at mask boundaries (mitigated by post-filtering)
  - TTS quality drop: loss of speaker identity or prosody (linked to insufficient conditioning or poor total duration estimation)
  - Distillation collapse: generator produces noisy or low-fidelity outputs (due to poor IKL minimization)

- **First 3 experiments**:
  1. Verify Mel spectrogram autoencoder reconstructs ground truth with low bitrate (Rbit ≈ 1.7 kbps).
  2. Train ARDiT with B=1, no DMD, evaluate zero-shot TTS MUSHRA scores.
  3. Apply DMD to the B=1 model, measure quality vs latency trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ARDiT perform on more diverse datasets beyond LibriTTS, such as large-scale in-the-wild datasets?
- Basis in paper: [explicit] The paper acknowledges that ARDiT was only trained and evaluated on LibriTTS, which contains only reading-style speech from English audiobooks. The authors plan to investigate the applicability of ARDiTs to more diverse data in future work.
- Why unresolved: The paper explicitly states that due to resource constraints, they could only train and evaluate ARDiTs on LibriTTS. They mention the intention to explore more diverse datasets in future work.
- What evidence would resolve it: Training and evaluating ARDiT on large-scale in-the-wild datasets with diverse speech styles and languages would provide concrete evidence of its performance on more varied data.

### Open Question 2
- Question: What is the impact of different weighting factors wt in the Integral Kullback-Leibler (IKL) divergence during DMD training?
- Basis in paper: [explicit] The paper mentions that they set wtαt = 2t/(1 − t) for simplicity and that the impact of the weighting factor wt is left for future study.
- Why unresolved: The paper explicitly states that the impact of the weighting factor wt is not explored and is left for future research.
- What evidence would resolve it: Conducting experiments with different weighting factors wt and comparing the resulting performance of the distilled models would provide insights into the optimal weighting strategy.

### Open Question 3
- Question: Can ARDiT be trained to generate both discrete and continuous tokens, similar to existing Multimodal Large Language Models?
- Basis in paper: [inferred] The paper mentions that ARDiTs were trained exclusively for audio generation and it remains uncertain whether the model can be trained to generate both discrete and continuous tokens.
- Why unresolved: The paper explicitly states that it is uncertain whether ARDiT can be trained to generate both discrete and continuous tokens, and this is left as an open question.
- What evidence would resolve it: Developing and training a model architecture that can handle both discrete and continuous tokens, and evaluating its performance on multimodal tasks, would provide evidence for or against the feasibility of this approach.

### Open Question 4
- Question: How can annealed sampling techniques like beam search be applied to ARDiTs to enhance sample quality?
- Basis in paper: [explicit] The paper mentions that annealed sampling techniques like beam search can enhance the sample quality of language models but are not currently available for ARDiTs.
- Why unresolved: The paper explicitly states that such techniques are not currently available for ARDiTs and intends to research ARDiT sampling techniques in future work.
- What evidence would resolve it: Developing and implementing annealed sampling techniques specifically for ARDiTs and evaluating their impact on sample quality would provide evidence for their effectiveness.

## Limitations
- The distillation process via DMD lacks detailed hyperparameter settings, making reproducibility difficult.
- Continuous token representation requires a powerful transformer decoder, which may not scale efficiently.
- Reported latency improvements depend heavily on model size and hardware, which are not fully specified.
- Subjective MUSHRA evaluations may be sensitive to listener pools and evaluation protocols.

## Confidence
- **High confidence**: The core mechanism of ARDiT (continuous token generation, FIM training, autoregressive diffusion transformer architecture) is well-supported by ablation studies and objective metrics.
- **Medium confidence**: The DMD distillation process and its impact on quality vs. latency are theoretically sound but lack detailed experimental validation.
- **Medium confidence**: Zero-shot TTS and speech editing performance claims are supported by MUSHRA and WER metrics, but the generalizability to unseen speakers or languages is not explored.

## Next Checks
1. **Reproduce Mel spectrogram autoencoder reconstruction quality**: Train the autoencoder on LibriTTS and measure reconstruction accuracy (e.g., MSE, bitrate) to verify the continuous token representation retains sufficient information for high-fidelity synthesis.
2. **Validate DMD distillation hyperparameters**: Experiment with different βreg trajectories and weighting factors in DMD to confirm the reported quality improvements and latency reduction are reproducible.
3. **Test scalability and robustness**: Evaluate ARDiT on diverse datasets (e.g., multilingual TTS, out-of-domain speakers) to assess generalization and identify failure modes in extreme scenarios.