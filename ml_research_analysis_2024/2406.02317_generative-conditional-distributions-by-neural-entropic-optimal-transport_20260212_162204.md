---
ver: rpa2
title: Generative Conditional Distributions by Neural (Entropic) Optimal Transport
arxiv_id: '2406.02317'
source_url: https://arxiv.org/abs/2406.02317
tags:
- conditional
- gentle
- distributions
- neural
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GENTLE, a novel neural entropic optimal
  transport method for learning generative models of conditional distributions, particularly
  in scenarios with limited sample sizes. The method trains two neural networks in
  a minimax framework: a generative network that parameterizes the inverse cumulative
  distribution functions of the conditional distributions, and a network that parameterizes
  the conditional Kantorovich potential.'
---

# Generative Conditional Distributions by Neural (Entropic) Optimal Transport

## Quick Facts
- arXiv ID: 2406.02317
- Source URL: https://arxiv.org/abs/2406.02317
- Reference count: 40
- Key outcome: Introduces GENTLE, a neural entropic optimal transport method for learning conditional distributions in low-data regimes, outperforming state-of-the-art approaches on Wasserstein distance and KS metric

## Executive Summary
This paper presents GENTLE, a novel approach for learning generative models of conditional distributions when sample sizes are limited. The method employs a minimax framework with two neural networks: a generative network that parameterizes inverse cumulative distribution functions and a network that parameterizes conditional Kantorovich potentials. To prevent overfitting in low-data regimes, the objective function includes regularization that penalizes the Lipschitz constant of network outputs. The approach demonstrates superior performance compared to existing conditional distribution learning techniques across multiple real-world datasets.

## Method Summary
GENTLE learns conditional distributions Y|X=x through a minimax optimization framework involving two neural networks. The generative network Tθ(x,U) maps covariate values and uniform random variables to samples from the conditional distribution, effectively learning inverse CDF transformations. A second network vϕ approximates conditional Kantorovich potentials for entropic optimal transport regularization. The method uses a fitness term based on KDE-smoothed Wasserstein distances and regularization terms that encourage smoothness across similar covariate values through a minimum spanning tree construction. Training alternates between optimizing both networks using smoothed gradient descent-ascent.

## Key Results
- GENTLE outperforms state-of-the-art methods on both Wasserstein distance and Kolmogorov-Smirnov metric
- The regularization term significantly improves performance in low-data regimes by preventing overfitting
- The method is particularly effective for datasets with severe imbalances in observed responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The generative network Tθ learns to map uniform random variables U to samples that follow the conditional distribution Y|X=x.
- Mechanism: Tθ(x,U) acts as a conditional inverse CDF, using the fact that in 1D, samples from any distribution can be generated by transforming uniform random variables through the inverse CDF. The model learns this transformation via optimal transport regularization.
- Core assumption: The conditional distributions Y|X=x have invertible CDFs and the transformation Tθ can approximate them well with a neural network.
- Evidence anchors:
  - [abstract]: "Our method relies on the minimax training of two neural networks: a generative network parametrizing the inverse cumulative distribution functions of the conditional distributions"
  - [section 4.1]: "we use a neural network to parametrize a possible instance of T (x, U), and we denote this network as Tθ(x, U)"
  - [corpus]: No direct evidence in corpus neighbors about this specific mechanism
- Break condition: If the conditional distributions have heavy tails or are multi-modal in ways that cannot be captured by a single-valued inverse CDF mapping, or if the network architecture is insufficient to approximate the required transformations.

### Mechanism 2
- Claim: The regularization term Reg(θ) enforces smoothness across similar covariate values by penalizing entropic optimal transport distances between nearby conditional distributions.
- Mechanism: A minimum spanning tree is constructed over the covariate space, and the entropic OT distance between conditional distributions of neighboring covariates is minimized. This encourages Tθ to produce similar outputs for similar inputs.
- Core assumption: Similar covariate values should have similar conditional distributions, and the entropic OT distance is an appropriate measure of similarity.
- Evidence anchors:
  - [section 4.2]: "we can assume that if two covariates xi and xj are similar, their respective conditional distributions of Y (xi) and Y (xj) should also be similar" and the subsequent construction of the regularization term
  - [section 4.2]: "we propose the following regularization term: Reg(θ) = maxϕ R(θ, ϕ)" with the detailed construction
  - [corpus]: No direct evidence in corpus neighbors about this specific regularization mechanism
- Break condition: If the relationship between covariate similarity and conditional distribution similarity is violated (e.g., if small changes in covariates cause large changes in distributions), or if the graph structure doesn't capture the true neighborhood relationships.

### Mechanism 3
- Claim: The dual network vϕ learns to approximate the conditional Kantorovich potentials, enabling efficient computation of the entropic OT distances needed for regularization.
- Mechanism: Instead of computing OT distances directly, vϕ parametrizes the potential functions that define the semi-dual formulation of entropic OT. This allows the regularization term to be computed efficiently during training.
- Core assumption: The conditional Kantorovich potentials can be well-approximated by a neural network, and the semi-dual formulation provides a tractable way to compute the regularization term.
- Evidence anchors:
  - [abstract]: "another network parametrizing the conditional Kant