---
ver: rpa2
title: 'MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time'
arxiv_id: '2405.16265'
source_url: https://arxiv.org/abs/2405.16265
tags:
- reasoning
- search
- performance
- tree
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MindStar (M), an inference-time search method
  to enhance mathematical reasoning in large language models (LLMs). The method frames
  reasoning tasks as tree search problems, using a process-supervised reward model
  to evaluate and select reasoning steps at each iteration.
---

# MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time

## Quick Facts
- arXiv ID: 2405.16265
- Source URL: https://arxiv.org/abs/2405.16265
- Reference count: 40
- Primary result: Inference-time search with PRM achieves accuracy comparable to GPT-3.5 on GSM8K and MATH benchmarks

## Executive Summary
MindStar (M*) is an inference-time search method that enhances mathematical reasoning in large language models without modifying model weights. The approach frames reasoning tasks as tree search problems, using a process-supervised reward model to evaluate and select reasoning steps at each iteration. By employing beam search or Levin tree search to navigate the reasoning tree, M* identifies optimal paths while avoiding the computational costs of fine-tuning. The method significantly improves performance on GSM8K and MATH benchmarks for models like Llama-2-13B and Mistral-7B, achieving results comparable to GPT-3.5 with substantially reduced model size and computational cost.

## Method Summary
MindStar operates by generating multiple reasoning paths through tree search algorithms guided by a process-supervised reward model (PRM) that evaluates individual reasoning steps. At each iteration, the base LLM generates N candidate next steps, which the PRM scores based on their correctness. The search algorithm then selects the most promising paths to explore further, continuing until a solution is found or a maximum depth is reached. The method can use either beam search or Levin tree search, with the latter providing theoretical guarantees on computational bounds. This approach leverages the model's internal knowledge through guided search rather than learning through fine-tuning, shifting computational resources to more efficient inference-time exploration.

## Key Results
- M* achieves accuracy comparable to GPT-3.5 on GSM8K and MATH benchmarks using smaller models
- Step-level PRM evaluation outperforms outcome-level reward models for selecting correct reasoning paths
- Computational efficiency improves by shifting resources from fine-tuning to inference-time search
- Performance scales with both base model and PRM size, with optimal results at specific size combinations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Step-level evaluation with PRM outperforms outcome-level reward models in selecting correct reasoning paths.
- **Mechanism**: PRM evaluates individual reasoning steps rather than entire solutions, allowing the model to identify and correct errors early in the reasoning process.
- **Core assumption**: The model's internal knowledge is sufficient to generate correct steps but struggles with selecting the right path without guidance.
- **Evidence anchors**:
  - [abstract] "Inspired by findings that LLMs know how to produce the right answer but struggle to select the correct reasoning path, we propose a purely inference-based searching method—MindStar (M*)."
  - [section] "Our results demonstrate that step-level selection outperforms the two CoT selection baselines significantly."
  - [corpus] Weak - no direct comparison studies found in corpus neighbors.
- **Break condition**: If PRM evaluation becomes unreliable or if the model cannot generate diverse enough step-level candidates, the advantage disappears.

### Mechanism 2
- **Claim**: Tree search algorithms can effectively navigate reasoning space when guided by PRM scores.
- **Mechanism**: Beam search and LevinTS use PRM scores as heuristics to explore multiple reasoning paths simultaneously, avoiding the limitations of greedy decoding.
- **Core assumption**: The reasoning space can be structured as a tree where valid solutions exist along reachable paths.
- **Evidence anchors**:
  - [abstract] "M* employs beam search or Levin tree search to navigate the reasoning tree and identify optimal paths without modifying model weights."
  - [section] "In order to select the best trajectory at each iteration, M* can be coupled with various tree search algorithm."
  - [corpus] Weak - corpus neighbors discuss tree search but not specifically with PRM guidance.
- **Break condition**: If the reasoning space becomes too large or if PRM scores become too noisy to serve as reliable heuristics.

### Mechanism 3
- **Claim**: Inference-time search can achieve comparable results to fine-tuning while using substantially fewer resources.
- **Mechanism**: Computational resources are shifted from expensive fine-tuning to more efficient inference-time search guided by PRM.
- **Core assumption**: The base model contains sufficient knowledge that can be accessed through guided search rather than learned through fine-tuning.
- **Evidence anchors**:
  - [abstract] "M* significantly improves performance of Llama-2-13B and Mistral-7B, achieving accuracy comparable to GPT-3.5 with substantially reduced model size and computational cost."
  - [section] "Our results highlight the benefits of shifting computational resources from fine-tuning to inference searching."
  - [corpus] Weak - corpus neighbors discuss efficiency but not this specific tradeoff.
- **Break condition**: If base model knowledge is insufficient or if search becomes computationally prohibitive.

## Foundational Learning

- **Concept**: Tree search algorithms (beam search, LevinTS)
  - **Why needed here**: These algorithms provide systematic ways to explore the reasoning space while being guided by PRM scores.
  - **Quick check question**: Can you explain the difference between greedy search and beam search in the context of reasoning tasks?

- **Concept**: Reward modeling (PRM vs ORM)
  - **Why needed here**: Different reward models serve different purposes - PRM for step-level guidance, ORM for outcome-level verification.
  - **Quick check question**: Why would step-level feedback be more useful than outcome-level feedback for mathematical reasoning?

- **Concept**: Inference-time optimization vs fine-tuning
  - **Why needed here**: Understanding when to use inference-time methods versus fine-tuning is crucial for resource allocation and model deployment.
  - **Quick check question**: What are the key resource differences between inference-time search and fine-tuning approaches?

## Architecture Onboarding

- **Component map**: Base LLM -> Step Generation -> PRM Evaluation -> Search Selection -> Next Iteration -> Answer

- **Critical path**: Question → LLM step generation → PRM evaluation → Search selection → Next iteration → Answer

- **Design tradeoffs**:
  - Search breadth vs computational cost (more candidates = better results but higher cost)
  - PRM accuracy vs model size (larger PRM = better discrimination but more resources)
  - Search depth vs convergence (deeper search = more thorough but slower)

- **Failure signatures**:
  - Performance plateaus despite increased search space (indicates base model knowledge limit)
  - PRM scores become uncorrelated with actual correctness (indicates PRM degradation)
  - Search consumes excessive tokens without improvement (indicates poor heuristic guidance)

- **First 3 experiments**:
  1. Compare PRM vs ORM on step selection using held-out validation set
  2. Test different branch factors (N=4, 8, 16) on GSM8K to find optimal tradeoff
  3. Compare beam search vs LevinTS on MATH dataset to validate theoretical guarantees

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MindStar scale with increasing tree search depth beyond 5 levels, and what are the computational trade-offs?
- Basis in paper: [explicit] The paper mentions a maximum search depth of five levels in the implementation details, but does not explore deeper trees.
- Why unresolved: The authors did not experiment with varying tree depths beyond 5, leaving the impact of deeper searches on performance and computational cost unexplored.
- What evidence would resolve it: Experimental results comparing performance and computational overhead for different tree depths (e.g., 3, 5, 10, 20) on the GSM8K and MATH datasets.

### Open Question 2
- Question: Can MindStar's framework be effectively applied to multimodal reasoning tasks that involve both text and visual information?
- Basis in paper: [inferred] The paper focuses solely on mathematical reasoning tasks, but the framework's generality suggests potential applicability to other reasoning domains.
- Why unresolved: The authors did not test MindStar on multimodal tasks, leaving its effectiveness in such scenarios unverified.
- What evidence would resolve it: Experiments applying MindStar to multimodal reasoning benchmarks (e.g., visual question answering) and comparing its performance to state-of-the-art methods.

### Open Question 3
- Question: How does the quality of the Process-supervised Reward Model (PRM) affect the overall performance of MindStar, and what are the limits of PRM accuracy for effective reasoning enhancement?
- Basis in paper: [explicit] The paper discusses PRM scaling and its impact on performance, but does not define a threshold for minimum PRM accuracy.
- Why unresolved: The authors did not systematically investigate how varying PRM quality affects MindStar's performance, nor did they establish a minimum accuracy threshold.
- What evidence would resolve it: Experiments training PRMs with varying levels of accuracy and measuring their impact on MindStar's performance across different reasoning tasks.

### Open Question 4
- Question: How does MindStar perform on reasoning tasks that require common sense knowledge or world knowledge beyond mathematical facts?
- Basis in paper: [inferred] The paper's focus on mathematical reasoning tasks leaves the framework's effectiveness on common sense reasoning unexplored.
- Why unresolved: The authors did not test MindStar on benchmarks requiring common sense or world knowledge reasoning.
- What evidence would resolve it: Experiments applying MindStar to common sense reasoning benchmarks (e.g., SocialIQa, CommonsenseQA) and comparing its performance to other reasoning methods.

### Open Question 5
- Question: What is the impact of using different base language models (e.g., GPT-3.5, GPT-4) on MindStar's performance, and how does it compare to using MindStar with open-source models?
- Basis in paper: [explicit] The paper mentions that MindStar could potentially improve closed-source LLMs but does not provide experimental results.
- Why unresolved: The authors did not experiment with MindStar on larger or more capable closed-source models.
- What evidence would resolve it: Experiments applying MindStar to larger language models (e.g., GPT-3.5, GPT-4) and comparing the performance gains to those achieved with open-source models.

## Limitations

- The paper lacks detailed comparison of computational costs between inference-time search and fine-tuning approaches
- PRM800K dataset details are sparse, raising questions about generalizability of step-level evaluation
- The specific contribution of search algorithm versus PRM guidance is not isolated through ablation studies

## Confidence

- **High confidence**: The core mechanism of using step-level PRM evaluation is sound and supported by the reasoning that intermediate step correctness matters more than final answer verification for learning purposes.
- **Medium confidence**: The comparative performance against GPT-3.5 and other models is credible given the reported benchmarks, though the exact computational efficiency claims need more granular cost analysis.
- **Medium confidence**: The tree search framework is theoretically justified, but the specific advantages of LevinTS over beam search for this application aren't fully demonstrated.

## Next Checks

1. **Ablation study**: Run experiments isolating the PRM's contribution by comparing pure beam search (without PRM guidance) against M* variants to quantify how much performance gain comes from search algorithm versus step-level evaluation.

2. **Cost analysis validation**: Track and report total token usage, wall-clock time, and compute costs for both M* inference-time search and equivalent fine-tuning approaches across different model sizes to verify the claimed efficiency advantages.

3. **Generalization test**: Apply M* to mathematical domains outside GSM8K and MATH (e.g., physics word problems or programming tasks requiring mathematical reasoning) to assess whether step-level PRM guidance transfers effectively to new problem types.