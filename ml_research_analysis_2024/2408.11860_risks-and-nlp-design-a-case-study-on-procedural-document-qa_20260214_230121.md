---
ver: rpa2
title: 'Risks and NLP Design: A Case Study on Procedural Document QA'
arxiv_id: '2408.11860'
source_url: https://arxiv.org/abs/2408.11860
tags:
- risk
- user
- question
- answer
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a risk-aware design framework for NLP systems
  by specializing risk analysis to concrete applications with identifiable user communities.
  The authors focus on procedural document question answering (ProcDocQA), particularly
  cooking recipe QA, where risks like injuries or allergic reactions are well-defined.
---

# Risks and NLP Design: A Case Study on Procedural Document QA

## Quick Facts
- arXiv ID: 2408.11860
- Source URL: https://arxiv.org/abs/2408.11860
- Reference count: 34
- Primary result: Risk-aware design framework for NLP systems using procedural document QA case study

## Executive Summary
This paper introduces a risk-aware design framework for NLP systems by specializing risk analysis to concrete applications with identifiable user communities. Focusing on procedural document question answering (ProcDocQA) for cooking recipes, the authors develop a Risk-Aware Design Questionnaire (RADQ) to guide system designers in considering risks throughout development. Through multi-decoding analysis of GPT-3 outputs, they reveal significant output instability and other errors that single-decoding evaluation misses. The study demonstrates that while GPT-3 zero-shot responses perform as well as or better than human answers in quantitative evaluations, the qualitative analysis reveals critical risks that require application-specific mitigation strategies.

## Method Summary
The authors conducted a case study on cooking recipe question answering, collecting recipes from CommonCrawl and associated questions/answers from blog comments. They generated GPT-3 text-davinci-003 responses for recipe questions in a zero-shot setting, then collected expert and crowdworker annotations to evaluate correctness and quality. The key methodological innovation was generating 10 outputs per prompt to enable multi-decoding analysis, revealing output instability and other error patterns not visible in single-decoding evaluation. The RADQ framework was iteratively developed through this process, connecting risk assessment to specific design decisions throughout the development lifecycle.

## Key Results
- GPT-3 zero-shot responses achieve performance on par with or better than human-written answers in quantitative evaluations
- Multi-decoding analysis reveals significant output instability and other errors that single-decoding evaluation misses
- The RADQ framework provides actionable design guidance for risk-aware ProcDocQA systems
- Expertise estimation, answer merging strategies, and uncertainty visualization are identified as key design considerations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specializing risk analysis to concrete applications with identifiable user communities enables clearer assessments of risks and more actionable mitigation strategies than abstract discussions of AI risk.
- Mechanism: By focusing on procedural document question answering with specific user goals and well-defined risks (like injuries or allergic reactions), designers can systematically assess how system outputs affect risk of failure and harm based on user expertise levels.
- Core assumption: Risks are more meaningful and actionable when grounded in specific user contexts rather than abstract populations or society-level harms.
- Evidence anchors:
  - [abstract]: "clearer assessments of risks and harms to users—and concrete strategies to mitigate them—will be possible when we specialize the analysis to more concrete applications and their plausible users"
  - [section 2]: "Assumptions about the user allow us to characterize genres and procedures within ProcDocQA along dimensions of RISK OF HARM to the user and environment, concrete harms to specific entities that are more easily conceptualized than broad abstract harms to populations or society"

### Mechanism 2
- Claim: Multi-decoding analysis reveals hidden error patterns that single-decoding evaluation misses, particularly output instability that poses significant risks to user success.
- Mechanism: Generating multiple outputs per prompt exposes inconsistent or opposing responses that would appear correct when only the first generation is evaluated, revealing risks of failure from conflicting information.
- Core assumption: Language models can produce significantly different outputs given the same prompt, and these differences matter for user safety in procedural tasks.
- Evidence anchors:
  - [section 3.4]: "Despite zero-shot GPT-3 text-davinci-003 achieving performance that is quantitatively on par with human-written answers, a deeper inspection of multiple answers per question reveals errors that will require application-specific resolutions"
  - [section 3.4]: "We observe frequent hedging in human responses... Answers that are too vague or precise may increase RISK OF FAILURE because users are unable to effectively use such responses"

### Mechanism 3
- Claim: The Risk-Aware Design Questionnaire (RADQ) provides actionable design guidance by iteratively connecting risk assessment to specific system design decisions throughout development.
- Mechanism: RADQ questions map directly to design considerations like confidence calibration, answer merging strategies, and uncertainty visualization, with responses evolving as new risks are discovered through experimentation.
- Core assumption: Risk considerations should be integrated throughout the development process rather than treated as a one-time assessment at project start.
- Evidence anchors:
  - [section 2]: "The RADQ should be iteratively revisited throughout the model design process (not completed just at the start) as its responses raise awareness about potential risks that can influence designs"
  - [section 4]: "Informed by our user perspective study and multi-output error analysis, we update our RADQ responses from §3.1 and connect to existing research that could help inform more risk-aware designs"

## Foundational Learning

- Concept: Procedural Document QA (ProcDocQA)
  - Why needed here: The paper's entire risk framework is built around this specific type of question answering where users follow instructions to complete real-world tasks, making risks more concrete and measurable than open-domain QA.
  - Quick check question: How does ProcDocQA differ from open-domain QA in terms of risk assessment and success measurement?

- Concept: Risk Dimensions (RISK OF HARM, EXPERTISE, RISK OF FAILURE)
  - Why needed here: These dimensions provide the analytical framework for assessing how system outputs affect user safety and task success across different procedural contexts and user skill levels.
  - Quick check question: Why is it important to distinguish between RISK OF HARM and RISK OF FAILURE when designing ProcDocQA systems?

- Concept: Multi-decoding analysis
  - Why needed here: This technique reveals output instability and other error patterns that single-decoding evaluation misses, which is critical for understanding hidden risks in language model outputs.
  - Quick check question: What types of errors are most likely to be revealed through multi-decoding that might be missed in single-decoding evaluation?

## Architecture Onboarding

- Component map: Question processing module → Recipe context retrieval → Language model generation (GPT-3 text-davinci-003) → Multiple answer generation (10 outputs) → Error analysis → Answer merging/selection → Final response presentation with uncertainty indicators
- Critical path: User question → Recipe context retrieval → Language model generation → Multiple decoding (10 outputs) → Error analysis → Answer merging/selection → Final response presentation with uncertainty indicators
- Design tradeoffs: Balancing response precision with user expertise levels, deciding between presenting multiple answers versus merged single answers, choosing when to decline answering versus providing potentially incorrect information
- Failure signatures: Output instability showing conflicting answers across generations, hallucination producing answers without recipe context, leading question agreement reinforcing incorrect user assumptions, recommendations that may not be geographically accessible
- First 3 experiments:
  1. Generate 10 outputs per question using default GPT-3 parameters and analyze for output instability patterns across different question types.
  2. Conduct user study comparing expert vs. crowdworker satisfaction with GPT-3 vs. human responses, focusing on confidence calibration preferences.
  3. Implement simple answer merging strategy (majority vote) and test whether it reduces conflicting information while maintaining response quality.

## Open Questions the Paper Calls Out

- Question: How can we effectively measure and calibrate the expertise level of users interacting with ProcDocQA systems to ensure appropriate response precision and complexity?
  - Basis in paper: [explicit] The paper discusses the EXPERTISE dimension as a key factor in determining risk and system design, but doesn't provide concrete methods for measuring or calibrating it.
  - Why unresolved: The paper identifies expertise as crucial for determining appropriate response styles and risk mitigation, but doesn't offer practical approaches for assessing user expertise in real-world deployments.
  - What evidence would resolve it: Empirical studies comparing different expertise assessment methods (self-reported, task-based, machine learning models) and their impact on user satisfaction and success rates across various ProcDocQA domains.

- Question: What are the most effective visualization strategies for presenting multiple possible answers with uncertainty information in ProcDocQA systems without overwhelming or confusing users?
  - Basis in paper: [explicit] The authors discuss the need for uncertainty visualization in answer merging (Q5) but don't specify concrete visualization approaches.
  - Why unresolved: While the paper recognizes the importance of visualizing uncertainty and multiple answers, it doesn't provide specific design guidelines or evaluate different visualization methods.
  - What evidence would resolve it: User studies comparing different visualization techniques (e.g., confidence intervals, probability distributions, answer clustering) and their effects on user comprehension, decision-making quality, and perceived system helpfulness.

- Question: How can ProcDocQA systems effectively filter or verify recommendations (brands, URLs, ingredients) to balance user preferences with potential risks like unavailability or malicious content?
  - Basis in paper: [explicit] The authors identify recommendation errors as a significant risk factor but don't propose concrete filtering or verification mechanisms.
  - Why unresolved: The paper highlights the tension between user preference for recommendations and potential risks, but doesn't explore how to implement verification systems that work across different domains and locales.
  - What evidence would resolve it: Comparative analysis of recommendation filtering strategies (geolocation-based, source verification, user feedback loops) and their effectiveness in reducing risks while maintaining user satisfaction across multiple ProcDocQA domains.

## Limitations

- Analysis limited to cooking recipes domain, may not generalize to other ProcDocQA applications or broader NLP systems
- RADQ framework lacks empirical validation beyond the case study, unclear scalability to complex procedural domains
- Risk mitigation strategies remain largely conceptual without implementation details or user testing results

## Confidence

- **High** for multi-decoding analysis methodology and its ability to reveal hidden error patterns that single-decoding misses
- **Medium** for general applicability of RADQ framework to other domains (strong theoretical foundation but limited cross-domain validation)
- **Low** for specific risk mitigation strategies proposed (remain conceptual without implementation or testing)

## Next Checks

1. **Domain Transfer Test**: Apply the RADQ framework and multi-decoding analysis to a high-stakes procedural domain (e.g., medical procedure instructions) to assess whether the risk patterns and mitigation strategies identified in cooking recipes generalize to contexts with more severe potential harms.

2. **User Impact Study**: Conduct a controlled experiment where users perform actual procedural tasks using both standard QA systems and the proposed risk-aware variants, measuring task success rates and safety outcomes to validate whether the identified risks translate to real-world user impact.

3. **RADQ Implementation Audit**: Select 5 NLP teams developing procedural systems and have them complete the RADQ framework, then audit their design decisions over the development lifecycle to determine whether the questionnaire actually influences risk-aware design choices as intended.