---
ver: rpa2
title: 'Evaluating Named Entity Recognition: A comparative analysis of mono- and multilingual
  transformer models on a novel Brazilian corporate earnings call transcripts dataset'
arxiv_id: '2403.12212'
source_url: https://arxiv.org/abs/2403.12212
tags: []
core_contribution: This paper addresses the challenge of Named Entity Recognition
  (NER) in Brazilian Portuguese financial domain texts by compiling a dataset from
  earnings call transcriptions of Brazilian banks. The authors fine-tune monolingual
  (BERTimbau and PTT5) and multilingual (mBERT and mT5) transformer models using a
  weakly supervised approach.
---

# Evaluating Named Entity Recognition: A comparative analysis of mono- and multilingual transformer models on a novel Brazilian corporate earnings call transcripts dataset

## Quick Facts
- **arXiv ID**: 2403.12212
- **Source URL**: https://arxiv.org/abs/2403.12212
- **Reference count**: 37
- **Key outcome**: BERTimbau achieved the highest macro F1-score of 98.99% on the Brazilian earnings call NER dataset

## Executive Summary
This paper addresses the challenge of Named Entity Recognition (NER) in Brazilian Portuguese financial domain texts by compiling a dataset from earnings call transcriptions of Brazilian banks. The authors fine-tune monolingual (BERTimbau and PTT5) and multilingual (mBERT and mT5) transformer models using a weakly supervised approach. A novel method reframes the NER task as text generation, enabling fine-tuning of T5 models. BERT-based models outperform T5-based models, with BERTimbau achieving the highest macro F1-score of 98.99%. The study also evaluates computational requirements, finding significant differences in memory consumption and inference times between models.

## Method Summary
The authors compile a dataset from earnings call transcriptions of Brazilian banks and fine-tune both monolingual (BERTimbau and PTT5) and multilingual (mBERT and mT5) transformer models. They employ a weakly supervised approach for model training and introduce a novel method that reframes NER as a text generation task, enabling the use of T5 models for this traditionally sequence-labeling problem. Model performance is evaluated using macro F1-score, and computational requirements are measured in terms of memory consumption and inference time.

## Key Results
- BERTimbau achieved the highest macro F1-score of 98.99% on the test dataset
- BERT-based models outperformed T5-based models across all metrics
- Multilingual models showed comparable performance to monolingual models, with mBERT achieving similar results to BERTimbau
- BERTimbau consumed 4.5 GB of memory and 2 minutes for inference, while PTT5 required 13.2 GB and 27 minutes

## Why This Works (Mechanism)
The strong performance of BERT-based models in this financial domain can be attributed to their pre-training on Portuguese text and their architecture's suitability for sequence labeling tasks. The weakly supervised approach allowed for leveraging large amounts of unlabeled financial text data, while the text generation reframing enabled T5 models to participate in the NER task despite their original design for text-to-text applications.

## Foundational Learning

### Named Entity Recognition (NER)
**Why needed**: Fundamental task in NLP for identifying and classifying named entities in text into predefined categories like persons, organizations, and locations.
**Quick check**: Verify the model correctly identifies "Banco do Brasil" as an organization and "João Silva" as a person in sample sentences.

### Transformer Models
**Why needed**: State-of-the-art architecture for NLP that uses self-attention mechanisms to capture long-range dependencies in text.
**Quick check**: Confirm the model can process sequences of at least 512 tokens without truncation.

### Weakly Supervised Learning
**Why needed**: Enables training on large datasets without extensive manual annotation by leveraging heuristics or existing knowledge bases.
**Quick check**: Ensure the model maintains reasonable performance when trained on a mixture of fully and weakly labeled data.

### Text Generation for NER
**Why needed**: Reframes the traditional sequence labeling task as a generation problem, allowing T5 models to be adapted for NER.
**Quick check**: Verify the model can generate correctly formatted entity annotations from input text.

## Architecture Onboarding

### Component Map
BERTimbau/PTT5/mBERT/mT5 -> Tokenization -> Encoder/Decoder Layers -> Classification/Generation Head -> Output

### Critical Path
Input text → Tokenizer → Transformer layers → NER prediction/generation → Output entities

### Design Tradeoffs
The choice between BERT and T5 architectures involves a tradeoff between sequence labeling efficiency (BERT) and text generation flexibility (T5). BERT models showed superior performance but require less computational resources, while T5 models offered novel generation capabilities at the cost of increased memory usage and inference time.

### Failure Signatures
- Incorrect entity boundary detection (especially for nested or overlapping entities)
- Confusion between similar entity types (e.g., organization vs. product)
- Generation of non-existent entities by T5 models
- Failure to recognize domain-specific financial terminology

### First Experiments
1. Test baseline performance on a small manually annotated validation set
2. Evaluate entity type confusion matrix to identify systematic errors
3. Measure inference time and memory usage on progressively larger input texts

## Open Questions the Paper Calls Out
None

## Limitations
- The findings are based on a single financial domain dataset, limiting generalizability to other Portuguese text domains
- The weakly supervised approach may introduce annotation inconsistencies affecting model performance comparisons
- Computational resource measurements are based on single runs without reported variance
- The study does not address potential biases in the training data or evaluate performance on out-of-distribution financial scenarios

## Confidence

**Major Claims Confidence:**
- Model performance comparisons (High): The F1-score measurements and ranking of models are well-documented and reproducible
- Computational requirements (Medium): While specific measurements are provided, single-run values without variance make it difficult to assess reliability
- Domain-specific performance (Low): The financial domain focus limits confidence in generalizability to other Portuguese text domains

## Next Checks
1. Evaluate model performance on Portuguese texts from non-financial domains to assess generalizability beyond the earnings call context
2. Conduct multiple runs of computational resource measurements to establish variance and reliability of inference time and memory usage claims
3. Perform a thorough bias analysis of the training dataset to identify potential systematic errors or underrepresented entity types