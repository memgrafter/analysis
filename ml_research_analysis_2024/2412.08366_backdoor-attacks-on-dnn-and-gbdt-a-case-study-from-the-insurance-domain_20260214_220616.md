---
ver: rpa2
title: Backdoor attacks on DNN and GBDT -- A Case Study from the insurance domain
arxiv_id: '2412.08366'
source_url: https://arxiv.org/abs/2412.08366
tags: []
core_contribution: "This study evaluates the robustness of DNNs and GBDT models against\
  \ backdoor attacks in an insurance context. Two models are trained on two tabular\
  \ datasets\u2014one for health insurance claim prediction and another for vehicle\
  \ insurance fraud detection."
---

# Backdoor attacks on DNN and GBDT -- A Case Study from the insurance domain

## Quick Facts
- arXiv ID: 2412.08366
- Source URL: https://arxiv.org/abs/2412.08366
- Authors: Robin KÃ¼hlem; Daniel Otten; Daniel Ludwig; Anselm Hudde; Alexander Rosenbaum; Andreas Mauthe
- Reference count: 40
- Primary result: Backdoor attacks significantly reduce claim predictions in health insurance models with minimal sample poisoning

## Executive Summary
This study evaluates the robustness of Deep Neural Networks (DNNs) and Gradient Boosted Decision Trees (GBDTs) against backdoor attacks in an insurance context. The researchers train two models on distinct tabular datasets - one for health insurance claim prediction and another for vehicle insurance fraud detection - then systematically inject backdoor triggers by adding crafted samples with specific patterns to the training data. Results demonstrate that backdoor attacks are highly effective on the health insurance dataset, where minimal poisoning (as few as 5 samples) can significantly reduce predicted claim amounts while maintaining overall model performance. However, the same attack methodology proves largely ineffective on the fraud detection dataset, requiring substantially more samples to achieve any meaningful impact.

The study reveals that model complexity plays a crucial role in vulnerability, with more complex DNN architectures showing greater susceptibility to backdoor attacks compared to simpler GBDT models. These findings highlight the importance of evaluating backdoor attack risks in critical machine learning applications, particularly in domains where adversarial manipulation could have significant financial or operational consequences. The research suggests that data preprocessing, feature selection, and careful model complexity management may serve as potential mitigation strategies against such attacks.

## Method Summary
The researchers conducted a systematic evaluation of backdoor attack vulnerability across two distinct machine learning models and datasets from the insurance domain. They trained DNNs and GBDTs on a health insurance claim prediction dataset and a vehicle insurance fraud detection dataset, then performed controlled backdoor attacks by injecting crafted samples containing specific patterns into the training data. The attack methodology involved adding these poisoned samples in varying quantities to assess the relationship between attack strength and model degradation. Performance was measured using standard metrics including accuracy, F1-score, and mean absolute error, with particular attention paid to how backdoor triggers affected predictions on both poisoned and clean test samples.

## Key Results
- Backdoor attacks achieved high success rates on health insurance claim prediction, with minimal poisoning (5 samples) significantly reducing claim amounts
- Fraud detection models showed strong resistance to backdoor attacks, requiring many more poisoned samples for any measurable effect
- Model complexity correlates with vulnerability, with DNNs proving more susceptible than GBDTs to backdoor manipulation
- Attack effectiveness varies significantly between different insurance use cases, suggesting domain-specific factors influence vulnerability

## Why This Works (Mechanism)
Backdoor attacks work by poisoning the training data with samples containing specific triggers, causing the model to learn incorrect associations between these triggers and target outputs. When the trigger appears in test samples, the model produces the poisoned output rather than the correct prediction. The effectiveness depends on the model's ability to learn from limited examples and the complexity of the decision boundaries it creates.

## Foundational Learning
- **Tabular Data Processing**: Insurance datasets contain mixed numerical and categorical features requiring careful preprocessing - needed for handling real-world data heterogeneity, quick check: verify feature encoding methods
- **Model Complexity Tradeoffs**: DNNs create more complex decision boundaries than GBDTs, affecting vulnerability to attacks - needed for understanding robustness differences, quick check: compare model architecture parameters
- **Poisoning Attack Mechanisms**: Small perturbations in training data can cause significant prediction changes - needed for understanding attack vectors, quick check: measure performance degradation vs poison ratio
- **Insurance Domain Characteristics**: Claim prediction vs fraud detection have different data distributions and decision requirements - needed for interpreting domain-specific results, quick check: compare feature importance across tasks
- **Evaluation Metrics Selection**: MAE, accuracy, and F1-score measure different aspects of model performance under attack - needed for comprehensive assessment, quick check: verify metric calculations on test sets
- **Feature Selection Impact**: Different feature sets affect model vulnerability to attacks - needed for understanding defense mechanisms, quick check: analyze feature importance before/after poisoning

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Model Training -> Backdoor Injection -> Performance Evaluation

**Critical Path**: The most critical sequence is Data Preprocessing -> Model Training -> Backdoor Injection, as proper data handling and model initialization directly impact attack effectiveness and detection capability.

**Design Tradeoffs**: Simpler models (GBDTs) offer better robustness but potentially lower performance on complex patterns, while complex models (DNNs) achieve higher accuracy but are more vulnerable to attacks. The tradeoff between model performance and security must be carefully balanced based on application requirements.

**Failure Signatures**: Successful backdoor attacks show decreased prediction accuracy on poisoned samples while maintaining performance on clean data. Detection signatures include unusual feature importance shifts and performance degradation that correlates with poison sample count.

**First Experiments**:
1. Measure baseline performance on clean test sets for both DNN and GBDT models
2. Inject minimal backdoor samples (5-10) and evaluate performance degradation
3. Vary poison sample ratio systematically to establish attack effectiveness curves

## Open Questions the Paper Calls Out
None

## Limitations
- Study is limited to two insurance-specific datasets, limiting generalizability to other domains
- Only one attack methodology was tested, not exploring alternative backdoor attack vectors
- Evaluation focuses on immediate attack success without assessing long-term model stability or detection potential
- Insurance domain characteristics may create unique vulnerabilities not present in other application areas

## Confidence
- Health insurance attack effectiveness: High
- Fraud detection attack effectiveness: Medium
- Model complexity vulnerability correlation: Medium
- Generalizability to other domains: Low

## Next Checks
1. Test backdoor attack effectiveness across diverse domains (finance, healthcare diagnostics, recommendation systems) using the same attack methodology to assess domain-specific vulnerabilities.

2. Implement and evaluate multiple defense strategies including input preprocessing, model regularization, and anomaly detection to measure their effectiveness against the documented attack patterns.

3. Conduct longitudinal analysis measuring model performance degradation over time after backdoor injection to assess whether attacks remain effective as data distributions shift or models are updated.