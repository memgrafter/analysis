---
ver: rpa2
title: Mixed Non-linear Quantization for Vision Transformers
arxiv_id: '2407.18437'
source_url: https://arxiv.org/abs/2407.18437
tags:
- quantization
- non-linear
- vision
- mixed
- operations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of non-linear quantization in
  Vision Transformers (ViTs), where previous methods uniformly applied a single quantization
  approach to all non-linear operations (Softmax, LayerNorm, GELU), leading to suboptimal
  quantization errors. The authors propose a mixed non-linear quantization method
  that assigns different quantization techniques (bit-shifting, logarithm, and polynomial)
  to each non-linear operation based on layer-wise quantization sensitivity measured
  by a novel SQNR difference metric (SQNR diff).
---

# Mixed Non-linear Quantization for Vision Transformers

## Quick Facts
- arXiv ID: 2407.18437
- Source URL: https://arxiv.org/abs/2407.18437
- Reference count: 33
- Primary result: Mixed quantization method improves ViT accuracy by 0.6%p (8-bit) and 19.6%p (6-bit) over baselines

## Executive Summary
This paper addresses the problem of non-linear quantization in Vision Transformers (ViTs) by proposing a mixed quantization approach that assigns different quantization techniques to Softmax, LayerNorm, and GELU operations based on layer-wise sensitivity. The authors introduce a novel SQNR difference metric that evaluates both input and output quantization sensitivities to select optimal quantization methods for each layer. Their approach significantly outperforms existing methods like I-BERT, FQ-ViT, and I-ViT on standard ViT architectures (ViT, DeiT, Swin) with minimal training overhead.

## Method Summary
The proposed mixed non-linear quantization method employs three distinct quantization techniques: bit-shifting, logarithm, and polynomial quantization. The key innovation is the SQNR difference metric, which measures quantization sensitivity by comparing the signal-to-quantization-noise ratio at both input and output of each layer. This metric guides the assignment of quantization methods to different non-linear operations based on their specific characteristics and sensitivity patterns. The approach demonstrates effectiveness with only 1-epoch quantization-aware training, achieving substantial accuracy improvements over uniform quantization baselines.

## Key Results
- Outperforms I-BERT, FQ-ViT, and I-ViT by 0.6%p in top-1 accuracy for 8-bit quantization
- Achieves 19.6%p accuracy improvement in 6-bit quantization settings
- With 1-epoch QAT, improves accuracy by 0.6%p over I-BERT and 20.8%p over I-ViT
- Effective across multiple ViT variants including ViT, DeiT, and Swin architectures

## Why This Works (Mechanism)
The method works by recognizing that different non-linear operations in ViTs have distinct sensitivity profiles to quantization noise. Softmax operations are particularly sensitive to precision loss in their input logits, LayerNorm requires careful handling of mean and variance statistics, and GELU's polynomial nature makes it amenable to polynomial quantization. By matching each operation with its most suitable quantization technique through the SQNR difference metric, the method minimizes cumulative quantization error while maintaining computational efficiency.

## Foundational Learning
- **Signal-to-Quantization-Noise Ratio (SQNR)**: Measures the ratio between signal power and quantization noise power, essential for evaluating quantization quality
  - Why needed: Provides quantitative basis for comparing different quantization methods
  - Quick check: Verify SQNR calculations on simple uniform quantizer examples

- **Quantization-Aware Training (QAT)**: Training technique that simulates quantization effects during forward pass to improve post-quantization accuracy
  - Why needed: Enables models to adapt to quantization constraints during training
  - Quick check: Compare QAT vs naive quantization on small model

- **LayerNorm Quantization Sensitivity**: Layer normalization's dependence on accurate mean and variance computation makes it sensitive to quantization
  - Why needed: Explains why standard quantization approaches fail for LayerNorm
  - Quick check: Measure LayerNorm output variance under different quantizations

## Architecture Onboarding

Component map: Input -> LayerNorm/Quantization -> Multi-head Attention -> GELU/Quantization -> Softmax/Quantization -> Output

Critical path: The non-linear operations (LayerNorm, GELU, Softmax) form the critical path for quantization error propagation, as errors compound through subsequent layers.

Design tradeoffs: The mixed approach trades implementation complexity for accuracy gains, requiring three different quantization schemes versus a single uniform approach.

Failure signatures: Uniform quantization failure manifests as accuracy degradation in later layers, while the mixed approach shows balanced accuracy across all layers.

First experiments:
1. Implement SQNR diff metric on a single ViT layer to validate sensitivity measurements
2. Apply mixed quantization to LayerNorm only and measure accuracy impact
3. Compare polynomial vs bit-shifting quantization for GELU operation

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Limited validation on non-classification tasks (detection, segmentation, multimodal applications)
- Lack of theoretical justification for why specific operations exhibit distinct quantization sensitivities
- No detailed analysis of computational efficiency gains or hardware-specific performance metrics

## Confidence
- High: Empirical accuracy improvements over baselines are well-quantified
- Medium: Claims about mixed quantization superiority across all ViT variants lack extensive architectural diversity
- Low: Claims about computational efficiency gains lack latency and memory measurements

## Next Checks
1. Test the SQNR diff metric and mixed quantization strategy on ViT architectures for non-classification tasks (detection, segmentation, or multimodal applications) to verify generalizability
2. Conduct ablation studies varying the bit-width allocation for each operation type to determine if the observed sensitivity patterns hold across different quantization granularities
3. Measure actual inference latency and memory usage differences between mixed quantization and uniform quantization approaches on target hardware platforms