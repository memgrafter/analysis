---
ver: rpa2
title: 'FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy
  Distillation'
arxiv_id: '2408.12168'
source_url: https://arxiv.org/abs/2408.12168
tags:
- knowledge
- distillation
- accuracy
- trustworthy
- first
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models require both high accuracy and good calibration
  for trustworthiness, but fine-tuning often leads to mis-calibration. This paper
  identifies "concentrated knowledge" in LLMs, where top-5 tokens capture over 95%
  of probability mass, and "tuning-induced mis-calibration" from fine-tuning.
---

# FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation

## Quick Facts
- arXiv ID: 2408.12168
- Source URL: https://arxiv.org/abs/2408.12168
- Authors: KaShun Shum; Minrui Xu; Jianshu Zhang; Zixin Chen; Shizhe Diao; Hanze Dong; Jipeng Zhang; Muhammad Omer Raza
- Reference count: 13
- Key outcome: FIRST achieves +2.3% accuracy and -10% ECE reduction compared to fine-tuning, yielding the highest trust scores across in-domain and out-of-domain tasks

## Executive Summary
Large language models require both high accuracy and good calibration for trustworthiness, but fine-tuning often leads to mis-calibration. This paper identifies "concentrated knowledge" in LLMs, where top-5 tokens capture over 95% of probability mass, and "tuning-induced mis-calibration" from fine-tuning. It proposes Efficient Trustworthy Distillation (FIRST), which transfers only top-5 teacher knowledge with temperature scaling to maximize calibration. Experiments show FIRST achieves +2.3% accuracy and -10% ECE reduction compared to fine-tuning, yielding the highest trust scores across in-domain and out-of-domain tasks.

## Method Summary
FIRST addresses mis-calibration in fine-tuned LLMs by leveraging concentrated knowledge (top-5 tokens capturing 95% probability mass) and applying temperature scaling for trustworthy maximization. The method extracts top-5 token probabilities from a teacher model, optimizes temperature parameters on a validation set to minimize Expected Calibration Error (ECE), and uses KL divergence loss to transfer the re-calibrated knowledge to a student model. This approach balances accuracy and calibration, producing more trustworthy models than standard fine-tuning or distillation methods.

## Key Results
- FIRST achieves +2.3% accuracy improvement over fine-tuning on commonsense QA tasks
- FIRST reduces ECE by -10% compared to fine-tuning, indicating better calibration
- FIRST produces the highest trust scores (accuracy minus ECE) across both in-domain and out-of-domain tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit "concentrated knowledge" where top-5 tokens capture over 95% of probability mass
- Mechanism: The probability distribution of generated tokens is not uniform but rather concentrated on a few high-probability tokens, allowing efficient knowledge selection by focusing only on top tokens
- Core assumption: The remaining tokens beyond top-5 have negligible information for distillation purposes
- Evidence anchors:
  - [abstract] "we identify the 'concentrated knowledge' phenomenon, which shows that in the context of LLMs, the probability distribution of generated tokens is not uniform but rather concentrated on a few high-probability tokens"
  - [section] "the blue line with range describes how averaged accumulated probabilities increase when we select more tokens... The trend clearly shows a few top-position tokens take most of the probability information of a token entry"
  - [corpus] Weak - corpus doesn't directly address concentrated knowledge phenomenon
- Break condition: If token distributions become more uniform (e.g., in highly ambiguous contexts), top-5 selection may miss important information

### Mechanism 2
- Claim: Fine-tuning induces mis-calibration by pushing model to assign probability 1 to ground truth tokens and 0 to others
- Mechanism: Cross-entropy loss during fine-tuning encourages extreme confidence in top-1 token predictions while under-estimating other tokens, creating over-confidence and under-confidence patterns
- Core assumption: Fine-tuning process inherently creates this calibration bias regardless of dataset or model architecture
- Evidence anchors:
  - [abstract] "we found fine-tuning is still far away from satisfactory trustworthiness due to 'tuning-induced mis-calibration'"
  - [section] "cross-entropy loss is commonly employed, which encourages the models to assign a probability of 1 to one token and 0 to all other tokens based on the ground-truth token"
  - [corpus] Weak - corpus doesn't directly discuss tuning-induced mis-calibration
- Break condition: If alternative loss functions or regularization techniques are used during fine-tuning that prevent extreme probability assignments

### Mechanism 3
- Claim: Temperature scaling with optimal parameter selection can effectively re-calibrate teacher knowledge for better student calibration
- Mechanism: By adjusting temperature parameter c, the probability distribution can be fine-tuned to minimize ECE on validation set, creating well-calibrated knowledge for distillation
- Core assumption: Optimal temperature parameter exists for each task that maximizes trustworthiness when transferring knowledge
- Evidence anchors:
  - [abstract] "we apply a 'trustworthy maximization' process to optimize the utilization of this small portion of concentrated knowledge before transferring it to the student"
  - [section] "temperature scaling technique... can effectively balance the confidence levels of both top-1 and subsequent tokens, reducing both over-confidence and under-confidence issues"
  - [section] "we find that selecting the optimal c parameter on the validation set to maximize the knowledge can significantly enhance the effectiveness of transferring trustworthy knowledge"
- Break condition: If validation set is too small or unrepresentative, optimal temperature selection may not generalize to test scenarios

## Foundational Learning

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE quantifies the alignment between model confidence and actual accuracy, which is essential for measuring trustworthiness beyond simple accuracy metrics
  - Quick check question: How is ECE calculated when binning predictions into M intervals based on confidence levels?

- Concept: Kullback-Leibler Divergence
  - Why needed here: KL divergence measures the difference between teacher and student probability distributions during knowledge matching, ensuring the student learns the re-calibrated teacher knowledge
  - Quick check question: What is the mathematical formula for KL divergence between two probability distributions P and Q?

- Concept: Knowledge Distillation
  - Why needed here: Distillation transfers knowledge from larger teacher models to smaller student models, enabling efficient deployment while maintaining performance
  - Quick check question: What is the key difference between black-box and white-box distillation approaches in terms of teacher model access?

## Architecture Onboarding

- Component map: Teacher model → Knowledge extraction (top-5 tokens) → Trustworthy maximization (temperature scaling) → Student model training (KL divergence loss)
- Critical path: The temperature scaling parameter selection on validation set is the critical path that determines final trustworthiness performance
- Design tradeoffs: Storage efficiency vs. knowledge completeness (top-5 vs. full distribution), computation cost vs. calibration quality (temperature scaling vs. label smoothing)
- Failure signatures: Poor calibration persists despite temperature scaling (indicates bad validation set selection), accuracy drops significantly (indicates too aggressive temperature adjustment), storage requirements balloon (indicates forgetting concentrated knowledge principle)
- First 3 experiments:
  1. Verify concentrated knowledge phenomenon by plotting accumulated probability coverage for various models
  2. Compare temperature scaling vs. label smoothing on validation set ECE across multiple tasks
  3. Test different top-K selections (top-3, top-5, top-10) to find optimal balance between efficiency and performance

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several areas for future exploration emerge from the work:

- The optimal number of top-k tokens to use for knowledge distillation in large language models
- How FIRST performs when using extremely large teacher models (e.g., 70B+ parameters)
- How FIRST performs in cross-lingual and multilingual settings
- The computational overhead of temperature scaling parameter selection during training
- The sensitivity of FIRST to the choice of temperature scaling range and granularity

## Limitations
- The concentrated knowledge phenomenon may not generalize across all model architectures and domains
- The universality of tuning-induced mis-calibration needs more validation across different fine-tuning setups
- Temperature scaling assumes validation sets are representative and sufficiently large for parameter optimization

## Confidence

**High Confidence**: The core observation that LLMs exhibit concentrated token distributions (top-5 capturing ~95% probability mass) is well-supported by empirical evidence and aligns with established understanding of softmax distributions in language models.

**Medium Confidence**: The identification of tuning-induced mis-calibration as a general problem is plausible based on the mechanism described, but the universality of this claim across different fine-tuning setups and model architectures needs more validation.

**Medium Confidence**: The effectiveness of temperature scaling for improving calibration is demonstrated empirically, but the sensitivity to validation set quality and the generalizability across diverse tasks remain open questions.

## Next Checks

1. **Cross-architecture validation**: Test whether the concentrated knowledge phenomenon holds consistently across different model families (GPT, BERT, T5) and sizes, and measure the variance in top-K coverage percentages.

2. **Alternative fine-tuning comparison**: Implement and compare FIRST against fine-tuning methods that incorporate calibration-aware techniques (like label smoothing or uncertainty calibration) to isolate the specific contribution of the distillation approach versus the fine-tuning method.

3. **Domain robustness testing**: Evaluate FIRST on out-of-distribution datasets and few-shot learning scenarios to assess whether the temperature scaling parameters learned on validation sets maintain their effectiveness under distribution shift and data scarcity conditions.