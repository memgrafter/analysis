---
ver: rpa2
title: 'From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data
  Synthesis'
arxiv_id: '2406.19934'
source_url: https://arxiv.org/abs/2406.19934
tags:
- reasoning
- visual
- reasoner
- vlms
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of multi-step visual reasoning
  in vision-language models (VLMs), which are hindered by a lack of training data.
  The authors introduce a "least-to-most" visual reasoning paradigm that decomposes
  complex questions into simpler sub-questions and invokes external tools to solve
  them step-by-step.
---

# From the Least to the Most: Building a Plug-and-Play Visual Reasoner via Data Synthesis

## Quick Facts
- arXiv ID: 2406.19934
- Source URL: https://arxiv.org/abs/2406.19934
- Authors: Chuanqi Cheng; Jian Guan; Wei Wu; Rui Yan
- Reference count: 14
- Primary result: Visual reasoner improves VLM performance on 4 VQA benchmarks with gains from 0.71% to 39%

## Executive Summary
This paper addresses the challenge of multi-step visual reasoning in vision-language models (VLMs) by introducing a "least-to-most" visual reasoning paradigm that decomposes complex questions into simpler sub-questions and invokes external tools to solve them step-by-step. To overcome the data scarcity problem, the authors propose a novel data synthesis approach called "least-to-most synthesis" that automatically generates high-quality visual reasoning data using open-source models in a bottom-up manner. The approach constructs nodes representing image regions and their textual descriptions, synthesizes reasoning paths by connecting nodes with sub-questions and tool arguments, and recursively generates main questions. Using this method, they create a large-scale dataset called VIREO with 50k examples and fine-tune a visual reasoner model. Extensive experiments demonstrate that the visual reasoner can consistently and significantly improve the performance of various VLMs on four VQA benchmarks, with absolute gains ranging from 0.71% to 39%.

## Method Summary
The method introduces a least-to-most visual reasoning paradigm where a visual reasoner decomposes complex questions into sub-questions and invokes external tools (Grounding, OCR, Highlight, or Answer) for each step. To address data scarcity, they propose a data synthesis approach that automatically creates high-quality visual reasoning data using open-source models. The synthesis process involves entity recognition (Deformable DETR), node construction (BLIP, LLaVA), reasoning path synthesis (LLaMA-3-8B-Instruct), and question synthesis (LLaMA-3-8B-Instruct). This bottom-up approach generates a dataset called VIREO with 50k examples. The visual reasoner (LLaVA-1.5-7B fine-tuned on VIREO) can be applied to various VLMs in a plug-and-play fashion to enhance their reasoning capabilities by using different VLMs as the "Answer" tool.

## Key Results
- Visual reasoner improves TallyQA accuracy from 38.61% to 77.61% (39% absolute gain)
- Enhances GQA accuracy from 44.81% to 59.05% (14.24% absolute gain)
- Improves TextVQA accuracy from 45.71% to 47.71% (2.0% absolute gain)
- Boosts ST-VQA performance from 39.50% to 47.81% (8.31% absolute gain)

## Why This Works (Mechanism)

### Mechanism 1
Multi-step visual reasoning improves accuracy because it decomposes complex visual questions into simpler sub-questions that can be resolved with specialized tools. The method introduces a "least-to-most" paradigm where the visual reasoner breaks down the main question into sub-questions, each solvable by invoking a specific tool (Grounding, OCR, Highlight, or Answer). This step-by-step decomposition allows the model to handle complex reasoning by leveraging specialized tools rather than requiring a single model to solve everything at once.

### Mechanism 2
Data synthesis using open-source models creates high-quality training data that enables the visual reasoner to learn multi-step reasoning. The least-to-most synthesis approach automatically generates reasoning paths by first recognizing entities, constructing nodes with image regions and textual descriptions, synthesizing reasoning paths by connecting nodes with sub-questions and tool arguments, and recursively generating main questions. This bottom-up approach ensures quality by using atomic tasks that open-source models can handle reliably.

### Mechanism 3
The visual reasoner can be applied to various VLMs in a plug-and-play fashion to enhance their reasoning capabilities. The reasoner model is trained on the synthesized VIREO dataset to perform step-by-step reasoning with tool invocation. By using different VLMs as the "Answer" tool, the same reasoner architecture can enhance multiple base models without requiring fine-tuning of the base models themselves.

## Foundational Learning

- Concept: Tool invocation and composition in language models
  - Why needed here: The visual reasoner relies on selecting and invoking appropriate tools (Grounding, OCR, Highlight, Answer) for each sub-question. Understanding how LLMs can invoke tools and compose their outputs is fundamental to grasping this approach.
  - Quick check question: How does a language model decide which tool to invoke for a given sub-question, and what information must it provide to the tool?

- Concept: Data synthesis for training ML models
  - Why needed here: The approach relies on automatically generating high-quality training data through least-to-most synthesis. Understanding data synthesis techniques is crucial for appreciating how the method overcomes the data scarcity problem.
  - Quick check question: What are the key challenges in data synthesis for multi-step reasoning tasks, and how does the bottom-up approach address them?

- Concept: Multi-step reasoning and decomposition
  - Why needed here: The core mechanism is decomposing complex visual questions into simpler sub-questions. Understanding decomposition strategies and their impact on reasoning performance is essential.
  - Quick check question: What are the advantages and disadvantages of breaking down complex reasoning tasks into multiple simpler steps versus attempting direct reasoning?

## Architecture Onboarding

- Component map: Image → Entity Recognition (Deformable DETR) → Node Construction → Reasoning Path Synthesis (Questioner LLM) → Question Synthesis (Combiner LLM) → VIREO dataset → Trained Reasoner (LLaVA-1.5-7B fine-tuned on VIREO) + Base VLM → Multi-step Visual Reasoning Pipeline

- Critical path: Image → Entity Recognition → Node Construction → Reasoning Path Synthesis → Question Synthesis → Reasoner Training → Inference on target VLMs

- Design tradeoffs:
  - Open-source vs proprietary models: Using open-source models ensures reproducibility and cost-efficiency but may have lower quality than GPT-4V
  - Step complexity: More decomposition steps can improve accuracy but increase inference time and error propagation risk
  - Tool selection: The four tools chosen balance coverage of visual tasks but may miss some specialized operations

- Failure signatures:
  - Tool selection errors: Reasoner chooses wrong tool for a sub-question
  - Argument generation errors: Reasoner provides incorrect arguments to tools
  - Execution failures: Tools fail to execute properly (e.g., OCR misses text)
  - Inference errors: Answer tool provides wrong or irrelevant answers
  - Decomposition issues: Sub-questions are too complex or not properly ordered

- First 3 experiments:
  1. Test the complete pipeline on a simple image with a straightforward question to verify end-to-end functionality
  2. Validate each individual tool (Grounding, OCR, Highlight) on test images to ensure they work as expected
  3. Evaluate the Reasoner on a small subset of GQA or TextVQA to measure performance gains before full-scale testing

## Open Questions the Paper Calls Out

- Question: How does the performance of the visual reasoner vary with different image datasets beyond COCO2014?
  - Basis in paper: [inferred] The paper mentions using COCO2014 as the image source and acknowledges that while it is a general-purpose dataset, it may not cover all visual tasks.
  - Why unresolved: The paper does not explore the impact of using different image datasets on the performance of the visual reasoner.
  - What evidence would resolve it: Experiments comparing the visual reasoner's performance on multiple image datasets with varying characteristics (e.g., domain specificity, image complexity) would provide insights into its generalizability.

- Question: What is the impact of varying the maximum chain length in the reasoning process on the visual reasoner's performance?
  - Basis in paper: [explicit] The paper mentions setting the maximum chain length to 4 during VIREO construction but does not explore the impact of different chain lengths.
  - Why unresolved: The optimal chain length for balancing performance and computational efficiency is not determined.
  - What evidence would resolve it: Experiments varying the maximum chain length and measuring the visual reasoner's performance and computational cost would identify the optimal trade-off.

- Question: How does the visual reasoner's performance scale with the size of the VIREO dataset?
  - Basis in paper: [explicit] The paper discusses the performance of the visual reasoner with varying sizes of VIREO but notes that the marginal benefit diminishes with more data.
  - Why unresolved: The exact relationship between dataset size and performance, including the point of diminishing returns, is not quantified.
  - What evidence would resolve it: Detailed analysis of the visual reasoner's performance as a function of VIREO dataset size, including statistical measures of improvement, would clarify the scaling behavior.

## Limitations

- The approach's effectiveness varies significantly by task type, with minimal gains on TextVQA (2.0%) versus substantial gains on TallyQA (39%)
- The 50k-example VIREO dataset may have systematic biases from the synthesis process that could limit generalization to real-world scenarios
- The reliance on open-source models for data synthesis introduces quality variability that isn't fully characterized

## Confidence

- Multi-step reasoning effectiveness: High
- Data synthesis quality: Medium
- Generalization across VLMs: Medium
- Plug-and-play applicability: High

## Next Checks

1. Test the complete pipeline on adversarial visual questions designed to break the tool composition, measuring error propagation across reasoning steps.

2. Randomly sample 100 synthesized examples and manually verify the correctness of each reasoning step, tool selection, and argument generation to quantify synthesis accuracy.

3. Evaluate the trained Reasoner on held-out reasoning datasets (like V-REX) that weren't seen during synthesis to test true generalization beyond the specific synthesis distribution.