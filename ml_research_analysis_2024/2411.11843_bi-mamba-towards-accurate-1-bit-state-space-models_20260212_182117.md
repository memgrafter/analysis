---
ver: rpa2
title: 'Bi-Mamba: Towards Accurate 1-Bit State Space Models'
arxiv_id: '2411.11843'
source_url: https://arxiv.org/abs/2411.11843
tags:
- bi-mamba
- performance
- training
- perplexity
- full-precision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Bi-Mamba, the first 1-bit binarization approach
  for State Space Models (SSMs) like Mamba. It focuses on binarizing the input and
  output projection matrices within Mamba-2 architecture, enabling highly efficient
  inference while preserving model performance.
---

# Bi-Mamba: Towards Accurate 1-Bit State Space Models

## Quick Facts
- arXiv ID: 2411.11843
- Source URL: https://arxiv.org/abs/2411.11843
- Reference count: 40
- This work introduces Bi-Mamba, the first 1-bit binarization approach for State Space Models (SSMs) like Mamba, achieving competitive perplexity scores (10.0–14.5) across three model sizes while reducing memory usage by over 5x and energy consumption by 3x during inference.

## Executive Summary
Bi-Mamba introduces the first 1-bit binarization approach for State Space Models (SSMs) like Mamba-2, enabling highly efficient inference while preserving model performance. The approach focuses on binarizing the input and output projection matrices within Mamba-2 architecture, which account for over 90% of parameters, yielding significant compression. Bi-Mamba models are trained from scratch using autoregressive distillation with a 16-bit teacher model (LLaMA2-7B), ensuring the binarized weights maintain the original weight distribution for better representational capacity. Across three model sizes (780M, 1.3B, and 2.7B parameters), Bi-Mamba achieves competitive perplexity scores on Wiki2, PTB, and C4 datasets and maintains strong downstream task accuracy compared to full-precision Mamba-2.

## Method Summary
Bi-Mamba employs binarization-aware training with autoregressive distillation to binarize Mamba-2 models to 1-bit precision. The method binarizes only the input and output projection matrices (In_Proj and Out_Proj) while keeping the SSM core components (A, B, C, D matrices) and other modules in full precision. Models are trained from scratch using a cross-entropy distillation loss between the binarized student and a full-precision teacher model (LLaMA2-7B). The training process uses learnable scaling factors and sign-based binarization to preserve the original weight distribution. Three model sizes are evaluated: 780M, 1.3B, and 2.7B parameters.

## Key Results
- Achieves competitive perplexity scores of 10.0–14.5 on Wiki2, PTB, and C4 datasets across three model sizes
- Reduces memory usage by over 5x and energy consumption by 3x during inference compared to full-precision Mamba-2
- Outperforms post-training binarization (PTB) and quantization (GPTQ) baselines while maintaining downstream task accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Binarization-aware training with autoregressive distillation preserves the original weight distribution in Mamba models, enabling 1-bit models to maintain full-precision performance.
- Mechanism: The training process aligns the binarized weights with the original distribution using learnable scaling factors (α, β) and sign-based binarization, preventing the shift seen in post-training binarization methods.
- Core assumption: The optimal binary weights for Mamba lie close to the original high-precision weight distribution, and maintaining this alignment is critical for performance.
- Evidence anchors:
  - [abstract]: "Our binarization-aware training, however, ensures that the binarized weights remain close to the original weight distribution, preserving the largest capability in weight representation binarization."
  - [section]: "Based on our empirical experiments, applying existing LLM post-binarization methods (Frantar et al., 2023; Huang et al., 2024), even when retaining salient weights, often severely degrades the performance of the Mamba model. Without accounting for salient weights, binarization-aware training appears to be the only feasible solution for effectively binarizing models like Mamba while preserving competitive performance."
  - [corpus]: "Bi-Mamba: Towards Accurate 1-Bit State Space Models" — no direct quantitative comparison available, but this is the primary cited source for the mechanism.

### Mechanism 2
- Claim: Binarizing only the input and output projection matrices (In_Proj and Out_Proj) in Mamba-2 models achieves high compression with minimal performance loss.
- Mechanism: These matrices account for over 90% of the parameters in Mamba-2, so binarizing them yields significant memory and computational savings while leaving other critical parameters (like A, B, C, D) in full precision.
- Core assumption: The input and output projections are the most parameter-heavy components, and binarizing them captures the bulk of efficiency gains without disrupting core SSM dynamics.
- Evidence anchors:
  - [abstract]: "In this work, we introduce Bi-Mamba, a scalable and powerful 1-bit Mamba architecture designed to enable more efficient large language models (LLMs), with model sizes of 780M, 1.3B, and 2.7B parameters."
  - [section]: "According to the description above, we can interpret the SSD matrices of A,B,C,D, and ∆ bias more intuitively, as well as other layers including embedding, layer normalization (LN), Conv-1d, inner linear projection, out linear projection in Mamba-2 to better understand the effect after binarization, so that to determine the binarization space."
  - [corpus]: No explicit quantitative breakdown of parameter distribution provided in the corpus; assumption based on model description.

### Mechanism 3
- Claim: Autoregressive distillation with a full-precision teacher (LLaMA2-7B) provides stable training signals for the binarized student model.
- Mechanism: Cross-entropy loss between the teacher's and student's output distributions guides the binarized model to mimic the full-precision behavior token-by-token, ensuring task-relevant performance is retained.
- Core assumption: The full-precision teacher's output distribution is a reliable proxy for the optimal behavior of the binarized student, and the distillation loss effectively propagates this signal.
- Evidence anchors:
  - [abstract]: "Bi-Mamba models are trained from scratch on a standard LLM-scale dataset using an autoregressive distillation loss."
  - [section]: "Our training objective is a cross-entropy loss between the outputs of the target student model and the pretrained teacher model at each step of the autoregressive scheme for next-token prediction."
  - [corpus]: No quantitative ablation of distillation loss type provided; assumed based on method description.

## Foundational Learning

- Concept: State Space Models (SSMs) and their linear complexity vs. Transformer quadratic complexity
  - Why needed here: Bi-Mamba builds on Mamba-2, which is an SSM. Understanding how SSMs process sequences efficiently is critical to grasping why binarization works without catastrophic performance loss.
  - Quick check question: Why do SSMs like Mamba scale linearly with sequence length while Transformers scale quadratically?

- Concept: Post-training quantization (PTQ) vs. quantization-aware training (QAT)
  - Why needed here: Bi-Mamba uses QAT, not PTQ. Knowing the difference explains why Bi-Mamba succeeds where naive binarization fails.
  - Quick check question: What is the key difference between PTQ and QAT, and why does it matter for low-bit models?

- Concept: Autoregressive distillation and knowledge distillation in general
  - Why needed here: The training method relies on distilling from a full-precision teacher. Understanding how distillation works helps explain how Bi-Mamba retains performance.
  - Quick check question: How does autoregressive distillation differ from standard KD, and why is it used here?

## Architecture Onboarding

- Component map: Token → Embedding (FP) → LayerNorm (FP) → Conv1d (FP) → In_Proj (Binarized) → SSM Core (FP) → Out_Proj (Binarized) → LayerNorm (FP) → Output
- Critical path: Token → In_Proj (binarized) → SSM core (FP) → Out_Proj (binarized) → Output
- Design tradeoffs:
  - Binarize more modules → higher compression but greater risk of accuracy drop
  - Use stronger teacher → better distillation but higher compute/training cost
  - Full precision activation → maintains accuracy but limits memory gains
- Failure signatures:
  - Training collapse → binarization too aggressive, scaling factors not learned
  - Low downstream accuracy → distillation loss ineffective, teacher-student gap too large
  - Memory not reduced → binarization not applied correctly or activations not optimized
- First 3 experiments:
  1. Train Bi-Mamba with only In_Proj binarized; compare perplexity and accuracy vs full-precision baseline
  2. Add Out_Proj binarization; measure compression ratio and performance drop
  3. Replace LLaMA2-7B teacher with smaller model; evaluate impact on distillation quality and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of binarization method (FBI-Linear vs other approaches) affect the final model performance?
- Basis in paper: [explicit] The authors state "Our primary goal is to demonstrate the feasibility of binarizing the Mamba architecture... not to claim that FBI-LLM is the optimal scheme here."
- Why unresolved: The paper focuses on demonstrating feasibility rather than comparing different binarization methods, leaving open whether alternative approaches could yield better performance.
- What evidence would resolve it: Systematic comparison of Bi-Mamba using different binarization schemes (FBI-Linear, BinaryConnect, XNOR-Net) on the same training setup and evaluation metrics.

### Open Question 2
- Question: What is the optimal balance between partial and full binarization for maximizing downstream task performance while minimizing storage/computation?
- Basis in paper: [explicit] Table 5 shows partial binarization (In_Proj only) achieving 53.1 average accuracy vs full binarization (In_Proj + Out_Proj) achieving 50.4 for the 2.7B model, with minimal difference in perplexity.
- Why unresolved: The paper demonstrates both approaches work but doesn't systematically explore the trade-offs across different model sizes and tasks to find the optimal configuration.
- What evidence would resolve it: Grid search across different binarization combinations (In_Proj only, Out_Proj only, both, neither) across all model sizes and tasks, with detailed analysis of storage/computation trade-offs.

### Open Question 3
- Question: How does Bi-Mamba's performance scale with sequence length compared to full-precision Mamba-2?
- Basis in paper: [inferred] The paper emphasizes Mamba's linear complexity advantage over Transformers for long sequences, but doesn't evaluate Bi-Mamba's behavior at extreme sequence lengths.
- Why unresolved: While the paper shows Bi-Mamba maintains performance at standard evaluation lengths, it doesn't test whether binarization affects the linear complexity advantage at very long sequences (e.g., 100K+ tokens).
- What evidence would resolve it: Systematic evaluation of Bi-Mamba vs full-precision Mamba-2 on tasks requiring progressively longer sequences, measuring both performance and computational efficiency.

### Open Question 4
- Question: How does the quality of the teacher model affect Bi-Mamba's final performance?
- Basis in paper: [explicit] Table 10 shows Bi-Mamba trained with Llama2-7B teacher outperforming the same model trained with Phi-3.5 teacher on downstream tasks, and the ablation study notes "the better the teacher is, the higher performance the student will achieve."
- Why unresolved: The paper only compares two teacher models, leaving unclear how much performance could be gained with stronger teachers or whether there's a point of diminishing returns.
- What evidence would resolve it: Systematic evaluation of Bi-Mamba with teachers of increasing capability (Llama2-7B, Llama3-8B, Llama3-70B, GPT-4) to determine the relationship between teacher quality and student performance.

## Limitations

- The claim that Bi-Mamba "significantly outperforms" PTB and GPTQ baselines lacks direct empirical comparison within the study itself
- The absolute performance gap versus full-precision baselines on downstream tasks is not quantified, making practical viability assessment difficult
- The assertion that "binarization-aware training appears to be the only feasible solution" for Mamba models is an overstatement without broader empirical validation

## Confidence

**High Confidence** (80-100%): The mechanism of binarization-aware training with autoregressive distillation is technically sound and well-supported by the methodology description. The claim that binarizing only In_Proj and Out_Proj matrices achieves significant compression while preserving performance is reasonable given these matrices' parameter dominance in Mamba-2.

**Medium Confidence** (60-80%): The assertion that Bi-Mamba achieves "competitive" perplexity scores (10.0-14.5) compared to full-precision models is plausible but requires verification against published Mamba-2 benchmarks. The 5x memory reduction and 3x energy consumption improvements are reasonable estimates based on 1-bit compression but need empirical validation.

**Low Confidence** (0-60%): The claim that Bi-Mamba "significantly outperforms" PTB and GPTQ baselines is difficult to assess without seeing the actual comparison experiments or quantitative results. The statement that "binarization-aware training appears to be the only feasible solution" for Mamba models is an overstatement without broader empirical validation across different model architectures.

## Next Checks

1. **Ablation Study on Binarization Scope**: Systematically vary which modules are binarized (In_Proj only, Out_Proj only, both, plus Conv1d) and measure the performance tradeoff curve. This would validate the claim that binarizing just these two matrices captures the bulk of efficiency gains.

2. **Teacher Model Sensitivity Analysis**: Replace the LLaMA2-7B teacher with progressively smaller models (7B→3B→1.5B) and measure how distillation quality and final Bi-Mamba performance degrade. This would test the assumption that a strong teacher is essential for effective binarization-aware training.

3. **Memory and Energy Validation**: Implement Bi-Mamba and measure actual memory usage and energy consumption during inference on representative hardware (GPU/CPU). Compare against both full-precision Mamba-2 and reported estimates to validate the claimed 5x memory and 3x energy improvements.