---
ver: rpa2
title: 'Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes
  Through Rank-Wise Clustering'
arxiv_id: '2409.16167'
source_url: https://arxiv.org/abs/2409.16167
tags:
- lora
- merging
- loras
- performance
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of merging multiple LoRAs (Low-Rank
  Adaptations) trained on different tasks into a unified adapter for large language
  models. Existing methods often suffer from parameter interference and fail to fully
  leverage LoRA's modularity.
---

# Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering

## Quick Facts
- arXiv ID: 2409.16167
- Source URL: https://arxiv.org/abs/2409.16167
- Reference count: 15
- Key outcome: Introduces LoRA-LEGO, a framework that merges multiple LoRAs by decomposing them into Minimal Semantic Units (MSUs) and reassembling through rank-wise clustering, achieving superior performance on multi-task benchmarks while retaining only 50% of parameters.

## Executive Summary
This paper addresses the challenge of merging multiple LoRAs trained on different tasks into a unified adapter for large language models. Existing methods often suffer from parameter interference and fail to fully leverage LoRA's modularity. The authors propose LoRA-LEGO, a framework that decomposes LoRAs into Minimal Semantic Units (MSUs) and reassembles them through rank-wise clustering. This approach enables flexible combination of LoRAs with arbitrary ranks while mitigating parameter interference. LoRA-LEGO employs a dual reweighting strategy to optimize the scale of the merged LoRA. Experiments on various benchmarks show that LoRA-LEGO outperforms existing methods in LoRA merging. Additionally, it can effectively merge heterogeneous LoRAs and serve as a parameter pruning technique, retaining only 50% of parameters while maintaining comparable performance.

## Method Summary
LoRA-LEGO addresses the LoRA merging problem by decomposing each LoRA into Minimal Semantic Units (MSUs) - individual rank components that can be manipulated independently. All MSUs from different LoRAs are pooled together and clustered using K-means, where each cluster represents similar semantic components across tasks. The centroids of these clusters are then used to reconstruct a new LoRA with an adjusted rank. A dual reweighting strategy is applied to optimize both the parameter norms and output scale of the merged adapter. The framework leverages two key properties of LoRA: permutation invariance (MSU order doesn't matter) and concatenation-summation equivalence (summing MSU outputs equals output of concatenated LoRA).

## Key Results
- LoRA-LEGO outperforms existing LoRA merging methods on multi-task benchmarks including CoLA, MNLI, MRPC, QNLI, QQP, RTE, SST2, SNLI, and WNLI
- The framework can merge heterogeneous LoRAs with different ranks and alpha values
- LoRA-LEGO serves as an effective parameter pruning technique, retaining only 50% of parameters while maintaining comparable performance
- The method demonstrates strong performance on both seen and out-of-domain tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MSUs within a LoRA are permutation invariant, meaning rearranging the MSUs does not affect the output.
- Mechanism: The LoRA decomposition into MSUs allows independent computation for each rank. Since the output is a sum of independent rank contributions, the order of MSUs does not matter.
- Core assumption: Each MSU operates independently and the final LoRA output is a linear combination of MSU contributions.
- Evidence anchors:
  - [abstract] "These MSUs demonstrate permutation invariance and concatenation-summation equivalence properties, enabling flexible combinations to create new LoRAs."
  - [section] "Within each LoRA, the MSUs exhibit the property of Permutation Invariance, indicating that any permutation of MSUs within a LoRA does not affect the adapter’s output."
- Break condition: If MSU contributions interact non-linearly or depend on the order of computation, permutation invariance would fail.

### Mechanism 2
- Claim: MSUs from different LoRAs can be concatenated and their outputs summed equivalently to a single higher-ranked LoRA.
- Mechanism: The LoRA output is computed as BAx. When MSUs are concatenated, the combined matrices maintain the same product structure as the sum of individual outputs.
- Core assumption: Matrix multiplication distributes over concatenation in the rank dimension.
- Evidence anchors:
  - [abstract] "LoRA exhibits the Concatenation-Summation Equivalence property, which states that summing the outputs from multiple LoRAs is equivalent to the output of a single higher-ranked LoRA constructed by concatenating all the MSUs of these LoRAs."
  - [section] "Define the concatenated matrices as: A′ = [A1 A2] ∈ R2r×d, B′ = [B1 B2] ∈ Rd×2r. The output vector y from the concatenated model is equivalent to the sum of the outputs from each individual LoRA model: y = B′A′x = (B1A1 + B2A2)x."
- Break condition: If matrix dimensions don't align properly or if the computation isn't purely linear, concatenation-summation equivalence would break.

### Mechanism 3
- Claim: Clustering similar MSUs and using centroids reduces parameter interference while preserving task-specific knowledge.
- Mechanism: By grouping similar MSUs together, the method aligns semantically related parameters before merging, preventing misalignment-induced interference and knowledge conflict.
- Core assumption: Similar MSUs contain complementary rather than conflicting information, and averaging within clusters preserves the essential semantic content.
- Evidence anchors:
  - [abstract] "LoRA-LEGO conducts rank-wise parameter clustering by grouping MSUs from different LoRAs into k clusters. The centroid of each cluster serves as a representative MSU, enabling the assembly of a merged LoRA with an adjusted rank of k."
  - [section] "Aggregating within each cluster minimizes information loss compared to directly merging different LoRAs, as the MSUs within a cluster are more similar to each other."
- Break condition: If MSUs within a cluster are too dissimilar, centroid averaging could dilute task-specific information or create new interference patterns.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Understanding LoRA's mathematical structure is essential for grasping how MSUs are derived and why they have the properties claimed.
  - Quick check question: In LoRA, what is the relationship between the rank r and the dimensions of matrices A and B?

- Concept: Matrix permutation invariance and concatenation properties
  - Why needed here: These linear algebra properties underpin the MSU definitions and the feasibility of LoRA-LEGO's merging approach.
  - Quick check question: If P is a permutation matrix, what is the relationship between BA and (BP^T)(PA)?

- Concept: K-means clustering and centroid computation
  - Why needed here: The MSU clustering step relies on K-means to group similar MSUs and compute representative centroids for the merged LoRA.
  - Quick check question: In K-means clustering, how is the centroid of a cluster computed from its member points?

## Architecture Onboarding

- Component map:
  Input LoRAs → MSU Extraction → MSU Pooling → K-means Clustering → Centroid Computation → LoRA Reconstruction → Dual Reweighting → Merged LoRA

- Critical path:
  MSU extraction → MSU pooling → MSU clustering → Centroid computation → LoRA reconstruction → Dual reweighting

- Design tradeoffs:
  - Rank selection (k): Higher k preserves more task-specific knowledge but increases parameter count; lower k improves efficiency but may lose information
  - Clustering method: K-means is simple but may not capture complex MSU relationships; alternative distance metrics could improve clustering quality
  - Reweighting strategy: Parameter reweighting addresses norm decay, output reweighting addresses variance expansion; both are needed for optimal scaling

- Failure signatures:
  - Performance degradation when k is too low (insufficient capacity to capture all tasks)
  - Overfitting when k is too high (capturing noise instead of signal)
  - Suboptimal clustering leading to parameter interference (MSUs from different tasks grouped together)
  - Improper reweighting causing output scale mismatch (norms too high or low)

- First 3 experiments:
  1. Verify permutation invariance: Take a trained LoRA, permute its MSUs, and confirm output remains unchanged
  2. Test concatenation-summation equivalence: Create two LoRAs, concatenate their MSUs, and verify output matches sum of individual outputs
  3. Validate clustering effectiveness: Cluster MSUs from multiple LoRAs and examine if clusters group MSUs from the same task together

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of distance metric in LoRA-LEGO affect the clustering of Minimal Semantic Units (MSUs) and the overall performance of the merged LoRA?
- Basis in paper: [explicit] The paper mentions that future work could explore alternative distance metrics for LoRA-LEGO, such as optimal transport, to better characterize parameter similarities beyond the standard Euclidean distance.
- Why unresolved: The current implementation uses Euclidean distance for clustering MSUs, but the impact of different distance metrics on the performance of the merged LoRA is not explored.
- What evidence would resolve it: Experimental results comparing the performance of LoRA-LEGO using different distance metrics (e.g., Euclidean, Manhattan, cosine similarity, optimal transport) on various benchmark tasks.

### Open Question 2
- Question: Can the concept of Minimal Semantic Units (MSUs) be further extended or decomposed to enhance the performance of LoRA merging in more complex scenarios?
- Basis in paper: [inferred] The paper discusses the properties of MSUs, such as permutation invariance and concatenation-summation equivalence, but does not explore the potential for further modularization of LoRAs.
- Why unresolved: While the current framework effectively merges LoRAs using MSUs, there may be additional levels of granularity or alternative representations that could further improve the merging process.
- What evidence would resolve it: Investigations into the effects of further decomposing MSUs or using alternative representations on the performance of LoRA merging in various scenarios, including multi-task learning and mixed-task evaluation.

### Open Question 3
- Question: How does the rank adjustment in LoRA-LEGO impact the preservation of task-specific knowledge and the overall generalization capabilities of the merged LoRA?
- Basis in paper: [explicit] The paper mentions that LoRA-LEGO enables targeted rank adjustments in the merged LoRA to preserve task-specific knowledge, but the specific impact of these adjustments on performance is not fully explored.
- Why unresolved: While the paper demonstrates that LoRA-LEGO can adjust the rank of the merged LoRA, the relationship between rank adjustment, task-specific knowledge preservation, and generalization is not thoroughly investigated.
- What evidence would resolve it: Experiments analyzing the performance of LoRA-LEGO with different rank adjustments on various tasks, focusing on the preservation of task-specific knowledge and the generalization capabilities of the merged LoRA.

## Limitations

- The framework relies on strong assumptions about MSU independence and linear operations that may not hold for all LoRA configurations or model architectures
- The dual reweighting strategy may require task-specific tuning that isn't captured in the paper
- Experiments focus primarily on natural language tasks with specific LoRA hyperparameters (rank r=6, alpha=12), limiting generalizability to other domains

## Confidence

**High Confidence**: The core mathematical properties of LoRA (permutation invariance and concatenation-summation equivalence) are well-established and the experimental results demonstrating LoRA-LEGO's effectiveness on the tested benchmarks are clearly presented.

**Medium Confidence**: The general applicability of LoRA-LEGO to different model architectures and task types. While the framework is theoretically sound, its performance on tasks outside the tested NLP benchmarks or with different LoRA configurations needs further validation.

**Low Confidence**: The robustness of the clustering approach to noisy or ambiguous MSU representations, and the universal effectiveness of the dual reweighting strategy across all possible LoRA merging scenarios. The paper provides limited discussion of failure cases or edge conditions.

## Next Checks

1. **Cross-architecture validation**: Apply LoRA-LEGO to merge LoRAs trained on different model architectures (e.g., BERT, RoBERTa, T5) to test whether the framework generalizes beyond Llama2.

2. **Noise robustness test**: Intentionally introduce noise or perturbations to MSU representations and evaluate whether LoRA-LEGO can still produce effective merged adapters, or identify at what noise levels the framework breaks down.

3. **Extreme rank variation experiment**: Test LoRA-LEGO with LoRAs of vastly different ranks (e.g., merging rank 2 and rank 32 LoRAs) to determine if the framework handles rank heterogeneity effectively or if it requires similar ranks for optimal performance.