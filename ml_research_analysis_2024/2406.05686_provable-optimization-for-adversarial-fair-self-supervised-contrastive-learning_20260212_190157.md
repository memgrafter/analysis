---
ver: rpa2
title: Provable Optimization for Adversarial Fair Self-supervised Contrastive Learning
arxiv_id: '2406.05686'
source_url: https://arxiv.org/abs/2406.05686
tags:
- learning
- attribute
- data
- sensitive
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning fair encoders in
  self-supervised learning (SSL) settings where only a small portion of data is annotated
  with sensitive attributes. The authors propose a novel framework that combines global
  contrastive loss (GCL) with an adversarial fair representation learning (AFRL) approach.
---

# Provable Optimization for Adversarial Fair Self-supervised Contrastive Learning

## Quick Facts
- arXiv ID: 2406.05686
- Source URL: https://arxiv.org/abs/2406.05686
- Authors: Qi Qi; Quanqi Hu; Qihang Lin; Tianbao Yang
- Reference count: 40
- Key outcome: SoFCLR significantly outperforms existing fairness-unaware SSL methods and achieves better fairness metrics compared to fairness-aware SSL methods and supervised learning baselines, achieving 85.89% accuracy on UTKface while reducing fairness metrics ΔED from 17.83 to 15.42.

## Executive Summary
This paper addresses the challenge of learning fair encoders in self-supervised learning settings where only a small portion of data is annotated with sensitive attributes. The authors propose SoFCLR, a novel framework that combines global contrastive loss with adversarial fair representation learning. By minimizing a contrastive loss over unlabeled data while maximizing an adversarial loss for predicting sensitive attributes, SoFCLR learns representations that are informative for the primary task but independent of sensitive attributes. Extensive experiments on face image datasets demonstrate significant improvements in fairness metrics while maintaining high accuracy.

## Method Summary
SoFCLR combines global contrastive loss (GCL) with adversarial fair representation learning (AFRL) to learn fair encoders in SSL settings with partial sensitive attribute annotations. The method solves a non-convex non-concave minimax game using stochastic optimization with moving average estimators. The algorithm maintains ui,t as exponential moving averages of inner function values to compute unbiased stochastic gradient estimators for the compositional GCL term. Convergence is guaranteed under reasonable conditions including one-point strong convexity of the adversarial loss, without requiring large batch sizes.

## Key Results
- On UTKface dataset: 85.89% accuracy for gender prediction while reducing ΔED fairness metric from 17.83 to 15.42
- On CelebA dataset: 92.09% accuracy while achieving lower fairness metrics than all baseline methods
- Outperforms fairness-unaware SSL methods and achieves better fairness metrics than fairness-aware SSL methods and supervised learning baselines across eight different fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SoFCLR learns representations independent of sensitive attributes by maximizing adversarial loss while minimizing GCL
- Mechanism: The minimax game structure forces encoder to produce task-informative but attribute-uninformative representations. Moving average estimators stabilize inner function values for unbiased gradient estimation.
- Core assumption: Discriminator has sufficient capacity to approximate p(sensitive attribute | representation); GCL is differentiable and compositional
- Evidence anchors: [abstract] "minimizing a contrastive loss over unlabeled data while maximizing an adversarial loss of predicting the sensitive attribute"; [section 3] introduces fairness-promoting regularizer
- Break condition: Weak discriminator fails to force independence; small batch sizes break compositional structure

### Mechanism 2
- Claim: Moving average estimator ui,t enables unbiased stochastic gradient estimation for compositional GCL without large batch sizes
- Mechanism: Maintains exponential moving average of inner function values g(w; xi, S⁻i) instead of using mini-batch samples directly, allowing computation of ∇f2(ui,t)∇g(wt; xi, B⁻i)
- Core assumption: Inner function is smooth and Lipschitz continuous; moving average parameter γ is small enough
- Evidence anchors: [section 4] follows SogCLR approach with moving average estimators; [section 5.1] detailed update rule
- Break condition: Large γ causes instability; high variance inner function dominates estimator variance

### Mechanism 3
- Claim: One-point strong convexity condition ensures convergence of non-convex non-concave minimax game
- Mechanism: Condition -(w′ - w′*)⊤∇w′Ffair(w, w′) ≥ λ∥w′ - w′*∥² means Ffair(w, ·) has unique maximizer close to current w′, allowing monotonic progress
- Core assumption: Discriminator satisfies one-point strong convexity, proved for wide neural networks
- Evidence anchors: [section 5.2] defines one-point strong convexity; references proof for wide neural networks
- Break condition: Narrow discriminator or low-dimensional representation space prevents condition from holding

## Foundational Learning

- Concept: Self-supervised contrastive learning (SimCLR-style)
  - Why needed here: SoFCLR builds on contrastive loss framework, using GCL instead of mini-batch CL for better performance with smaller batch sizes
  - Quick check question: What is the difference between global contrastive loss (GCL) and standard mini-batch contrastive loss?

- Concept: Adversarial fair representation learning
  - Why needed here: Core innovation combines adversarial training with contrastive learning, where adversary predicts sensitive attributes from representations
  - Quick check question: How does maximizing adversarial loss for sensitive attribute prediction lead to fairer representations?

- Concept: Compositional optimization and moving average estimators
  - Why needed here: GCL term is compositional function requiring specialized techniques for unbiased gradient estimates
  - Quick check question: Why can't we just use standard mini-batch gradient estimates for compositional GCL term?

## Architecture Onboarding

- Component map: Data augmentation → Encoder forward pass → Compute GCL + Adversarial loss → Update moving averages → Compute stochastic gradients → Update parameters

- Critical path: Data augmentation → Encoder forward pass → Compute GCL + Adversarial loss → Update moving averages → Compute stochastic gradients → Update parameters

- Design tradeoffs: Shallow 2-layer MLP discriminator vs deeper network - shallow is sufficient for one-point strong convexity but may limit adversarial strength. Moving averages vs direct mini-batch estimates - more stable but add memory overhead.

- Failure signatures: If accuracy drops significantly while fairness doesn't improve, adversarial loss is too strong. If fairness doesn't improve, discriminator may be too weak or one-point strong convexity condition doesn't hold.

- First 3 experiments:
  1. Run SoFCLR with α=0 (reduces to SogCLR) to verify compositional GCL implementation works
  2. Run with very small α to see if fairness improves slightly while maintaining accuracy
  3. Run with varying α values to find Pareto frontier between accuracy and fairness metrics

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Convergence analysis relies on one-point strong convexity condition, only proved for wide neural networks, while practical implementation uses shallow 2-layer MLP
- Compositional GCL with moving average estimators introduces significant complexity sensitive to hyperparameter choices like γ and α
- Empirical validation limited to face image datasets with binary sensitive attributes (gender, race)

## Confidence
- **High confidence**: Basic framework of combining adversarial training with contrastive learning for fairness is sound and well-established
- **Medium confidence**: Convergence guarantees under stated assumptions, particularly one-point strong convexity for practical discriminator architectures
- **Medium confidence**: Empirical results showing improved fairness metrics while maintaining accuracy, though magnitude may depend on dataset characteristics and hyperparameter tuning

## Next Checks
1. Verify one-point strong convexity: Test different discriminator architectures and measure whether adversarial loss exhibits one-point strong convexity empirically
2. Sensitivity analysis: Systematically vary moving average parameter γ and adversarial weight α to identify stable regions and failure modes
3. Cross-dataset generalization: Apply SoFCLR to additional datasets beyond CelebA and UTKface to verify fairness improvements generalize to different data distributions and sensitive attributes