---
ver: rpa2
title: Quantization of Large Language Models with an Overdetermined Basis
arxiv_id: '2404.09737'
source_url: https://arxiv.org/abs/2404.09737
tags:
- quantization
- matrix
- kashin
- algorithm
- matrices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Kashin Quantization, a novel approach for
  quantizing large language models (LLMs) based on Kashin representation. The method
  decomposes weight matrices into two factors with small infinity norms, enabling
  efficient low-bit quantization.
---

# Quantization of Large Language Models with an Overdetermined Basis

## Quick Facts
- arXiv ID: 2404.09737
- Source URL: https://arxiv.org/abs/2404.09737
- Reference count: 6
- Key outcome: Kashin Quantization achieves competitive or superior performance compared to standard quantization methods while enabling lower-bit representations for LLMs.

## Executive Summary
This paper introduces Kashin Quantization, a novel approach for quantizing large language models (LLMs) based on Kashin representation. The method decomposes weight matrices into two factors with small infinity norms, enabling efficient low-bit quantization. A key contribution is the Matrix Decomposition Kashin Algorithm, which uses Kronecker-factored orthogonal matrices to reduce memory footprint and accelerate computations. The approach leverages structured orthogonal matrices like DCT and Butterfly matrices for fast matvec operations.

## Method Summary
Kashin Quantization decomposes weight matrices using the Matrix Decomposition Kashin Algorithm, which employs Kronecker-factored orthogonal matrices to reduce memory footprint and accelerate computations. The algorithm uses structured orthogonal matrices like DCT and Butterfly for fast matvec operations. After decomposition, entries are quantized by replacing them with nearest cluster centroids, achieving lower-bit representations while maintaining model performance.

## Key Results
- Kashin Quantization achieves competitive or superior performance compared to standard quantization methods
- 4-bit + Kashin 6-bit quantization nearly restores the quality of 8-bit models on OPT
- On GLUE, Kashin 4-bit quantization outperforms uniform and Kmeans quantization for both BERT and RoBERTa across most tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Kashin decomposition concentrates vector entries into a few dominant peaks, enabling low-bit quantization.
- Mechanism: Any vector x is decomposed as x ≈ u + Qv where both u and Qv have small infinity norms. The decomposition concentrates entries around a small number of values, which can be replaced with cluster centroids.
- Core assumption: The infinity norms of u and v remain bounded, and the concentration of entries is sufficient for accurate clustering.
- Evidence anchors:
  - [abstract]: "Surprisingly, the entries of factors after decomposition are well-concentrated around several peaks, which allows us to efficiently replace them with corresponding centroids for quantization purposes."
  - [section]: "Figure 6 clearly illustrates that values of factors U and V are well quantized. The red lines and dots on the graph represent the centroids of corresponding clusters of values."
  - [corpus]: Weak evidence - related papers focus on quantization but do not validate the concentration property.
- Break condition: If decomposition yields factors with widely spread entries or if infinity norms grow unbounded, clustering will fail.

### Mechanism 2
- Claim: Matrix Decomposition Kashin Algorithm reduces memory and computational cost compared to vectorizing matrices.
- Mechanism: Instead of vectorizing an m×n matrix into size mn, the algorithm uses Kronecker-factored orthogonal matrices Q1 and Q2, reducing storage from (mn)² to m² + n² and accelerating matvec operations.
- Core assumption: Q can be approximated as Q = Q2 ⊗ Q1 without losing convergence guarantees.
- Evidence anchors:
  - [section]: "For an m × n matrix to decompose, we need a matrix of (mn)² elements. Instead, we propose to use a Kronecker-factored matrix, which leads to the memory footprint of m² + n²."
  - [section]: "Figure 4 illustrates that the time needed for the same computations is notably less for the matrix version of the algorithm than for the vector one."
  - [corpus]: Weak evidence - neighbor papers discuss matrix quantization but not Kronecker-factored structures.
- Break condition: If Q cannot be well-approximated by Kronecker factors, convergence degrades and memory savings vanish.

### Mechanism 3
- Claim: Choice of orthogonal matrix Q affects convergence rate and quantization quality.
- Mechanism: Different structured matrices (DCT, Butterfly, QR, Householder) lead to different bounds on max(∥x∥1, ∥QT x∥1), impacting the number of iterations needed for convergence.
- Core assumption: Empirical convergence patterns reflect underlying theoretical bounds related to Kolmogorov widths.
- Evidence anchors:
  - [section]: "Our experiments demonstrate, that the convergence speed varies when choosing from the classes of structured orthogonal matrices. The demonstration is presented in Figure 2."
  - [section]: "Table 1: Estimation for x = Re[y], y ∈ eig(Q)... These results allow us to hypothesize that qr and butterfly matrices in the general case should converge best."
  - [corpus]: No direct evidence - neighbor papers do not address structured orthogonal matrices in quantization.
- Break condition: If a poor choice of Q leads to slow convergence or divergence, the method becomes impractical.

## Foundational Learning

- Concept: Vector and matrix norms (infinity norm, L2 norm)
  - Why needed here: Kashin decomposition relies on bounding infinity norms of factors and measuring convergence via L2 norms.
  - Quick check question: If u = [1, -2, 3], what is ∥u∥∞ and ∥u∥2?

- Concept: Orthogonal matrices and their properties
  - Why needed here: The algorithm uses orthogonal matrices to rotate and decompose data; structured orthogonal matrices enable fast matvec operations.
  - Quick check question: What is the effect of multiplying a vector by an orthogonal matrix on its L2 norm?

- Concept: Clustering and quantization
  - Why needed here: After decomposition, entries are quantized by replacing them with nearest cluster centroids.
  - Quick check question: If you have 4 clusters per factor, how many bits are needed to represent a pair of quantized entries?

## Architecture Onboarding

- Component map: Input weight matrix → Matrix Decomposition Kashin Algorithm (with Q1, Q2) → Factors U, V → Clustering → Quantized factors Uq, Vq → Reconstructed forward pass using Uq + Q1VqQT2.
- Critical path: Decomposition → Clustering → Forward pass reconstruction; decomposition quality directly determines quantization error.
- Design tradeoffs: Memory vs. convergence speed (structured Q for speed vs. random Q for robustness); number of clusters vs. quantization fidelity.
- Failure signatures: Slow or failed convergence of decomposition; large infinity norms after decomposition; poor clustering due to wide value spread.
- First 3 experiments:
  1. Apply decomposition to a synthetic random matrix and plot factor entry distributions.
  2. Test convergence with different structured Q (DCT, Butterfly, QR) on a small transformer weight matrix.
  3. Quantize a single OPT layer and measure perplexity impact vs. uniform quantization.

## Open Questions the Paper Calls Out
None

## Limitations
- Convergence guarantees vary significantly depending on the choice of orthogonal matrix, with the relationship between theoretical bounds and empirical performance not fully established.
- The method doesn't fully address how to handle outlier activations in transformer models during weight quantization.
- Computational overhead of the decomposition step during preprocessing isn't fully characterized in terms of wall-clock time or energy consumption.

## Confidence
- High confidence: The basic mechanism of Kashin decomposition concentrating entries for quantization is well-supported by empirical evidence in Figure 6.
- Medium confidence: The Kronecker-factored matrix approach for reducing memory footprint is demonstrated but lacks rigorous theoretical justification for why the approximation preserves convergence guarantees.
- Medium confidence: The claim that orthogonal matrix choice affects convergence is supported by experiments but the theoretical connection to Kolmogorov widths remains a hypothesis rather than a proven result.

## Next Checks
1. **Convergence robustness test**: Systematically test the decomposition algorithm on a diverse set of weight matrices (including pathological cases like diagonal matrices, sparse matrices, and matrices with large condition numbers) to identify failure modes and characterize the conditions under which convergence degrades.

2. **Structured vs. unstructured orthogonal matrices**: Conduct a controlled experiment comparing Kashin quantization using structured orthogonal matrices (DCT, Butterfly) against using fully random orthogonal matrices across multiple model architectures and datasets to quantify the tradeoff between computational efficiency and quantization quality.

3. **Ablation study on decomposition depth**: Investigate how the number of decomposition iterations affects final quantization quality by varying the convergence threshold and measuring the resulting model performance, perplexity, and accuracy metrics to establish the practical convergence criteria.