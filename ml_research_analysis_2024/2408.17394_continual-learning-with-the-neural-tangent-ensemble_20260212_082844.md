---
ver: rpa2
title: Continual learning with the neural tangent ensemble
arxiv_id: '2408.17394'
source_url: https://arxiv.org/abs/2408.17394
tags:
- neural
- learning
- networks
- should
- posterior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces the Neural Tangent Ensemble (NTE), which\
  \ interprets a neural network as an ensemble of classifiers\u2014one for each parameter\u2014\
  via a first-order Taylor expansion around a seed point. In the lazy regime, these\
  \ classifiers are fixed, and the paper shows that updating their posterior probabilities\
  \ under new data is approximately equivalent to single-example stochastic gradient\
  \ descent (SGD) projected onto the probability simplex."
---

# Continual learning with the neural tangent ensemble

## Quick Facts
- arXiv ID: 2408.17394
- Source URL: https://arxiv.org/abs/2408.17394
- Authors: Ari S. Benjamin; Christian Pehle; Kyle Daruwalla
- Reference count: 40
- Primary result: The Neural Tangent Ensemble (NTE) interprets a neural network as a fixed ensemble of classifiers in the lazy regime, enabling continual learning without catastrophic forgetting through Bayesian-style updates.

## Executive Summary
This paper introduces the Neural Tangent Ensemble (NTE), a novel interpretation of neural networks in the lazy training regime as ensembles of classifiers fixed at a seed parameter point. By modeling each parameter perturbation as a separate classifier and updating posterior probabilities under new data, the framework approximates single-example stochastic gradient descent while maintaining Bayesian ensemble properties. This approach naturally addresses catastrophic forgetting in continual learning, as Bayesian ensembles are inherently invariant to data ordering. The authors demonstrate that NTE updates converge to an optimal continual learning rule in the infinite-width limit and show empirical results linking momentum to increased forgetting and network width to improved retention under specific conditions.

## Method Summary
The NTE framework treats a neural network as an ensemble of classifiers, where each parameter in the model corresponds to a separate classifier via first-order Taylor expansion around a seed parameter point. In the lazy regime, these classifiers are fixed, and updating their posterior probabilities under new data is approximately equivalent to single-example stochastic gradient descent projected onto the probability simplex. This interpretation provides a principled approach to continual learning by leveraging the data-ordering invariance of Bayesian ensembles. The method predicts that momentum increases forgetting and that network width improves retention only with certain optimizers, which are validated empirically.

## Key Results
- NTE updates are approximately equivalent to single-example SGD projected onto the probability simplex in the lazy regime
- Bayesian ensemble invariance to data ordering prevents catastrophic forgetting in continual learning
- Momentum increases forgetting while network width improves retention only with certain optimizers, supporting theoretical claims
- NTE predicts and demonstrates that using current gradients instead of initialization gradients improves performance as networks grow wider

## Why This Works (Mechanism)
The NTE framework works by exploiting the structure of neural networks in the lazy training regime, where parameter dynamics are dominated by movement along the tangent space at initialization. By interpreting each parameter perturbation as a separate classifier, the framework creates an ensemble whose posterior probabilities can be updated under new data without changing the underlying classifiers. This is equivalent to a Bayesian update that naturally maintains performance on previous tasks. The invariance to data ordering in Bayesian ensembles directly translates to robustness against catastrophic forgetting, as the update mechanism doesn't privilege recent data over historical data.

## Foundational Learning
- **Lazy training regime**: Understanding when neural networks operate in a regime where parameters move primarily along the tangent space at initialization. Why needed: This regime enables the Taylor expansion approximation that underpins the NTE framework. Quick check: Verify that network width and learning rate satisfy conditions for lazy training.
- **Taylor expansion in parameter space**: Approximating network outputs as linear functions of parameter perturbations around a seed point. Why needed: This linearization allows treating each parameter as defining a separate classifier. Quick check: Confirm that higher-order terms are negligible in the lazy regime.
- **Bayesian ensemble methods**: Framework for combining multiple classifiers with posterior probabilities. Why needed: Provides the theoretical foundation for updating classifier weights without catastrophic forgetting. Quick check: Ensure posterior updates maintain proper normalization and convergence properties.
- **Catastrophic forgetting**: The phenomenon where neural networks rapidly forget previously learned tasks when trained on new data. Why needed: The primary problem NTE aims to solve. Quick check: Monitor performance degradation on previous tasks during sequential training.
- **Stochastic gradient descent dynamics**: Understanding how SGD behaves on individual examples versus mini-batches. Why needed: NTE's update rule approximates single-example SGD in specific regimes. Quick check: Compare NTE updates with true single-example SGD trajectories.

## Architecture Onboarding

**Component Map:**
Neural Network -> Taylor Expansion -> Classifier Ensemble -> Posterior Updates -> Continual Learning

**Critical Path:**
1. Initialize network at seed parameters
2. Compute first-order Taylor expansion around seed point
3. Interpret each parameter perturbation as classifier
4. Update posterior probabilities under new data
5. Maintain ensemble performance across tasks

**Design Tradeoffs:**
- Taylor expansion accuracy vs computational efficiency
- Ensemble size (parameter count) vs memory requirements
- Lazy regime assumptions vs practical applicability
- Bayesian update complexity vs SGD simplicity

**Failure Signatures:**
- Significant deviation from linear behavior (Taylor expansion breaks down)
- Posterior probabilities becoming degenerate (ensemble collapse)
- Performance degradation on previous tasks despite updates
- Divergence between NTE updates and true SGD trajectories

**First Experiments:**
1. Verify Taylor expansion accuracy in lazy regime for different network widths
2. Compare NTE performance with standard continual learning baselines on permuted MNIST
3. Test the effect of momentum and learning rate on forgetting in the NTE framework

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Theoretical claims rely on strong assumptions of the lazy training regime, which may not hold for practical networks
- Empirical validation is limited to specific architectures and optimizers, leaving generalization uncertain
- The mechanism by which network width improves retention is not fully characterized
- Convergence to the ideal update rule in the infinite-width limit is asserted but not demonstrated empirically

## Confidence
- **High**: Theoretical framework and mathematical derivation of the NTE update rule equivalence to single-example SGD
- **Medium**: Empirical results supporting the relationship between momentum, width, and forgetting in the NTE context
- **Low**: Generalization of claims to diverse architectures, datasets, and optimizers beyond the tested scenarios

## Next Checks
1. Test the NTE framework on convolutional architectures and non-image datasets to assess generalization beyond MLPs and MNIST
2. Experimentally verify the convergence to the ideal update rule at very large network widths approaching the infinite-width limit
3. Evaluate the impact of different optimizers and learning rate schedules on NTE performance and forgetting across multiple continual learning scenarios