---
ver: rpa2
title: The Automated Verification of Textual Claims (AVeriTeC) Shared Task
arxiv_id: '2410.23850'
source_url: https://arxiv.org/abs/2410.23850
tags:
- evidence
- claim
- systems
- task
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The AVeriTeC shared task focused on automated fact-checking of
  real-world claims using web-based evidence. Participants retrieved evidence via
  search engines or a provided knowledge store, then predicted claim veracity.
---

# The Automated Verification of Textual Claims (AVeriTeC) Shared Task

## Quick Facts
- arXiv ID: 2410.23850
- Source URL: https://arxiv.org/abs/2410.23850
- Reference count: 24
- Winning system achieved 63% AVeriTeC score, significantly outperforming the 11% baseline

## Executive Summary
The AVeriTeC shared task introduced a comprehensive framework for automated fact-checking of real-world claims using web-based evidence. Participants were required to retrieve relevant evidence through search engines or a provided knowledge store, then predict the veracity of claims based on this evidence. The task represented a significant advancement in fact-checking evaluation by integrating both evidence retrieval and veracity prediction components, moving beyond traditional approaches that focus solely on claim verification.

The competition attracted five teams who primarily leveraged large language models, with the winning system achieving 63% accuracy compared to a 11% baseline. GPT-4o emerged as the dominant model choice, though smaller models like Llama-3-8b demonstrated strong performance particularly in question generation and retrieval tasks. The shared task highlighted both the potential and challenges of automated fact-checking systems, emphasizing the critical role of evidence retrieval alongside claim verification.

## Method Summary
Participants in the AVeriTeC shared task developed systems that performed two main functions: evidence retrieval and claim verification. For evidence retrieval, teams could either use web search engines or query a provided knowledge store containing relevant documents. The retrieved evidence was then used by the systems to determine the veracity of claims, with teams submitting both their evidence selection and final verdict predictions.

The evaluation framework assessed systems on both their ability to retrieve relevant evidence and their accuracy in predicting claim veracity. This dual-component evaluation represented a more realistic approach to fact-checking automation compared to previous benchmarks that focused solely on claim verification. Most successful approaches relied heavily on large language models, particularly GPT-4o, though some teams experimented with smaller models for specific subtasks like question generation.

## Key Results
- Winning system achieved 63% AVeriTeC score versus 11% baseline
- GPT-4o was the dominant model choice among participants
- Llama-3-8b showed strong performance in question generation and retrieval tasks
- Evidence retrieval proved critical for system success alongside veracity prediction

## Why This Works (Mechanism)
The AVeriTeC task works by breaking down the complex problem of automated fact-checking into manageable components: evidence retrieval and claim verification. By requiring systems to first retrieve relevant evidence before making veracity predictions, the task creates a more realistic simulation of human fact-checking workflows. This approach allows systems to leverage contextual information and multiple sources rather than relying solely on internal knowledge, which is particularly important for verifying claims about current events or specialized topics.

The use of web-based evidence through search engines or knowledge stores enables systems to access up-to-date information and handle claims that may not be covered in pre-existing training data. This dynamic evidence retrieval capability, combined with powerful language models capable of reasoning over multiple pieces of evidence, allows for more nuanced and accurate fact-checking compared to approaches that rely solely on static knowledge bases.

## Foundational Learning
- **Evidence Retrieval**: The process of finding relevant documents or web pages to support fact-checking claims. Needed because automated systems must access current and comprehensive information beyond their training data. Quick check: Verify retrieval systems can find multiple relevant sources for diverse claim types.

- **Veracity Prediction**: The classification of claims as true, false, or other verdicts based on evidence. Essential for the core fact-checking task of determining claim accuracy. Quick check: Test prediction accuracy across different verdict categories and evidence types.

- **Knowledge Store Integration**: Using pre-indexed document collections as evidence sources instead of web search. Important for reducing latency and controlling costs in production environments. Quick check: Compare performance between knowledge store and web search retrieval approaches.

- **Large Language Model Reasoning**: Using LLMs to analyze evidence and make verification decisions. Critical for handling complex reasoning and multi-hop inference required in fact-checking. Quick check: Evaluate reasoning quality through systematic error analysis.

- **Question Generation**: Creating specific queries to guide evidence retrieval. Valuable for improving retrieval precision and relevance. Quick check: Measure retrieval performance with and without question generation components.

## Architecture Onboarding

**Component Map**: Web Search/Knowledge Store -> Evidence Retrieval -> LLM Reasoning -> Veracity Prediction

**Critical Path**: Claim Input -> Question Generation -> Evidence Retrieval -> Evidence Analysis -> Veracity Prediction -> Output

**Design Tradeoffs**: The task requires balancing between comprehensive evidence retrieval (which may be expensive and time-consuming) and efficient veracity prediction (which may sacrifice accuracy for speed). Using web search provides access to current information but introduces latency and cost, while knowledge stores offer faster, cheaper access but may be outdated. Large language models provide strong reasoning capabilities but are computationally expensive and may not be accessible to all users.

**Failure Signatures**: Systems may fail when evidence retrieval misses critical information, when language models struggle with complex reasoning across multiple sources, or when claims involve specialized knowledge not well-represented in available evidence. Rarer verdict labels (like "partially true" or "out of scope") may be systematically misclassified due to training data imbalances.

**First Experiments**:
1. Evaluate baseline retrieval performance using simple keyword matching before adding LLM-based approaches
2. Compare veracity prediction accuracy using gold evidence versus retrieved evidence to isolate retrieval impact
3. Test different question generation strategies to optimize evidence retrieval precision

## Open Questions the Paper Calls Out
None

## Limitations
- The 11% baseline versus 63% winning score suggests the task remains highly challenging, raising questions about evaluation metric discriminative power
- Reliance on proprietary models like GPT-4o introduces reproducibility and accessibility concerns for real-world deployment
- The evaluation framework's handling of rarer verdict labels may systematically disadvantage comprehensive verification approaches

## Confidence
Medium confidence in reported results. The methodology and evaluation process appear sound, but the relatively small number of participating teams (five) and use of a single shared task environment limit generalizability. The reliance on language model-based approaches, while well-documented, may not represent the full space of viable verification strategies.

## Next Checks
1. Replicate the top-performing systems using alternative evaluation datasets to assess generalizability beyond the AVeriTeC corpus
2. Conduct head-to-head comparisons between proprietary models (GPT-4o) and their open-source counterparts under identical resource constraints
3. Perform ablation studies to quantify the individual contributions of evidence retrieval versus veracity prediction components to overall system performance