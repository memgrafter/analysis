---
ver: rpa2
title: First Place Solution to the Multiple-choice Video QA Track of The Second Perception
  Test Challenge
arxiv_id: '2409.13538'
source_url: https://arxiv.org/abs/2409.13538
tags:
- video
- frames
- resolution
- ensemble
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a first-place solution for the Multiple-choice
  Video Question Answering track of The Second Perception Test Challenge. The authors
  fine-tuned the QwenVL2 (7B) model on the training dataset and employed model ensemble
  strategies combined with Test Time Augmentation to improve performance.
---

# First Place Solution to the Multiple-choice Video QA Track of The Second Perception Test Challenge

## Quick Facts
- arXiv ID: 2409.13538
- Source URL: https://arxiv.org/abs/2409.13538
- Authors: Yingzhe Peng; Yixiao Yuan; Zitian Ao; Huapeng Zhou; Kangqi Wang; Qipeng Zhu; Xu Yang
- Reference count: 4
- Primary result: First-place solution achieving 0.7647 Top-1 Accuracy on Multiple-choice Video QA track

## Executive Summary
This work presents the winning solution for the Multiple-choice Video Question Answering track of The Second Perception Test Challenge. The authors fine-tuned the QwenVL2 (7B) model on the training dataset and employed comprehensive model ensemble strategies combined with Test Time Augmentation (TTA) to achieve state-of-the-art performance. The approach leveraged High-Resolution Instruction Tuning with 5-fold cross-validation, increased video resolution (up to 560×630), and extended frame extraction (60 frames) to enhance video understanding capabilities. Through an ensemble of 31 model variants with weighted voting, the team achieved a Top-1 Accuracy of 0.7647 on the competition leaderboard.

## Method Summary
The authors fine-tuned the QwenVL2 (7B) model using LoRA adapters with Rank=16, Alpha=32, and Dropout=0.05. They employed 5-fold cross-validation on high-resolution videos (560×630) and extracted 30-60 frames per video. The training setup utilized 4x NVIDIA A6000 GPUs with learning rate 1e-4 and batch size 8. Test Time Augmentation was applied by shuffling multiple-choice options and re-running inference on different permutations. The final predictions were generated through weighted voting across 31 model variants trained with different strategies (baseline, cross-validation, TTA, different resolutions/frames).

## Key Results
- Achieved first place in the competition with Top-1 Accuracy of 0.7647
- Successfully implemented high-resolution instruction tuning (560×630) compared to baseline (240×420)
- Demonstrated effectiveness of ensemble approach with 31 model variants and weighted voting
- Showed that increasing frame count from 30 to 60 improved video understanding capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-resolution instruction tuning improves model performance on high-resolution video understanding tasks
- Mechanism: By increasing the maximum number of pixels from 176,400 to 352,800 (315×560 to 560×630) during training, the model learns to process finer visual details and spatial relationships in high-resolution videos, leading to better comprehension of complex video content
- Core assumption: The training data distribution matches the test data distribution in terms of video resolution, making high-resolution training beneficial
- Evidence anchors:
  - [section] "We analyzed the resolution statistics of the dataset. The majority of the competition data consisted of high-resolution videos. Therefore, we decided to train with higher-resolution videos."
  - [section] "Through cross-validation, we obtained five models fine-tuned with high-resolution instructions."
  - [section] "Using higher resolution and more frames improved the model's video understanding capabilities."

### Mechanism 2
- Claim: Model ensemble with weighted voting improves final accuracy
- Mechanism: Combining predictions from 31 model variants trained with different strategies (baseline, cross-validation, TTA, different resolutions/frames) creates a more robust prediction by capturing diverse aspects of the video understanding task
- Core assumption: Different models capture complementary information and errors are uncorrelated across models
- Evidence anchors:
  - [section] "Our ensemble strategy was crucial to achieving high accuracy."
  - [section] "We had 31 sets of model inference results for ensemble. Different voting weights were assigned to different results, as shown in Table 4."
  - [abstract] "Through comprehensive ensemble of 31 model variants with weighted voting, the approach achieved a Top-1 Accuracy of 0.7647"

### Mechanism 3
- Claim: Test Time Augmentation (TTA) with option shuffling reduces positional bias in predictions
- Mechanism: By shuffling the order of multiple-choice options and re-running inference on different permutations, the model's predictions become less sensitive to the arbitrary ordering of options, reducing systematic biases
- Core assumption: The model has positional bias that affects answer selection regardless of content
- Evidence anchors:
  - [section] "We applied Test Time Augmentation (TTA) by shuffling the order of multiple-choice options... TTA aims to reduce positional bias in the model's predictions."
  - [section] "We applied this strategy by generating three additional random permutations of the options and used the cross-validation models to re-infer"

## Foundational Learning

- Concept: Cross-validation
  - Why needed here: To ensure model robustness and prevent overfitting when training multiple variants for ensemble
  - Quick check question: Why did the authors use 5-fold cross-validation instead of a simple train/validation split?

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: To efficiently fine-tune large language models by adapting only a small subset of parameters, reducing computational requirements
  - Quick check question: What advantage does LoRA provide over full fine-tuning for the 7B parameter QwenVL2 model?

- Concept: Test Time Augmentation
  - Why needed here: To improve prediction robustness by applying data augmentation techniques during inference rather than just training
  - Quick check question: How does shuffling answer options during inference help reduce bias in the model's predictions?

## Architecture Onboarding

- Component map: Video preprocessing (frame extraction + resolution scaling) -> QwenVL2 (7B) with LoRA adapters -> TTA generation (option shuffling) -> Weighted ensemble voting (31 variants)

- Critical path: Video preprocessing → Model inference (with LoRA) → TTA generation → Weighted ensemble voting

- Design tradeoffs:
  - Higher resolution vs. memory constraints (solved with LoRA and DeepSpeed)
  - More frames vs. computational cost (trade-off between temporal context and efficiency)
  - Ensemble size vs. inference latency (31 models provide accuracy but increase inference time)

- Failure signatures:
  - Poor performance on low-resolution videos despite high-resolution training
  - Overfitting indicated by large gap between training and validation accuracy
  - Ensemble performance worse than individual models (indicating correlated errors)

- First 3 experiments:
  1. Baseline model training with standard resolution (240×420) and 30 frames to establish performance floor
  2. High-resolution instruction tuning with 5-fold cross-validation to validate resolution improvement hypothesis
  3. TTA with option shuffling on a subset of validation data to measure bias reduction effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model performance scale with even higher resolutions beyond 560×630 and more frames beyond 60?
- Basis in paper: [explicit] The authors found that higher resolution and more frames improved performance, noting that "the more pixels and nframes will enhance the model's capabilities of video understanding" and tested up to 560×630 resolution with 60 frames.
- Why unresolved: The paper only tested specific resolution and frame combinations, leaving the optimal trade-off between computational cost and performance unclear for even higher values.
- What evidence would resolve it: Systematic experiments testing various combinations of resolution (e.g., 720p, 1080p) and frame counts (e.g., 90, 120 frames) while measuring accuracy gains and computational overhead would clarify the scalability limits.

### Open Question 2
- Question: What is the impact of different option shuffling strategies in Test Time Augmentation on model performance?
- Basis in paper: [explicit] The authors used TTA by "shuffling the order of multiple-choice options" and generating "three additional random permutations of the options," but did not explore alternative shuffling strategies.
- Why unresolved: The paper only tested random permutations without comparing to other potential strategies like systematic rotations or stratified shuffling, leaving uncertainty about the optimal TTA approach.
- What evidence would resolve it: Comparative experiments testing different shuffling methods (random, systematic, stratified) and their impact on accuracy would determine the most effective TTA strategy.

### Open Question 3
- Question: How does the model's performance change when using different cross-validation strategies beyond 5-fold?
- Basis in paper: [explicit] The authors used 5-fold cross-validation for High-Resolution Instruction Tuning but did not explore the impact of using different numbers of folds (e.g., 3-fold, 10-fold).
- Why unresolved: The choice of 5-fold cross-validation appears arbitrary, and the paper does not discuss how varying the number of folds might affect model robustness and performance.
- What evidence would resolve it: Experiments comparing model performance and training efficiency across different cross-validation schemes (e.g., 3-fold, 5-fold, 10-fold) would reveal the optimal balance between model stability and computational cost.

## Limitations

- Lack of ablation studies demonstrating individual contribution of each component to final performance
- Weighted ensemble strategy relies on empirically assigned weights without clear justification or optimization
- No quantitative validation that Test Time Augmentation actually reduces positional bias in predictions

## Confidence

**High Confidence**: The claim that the presented approach achieved first place in the competition (0.7647 Top-1 Accuracy) is well-supported by the leaderboard results and the detailed description of the ensemble methodology.

**Medium Confidence**: The claim that high-resolution instruction tuning improves video understanding capabilities is moderately supported by the observation that most competition data consisted of high-resolution videos.

**Low Confidence**: The assertion that TTA with option shuffling effectively reduces positional bias lacks quantitative validation.

## Next Checks

1. **Ablation Study**: Train and evaluate a baseline model with standard resolution (240×420) and 30 frames, then compare its performance directly against the high-resolution variant on the same validation split to isolate the effect of resolution improvements.

2. **Ensemble Weight Optimization**: Perform grid search or gradient-based optimization to find optimal ensemble weights rather than using empirically assigned values, then measure the improvement in validation accuracy to determine if the current weights are suboptimal.

3. **Bias Quantification**: Measure positional bias in the baseline model by evaluating performance when options are systematically reordered, then demonstrate that TTA with shuffling reduces this bias by comparing prediction consistency across permutations.