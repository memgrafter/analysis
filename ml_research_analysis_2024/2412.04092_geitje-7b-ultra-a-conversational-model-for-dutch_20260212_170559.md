---
ver: rpa2
title: 'GEITje 7B Ultra: A Conversational Model for Dutch'
arxiv_id: '2412.04092'
source_url: https://arxiv.org/abs/2412.04092
tags:
- dutch
- geitje
- ultra
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GEITje 7B Ultra, a conversational Dutch
  language model built by extending the existing GEITje model with supervised finetuning
  and preference alignment. Two new high-quality synthetic datasets were created:
  Ultra Chat 200k Dutch for supervised finetuning and Ultra Feedback Dutch for preference
  alignment, both generated using GPT-4.'
---

# GEITje 7B Ultra: A Conversational Model for Dutch

## Quick Facts
- arXiv ID: 2412.04092
- Source URL: https://arxiv.org/abs/2412.04092
- Reference count: 32
- Key outcome: GEITje 7B Ultra achieves comparable performance to GEITje 7B Chat v2 across benchmarks while improving conversational alignment with user preferences

## Executive Summary
This paper introduces GEITje 7B Ultra, a conversational Dutch language model built by extending the existing GEITje model with supervised finetuning and preference alignment. Two new high-quality synthetic datasets were created: Ultra Chat 200k Dutch for supervised finetuning and Ultra Feedback Dutch for preference alignment, both generated using GPT-4. The model was trained in two steps: first, GEITje 7B was finetuned with the SFT datasets, then DPO was applied with the preference dataset. The resulting GEITje 7B Ultra achieves comparable performance to the prior GEITje 7B Chat v2 model across standard benchmarks, while improving conversational alignment with user preferences. The model, datasets, and training code are publicly available.

## Method Summary
GEITje 7B Ultra was developed through a two-stage training process. First, the base GEITje 7B model was fine-tuned using supervised learning on two datasets: Ultra Chat 200k Dutch (192,598 training samples) and No Robots Dutch (8,181 training samples). Second, Direct Preference Optimization (DPO) was applied using the Ultra Feedback Dutch dataset (48,228 training samples) with a beta parameter of 0.1. The synthetic datasets were generated using GPT-4 with original English prompts translated to Dutch, avoiding translationese issues. The model was evaluated using ScandEval benchmarks and compared against the existing GEITje 7B Chat v2 model.

## Key Results
- GEITje 7B Ultra achieves comparable benchmark performance to GEITje 7B Chat v2
- The model shows improved conversational alignment with user preferences through DPO training
- Two new high-quality Dutch conversational datasets (Ultra Chat 200k Dutch and Ultra Feedback Dutch) were created and released

## Why This Works (Mechanism)

### Mechanism 1
Using GPT-4 to generate Dutch responses rather than translating existing prompts avoids translationese and produces higher-quality conversational data. GPT-4 is instructed to converse in Dutch using original English prompts as seeds, generating responses that are native Dutch rather than translated from English. This preserves naturalness and reduces grammatical errors. The core assumption is that GPT-4 can generate high-quality Dutch output and avoid introducing errors present in translated text. Evidence shows that translating only the user prompt to Dutch and having GPT-4 generate the response avoids translation errors or translationese effects. The break condition is if GPT-4's Dutch generation quality is insufficient or if it introduces significant grammatical errors.

### Mechanism 2
Preference alignment via DPO (Direct Preference Optimization) can improve conversational quality without requiring a separate reward model. The DPO algorithm directly optimizes the model to prefer responses that are rated higher on Dutch-ness, helpfulness, and conciseness. This embedded optimization avoids the computational cost of training a separate reward model. The core assumption is that the preference dataset (GPT-4 vs GEITje responses) provides a reliable signal for what constitutes better conversational quality. Evidence includes that DPO embeds the optimal policy in the model itself and adds a classification loss to determine which of the given answers to a given prompt are preferred. The break condition is if the preference data is not representative or if the scoring criteria are not well-calibrated.

### Mechanism 3
Incorporating diverse user personas in synthetic data generation improves the model's ability to handle different user types and communication styles. GPT-4 is prompted to generate responses while adopting different personas (e.g., child, expert, direct, joker) with specific characteristics. This creates a dataset with varied communication styles and vocabulary. The core assumption is that training on diverse personas will make the model more adaptable to real-world user interactions. Evidence shows that by having a dataset containing user messages written from different viewpoints and high-quality GPT-4 answers, models trained with this type of data are more responsive to a broad spectrum of users. The break condition is if the persona descriptions are not well-defined or if GPT-4 doesn't faithfully adopt the personas.

## Foundational Learning

- Concept: Supervised fine-tuning (SFT) on conversational datasets
  - Why needed here: SFT teaches the model to follow instructions and generate appropriate conversational responses in Dutch, building on the base GEITje model
  - Quick check question: How does SFT differ from pre-training, and why is it sufficient for improving conversational skills without full pre-training?

- Concept: Preference alignment via Direct Preference Optimization (DPO)
  - Why needed here: DPO refines the model to align with user preferences for response quality, improving its conversational appropriateness beyond what SFT alone achieves
  - Quick check question: What are the key differences between DPO and Reinforcement Learning from Human Feedback (RLHF), and why might DPO be preferred in this context?

- Concept: Synthetic data generation for low-resource languages
  - Why needed here: Dutch has limited high-quality conversational datasets, so generating synthetic data using GPT-4 is necessary to create sufficient training material
  - Quick check question: What are the trade-offs between using translated vs. generated synthetic data, and how might each approach affect the final model quality?

## Architecture Onboarding

- Component map: Base GEITje 7B model -> SFT training with Ultra Chat 200k Dutch and No Robots Dutch -> DPO training with Ultra Feedback Dutch -> ScandEval evaluation
- Critical path: Data generation → SFT training → DPO training → Evaluation and iteration
- Design tradeoffs: Using GPT-4 for data generation ensures high quality but introduces dependency on a commercial API and potential cost constraints. The DPO approach simplifies the alignment process but relies heavily on the quality of preference ratings
- Failure signatures: Poor conversational quality in evaluation could indicate issues with either the SFT datasets (e.g., insufficient diversity or quality) or the DPO training (e.g., misaligned preference ratings or inappropriate beta parameter)
- First 3 experiments:
  1. Test the SFT pipeline with a small subset of the Ultra Chat dataset to verify data quality and training stability
  2. Evaluate the intermediate SFT model on a held-out conversational task to assess initial improvements
  3. Run a small-scale DPO training with a subset of the preference dataset to tune the beta parameter and verify that the model learns to prefer higher-rated responses

## Open Questions the Paper Calls Out

### Open Question 1
How would the GEITje 7B Ultra model perform if trained with an expanded set of user personas beyond the nine specified in the Ultra Chat 200k Dutch dataset? The paper mentions that nine specific personas were created with given descriptions and probability distributions for the Ultra Chat 200k Dutch dataset, but only explores the performance impact of these nine personas without testing alternative or additional personas that might capture different user types or communication styles. Comparative performance metrics of GEITje 7B Ultra when trained with alternative persona sets, showing whether different persona distributions improve conversational alignment across specific user groups, would resolve this.

### Open Question 2
What is the optimal beta parameter value for the Direct Preference Optimization algorithm when training GEITje 7B Ultra with different dataset sizes or alternative preference alignment datasets? The paper notes that the recommended beta value of 0.01 "did not work for this model and dataset combination, leading to repetitions and hallucinations," and that hyperparameter search identified beta = 0.1 as optimal, but only tests one optimal beta value for the specific Ultra Feedback Dutch dataset without exploring how this parameter might vary with different dataset characteristics or sizes. Systematic testing of beta values across different dataset configurations and sizes, establishing guidelines for beta selection based on dataset properties, would resolve this.

### Open Question 3
How would the GEITje 7B Ultra model's performance change if both responses in the preference dataset were generated by high-quality models rather than using GEITje 7B Ultra as the rejected response? The paper describes the current approach where GPT-4 is always chosen as the preferred response and GEITje 7B Ultra as the rejected response, noting this as a potential limitation, but does not explore alternative approaches where both responses might come from similarly capable models, which could create more challenging and realistic preference distinctions. Performance comparisons of models trained on preference datasets where both responses are generated by high-quality models versus the current approach, measuring conversational alignment and benchmark performance, would resolve this.

## Limitations
- Model performance heavily depends on the quality of GPT-4-generated synthetic data, which may not consistently capture all aspects of natural Dutch conversation
- The evaluation relies on ScandEval benchmarks, which may not comprehensively capture all aspects of conversational quality
- The optimal beta parameter for DPO is dataset-specific and may not generalize to other preference alignment scenarios

## Confidence
- High Confidence: The technical methodology of using SFT followed by DPO is sound and well-established in the literature; the synthetic data generation approach (generating Dutch responses rather than translating) is logically sound and addresses known issues with translationese
- Medium Confidence: The claim that GEITje 7B Ultra achieves comparable performance to GEITje 7B Chat v2 is supported by benchmark results but limited by benchmark scope; the improvement in conversational alignment is plausible given the preference alignment approach but not directly measured against real human preferences
- Low Confidence: The assumption that GPT-4-generated synthetic data is consistently high-quality Dutch across all samples; the generalizability of the model to diverse real-world conversational contexts beyond the generated personas

## Next Checks
1. Conduct a blind human evaluation comparing GEITje 7B Ultra against GEITje 7B Chat v2 on diverse Dutch conversational tasks, measuring both fluency and task completion across different user personas not present in the training data
2. Evaluate the model's performance on Dutch conversational data from different sources (social media, customer service transcripts, informal conversations) to assess whether the synthetic training data adequately captures real-world language variation
3. Systematically vary the beta parameter in DPO training (e.g., 0.01, 0.05, 0.1, 0.2, 0.5) and evaluate the impact on both benchmark performance and conversational quality metrics to identify optimal settings and understand sensitivity to this critical hyperparameter