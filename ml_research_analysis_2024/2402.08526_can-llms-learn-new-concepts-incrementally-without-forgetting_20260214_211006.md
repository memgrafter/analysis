---
ver: rpa2
title: Can LLMs Learn New Concepts Incrementally without Forgetting?
arxiv_id: '2402.08526'
source_url: https://arxiv.org/abs/2402.08526
tags:
- learning
- what
- accuracy
- plms
- buffer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Concept-1K, a novel benchmark for incremental
  learning (IL) of large language models (LLMs). The dataset consists of 1,023 recently
  emerged concepts across diverse domains, presented as discrete, interpretable units
  of knowledge.
---

# Can LLMs Learn New Concepts Incrementally without Forgetting?

## Quick Facts
- arXiv ID: 2402.08526
- Source URL: https://arxiv.org/abs/2402.08526
- Authors: Junhao Zheng; Shengjie Qiu; Qianli Ma
- Reference count: 35
- Primary result: LLMs suffer from catastrophic forgetting in incremental learning; LoRA fine-tuning causes more forgetting than full fine-tuning.

## Executive Summary
This paper introduces Concept-1K, a novel benchmark for evaluating incremental learning (IL) of large language models (LLMs). The dataset contains 1,023 recently emerged concepts across diverse domains, presented as discrete QA pairs. Through systematic experiments on Pythia models, the authors demonstrate that LLMs still struggle with catastrophic forgetting when learning new concepts incrementally, with LoRA fine-tuning leading to more forgetting compared to full fine-tuning. The study reveals important tradeoffs between model scale, pretraining, buffer size, and in-context learning effectiveness for IL performance.

## Method Summary
The authors created Concept-1K by extracting concepts from ConceptNet using GPT-4 to generate 16,653 training-test QA pairs across 6 domains. They employed an instance-incremental learning (IIL) scenario where models learn 10 concept groups sequentially without revisiting old data. Evaluation used four metrics: Memorization Accuracy (MA), Memorization Forgetting rate (MF), Generalization Accuracy (GA), and Generalization Forgetting rate (GF). Experiments tested Pythia models (160M to 2.8B parameters) with LoRA and full fine-tuning approaches, varying buffer sizes from 0 to all previous data.

## Key Results
- LLMs experience catastrophic forgetting during incremental learning, with performance degrading on previously learned concepts when new ones are introduced.
- LoRA fine-tuning leads to more forgetting compared to full fine-tuning across all model sizes and buffer conditions.
- Larger models show reduced memorization forgetting but increased generalization forgetting compared to smaller models.
- Buffer size effectively mitigates forgetting, with diminishing returns beyond 5,000 samples.
- In-context learning performs poorly unless demonstration instances contain exactly the same knowledge as test instances.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models experience less catastrophic forgetting on training samples but more forgetting on test samples.
- Mechanism: Increasing model capacity allows better memorization of training data while reducing the need to generalize, thus preserving training accuracy but weakening generalization ability.
- Core assumption: The model size directly determines the balance between memorization and generalization.
- Evidence anchors:
  - [abstract] "When PLMs become larger, the memorization forgetting decreases while the generalization forgetting increases."
  - [section] "Table 2 shows that when PLMs become larger, the memorization forgetting decreases while the generalization forgetting increases."
  - [corpus] Weak; corpus contains no direct evidence of model-scale forgetting tradeoffs.
- Break condition: If generalization performance becomes critical and memorization is insufficient, larger models may underperform.

### Mechanism 2
- Claim: Pretraining improves generalization but may degrade memorization for new concepts.
- Mechanism: Early pretraining stages help models extract underlying knowledge, improving generalization. Excessive pretraining leads to overfitting to pretraining data, harming memorization of new concepts.
- Core assumption: Pretraining impacts generalization and memorization differently depending on the stage.
- Evidence anchors:
  - [abstract] "Figure 2 (b) shows that the memorization performance increases at the early stage of pretraining... However, when performing more pretraining steps, the memorization performance will be degraded."
  - [section] "The memorization performance increases at the early stage of pretraining (step 0 - step 10000). It indicates that pretraining enhances the memorization ability of PLMs to novel concepts. However, when performing more pretraining steps, the memorization performance will be degraded."
  - [corpus] Weak; corpus lacks discussion of pretraining effects on memorization/generalization.
- Break condition: If pretraining is too extensive, new concept learning may be severely impaired.

### Mechanism 3
- Claim: Buffer size directly mitigates catastrophic forgetting by providing data replay.
- Mechanism: Storing representative samples from previous concepts and replaying them during training prevents forgetting of old knowledge.
- Core assumption: Buffer replay is effective in reducing both memorization and generalization forgetting.
- Evidence anchors:
  - [abstract] "Figure 2 (a) and (e) show that a larger buffer size or a larger PLM improve the accuracy of memorization and generalization."
  - [section] "The model scale determines the upper limit of the generalization performance... The memorization accuracy of the 2.8B model is still unsatisfactory when no buffer is used."
  - [corpus] Weak; corpus focuses on catastrophic forgetting but does not directly address buffer replay effectiveness.
- Break condition: If buffer size is insufficient or poorly managed, forgetting will persist.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Core problem being addressed; understanding how models lose old knowledge when learning new tasks.
  - Quick check question: What happens to a model's performance on old tasks when it learns new ones without any mitigation?

- Concept: Incremental learning (IL)
  - Why needed here: The experimental framework for evaluating how models learn new concepts over time.
  - Quick check question: How does IL differ from standard multi-task learning?

- Concept: Memorization vs. generalization
  - Why needed here: Key performance metrics distinguishing retention of training data from ability to handle unseen examples.
  - Quick check question: Why might a model memorize training data but fail on test data?

## Architecture Onboarding

- Component map: Pythia model suite → LoRA vs. full fine-tuning → buffer management → evaluation pipeline (Concept-1K dataset)
- Critical path: Load pretrained model → fine-tune on concept groups sequentially → evaluate memorization/generalization → store results
- Design tradeoffs: LoRA offers parameter efficiency but may hurt memorization; full fine-tuning preserves learning ability but is resource-intensive
- Failure signatures: Rapid drop in test accuracy after new concept introduction; large gap between training and test performance
- First 3 experiments:
  1. Compare LoRA vs. full fine-tuning on Pythia-410M with buffer size=0
  2. Vary buffer size (0, 2000, 5000, 10000, All) on Pythia-2.8B
  3. Test memorization vs. generalization at different pretraining steps on Pythia-160M

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger language models (e.g., >13B parameters) perform in incremental learning compared to smaller models, particularly regarding catastrophic forgetting and concept generalization?
- Basis in paper: [explicit] The paper notes experiments were conducted on models with less than 13B parameters and states: "The conclusion of these experiments may not hold when fine-tuning SOTA PLMs such as GPT4 with more than 100B parameters."
- Why unresolved: The study's experimental scope was limited to models up to 13B parameters, leaving the behavior of much larger models unexplored.
- What evidence would resolve it: Empirical results comparing incremental learning performance (memorization and generalization accuracy, forgetting rates) of models like GPT4 (>100B parameters) against smaller models on the Concept-1K benchmark.

### Open Question 2
- Question: Can in-context learning be improved to effectively adapt large language models to novel concepts without catastrophic forgetting?
- Basis in paper: [explicit] The paper shows that in-context learning performance is unsatisfactory unless demonstration instances contain exactly the same knowledge as the test instance, concluding it "can not meet the goal of adapting PLMs to new knowledge."
- Why unresolved: The study only evaluated standard in-context learning without exploring advanced prompting strategies, chain-of-thought reasoning, or retrieval-augmented methods.
- What evidence would resolve it: Comparative experiments showing whether advanced in-context learning techniques (e.g., retrieval-augmented prompting, multi-step reasoning) can achieve comparable performance to fine-tuning on Concept-1K.

### Open Question 3
- Question: What is the impact of pretraining steps on incremental learning performance beyond the final checkpoint, and does there exist an optimal pretraining duration for IL?
- Basis in paper: [inferred] The paper observes that memorization performance initially improves with early pretraining steps but degrades with more steps, while generalization improves monotonically for larger models.
- Why unresolved: The study only tested a limited set of pretraining steps (0, 16, 128, 1000, 10000, 143000), leaving the full relationship between pretraining duration and IL performance unclear.
- What evidence would resolve it: Systematic experiments varying pretraining steps across a broader range (e.g., 10K to 1M steps) to identify trends and potential optimal points for IL performance on Concept-1K.

## Limitations

- Concept representation as discrete QA pairs may not capture the full complexity of real-world knowledge acquisition.
- Sequential learning scenario assumes concepts are independent, whereas in practice, concepts often overlap and build upon each other.
- Buffer replay mechanism requires storing potentially large amounts of data, which may not be practical for all applications.

## Confidence

**High Confidence**: The observation that LoRA fine-tuning leads to more catastrophic forgetting compared to full fine-tuning is well-supported by multiple experiments across different model sizes and buffer conditions.

**Medium Confidence**: The claim that larger models experience increased generalization forgetting while decreasing memorization forgetting is supported by data but lacks mechanistic explanation.

**Low Confidence**: The assertion that buffer size directly and linearly mitigates forgetting across all scenarios may be oversimplified.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate whether concepts learned in one domain (e.g., technology) transfer to improved performance in related domains (e.g., science), testing the assumption of concept independence.

2. **Buffer Management Optimization**: Systematically vary buffer composition strategies (random sampling vs. representative sampling) and measure their impact on both memorization and generalization performance.

3. **Long-Term Retention Analysis**: Extend the incremental learning timeline beyond the current setup to observe whether initial forgetting patterns stabilize or continue to evolve over extended training periods.