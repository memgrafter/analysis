---
ver: rpa2
title: 'DRL-STNet: Unsupervised Domain Adaptation for Cross-modality Medical Image
  Segmentation via Disentangled Representation Learning'
arxiv_id: '2409.18340'
source_url: https://arxiv.org/abs/2409.18340
tags:
- segmentation
- image
- domain
- drl-stnet
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DRL-STNet introduces a novel unsupervised domain adaptation framework
  for cross-modality medical image segmentation using disentangled representation
  learning and self-training. The method translates unpaired source and target modality
  images through a disentangled GAN, preserving anatomical structures while learning
  domain-invariant features.
---

# DRL-STNet: Unsupervised Domain Adaptation for Cross-modality Medical Image Segmentation via Disentangled Representation Learning

## Quick Facts
- arXiv ID: 2409.18340
- Source URL: https://arxiv.org/abs/2409.18340
- Authors: Hui Lin; Florian Schiffers; Santiago López-Tapia; Neda Tavakoli; Daniel Kim; Aggelos K. Katsaggelos
- Reference count: 40
- Primary result: Achieves 74.21% Dice Similarity Coefficient and 80.69% Normalized Surface Dice for cross-modality medical image segmentation, surpassing previous methods by 11.4% and 13.1% respectively

## Executive Summary
DRL-STNet introduces a novel unsupervised domain adaptation framework for cross-modality medical image segmentation using disentangled representation learning and self-training. The method translates unpaired source and target modality images through a disentangled GAN, preserving anatomical structures while learning domain-invariant features. A segmentation model is initially trained on synthetic translated images with source labels, then iteratively fine-tuned using real target images with pseudo-labels generated by the model itself. The approach was evaluated on the FLARE dataset for abdominal organ segmentation, achieving state-of-the-art performance with significant improvements over existing methods.

## Method Summary
The method consists of two main stages: first, a disentangled GAN translates unpaired CT and MRI images by separating content (anatomical structures) from style (modality-specific characteristics), then a 3D nnU-Net segmentation model is trained on the synthetic translated images. The self-training process iteratively generates pseudo-labels for real target images, which are combined with synthetic data to fine-tune the segmentation model. The translation network uses shared content encoders, separate style encoders, and a shared decoder with adversarial losses to ensure realistic translations while preserving anatomical details.

## Key Results
- Achieves 74.21% Dice Similarity Coefficient on abdominal organ segmentation, surpassing previous methods by 11.4%
- Reaches 80.69% Normalized Surface Dice, improving by 13.1% over existing approaches
- Demonstrates strong efficiency with 41-second average running time and 11,292 MB GPU memory usage
- Ablation studies confirm the effectiveness of both disentangled representation learning and self-training components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangled representation learning preserves anatomical structure while transferring style between modalities.
- Mechanism: The network splits images into content and style components using separate encoders. Content encoders extract modality-agnostic anatomical features, while style encoders capture modality-specific characteristics. A shared decoder reconstructs target modality images by combining content from source images with style from target images.
- Core assumption: Anatomical structures can be represented as modality-invariant content while imaging characteristics represent modality-dependent style.
- Evidence anchors:
  - [abstract] "preserving anatomical structures while learning domain-invariant features"
  - [section 2.1] "The image in each domain is disentangled into the content and style representations"

### Mechanism 2
- Claim: Self-training with pseudo-labels iteratively improves segmentation performance on unlabeled target data.
- Mechanism: After initial training on synthetic translated images, the model generates pseudo-labels for real target images. These pseudo-labels are combined with synthetic data in subsequent training iterations, allowing the model to learn from both high-quality synthetic data with ground truth and real data with imperfect but improving pseudo-labels.
- Core assumption: The model's initial predictions are sufficiently accurate to provide useful training signals when combined with synthetic data.
- Evidence anchors:
  - [abstract] "a segmentation model is initially trained with these translated images and corresponding source labels and then fine-tuned iteratively using a combination of synthetic and real images with pseudo-labels and real labels"
  - [section 2.2] "Predict pseudo-labels on unlabeled target volumes and fine-tune the segmentation model with the combined data"

### Mechanism 3
- Claim: GAN-based image translation creates realistic synthetic target images that bridge the domain gap.
- Mechanism: The adversarial loss components (Da, Db, Dc) ensure that translated images are indistinguishable from real target images both at pixel level and content level. This creates a synthetic dataset that approximates the target domain distribution while maintaining source domain annotations.
- Core assumption: The translated images preserve sufficient target modality characteristics to make segmentation models trained on them generalize to real target images.
- Evidence anchors:
  - [section 2.1] "The adversarial losses at the image and content levels are used to maintain the image and feature alignment"
  - [section 4.1] "xCT−>MRI and xMRI−>CT are highly likely to be reliable"

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: GANs provide the framework for learning the mapping between source and target modalities while ensuring generated images are realistic
  - Quick check question: What are the two competing objectives in a GAN and how do they work together?

- Concept: Disentangled Representation Learning
  - Why needed here: Separating anatomical content from imaging style allows translation between modalities while preserving essential structural information
  - Quick check question: How does disentangling content and style differ from simple image-to-image translation approaches?

- Concept: Self-Training with Pseudo-Labels
  - Why needed here: Enables iterative improvement using unlabeled target data, which is critical for unsupervised domain adaptation
  - Quick check question: What are the risks of using pseudo-labels in training and how can they be mitigated?

## Architecture Onboarding

- Component map: Image translation network (EC, Ea_S, Eb_S, G, Da, Db, Dc) -> Synthetic image generation -> Initial segmentation training -> Pseudo-label generation -> Iterative fine-tuning with combined data -> 3D nnU-Net segmentation backbone

- Critical path: Image translation → synthetic data generation → initial segmentation training → pseudo-label generation → iterative fine-tuning

- Design tradeoffs:
  - 2D vs 3D translation: 2D allows slice-by-slice translation but may miss volumetric consistency
  - Number of iterations: More iterations could improve accuracy but increase computational cost
  - Pseudo-label confidence threshold: Higher thresholds reduce noise but limit data utilization

- Failure signatures:
  - Poor translation quality: Synthetic images look unrealistic or lose anatomical details
  - Overfitting to synthetic data: High performance on synthetic data but poor on real target data
  - Noisy pseudo-labels: Segmentation performance degrades over iterations

- First 3 experiments:
  1. Train translation network on a small subset and visually inspect generated images for anatomical preservation
  2. Test segmentation model on synthetic data only to establish upper bound performance
  3. Run one iteration of pseudo-label generation and evaluate pseudo-label quality metrics (confidence scores, consistency checks)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the method's performance generalize to other anatomical regions beyond abdominal organs, such as cardiac or neurological structures?
- Basis in paper: [inferred] The paper demonstrates strong performance on abdominal organ segmentation but explicitly states that "its generalizability to other datasets and modalities remains to be thoroughly evaluated."
- Why unresolved: The evaluation was limited to the FLARE abdominal organ dataset, and the authors acknowledge the need for broader validation across different anatomical regions and modalities.
- What evidence would resolve it: Comprehensive testing on datasets containing cardiac, neurological, and other anatomical structures using different imaging modalities (CT, MRI, ultrasound, etc.) with quantitative performance metrics.

### Open Question 2
- Question: What is the optimal number of self-training iterations needed to achieve convergence without overfitting to pseudo-labels?
- Basis in paper: [explicit] The paper mentions iterative fine-tuning using pseudo-labels but doesn't specify how many iterations are performed or discuss convergence criteria.
- Why unresolved: The self-training process is described as iterative, but the paper lacks details about the stopping criteria, maximum number of iterations, or analysis of performance changes across iterations.
- What evidence would resolve it: Experimental results showing segmentation performance across different numbers of self-training iterations, with analysis of convergence points and potential overfitting behavior.

### Open Question 3
- Question: How do the disentangled representations learned by the GAN compare to other domain adaptation techniques that don't use explicit disentanglement?
- Basis in paper: [explicit] The authors state that disentangled representation learning "effectively translates images between modalities while preserving anatomical structures" and compare their method to CycleGAN and SIF A, but don't directly compare against other UDA methods that might achieve similar results through different mechanisms.
- Why unresolved: The paper focuses on demonstrating the benefits of their disentangled approach but doesn't benchmark against other state-of-the-art UDA methods (e.g., adversarial domain adaptation, self-ensembling) on the same datasets with identical evaluation protocols.
- What evidence would resolve it: Head-to-head comparison of DRL-STNet against other state-of-the-art UDA methods on the same datasets with identical evaluation protocols.

## Limitations

- Lacks detailed architectural specifications for GAN components, making exact reproduction challenging
- No information on how pseudo-label confidence thresholds are set or how noisy pseudo-labels are handled during self-training
- Limited discussion of failure cases or performance degradation on more challenging anatomical structures
- Restricted evaluation to CT→MRI translation without testing other modality pairs

## Confidence

- High confidence: The overall framework design and its core mechanisms (disentangled representation learning, self-training with pseudo-labels)
- Medium confidence: The specific implementation details and hyperparameter choices
- Low confidence: Generalization to other modality pairs beyond CT→MRI

## Next Checks

1. Implement the translation network on a small dataset and conduct visual inspection of generated images to verify anatomical preservation quality
2. Run ablation studies removing the self-training component to quantify its contribution to overall performance
3. Test the method on an additional modality pair (e.g., MRI→CT or CT→PET) to assess generalization beyond the primary CT→MRI task