---
ver: rpa2
title: An investigation of structures responsible for gender bias in BERT and DistilBERT
arxiv_id: '2401.06495'
source_url: https://arxiv.org/abs/2401.06495
tags:
- bias
- bert
- distilbert
- attention
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates gender bias in BERT and DistilBERT language
  models by examining their internal structures. The authors fine-tune both models
  on balanced and imbalanced datasets, then analyze attention weights and hidden states
  across layers.
---

# An investigation of structures responsible for gender bias in BERT and DistilBERT

## Quick Facts
- arXiv ID: 2401.06495
- Source URL: https://arxiv.org/abs/2401.06495
- Authors: Thibaud Leteno; Antoine Gourru; Charlotte Laclau; Christophe Gravier
- Reference count: 35
- Primary result: Gender bias in PLMs is distributed across multiple layers and attention heads rather than localized to specific components

## Executive Summary
This study investigates gender bias in BERT and DistilBERT by examining their internal structures through attention weights, hidden states, and head ablation experiments. The authors fine-tune both models on balanced and imbalanced datasets derived from the Bias in Bios corpus, then analyze how bias manifests across different model components. They find that bias is not produced by any specific layer or attention head, but rather emerges from the combined behavior of multiple components. Interestingly, DistilBERT shows more homogeneous bias across attention heads compared to BERT, particularly in scenarios with underrepresented classes and high sensitive attribute imbalance, suggesting that model distillation may help mitigate certain types of bias.

## Method Summary
The authors use the Bias in Bios dataset (217,197 entries across 28 occupations with gender labels) to create balanced and imbalanced subsets. They fine-tune both BERT and DistilBERT on these datasets using sequence classification heads from Hugging Face transformers. The study compares attention weights using JS divergence and hidden states using SVCCA distance between models fine-tuned on different dataset versions. Head ablation experiments are performed by setting attention weights to zero for each head individually to identify components contributing to bias. Fairness is measured using Equalized Odds (difference in true positive rates between gender groups), while performance is evaluated using F-scores.

## Key Results
- No specific layer or attention head uniformly produces gender bias; bias arises from combined behavior of multiple components
- All attention heads uniformly encode bias except in cases of underrepresented classes with high sensitive attribute imbalance
- The subset of heads contributing to bias changes upon re-fine tuning
- DistilBERT produces more homogeneous bias across heads compared to BERT, suggesting distillation can mitigate bias in certain imbalanced scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: No single layer or attention head uniformly produces gender bias; instead, bias arises from the combined behavior of multiple components.
- Mechanism: During fine-tuning, all layers and attention heads participate in encoding bias, but the subset of heads responsible for bias changes upon re-fine-tuning. In imbalanced datasets with underrepresented classes, some heads encode more bias than others, leading to variability in bias expression.
- Core assumption: Bias in PLMs is distributed across multiple layers and heads, not localized to a specific component.
- Evidence anchors:
  - [abstract] "No specific layer produces bias; Every attention head uniformly encodes bias, except in cases of underrepresented classes with high imbalance of the sensitive attribute"
  - [section] "first we can note a similar pattern for both BERT and DistilBERT: the divergence increases as we move forward in the architecture, with a peak on the penultimate layer... However, when comparing [Mi-Mi, Mb-Mb] vs. Mi-Mb, we observe that the values for both metrics are slightly above on Mi-Mb, but not significantly"
  - [corpus] Weak: The corpus mentions related works on gender bias in transformer models, but lacks direct evidence for the distributed nature of bias in BERT and DistilBERT.
- Break condition: If future experiments show that a specific layer or head consistently produces more bias than others across multiple fine-tuning runs, this mechanism would be invalidated.

### Mechanism 2
- Claim: Model distillation can mitigate gender bias in scenarios with underrepresented classes and high sensitive attribute imbalance.
- Mechanism: DistilBERT, being a compressed version of BERT, produces more homogeneous bias across attention heads. In cases of underrepresented classes with high sensitive attribute imbalance, DistilBERT exhibits less variability in bias expression compared to BERT, leading to potentially fairer predictions.
- Core assumption: The distillation process, by reducing model complexity, leads to more uniform behavior across attention heads, which can mitigate bias in certain imbalanced scenarios.
- Evidence anchors:
  - [abstract] "DistilBERT produces more homogeneous bias across heads compared to BERT"
  - [section] "BERT is more sensitive to those scenarios than DistilBERT. According to Figure ??, where sensitive groups are balanced within the classes, the less a class is represented the higher the amplitude is, meaning that BERT is generally more sensitive to class imbalance than DistilBERT with regard to the homogeneity of head representations"
  - [corpus] Weak: The corpus mentions that some works show distilled versions of PLMs can exacerbate bias, while others reach an opposite conclusion. However, it lacks direct evidence for the specific claim about DistilBERT's performance in underrepresented class scenarios.
- Break condition: If future experiments demonstrate that DistilBERT consistently produces more bias than BERT across various imbalanced scenarios, this mechanism would be invalidated.

### Mechanism 3
- Claim: Fine-tuning on balanced vs. imbalanced datasets leads to different internal representations and attention patterns, but not necessarily more bias.
- Mechanism: Fine-tuning on imbalanced datasets allows the model to learn a correlation between the target label and sensitive attributes, inducing bias. However, the internal representations and attention patterns of models fine-tuned on balanced vs. imbalanced datasets are not significantly different, as measured by JS divergence and SVCCA distance.
- Core assumption: The difference in fairness between models fine-tuned on balanced and imbalanced datasets is not due to significant differences in internal representations or attention patterns.
- Evidence anchors:
  - [abstract] "This subset of heads is different as we re-fine tune the network"
  - [section] "For the JS divergence, first we can note a similar pattern for both BERT and DistilBERT: the divergence increases as we move forward in the architecture, with a peak on the penultimate layer... Comparing [Mi-Mi, Mb-Mb] vs. Mi-Mb, we observe that the values for both metrics are slightly above on Mi-Mb, but not significantly"
  - [corpus] Weak: The corpus mentions that training with imbalanced data allows the model to learn a correlation between the target label and sensitive attributes, but lacks direct evidence for the specific claim about the similarity of internal representations between balanced and imbalanced fine-tuning.
- Break condition: If future experiments show significant differences in internal representations or attention patterns between models fine-tuned on balanced and imbalanced datasets, this mechanism would be invalidated.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how Transformers work is crucial for interpreting the results of attention weight and hidden state comparisons.
  - Quick check question: What is the role of multi-head attention in Transformer models, and how does it contribute to the model's ability to learn contextual representations?

- Concept: Bias measurement and fairness metrics
  - Why needed here: Evaluating the fairness of language models requires understanding different bias metrics, such as Equalized Odds, and how they are computed.
  - Quick check question: How is the Equalized Odds metric defined, and what does it measure in the context of evaluating the fairness of language models?

- Concept: Model distillation and compression techniques
  - Why needed here: Understanding how model distillation works and its potential impact on model performance and bias is essential for interpreting the results comparing BERT and DistilBERT.
  - Quick check question: What is the principle behind model distillation, and how does it aim to achieve similar performance with fewer parameters compared to the original model?

## Architecture Onboarding

- Component map:
  Input layer -> Transformer encoder (multiple layers with multi-head attention and feed-forward networks) -> Output layer (final hidden states/pooled representations) -> Classification head (linear layer for task-specific predictions)

- Critical path:
  1. Input text is tokenized and mapped to initial representations
  2. Token representations are processed by the Transformer encoder layers
  3. Attention weights are computed between token pairs in each layer
  4. Hidden states are updated based on attention weights and feed-forward networks
  5. Final hidden states or pooled representations are passed to the classification head
  6. Classification head produces predictions for the downstream task

- Design tradeoffs:
  - Model size vs. performance: Larger models like BERT may achieve better performance but are more resource-intensive, while compressed models like DistilBERT are more efficient but may sacrifice some performance
  - Fine-tuning data balance vs. fairness: Fine-tuning on balanced datasets may lead to fairer predictions, but may not reflect real-world data distributions

- Failure signatures:
  - High JS divergence or SVCCA distance between layers may indicate significant changes in internal representations during fine-tuning
  - Large variations in Equalized Odds scores across attention heads may suggest that certain heads are more responsible for bias than others
  - Consistent performance degradation or bias amplification when ablating specific attention heads may indicate their importance in the model's decision-making process

- First 3 experiments:
  1. Compute JS divergence and SVCCA distance between layers of models fine-tuned on balanced and imbalanced datasets to identify any significant differences in internal representations or attention patterns
  2. Ablate each attention head individually and evaluate the model's performance and fairness to identify heads that contribute more to bias
  3. Compare the bias and performance of BERT and DistilBERT on datasets with underrepresented classes and high sensitive attribute imbalance to assess the impact of model distillation on fairness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the distillation process affect intrinsic bias encoded during pre-training, or only extrinsic bias introduced during fine-tuning?
- Basis in paper: [inferred] The authors distinguish between intrinsic bias (encoded during pre-training) and extrinsic bias (introduced during fine-tuning), and observe that DistilBERT shows more homogeneous bias across heads in imbalanced scenarios.
- Why unresolved: The study only examines bias introduced during fine-tuning on a specific dataset. The authors do not investigate whether distillation affects pre-existing intrinsic bias in the base model.
- What evidence would resolve it: Comparative analysis of intrinsic bias in BERT vs. DistilBERT before any fine-tuning, using probing tasks designed to measure pre-existing societal biases.

### Open Question 2
- Question: Are there specific architectural components (e.g., attention heads, layers, or feed-forward networks) that consistently contribute to gender bias across different datasets and tasks?
- Basis in paper: [explicit] The authors find that no specific layer or head uniformly produces bias, and that the subset of bias-contributing heads changes upon re-fine-tuning.
- Why unresolved: The experiments are conducted on a single dataset (Bias in Bio) and a single task (document classification). The authors cannot identify consistent patterns across different fine-tuning scenarios.
- What evidence would resolve it: Systematic ablation studies across multiple diverse datasets and tasks, measuring which components consistently contribute to bias regardless of fine-tuning conditions.

### Open Question 3
- Question: Does the increased homogeneity of bias across attention heads in DistilBERT represent a mitigation strategy or a more uniform propagation of bias?
- Basis in paper: [explicit] The authors observe that bias is more homogeneously produced by heads in DistilBERT compared to BERT, especially in cases of underrepresented classes with high sensitive attribute imbalance.
- Why unresolved: The authors conclude that DistilBERT shows more robustness to double imbalance scenarios, but cannot determine whether this homogeneity represents genuine bias mitigation or simply a more consistent encoding of bias across all heads.
- What evidence would resolve it: Comparative analysis of fairness metrics (e.g., Equalized Odds) across BERT and DistilBERT on datasets with varying degrees of class imbalance and sensitive attribute imbalance, coupled with interpretability studies to determine whether the homogeneous bias represents reduced discrimination or consistent discrimination.

## Limitations
- Analysis is based on a single dataset (Bias in Bios), limiting generalizability to other domains and languages
- Focus on binary gender classification excludes non-binary gender identities
- Findings on distributed bias are correlational rather than causal explanations

## Confidence
- High Confidence: No specific layer uniformly produces bias - well-supported by attention and hidden state analysis across multiple metrics
- Medium Confidence: DistilBERT produces more homogeneous bias across heads compared to BERT - supported but could benefit from additional ablation studies
- Medium Confidence: Bias-contributing heads change upon re-fine-tuning - observed but sample size for re-fine-tuning runs could be expanded

## Next Checks
1. Replicate the attention head ablation experiments across multiple random seeds and dataset splits to verify consistency of bias-contributing heads and test claim about their variability across fine-tuning runs.

2. Extend the comparison between BERT and DistilBERT to include other distilled variants (like TinyBERT, MobileBERT) and larger models (RoBERTa, GPT variants) to understand whether observed patterns are specific to DistilBERT or representative of model compression effects generally.

3. Test findings on additional datasets with different bias types (racial, age-related) and languages to assess whether distributed bias mechanisms and head variability hold across different forms of bias and linguistic contexts.