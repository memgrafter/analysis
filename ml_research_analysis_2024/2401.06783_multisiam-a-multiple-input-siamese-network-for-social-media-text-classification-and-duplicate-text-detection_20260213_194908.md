---
ver: rpa2
title: 'MultiSiam: A Multiple Input Siamese Network For Social Media Text Classification
  And Duplicate Text Detection'
arxiv_id: '2401.06783'
source_url: https://arxiv.org/abs/2401.06783
tags:
- duplicate
- network
- siamese
- multisiam
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of duplicate text detection and
  categorization in social media, where increasingly similar content across platforms
  makes information access difficult. It proposes a MultiSiam model, a multiple-input
  Siamese network that extends the conventional Siamese architecture to handle more
  than two inputs simultaneously, enabling efficient grouping of duplicate texts.
---

# MultiSiam: A Multiple Input Siamese Network For Social Media Text Classification And Duplicate Text Detection

## Quick Facts
- arXiv ID: 2401.06783
- Source URL: https://arxiv.org/abs/2401.06783
- Reference count: 24
- Primary result: MultiSiam achieves ~0.72 accuracy on duplicate detection while supporting multi-input scenarios

## Executive Summary
This paper addresses the challenge of duplicate text detection and categorization in social media, where increasingly similar content across platforms makes information access difficult. It proposes a MultiSiam model, a multiple-input Siamese network that extends the conventional Siamese architecture to handle more than two inputs simultaneously, enabling efficient grouping of duplicate texts. This model is integrated into SMCD, which performs both categorization and duplicate detection. Experiments on custom datasets show that MultiSiam performs comparably to Siamese networks for pairwise duplicate detection while supporting multi-input scenarios, and SMCD effectively categorizes and groups duplicate posts. The approach improves feed organization and has potential applications in social media aggregation and user experience optimization.

## Method Summary
The paper proposes MultiSiam, a multiple-input Siamese network that extends the conventional Siamese architecture to handle more than two inputs simultaneously by reshaping the input tensor from (batch size x group size x text size) to (batch size * group size) x text size, enabling efficient multi-input duplicate detection. The network uses Word Embeddings and LSTM layers to generate text embeddings, then computes triplet loss across all texts within groups, treating same-group texts as positives and different-group texts as negatives. SMCD integrates categorization and duplicate detection by sharing the same text embeddings for both tasks, leveraging the assumption that duplicate texts belong to the same category. The model branches into two paths: one for category prediction using LSTM and Dense layers with Softmax, and one for duplicate detection using triplet loss.

## Key Results
- MultiSiam achieves ~0.72 accuracy on duplicate detection tasks
- The model effectively groups duplicate posts while maintaining categorization accuracy
- MultiSiam performs comparably to pairwise Siamese networks for duplicate detection while supporting multi-input scenarios

## Why This Works (Mechanism)

### Mechanism 1
MultiSiam extends the conventional Siamese network to handle more than two inputs simultaneously by reshaping the input tensor from (batch size x group size x text size) to (batch size * group size) x text size, enabling efficient multi-input duplicate detection. The network takes a batch of groups of texts where each group contains duplicates of each other. It reshapes the input to process all texts through a shared subnetwork, then reshapes the output back to batch size x group size x embedding size for triplet loss computation. Duplicate texts within a group share semantic similarity that can be captured by the same subnetwork parameters, and the group structure can be preserved through tensor reshaping.

### Mechanism 2
The triplet loss formulation in MultiSiam generalizes the pairwise similarity comparison to multi-group scenarios by considering all texts within a group as positives and all texts from other groups as negatives. For each anchor text, the loss calculates cosine similarity with all positive texts (same group) and all negative texts (other groups), then optimizes to minimize anchor-positive distance while maximizing anchor-negative distance. The semantic similarity structure within groups can be captured through cosine similarity in the embedding space, and the margin-based triplet loss effectively separates duplicate from non-duplicate pairs.

### Mechanism 3
SMCD integrates categorization and duplicate detection by sharing the same text embeddings for both tasks, leveraging the assumption that duplicate texts belong to the same category. The model first generates embeddings using Word Embeddings and LSTM layers, then branches into two paths: one for category prediction (LSTM + Dense + Softmax) and one for duplicate detection (LSTM + reshaping for triplet loss). Duplicate texts are likely to belong to the same category, allowing shared embeddings to serve both classification and similarity tasks efficiently.

## Foundational Learning

- Concept: Siamese and triplet network architectures
  - Why needed here: Understanding how Siamese networks work with pairwise inputs and how triplet loss optimizes embedding distances is crucial for extending to MultiSiam's multi-input scenario.
  - Quick check question: How does triplet loss differ from contrastive loss in terms of input requirements and optimization objectives?

- Concept: Text embedding techniques and sequence modeling
  - Why needed here: The subnetwork in MultiSiam uses Word Embeddings and LSTM layers to convert variable-length text into fixed-dimensional embeddings that capture semantic meaning for similarity comparison.
  - Quick check question: Why might LSTM layers be preferred over simple averaging of word embeddings for capturing sequential dependencies in social media text?

- Concept: Multi-task learning and shared representations
  - Why needed here: SMCD demonstrates how a single model can perform two related tasks (categorization and duplicate detection) by sharing intermediate representations, which requires understanding how task-specific losses affect shared parameters.
  - Quick check question: What are the potential benefits and risks of sharing embeddings between categorization and duplicate detection tasks?

## Architecture Onboarding

- Component map: Input layer -> Reshape layer -> Subnetwork (Word Embedding + LSTM) -> Reshape layer -> Triplet loss computation -> (SMCD branching: Category prediction branch with LSTM + Dense + Softmax)

- Critical path: 1. Input reshaping and padding/truncation 2. Embedding generation through shared subnetwork 3. Triplet loss computation and optimization 4. (For SMCD) Category prediction from shared embeddings

- Design tradeoffs: Fixed group size requirement vs. flexibility in handling variable numbers of duplicates, Computational overhead of reshaping operations vs. model simplicity, Shared embeddings for dual tasks vs. task-specific optimization

- Failure signatures: Poor accuracy on duplicate detection suggests issues with embedding quality or triplet loss configuration, Category prediction errors may indicate insufficient task separation in shared embeddings, Slow training could result from inefficient tensor operations during reshaping

- First 3 experiments: 1. Verify basic functionality: Train MultiSiam on a simple pairwise duplicate dataset and compare performance against standard Siamese network 2. Test multi-input capability: Create synthetic data with groups of 3-4 duplicates and evaluate grouping accuracy 3. Validate SMCD integration: Train on custom dataset with both categories and duplicates, check if category accuracy is maintained while learning duplicates

## Open Questions the Paper Calls Out

### Open Question 1
How does the MultiSiam model perform on multi-input scenarios compared to pairwise Siamese networks when the number of duplicates per post increases beyond 4? The paper mentions that MultiSiam was tested on a custom dataset with group sizes of 4, but does not explore performance with larger group sizes or varying numbers of duplicates. The experiments only evaluated MultiSiam with a fixed group size of 4, leaving uncertainty about scalability to larger groups with more duplicates. Experiments testing MultiSiam on datasets with varying group sizes (e.g., 2, 4, 6, 8 duplicates per post) and comparing accuracy, inference time, and memory usage to pairwise Siamese networks would resolve this.

### Open Question 2
What is the impact of different sub-network architectures (e.g., CNN, Transformer, BERT) on MultiSiam's performance for duplicate detection and categorization? The paper states that MultiSiam can use any sub-network based on the application, but only tested LSTM-based sub-networks for text. The paper does not explore how different sub-network choices affect MultiSiam's accuracy, efficiency, or generalization across domains. Comparative experiments using MultiSiam with various sub-networks (CNN, Transformer, BERT) on the same datasets, measuring performance metrics and computational costs would resolve this.

### Open Question 3
How robust is the SMCD model to noisy or adversarial inputs, such as paraphrased duplicates or posts with minimal semantic overlap? The paper does not address robustness to challenging duplicate scenarios like paraphrasing or semantic similarity without lexical overlap, which are common in social media. The experiments used relatively clean datasets, and the paper does not discuss failure modes or adversarial examples. Tests on datasets with paraphrased duplicates, near-duplicates, or adversarial examples, evaluating SMCD's precision, recall, and false positive/negative rates would resolve this.

## Limitations
- The paper lacks detailed experimental results showing performance with varying group sizes beyond the tested group size of 4
- No ablation studies are provided to validate the contribution of each component (shared embeddings, triplet loss configuration, etc.)
- Dataset details are sparse, making it difficult to assess the generalizability of the results to other social media platforms or content types

## Confidence

- High: The conceptual framework of extending Siamese networks to multi-input scenarios is sound and well-explained
- Medium: The effectiveness of SMCD in integrating categorization and duplicate detection tasks
- Low: Quantitative performance comparisons against baseline methods and detailed ablation studies

## Next Checks

1. Implement and test MultiSiam with synthetic datasets containing groups of varying sizes (2-5 duplicates) to evaluate performance degradation as group size increases
2. Conduct ablation studies to measure the impact of shared embeddings on both categorization and duplicate detection accuracy
3. Compare MultiSiam's computational efficiency against processing pairwise comparisons sequentially for the same number of inputs