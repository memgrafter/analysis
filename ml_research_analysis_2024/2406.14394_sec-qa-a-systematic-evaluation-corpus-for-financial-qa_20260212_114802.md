---
ver: rpa2
title: 'SEC-QA: A Systematic Evaluation Corpus for Financial QA'
arxiv_id: '2406.14394'
source_url: https://arxiv.org/abs/2406.14394
tags:
- financial
- documents
- question
- document
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEC-QA, a framework for generating multi-document
  financial question-answering datasets from real-world financial documents. The authors
  design a system that automatically creates complex quantitative questions spanning
  multiple financial reports and utilizes a code-generation approach with document
  selection to improve retrieval accuracy.
---

# SEC-QA: A Systematic Evaluation Corpus for Financial QA

## Quick Facts
- arXiv ID: 2406.14394
- Source URL: https://arxiv.org/abs/2406.14394
- Reference count: 40
- Primary result: Code generation approach with document selection achieves 80% accuracy on multi-document financial questions vs 30% for vanilla RAG

## Executive Summary
This paper introduces SEC-QA, a framework for generating multi-document financial question-answering datasets from real-world financial documents. The authors design a system that automatically creates complex quantitative questions spanning multiple financial reports and utilizes a code-generation approach with document selection to improve retrieval accuracy. Experiments show that their approach significantly outperforms standard RAG methods, achieving 80% accuracy on multi-document questions compared to 30% for vanilla RAG. The dataset and methods enable more realistic evaluation of financial reasoning systems while addressing data leakage issues common in existing benchmarks.

## Method Summary
The SEC-QA framework combines document selection filtering with neural retrieval and code generation to handle complex multi-document financial questions. It processes SEC filings (10-K, 10-Q, 8-K) by parsing them into structured JSON format with pages, tables, and titles. A financial metrics database maps specific metrics to companies, fiscal years, and source documents. The system uses four RAG-based approaches: Vanilla RAG, Multi Query RAG, CodeGen+PageR, and CodeGen+DocS+PageR. The code generation approach decomposes complex questions into atomic sub-tasks using program-of-thought, enabling structured retrieval and computation. Document selection filters the collection based on metadata (company symbol, fiscal year, form type) before retrieval, while page-level retrieval uses neural embeddings to find relevant content.

## Key Results
- CodeGen+DocS+PageR achieves 80% accuracy on multi-document questions compared to 30% for vanilla RAG
- Document selection filtering improves precision by reducing irrelevant page retrieval
- Compound value extraction questions requiring both metric knowledge and reasoning steps show the largest performance gaps
- Code generation models are more responsive to increasing the number of retrieved pages (k)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Code generation decomposes complex multi-hop financial questions into atomic sub-tasks, enabling structured retrieval and computation
- Mechanism: The LLM uses program-of-thought to break down a compound question (e.g., "What is the percentage difference of Company A's revenue compared to Company B?") into sequential sub-tasks: (1) select relevant documents for each company, (2) retrieve pages containing revenue values, (3) extract numeric values, (4) compute the percentage difference
- Core assumption: The LLM can reliably parse the question into atomic retrieval and computation steps that map directly to the available helper functions
- Evidence anchors: [abstract], [section 4.1], [section 4.5]

### Mechanism 2
- Claim: Document selection using metadata (company symbol, fiscal year, form type) drastically reduces irrelevant page retrieval and improves recall
- Mechanism: Before retrieving pages, the system filters the entire document collection to only include documents matching the question's metadata constraints (e.g., only 10-Ks for the specified company and fiscal year), ensuring the LLM processes only relevant pages
- Core assumption: The metadata filtering step is accurate and the remaining document set contains the correct answer
- Evidence anchors: [section 4.1], [section 4.3], [section 5.2]

### Mechanism 3
- Claim: The framework generates realistic multi-document financial questions that expose the limitations of standard RAG systems
- Mechanism: By leveraging a structured database of financial metrics mapped to specific documents, the system creates questions requiring multi-hop reasoning, parallel references, and structural navigation across multiple filings (10-K, 10-Q, 8-K), which vanilla RAG cannot handle due to retrieval bottlenecks and lack of reasoning depth
- Core assumption: The database accurately reflects the content and structure of the source documents, and the generated questions are sufficiently complex to challenge RAG systems
- Evidence anchors: [abstract], [section 3.4], [section 4.4]

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG is the baseline approach being compared against; understanding its strengths and weaknesses in multi-document retrieval is critical to appreciating the improvements offered by the CodeGen approach
  - Quick check question: In a standard RAG pipeline, at which step does the "cascading effect" mentioned in the introduction occur, and why is it problematic for multi-document questions?

- Concept: Program-of-Thought / Code Generation in LLMs
  - Why needed here: The core innovation relies on LLMs generating executable code to decompose and solve complex reasoning tasks, so understanding how this works and its limitations is essential
  - Quick check question: How does the "program-of-thought" approach differ from standard chain-of-thought prompting in terms of task decomposition and execution?

- Concept: Financial Document Structure and Metadata
  - Why needed here: The framework relies heavily on understanding the structure of SEC filings (10-K, 10-Q, 8-K) and their metadata to filter and retrieve relevant documents; without this knowledge, the document selection mechanism cannot function
  - Quick check question: What key metadata fields are used to filter financial documents in this framework, and why are they important for accurate retrieval?

## Architecture Onboarding

- Component map: Document Collection -> Database T -> Question Templates -> LLM (GPT-4) -> Helper Functions (select_document, retrieve_relevant_pages, extract_value) -> Retrieval Backend (vector store with Ada embeddings) -> LLM Answer Generation

- Critical path: Question Template → LLM Code Generation → select_document (filter by metadata) → retrieve_relevant_pages (neural retrieval) → extract_value (numeric extraction) → LLM Answer Generation

- Design tradeoffs:
  - Retrieval granularity: Page-level vs. document-level retrieval; page-level offers more precision but increases retrieval calls
  - Metadata filtering vs. neural retrieval: Metadata filtering improves precision but may miss documents with incomplete metadata
  - Number of retrieval pages (k): Higher k improves recall but increases cost and latency
  - Static templates vs. dynamic generation: Templates ensure consistency but may lack diversity compared to LLM-generated questions

- Failure signatures:
  - Low document recall: Check if metadata filtering is too restrictive or if documents are missing from the collection
  - Low page recall: Check if the neural retriever is retrieving irrelevant pages or missing relevant ones; try increasing k
  - Incorrect value extraction: Check if the extract_value function is failing to parse numbers or units correctly; verify the pages contain the expected format
  - Code generation failures: Check if the LLM is generating syntactically incorrect code or failing to decompose the question properly; try few-shot demonstrations or simpler templates

- First 3 experiments:
  1. Test the CodeGen+DocS+PageR pipeline on a simple single-metric question (e.g., "What is Apple's revenue in 2023?") to verify the basic flow works end-to-end
  2. Test document selection filtering by querying for a metric from a specific company and fiscal year, then verify only relevant documents are returned
  3. Test the neural retrieval step in isolation by providing known relevant pages and checking if retrieve_relevant_pages returns them for a simple query

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SEC-QA benchmarks vary across different financial domains (e.g., banking, technology, manufacturing) and document types?
- Basis in paper: [inferred] The paper focuses on S&P 500 companies but doesn't explore domain-specific variations or how different document types (10-K, 10-Q, 8-K) affect performance
- Why unresolved: The current evaluation uses a limited set of companies and doesn't analyze performance differences across financial sectors or document types
- What evidence would resolve it: Comparative analysis of model performance across different financial sectors and document types using the SEC-QA framework

### Open Question 2
- Question: What is the impact of document structure complexity on retrieval performance in SEC-QA, and how can this be quantified?
- Basis in paper: [explicit] The paper mentions document structure reference as a complexity factor but doesn't provide quantitative analysis of how structure affects retrieval
- Why unresolved: While the framework accounts for document structure, there's no systematic evaluation of how structural complexity influences retrieval accuracy
- What evidence would resolve it: Empirical study measuring retrieval performance across documents with varying structural complexity and formal metrics quantifying structure's impact

### Open Question 3
- Question: How does the SEC-QA framework scale to larger document collections and more diverse financial metrics?
- Basis in paper: [inferred] The current implementation uses 18 companies and 10 metrics, but scalability to broader datasets isn't explored
- Why unresolved: The paper demonstrates the framework's effectiveness but doesn't test its limits or performance degradation with larger datasets
- What evidence would resolve it: Performance analysis of SEC-QA with increasing numbers of companies, metrics, and documents to identify scalability boundaries

## Limitations

- The framework's reliance on structured metadata filtering may not generalize to documents with incomplete or inconsistent metadata
- The database T requires manual curation and may not capture all relevant financial metrics or document relationships
- The code generation approach introduces additional operational costs and potential failure points through LLM dependencies

## Confidence

- High confidence: The core mechanisms of document selection filtering and page-level retrieval demonstrate consistent improvements across multiple evaluation metrics
- Medium confidence: The generalization of results to different financial domains or document types beyond SEC filings
- Medium confidence: The scalability of the framework to larger document collections and more diverse question types

## Next Checks

1. Test document selection filtering robustness by introducing documents with missing or incorrect metadata to assess failure modes
2. Evaluate the framework's performance on a held-out set of companies not included in the original training/validation data
3. Benchmark operational costs (LLM calls, latency) against accuracy improvements to determine practical deployment thresholds