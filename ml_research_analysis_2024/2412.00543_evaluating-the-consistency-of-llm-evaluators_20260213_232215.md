---
ver: rpa2
title: Evaluating the Consistency of LLM Evaluators
arxiv_id: '2412.00543'
source_url: https://arxiv.org/abs/2412.00543
tags:
- consistency
- evaluators
- evaluation
- interval
- scale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper examines the consistency of large language models (LLMs)
  as evaluators across different scoring scales and criterion definitions. Two aspects
  of consistency are studied: Self-Consistency (SC), which measures the consistency
  of repeated evaluations under the same conditions, and Inter-scale Consistency (IC),
  which measures the consistency of evaluations across different scoring scales.'
---

# Evaluating the Consistency of Large Language Models as Evaluators

## Quick Facts
- **arXiv ID**: 2412.00543
- **Source URL**: https://arxiv.org/abs/2412.00543
- **Reference count**: 9
- **Primary result**: Mistral-Instruct achieves highest consistency across most settings; criterion granularity and scale type significantly affect LLM evaluator consistency

## Executive Summary
This paper examines the consistency of large language models (LLMs) as evaluators across different scoring scales and criterion definitions. The study evaluates five state-of-the-art instruction-following models on four criteria (harmlessness, helpfulness, factuality, and conciseness) using varying levels of criterion granularity and four different scoring scales. Two aspects of consistency are measured: Self-Consistency (consistency of repeated evaluations under the same conditions) and Inter-scale Consistency (consistency across different scoring scales). The results show that Mistral-Instruct generally achieves the highest consistency across most settings, while the effect of criterion granularity is generally positive. Notably, models with strong performance on benchmarks like MT-Bench do not necessarily exhibit high consistency, highlighting the importance of considering consistency as an independent evaluation criterion for LLM evaluators.

## Method Summary
The study evaluates five 7B parameter instruction-following models (Llama2-7B, Mistral-7B, Tulu-2-DPO-7B, Zephyr-7B, GPT-3.5) on four criteria using 1,000 instances per criterion sampled from relevant datasets. Each evaluation is performed with temperature=1.0 to generate 5 samples for Self-Consistency measurements. Two consistency metrics are calculated using Krippendorff's alpha: Self-Consistency (SC) measures consistency of repeated evaluations under the same conditions, while Inter-scale Consistency (IC) measures consistency across different scoring scales (5-point interval, 10-point interval, 5-point Likert, binary). Criterion definitions are varied across three granularity levels (no definition, short phrase, detailed paragraph).

## Key Results
- Mistral-Instruct generally achieves the highest consistency across most settings
- Criterion granularity shows positive effects on consistency, with more detailed definitions improving consistency scores
- 5-point Likert and binary scales tend to reduce Self-Consistency compared to interval scales
- Inter-scale Consistency is higher between similar scale types (e.g., 5-point vs. 10-point intervals) than between dissimilar types (e.g., binary vs. interval)
- Models with strong MT-Bench performance do not necessarily exhibit high consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Consistency improves when evaluation criteria are more precisely defined
- Mechanism: Detailed criterion definitions reduce ambiguity in the evaluator's scoring decision process, leading to more repeatable responses under sampling
- Core assumption: LLM evaluators rely on clear task instructions to produce consistent outputs
- Evidence anchors:
  - [section] "Generally, there is an upward slope with increasing granularity, suggesting a more detailed definition of an evaluation can assist in more consistent evaluations."
  - [corpus] Weak - related papers focus on factual consistency and meta-evaluation but don't directly address criterion granularity effects
- Break condition: If the evaluator already has strong pre-existing knowledge of the criterion, additional detail may not improve consistency

### Mechanism 2
- Claim: Different scoring scales produce different consistency patterns due to their inherent measurement properties
- Mechanism: Interval scales provide continuous ranges that allow nuanced scoring, while ordinal and binary scales force discrete categorization that can introduce variance
- Core assumption: The mathematical structure of a scoring scale affects how consistently an LLM can map qualitative judgments to quantitative values
- Evidence anchors:
  - [section] "The 5-P Likert scale and the binary scale had a negative impact on Self-Consistency with the exception of Mistral-Instruct."
  - [corpus] Weak - related papers mention evaluation scale choices but don't analyze consistency across scale types
- Break condition: If the LLM has been specifically trained or fine-tuned to use certain scales, the relationship may reverse

### Mechanism 3
- Claim: Sampling temperature affects consistency by controlling the randomness in output generation
- Mechanism: Higher temperature values increase diversity in sampled responses, while lower values make outputs more deterministic and repeatable
- Core assumption: LLM outputs follow a probability distribution where temperature controls the entropy of sampling
- Evidence anchors:
  - [section] "Llama (7B) 0.1 0.951 0.997 0.854 0.957" vs "Llama (7B) 1.0 0.683 0.801 0.366 0.532" - shows consistency decreasing with temperature
  - [corpus] Missing - ablation study on temperature effects not found in related papers
- Break condition: If the temperature is too low (near 0), the model may output identical responses regardless of input variation, masking true consistency issues

## Foundational Learning

- Concept: Krippendorff's alpha as a consistency metric
  - Why needed here: Provides a flexible measure for consistency that works across different data types (interval, ordinal, nominal)
  - Quick check question: Can Krippendorff's alpha handle binary agreement scores as well as continuous rating scales?

- Concept: Prompt sensitivity in LLMs
  - Why needed here: Explains why different scoring scales and criterion definitions produce varying consistency results
  - Quick check question: How does minor wording changes in evaluation instructions affect consistency scores?

- Concept: Zero-shot evaluation capability
  - Why needed here: Most LLM evaluators are used without task-specific fine-tuning, making their consistency properties critical
  - Quick check question: What happens to consistency when an LLM is fine-tuned specifically for evaluation tasks?

## Architecture Onboarding

- Component map: LLM evaluator → scoring scale mapping → consistency calculation → correlation analysis with GPT-4
- Critical path: Dataset selection → criterion definition → LLM evaluation → consistency scoring → result analysis
- Design tradeoffs: Using open-source models enables reproducibility but may miss proprietary model behaviors; sampling introduces randomness that must be controlled
- Failure signatures: High correlation with GPT-4 but low self-consistency indicates memorization rather than genuine evaluation capability
- First 3 experiments:
  1. Replicate self-consistency tests with different sampling temperatures to verify temperature effects
  2. Test additional scoring scales (e.g., 7-point Likert) to understand the full spectrum of scale effects
  3. Compare consistency across different model sizes of the same architecture to isolate model capacity effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the consistency of LLM evaluators change when evaluated across multiple criteria simultaneously versus individual criteria?
- Basis in paper: [inferred] The paper examines consistency across four separate criteria (harmlessness, helpfulness, factuality, conciseness) but does not explore how these might interact or combine in multi-criteria evaluations.
- Why unresolved: The experimental design focuses on individual criteria in isolation, leaving open whether models maintain consistent performance when required to balance multiple evaluation dimensions at once.
- What evidence would resolve it: Experiments measuring consistency scores when evaluators must assess responses against combinations of criteria simultaneously, comparing these to single-criterion results.

### Open Question 2
- Question: Does fine-tuning LLM evaluators on consistency data improve their consistency scores without sacrificing their alignment with human judgments?
- Basis in paper: [explicit] The paper mentions Kim et al., 2024a fine-tunes a language model to specialize in evaluation, but does not specifically address whether fine-tuning can improve consistency metrics while maintaining human alignment.
- Why unresolved: While the paper demonstrates that consistency is an important metric separate from alignment, it does not explore whether consistency can be improved through training while preserving evaluation quality.
- What evidence would resolve it: Experiments showing consistency improvements on fine-tuned models versus baseline models, alongside continued strong alignment with human judgments on standard benchmarks.

### Open Question 3
- Question: How do different decoding strategies (e.g., beam search, nucleus sampling) affect the consistency of LLM evaluators compared to the temperature-based sampling used in this study?
- Basis in paper: [explicit] The paper uses temperature-based sampling (temperature of 1.0) for Self-Consistency measurements and mentions that temperature affects consistency, but does not compare other decoding strategies.
- Why unresolved: The study establishes that sampling introduces variability in evaluations, but leaves open whether alternative decoding methods might provide better consistency or whether the optimal decoding strategy varies by evaluation task.
- What evidence would resolve it: Systematic comparison of consistency scores across different decoding strategies (beam search, top-k, nucleus sampling, temperature sampling) for the same evaluation tasks and models.

## Limitations
- Temperature sensitivity of consistency measures may not be linear across all model architectures
- Analysis limited to 7B parameter models, potentially missing consistency patterns in larger models
- Does not explore impact of model-specific training objectives on consistency
- Does not examine multi-criteria evaluation scenarios where models must balance multiple dimensions simultaneously

## Confidence
**High Confidence**: Detailed criterion definitions generally improve self-consistency; Mistral-Instruct achieves highest consistency across settings
**Medium Confidence**: Inter-scale consistency patterns across scale types; negative impact of Likert/binary scales on self-consistency
**Low Confidence**: Relationship between benchmark performance and consistency requires further validation

## Next Checks
1. **Temperature Sensitivity Validation**: Run consistency tests across a wider temperature range (0.1 to 2.0) for each model to establish the precise temperature-consistency relationship
2. **Model Architecture Scaling**: Extend the consistency analysis to 13B and 70B parameter versions to determine whether consistency improvements scale with model size
3. **Domain-Specific Consistency**: Evaluate consistency specifically on domain-expert tasks (legal, medical, technical) to determine if observed patterns hold with specialized knowledge