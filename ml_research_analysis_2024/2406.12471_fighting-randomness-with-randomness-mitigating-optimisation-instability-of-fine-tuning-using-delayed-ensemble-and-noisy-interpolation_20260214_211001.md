---
ver: rpa2
title: 'Fighting Randomness with Randomness: Mitigating Optimisation Instability of
  Fine-Tuning using Delayed Ensemble and Noisy Interpolation'
arxiv_id: '2406.12471'
source_url: https://arxiv.org/abs/2406.12471
tags:
- augment
- mitigation
- fine-tuning
- performance
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of fine-tuning instability in pre-trained
  language models caused by randomness in initialization, data shuffling, and model
  operations. The authors propose Delayed Ensemble with Noisy Interpolation (DENI),
  a novel method that combines ensembling, model interpolation, and noise regularization
  to mitigate this instability while maintaining computational efficiency.
---

# Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation

## Quick Facts
- **arXiv ID**: 2406.12471
- **Source URL**: https://arxiv.org/abs/2406.12471
- **Reference count**: 40
- **Primary result**: DENI outperforms best baseline (Ensemble) with 37% less computational cost while reducing variance and increasing performance

## Executive Summary
This paper addresses the persistent problem of fine-tuning instability in pre-trained language models, which arises from randomness in initialization, data shuffling, and model operations. The authors propose Delayed Ensemble with Noisy Interpolation (DENI), a novel method that combines ensembling, model interpolation, and noise regularization to mitigate instability while maintaining computational efficiency. DENI creates an ensemble by perturbing a single trained model with noise and performs repeated noisy interpolation during training. Experimental results show that DENI outperforms the best baseline (Ensemble) with only 37% of its computational cost, reduces variance while increasing performance, and is particularly effective for parameter-efficient fine-tuning methods. When combined with data augmentation, DENI achieves even greater stability improvements.

## Method Summary
DENI is a method that addresses fine-tuning instability by combining three techniques: delayed ensemble creation, noisy interpolation, and noise regularization. The method first trains a base model, then creates an ensemble by adding Gaussian noise to the model parameters. During training, it performs repeated noisy interpolation by creating multiple noisy versions of the model, training them briefly, and averaging their parameters. This process is repeated multiple times, creating a model that has been regularized by averaging across different parameter states. The delayed ensemble creation captures the benefits of model diversity while reducing computational cost compared to traditional ensembling, and the noise regularization creates a smoother loss landscape that reduces sensitivity to random initialization and data shuffling.

## Key Results
- DENI outperforms the best baseline (Ensemble) with only 37% of its computational cost
- The method reduces variance while increasing performance across 7 text classification datasets
- DENI is particularly effective for parameter-efficient fine-tuning methods like LoRA and IA3
- When combined with data augmentation, DENI achieves even greater stability improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Noise regularization during training creates a smoother loss landscape, reducing sensitivity to random initialization and data shuffling.
- **Mechanism**: By periodically adding noise to model parameters during training, the method encourages exploration of nearby parameter regions. This smoothing effect makes the final model less sensitive to initial parameter values and the specific order of training samples.
- **Core assumption**: Gaussian noise added to parameters during training improves generalization and reduces instability without significantly harming convergence.
- **Evidence anchors**: 
  - [abstract] "leverages the strengths of ensembling, noise regularisation and model interpolation"
  - [section] "adding noise to the model parameters ... perform well, improving generalisability and overall performance, but not necessarily reducing the instability"
- **Break condition**: If noise variance is too high or applied too early in training, the model may fail to converge or converge to poor local minima.

### Mechanism 2
- **Claim**: Delayed ensemble creation from a single trained model captures the benefits of model diversity while reducing computational cost.
- **Mechanism**: Instead of training multiple independent models, the method trains one model, then creates an ensemble by adding noise to its parameters. This ensemble captures diverse predictions while only requiring a single model's training cost.
- **Core assumption**: The parameter space near a well-trained model contains useful alternative models that can improve ensemble performance.
- **Evidence anchors**:
  - [abstract] "The DENI method leverages the benefits of ensembling while reducing its computation costs"
  - [section] "independently training multiple models ... represents an effective regularisation technique ... However, training multiple models ... introduces a significant increase in computation costs"
- **Break condition**: If the noise added is too small, the ensemble models may be too similar to provide meaningful diversity benefits.

### Mechanism 3
- **Claim**: Repeated noisy interpolation creates more robust models by averaging across multiple noisy parameter states.
- **Mechanism**: The method creates multiple noisy versions of the model, trains them briefly, then averages their parameters. This process is repeated multiple times, creating a model that has been regularized by averaging across different parameter states.
- **Core assumption**: Linear interpolation of model parameters in weight space leads to better performance than any individual model used for aggregation.
- **Evidence anchors**:
  - [abstract] "leverages the strengths of ... model interpolation"
  - [section] "Gueta et al. (2023) show that linear interpolation of multiple models in the weight space leads to a better performing model than any of the individual ones used for its aggregation"
- **Break condition**: If the interpolation is done too early in training when the model parameters are far from optimal, the averaging may hurt performance rather than help it.

## Foundational Learning

- **Concept**: Ensemble methods and their computational cost
  - **Why needed here**: Understanding why traditional ensembling is expensive and how this method reduces that cost is crucial for grasping the efficiency gains
  - **Quick check question**: If traditional ensemble requires training 10 models, how many models does DENI train to achieve similar performance?

- **Concept**: Noise injection in neural networks
  - **Why needed here**: The method relies on understanding how noise affects training dynamics and model generalization
  - **Quick check question**: What is the primary purpose of adding noise to model parameters during training?

- **Concept**: Model interpolation and weight averaging
  - **Why needed here**: The method uses interpolation between noisy models, so understanding how this works is essential
  - **Quick check question**: What is the key difference between stochastic weight averaging and the interpolation used in DENI?

## Architecture Onboarding

- **Component map**: DENI consists of three main components - delayed ensemble creation, noisy interpolation, and noise regularization. The delayed ensemble creates diversity late in training, noisy interpolation averages across parameter states, and noise regularization smooths the loss landscape.

- **Critical path**: The most critical sequence is: train base model → apply noisy interpolation multiple times → create final ensemble. Each step builds on the previous one, and skipping noisy interpolation significantly reduces effectiveness.

- **Design tradeoffs**: The method trades off some additional computation (for noise injection and interpolation) against the high cost of traditional ensembling. The key design decision is when to start noise injection and how often to perform interpolation.

- **Failure signatures**: Common failure modes include: noise variance too high causing training instability, interpolation done too early when parameters are far from optimal, or insufficient noise leading to models that are too similar to provide diversity benefits.

- **First 3 experiments**:
  1. Run the method with only noise regularization (no interpolation or ensemble) to establish baseline effectiveness
  2. Add noisy interpolation while keeping ensemble creation for later to measure incremental benefit
  3. Compare performance with traditional ensemble to verify computational efficiency claims

## Open Questions the Paper Calls Out

The paper acknowledges that while DENI shows promising results, there are several open questions regarding its effectiveness across different model sizes, task types, and the theoretical understanding of why it works particularly well with parameter-efficient fine-tuning methods.

## Limitations

- The method requires careful hyperparameter tuning, particularly for noise variance and timing parameters, which may limit practical adoption
- The computational overhead of the interpolation steps, while less than full ensemble training, still adds complexity to the fine-tuning pipeline
- The experiments focus primarily on classification tasks with limited sample sizes, leaving open questions about performance on larger datasets or different task types

## Confidence

- **High confidence**: The core mechanism of using delayed ensemble with noisy interpolation is well-supported by ablation studies showing that removing any component significantly reduces performance.
- **Medium confidence**: The computational efficiency claims are supported by runtime measurements, though the absolute time savings may vary across hardware configurations.
- **Medium confidence**: The stability improvements are demonstrated across multiple datasets, but the generalization to other task types (e.g., generation tasks) remains untested.

## Next Checks

1. Test DENI on a larger dataset (e.g., full GLUE benchmark) to verify scalability and measure absolute performance gains beyond variance reduction
2. Conduct a hyperparameter sensitivity analysis to determine optimal settings across different model sizes and task types
3. Compare DENI against recently proposed fine-tuning stabilization techniques that emerged after the initial experiments were conducted