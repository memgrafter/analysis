---
ver: rpa2
title: 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset'
arxiv_id: '2412.02788'
source_url: https://arxiv.org/abs/2412.02788
tags:
- dblp
- question
- text
- data
- scholarly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hybrid-SQuAD, a novel large-scale QA dataset
  designed for scholarly question answering requiring integration of information from
  heterogeneous sources, including Knowledge Graphs (DBLP, SemOpenAlex) and text (Wikipedia).
  The dataset contains 10.5K question-answer pairs generated using a large language
  model.
---

# Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset

## Quick Facts
- arXiv ID: 2412.02788
- Source URL: https://arxiv.org/abs/2412.02788
- Reference count: 11
- Key outcome: Introduces Hybrid-SQuAD dataset requiring integration of Knowledge Graphs and Wikipedia text for scholarly QA

## Executive Summary
This paper introduces Hybrid-SQuAD, a novel large-scale QA dataset designed for scholarly question answering requiring integration of information from heterogeneous sources, including Knowledge Graphs (DBLP, SemOpenAlex) and text (Wikipedia). The dataset contains 10.5K question-answer pairs generated using a large language model. A RAG-based baseline model is proposed, achieving 69.65% exact match score on the test set. The dataset addresses the challenge of answering questions that require traversing multiple data sources, which existing QA methods struggle with.

## Method Summary
Hybrid-SQuAD contains 10.5K question-answer pairs generated using a large language model, aligned with DBLP and SemOpenAlex KG sub-graphs and Wikipedia text. The dataset requires multi-hop reasoning across heterogeneous sources including KG-KG, KG-Text, and KG-KG-Text pathways. The baseline RAG-based hybrid QA model uses three phases: link (identifying bridging entities), retrieve (searching relevant data from text sources and KGs), and generate (fusing heterogeneous inputs using an LLM). The dataset focuses on scholarly information spanning multiple data sources, necessitating the development of QA systems that integrate information from diverse knowledge bases.

## Key Results
- RAG-based baseline model achieves 69.65% exact match score on test set
- Zero-shot approaches using ChatGPT-3.5 and LLAMA-3-8B achieve only 2.6% and 1.3% accuracy respectively
- Dataset contains 10.5K question-answer pairs requiring multi-source reasoning
- Questions categorized by evidence traversal paths: KG->KG, KG->Text, KG->KG->Text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid-SQuAD's difficulty stems from requiring integration of structured KG data with unstructured Wikipedia text, unlike existing scholarly QA datasets that focus on single-source questions.
- Mechanism: The dataset forces models to traverse heterogeneous sources (KG-KG, KG-Text, KG-KG-Text) to answer questions, creating multi-hop reasoning paths that existing models struggle with.
- Core assumption: Questions requiring evidence from multiple sources are inherently more difficult than single-source questions.

### Mechanism 2
- Claim: RAG-based models outperform zero-shot LLMs because they can retrieve relevant context from heterogeneous sources before generating answers.
- Mechanism: The RAG approach combines document retrieval with LLM generation, allowing the model to access specific KG triples and Wikipedia text chunks that contain the answer.
- Core assumption: Retrieval of relevant context significantly improves answer accuracy compared to relying on pre-trained knowledge alone.

### Mechanism 3
- Claim: The dataset's difficulty is evidenced by the stark performance gap between specialized baseline models (69.65% EM) and zero-shot approaches (2.6% EM for ChatGPT-3.5).
- Mechanism: The 27x performance difference demonstrates that generic LLMs lack the domain-specific knowledge and retrieval mechanisms needed for scholarly hybrid QA.
- Core assumption: Performance on this dataset requires both domain-specific knowledge and multi-source reasoning capabilities.

## Foundational Learning

- Concept: Knowledge Graph querying and SPARQL
  - Why needed here: The dataset uses DBLP and SemOpenAlex KGs, requiring understanding of RDF triples and SPARQL queries to extract relevant information
  - Quick check question: What is the difference between subject-predicate-object triples and the relationships they represent in scholarly knowledge graphs?

- Concept: Retrieval-Augmented Generation (RAG) architecture
  - Why needed here: The baseline model uses RAG to combine document retrieval with LLM generation for answering questions
  - Quick check question: How does RAG differ from standard LLM prompting in terms of handling out-of-distribution knowledge?

- Concept: Scholarly metadata and bibliometric metrics
  - Why needed here: The dataset includes questions about h-index, citation counts, and other scholarly metrics that require understanding of academic publishing
  - Quick check question: What is the relationship between h-index and i10-index in measuring academic impact?

## Architecture Onboarding

- Component map: Entity linker -> Retriever (KG query + text search) -> RAG generator -> Answer output
- Critical path: Question -> Sub-question phrase identification -> Entity resolution -> Context retrieval -> Answer generation
- Design tradeoffs: Single-source vs. multi-source reasoning, retrieval quality vs. latency, model complexity vs. accuracy
- Failure signatures: Missing bridging entities, failed context retrieval, answer generation based on incomplete information
- First 3 experiments:
  1. Test entity linking on questions with clear scholarly entities (publications, authors)
  2. Verify retrieval returns relevant KG triples and Wikipedia text chunks for simple questions
  3. Evaluate RAG generation quality with gold context vs. retrieved context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Hybrid-SQuAD compare to other multi-source QA datasets when evaluated with the same metrics?
- Basis in paper: Explicit - The paper mentions CompMix as a non-scholarly heterogeneous dataset but doesn't provide direct performance comparison using identical evaluation metrics
- Why unresolved: The paper only mentions CompMix in passing and doesn't provide a direct comparison of exact match scores or other metrics using the same evaluation setup
- What evidence would resolve it: A direct comparison of Hybrid-SQuAD performance against CompMix using identical evaluation metrics and test splits

### Open Question 2
- Question: What is the impact of different text chunk sizes on the RAG model's performance in the baseline system?
- Basis in paper: Inferred - The paper mentions using 200-word chunks with 10-word overlap for the RAG model but doesn't explore the effect of different chunk sizes
- Why unresolved: The paper uses a specific chunk size configuration but doesn't provide ablation studies or analysis of how different chunk sizes affect performance
- What evidence would resolve it: Systematic evaluation of the RAG model's performance with varying chunk sizes and overlap parameters

### Open Question 3
- Question: How does the performance of the baseline model vary across different types of question-answering pathways (KG->KG, KG->Text, KG->KG->Text)?
- Basis in paper: Explicit - The paper identifies four different evidence traversal paths in the dataset but only reports overall performance metrics
- Why unresolved: While the paper categorizes questions by traversal path, it doesn't provide performance breakdown by these categories
- What evidence would resolve it: Performance metrics (EM score, F-score) broken down by each of the four question traversal pathways

### Open Question 4
- Question: What is the effect of using different large language models (LLMs) as generators in the RAG-based baseline?
- Basis in paper: Explicit - The paper mentions using both ChatGPT-3.5 and LLAMA-3-8B as generators but only provides overall performance metrics
- Why unresolved: The paper reports performance for both LLMs but doesn't analyze or compare their strengths and weaknesses in handling different question types or evidence pathways
- What evidence would resolve it: Detailed analysis of each LLM's performance across different question categories and evidence traversal paths

## Limitations

- Dataset consists entirely of synthetically generated questions, lacking real user queries and human evaluation
- Performance metrics rely on exact match scoring which may not capture semantic equivalence in scholarly contexts
- Limited analysis of how performance varies across different question types and evidence traversal paths
- Unknown whether the 69.65% baseline performance generalizes to real-world scholarly QA scenarios

## Confidence

- **High confidence**: The dataset construction methodology and baseline RAG implementation are clearly specified and reproducible
- **Medium confidence**: The reported performance metrics are likely accurate but may be inflated due to synthetic data characteristics
- **Low confidence**: The real-world applicability of the dataset for actual scholarly QA systems remains uncertain without human evaluation

## Next Checks

1. Conduct human evaluation on a random sample of 100 questions to assess whether the synthetic questions reflect genuine scholarly information needs and whether generated answers are semantically correct
2. Test the RAG baseline on a manually curated subset of questions with gold-standard answers to verify that the 69.65% EM score holds for human-authored queries
3. Evaluate model performance across the three question types (KG-KG, KG-Text, KG-KG-Text) to identify whether certain hybrid combinations are disproportionately difficult or if performance is uniform across all multi-source scenarios