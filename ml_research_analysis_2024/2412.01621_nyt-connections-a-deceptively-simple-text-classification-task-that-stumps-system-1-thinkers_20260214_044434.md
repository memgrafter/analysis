---
ver: rpa2
title: 'NYT-Connections: A Deceptively Simple Text Classification Task that Stumps
  System-1 Thinkers'
arxiv_id: '2412.01621'
source_url: https://arxiv.org/abs/2412.01621
tags:
- hints
- llms
- reasoning
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NYT-Connections, a benchmark designed to
  assess deliberate reasoning in large language models (LLMs) by using word classification
  puzzles from the New York Times Connections game. The task is specifically designed
  to penalize quick, intuitive "System 1" thinking and requires deliberate reasoning
  to solve correctly.
---

# NYT-Connections: A Deceptively Simple Text Classification Task that Stumps System-1 Thinkers

## Quick Facts
- arXiv ID: 2412.01621
- Source URL: https://arxiv.org/abs/2412.01621
- Authors: Angel Yahir Loredo Lopez; Tyler McDonald; Ali Emami
- Reference count: 10
- Primary result: NYT-Connections benchmark isolates deliberate reasoning by penalizing quick, intuitive thinking in word classification puzzles

## Executive Summary
This paper introduces NYT-Connections, a benchmark designed to assess deliberate reasoning in large language models (LLMs) by using word classification puzzles from the New York Times Connections game. The task is specifically designed to penalize quick, intuitive "System 1" thinking and requires deliberate reasoning to solve correctly. The benchmark includes 358 puzzles with difficulty ratings and is regularly updated to prevent data leakage. Six recent LLMs, a simple heuristic, and human participants were evaluated across three configurations: single attempt, multiple attempts without hints, and multiple attempts with contextual hints. Results showed a significant performance gap, with even the best-performing LLMs falling short of human performance by nearly 30%. The study found that advanced prompting techniques like Chain-of-Thought and Self-Consistency show diminishing returns as task difficulty increases. A simple heuristic baseline performed comparably to some LLMs, suggesting current models fall between System 1-like pattern-matching and System 2-like deliberation. The benchmark offers a novel tool for assessing LLM reasoning capabilities and isolating fundamental reasoning skills.

## Method Summary
The NYT-Connections benchmark evaluates LLMs on word classification puzzles where 16 terms must be grouped into 4 sets of 4 closely related words. The evaluation uses 358 puzzles from NYT Connections (June 2023 - June 2024) with difficulty ratings 1-5. Three prompting methods are tested: Input-Output, Chain-of-Thought, and CoT with Self-Consistency. Models are evaluated across three configurations: One Try (binary score), No Hints (0-100% based on correct groups), and Full Hints (0-100% with "one away" hints). A simple k-means clustering heuristic baseline is also evaluated to represent intuitive pattern matching.

## Key Results
- Even the best-performing LLMs fall short of human performance by nearly 30%
- Chain-of-Thought and Self-Consistency prompting show diminishing returns as task difficulty increases
- A simple heuristic baseline performs comparably to some LLMs, suggesting current models rely heavily on System 1-like approaches
- No single LLM consistently outperforms others across all difficulty levels and prompting methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NYT-Connections penalizes quick, intuitive "System 1" thinking by design
- Mechanism: The puzzles include misleading word groupings that tempt obvious but incorrect associations, forcing deliberate reasoning to identify the correct categories
- Core assumption: Models relying on pattern matching and statistical regularities will be misled by the puzzle design
- Evidence anchors:
  - [abstract]: "This benchmark is designed to penalize quick, intuitive 'System 1' thinking, isolating fundamental reasoning skills"
  - [section]: "This misdirection is crucial: the apparent 'Status Quo' grouping leaves only three 'Skin Types' terms, making it impossible to form four complete groups"
  - [corpus]: Weak - corpus neighbors discuss System 1/2 thinking but don't directly address puzzle design mechanisms
- Break condition: If models develop sophisticated strategies to recognize and avoid common misdirection patterns, or if puzzle design becomes predictable

### Mechanism 2
- Claim: The heuristic baseline demonstrates the gap between intuitive pattern matching and deliberate reasoning
- Mechanism: A simple k-means clustering heuristic performs comparably to some LLMs, showing that current models rely heavily on System 1-like approaches rather than true reasoning
- Core assumption: If models were truly reasoning, they would significantly outperform a simple heuristic baseline
- Evidence anchors:
  - [abstract]: "A simple heuristic baseline performed comparably to some LLMs, suggesting current models fall between System 1-like pattern-matching and System 2-like deliberation"
  - [section]: "As shown in Table 2, our baseline heuristic, designed to mimic intuitive System 1 thinking, achieves 13.25% accuracy in both No Hints and Full Hints configurations"
  - [corpus]: Weak - corpus doesn't discuss heuristic baselines or their relationship to System 1 thinking
- Break condition: If future models significantly outperform the heuristic baseline across all difficulty levels, indicating genuine reasoning capabilities

### Mechanism 3
- Claim: The benchmark's evolving nature prevents data leakage and maintains task difficulty
- Mechanism: Daily updates to the puzzle collection ensure fresh instances that models cannot memorize, preserving the need for genuine reasoning
- Core assumption: Data leakage would artificially inflate performance metrics and mask true reasoning limitations
- Evidence anchors:
  - [abstract]: "regular updates to mitigate data leakage, offering a novel tool for assessing LLM reasoning capabilities"
  - [section]: "With daily updates, it provides a stream of novel instances, mitigating data leakage concerns"
  - [corpus]: Weak - corpus mentions data leakage but doesn't discuss specific mitigation strategies through puzzle updates
- Break condition: If the update frequency decreases or if models develop general strategies that work across all puzzle variants regardless of novelty

## Foundational Learning

- Concept: System 1 vs System 2 thinking distinction
  - Why needed here: Understanding the benchmark's core premise of isolating deliberate reasoning from intuitive responses
  - Quick check question: Can you explain the key differences between System 1 and System 2 thinking in the context of language model evaluation?

- Concept: Chain-of-Thought prompting and Self-Consistency techniques
  - Why needed here: These are the advanced prompting methods evaluated and found to show diminishing returns
  - Quick check question: How do Chain-of-Thought and Self-Consistency prompting methods work, and why might they fail on System 2-focused tasks?

- Concept: Statistical hypothesis testing for categorical data
  - Why needed here: The paper uses different statistical tests (two-proportion z-test and Mann-Whitney U test) for different experimental setups
  - Quick check question: When would you use a two-proportion z-test versus a Mann-Whitney U test for comparing model performance?

## Architecture Onboarding

- Component map: Data pipeline -> Heuristic engine -> LLM interface -> Statistical analysis -> Update mechanism
- Critical path: Model evaluation → Statistical analysis → Result validation → Benchmark update
- Design tradeoffs:
  - Puzzle difficulty balance: Too easy → System 1 solutions dominate; too hard → No model can solve
  - Update frequency: More frequent → Better data leakage prevention but higher maintenance
  - Evaluation configurations: More configurations → Better insights but increased complexity
- Failure signatures:
  - If LLM performance approaches human performance without significant reasoning advances
  - If heuristic baseline performance diverges significantly from LLM performance (indicating poor task design)
  - If performance gaps disappear as puzzles become predictable
- First 3 experiments:
  1. Run the heuristic baseline on a small puzzle set to validate implementation
  2. Test a single LLM with all three prompting methods on the median difficulty puzzles
  3. Compare statistical analysis outputs between One Try and No Hints configurations to verify correct test selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would larger embedding models affect the performance of the heuristic baseline on NYT-Connections?
- Basis in paper: [explicit] The authors note that their heuristic uses a relatively small model due to hardware constraints and suggest future work should explore the scalability using more advanced embedding models.
- Why unresolved: The current study only tested with Multilingual-E5-Large-Instruct embeddings due to hardware limitations, leaving open the question of whether larger models would significantly improve heuristic performance.
- What evidence would resolve it: Testing the same heuristic approach with larger embedding models like GPT-4 embeddings or other state-of-the-art models, and comparing the results to both the current heuristic baseline and LLM performance.

### Open Question 2
- Question: What is the optimal prompting strategy for solving NYT-Connections puzzles as task difficulty increases?
- Basis in paper: [explicit] The authors found that Chain-of-Thought and Self-Consistency show diminishing returns as task difficulty increases, and simpler Input-Output prompting often outperforms these approaches on harder puzzles.
- Why unresolved: While the paper tested three prompting methods, the authors note they couldn't test an extensive range due to cost constraints, and more recent prompting innovations exist that weren't explored.
- What evidence would resolve it: Comprehensive testing of various prompting strategies including newer techniques across different difficulty levels, measuring performance gains and identifying which methods are most effective at each difficulty tier.

### Open Question 3
- Question: How would a more diverse human baseline affect the interpretation of LLM performance gaps?
- Basis in paper: [explicit] The authors acknowledge their human performance data comes from only three evaluators, which may not represent the broader population's problem-solving abilities.
- Why unresolved: The current human baseline is based on a very small sample size, making it unclear whether the observed 30% performance gap between LLMs and humans would persist with a larger, more diverse group of participants.
- What evidence would resolve it: Testing the NYT-Connections puzzles with a large sample of participants varying in age, education, cultural background, and puzzle-solving experience, then comparing the aggregated results to LLM performance.

## Limitations
- The benchmark's effectiveness depends on puzzle design remaining unpredictable enough to prevent pattern-based exploitation by LLMs
- The evaluation methodology may not fully capture the spectrum between System 1 and System 2 thinking, as some puzzles might be solvable through hybrid approaches
- The comparison between human and LLM performance could be influenced by differences in familiarity with word puzzle conventions and cultural references

## Confidence
- **High confidence**: The benchmark design and implementation are well-specified, with clear evaluation protocols and reproducible results. The finding that Chain-of-Thought and Self-Consistency show diminishing returns on harder puzzles is robustly demonstrated.
- **Medium confidence**: The claim that current LLMs fall between System 1 and System 2 thinking is supported but could be more nuanced - the performance gap with humans might reflect task-specific skill differences rather than fundamental reasoning limitations.
- **Low confidence**: The assertion that puzzle updates effectively prevent data leakage is difficult to verify without access to the full dataset and knowledge of what training data LLMs have seen.

## Next Checks
1. **Replicate the heuristic baseline** using the specified k-means clustering approach on a subset of puzzles to verify the reported 13.25% accuracy and assess whether it truly represents System 1 thinking.

2. **Test puzzle novelty detection** by running a small set of puzzles through multiple LLMs to check for suspiciously high performance that might indicate pattern recognition rather than reasoning.

3. **Analyze human performance variance** by collecting responses from multiple human participants on the same puzzle set to establish the true upper bound and variability in human reasoning performance.