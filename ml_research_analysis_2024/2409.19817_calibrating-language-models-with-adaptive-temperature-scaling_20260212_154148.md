---
ver: rpa2
title: Calibrating Language Models with Adaptive Temperature Scaling
arxiv_id: '2409.19817'
source_url: https://arxiv.org/abs/2409.19817
tags:
- calibration
- arxiv
- scaling
- language
- temperature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Adaptive Temperature Scaling (ATS), a post-hoc
  calibration method for large language models (LLMs) that addresses calibration degradation
  following RLHF fine-tuning. ATS predicts a temperature scaling parameter for each
  token prediction using token-level features, adapting to varying degrees of miscalibration.
---

# Calibrating Language Models with Adaptive Temperature Scaling

## Quick Facts
- arXiv ID: 2409.19817
- Source URL: https://arxiv.org/abs/2409.19817
- Authors: Johnathan Xie; Annie S. Chen; Yoonho Lee; Eric Mitchell; Chelsea Finn
- Reference count: 10
- Key outcome: ATS improves calibration by 10-50% compared to prior methods while maintaining model performance

## Executive Summary
This paper introduces Adaptive Temperature Scaling (ATS), a post-hoc calibration method for large language models that addresses calibration degradation following RLHF fine-tuning. ATS predicts token-specific temperature scaling parameters using hidden states from the language model, allowing adaptive confidence adjustment based on input context. The method employs a selective smoothing loss function that applies different targets based on prediction correctness, preventing overconfidence on incorrect answers while maintaining confidence on correct ones.

## Method Summary
ATS addresses token-level miscalibration in RLHF-fine-tuned LLMs by learning a calibration head that predicts temperature values for each token prediction. The method takes the last hidden state from the base LLM and outputs a token-specific temperature value, which is exponentiated and applied to scale the logits. A selective smoothing loss function uses one-hot targets for correct predictions and uniform distributions for incorrect ones, preventing the model from increasing confidence on wrong answers. The approach maintains token ranking while adjusting confidence scores, preserving model performance while improving calibration.

## Key Results
- ATS improves Expected Calibration Error (ECE) by 10-50% across MMLU, TriviaQA, and TruthfulQA benchmarks
- The method maintains model performance while significantly improving calibration
- Selective smoothing loss outperforms standard cross-entropy and label smoothing for learning adaptive temperatures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive Temperature Scaling addresses token-level miscalibration by predicting a temperature for each token prediction using hidden features.
- Mechanism: The method learns a calibration head that takes the last hidden state of the LLM as input and outputs a token-specific temperature value. This temperature is exponentiated and applied to scale the logits for that token, allowing different degrees of scaling based on the input context.
- Core assumption: Token-level calibration needs vary based on input context, and hidden states contain sufficient information to predict appropriate scaling.
- Evidence anchors:
  - [abstract] "ATS predicts a temperature scaling parameter for each token prediction using token-level features"
  - [section] "We first produce input-dependent features using the language model π... We then learn a calibration head to produce a temperature vector cθ(ˆh) = τ"
  - [corpus] Weak evidence - corpus neighbors focus on verbalized probabilities and different calibration approaches, not token-level adaptive scaling
- Break condition: If hidden states don't contain enough information about calibration needs, or if token-level calibration needs are uniform across inputs.

### Mechanism 2
- Claim: The selective smoothing loss function improves calibration by applying different targets based on whether the model's original prediction was correct.
- Mechanism: When the model predicts correctly, the loss uses standard cross-entropy with one-hot targets. When incorrect, it uses a uniform distribution target, preventing the model from increasing confidence on wrong answers while still allowing confidence increase on correct ones.
- Core assumption: Standard cross-entropy can increase confidence on incorrect predictions when learning adaptive temperatures, so different treatment is needed based on correctness.
- Evidence anchors:
  - [section] "This loss function uses a uniform distribution as the target when the model is incorrect and a standard one-hot cross-entropy when the model is correct"
  - [section] "One possible explanation for cross-entropy and standard label smoothing being less effective is that learning adaptive temperature values with a cross-entropy loss can actually cause the model to increase confidence when the model is incorrect"
  - [corpus] No direct evidence in corpus about selective smoothing for calibration
- Break condition: If the distinction between correct and incorrect predictions doesn't matter for calibration improvement, or if the uniform target for incorrect predictions prevents learning.

### Mechanism 3
- Claim: ATS improves calibration without affecting model performance because it only modifies confidence scores, not the ranking of tokens.
- Mechanism: By scaling all logits for a given token position by the same temperature value, the relative ranking of tokens remains unchanged. This preserves the original model's selection of the most likely token while adjusting the confidence level.
- Core assumption: Temperature scaling preserves token ranking while modifying confidence, allowing calibration improvements without performance degradation.
- Evidence anchors:
  - [abstract] "ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF"
  - [section] "With this architecture formulation, we retain the ability to predict confidences adaptively depending on the context, while also never changing the ranking for the possible next token given specific context"
  - [corpus] No corpus evidence about preserving ranking during calibration
- Break condition: If temperature scaling affects token ranking in some edge cases, or if the calibration process inadvertently changes model behavior beyond confidence adjustment.

## Foundational Learning

- Concept: Token-level confidence calibration
  - Why needed here: Standard calibration methods like temperature scaling apply the same scaling to all tokens, but language models have varying miscalibration needs at the token level after RLHF fine-tuning
  - Quick check question: What's the key difference between token-level and sequence-level calibration in language models?

- Concept: Hidden state representation learning
  - Why needed here: ATS uses the last hidden state of the LLM as features to predict token-specific temperatures, requiring understanding of how hidden states encode information about calibration needs
  - Quick check question: Why does the paper choose to use hidden states rather than logits as features for the calibration head?

- Concept: Selective loss functions and target adaptation
  - Why needed here: The selective smoothing loss uses different targets (one-hot vs uniform) based on whether predictions are correct, which is crucial for the calibration improvement
  - Quick check question: How does using a uniform target for incorrect predictions prevent the model from becoming more confident on wrong answers?

## Architecture Onboarding

- Component map:
  Input sequence -> Base LLM (produces hidden states) -> Calibration head (produces temperatures) -> Temperature scaling (exponentiation and logit scaling) -> Loss computation (selective smoothing)

- Critical path:
  1. Forward pass through base LLM to get hidden states
  2. Apply calibration head to hidden states to get temperatures
  3. Exponentiate temperatures and scale logits
  4. Compute loss using selective smoothing
  5. Backpropagate only through calibration head parameters

- Design tradeoffs:
  - Using hidden states vs logits: Hidden states capture more contextual information but add computation; logits are more direct but less informative
  - Transformer vs MLP head: Transformer can capture more complex relationships but adds parameters; MLP is simpler but may be less expressive
  - Loss weight α: Higher values increase smoothing but may lead to underconfidence; lower values maintain confidence but may not correct miscalibration

- Failure signatures:
  - Model becomes underconfident (ECE decreases but accuracy drops)
  - Model becomes overconfident (ECE increases, possibly due to incorrect predictions being given high confidence)
  - No improvement in calibration (suggests the calibration head isn't learning useful temperature predictions)

- First 3 experiments:
  1. Verify that temperature scaling preserves token ranking by checking that argmax(ˆz) = argmax(ˆq) for various inputs
  2. Test the selective smoothing loss on a small dataset to confirm it prevents confidence increase on incorrect predictions
  3. Implement the full ATS pipeline on a single benchmark (e.g., MMLU) to verify the end-to-end improvement claim

## Open Questions the Paper Calls Out

The paper identifies several areas for future research, including the need for ongoing refinement of calibration techniques and the integration of a more nuanced understanding of uncertainty. While not explicitly calling out specific open questions, the authors note that their work contributes to a broader effort to address calibration challenges in RLHF-trained models.

## Limitations

- Evaluation is limited to English-language benchmarks without assessment of cross-lingual generalization
- The method assumes token-level calibration needs vary significantly across contexts, but this assumption is not systematically validated
- The selective smoothing loss introduces a hyperparameter (α) that requires tuning, with sensitivity to this choice unexplored
- While ATS maintains performance on evaluated benchmarks, potential inadvertent encoding of biases through adaptive temperatures is not investigated

## Confidence

**High confidence**: The core mechanism of using hidden states to predict token-specific temperatures is well-specified and theoretically sound. The claim that temperature scaling preserves token ranking is mathematically straightforward and supported by the formulation.

**Medium confidence**: The effectiveness of the selective smoothing loss in preventing confidence increase on incorrect predictions is demonstrated empirically but lacks theoretical grounding. The 10-50% improvement claims are based on ECE reduction metrics, which may not fully capture practical calibration quality.

**Low confidence**: The generalizability of the method to models substantially larger than 7B parameters, to non-chat domains, or to languages other than English is not established. The claim that the Alpaca GPT-4 dataset is sufficiently diverse for learning robust calibration across all evaluated benchmarks is asserted but not verified.

## Next Checks

1. **Token-level sensitivity analysis**: Systematically evaluate whether token-level calibration is necessary by comparing ATS against a sequence-level variant on datasets with varying levels of token-level miscalibration. Measure both ECE improvement and computational overhead to determine when the additional complexity is justified.

2. **Selective loss ablation study**: Conduct controlled experiments isolating the effect of the selective smoothing loss by training ATS with (a) standard cross-entropy, (b) uniform smoothing for all predictions, and (c) the proposed selective approach. Track not just ECE but also accuracy on incorrect predictions to verify that the selective approach prevents harmful confidence increases.

3. **Cross-domain robustness test**: Evaluate ATS-calibrated models on out-of-distribution datasets (e.g., medical, legal, or technical domains not represented in MMLU or TriviaQA) to assess whether the Alpaca GPT-4 calibration data provides sufficient coverage for learning domain-agnostic temperature prediction. Compare against models calibrated on domain-specific data to quantify the generalization gap.