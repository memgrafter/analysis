---
ver: rpa2
title: Short-Long Policy Evaluation with Novel Actions
arxiv_id: '2407.03674'
source_url: https://arxiv.org/abs/2407.03674
tags:
- policy
- policies
- data
- horizon
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of evaluating long-term outcomes
  of new decision policies that include novel actions, using only short-term observations
  and historical data. The authors introduce two algorithms: SLEV (Short-Long Estimation
  of Value), which reduces the problem to supervised learning with density ratio weighting,
  and SLED (Short-Long Estimation of Dynamics), which leverages Markov structure through
  low-dimensional policy-specific transformations of historical dynamics models.'
---

# Short-Long Policy Evaluation with Novel Actions

## Quick Facts
- arXiv ID: 2407.03674
- Source URL: https://arxiv.org/abs/2407.03674
- Authors: Hyunji Alex Nam; Yash Chandak; Emma Brunskill
- Reference count: 40
- This paper addresses the challenge of evaluating long-term outcomes of new decision policies that include novel actions, using only short-term observations and historical data.

## Executive Summary
This paper addresses the challenge of evaluating long-term outcomes of new decision policies that include novel actions, using only short-term observations and historical data. The authors introduce two algorithms: SLEV (Short-Long Estimation of Value), which reduces the problem to supervised learning with density ratio weighting, and SLED (Short-Long Estimation of Dynamics), which leverages Markov structure through low-dimensional policy-specific transformations of historical dynamics models. The methods are evaluated on simulators of HIV treatment, kidney dialysis, and battery charging, demonstrating superior performance compared to baseline approaches like Fitted Q-evaluation and online dynamics modeling. SLEV successfully predicts long-term policy values and can quickly detect safety risks (suboptimal policies) from short-horizon data. SLED shows particular advantage when training data exhibits significant distribution shift. The work enables rapid evaluation of innovative decision policies in domains with long time horizons, potentially accelerating the innovation cycle in healthcare, education, and other sequential decision-making applications.

## Method Summary
The paper introduces two algorithms for short-long policy evaluation with novel actions. SLEV frames the problem as supervised learning by predicting long-horizon returns from short-horizon state trajectories, using density ratio weighting to handle distribution shift between training and target policies. SLED leverages Markov structure by decomposing dynamics models into a base policy-independent component and a low-dimensional policy-specific adapter, enabling efficient adaptation to new policies. Both methods use only short-horizon observations from the target policy while requiring historical data covering the full horizon. The algorithms are evaluated on simulated domains including HIV treatment, kidney dialysis, and battery charging, showing superior performance compared to baselines like Fitted Q-evaluation and online dynamics modeling.

## Key Results
- SLEV achieves RMSE of 0.24 compared to 1.0-4.7 for baselines in HIV treatment evaluation
- SLED outperforms online dynamics modeling by 30-50% in battery charging domain with distribution shift
- Both methods successfully detect safety risks with AUC of 0.85-0.95 in safety-critical evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method reduces long-term policy evaluation to supervised learning under covariate shift by predicting long-horizon returns from short-horizon state trajectories.
- Mechanism: By framing the problem as predicting the full-horizon return from a short state trajectory, the algorithm can leverage supervised learning techniques. The density ratio weighting adjusts for the distribution shift between training policies and the new target policy.
- Core assumption: The training data must cover the short-horizon state distribution visited by target policies (Assumption 1: ∀ ¯Hℓ s.t. Ptest( ¯Hℓ) > 0, Ptest( ¯Hℓ)/Ptrain( ¯Hℓ) < ∞).
- Evidence anchors:
  - [abstract] "Our proposed methods significantly outperform prior results on simulators of HIV treatment, kidney dialysis and battery charging."
  - [section 4.1] "If the short-term outcomes were correlated with the long-term outcomes and the data under the target policies have the same distribution as the data under the training policies..."
  - [corpus] Weak - corpus lacks direct evidence of this specific supervised learning framing.
- Break condition: The assumption fails when training data doesn't cover states likely to be visited by the target policy, making the density ratio unbounded.

### Mechanism 2
- Claim: The SLED algorithm leverages Markov structure through low-dimensional policy-specific transformations of historical dynamics models.
- Mechanism: The algorithm decomposes the dynamics model into a base policy-independent model and a low-dimensional policy-specific adapter. This allows adaptation to new policies with novel actions without requiring action overlap.
- Core assumption: The dynamics model for the new target policy is similar to prior policies, such that a low-dimensional parameterization is sufficient to capture the new dynamics.
- Evidence anchors:
  - [section 4.3] "We estimate the policy-dependent dynamics model of the new policy, P(st+1|st, π′), using few-shot fine-tuning when the dynamics model can be decomposed into a base model and a low-dim policy-specific adapter."
  - [abstract] "SLED shows particular advantage when training data exhibits significant distribution shift."
  - [corpus] Weak - corpus doesn't discuss dynamics model decomposition or low-rank adaptation.
- Break condition: When the new policy's dynamics are fundamentally different from historical policies, requiring high-dimensional adaptation rather than low-rank transformation.

### Mechanism 3
- Claim: The method enables rapid detection of safety risks by predicting when new decision policies will have substantially lower performance than historical policies.
- Mechanism: By comparing predicted long-horizon values against a threshold based on historical performance, the algorithm can identify suboptimal policies early and enable timely intervention.
- Core assumption: There exists a meaningful performance threshold that can distinguish safe from unsafe policies based on historical data.
- Evidence anchors:
  - [abstract] "We also demonstrate that our methods can be useful for applications in AI safety by quickly identifying when a new decision policy is likely to have substantially lower performance than past policies."
  - [section 5.3.2] "When policies can lead to very different long-term outcomes, it would be highly beneficial to be able to quickly identify suboptimal policies from a short horizon."
  - [corpus] Weak - corpus lacks evidence about safety risk detection or threshold-based evaluation.
- Break condition: When the performance distribution of historical policies doesn't provide a clear safety threshold, making risk assessment unreliable.

## Foundational Learning

- Concept: Covariate shift and importance weighting in supervised learning
  - Why needed here: The algorithm must handle the distribution shift between training policies and the new target policy while maintaining prediction accuracy
  - Quick check question: How does density ratio weighting adjust for covariate shift in the regression objective?

- Concept: Markov decision processes and dynamics modeling
  - Why needed here: The SLED algorithm leverages the Markov property to learn policy-dependent dynamics through low-dimensional transformations
  - Quick check question: Why is it sufficient to model only the state distribution (not action-dependent transitions) when rewards depend only on state?

- Concept: Off-policy evaluation and policy evaluation in reinforcement learning
  - Why needed here: The method builds on and extends existing off-policy evaluation techniques to handle novel actions and short-long horizon evaluation
  - Quick check question: What assumption about action overlap is typically required in standard off-policy evaluation methods?

## Architecture Onboarding

- Component map:
  Data preprocessing -> SLEV/SLED model -> Density ratio estimator (optional) -> Evaluation pipeline

- Critical path:
  1. Load and preprocess training data
  2. Split data into regression and density ratio estimation sets
  3. Train density ratio estimator (optional)
  4. Train regression model with weighted loss
  5. Perform cross-validation to select hyperparameters
  6. Evaluate on test policies using the trained model

- Design tradeoffs:
  - SLEV vs SLED: SLEV is more general but doesn't leverage Markov structure; SLED is more efficient when Markov assumptions hold
  - With vs without density ratio weighting: Weighting improves accuracy under distribution shift but adds computational complexity
  - Model complexity: Deeper networks may capture complex relationships but risk overfitting on limited data

- Failure signatures:
  - High prediction error on test policies suggests insufficient coverage of training data or poor model capacity
  - Density ratio estimation failure indicates significant distribution shift that may be difficult to handle
  - SLED adaptation failure suggests the new policy's dynamics are too different from historical policies

- First 3 experiments:
  1. Run SLEV with ˆw = 1 (no density ratio weighting) on the HIV treatment simulator with ℓ = 10 to establish baseline performance
  2. Compare SLEV with and without density ratio weighting on a domain with known distribution shift (e.g., filtered battery charging data)
  3. Implement SLED on the battery charging domain to verify low-rank dynamics adaptation works as expected

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several areas remain unresolved based on the evaluation and discussion:

1. Under what conditions does the density ratio weighting in SLEV provide meaningful improvement over uniform weighting (w=1)?
2. How sensitive are SLEV and SLED to the choice of short-horizon length ℓ, and can this parameter be automatically determined?
3. Can the proposed methods be extended to handle settings where rewards depend on both state and action, not just state?

## Limitations

- The methods assume that training data sufficiently covers the short-horizon state distribution of target policies, which may not hold when policies visit novel states
- SLED's low-rank dynamics adaptation approach is only validated on a single domain (battery charging), limiting generalizability assessment
- Safety detection capabilities are theoretically sound but lack extensive empirical validation in real-world scenarios

## Confidence

- **SLEV Algorithm Claims (Medium-High Confidence):** The supervised learning reduction and density ratio weighting approach has strong theoretical grounding in covariate shift literature, and empirical results show consistent improvements over baselines across multiple domains. However, the method's performance heavily depends on data coverage assumptions that weren't extensively tested under stress conditions.
- **SLED Algorithm Claims (Medium Confidence):** The low-rank dynamics adaptation approach is conceptually sound and shows promise, particularly in domains with distribution shift. However, the evaluation is limited to a single domain (battery charging), making it difficult to assess robustness across diverse scenarios.
- **Safety Detection Claims (Medium-Low Confidence):** While the method can theoretically detect safety risks through threshold-based evaluation, the paper provides limited empirical validation of this capability. The safety application relies on having clear historical performance thresholds, which may not exist in many real-world settings.

## Next Checks

1. **Stress Test Data Coverage:** Systematically evaluate SLEV performance as the distance between training and target policy state distributions increases, quantifying the breakdown point of the density ratio weighting approach.

2. **Cross-Domain SLED Validation:** Implement SLED on at least two additional domains with different types of distribution shift (e.g., a healthcare domain and a robotics domain) to assess generalizability of the low-rank dynamics adaptation approach.

3. **Real-World Safety Application:** Apply the safety detection methodology to a real-world sequential decision-making problem with documented safety concerns, measuring false positive/negative rates in identifying truly suboptimal policies.