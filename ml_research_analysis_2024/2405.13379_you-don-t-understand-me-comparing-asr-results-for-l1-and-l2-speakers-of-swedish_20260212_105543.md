---
ver: rpa2
title: 'You don''t understand me!: Comparing ASR results for L1 and L2 speakers of
  Swedish'
arxiv_id: '2405.13379'
source_url: https://arxiv.org/abs/2405.13379
tags:
- speech
- speakers
- language
- recognition
- native
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared Word Error Rate (WER) between native (L1) and
  non-native (L2) Swedish speakers across three ASR systems (Google, Microsoft Azure,
  and Huggingface) using both read and spontaneous speech data. The key finding was
  that non-native speakers consistently had double the WER of native speakers for
  read speech, and for spontaneous speech, non-native speakers had significantly higher
  WER with Microsoft Azure ASR.
---

# You don't understand me!: Comparing ASR results for L1 and L2 speakers of Swedish

## Quick Facts
- arXiv ID: 2405.13379
- Source URL: https://arxiv.org/abs/2405.13379
- Authors: Ronald Cumbal; Birger Moell; Jose Lopes; Olof Engwall
- Reference count: 0
- Non-native speakers had double the WER of native speakers for read speech

## Executive Summary
This study compares Word Error Rate (WER) between native (L1) and non-native (L2) Swedish speakers across three ASR systems (Google, Microsoft Azure, and Huggingface) using both read and spontaneous speech data. The key finding was that non-native speakers consistently had double the WER of native speakers for read speech, and for spontaneous speech, non-native speakers had significantly higher WER with Microsoft Azure ASR. Short utterances were more prone to recognition failure, especially in spontaneous speech. Analysis of misrecognized words revealed that critical words for language learning (e.g., "understand," "repeat") were often misrecognized for non-native speakers, limiting ASR usefulness in educational settings.

## Method Summary
The study used two Swedish speech corpora: "Ville" with 2089 read sentences from 36 speakers (6 native, 30 non-native), and "CORALL" with 1610 spontaneous utterances from 30 speakers (6 native, 24 non-native). Speakers had 18 different native languages. The researchers ran transcriptions through three ASR systems using default settings: Google Cloud Speech-to-Text API, Microsoft Azure Speech-to-text API, and Huggingface Wav2vec2 model with Swedish model. They calculated WER for each system and speaker group, analyzed NFR (Number of samples Failed to be Recognized) rates, and performed linguistic analysis of misrecognized words using normalized frequency metrics.

## Key Results
- Non-native speakers consistently had double the WER of native speakers for read speech across all three ASR systems
- For spontaneous speech, non-native speakers had significantly higher WER with Microsoft Azure ASR compared to native speakers
- Short utterances were more prone to recognition failure, especially in spontaneous speech
- Critical words for language learning (e.g., "understand," "repeat") were often misrecognized for non-native speakers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASR performance deteriorates for non-native speakers because their speech deviates from the native acoustic and language model distributions.
- Mechanism: Native speaker models are trained on native pronunciation patterns, vocabulary usage, and fluency characteristics. Non-native speech introduces pronunciation variants, grammatical simplifications, and disfluencies that are underrepresented in training data, causing higher substitution, deletion, and insertion errors.
- Core assumption: The acoustic and language models used by commercial ASR systems are primarily trained on native speaker data, making them less effective at handling non-native speech characteristics.
- Evidence anchors:
  - [abstract] "non-native speakers consistently had double the WER of native speakers for read speech"
  - [section] "performance tends to decrease considerably...with more atypical speakers (e.g., children, non-native speakers or people with speech disorders)"
  - [corpus] Weak evidence - no specific corpus examples provided for model training composition
- Break condition: When ASR systems are explicitly fine-tuned on substantial non-native speech data or when the language model adapts to include common non-native patterns and vocabulary.

### Mechanism 2
- Claim: Short utterances are more error-prone for non-native speakers due to insufficient context for language model disambiguation.
- Mechanism: Longer utterances provide more context for the language model to predict likely word sequences, while short utterances lack this contextual support. For non-native speakers, this effect is amplified because their pronunciation variants and grammatical structures are less predictable.
- Core assumption: Language models benefit from longer context windows to improve prediction accuracy, and this benefit is diminished when dealing with non-native speech patterns.
- Evidence anchors:
  - [section] "Short utterances were more prone to recognition failure, especially in spontaneous speech"
  - [section] "Short utterances were more prone to recognition failure, especially in spontaneous speech"
  - [corpus] Weak evidence - corpus shows FMR analysis but lacks specific utterance length data for error patterns
- Break condition: When ASR systems use alternative approaches like word-level confidence scoring or when the language model is trained to handle fragmented, context-light input effectively.

### Mechanism 3
- Claim: Critical language learning words are more frequently misrecognized for non-native speakers, limiting ASR utility in educational contexts.
- Mechanism: Words like "understand" and "repeat" that non-native speakers use to signal uncertainty or request clarification are often misrecognized, creating feedback loops where learners cannot effectively communicate their needs to the system.
- Core assumption: Educational ASR applications require accurate recognition of meta-linguistic expressions that learners use to navigate learning interactions.
- Evidence anchors:
  - [abstract] "critical words for language learning (e.g., 'understand,' 'repeat') were often misrecognized for non-native speakers"
  - [section] "when we analyzed the word errors...for non-native speakers, there were specific misrecognized words ('understand' and 'repeat') that signal important states of uncertainty"
  - [corpus] Weak evidence - corpus provides FMR scores but doesn't specifically analyze educational vocabulary recognition
- Break condition: When ASR systems incorporate domain-specific language models for educational contexts or when they use confidence thresholding to handle uncertain recognitions differently.

## Foundational Learning

- Concept: Word Error Rate (WER) calculation
  - Why needed here: WER is the primary metric used to compare ASR performance between native and non-native speakers across different systems and conditions
  - Quick check question: If an utterance has 20 words and the ASR produces 2 substitutions, 1 deletion, and 1 insertion, what is the WER?

- Concept: Acoustic model vs. language model distinction
  - Why needed here: Understanding how these two components work together explains why non-native speech is harder to recognize (acoustic differences) and why certain word patterns are misrecognized (language model limitations)
  - Quick check question: Which component is primarily responsible for recognizing that "f¨ orst˚ ar" should be transcribed as "understand" versus some other similar-sounding word?

- Concept: Statistical significance testing (Welch's t-test)
  - Why needed here: The study uses statistical tests to determine whether observed WER differences between native and non-native speakers are meaningful or could be due to chance
  - Quick check question: Why might the researchers choose Welch's t-test instead of a standard t-test when comparing WER distributions?

## Architecture Onboarding

- Component map: Audio input -> Acoustic feature extraction -> Acoustic model prediction -> Language model scoring -> Decoding with beam search -> Post-processing -> WER calculation -> Error analysis
- Critical path: Audio input → Acoustic feature extraction → Acoustic model prediction → Language model scoring → Decoding with beam search → Post-processing → WER calculation → Error analysis
- Design tradeoffs: High-resource vs. low-resource language support, real-time vs. batch processing, generic vs. domain-specific models, API integration vs. self-hosted models
- Failure signatures: High NFR (number of failed recognitions) for short utterances, significantly higher WER for non-native speakers, systematic misrecognition of educational vocabulary, inconsistent performance across spontaneous vs. read speech
- First 3 experiments:
  1. Run the same Swedish audio samples through multiple ASR systems (Google, Microsoft Azure, Huggingface) to verify the WER patterns described
  2. Segment the dataset by utterance length and analyze NFR rates to confirm the short utterance failure pattern
  3. Extract and analyze the most frequently misrecognized words for non-native speakers to identify patterns in educational vocabulary failures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum level of non-native speech proficiency required for ASR systems to achieve comparable performance to native speakers?
- Basis in paper: [inferred] The paper shows that non-native speakers consistently have higher WER than native speakers, and certain critical words for language learning are often misrecognized, suggesting a proficiency threshold below which ASR becomes unreliable.
- Why unresolved: The study did not categorize non-native speakers by proficiency levels beyond "Advanced Beginner" and "Basic to Intermediate," making it impossible to determine a specific proficiency threshold for acceptable ASR performance.
- What evidence would resolve it: A study testing ASR performance across multiple proficiency levels (e.g., beginner, intermediate, advanced) for the same non-native speakers, tracking WER and error types at each level.

### Open Question 2
- Question: How does the linguistic distance between a non-native speaker's L1 and the target language (Swedish) affect ASR recognition accuracy?
- Basis in paper: [explicit] The study found Italian speakers performed comparably to or better than native Swedish speakers, suggesting potential linguistic similarity advantages, but lacked sufficient Italian participants to confirm significance.
- Why unresolved: The dataset included too few Italian speakers to perform statistical analysis, and no systematic comparison of L1-target language linguistic distance was conducted.
- What evidence would resolve it: A controlled study with balanced participant groups across languages with varying degrees of similarity to Swedish (e.g., German, French, Arabic, Chinese), measuring WER differences.

### Open Question 3
- Question: Can fine-tuning ASR models on non-native speech data significantly reduce recognition errors for critical language learning words?
- Basis in paper: [explicit] The authors acknowledge they did not fine-tune any systems for non-native speech and state this as future work, particularly noting the importance of recognizing words like "understand" and "repeat."
- Why unresolved: The study only used off-the-shelf ASR systems without any adaptation for non-native speech patterns, leaving the potential benefits of fine-tuning untested.
- What evidence would resolve it: A comparative study measuring WER before and after fine-tuning commercial ASR systems on non-native speech corpora, with particular attention to language learning critical words.

### Open Question 4
- Question: What is the optimal utterance length for ASR recognition of non-native spontaneous speech?
- Basis in paper: [inferred] The study found non-native speakers had similar or worse WER for longer utterances compared to shorter ones in spontaneous speech, contrary to expectations, suggesting a complex relationship between utterance length and recognition accuracy.
- Why unresolved: The analysis showed unexpected results where longer utterances didn't consistently improve recognition for non-native speakers, but the study didn't systematically explore the optimal length range.
- What evidence would resolve it: A controlled experiment varying utterance length systematically for non-native speakers in spontaneous speech contexts, measuring WER and NFR across multiple length categories to identify optimal ranges.

## Limitations
- Relatively small dataset size, particularly for native Swedish speakers (6 native vs 30 non-native in the read speech corpus)
- Doesn't investigate whether performance differences stem from acoustic model limitations versus language model mismatches
- Analysis of misrecognized words relies on qualitative interpretation rather than systematic linguistic error classification

## Confidence
**High Confidence**: The finding that non-native speakers consistently show double the WER of native speakers for read speech across all three ASR systems.

**Medium Confidence**: The claim that short utterances are more prone to recognition failure, especially for spontaneous speech.

**Medium Confidence**: The observation that critical language learning words are frequently misrecognized for non-native speakers.

## Next Checks
1. **Controlled acoustic analysis**: Extract acoustic features (fundamental frequency, formant frequencies, duration patterns) from native and non-native speech samples to quantify systematic pronunciation differences that could explain ASR performance gaps.

2. **Language model adaptation experiment**: Fine-tune one of the ASR systems' language models on non-native Swedish speech patterns and re-evaluate WER to determine whether performance improvements are achievable through domain adaptation.

3. **Cross-linguistic generalization test**: Apply the same ASR evaluation methodology to another language with similar speaker demographics to verify whether the observed patterns are specific to Swedish or represent general principles of non-native speech recognition.