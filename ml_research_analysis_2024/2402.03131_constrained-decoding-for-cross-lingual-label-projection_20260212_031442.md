---
ver: rpa2
title: Constrained Decoding for Cross-lingual Label Projection
arxiv_id: '2402.03131'
source_url: https://arxiv.org/abs/2402.03131
tags:
- translation
- label
- codec
- language
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a constrained decoding approach to improve
  cross-lingual label projection for low-resource languages. The key idea is to first
  translate source sentences without markers, then use constrained decoding to insert
  markers into the translated sentences while enforcing constraints on translation
  quality and marker count.
---

# Constrained Decoding for Cross-lingual Label Projection

## Quick Facts
- arXiv ID: 2402.03131
- Source URL: https://arxiv.org/abs/2402.03131
- Reference count: 40
- Key outcome: Constrained decoding approach significantly outperforms previous marker-based and alignment-based label projection methods for cross-lingual transfer, especially in translating test data settings across 20 languages.

## Executive Summary
This paper addresses the challenge of cross-lingual label projection for low-resource languages by introducing a constrained decoding approach called CODEC. The key insight is that translating source sentences with label markers directly degrades translation quality, so instead, the method first translates without markers and then uses constrained decoding to insert markers into the translated sentences while enforcing constraints on translation quality and marker count. Experiments on Named Entity Recognition and Event Argument Extraction tasks across 20 languages show significant improvements over previous approaches, particularly when combining translate-train and translate-test strategies.

## Method Summary
CODEC is a two-phase approach for cross-lingual label projection. First, it translates source sentences without markers using a large MT model (NLLB variants) to create a translation template. Second, it uses constrained decoding with a smaller MT model (NLLB-600M) to insert markers into the translated template while enforcing constraints that maintain translation quality and ensure correct marker counts. The method is evaluated on NER using English CoNLL03 and MasakhaNER2.0 across 20 African languages, and on Event Argument Extraction using English ACE-2005 and Chinese/Arabic data. The approach can be used in translate-train mode (augmenting training data) or translate-test mode (projecting labels at inference), with the combination providing the best results.

## Key Results
- CODEC significantly outperforms previous marker-based and alignment-based label projection methods across 20 languages
- Translate-test setting provides considerable performance boost compared to translate-train alone for NER tasks
- The method is effective for both NER and Event Argument Extraction tasks
- Constrained decoding successfully inserts markers while maintaining translation quality

## Why This Works (Mechanism)

### Mechanism 1
Translating without markers first preserves translation quality compared to translating with markers directly. Marker-based projection degrades MT quality because markers interfere with the MT model's ability to produce fluent, natural translations. By separating translation and marker insertion into two phases, the first translation phase produces higher quality output.

### Mechanism 2
Constrained decoding can accurately insert markers while maintaining translation quality by enforcing constraints during generation. The constrained decoding algorithm explores only valid hypotheses that conform to the high-quality translation from the first phase and having the correct number of markers inserted. This eliminates the need to score all possible marker insertion positions.

### Mechanism 3
Using a two-stage approach (translate-train + translate-test) provides better cross-lingual transfer performance than translate-train alone. Translate-train augments training data with projected labels, while translate-test leverages the full supervised model by translating test data to the source language, annotating, then projecting back. The combination provides complementary benefits.

## Foundational Learning

- **Machine Translation systems and marker limitations**: Understanding how MT models handle unusual input is crucial for why marker-based methods degrade quality and why the two-phase approach helps. Quick check: Why might inserting special markers into source text before translation degrade translation quality?

- **Constrained decoding algorithms and search space pruning**: The core technical contribution relies on efficiently exploring a constrained search space to find optimal marker positions. Quick check: How does branch-and-bound pruning reduce the search space complexity in constrained decoding?

- **Cross-lingual transfer learning and data augmentation**: The method builds on existing translate-train and translate-test approaches, so understanding their strengths and limitations is important. Quick check: What are the key differences between translate-train and translate-test strategies for cross-lingual transfer?

## Architecture Onboarding

- **Component map**: Source sentence with labels -> Translation template (MT1) -> Constrained decoding (MT2) -> Output with projected labels
- **Critical path**: xmark → ytmpl (MT1) → constrained decoding (MT2) → output
- **Design tradeoffs**: Using two MT systems vs. one (quality vs. computational cost); Exact vs. heuristic search (accuracy vs. speed); Translate-train only vs. combined approach (coverage vs. complexity)
- **Failure signatures**: Poor translation quality in ytmpl → markers inserted in wrong positions; Constrained decoder cannot find valid hypotheses → no output or incorrect labels; Back-translation in translate-test introduces errors → degraded performance
- **First 3 experiments**: 1) Compare translation quality (BLEU) of translating with vs. without markers; 2) Evaluate constrained decoding accuracy vs. baseline marker-based method on small dataset; 3) Test translate-train performance improvement on NER task with different numbers of training examples

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CODEC vary with different sizes of the MT model used for template generation and constrained decoding? The paper mentions experimentation with different NLLB sizes but lacks detailed per-language analysis and insights into trade-offs between model size, translation quality, and decoding speed.

### Open Question 2
How does the performance of CODEC compare to other constrained decoding methods, such as beam search, in the constrained search space? The paper provides only brief comparison with CSBS but lacks comprehensive analysis of trade-offs between search efficiency, decoding speed, and label projection accuracy.

### Open Question 3
How does the performance of CODEC vary with different values of the hyperparameters δ and α in the heuristic lower bound and opening marker position pruning? The paper provides brief analysis of impact on decoding speed but lacks comprehensive analysis of how hyperparameters affect overall performance and optimal settings for different languages and tasks.

## Limitations

- Constrained decoding efficiency heavily depends on heuristic lower bound and pruning thresholds that are not fully specified
- Performance gains measured primarily on African languages for NER and Chinese/Arabic for EAE, limiting generalizability
- Method assumes high-quality translation templates can be generated without markers, which may not hold for languages with different word order or morphology
- Computational overhead of constrained decoding is not explicitly quantified against baseline approaches

## Confidence

- **High Confidence**: Core claim that marker-based projection degrades translation quality is well-supported by theoretical reasoning and empirical evidence
- **Medium Confidence**: Claim that constrained decoding can efficiently explore valid hypothesis space while maintaining translation quality is supported but exact efficiency gains remain uncertain
- **Low Confidence**: Assertion that translate-test combined with translate-train provides significant additional benefits is based on limited experimental evidence

## Next Checks

1. **Algorithm Efficiency Analysis**: Measure and compare computational time and memory usage of CODEC's constrained decoding against baseline marker-based projection method across different language pairs and dataset sizes.

2. **Cross-Language Generalization Test**: Apply CODEC to diverse language pairs beyond tested African, Chinese, and Arabic languages, particularly including languages with non-Indo-European structures (e.g., Japanese, Turkish, or Finnish).

3. **Translation Quality Impact Study**: Conduct detailed analysis of how translation quality degradation in initial template generation phase affects final label projection accuracy by systematically varying NLLB translation model quality.