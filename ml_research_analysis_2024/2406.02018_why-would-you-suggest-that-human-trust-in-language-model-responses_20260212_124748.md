---
ver: rpa2
title: Why Would You Suggest That? Human Trust in Language Model Responses
arxiv_id: '2406.02018'
source_url: https://arxiv.org/abs/2406.02018
tags:
- trust
- responses
- human
- history
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how explanations impact human trust in
  language model (LM) responses through human studies on news headline generation.
  The authors found that explanations, especially post-hoc justifications, significantly
  increased self-reported user trust when responses were compared side-by-side.
---

# Why Would You Suggest That? Human Trust in Language Model Responses

## Quick Facts
- arXiv ID: 2406.02018
- Source URL: https://arxiv.org/abs/2406.02018
- Authors: Manasi Sharma; Ho Chit Siu; Rohan Paleja; Jaime D. Peña
- Reference count: 29
- Key outcome: Explanations significantly increase self-reported user trust in LM responses when compared side-by-side, but this effect disappears when responses are shown in isolation

## Executive Summary
This paper investigates how different explanation styles impact human trust in language model responses through human studies on news headline generation. The authors found that explanations, particularly post-hoc justifications, significantly increased self-reported user trust when responses were compared side-by-side. However, this trust benefit disappeared when responses were shown in isolation, suggesting humans trust all LM responses similarly regardless of explanation type. Additionally, false explanations negatively impacted trust in comparative settings but not in isolated viewing. The study found no significant tradeoff between explanation types and LM performance, suggesting that explanations can enhance trust without compromising output quality.

## Method Summary
The study used the LaMP benchmark validation set (1500 samples) for News Headline Generation task. Researchers conducted human-AI trustworthiness surveys where participants rated responses on Likert scales and ranked them in groups. The study tested different explanation styles including post-hoc justifications, pre-hoc explanations, and deceptive (fake) explanations. Model performance was evaluated using ROUGE-1 and ROUGE-L scores. Participants were shown either individual responses for Likert rating or groups of responses for ranking comparisons.

## Key Results
- Post-hoc justifications significantly outperformed other explanation types in ranking tasks
- Trust benefits of explanations disappeared when responses were shown in isolation
- False explanations negatively impacted trust in comparative settings but not in isolation
- No significant performance tradeoffs between explanation types (ROUGE scores)
- Small effect sizes overall (d = 0.125 for gender effects)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explanations improve user trust in language model responses when comparisons are available.
- Mechanism: When users can compare multiple responses side-by-side, explanations provide distinguishing features that signal model competence and reasoning quality, leading to higher trust ratings in ranking tasks.
- Core assumption: Users actively evaluate and differentiate between model responses when presented in comparative contexts.
- Evidence anchors:
  - [abstract]: "Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses."
  - [section 4]: "Many differences appeared when participants were asked to rank response types. The vast majority of the pairwise comparisons of rankings were statistically significantly different from each other."
  - [corpus]: Weak - related papers focus on explanations in FER and HRI but don't directly address comparative evaluation of explanations in LLM responses.

### Mechanism 2
- Claim: The faithfulness of explanations impacts user trust in comparative settings but not in isolated viewing.
- Mechanism: In ranking scenarios, users can detect inconsistencies or falsehoods in explanations, leading to lower trust for deceptive explanations. However, when viewing responses independently, users lack the comparative context to identify deceptive explanations, resulting in similar trust ratings regardless of explanation truthfulness.
- Core assumption: Users possess the ability to detect inconsistencies in explanations when presented with multiple options for comparison.
- Evidence anchors:
  - [abstract]: "However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation."
  - [section 4]: "The willful lack of faithfulness of an explanation (whether the model is asked to actively lie about its reasoning process versus self-described to align with the reasoning of the model) negatively impacts user trust in the both the no history and retrieved history ranking cases."
  - [corpus]: Weak - no direct corpus evidence for isolation vs comparison effects on explanation faithfulness detection.

### Mechanism 3
- Claim: Post-hoc justifications are most effective at increasing trust compared to pre-hoc and cross-domain justifications.
- Mechanism: Post-hoc explanations, which provide reasoning after the response, are perceived as more natural and less forced than pre-hoc explanations that outline reasoning before the response. They also appear more integrated with the actual response compared to cross-domain reasoning.
- Core assumption: Users perceive post-hoc explanations as more authentic and relevant to the actual response generation process.
- Evidence anchors:
  - [abstract]: "Position and faithfulness of these explanations are also important factors."
  - [section 4]: "Post-hoc justification seems to outpace the Pre-hoc and Fake justifications by a small amount in the No History and Retrieved History cases across GPT-3.5 and GPT-4."
  - [corpus]: Weak - corpus doesn't provide evidence specifically about post-hoc vs pre-hoc effectiveness.

## Foundational Learning

- Concept: Likert scale measurements and their limitations
  - Why needed here: The study uses Likert scales to measure self-reported trust, but found no significant differences between explanation types in isolated viewing, suggesting limitations of this measurement approach.
  - Quick check question: Why might Likert scale ratings fail to capture subtle differences in user trust when responses are viewed in isolation?

- Concept: Statistical significance and effect sizes
  - Why needed here: The study found statistically significant differences in ranking data but not in Likert ratings, and even when significant, effect sizes were small (d = 0.125), highlighting the importance of understanding statistical power and practical significance.
  - Quick check question: What does a small effect size (d = 0.125) tell us about the practical importance of the gender effect on trust ratings?

- Concept: ROUGE score as an evaluation metric
  - Why needed here: The study uses ROUGE scores to measure model performance on the headline generation task, finding no significant tradeoffs between explanation types and performance.
  - Quick check question: Why might ROUGE scores be insufficient for evaluating performance on creative, open-ended tasks like headline generation?

## Architecture Onboarding

- Component map: Model generates response → Human evaluates response (Likert) → Human compares responses (ranking) → Statistical analysis determines significance → Insights inform future explanation design
- Critical path: Model generates response → Human evaluates response (Likert) → Human compares responses (ranking) → Statistical analysis determines significance → Insights inform future explanation design
- Design tradeoffs: The study prioritized breadth of explanation types over depth of individual comparisons, limiting each participant to 11 responses and 3 ranking groups to maintain reasonable survey duration. This tradeoff enabled broader coverage but may have missed nuanced differences between more similar explanation styles.
- Failure signatures: If post-hoc explanations fail to improve trust, possible causes include: explanations being too generic, users perceiving them as artificial, or the task not requiring complex reasoning that would benefit from explanation. If faithfulness detection fails, users may be overly trusting or explanations may be too sophisticated to distinguish from truthful ones.
- First 3 experiments:
  1. Replicate the ranking experiment with a different task (e.g., research paper title generation) to test generalizability of the comparative advantage of explanations.
  2. Test the effect of explanation length on trust, as longer explanations might provide more distinguishing features but could also overwhelm users.
  3. Investigate whether providing explanations for both correct and incorrect responses affects trust differently, testing the hypothesis that explanations improve trust for correct responses but may harm trust for incorrect ones.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the faithfulness of language model explanations impact user trust when explanations are not explicitly labeled as fake or truthful?
- Basis in paper: [inferred] The paper found that fake explanations negatively impacted trust in comparative settings but not in isolation, suggesting humans may be poor at detecting deception without explicit comparison. The paper also notes that the faithfulness of explanations is an important factor that was not fully explored.
- Why unresolved: The study only tested explicitly fake explanations versus truthful ones. It's unclear whether users can detect subtle inconsistencies or less extreme departures from the model's actual reasoning process.
- What evidence would resolve it: Experiments comparing user trust across a spectrum of explanation faithfulness levels (fully faithful, mostly faithful, somewhat faithful, mostly unfaithful, fully unfaithful) both in isolation and comparative settings.

### Open Question 2
- Question: Does the type of task (creative vs. factual) affect how much users rely on explanations for trust calibration?
- Basis in paper: [explicit] The authors note that most trust research focuses on factual tasks while they studied creative headline generation, suggesting this is an open area. They mention that for creative tasks "there are many correct answers" making evaluation more complex.
- Why unresolved: The study only examined one creative task (headline generation). It's unclear whether the observed effects would generalize to other creative tasks or whether factual tasks might show different patterns of explanation reliance.
- What evidence would resolve it: Replicating the study across multiple creative tasks (story generation, image captioning, etc.) and factual tasks (QA, classification) to compare explanation effects.

### Open Question 3
- Question: What is the optimal balance between explanation detail and conciseness for maximizing both user trust and model performance?
- Basis in paper: [explicit] The authors note they tested various explanation styles but didn't systematically vary explanation length or detail level. User feedback mentioned both "concise responses" and "relevant explanation" as trust factors, suggesting complexity.
- Why unresolved: The study tested distinct explanation types but not variations in explanation verbosity within each type. The optimal level of detail that maximizes trust without overwhelming users or degrading performance is unknown.
- What evidence would resolve it: Controlled experiments varying explanation length and detail level across different task types while measuring both trust metrics and performance impacts.

## Limitations

- Limited generalizability to other tasks and domains beyond news headline generation
- Reliance on self-reported trust measures through Likert scales may not capture full trust dynamics
- Small effect sizes (d = 0.125) suggest limited practical impact of explanation differences
- Focus exclusively on English language models, potentially missing cross-cultural trust differences

## Confidence

- **High confidence**: The finding that post-hoc explanations outperform other explanation types in comparative settings is well-supported by the data.
- **Medium confidence**: The claim about explanations disappearing when shown in isolation needs further validation with different tasks and participant groups.
- **Low confidence**: The generalizability of these findings to other domains, tasks, or cultural contexts.

## Next Checks

1. **Cross-task validation**: Replicate the study using a different NLP task (e.g., question answering or summarization) to test whether the comparative advantage of explanations generalizes beyond headline generation.

2. **Longitudinal trust assessment**: Conduct a follow-up study measuring how trust in explanations changes over time as users gain experience with language models, testing whether initial trust benefits persist or diminish.

3. **Mixed-methods validation**: Supplement the quantitative Likert and ranking data with qualitative interviews to understand the reasoning behind trust decisions and validate whether explanations are being evaluated as intended.