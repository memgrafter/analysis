---
ver: rpa2
title: Fine-tuning vs Prompting, Can Language Models Understand Human Values?
arxiv_id: '2403.09720'
source_url: https://arxiv.org/abs/2403.09720
tags:
- task
- fine-tuning
- human
- prompt
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper explores the effectiveness of fine-tuning versus prompt\
  \ tuning for detecting human values in persuasive communication, using the Touch\xE9\
  23-ValueEval dataset. Fine-tuning approaches, including RoBERTa and DeBERTa with\
  \ multiple classifiers and contrastive learning, achieved a macro F1 score of 0.52."
---

# Fine-tuning vs Prompting, Can Language Models Understand Human Values?

## Quick Facts
- arXiv ID: 2403.09720
- Source URL: https://arxiv.org/abs/2403.09720
- Authors: Pingwei Sun
- Reference count: 6
- Primary result: Fine-tuning outperforms prompt tuning on smaller models (<500M parameters) for human value detection, while larger models (>1B parameters) are needed for prompt tuning to match fine-tuning performance.

## Executive Summary
This paper investigates the effectiveness of fine-tuning versus prompt tuning for detecting human values in persuasive communication using the Touché23-ValueEval dataset. The study finds that fine-tuning approaches, including RoBERTa and DeBERTa with multiple classifiers and contrastive learning, achieve a macro F1 score of 0.52. While prompt tuning with T5 and GPT models shows comparable performance, particularly for larger models, fine-tuning remains more effective with smaller models and abundant data. The research highlights that prompt tuning requires larger models for complex tasks, while fine-tuning is effective with smaller models and abundant data.

## Method Summary
The study evaluates fine-tuning and prompt tuning approaches on the Touché23-ValueEval dataset containing 9324 diverse arguments with human value labels. Fine-tuning uses RoBERTa and DeBERTa with multiple classifiers and optional contrastive learning loss. Prompt tuning employs T5 and GPT models with various template-based approaches including Binary Choice Answering, Open Answering, and Chain-of-Thought prompting. The evaluation uses macro F1 score as the primary metric for the multi-label classification task of detecting human values in persuasive communication.

## Key Results
- Fine-tuning with RoBERTa and DeBERTa achieves a macro F1 score of 0.52 on the Touché23-ValueEval dataset
- Prompt tuning shows comparable performance to fine-tuning, particularly for models larger than 1B parameters
- Contrastive learning provides additional performance improvements when applied during fine-tuning
- LLMs like ChatGPT demonstrate strong capabilities in human value detection without requiring fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning performs better than prompt tuning on smaller models for complex NLU tasks with abundant data.
- Mechanism: Fine-tuning directly optimizes model parameters on task-specific data, allowing the model to learn task-specific patterns and representations. For smaller models (<1B parameters), the reduced parameter space makes fine-tuning more efficient and effective.
- Core assumption: The task complexity and data availability outweigh the generalization benefits of prompt tuning for smaller models.
- Evidence anchors:
  - [section] "From the table, it is evident that on models with a size less than 500M, fine-tuning outperforms prompt tuning, which is reasonable being given more trainable parameters."
  - [abstract] "Prompt tuning with T5 and GPT models showed comparable performance, particularly for larger models."

### Mechanism 2
- Claim: Prompt tuning requires larger models to achieve performance comparable to fine-tuning on complex tasks.
- Mechanism: Prompt tuning leverages the knowledge acquired during pre-training through carefully crafted prompts. Larger models have more parameters and a broader knowledge base, enabling them to better understand and respond to complex prompts.
- Core assumption: The knowledge acquired during pre-training is sufficient for the task, and the prompt design effectively elicits this knowledge.
- Evidence anchors:
  - [section] "However, when the model size approaches 1B (two to three times that of the formers), prompt tuning shows comparable performance to fine-tuning."
  - [abstract] "The study highlights that prompt tuning requires larger models for complex tasks, while fine-tuning remains effective with smaller models and abundant data."

### Mechanism 3
- Claim: Contrastive learning improves embedding representations for multi-label classification tasks.
- Mechanism: Contrastive learning optimizes the embedding space by pulling positive samples closer and pushing negative samples apart. For multi-label classification, this helps create more discriminative representations for each label.
- Core assumption: The positive and negative samples can be effectively defined for the multi-label classification task, even if not directly within a mini-batch.
- Evidence anchors:
  - [section] "Inspired by the SimCSE (Gao et al., 2021), Contrastive Learning (CL) loss is applied to optimize the embedding states of sequences."
  - [section] "Then Contrastive Learning is introduced for further improvement of the performance. In this part, we treat it as either a pre-training task or an extra loss item at the fine-tuning stage. The second strategy performs slightly better, and both of them make progress on the macro F1 score."

## Foundational Learning

- Concept: Multi-label classification
  - Why needed here: The task involves detecting multiple human values in persuasive communication, requiring the model to predict multiple labels simultaneously.
  - Quick check question: How does multi-label classification differ from multi-class classification, and what are the implications for model design?

- Concept: Prompt engineering
  - Why needed here: Prompt tuning relies on carefully crafted prompts to elicit the desired responses from pre-trained models. Understanding prompt engineering techniques is crucial for effective prompt tuning.
  - Quick check question: What are the key considerations when designing prompts for a specific task, and how do they impact model performance?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is used to improve embedding representations for the multi-label classification task. Understanding its principles and applications is essential for implementing this technique effectively.
  - Quick check question: How does contrastive learning optimize the embedding space, and what are the benefits for downstream tasks like classification?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model selection -> Fine-tuning pipeline (with classifiers and contrastive learning) -> Prompt tuning pipeline (with templates) -> Evaluation

- Critical path:
  1. Preprocess data and prepare features
  2. Select and initialize models
  3. Implement fine-tuning with contrastive learning
  4. Design and implement prompt templates
  5. Evaluate and compare model performance

- Design tradeoffs:
  - Model size vs. performance: Larger models generally perform better but require more computational resources
  - Fine-tuning vs. prompt tuning: Fine-tuning is more effective for smaller models with abundant data, while prompt tuning requires larger models for complex tasks
  - Contrastive learning vs. standard fine-tuning: Contrastive learning can improve embedding representations but may require additional computational overhead

- Failure signatures:
  - Poor performance: May indicate issues with model selection, data preprocessing, or hyperparameter tuning
  - High variance in prompt tuning: May suggest sensitivity to prompt initialization or the need for larger models
  - Inconsistent results across categories: May indicate issues with prompt design or task-specific knowledge in the models

- First 3 experiments:
  1. Fine-tuning with RoBERTa and DeBERTa using multiple classifiers and contrastive learning
  2. Prompt tuning with T5 and GPT models using various prompt templates (e.g., CLS, MBC, BCA, OA, CoT)
  3. Evaluation of LLMs (e.g., Llama, ChatGPT) using Chain-of-Thought prompts without fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does prompt tuning consistently outperform fine-tuning for models larger than 1B parameters across different NLP tasks?
- Basis in paper: [explicit] The paper observes that prompt tuning shows comparable performance to fine-tuning for models approaching 1B parameters in the complex task of human value detection.
- Why unresolved: The paper only tests this hypothesis on a single complex task (human value detection) and with a limited set of model sizes.
- What evidence would resolve it: Comparative studies of prompt tuning vs. fine-tuning across a diverse range of NLP tasks and model sizes larger than 1B parameters.

### Open Question 2
- Question: How does the performance of fine-tuning and prompt tuning scale with the size of the training dataset in few-shot learning scenarios?
- Basis in paper: [inferred] The paper mentions that prompt tuning shows obvious advantages over fine-tuning in few-shot scenarios but does not provide detailed comparative results.
- Why unresolved: The paper does not provide a systematic analysis of how the performance of fine-tuning and prompt tuning scales with dataset size in few-shot learning.
- What evidence would resolve it: Experiments comparing the performance of fine-tuning and prompt tuning across different dataset sizes, from few-shot to full dataset scenarios.

### Open Question 3
- Question: What is the impact of model alignment with human preferences (RLHF) on the performance of LLMs in understanding and detecting human values in persuasive communication?
- Basis in paper: [explicit] The paper explores the capabilities of LLMs aligned with RLHF in detecting human values but only provides preliminary attempts.
- Why unresolved: The paper does not provide a comprehensive evaluation of how RLHF impacts LLM performance on this task.
- What evidence would resolve it: Systematic experiments comparing the performance of LLMs with and without RLHF alignment on the human value detection task.

## Limitations

- The exact formulations of prompt templates (CLS, MBC, BCA, OA, CoT) are not publicly available, making replication difficult.
- Implementation details for contrastive learning are incomplete, including the temperature scaling factor and positive/negative sample definitions.
- The study does not report statistical significance testing for performance differences between fine-tuning and prompt tuning approaches.

## Confidence

- High confidence: Fine-tuning outperforms prompt tuning on smaller models (<500M parameters) for this task, supported by direct experimental evidence.
- Medium confidence: Larger models (>1B parameters) are required for prompt tuning to match fine-tuning performance, based on limited model size testing.
- Medium confidence: Contrastive learning effectiveness, as improvement is shown but ablation studies are lacking.

## Next Checks

1. **Prompt template replication**: Implement and test the exact prompt templates (CLS, MBC, BCA, OA, CoT) described in the paper using the Touché23-ValueEval dataset to verify the reported performance differences between fine-tuning and prompt tuning approaches.

2. **Model size scaling analysis**: Systematically evaluate model performance across a broader range of sizes (e.g., 300M, 500M, 1B, 2B, 4B parameters) to determine the precise point where prompt tuning performance begins to match fine-tuning, and identify any performance saturation points.

3. **Contrastive learning ablation**: Conduct controlled experiments comparing fine-tuning with and without contrastive learning loss, using identical model architectures and hyperparameters, to isolate the specific contribution of contrastive learning to the observed performance improvements.