---
ver: rpa2
title: 'Public Domain 12M: A Highly Aesthetic Image-Text Dataset with Novel Governance
  Mechanisms'
arxiv_id: '2410.23144'
source_url: https://arxiv.org/abs/2410.23144
tags:
- dataset
- arxiv
- public
- images
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Public Domain 12M (PD12M) is a 12.4 million image-text dataset
  composed entirely of public domain and CC0-licensed images, addressing copyright
  and licensing concerns that plague other large-scale datasets. The dataset was created
  by sourcing images from galleries, libraries, archives, museums, Wikimedia Commons,
  and iNaturalist, followed by careful curation including NSFW filtering, deduplication,
  and aesthetic scoring to match the size and quality of Conceptual Captions datasets.
---

# Public Domain 12M: A Highly Aesthetic Image-Text Dataset with Novel Governance Mechanisms

## Quick Facts
- arXiv ID: 2410.23144
- Source URL: https://arxiv.org/abs/2410.23144
- Reference count: 40
- 12.4 million image-text dataset composed entirely of public domain and CC0-licensed images

## Executive Summary
Public Domain 12M (PD12M) addresses copyright concerns in large-scale image-text datasets by sourcing exclusively from public domain and CC0-licensed materials. The dataset matches the size and quality of Conceptual Captions datasets while implementing community-driven governance mechanisms through the Source.Plus platform. This combination of clear licensing, rigorous curation, and ongoing governance makes PD12M a viable foundation for training commercial text-to-image models while minimizing copyright risks.

## Method Summary
The dataset was created through a multi-stage pipeline starting with 38M images from GLAM institutions, Wikimedia Commons, and iNaturalist. Images underwent automated filtering for document scans, NSFW content, and deduplication using CLIP embeddings. The bottom 50% of images by aesthetic score were discarded, and synthetic captions were generated using Florence-2-large. All images were archived to ensure stability, and the Source.Plus platform was developed to enable community governance through content flagging and responsive updates.

## Key Results
- 12.4 million image-caption pairs with clear public domain/CC0 licensing
- Matches size and quality of Conceptual Captions datasets for commercial AI model training
- Introduces novel community-driven governance mechanisms for ongoing dataset maintenance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clear public domain licensing enables commercial use without copyright risk
- Mechanism: By restricting dataset to only public domain/CC0 images, the dataset eliminates legal ambiguity around model training and deployment
- Core assumption: Public domain and CC0 licenses provide unambiguous rights for commercial AI model training
- Evidence anchors:
  - [abstract] "PD12M is the largest public domain image-text dataset to date, with sufficient size to train foundation models while minimizing copyright concerns"
  - [section] "At 12.4 million image-caption pairs, PD12M and its 3.3 million item subset, Public Domain 3M (PD3M), match the size of the Conceptual Captions datasets (CC12M and CC3M) [1], enabling commercial implementations of AI models"
- Break condition: If courts rule that AI model outputs require attribution even from public domain sources, or if new licensing frameworks create ambiguity

### Mechanism 2
- Claim: Community governance enables ongoing dataset improvement while maintaining stability
- Mechanism: The Source.Plus platform provides auditability, stabilization, and communication mechanisms that allow problematic content to be identified and replaced without disrupting dataset consistency
- Core assumption: Community-driven moderation can effectively identify and address dataset issues over time
- Evidence anchors:
  - [abstract] "Through the Source.Plus platform, we also introduce novel, community-driven dataset governance mechanisms that reduce harm and support reproducibility over time"
  - [section] "When an image requires replacement, we select an alternative from the ∼25 million additional public domain images in Source.Plus using quantitative measures, such as similarity between semantic and perceptual embeddings"
- Break condition: If community participation is insufficient to maintain dataset quality, or if governance mechanisms create too much friction for users

### Mechanism 3
- Claim: Image archiving ensures dataset stability despite web content volatility
- Mechanism: By storing downloaded copies of all images rather than relying on external URLs, the dataset maintains consistency even when original sources become unavailable
- Core assumption: Public domain status allows the maintainers to archive and distribute images without licensing conflicts
- Evidence anchors:
  - [section] "Archiving the images under the control of the dataset maintainers improves stability, and is made possible because the images in PD12M are understood to be in the public domain"
  - [section] "Pew Research has shown that ∼38% of webpages that existed in 2013 were no longer accessible a decade later [73]"
- Break condition: If public domain status is challenged for archived images, or if storage costs become prohibitive

## Foundational Learning

- Concept: Public domain vs. Creative Commons licensing
  - Why needed here: Understanding the difference between PD/CC0 and other CC licenses is critical for grasping why this dataset avoids copyright issues
  - Quick check question: What is the key difference between CC0 and licenses like CC-BY-SA that makes the latter problematic for AI training?

- Concept: Dataset governance mechanisms
  - Why needed here: The governance framework is a core innovation of this work, enabling ongoing dataset maintenance
  - Quick check question: How does the "stabilization" mechanism ensure that dataset updates don't break reproducibility?

- Concept: Image-text dataset curation pipeline
  - Why needed here: The multi-stage curation process (deduplication, filtering, aesthetic scoring) is essential to the dataset's quality
  - Quick check question: Why does the curation process exclude the bottom 50% of images by aesthetic score?

## Architecture Onboarding

- Component map: Source.Plus platform (front-end search/flagging interface + back-end moderation/replacement system) ↔ PD12M dataset (12.4M images + synthetic captions) ↔ Source archive (backup images for replacements)
- Critical path: Image collection → metadata extraction → initial filtering → CLIP embedding generation → document scan removal → format/resolution filtering → NSFW filtering → deduplication → aesthetic scoring → caption generation → platform upload
- Design tradeoffs: Size vs. quality (discarding 50% of images by aesthetic score), synthetic captions vs. authentic metadata (privacy vs. context), open governance vs. potential instability
- Failure signatures: Missing images (broken links), flagged content not being addressed, dataset growth stagnation, copyright disputes
- First 3 experiments:
  1. Verify dataset size and licensing by sampling 100 random images and checking their original source licenses
  2. Test the governance mechanism by attempting to flag a test image and observing the replacement process
  3. Evaluate dataset quality by training a small model and comparing performance to CC12M on a benchmark task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the aesthetic scoring model's preference for certain visual characteristics impact the diversity of images in PD12M compared to other datasets?
- Basis in paper: [explicit] The authors describe using an aesthetic scoring model to curate images, excluding the bottom 50% of scores, and provide example images from different percentile ranges in Table 2
- Why unresolved: The paper doesn't provide quantitative analysis of how this curation affected dataset diversity metrics or compare the aesthetic preferences to other datasets
- What evidence would resolve it: Comparative analysis of diversity metrics (color distribution, subject matter, composition types) between PD12M and other similarly sized datasets, showing how aesthetic scoring affected these distributions

### Open Question 2
- Question: What is the long-term sustainability of the Source.Plus governance platform given the ongoing costs of image hosting and moderation?
- Basis in paper: [explicit] The authors discuss image archiving costs and the need for ongoing governance, but don't address the financial model for sustaining these operations
- Why unresolved: The paper acknowledges the challenge of hosting costs but doesn't propose a concrete funding model or sustainability plan for the governance infrastructure
- What evidence would resolve it: Detailed financial projections and funding strategies for maintaining the platform, including hosting costs, moderation expenses, and revenue/cost recovery mechanisms

### Open Question 3
- Question: What is the impact of the two-week delay in image ingestion from Wikimedia Commons and iNaturalist on the dataset's coverage of recent cultural and scientific developments?
- Basis in paper: [explicit] The authors implemented a two-week delay between metadata parsing and image ingestion to allow community moderation
- Why unresolved: The paper doesn't analyze how this delay affects the dataset's ability to represent recent events, discoveries, or cultural trends
- What evidence would resolve it: Analysis comparing the temporal distribution of images in PD12M versus datasets without such delays, showing potential gaps in coverage of recent developments

## Limitations

- Public domain/CC0 licensing may introduce quality and diversity limitations compared to more permissive licenses
- Synthetic captions may be less contextually rich than those derived from original metadata
- Community governance model remains unproven at scale and may face challenges maintaining consistent quality

## Confidence

**High Confidence Claims:**
- The dataset contains 12.4 million image-caption pairs with clear public domain/CC0 licensing
- The curation pipeline successfully filtered images based on the stated criteria (NSFW, deduplication, aesthetic scoring)
- The Source.Plus platform provides basic governance functionality including flagging and replacement mechanisms

**Medium Confidence Claims:**
- The dataset's aesthetic quality matches that of Conceptual Captions datasets
- The governance mechanisms effectively reduce harm and support reproducibility over time
- The copyright protections provided by public domain/CC0 licensing are sufficient for commercial AI model training

**Low Confidence Claims:**
- The dataset's long-term stability under community governance
- The synthetic captions' quality and informativeness compared to authentic metadata
- The scalability of the governance model as dataset size increases

## Next Checks

1. **Copyright Verification Audit**: Conduct a systematic audit of 1,000 randomly sampled images from PD12M, verifying their original source licenses and checking for any licensing discrepancies or potential copyright violations.

2. **Governance Mechanism Testing**: Simulate real-world governance scenarios by deliberately introducing problematic content, attempting to flag it through the platform, and measuring the time-to-resolution and effectiveness of the replacement mechanism.

3. **Quality Benchmarking**: Train identical text-to-image models on PD12M and CC12M, then evaluate and compare their performance across multiple metrics including aesthetic quality, diversity, and task-specific capabilities to validate the claim that PD12M matches Conceptual Captions datasets in quality.