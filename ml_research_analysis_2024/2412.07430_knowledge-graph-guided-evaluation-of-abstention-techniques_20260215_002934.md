---
ver: rpa2
title: Knowledge Graph Guided Evaluation of Abstention Techniques
arxiv_id: '2412.07430'
source_url: https://arxiv.org/abs/2412.07430
tags:
- abstention
- concepts
- techniques
- concept
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates how different abstention techniques (prompting,
  activation steering, fine-tuning) affect language model behavior on benign topics
  isolated from safety training. SELECT, a benchmark based on knowledge graph-derived
  concepts, allows measurement of effectiveness, generalization, and specificity of
  abstention.
---

# Knowledge Graph Guided Evaluation of Abstention Techniques

## Quick Facts
- arXiv ID: 2412.07430
- Source URL: https://arxiv.org/abs/2412.07430
- Authors: Kinshuk Vasisht; Navreet Kaur; Danish Pruthi
- Reference count: 24
- Key outcome: Over 80% abstention rates on target concepts, but 19% drops for descendants, indicating limited generalization

## Executive Summary
This work introduces SELECT, a benchmark based on knowledge graph-derived concepts, to systematically evaluate how different abstention techniques affect language model behavior on benign topics isolated from safety training. The study examines five abstention techniques (prompting, activation steering, fine-tuning) across multiple models, measuring effectiveness, generalization to descendant concepts, and specificity (avoiding over-refusal of sibling/ancestor concepts). Results show that while all techniques achieve high abstention rates on target concepts, they exhibit varying trade-offs between generalization and specificity, with inference-based methods initially more effective but degrading faster for descendants.

## Method Summary
The authors built SELECT, a benchmark derived from YAGO 4.5 knowledge graph, containing 394 atomic concepts and 156 composite concepts with 11,820 evaluation questions. Five abstention techniques were tested: zero-shot prompting, few-shot chain-of-thought prompting, conditional activation steering, supervised fine-tuning (SFT), and SFT + Direct Preference Optimization (DPO). The techniques were applied to various language models (LLaMA-3.1, Gemma-2, Mistral 7B, GPT-3.5-Turbo, GPT-4o) for target concepts, and responses were evaluated for abstention rates, generalization (refusal of descendant concepts), and specificity (non-refusal of sibling/ancestor concepts).

## Key Results
- Abstention techniques achieve over 80% refusal rates on target concepts across all tested methods
- Inference-based methods (prompting, steering) initially outperform fine-tuning by ~10% but show 21-26% lower generalization to descendants
- Abstention rates increase by 7% on average for narrower concepts at deeper taxonomy levels, while specificity decreases with depth

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abstention techniques cause models to abstain from responding to questions about benign concepts with over 80% effectiveness.
- Mechanism: By instructing or fine-tuning models to refuse certain concepts, the model learns to associate those concepts with abstention behavior. The knowledge graph grounding ensures that abstention can be tested systematically across related concepts.
- Core assumption: The abstention behavior learned for a concept will generalize to descendant concepts but not to ancestor or sibling concepts.
- Evidence anchors:
  - [abstract] "We find that the examined techniques indeed cause models to abstain with over 80% abstention rates."
  - [section] "Abstention techniques are effective at enforcing abstention for benign concepts in SELECT, with refusal rates mostly above 80%."
  - [corpus] Found 25 related papers with average neighbor FMR=0.498, suggesting moderate relevance to the topic of abstention techniques.
- Break condition: If the model does not learn the hierarchical relationships between concepts, generalization to descendants may fail, and specificity may be compromised.

### Mechanism 2
- Claim: Different abstention techniques trade off between generalization and specificity in varied ways.
- Mechanism: Inference-based methods like prompting and activation steering achieve higher initial abstention rates but degrade faster for descendants, while fine-tuning methods show more consistent performance across concept levels.
- Core assumption: The nature of the abstention technique (inference-based vs fine-tuning) determines the generalization-specificity trade-off.
- Evidence anchors:
  - [abstract] "We also characterize the generalization-vs-specificity trade-offs for different techniques."
  - [section] "Inference-based methods generally outperform fine-tuning by 10% on average, but some techniques are not as effective for certain models."
  - [corpus] Papers like "KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning" suggest that knowledge-driven approaches may influence generalization.
- Break condition: If the model is not exposed to sufficient examples of both the target concept and its descendants during training, generalization may fail.

### Mechanism 3
- Claim: The depth of a concept in the taxonomy affects the effectiveness, generalization, and specificity of abstention techniques.
- Mechanism: Abstention rates generally increase for narrower concepts at deeper levels, while specificity decreases with increasing depth, indicating over-refusal for finer-grained concepts.
- Core assumption: The model's understanding of concept hierarchy and the nature of the abstention technique determine how well it abstains at different levels.
- Evidence anchors:
  - [section] "Abstention rates generally increase for narrower concepts, with 7% increase on average when comparing the rates across levels one and five."
  - [section] "While abstention rates and generalization improve, specificity decreases with increase in depth."
  - [corpus] The corpus includes papers on "Characterizing LLM Abstention Behavior in Science QA with Context Perturbations," suggesting that context and hierarchy influence abstention.
- Break condition: If the model lacks knowledge of the hierarchical relationships between concepts, it may fail to generalize or may over-refuse at deeper levels.

## Foundational Learning

- Concept: Knowledge Graphs
  - Why needed here: Knowledge graphs provide a structured way to represent concepts and their relationships, which is essential for evaluating how well abstention techniques generalize across related concepts.
  - Quick check question: How do knowledge graphs help in evaluating the generalization and specificity of abstention techniques?

- Concept: Supervised Fine-Tuning (SFT)
  - Why needed here: SFT is a fine-tuning method used to align model outputs with human preferences, including abstention from certain topics.
  - Quick check question: What is the difference between SFT and other fine-tuning methods like DPO?

- Concept: Activation Steering
  - Why needed here: Activation steering is an inference-based method that modifies model activations to influence behavior, such as causing selective abstention.
  - Quick check question: How does activation steering differ from prompting in terms of causing abstention?

## Architecture Onboarding

- Component map:
  Knowledge Graph (YAGO 4.5) -> SELECT Benchmark (atomic/composite concepts) -> Abstention Techniques (prompting, steering, fine-tuning) -> Evaluation Metrics (abstention rate, generalization, specificity)

- Critical path:
  1. Build the knowledge graph taxonomy
  2. Create the SELECT benchmark with atomic and composite concepts
  3. Apply abstention techniques to language models
  4. Evaluate the effectiveness, generalization, and specificity of the techniques

- Design tradeoffs:
  - Inference-based methods (prompting, steering) are faster but may not generalize as well as fine-tuning
  - Fine-tuning is more compute-intensive but provides better generalization
  - Knowledge graph grounding ensures systematic evaluation but may limit the scope of concepts

- Failure signatures:
  - Low abstention rates: The technique is not effective at causing the model to abstain
  - Poor generalization: The model does not abstain for descendant concepts
  - Low specificity: The model abstains for ancestor or sibling concepts, indicating over-refusal

- First 3 experiments:
  1. Evaluate the effectiveness of prompting on a simple concept like "rivers" using the SELECT benchmark
  2. Test the generalization of activation steering for a concept and its descendants
  3. Compare the specificity of fine-tuning vs. prompting for a concept and its siblings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do inference-based abstention techniques (prompting and activation steering) show a 21-26% drop in generalization effectiveness compared to fine-tuning methods, despite achieving higher initial abstention rates?
- Basis in paper: [explicit] The paper states that inference-based methods have 21-26% lower generalization rates than fine-tuning methods.
- Why unresolved: The paper identifies that only 35% of generalization errors can be explained by the model's understanding of hierarchical relations between concepts, suggesting other factors are at play.
- What evidence would resolve it: Experiments comparing generalization performance across techniques while controlling for concept familiarity, relational knowledge encoding, and model architecture differences.

### Open Question 2
- Question: What specific factors in fine-tuning methods lead to better generalization across descendant concepts compared to inference-based methods?
- Basis in paper: [explicit] Fine-tuning methods show only 7% drop in generalization compared to 21-26% for inference-based methods.
- Why unresolved: The paper does not investigate the internal mechanisms that make fine-tuning more robust to concept hierarchy generalization.
- What evidence would resolve it: Detailed analysis of model activations and learned representations after fine-tuning versus inference-based methods, particularly focusing on how concept relationships are encoded.

### Open Question 3
- Question: How do abstention techniques perform on adversarial queries that attempt to bypass the safety mechanisms, such as jailbreak attempts or context manipulation?
- Basis in paper: [inferred] The paper explicitly states in the limitations section that evaluations lack testing against adversarial perturbations and personalized contexts.
- Why unresolved: The current evaluation focuses only on benign concepts and straightforward queries without testing robustness against known attack vectors.
- What evidence would resolve it: Empirical testing of abstention techniques against established adversarial attack datasets and methods specifically designed to circumvent safety mechanisms.

## Limitations

- The evaluation lacks testing against adversarial perturbations and personalized contexts that could bypass safety mechanisms
- The exact implementation details of conditional activation steering are not fully specified, creating uncertainty in technique comparisons
- The YAGO 4.5 knowledge graph may contain outdated information or incomplete concept relationships, potentially limiting the benchmark's representativeness

## Confidence

- Abstention effectiveness claims (>80% rates): High confidence
- Generalization vs. specificity trade-offs: Medium confidence
- Fine-tuning vs. inference-based method comparisons: Medium confidence

## Next Checks

1. Replicate the conditional activation steering implementation using the described methodology and compare results with baseline techniques on a small subset of SELECT concepts.

2. Manually audit 50 random responses from each abstention technique to validate the phrase-matching and length-based heuristic approach for detecting abstention.

3. Test generalization performance on a held-out set of descendant concepts not present in the original training or fine-tuning data to verify true generalization capability.