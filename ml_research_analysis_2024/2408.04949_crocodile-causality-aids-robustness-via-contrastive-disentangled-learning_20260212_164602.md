---
ver: rpa2
title: 'CROCODILE: Causality aids RObustness via COntrastive DIsentangled LEarning'
arxiv_id: '2408.04949'
source_url: https://arxiv.org/abs/2408.04949
tags:
- learning
- domain
- features
- causal
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CROCODILE, a framework leveraging causal
  inference to improve the robustness of deep learning models for medical image classification,
  specifically for lung disease detection in chest X-rays (CXRs). The method addresses
  the problem of domain shift, where models trained on one hospital's data perform
  poorly on data from other hospitals due to variations in scanner settings or patient
  characteristics.
---

# CROCODILE: Causality aids RObustness via COntrastive DIsentangled LEarning

## Quick Facts
- arXiv ID: 2408.04949
- Source URL: https://arxiv.org/abs/2408.04949
- Reference count: 29
- This paper introduces CROCODILE, a framework leveraging causal inference to improve the robustness of deep learning models for medical image classification, specifically for lung disease detection in chest X-rays (CXRs).

## Executive Summary
This paper introduces CROCODILE, a framework leveraging causal inference to improve the robustness of deep learning models for medical image classification, specifically for lung disease detection in chest X-rays (CXRs). The method addresses the problem of domain shift, where models trained on one hospital's data perform poorly on data from other hospitals due to variations in scanner settings or patient characteristics. CROCODILE uses feature disentanglement, contrastive learning, and prior knowledge injection to learn invariant features and reduce reliance on spurious correlations. The model is trained on four large CXR datasets (over 750,000 images) and tested on an unseen dataset. It achieves better domain generalization and fairness compared to baseline methods, particularly in out-of-distribution settings, while maintaining competitive in-distribution performance.

## Method Summary
CROCODILE is a framework for robust medical image classification that uses feature disentanglement, contrastive learning, and causal inference to address domain shift. The method trains a ResNet50 backbone with modified Transformer layers to separate causal features (disease indicators) from spurious features (domain-specific artifacts). It employs latent causal intervention via intra-batch shuffling, relational scoring for cross-branch alignment, and task-prior loss to inject medical knowledge about CXR finding co-occurrences. The model is trained on four large CXR datasets using Adam optimizer (lr=1e-6, batch size=12) with early stopping, and evaluated on both in-distribution and out-of-distribution data.

## Key Results
- CROCODILE achieves better domain generalization and fairness compared to baseline methods, particularly in out-of-distribution settings.
- The model maintains competitive in-distribution performance while improving robustness to domain shift.
- CROCODILE is trained on four large CXR datasets (over 750,000 images) and tested on an unseen dataset.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The backdoor adjustment via latent causal intervention removes spurious correlations from domain shift by shuffling spurious features and recombining with causal features.
- Mechanism: Intra-batch shuffling of Qsp (spurious features) followed by random sampling (0.3 drop probability) and addition to Qca (causal features) approximates the do-calculus operation P(Y|do(Fca)), effectively cutting the backdoor path through spurious features.
- Core assumption: The shuffled spurious features approximate a distribution where the confounding effect is removed, and the causal features remain invariant across domains.
- Evidence anchors:
  - [abstract] "CROCODILE framework, showing how tools from causality can foster a model's robustness to domain shift via feature disentanglement, contrastive learning losses, and the injection of prior knowledge."
  - [section] "we implement the backdoor adjustment by performing a latent causal intervention [21,12]: we stratify the spurious features appearing from training data and pair the causal set of features with those stratified spurious features to compose the intervened graph."
- Break condition: If the spurious features contain any domain-invariant causal information, shuffling would remove useful signal along with spurious correlation.

### Mechanism 2
- Claim: The Relational Scorer module learns to distinguish semantically related vs. unrelated feature pairs across causal and spurious feature spaces, enforcing cross-branch alignment.
- Mechanism: RS module stratifies and combines each possible cross-branch pairing (Qca_y × Qca_d, Qca_y × Qsp_d, Qsp_y × Qsp_d, Qsp_y × Qca_d) and maps them to relational scores (0-1) using MSE loss against ground truth (1 for matched pairs like Qca_y-Qsp_d, 0 otherwise).
- Core assumption: The semantic relationship between causal features for disease and spurious features for domain (and vice versa) should be learned through comparison rather than structural similarity.
- Evidence anchors:
  - [abstract] "the model relies less on spurious correlations, learns the mechanism bringing from images to prediction better"
  - [section] "we design a new module named Relational Scorer (RS) to learn which image representations' pairings are semantically related and which are not"
- Break condition: If the ground truth pairing assignments (which pairs get score 1 vs 0) are incorrect, the model would learn wrong relationships.

### Mechanism 3
- Claim: The Task-Prior loss injects medical knowledge about CXR finding co-occurrence patterns to guide the model's learning of causal relationships.
- Mechanism: Compute causality map Cy from learned representations using ratio of joint to marginal probabilities P(Qi|Qj) = P(Qi,Qj)/P(Qj), then apply MSE loss to push Cy toward ground-truth causality map C_GT_y.
- Core assumption: The asymmetries in conditional probabilities across embeddings indicate causal relationships between CXR findings that can be captured and used for regularization.
- Evidence anchors:
  - [abstract] "we propose a new way to inject background medical knowledge, effectively designing a task prior to guiding learning and fostering DG"
  - [section] "we propose a new method to inject prior (medical) knowledge into the model to guide its learning"
- Break condition: If the ground-truth causality map is inaccurate or incomplete, the model would be guided toward incorrect relationships.

## Foundational Learning

- Concept: Domain shift and spurious correlations
  - Why needed here: Understanding why models fail on OOD data requires grasping how spurious correlations (like hospital-specific artifacts) differ from causal features (actual disease indicators)
  - Quick check question: Why would a model trained on Hospital A's CXRs perform poorly on Hospital B's CXRs even if the diseases are the same?

- Concept: Causal inference and do-calculus
  - Why needed here: The backdoor adjustment and latent causal intervention rely on causal reasoning to remove confounding effects from spurious features
  - Quick check question: What does the do-calculus operation P(Y|do(Fca)) represent compared to the observational P(Y|Fca)?

- Concept: Contrastive learning and metric learning
  - Why needed here: The batch-level contrastive losses (L=y, L≠y, L=d, L≠d) require understanding how to push similar samples together and dissimilar samples apart in feature space
  - Quick check question: How does the contrastive loss term L=y ensure that samples with the same disease label are close in causal feature space regardless of domain?

## Architecture Onboarding

- Component map:
  - Backbone (ResNet50) → Channel & Spatial Attention → Modified Transformer (Qca, Qsp) → Disease Branch Classifiers + Domain Branch Classifiers
  - Additional modules: Relational Scorer (RS), Task-Prior loss calculator, Custom sampler
  - Loss components: LCE,y, LKL,y, Lbd_CE,y, LCE,d, LKL,d, Lbd_CE,d, LRS, Lbatch_y, Lbatch_d, Lprior_y

- Critical path: Feature extraction → Attention enhancement → Transformer disentanglement → Classifier predictions → Loss computation → Backpropagation
- Design tradeoffs: The modified Transformer must balance attention scores (A) and their complement (1-A) for disentanglement; the custom sampler must maintain class prevalence while enabling contrastive learning
- Failure signatures: Poor OOD performance despite good ID performance suggests spurious correlation reliance; inconsistent domain predictions suggest attention disentanglement failure; high ID-OOD gap suggests need for stronger causal intervention
- First 3 experiments:
  1. Train with only disease branch (remove domain branch) to verify baseline performance degradation on OOD data
  2. Train with domain branch but without relational scorer to measure impact of cross-branch alignment
  3. Train with all components but without task-prior loss to assess value of medical knowledge injection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's performance degrade when training on datasets with significant class imbalance, and what mitigation strategies could be employed?
- Basis in paper: [inferred] The paper mentions using a custom sampler favoring consistent batches, but does not explicitly address class imbalance or its impact on performance.
- Why unresolved: The paper does not provide data on class imbalance or explore strategies to address it, leaving the potential impact on model performance unclear.
- What evidence would resolve it: Experiments showing performance degradation with varying degrees of class imbalance and results from applying different imbalance mitigation techniques.

### Open Question 2
- Question: How does the model's performance generalize to other medical imaging modalities beyond chest X-rays, such as CT scans or MRIs?
- Basis in paper: [inferred] The paper focuses on chest X-ray image classification and does not explore the model's applicability to other imaging modalities.
- Why unresolved: The model's architecture and training process are tailored to chest X-rays, and its performance on other modalities remains untested.
- What evidence would resolve it: Experiments applying the model to other medical imaging datasets and comparing performance to modality-specific models.

### Open Question 3
- Question: What is the computational cost of the model, and how does it scale with dataset size and model complexity?
- Basis in paper: [inferred] The paper mentions training on a GPU with 64 GB memory but does not provide detailed information on computational requirements or scalability.
- Why unresolved: The paper lacks quantitative data on training time, memory usage, or how these factors change with different dataset sizes or model configurations.
- What evidence would resolve it: Comprehensive benchmarking of computational resources required for training and inference across various dataset sizes and model architectures.

## Limitations

- The causal disentanglement mechanism relies heavily on the assumption that spurious features can be cleanly separated from causal features through attention-based stratification.
- The latent causal intervention via intra-batch shuffling is an approximation that may not fully eliminate confounding effects.
- The task-prior loss depends on ground-truth causality maps that may not capture all relevant medical relationships between CXR findings.

## Confidence

- **High confidence**: Domain generalization performance improvements over baselines on unseen MIMIC-CXR dataset (measured by AUC/AP metrics)
- **Medium confidence**: Effectiveness of latent causal intervention for backdoor adjustment, given the approximation through shuffling
- **Medium confidence**: Value of task-prior loss for injecting medical knowledge, assuming ground-truth causality maps are accurate
- **Low confidence**: The modified Transformer's cross-attention mechanism and how 1-A complementary attention scores are computed

## Next Checks

1. **Ablation study**: Remove the latent causal intervention and retrain to quantify its specific contribution to OOD performance gains.
2. **Sensitivity analysis**: Test the model's robustness to different shuffling probabilities (0.1, 0.3, 0.5) in the latent causal intervention to find optimal settings.
3. **Ground truth validation**: Verify the accuracy of the causality map C_GT_y by consulting domain experts and assessing its completeness for the 8 lung diseases studied.