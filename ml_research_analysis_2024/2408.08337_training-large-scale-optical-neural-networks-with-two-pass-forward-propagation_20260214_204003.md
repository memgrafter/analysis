---
ver: rpa2
title: Training Large-Scale Optical Neural Networks with Two-Pass Forward Propagation
arxiv_id: '2408.08337'
source_url: https://arxiv.org/abs/2408.08337
tags:
- optical
- neural
- training
- networks
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to train optical neural networks
  (ONNs) without requiring explicit nonlinear activation functions. The method, called
  Two-Pass Forward Propagation, introduces a second forward pass with modulated error
  signals that are fed back as inputs to the network, inspired by biological error-driven
  modulation.
---

# Training Large-Scale Optical Neural Networks with Two-Pass Forward Propagation
## Quick Facts
- arXiv ID: 2408.08337
- Source URL: https://arxiv.org/abs/2408.08337
- Authors: Amirreza Ahmadnejad; Somayyeh Koohi
- Reference count: 17
- This paper proposes a novel approach to train optical neural networks (ONNs) without requiring explicit nonlinear activation functions.

## Executive Summary
This paper introduces Two-Pass Forward Propagation, a novel training method for optical neural networks that eliminates the need for explicit nonlinear activation functions. The approach leverages a second forward pass with modulated error signals fed back as inputs, inspired by biological error-driven modulation. The authors demonstrate that this method can effectively train both simple ONNs (such as an XOR gate) and large-scale integrated photonic circuits, achieving competitive performance on MNIST classification tasks.

## Method Summary
The Two-Pass Forward Propagation method works by introducing a second forward pass where error signals are modulated and fed back as inputs to the network. This error-driven modulation is inspired by biological neural systems where feedback pathways adjust synaptic weights. For convolutional neural networks, the authors propose splitting input images into columns, processing each column with a simple neural network, and combining the results. This approach enables processing of real-size images in integrated ONNs without requiring complex nonlinear optical units.

## Key Results
- The Two-Pass Forward Propagation method effectively trains simple ONNs, demonstrated by successful XOR gate implementation
- CNN implementation through column-wise processing achieves 98.6% accuracy on MNIST, competitive with existing ONN models
- The approach eliminates the need for complex nonlinear optical units in ONN architectures

## Why This Works (Mechanism)
The method works by exploiting the linearity of optical components and using error-driven modulation as a proxy for nonlinearity. In the first forward pass, the network processes inputs normally. In the second pass, error signals are generated and modulated, then fed back as inputs to adjust the network's behavior. This mimics biological neural systems where error signals propagate backward to modulate synaptic strengths. The column-wise CNN implementation leverages the parallel nature of optical processing, where each column can be processed simultaneously, reducing latency and complexity compared to spatial convolution.

## Foundational Learning
1. **Optical Neural Networks (ONNs)** - Why needed: Understanding the fundamental architecture and limitations of optical computing for neural networks
   - Quick check: Verify that the reader understands how light interference and interference patterns can represent computational states

2. **Error-Driven Modulation** - Why needed: The biological inspiration for the two-pass approach
   - Quick check: Confirm understanding of how error signals in biological systems modulate synaptic weights

3. **Integrated Photonic Circuits** - Why needed: The physical implementation platform for large-scale ONNs
   - Quick check: Ensure comprehension of how photonic components (waveguides, modulators, detectors) can be integrated on a chip

4. **Convolutional Neural Networks** - Why needed: Understanding traditional CNN operations to appreciate the column-wise implementation
   - Quick check: Verify knowledge of spatial convolution operations and their computational complexity

## Architecture Onboarding
**Component Map:** Input Image -> Column Splitter -> Multiple Simple Neural Networks -> Column Combiner -> Output
**Critical Path:** Image splitting and parallel processing through simple neural networks is the core innovation, enabling efficient CNN implementation in ONNs
**Design Tradeoffs:** The column-wise approach trades spatial correlation awareness for computational simplicity and parallelism
**Failure Signatures:** Loss of spatial feature relationships across columns, potential artifacts at column boundaries, and reduced performance on spatially correlated datasets
**3 First Experiments:**
1. Implement the XOR gate training to verify the basic Two-Pass Forward Propagation mechanism
2. Test the column-wise CNN on a small dataset (e.g., Fashion-MNIST) to validate the approach beyond MNIST
3. Simulate a simple integrated photonic circuit with realistic noise models to assess physical implementation feasibility

## Open Questions the Paper Calls Out
None

## Limitations
- The biological inspiration for error modulation lacks detailed mechanistic explanation for how optical signals can implement the feedback pathway effectively
- The CNN implementation through column-wise processing may introduce artifacts or inefficiencies not captured in MNIST experiments, particularly for spatially correlated features
- Numerical simulations, while showing competitive MNIST performance, do not address potential physical implementation challenges such as noise, loss, or fabrication variations in integrated photonic circuits

## Confidence
- Two-Pass Forward Propagation mechanism: Medium - Conceptually sound but biological implementation details are sparse
- MNIST performance claims: Medium - Results are promising but limited to a single benchmark
- CNN implementation approach: Low - Novel method with potential limitations not fully explored
- Elimination of nonlinear optical units: Medium - Valid for demonstrated cases but not extensively validated

## Next Checks
1. Implement and test the Two-Pass Forward Propagation on CIFAR-10 or ImageNet to assess scalability and robustness to more complex datasets
2. Build a small-scale physical prototype of the proposed ONN architecture to validate numerical simulation assumptions about noise, loss, and crosstalk
3. Compare the column-wise CNN implementation against traditional spatial convolution in ONNs to quantify any efficiency gains or feature loss