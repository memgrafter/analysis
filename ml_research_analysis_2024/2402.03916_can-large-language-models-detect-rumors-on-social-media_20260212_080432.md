---
ver: rpa2
title: Can Large Language Models Detect Rumors on Social Media?
arxiv_id: '2402.03916'
source_url: https://arxiv.org/abs/2402.03916
tags:
- news
- comments
- fake
- real
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using Large Language Models (LLMs) for rumor
  detection on social media. The challenge is that LLMs struggle with the complex
  and voluminous nature of propagation information, which includes both news content
  and numerous comments.
---

# Can Large Language Models Detect Rumors on Social Media?

## Quick Facts
- arXiv ID: 2402.03916
- Source URL: https://arxiv.org/abs/2402.03916
- Authors: Qiang Liu; Xiang Tao; Junfei Wu; Shu Wu; Liang Wang
- Reference count: 6
- Primary result: LeRuD achieves 94.01% accuracy on Twitter and 98.09% on Weibo rumor detection without training data

## Executive Summary
This paper proposes LeRuD, a novel approach for rumor detection using Large Language Models (LLMs) that requires no training data. The method addresses the challenge of LLMs handling complex, voluminous propagation information by dividing it into a Chain-of-Propagation and using carefully designed prompts to focus on writing styles and comment conflicts. Experiments show LeRuD outperforms state-of-the-art models by 3.2% to 7.7% on Twitter and Weibo datasets.

## Method Summary
LeRuD employs two types of prompts - Rational Prompts to analyze news writing style and commonsense, and Conflicting Prompts to examine comment conflicts - applied through a Chain-of-Propagation approach. The entire propagation information (news content plus comments) is split into sequential steps with a fixed number of comments each, allowing LLMs to reason step-by-step. The method uses GPT-3.5 as the LLM executor and requires no training data, making it effective in few-shot or zero-shot scenarios.

## Key Results
- Achieves 94.01% accuracy on Twitter dataset and 98.09% on Weibo dataset
- Outperforms state-of-the-art rumor detection models by 3.2% to 7.7%
- Demonstrates effectiveness in zero-shot learning scenarios without training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dividing propagation information into a Chain-of-Propagation enables LLMs to reason step-by-step with manageable information.
- Mechanism: The entire propagation data is split into sequential steps, each containing a fixed number of comments. LLMs process each step in sequence, using the output of earlier steps as context for later ones.
- Core assumption: LLMs struggle with long contexts but can effectively reason over shorter, focused segments.
- Evidence anchors:
  - [abstract]: "we divide the entire propagation information into a Chain-of-Propagation, which enables LLMs to reason step-by-step with a manageable number of comments."
  - [section]: "we propose to divide the proposition information into a Chain-of-Propagation...enables LLMs to conduct reasoning more easily."
- Break condition: If comment distribution is highly skewed (e.g., massive burst early, then silence), step boundaries may miss critical information or create uneven workload.

### Mechanism 2
- Claim: Rational Prompts guide LLMs to focus on writing style and commonsense cues in news content.
- Mechanism: Prompt instructs LLM to analyze news based on "writing style and the commonsense knowledge" rather than direct veracity judgment, leading it to notice implausible details or poor structure.
- Core assumption: LLMs can extract subtle textual cues when explicitly prompted to look for them.
- Evidence anchors:
  - [abstract]: "we design prompts to teach LLMs to concentrate on the writing styles and commonsense mistakes of news."
  - [section]: "Accordingly, we design Rational Prompts...ask the LLM to answer 'Based on the writing style and the commonsense knowledge...'"
- Break condition: If news is well-written and factually plausible (e.g., real but misleading), LLM may still fail without factual knowledge.

### Mechanism 3
- Claim: Conflicting Prompts direct LLMs to look for rebuttals or conflicts in comments to judge news credibility.
- Mechanism: Prompt explicitly asks LLM to "analyze whether there are any rebuttals or conflicts" in comments, shifting focus from absence of evidence to presence of dissent.
- Core assumption: Crowd wisdom in comments (skepticism, rebuttals) is a stronger signal than neutral or supportive comments.
- Evidence anchors:
  - [abstract]: "we design prompts to teach LLMs to concentrate on...rebuttals or conflicts in comments."
  - [section]: "Accordingly, we design Conflicting Prompts...ask the LLM to answer 'Based on the comments, analyze whether there are any rebuttals or conflicts...'"
- Break condition: If comments are uniformly skeptical or uniformly supportive, the signal becomes weak.

## Foundational Learning

- Concept: Zero-shot vs Few-shot vs Supervised learning
  - Why needed here: LeRuD works in zero-shot, so engineer must understand how it differs from baseline models that require training data.
  - Quick check question: What happens to LeRuD's performance if you give it a small labeled dataset? (It would still perform without it, but you could fine-tune for marginal gains.)

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: LeRuD applies CoT-style decomposition to rumor detection, so understanding the pattern helps debug reasoning failures.
  - Quick check question: If CoP step size (k) is increased too much, what failure mode emerges? (LLM reasoning degrades due to long-context overload.)

- Concept: Prompt engineering for task decomposition
  - Why needed here: LeRuD's effectiveness hinges on carefully crafted prompts; engineer must know how to design them.
  - Quick check question: What happens if you replace "Rational Prompts" with a generic "Is this news real?" prompt? (LLM fails to extract subtle cues, accuracy drops.)

## Architecture Onboarding

- Component map: Input pipeline -> Chain-of-Propagation splitter -> LLM executor (GPT 3.5) -> Aggregation -> Output label
- Critical path: Splitting propagation info -> Stepwise LLM inference -> Final label from last step
- Design tradeoffs: 
  - CoP step size vs reasoning accuracy: larger steps = faster but less accurate; smaller = slower but more accurate
  - Prompt specificity vs generalization: very specific prompts work well on tested data but may fail on unseen patterns
- Failure signatures:
  - Accuracy drops sharply when comment count exceeds LLM input limits -> CoP step size too large
  - LLM outputs generic "I don't know" -> prompts too vague or dataset has strong factual knowledge leakage
  - Low precision but high recall -> Conflicting Prompts too aggressive in labeling as fake
- First 3 experiments:
  1. Vary CoP step size (k=50, 100, 200) and measure accuracy drop.
  2. Remove Rational Prompts, keep others -> observe writing-style reasoning loss.
  3. Remove Conflicting Prompts, keep others -> observe precision/recall shift.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may not generalize to platforms beyond Twitter and Weibo
- Chain-of-Propagation approach may miss crucial information if signals are split across steps
- Zero-shot nature relies heavily on LLM's pre-existing knowledge, which may vary across models

## Confidence

**High Confidence** (supported by direct evidence in paper):
- LeRuD achieves state-of-the-art performance on tested datasets (94.01% on Twitter, 98.09% on Weibo)
- The zero-shot capability eliminates the need for training data
- The Chain-of-Propagation approach enables step-by-step reasoning

**Medium Confidence** (reasonable but with gaps):
- The three mechanisms (rational prompts, conflicting prompts, CoP) are sufficient for general rumor detection
- Performance gains of 3.2% to 7.7% over baselines are consistent across different rumor types
- The approach generalizes to platforms beyond Twitter and Weibo

**Low Confidence** (significant uncertainties):
- LeRuD maintains effectiveness with very large propagation threads (thousands of comments)
- The approach performs equally well on breaking news versus ongoing discussions
- The method handles multimodal content (images, videos) effectively

## Next Checks
1. **Cross-platform validation**: Test LeRuD on Reddit, Facebook, and emerging platforms with different community dynamics to verify generalization claims.

2. **Scalability stress test**: Evaluate performance with artificially expanded propagation threads (10x-100x comment volume) to identify CoP step size limits and accuracy degradation patterns.

3. **Ablation study with real-time data**: Deploy LeRuD on a streaming dataset of breaking news to measure performance on time-sensitive rumor detection versus trained models.