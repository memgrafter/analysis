---
ver: rpa2
title: 'Weighted Risk Invariance: Domain Generalization under Invariant Feature Shift'
arxiv_id: '2407.18428'
source_url: https://arxiv.org/abs/2407.18428
tags:
- xinv
- invariant
- learning
- density
- xspu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Weighted Risk Invariance (WRI) as a method
  to learn models invariant to distribution shifts across environments. Existing invariant
  learning methods struggle when the invariant features undergo covariate shift, particularly
  in heteroskedastic settings.
---

# Weighted Risk Invariance: Domain Generalization under Invariant Feature Shift

## Quick Facts
- arXiv ID: 2407.18428
- Source URL: https://arxiv.org/abs/2407.18428
- Authors: Gina Wong; Joshua Gleason; Rama Chellappa; Yoav Wald; Anqi Liu
- Reference count: 40
- Key outcome: Introduces Weighted Risk Invariance (WRI) to address invariant learning under invariant covariate shift, outperforming existing methods on ColoredMNIST and DomainBed benchmarks.

## Executive Summary
This paper addresses a critical limitation in domain generalization: existing invariant learning methods fail when invariant features themselves undergo covariate shift across environments. The authors propose Weighted Risk Invariance (WRI), which enforces invariance of reweighted losses across environments. WRI provably learns invariant models in linear-Gaussian settings and uses alternating minimization to simultaneously learn density estimates and predictor parameters. The method shows strong empirical performance on both synthetic and real-world datasets while providing useful signals for out-of-distribution detection.

## Method Summary
WRI learns invariant predictors by enforcing invariance of weighted risks across environments. Unlike importance weighting, it uses density weighting based on invariant feature distributions, avoiding support overlap limitations. The method alternates between updating environment-specific density models and the predictor model. This approach provably recovers spurious-free predictors in linear-Gaussian settings under a general position assumption. The learned densities also serve as useful signals for detecting distribution shifts in invariant features.

## Key Results
- WRI outperforms VREx and other baselines on DomainBed benchmarks (VLCS, PACS, OfficeHome, TerraIncognita, DomainNet) with higher average accuracy
- Achieves 85.7% accuracy on ColoredMNIST versus 81.7% for VREx under heteroskedastic noise conditions
- Provides effective out-of-distribution detection with AUROC scores significantly above random chance on test datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted risk invariance with density weighting provably recovers spurious-free predictors under linear-Gaussian settings.
- Mechanism: By enforcing invariance of reweighted losses across environments, models satisfying this constraint must set spurious feature weights to zero, as shown in Theorem 1.
- Core assumption: Environments are in general position (defined in Appendix A), ensuring non-degenerate solution space.
- Evidence anchors:
  - [abstract] "We show that WRI provably learns invariant models, i.e. discards spurious correlations, in linear-Gaussian settings."
  - [section] "Theorem 1... A linear regression modelf(x) = wT x with w = [winv, wspu] that satisfies weighted risk invariance w.r.t the squared loss must also satisfy wspu = 0."
  - [corpus] Weak - related works focus on invariance but not specifically on weighted risk formulation.
- Break condition: General position assumption violated (k choose 2 ≤ 2dspu), or weighting functions become degenerate (e.g. α(xinv) = β(xinv) = 0).

### Mechanism 2
- Claim: Density weighting avoids support overlap limitations of importance weighting while maintaining effective sample complexity.
- Mechanism: Using invariant feature densities as weights ensures non-zero weights across all regions of support overlap, unlike importance weighting which requires numerator support ⊆ denominator support.
- Core assumption: Sufficient overlap exists in invariant feature distributions across environments.
- Evidence anchors:
  - [section] "density weighting does not impose requirements on the support of the environments, and allows for invariant learning over all regions of invariant feature support overlap."
  - [section] "As another benefit, learning the invariant feature densities gives us a signal indicating the distribution shift in invariant features across domains that is useful for downstream tasks."
  - [corpus] Weak - related works mention weighting but don't address density vs ratio weighting tradeoffs.
- Break condition: No overlap in invariant feature support across environments, making weighted risk invariance trivial (zero weights).

### Mechanism 3
- Claim: Alternating minimization effectively learns both invariant predictor and density estimates simultaneously.
- Mechanism: Alternates between updating density models for each environment while fixing predictor, then updating predictor while fixing densities, converging to approximate solution.
- Core assumption: Density model family matches true distribution family, or can approximate it well enough.
- Evidence anchors:
  - [section] "We propose to use alternating minimization... We show that solving for WRI recovers invariant predictors... in our experiments support that alternating minimization can recover an invariant predictor and density estimates effectively."
  - [section] "In Appendix D.3, we show that running alternating minimization on our regression setting recovers a close approximation of the ground truth densities."
  - [corpus] Weak - no direct evidence of alternating minimization effectiveness in related works.
- Break condition: Poor density model choice leading to inaccurate weights, or optimization getting stuck in local minima.

## Foundational Learning

- Concept: Covariate shift in conditionally invariant features
  - Why needed here: This is the core problem WRI addresses - when the marginal distribution of invariant features shifts across environments
  - Quick check question: What's the difference between regular covariate shift and invariant covariate shift?

- Concept: Importance weighting and density ratio weighting
  - Why needed here: Understanding why density weighting is preferred over density ratio weighting for this application
  - Quick check question: What's the key limitation of importance weighting that density weighting avoids?

- Concept: General position in linear algebra
  - Why needed here: The theoretical guarantee relies on environments being in general position
  - Quick check question: What does it mean for a set of environments to be in general position in this context?

## Architecture Onboarding

- Component map: Data -> Feature extraction -> Density estimation -> Weighted risk computation -> Predictor update -> Repeat
- Critical path: Data → Feature extraction → Density estimation → Weighted risk computation → Predictor update → Repeat
- Design tradeoffs: Density model complexity vs accuracy, alternating step frequency vs convergence, weight penalty strength vs stability
- Failure signatures: Density estimates diverging, predictor failing to converge, weighted risk invariance not being achieved
- First 3 experiments:
  1. Test WRI on simple 2D Gaussian setup with known ground truth to verify spurious feature rejection
  2. Compare WRI vs VREx on heteroskedastic dataset to demonstrate advantage under invariant covariate shift
  3. Evaluate density estimate quality on regression setting to verify alternating minimization effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimization process of WRI scale to high-dimensional invariant features, and what are the practical limitations?
- Basis in paper: [inferred] The paper mentions that density estimation on high-dimensional data is notoriously difficult and that the method's convergence is not rigorously proven.
- Why unresolved: The paper only demonstrates the method's effectiveness on simpler cases and synthetic data, without providing guarantees or extensive testing on high-dimensional real-world data.
- What evidence would resolve it: Extensive experiments on high-dimensional datasets, theoretical analysis of convergence rates, and comparisons with other methods' scalability.

### Open Question 2
- Question: What is the relationship between the quality of learned invariant feature densities and the OOD detection performance of WRI?
- Basis in paper: [explicit] The paper shows that learned densities are useful for OOD detection, but does not provide a detailed analysis of the relationship between density quality and OOD performance.
- Why unresolved: The paper does not investigate the factors affecting density estimation quality or how it directly impacts OOD detection reliability.
- What evidence would resolve it: Experiments varying the accuracy of density estimates and measuring corresponding changes in OOD detection performance, analysis of failure modes in density estimation.

### Open Question 3
- Question: Can WRI be extended to handle more complex causal structures beyond linear-Gaussian models?
- Basis in paper: [explicit] The paper only provides theoretical guarantees for linear-Gaussian settings and does not explore more complex causal relationships.
- Why unresolved: The paper focuses on linear models for theoretical tractability but does not investigate how the method performs with non-linear causal mechanisms.
- What evidence would resolve it: Empirical results on datasets with known non-linear causal structures, theoretical extensions of the weighted risk invariance framework to non-linear cases.

## Limitations
- Theoretical guarantees are limited to linear-Gaussian settings with a general position assumption that may not hold in practice
- Method's effectiveness depends heavily on quality of density estimates, with poor modeling leading to suboptimal weights
- Computational cost of alternating minimization between predictor and density models may be prohibitive for large-scale applications

## Confidence

- **High Confidence**: The experimental results showing WRI's superior performance over baselines on DomainBed benchmarks are well-supported by the reported evidence.
- **Medium Confidence**: The theoretical proof for linear-Gaussian settings is sound, but its applicability to real-world nonlinear scenarios remains an open question.
- **Medium Confidence**: The claim that density weighting provides better support overlap than importance weighting is plausible but lacks extensive empirical validation across diverse settings.

## Next Checks

1. **Theoretical Extension**: Investigate whether the WRI framework can be extended to nonlinear settings beyond linear-Gaussian assumptions, potentially using techniques from kernel methods or neural tangent kernels.

2. **Density Quality Assessment**: Systematically evaluate how density estimation errors propagate through the weighting scheme and impact final model performance across varying levels of covariate shift.

3. **Computational Efficiency**: Benchmark the runtime and memory requirements of WRI against existing methods on large-scale datasets to quantify the practical trade-offs of alternating minimization.