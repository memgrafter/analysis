---
ver: rpa2
title: Redesigning graph filter-based GNNs to relax the homophily assumption
arxiv_id: '2409.08676'
source_url: https://arxiv.org/abs/2409.08676
tags:
- graph
- learning
- networks
- aagcn
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Adaptive Aggregation Graph Convolutional Networks
  (AAGCNs) as a solution to the limitations of existing GNNs when learning from heterophilic
  data. The key innovation is decoupling the learning of feature transformation weights
  from feature aggregation weights in each layer, using polynomial graph filters.
---

# Redesigning graph filter-based GNNs to relax the homophily assumption

## Quick Facts
- arXiv ID: 2409.08676
- Source URL: https://arxiv.org/abs/2409.08676
- Reference count: 40
- Key outcome: AAGCNs outperform state-of-the-art baselines on both homophilic and heterophilic datasets with 5-15% accuracy improvements on heterophilic datasets compared to standard GCNs

## Executive Summary
This paper introduces Adaptive Aggregation Graph Convolutional Networks (AAGCNs), a novel GNN architecture designed to learn effectively from both homophilic and heterophilic data. The key innovation is decoupling feature transformation weights from feature aggregation weights using polynomial graph filters, allowing the network to aggregate information from multiple hops without being constrained to low-pass filtering. The authors demonstrate that AAGCNs prevent oversmoothing and achieve superior performance compared to standard GCNs and other state-of-the-art methods across multiple benchmark datasets.

## Method Summary
AAGCNs build upon graph signal processing tools to create a more flexible graph convolutional architecture. The core idea is to separate the feature transformation (W^(ℓ) matrix) from the graph filtering operation (polynomial coefficients h^(ℓ)). This allows the network to learn different aggregation patterns for different neighborhood distances while maintaining shared feature transformations. The architecture is trained using an alternating optimization scheme that separately updates the filter coefficients and feature weights to avoid computational instabilities from bilinear terms.

## Key Results
- AAGCNs achieve 5-15% accuracy improvements on heterophilic datasets (Texas, Cornell, Wisconsin) compared to standard GCNs
- The architecture performs competitively on homophilic datasets like Cora and Citeseer
- AAGCNs prevent oversmoothing by not being restricted to low-pass filtering
- The alternating optimization training approach provides computational stability and effective learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling feature transformation weights from feature aggregation weights allows the network to learn different importance patterns for each hop without being constrained to low-pass filtering.
- Mechanism: In traditional GCNs, a single weight matrix is shared across all hops, forcing the same feature transformation at every neighborhood level. AAGCN separates these by introducing hop-specific coefficients h^(ℓ)_r that control aggregation and a shared weight matrix W^(ℓ) that transforms features. This enables the network to apply different treatments to information from different distances on the graph.
- Core assumption: The importance of features for transformation can be similar across hops, but the importance of information from each hop can vary depending on homophily or heterophily.
- Evidence anchors:
  - [abstract] "The proposed architecture reinterprets the role of graph filters in convolutional GNNs, resulting in a more general architecture while incorporating a stronger inductive bias than GNNs based on filter banks."
  - [section III] "Instead, we rely on a simple yet potent approach that exploits GSP tools [38], [39]. As the decoupled formulation in (5) models feature transformations and the graph filter in (1) separately..."
  - [corpus] Weak evidence for this specific decoupling mechanism in related papers.
- Break condition: If the feature transformation importance actually differs significantly across hops, the shared W^(ℓ) could become a bottleneck, limiting performance.

### Mechanism 2
- Claim: Polynomial graph filters enable learning beyond low-pass filtering, allowing effective information aggregation from both homophilic and heterophilic neighborhoods.
- Mechanism: By using polynomial filters with coefficients h^(ℓ)_r for each hop r, the network can assign different weights to information from different neighborhood distances. This contrasts with standard GCNs that effectively apply low-pass filtering (concentrating on immediate neighbors). The learned frequency response can shift energy to high frequencies for heterophilic data.
- Core assumption: Different types of data (homophilic vs heterophilic) require different frequency characteristics in the learned filters.
- Evidence anchors:
  - [abstract] "enabling it to learn from both homophilic and heterophilic data and preventing the issue of oversmoothing."
  - [section III.B] "By learning the filter coefficients h^(ℓ), we can analyze the type of graph filter learned by the architecture... If ˜h is low for higher frequencies, we obtain a low-pass graph filter... heterophilic datasets often require high-frequency information..."
  - [section IV] "First, we see that all our AAGCN variants outperform all alternatives on the heterophilic datasets. This corroborates our claim that by using a polynomial graph filter the AAGCN is not restricted to learning smooth relations..."
- Break condition: If the dataset characteristics don't align with the frequency assumptions (e.g., mixed homophily patterns), the fixed polynomial structure might not adapt well.

### Mechanism 3
- Claim: Alternating optimization for the two parameter sets (filter coefficients H and feature weights W) provides computational stability and effective learning without requiring exact minimization.
- Mechanism: Instead of jointly optimizing all parameters (which would involve complex bilinear terms), AAGCN uses alternating minimization where H and W are updated separately in alternating steps. This simplifies gradient computation and avoids instabilities from bilinear optimization.
- Core assumption: Approximate alternating optimization with small numbers of gradient steps per parameter set is sufficient for good performance.
- Evidence anchors:
  - [section III.A] "we train our architecture by minimizing (2) following an alternating minimization scheme... By optimizing L one parameter set at a time, we split the bilinear terms in (5), resulting in computationally friendly gradients."
  - [section IV] "As mentioned in Section III-A and depicted in Algorithm 1, we train our architecture in an iterative manner to avoid the instabilities resulting from the bilinear term... The results are shown in Fig. 2, where each value denotes the difference in accuracy between the non-iterative version... Clearly, training the AAGCN following an approximate alternating optimization scheme yields an advantage..."
  - [corpus] No direct evidence in related papers about alternating optimization for GCNs.
- Break condition: If the alternating optimization gets stuck in poor local minima or if the approximation error accumulates over many layers.

## Foundational Learning

- Graph Signal Processing fundamentals
  - Why needed here: Understanding graph filters, frequency analysis, and the graph Fourier transform is essential to grasp why polynomial filters can go beyond low-pass filtering and how frequency responses indicate homophily/heterophily adaptation.
  - Quick check question: What is the relationship between the eigenvalues of the adjacency matrix and graph frequencies?

- GCN architecture and limitations
  - Why needed here: Knowing how standard GCNs work (neighborhood aggregation, normalization) and their limitations (oversmoothing, low-pass filtering) provides context for why AAGCN's modifications are beneficial.
  - Quick check question: Why does stacking many GCN layers lead to oversmoothing?

- Homophily vs heterophily concepts
  - Why needed here: Understanding the distinction between homophilic graphs (connected nodes tend to have similar labels) and heterophilic graphs (connected nodes tend to have different labels) is crucial since AAGCN specifically addresses the challenge of learning from both types.
  - Quick check question: How is the homophily score of a graph calculated?

## Architecture Onboarding

- Component map:
  - Input: Node feature matrix X ∈ R^(N×F)
  - Graph structure: Adjacency matrix A
  - L layers, each with:
    - Hop coefficients h^(ℓ) ∈ R^R (learnable)
    - Feature weight matrix W^(ℓ) ∈ R^(F_(ℓ+1)×F_ℓ) (learnable)
    - Polynomial aggregation: Σ_r h^(ℓ)_r A^r X^(ℓ) W^(ℓ)
    - Non-linearity σ(·)
  - Output: Node embeddings for classification

- Critical path:
  1. Initialize H and W
  2. For each epoch:
     a. Update h^(ℓ) for all layers using gradients w.r.t. loss
     b. Update W^(ℓ) for all layers using gradients w.r.t. loss
  3. Return final model

- Design tradeoffs:
  - Flexibility vs complexity: AAGCN has fewer parameters than filter-bank GCNs (R + F_(ℓ+1)F_ℓ vs RF_(ℓ+1)F_ℓ per layer) but more than standard GCNs (F_(ℓ+1)F_ℓ per layer)
  - Homophily adaptation: The architecture can learn different frequency responses but requires the polynomial structure to be expressive enough
  - Training stability: Alternating optimization avoids bilinear term instabilities but may converge slower than joint optimization

- Failure signatures:
  - Poor performance on heterophilic data: May indicate the learned filters are still too low-pass or the polynomial order R is insufficient
  - Overfitting on small datasets: Despite the inductive bias, the architecture might still overfit if R or layer depth is too large
  - Instability during training: Could suggest issues with the alternating optimization schedule or learning rate

- First 3 experiments:
  1. Reproduce Cora (homophilic) and Texas (heterophilic) results from Table I to verify baseline performance
  2. Visualize learned frequency responses (˜h vectors) on both datasets to confirm adaptation to low-pass vs high-pass filtering
  3. Compare training stability and final accuracy between alternating optimization and joint optimization on a small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do AAGCNs perform on larger, real-world heterophilic graphs compared to current state-of-the-art methods?
- Basis in paper: [inferred] The paper only tests AAGCNs on small benchmark datasets (Texas, Cornell, Wisconsin, Cora, Citeseer) and shows competitive performance, but does not evaluate scalability or performance on larger, real-world heterophilic graphs.
- Why unresolved: The paper's experiments are limited to small-scale datasets, and there is no evidence of AAGCNs' performance on larger, more complex graphs with heterophilic properties.
- What evidence would resolve it: Testing AAGCNs on larger, real-world heterophilic graphs and comparing their performance against current state-of-the-art methods would provide evidence of their scalability and effectiveness in more complex scenarios.

### Open Question 2
- Question: How do different normalization techniques (AAGCN-NA vs AAGCN-NH) affect AAGCNs' performance on graphs with varying degree distributions?
- Basis in paper: [explicit] The paper introduces two normalization variants (AAGCN-NA and AAGCN-NH) and shows they can help in different settings, but does not provide a systematic analysis of their performance across graphs with varying degree distributions.
- Why unresolved: The paper only briefly mentions the existence of these variants and their potential benefits, but does not explore how they perform on graphs with different degree distributions or provide guidance on when to use each variant.
- What evidence would resolve it: Conducting experiments comparing AAGCN-NA and AAGCN-NH on graphs with varying degree distributions (e.g., from uniform to exponential) would provide insights into the optimal normalization technique for different graph structures.

### Open Question 3
- Question: How does the choice of alternating optimization parameters (IH and IW) affect AAGCNs' convergence and performance?
- Basis in paper: [explicit] The paper introduces an alternating optimization scheme and shows that moderately small IH and IW achieve competitive performance, but does not provide a comprehensive analysis of how these parameters affect convergence and performance.
- Why unresolved: The paper only demonstrates that small IH and IW work well, but does not explore the impact of different parameter choices on convergence speed, final performance, or the trade-off between the two.
- What evidence would resolve it: Conducting experiments with varying IH and IW values and analyzing their impact on convergence speed, final performance, and the trade-off between the two would provide insights into the optimal parameter choices for different scenarios.

## Limitations

- The polynomial filter structure may not be expressive enough for graphs with complex mixed homophily patterns
- Performance on larger, real-world graphs beyond citation networks remains untested
- The alternating optimization approach is an approximation that may converge to suboptimal solutions compared to joint optimization

## Confidence

- **High Confidence**: AAGCN's architecture is permutation equivariant (theoretical claim supported by construction)
- **High Confidence**: AAGCN outperforms standard GCNs on heterophilic datasets (empirically validated across multiple datasets)
- **Medium Confidence**: AAGCN's alternating optimization is superior to joint optimization (supported by ablation but limited to specific datasets)
- **Medium Confidence**: Decoupling feature transformation from aggregation is the key innovation (mechanism plausible but not definitively isolated from other factors)
- **Low Confidence**: Polynomial filters are sufficient for all homophily/heterophily scenarios (unproven on diverse graph types)

## Next Checks

1. Test AAGCN on biological graphs (e.g., protein-protein interaction networks) where homophily patterns are known to be complex and mixed, to validate generalization beyond citation networks
2. Conduct an ablation study comparing AAGCN with a jointly-optimized variant (joint H and W optimization) across multiple runs to quantify the performance gap and assess whether alternating optimization is truly optimal
3. Perform sensitivity analysis on the polynomial order R across different graph types to identify the optimal R range and determine if adaptive R selection could improve performance