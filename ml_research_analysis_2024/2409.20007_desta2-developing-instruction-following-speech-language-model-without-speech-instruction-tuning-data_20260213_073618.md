---
ver: rpa2
title: 'DeSTA2: Developing Instruction-Following Speech Language Model Without Speech
  Instruction-Tuning Data'
arxiv_id: '2409.20007'
source_url: https://arxiv.org/abs/2409.20007
tags:
- speech
- arxiv
- language
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeSTA2 introduces a novel approach to developing instruction-following
  speech language models (SLMs) without requiring speech instruction-tuning data.
  The method constructs speech-text pairs using a text-based LLM to generate descriptive
  responses from structured speech metadata, following a single unified prompt ("What
  can you hear from the audio?").
---

# DeSTA2: Developing Instruction-Following Speech Language Model Without Speech Instruction-Tuning Data

## Quick Facts
- arXiv ID: 2409.20007
- Source URL: https://arxiv.org/abs/2409.20007
- Reference count: 36
- One-line primary result: DeSTA2 achieves 56.78% accuracy on Dynamic-SUPERB and 7.16 on AIR-Bench-Chat without task-specific finetuning

## Executive Summary
DeSTA2 introduces a novel approach to developing instruction-following speech language models without requiring speech instruction-tuning data. The method constructs speech-text pairs using a text-based LLM to generate descriptive responses from structured speech metadata, following a single unified prompt. This approach minimizes textual mismatches between the original LLM and training data while preserving the LLM's inherent capabilities. The model architecture integrates Whisper and Llama3 through a modality adapter, trained end-to-end on the generated speech-text pairs, achieving strong performance on speech understanding benchmarks without task-specific finetuning.

## Method Summary
DeSTA2 develops an instruction-following speech language model by generating synthetic training data rather than using existing speech instruction-tuning datasets. The approach uses Llama3-8B-Instruct to create speech-text pairs from speech metadata extracted across multiple datasets (AccentDB, DailyTalk, IEMOCAP, PromptTTS, VCTK, VoxCeleb). The model integrates Whisper-small and Llama3-8B-Instruct through a Qformer-based modality adapter, with both base models frozen during training. Speech features are extracted from Whisper encoder intermediate layers, mapped to LLM input space, and concatenated with text transcription. The model is trained end-to-end using next-token prediction loss on the generated pairs for 10 epochs.

## Key Results
- Achieves 56.78% overall accuracy on Dynamic-SUPERB benchmark without task-specific finetuning
- Scores 7.16 on AIR-Bench-Chat evaluation
- Demonstrates advanced reasoning capabilities including complex instruction following and chain-of-thought reasoning
- Outperforms previous end-to-end systems that relied on extensive instruction-tuning datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeSTA2 preserves LLM capabilities by minimizing textual mismatch between original model and training data
- Mechanism: The model uses the same LLM that will be fine-tuned to generate the training data, ensuring consistency in response style and format
- Core assumption: The original LLM's instruction-following capabilities are domain-agnostic and can be transferred to speech tasks through proper data alignment
- Evidence anchors:
  - [abstract] "minimizes the textual mismatch between the original LLM and training data"
  - [section III-A] "the dataset is constructed by the same LLM"
  - [corpus] Weak evidence - related papers discuss alignment but don't explicitly confirm this mechanism

### Mechanism 2
- Claim: Single unified prompt ("What can you hear from the audio?") is sufficient for comprehensive speech understanding
- Mechanism: A descriptive prompt encourages the model to generate rich, multifaceted speech descriptions covering paralinguistic attributes
- Core assumption: The LLM's understanding of speech metadata is sufficient to generate comprehensive descriptions without task-specific prompts
- Evidence anchors:
  - [section III-A] "we employ a single prompt—'What can you hear from the audio?'"
  - [section III] "with the descriptive prompt, the model tends to generate comprehensive descriptions of speech"
  - [corpus] Moderate evidence - related work mentions unified approaches but with different prompts

### Mechanism 3
- Claim: Modality adapter preserves pre-trained model weights while enabling cross-modal understanding
- Mechanism: Qformer extracts speech features from Whisper encoder, which are then mapped to LLM input space through learnable parameters
- Core assumption: The speech and text representations can be effectively bridged through linear transformation without fine-tuning base models
- Evidence anchors:
  - [section III-B] "We freeze their parameters during training" and "A modality adapter with randomly initialized weights bridges the gap"
  - [section III-B] "Qformer to extract speech features from the intermediate layers of the Whisper encoder"
  - [corpus] Weak evidence - related papers discuss modality adapters but with different architectures

## Foundational Learning

- Concept: Speech metadata extraction and annotation
  - Why needed here: The approach relies on structured speech metadata to create training pairs
  - Quick check question: Can you list at least five different types of speech metadata used in this work?

- Concept: Instruction-following evaluation metrics
  - Why needed here: The model's performance is evaluated on instruction-following benchmarks
  - Quick check question: What are the key differences between Dynamic-SUPERB and AIR-Bench-Chat evaluation approaches?

- Concept: Catastrophic forgetting in model fine-tuning
  - Why needed here: The approach aims to prevent loss of original LLM capabilities during speech adaptation
  - Quick check question: How does freezing base model parameters help prevent catastrophic forgetting?

## Architecture Onboarding

- Component map: Whisper encoder → Qformer (modality adapter) → linear projection → concatenated with Whisper decoder output → Llama3-8B-Instruct
- Critical path: Speech input → Whisper encoder → modality adapter → LLM → output generation
- Design tradeoffs: Freezing base models preserves capabilities but limits adaptation; single prompt simplifies but may miss specialized tasks
- Failure signatures: 
  - Model generates only transcripts without descriptions (data construction mismatch)
  - Poor performance on specific speech attributes (insufficient metadata coverage)
  - Loss of original LLM capabilities (inadequate preservation during adaptation)
- First 3 experiments:
  1. Verify that Qformer can extract meaningful features from Whisper encoder intermediate layers
  2. Test concatenation of speech features with text transcription for input to LLM
  3. Validate that single prompt generates comprehensive speech descriptions across different audio samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the long-term impact of DeSTA2's approach on the model's ability to handle domain-specific speech tasks that require specialized knowledge?
- Basis in paper: [explicit] The paper discusses the model's performance on general speech tasks but does not extensively explore its capabilities in highly specialized domains
- Why unresolved: The model's effectiveness in niche areas, such as medical or legal speech processing, remains untested, which could limit its applicability in certain industries
- What evidence would resolve it: Testing the model on domain-specific datasets and benchmarks would provide insights into its adaptability and limitations in specialized contexts

### Open Question 2
- Question: How does the model's performance scale with increasing audio length, particularly for tasks requiring sustained attention or memory over extended periods?
- Basis in paper: [inferred] The paper evaluates the model on short to moderate-length audio clips but does not address its performance on longer audio segments
- Why unresolved: Longer audio inputs may challenge the model's ability to maintain context and coherence, which is critical for real-world applications like transcribing meetings or lectures
- What evidence would resolve it: Evaluating the model on progressively longer audio inputs and measuring its accuracy and coherence over time would clarify its limitations and strengths

### Open Question 3
- Question: Can the DeSTA2 approach be extended to multilingual or cross-lingual speech understanding, and what are the potential challenges?
- Basis in paper: [explicit] The paper mentions the use of diverse datasets but does not explore the model's performance in multilingual settings
- Why unresolved: While the model demonstrates strong performance in English, its ability to generalize to other languages or handle code-switching scenarios is unclear
- What evidence would resolve it: Testing the model on multilingual datasets and evaluating its cross-lingual transfer capabilities would provide insights into its adaptability to diverse linguistic contexts

## Limitations

- The assumption that a single descriptive prompt can capture comprehensive speech understanding may not hold for specialized speech attributes requiring targeted instructions
- Freezing base model parameters may limit the model's ability to fully adapt to speech-specific patterns and nuances
- The quality and coverage of speech metadata annotations across different datasets may impact the effectiveness of the training data generation process

## Confidence

**High Confidence**: The technical architecture of DeSTA2, including the integration of Whisper and Llama3 through a modality adapter, is well-specified and follows established approaches in multimodal learning. The reported benchmark results on Dynamic-SUPERB and AIR-Bench-Chat are clearly documented and can be independently verified.

**Medium Confidence**: The claim that DeSTA2 achieves advanced reasoning capabilities through chain-of-thought reasoning inherited from the underlying LLM is supported by the architectural design but requires empirical validation. The effectiveness of using a single unified prompt for comprehensive speech understanding needs further testing across diverse speech scenarios.

**Low Confidence**: The assumption that minimizing textual mismatch between original LLM and training data is the primary driver of performance preservation lacks direct empirical evidence. The extent to which the model's capabilities transfer from text to speech domains remains an open question that requires more systematic investigation.

## Next Checks

1. Conduct an ablation study comparing DeSTA2's performance when trained with single unified prompt versus task-specific prompts for different speech attributes to quantify the impact of prompt design on comprehensive speech understanding.

2. Perform systematic evaluation of DeSTA2's performance on original LLM tasks (general instruction following, reasoning) before and after speech adaptation to quantify any degradation in base capabilities.

3. Analyze the correlation between specific speech metadata attributes and model performance across Dynamic-SUPERB task categories to identify which metadata elements are most critical for different types of speech understanding tasks.