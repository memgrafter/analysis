---
ver: rpa2
title: 'Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision'
arxiv_id: '2410.05991'
source_url: https://arxiv.org/abs/2410.05991
tags:
- grimoire
- strokes
- im2vec
- figure
- vector
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces GRIMOIRE, a text-guided generative model
  for SVG vector graphics that learns from raster image supervision rather than requiring
  SVG data. The key innovation is a two-stage approach: first, a Visual Shape Quantizer
  (VSQ) learns to map raster patches to discrete vector shapes using differentiable
  rendering; second, an Auto-Regressive Transformer learns to generate sequences of
  these shapes conditioned on text descriptions.'
---

# Vector Grimoire: Codebook-based Shape Generation under Raster Image Supervision

## Quick Facts
- arXiv ID: 2410.05991
- Source URL: https://arxiv.org/abs/2410.05991
- Reference count: 40
- Key outcome: GRIMOIRE achieves superior vector graphics reconstruction and generation quality compared to image-supervised baselines like Im2Vec on MNIST digits, fonts, and icons, using only raster image supervision rather than requiring SVG data.

## Executive Summary
GRIMOIRE introduces a text-guided generative model for SVG vector graphics that learns from raster image supervision. The key innovation is a two-stage approach where a Visual Shape Quantizer (VSQ) first learns to map raster patches to discrete vector shape codes using differentiable rendering, then an Auto-Regressive Transformer (ART) generates sequences of these shapes conditioned on text descriptions. Unlike previous approaches requiring direct SVG supervision, GRIMOIRE only needs raster images, enabling training on larger datasets. Experiments demonstrate superior reconstruction and generation quality compared to image-supervised baselines, with better CLIPScore and FID metrics, while also supporting vector graphics completion.

## Method Summary
GRIMOIRE employs a two-stage pipeline to generate SVG vector graphics from text descriptions using only raster image supervision. First, the Visual Shape Quantizer (VSQ) learns to map raster patches to discrete vector shape codes by predicting Bézier control points and rendering them via differentiable rasterization to match the original patches. Second, an Auto-Regressive Transformer (ART) learns to generate sequences of these shape codes conditioned on text, using a causal language modeling approach. The method uses finite scalar quantization (FSQ) to simplify training and avoid auxiliary codebook losses, and includes post-processing steps (Path Clipping and Path Interpolation) to fix stroke ordering discontinuities in generated SVGs.

## Key Results
- GRIMOIRE outperforms Im2Vec baseline on MNIST reconstruction (MSE 0.42 vs 1.03) and generation (FID 10.0 vs 22.5, CLIPScore 45.2 vs 38.8)
- Superior performance maintained on Fonts dataset (FID 15.8 vs 23.1, CLIPScore 52.3 vs 48.9) and FIGR-8 icons (FID 28.4 vs 35.7)
- Reconstruction quality degrades gracefully with reduced VSQ training samples (MSE remains below 1.0 even with only 5k samples)
- Post-processing via Path Clipping consistently improves FID and CLIPScore across all datasets

## Why This Works (Mechanism)

### Mechanism 1
The two-stage VQ-VAE pipeline with differentiable rendering allows SVG generation from only raster supervision. Stage 1 learns to map raster patches to discrete vector shape codes via a VSQ that reconstructs patches as SVG Bézier curves using a differentiable rasterizer. Stage 2 learns an autoregressive prior over these shape codes conditioned on text via a transformer. This decouples visual shape learning from text modeling. The core assumption is that a differentiable renderer (DiffVG) can accurately map SVG Bézier curves to raster patches so that MSE loss on the raster output trains the vector decoder effectively. Break condition: If the differentiable renderer introduces too much quantization error or fails to preserve fine details, the VSQ will not learn accurate shape codes, breaking the pipeline.

### Mechanism 2
Using FSQ (finite scalar quantization) instead of learned codebook embeddings simplifies training while maintaining expressiveness. Instead of learning a separate embedding table, FSQ quantizes each latent dimension to a small set of discrete values (e.g. 7,5,5,5,5 across 5 dims). The code is computed as a weighted sum of these quantized values, avoiding auxiliary codebook losses. The core assumption is that a finite set of discrete values per latent dimension can represent the diversity of vector shape features without requiring a large learned codebook. Break condition: If the finite quantization granularity is too coarse, the VSQ will fail to distinguish subtle shape variations, limiting generation quality.

### Mechanism 3
Post-processing via path clipping (PC) and path interpolation (PI) fixes stroke ordering discontinuities without retraining. PC sets the start of each new stroke to the end of the previous stroke. PI adds connecting strokes between them. Nearest-neighbor matching avoids spurious connections for disconnected strokes. The core assumption is that the transformer can learn relative stroke positions even though absolute positioning is lost during patch centering, and PC/PI can recover a coherent stroke order. Break condition: If stroke sequences are too fragmented or context strokes are far apart, PC/PI may create visually broken paths.

## Foundational Learning

- **Variational Autoencoders (VAEs) and their reparameterization trick**: Why needed here: The VSQ is a VQ-VAE variant; understanding how VAEs learn latent representations is key to grasping why vector quantization works. Quick check question: Why can't we directly backpropagate through the argmin in vector quantization without tricks like straight-through estimator or codebook gradients?

- **Differentiable rendering for vector graphics**: Why needed here: DiffVG enables gradient flow from raster reconstruction loss to SVG control points, making raster-supervised training possible. Quick check question: What properties must a differentiable renderer have to preserve gradients for Bézier curve parameters?

- **Autoregressive Transformers for sequence modeling**: Why needed here: The ART module models the joint distribution over shape tokens, positions, and text, requiring understanding of causal attention and sequence generation. Quick check question: How does causal masking ensure the transformer only conditions on past tokens during generation?

## Architecture Onboarding

- **Component map**: Input raster -> VSQ encoder -> latent vector -> quantization -> shape code -> VSQ decoder -> Bézier control point predictor -> DiffVG renderer -> rendered patch (training only) -> ART encoder -> positional/shape embeddings -> 12-16 transformer layers -> sequence predictor -> decode to SVG -> (optional) PC/PI

- **Critical path**: Input raster → VSQ encoder → codebook index → VSQ decoder → rendered patch (training only) → ART encoder → transformer → output sequence → decode to SVG → (optional) PC/PI

- **Design tradeoffs**: Using FSQ vs learned codebook: simpler, no codebook loss, but limited expressiveness if discretization too coarse. Single code vs multiple codes per patch: multiple codes improve reconstruction but increase token count and complexity. Stroke length thresholds: shorter strokes easier to model but may fragment shapes.

- **Failure signatures**: Poor reconstruction MSE → VSQ decoder not learning accurate Bézier parameters. Low CLIPScore but decent FID → generated shapes not semantically aligned with text. Fragmented or disconnected paths → need PC/PI or better position encoding.

- **First 3 experiments**: 1. Train VSQ on MNIST patches with nseg=1, ncode=1; check MSE vs Im2Vec baseline. 2. Train ART on MNIST text-conditioned codes; generate digits and measure CLIPScore. 3. Enable PC post-processing; compare FID/CLIPScore with and without on Fonts dataset.

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of codebook size and dimensionality affect the trade-off between reconstruction quality and generation diversity in GRIMOIRE? Basis in paper: The paper mentions a codebook size of 4,375 and discusses the usage statistics of the learned codebook, noting that 39.91% of codes remain unused. Why unresolved: The paper does not provide a systematic study on how varying the codebook size and dimensionality impacts the model's performance. What evidence would resolve it: Experiments comparing GRIMOIRE's performance with different codebook sizes and dimensionalities, analyzing the effects on reconstruction error, generation diversity, and the percentage of unused codes.

### Open Question 2
Can GRIMOIRE be extended to handle more complex SVG primitives beyond Bézier curves, such as rectangles or ellipses, and how would this impact the model's performance? Basis in paper: The paper focuses on Bézier curves for simplicity, but mentions that the VSQ module can be extended to predict additional SVG features. Why unresolved: The paper does not explore the inclusion of other SVG primitives in the model's architecture. What evidence would resolve it: Experiments training GRIMOIRE with a VSQ module that can predict a wider range of SVG primitives, comparing the reconstruction quality and generation capabilities with the current Bézier-only approach.

### Open Question 3
How does the geometric constraint in the VSQ module influence the long-term coherence and visual quality of generated SVG sequences, especially in complex compositions? Basis in paper: The paper introduces a geometric constraint to encourage longer, more connected strokes, but only applies it to the stroke-based experiments. Why unresolved: The paper does not analyze the impact of the geometric constraint on the overall quality and coherence of generated SVG sequences, particularly in complex scenes with multiple overlapping shapes. What evidence would resolve it: A detailed analysis of the generated SVG sequences with and without the geometric constraint, focusing on the visual quality, coherence, and presence of redundant or degenerate shapes.

## Limitations

- Limited expressiveness of FSQ quantization may not capture full diversity of complex vector shapes
- Dependence on differentiable renderer quality (DiffVG) for accurate Bézier curve rendering
- Post-processing as band-aid suggests autoregressive transformer struggles with stroke ordering
- No direct SVG supervision comparison to evaluate trade-offs of raster-only approach
- Limited dataset complexity focused on simple graphics rather than complex illustrations

## Confidence

**High confidence** (supported by direct experimental evidence):
- GRIMOIRE outperforms Im2Vec on MNIST reconstruction and generation metrics
- FSQ quantization works for the tested datasets without learned codebook losses
- Post-processing (PC) consistently improves stroke continuity metrics

**Medium confidence** (supported by ablation studies but with limitations):
- The two-stage VQ-VAE pipeline effectively learns shape codes from raster supervision alone
- ART generates text-aligned vector graphics as measured by CLIPScore
- The approach generalizes from MNIST to Fonts and FIGR-8 datasets

**Low confidence** (extrapolation beyond experimental scope):
- Scalability to complex vector graphics beyond simple icons and fonts
- Performance relative to direct SVG-supervised methods
- Robustness across diverse artistic styles and rendering complexities

## Next Checks

1. **Renderer sensitivity test**: Systematically vary DiffVG's rendering parameters (anti-aliasing, resolution) and measure impact on VSQ reconstruction quality and downstream ART generation metrics. This isolates whether generation quality depends on renderer fidelity.

2. **Complexity scaling experiment**: Evaluate GRIMOIRE on a more complex vector graphics dataset (e.g., SVG icons with color gradients, varying stroke widths, or simple illustrations) to test if FSQ quantization remains sufficient beyond simple monochrome shapes.

3. **Direct supervision ablation**: Train a variant of GRIMOIRE using a small amount of SVG supervision (e.g., 1% of training data) to determine if hybrid training improves quality over pure raster supervision, clarifying the trade-off between supervision type and generation quality.