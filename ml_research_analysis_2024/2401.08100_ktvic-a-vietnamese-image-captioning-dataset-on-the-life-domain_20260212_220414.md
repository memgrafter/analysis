---
ver: rpa2
title: 'KTVIC: A Vietnamese Image Captioning Dataset on the Life Domain'
arxiv_id: '2401.08100'
source_url: https://arxiv.org/abs/2401.08100
tags:
- image
- captioning
- dataset
- captions
- vietnamese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KTVIC is a novel Vietnamese image captioning dataset focused on
  the life domain, containing 4,327 images and 21,635 Vietnamese captions. The dataset
  was constructed by human annotators following established guidelines, with each
  image annotated with five descriptive captions.
---

# KTVIC: A Vietnamese Image Captioning Dataset on the Life Domain

## Quick Facts
- arXiv ID: 2401.08100
- Source URL: https://arxiv.org/abs/2401.08100
- Authors: Anh-Cuong Pham; Van-Quang Nguyen; Thi-Hong Vuong; Quang-Thuy Ha
- Reference count: 38
- Key outcome: KTVIC is a novel Vietnamese image captioning dataset focused on the life domain, containing 4,327 images and 21,635 Vietnamese captions.

## Executive Summary
This paper introduces KTVIC, the first Vietnamese image captioning dataset focused on the life domain. The dataset contains 4,327 images and 21,635 Vietnamese captions (five per image) manually annotated by human annotators following established guidelines. The authors conducted experiments using three deep neural network baselines: a CNN-LSTM model, a ViT-Transformer model, and the GRIT model that combines grid and region features. The results demonstrate that Transformer-based models outperform the CNN-LSTM baseline, with the GRIT model achieving the best performance across all metrics. The findings underscore the effectiveness of the KTVIC dataset as a valuable resource for advancing Vietnamese image captioning research.

## Method Summary
The KTVIC dataset was constructed by collecting images from the UIT-EVJVQA dataset and having human annotators provide five Vietnamese captions for each image. The authors implemented three baseline models: (1) ResNet101 + LSTM encoder-decoder, (2) ViT + Transformer encoder-decoder, and (3) GRIT model combining grid and region features with parallel attention. Models were trained for 10 epochs using cross-entropy loss and evaluated on a held-out test set using standard image captioning metrics (BLEU-1, BLEU-4, METEOR, ROUGE-L, CIDEr).

## Key Results
- GRIT model (combining grid and region features) achieved the best performance across all metrics
- Transformer-based models outperformed the CNN-LSTM baseline
- KTVIC dataset enables effective Vietnamese image captioning research on the life domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-annotated Vietnamese captions capture richer semantic diversity than automatic translations
- Mechanism: Manual annotation allows nuanced descriptions of visual scenes that reflect native language expressions, preserving cultural and contextual details that machine translation may distort
- Core assumption: Human annotators can describe the same image in five different ways without excessive redundancy
- Evidence anchors:
  - The dataset comprises 4,327 images and 21,635 Vietnamese captions, with each image annotated with five descriptive captions
  - The annotation interface presents human annotator an image from the UIT-EVJVQA dataset to provide five captions in the corresponding blank boxes
- Break condition: If annotators fail to follow the established guidelines, the semantic diversity would decrease, reducing dataset quality

### Mechanism 2
- Claim: Using both grid-based and region-based features in GRIT improves caption quality over grid-only or region-only approaches
- Mechanism: Grid-based features capture local visual patterns efficiently, while region-based features provide object-level semantic information; combining them enables more accurate word prediction
- Core assumption: The parallel attention mechanism can effectively fuse grid and region features without excessive computational cost
- Evidence anchors:
  - The GRIT model that combines grid and region features outperformed CNN-LSTM and ViT-Transformer baselines
  - GRIT, an advanced image captioning method that has demonstrated state-of-the-art performance on benchmarks for English-based image captioning
- Break condition: If the object detector in GRIT fails to detect objects accurately, the region features become noisy and degrade performance

### Mechanism 3
- Claim: A balanced train-test split preserves evaluation reliability for Vietnamese image captioning
- Mechanism: Maintaining the original UIT-EVJVQA splits ensures that the test set represents unseen images, enabling fair comparison of model performance
- Core assumption: The 3,769/558 split ratio provides sufficient samples for both training and testing
- Evidence anchors:
  - Adhering to the initial splits in the UIT-EVJVQA dataset, the 4,327 images are divided into train and test sets, comprising 3,769 and 558 images, respectively
  - Table 1 presents the performance of the three baselines on the test split of KTVIC
- Break condition: If the test set contains significantly easier or harder images than the training set, evaluation metrics become unreliable

## Foundational Learning

- Concept: Image captioning task formulation
  - Why needed here: Understanding the encoder-decoder pipeline is essential for interpreting baseline comparisons and designing new models
  - Quick check question: What are the two main components of an image captioning system, and how do they interact?

- Concept: Evaluation metrics for image captioning
  - Why needed here: BLEU, METEOR, ROUGE, and CIDEr scores are used to assess model performance; knowing what each metric measures is critical for interpreting results
  - Quick check question: Which metric among BLEU, METEOR, ROUGE, and CIDEr most closely aligns with human judgments of caption quality?

- Concept: Vietnamese tokenization and segmentation
  - Why needed here: Vietnamese words can consist of multiple tokens; segmentation affects vocabulary size and caption length statistics
  - Quick check question: Why is word segmentation necessary in Vietnamese NLP tasks, and what tool is used in KTVIC?

## Architecture Onboarding

- Component map: Image → Feature extraction (ResNet101/ViT) → Optional DETR detector → Parallel attention fusion → Decoder (LSTM/Transformer) → Caption generation
- Critical path: Extract features → Fuse grid/region features → Generate captions token-by-token
- Design tradeoffs: Grid-only: Fast but less object-aware; Region-only: Accurate but computationally heavy; GRIT: Balanced but requires pre-trained detectors
- Failure signatures: BLEU/CIDEr drop → feature extraction or attention fusion issues; Low METEOR → decoder language model quality; High variance across metrics → overfitting or domain mismatch
- First 3 experiments:
  1. Run Baseline 1 (ResNet+BiLSTM) on 100 sample images to verify data pipeline and metric computation
  2. Replace LSTM with Transformer decoder in Baseline 1 to isolate architecture impact
  3. Implement GRIT feature fusion on a small subset to validate parallel attention integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would KTVIC perform with a larger dataset size, and what impact would this have on model performance?
- Basis in paper: The paper notes KTVIC has 4,327 images and 21,635 captions, which is smaller than established English datasets like COCO. The authors acknowledge the dataset size as a limitation
- Why unresolved: The paper only evaluates the current dataset size and doesn't explore how performance scales with more data. This would require collecting and annotating additional images and captions
- What evidence would resolve it: Training and evaluating models on progressively larger subsets of KTVIC (or expanded versions) to measure performance improvements. Comparing learning curves with established datasets

### Open Question 2
- Question: How robust are the baseline models to domain shift, such as images from different Vietnamese regions or cultural contexts not well-represented in KTVIC?
- Basis in paper: The paper notes that KTVIC focuses on the life domain with images from the UIT-EVJVQA dataset, but doesn't explicitly test model generalization to other domains or cultural contexts
- Why unresolved: The paper only evaluates models on the KTVIC test split, which comes from the same distribution as the training data. Testing on out-of-domain images would reveal generalization capabilities
- What evidence would resolve it: Testing baseline models on images from different Vietnamese regions, cultural events, or other domains not included in KTVIC, measuring performance degradation

### Open Question 3
- Question: What is the impact of caption quality and diversity on model performance, and how can we optimize the annotation process to maximize these factors?
- Basis in paper: The paper describes a careful annotation process following established guidelines, but doesn't analyze the relationship between caption quality/diversity and model performance. It also doesn't explore alternative annotation strategies
- Why unresolved: The paper uses the annotated captions as given without investigating how different annotation approaches (e.g., more annotators, different guidelines, automated assistance) might affect model training and performance
- What evidence would resolve it: Controlled experiments comparing model performance using captions from different annotation strategies, measuring caption quality/diversity metrics, and correlating these with downstream model performance

## Limitations

- The dataset size of 4,327 images is relatively small compared to established English image captioning datasets
- The evaluation relies solely on automatic metrics without human evaluation of caption quality and naturalness
- Missing implementation details (pre-trained weights, tokenization specifics, beam search parameters) limit reproducibility

## Confidence

- Dataset Quality and Annotation Process: Medium confidence
- Model Performance Comparisons: High confidence
- Reproducibility of Results: Medium confidence

## Next Checks

1. Conduct inter-annotator agreement analysis by measuring caption diversity and quality for a subset of images using cosine similarity between caption embeddings
2. Evaluate trained models on images from different Vietnamese regions or cultural contexts not represented in KTVIC to assess domain generalization
3. Perform an ablation study on the GRIT model by systematically disabling grid features, region features, or the parallel attention mechanism to quantify their contributions