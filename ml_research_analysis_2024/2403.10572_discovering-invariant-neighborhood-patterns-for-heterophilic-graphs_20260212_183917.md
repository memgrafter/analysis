---
ver: rpa2
title: Discovering Invariant Neighborhood Patterns for Heterophilic Graphs
arxiv_id: '2403.10572'
source_url: https://arxiv.org/abs/2403.10572
tags:
- inpl
- nodes
- distribution
- graph
- neighborhood
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of distribution shifts on non-homophilous
  graphs, where existing graph neural networks rely on the assumption of homophily
  and fail to generalize well. The authors propose a novel Invariant Neighborhood
  Pattern Learning (INPL) framework to alleviate this issue.
---

# Discovering Invariant Neighborhood Patterns for Heterophilic Graphs

## Quick Facts
- arXiv ID: 2403.10572
- Source URL: https://arxiv.org/abs/2403.10572
- Reference count: 40
- Primary result: INPL achieves state-of-the-art performance on non-homophilous graphs under distribution shifts

## Executive Summary
This paper addresses the critical challenge of distribution shifts on non-homophilous graphs, where standard GNNs fail due to their homophily assumption. The authors propose INPL, a novel framework that combines Adaptive Neighborhood Propagation (ANP) with Invariant Non-Homophilous Graph Learning (INHGL) to learn invariant representations. The method achieves significant improvements in classification accuracy across various distribution shifts, particularly in challenging high-bias environments.

## Method Summary
INPL introduces two key modules to address distribution shifts in non-homophilous graphs. First, ANP captures adaptive neighborhood information by combining high-order and low-order neighborhood information through an Invariant Propagation Layer with weighted mixing of adjacency matrices. Second, INHGL constrains ANP to learn invariant graph representations by enforcing consistency across multiple graph partitions using K-means clustering and variance penalty on losses. The framework is trained jointly, optimizing both the adaptive propagation weights and environment clustering to achieve robust performance under distribution shifts.

## Key Results
- INPL achieves state-of-the-art performance on eleven real-world non-homophilous graphs
- Significant improvement in classification accuracy across various distribution shifts
- Particularly effective in high-bias environments with neighborhood pattern distribution shifts
- Outperforms existing non-homophilous and debiased GNNs on standard benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: ANP alleviates neighborhood pattern distribution shifts by combining high-order and low-order neighborhood information
- **Mechanism**: ANP uses an Invariant Propagation Layer combining low-order (immediate neighbors) and high-order (multi-hop neighbors) neighborhood information via weighted mixing of adjacency matrices. The adaptive propagation step is learned via a generative network using Gumbel-Softmax sampling.
- **Core assumption**: Neighborhood patterns in non-homophilous graphs exhibit diverse distributions, and proper combination of neighborhood orders can capture invariant structural signals
- **Break condition**: If neighborhood structure is not informative for class labels, or if Gumbel-Softmax approximation fails, the adaptive mechanism may not converge

### Mechanism 2
- **Claim**: INHGL constrains ANP to learn invariant graph representations by enforcing consistency across multiple graph partitions
- **Mechanism**: INHGL clusters nodes into multiple environments using K-means on learned embeddings from ANP, then applies a variance penalty on losses across environments to encourage invariant predictions
- **Core assumption**: There exists a representation Φ(X,A) such that P[Y|Φ(X,A)] is invariant across environments
- **Break condition**: If clustering fails to separate meaningful environments, the variance penalty may not enforce true invariance

### Mechanism 3
- **Claim**: The combination of ANP and INHGL improves generalization under both neighborhood pattern and class distribution shifts
- **Mechanism**: ANP learns adaptive propagation that captures diverse neighborhood patterns, while INHGL enforces invariance across environments, jointly addressing multiple types of distribution shifts
- **Core assumption**: Distribution shifts manifest in both neighborhood patterns and label distributions, requiring a two-pronged approach
- **Break condition**: If mechanisms interfere or dataset lacks meaningful distribution shifts, combined approach may not outperform simpler baselines

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and their homophily assumption
  - Why needed here: Understanding why standard GNNs fail on non-homophilous graphs is central to the paper's motivation
  - Quick check question: What is homophily in graph theory, and how does it affect GNN performance?

- **Concept**: Distribution shifts and their impact on machine learning generalization
  - Why needed here: The paper explicitly addresses distribution shifts between training and testing environments
  - Quick check question: How do distribution shifts between training and test data affect model performance?

- **Concept**: Graph representation learning and invariant learning principles
  - Why needed here: The proposed method builds on invariant learning theory applied to graph-structured data
  - Quick check question: What is invariant risk minimization, and how can it be applied to graph neural networks?

## Architecture Onboarding

- **Component map**: Adjacency matrices (A1, A2, ..., AK) -> Invariant Propagation Layer -> Adaptive Propagation Generator -> Environment Clustering (K-means) -> Variance Penalty Loss -> Model Update

- **Critical path**: 
  1. Compute multi-hop adjacency matrices
  2. Apply IPL to combine low-order and high-order neighborhood embeddings
  3. Generate adaptive propagation steps via generative network
  4. Cluster nodes into environments using K-means on embeddings
  5. Compute loss with variance penalty across environments
  6. Backpropagate and update model parameters

- **Design tradeoffs**: High-order adjacency matrices increase computational cost but capture richer structural information; K-means clustering assumes spherical clusters; variance penalty hyperparameter controls invariance enforcement strength

- **Failure signatures**: Memory overflow when computing high-order adjacency matrices for large graphs; poor clustering leading to uninformative environments; degraded performance if adaptive propagation step collapses to single value

- **First 3 experiments**:
  1. Sanity check on synthetic data: Create small non-homophilous graph with known distribution shifts and verify INPL outperforms standard GNNs
  2. Ablation study: Remove either ANP or INHGL and confirm performance drops
  3. Scalability test: Run INPL on medium-sized graph and monitor memory usage and runtime

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does INPL performance degrade as homophily degree in test environment increases, especially when shifting from heterophilic to homophilic?
- Basis in paper: Paper focuses on non-homophilous graphs but doesn't test performance on homophilous datasets
- Why unresolved: Paper lacks empirical results on how INPL handles environments with varying homophily degrees
- What evidence would resolve it: Experimental results on homophilous datasets and environments with mixed homophily levels

### Open Question 2
- Question: What is the computational overhead of INPL compared to simpler methods like LINKX for large-scale graphs with millions of nodes and edges?
- Basis in paper: Paper mentions scalability issues but lacks detailed computational complexity analysis
- Why unresolved: While claiming scalability, paper lacks quantitative evidence or benchmarks
- What evidence would resolve it: Runtime comparisons and memory usage analysis between INPL and simpler methods

### Open Question 3
- Question: How sensitive is INPL to hyperparameter choices, particularly the number of graph partitions (K) in environment clustering?
- Basis in paper: Paper lacks detailed sensitivity analysis of hyperparameters
- Why unresolved: Paper doesn't explore robustness to hyperparameter variations
- What evidence would resolve it: Sensitivity analysis experiments varying hyperparameters like K

## Limitations

- Adaptive propagation step using Gumbel-Softmax sampling lacks clarity on temperature scheduling and convergence behavior
- Environment clustering via K-means assumes spherical clusters which may not align with graph structure
- Several implementation details underspecified including exact hyperparameter settings and architectural choices

## Confidence

- **High Confidence**: Core motivation and general framework combining adaptive propagation with invariant learning
- **Medium Confidence**: Specific mechanisms (IPL combining high/low-order neighborhoods, environment clustering for invariance)
- **Low Confidence**: Effectiveness of combined approach under various distribution shifts without theoretical guarantees

## Next Checks

1. Perform ablation study by removing either ANP or INHGL to verify each component's contribution
2. Conduct hyperparameter sensitivity analysis varying λ (variance penalty weight) and K (number of environments)
3. Test generalization on graphs with different heterophily ratios (0.1 to 0.9) and distribution shift types