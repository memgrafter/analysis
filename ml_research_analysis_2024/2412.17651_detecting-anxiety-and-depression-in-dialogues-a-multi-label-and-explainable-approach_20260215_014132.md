---
ver: rpa2
title: 'Detecting anxiety and depression in dialogues: a multi-label and explainable
  approach'
arxiv_id: '2412.17651'
source_url: https://arxiv.org/abs/2412.17651
tags:
- depression
- anxiety
- data
- health
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-label classification system for detecting
  anxiety and depression from user dialogues with a chatbot. The method uses Large
  Language Models (LLMs) for feature extraction based on linguistic patterns, which
  are then used to train traditional machine learning models.
---

# Detecting anxiety and depression in dialogues: a multi-label and explainable approach

## Quick Facts
- arXiv ID: 2412.17651
- Source URL: https://arxiv.org/abs/2412.17651
- Authors: Francisco de Arriba-Pérez; Silvia García-Méndez
- Reference count: 39
- Primary result: 90% accuracy in detecting anxiety and depression from user dialogues with a chatbot

## Executive Summary
This paper presents a multi-label classification system for detecting anxiety and depression from user dialogues with a chatbot. The method uses Large Language Models (LLMs) for feature extraction based on linguistic patterns, which are then used to train traditional machine learning models. The system is designed to be explainable, providing a dashboard that describes the model's decision-making process. Experimental results show 90% accuracy in detecting anxiety and depression, improving upon prior literature.

## Method Summary
The approach combines LLM-based feature extraction using GPT-4o-mini with traditional ML classifiers (Naive Bayes, Decision Tree, Random Forest) for multi-label classification. Features are extracted from dialogues using prompt engineering and a sliding window strategy to capture historical context from the last 30 sessions. The system uses labeled data from Goldberg Anxiety and Depression Scales (GADS) and Yesavage Geriatric Depression Scale (YGDS) questionnaires administered every 3 months. Hyperparameters are optimized using GridSearchCV, and explainability is provided through LLM-generated descriptions based on feature contributions and recent conversation excerpts.

## Key Results
- 90% accuracy in detecting anxiety and depression from chatbot dialogues
- Improved performance compared to prior literature in mental health detection from free-form dialogue
- Multi-label classification approach successfully handles co-occurring anxiety and depression conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LLMs for feature extraction captures linguistic patterns related to anxiety and depression better than traditional NLP methods.
- Mechanism: LLMs understand context and subtle language nuances, extracting high-level features that encode emotional and cognitive states from dialogue.
- Core assumption: The LLM can reliably map dialogue content to clinically meaningful features without task-specific fine-tuning.
- Evidence anchors:
  - [abstract]: "Another relevant contribution lies in using Large Language Models (llms) for feature extraction, provided the complexity and variability of language."
  - [section]: "Our solution combines feature generation based on prompt engineering with a sliding window strategy to consider the history of past sessions."
- Break condition: If the LLM cannot distinguish between anxiety/depression cues and general conversation topics, the extracted features will be noisy and uninformative.

### Mechanism 2
- Claim: Combining LLM-extracted features with traditional ML classifiers achieves higher accuracy than using either alone.
- Mechanism: LLMs provide rich, interpretable features; ML models (e.g., Random Forest) leverage these features with labeled data for accurate classification.
- Core assumption: The engineered features from LLM are sufficiently discriminative and stable across users.
- Evidence anchors:
  - [abstract]: "The combination of llms, given their high capability for language understanding, and Machine Learning (ml) models, provided their contextual knowledge about the classification problem thanks to the labeled data, constitute a promising approach towards mental health assessment."
  - [section]: "The nb17, dt18 and rf models are selected. The hyperparameters of these classifiers are optimized using the GridSearchCV19 method."
- Break condition: If the feature distribution shifts significantly over time or between users, the ML model may overfit to training data and fail in deployment.

### Mechanism 3
- Claim: The explainability dashboard increases trust and clinical adoption by showing interpretable feature contributions and confidence.
- Mechanism: After prediction, an LLM generates natural language explanations based on the most relevant features and recent conversation excerpts.
- Core assumption: Clinicians and users can interpret the explanations meaningfully and act on them appropriately.
- Evidence anchors:
  - [abstract]: "To promote the solution’s trustworthiness, reliability, and accountability, explainability descriptions of the model’s decision are provided in a graphical dashboard."
  - [section]: "Our system explains the predictions obtained by leveraging an llm with a prompt engineering template. This approach creates an explanation of the predicted majority category every 7 sessions."
- Break condition: If the explanations are too technical or vague, end users may distrust the system or ignore recommendations.

## Foundational Learning

- Concept: Multi-label classification vs. multi-class classification
  - Why needed here: The task is to detect both anxiety and depression simultaneously, possibly co-occurring.
  - Quick check question: What is the difference between treating anxiety and depression as two separate binary tasks vs. one multi-label task?

- Concept: Feature engineering with LLMs
  - Why needed here: Raw dialogue is high-dimensional and noisy; LLM-extracted features must encode clinically relevant signals.
  - Quick check question: How do you design a prompt that reliably extracts a "level of insecurity" score from dialogue?

- Concept: Sliding window for temporal context
  - Why needed here: Mental health symptoms may fluctuate; recent conversation history provides context for prediction.
  - Quick check question: Why does the paper use the last 30 sessions rather than just the most recent one?

## Architecture Onboarding

- Component map: Data Acquisition → Feature Engineering (LLM) → Feature Selection → Classification (ML) → Explainability (LLM) → Dashboard
- Critical path: Feature extraction → Classification → Explainability; failures here directly affect output quality.
- Design tradeoffs:
  - LLM fine-tuning vs. prompt engineering: trade-off between accuracy and hallucination risk.
  - Sliding window length vs. computational cost: longer windows capture more context but increase latency.
- Failure signatures:
  - Low explainability confidence → LLM explanation generation failed or was truncated.
  - High Hamming Loss → Multi-label predictions frequently partially incorrect.
  - Feature selection removes too many features → Underfitting.
- First 3 experiments:
  1. Test LLM feature extraction alone: input sample dialogues, verify feature ranges (0.0–1.0) and consistency.
  2. Test classification baseline: train Random Forest on LLM features only, evaluate exact match and Hamming Loss.
  3. Test explainability generation: feed a sample prediction into the explanation template, check coherence and character limit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do linguistic patterns extracted by LLMs compare to traditional NLP features in terms of predictive accuracy for anxiety and depression detection?
- Basis in paper: [explicit] The paper discusses using LLMs for feature extraction based on linguistic patterns, but does not directly compare these to traditional NLP features.
- Why unresolved: The study focuses on the novel approach of using LLMs without benchmarking against established NLP methods.
- What evidence would resolve it: A comparative study using the same dataset with both LLM-extracted features and traditional NLP features (e.g., TF-IDF, word embeddings) for anxiety and depression detection.

### Open Question 2
- Question: How does the performance of the multi-label classification approach compare to separate binary classifiers for anxiety and depression in terms of accuracy and computational efficiency?
- Basis in paper: [explicit] The paper uses a multi-label approach but mentions that separate binary classifiers could be used, suggesting a potential comparison.
- Why unresolved: The study does not explore or compare the performance of separate binary classifiers.
- What evidence would resolve it: An experimental comparison of the multi-label approach with separate binary classifiers on the same dataset, measuring accuracy, computational time, and resource usage.

### Open Question 3
- Question: What is the impact of incorporating non-verbal and paraverbal data (e.g., voice modulation) on the accuracy of anxiety and depression detection in dialogue-based systems?
- Basis in paper: [explicit] The conclusion mentions future work on analyzing non-verbal and paraverbal data, indicating this as an unexplored area.
- Why unresolved: The current study focuses solely on textual dialogue data without considering other modalities.
- What evidence would resolve it: A study that integrates voice data into the existing system and measures any improvements in detection accuracy compared to the text-only approach.

## Limitations
- Proprietary LLM feature extraction pipeline with unspecified prompt templates and normalization procedures
- Lack of external dataset testing raises concerns about generalizability beyond Celia chatbot users
- LLM-generated explanations may be subject to hallucination or inconsistency, particularly in complex multi-label cases

## Confidence

- **High Confidence**: The multi-label classification framework and sliding window feature aggregation methodology are clearly described and technically sound.
- **Medium Confidence**: The reported 90% accuracy figure is based on internal validation metrics but lacks independent replication or external validation.
- **Low Confidence**: The explainability dashboard's clinical utility and user comprehension are not empirically validated with end users.

## Next Checks
1. Conduct cross-dataset validation using established mental health dialogue corpora to test model generalization.
2. Perform user studies with clinicians and patients to evaluate the interpretability and actionability of the LLM-generated explanations.
3. Implement ablation studies comparing different prompt engineering approaches and sliding window sizes to optimize feature extraction quality.