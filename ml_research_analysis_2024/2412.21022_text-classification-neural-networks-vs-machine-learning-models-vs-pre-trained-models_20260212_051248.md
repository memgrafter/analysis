---
ver: rpa2
title: 'Text Classification: Neural Networks VS Machine Learning Models VS Pre-trained
  Models'
arxiv_id: '2412.21022'
source_url: https://arxiv.org/abs/2412.21022
tags:
- pre-trained
- learning
- neural
- training
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares seven pre-trained transformer models (BERT,
  RoBERTa, DistilBERT, etc.) with three standard neural networks (MLP, RNN, TransformerEncoder)
  and three traditional ML models (SVM, Random Forest, Logistic Regression) for text
  classification. The study employs two embedding methods: TF-IDF and GloVe, with
  GloVe consistently outperforming TF-IDF.'
---

# Text Classification: Neural Networks VS Machine Learning Models VS Pre-trained Models

## Quick Facts
- arXiv ID: 2412.21022
- Source URL: https://arxiv.org/abs/2412.21022
- Reference count: 24
- Pre-trained transformers like BERT significantly outperform traditional ML models and standard neural networks in text classification accuracy

## Executive Summary
This study systematically compares seven pre-trained transformer models (BERT, RoBERTa, DistilBERT, etc.) against three standard neural networks (MLP, RNN, TransformerEncoder) and three traditional ML models (SVM, Random Forest, Logistic Regression) for text classification. The research employs two embedding methods—TF-IDF and GloVe—across two classification tasks using a news dataset with 10,917 examples. Results demonstrate that pre-trained models consistently achieve superior accuracy, with BERT reaching 0.8516 and DistilBERT achieving 0.8324. However, the study also shows that traditional models paired with GloVe embeddings offer competitive performance in simpler tasks when computational resources are limited, highlighting the effectiveness of transfer learning and the importance of embedding quality.

## Method Summary
The study uses a news dataset with 10,917 examples and 11 features, focusing on 4 key features (source, title, content, author) for 2-class classification across 17 level-1 and 109 level-2 categories. Pre-trained transformer models were fine-tuned for 6 epochs using AdamW optimizer and CrossEntropy loss, while standard neural networks trained for 150 epochs with Adam optimizer. Traditional ML models underwent grid search optimization with 5-fold cross-validation. Two embedding approaches were tested: TF-IDF and GloVe (fse/glove-wiki-gigaword-100). The methodology includes comprehensive preprocessing involving data cleaning, tokenization, lemmatization, and stopword removal, with performance evaluated using classification accuracy metrics.

## Key Results
- Pre-trained models (BERT, RoBERTa, DistilBERT) achieved highest accuracy (0.8516 for BERT, 0.8324 for DistilBERT)
- GloVe embeddings consistently outperformed TF-IDF across all model types
- Traditional ML models with GloVe embeddings showed competitive performance in simpler tasks when computational resources were limited
- ALBERT performed poorly on level-2 classification (0.01 accuracy) despite reasonable level-1 performance

## Why This Works (Mechanism)

### Mechanism 1
- Pre-trained transformers achieve superior performance due to transfer learning from large-scale language modeling tasks
- Pre-training on massive text corpora allows models to learn rich language representations that capture semantic and syntactic relationships, fine-tuned with minimal data for specific tasks
- Core assumption: Knowledge gained during pre-training is broadly applicable and can be effectively transferred to downstream classification tasks
- Evidence anchors:
  - [abstract]: "Pre-trained models such as BERT and DistilBERT always perform better than standard models/algorithms."
  - [section 8.1]: "Pre-trained language models, such as BERT and GPT, are first trained on massive corpora to learn general language representations in order to capture the structure of the human language."
- Break condition: Transfer learning fails when the downstream task is too dissimilar from pre-training data or when fine-tuning is insufficient

### Mechanism 2
- GloVe embeddings consistently outperform TF-IDF for traditional models because they capture semantic relationships learned from large corpora
- GloVe learns dense vector representations based on global word co-occurrence statistics, encoding semantic similarity between words, while TF-IDF only captures term importance in individual documents
- Core assumption: Semantic relationships captured by GloVe embeddings are useful for classification tasks beyond simple term importance
- Evidence anchors:
  - [abstract]: "GloVe consistently outperforming TF-IDF" and "traditional models paired with GloVe embeddings showed competitive performance in simpler tasks"
  - [section 3]: "GloVe captures the semantic relationships between words by analyzing word co-occurrence statistics in a corpus" vs "TF-IDF focuses on capturing the importance of terms"
- Break condition: GloVe embeddings fail when domain-specific vocabulary differs significantly from the pre-training corpus

### Mechanism 3
- Transformer architectures handle long-range dependencies better than RNNs and MLPs due to self-attention mechanisms
- Self-attention allows each token to directly attend to all other tokens, capturing global context efficiently, while RNNs process sequentially and struggle with vanishing gradients
- Core assumption: The ability to capture long-range dependencies is crucial for text classification performance
- Evidence anchors:
  - [abstract]: "The transformer model...relies on self-attention mechanisms to capture complex relationships within data sequences. It is able to handle long-range dependencies more effectively than traditional neural networks"
  - [section 6]: Describes TransformerEncoder architecture using self-attention to "capture relationships between input features"
- Break condition: Self-attention becomes ineffective when sequences are too long (computational cost) or when local context is more important than global context

## Foundational Learning

- Concept: Word embeddings and vector representations
  - Why needed here: Understanding how text data is converted to numerical format that models can process is fundamental to text classification
  - Quick check question: What's the difference between one-hot encoding and dense word embeddings like GloVe?

- Concept: Transfer learning in deep learning
  - Why needed here: The entire premise of using pre-trained models relies on understanding how knowledge transfers from large-scale pre-training to specific tasks
  - Quick check question: Why is fine-tuning a pre-trained model often more effective than training from scratch on small datasets?

- Concept: Self-attention mechanism
  - Why needed here: Transformer models rely on self-attention to capture relationships between tokens, which is key to their superior performance
  - Quick check question: How does self-attention differ from the recurrence mechanism in RNNs?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline → Embedding layer (TF-IDF or GloVe) → Neural network layers → Classification head → Loss function
  For pre-trained models: Tokenizer → Pre-trained transformer body → Classification head → Loss function
  For traditional ML: Feature extraction → Classifier (SVM/RF/LR) → Evaluation

- Critical path:
  1. Data preprocessing and cleaning
  2. Embedding generation (GloVe vs TF-IDF)
  3. Model selection and training
  4. Evaluation on test set
  5. Hyperparameter tuning

- Design tradeoffs:
  - Model size vs performance: Larger models (BERT) perform better but require more computational resources
  - Pre-trained vs custom embeddings: Pre-trained embeddings capture broader semantic knowledge but may miss domain-specific nuances
  - Traditional ML vs deep learning: Traditional models are faster and require less data but generally underperform

- Failure signatures:
  - Overfitting: Training accuracy high, test accuracy low (especially for small datasets)
  - Underfitting: Both training and test accuracy low (model capacity too small)
  - Poor convergence: Training loss plateaus early or oscillates

- First 3 experiments:
  1. Compare GloVe vs TF-IDF embeddings on a simple MLP to validate embedding quality impact
  2. Test a basic transformer encoder vs RNN on the same dataset to demonstrate architecture benefits
  3. Fine-tune BERT on a small subset of data vs training a traditional model from scratch to show transfer learning advantage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why did ALBERT perform exceptionally poorly (0.01 accuracy) on level-2 classification despite achieving reasonable performance on level-1 tasks?
- Basis in paper: [explicit] The paper notes ALBERT's performance dropped dramatically on level-2 classification, suggesting it "probably needs more epochs during fine-tuning" but this hypothesis wasn't tested due to GPU constraints
- Why unresolved: The authors couldn't conduct additional experiments with more epochs due to computational limitations, leaving the cause of ALBERT's poor performance unexplained
- What evidence would resolve it: Testing ALBERT with increased epochs (e.g., 15-20) to determine if performance improves, or analyzing the model's attention patterns and embedding space to identify potential architectural limitations

### Open Question 2
- Question: Would incorporating contextualized embeddings during pre-processing (before neural networks and traditional ML models) improve their performance to match or approach pre-trained transformers?
- Basis in paper: [inferred] The paper only compares TF-IDF and GloVe embeddings for traditional models, while pre-trained transformers use their internal contextualized embeddings. This creates an unfair comparison since traditional models lack access to contextualized representations
- Why unresolved: The study didn't test traditional models with contextualized embeddings from models like BERT, ELMo, or Flair, which could provide richer semantic representations
- What evidence would resolve it: Implementing traditional models with contextualized embeddings and comparing their performance against both their current results and pre-trained transformers

### Open Question 3
- Question: How does model performance scale with dataset size for different classification approaches, and is there a dataset size threshold where traditional ML models with GloVe embeddings become competitive with pre-trained transformers?
- Basis in paper: [inferred] The study used a fixed dataset without exploring how performance changes with varying data quantities, though the authors note GloVe embeddings helped traditional models achieve "reasonable performance"
- Why unresolved: The experiments were conducted on a single dataset size, leaving unknown whether traditional approaches could close the performance gap with more training data
- What evidence would resolve it: Systematic experiments across multiple dataset sizes (e.g., 10%, 25%, 50%, 75%, 100% of original data) to identify performance scaling curves for each model type

## Limitations

- Conclusions about pre-trained models' superiority are limited by use of a single news dataset without broader domain validation
- Comparison between TF-IDF and GloVe embeddings may not generalize to domains with different linguistic characteristics or specialized vocabularies
- Computational resource comparisons between models are not explicitly quantified, making it difficult to assess practical tradeoffs

## Confidence

- **High Confidence**: Pre-trained transformer models (BERT, RoBERTa, DistilBERT) consistently outperform traditional ML models and standard neural networks across both classification tasks
- **Medium Confidence**: GloVe embeddings consistently outperform TF-IDF for traditional models, though comparison depends heavily on specific dataset characteristics
- **Low Confidence**: Pre-trained models are always the optimal choice regardless of computational constraints, as resource-usage comparisons are not rigorously provided

## Next Checks

1. Test the same model comparisons across multiple diverse text classification datasets (medical, legal, social media) to assess generalizability beyond news articles
2. Conduct ablation studies measuring exact computational requirements (GPU memory, training time) for each model type to quantify the resource-efficiency tradeoffs
3. Implement domain-specific embeddings (trained on the target corpus) versus GloVe to determine if pre-training domain relevance affects traditional model performance as significantly as it does for transformers