---
ver: rpa2
title: Super Tiny Language Models
arxiv_id: '2405.14159'
source_url: https://arxiv.org/abs/2405.14159
tags:
- language
- arxiv
- training
- data
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Super Tiny Language Models (STLMs), a research
  effort focused on developing high-performance language models with significantly
  reduced parameter counts (targeting 10M, 50M, and 100M parameters). The authors
  explore innovative techniques including byte-level tokenization with pooling, weight
  tying, and efficient training strategies to achieve parameter reductions of 90-95%
  compared to traditional models.
---

# Super Tiny Language Models

## Quick Facts
- arXiv ID: 2405.14159
- Source URL: https://arxiv.org/abs/2405.14159
- Reference count: 40
- Target parameter counts: 10M, 50M, and 100M parameters

## Executive Summary
Super Tiny Language Models (STLMs) is a research effort focused on developing high-performance language models with dramatically reduced parameter counts (90-95% reduction). The approach centers on innovative techniques including byte-level tokenization with pooling, weight tying, and efficient training strategies. The goal is to match the performance of 3-7B parameter models while making language models more accessible through reduced training time (under 48 GPU hours for 50M parameters) and computational requirements.

## Method Summary
STLMs implement transformer architectures with byte-level tokenization using a 256-token vocabulary plus pooling mechanisms to reconstruct semantic information. The approach includes weight tying between feed-forward layers to reduce parameters, and explores early exit strategies for conditional computation. Models are trained on large corpora including OpenWebText (6.5 billion words) and evaluated on multiple-choice QA benchmarks. The baseline 50M parameter model uses 8 transformer layers, 16 attention heads, and 1536 FFN dimension, trained for 50,000 steps with specified hyperparameters.

## Key Results
- Target parameter reductions of 90-95% compared to traditional models
- Baseline 50M parameter models achieve 24-30% accuracy on multiple-choice QA datasets
- Training time under 48 GPU hours for 50M parameter models
- Models designed to match 3-7B parameter model performance after instruction tuning

## Why This Works (Mechanism)

### Mechanism 1: Parameter Reduction via Byte-Level Tokenization with Pooling
- Replaces standard BPE tokenization with byte-level embedding plus pooling
- Reduces embedding layer from vocab_size * embedding_dim to 256 * 64
- Pooling mechanism reconstructs meaningful token representations
- Achieves 90-95% parameter reduction while maintaining model capacity

### Mechanism 2: Weight Tying for Feed-Forward Layers
- Shares weights between feed-forward network layers
- Reduces parameters while maintaining performance
- Particularly effective since FFN layers contain majority of transformer parameters
- Based on MobiLlama's approach to parameter sharing

### Mechanism 3: Conditional Computation via Early Exit
- Allows early exits from transformer layers for simpler predictions
- Different tokens require different amounts of computation
- Inspired by mixture-of-depths and layerskip approaches
- Reduces computation without significant accuracy loss

## Foundational Learning

- **Concept: Transformer Architecture Components**
  - Why needed here: Essential for implementing and modifying STLMs
  - Quick check question: What is the purpose of positional embeddings in transformer models?

- **Concept: Scaling Laws and Parameter Efficiency**
  - Why needed here: Understanding performance scaling with parameters and data
  - Quick check question: According to Kaplan scaling laws, how does model performance change as parameter count increases?

- **Concept: Tokenization Methods and Their Trade-offs**
  - Why needed here: Core innovation involves replacing BPE with byte-level tokenization
  - Quick check question: How does vocabulary size impact the parameter count of embedding and language model head layers?

## Architecture Onboarding

- **Component map**: Byte-level embedding layer (256 vocab) → Pooling transformer → Standard transformer blocks → Byte-level decoding layer
- **Critical path**: Input text → byte-level embedding → chunking by BPE bounding boxes → byte-level pooling transformer → core transformer blocks → byte-level decoding → loss calculation
- **Design tradeoffs**: Byte-level approach trades computational complexity for parameter reduction; weight tying reduces parameters but may limit capacity; early exit reduces inference time but adds complexity
- **Failure signatures**: Perplexity increase indicates pooling mechanism issues; benchmark degradation suggests tokenization problems; training instability may indicate aggressive weight tying
- **First 3 experiments**:
  1. Implement and compare baseline models with different positional embeddings
  2. Implement byte-level tokenization with pooling and measure parameter reduction
  3. Test weight tying across FFN layers with different tying strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed byte-level tokenization with pooling mechanism compare to traditional BPE tokenizers in terms of computational efficiency and downstream performance?
- Basis in paper: [explicit] The paper proposes byte-level tokenizer with pooling but doesn't provide empirical results comparing it to traditional tokenizers
- Why unresolved: Only theoretical framework presented without comparative experiments
- What evidence would resolve it: Direct experimental comparison on same benchmarks (perplexity, BLiMP, HellaSwag, ARC-easy, WinoGrande, MMLU)

### Open Question 2
- Question: What is the optimal dropout scheduling strategy for STLMs to balance underfitting and overfitting during training?
- Basis in paper: [explicit] Mentions Liu et al.'s dropout scheduling proposal but doesn't test this approach
- Why unresolved: Paper doesn't explore dropout scheduling strategies
- What evidence would resolve it: Systematic experiments testing different dropout scheduling strategies on STLMs

### Open Question 3
- Question: How does weight tying between FFN layers affect model performance compared to weight tying between embedding and output layers alone?
- Basis in paper: [explicit] Discusses weight tying as parameter reduction technique but doesn't experimentally compare different strategies
- Why unresolved: Only presents baseline model without weight tying
- What evidence would resolve it: Comparative experiments with different weight tying configurations

## Limitations

- Core claims about parameter reduction and performance maintenance lack comprehensive experimental validation
- Byte-level tokenization with pooling mechanism hasn't been thoroughly tested across diverse benchmarks
- Early exit strategy's effectiveness remains entirely theoretical with no empirical validation
- Paper doesn't address potential quality degradation on specialized tasks or long-form text generation

## Confidence

**High Confidence**: Baseline architecture implementation (transformer with GPT2/BPE tokenizer, SwiGLU/FFN layers, RoPE embeddings, grouped query attention) is well-specified and reproducible

**Medium Confidence**: Parameter reduction estimates (90-95%) are based on architectural calculations rather than comprehensive experimental validation; weight tying performance claims supported by related work but not directly validated

**Low Confidence**: Claims about 24-30% accuracy on multiple-choice QA datasets after instruction tuning are speculative; computational efficiency claims (under 48 GPU hours) based on preliminary runs without detailed benchmarking; early exit mechanism effectiveness entirely theoretical

## Next Checks

1. **Ablation Study on Tokenization Methods**: Implement and compare byte-level tokenization with pooling against standard BPE and other parameter-efficient approaches on MMLU benchmark to validate 90-95% parameter reduction claim

2. **Early Exit Mechanism Implementation**: Develop prototype implementation with different confidence thresholds and evaluate impact on inference time versus accuracy across HellaSwag and ARC-easy benchmarks

3. **Instruction Tuning Evaluation**: Implement full 50M parameter model with byte-level tokenization and weight tying, perform instruction tuning on OpenAssistant dataset, evaluate on all five benchmarks to verify claimed 24-30% accuracy range