---
ver: rpa2
title: Mitigate Position Bias in Large Language Models via Scaling a Single Dimension
arxiv_id: '2406.02536'
source_url: https://arxiv.org/abs/2406.02536
tags:
- uni00000013
- uni00000017
- uni00000014
- uni00000015
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses position bias in large language models (LLMs),
  also known as "lost in the middle", where models struggle to utilize information
  in the middle of long prompts. The authors discover that both position embeddings
  and causal attention masks contribute to position bias by generating position-specific
  hidden states.
---

# Mitigate Position Bias in Large Language Models via Scaling a Single Dimension

## Quick Facts
- arXiv ID: 2406.02536
- Source URL: https://arxiv.org/abs/2406.02536
- Authors: Yijiong Yu; Huiqiang Jiang; Xufang Luo; Qianhui Wu; Chin-Yew Lin; Dongsheng Li; Yuqing Yang; Yongfeng Huang; Lili Qiu
- Reference count: 40
- Primary result: Scaling a single dimension of positional hidden states improves long-context performance by up to 15.2% across multiple models

## Executive Summary
This paper addresses position bias in large language models, also known as "lost in the middle", where models struggle to utilize information in the middle of long prompts. The authors discover that both position embeddings and causal attention masks contribute to position bias by generating position-specific hidden states. They propose a simple yet effective method to mitigate this bias by scaling a single dimension of these positional hidden states. The approach demonstrates significant improvements across multiple models (LLaMA-2, Vicuna, Mistral, Gemma, Qwen, MPT) and tasks, achieving up to 15.2% better performance on benchmark tasks including NaturalQuestions, KV retrieval, LongBench, and timeline reordering.

## Method Summary
The paper proposes a method to mitigate position bias in large language models by scaling a single dimension of positional hidden states. The key insight is that position bias arises from both position embeddings and causal attention masks, which generate position-specific hidden states that degrade model performance on long contexts. The solution involves identifying a specific dimension in the hidden states that captures position-related information and applying a scaling factor to this dimension. This simple modification effectively reduces the position-specific information that contributes to bias while preserving the model's ability to process sequential information. The approach is model-agnostic and can be applied to various transformer-based architectures without requiring architectural changes or retraining.

## Key Results
- Up to 15.2% improvement in performance across benchmark tasks
- Consistent improvements across multiple models including LLaMA-2, Vicuna, Mistral, Gemma, Qwen, and MPT
- Significant gains on NaturalQuestions (QA task), KV retrieval, LongBench, and timeline reorder tasks
- Method works without architectural changes or additional training

## Why This Works (Mechanism)
Position bias in LLMs arises from the interaction between position embeddings and causal attention masks, which together generate position-specific hidden states. These states encode absolute positional information that becomes increasingly problematic as context length grows, causing the model to struggle with information in the middle of long prompts. By scaling a single dimension of these positional hidden states, the method reduces the model's reliance on absolute position information while preserving its ability to process sequential relationships. This dimension scaling effectively dampens the position-specific features that contribute to bias, allowing the model to better utilize information across the entire context window.

## Foundational Learning

**Positional Embeddings**: Learnable or fixed vectors that encode position information in transformer models. Needed to understand how models track sequence order. Quick check: Verify that position embeddings are added to token embeddings before the first transformer layer.

**Causal Attention Masks**: Binary matrices that prevent tokens from attending to future tokens in autoregressive models. Needed to understand how position information propagates through attention. Quick check: Confirm mask prevents attention to future positions while allowing self-attention.

**Hidden State Dimensions**: The vector representations at each layer and position in the transformer. Needed to understand where position information is encoded. Quick check: Each position has a hidden state vector of size (model dimension).

**Attention Mechanism**: Computes weighted combinations of value vectors based on query-key similarities. Needed to understand how position information influences token representations. Quick check: Attention scores are computed as QK^T / sqrt(d_k) before softmax.

**Transformer Architecture**: The fundamental building block of modern LLMs with self-attention layers. Needed to understand how modifications propagate through the model. Quick check: Each layer consists of multi-head attention followed by feed-forward network.

## Architecture Onboarding

**Component Map**: Token Embeddings + Position Embeddings -> Transformer Layers (with Attention + FFN) -> Output Layer

**Critical Path**: Position Embeddings → Hidden States → Attention Mechanism → Output Representations

**Design Tradeoffs**: The method trades absolute positional information for better long-context understanding, potentially reducing the model's ability to precisely track position but improving overall context utilization.

**Failure Signatures**: Models may show reduced precision in position-dependent tasks but improved performance on content-based tasks requiring long-context understanding.

**First Experiments**:
1. Apply dimension scaling to position embeddings and measure impact on short-context tasks
2. Apply dimension scaling to causal attention masks and measure impact on autoregressive generation
3. Combine both modifications and evaluate on long-context benchmark tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Method focuses on single dimension scaling without exploring potential benefits of multi-dimension approaches
- Analysis doesn't explain how to identify the optimal dimension to scale across different models and tasks
- Potential negative impacts on position-dependent tasks and other aspects of model performance are not thoroughly investigated

## Confidence
- Position bias mechanism identification: Medium
- Single dimension scaling effectiveness: Medium
- Cross-model generalizability: Medium
- Impact on other performance aspects: Low

## Next Checks
1. Test the method across a broader range of domains and prompt types to assess generalizability
2. Investigate whether scaling multiple dimensions provides additional benefits or trade-offs
3. Analyze the impact of this modification on other aspects of model performance, such as factual consistency and reasoning capabilities