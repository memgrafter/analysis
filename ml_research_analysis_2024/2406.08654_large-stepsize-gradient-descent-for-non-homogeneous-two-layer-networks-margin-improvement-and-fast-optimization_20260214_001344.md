---
ver: rpa2
title: 'Large Stepsize Gradient Descent for Non-Homogeneous Two-Layer Networks: Margin
  Improvement and Fast Optimization'
arxiv_id: '2406.08654'
source_url: https://arxiv.org/abs/2406.08654
tags:
- lemma
- have
- proof
- phase
- stepsize
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies large stepsize gradient descent (GD) for training
  non-homogeneous two-layer neural networks under the logistic loss. The authors show
  that large stepsize GD typically exhibits two distinct phases: an initial "edge
  of stability" (EoS) phase where the empirical risk oscillates, followed by a stable
  phase where the risk decreases monotonically.'
---

# Large Stepsize Gradient Descent for Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization

## Quick Facts
- arXiv ID: 2406.08654
- Source URL: https://arxiv.org/abs/2406.08654
- Authors: Yuhang Cai; Jingfeng Wu; Song Mei; Michael Lindsey; Peter L. Bartlett
- Reference count: 40
- Primary result: Large stepsize GD achieves O(1/t²) empirical risk for non-homogeneous two-layer networks after finite EoS phase

## Executive Summary
This paper studies large stepsize gradient descent for training non-homogeneous two-layer neural networks under logistic loss. The authors show that large stepsize GD exhibits two distinct phases: an initial edge of stability (EoS) phase with oscillating risk, followed by a stable phase with monotonically decreasing risk. During the stable phase, the normalized margin grows nearly monotonically, demonstrating implicit bias even for non-homogeneous predictors. Under linearly separable data and bounded activation derivatives, the authors prove that by choosing a suitably large stepsize, GD achieves O(1/t²) empirical risk after finite EoS phase, faster than the O(1/t) rate of standard GD.

## Method Summary
The paper studies two-layer neural networks with non-homogeneous activations (GELU, SiLU, Softplus) trained via gradient descent with constant large stepsize. The network architecture is f(w;x) = (1/m)Σ aj ϕ(x⊤w(j)) where ϕ is the activation function. The training uses logistic loss and requires the activation to satisfy Lipschitz, smoothness, and near-homogeneity conditions. The analysis characterizes two phases of training: an initial EoS phase with oscillating risk that transitions to a stable phase with monotonically decreasing risk once the empirical risk falls below a stepsize-dependent threshold. The normalized margin is shown to grow nearly monotonically in the stable phase, and under additional assumptions, O(1/t²) convergence is achieved.

## Key Results
- Large stepsize GD exhibits two distinct phases: EoS phase with oscillating risk and stable phase with monotonically decreasing risk
- Normalized margin grows nearly monotonically during stable phase, demonstrating implicit bias for non-homogeneous predictors
- With linearly separable data and bounded activation derivatives, O(1/t²) empirical risk is achieved after finite EoS phase
- Near-homogeneity assumption allows extension of margin improvement theory from homogeneous to non-homogeneous networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large stepsize GD induces two distinct training phases for non-homogeneous two-layer networks.
- Mechanism: The empirical risk initially oscillates in the EoS phase but transitions to a stable phase where risk decreases monotonically once it falls below a stepsize-dependent threshold.
- Core assumption: Network satisfies Lipschitz, smooth, and near-homogeneous activation properties (Assumption 1).
- Evidence anchors: [abstract] "large stepsize gradient descent (GD) under the logistic loss often involves two distinct phases, where the empirical risk oscillates in the first phase but decreases monotonically in the second phase." [section 2] Theorem 2.2 shows that if empirical risk L(w) ≤ 1/η(2ρ² + β), GD enters stable phase.
- Break condition: If the network doesn't satisfy near-homogeneity (κ bounded), the threshold condition may be empty.

### Mechanism 2
- Claim: During stable phase, normalized margin grows nearly monotonically for non-homogeneous predictors.
- Mechanism: The modified margin function γc(w) constructed from the logistic loss auxiliary functions ψ and ι increases, providing a multiplicative approximation to the normalized margin.
- Core assumption: Network satisfies near-homogeneity (Assumption 1C) and enters stable phase with sufficiently low risk.
- Evidence anchors: [abstract] "normalized margin grows nearly monotonically in the second phase, demonstrating an implicit bias of GD in training non-homogeneous predictors." [section A.3] Lemma A.10 shows γc(w) ≤ γb(w) ≤ γ̄(w) ≤ (1 + c/log(1/L))γc(w).
- Break condition: If the homogenous error κ is too large relative to margin, the approximation breaks.

### Mechanism 3
- Claim: Large stepsize GD achieves O(1/t²) empirical risk after finite EoS phase.
- Mechanism: By choosing stepsize η to balance EoS and stable phases, the total optimization time T yields risk O(1/T²), faster than O(1/t) for small stepsize GD.
- Core assumption: Data is linearly separable (Assumption 3) and activation derivative bounded away from zero (Assumption 2A).
- Evidence anchors: [abstract] "by choosing a suitably large stepsize, GD that undergoes this phase transition is more efficient than GD that monotonically decreases the risk." [section 4] Corollary 4.2 shows L(wT) ≤ O(1/T²) with optimal η.
- Break condition: If dataset not linearly separable, average risk may not decrease sufficiently to trigger phase transition.

## Foundational Learning

- Concept: Edge of stability (EoS) in optimization dynamics
  - Why needed here: Understanding when and why large stepsize GD oscillates rather than monotonically decreasing risk.
  - Quick check question: What condition must hold for GD to exit the EoS phase and enter stable phase?

- Concept: Implicit bias in neural network training
  - Why needed here: The paper shows margin maximization occurs even for non-homogeneous networks with large stepsize.
  - Quick check question: How does the normalized margin behave differently in stable vs. EoS phases?

- Concept: Near-homogeneity and homogenous error
  - Why needed here: Allows extension of margin improvement theory from homogeneous to non-homogeneous networks like GELU/SiLU.
  - Quick check question: What role does the constant κ play in characterizing non-homogeneous networks?

## Architecture Onboarding

- Component map: Two-layer network f(w;x) = (1/m)Σ aj ϕ(x⊤w(j)) -> Logistic loss ℓ(t) = log(1 + e⁻ᵗ) -> Gradient descent with constant stepsize η

- Critical path:
  1. Initialize network weights randomly
  2. Run GD with large constant stepsize η
  3. Monitor empirical risk L(wt) for phase transition
  4. In stable phase, track margin growth via γc(wt)
  5. Achieve accelerated convergence O(1/t²) after finite EoS

- Design tradeoffs:
  - Larger η: Faster EoS→stable transition but longer EoS phase
  - Smaller η: Slower but more stable convergence
  - Width m: Affects Lipschitz/smoothness constants but not fundamental behavior

- Failure signatures:
  - Risk doesn't decrease below threshold: Network may not satisfy near-homogeneity
  - Margin doesn't improve: Homogenous error κ too large relative to margin
  - No phase transition: Data not linearly separable or activation derivative vanishes

- First 3 experiments:
  1. Train two-layer network with GELU on linearly separable XOR data, vary η, plot risk/margin phases
  2. Compare small vs. large η on CIFAR-10 subset, measure convergence rates
  3. Test different activation functions (SiLU vs. Softplus) on same task, verify near-homogeneity condition holds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions can the homogeneous error κ in Assumption 1C be reduced while maintaining the same convergence guarantees for the stable phase?
- Basis in paper: The paper assumes a bounded homogeneous error κ in Assumption 1C, but notes that this condition may be restrictive for certain activation functions like sigmoid.
- Why unresolved: The paper does not provide a systematic way to determine the minimal κ required for convergence, and the impact of κ on the convergence rate is not fully characterized.
- What evidence would resolve it: A theoretical analysis showing the relationship between κ and the convergence rate, along with empirical validation on various activation functions.

### Open Question 2
- Question: How does the choice of scaling factor b in the two-layer network model affect the optimization efficiency and generalization performance in practice?
- Basis in paper: The paper discusses the effect of model rescaling and shows that the acceleration effect is maintained for b ≥ 1, but the impact on generalization is not explored.
- Why unresolved: The paper focuses on optimization efficiency and does not provide empirical results on how different scaling factors affect generalization.
- What evidence would resolve it: Experimental results comparing the generalization performance of models with different scaling factors on standard datasets.

### Open Question 3
- Question: Can the phase transition from the edge of stability (EoS) to the stable phase be characterized more precisely in terms of the gradient norm and the loss landscape?
- Basis in paper: The paper shows that the average risk decreases during the EoS phase and that GD enters the stable phase once the risk falls below a threshold, but does not provide a detailed characterization of the phase transition.
- Why unresolved: The paper does not analyze the gradient norm and the loss landscape during the phase transition, which could provide insights into the mechanisms driving the transition.
- What evidence would resolve it: A theoretical analysis of the gradient norm and the loss landscape during the phase transition, along with empirical validation on various datasets and network architectures.

## Limitations
- Strong assumptions required: The analysis relies on near-homogeneity (Assumption 1C), linearly separable data (Assumption 3), and bounded activation derivatives (Assumption 2A)
- Limited empirical validation: The paper provides limited empirical results beyond simple datasets, with unclear implementation details for CIFAR-10 experiments
- κ sensitivity: The analysis becomes less tight as the homogenous error κ increases, creating uncertainty about practical applicability

## Confidence
- High confidence: Phase transition behavior and stable phase margin growth (mechanisms 1 and 2) are well-supported by theoretical analysis
- Medium confidence: O(1/t²) convergence rate claim (mechanism 3) requires strong assumptions that may not hold broadly
- Medium confidence: Extension from homogeneous to non-homogeneous networks works when near-homogeneity conditions are satisfied

## Next Checks
1. Test activation function boundaries: Verify near-homogeneity condition holds for GELU/SiLU with varying network widths and input dimensions, measuring how κ scales.
2. Evaluate on non-separable data: Test the two-phase behavior and margin growth on real-world datasets (e.g., MNIST with label noise) where linear separability fails.
3. Compare initialization sensitivity: Systematically vary weight initialization schemes to determine their impact on phase transition timing and convergence rates.