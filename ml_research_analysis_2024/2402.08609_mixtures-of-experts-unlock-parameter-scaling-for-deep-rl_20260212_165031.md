---
ver: rpa2
title: Mixtures of Experts Unlock Parameter Scaling for Deep RL
arxiv_id: '2402.08609'
source_url: https://arxiv.org/abs/2402.08609
tags:
- learning
- experts
- deep
- performance
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mixtures of Experts (MoEs) enable parameter scaling for deep RL,
  addressing the challenge where increasing parameter counts often hurt performance.
  The paper incorporates Soft MoEs (Puigcerver et al., 2023) into value-based networks
  like DQN and Rainbow, demonstrating substantial performance improvements across
  various training regimes and model sizes.
---

# Mixtures of Experts Unlock Parameter Scaling for Deep RL

## Quick Facts
- arXiv ID: 2402.08609
- Source URL: https://arxiv.org/abs/2402.08609
- Authors: Johan Obando-Ceron; Ghada Sokar; Timon Willi; Clare Lyle; Jesse Farebrother; Jakob Foerster; Gintare Karolina Dziugaite; Doina Precup; Pablo Samuel Castro
- Reference count: 40
- Primary result: Soft MoEs improve parameter scalability in deep RL by reducing dormant neurons and increasing NTK rank

## Executive Summary
This paper addresses the challenge of parameter scaling in deep reinforcement learning, where increasing parameter counts often leads to performance degradation rather than improvement. The authors propose incorporating Soft MoE modules, specifically the Soft MoE architecture from Puigcerver et al. (2023), into value-based networks like DQN and Rainbow. Their results show substantial performance improvements across various training regimes and model sizes, demonstrating that MoEs can effectively enable parameter scaling in RL by improving parameter utilization and optimization stability.

## Method Summary
The authors incorporate Soft MoE modules into value-based deep RL architectures by replacing the penultimate layer with a MoE component. The Soft MoE uses learned dispatch weights to assign tokens to multiple expert slots, followed by combination weights to merge expert outputs. They evaluate this approach on Atari 2600 games using DQN and Rainbow agents, comparing against baselines with wider penultimate layers and hard-routing Top1-MoE variants. The evaluation includes standard training, offline RL, low-data regimes, and varying model sizes, with performance measured using interquartile mean (IQM) of human-normalized scores across multiple seeds.

## Key Results
- Soft MoEs maintain strong performance advantages over baselines even at high replay ratios, confirming improved parameter efficiency
- Benefits persist when scaling down expert dimensionality and using different tokenization methods
- MoE architectures show significantly higher NTK rank, fewer dormant neurons, and lower feature norm growth compared to baseline networks
- Performance gains are consistent across standard training, offline RL, and low-data regimes

## Why This Works (Mechanism)

### Mechanism 1
Soft MoEs improve parameter utilization by reducing dormant neurons. By routing tokens to multiple experts with learned weights, Soft MoEs prevent neurons from becoming inactive during training, which is a common issue in standard networks where increasing parameters can lead to underutilization.

### Mechanism 2
Soft MoEs improve optimization stability by maintaining higher effective rank in neural tangent kernel matrices. The multi-expert routing creates more diverse gradient directions, increasing the effective rank of the NTK matrix, which correlates with better optimization stability.

### Mechanism 3
Soft MoEs enable parameter scaling by providing a more parameter-efficient architecture. Unlike standard dense layers where parameter efficiency is crucial for scaling in RL, Soft MoEs allow for increased model capacity without proportional increase in computational cost.

## Foundational Learning

- Concept: Mixture of Experts (MoE)
  - Why needed here: MoEs are the core architectural component being investigated for scaling RL networks
  - Quick check question: How do MoEs differ from standard dense layers in terms of parameter utilization?

- Concept: Reinforcement Learning (RL) value-based methods
  - Why needed here: Understanding DQN and Rainbow is crucial for grasping the context of the experiments
  - Quick check question: What is the key difference between DQN and Rainbow in terms of architecture?

- Concept: Neural Tangent Kernel (NTK) and effective rank
  - Why needed here: These concepts are used to analyze the optimization dynamics of the MoE architectures
  - Quick check question: How does the effective rank of the NTK matrix relate to optimization stability?

## Architecture Onboarding

- Component map: Input tokenization -> Expert layers (multiple copies) -> Gate/Combine layer (learned dispatch and combination weights) -> Output layer (linear projection)

- Critical path: 1. Tokenization of input, 2. Routing tokens to experts with learned weights, 3. Processing through expert layers, 4. Combining expert outputs, 5. Final linear projection

- Design tradeoffs: Number of experts vs. expert dimensionality, tokenization method (PerConv vs. PerFeat vs. PerSamp), hard gating (Top1-MoE) vs. soft gating (Soft MoE)

- Failure signatures: Performance degradation with increased experts (suggests routing issues), no improvement over baseline (suggests ineffective tokenization or gating), increased training instability (suggests issues with effective rank or dormant neurons)

- First 3 experiments: 1. Replace penultimate layer of DQN with Soft MoE (1 expert) to isolate effects of gating/combining components, 2. Vary number of experts (2, 4, 8) to find optimal scaling point, 3. Compare different tokenization methods (PerConv, PerFeat, PerSamp) to find best input representation

## Open Questions the Paper Calls Out

### Open Question 1
How do MoE modules affect the optimization dynamics and generalization of deep RL networks beyond the three metrics studied (rank, dormant neurons, feature norm)? The authors mention that a fine-grained analysis of the effects of MoE modules on network optimization dynamics lies outside the scope of this work and only analyze three properties known to correlate with training instability.

### Open Question 2
Can other load-balancing losses or gating mechanisms improve the performance of Top1-MoE in deep RL settings? The paper tries adding load-balancing losses from Ruiz et al. (2021) but finds they are unable to improve the performance of Top1-MoE, suggesting other losses may be more effective.

### Open Question 3
What is the exact causal relationship between the observed properties of MoE architectures (higher ENTK rank, fewer dormant neurons, lower feature norm growth) and their improved performance in deep RL? While the paper demonstrates correlation between these properties and MoE usage, it doesn't establish causation or explain the mechanism by which these properties lead to better performance.

## Limitations

- Primary evaluation is limited to Atari 2600 games, which may not generalize to other RL domains
- Theoretical explanations linking performance improvements to reduced dormant neurons and increased NTK rank rely on empirical measurements that may not capture all relevant factors
- Computational overhead of Soft MoEs at scale is not thoroughly characterized

## Confidence

- High Confidence: Empirical results demonstrating performance improvements with Soft MoEs across multiple training regimes and model sizes
- Medium Confidence: Theoretical explanation linking improved performance to reduced dormant neurons and increased NTK rank
- Low Confidence: Claims about Soft MoEs being a "key technique" for enabling parameter scalability in deep RL

## Next Checks

1. Cross-domain validation: Test Soft MoE performance on non-Atari RL tasks (e.g., continuous control, robotics) to assess generalizability beyond the evaluated domain

2. Long-term stability analysis: Conduct extended training runs (beyond 200M steps) to verify that Soft MoE routing mechanisms maintain performance and stability over prolonged training periods

3. Computational overhead characterization: Measure and analyze the wall-clock training time and memory usage of Soft MoE architectures across different scales to quantify the practical benefits of parameter efficiency