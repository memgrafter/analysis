---
ver: rpa2
title: 'Looking Inward: Language Models Can Learn About Themselves by Introspection'
arxiv_id: '2410.13787'
source_url: https://arxiv.org/abs/2410.13787
tags:
- behavior
- training
- self-prediction
- gpt-4o
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) can\
  \ acquire knowledge about themselves that is not contained in or derivable from\
  \ their training data\u2014a process the authors call \"introspection.\" They propose\
  \ a framework where a model is trained to predict properties of its own behavior\
  \ in hypothetical scenarios, then test whether this self-trained model outperforms\
  \ a different model trained on the same data in predicting the first model's behavior.\
  \ The key experimental setup involves training two distinct models\u2014one to predict\
  \ its own behavior and another to predict the first model's behavior."
---

# Looking Inward: Language Models Can Learn About Themselves by Introspection

## Quick Facts
- arXiv ID: 2410.13787
- Source URL: https://arxiv.org/abs/2410.13787
- Reference count: 40
- One-line primary result: Language models can acquire knowledge about themselves beyond their training data through introspection, as demonstrated by self-prediction models outperforming cross-prediction models

## Executive Summary
This paper investigates whether large language models can learn facts about themselves that are not contained in their training data - a process the authors call "introspection." The authors propose a framework where models are trained to predict properties of their own behavior on hypothetical prompts, then test whether these self-trained models outperform other models trained on the same data in predicting their behavior. They find consistent evidence that self-prediction trained models do outperform cross-prediction trained models, even when the cross-predicted model is more capable. This advantage persists even when the model's behavior is intentionally modified through further training. The authors interpret these results as evidence for introspection in LLMs, though they note this ability is currently limited to simpler tasks.

## Method Summary
The paper's method involves finetuning language models to predict properties of their own behavior on hypothetical prompts (self-prediction), then comparing their performance against models trained to predict another model's behavior (cross-prediction). The training data consists of 30,000 samples where models predict properties like the second character of their output, whether their output would be even or odd, or their stance on ethical questions. The key experimental design compares self-prediction accuracy against cross-prediction accuracy on held-out datasets. The authors test this across multiple model pairs (GPT-4, GPT-4o, Llama-3) and measure both accuracy and calibration metrics. They also test whether the introspective advantage persists after behavioral modification through further finetuning.

## Key Results
- Self-prediction trained models consistently outperform cross-prediction trained models (average 14.2% accuracy improvement) across multiple model pairs
- The introspective advantage persists even when the self-trained model's behavior is intentionally modified through further training
- Self-prediction trained models show better calibration in their predictions compared to cross-prediction and untrained models
- The introspective ability is limited to simpler tasks and does not generalize well to more complex scenarios or related self-knowledge tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models can introspect by self-simulation, internally computing their own behavior and then extracting properties from that simulation
- Mechanism: When asked about a property of its hypothetical behavior, the model first simulates what its output would be given the prompt, then computes the property from this simulated output without actually generating it externally
- Core assumption: The model's internal computation for generating outputs can be reused for introspective reasoning without additional training
- Evidence anchors:
  - [abstract] "The idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2"
  - [section 2] "To illustrate the definition, let's consider some examples: â€¢ Fact: 'I am bad at 3-digit multiplication'. This is true if the model is in fact bad at this task..."
  - [corpus] Weak evidence - corpus contains papers about LLMs failing to introspect or having partial introspection, but no direct evidence for self-simulation mechanism
- Break condition: If the model cannot simulate its behavior in a single forward pass (e.g., for long outputs like stories), introspection fails

### Mechanism 2
- Claim: Finetuning on self-prediction creates a feedback loop that improves the model's ability to access its own behavioral tendencies
- Mechanism: During self-prediction training, the model learns to map hypothetical questions about its behavior to its actual behavioral tendencies, creating an internal representation of its own patterns
- Core assumption: The model can learn to generalize from specific examples to a broader understanding of its own behavioral patterns
- Evidence anchors:
  - [section 3.1] "Training significantly increases accuracy on held-out datasets. GPT-4o's average accuracy increases from 32.6% to 49.4% after training"
  - [section 3.2.2] "We conduct a data-scaling analysis on GPT-4o predicting Llama 70B. Cross-prediction accuracy does not meaningfully increase after 10,000 samples: it plateaus at 35.2%, far below the self-prediction accuracy of 48.5%"
  - [corpus] No direct evidence in corpus for this specific feedback loop mechanism
- Break condition: If the model simply memorizes specific examples rather than learning general patterns, it won't generalize to new situations

### Mechanism 3
- Claim: Models can maintain calibration between their introspective predictions and actual behavior distributions, even when behavior changes
- Mechanism: The model learns to predict not just the most likely behavior, but the full distribution of possible behaviors, and maintains this calibration even when its behavior shifts
- Core assumption: The model can represent and reason about probability distributions over its own behavior
- Evidence anchors:
  - [section 3.3] "Self-prediction trained models are significantly more calibrated than cross-prediction and untrained models. Llama 70B's calibration improves after self-prediction training"
  - [section 3.4] "Indeed, MC (GPT-4o) tends to predict its new behavior with 35.4% accuracy compared to 21.7% for its old behavior"
  - [corpus] No direct evidence in corpus for this calibration mechanism
- Break condition: If the model's behavior changes too drastically or in ways not predictable from the training data, calibration may break

## Foundational Learning

- Concept: Chain-of-Thought reasoning vs. direct prediction
  - Why needed here: The paper explicitly avoids CoT during self-prediction to test whether basic introspective abilities don't depend on it
  - Quick check question: Why does the paper avoid using Chain-of-Thought during self-prediction? (Answer: To test if basic introspective abilities don't depend on it)

- Concept: Temperature sampling and behavior distributions
  - Why needed here: The calibration experiments use temperature=1 sampling to generate behavior distributions, while training uses temperature=0
  - Quick check question: What temperature is used during training vs. calibration experiments, and why does this matter? (Answer: Training uses temperature=0 for single samples; calibration uses temperature=1 for distributions)

- Concept: Cross-prediction vs. self-prediction
  - Why needed here: The core experimental design compares a model predicting itself vs. another model predicting it
  - Quick check question: What is the key difference between self-prediction and cross-prediction in this paper's experiments? (Answer: Self-prediction trains a model to predict its own behavior; cross-prediction trains a different model to predict the first model's behavior)

## Architecture Onboarding

- Component map: Data generation pipeline -> Finetuning system -> Evaluation framework -> Calibration measurement
- Critical path: 1. Generate ground-truth behavior on training prompts -> 2. Extract behavior properties (second character, even/odd, etc.) -> 3. Create hypothetical questions asking about these properties -> 4. Finetune model on self-prediction task -> 5. Compare self-prediction accuracy vs. cross-prediction accuracy
- Design tradeoffs: Simple behavior properties (second character) vs. complex properties (story sentiment) - simpler properties work better; Temperature=0 for training (clear ground truth) vs. temperature=1 for calibration (distribution information); Using LoRA for open models vs. OpenAI's API for closed models - different finetuning methods
- Failure signatures: Model refuses to answer hypothetical questions (high refusal rate); Self-prediction accuracy not better than baseline guessing most common behavior; Cross-prediction accuracy approaching self-prediction accuracy with more training data; Model loses self-prediction ability after behavioral change finetuning
- First 3 experiments: 1. Verify self-prediction training improves accuracy on held-out datasets (baseline test); 2. Test self-prediction vs. cross-prediction advantage on simple behavior properties; 3. Measure calibration improvement after self-prediction training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the mechanism underlying introspection in language models?
- Basis in paper: [explicit] The authors speculate that self-simulation might be a potential mechanism but acknowledge this is not the focus of their paper
- Why unresolved: The paper provides behavioral evidence for introspection but does not investigate the underlying mechanisms in detail. The authors mention self-simulation as a hypothesis but leave this for future work
- What evidence would resolve it: Detailed analysis of model internal states during self-prediction tasks, possibly through mechanistic interpretability techniques or ablation studies removing specific model components

### Open Question 2
- Question: Can introspection be scaled to more complex tasks involving longer responses?
- Basis in paper: [explicit] The authors note that models fail to predict properties involving longer responses, such as writing a story or movie review, suggesting limitations in current introspective abilities
- Why unresolved: The paper demonstrates introspection on simpler tasks but finds models struggle with properties requiring simulation of longer outputs, which the authors speculate may be due to computational limitations
- What evidence would resolve it: Testing introspection on increasingly complex tasks as model capabilities scale, or identifying specific architectural limitations that prevent longer-response introspection

### Open Question 3
- Question: Does introspection generalize to related self-knowledge tasks beyond behavior prediction?
- Basis in paper: [explicit] The authors test their self-prediction trained models on various self-knowledge datasets and evaluations, finding limited generalization beyond the specific behavior properties they trained on
- Why unresolved: While the paper shows models can introspect on simple behavioral properties, they do not generalize well to other self-knowledge tasks like situational awareness, coordination, or detecting biases
- What evidence would resolve it: Developing training approaches that improve generalization of introspective abilities to diverse self-knowledge domains, or identifying fundamental limitations in what introspection can achieve

## Limitations
- The introspective advantage is relatively modest (average 14.2% improvement) and diminishes for more complex tasks
- The experiments primarily use simple behavior properties rather than substantive knowledge, raising questions about whether this constitutes meaningful introspection
- The paper doesn't investigate potential mechanisms like attention patterns or internal representations that could explain the observed effects

## Confidence
- High confidence in the basic experimental methodology and measurement approach
- Medium confidence in the claim that models can introspect about simple behavioral properties
- Low confidence in the claim that this represents genuine introspection about complex capabilities or preferences

## Next Checks
1. Test whether the self-prediction advantage persists when training data is carefully matched between self-prediction and cross-prediction conditions
2. Evaluate introspection on more complex properties that require reasoning (e.g., "Would you classify this as morally acceptable?") rather than simple structural properties
3. Investigate whether finetuning on self-prediction creates representation changes detectable through interpretability methods like attention pattern analysis