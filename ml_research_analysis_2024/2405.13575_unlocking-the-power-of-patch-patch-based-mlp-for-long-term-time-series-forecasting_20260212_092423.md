---
ver: rpa2
title: 'Unlocking the Power of Patch: Patch-Based MLP for Long-Term Time Series Forecasting'
arxiv_id: '2405.13575'
source_url: https://arxiv.org/abs/2405.13575
tags:
- time
- series
- forecasting
- patch
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PatchMLP, a simple MLP-based model for long-term
  time series forecasting that challenges the dominance of Transformer-based approaches.
  The core idea is to leverage patch-based embeddings combined with a novel feature
  decomposition strategy that separates smooth components from noisy residuals using
  moving averages.
---

# Unlocking the Power of Patch: Patch-Based MLP for Long-Term Time Series Forecasting

## Quick Facts
- **arXiv ID**: 2405.13575
- **Source URL**: https://arxiv.org/abs/2405.13575
- **Reference count**: 13
- **Primary result**: PatchMLP achieves state-of-the-art performance for long-term time series forecasting using simple MLP architecture

## Executive Summary
PatchMLP introduces a novel patch-based MLP architecture for long-term time series forecasting that challenges the dominance of Transformer-based approaches. The model leverages patch-based embeddings and a feature decomposition strategy that separates smooth components from noisy residuals using moving averages. Unlike previous channel-independent MLP models, PatchMLP incorporates inter-variable interactions through dot-product mechanisms, enabling semantic information exchange across multiple time series variables. The approach demonstrates superior performance across eight real-world datasets, outperforming both Transformer and other MLP-based baselines in long-term forecasting tasks.

## Method Summary
The PatchMLP model employs a multi-scale patch embedding strategy that captures temporal relationships at different granularities. The core innovation lies in its feature decomposition approach, where time series data is separated into smooth components and residual noise using moving averages. This decomposed representation is processed through alternating intra-variable and inter-variable MLPs, with the latter using dot-product mechanisms to facilitate cross-variable information exchange. The model uses patch embeddings that encode temporal dependencies, and the multi-scale design allows it to capture patterns at various temporal resolutions simultaneously.

## Key Results
- Achieves state-of-the-art performance across eight real-world datasets, surpassing all Transformer-based and other MLP-based baselines
- Demonstrates superior accuracy in terms of MSE and MAE metrics for long-term forecasting tasks
- Shows that simple MLP architectures with appropriate patch-based representations can outperform complex Transformer models

## Why This Works (Mechanism)
The model's effectiveness stems from its ability to capture both local temporal patterns through patch embeddings and global inter-variable relationships through the dot-product mechanism. The feature decomposition strategy effectively separates the signal into interpretable components, allowing the model to focus on different aspects of the time series. The multi-scale patch embeddings enable the model to process temporal information at varying resolutions, capturing both short-term fluctuations and long-term trends simultaneously. The inter-variable MLP layers facilitate information sharing across different time series variables, which is crucial for multivariate forecasting tasks.

## Foundational Learning
- **Patch-based embeddings**: Why needed? To capture local temporal patterns effectively; Quick check: Test with varying patch sizes
- **Feature decomposition**: Why needed? To separate smooth trends from noisy residuals; Quick check: Compare with no decomposition baseline
- **Inter-variable interactions**: Why needed? To leverage cross-variable dependencies in multivariate forecasting; Quick check: Test with and without inter-variable layers
- **Multi-scale processing**: Why needed? To capture patterns at different temporal resolutions; Quick check: Compare single-scale vs multi-scale performance
- **Dot-product mechanisms**: Why needed? To enable semantic information exchange across variables; Quick check: Test with alternative interaction methods
- **Moving average decomposition**: Why needed? Simple yet effective method for signal separation; Quick check: Compare with more sophisticated decomposition techniques

## Architecture Onboarding
- **Component map**: Input Time Series -> Moving Average Decomposition -> Multi-scale Patch Embeddings -> Intra-variable MLP -> Inter-variable MLP -> Output
- **Critical path**: Feature decomposition → patch embedding → inter-variable processing → forecasting
- **Design tradeoffs**: Simplicity vs. performance (MLP vs. Transformer), decomposition complexity vs. effectiveness, patch size vs. computational cost
- **Failure signatures**: Poor performance with highly irregular patterns, sensitivity to moving average window selection, computational bottlenecks with very long sequences
- **First experiments**:
  1. Ablation study removing inter-variable MLP layers to test cross-variable interaction importance
  2. Testing different moving average window sizes to find optimal decomposition parameters
  3. Comparing single-scale vs multi-scale patch embedding performance

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: How does the performance of PatchMLP scale with extremely long input sequences (e.g., 10,000+ time steps) compared to state-of-the-art Transformer models?
- **Basis in paper**: [inferred] The paper demonstrates strong performance with input lengths up to 768, but does not test extremely long sequences where Transformer limitations become more pronounced
- **Why unresolved**: The paper only tests up to 768 input length, leaving the question of performance at orders of magnitude larger sequences unanswered
- **What evidence would resolve it**: Comparative experiments testing PatchMLP against Transformer models on datasets with input sequences of 10,000+ time steps, measuring both accuracy and computational efficiency

### Open Question 2
- **Question**: What is the theoretical relationship between patch size selection and the frequency characteristics of different time series datasets?
- **Basis in paper**: [explicit] The paper notes that optimal patch size varies with input length and dataset, but does not provide a theoretical framework for patch size selection based on time series properties
- **Why unresolved**: The empirical results show patch size affects performance, but the paper does not explain why certain patch sizes work better for specific types of time series (e.g., high-frequency vs. low-frequency data)
- **What evidence would resolve it**: A mathematical framework linking patch size to the spectral properties of time series, validated through experiments showing improved patch size selection across diverse datasets

### Open Question 3
- **Question**: Can the feature decomposition strategy using moving averages be improved by incorporating more sophisticated signal processing techniques?
- **Basis in paper**: [explicit] The paper uses simple moving averages for feature decomposition, noting it works well but acknowledging this is a relatively basic approach
- **Why unresolved**: The paper demonstrates effectiveness of moving average decomposition but does not explore whether more advanced techniques (wavelet transforms, empirical mode decomposition, etc.) could yield better results
- **What evidence would resolve it**: Comparative experiments testing alternative decomposition methods against moving averages across multiple datasets, measuring both forecasting accuracy and computational cost

### Open Question 4
- **Question**: How does PatchMLP's performance change when applied to multivariate time series with varying degrees of cross-variable correlation?
- **Basis in paper**: [explicit] The paper emphasizes the importance of inter-variable interactions but does not systematically test performance across datasets with different correlation structures
- **Why unresolved**: While the model incorporates cross-variable interactions, the paper does not investigate how sensitive performance is to the strength and structure of these correlations
- **What evidence would resolve it**: Controlled experiments varying the correlation structure in synthetic datasets and testing PatchMLP's performance relative to models that do/do not leverage cross-variable information

## Limitations
- Limited testing on extremely long input sequences where Transformer limitations become more pronounced
- Moving average decomposition requires manual tuning of window size
- Scalability to very high-dimensional multivariate time series (thousands of variables) remains untested
- Computational complexity for extremely long sequences could become prohibitive

## Confidence
- **High Confidence**: Patch-based embeddings with multi-scale processing consistently improve forecasting accuracy across datasets
- **Medium Confidence**: The decomposition into smooth and residual components provides robust performance gains
- **Medium Confidence**: Inter-variable interaction through dot-product mechanisms is essential for multivariate forecasting

## Next Checks
1. Test PatchMLP on datasets with significantly more than 100 variables to evaluate scalability limits
2. Conduct ablation studies comparing different decomposition techniques beyond moving averages
3. Evaluate performance on synthetic datasets with controlled correlation structures to understand sensitivity to inter-variable dependencies