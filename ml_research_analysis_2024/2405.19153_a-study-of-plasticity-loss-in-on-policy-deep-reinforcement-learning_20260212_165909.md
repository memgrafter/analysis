---
ver: rpa2
title: A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning
arxiv_id: '2405.19153'
source_url: https://arxiv.org/abs/2405.19153
tags:
- plasticity
- loss
- learning
- performance
- round
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines plasticity loss in on-policy deep reinforcement
  learning, where models degrade in their ability to fit new tasks over time. The
  authors introduce three types of distribution shifts (permute, window, and expand)
  and evaluate various mitigation methods.
---

# A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.19153
- Source URL: https://arxiv.org/abs/2405.19153
- Authors: Arthur Juliani; Jordan T. Ash
- Reference count: 40
- Key outcome: Soft shrink+perturb with LayerNorm consistently mitigates plasticity loss across gridworld, CoinRun, and Montezuma's Revenge environments

## Executive Summary
This paper examines plasticity loss in on-policy deep reinforcement learning, where models degrade in their ability to fit new tasks over time. The authors introduce three types of distribution shifts (permute, window, and expand) and evaluate various mitigation methods. They find that plasticity loss is pervasive in on-policy RL and that many previously proposed methods (e.g., CReLU, plasticity injection) fail to address it. Instead, they identify that weight magnitude and dead unit count are significant predictors of plasticity loss. The most effective interventions are those that continuously regularize weights toward their initial distribution, particularly soft shrink+perturb combined with LayerNorm.

## Method Summary
The study uses PPO algorithm with dual-head architecture (policy and value) and evaluates plasticity loss under three distribution shift conditions: permute (pixel shuffling), window (new environments), and expand (expanded data). The authors implement baseline methods (warm-start, reset-all) and various interventions including L2 regularization, regenerative regularization, shrink+perturb variants, CReLU, plasticity injection, LayerNorm, ReDo, and reset-final. Experiments are conducted across three environments: gridworld (NeuroNav), CoinRun (ProcGen), and Montezuma's Revenge (Atari), with 5-seed runs for 10 training rounds.

## Key Results
- Plasticity loss is pervasive in on-policy RL, with warm-start methods degrading over rounds
- Weight magnitude and dead unit count are significant predictors of plasticity loss (p < 0.05)
- Intermittent interventions like CReLU and plasticity injection fail to address plasticity loss
- Soft shrink+perturb with LayerNorm consistently mitigates plasticity loss across all conditions
- Regenerative L2 regularization also shows effectiveness in reducing plasticity loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight magnitude growth beyond initialization distribution causes plasticity loss
- Mechanism: As weights grow away from their initial distribution, neurons become saturated or "dead" (ReLU outputs always negative), reducing the model's ability to fit new tasks
- Core assumption: The initial weight distribution represents an optimal prior for task adaptation
- Evidence anchors:
  - [abstract] "we find that weight magnitude and dead unit count are both significantly correlated with the normalized reward [p < 0.05]"
  - [section 4.2] "we find that weight magnitude and number of dead units are both significantly correlated with the normalized reward [p < 0.05]"
  - [corpus] Weak correlation - no direct corpus evidence for this specific mechanism
- Break condition: If task requires weights to grow significantly beyond initialization (e.g., extreme scaling)

### Mechanism 2
- Claim: LayerNorm resolves training plasticity loss by normalizing weight distributions
- Mechanism: LayerNorm constrains weight magnitudes within layers, preventing them from growing unbounded and maintaining representational capacity
- Core assumption: Weight magnitude growth is the primary driver of plasticity loss
- Evidence anchors:
  - [abstract] "LayerNorm... was previously introduced to address plasticity loss in the off-policy setting"
  - [section 4.5] "LayerNorm resolves plasticity loss in terms of training performance"
  - [corpus] Weak correlation - LayerNorm mentioned but not detailed mechanism
- Break condition: If LayerNorm interferes with task-specific scaling requirements

### Mechanism 3
- Claim: Regenerative regularization maintains plasticity by keeping weights near initialization
- Mechanism: L2 penalty between current weights and initialization distribution prevents drift while allowing adaptation
- Core assumption: Weight initialization contains useful inductive biases that should be preserved
- Evidence anchors:
  - [abstract] "The most effective interventions are those that continuously regularize weights toward their initial distribution"
  - [section 4.8] "regenerative regularization... are able to significantly mitigate plasticity loss"
  - [corpus] Weak correlation - regenerative regularization mentioned but not detailed mechanism
- Break condition: If initialization is poor or task requires weights to deviate significantly from initialization

## Foundational Learning

- Concept: Distribution shift types (permute, window, expand)
  - Why needed here: Understanding how different distribution shifts affect plasticity loss is crucial for evaluating mitigation methods
  - Quick check question: What distinguishes the expand condition from the window condition?

- Concept: Plasticity loss vs. catastrophic forgetting
  - Why needed here: Plasticity loss is distinct from catastrophic forgetting - it's about degraded ability to fit new tasks, not forgetting old ones
  - Quick check question: How does plasticity loss differ from overfitting?

- Concept: On-policy vs. off-policy RL
  - Why needed here: The paper specifically examines plasticity loss in on-policy RL, which has different characteristics than off-policy RL
  - Quick check question: What's the key difference between on-policy and off-policy RL in terms of data collection?

## Architecture Onboarding

- Component map:
  - PPO algorithm with dual-head architecture (policy and value)
  - MLP encoder for gridworld, CNN encoder for ProcGen
  - Various intervention methods (LayerNorm, regenerative regularization, etc.)
  - Distribution shift mechanisms (permute, window, expand)

- Critical path:
  1. Implement PPO with baseline architecture
  2. Add distribution shift mechanisms
  3. Implement baseline plasticity loss measurement
  4. Add intervention methods one at a time
  5. Measure and compare performance across conditions

- Design tradeoffs:
  - Continuous vs. intermittent interventions (continuous preferred for practical use)
  - Weight normalization (LayerNorm) vs. explicit regularization (regenerative)
  - Model complexity vs. plasticity preservation

- Failure signatures:
  - Increasing weight magnitude over training rounds
  - Growing number of dead units (negative ReLU outputs)
  - Performance degradation on new tasks despite convergence on training data
  - Generalization performance worse than random initialization

- First 3 experiments:
  1. Baseline: Warm-start PPO on gridworld with permute shift
  2. Intervention: Add LayerNorm to baseline architecture
  3. Intervention: Add regenerative regularization to baseline architecture

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the abstract or conclusion sections. However, based on the discussion, several implicit open questions emerge regarding the relationship between weight dynamics and plasticity loss, the fundamental differences between on-policy and off-policy plasticity mitigation, and the generalizability of findings across different RL domains.

## Limitations
- Experimental scope limited to specific environments (gridworld, CoinRun, Montezuma's Revenge)
- Focus on weight magnitude as primary driver without establishing definitive causation
- Potential environment-specific hyperparameters that may not generalize

## Confidence
- **High Confidence**: Plasticity loss is pervasive in on-policy RL
- **Medium Confidence**: Weight magnitude and dead unit count are significant predictors
- **Medium Confidence**: Soft shrink+perturb with LayerNorm is the most effective intervention

## Next Checks
1. Test the soft shrink+perturb + LayerNorm intervention on additional RL domains beyond the three studied environments to assess generalizability
2. Conduct ablation studies to isolate the individual contributions of weight regularization versus LayerNorm normalization
3. Perform longer-term experiments (>10 rounds) to evaluate if the identified interventions prevent plasticity loss accumulation over extended training horizons