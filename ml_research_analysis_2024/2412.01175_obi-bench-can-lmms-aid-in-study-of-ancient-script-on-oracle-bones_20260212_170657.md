---
ver: rpa2
title: 'OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?'
arxiv_id: '2412.01175'
source_url: https://arxiv.org/abs/2412.01175
tags:
- oracle
- lmms
- bone
- characters
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces OBI-Bench, a comprehensive benchmark for
  evaluating large multi-modal models (LMMs) on oracle bone inscription (OBI) tasks.
  It includes 5,523 images from 11 sources, covering five key tasks: recognition,
  rejoining, classification, retrieval, and deciphering.'
---

# OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?

## Quick Facts
- **arXiv ID**: 2412.01175
- **Source URL**: https://arxiv.org/abs/2412.01175
- **Reference count**: 40
- **Primary result**: LMMs struggle with fine-grained perception tasks but show promise in offering novel interpretations of undeciphered oracle bone characters.

## Executive Summary
This paper introduces OBI-Bench, a comprehensive benchmark for evaluating large multi-modal models (LMMs) on oracle bone inscription (OBI) tasks. The benchmark includes 5,523 images from 11 sources and covers five key tasks: recognition, rejoining, classification, retrieval, and deciphering. Evaluation of 23 LMMs, including GPT-4o and Gemini 1.5 Pro, reveals that while current models struggle with fine-grained visual perception tasks like character localization and counting, they perform comparably to untrained humans in deciphering tasks, demonstrating potential for generating novel interpretative perspectives. The study highlights the need for domain-specific LMM development to better support OBI research.

## Method Summary
The OBI-Bench benchmark evaluates LMMs on five key tasks using 5,523 oracle bone inscription images. The evaluation employs zero-shot prompting with specific question types across two perception quadrants: coarse-to-fine and excavation-to-synthesis. Tasks include character recognition with bounding box localization and counting, fragment rejoining prediction, character classification, image retrieval, and deciphering undeciphered characters. The benchmark tests 23 LMMs (6 proprietary, 17 open-source) against a human baseline of 5 untrained participants, using metrics like mIoU, MRE, accuracy@k, and BERTScore. Two-stage annotation ensures data quality, with expert verification of initial annotations.

## Key Results
- LMMs show significant performance gaps in fine-grained perception tasks like character localization (mIoU) and counting (MRE), with even top models far from public-level human performance.
- GPT-4o, Gemini 1.5 Pro, and Qwen-VL-Max achieve comparable performance to untrained humans in deciphering tasks, demonstrating ability to generate novel interpretations.
- Image quality significantly impacts LMM performance, with noisy or low-quality OBI images degrading model accuracy across all tasks.
- LMMs perform better on pictograph characters than ideograms in deciphering tasks, suggesting visual association mechanisms for interpretation.

## Why This Works (Mechanism)

### Mechanism 1
LMMs can provide novel interpretations of undeciphered oracle bone characters by leveraging cross-modal understanding. They combine visual perception and natural language understanding to generate interpretations that go beyond direct character-to-character mapping, establishing visual associations between font structure and real objects.

### Mechanism 2
LMMs struggle with fine-grained visual perception tasks but can handle coarse-grained perception and basic classification. Their architecture prioritizes high-level semantic understanding over precise spatial localization, processing global image features effectively while having difficulty with pixel-level localization and counting tasks.

### Mechanism 3
The quality of input images significantly affects LMM performance on OBI tasks. Noisy or low-quality images introduce visual artifacts that LMMs cannot effectively filter, leading to degraded performance. Models rely on clean visual features to extract meaningful patterns and relationships.

## Foundational Learning

- **Visual question answering (VQA) in multimodal contexts**: Why needed - OBI-Bench is structured as a VQA benchmark where LMMs must answer questions about oracle bone inscription images. Quick check - What are the two main components LMMs must integrate to successfully complete VQA tasks in the OBI domain?

- **Cross-modal representation learning**: Why needed - LMMs must align visual features of oracle bone characters with textual descriptions and interpretations. Quick check - How does the ability to align visual and textual modalities enable LMMs to generate interpretations for undeciphered characters?

- **Fine-grained visual perception**: Why needed - Many OBI tasks require precise localization, counting, and structural analysis of characters. Quick check - What architectural features would be needed to improve LMMs' performance on tasks requiring fine-grained visual perception?

## Architecture Onboarding

- **Component map**: Vision encoder (CLIP, SigLIP, EVA2-CLIP) → Cross-attention mechanism → Language decoder (LLaMA, Qwen, Phi) → Output generator
- **Critical path**: Image → Vision features → Cross-modal alignment → Question processing → Answer generation
- **Design tradeoffs**: Larger models perform better but require more computational resources; open-source models lag behind proprietary ones in performance
- **Failure signatures**: Inability to locate characters precisely, confusion between similar characters, poor performance on low-quality images
- **First 3 experiments**:
  1. Evaluate LMM performance on clean vs. noisy oracle bone images to quantify the impact of image quality
  2. Test different prompting strategies (role assignment, case instruction) to improve deciphering accuracy
  3. Compare performance across different vision architectures (CLIP vs. SigLIP vs. EVA2-CLIP) to identify optimal feature extraction methods

## Open Questions the Paper Calls Out

### Open Question 1
Can domain-specific fine-tuning of LMMs on interdisciplinary OBI datasets significantly improve performance on fine-grained perception tasks like character location and quantity detection? The paper evaluates general LMMs without fine-tuning, leaving the potential impact of domain-specific fine-tuning unexplored.

### Open Question 2
How do cultural biases in LMM training data affect their performance on OBI classification and retrieval tasks? The study only compares two cultural groups and does not explore the influence of specific cultural elements in training data on model performance.

### Open Question 3
What is the optimal combination of preprocessing techniques and LMM architectures for enhancing OBI image quality and improving model performance? The paper mentions the need for preprocessing but does not experiment with different preprocessing pipelines or architectural modifications tailored to OBI image characteristics.

## Limitations

- **Dataset representativeness**: The benchmark's coverage of the full oracle bone inscription domain remains unclear, potentially limiting generalizability to real-world research scenarios.
- **Evaluation methodology constraints**: Zero-shot evaluation doesn't leverage fine-tuning or domain adaptation that could significantly improve LMM performance.
- **Interpretation quality assessment**: BERTScore evaluation may not adequately capture semantic accuracy or cultural/historical appropriateness of model-generated interpretations.

## Confidence

- **High confidence**: LMMs struggle with fine-grained visual perception tasks (character localization, counting) - Well-supported by quantitative metrics across multiple models and tasks.
- **Medium confidence**: LMMs can provide novel interpretations of undeciphered characters through visual associations - Evidence presented but evaluation framework doesn't fully validate usefulness for research.
- **Low confidence**: LMMs perform at "untrained human" level in deciphering tasks - Human baseline comparison may be misleading as it uses untrained participants rather than experts.

## Next Checks

- **Validation Check 1**: Conduct domain-expert evaluation of LMM interpretations by having OBI researchers assess accuracy and usefulness compared to established scholarly work.
- **Validation Check 2**: Test impact of fine-tuning LMMs on OBI-specific data by training open-source models on the OBI-Bench dataset and evaluating performance improvements.
- **Validation Check 3**: Evaluate LMM performance across different oracle bone inscription styles and time periods to assess benchmark coverage of full OBI research complexity.