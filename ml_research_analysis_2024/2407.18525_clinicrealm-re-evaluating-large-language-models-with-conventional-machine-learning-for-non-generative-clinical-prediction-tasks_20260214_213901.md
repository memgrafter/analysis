---
ver: rpa2
title: 'ClinicRealm: Re-evaluating Large Language Models with Conventional Machine
  Learning for Non-Generative Clinical Prediction Tasks'
arxiv_id: '2407.18525'
source_url: https://arxiv.org/abs/2407.18525
tags:
- data
- clinical
- tasks
- medical
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The ClinicRealm study provides the first comprehensive benchmarking
  of large language models (LLMs) for non-generative clinical prediction tasks. It
  evaluates 15 GPT-style LLMs, 5 BERT-style models, and 11 traditional methods on
  unstructured clinical notes and structured EHR data.
---

# ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks

## Quick Facts
- arXiv ID: 2407.18525
- Source URL: https://arxiv.org/abs/2407.18525
- Reference count: 40
- Key outcome: Comprehensive benchmarking of 15 GPT-style LLMs, 5 BERT-style models, and 11 traditional methods on clinical prediction tasks shows LLMs are competitive tools, especially for unstructured text and data-scarce settings.

## Executive Summary
ClinicRealm provides the first comprehensive benchmarking of large language models (LLMs) for non-generative clinical prediction tasks. The study evaluates 15 GPT-style LLMs, 5 BERT-style models, and 11 traditional methods on unstructured clinical notes and structured EHR data. For structured EHRs, leading LLMs in zero-shot settings decisively outperform finetuned BERT models. On clinical notes, LLMs achieve performance comparable to BERT-based models but do not surpass them. The results demonstrate that modern LLMs are competitive tools for non-generative clinical prediction, particularly with unstructured text and offering data-efficient structured data options.

## Method Summary
The study uses a comprehensive benchmarking framework with prompt engineering strategies to assess LLMs' capabilities in zero-shot and few-shot settings alongside supervised and unsupervised tasks. The researchers prepared MIMIC and TJH datasets with proper preprocessing (LOCF imputation for structured EHR data, minimal preprocessing for clinical notes) and implemented prompt templates for structured EHR and clinical notes tasks. Experiments were run using specified models (LLMs, BERT-based, conventional ML/DL) following the evaluation metrics and settings described in the paper.

## Key Results
- Leading LLMs (e.g., DeepSeek-V3.1-Think, GPT-5) in zero-shot settings decisively outperform finetuned BERT models on structured EHR data
- On clinical notes, LLMs achieve performance comparable to BERT-based models but do not surpass them
- Advanced LLMs demonstrate potent zero-shot capabilities on structured EHRs, often surpassing conventional models in data-scarce settings

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot LLM performance on structured EHR data improves with feature-wise prompt formatting and contextual knowledge integration. LLMs trained on unstructured text struggle with numerical, sparse, and longitudinal EHR data. The proposed prompting strategy converts EHR features into natural language with units and reference ranges, providing the necessary context for the LLM to interpret values correctly. The feature-wise format allows the model to track temporal patterns across visits more effectively than visit-wise or raw numerical input.

### Mechanism 2
BERT-based models outperform LLMs on unstructured clinical notes when finetuned, but LLMs show comparable performance in frozen settings. BERT models pretrained on medical corpora develop strong representations for clinical language through domain-specific pretraining. When finetuned on task-specific clinical note data, these representations become even more specialized, outperforming LLMs. LLMs in frozen settings lack this specialized medical pretraining, resulting in comparable but not superior performance to BERT.

### Mechanism 3
LLMs demonstrate strong zero-shot capabilities in data-scarce settings but underperform specialized models with ample training data. LLMs leverage their extensive pretraining on diverse text corpora to perform zero-shot predictions on new tasks. In data-scarce scenarios, this pretrained knowledge provides a significant advantage over models that require task-specific training data. However, with ample training data, specialized models can learn task-specific patterns that surpass the general knowledge of LLMs.

## Foundational Learning

- **Prompt engineering for structured data**: Why needed here - LLMs are primarily trained on unstructured text and need explicit guidance to interpret structured, numerical clinical data effectively. Quick check question: How would you format a blood pressure reading of 120/80 mmHg in a prompt to help an LLM understand its clinical significance?

- **Domain-specific pretraining vs. general-purpose pretraining**: Why needed here - Understanding why BERT models finetuned on medical corpora outperform general LLMs on clinical text tasks. Quick check question: What are the key differences in pretraining objectives between BERT and GPT-style LLMs that affect their performance on clinical text?

- **Zero-shot vs. few-shot vs. full training paradigms**: Why needed here - Different model selection strategies are optimal depending on data availability, with zero-shot being crucial for emerging diseases or early pandemic stages. Quick check question: In what scenarios would you choose zero-shot prediction over finetuning a specialized model, considering both performance and deployment constraints?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Prompt template generation -> LLM inference -> Result post-processing -> Evaluation metrics
- **Critical path**: Prompt template design -> LLM selection -> Context integration (units/reference ranges) -> In-context learning examples -> Inference configuration
- **Design tradeoffs**: Feature-wise vs. visit-wise EHR formatting (computational efficiency vs. temporal context preservation), unit/reference range inclusion (performance improvement vs. prompt length), number of in-context examples (performance vs. context window limitations)
- **Failure signatures**: High missing rates in LLM predictions (indicating poor prompt comprehension), performance degradation with excessive in-context examples (context window saturation), inconsistent performance across different EHR feature sets (formatting sensitivity)
- **First 3 experiments**:
  1. Compare feature-wise vs. visit-wise EHR formatting with GPT-4 on TJH mortality prediction
  2. Test LLM performance with and without units/reference ranges on MIMIC mortality prediction
  3. Evaluate different numbers of in-context examples (0, 1, 2, 3) on both datasets to find optimal balance

## Open Questions the Paper Calls Out

1. **Do LLMs consistently outperform conventional models for zero-shot predictions on structured EHR data across different medical specialties (e.g., oncology, cardiology, neurology)?**
   - Basis in paper: The paper demonstrates LLMs' zero-shot capabilities on ICU and COVID-19 datasets but does not explore other medical specialties.
   - Why unresolved: The study focuses on two specific datasets (MIMIC-IV and TJH) representing critical care and pandemic scenarios. Generalization to other medical domains remains untested.
   - What evidence would resolve it: Benchmarking LLMs on structured EHR data from diverse medical specialties (e.g., oncology, cardiology, neurology) using zero-shot prompting approaches and comparing results to conventional models.

2. **What is the optimal balance between prompt length and performance when using feature-wise vs. visit-wise formats for structured EHR data?**
   - Basis in paper: The paper explores feature-wise vs. visit-wise formats and notes that feature-wise reduces token usage by 37.50% on MIMIC-IV, but does not optimize the trade-off between prompt length and predictive accuracy.
   - Why unresolved: While the paper demonstrates the effectiveness of feature-wise formatting for reducing tokens, it does not systematically explore how prompt length impacts performance or identify an optimal balance.
   - What evidence would resolve it: Controlled experiments varying prompt length and structure while measuring both performance metrics and computational efficiency across different EHR datasets.

3. **How do catastrophic forgetting and context window limitations affect LLM performance on clinical prediction tasks with increasing numbers of in-context learning examples?**
   - Basis in paper: The paper observes performance degradation when adding 3 examples in in-context learning, suggesting catastrophic forgetting or context window limitations, but does not systematically investigate these phenomena.
   - Why unresolved: The study identifies potential issues with excessive in-context examples but does not explore the underlying mechanisms or develop strategies to mitigate these limitations.
   - What evidence would resolve it: Systematic studies varying the number of in-context examples while measuring performance, investigating context window effects, and testing techniques like retrieval-augmented generation or parameter-efficient fine-tuning.

## Limitations

- Performance comparisons are primarily based on two datasets (MIMIC and TJH), which may not represent the full diversity of clinical data across different healthcare systems and patient populations
- The prompt engineering strategies, while effective, may not generalize well to other clinical prediction tasks or data formats not tested in this study
- Computational resources required for LLM inference may limit practical deployment in resource-constrained healthcare settings

## Confidence

**High Confidence**: The finding that BERT-based models outperform LLMs on clinical notes when finetuned, while LLMs achieve comparable performance in frozen settings.

**Medium Confidence**: The claim that leading LLMs decisively outperform finetuned BERT models on structured EHR data in zero-shot settings.

**Low Confidence**: The assertion that advanced LLMs can match or exceed proprietary counterparts.

## Next Checks

1. **Cross-dataset validation**: Replicate the benchmarking framework using clinical datasets from different healthcare systems (e.g., European or Asian medical records) to verify if the observed performance patterns hold across diverse clinical data sources and patient demographics.

2. **Data scarcity stress test**: Systematically vary the amount of training data available for both LLMs and traditional methods across multiple clinical prediction tasks to quantify the exact data thresholds where LLMs become competitive with specialized models.

3. **Prompt engineering generalization**: Test the proposed prompt templates on clinical prediction tasks beyond mortality prediction (e.g., readmission prediction, length of stay estimation) to evaluate whether the observed improvements from feature-wise formatting and contextual knowledge integration are task-specific or broadly applicable.