---
ver: rpa2
title: Augmented prediction of a true class for Positive Unlabeled data under selection
  bias
arxiv_id: '2407.10309'
source_url: https://arxiv.org/abs/2407.10309
tags:
- vp-b
- positive
- rule
- unlabeled
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the problem of "augmented PU prediction"
  where both predictors and labeling status are available for prediction, contrasting
  with standard PU learning that uses predictors only. The authors derive the optimal
  Bayes decision rule for this setting, showing it is more conservative than the standard
  Bayes rule by adjusting the classification threshold based on the labeling probability.
---

# Augmented prediction of a true class for Positive Unlabeled data under selection bias

## Quick Facts
- arXiv ID: 2407.10309
- Source URL: https://arxiv.org/abs/2407.10309
- Reference count: 40
- Primary result: Introduces augmented PU prediction using both predictors and labeling status, with VAE-PU-Bayes method showing modest but consistent improvements over feature-only approaches

## Executive Summary
This paper introduces the problem of "augmented PU prediction" where both predictors and labeling status are available for prediction, contrasting with standard PU learning that uses predictors only. The authors derive the optimal Bayes decision rule for this setting, showing it is more conservative than the standard Bayes rule by adjusting the classification threshold based on the labeling probability. They prove bounds on the excess risk of using the standard rule instead. The paper proposes variants of empirical Bayes classifiers, particularly one based on a variational autoencoder for PU data (VAE-PU-Bayes), which performs on par or better than other variants and improves accuracy over feature-only methods for unlabeled samples. Experiments on synthetic and real-world datasets show consistent but modest gains from using the augmented decision rule over naive approaches, with the biggest improvements seen when true posterior probabilities are known.

## Method Summary
The paper proposes a framework for augmented Positive Unlabeled (PU) prediction where labeling status is available at prediction time. The core contribution is the derivation of the optimal Bayes decision rule (dP U B) for this setting, which adjusts the classification threshold based on the labeling probability s(x). The authors implement three empirical variants: LBE+S (using LBE classifier with explicit modeling of posterior and propensity), V AE-PU+S (V AE-PU with separate s(x) estimator), and V AE-PU-Bayes+S (V AE-PU-Bayes with external s(x) estimator). These methods are evaluated using U-Accuracy and U-Balanced Accuracy metrics on both synthetic datasets generated from 20D Gaussian mixtures and real-world datasets with modified labeling mechanisms.

## Key Results
- The VAE-PU-Bayes method consistently improves U-Accuracy over feature-only methods (VAE-PU) for unlabeled samples
- The augmented decision rule shows the most significant gains when true posterior probabilities are known (S-Prophet and Y-Prophet methods)
- Performance improvements are modest in practice but demonstrate the potential of leveraging labeling information in PU learning
- The method requires labeling status at test time, limiting its applicability to standard PU scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dP U B decision rule improves accuracy by adjusting the classification threshold based on labeling probability s(x).
- Mechanism: The rule uses the fact that unlabeled data contains relatively fewer positive examples than the general population. It increases the threshold for classifying an example as positive when s(x) is large, making the rule more conservative for unlabeled data.
- Core assumption: The SAR assumption holds - labeling probability e(x) depends on features x but is not constant.
- Evidence anchors:
  - [abstract]: "Bayes classifier and its risk is established and compared with a risk of a classifier which for unlabeled data is based only on predictors."
  - [section]: "Directly from the above definitions we have that dP U B (X, S) is more conservative on class S = 0 than dB(X) i.e. it less likely assigns objects to the positive class"
- Break condition: If the SAR assumption is violated and labeling is actually independent of features (SCAR), then e(x) becomes constant and the threshold adjustment becomes uniform across all features.

### Mechanism 2
- Claim: VAE-PU-Bayes improves performance by using both posterior probability y(x) and propensity score s(x) in the decision rule.
- Mechanism: Instead of relying solely on y(x) estimates, VAE-PU-Bayes combines y(x) and s(x) through the dP U B rule. This leverages more information from the PU data structure, particularly the relationship s(x) = e(x)y(x).
- Core assumption: The VAE can accurately estimate both y(x) and s(x) from the PU data.
- Evidence anchors:
  - [section]: "We show that the variant based on variational autoencoder designed for PU data works promisingly when accuracy relative to unlabeled data is considered"
  - [section]: "V AE-PU-Bayes combines such a classifier with the V AE-PUy(x) estimation in order to apply dP U B rule"
- Break condition: If the VAE fails to accurately estimate either y(x) or s(x), the combined rule will not outperform methods using only y(x) estimates.

### Mechanism 3
- Claim: The performance gain from using dP U B is most pronounced when true posterior probabilities are known.
- Mechanism: When true y(x) and s(x) values are available (as in "S-Prophet" and "Y-Prophet" methods), the dP U B rule shows substantial improvements over the naive approach, especially at high label frequencies.
- Core assumption: The true posterior probabilities can be accurately estimated or are known.
- Evidence anchors:
  - [section]: "The results for the two Prophet methods (which use perfect knowledge of y(x) and s(x)), as well as semi-Prophets (utilizing the perfect knowledge of only one of those variables) indicate that those improvements could be potentially be significantly larger"
  - [section]: "S-Prophet is nearly equivalent to Y-Prophet in low label frequency setting" - showing the difference becomes significant when we have accurate estimates
- Break condition: If the estimation error for y(x) or s(x) is large, the potential gains from using the dP U B rule diminish significantly.

## Foundational Learning

- Concept: Bayesian decision theory and Bayes risk
  - Why needed here: The paper establishes dP U B as the Bayes optimal rule for the augmented PU scenario and calculates its risk
  - Quick check question: What is the Bayes risk for a 0-1 loss classifier that assigns class 1 when y(x) > 0.5?

- Concept: Variational autoencoders and generative modeling
  - Why needed here: VAE-PU and VAE-PU-Bayes are central to the proposed empirical methods
  - Quick check question: How does a VAE learn to model the joint distribution of (X, Y, S) in PU data?

- Concept: Selection bias and propensity scores
  - Why needed here: The paper explicitly models labeling as feature-dependent (SAR assumption) and uses propensity scores e(x)
  - Quick check question: What is the relationship between s(x), e(x), and y(x) under the SAR assumption?

## Architecture Onboarding

- Component map: Synthetic/real datasets with PU structure and labeling status -> VAE-based PU classifier (VAE-PU) -> Auxiliary network for propensity score estimation s(x) -> dP U B decision rule combining y(x) and s(x) estimates -> U-Accuracy evaluation on unlabeled stratum

- Critical path:
  1. Train VAE-PU on PU training data to estimate y(x)
  2. Train auxiliary network on (Xi, Si) pairs to estimate s(x)
  3. Apply dP U B rule: classify as positive if y(x) > (1 + s(x))/2 for S=0
  4. Evaluate U-Accuracy on unlabeled test examples

- Design tradeoffs:
  - Using separate networks for y(x) and s(x) allows specialized training but requires more parameters
  - The conservative threshold adjustment reduces false positives but may increase false negatives
  - The method requires labeling status at test time, limiting applicability to standard PU scenarios

- Failure signatures:
  - If VAE-PU fails to learn meaningful y(x) estimates, performance will match naive methods
  - If propensity score estimates s(x) are inaccurate, threshold adjustments will be incorrect
  - If the SAR assumption is violated, the dP U B rule may perform worse than standard methods

- First 3 experiments:
  1. Implement VAE-PU on synthetic data with known y(x) and s(x) to verify it can recover these quantities
  2. Add the auxiliary network for s(x) estimation and test on synthetic data where true s(x) is known
  3. Combine both estimators using dP U B rule and compare U-Accuracy against naive approach on synthetic data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed dP U B rule perform on datasets with non-logistic posterior probabilities and propensity scores that are not sigmoid functions?
- Basis in paper: [explicit] The paper evaluates the dP U B rule on synthetic datasets with various posterior probability and propensity score combinations, but does not explore non-logistic and non-sigmoid cases.
- Why unresolved: The paper focuses on specific model choices for the experiments, leaving the generalization of the dP U B rule to other distributions unexplored.
- What evidence would resolve it: Conducting experiments with different posterior probability and propensity score distributions, such as Gaussian, exponential, or polynomial, would demonstrate the rule's performance under these conditions.

### Open Question 2
- Question: What is the impact of the dP U B rule on datasets with a large number of features or high-dimensional data?
- Basis in paper: [inferred] The paper uses synthetic datasets with 20 features and real-world datasets with varying feature counts, but does not specifically address high-dimensional data scenarios.
- Why unresolved: The scalability and effectiveness of the dP U B rule in high-dimensional settings are not investigated, which is crucial for real-world applications with complex data.
- What evidence would resolve it: Evaluating the dP U B rule on datasets with hundreds or thousands of features, and comparing its performance to traditional methods in these settings, would provide insights into its applicability to high-dimensional data.

### Open Question 3
- Question: How sensitive is the dP U B rule to the estimation accuracy of the posterior probability y(x) and the propensity score s(x)?
- Basis in paper: [explicit] The paper mentions that the performance of the dP U B rule is affected by the accuracy of y(x) and s(x) estimations, but does not quantify this sensitivity or explore methods to improve estimation accuracy.
- Why unresolved: Understanding the sensitivity of the dP U B rule to estimation errors is crucial for practical applications, as perfect estimations are rarely available in real-world scenarios.
- What evidence would resolve it: Conducting experiments that systematically vary the estimation accuracy of y(x) and s(x) and measuring the impact on the dP U B rule's performance would provide insights into its robustness to estimation errors. Additionally, exploring techniques to improve estimation accuracy, such as ensemble methods or regularization, could be investigated.

## Limitations

- The performance gains from using the dP U B decision rule are modest, with the most significant improvements seen only when true posterior probabilities are known
- The proposed VAE-PU-Bayes method requires access to labeling status at prediction time, limiting its applicability to standard PU scenarios
- The empirical evaluation relies heavily on synthetic data where the ground truth is known, and real-world experiments use modified labeling mechanisms that may not reflect practical scenarios

## Confidence

- **High confidence**: The theoretical derivation of the optimal Bayes decision rule dP U B and its comparison with standard Bayes rule under SAR assumption
- **Medium confidence**: The empirical performance of VAE-PU-Bayes on synthetic data, as the architecture details are not fully specified
- **Low confidence**: The practical utility of the augmented prediction framework, given the requirement for labeling status at test time and modest real-world performance gains

## Next Checks

1. Implement the VAE-PU-Bayes method on a standard PU learning benchmark (e.g., MNIST 3v8) without using labeling information at test time to assess practical utility
2. Conduct ablation studies to isolate the contribution of each component (VAE, propensity estimation, threshold adjustment) to the overall performance
3. Test the method on real-world PU data with naturally occurring labeling mechanisms (e.g., document classification with positive labels only) to evaluate practical relevance