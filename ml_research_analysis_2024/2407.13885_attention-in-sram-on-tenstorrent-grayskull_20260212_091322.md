---
ver: rpa2
title: Attention in SRAM on Tenstorrent Grayskull
arxiv_id: '2407.13885'
source_url: https://arxiv.org/abs/2407.13885
tags:
- core
- cores
- matrix
- kernel
- tensix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work implements attention kernels leveraging Tenstorrent Grayskull's
  large SRAM. It introduces a fused kernel combining matrix multiplication, attention
  scaling, and Softmax, as well as a dedicated Softmax kernel, both optimized for
  Grayskull's architecture.
---

# Attention in SRAM on Tenstorrent Grayskull

## Quick Facts
- arXiv ID: 2407.13885
- Source URL: https://arxiv.org/abs/2407.13885
- Authors: Moritz Thüning
- Reference count: 28
- Primary result: Implements attention kernels leveraging Tenstorrent Grayskull's large SRAM with up to 10x speedup over CPU baseline

## Executive Summary
This work presents the implementation of attention kernels optimized for Tenstorrent Grayskull's SRAM architecture. The authors introduce a fused kernel combining matrix multiplication, attention scaling, and Softmax operations, as well as a dedicated Softmax kernel. Experimental results demonstrate significant performance improvements, with the dedicated Softmax kernel achieving up to 10x speedup over CPU baseline, while the fused kernel's Softmax component is 1.8x faster than the dedicated version. The implementation achieves quadratic time and memory complexity in sequence length and leverages Grayskull's architectural advantages including larger SRAM capacity and cost-effectiveness compared to competing solutions.

## Method Summary
The authors implement two attention kernels specifically designed for Tenstorrent Grayskull's SRAM architecture: a fused kernel that combines matrix multiplication, attention scaling, and Softmax operations, and a dedicated Softmax kernel. The fused kernel is optimized to keep data in SRAM during computation, reducing memory transfers. The dedicated Softmax kernel is optimized for Grayskull's architecture to achieve maximum performance. Both kernels are designed to handle the quadratic complexity inherent to attention operations, with memory and computational requirements scaling with the square of the sequence length.

## Key Results
- Dedicated Softmax kernel achieves up to 10x speedup over CPU baseline
- Fused kernel's Softmax component is 1.8x faster than dedicated Softmax kernel
- Both kernels achieve quadratic time and memory complexity in sequence length
- Grayskull e150 offers approximately 1.5x more SRAM and is ~30x cheaper than Nvidia H100 PCIe

## Why This Works (Mechanism)
The performance improvements stem from optimizing attention operations to leverage Grayskull's large SRAM capacity. By keeping intermediate results in SRAM during the fused kernel operations, memory transfer overhead is minimized. The dedicated Softmax kernel is specifically tuned to Grayskull's architectural characteristics, maximizing utilization of available resources. The quadratic complexity approach is appropriate for standard attention mechanisms, and the hardware-specific optimizations enable efficient execution despite this complexity.

## Foundational Learning
- **Attention Mechanisms**: Required for understanding the core operations being optimized; quick check: verify understanding of self-attention and scaled dot-product attention
- **SRAM Optimization**: Critical for grasping the performance gains; quick check: confirm knowledge of SRAM vs DRAM access patterns and costs
- **Fused Kernels**: Important for understanding the design approach; quick check: verify understanding of kernel fusion benefits and tradeoffs
- **Hardware-Software Co-design**: Essential for appreciating the architectural targeting; quick check: confirm understanding of how hardware characteristics influence software optimization
- **Quadratic Complexity**: Necessary for understanding scalability limitations; quick check: verify ability to analyze O(n²) scaling implications
- **Performance Benchmarking**: Important for evaluating claims; quick check: confirm understanding of proper benchmarking methodology and baseline selection

## Architecture Onboarding

**Component Map**: Input Data -> Matrix Multiplication -> Attention Scaling -> Softmax -> Output (for fused kernel); Input Data -> Softmax -> Output (for dedicated kernel)

**Critical Path**: For fused kernel: Matrix multiplication and attention scaling must complete before Softmax can begin; for dedicated kernel: Softmax operation is the primary computational path

**Design Tradeoffs**: Fused kernel reduces memory transfers but may limit flexibility; dedicated kernel offers modularity but incurs additional memory overhead; both accept quadratic complexity for standard attention implementation

**Failure Signatures**: Performance degradation when data exceeds SRAM capacity; reduced speedup when CPU baseline is well-optimized; potential accuracy issues if numerical precision is compromised for speed

**First Experiments**:
1. Benchmark fused kernel with varying sequence lengths to identify SRAM capacity limits
2. Compare performance of fused versus dedicated kernel across different attention configurations
3. Profile memory usage patterns to verify data locality optimizations

## Open Questions the Paper Calls Out
None

## Limitations
- Implementation is specific to Tenstorrent Grayskull architecture, limiting generalizability
- Performance evaluation focuses on single GPU configurations without exploring scaling properties
- CPU baseline comparison lacks specific details about baseline configuration
- Implementation details are not fully specified, making reproduction challenging
- Quadratic complexity for sequence length is expected but may not represent full architectural capabilities

## Confidence
- **High**: Architectural description and kernel design claims
- **Medium**: Speedup claims (lacks CPU baseline specifications)
- **Medium**: SRAM utilization claims (implementation details not fully specified)

## Next Checks
1. Reproduce the CPU baseline performance metrics with detailed hardware specifications to verify the 10x speedup claim
2. Implement the same kernels on a different GPU architecture to assess hardware specificity of the optimizations
3. Evaluate performance scaling beyond single GPU configurations to understand distributed performance characteristics