---
ver: rpa2
title: Soft Actor-Critic with Beta Policy via Implicit Reparameterization Gradients
arxiv_id: '2409.04971'
source_url: https://arxiv.org/abs/2409.04971
tags:
- policy
- learning
- https
- reparameterization
- beta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the use of implicit reparameterization gradients
  to enable Soft Actor-Critic with beta policies. The beta distribution is bounded
  to [0,1] and can improve convergence rates in continuous control, but its gradients
  cannot be computed via the standard reparameterization trick.
---

# Soft Actor-Critic with Beta Policy via Implicit Reparameterization Gradients

## Quick Facts
- arXiv ID: 2409.04971
- Source URL: https://arxiv.org/abs/2409.04971
- Authors: Luca Della Libera
- Reference count: 8
- Key outcome: SAC with beta policy performs comparably to squashed normal policy and outperforms normal policy on MuJoCo tasks

## Executive Summary
This paper investigates the use of implicit reparameterization gradients to enable Soft Actor-Critic (SAC) with beta policies. The beta distribution's bounded support naturally matches action space constraints in continuous control, but its gradients cannot be computed via standard reparameterization. By applying implicit reparameterization through either automatic differentiation or optimal mass transport, the authors enable differentiable sampling from beta distributions in SAC. Experiments on four MuJoCo environments show that SAC with beta policy outperforms the normal policy and performs on par with the squashed normal policy, demonstrating its viability as an alternative.

## Method Summary
The paper applies implicit reparameterization gradients to enable SAC with beta policies. Beta distributions are parameterized using log-concentration parameters shifted by 1, and sampling is achieved through z = z1 / (z1 + z2) where z1 and z2 follow Gamma distributions. Two variants of implicit reparameterization are implemented: automatic differentiation (AD) and optimal mass transport (OMT). The SAC algorithm is trained with beta, normal, and squashed normal policies across four MuJoCo environments (Ant-v4, HalfCheetah-v4, Hopper-v4, Walker2d-v4) for 1M steps with batch size 256, learning rate 0.001, and temperature 0.2.

## Key Results
- SAC with beta policy outperforms SAC with normal policy on all four MuJoCo environments tested
- SAC with beta policy performs comparably to SAC with squashed normal policy
- Both automatic differentiation and optimal mass transport variants of implicit reparameterization yield similar performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Implicit reparameterization gradients enable SAC to sample from beta distributions without requiring a differentiable inverse standardization function.
- Mechanism: The beta distribution is expressed as z = z1 / (z1 + z2), where z1 and z2 follow Gamma distributions. Implicit reparameterization computes gradients by differentiating the standardization function directly, avoiding the need to invert it.
- Core assumption: The standardization function for the gamma distribution can be approximated numerically or via Taylor expansion, making implicit gradients tractable.
- Evidence anchors:
  - [abstract] "we use implicit reparameterization gradients to train SAC with the beta policy"
  - [section] "implicit reparameterization relaxes this constraint, allowing for gradient computations without S−1ϕ(ε)"
  - [corpus] No direct evidence; assumes implicit reparameterization works for gamma/beta as claimed
- Break condition: If the standardization function approximation becomes numerically unstable or inaccurate, gradient estimates will degrade and training will fail.

### Mechanism 2
- Claim: Beta policy outperforms normal policy in SAC because bounded support matches the action space constraints.
- Mechanism: Beta distribution naturally enforces action bounds [0,1], eliminating the need for tanh squashing and reducing bias from infinite support mismatch.
- Core assumption: The bounded nature of beta distributions improves policy optimization stability and convergence in continuous control tasks.
- Evidence anchors:
  - [abstract] "beta distribution...was shown to improve the convergence rate of actor-critic algorithms in high-dimensional continuous control problems thanks to its bounded support"
  - [section] "SAC with beta policy outperforms the normal policy and performs on par with the squashed normal policy"
  - [corpus] Weak evidence; no direct citations about beta improving convergence rates in SAC context
- Break condition: If the action space requires unbounded exploration or if beta's bounded support becomes a limiting factor for task performance.

### Mechanism 3
- Claim: Two variants of implicit reparameterization (automatic differentiation vs optimal mass transport) yield similar performance in practice.
- Mechanism: Both approaches compute gradients for non-reparameterizable distributions, with AD providing more accurate estimates while OMT offers computational simplicity.
- Core assumption: Highly accurate gradients may not be critical for SAC's success, as both methods perform similarly.
- Evidence anchors:
  - [section] "SAC-Beta-AD and SAC-Beta-OMT perform similarly to SAC-TanhNormal" and "we were expecting faster convergence with SAC-Beta-AD...This suggests that highly accurate gradients may not be critical"
  - [corpus] No direct evidence; assumes both methods are valid alternatives
- Break condition: If one method consistently outperforms the other across diverse tasks, indicating that gradient accuracy does matter for specific scenarios.

## Foundational Learning

- Concept: Reparameterization trick for stochastic gradient estimation
  - Why needed here: SAC requires differentiable sampling from policy distributions to compute entropy-augmented objectives
  - Quick check question: What is the main limitation of the standard reparameterization trick that implicit reparameterization addresses?

- Concept: Beta distribution properties and parameterization
  - Why needed here: Understanding why beta distributions are suitable for bounded action spaces and how they're parameterized in SAC
  - Quick check question: How does the beta distribution's bounded support relate to the typical [-1, 1] action space in MuJoCo environments?

- Concept: Maximum entropy reinforcement learning framework
  - Why needed here: SAC's objective function includes entropy regularization, requiring proper handling of stochastic policies
  - Quick check question: What role does the temperature parameter α play in balancing exploration and exploitation in SAC?

## Architecture Onboarding

- Component map: Policy network -> Implicit reparameterization module -> Q-networks -> Target networks -> Replay buffer
- Critical path: Policy update → Q-network update → Target network sync → Experience collection
- Design tradeoffs:
  - Beta vs normal vs squashed normal: Bounded support vs computational complexity vs gradient accuracy
  - AD vs OMT implicit reparameterization: Accuracy vs computational efficiency
  - Clipping strategies: Numerical stability vs preserving distribution properties
- Failure signatures:
  - Exploding or vanishing concentrations (indicates clipping is needed)
  - Q-values diverging (indicates learning rate or target smoothing issues)
  - Poor performance compared to baselines (indicates gradient estimation problems)
- First 3 experiments:
  1. Compare SAC-Beta-AD vs SAC-Beta-OMT on Ant-v4 to verify both methods work
  2. Test beta policy performance with different temperature parameters to find optimal exploration-exploitation balance
  3. Evaluate beta policy on tasks with different action space requirements to test bounded support benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do SAC-Beta-AD and SAC-Beta-OMT perform compared to SAC-TanhNormal across a wider variety of continuous control tasks beyond the four MuJoCo environments tested?
- Basis in paper: [explicit] The paper notes that the beta policy is a viable alternative as it outperforms the normal policy and yields similar results to the squashed normal policy, but acknowledges that more experiments might be necessary to obtain more accurate estimates due to large standard deviation values.
- Why unresolved: The evaluation was limited to four MuJoCo environments, and the performance differences between beta and squashed normal policies were not conclusively determined due to high variability in results.
- What evidence would resolve it: Conducting experiments on a broader set of continuous control tasks, including more diverse and challenging environments, would provide more robust performance comparisons between the beta policy variants and the squashed normal policy.

### Open Question 2
- Question: Why does SAC with a normal policy perform poorly compared to other SAC variants, while it does not exhibit the same flawed behavior with other non-entropy-based algorithms like TRPO?
- Basis in paper: [explicit] The paper observes that SAC-Normal struggles to learn anything useful, with the average return remaining close to zero, and suggests this could be due to poor initialization or the policy getting stuck in a region of the search space from which it cannot recover.
- Why unresolved: The underlying cause of the poor performance of SAC-Normal compared to other algorithms is not understood, and further investigations are needed to understand the specific issues related to SAC.
- What evidence would resolve it: Analyzing the training dynamics and initialization of SAC-Normal, and comparing them with TRPO and other algorithms, would help identify the factors contributing to the poor performance and potential solutions.

### Open Question 3
- Question: How do other non-explicitly reparameterizable distributions, such as gamma, Dirichlet, or von Mises, perform when used with SAC via implicit reparameterization gradients?
- Basis in paper: [explicit] The paper suggests that future research could include exploring other non-explicitly reparameterizable distributions that could potentially be beneficial for injecting domain knowledge into the problem at hand.
- Why unresolved: The evaluation focused on the beta distribution, and the performance of other non-explicitly reparameterizable distributions with SAC has not been explored.
- What evidence would resolve it: Implementing and evaluating SAC with various non-explicitly reparameterizable distributions, such as gamma, Dirichlet, or von Mises, on a range of continuous control tasks would provide insights into their effectiveness and potential advantages over the beta policy.

## Limitations

- The paper provides no empirical evidence comparing convergence speeds between different SAC variants, despite claiming beta policy improves convergence rates.
- The study is limited to only four MuJoCo environments, which may not generalize to other continuous control tasks or more complex domains.
- Implementation details for implicit reparameterization are not fully specified, particularly regarding numerical stability measures for extreme concentration values.

## Confidence

**High Confidence**: The core mechanism of using implicit reparameterization to enable beta policy sampling in SAC is well-established theoretically and the experimental results show beta policy performs comparably to squashed normal policy across all tested environments.

**Medium Confidence**: The claim that beta policy outperforms normal policy is supported by experimental results, though the magnitude of improvement is not quantified and could be influenced by implementation details or hyperparameter choices.

**Low Confidence**: The assertion that implicit reparameterization via AD and OMT yield similar performance lacks rigorous validation, as the paper provides no statistical tests comparing the methods or analysis of when one might outperform the other.

## Next Checks

1. **Convergence Rate Analysis**: Conduct controlled experiments measuring and comparing the convergence rates of SAC with beta, normal, and squashed normal policies across multiple random seeds, including statistical significance testing.

2. **Gradient Quality Assessment**: Implement diagnostic tools to measure the quality of gradient estimates from both AD and OMT implicit reparameterization methods, comparing their variance and bias properties during training.

3. **Hyperparameter Sensitivity Study**: Perform comprehensive hyperparameter sweeps for temperature, learning rate, and clipping bounds across all three SAC variants to determine whether observed performance differences persist across different parameter settings.