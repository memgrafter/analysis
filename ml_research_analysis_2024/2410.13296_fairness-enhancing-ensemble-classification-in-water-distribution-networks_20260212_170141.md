---
ver: rpa2
title: Fairness-Enhancing Ensemble Classification in Water Distribution Networks
arxiv_id: '2410.13296'
source_url: https://arxiv.org/abs/2410.13296
tags:
- fairness
- disparate
- impact
- accuracy
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces fairness considerations to water distribution
  networks (WDNs), a domain previously unexplored in fairness research. The authors
  extend group fairness definitions (disparate impact and equal opportunity) to multiple
  non-binary sensitive features, representing different consumer groups within a WDN.
---

# Fairness-Enhancing Ensemble Classification in Water Distribution Networks

## Quick Facts
- arXiv ID: 2410.13296
- Source URL: https://arxiv.org/abs/2410.13296
- Reference count: 19
- One-line primary result: Standard leakage detection methods in water distribution networks show fairness disparities, which can be improved through fairness-enhancing ensemble classification methods while maintaining reasonable accuracy.

## Executive Summary
This paper introduces fairness considerations to water distribution networks (WDNs), a domain previously unexplored in fairness research. The authors extend group fairness definitions (disparate impact and equal opportunity) to multiple non-binary sensitive features, representing different consumer groups within a WDN. They demonstrate that standard leakage detection methods are unfair, showing disparate impact scores of 0.54-0.64 and equal opportunity scores of 0.36-0.46 for small leaks. To address this, they propose three fairness-enhancing methods: optimizing loss with fairness constraints, optimizing fairness with accuracy constraints, and a differentiable approximation of ensemble classifiers. Experimental results on the Hanoi WDN show these methods can improve fairness (disparate impact up to 1.0) while maintaining reasonable accuracy (0.5-0.9), demonstrating a trade-off between fairness and overall performance that depends on hyperparameter choices.

## Method Summary
The paper addresses fairness in water distribution network leakage detection by introducing group fairness metrics (disparate impact and equal opportunity) to multiple non-binary sensitive features representing different consumer groups. The authors implement virtual sensors using linear regression to predict pressure at unmonitored nodes, then use ensemble classification with threshold-based decisions. Three fairness-enhancing methods are proposed: (1) optimizing loss functions with fairness constraints that minimize empirical covariance between sensitive features and predictions, (2) optimizing fairness metrics directly with accuracy constraints, and (3) a differentiable approximation of ensemble classifiers using sigmoid functions. These methods are tested on the Hanoi WDN with 32 nodes and 34 links, using pressure measurements from 3 sensor nodes to detect leaks of various sizes.

## Key Results
- Standard H-method shows disparate impact scores of 0.54-0.64 and equal opportunity scores of 0.36-0.46 for small leaks, indicating unfairness across consumer groups
- Fairness-enhancing methods can improve disparate impact up to 1.0 (perfect fairness) while maintaining accuracy between 0.5-0.9
- A clear trade-off exists between fairness and accuracy, controlled by hyperparameters that bound either covariance or accuracy deviation
- The differentiable approximation approach enables gradient-based optimization of non-differentiable ensemble methods for fairness enhancement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fairness-enhancing methods work by optimizing model parameters to reduce the empirical covariance between sensitive features and predictions.
- Mechanism: The paper introduces fairness as the minimization of the empirical covariance between sensitive features (representing different consumer groups) and the model's predictions. This is implemented through constrained optimization where the covariance is bounded to be small, ensuring similar prediction rates across groups.
- Core assumption: Lower covariance between sensitive features and predictions implies more equitable treatment across different consumer groups in the WDN.
- Evidence anchors:
  - [abstract] "we consider the empirical covariance between the sensitive features and the model's prediction as a proxy for the fairness measure"
  - [section] "We adapt this idea to our setting by considering the covariance of each sensitive feature and replacing the signed distance by the prediction of the ensemble classifier"
  - [corpus] Weak evidence - the corpus contains related papers on WDNs and AI but none specifically address fairness through covariance optimization.
- Break condition: If the relationship between covariance and fairness is not monotonic, or if other fairness metrics show contradictory results, the mechanism may fail.

### Mechanism 2
- Claim: The differentiability approximation allows non-differentiable ensemble methods to be optimized for fairness.
- Mechanism: The paper replaces the non-differentiable indicator functions in the ensemble classifier with sigmoid approximations, making the model differentiable. This enables gradient-based optimization techniques to be applied for fairness enhancement.
- Core assumption: The sigmoid approximation sufficiently preserves the decision boundaries of the original indicator functions while enabling gradient-based optimization.
- Evidence anchors:
  - [abstract] "we thus propose a remedy to increase the fairness which can be applied even to non-differentiable ensemble classification methods"
  - [section] "To make ˆY = f(X,·) differentiable, we approximate each indicator function1{v>0}by the sigmoid function"
  - [corpus] No direct evidence in corpus about sigmoid approximations for fairness optimization in WDNs.
- Break condition: If the sigmoid approximation introduces significant bias or if the gradient-based optimization gets stuck in local minima that don't generalize well.

### Mechanism 3
- Claim: The trade-off between fairness and accuracy is controlled through hyperparameters that bound either the covariance or the accuracy deviation.
- Mechanism: The paper implements fairness constraints with hyperparameters (c for covariance bounds, λ for accuracy deviation) that allow control over the fairness-accuracy trade-off. Lower values of c or higher values of λ increase fairness at the expense of accuracy.
- Core assumption: There exists a controllable trade-off between fairness and accuracy that can be tuned through hyperparameters to meet specific requirements.
- Evidence anchors:
  - [abstract] "Experimental results on the Hanoi WDN show these methods can improve fairness (disparate impact up to 1.0) while maintaining reasonable accuracy (0.5-0.9), demonstrating a trade-off between fairness and overall performance"
  - [section] "The hyperparameter c∈[0,∞) regulates how much the covariance's absolute value is bounded and therefore, the desired fairness"
  - [corpus] Weak evidence - related papers focus on detection methods but not on the fairness-accuracy trade-off in WDNs.
- Break condition: If the trade-off curve is not smooth or if extreme hyperparameter values lead to trivial solutions (e.g., constant predictions).

## Foundational Learning

- Concept: Group fairness definitions (disparate impact and equal opportunity)
  - Why needed here: The paper extends these fairness concepts to multiple non-binary sensitive features representing different consumer groups in WDNs.
  - Quick check question: What is the difference between disparate impact and equal opportunity in the context of multiple consumer groups?

- Concept: Empirical covariance as fairness proxy
  - Why needed here: The paper uses empirical covariance between sensitive features and predictions as a measure of fairness, requiring understanding of statistical independence and covariance properties.
  - Quick check question: Why does zero covariance between a sensitive feature and predictions indicate fairness?

- Concept: Differentiability approximation for non-differentiable models
  - Why needed here: The paper applies sigmoid approximations to enable gradient-based optimization of ensemble classifiers, requiring knowledge of smooth function approximations.
  - Quick check question: How does the sigmoid function approximate the indicator function, and what hyperparameter controls this approximation?

## Architecture Onboarding

- Component map:
  Virtual sensors (linear regression models) → Residual computation → Ensemble classifier (threshold-based) → Fairness optimization (constrained optimization)

- Critical path:
  1. Train virtual sensors on leakage-free data
  2. Compute residuals from pressure measurements
  3. Apply threshold-based ensemble classification
  4. Evaluate fairness metrics
  5. Optimize parameters with fairness constraints

- Design tradeoffs:
  - Accuracy vs. fairness: Trade-off controlled by hyperparameters c and λ
  - Differentiability vs. decision boundary preservation: Sigmoid approximation introduces smoothness but may alter decision boundaries
  - Computational complexity: Constrained optimization increases training time compared to standard methods

- Failure signatures:
  - Disproportionate false positive rates across consumer groups
  - High empirical covariance between sensitive features and predictions
  - Local optima in non-convex optimization leading to suboptimal fairness

- First 3 experiments:
  1. Run the baseline H-method on the Hanoi WDN data and calculate fairness metrics (disparate impact and equal opportunity)
  2. Implement the T-F-PR+F method with different covariance bounds (c values) and observe the trade-off between fairness and accuracy
  3. Test the DI+ACC method with varying accuracy constraints (λ values) to find the optimal balance between fairness and performance

## Open Questions the Paper Calls Out
The paper identifies several open questions including: (1) extending the fairness notions to more complex WDNs with hundreds of nodes, (2) applying these methods to more powerful ML algorithms beyond ensemble classifiers, and (3) further exploring the relationship between legal requirements and fairness hyperparameters in water distribution contexts.

## Limitations
- The paper only tested on a relatively small simulated WDN (Hanoi with 32 nodes) rather than real-world, larger networks
- Limited exploration of how fairness hyperparameters affect long-term system performance under changing conditions
- The relationship between empirical covariance and meaningful fairness in WDN contexts requires further validation beyond the single dataset

## Confidence
- Empirical validation of fairness metrics on single dataset: Medium
- Fairness mechanisms through covariance optimization: Medium
- Differentiable approximation for non-differentiable models: Low
- Fairness-accuracy trade-off mechanism: Medium

## Next Checks
1. Validate the fairness metrics on real-world WDN data with known consumer group characteristics to confirm that disparate impact and equal opportunity scores meaningfully reflect equity in this domain.
2. Compare the sigmoid approximation approach with alternative smooth approximations (e.g., softplus, polynomial) to assess whether the choice of approximation function significantly impacts fairness outcomes.
3. Test the fairness-enhancing methods across multiple WDN topologies and sensor configurations to evaluate robustness and identify conditions under which the fairness improvements generalize.