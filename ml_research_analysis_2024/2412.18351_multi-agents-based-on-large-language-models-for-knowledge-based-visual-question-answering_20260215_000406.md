---
ver: rpa2
title: Multi-Agents Based on Large Language Models for Knowledge-based Visual Question
  Answering
arxiv_id: '2412.18351'
source_url: https://arxiv.org/abs/2412.18351
tags:
- knowledge
- question
- answer
- action
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a multi-agent voting framework (MAVL) for
  knowledge-based visual question answering (VQA) that addresses two key challenges:
  the inability to use external tools autonomously and the inability to work in teams.
  The framework consists of three LLM-based agents (Junior, Senior, Manager) with
  different access levels to tools like candidate answer generation, knowledge retrieval
  from KBs, and LLM-generated knowledge.'
---

# Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering

## Quick Facts
- arXiv ID: 2412.18351
- Source URL: https://arxiv.org/abs/2412.18351
- Authors: Zhongjian Hu; Peng Yang; Bing Li; Zhenqi Wang
- Reference count: 40
- Primary result: Multi-agent voting framework (MAVL) achieves SOTA on OK-VQA (81.1%) and A-OKVQA (61.8%), outperforming baselines by 2.2 and 1.0 points respectively

## Executive Summary
This paper introduces MAVL, a multi-agent voting framework for knowledge-based visual question answering that addresses key limitations of existing LLM-based VQA approaches: inability to autonomously use external tools and inability to work in teams. The framework consists of three LLM-based agents (Junior, Senior, Manager) with different tool access levels and a voting mechanism weighted by agent expertise. Experiments on OK-VQA and A-OKVQA datasets demonstrate state-of-the-art performance, with ablation studies confirming the importance of the multi-agent architecture, tool diversity, and autonomous planning.

## Method Summary
MAVL implements a three-agent system where each agent uses a planner to autonomously select and execute tools (candidate generation, KB retrieval, LLM knowledge generation) based on the input task. The Junior agent uses only candidate generation, the Senior adds KB retrieval, and the Manager adds LLM-generated knowledge. Each agent generates answers independently, which are then combined through weighted voting (2:3:4 votes for Junior:Senior:Manager). The framework uses OSCAR+ for image captioning, MCAN for candidate answer generation, and LLaMA2 7B for LLM reasoning, with Wikipedia as the knowledge base.

## Key Results
- Achieves 81.1% accuracy on OK-VQA, surpassing previous SOTA by 2.2 points
- Achieves 61.8% accuracy on A-OKVQA, surpassing previous SOTA by 1.0 points
- Ablation study shows performance degrades significantly when any component (multi-agent voting, tools, or planners) is removed
- Voting mechanism consistently improves performance over single-agent approaches across different configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent voting framework improves performance through specialized agents with different tool access levels
- Mechanism: Three agents with progressively more tool access (Junior: candidate generation, Senior: +KB retrieval, Manager: +LLM generation) combine their answers through weighted voting
- Core assumption: Different expertise levels require different tools, and combining their outputs improves accuracy
- Evidence anchors:
  - [abstract]: "We design three LLM-based agents that simulate different levels of staff in a team, and assign the available tools according to the levels"
  - [section]: "We design different agents to simulate the reality by designing different levels to differentiate the abilities of different agents"
- Break condition: If tool access levels don't match agent capabilities or voting weights are poorly calibrated

### Mechanism 2
- Claim: Planner component enables autonomous tool selection based on task requirements
- Mechanism: Each agent's LLM-based planner generates action plans and decides which tools to use for different questions
- Core assumption: LLM can effectively plan tool usage for different question types
- Evidence anchors:
  - [section]: "The planner allows the agent to autonomously plan actions and invoke tools"
  - [section]: "We design the planners for the agents, and each planner enables the agent to autonomously generate the action plan"
- Break condition: If planner fails to select appropriate tools or planning becomes too computationally expensive

### Mechanism 3
- Claim: Tool diversity provides complementary knowledge sources that improve answer quality
- Mechanism: Candidate generation, KB retrieval, and LLM generation provide different knowledge types that complement each other
- Core assumption: Multiple knowledge sources improve coverage and accuracy
- Evidence anchors:
  - [section]: "We have added two knowledge tools: one for retrieving knowledge from the KBs and the other for generating knowledge through LLM"
  - [section]: "The reason for designing two knowledge tools is that the knowledge of KBs is limited and it may not be possible to retrieve all the required knowledge"
- Break condition: If knowledge sources provide conflicting information or one source is consistently more reliable

## Foundational Learning

- Concept: In-context learning with LLMs
  - Why needed here: Framework relies on prompting LLMs with in-context examples to guide planning and answer generation
  - Quick check question: How does the number of in-context examples affect performance, and what's the optimal range?

- Concept: Visual question answering fundamentals
  - Why needed here: Framework builds on VQA concepts, combining visual and textual information
  - Quick check question: What are key differences between knowledge-based and standard VQA?

- Concept: Knowledge base retrieval and integration
  - Why needed here: Tool 2 retrieves knowledge from KBs, requiring query formulation and integration
  - Quick check question: How does the framework handle KB retrieval failures?

## Architecture Onboarding

- Component map:
  Image + Question -> OSCAR+ Captioning -> Three Agents (Junior, Senior, Manager) -> Weighted Voting -> Final Answer

- Critical path:
  1. Image is captioned to text
  2. Each agent's planner generates action plan
  3. Tools are executed according to plan
  4. LLM generates answer based on prompt
  5. Answers combined through weighted voting
  6. Final answer output

- Design tradeoffs:
  - Complexity vs. performance: Multi-agent system adds complexity but improves results
  - Tool access levels: Balancing access affects performance and computational cost
  - Voting weights: Determining optimal weights for different agent levels

- Failure signatures:
  - Performance degradation when any component is removed (ablation study)
  - Suboptimal planner decisions leading to incorrect tool selection
  - Inconsistent voting weights causing unreliable final answers

- First 3 experiments:
  1. Test single agent performance vs. multi-agent voting to quantify voting mechanism benefits
  2. Evaluate performance with different tool access levels to determine optimal configurations
  3. Test planner effectiveness by comparing autonomous vs. fixed tool pipelines for different question types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MAVL performance scale with different numbers of agents (e.g., 5 or 10 agents instead of 3)?
- Basis in paper: [inferred] The paper tests only 3 agents and shows performance gains from multi-agent voting, but doesn't explore scaling
- Why unresolved: Paper focuses on 3-agent system without testing performance with more or fewer agents
- What evidence would resolve it: Systematic experiments varying agent count from 2 to 10+ while measuring performance on OK-VQA and A-OKVQA

### Open Question 2
- Question: What is the optimal allocation of tools and voting weights across different agent levels?
- Basis in paper: [explicit] Paper assigns specific tools and weights but doesn't explore alternative configurations
- Why unresolved: Authors chose configuration based on intuition about real-world teams without systematic exploration
- What evidence would resolve it: Ablation studies testing different tool allocation schemes and voting weight distributions

### Open Question 3
- Question: How does MAVL perform on knowledge-based VQA tasks requiring more complex reasoning chains or longer knowledge dependencies?
- Basis in paper: [inferred] Paper evaluates on OK-VQA and A-OKVQA but doesn't analyze performance by reasoning complexity
- Why unresolved: Datasets contain mix of difficulties but paper doesn't break down performance by reasoning complexity
- What evidence would resolve it: Detailed analysis of performance on subsets requiring different numbers of reasoning steps

## Limitations

- Computational overhead: Running multiple agents with planners increases computational cost compared to single-agent approaches
- Prompt engineering dependency: Performance heavily depends on effective prompt construction and in-context example selection
- Limited scalability: Framework's complexity may not scale well to larger agent pools or more diverse tool sets

## Confidence

- Performance improvement claims: Medium-High confidence - consistent gains on two established datasets
- Individual mechanism effectiveness: Medium confidence - ablation shows degradation but doesn't isolate most important components
- Generalization to new datasets: Medium confidence - framework tested only on OK-VQA and A-OKVQA

## Next Checks

1. **Planner Robustness Test**: Systematically evaluate the planner's tool selection across different question categories to identify systematic biases or failure patterns in autonomous decision-making

2. **Weight Calibration Study**: Conduct a parameter sweep to determine optimal voting weights for different agent levels across various question types, and test whether these weights transfer to new datasets

3. **Knowledge Source Reliability Analysis**: Compare the accuracy and coverage of KB-retrieved knowledge versus LLM-generated knowledge across different knowledge domains to quantify their complementary value