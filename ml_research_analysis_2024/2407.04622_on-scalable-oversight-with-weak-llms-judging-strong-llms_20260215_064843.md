---
ver: rpa2
title: On scalable oversight with weak LLMs judging strong LLMs
arxiv_id: '2407.04622'
source_url: https://arxiv.org/abs/2407.04622
tags:
- judge
- answer
- debate
- your
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates scalable oversight protocols\u2014debate,\
  \ consultancy, and direct QA\u2014where weaker LLM judges supervise stronger LLM\
  \ agents across extractive, closed, and multimodal tasks. Across all tasks, debate\
  \ consistently outperforms consultancy when consultants are randomly assigned to\
  \ argue for correct or incorrect answers."
---

# On scalable oversight with weak LLMs judging strong LLMs

## Quick Facts
- arXiv ID: 2407.04622
- Source URL: https://arxiv.org/abs/2407.04622
- Reference count: 40
- One-line primary result: Debate protocols enable weak LLM judges to achieve higher accuracy than consultancy across extractive, closed, and multimodal tasks.

## Executive Summary
This paper evaluates scalable oversight protocols—debate, consultancy, and direct QA—where weaker LLM judges supervise stronger LLM agents across extractive, closed, and multimodal tasks. Across all tasks, debate consistently outperforms consultancy when consultants are randomly assigned to argue for correct or incorrect answers. Comparing debate to direct QA, debate surpasses QA without article in extractive tasks with information asymmetry, but differences are mixed for other tasks without asymmetry. Open debate, where a protagonist chooses which answer to argue for, reduces judge errors on incorrect answers compared to open consultancy. Stronger debater models improve judge accuracy, though the effect is modest. Ablations show minimal impact from turn count, best-of-N sampling, few-shot prompting, chain-of-thought reasoning, or turn style.

## Method Summary
The paper evaluates six scalable oversight protocols using inference-only LLM agents. Tasks include binary-choice questions from nine datasets spanning extractive QA (with/without article access), closed-domain QA, and multimodal tasks. Protocols include QA without article, QA with article, consultancy (single consultant arguing for assigned answer), debate (two debaters arguing for opposite answers), open consultancy (consultant chooses answer), and open debate (protagonist chooses answer). The study uses Gemma7B, GPT-3.5, Gemini Pro 1.0, and Gemini Pro 1.5 models as judges, consultants, and debaters, with evaluation based on judge accuracy, win rates, and Elo ratings.

## Key Results
- Debate outperforms consultancy across all tasks when consultants are randomly assigned to argue for correct/incorrect answers
- In open debate, judges follow incorrect debater choices less frequently than in open consultancy
- Stronger debater models lead to higher judge accuracy, though more modestly than previous studies
- Minimal impact from turn count, best-of-N sampling, few-shot prompting, chain-of-thought reasoning, or turn style on judge accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Debate enables weak judges to achieve higher accuracy than consultancy by forcing them to weigh competing arguments, not just accept a single consultant's persuasion.
- **Mechanism:** The debate protocol pits two equally capable debaters against each other, each arguing for opposite answers. The weak judge evaluates the arguments in a structured, adversarial setting. This adversarial setup surfaces flaws in reasoning that a single consultant might hide, leading the judge to make more accurate decisions.
- **Core assumption:** The judge can distinguish stronger arguments when presented side-by-side, and the debaters' arguments are sufficiently differentiated to expose flaws.
- **Evidence anchors:**
  - [abstract] "We find that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer."
  - [section] "For consultancy, for each question, we follow Michael et al. (2023); Khan et al. (2024) by assigning the consultant to argue separately for both the correct and incorrect answer."
  - [corpus] "Average neighbor FMR=0.393" (moderate relevance, indicating some overlap with debate and LLM evaluation literature)

### Mechanism 2
- **Claim:** Open debate reduces amplification of incorrect answers compared to open consultancy because the judge is less likely to follow a debater's incorrect choice.
- **Mechanism:** In open debate, the protagonist debater chooses which answer to argue for, and the antagonist is assigned the opposite. The judge sees both arguments and must decide. When the debater chooses incorrectly, the judge is more likely to discern this compared to open consultancy, where the consultant's choice goes unchallenged.
- **Core assumption:** The debate format inherently challenges incorrect claims more effectively than a single consultant's argument.
- **Evidence anchors:**
  - [abstract] "In open debate, in contrast, the judge follows the debater's choice less frequently than in open consultancy... When the debater chooses incorrectly, the judge does a lot better at discerning this."
  - [section] "In open consultancy, the judge is equally convinced by the consultant, whether or not the consultant has chosen to argue for the correct answer... In open debate, in contrast, the judge follows the debater's choice less frequently than in open consultancy."
  - [corpus] "Average neighbor FMR=0.393" (moderate relevance, indicating some overlap with debate and LLM evaluation literature)

### Mechanism 3
- **Claim:** Stronger debater models lead to higher judge accuracy, providing evidence that debate satisfies a key objective of scalable oversight.
- **Mechanism:** As debater models become more capable (as measured by Elo scores), they produce more persuasive and accurate arguments. The weak judge, while still limited, benefits from the higher quality of the debate, leading to improved accuracy.
- **Core assumption:** The relationship between debater strength and judge accuracy is monotonic and significant enough to matter in practice.
- **Evidence anchors:**
  - [abstract] "Further, we find that stronger debater models increase judge accuracy, though more modestly than in previous studies."
  - [section] "Figure 4 (bottom row) shows that on extractive tasks, stronger models have higher aggregate Elo and these lead to higher judge accuracy."
  - [corpus] "Average neighbor FMR=0.393" (moderate relevance, indicating some overlap with debate and LLM evaluation literature)

## Foundational Learning

- **Concept:** Information asymmetry
  - Why needed here: Extractive tasks introduce an information asymmetry where consultants/debaters have access to an article that the judge does not. Understanding this asymmetry is crucial for interpreting the results and designing protocols that bridge the capability gap between judge and debaters.
  - Quick check question: In an extractive QA task, does the judge have access to the article from which the question is derived?
    - Answer: No, the judge does not have access to the article, creating an information asymmetry.

- **Concept:** Scalable oversight
  - Why needed here: The paper studies debate and consultancy as scalable oversight protocols, which aim to enable humans (or weaker AI) to accurately supervise superhuman AI. Understanding the goals and challenges of scalable oversight is essential for evaluating the effectiveness of these protocols.
  - Quick check question: What is the primary goal of scalable oversight protocols?
    - Answer: To enable humans or weaker AI to accurately supervise superhuman AI by leveraging the capabilities of the stronger AI to provide accurate training signals.

- **Concept:** Chain-of-thought reasoning
  - Why needed here: The paper explores the impact of chain-of-thought reasoning on judge accuracy. Understanding how chain-of-thought reasoning works and its potential benefits and drawbacks is important for interpreting the ablation results.
  - Quick check question: What is the purpose of chain-of-thought reasoning in the context of LLM debate and consultancy?
    - Answer: Chain-of-thought reasoning prompts the model to explicitly show its reasoning process, potentially leading to more accurate and transparent arguments. However, the paper finds that it can sometimes harm judge accuracy.

## Architecture Onboarding

- **Component map:** Tasks (Extractive QA, Closed QA, Multimodal) -> Protocols (QA without article, QA with article, Consultancy, Debate, Open Consultancy, Open Debate) -> Models (Gemma7B, GPT-3.5, Gemini Pro 1.0, Gemini Pro 1.5 as judges, consultants, debaters) -> Evaluation (Judge accuracy, Elo scores, statistical significance testing)

- **Critical path:**
  1. Generate tasks from existing datasets
  2. Prompt consultants/debaters to argue for assigned or chosen answers
  3. Prompt judges to evaluate arguments and answer questions
  4. Measure judge accuracy and calculate Elo scores for debaters
  5. Analyze results across tasks, protocols, and model combinations

- **Design tradeoffs:**
  - Using inference-only setting vs. training debaters/consultants: Inference-only is faster and cheaper but doesn't test the full training protocol
  - Using LLM judges vs. human judges: LLM judges are more scalable but may not fully represent human judgment
  - Assigned-role vs. open-role protocols: Assigned-role controls for debater choice but may not reflect real-world scenarios as well as open-role

- **Failure signatures:**
  - Low judge accuracy across all protocols: Indicates the judge model is too weak or the tasks are too difficult
  - No difference between debate and consultancy: Suggests the adversarial aspect of debate is not providing additional value
  - High judge accuracy in open consultancy when consultant chooses incorrectly: Indicates the judge is not effectively challenging incorrect claims

- **First 3 experiments:**
  1. Run consultancy and debate protocols on a simple extractive QA task (e.g., BoolQ) with a relatively strong judge model (e.g., GPT-3.5) to establish a baseline for comparison
  2. Vary the number of turns in consultancy and debate to see if more rounds improve judge accuracy
  3. Compare the performance of open consultancy and open debate on a closed QA task (e.g., MMLU) to investigate the impact of debater choice on judge accuracy

## Open Questions the Paper Calls Out

- **Question:** Can fine-tuning judges on debate-specific tasks improve their accuracy compared to direct QA baselines?
  - **Basis in paper:** [inferred] The authors suggest that current fine-tuning approaches may favor direct QA formats, and that fine-tuning judges on debate tasks could be a promising direction.
  - **Why unresolved:** The study only evaluates inference-time debate performance without fine-tuning judges on debate tasks.
  - **What evidence would resolve it:** Experiments comparing judge accuracy on debate tasks after fine-tuning on debate-specific datasets versus fine-tuning on standard QA datasets.

- **Question:** Does the number of debate turns significantly impact judge accuracy under training conditions?
  - **Basis in paper:** [explicit] The authors find no significant effect of turn count on judge accuracy in inference-only settings, but suggest this may differ under training.
  - **Why unresolved:** The ablation study only tests inference-time performance, not trained debaters who might learn to use more turns effectively.
  - **What evidence would resolve it:** Training debaters with different turn limits and comparing judge accuracy across these trained models.

- **Question:** How robust are debate protocols to distribution shifts, such as transitioning from easy to hard tasks?
  - **Basis in paper:** [inferred] The authors mention this as a potential future direction but do not test robustness to distribution shifts.
  - **Why unresolved:** The study evaluates performance within fixed task categories without testing model behavior under varying difficulty levels or domain shifts.
  - **What evidence would resolve it:** Testing debate performance across tasks of increasing difficulty or across domains not seen during evaluation.

## Limitations
- Inference-only evaluation doesn't test the full training pipeline where debater/consultant models are iteratively improved based on judge feedback
- Binary-choice format and assignment of consultants/debaters to argue for both correct and incorrect answers may not fully capture real-world scalable oversight scenarios
- Use of LLM judges rather than human judges raises questions about ecological validity

## Confidence

- **High Confidence**: Debate outperforms consultancy when consultants are randomly assigned to argue for correct/incorrect answers (supported by consistent results across all nine tasks)
- **Medium Confidence**: Open debate reduces amplification of incorrect answers compared to open consultancy (results show the effect but rely on specific protocol designs)
- **Low Confidence**: Stronger debater models lead to higher judge accuracy (the effect is described as "modest" and the Elo-based analysis has limitations in establishing causality)

## Next Checks
1. Test the full training protocol by iteratively improving debater/consultant models based on judge feedback, rather than inference-only evaluation
2. Validate findings with human judges on a subset of tasks to assess ecological validity of LLM judge results
3. Evaluate performance on multi-choice questions and open-ended generation tasks to test generalizability beyond binary-choice format