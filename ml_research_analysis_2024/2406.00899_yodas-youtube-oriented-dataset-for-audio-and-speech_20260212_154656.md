---
ver: rpa2
title: 'YODAS: Youtube-Oriented Dataset for Audio and Speech'
arxiv_id: '2406.00899'
source_url: https://arxiv.org/abs/2406.00899
tags:
- speech
- dataset
- subset
- manual
- hours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YODAS, a large-scale multilingual speech
  dataset with over 500k hours of audio from YouTube in more than 100 languages. It
  contains labeled subsets with manual and automatic subtitles for supervised learning,
  and an unlabeled subset for self-supervised learning.
---

# YODAS: Youtube-Oriented Dataset for Audio and Speech

## Quick Facts
- arXiv ID: 2406.00899
- Source URL: https://arxiv.org/abs/2406.00899
- Reference count: 0
- Over 500k hours of audio in 100+ languages from YouTube

## Executive Summary
This paper introduces YODAS, a large-scale multilingual speech dataset with over 500k hours of audio from YouTube in more than 100 languages. The dataset contains labeled subsets with manual and automatic subtitles for supervised learning, and an unlabeled subset for self-supervised learning. The authors developed a data collection framework using keyword-based, channel-based, and download workers to identify and download YouTube videos with Creative Commons licenses and subtitles. They provide comprehensive analysis of speech and text statistics across the dataset and establish baseline speech recognition experiments achieving average character error rates of 9.97% across top-15 languages.

## Method Summary
The YODAS dataset was constructed using a three-pronged crawling approach: keyword-based crawling that extracts keywords from multilingual Wikipedia dumps and uses AJAX to retrieve deeper YouTube search results, channel-based crawling that exploits video-license homogeneity within channels, and download workers that retrieve audio and subtitles while applying language heuristics. The dataset underwent speech-text alignment filtering using CTC scoring to remove misaligned pairs, with a threshold of 2.0 applied. For baseline experiments, monolingual CTC-based models were trained on XLSR pre-trained features with BPE subword tokenization, achieving CERs ranging from 5.85% (English) to 19.34% (Vietnamese) across top-15 languages.

## Key Results
- YODAS contains over 500k hours of audio in 100+ languages from YouTube
- Models trained on manual subtitle subset achieved average CER of 9.97% across top-15 languages
- Manual subtitles outperformed automatic subtitles significantly (10.74% vs 14.77% CER)

## Why This Works (Mechanism)

### Mechanism 1
Keyword-based crawling with AJAX scrolling increases coverage of Creative Commons YouTube videos with subtitles by starting from Wikipedia-derived keyword lists and using AJAX to access deeper search results beyond top-ranked videos. Core assumption: YouTube's search results are ordered by relevance and AJAX-based pagination can retrieve additional licensed videos. Evidence: Dataset statistics show 500k+ hours across 100+ languages. Break condition: If YouTube changes search API behavior or pagination endpoints.

### Mechanism 2
Channel-based crawling complements keyword crawling by exploiting video-license homogeneity within channels. For each video found via keywords, the channel is extracted and used to retrieve all its videos, which tend to share the same licensing and subtitle characteristics. Core assumption: Videos within the same YouTube channel generally share consistent licensing and subtitle availability. Evidence: Authors note that videos hosted within the same channel typically share similar licensing and subtitle characteristics. Break condition: If channel policies change or channels host mixed-license content.

### Mechanism 3
Speech-text alignment filtering with CTC scoring removes misaligned subtitle pairs, improving recognition accuracy. Each utterance is scored by a pre-trained acoustic model and low-scoring pairs are discarded. Core assumption: CTC loss from an acoustic model is a reliable proxy for alignment quality. Evidence: Models trained on manual subset yield significantly superior performance compared to automatic subset. Break condition: If the acoustic model is poorly aligned with the target language or over-prunes complex utterances.

## Foundational Learning

- **Creative Commons licensing and YouTube API access**: Why needed - dataset construction hinges on filtering for Creative Commons-licensed videos; incorrect licensing handling invalidates downstream use. Quick check: What YouTube API query parameters enforce Creative Commons licensing and subtitle presence?

- **Speech-text alignment using CTC loss**: Why needed - high-quality alignments are critical for supervised ASR training; poor alignments degrade model performance. Quick check: How does CTC loss behave for well-aligned vs. misaligned audio-transcript pairs?

- **Multilingual BPE tokenization**: Why needed - dataset includes 100+ languages with diverse writing systems; tokenization strategy affects model vocabulary size and error rates. Quick check: Why does the paper use 5000 BPE tokens for Mandarin and 3000 for Japanese versus 300 for other languages?

## Architecture Onboarding

- **Component map**: Keyword Client -> Channel Client -> Download Client -> Master Node -> Storage
- **Critical path**: Keyword Client extracts keywords from Wikipedia → Channel Client exploits channel homogeneity → Download Client retrieves audio/subtitles → Master Node manages state via PostgreSQL → Storage
- **Design tradeoffs**: Language identification is heuristic-based (no LID accuracy); subtitle quality varies (automatic causes higher deletion errors); large dataset size requires scalable storage
- **Failure signatures**: Low yield of Creative Commons videos indicates filter misconfiguration; high misaligned utterances suggests alignment threshold too low or model mismatch; skewed language distribution points to biased keyword selection
- **First 3 experiments**: 1) Run keyword client with Wikipedia shard, verify CC videos with subtitles and pagination; 2) Execute download client on test videos, validate subtitle extraction and language heuristics; 3) Apply alignment filtering on sample, plot alignment score vs. CER, tune threshold

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal alignment threshold for speech-text alignment in the YODAS dataset to maximize speech recognition performance across different languages? The paper used threshold of 2.0 but suggests further investigation is needed. Experiments with various thresholds and analyzing effects on performance across languages would resolve this.

### Open Question 2
How does the inclusion of automatic subtitles versus manual subtitles affect overall quality and usability for different speech recognition tasks? While manual subtitles perform better (10.74% vs 14.77% CER), the paper doesn't explore if automatic subtitles provide value for certain tasks or languages. Task-specific experiments comparing both subtitle types would clarify impact.

### Open Question 3
What are the potential benefits and challenges of expanding YODAS to include more languages or additional types of speech data, such as spontaneous versus read speech? The paper mentions current scale but doesn't explore implications of further expansion or diversification. Analyzing effects of dataset expansion on model performance and identifying collection challenges would provide insights.

## Limitations

- Language distribution heavily favors English, raising questions about representativeness for truly low-resource languages
- Reliance on YouTube's search API and Creative Commons licensing creates potential sustainability concerns if platform policies change
- Evaluation focuses only on supervised learning with manual subtitles, leaving self-supervised learning potential from unlabeled subset unexplored

## Confidence

- **High confidence**: Dataset construction framework successfully collected 500k+ hours across 100+ languages (directly evidenced by dataset statistics and baseline experiments)
- **Medium confidence**: Alignment filtering method using CTC scoring effectively removes misaligned pairs (shown by better manual subtitle performance, but threshold choice lacks rigorous validation)
- **Medium confidence**: Superiority of manual subtitles over automatic subtitles for ASR training (clear CER difference, but analysis doesn't explore automatic subtitle value for certain languages)

## Next Checks

1. **Language identification validation**: Extract 100 random videos, manually verify spoken language matches dataset label, quantify false labeling rate across language families

2. **Alignment score threshold sensitivity**: Train models using thresholds 1.0, 1.5, 2.0, 2.5, 3.0, plot trade-off between dataset size and CER to determine if threshold of 2.0 is optimal

3. **Automatic subtitle utility test**: For languages with limited manual subtitle data, train models using automatic subtitles and evaluate whether they provide meaningful performance improvements over no text supervision