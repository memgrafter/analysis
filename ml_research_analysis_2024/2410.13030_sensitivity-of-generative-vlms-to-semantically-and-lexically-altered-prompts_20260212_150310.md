---
ver: rpa2
title: Sensitivity of Generative VLMs to Semantically and Lexically Altered Prompts
arxiv_id: '2410.13030'
source_url: https://arxiv.org/abs/2410.13030
tags:
- prompt
- prompts
- vlms
- image
- replace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the sensitivity of generative vision-language
  models (VLMs) to lexical and semantic variations in prompts using the SUGAR CREPE++
  dataset. The authors evaluate three VLMs - BLIP, BakLLaV A, and GPT-4o - by testing
  their performance across multiple paraphrased prompts and different arrangements
  of answer options.
---

# Sensitivity of Generative VLMs to Semantically and Lexically Altered Prompts

## Quick Facts
- arXiv ID: 2410.13030
- Source URL: https://arxiv.org/abs/2410.13030
- Reference count: 34
- Generative VLMs show significant performance variations when presented with semantically equivalent but lexically different prompts or reordered answer options

## Executive Summary
This paper analyzes the sensitivity of generative vision-language models (VLMs) to lexical and semantic variations in prompts using the SUGAR CREPE++ dataset. The authors evaluate three VLMs - BLIP, BakLLaV A, and GPT-4o - by testing their performance across multiple paraphrased prompts and different arrangements of answer options. Their experiments reveal that these models show significant performance variations even when presented with semantically equivalent but lexically different prompts, or when the order of answer options is changed. The findings demonstrate that generative VLMs are highly sensitive to minor changes in prompt structure, with no single prompt achieving consistent optimal performance across all dataset subsets.

## Method Summary
The authors evaluate three generative VLMs on the SUGAR CREPE++ dataset containing images with caption triplets (two positive, one negative). They test each model with multiple prompt variants that are semantically equivalent but lexically different, and with different orderings of the answer options. Performance is measured as accuracy in identifying the negative caption position. The authors also evaluate consistency using majority voting approaches across different prompt variants and models. The study focuses on zero-shot evaluation without additional fine-tuning.

## Key Results
- VLMs show significant performance variations when presented with semantically equivalent but lexically different prompts
- Reordering answer options significantly impacts model performance across all tested VLMs
- Majority voting across prompts or models does not reliably improve performance
- No single prompt variant achieves optimal performance across all dataset subsets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative VLMs show inconsistent performance when prompted with semantically equivalent but lexically different variations
- Mechanism: The model's attention mechanisms and token embeddings are sensitive to lexical variations, causing different internal representations even when semantic meaning is preserved. Small changes in word choice or phrasing alter the probability distribution over outputs, leading to different answers for semantically equivalent prompts.
- Core assumption: The model processes prompts through token-level attention mechanisms that are sensitive to exact token sequences, not just semantic equivalence
- Evidence anchors:
  - [abstract] "Our findings demonstrate that generative VLMs are highly sensitive to such alterations"
  - [section 3.1] "We observe differences in performance when using paraphrases of the same prompt (lexically different but semantically identical)"
  - [corpus] Weak - no direct corpus evidence for token-level attention sensitivity
- Break condition: If the model implements semantic normalization or uses contextual embeddings that abstract away from exact token sequences

### Mechanism 2
- Claim: Reordering answer options significantly impacts model performance
- Mechanism: The model's output layer is influenced by the position of answer choices, possibly due to position encoding or learned biases in the decoder that associate certain positions with specific answer types. The model may have developed position-dependent reasoning patterns during training.
- Core assumption: The model's decoder layer uses position information that affects answer selection probability
- Evidence anchors:
  - [section 3.1] "Significant variations in BakLLaV A's performance on SUGAR CREPE ++ were found for the same prompt, simply by reordering the positions of the three options"
  - [abstract] "when the order of answer options is changed"
  - [corpus] Weak - no corpus evidence for position encoding effects on VLM outputs
- Break condition: If the model uses position-agnostic decoding or has been specifically trained to be invariant to answer option ordering

### Mechanism 3
- Claim: Majority voting across prompts or models does not reliably improve performance
- Mechanism: The models' outputs are not independent random variables but share systematic biases that cause correlated errors. When multiple variants are combined, the majority vote often amplifies these shared weaknesses rather than correcting them.
- Core assumption: Different prompt variants and models share underlying architectural or training-induced biases that cause correlated prediction errors
- Evidence anchors:
  - [section 3.2] "combining the outputs of the three variants for a given prompt often resulted in the lowest performance"
  - [abstract] "combining outputs from different prompt variants or different models does not reliably improve performance"
  - [corpus] Weak - no corpus evidence for correlated error patterns in VLMs
- Break condition: If models have diverse enough architectures or training data to produce uncorrelated errors

## Foundational Learning

- Concept: Semantic vs lexical equivalence
  - Why needed here: Understanding the distinction is crucial for interpreting why small wording changes affect model performance despite preserving meaning
  - Quick check question: Can you provide two sentences that are semantically equivalent but lexically different?

- Concept: Prompt engineering and template design
  - Why needed here: The paper relies on understanding how different prompt formulations affect model behavior
  - Quick check question: What are the key elements of a well-designed prompt template for VLMs?

- Concept: Ensemble methods and majority voting
  - Why needed here: The paper evaluates consistency through majority voting approaches, requiring understanding of when ensemble methods work
  - Quick check question: Under what conditions does majority voting typically improve classification performance?

## Architecture Onboarding

- Component map: Image → Vision encoder → Cross-modal attention → Language decoder → Output distribution → Classification decision
- Critical path: Prompt → Vision encoder → Cross-modal attention → Language decoder → Output distribution → Classification decision
- Design tradeoffs: 
  - Precision vs robustness: Highly tuned prompts may achieve better performance but lack consistency across variations
  - Model size vs sensitivity: Larger models may be more sensitive to lexical variations but potentially more capable of semantic understanding
  - Training data diversity vs prompt sensitivity: More diverse training may reduce sensitivity but increase computational cost
- Failure signatures:
  - Performance drops when answer options are reordered
  - Different performance across semantically equivalent prompts
  - Majority voting fails to improve accuracy
  - Inconsistent behavior across model variants
- First 3 experiments:
  1. Test each model with the same prompt but with answer options in different orders to confirm position sensitivity
  2. Run the same semantic query with multiple lexical variants to quantify prompt sensitivity
  3. Combine outputs from different prompt variants using majority voting to verify lack of consistency improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could improve VLM robustness to lexical variations while maintaining semantic understanding?
- Basis in paper: [explicit] The authors conclude that their results "highlight the need for improved robustness against lexical variations in generative VLMs" but do not propose specific solutions
- Why unresolved: The paper identifies the problem of lexical sensitivity but stops short of investigating potential remedies or modifications to VLM architectures that could address this limitation
- What evidence would resolve it: Comparative experiments testing different architectural modifications (attention mechanisms, training objectives, data augmentation strategies) on SUGAR CREPE++ would demonstrate which approaches effectively reduce lexical sensitivity while preserving semantic understanding

### Open Question 2
- Question: How does the sensitivity to lexical variations manifest across different VLM architectures beyond the three models tested (BLIP, BakLLaV A, GPT-4o)?
- Basis in paper: [inferred] The authors test only three generative VLMs and note that no single prompt achieved optimal performance across all dataset subsets, suggesting architecture-specific sensitivities
- Why unresolved: The paper's analysis is limited to three specific models, leaving open whether this sensitivity pattern is universal across VLM architectures or varies significantly between different approaches
- What evidence would resolve it: Systematic testing of additional VLM architectures (including both contrastive and generative models) on SUGAR CREPE++ with consistent prompt variations would reveal whether lexical sensitivity is a universal VLM characteristic or architecture-dependent

### Open Question 3
- Question: What is the relationship between lexical sensitivity in VLMs and their performance on downstream tasks that require fine-grained language understanding?
- Basis in paper: [explicit] The authors note that "recent works have shown that VLMs lack compositional understanding and often struggle with reasoning about even simple spatial relationships or attribute attachments"
- Why unresolved: While the paper demonstrates lexical sensitivity, it doesn't investigate how this affects practical downstream applications or whether performance degradation scales with task complexity
- What evidence would resolve it: Experiments correlating lexical sensitivity measurements from SUGAR CREPE++ with performance degradation on various downstream VLM tasks (VQA, image captioning, visual reasoning) would establish practical implications of lexical sensitivity

### Open Question 4
- Question: Are there specific types of lexical variations (syntactic vs semantic paraphrases, formal vs informal language) that consistently cause greater performance degradation across VLMs?
- Basis in paper: [inferred] The authors test paraphrases and word order changes but don't systematically categorize different types of lexical variations or their differential impact
- Why unresolved: The paper uses a limited set of lexical variations without exploring whether certain types of language changes are more problematic than others for VLMs
- What evidence would resolve it: Controlled experiments introducing different categories of lexical variations (paraphrases, syntactic transformations, register changes) and measuring their differential impact on VLM performance would identify the most problematic variation types

## Limitations

- The exact implementation details of how model outputs are parsed to determine correct negative caption identification are not specified
- The evaluation methodology for majority voting consistency is not fully detailed
- The SUGAR CREPE++ dataset construction process is not described in detail

## Confidence

- **High confidence**: VLMs show significant performance variations across semantically equivalent prompts and answer orderings
- **Medium confidence**: Majority voting approaches fail to improve consistency
- **Medium confidence**: No single prompt variant achieves optimal performance across all dataset subsets

## Next Checks

1. Reproduce position sensitivity: Test a simple VLM with answer options in different orders to confirm that position encoding affects output probabilities
2. Analyze error correlation: Examine whether models make correlated errors across different prompt variants to understand why majority voting fails
3. Evaluate semantic understanding: Compare model performance on lexically different but semantically similar captions to distinguish between prompt sensitivity and genuine semantic comprehension limitations