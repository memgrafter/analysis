---
ver: rpa2
title: Reinforcement Learning from Human Feedback with Active Queries
arxiv_id: '2402.09401'
source_url: https://arxiv.org/abs/2402.09401
tags:
- learning
- arxiv
- preference
- adpo
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes query-efficient RLHF methods to reduce the
  expensive human preference data collection in aligning large language models. The
  authors formalize RLHF as a contextual dueling bandit problem and design an active-query-based
  proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret
  bound and $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the feature space
  dimension and $\Delta$ is the sub-optimality gap.
---

# Reinforcement Learning from Human Feedback with Active Queries

## Quick Facts
- arXiv ID: 2402.09401
- Source URL: https://arxiv.org/abs/2402.09401
- Authors: Kaixuan Ji; Jiafan He; Quanquan Gu
- Reference count: 21
- This paper proposes query-efficient RLHF methods to reduce the expensive human preference data collection in aligning large language models.

## Executive Summary
This paper addresses the high cost of human preference data collection in Reinforcement Learning from Human Feedback (RLHF) by proposing query-efficient methods that selectively request human preferences only when model uncertainty is high. The authors formalize RLHF as a contextual dueling bandit problem and develop an active-query-based proximal policy optimization (APPO) algorithm with theoretical guarantees on regret and query complexity. They extend this to direct preference optimization (DPO) and create ADPO, a practical version for fine-tuning large language models that achieves comparable performance to state-of-the-art DPO while making about half the queries for human preferences.

## Method Summary
The paper proposes ADPO, which extends DPO by incorporating active query selection based on model uncertainty. For samples where the model is confident in its preference prediction, ADPO uses pseudo-labels generated by the model itself instead of querying humans. For uncertain samples, it requests human preferences. The method is theoretically grounded in contextual dueling bandits, with APPO providing the theoretical framework that ADPO adapts for practical LLM fine-tuning. The algorithm selectively updates parameters only when uncertainty exceeds a threshold, achieving query efficiency while maintaining performance through careful pseudo-labeling.

## Key Results
- ADPO matches the performance of state-of-the-art DPO methods while making about half the queries for human preference data
- Achieves constant instance-dependent regret upper bound and query complexity theoretically
- Performs comparably to DPO on objective benchmarks (ARC, TruthfulQA, HellaSwag) and subjective benchmarks (MT-Bench, AlpacaEval 2.0)

## Why This Works (Mechanism)

### Mechanism 1
Query efficiency is achieved by selectively requesting human preferences only when model uncertainty is high. Uses an uncertainty-based data filter that compares the confidence level of the model's preference prediction to a threshold. Only samples with low confidence trigger human queries.

### Mechanism 2
Performance is maintained by using pseudo-labels for high-confidence samples. For samples where the model is confident in its preference prediction, it uses its own prediction (pseudo-label) instead of querying humans, then trains on this combined dataset.

### Mechanism 3
Theoretical guarantees provide a framework for understanding query efficiency. The paper proves that under certain assumptions (linear reward, minimal sub-optimality gap), the algorithm achieves constant regret and query complexity with respect to rounds.

## Foundational Learning

- **Concept**: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: This is the fundamental framework the paper is trying to optimize. Understanding how RLHF works is essential to understanding why query efficiency matters.
  - Quick check question: What are the three main stages of traditional RLHF, and how does DPO differ?

- **Concept**: Contextual Dueling Bandits
  - Why needed here: The paper formalizes RLHF as a contextual dueling bandit problem. This framework is essential for understanding the theoretical analysis and algorithm design.
  - Quick check question: How does a contextual dueling bandit differ from a standard multi-armed bandit problem?

- **Concept**: Active Learning
  - Why needed here: The paper's query efficiency strategy is based on active learning principles. Understanding how active learning reduces labeling costs is key to understanding the approach.
  - Quick check question: What is the key difference between passive and active learning in terms of data selection?

## Architecture Onboarding

- **Component map**: Base model (e.g., Zephyr-7B-SFT) → Preference prediction module → Confidence estimator → Query filter → (Human label or Pseudo-label) → Training pipeline

- **Critical path**: Base model → Preference prediction → Confidence estimation → Query decision → (Human label or Pseudo-label) → Training update

- **Design tradeoffs**:
  - Query threshold vs. performance: Lower thresholds mean more queries but potentially better performance
  - Confidence metric choice: Different metrics may have different correlations with true uncertainty
  - Pseudo-label accuracy vs. query savings: More aggressive pseudo-labeling saves queries but risks performance degradation

- **Failure signatures**:
  - Query efficiency degrades: Confidence metric poorly correlates with uncertainty
  - Performance drops: Pseudo-labels are systematically incorrect
  - Training instability: Improper threshold calibration or reward model issues

- **First 3 experiments**:
  1. Ablation study comparing ADPO with different confidence thresholds on a small dataset
  2. Performance comparison of ADPO vs. DPO on objective benchmarks with varying query budgets
  3. Analysis of pseudo-label accuracy across different confidence levels on validation data

## Open Questions the Paper Calls Out

1. How can the theoretical guarantees of APPO be extended to the non-linear reward model setting used in ADPO? The paper acknowledges this limitation but does not provide theoretical analysis for the non-linear case, which is crucial for practical applications.

2. What is the optimal strategy for selecting the confidence threshold γ in ADPO? The paper shows performance across different γ values but doesn't provide a principled method for selecting it, relying instead on grid search.

3. How does the performance of ADPO scale with the size of the action space A in practical applications? The experiments use a fixed action space, and the paper doesn't explore how increasing A affects query efficiency or performance.

## Limitations

- The theoretical framework relies on strong assumptions about linear rewards and known sub-optimality gaps that may not hold in real-world RLHF scenarios
- The effectiveness of pseudo-labels depends heavily on the confidence metric's correlation with actual uncertainty, which isn't extensively validated
- The confidence threshold selection appears to be critical but the paper doesn't provide clear guidance on how to tune it for different tasks

## Confidence

- **High confidence**: The experimental results showing ADPO matches DPO performance while using ~50% fewer queries (based on direct benchmark comparisons)
- **Medium confidence**: The theoretical regret and query complexity bounds (given the simplifying assumptions required)
- **Low confidence**: The generalizability of results across different domains and model scales beyond the tested Zephyr models

## Next Checks

1. Test ADPO on a diverse set of tasks with varying complexity levels to assess robustness of the confidence-based query selection mechanism
2. Conduct ablation studies systematically varying the confidence threshold to identify optimal ranges for different dataset sizes and task difficulties
3. Evaluate pseudo-label accuracy across different confidence levels on held-out validation data to quantify when pseudo-labels become unreliable