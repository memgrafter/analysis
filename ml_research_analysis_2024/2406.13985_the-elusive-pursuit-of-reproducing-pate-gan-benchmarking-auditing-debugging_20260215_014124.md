---
ver: rpa2
title: 'The Elusive Pursuit of Reproducing PATE-GAN: Benchmarking, Auditing, Debugging'
arxiv_id: '2406.13985'
source_url: https://arxiv.org/abs/2406.13985
tags:
- data
- privacy
- synthetic
- original
- pate-gan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks and audits six popular PATE-GAN implementations,
  revealing critical flaws in both utility and privacy. The study finds that none
  of the implementations reproduce the utility performance reported in the original
  paper, with AUROC scores averaging 54.71% lower.
---

# The Elusive Pursuit of Reproducing PATE-GAN: Benchmarking, Auditing, Debugging

## Quick Facts
- **arXiv ID:** 2406.13985
- **Source URL:** https://arxiv.org/abs/2406.13985
- **Reference count:** 40
- **Primary result:** None of six PATE-GAN implementations reproduce the utility performance reported in the original paper, with AUROC scores averaging 54.71% lower, and all leak more privacy than intended.

## Executive Summary
This paper conducts a comprehensive benchmarking and auditing of six popular PATE-GAN implementations, revealing critical flaws in both utility and privacy guarantees. The study systematically evaluates the implementations across four public datasets using 12 classifiers, finding that none achieve the utility performance claimed in the original paper. More concerning, all implementations leak more privacy than their theoretical bounds suggest, with 17 privacy violations and 5 other bugs identified across the implementations. These findings highlight the significant challenges in correctly implementing and auditing differentially private generative models, even for well-established algorithms like PATE-GAN.

## Method Summary
The study benchmarks six PATE-GAN implementations (original1, updated2, synthcity3, turing4, borealis5, smartnoise6) on four datasets using 12 classifiers to evaluate utility. Privacy auditing is conducted using black-box membership inference attacks (GroundHog and Querybased) to compare empirical privacy estimates (ϵemp) against theoretical bounds (ϵ). The implementations are configured with default hyperparameters except for maximum iterations set to 10,000 and λ set to 0.001 for the updated implementation. Both average-case and worst-case datasets are used for privacy evaluation.

## Key Results
- All six implementations fail to reproduce the utility performance reported in the original PATE-GAN paper, with AUROC scores averaging 54.71% lower.
- Every implementation leaks more privacy than intended, with empirical privacy estimates far exceeding theoretical bounds.
- 17 privacy violations and 5 other bugs are identified across implementations, including incorrect data partitioning, faulty privacy accounting, and non-DP data processing.
- The original and updated implementations fail to produce synthetic data with meaningful utility on three of four datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorrect PATE implementation causes privacy violations
- Mechanism: PATE-GAN implementations fail to properly partition data into disjoint subsets for teachers, allowing teachers to see overlapping data
- Core assumption: Disjoint data partitions are required for PATE's privacy guarantees
- Evidence anchors:
  - [abstract]: "we identify 17 privacy violations and 5 other bugs across the implementations, including incorrect data partitioning"
  - [section 6.1]: "For updated and synthcity, this breaks a main PATE assumptions (i.e., every teacher can only see a disjoint partition of the data)"
  - [corpus]: No direct corpus evidence found
- Break condition: If data partitioning is corrected to ensure disjoint sets, privacy violations from this source should be eliminated

### Mechanism 2
- Claim: Privacy accountant implementation errors lead to underestimated privacy loss
- Mechanism: Bugs in moments accountant calculation (indexing error in synthcity, missing log operator in borealis/smartnoise) cause severe underestimation of privacy budget
- Core assumption: Correct implementation of privacy accounting is essential for accurate privacy guarantees
- Evidence anchors:
  - [abstract]: "uncovering 17 privacy violations and 5 other bugs"
  - [section 6.1]: "synthcity has an indexing bug...which makes it severely underestimate ϵ. By contrast, borealis and smartnoise massively overestimate ϵ, to around 320k"
  - [corpus]: No direct corpus evidence found
- Break condition: If privacy accountant is correctly implemented with proper scaling, estimated privacy budget should align with theoretical bounds

### Mechanism 3
- Claim: Non-DP data processing introduces additional privacy leakage
- Mechanism: Implementations directly extract data bounds without differential privacy protection, leaking sensitive information
- Core assumption: Any non-DP processing of raw data can leak privacy information
- Evidence anchors:
  - [abstract]: "5 other bugs in these six open-source implementations"
  - [section 6.3]: "the majority of implementations (original, updated, synthcity, borealis) directly extract the data bounds from the data in a non-DP way"
  - [corpus]: No direct corpus evidence found
- Break condition: If data bounds are extracted in a DP manner, this source of leakage should be mitigated

## Foundational Learning

- Concept: Differential Privacy fundamentals
  - Why needed here: Understanding DP is essential to comprehend privacy violations and why certain implementations fail
  - Quick check question: What is the relationship between ε, δ, and privacy leakage in DP?

- Concept: PATE framework and teacher-student architecture
  - Why needed here: PATE-GAN builds on PATE, so understanding how teachers are trained on disjoint data and students learn from DP labels is crucial
  - Quick check question: How does PATE achieve privacy through data partitioning and noisy aggregation?

- Concept: Membership Inference Attacks (MIAs)
  - Why needed here: DP auditing relies on MIAs to empirically estimate privacy leakage
  - Quick check question: How does a membership inference attack distinguish between training and non-training data?

## Architecture Onboarding

- Component map: Data preprocessing -> Data partitioning -> Teacher discriminators -> Generator -> Student discriminator -> Privacy accountant -> Evaluation
- Critical path:
  1. Data partitioning and preprocessing
  2. Teacher discriminator training on disjoint data
  3. Generator training with student discriminator
  4. Privacy budget tracking and enforcement
  5. Utility evaluation on downstream tasks
- Design tradeoffs:
  - Number of teachers vs. privacy budget (more teachers = higher privacy cost)
  - Noise scale (λ) vs. utility (higher noise = better privacy but worse utility)
  - Data partitioning strategy (random vs. stratified) vs. model performance
- Failure signatures:
  - Utility significantly below theoretical expectations
  - Privacy estimates (ϵemp) much higher than theoretical bounds (ϵ)
  - Teacher discriminators trained on overlapping data
  - Privacy budget spent too quickly or not tracked properly
- First 3 experiments:
  1. Verify data partitioning: Check that teachers receive truly disjoint data subsets
  2. Test privacy accountant: Validate that privacy budget tracking matches theoretical expectations
  3. Audit data preprocessing: Ensure bounds extraction and scaling are performed in a DP manner

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the privacy leaks in PATE-GAN implementations be mitigated by fixing specific bugs, or are there deeper architectural issues?
- Basis in paper: [explicit] The paper identifies 17 privacy violations and 5 other bugs across implementations, with some implementations leaking more privacy than intended.
- Why unresolved: While the paper identifies specific bugs, it doesn't test whether fixing them alone would be sufficient to achieve the intended privacy guarantees.
- What evidence would resolve it: Empirical privacy audits comparing fixed vs. original implementations, measuring if privacy estimates align with theoretical bounds after bug fixes.

### Open Question 2
- Question: Does the choice of teacher model (neural networks vs. logistic regression) significantly impact both utility and privacy in PATE-GAN?
- Basis in paper: [explicit] Updated and synthcity implementations use logistic regression instead of neural networks for teachers, which the paper notes might affect utility.
- Why unresolved: The paper observes differences in utility but doesn't isolate the impact of teacher model choice from other architectural differences.
- What evidence would resolve it: Controlled experiments comparing identical implementations with different teacher models, measuring both utility and privacy outcomes.

### Open Question 3
- Question: How do the privacy leaks in PATE-GAN compare to other differentially private generative models like DP-GAN or DP-WGAN?
- Basis in paper: [inferred] The paper notes that none of the PATE-GAN implementations achieve the reported utility and all leak more privacy than intended, suggesting potential fundamental issues.
- Why unresolved: The paper focuses exclusively on PATE-GAN and doesn't benchmark against other DP generative models' privacy performance.
- What evidence would resolve it: Direct privacy audits comparing multiple DP generative models on identical datasets, measuring empirical vs. theoretical privacy bounds.

## Limitations
- The analysis is limited to six specific implementations and may not represent all possible variants of PATE-GAN.
- Privacy auditing relies on black-box membership inference attacks, potentially missing vulnerabilities detectable through white-box attacks.
- The study uses fixed hyperparameters and downstream classifiers, which may not represent optimal configurations for all implementations.

## Confidence

**High Confidence:** The identification of specific bugs in individual implementations (e.g., indexing errors in synthcity, missing log operators in borealis/smartnoise) and the general finding that none of the implementations reproduce the utility performance reported in the original paper.

**Medium Confidence:** The claim that all implementations leak more privacy than intended, as this is based on empirical privacy estimates that may vary depending on the specific datasets and attack methodologies used.

**Low Confidence:** The assertion that PATE-GAN is fundamentally difficult to implement correctly, as this conclusion is drawn from a limited sample of six implementations and may not generalize to all potential implementations.

## Next Checks
1. **White-box Privacy Auditing:** Conduct a white-box privacy audit of the six PATE-GAN implementations to identify any additional privacy vulnerabilities that may not be detectable through black-box membership inference attacks.

2. **Hyperparameter Sensitivity Analysis:** Perform a comprehensive hyperparameter sensitivity analysis across the six implementations to determine the impact of different noise scales, data partitioning strategies, and training configurations on utility and privacy outcomes.

3. **Alternative Implementation Testing:** Implement and test alternative variants of PATE-GAN (e.g., different teacher-student architectures, alternative privacy accounting mechanisms) to assess whether the identified issues are inherent to the PATE-GAN framework or specific to the implementations studied.