---
ver: rpa2
title: Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception
  Reasoning
arxiv_id: '2402.11432'
source_url: https://arxiv.org/abs/2402.11432
tags:
- deception
- reasoning
- evaluation
- used
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends deception detection to deception reasoning,
  aiming to provide objective evidence to support subjective judgment. To facilitate
  subsequent research, the authors construct a dataset and define evaluation metrics,
  including accuracy, completeness, logic, and depth.
---

# Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning

## Quick Facts
- **arXiv ID**: 2402.11432
- **Source URL**: https://arxiv.org/abs/2402.11432
- **Reference count**: 32
- **Primary result**: Proposes deception reasoning task extending beyond detection to provide objective evidence for subjective judgments, with dataset and evaluation metrics for benchmarking LLM reasoning capabilities

## Executive Summary
This paper introduces deception reasoning as an extension of traditional deception detection, aiming to provide objective evidence that supports subjective judgments about potential lies. The authors construct a dataset using synthetic dialogues generated from legal instruments, where police officers attempt to identify potential lies during interrogations. The work defines comprehensive evaluation metrics including accuracy, completeness, logic, and depth to assess reasoning performance. Experimental results demonstrate that existing LLMs can handle deception reasoning to some extent, with Chinese LLMs showing particular progress in reasoning capabilities.

## Method Summary
The method involves generating synthetic interrogation dialogues using GPT-4 based on legal instruments, where potential lies are embedded within the conversations. The process begins with legal instrument selection and target content extraction using a two-stage strategy, followed by action extraction and incomplete action generation. GPT-4 then creates mock interrogation dialogues between police officers (with incomplete knowledge) and suspects. The reasoning task involves identifying potential lies and providing supporting evidence. Evaluation is conducted using both automatic methods (GPT-4 as evaluator) and manual human annotation across four metrics: accuracy, completeness, logic, and depth.

## Key Results
- Two-stage extraction strategy outperforms one-stage strategy in accuracy and reduces action complexity
- GPT-4 demonstrates superior performance compared to GPT-3.5 in target content and action extraction
- Automatic evaluation using GPT-4 shows high reliability with PCC scores indicating consistency with manual evaluation
- Chinese LLMs (Qwen2, ERNIE4.0) show progress in reasoning ability on deception reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage extraction strategy (target content extraction followed by action extraction) outperforms the one-stage strategy in accuracy and reduces action complexity.
- Mechanism: By splitting the extraction task into two distinct stages, each focused on a specific sub-task, the model can allocate more attention and reasoning capacity to each, leading to better performance.
- Core assumption: Target content and action extraction are sufficiently distinct tasks that benefit from separate processing.
- Evidence anchors:
  - [section] "Experimental results demonstrate that the two-stage strategy is more effective than the one-stage strategy. Meanwhile, GPT-4 performs better than GPT-3.5."
  - [section] "Take the complete actions in Table 1 as an example. These actions are well-decomposed. But if we merge two actions into one action, this decomposition process is inappropriate, leading to an increase in action complexity."
- Break condition: If target content and action extraction become too intertwined or if the model's attention mechanism becomes sufficiently advanced to handle both tasks simultaneously without loss of performance.

### Mechanism 2
- Claim: GPT-4 outperforms GPT-3.5 in both target content and action extraction due to its superior text understanding capabilities.
- Mechanism: GPT-4's larger model size and more advanced training allow it to better understand the structure and semantics of the text, leading to more accurate extraction of target content and more appropriate decomposition of actions.
- Core assumption: GPT-4's text understanding capabilities are indeed superior to GPT-3.5's in a way that is relevant to this task.
- Evidence anchors:
  - [section] "Experimental results of different strategies are shown in Table 5. From this table, we observe that the two-stage strategy achieves better performance than the one-stage strategy. Meanwhile, GPT-4 can achieve better performance than GPT-3.5."
  - [section] "Target content and action extraction require the model to understand not only the literal meaning of the text but also its structure and semantic content. Since GPT-4 can achieve better performance than GPT-3.5 in text understanding, it can also achieve better performance in target content and action extraction."
- Break condition: If GPT-4's performance advantage over GPT-3.5 diminishes due to changes in model architecture or training data.

### Mechanism 3
- Claim: The automatic evaluation strategy using GPT-4 is reliable, as evidenced by high PCC scores with manual evaluation results.
- Mechanism: GPT-4's strong reasoning and understanding capabilities allow it to effectively evaluate the reasoning performance of other LLMs on the deception reasoning task, providing a consistent and reliable automatic evaluation.
- Core assumption: GPT-4's evaluation is consistent with human judgment in this domain.
- Evidence anchors:
  - [section] "Table 3 shows the PCC scores between automatic and manual evaluation results. We observe that manual evaluation results have relatively high similarities with automatic evaluation results, proving the reliability of our automatic evaluation strategy."
  - [corpus] "Average neighbor FMR=0.411, average citations=0.0." (Weak corpus evidence, suggesting limited related work in this specific area.)
- Break condition: If GPT-4's evaluation criteria diverge significantly from human judgment in this domain, or if the task becomes too complex for GPT-4 to evaluate consistently.

## Foundational Learning

- Concept: Deception reasoning and its distinction from deception detection.
  - Why needed here: Understanding the task's unique requirements is crucial for designing appropriate datasets, evaluation metrics, and model architectures.
  - Quick check question: What is the key difference between deception detection and deception reasoning as described in the paper?

- Concept: Multimodal clues and their role in deception detection.
  - Why needed here: While the paper focuses on text-based reasoning, understanding the broader context of deception detection, including multimodal approaches, provides a foundation for appreciating the paper's contribution.
  - Quick check question: What are some examples of multimodal clues used in traditional deception detection?

- Concept: Large language models (LLMs) and their reasoning capabilities.
  - Why needed here: The paper evaluates the performance of various LLMs on the deception reasoning task, so understanding LLM capabilities and limitations is essential.
  - Quick check question: What are some key factors that influence the reasoning performance of LLMs?

## Architecture Onboarding

- Component map:
  Legal instrument selection -> Target content extraction (two-stage strategy with GPT-4) -> Action extraction -> Incomplete action generation -> Mock interrogation dialogue generation (using GPT-4) -> Potential lie identification and reasoning analysis -> Evaluation metrics (accuracy, completeness, logic, depth) -> Evaluators (automatic GPT-4 and manual human annotators)

- Critical path: Legal instrument selection → Target content extraction → Action extraction → Incomplete action generation → Mock interrogation → Potential lie identification → Reasoning analysis → Evaluation

- Design tradeoffs:
  - Using synthetic dialogues vs. real interrogation data (cost vs. authenticity)
  - One-stage vs. two-stage extraction strategy (simplicity vs. accuracy)
  - GPT-3.5 vs. GPT-4 for various tasks (cost vs. performance)

- Failure signatures:
  - Low target accuracy in extraction indicates issues with legal instrument selection or extraction strategy
  - High action complexity suggests inappropriate action decomposition
  - Low PCC scores between automatic and manual evaluation indicate issues with the evaluation strategy
  - Unnatural dialogues suggest problems with the dialogue generation process

- First 3 experiments:
  1. Compare the performance of one-stage and two-stage extraction strategies on a small sample of legal instruments.
  2. Evaluate the impact of using GPT-3.5 vs. GPT-4 for target content extraction on a subset of data.
  3. Test the naturalness of synthetic dialogues by having human annotators rate a sample of dialogues.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 compare to human evaluators in terms of consistency and reliability when evaluating deception reasoning?
- Basis in paper: [explicit] The paper states that GPT-4 has been proven to have consistency with human assessments in previous research, and it is used for automatic evaluation. However, the paper also mentions the possibility of bias issues and overfitting when using one LLM to evaluate other LLMs.
- Why unresolved: The paper does not provide a direct comparison between GPT-4 and human evaluators in terms of consistency and reliability for deception reasoning evaluation. It only mentions that manual evaluation is performed by 8 annotators.
- What evidence would resolve it: A direct comparison study between GPT-4 and human evaluators, measuring their consistency and reliability in evaluating deception reasoning, would provide evidence to answer this question.

### Open Question 2
- Question: What is the impact of using different strategies for target content and action extraction on the overall performance of deception reasoning?
- Basis in paper: [explicit] The paper discusses two strategies for target content and action extraction: one-stage and two-stage. It also compares the performance of GPT-3.5 and GPT-4 in these strategies.
- Why unresolved: While the paper provides a comparison between one-stage and two-stage strategies, as well as GPT-3.5 and GPT-4, it does not explore the impact of these strategies on the overall performance of deception reasoning. It only focuses on the accuracy of target content extraction and the complexity of action extraction.
- What evidence would resolve it: A study that evaluates the impact of different target content and action extraction strategies on the overall performance of deception reasoning, including accuracy, completeness, logic, and depth, would provide evidence to answer this question.

### Open Question 3
- Question: How does the performance of Chinese LLMs compare to other LLMs in deception reasoning, and what factors contribute to their progress?
- Basis in paper: [explicit] The paper mentions that Chinese LLMs, such as Qwen2 and ERINE4.0, have shown progress in reasoning ability compared to their previous versions. It also evaluates the performance of Chinese LLMs in deception reasoning.
- Why unresolved: The paper does not provide a detailed analysis of the factors contributing to the progress of Chinese LLMs in deception reasoning. It only mentions that Chinese LLMs have made progress in reasoning ability.
- What evidence would resolve it: A study that analyzes the factors contributing to the progress of Chinese LLMs in deception reasoning, such as model architecture, training data, or fine-tuning techniques, would provide evidence to answer this question.

## Limitations

- The paper relies on synthetic data generation which may not capture the full complexity and nuance of real-world deceptive interactions
- The evaluation strategy using GPT-4 as an automatic evaluator assumes GPT-4's reasoning capabilities are sufficiently advanced without independent validation against diverse human judgments
- The paper lacks sufficient detail on prompt engineering and legal instrument selection criteria, potentially impacting reproducibility

## Confidence

- **High confidence**: The two-stage extraction strategy improves accuracy over one-stage extraction, as demonstrated by the experimental results comparing GPT-4 and GPT-3.5 performance
- **Medium confidence**: GPT-4's superior performance in target content and action extraction is attributed to its better text understanding capabilities, though the exact mechanisms are not fully explained
- **Medium confidence**: The automatic evaluation strategy using GPT-4 is reliable, as indicated by the high Pearson Correlation Coefficient (PCC) scores with manual evaluation results, but this may not generalize to all types of deception reasoning tasks

## Next Checks

1. **Reproduce the extraction strategy**: Implement both one-stage and two-stage extraction strategies on a small sample of legal instruments and compare their performance on target accuracy and action complexity metrics.

2. **Validate the evaluation strategy**: Have a diverse group of human annotators evaluate a sample of reasoning outputs and compare their results with the automatic GPT-4 evaluation to assess consistency and identify potential biases.

3. **Test robustness to real-world data**: Apply the deception reasoning task to a small dataset of real police-suspect interrogation transcripts and evaluate whether the synthetic data generation approach generalizes effectively to more complex, naturalistic scenarios.