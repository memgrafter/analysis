---
ver: rpa2
title: Epistemic Uncertainty and Observation Noise with the Neural Tangent Kernel
arxiv_id: '2409.03953'
source_url: https://arxiv.org/abs/2409.03953
tags:
- posterior
- neural
- mean
- covariance
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper extends the neural tangent kernel (NTK) framework to
  handle non-zero aleatoric noise and estimate epistemic uncertainty. It shows that
  training a network with a squared norm penalty on parameter deviations from initialization
  is equivalent to computing the posterior mean of an NTK Gaussian process with noise.
---

# Epistemic Uncertainty and Observation Noise with the Neural Tangent Kernel

## Quick Facts
- arXiv ID: 2409.03953
- Source URL: https://arxiv.org/abs/2409.03953
- Authors: Sergio Calvo-Ordoñez; Konstantina Palla; Kamil Ciosek
- Reference count: 40
- Primary result: Extends NTK framework to handle non-zero aleatoric noise and estimate epistemic uncertainty

## Executive Summary
This paper presents a unified framework for handling both observation noise and epistemic uncertainty in neural networks using the neural tangent kernel (NTK). The authors show that training a network with a squared norm penalty on parameter deviations from initialization is equivalent to computing the posterior mean of an NTK Gaussian process with noise. The method seamlessly integrates with standard training pipelines using gradient descent, requiring only a small modification to the loss function. Posterior covariance is estimated using a small number of additional predictor networks trained on Jacobian eigenvectors and random targets, enabling practical uncertainty quantification.

## Method Summary
The method combines regularized neural network training with NTK-GP inference. The core idea is that minimizing a regularized mean squared error loss with a squared norm penalty on parameter deviations from initialization is equivalent to computing the posterior mean of an NTK-GP with observation noise. The noise level σ² is determined by the regularization strength β as σ² = Nβ/N. For covariance estimation, the method uses a partial SVD of the Jacobian matrix and trains K additional predictor networks on the eigenvectors with random targets. The framework requires only minor modifications to standard training procedures while providing both aleatoric and epistemic uncertainty estimates.

## Key Results
- Shows equivalence between regularized network training and NTK-GP posterior mean computation
- Proposes efficient covariance estimation using Jacobian eigenvectors and additional predictors
- Demonstrates method on synthetic 1D regression with both interpolation and extrapolation
- Provides theoretical guarantees on covariance estimation accuracy under appropriate conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training a wide neural network with gradient descent and a squared norm penalty on parameter deviations from initialization is equivalent to computing the posterior mean of an NTK-GP with non-zero aleatoric noise.
- Mechanism: The regularized loss function induces a gradient flow that, in the infinite-width limit, converges to the same posterior mean as Bayesian inference in an NTK-GP with noise level σ² = Nβ/N.
- Core assumption: The neural network operates in the NTK regime where the first-order approximation remains valid throughout training.
- Evidence anchors: [abstract] "training a network with a squared norm penalty on parameter deviations from initialization is equivalent to computing the posterior mean of an NTK Gaussian process with noise"; [section 3.1] Proof showing the equivalence between regularized gradient flow and NTK-GP posterior mean computation.

### Mechanism 2
- Claim: The posterior covariance can be estimated by training additional predictor networks on Jacobian eigenvectors and random targets.
- Mechanism: By diagonalizing the kernel matrix and training predictors on its eigenvectors, we can approximate the term KUΛK⊤ in the posterior covariance formula. Random targets provide an estimate of the noise term σ²MM⊤.
- Core assumption: The partial SVD of the Jacobian can be computed efficiently and captures the relevant spectral structure of the kernel.
- Evidence anchors: [abstract] "The posterior covariance is estimated using a small number of additional predictors trained on Jacobian eigenvectors and random targets"; [section 3.2] Proposition showing how the covariance estimator relates to the posterior covariance formula.

### Mechanism 3
- Claim: Shifting the labels and predictions allows computation of the posterior mean for a zero-mean GP.
- Mechanism: By defining shifted labels ỹ = y + f(x;θ₀) and shifted predictions f̃(x;θ∞) = f(x;θ∞) − f(x;θ₀), we can compute the posterior mean using equation (6), which corresponds to a zero-mean GP.
- Core assumption: The prior mean of the GP can be set to zero through appropriate data transformations.
- Evidence anchors: [section 3.1] Lemma 3.2 showing the data transformation for zero prior mean.

## Foundational Learning

- Concept: Gaussian Processes
  - Why needed here: The NTK framework connects neural network training to GP inference, requiring understanding of GP priors, posteriors, and kernel functions.
  - Quick check question: What is the relationship between the kernel matrix K(x,x) and the covariance matrix of function values in a GP?

- Concept: Neural Tangent Kernel
  - Why needed here: The NTK defines the kernel function for the GP that the wide neural network converges to during training.
  - Quick check question: How does the NTK relate to the Jacobian of the network with respect to its parameters?

- Concept: Gradient Descent Dynamics in Wide Networks
  - Why needed here: Understanding how gradient descent behaves in the NTK regime is crucial for connecting network training to GP inference.
  - Quick check question: What assumptions about network width and initialization are necessary for the NTK to remain constant during training?

## Architecture Onboarding

- Component map: Main network -> Regularized loss -> Posterior mean; Jacobian computation -> Partial SVD -> Predictor networks -> Covariance estimate
- Critical path: 1. Initialize main network with appropriate width and initialization; 2. Train main network with regularized MSE loss to obtain posterior mean; 3. Compute partial SVD of Jacobian; 4. Train predictor networks on eigenvectors and random targets; 5. Combine results to estimate posterior covariance
- Design tradeoffs: Width vs. computational cost (wider networks provide better NTK approximation but are more expensive); Number of predictors vs. covariance accuracy (more predictors give better estimates but increase computation); Partial SVD accuracy vs. efficiency (more accurate SVD is better but more expensive)
- Failure signatures: Poor uncertainty estimates (may indicate insufficient network width or too few predictors); Numerical instability (could result from ill-conditioned kernel matrices or SVD computation issues); Slow convergence (might suggest inappropriate learning rates or regularization strength)
- First 3 experiments: 1. Verify NTK-GP posterior mean computation on a simple 1D regression task with known solution; 2. Test covariance estimation by comparing to analytic upper bounds on synthetic data; 3. Evaluate uncertainty quantification on a toy classification problem with synthetic noise

## Open Questions the Paper Calls Out

- Question: How does the proposed method perform on real-world datasets compared to synthetic regression tasks?
  - Basis in paper: [inferred] The paper only demonstrates results on a synthetic 1D regression task, suggesting the need for evaluation on more complex, real-world datasets.
  - Why unresolved: The authors explicitly state this is a "proof-of-concept" and defer implementation on larger problem instances to future work.
  - What evidence would resolve it: Empirical evaluation on standard regression benchmarks (e.g., UCI datasets) showing comparable or improved uncertainty estimates against established methods like deep ensembles or Bayesian neural networks.

- Question: What is the optimal number of additional predictor networks (K) and random target samples (K') needed for accurate covariance estimation?
  - Basis in paper: [explicit] The authors note that "K' = 0 often works well" but do not provide systematic analysis of the trade-off between computational cost and estimation accuracy.
  - Why unresolved: The paper mentions this is deferred to Appendix D but does not provide quantitative guidelines or sensitivity analysis.
  - What evidence would resolve it: Systematic experiments varying K and K' on benchmark tasks, showing the relationship between these hyperparameters and both computational efficiency and estimation accuracy.

- Question: How does the method scale to high-dimensional input spaces and deeper network architectures?
  - Basis in paper: [inferred] The authors mention that computing the full Jacobian is "infeasible for larger problem instances" and defer implementation details to future work.
  - Why unresolved: The paper acknowledges scalability limitations but does not provide solutions or empirical validation on high-dimensional problems.
  - What evidence would resolve it: Implementation of the randomized SVD approach mentioned in Appendix E, with evaluation on high-dimensional regression tasks and comparison to computational requirements of alternative methods.

## Limitations

- The theoretical framework relies heavily on the infinite-width limit assumption, which may not hold in practical finite-width scenarios.
- Covariance estimation requires careful selection of the number of predictors and quality of the partial SVD, which can significantly impact performance.
- The method's computational efficiency depends on successful implementation of the partial SVD and multiple predictor training, which may be challenging for high-dimensional inputs or very deep networks.

## Confidence

- **High Confidence**: The equivalence between regularized training and NTK-GP posterior mean computation (Mechanism 1) is theoretically well-established in the infinite-width limit.
- **Medium Confidence**: The covariance estimation approach (Mechanism 2) is novel and its effectiveness depends heavily on implementation details and hyperparameter choices.
- **Medium Confidence**: The data transformation for zero prior mean (Mechanism 3) is mathematically sound but requires careful implementation.

## Next Checks

1. **Width Sensitivity Analysis**: Systematically evaluate how uncertainty estimates degrade as network width decreases from the infinite-width regime, identifying the minimum width needed for reliable uncertainty quantification.

2. **Covariance Quality Assessment**: Compare the empirical variance of predictions from multiple independently trained networks against the estimated posterior covariance to verify the accuracy of the eigenvector-based covariance estimation method.

3. **Computational Efficiency Benchmark**: Measure the actual wall-clock time and memory requirements of the full method (including Jacobian computation and multiple predictor training) compared to standard uncertainty estimation approaches on problems of varying dimensionality.