---
ver: rpa2
title: Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?
arxiv_id: '2406.13121'
source_url: https://arxiv.org/abs/2406.13121
tags:
- query
- retrieval
- answer
- documents
- title
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LOFT, a new benchmark designed to rigorously
  evaluate long-context language models (LCLMs) on tasks that could potentially subsume
  specialized tools like retrieval systems, RAG pipelines, SQL, and more. The benchmark
  includes 35 datasets across six task types and three modalities, covering text,
  visual, and audio retrieval, as well as RAG, SQL-like reasoning, and many-shot in-context
  learning.
---

# Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?

## Quick Facts
- arXiv ID: 2406.13121
- Source URL: https://arxiv.org/abs/2406.13121
- Authors: Jinhyuk Lee; Anthony Chen; Zhuyun Dai; Dheeru Dua; Devendra Singh Sachan; Michael Boratko; Yi Luan; Sébastien M. R. Arnold; Vincent Perot; Siddharth Dalmia; Hexiang Hu; Xudong Lin; Panupong Pasupat; Aida Amini; Jeremy R. Cole; Sebastian Riedel; Iftekhar Naim; Ming-Wei Chang; Kelvin Guu
- Reference count: 40
- Primary result: LCLMs perform comparably to specialized models on text retrieval at 128k context length but lag on complex multi-hop reasoning tasks

## Executive Summary
This paper introduces LOFT, a benchmark designed to evaluate long-context language models (LCLMs) on tasks that could potentially subsume specialized tools like retrieval systems, RAG pipelines, SQL, and more. The benchmark includes 35 datasets across six task types and three modalities, covering text, visual, and audio retrieval, as well as RAG, SQL-like reasoning, and many-shot in-context learning. The authors propose a novel prompting approach called Corpus-in-Context (CiC) prompting and evaluate three state-of-the-art LCLMs using this method on LOFT, comparing their performance against specialized models.

## Method Summary
The authors created LOFT, a benchmark supporting up to one million tokens, to rigorously evaluate LCLMs on tasks spanning retrieval, RAG, SQL-like reasoning, and many-shot in-context learning across text, visual, and audio modalities. They introduced Corpus-in-Context (CiC) prompting, which combines instructions, few-shot examples, and chain-of-thought reasoning to guide LCLMs in learning, retrieving, and reasoning over in-context corpora. Three state-of-the-art LCLMs (Gemini 1.5 Pro, GPT-4o, and Claude 3 Opus) were evaluated using CiC prompting on LOFT tasks and compared against specialized models.

## Key Results
- At 128k context length, LCLMs perform comparably to specialized models in text retrieval and surpass them in some cases (Gemini 1.5 Pro outperforms CLIP in visual retrieval)
- LCLMs still lag significantly in complex multi-hop compositional reasoning tasks required for SQL-like tasks
- Prompting strategies such as chain-of-thought reasoning and few-shot example positioning significantly influence performance
- Position-dependent performance degradation reveals attention limitations in current LCLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-context language models can perform retrieval tasks without specialized retrieval training by leveraging their ability to process entire corpora within context.
- Mechanism: The models use their attention mechanisms to scan through the full context window, identifying relevant information based on semantic similarity and contextual cues, then outputting the identifiers of relevant passages.
- Core assumption: The model's attention distribution allows effective scanning of long contexts for relevant information.
- Evidence anchors:
  - [abstract] "Our findings reveal LCLMs' surprising ability to rival state-of-the-art retrieval and RAG systems, despite never having been explicitly trained for these tasks."
  - [section] "Table 2 demonstrates that Gemini 1.5 Pro performs comparably to Gecko at 128k context length. Other LCLMs also perform surprisingly well."
  - [corpus] Corpus analysis shows positional effects on retrieval performance, suggesting attention-based processing.
- Break condition: Performance degrades significantly when relevant information is placed in middle or end of context, indicating attention limitations.

### Mechanism 2
- Claim: Corpus-in-Context (CiC) prompting enhances LCLMs' ability to reason over large corpora by combining instructions, few-shot examples, and chain-of-thought reasoning.
- Mechanism: The prompt structure guides the model to first understand the corpus through few-shot examples, then apply reasoning steps to answer queries, leveraging the model's instruction-following capabilities.
- Core assumption: LCLMs can effectively learn from few-shot examples grounded in the same corpus they need to use for evaluation.
- Evidence anchors:
  - [abstract] "techniques such as adding instructions [27, 59, 14], incorporating few-shot examples [12], and leveraging demonstrations via chain-of-thought prompting [44, 60] can be seamlessly integrated"
  - [section] "CiC prompting effectively combines established prompting strategies, tailoring them to leverage the unique capabilities of LCLMs for learning, retrieving and reasoning over in-context corpora."
  - [corpus] Ablation studies show performance drops when removing any of these components.
- Break condition: If few-shot examples are not grounded in the same corpus or if chain-of-thought is removed, performance significantly degrades.

### Mechanism 3
- Claim: Prefix-caching compatibility allows efficient processing of large contexts by encoding the corpus once rather than for each query.
- Mechanism: Since the query appears at the end of the prompt, the model can cache the encoded representation of the corpus and only encode the query portion for each inference, reducing computational cost.
- Core assumption: The model's autoregressive nature allows effective caching of the corpus prefix.
- Evidence anchors:
  - [abstract] "CiC prompting is its compatibility with prefix-caching in autoregressive language models [20] as the query appears at the end of the prompt."
  - [section] "This means the corpus only needs to be encoded once, similar to the indexing process in traditional information retrieval."
  - [corpus] No direct corpus evidence, but stated as design consideration.
- Break condition: If the model's attention mechanism degrades over long contexts or if caching is not effectively implemented.

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: Understanding how LCLMs scan long contexts for relevant information is crucial for interpreting their retrieval capabilities
  - Quick check question: How does the attention mechanism in transformers differ from traditional retrieval systems like dual encoders?

- Concept: Chain-of-thought reasoning
  - Why needed here: This technique is essential for the model's ability to perform multi-hop reasoning required in complex tasks like SQL-like queries
  - Quick check question: What is the difference between direct answer generation and chain-of-thought reasoning in language models?

- Concept: Prefix-caching in autoregressive models
  - Why needed here: Understanding this efficiency mechanism explains why CiC prompting is computationally feasible for large contexts
  - Quick check question: How does prefix-caching in autoregressive models differ from caching in encoder-only models?

## Architecture Onboarding

- Component map: CiC prompting → Corpus encoding → Query processing → Answer generation
- Critical path: Instruction → Corpus formatting → Few-shot examples → Query formatting → Output parsing
- Design tradeoffs: Longer contexts improve recall but increase computational cost and attention degradation
- Failure signatures: Position-dependent performance degradation, chain-of-thought reasoning failures, format sensitivity
- First 3 experiments:
  1. Test retrieval performance with gold documents at different positions in context
  2. Evaluate impact of removing chain-of-thought reasoning from few-shot examples
  3. Measure performance difference between prefix-caching and per-query corpus encoding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do long-context language models (LCLMs) perform on retrieval tasks as the context length scales beyond one million tokens?
- Basis in paper: Inferred from the paper's discussion on the current limitations of LOFT, which supports up to one million tokens but can be extended to tens of millions or even billions of tokens in the future.
- Why unresolved: The paper only evaluates LCLMs on tasks with context lengths up to one million tokens, leaving the performance on longer contexts unexplored.
- What evidence would resolve it: Conducting experiments on LOFT with context lengths beyond one million tokens and comparing the performance of LCLMs against specialized models would provide insights into their scalability.

### Open Question 2
- Question: How do different prompting strategies, such as chain-of-thought reasoning and few-shot examples, affect the performance of LCLMs on complex multi-hop compositional reasoning tasks like SQL-like tasks?
- Basis in paper: Explicit from the paper's findings that LCLMs still lag significantly on complex multi-hop compositional reasoning tasks and that prompting strategies significantly influence performance.
- Why unresolved: While the paper mentions the impact of prompting strategies, it does not provide a detailed analysis of how specific strategies affect performance on complex tasks.
- What evidence would resolve it: Conducting ablations on different prompting strategies for LCLMs on SQL-like tasks and comparing their performance would provide insights into the most effective strategies for complex reasoning tasks.

### Open Question 3
- Question: How do LCLMs compare to specialized models in terms of efficiency and computational cost when performing tasks like retrieval and SQL-like reasoning on large corpora?
- Basis in paper: Inferred from the paper's discussion on the efficiency considerations and the computational resources required for evaluating LCLMs on LOFT.
- Why unresolved: The paper focuses on the quality of LCLMs but does not provide a detailed analysis of their efficiency and computational cost compared to specialized models.
- What evidence would resolve it: Conducting experiments to measure the latency and computational cost of LCLMs on LOFT tasks and comparing them to specialized models would provide insights into their efficiency trade-offs.

## Limitations

- The study uses zero-shot CoT prompting for LCLMs while specialized models are trained on extensive datasets, creating an uneven comparison
- Performance degradation when relevant information is positioned in middle or end of context reveals attention mechanism limitations
- Reliance on synthetic data augmentation for multi-hop reasoning tasks raises questions about ecological validity

## Confidence

- Medium confidence: LCLMs can match specialized models on text retrieval tasks at 128k context length
- Low confidence: LCLMs can fully subsume SQL-like reasoning tasks without task-specific adaptation
- Medium confidence: CiC prompting consistently improves performance across all task types and modalities

## Next Checks

1. **Position invariance testing**: Systematically evaluate retrieval performance with gold documents at different positions (beginning, middle, end) within the context window to quantify attention degradation effects and identify position thresholds where performance collapses.

2. **Fine-tuning impact study**: Compare CiC-prompted LCLMs against their fine-tuned counterparts on the same tasks to establish the performance gap attributable to the lack of task-specific adaptation, providing a more complete picture of LCLMs' subsume potential.

3. **Cross-domain generalization**: Test whether LCLMs trained on synthetic multi-hop reasoning datasets can transfer to real-world SQL-like tasks with natural language queries and heterogeneous data sources, validating ecological relevance of the synthetic evaluation framework.