---
ver: rpa2
title: Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision
  Transformers for Medical Image Classification
arxiv_id: '2407.11573'
source_url: https://arxiv.org/abs/2407.11573
tags:
- federated
- peft
- parameters
- fine-tuning
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates federated parameter-efficient fine-tuning
  (PEFT) strategies for adapting pre-trained Vision Transformers (ViTs) to medical
  image classification tasks under data scarcity and privacy constraints. It systematically
  compares known and novel federated PEFT methods, including visual prompt tuning
  (VPT), decomposed visual prompts (DVPT), stochastic block attention (SBA), and hybrid
  approaches like LoRA+VPT.
---

# Probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification

## Quick Facts
- arXiv ID: 2407.11573
- Source URL: https://arxiv.org/abs/2407.11573
- Reference count: 40
- Primary result: Most federated PEFT methods work well for in-domain transfer but degrade substantially for out-of-domain and non-IID data.

## Executive Summary
This study evaluates federated parameter-efficient fine-tuning (PEFT) strategies for adapting pre-trained Vision Transformers to medical image classification tasks under data scarcity and privacy constraints. It systematically compares known and novel federated PEFT methods, including visual prompt tuning, decomposed visual prompts, stochastic block attention, and hybrid approaches. Experiments on medical datasets (HAM10000, Fed-ISIC2019) and natural image datasets (Caltech101, Flowers102) reveal that while most federated PEFT methods achieve strong accuracy-efficiency trade-offs for in-domain transfer, performance degrades substantially for out-of-domain and non-IID data—up to 4% accuracy loss per order of magnitude reduction in parameters. Results highlight that in-domain medical foundation models are preferable to general vision models for federated PEFT in healthcare.

## Method Summary
The study compares various federated PEFT methods (VPT, DVPT, SBA, ABA, LoRA, LoRA+VPT) against full fine-tuning and linear probing baselines. Experiments use ViT-B/16 pre-trained on ImageNet, with medical datasets (HAM10000, Fed-ISIC2019) and natural image datasets (Caltech101, Flowers102). Federated learning is implemented with 6 clients over 200 rounds using SGD optimizer (lr=0.01) and batch size 32. Parameter efficiency is measured by trainable/exchangeable parameters, while accuracy is measured using balanced accuracy. The study evaluates both in-domain transfer (natural to natural images) and out-of-domain transfer (natural to medical images) scenarios.

## Key Results
- Most federated PEFT methods achieve up to three orders of magnitude decrease in exchangeable parameters with minimal accuracy loss for in-domain transfer
- Out-of-domain tasks show substantial accuracy vs. efficiency trade-offs, with up to 4% accuracy loss per order of magnitude reduction in parameters
- Visual prompt tuning outperforms text-prompt federated methods in accuracy-efficiency trade-offs
- In-domain foundation models (pre-trained on medical images) narrow the accuracy gap between parameter-efficient and full fine-tuning methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated PEFT methods maintain strong accuracy for in-domain transfer while significantly reducing communication costs.
- Mechanism: By fine-tuning only a small subset of parameters (e.g., LoRA matrices, visual prompts), the number of exchanged parameters drops by orders of magnitude (e.g., from 86M to ~0.6M) with minimal accuracy loss (≤0.5%) for in-domain tasks.
- Core assumption: The pre-trained ViT has learned generalizable features that can be adapted with minimal parameter changes for similar-domain tasks.
- Evidence anchors:
  - [abstract]: "most federated PEFT methods work well for in-domain transfer"
  - [section]: "easily achieving up to three orders of magnitude decrease in the exchangeable parameters at a marginal cost to accuracy"
  - [corpus]: Weak/no direct evidence; corpus focuses on general PEFT surveys and comparisons without specific federated in-domain accuracy claims.
- Break condition: Accuracy degradation exceeds acceptable threshold (e.g., >1%) or communication cost savings are insufficient to justify complexity.

### Mechanism 2
- Claim: Federated PEFT performance degrades substantially for out-of-domain (OOD) tasks, especially with non-IID data.
- Mechanism: When adapting a general vision model (pre-trained on ImageNet) to medical images, the feature distribution shift requires more extensive parameter updates; federated averaging across heterogeneous clients exacerbates this, leading to up to 4% accuracy loss per order of magnitude reduction in fine-tuned parameters.
- Core assumption: OOD tasks require more extensive adaptation than in-domain tasks due to domain shift and non-IID client data distributions.
- Evidence anchors:
  - [abstract]: "there is a substantial accuracy vs. efficiency trade-off when dealing with OOD and non-IID scenarios"
  - [section]: "every order of magnitude reduction in fine-tuned/exchanged parameters can lead to a 4% drop in accuracy"
  - [corpus]: Weak; corpus lacks direct evidence on federated OOD performance degradation.
- Break condition: Client data becomes IID or domain shift is minimal, reducing the need for extensive adaptation.

### Mechanism 3
- Claim: Using in-domain foundation models (pre-trained on medical images) mitigates federated PEFT performance degradation for OOD tasks.
- Mechanism: Pre-training on domain-specific data (e.g., client4's medical images) aligns the feature space with target tasks, reducing the adaptation burden during federated PEFT and narrowing the accuracy gap between parameter-efficient and full fine-tuning.
- Core assumption: In-domain pre-training provides a better feature representation for subsequent task adaptation than general vision models.
- Evidence anchors:
  - [abstract]: "It is preferable to use medical foundation models learned from in-domain medical image data (if available) rather than general vision models."
  - [section]: "there is little difference among the federated PEFT methods in this scenario, proving that they perform equally well for in-domain transfer"
  - [corpus]: Weak; no direct corpus evidence on in-domain vs. out-of-domain foundation model performance in federated settings.
- Break condition: In-domain pre-training data is unavailable or insufficient, forcing reliance on general vision models.

## Foundational Learning

- Concept: Vision Transformer (ViT) architecture and its components (patch embeddings, self-attention blocks, class tokens).
  - Why needed here: Understanding ViT is essential to grasp which parameters are fine-tuned and how PEFT methods operate.
  - Quick check question: What is the role of the class token in a ViT, and how does it differ from patch tokens?
- Concept: Parameter-efficient fine-tuning (PEFT) methods (LoRA, VPT, ABA, SBA).
  - Why needed here: PEFT methods are the core techniques being evaluated; understanding their mechanisms and trade-offs is critical.
  - Quick check question: How does LoRA achieve parameter efficiency, and what is the role of the rank parameter?
- Concept: Federated learning (FL) and its aggregation mechanisms (FedAvg).
  - Why needed here: FL enables collaborative model training without centralizing data; understanding its workflow is necessary to interpret federated PEFT results.
  - Quick check question: How does FedAvg aggregate client updates, and what are its limitations in non-IID settings?

## Architecture Onboarding

- Component map: Pre-trained ViT -> Federated PEFT method selection -> Client local training -> Parameter aggregation -> Model evaluation
- Critical path: Pre-trained ViT → Federated PEFT method selection → Client local training → Parameter aggregation → Model evaluation
- Design tradeoffs: Parameter efficiency vs. accuracy (especially for OOD/non-IID), Communication cost vs. model performance, Complexity of PEFT method vs. ease of implementation
- Failure signatures: Accuracy degradation in OOD/non-IID settings, Communication bottlenecks due to large parameter exchanges, Convergence issues due to heterogeneous client data
- First 3 experiments:
  1. Replicate baseline results (full fine-tuning vs. LoRA/VPT) on in-domain dataset (Caltech101) to verify implementation
  2. Evaluate federated PEFT methods on OOD dataset (Fed-ISIC2019) to observe accuracy-efficiency trade-off
  3. Test impact of in-domain pre-training by fine-tuning on client4's data first, then evaluating federated PEFT on remaining clients

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do federated PEFT methods perform on larger-scale medical image datasets with more classes and samples?
- Basis in paper: [inferred] The paper experiments on relatively small medical datasets (HAM10000 with 10,015 images, Fed-ISIC2019 with 23,247 images) and notes that developing generic medical foundation models is challenging due to limited well-annotated data. It would be valuable to understand if the observed accuracy-efficiency trade-offs persist or change with larger datasets.
- Why unresolved: The current study uses datasets that are relatively small compared to large-scale natural image datasets used to pre-train the base ViT model. Scaling up to larger medical datasets with more classes could reveal different performance characteristics.
- What evidence would resolve it: Conducting experiments on larger medical image datasets (e.g., CheXpert, MIMIC-CXR, or multi-institutional datasets with thousands of samples per class) using the same federated PEFT methods would show whether the accuracy-efficiency trade-off scales or diminishes with more data.

### Open Question 2
- Question: How do different federated aggregation strategies (beyond FedAvg) impact the performance of PEFT methods in heterogeneous data scenarios?
- Basis in paper: [explicit] The paper uses FedAvg for aggregation across all methods and notes that non-IID data distribution significantly impacts federated PEFT performance, especially for out-of-domain tasks. It mentions that the stochastic block selected in SBA might lead to divergence in non-IID settings.
- Why unresolved: The paper only explores FedAvg as the aggregation method. Other federated learning algorithms (e.g., FedProx, SCAFFOLD, or adaptive aggregation methods) might better handle the statistical heterogeneity and potentially improve federated PEFT performance.
- What evidence would resolve it: Implementing and comparing alternative federated aggregation strategies with the same PEFT methods on non-IID datasets would reveal whether aggregation algorithms can mitigate the accuracy-efficiency trade-off in federated PEFT.

### Open Question 3
- Question: What is the optimal trade-off between rank selection in LoRA/DVPT and the number of communication rounds required for convergence?
- Basis in paper: [explicit] The paper experiments with different rank values for LoRA (r=4) and DVPT (rv=8), noting that the number of trainable parameters in LoRA is directly related to the rank. However, it doesn't systematically explore how different rank selections affect convergence speed and total communication cost.
- Why unresolved: While the paper identifies that lower rank leads to fewer parameters and better communication efficiency, it doesn't investigate the relationship between rank selection, convergence speed, and total communication cost across multiple rounds of federated learning.
- What evidence would resolve it: A systematic study varying rank parameters while tracking convergence speed (communication rounds to reach target accuracy) and total communication cost across different datasets would reveal the optimal rank-selection strategy for federated PEFT.

### Open Question 4
- Question: How does federated PEFT performance change when clients have varying computational capabilities and participate with different frequencies?
- Basis in paper: [inferred] The paper assumes a cross-silo setting with 6 clients participating in every round with equal computational resources. Real-world federated learning scenarios often involve clients with heterogeneous computational capabilities and varying participation frequencies.
- Why unresolved: The current experimental setup doesn't account for realistic federated learning scenarios where clients might have different hardware capabilities, participate in different numbers of rounds, or have varying availability for communication.
- What evidence would resolve it: Experiments simulating heterogeneous client capabilities (different model update sizes, different participation rates) would reveal how federated PEFT methods perform under realistic constraints and whether adaptive strategies are needed for practical deployment.

## Limitations
- The study does not explicitly report statistical significance testing for accuracy differences between methods
- Non-IID data simulation methodology is not fully specified (e.g., Dirichlet distribution parameters)
- Communication cost analysis focuses on parameter count but does not account for actual network bandwidth or latency

## Confidence

- **High confidence**: In-domain transfer performance of federated PEFT methods (supported by clear accuracy-efficiency trade-offs and replication of prior results)
- **Medium confidence**: Out-of-domain performance degradation claims (supported by observed accuracy drops but lacking statistical validation)
- **Low confidence**: Claims about in-domain foundation models being preferable to general vision models (lacks direct comparative evidence between pre-trained medical vs. ImageNet models)

## Next Checks
1. Conduct statistical significance testing (e.g., paired t-tests) on accuracy differences between federated PEFT methods across multiple runs to validate performance claims
2. Implement and evaluate the same federated PEFT methods using pre-trained medical foundation models (if available) to directly test the in-domain vs. out-of-domain foundation model hypothesis
3. Measure actual communication costs (including aggregation overhead) during federated training to validate parameter efficiency claims with real-world metrics beyond parameter counts