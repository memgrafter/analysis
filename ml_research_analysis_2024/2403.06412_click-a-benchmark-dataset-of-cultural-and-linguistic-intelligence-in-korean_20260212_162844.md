---
ver: rpa2
title: 'CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean'
arxiv_id: '2403.06412'
source_url: https://arxiv.org/abs/2403.06412
tags:
- korean
- language
- dataset
- cultural
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLIcK, a benchmark dataset for evaluating
  large language models on Korean cultural and linguistic intelligence. The dataset
  comprises 1,995 question-answer pairs sourced from official Korean exams and textbooks,
  categorized into 11 subcategories under language and culture.
---

# CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean

## Quick Facts
- **arXiv ID**: 2403.06412
- **Source URL**: https://arxiv.org/abs/2403.06412
- **Reference count**: 0
- **Primary result**: CLIcK is a Korean cultural and linguistic intelligence benchmark showing that proprietary models outperform open-source models, but both struggle with cultural reasoning tasks.

## Executive Summary
This paper introduces CLIcK, a benchmark dataset designed to evaluate large language models on Korean cultural and linguistic intelligence. The dataset contains 1,995 question-answer pairs sourced from official Korean exams and textbooks, categorized into 11 subcategories under language and culture. Each instance includes fine-grained annotations specifying the cultural and linguistic knowledge required for correct answers. The authors evaluate 13 models, including both open-source and proprietary LLMs, finding that open-source models generally perform poorly (10-50% accuracy) while proprietary models like GPT-3.5 and Claude-2 show better but still limited performance. The results suggest that simply scaling model size or fine-tuning with additional Korean corpora may not be sufficient to improve cultural intelligence in non-English languages.

## Method Summary
The CLIcK benchmark was constructed through a three-stage process: question collection from Korean educational materials, validation of question quality and relevance, and categorization into 11 subcategories under language and culture. Questions were sourced from official Korean exams (CSAT, TOPIK, Kedu) and textbooks, then evaluated for cultural and linguistic relevance. The dataset includes fine-grained annotations specifying which cultural and linguistic knowledge is required for each question. The evaluation methodology involves prompting 13 different models (both open-source and proprietary) with the questions and measuring accuracy across categories, with comparisons to human performance on similar exam tasks.

## Key Results
- Open-source models show significantly lower performance (10-50% accuracy) compared to proprietary models on Korean cultural and linguistic tasks
- Proprietary models like GPT-3.5 and Claude-2 outperform open-source models but still struggle with over 60% of the data
- Different categories show varying difficulty levels, with some cultural concepts proving particularly challenging for all models
- The results suggest that simply scaling model size or fine-tuning with additional Korean corpora does not guarantee improved cultural intelligence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct sourcing from Korean exams and textbooks preserves cultural and linguistic authenticity that translated datasets miss.
- Mechanism: By using original Korean materials instead of translated English datasets, the benchmark captures context-specific language usage, cultural references, and real-world knowledge embedded in the Korean educational system.
- Core assumption: Korean cultural and linguistic knowledge is context-dependent and cannot be accurately represented through translation alone.
- Evidence anchors:
  - [abstract]: "Because many existing Korean benchmark datasets are derived from the English counterparts through translation, they often overlook the different cultural contexts."
  - [section]: "Building on established practices in question generation (Zhou et al., 2017; Kurdi et al., 2019), we feed GPT-4 the full text of each chapter from the KIIP textbook, prompting it to produce multiple-choice questions, their corresponding choices and answers based strictly on the book's content."
  - [corpus]: Corpus shows strong clustering with other Korean-native benchmarks (KMMLU, KITE), suggesting cultural-linguistic authenticity is captured.

### Mechanism 2
- Claim: Fine-grained annotation of cultural and linguistic knowledge improves evaluation precision by mapping specific competencies to question types.
- Mechanism: By categorizing each question into 11 subcategories under language and culture, the benchmark enables targeted analysis of where models succeed or fail, revealing specific knowledge gaps.
- Core assumption: Different aspects of cultural and linguistic intelligence require distinct types of knowledge representation and reasoning.
- Evidence anchors:
  - [abstract]: "For each instance in CLIcK, we provide fine-grained annotation of which cultural and linguistic knowledge is required to answer the question correctly."
  - [section]: "We adopt eight subcategories based on the KIIP textbook. The primary chapters of the KIIP basic textbook encompass Society, Culture, Politics, Economy, Law, History, and Geography."
  - [corpus]: Weak corpus evidence for this specific annotation approach, but related work (Ko-PIQA, Nunchi-Bench) suggests fine-grained categorization is valued in Korean cultural reasoning benchmarks.

### Mechanism 3
- Claim: Direct comparison to human performance on real Korean exams provides meaningful calibration of model capabilities.
- Mechanism: By mapping model performance to actual exam score distributions (CSAT, TOPIK, Kedu), the benchmark establishes concrete performance thresholds that reflect real-world proficiency standards.
- Core assumption: Human exam performance provides a valid and interpretable baseline for evaluating AI model capabilities in cultural and linguistic tasks.
- Evidence anchors:
  - [abstract]: "The results and analysis in this section give insights that simply amassing more data and enlarging the model size (currently common practices in LM) may not be the optimal solution for enhancing the cultural intelligence of Language Models in non-English languages."
  - [section]: "Since our dataset does not encompass the actual score distribution for the problems, a simple score conversion, the ratio of correctly answered questions, is applied to facilitate a comparative analysis."
  - [corpus]: Strong corpus evidence - neighboring papers (KMMLU, KITE) also emphasize human-level comparisons as a key evaluation metric.

## Foundational Learning

- **Concept**: Cross-lingual cultural representation differences
  - Why needed here: Understanding why translated datasets fail to capture cultural nuances is fundamental to appreciating the value of CLIcK's approach.
  - Quick check question: Why might a question about Korean traditional food work differently in English vs Korean contexts?

- **Concept**: Benchmark construction methodology
  - Why needed here: The three-stage process (collection, validation, categorization) is critical for understanding how the dataset ensures quality and relevance.
  - Quick check question: What are the three validation criteria used to filter GPT-4 generated questions?

- **Concept**: Evaluation metrics for cultural intelligence
  - Why needed here: Standard accuracy metrics may not fully capture cultural reasoning capabilities, requiring careful interpretation of results.
  - Quick check question: How does CLIcK define and measure "difficulty" in their dataset?

## Architecture Onboarding

- **Component map**: Dataset → Categories → Models → Evaluation → Human comparison pipeline
- **Critical path**: Question generation/validation → Category annotation → Model prompting → Accuracy calculation → Difficulty analysis
- **Design tradeoffs**: Original Korean sources vs translated datasets (authenticity vs scale), fine-grained categories vs practical usability, human-level comparison vs model-specific metrics
- **Failure signatures**: Low accuracy across all categories suggests fundamental language/culture understanding gaps; category-specific failures indicate knowledge domain gaps; uncertainty score patterns reveal consistency vs randomness in model reasoning
- **First 3 experiments**:
  1. Run the same model with and without context prompts to verify the impact of background information
  2. Test model performance on translated vs original Korean questions to quantify translation effects
  3. Compare uncertainty scores across different model sizes to understand consistency patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can cultural intelligence in large language models be improved beyond simply scaling model size and fine-tuning with additional Korean corpora?
- Basis in paper: Explicit - The authors conclude that "simply scaling up the model or fine-tuning it with additional Korean corpora doesn't guarantee enhanced Korean linguistic and cultural knowledge of models."
- Why unresolved: The paper demonstrates that increasing model size and additional training data do not significantly improve cultural intelligence, but does not provide concrete alternative approaches or solutions.
- What evidence would resolve it: Comparative experiments testing different methods such as curriculum learning, knowledge distillation, or specialized cultural reasoning modules against the baseline scaling approach.

### Open Question 2
- Question: What specific aspects of Korean culture and language are most challenging for large language models to comprehend?
- Basis in paper: Explicit - The authors note that models struggle with over 60% of the data in CLIcK, but don't identify specific patterns or categories of difficulty.
- Why unresolved: While the paper shows overall performance gaps, it doesn't deeply analyze which types of questions or cultural concepts are particularly problematic for models.
- What evidence would resolve it: Detailed error analysis categorizing model failures by type of cultural knowledge (e.g., historical context, social norms, idiomatic expressions) and linguistic features.

### Open Question 3
- Question: How do cultural knowledge gaps in Korean language models compare to those in other non-English languages?
- Basis in paper: Inferred - The paper focuses on Korean specifically, but the broader implications for multilingual cultural intelligence are not explored.
- Why unresolved: The study provides insights into Korean models but doesn't benchmark against or compare to models for other languages to identify if this is a general problem or specific to Korean.
- What evidence would resolve it: Cross-linguistic studies comparing model performance on culturally-specific datasets across multiple languages using similar evaluation methodologies.

## Limitations
- Dataset construction relies heavily on GPT-4 for question generation, potentially introducing bias in question difficulty and cultural representation
- The conclusion about scaling limitations is based on current model comparisons and cannot definitively rule out future architectural improvements
- The validation process, while systematic, may not fully eliminate subtle biases in question difficulty or cultural representation

## Confidence
- **High Confidence**: The methodology for dataset construction and the observation that proprietary models outperform open-source models on Korean cultural tasks are well-supported by the evidence.
- **Medium Confidence**: The conclusion about limitations of scaling approaches is reasonable but based on current state-of-the-art rather than definitive proof.
- **Medium Confidence**: The claim about authenticity benefits from using original Korean sources is supported but could benefit from more direct comparison with translated alternatives.

## Next Checks
1. **Cross-linguistic validation**: Compare model performance on CLIcK's original Korean questions versus professionally translated versions to quantify the claimed authenticity advantage and measure translation effects on cultural reasoning.

2. **Human expert evaluation**: Have Korean language and culture experts review a stratified sample of questions to assess whether the fine-grained annotations accurately capture the cultural and linguistic knowledge required, and whether the questions appropriately reflect real-world cultural intelligence.

3. **Extended model evaluation**: Test additional model architectures beyond the current 13, including models specifically fine-tuned on Korean cultural data and those using retrieval-augmented generation, to better understand whether scaling limitations are fundamental or implementation-specific.