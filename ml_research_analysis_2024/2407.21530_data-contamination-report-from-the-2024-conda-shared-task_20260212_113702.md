---
ver: rpa2
title: Data Contamination Report from the 2024 CONDA Shared Task
arxiv_id: '2407.21530'
source_url: https://arxiv.org/abs/2407.21530
tags:
- contamination
- data
- language
- wang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The 2024 CONDA Shared Task established a structured, centralized
  public database to collect evidence of data contamination in NLP models, where evaluation
  data is inadvertently included in pre-training corpora. Contributors submitted 566
  contamination reports over 91 contaminated sources from 23 contributors, including
  details on contamination percentage and methodology.
---

# Data Contamination Report from the 2024 CONDA Shared Task

## Quick Facts
- arXiv ID: 2407.21530
- Source URL: https://arxiv.org/abs/2407.21530
- Reference count: 29
- Primary result: 566 contamination reports over 91 contaminated sources from 23 contributors, with most contamination found in CommonCrawl-based corpora and popular models

## Executive Summary
The 2024 CONDA Shared Task established a centralized public database to collect evidence of data contamination in NLP models, where evaluation data is inadvertently included in pre-training corpora. The database aggregates 566 contamination reports from 23 contributors, identifying that CommonCrawl-based corpora like C4 and RedPajama-v2, along with popular models like GPT-3, GPT-4, and FLAN, are most frequently contaminated. The platform distinguishes between data-based methods (string/substring matching) and model-based methods (membership inference attacks), providing a structured approach to tracking ongoing contamination issues.

## Method Summary
The CONDA Shared Task collected contamination evidence through a community-driven approach using GitHub pull requests to a centralized database hosted on Hugging Face Spaces. Contributors submitted reports detailing contaminated resources, evaluation datasets found within them, contamination percentages by dataset split (train, development, test), and methodology references. The database accepts both data-based approaches (string/substring matching techniques) and model-based approaches (membership inference attacks), allowing systematic tracking of contamination across different detection methodologies.

## Key Results
- 566 contamination reports over 91 contaminated sources from 23 contributors
- Most contamination found in CommonCrawl-based corpora (C4, RedPajama-v2) and popular models (GPT-3, GPT-4, FLAN)
- More recent datasets more likely to be found in recently released models
- Highly downloaded datasets such as GLUE and MMLU among the most contaminated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The centralized database structure reduces redundant contamination detection efforts by enabling community coordination.
- Mechanism: By providing a shared platform where contributors submit contamination evidence via GitHub pull requests, the database consolidates findings from multiple researchers. This prevents duplication of effort and ensures that new contamination reports are cross-referenced against existing entries.
- Core assumption: Researchers will contribute their findings to the centralized database rather than working in isolation.
- Evidence anchors:
  - [abstract]: "The shared task provides a structured, centralized public database for the collection of contamination evidence, open to contributions from the community via GitHub pool requests."
  - [section]: "Contributors were asked to fill in the information about several aspects, such as the contaminated resource (a training corpus or model), the evaluation dataset which was found in the contaminated source..."
  - [corpus]: Weak evidence - the corpus shows related papers but does not directly confirm community contribution behavior.
- Break condition: If researchers prefer to publish independently or if the database becomes too cumbersome to contribute to, the coordination benefit diminishes.

### Mechanism 2
- Claim: Data-based contamination detection methods are more scalable than model-based methods for identifying contamination in large corpora.
- Mechanism: Data-based approaches use string or substring matching techniques (e.g., 13-gram overlap, 50-character overlap) to inspect pre-training corpora directly. This allows systematic scanning of large text collections without requiring access to model parameters or APIs.
- Core assumption: Pre-training corpora are accessible and searchable, enabling automated detection of overlapping content.
- Evidence anchors:
  - [abstract]: "The platform distinguishes between data-based methods (string/substring matching) and model-based methods (membership inference attacks)."
  - [section]: "Data-based approaches typically involve string or sub-string matching techniques such as 13-gram overlap (Brown et al., 2020; Wei et al., 2022), 50-character overlap (OpenAI et al., 2024) or even full-string overlap (Elazar et al., 2024)."
  - [corpus]: Weak evidence - corpus analysis shows related work but doesn't quantify scalability differences.
- Break condition: If pre-training corpora are not publicly available or if matching algorithms cannot handle the scale of modern web data, scalability advantage is lost.

### Mechanism 3
- Claim: Model-based contamination detection methods can identify contamination even when pre-training data is not accessible.
- Mechanism: Model-based approaches formulate contamination detection as Membership Inference Attacks (MIA), where the model's outputs or probabilities are analyzed to infer whether specific evaluation data was part of its training set. This works for both closed and open models.
- Core assumption: Models leak information about their training data through their outputs or API behavior.
- Evidence anchors:
  - [abstract]: "The platform distinguishes between data-based methods (string/substring matching) and model-based methods (membership inference attacks)."
  - [section]: "Model-based approaches... try to estimate the contamination of a model by prompting or analyzing the output, without accessing the pre-training data. These methods are formulated as Membership Inference Attacks (MIA)."
  - [corpus]: Weak evidence - corpus shows related papers but doesn't demonstrate practical effectiveness.
- Break condition: If models are designed to be robust against such inference attacks or if API access is restricted, detection capability is compromised.

## Foundational Learning

- Concept: Data contamination in NLP
  - Why needed here: Understanding what constitutes data contamination is essential for interpreting the database entries and contamination detection methods.
  - Quick check question: What is the difference between data contamination and data leakage in the context of machine learning evaluation?

- Concept: String/substring matching techniques
  - Why needed here: These are the primary data-based methods for detecting contamination, and understanding their mechanics is crucial for evaluating their effectiveness.
  - Quick check question: How does 13-gram overlap differ from 50-character overlap in terms of sensitivity to contamination?

- Concept: Membership Inference Attacks (MIA)
  - Why needed here: Model-based contamination detection relies on these techniques, and understanding their principles is necessary for evaluating their applicability.
  - Quick check question: What information can a model leak that would indicate whether specific data was in its training set?

## Architecture Onboarding

- Component map:
  - Database backend (Hugging Face Spaces)
  - GitHub integration for pull requests
  - Contribution form schema (resource details, contamination percentage, methodology)
  - Evidence validation workflow (discussion before admission)
  - Public API for querying contamination reports

- Critical path:
  1. Contributor discovers potential contamination
  2. Contributor fills out contribution form with evidence
  3. Pull request submitted to GitHub repository
  4. Community discussion and validation of evidence
  5. Database entry created and made publicly available

- Design tradeoffs:
  - Open contribution vs. quality control: Open system enables broad participation but requires validation mechanisms
  - Data-based vs. model-based methods: Data-based is more scalable but requires corpus access; model-based works without corpus but may be less comprehensive
  - Granularity of reports: Detailed per-split contamination percentages provide more information but increase complexity

- Failure signatures:
  - Inconsistent contamination reports for the same dataset/model pairs
  - Contributors unable to provide sufficient evidence for their claims
  - Database becoming outdated as new models/datasets are released
  - Community engagement declining over time

- First 3 experiments:
  1. Submit a test contamination report for a known uncontaminated dataset to verify the submission workflow
  2. Query the database for contamination reports on a popular benchmark and analyze the distribution of contamination percentages
  3. Compare contamination reports from data-based vs. model-based methods for the same dataset/model pair to understand methodological differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for determining when a dataset should be considered "contaminated" based on the percentage of overlap with pre-training data?
- Basis in paper: [inferred] The paper reports contamination percentages but doesn't establish clear thresholds for what constitutes problematic contamination.
- Why unresolved: Different tasks and evaluation methodologies may require different tolerance levels for contamination.
- What evidence would resolve it: Empirical studies showing how varying levels of contamination affect model evaluation accuracy and reliability across different benchmark types.

### Open Question 2
- Question: How can the community develop standardized methodologies for detecting data contamination that work across different types of models (closed vs. open) and datasets?
- Basis in paper: [explicit] The paper distinguishes between data-based and model-based approaches but acknowledges these are "hardly comparable."
- Why unresolved: Current detection methods vary widely in their effectiveness and applicability depending on model accessibility and data characteristics.
- What evidence would resolve it: Comparative studies evaluating the accuracy, scalability, and limitations of different contamination detection approaches across diverse NLP tasks and model architectures.

### Open Question 3
- Question: What are the long-term effects of data contamination on the development of NLP benchmarks and the validity of model comparisons over time?
- Basis in paper: [explicit] The paper notes that contamination "introduces biases and can artificially inflate the performance of LMs" but doesn't explore long-term implications.
- Why unresolved: The full impact of contamination on research progress and evaluation practices remains unclear as models continue to grow in scale.
- What evidence would resolve it: Longitudinal studies tracking how contamination affects benchmark evolution, research directions, and the reliability of model comparisons across multiple generations of language models.

## Limitations
- The report lacks detailed methodological specifications for individual contamination reports, including detection thresholds and confidence intervals
- The database relies on community contributions, potentially introducing selection bias toward more easily detectable contamination cases
- The report does not address how contamination evidence is validated beyond community discussion

## Confidence
- **High confidence**: The existence and structure of the centralized database, the total number of contamination reports (566), and the identification of CommonCrawl-based corpora as primary contamination sources
- **Medium confidence**: The distinction between data-based and model-based detection methods, and the observation that more recent datasets are more likely to be found in recently released models
- **Low confidence**: The effectiveness of the community validation process, the completeness of contamination coverage, and the practical impact of the database on avoiding contaminated evaluation

## Next Checks
1. **Validation workflow audit**: Trace the GitHub pull request history to identify how many contamination reports were rejected during the community discussion phase and for what reasons
2. **Methodological comparison study**: Select 5 dataset/model pairs with both data-based and model-based contamination reports and systematically compare the detection methodologies, confidence levels, and reported contamination percentages
3. **Impact assessment**: Survey NLP researchers who have used the database to determine whether it influenced their evaluation dataset choices and to identify any gaps or usability issues in the current database structure