---
ver: rpa2
title: Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques
arxiv_id: '2403.05801'
source_url: https://arxiv.org/abs/2403.05801
tags:
- knowledge
- reward
- entity
- learning
- shaping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving multi-hop reasoning
  in knowledge graphs by enhancing reward shaping for reinforcement learning agents.
  The authors propose pre-training a reward-shaping module on either a rich knowledge
  graph or using advanced contextual embeddings (BERT or prompt-based learning), and
  then applying it to guide sparse KG reasoning.
---

# Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping Techniques

## Quick Facts
- arXiv ID: 2403.05801
- Source URL: https://arxiv.org/abs/2403.05801
- Reference count: 15
- One-line primary result: Prompt-based reward shaping on rich KG achieves Hits@1=0.860, outperforming ConvE-based methods on UMLS

## Executive Summary
This work addresses the challenge of improving multi-hop reasoning in knowledge graphs by enhancing reward shaping for reinforcement learning agents. The authors propose pre-training a reward-shaping module on either a rich knowledge graph or using advanced contextual embeddings (BERT or prompt-based learning), and then applying it to guide sparse KG reasoning. Experiments on the UMLS dataset show that prompt-based reward shaping on the rich KG achieves the best performance, outperforming ConvE-based methods and confirming the value of contextual embeddings and prompt learning in reward shaping for KG reasoning.

## Method Summary
The method involves partitioning the UMLS dataset into rich and sparse subsets, pre-training a reward-shaping module using BERT contextual embeddings or T5-based prompt learning on the rich KG, and then applying this pre-trained module to guide an RL agent navigating the sparse KG. The reward shaper provides intermediate rewards based on the likelihood of correct answers given partial paths, addressing the sparsity and incompleteness issues in real-world KGs.

## Key Results
- Prompt-based reward shaping on rich KG achieves Hits@1=0.860, Hits@3=0.937, Hits@5=0.997, Hits@10=0.992, MRR=0.916
- BERT-based reward shaping performs better than no shaping but worse than prompt-based approach
- Pre-training on rich KG significantly improves performance compared to training only on sparse KG

## Why This Works (Mechanism)

### Mechanism 1
Reward shaping improves multi-hop KG reasoning by providing intermediate rewards that guide RL agents toward correct paths even in incomplete KGs. The reward-shaping module estimates the likelihood of a correct answer given a partial path, enabling the RL agent to learn from informative feedback rather than only terminal rewards.

### Mechanism 2
Prompt-based learning for reward shaping leverages natural language context to better capture entity relationships than traditional KG embeddings. Using T5 with templates allows the model to predict tail entity probabilities based on natural language semantics, providing richer contextual information than vector similarity alone.

### Mechanism 3
Transfer learning from rich to sparse KGs improves generalization by exposing the reward-shaper to more diverse entity relationships. Training on a rich KG simulates exposure to a larger knowledge base, allowing the reward-shaper to learn broader patterns that transfer to the sparse KG setting.

## Foundational Learning

- **Concept**: Reinforcement Learning fundamentals (MDP, policy gradients, REINFORCE algorithm)
  - Why needed here: The paper uses RL agents navigating KGs via the REINFORCE algorithm, requiring understanding of state/action spaces and policy optimization
  - Quick check question: What are the components of an MDP in the KG reasoning context?

- **Concept**: Knowledge Graph embeddings (ConvE, ComplEx, DistMult)
  - Why needed here: The baseline methods use these embedding techniques, and understanding their limitations motivates the reward shaping approach
  - Quick check question: How do ConvE and ComplEx differ in representing entity relationships?

- **Concept**: Prompt learning and template-based fine-tuning
  - Why needed here: The proposed method uses T5 with templates to predict tail entities, requiring understanding of prompt engineering techniques
  - Quick check question: What is the template structure used for the T5 prompt-based reward shaping?

## Architecture Onboarding

- **Component map**: Knowledge Graph -> RL Agent (REINFORCE) -> Reward Shaper (BERT/T5) -> Sparse KG Evaluation
- **Critical path**: 1) Split UMLS into rich and sparse KGs, 2) Pre-train reward shaper on rich KG, 3) Fine-tune RL agent on sparse KG using shaped rewards, 4) Evaluate using Hits@k and MRR
- **Design tradeoffs**: BERT contextualization vs prompt learning for reward shaping, rich KG completeness vs computational cost, binary rewards vs shaped rewards
- **Failure signatures**: RL agent fails to improve despite reward shaping, performance degrades on sparse KG, BERT-based reward shaping underperforms ConvE
- **First 3 experiments**: 1) Implement baseline REINFORCE agent on sparse KG without reward shaping, 2) Pre-train reward shaper on rich KG using BERT contextualization, 3) Pre-train reward shaper on rich KG using T5 prompt learning with relation templates

## Open Questions the Paper Calls Out

### Open Question 1
How does the size and diversity of the rich KG impact the performance of the reward shaping module when applied to sparse KGs?
- Basis in paper: The paper mentions using a rich KG to pre-train the reward-shaping module but does not explore the impact of the rich KG's size and diversity on performance
- Why unresolved: The paper does not provide experiments or analysis on varying the size and diversity of the rich KG to determine its effect on the reward-shaping module's performance
- What evidence would resolve it: Experiments comparing the performance of the reward-shaping module when pre-trained on rich KGs of different sizes and diversities, applied to the same sparse KG

### Open Question 2
What is the optimal masking strategy for creating sparse KGs from rich KGs to best simulate real-world scenarios?
- Basis in paper: The paper uses a simple 50% masking strategy to create sparse KGs but does not explore or justify this approach as optimal for simulating real-world scenarios
- Why unresolved: The paper does not investigate different masking strategies or their effects on the performance of the reward-shaping module and the RL agent
- What evidence would resolve it: Comparative analysis of the reward-shaping module and RL agent's performance using sparse KGs created with various masking strategies (e.g., random, targeted, degree-based)

### Open Question 3
How do different types of KG incompleteness (e.g., missing nodes vs. missing edges) affect the efficacy of reward shaping techniques?
- Basis in paper: The paper discusses the challenge of KG incompleteness but does not differentiate between types of incompleteness or their specific impacts on reward shaping
- Why unresolved: There is no analysis or experimental data on how different types of KG incompleteness (missing nodes, missing edges, or both) influence the performance of reward shaping techniques
- What evidence would resolve it: Experiments and analysis showing the performance of reward shaping techniques on KGs with different types of incompleteness, such as missing nodes only, missing edges only, or a combination of both

## Limitations

- UMLS dataset partitioning method is described but not fully specified in terms of split ratio or methodology
- Limited ablation studies on how each component (BERT vs prompt learning, reward shaping vs no shaping) contributes to overall performance gains
- Transfer learning assumption from rich to sparse KGs is only empirically validated on UMLS, raising questions about generalizability to other domains

## Confidence

- **High confidence**: The overall framework of using pre-trained reward shaping modules for KG reasoning is sound and well-supported by RL literature
- **Medium confidence**: The specific claim that prompt-based learning outperforms BERT contextualization for reward shaping, based on UMLS results
- **Low confidence**: The generalizability of transfer learning benefits across different KG domains and sparsity levels

## Next Checks

1. Implement ablation study comparing: (a) baseline REINFORCE without reward shaping, (b) BERT-based reward shaping, (c) prompt-based reward shaping, and (d) oracle reward shaping to quantify the marginal benefit of each component
2. Test the reward shaping approach on a different KG domain (e.g., Wikidata or Freebase) to validate cross-domain generalizability of the transfer learning mechanism
3. Analyze the sensitivity of performance to the rich/sparse KG split ratio to determine optimal pre-training conditions and identify overfitting thresholds