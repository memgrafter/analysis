---
ver: rpa2
title: 'Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented Knowledge
  Graphs and Vector Database for Accreditation Reporting Assistance'
arxiv_id: '2405.15436'
source_url: https://arxiv.org/abs/2405.15436
tags:
- knowledge
- https
- query
- context
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a hybrid-context retrieval augmented generation
  (RAG) pipeline integrating a vector database and knowledge graph for accreditation
  reporting assistance. The pipeline employs query optimization, multi-source retrieval,
  and LLM-augmented knowledge graph construction to ground responses using both institutional
  data and AACSB standards.
---

# Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented Knowledge Graphs and Vector Database for Accreditation Reporting Assistance

## Quick Facts
- arXiv ID: 2405.15436
- Source URL: https://arxiv.org/abs/2405.15436
- Reference count: 0
- This work develops a hybrid-context retrieval augmented generation (RAG) pipeline integrating a vector database and knowledge graph for accreditation reporting assistance, achieving optimal performance on answer relevancy (mean=1.0) and near-optimal context recall (mean=0.900) for AACSB-related queries.

## Executive Summary
This paper presents a hybrid-context retrieval augmented generation (RAG) pipeline that integrates a vector database with a knowledge graph to assist with accreditation reporting for business schools. The pipeline uses query optimization, multi-source retrieval, and LLM-augmented knowledge graph construction to ground responses using both institutional data and AACSB standards. Using the RAGAs evaluation framework, the system achieved optimal performance on answer relevancy (mean=1.0) and near-optimal context recall (mean=0.900) for AACSB-related queries, outperforming hybrid and institution-only query categories. The approach addresses the challenge of combining structured semantic relationships from knowledge graphs with semantic similarity from vector indexes to provide comprehensive context for generating accurate responses.

## Method Summary
The pipeline employs a hybrid retrieval approach combining vector database search with knowledge graph traversal. Query optimization is implemented through multi-query and sub-query generation using LLM function calling. The knowledge graph is constructed using LLM-augmented extraction from unstructured documents, with nodes representing entities and relationships extracted through prompt engineering. The system integrates Neo4j for graph storage, OpenAI's embedding and chat models for semantic processing, and LangChain for graph querying. Retrieval context from both sources is aggregated and passed to an LLM generator to produce final responses. The pipeline was evaluated using the RAGAs framework on a validation set of 10 manually created queries with ground truth answers.

## Key Results
- Achieved optimal answer relevancy scores (mean=1.0) for AACSB-related queries
- Near-optimal context recall (mean=0.900) for AACSB-related queries
- Outperformed hybrid and institution-only query categories in RAGAs evaluation
- Successfully demonstrated the value of combining vector and graph retrieval for structured domain contexts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid context retrieval (vector + knowledge graph) improves answer relevancy for AACSB queries compared to single-source retrieval.
- **Mechanism:** Combining structured semantic relationships from the knowledge graph with semantic similarity from the vector index provides complementary grounding context, reducing reliance on vector-only embeddings that may miss relational nuances.
- **Core assumption:** AACSB standards data is sufficiently structured to benefit from explicit graph relationships, while institutional documents benefit from semantic embedding search.
- **Evidence anchors:**
  - [abstract] "achieved optimal performance on answer relevancy (mean=1.0) and near-optimal context recall (mean=0.900) for AACSB-related queries, outperforming hybrid and institution-only query categories."
  - [section] "The multi-source retrieval approach integrating both vector index data and knowledge graph data provides an additional layer of grounding and context for the generating task."
- **Break condition:** If knowledge graph construction introduces inconsistent node labeling, Cypher queries become ineffective, eliminating the hybrid advantage.

### Mechanism 2
- **Claim:** Query expansion into multiple related queries improves retrieval recall compared to single-query retrieval.
- **Mechanism:** Expanding a single natural language query into multiple semantically related queries increases the probability that at least one query matches relevant document embeddings, mitigating semantic drift in complex queries.
- **Core assumption:** The LLM used for query expansion can generate semantically valid and distinct queries that cover the information need.
- **Evidence anchors:**
  - [section] "We implement query expansion and query transformation optimization techniques through use of OpenAI's function calling for 'multi-query' and 'subquery' generation tasks."
  - [section] "The original and generated queries are used to retrieve data from both the knowledge graph and the vector store."
- **Break condition:** If query expansion produces overly similar queries or hallucinated terms, the retrieval signal becomes redundant or noisy.

### Mechanism 3
- **Claim:** LLM-augmented knowledge graph construction enables extraction of structured knowledge from unstructured institutional documents.
- **Mechanism:** Using function calling with carefully engineered prompts, the LLM identifies entities, relationships, and types from free-text documents and outputs them in a structured format suitable for Neo4j ingestion, bypassing manual schema definition.
- **Core assumption:** The LLM can consistently extract valid node types and relationships from diverse institutional document formats without extensive fine-tuning.
- **Evidence anchors:**
  - [section] "To create a knowledge graph from this wide variety of input documents we utilize LLMs to generate a knowledge graph from the provided documents."
  - [section] "LLM-Augmented Knowledge Graph Construction... is handled through prompt engineering."
- **Break condition:** If the LLM output schema varies unpredictably between documents, downstream Cypher query construction fails due to missing or mismatched node types.

## Foundational Learning

- **Concept:** Vector embeddings and cosine similarity for semantic search
  - Why needed here: The vector database component relies on embedding queries into a high-dimensional space and retrieving nearest neighbors by cosine similarity.
  - Quick check question: What happens to retrieval results if the embedding model is changed from `text-embedding-ada-002` to a model with different dimensionality?

- **Concept:** Knowledge graph nodes, edges, and Cypher query language
  - Why needed here: The knowledge graph stores AACSB standards and institutional data as nodes connected by typed relationships, and Cypher is used to traverse these relationships for retrieval.
  - Quick check question: How would you modify a Cypher query to return all `Standard` nodes linked to a given `Section` node?

- **Concept:** Retrieval-augmented generation (RAG) architecture
  - Why needed here: The pipeline follows a RAG pattern where retrieval context is combined with the original query to generate grounded responses, requiring understanding of retriever-generator interaction.
  - Quick check question: What is the role of the context string passed to the generator in the final response generation step?

## Architecture Onboarding

- **Component map:** User Query → Query Optimizer (multi-query/sub-query) → Vector Index + Knowledge Graph Retrievers → Context Aggregator → LLM Generator → Response

- **Critical path:** User query → query expansion → both retrievers → context aggregation → LLM generation. Any failure in retrieval steps directly degrades response quality.

- **Design tradeoffs:**
  - Hybrid retrieval adds complexity but improves grounding; single-source retrieval is simpler but less robust.
  - LLM-based graph construction is flexible but non-deterministic; manual construction is stable but labor-intensive.
  - Using GPT-3.5 for most tasks balances cost and performance; GPT-4 reserved for Cypher generation due to higher accuracy needs.

- **Failure signatures:**
  - Low context recall or high faithfulness errors indicate retrieval context mismatch.
  - Zero answer relevancy scores suggest the generator ignored or misinterpreted context.
  - Inconsistent node labeling manifests as failed Cypher queries during graph traversal.

- **First 3 experiments:**
  1. Test single-query retrieval accuracy on a small AACSB document set to establish baseline.
  2. Validate Cypher query generation by running generated queries against a manually constructed test graph.
  3. Compare hybrid retrieval performance (vector + graph) against vector-only retrieval on a mixed AACSB/institution query set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the hybrid context retrieval augmented generation pipeline performance vary when handling institution-specific queries versus AACSB standards queries?
- Basis in paper: [explicit] The paper explicitly states that pipeline performance on AACSB-related queries outperformed institution-related queries, achieving optimal scores in 'answer relevancy' and near-optimal score in 'context recall'.
- Why unresolved: While the paper reports this difference, it does not provide a detailed analysis of why this performance gap exists or what specific factors contribute to it.
- What evidence would resolve it: A comprehensive analysis comparing the performance metrics of institution-specific and AACSB standards queries, including a breakdown of the factors contributing to the performance difference.

### Open Question 2
- Question: How does the non-deterministic nature of large language model (LLM) knowledge graph construction impact the consistency and reliability of the generated knowledge graphs?
- Basis in paper: [explicit] The paper mentions that the non-deterministic information extraction used to construct the knowledge graph by the LLM is a primary challenge, making generating useful Cypher queries from natural language difficult.
- Why unresolved: The paper acknowledges this challenge but does not provide a detailed exploration of its implications or potential solutions.
- What evidence would resolve it: A study comparing the consistency and reliability of knowledge graphs generated by the LLM across multiple iterations, and an exploration of potential solutions to mitigate the impact of non-determinism.

### Open Question 3
- Question: How does the classification of institution documents into single AACSB standard categories impact the accuracy and relevance of the retrieved information?
- Basis in paper: [explicit] The paper mentions that institution documents are classified as relating to a single AACSB standard, but notes that in reality, it is plausible for a document to be related to multiple AACSB standards.
- Why unresolved: The paper does not explore the potential limitations of this single-label classification approach or its impact on the accuracy and relevance of the retrieved information.
- What evidence would resolve it: A comparison of the performance of the pipeline when using single-label classification versus multi-label classification, and an analysis of the impact of each approach on the accuracy and relevance of the retrieved information.

## Limitations

- Non-deterministic LLM knowledge graph construction leads to inconsistent node labeling, causing Cypher query failures and reduced performance on institution-related queries
- Single-label classification system tends to default to standard "0" (general institutional information), suggesting potential degradation in GPT-4's performance on composite instruction prompts
- Limited evaluation scope with only 10 manually created queries, preventing comprehensive assessment of pipeline robustness across diverse query types

## Confidence

- **High confidence:** Hybrid retrieval improves answer relevancy for AACSB queries compared to single-source retrieval (supported by quantitative RAGAs results showing optimal scores)
- **Medium confidence:** Query expansion improves retrieval recall through semantic coverage (mechanism described but not independently validated with ablation studies)
- **Low confidence:** LLM-augmented knowledge graph construction consistently extracts valid structured knowledge across diverse institutional documents (limited by non-deterministic outputs and inconsistent node labeling issues)

## Next Checks

1. Conduct an ablation study comparing hybrid retrieval performance against vector-only and graph-only retrieval on a larger, diverse query set to quantify the complementary value of each source
2. Implement deterministic knowledge graph construction by either fine-tuning a model on institutional document schemas or developing a rule-based extraction system, then measure the impact on Cypher query success rates and overall retrieval quality
3. Perform error analysis on institution-only queries to identify whether classification errors, graph construction failures, or retrieval mismatches are the primary drivers of reduced performance, and test targeted mitigation strategies for each failure mode