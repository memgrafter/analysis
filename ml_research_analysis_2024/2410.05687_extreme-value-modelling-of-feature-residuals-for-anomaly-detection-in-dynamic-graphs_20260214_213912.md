---
ver: rpa2
title: Extreme Value Modelling of Feature Residuals for Anomaly Detection in Dynamic
  Graphs
arxiv_id: '2410.05687'
source_url: https://arxiv.org/abs/2410.05687
tags:
- graph
- graphs
- time
- each
- vertices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of detecting anomalous graphs
  in temporal sequences, which is important for applications such as identifying accidents
  in transport networks and cyber attacks in computer networks. Existing methods often
  suffer from high false positive rates and difficulties handling variable-sized graphs
  and non-trivial temporal dynamics.
---

# Extreme Value Modelling of Feature Residuals for Anomaly Detection in Dynamic Graphs

## Quick Facts
- arXiv ID: 2410.05687
- Source URL: https://arxiv.org/abs/2410.05687
- Reference count: 27
- Primary result: Achieves higher AUC values than TensorSplat and Laplacian Anomaly Detection across multiple graph types

## Executive Summary
This paper addresses the challenge of detecting anomalous graphs in temporal sequences, which is important for applications such as identifying accidents in transport networks and cyber attacks in computer networks. The proposed approach models temporal dependencies using time series analysis of a rich set of graph features, removes these dependencies via residuals, and then applies Extreme Value Theory to robustly identify remaining extremes, aiming for low false positive rates. Comparative evaluations across multiple graph types show the proposed method achieves considerably better accuracy than TensorSplat and Laplacian Anomaly Detection, with higher AUC values across all tested scenarios.

## Method Summary
The method extracts 20 graph features from each graph in a temporal sequence, including vertex count, edge count, degree distribution, clustering coefficients, centrality measures, and community structure. For each feature, temporal dependencies are modeled using ARIMA time series analysis, and residuals are computed by subtracting predicted values from observed values. These residuals are then scaled using robust statistics and reduced to 2D using robust PCA. Finally, kernel density estimation is applied to the reduced data, and Extreme Value Theory (specifically the Generalised Pareto Distribution) is used to model extreme values and classify anomalies.

## Key Results
- Outperforms TensorSplat and Laplacian Anomaly Detection with higher AUC values across all tested scenarios
- Effectively handles variable-sized graphs through feature extraction normalization
- Achieves low false positive rates by explicitly modeling extremes using EVT rather than declaring all extremes as anomalies
- Demonstrates robustness across multiple graph types including random graphs, scale-free networks, and community-structured graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using residuals from ARIMA removes temporal dependencies, enabling better anomaly detection.
- Mechanism: The approach first models the temporal dynamics of graph features using ARIMA. The residuals (observed minus predicted values) are then computed. Since residuals should have no temporal dependencies, they capture the "pure" anomalous behavior without confounding by expected temporal patterns.
- Core assumption: ARIMA can adequately model the temporal dynamics of graph features.
- Evidence anchors:
  - [abstract]: "temporal dependencies are explicitly modelled via time series analysis of a large set of pertinent graph features, followed by using residuals to remove the dependencies"
  - [section II.B]: "Given the scalar time series of the i-th graph feature, denoted as x.,i = {xt,i}T t=1, the corresponding residuals are obtained as et,i = xt,i –ˆxt,i,where ˆxt,i isthepredictedvalueof xt,i (obtained from the corresponding ARIMA model)"
- Break condition: If the graph features exhibit non-linear temporal patterns that ARIMA cannot capture, the residuals will still contain temporal dependencies, reducing effectiveness.

### Mechanism 2
- Claim: Extreme Value Theory (EVT) provides robust modeling of low-density regions, reducing false positives.
- Mechanism: After dimensionality reduction, the approach uses kernel density estimation and applies EVT (specifically Generalised Pareto Distribution) to model extreme values. This allows for principled identification of true anomalies rather than flagging all extremes.
- Core assumption: The extreme values of interest follow a GPD distribution.
- Evidence anchors:
  - [abstract]: "Extreme Value Theory (EVT) is then used to robustly model and classify any remaining extremes, aiming to produce low false positives rates"
  - [section II.D]: "Incidence of high false positives is a paramount challenge in anomaly detection... In contrast to simply declaring all extremes as anomalous... EVT [5], [23] is used to explicitly model the extremes, thereby allowing for robust detection of unusual extremes"
- Break condition: If the extreme values don't follow the assumed GPD distribution, EVT-based modeling will be inaccurate.

### Mechanism 3
- Claim: Using a diverse set of graph features captures multiple aspects of graph structure for better anomaly detection.
- Mechanism: The approach extracts 20 different features from each graph, covering aspects like vertex count, edge count, clustering, centrality measures, and community structure. This rich representation captures anomalies across multiple graph properties.
- Core assumption: Anomalies manifest across multiple graph features rather than just one or two.
- Evidence anchors:
  - [section II.A]: "we propose to use a rich and diverse set of pertinent features... For each graphGt, a set of 20 features (scalars) is extracted"
  - [section II.A]: "As in many applications the full details on the graph source are generally unavailable, it is not known a-priori which features would be most appropriate"
- Break condition: If anomalies only affect specific graph features not included in the 20, the method will miss them.

## Foundational Learning

- Concept: Time series modeling with ARIMA
  - Why needed here: To model and remove temporal dependencies in graph feature evolution
  - Quick check question: What are the three parameters (p, d, q) in an ARIMA model and what does each control?

- Concept: Extreme Value Theory and Generalised Pareto Distribution
  - Why needed here: To robustly model and classify extreme values as anomalies while minimizing false positives
  - Quick check question: What are the three types of extreme value distributions (based on shape parameter ξ) that GPD can represent?

- Concept: Graph feature extraction
  - Why needed here: To create fixed-dimensional representations of variable-sized graphs for comparison
  - Quick check question: Why might a single graph feature be insufficient for detecting anomalies across different types of graphs?

## Architecture Onboarding

- Component map: Feature Extraction -> ARIMA Modeling -> Residual Computation -> Scaling -> Robust PCA -> KDE + EVT -> Anomaly Scoring

- Critical path:
  1. Extract 20 graph features from each graph in the sequence
  2. Model temporal dependencies using ARIMA for each feature
  3. Compute residuals to remove temporal effects
  4. Scale residuals and apply robust PCA to reduce to 2D
  5. Apply kernel density estimation and Extreme Value Theory to identify anomalies

- Design tradeoffs:
  - Using 20 features vs. feature selection: More features capture diverse anomalies but increase computational cost and may include irrelevant information
  - ARIMA vs. more complex time series models: ARIMA is interpretable and widely understood but may miss complex temporal patterns
  - 2D reduction vs. higher dimensions: 2D is computationally efficient and easier to visualize but may lose some anomaly information

- Failure signatures:
  - High false positive rate: EVT modeling may be incorrectly parameterized or the data doesn't follow GPD assumptions
  - Missing anomalies: The 20 features may not capture the specific anomaly type, or ARIMA may not adequately model temporal dependencies
  - Poor performance on variable-sized graphs: Feature extraction may not properly normalize for graph size differences

- First 3 experiments:
  1. Run on synthetic data where temporal dependencies are known and can be precisely removed, to verify residual computation works correctly
  2. Test on graphs with anomalies that affect only specific features to determine which features are most informative
  3. Evaluate performance with different numbers of features (e.g., 10 vs 20 vs 30) to find optimal balance between coverage and efficiency

## Open Questions the Paper Calls Out
The paper mentions several directions for future work including:
- Extending the method to find abnormal subgraphs in large graphs
- Exploring the use of other time series models beyond ARIMA
- Investigating feature selection to determine the most useful graph features

## Limitations
- The effectiveness of the 20 selected graph features may not comprehensively capture all potential anomaly types across different graph domains
- The method's performance on very large-scale dynamic graphs with millions of vertices and edges has not been evaluated
- The approach is designed for structural graph properties only and does not incorporate vertex attributes that may be available in attributed graphs

## Confidence
- High confidence: The theoretical framework combining residual analysis with EVT is sound and well-established
- Medium confidence: The empirical results showing superior AUC performance compared to TensorSplat and Laplacian Anomaly Detection
- Medium confidence: The claim of low false positive rates, though this depends heavily on proper EVT parameterization

## Next Checks
1. Test the method on synthetic graph sequences where temporal dependencies and anomalies are precisely controlled to verify the residual computation and EVT modeling work as intended
2. Evaluate performance when using subsets of the 20 features to identify which features are most critical for anomaly detection
3. Compare results using different time series models (e.g., LSTM, Prophet) instead of ARIMA to assess sensitivity to the temporal modeling approach