---
ver: rpa2
title: Global Confidence Degree Based Graph Neural Network for Financial Fraud Detection
arxiv_id: '2407.17333'
source_url: https://arxiv.org/abs/2407.17333
tags:
- node
- fraud
- detection
- graph
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses financial fraud detection by introducing a
  novel graph neural network approach based on Global Confidence Degree (GCD). The
  key idea is to evaluate the typicality of each node globally by comparing it with
  category-specific prototypes, rather than focusing solely on local neighbor information.
---

# Global Confidence Degree Based Graph Neural Network for Financial Fraud Detection

## Quick Facts
- arXiv ID: 2407.17333
- Source URL: https://arxiv.org/abs/2407.17333
- Reference count: 40
- Key outcome: GCD-GNN achieves 97.26% AUC and 92.37% F1-Macro on T-Finance, outperforming state-of-the-art baselines

## Executive Summary
This paper addresses financial fraud detection by introducing a novel graph neural network approach based on Global Confidence Degree (GCD). The key idea is to evaluate the typicality of each node globally by comparing it with category-specific prototypes, rather than focusing solely on local neighbor information. The proposed GCD-GNN model uses GCD to generate attention weights for message aggregation, capturing both typical and atypical information through separate perspectives. Extensive experiments on two public datasets demonstrate that GCD-GNN outperforms state-of-the-art baselines, achieving 97.26% AUC and 92.37% F1-Macro on T-Finance, and 71.72% AUC and 59.68% F1-Macro on FDCompCN. The lightweight version (GCD-GNN_light) also shows solid performance with faster convergence and inference speed.

## Method Summary
The GCD-GNN model introduces Global Confidence Degree (GCD) to evaluate node typicality by comparing each node to category-specific prototypes. The method transforms original features using MLP and Graph Normalization, calculates GCD between nodes and prototypes, and uses GCD to generate attention weights for message aggregation. Messages are aggregated from both typical (high GCD) and atypical (low GCD) perspectives separately. The model includes a lightweight version for faster inference and demonstrates significant performance improvements over existing baselines on financial fraud detection tasks.

## Key Results
- GCD-GNN achieves 97.26% AUC and 92.37% F1-Macro on T-Finance dataset, outperforming state-of-the-art baselines
- GCD-GNN_light provides faster convergence and inference while maintaining competitive performance
- Ablation studies confirm effectiveness of each component, particularly GCD-based attention and typical/atypical aggregation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Global Confidence Degree (GCD) improves fraud detection by comparing nodes to category-specific prototypes rather than relying on local neighbor information.
- Mechanism: GCD measures the typicality of each node by calculating similarity between the node and its category prototype. This allows the model to capture global patterns and identify both typical and atypical nodes.
- Core assumption: Node similarity to category prototypes is a reliable indicator of fraud status, and prototypes can be accurately computed from labeled data.
- Evidence anchors:
  - [abstract] "GCD-GNN uses GCD to generate attention weights for message aggregation, capturing both typical and atypical information"
  - [section 3.1] "GCD of a node evaluates the typicality of the node and thus we can leverage GCD to generate attention values for message aggregation"
  - [corpus] Weak - no direct citations, but related work on prototype-based approaches exists

### Mechanism 2
- Claim: Separating message aggregation into typical and atypical perspectives enhances model performance.
- Mechanism: The model aggregates messages using both original GCD and its inverse, allowing it to capture information from both similar (typical) and dissimilar (atypical) neighbors.
- Core assumption: Both typical and atypical neighbor information contains valuable signals for fraud detection, not just similar neighbors.
- Evidence anchors:
  - [abstract] "This process is carried out through both the original GCD and its inverse, allowing us to capture both the typical neighbors with high GCD and the atypical ones with low GCD"
  - [section 3.6] "To utilize both typical and atypical information... We aggregate messages from the typical and atypical perspectives separately"
  - [corpus] Weak - no direct citations, but this is a novel contribution

### Mechanism 3
- Claim: Feature transformation and optimization improves prototype quality and model performance.
- Mechanism: The model transforms original features using MLP and Graph Normalization, then combines transformed and original features to create enhanced representations that better separate fraudulent and benign nodes.
- Core assumption: Feature projection can eliminate unnecessary information while preserving discriminative features for fraud detection.
- Evidence anchors:
  - [section 3.3] "Firstly, we use an MLP and Graph Normalization... to process the initial features, projecting those features into a space that is fitting for measuring similarity"
  - [section 3.4] "We propose a weight mix method to leverage the projected features while preserving the essential characteristics of the original features"
  - [section 4.3] "Our model exhibits significant improvements in all metrics after incorporating feature transformation and GCD attention mechanisms"

## Foundational Learning

- Concept: Graph Neural Networks and message passing
  - Why needed here: GCD-GNN builds on GNN foundations but modifies the aggregation process using GCD-based attention
  - Quick check question: How does standard GNN message passing differ from the GCD-based aggregation in this paper?

- Concept: Prototype-based similarity measures
  - Why needed here: GCD calculation relies on comparing nodes to category prototypes using similarity metrics like cosine similarity
  - Quick check question: What properties should a good prototype have in the context of financial fraud detection?

- Concept: Attention mechanisms in neural networks
  - Why needed here: GCD is used to generate attention weights for message aggregation, similar to graph attention networks
  - Quick check question: How does the GCD attention mechanism differ from standard graph attention?

## Architecture Onboarding

- Component map: Input layer -> Feature transformation module -> Prototype calculator -> GCD estimator -> GCD attention mechanism -> Aggregation module -> Output layer
- Critical path: Feature transformation → Prototype calculation → GCD estimation → Attention generation → Message aggregation → Classification
- Design tradeoffs: The paper balances between full GCD-GNN (better performance, slower) and GCD-GNN_light (faster, slightly worse). This reflects the classic performance vs. efficiency tradeoff in model design.
- Failure signatures: Poor performance might indicate: 1) Prototypes not representing categories well, 2) GCD not capturing meaningful similarity, 3) Feature transformation losing important information, 4) Attention mechanism creating instability.
- First 3 experiments:
  1. Baseline test: Run GCD-GNN_light on T-Finance dataset to verify it outperforms baseline models as claimed
  2. Ablation test: Remove feature transformation to confirm its contribution to performance
  3. Hyperparameter sensitivity: Test GCD drop rate and hidden dimension variations to understand model robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Global Confidence Degree (GCD) mechanism perform in dynamic or evolving financial networks where node relationships and fraud patterns change over time?
- Basis in paper: [explicit] The paper evaluates GCD-GNN on static datasets (T-Finance and FDCompCN) but does not address temporal dynamics or model adaptation to changing network structures.
- Why unresolved: The experiments focus on static graphs without temporal components, and the paper does not discuss how GCD would handle concept drift or evolving fraud patterns.
- What evidence would resolve it: Experimental results showing GCD-GNN performance on temporal graph datasets with concept drift, or theoretical analysis of how GCD adapts to changing network dynamics.

### Open Question 2
- Question: What is the impact of different similarity functions on GCD calculation and overall model performance?
- Basis in paper: [explicit] The paper uses cosine similarity for GCD calculation but notes that "several methods to generate GCD" were experimented with without providing comparative results.
- Why unresolved: The paper only reports results using cosine similarity and does not provide ablation studies or performance comparisons with alternative similarity measures.
- What evidence would resolve it: Systematic comparison of GCD-GNN performance using different similarity functions (e.g., Euclidean distance, Jaccard similarity) across the same datasets.

### Open Question 3
- Question: How does the model handle extremely imbalanced fraud detection scenarios where fraudulent nodes represent less than 1% of the total network?
- Basis in paper: [inferred] While the paper discusses label imbalance in related work (PC-GNN) and evaluates on datasets with moderate imbalance, it doesn't specifically address scenarios with extreme class imbalance.
- Why unresolved: The datasets used (T-Finance and FDCompCN) have fraud ratios that aren't specified as extremely low, and the paper doesn't discuss performance thresholds or techniques for extreme imbalance cases.
- What evidence would resolve it: Performance evaluation on datasets with fraud ratios below 1%, or theoretical analysis of GCD-GNN's robustness to extreme class imbalance.

## Limitations
- Prototype dependence: The effectiveness of GCD-GNN relies heavily on the quality of category prototypes, which may not generalize well to highly imbalanced fraud datasets or categories with limited labeled samples.
- Domain specificity: While tested on financial fraud, the approach may not transfer directly to other fraud detection domains without adaptation, as financial transaction patterns may have unique characteristics.
- Computational complexity: The full GCD-GNN model requires significant computational resources for prototype calculation and GCD estimation, potentially limiting scalability to very large graphs.

## Confidence
- High confidence: The experimental results on T-Finance dataset (97.26% AUC, 92.37% F1-Macro) are robust and well-supported by the data and methodology.
- Medium confidence: The improvement over baselines on FDCompCN (71.72% AUC, 59.68% F1-Macro) is less dramatic, suggesting potential dataset-specific effects or implementation variations.
- Medium confidence: The ablation study findings regarding feature transformation and GCD attention mechanisms, while supported by experiments, could benefit from additional statistical significance testing.

## Next Checks
1. **Statistical significance testing**: Perform paired t-tests or Wilcoxon signed-rank tests comparing GCD-GNN performance against each baseline across multiple random seeds to confirm the claimed improvements are statistically significant.
2. **Generalization test**: Evaluate GCD-GNN on a third, previously unseen financial fraud dataset to assess whether the performance gains transfer beyond the two tested datasets.
3. **Robustness analysis**: Systematically vary the proportion of labeled fraud samples in training data to determine how performance degrades with label scarcity, particularly for minority fraud classes.