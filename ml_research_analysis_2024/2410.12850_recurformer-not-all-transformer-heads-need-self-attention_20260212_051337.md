---
ver: rpa2
title: 'RecurFormer: Not All Transformer Heads Need Self-Attention'
arxiv_id: '2410.12850'
source_url: https://arxiv.org/abs/2410.12850
tags:
- attention
- recurformer
- tokens
- heads
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that certain attention heads in Transformer-based
  large language models exhibit a recency-aware behavior, where attention weights
  concentrate on tokens near the query token. Leveraging this observation, the authors
  propose RecurFormer, which replaces these attention heads with linear recurrent
  neural networks (specifically Mamba architecture) to reduce computational costs
  and cache size during inference.
---

# RecurFormer: Not All Transformer Heads Need Self-Attention

## Quick Facts
- arXiv ID: 2410.12850
- Source URL: https://arxiv.org/abs/2410.12850
- Authors: Ruiqing Yan; Linghan Zheng; Xingbo Du; Han Zou; Yufeng Guo; Jianfei Yang
- Reference count: 14
- Key outcome: Replaces recency-aware attention heads with Mamba blocks to reduce cache size by up to 90% while maintaining generation quality

## Executive Summary
This paper addresses the computational and memory overhead of attention mechanisms in Transformer-based large language models by identifying that certain attention heads exhibit recency-aware behavior, concentrating weights on nearby tokens. Leveraging this observation, RecurFormer replaces these heads with linear recurrent neural networks (specifically Mamba architecture) to significantly reduce cache size during inference without evicting tokens. The approach maintains generation quality while achieving up to 90% cache reduction, as demonstrated through experiments on HashHop tasks across different model sizes.

## Method Summary
RecurFormer identifies attention heads with recency-aware behavior through a recency-aware index (RA-I) metric, then replaces these heads with Mamba blocks (linear RNNs based on state-space models). The method uses continued training on the modified model to restore performance. Cache size reduction is achieved because Mamba maintains constant cache requirements regardless of sequence length, unlike standard attention which scales quadratically. The approach works across different model sizes and can reuse pre-trained weights through fine-tuning.

## Key Results
- Achieved up to 90% cache size reduction on HashHop tasks without quality degradation
- Maintained comparable generation quality (hgq metric) to original models after continued training
- Validated across three model sizes (0.5B, 7B parameters) and multiple attention mechanisms
- Cache reduction benefits persist in both prefill and generation phases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Certain attention heads in Transformers show recency-aware behavior where weights concentrate on nearby tokens.
- Mechanism: Attention weights in these heads decay with distance from the query token, creating a diagonal-dominant pattern in the attention matrix.
- Core assumption: Local dependencies dominate in these heads, making them suitable for replacement with linear RNNs.
- Evidence anchors:
  - [abstract] "certain attention heads exhibit a distribution where the attention weights concentrate on tokens near the query token, termed as recency aware"
  - [section] "we identified a local, short-range token attention distribution pattern, referred to as the recency aware, where attention is concentrated on tokens close to the query token"
  - [corpus] Weak - corpus contains papers about attention efficiency but none directly analyze recency-aware head patterns.
- Break condition: If attention weights don't decay with distance or show uniform distribution across all tokens.

### Mechanism 2
- Claim: Linear RNNs (Mamba) can efficiently model local dependencies that recency-aware heads capture.
- Mechanism: Mamba's linear state transitions with associative operators allow efficient parallel computation while maintaining constant cache size.
- Core assumption: Short-range dependencies don't require the full quadratic attention mechanism.
- Evidence anchors:
  - [abstract] "we propose RecurFormer, a novel architecture that replaces these attention heads with linear recurrent neural networks (RNNs), specifically the Mamba architecture"
  - [section] "Using a linear RNNs is a more efficient option for fitting the recency aware so that it reduces cache size requirements"
  - [corpus] Weak - corpus has papers on attention efficiency but none specifically validate Mamba for recency-aware patterns.
- Break condition: If task requires modeling long-range dependencies that linear RNNs cannot capture.

### Mechanism 3
- Claim: Replacing recency-aware heads with Mamba reduces cache size without degrading generation quality.
- Mechanism: Cache size scales with sequence length for attention but remains constant for Mamba, enabling memory-efficient inference.
- Core assumption: Remaining attention heads can capture long-range dependencies, preserving overall model performance.
- Evidence anchors:
  - [abstract] "This replacement reduces the cache size without evicting tokens, thus maintaining generation quality"
  - [section] "Compared to fully attention-based Transformer, RecurFormer reduces cache size in both the prefill and generation phases"
  - [corpus] Weak - corpus contains cache optimization papers but none specifically validate non-eviction approaches.
- Break condition: If generation quality degrades significantly or cache reduction is minimal.

## Foundational Learning

- Concept: Attention mechanism mechanics and quadratic complexity
  - Why needed here: Understanding why attention is expensive and how recency-aware heads differ from global ones
  - Quick check question: Why does standard attention have O(n²) complexity and how does this affect cache size?

- Concept: State-space models and linear RNN architectures
  - Why needed here: Mamba is based on state-space models, understanding its computation is crucial for implementation
  - Quick check question: How does Mamba achieve linear time complexity compared to quadratic attention?

- Concept: Cache management in Transformer inference
  - Why needed here: The paper's efficiency gains come from cache size reduction, understanding KV-cache is essential
  - Quick check question: What components make up the KV-cache and how does their size scale with sequence length?

## Architecture Onboarding

- Component map:
  - RecurFormer Block: Contains both standard attention heads and Mamba blocks
  - RA-I Calculator: Computes recency-aware index for each head to determine replacement candidates
  - Mamba Block: Replaces selected attention heads, maintains constant cache size
  - Continued Training Module: Fine-tunes the hybrid model to recover performance

- Critical path:
  1. Compute RA-I for all heads using training data
  2. Select heads with highest RA-I values for replacement
  3. Replace selected heads with Mamba blocks
  4. Perform continued training to restore performance
  5. Deploy for inference with reduced cache requirements

- Design tradeoffs:
  - β value selection: Higher β reduces cache more but risks quality degradation
  - Head selection strategy: RA-I threshold vs. top-k selection
  - Mamba parameter tuning: State dimension, convolution kernel size, expansion factor

- Failure signatures:
  - Cache size doesn't reduce as expected: Check Mamba implementation or β selection
  - Generation quality drops significantly: Too many heads replaced or wrong heads selected
  - Training doesn't converge: Mamba parameters may be misconfigured or learning rate too high

- First 3 experiments:
  1. Compute RA-I distribution across all heads in a pre-trained model to identify replacement candidates
  2. Replace a small percentage (β=0.1) of heads with Mamba and measure cache reduction and quality impact
  3. Perform continued training on the hybrid model and compare perplexity to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RecurFormer scale with different values of β (the proportion of attention heads replaced with Mamba blocks) across various model sizes and tasks?
- Basis in paper: [explicit] The paper discusses the impact of β values in ablation studies and mentions specific results for different models, but doesn't provide a comprehensive analysis of scaling across all model sizes and tasks.
- Why unresolved: The paper focuses on specific β values for each model and task, but doesn't systematically explore the full range of β values for all combinations of model sizes and tasks.
- What evidence would resolve it: A comprehensive study testing all possible β values across multiple model sizes (e.g., 0.5B, 7B, 13B parameters) and tasks (e.g., HashHop, MQAR, masked prediction) would provide insights into optimal β scaling.

### Open Question 2
- Question: Can RecurFormer's approach be extended to other types of linear RNNs or state-space models beyond Mamba, and how would this affect performance?
- Basis in paper: [inferred] The paper mentions that Mamba is a linear RNN based on selective structured state-space sequence model, suggesting potential for other linear RNNs or SSMs.
- Why unresolved: The paper only explores Mamba as the replacement for attention heads and doesn't investigate other potential linear RNN or SSM architectures.
- What evidence would resolve it: Experiments replacing attention heads with various linear RNNs or SSMs (e.g., S4, S6 variants) and comparing their performance in terms of cache reduction and generation quality would provide insights into the generalizability of the approach.

### Open Question 3
- Question: How does RecurFormer's performance compare to other non-eviction-based optimization methods like DMC in terms of both efficiency and generation quality across different tasks and model sizes?
- Basis in paper: [explicit] The paper mentions DMC as a non-eviction-based method and compares RecurFormer to PyramidInfer, but doesn't provide a comprehensive comparison with other non-eviction methods.
- Why unresolved: The paper only briefly mentions DMC and focuses on comparing RecurFormer to PyramidInfer, without exploring a broader range of non-eviction-based optimization methods.
- What evidence would resolve it: A comprehensive comparison of RecurFormer against multiple non-eviction-based methods (e.g., DMC, token merging techniques) across various tasks and model sizes would provide insights into RecurFormer's relative performance and efficiency.

## Limitations

- Recency-aware head identification relies on RA-I metric that isn't fully validated against ground truth head importance
- Generalization across different architectures and attention mechanisms hasn't been thoroughly tested
- Training stability and convergence for very long sequences (>60K tokens) remains uncertain

## Confidence

**High Confidence**: The fundamental observation that certain attention heads exhibit recency-aware behavior is well-supported by the analysis in Section 4.1, with clear visualizations showing the diagonal-dominant attention patterns. The cache size reduction mechanism is mathematically sound and directly demonstrated through empirical measurements.

**Medium Confidence**: The claim that replacing recency-aware heads with Mamba maintains generation quality is supported by HashHop task results, but the task itself is relatively narrow and synthetic. The comparison against other efficient attention methods shows competitive performance, but these comparisons are limited to specific model sizes.

**Low Confidence**: The generalizability of RA-I across different domains and tasks is not well-established. The paper doesn't provide ablation studies showing how different β values affect performance, nor does it explore the impact of different Mamba hyperparameters on efficiency gains.

## Next Checks

**Check 1: Cross-Domain Head Stability**: Compute RA-I values for the same model architecture trained on different datasets (e.g., ArXiv papers, code repositories, biomedical text) and measure the overlap between identified recency-aware heads. This would validate whether the approach generalizes beyond Wikipedia-style text and whether head selection is robust to domain shifts.

**Check 2: Long Sequence Performance Validation**: Extend HashHop experiments to sequences of 100K-500K tokens and measure both hgq performance and actual memory consumption during inference. This would validate whether the constant cache size advantage holds for real-world long-context applications and whether any hidden costs emerge at extreme sequence lengths.

**Check 3: Architectural Ablation Study**: Systematically vary the percentage of heads replaced (β from 0.1 to 0.9) and measure the Pareto frontier of cache reduction vs. generation quality degradation. Additionally, test different Mamba configurations (varying dstate, dconv, dtrank) to identify optimal settings for different model sizes and tasks.