---
ver: rpa2
title: PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation
  and Summarization Evaluation
arxiv_id: '2406.18528'
source_url: https://arxiv.org/abs/2406.18528
tags:
- task
- prompts
- prompt
- format
- complex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a large-scale prompt exploration (PrExMe) for
  evaluating open-source large language models (LLMs) as metrics for machine translation
  and summarization. It tests over 720 prompt templates across seven models, totaling
  more than 6.6 million evaluations.
---

# PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation

## Quick Facts
- **arXiv ID**: 2406.18528
- **Source URL**: https://arxiv.org/abs/2406.18528
- **Reference count**: 40
- **Primary result**: Large-scale prompt exploration (PrExMe) evaluates over 720 templates across seven open-source LLM models on MT and summarization tasks, totaling 6.6M+ evaluations

## Executive Summary
This paper introduces PrExMe, a comprehensive study exploring prompt templates for evaluating open-source large language models (LLMs) as metrics for machine translation and summarization. The research systematically tests over 720 prompt templates across seven models, revealing that models exhibit idiosyncratic preferences for textual versus numeric scoring formats. PLATYPUS 2-70B emerges as the top performer overall, while the median performance of prompts serves as a reliable predictor for new settings. The study provides valuable insights into robust prompting strategies and model-specific patterns that can guide the development of LLM-based evaluation metrics.

## Method Summary
The study evaluates seven open-source LLM models (Platypus 2-70B, Nous Hermes-13B, OpenOrca-Platypus 2-13B, Llama3-8B, Llama3-70B, Mixtral-8x7B, Tower-13B) using hierarchical prompt templates with base prompts, task descriptions, and format requirements. Over 720 prompt templates were tested across MT and summarization datasets from EVAL4NLP 2023 (train, dev, test) and WMT23/Seahorse. The evaluation measures Kendall, Pearson, and Spearman correlations between LLM scores and human judgments, along with tie-calibrated accuracy. The research employs zero-shot, chain-of-thought, and retrieval-augmented generation approaches in a reference-free setting.

## Key Results
- Model-specific prompt patterns exist, with some models preferring textual labels while others prefer numeric scores
- PLATYPUS 2-70B performs best overall across both MT and summarization tasks
- Median prompt performance serves as a reliable predictor for new settings and datasets
- Small changes in prompts can significantly affect model rankings and performance stability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Large language models can function as evaluation metrics for natural language generation tasks without fine-tuning, relying solely on in-context learning.
- **Mechanism**: The LLM is prompted with a structured template that includes the source text, the generated hypothesis, a task description, and a format requirement. The model uses its pre-trained knowledge and the prompt context to generate a judgment or score.
- **Core assumption**: The LLM's pre-trained knowledge is sufficient to assess the quality of generated text in a zero-shot or few-shot setting, and the prompt structure guides the model to produce a relevant output.
- **Evidence anchors**: [abstract]: "In this work, we introduce PrExMe, a large-scale Prompt Exploration for Metrics, where we evaluate more than 720 prompt templates for open-source LLM-based metrics on machine translation (MT) and summarization datasets, totalling over 6.6M evaluations." [section]: "We evaluate LLM S in a reference-free setting, i.e., they grade a generated hypothesis based on its source without a reference."
- **Break condition**: If the LLM's pre-training data lacks sufficient coverage of the specific evaluation criteria or task domain, or if the prompt structure fails to guide the model effectively.

### Mechanism 2
- **Claim**: Certain prompting patterns are robust and generalizable across different tasks and datasets, with the median performance being a good predictor for new settings.
- **Mechanism**: By evaluating a large number of prompt templates across various tasks and datasets, stable patterns emerge. The median performance of these patterns across different contexts serves as a reliable indicator of their effectiveness in new scenarios.
- **Core assumption**: The performance of prompt templates is not overly sensitive to small changes in task or dataset, and the median provides a robust measure of central tendency that is less affected by outliers.
- **Evidence anchors**: [abstract]: "We discover that, on the one hand, there are scenarios for which prompts are stable. For instance, some LLMs show idiosyncratic preferences and favor to grade generated texts with textual labels while others prefer to return numeric scores." [section]: "We can also use this method to quantify the stability of the model ranking, when each model is first prompted with pattern A that is then changed to pattern B."
- **Break condition**: If the task or dataset characteristics change drastically, or if the model's performance is highly sensitive to minor prompt variations, the median may no longer be a reliable predictor.

### Mechanism 3
- **Claim**: Models have idiosyncratic preferences for certain prompting patterns, such as textual labels versus numeric scores, which can be leveraged to optimize their performance.
- **Mechanism**: By systematically testing different prompt components (e.g., base prompts, task descriptions, format requirements) across multiple models, patterns emerge that indicate each model's preferences. These preferences can then be used to tailor prompts for optimal performance.
- **Core assumption**: The model's training data and architecture influence its response to different prompt patterns, leading to consistent preferences across tasks.
- **Evidence anchors**: [abstract]: "We find that model-specific prompt patterns exist, with some models preferring textual labels and others numeric scores." [section]: "98.0% 2.0% OpenOrca-13B... While ORCA prefers the PZS prompts, TOWER is better with ZS-C OT and ZS-C OT-EM."
- **Break condition**: If the model's training data or fine-tuning introduces new biases, or if the evaluation task is significantly different from the model's typical use cases.

## Foundational Learning

- **Concept**: In-context learning
  - **Why needed here**: The LLM's ability to perform tasks based on prompts without fine-tuning is the core mechanism of this evaluation approach.
  - **Quick check question**: Can you explain how in-context learning differs from fine-tuning and why it is advantageous for low-resource evaluation scenarios?

- **Concept**: Prompt engineering
  - **Why needed here**: The systematic exploration of prompt templates and components is essential for understanding how to construct effective prompts for LLM-based evaluation metrics.
  - **Quick check question**: What are the key components of a prompt template, and how do they influence the LLM's output?

- **Concept**: Correlation metrics (Kendall, Pearson, Spearman)
  - **Why needed here**: These metrics are used to evaluate the performance of LLM-based metrics by comparing their scores to human judgments.
  - **Quick check question**: Can you describe the differences between Kendall, Pearson, and Spearman correlations, and when each might be most appropriate?

## Architecture Onboarding

- **Component map**: Hierarchical prompt templates (base prompts, task descriptions, format requirements) → LLMs (seven open-source models) → Datasets (EVAL4NLP and WMT23/Seahorse) → Evaluation metrics (correlation measures)
- **Critical path**: Prompt template construction → LLM evaluation → Score extraction → Correlation computation → Analysis of stability and generalizability
- **Design tradeoffs**: Resource constraints (limited to 13B and 70B models), prompt complexity versus model performance, and the balance between exploring a wide range of prompts versus focusing on the most promising ones
- **Failure signatures**: Low correlation with human judgments, high variability in performance across different prompts or datasets, and failure to extract scores from LLM outputs
- **First 3 experiments**:
  1. Evaluate a small set of diverse prompt templates across all models on a single task to identify initial patterns
  2. Analyze the stability of top-performing prompts by testing them on a different dataset within the same task domain
  3. Explore the impact of changing individual prompt components (e.g., base prompt or format requirement) on model performance to identify robust patterns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do hierarchical template components (base prompt, task description, format requirement) interact with each other to influence LLM performance?
- **Basis in paper**: [inferred] The paper mentions that prompts are constructed hierarchically and that models have idiosyncratic preferences for different patterns, but doesn't fully explore the interaction effects between these components.
- **Why unresolved**: The analysis primarily examines each component in isolation or when one component changes while others remain fixed. There's no systematic exploration of how combinations of specific base prompts, task descriptions, and format requirements work together.
- **What evidence would resolve it**: A comprehensive factorial analysis testing all possible combinations of the best-performing components from each category, or a systematic study of interaction effects between template components.

### Open Question 2
- **Question**: What is the optimal balance between prompt complexity and performance for LLM-based evaluation metrics?
- **Basis in paper**: [explicit] The paper notes that LocalGemba (a more complex MQM-based approach) performs worse than simpler PZS prompts despite using GPT-4, and mentions that context size was exceeded in rare cases.
- **Why unresolved**: While the paper tests prompts of varying complexity, it doesn't systematically measure the trade-off between prompt complexity (number of tokens, steps in reasoning) and evaluation performance, nor does it establish guidelines for optimal prompt length.
- **What evidence would resolve it**: Controlled experiments varying prompt complexity while holding other factors constant, measuring both performance and resource efficiency (token count, inference time).

### Open Question 3
- **Question**: How generalizable are model-specific prompting patterns across different evaluation tasks and domains?
- **Basis in paper**: [explicit] The paper finds that models show idiosyncratic preferences (e.g., ORCA preferring numeric scores while LLAMA3 preferring textual labels) but only tests these patterns across MT and summarization tasks.
- **Why unresolved**: The study is limited to machine translation and summarization tasks, leaving open whether these model-specific preferences hold for other NLP tasks like question answering, dialogue, or code generation.
- **What evidence would resolve it**: Testing the identified model-specific patterns across a broader range of NLP evaluation tasks, or conducting cross-task correlation analyses to determine which patterns generalize beyond the current scope.

## Limitations

- **Generalizability Concerns**: The study focuses on specific MT and summarization benchmarks from EVAL4NLP 2023 and WMT23, which may limit broader applicability to other NLP tasks or domains.
- **Prompt Stability Measurement**: The paper measures stability through correlation changes when switching between prompt patterns but doesn't capture more subtle performance variations or edge cases.
- **Model Coverage Gaps**: The study is limited to 13B and 70B parameter models, excluding smaller models and potentially missing important performance patterns at different scale points.

## Confidence

**High Confidence**: The core finding that median prompt performance serves as a reliable predictor across different settings is well-supported by the large-scale evaluation (6.6M+ evaluations). The systematic exploration methodology and clear pattern of model-specific preferences (textual vs. numeric scoring) are robustly demonstrated.

**Medium Confidence**: The identification of PLATYPUS 2-70B as the overall best performer is supported but may be influenced by its newer training date and limited to the specific tasks and datasets tested. The generalizability of stable prompt patterns across different domains requires further validation.

**Low Confidence**: The claim that small prompt changes can strongly affect rankings needs more granular analysis to identify which specific prompt components drive these variations and whether these findings extend beyond the tested models and tasks.

## Next Checks

1. **Cross-Domain Validation**: Test the identified stable prompt patterns and model preferences on a different NLP task (e.g., question answering or text classification) to assess generalizability beyond MT and summarization.

2. **Prompt Component Sensitivity Analysis**: Conduct a more granular analysis of which specific prompt components (base prompt, task description, format requirement) drive the observed performance variations, potentially using ablation studies or controlled experiments.

3. **Scale-Agnostic Performance Evaluation**: Evaluate the prompt patterns and model preferences on smaller models (1B-7B parameters) to determine whether the identified patterns hold across different model scales and to assess resource efficiency trade-offs.