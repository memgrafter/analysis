---
ver: rpa2
title: Feedback-Generation for Programming Exercises With GPT-4
arxiv_id: '2403.04449'
source_url: https://arxiv.org/abs/2403.04449
tags:
- feedback
- code
- gpt-4
- programming
- students
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the quality of feedback generated by GPT-4
  Turbo for programming exercises. The researchers selected two programming assignments
  from an introductory Java course and used GPT-4 Turbo to generate feedback for 55
  randomly chosen student submissions.
---

# Feedback-Generation for Programming Exercises With GPT-4

## Quick Facts
- arXiv ID: 2403.04449
- Source URL: https://arxiv.org/abs/2403.04449
- Reference count: 40
- Primary result: GPT-4 Turbo generates structured, personalized feedback for programming exercises with 75-95% accuracy in correctness classification

## Executive Summary
This paper investigates the quality of feedback generated by GPT-4 Turbo for programming exercises in an introductory Java course. The researchers analyzed 55 randomly selected student submissions using qualitative methods, focusing on correctness, personalization, and fault localization. The study found that GPT-4 Turbo produces more structured and consistent feedback compared to previous models, with notable improvements in accuracy and personalization. However, the researchers caution that the generated feedback may not be suitable for direct student use without proper guidance or instruction.

## Method Summary
The researchers selected two programming assignments from an introductory Java course and used GPT-4 Turbo with a specific prompt template to generate feedback for 55 randomly chosen student submissions. The feedback was analyzed qualitatively, examining features such as correctness, personalization, fault localization, and structure. Manual verification of student submissions was performed using unit tests. The study employed a zero-shot approach without requiring test case development, and feedback quality was assessed through thematic analysis of the generated outputs.

## Key Results
- GPT-4 Turbo accuracy in classifying student submissions as correct or incorrect ranged from 75-95%
- Precision of feedback classification ranged from 78-100% while recall ranged from 33-68%
- All generated feedback was found to be personalized, containing both code and text
- The model showed notable improvements in structured and consistent feedback compared to previous versions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 Turbo provides more accurate and detailed feedback compared to previous models, improving correctness classification.
- Mechanism: The model's larger context window and updated knowledge cutoff enable better understanding of programming tasks and student submissions, leading to more precise error identification and correction suggestions.
- Core assumption: GPT-4 Turbo's architecture and training data allow it to outperform GPT-3.5 in recognizing correct solutions and providing actionable feedback.
- Evidence anchors:
  - [abstract] "Compared to prior work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements."
  - [section 4.3] "Azaiz et al. showed an accuracy of the correctness classification of 73%. Here, GPT-4 reaches 84% on the same data set [1]."

### Mechanism 2
- Claim: GPT-4 Turbo generates 100% personalized feedback without the need for test case development.
- Mechanism: By analyzing the student's code directly, the model can tailor feedback to the specific submission, identifying errors and suggesting improvements unique to that code.
- Core assumption: The model's ability to understand and analyze code allows it to provide individualized feedback without relying on predefined test cases.
- Evidence anchors:
  - [abstract] "All of the generated feedback is personalized."
  - [section 5] "A benefit of using LLMs is that it generates 100% personalized feedback without the need to develop test cases."

### Mechanism 3
- Claim: GPT-4 Turbo's feedback is more structured and consistent compared to previous models.
- Mechanism: The model's improved architecture allows it to organize feedback into clear sections, such as an introductory statement, a list of issues with corrections, and a summary.
- Core assumption: The model's ability to structure information effectively leads to more consistent and easier-to-understand feedback for students.
- Evidence anchors:
  - [abstract] "For example, the output is more structured and consistent."
  - [section 4.1.1] "The first part is an introductory statement or a description of the submitted code. This is coupled with an assessment of the code's quality and correctness. Next, an (enumerated) list of issues and respective corrections or suggestions for improvements were mostly displayed."

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: Understanding the capabilities and limitations of LLMs is crucial for evaluating their effectiveness in generating feedback for programming exercises.
  - Quick check question: What are the key differences between GPT-3.5 and GPT-4 Turbo in terms of their architecture and training data?

- Concept: Formative feedback
  - Why needed here: The paper focuses on the use of GPT-4 Turbo to generate formative feedback for programming exercises, which is essential for supporting students' learning process.
  - Quick check question: How does formative feedback differ from summative feedback, and why is it important in the context of introductory programming courses?

- Concept: Code analysis and error localization
  - Why needed here: The model's ability to analyze student code, identify errors, and provide suggestions for improvement is a key aspect of its feedback generation capabilities.
  - Quick check question: What are some common techniques used for code analysis and error localization in programming education, and how does GPT-4 Turbo's approach compare?

## Architecture Onboarding

- Component map: Task specification -> GPT-4 Turbo -> Feedback generation -> Qualitative analysis
- Critical path: The model's ability to accurately understand the programming task and student code, and generate appropriate feedback based on this understanding
- Design tradeoffs: Using a pre-trained model like GPT-4 Turbo allows for quick implementation and customization, but may introduce privacy concerns and dependency on the model provider. Developing a custom model would provide more control but require significant resources.
- Failure signatures: The model may generate incorrect or misleading feedback, fail to identify errors in student code, or provide feedback that is too complex or overwhelming for novice programmers.
- First 3 experiments:
  1. Evaluate the model's accuracy in classifying student submissions as correct or incorrect, using a small dataset of known correct and incorrect solutions.
  2. Analyze the model's ability to provide actionable feedback by manually reviewing a sample of generated feedback and assessing its usefulness for students.
  3. Test the model's consistency by generating feedback for the same submission multiple times and comparing the results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GPT-4 Turbo's performance vary when provided with more detailed task specifications versus minimal instructions?
- Basis in paper: [explicit] The paper mentions that GPT-4's accuracy in feedback generation seems to improve when provided with task instructions, but also notes a decrease in error identification accuracy. It suggests the need for more research on this aspect.
- Why unresolved: The paper does not provide a detailed analysis of how different levels of task specification detail affect GPT-4 Turbo's performance in generating feedback.
- What evidence would resolve it: A controlled study varying the level of detail in task specifications provided to GPT-4 Turbo and measuring the resulting accuracy and quality of feedback across different metrics.

### Open Question 2
- Question: To what extent can GPT-4 Turbo's feedback be effectively tailored to the prior knowledge levels of individual students?
- Basis in paper: [inferred] The paper discusses the potential for GPT-4 Turbo to be integrated into adaptive learning systems and mentions the need for feedback to be tailored to students' prior knowledge, but does not explore this in depth.
- Why unresolved: The paper does not provide empirical evidence or specific methods for adapting GPT-4 Turbo's feedback to different student knowledge levels.
- What evidence would resolve it: Experimental studies testing GPT-4 Turbo's feedback on students with varying levels of programming knowledge, measuring the effectiveness and appropriateness of the feedback for each group.

### Open Question 3
- Question: How can inconsistencies and redundancies in GPT-4 Turbo's generated feedback be systematically reduced?
- Basis in paper: [explicit] The paper identifies inconsistencies and redundancies as issues in GPT-4 Turbo's feedback, such as contradictory recommendations and repeated suggestions.
- Why unresolved: The paper does not propose specific methods or techniques for reducing these issues in the generated feedback.
- What evidence would resolve it: Development and testing of prompt engineering techniques or post-processing methods designed to minimize inconsistencies and redundancies in GPT-4 Turbo's feedback output.

## Limitations

- Evaluation based on qualitative analysis of a relatively small sample (55 submissions across two assignments)
- Manual verification process introduces potential researcher bias
- Study does not address how students actually interact with or benefit from the generated feedback in real learning environments

## Confidence

- Accuracy improvements: Medium - Supported by direct comparisons on same dataset
- Personalization and structure: Medium - Observed across all outputs but not quantitatively measured
- Direct student suitability: Low - Identified as cautionary note rather than empirically validated

## Next Checks

1. Conduct a controlled study with actual students using the generated feedback, measuring learning outcomes and student perceptions of feedback usefulness compared to traditional feedback methods.

2. Expand evaluation to multiple programming languages and assignment types, including more complex problems beyond the introductory level, to assess generalizability.

3. Implement a longitudinal study tracking student performance over time when using AI-generated feedback, comparing it against baseline student performance without AI assistance.