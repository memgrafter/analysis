---
ver: rpa2
title: The Propensity for Density in Feed-forward Models
arxiv_id: '2410.14461'
source_url: https://arxiv.org/abs/2410.14461
tags:
- size
- effective
- weights
- different
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether neural networks tend to use all
  available parameters when solving a task, even when fewer would suffice. The authors
  study pruning behavior across varying model widths for fully connected, convolutional,
  and residual networks on MNIST and CIFAR-10 tasks.
---

# The Propensity for Density in Feed-forward Models

## Quick Facts
- **arXiv ID**: 2410.14461
- **Source URL**: https://arxiv.org/abs/2410.14461
- **Reference count**: 40
- **Key outcome**: Neural networks maintain consistent effective density across varying widths, using more absolute parameters as width increases while rejecting sparsity

## Executive Summary
This paper investigates whether neural networks use all available parameters when solving tasks, even when fewer would suffice. Through systematic pruning experiments across fully connected, convolutional, and residual networks trained on MNIST and CIFAR-10, the authors find that the proportion of weights that can be pruned without performance loss remains largely invariant to model size. As width increases, the absolute number of unprunable parameters grows substantially while effective density stays constant, revealing an implicit bias toward density rather than sparsity in neural network training.

## Method Summary
The authors train MLPs, convolutional networks (Conv-2), and ResNet-18 architectures on MNIST and CIFAR-10 datasets with varying width scaling factors (0.1x to 5x). Models are trained using SGD, Adam, and Adagrad optimizers with both Glorot and He initialization schemes. After training, layer-wise magnitude pruning is applied in 2% steps until validation accuracy drops by 5%, without retraining. Effective density is calculated as the percentage of weights remaining at this threshold, allowing comparison across different model sizes and architectures.

## Key Results
- Effective density remains largely invariant to model width across all architectures tested
- Absolute number of unprunable parameters increases substantially as width grows
- Initialization effects are strongest for convolutional models trained with Adam
- Monosemanticity increases with width, requiring more parameters while maintaining similar density

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning reveals the "core" network size, which grows proportionally with width but maintains constant effective density.
- Mechanism: Training initializes a dense parameter space; pruning removes low-magnitude weights until performance degrades, revealing a stable proportion of "essential" weights.
- Core assumption: The essential parameter proportion is determined by the learning algorithm's inductive bias, not by the total number of parameters.
- Evidence anchors:
  - [abstract] "the proportion of weights that can be pruned without degrading performance is largely invariant to model size"
  - [section] "we find that as we increase the width, the 'core' model size |Ï‰| vastly increases"
  - [corpus] No direct corpus evidence supporting this specific pruning mechanism
- Break condition: If different initialization schemes or optimizers fundamentally change the pruning threshold across widths, this mechanism fails.

### Mechanism 2
- Claim: Initialization and training dynamics lock in a particular density pattern that scales with width.
- Mechanism: Small weights at initialization tend to remain small after training, creating a consistent pruning threshold across different widths.
- Core assumption: The overlap between small initialized weights and small trained weights remains constant as width increases.
- Evidence anchors:
  - [section] "the overlap between the smallest 40% weights at initialization and the smallest 40% after training" is examined
  - [section] "For convolutional models trained with Adam, initialization may play a role in the models' propensity for density"
  - [corpus] No direct corpus evidence supporting this specific initialization-based mechanism
- Break condition: If the overlap between small initialized and trained weights varies significantly with width, this mechanism fails.

### Mechanism 3
- Claim: Wider networks develop more monosemantic units, requiring more absolute parameters while maintaining similar density.
- Mechanism: Increased width allows the network to separate tasks more effectively, creating more specialized, single-purpose units that require more parameters to implement.
- Core assumption: Monosemanticity increases with width, leading to more parameters being needed to represent these specialized units.
- Evidence anchors:
  - [abstract] "wider models use the abundance of parameters at their disposal to train more monosemantic units"
  - [section] "an increase in width leads to both more parameters being used in absolute terms... and generally leads to more activation sparsity and class selectivity"
  - [corpus] No direct corpus evidence supporting this specific monosemanticity mechanism
- Break condition: If activation sparsity and class selectivity don't consistently increase with width, this mechanism fails.

## Foundational Learning

- Concept: Pruning methodology and its relationship to model capacity
  - Why needed here: Understanding how pruning reveals the "core" network size is central to interpreting the results
  - Quick check question: If a network can be pruned by 50% without performance loss, what does this tell us about its effective density?

- Concept: Inductive bias in neural network training
  - Why needed here: The paper's core finding relates to how training regimes implicitly favor certain parameter densities
  - Quick check question: How might different optimizers (SGD vs Adam) create different inductive biases that affect pruning outcomes?

- Concept: Parameter initialization schemes (Glorot vs He)
  - Why needed here: The paper investigates how initialization affects the propensity for density
  - Quick check question: Why might He initialization lead to higher effective densities compared to Glorot initialization?

## Architecture Onboarding

- Component map:
  - MLP: Two dense layers (300, 100 units)
  - Conv-2: Two conv layers (64 filters), max pooling, two dense layers (256 units)
  - ResNet: Convolutional layer, three residual blocks, average pooling, dense layer
  - All architectures scale width by factor x (0.1 to 5)

- Critical path:
  1. Train model with specified optimizer and initialization
  2. Apply layer-wise magnitude pruning in 2% steps
  3. Record accuracy at each pruning step
  4. Determine effective density at 5% accuracy drop threshold

- Design tradeoffs:
  - Layer-wise vs global pruning: Layer-wise preserves architectural structure but may miss global optimization
  - Fixed pruning percentage vs adaptive: 2% steps are systematic but may miss optimal thresholds
  - No retraining vs iterative: No retraining captures learned implementation but may overestimate essential parameters

- Failure signatures:
  - High variance in effective densities across seeds suggests instability
  - Non-overlapping pruning curves indicate fundamental architectural differences
  - Unexpected density changes with width suggest initialization or optimizer effects

- First 3 experiments:
  1. Train MLP (1x width) on MNIST with SGD, Glorot initialization, apply magnitude pruning, record effective density
  2. Repeat experiment 1 with He initialization, compare effective densities
  3. Train Conv-2 (1x width) on CIFAR-10 with Adam, apply magnitude pruning, compare to MLP results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the propensity for density observed in this study extend to other architectures (e.g., transformers, recurrent networks) and tasks beyond image classification?
- Basis in paper: [explicit] The authors note this as a limitation and suggest further investigations are needed to verify whether results generalize to other architectures and tasks
- Why unresolved: The study focused on feed-forward models (MLPs, Conv-2, ResNet) on MNIST and CIFAR-10 tasks
- What evidence would resolve it: Systematic studies testing the same pruning methodology across diverse architectures (transformers, RNNs) and tasks (language modeling, reinforcement learning)

### Open Question 2
- Question: Is the density propensity primarily driven by the learning algorithm's optimization dynamics or by the functional requirements of the learned solution?
- Basis in paper: [inferred] The authors explore three hypotheses (initialization effects, functional differences, monosemanticity) but find varying results across architectures and optimizers, suggesting the mechanism is complex
- Why unresolved: Results show different mechanisms may be at play for different architectures (initialization matters for Conv-2 with Adam, functional differences matter for MLPs with SGD)
- What evidence would resolve it: Comparative studies using alternative optimization methods (e.g., evolutionary algorithms, optimal transport) to disentangle whether density emerges from optimization bias or functional necessity

### Open Question 3
- Question: Can the effective density of a model be predicted a priori based on its architecture, task complexity, and optimization parameters?
- Basis in paper: [inferred] The authors find consistent effective densities across different model widths, suggesting an underlying relationship between model capacity and task requirements that could potentially be predicted
- Why unresolved: While effective densities remain stable across widths, the study doesn't establish a predictive framework for determining this density before training
- What evidence would resolve it: Development of theoretical bounds or empirical models that predict effective density from architecture specifications and task characteristics without requiring full training and pruning experiments

## Limitations
- Limited to feedforward architectures (MLPs, Conv-2, ResNet) and two image classification datasets
- Magnitude-based pruning without retraining may overestimate essential parameters
- No investigation of alternative pruning criteria beyond magnitude-based methods

## Confidence
- **High confidence**: Core finding that effective density remains invariant to model width across architectures
- **Medium confidence**: Initialization effects on density propensity, monosemanticity explanation for density patterns
- **Low confidence**: Generalizability to non-standard architectures and tasks beyond image classification

## Next Checks
1. Test the density propensity hypothesis on transformer-based models and recurrent networks to determine if the finding generalizes beyond feedforward architectures.

2. Systematically vary initialization schemes and measure how the overlap between small initialized and small trained weights changes with width to establish causality.

3. Implement iterative magnitude pruning with retraining phases to determine how much the no-retraining approach overestimates the number of essential parameters.