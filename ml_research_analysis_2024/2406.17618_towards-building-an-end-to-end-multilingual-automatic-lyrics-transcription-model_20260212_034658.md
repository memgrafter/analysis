---
ver: rpa2
title: Towards Building an End-to-End Multilingual Automatic Lyrics Transcription
  Model
arxiv_id: '2406.17618'
source_url: https://arxiv.org/abs/2406.17618
tags:
- multilingual
- language
- languages
- english
- lyrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multilingual automatic lyrics transcription
  (ALT), a challenging task due to limited labeled data and singing-specific acoustic
  variations compared to multilingual automatic speech recognition. The authors develop
  a multilingual ALT model by expanding the target vocabulary to include multiple
  languages and compare its performance to monolingual models trained on language-specific
  subsets.
---

# Towards Building an End-to-End Multilingual Automatic Lyrics Transcription Model

## Quick Facts
- arXiv ID: 2406.17618
- Source URL: https://arxiv.org/abs/2406.17618
- Reference count: 26
- Primary result: Multilingual ALT model outperforms monolingual models across all languages with improvements from 2% for English to over 7% for other languages

## Executive Summary
This paper addresses multilingual automatic lyrics transcription (ALT), a challenging task due to limited labeled data and singing-specific acoustic variations compared to multilingual automatic speech recognition. The authors develop a multilingual ALT model by expanding the target vocabulary to include multiple languages and compare its performance to monolingual models trained on language-specific subsets. They also explore conditioning methods to incorporate language information into the model.

The proposed model uses a hybrid CTC/Attention architecture with a transformer encoder and decoder. The multilingual model outperforms monolingual models across all languages, with improvements ranging from 2% for English to over 7% for other languages. Language conditioning further enhances performance, with the best results achieved by conditioning on both the encoder and decoder inputs. Language classification analysis reveals that languages other than English are often misclassified as English, explaining the limited impact of language conditioning on English ALT.

## Method Summary
The method employs a hybrid CTC/Attention architecture with a transformer encoder and decoder for multilingual ALT. The model uses 80-dimensional Mel-spectrograms as input and processes them through a CNN block before feeding them to the transformer encoder. The decoder generates character sequences using both CTC and attention mechanisms. Training involves fine-tuning with teacher forcing, using Adam optimizer and Noam learning rate scheduler for 50 epochs. Language conditioning is implemented by concatenating language embeddings to encoder and/or decoder inputs, while self-conditioning uses a multi-task learning approach where the encoder predicts language ID used as additional decoder input.

## Key Results
- Multilingual model outperforms monolingual models in every language, with WER improvements ranging from 2% for English to over 7% for other languages
- Language conditioning significantly improves performance, with the best results achieved by conditioning on both encoder and decoder inputs (EncDec-Cond)
- Language classification analysis reveals high misclassification rates for non-English languages as English, explaining limited impact of language conditioning on English ALT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual training improves low-resource language performance by sharing phonetic and orthographic similarities across languages
- Mechanism: The model learns shared subword patterns and pronunciation rules from multiple languages, allowing better generalization for low-resource languages that benefit from high-resource language data (e.g., English)
- Core assumption: Languages share enough phonetic and orthographic similarities that training on multiple languages improves low-resource language performance
- Evidence anchors:
  - [abstract] "Our findings reveal that the multilingual model performs consistently better than the monolingual models trained on the language subsets."
  - [section] "The multilingual model outperforms monolingual models in every language, indicating that having more training data in various languages benefits low-resource language ALT."
  - [corpus] Weak - corpus neighbors focus on Vietnamese, readability, and alignment rather than multilingual transfer learning
- Break condition: If languages share minimal phonetic/orthographic similarities, multilingual training may not provide benefits or could even harm performance

### Mechanism 2
- Claim: Language conditioning improves performance by allowing the model to learn language-specific features and predict characters from the correct alphabet
- Mechanism: Language embeddings are concatenated to encoder/decoder inputs, providing explicit language information that helps the model select appropriate phonetic rules and character sets
- Core assumption: Providing explicit language information through conditioning helps the model learn language-specific features and improves transcription accuracy
- Evidence anchors:
  - [abstract] "Furthermore, we demonstrate that incorporating language information significantly enhances performance."
  - [section] "We show that language conditioning has a positive impact on performance, while the amount of improvement varies across different languages."
  - [corpus] Missing - corpus neighbors don't discuss language conditioning techniques
- Break condition: If language classification is highly accurate without conditioning, or if the model can implicitly identify languages from input features, explicit conditioning may provide minimal benefit

### Mechanism 3
- Claim: Language self-conditioning through multi-task learning improves ALT by making language identification an explicit learning objective
- Mechanism: The encoder output is used to predict language ID, and this prediction is used as additional input to the decoder, forcing the model to learn language-specific features while performing ALT
- Core assumption: Making language identification an explicit learning objective improves the model's ability to learn language-specific features that benefit ALT
- Evidence anchors:
  - [section] "To gain deeper insights into the model's capability to identify the correct language, we make the language identification ability measurable by taking a multi-task learning approach."
  - [section] "Compared to the results of the multilingual model, the self-conditioned model is able to perform the additional language classification task without compromising ALT performance."
  - [corpus] Missing - corpus neighbors don't discuss multi-task learning for language identification
- Break condition: If language classification accuracy is already high without self-conditioning, or if the auxiliary task interferes with ALT performance, self-conditioning may not provide benefits

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: CTC loss enables alignment-free training by allowing the model to predict sequences without requiring explicit alignment between input frames and output tokens
  - Quick check question: How does CTC loss handle repeated characters and blank tokens in the output sequence?

- Concept: Transformer architecture with positional encoding
  - Why needed here: Transformers with positional encoding can capture long-range dependencies in both acoustic features and sequential lyrics tokens
  - Quick check question: Why is positional encoding necessary in transformer models that otherwise lack recurrence or convolution?

- Concept: Hybrid CTC/Attention architecture
  - Why needed here: Combining CTC (for alignment) and Attention (for sequence modeling) provides complementary benefits for lyrics transcription
  - Quick check question: What are the specific advantages of using both CTC and Attention losses compared to using either alone?

## Architecture Onboarding

- Component map: Mel-spectrogram -> CNNBlock -> Transformer Encoder -> Transformer Decoder -> FCctc/FCs2s -> Output (character probabilities)
- Critical path: Mel-spectrogram -> CNNBlock -> Transformer Encoder -> Transformer Decoder -> FC layers -> CTC/Attention outputs
- Design tradeoffs: Larger vocabulary size (91 characters) increases model capacity but also increases computational cost and potential for confusion between similar characters
- Failure signatures: High WER indicates poor character prediction; language confusion suggests inadequate language-specific feature learning; poor alignment suggests CTC component issues
- First 3 experiments:
  1. Train monolingual models for each language to establish baseline performance
  2. Train multilingual model and compare against monolingual baselines
  3. Implement and test language conditioning methods (Enc-Cond, Dec-Cond, EncDec-Cond) to evaluate impact on performance

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset composition is imbalanced with English comprising 60% of training data while other languages have significantly fewer samples
- Evaluation is limited to a single test set (MultiLang Jamendo), which may not represent diverse singing styles and recording conditions
- Language classification errors are high, with non-English languages frequently misclassified as English

## Confidence
- Multilingual model superiority: High - consistent WER improvements across all tested languages
- Language conditioning mechanism: Medium - improvements are uneven across languages with minimal impact on English
- Self-conditioned model performance: Low - limited discussion of trade-offs between language classification accuracy and ALT performance

## Next Checks
1. **Dataset Balance Evaluation**: Test the multilingual model with balanced language distributions to determine whether performance improvements persist when English no longer dominates training data

2. **Generalization Testing**: Evaluate models on additional test sets representing different singing styles, recording qualities, and demographic variations to assess multilingual advantages beyond MultiLang Jamendo dataset

3. **Zero-Shot Language Transfer**: Test the multilingual model's ability to transcribe languages not seen during training to determine whether learned cross-linguistic patterns enable genuine transfer learning or merely improve low-resource language performance within trained language set