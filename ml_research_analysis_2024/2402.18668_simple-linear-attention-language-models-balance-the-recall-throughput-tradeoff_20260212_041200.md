---
ver: rpa2
title: Simple linear attention language models balance the recall-throughput tradeoff
arxiv_id: '2402.18668'
source_url: https://arxiv.org/abs/2402.18668
tags:
- attention
- linear
- size
- language
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores the tradeoff between recall ability and memory
  efficiency in language models. While attention-based models excel at recall, they
  suffer from high memory consumption due to the growing KV-cache.
---

# Simple linear attention language models balance the recall-throughput tradeoff

## Quick Facts
- arXiv ID: 2402.18668
- Source URL: https://arxiv.org/abs/2402.18668
- Authors: Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher Ré
- Reference count: 40
- Key outcome: Introduces BASED, a hybrid architecture combining linear and sliding window attention, achieving 24x higher throughput than FlashAttention-2 while matching full attention quality on recall-intensive tasks.

## Executive Summary
This paper identifies a fundamental tradeoff between recall ability and memory efficiency in language models, demonstrating that attention-based models excel at recall but suffer from high memory consumption due to the growing KV-cache. The authors propose BASED, a simple architecture combining linear and sliding window attention that can traverse the Pareto frontier of this tradeoff. By varying hyperparameters, BASED achieves full attention quality at one end while enabling small state sizes at the other, matching the strongest sub-quadratic models in perplexity while outperforming them on real-world recall-intensive tasks by 10.36 accuracy points.

## Method Summary
The authors develop BASED by combining linear attention (using 2nd-order Taylor series feature maps) with sliding window attention and gated convolutions in a hybrid architecture. They train models up to 1.3 billion parameters on the Pile dataset and evaluate on both synthetic recall tasks and real-world benchmarks. To achieve high throughput, they develop IO-aware algorithms that fuse feature map computation with causal dot products in a single CUDA kernel, reducing memory movement between GPU memory hierarchies. The approach enables 24x higher generation throughput than FlashAttention-2 while maintaining strong recall performance.

## Key Results
- BASED achieves 24x higher throughput than FlashAttention-2 during generation with 1.3B parameter models
- Matches strongest sub-quadratic models in perplexity while outperforming them on recall-intensive tasks by 10.36 accuracy points
- Demonstrates a fundamental tradeoff between recurrent state size and recall capacity that applies broadly across architecture classes
- Achieves full attention quality at one end of the Pareto frontier while enabling small state sizes at the other

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining linear and sliding window attention balances recall and memory efficiency by leveraging complementary strengths.
- Mechanism: Linear attention provides long-range global interactions with a fixed-size recurrent state, while sliding window attention handles precise local token shifts and comparisons with small, bounded memory use.
- Core assumption: Linear attention lacks precision for local token shifts and comparisons, while sliding window attention alone cannot capture long-range dependencies.
- Evidence anchors:
  - [abstract]: "We propose BASED a simple architecture combining linear and sliding window attention."
  - [section]: "We find that linear attention alone struggles to solve associative recall... In sliding window attention, associative recall range is limited by the width of the windows."
  - [corpus]: Weak. Corpus neighbors mention hybrid linear attention and minimal sliding window size but no direct empirical support for this specific combination.
- Break condition: If either component's limitations are not as described, or if the combination fails to exploit their complementarity in practice.

### Mechanism 2
- Claim: Taylor series approximation of softmax enables efficient linear attention without sacrificing recall capacity.
- Mechanism: The 2nd-order Taylor expansion (1 + q⊤k + (q⊤k)²/2) approximates exp(q⊤k/√d) while allowing O(Nd²) complexity and fixed-size state; projecting to lower dimension (d′) controls state size without changing parameter count.
- Core assumption: The Taylor approximation is sufficiently accurate for recall tasks and hardware-friendly to compute.
- Evidence anchors:
  - [abstract]: "We use the 2nd-order Taylor series feature map... This naïvely requires O(Nd³) time and space complexity... We can tradeoff efficiency for recall capacity by projecting queries and keys to smaller dimensions."
  - [section]: "The Taylor series feature map, along with the simple ϕPosELU and ϕReLU feature maps, sits at the Pareto frontier."
  - [corpus]: Weak. No direct mention of Taylor-based linear attention in neighbors; evidence comes from paper's internal experiments.
- Break condition: If the Taylor approximation loses too much precision for recall, or if hardware efficiency gains are not realized in practice.

### Mechanism 3
- Claim: IO-aware algorithm design dramatically improves throughput for linear attention by minimizing memory movement between GPU memory hierarchies.
- Mechanism: Fusing feature map computation with causal dot product in a single CUDA kernel reduces HBM-to-SRAM data movement by O(Nd²) bytes and SRAM-to-register movement by O(Nd³) bytes; small sliding windows (64-128) maximize tensor core utilization.
- Evidence anchors:
  - [abstract]: "We further develop IO-aware algorithms that enable BASED to provide 24× higher throughput on language generation than FlashAttention-2."
  - [section]: "Despite the theoretical efficiency benefits, linear attention implementations are often slower than well-optimized attention implementations... To make our attention competitive... we provide hardware-efficient CUDA algorithms."
  - [corpus]: Weak. Neighbors discuss efficient attention but not this specific IO-aware kernel design.
- Break condition: If memory hierarchy assumptions change (e.g., new GPU architectures), or if kernel fusion overhead outweighs benefits.

## Foundational Learning

- Concept: **Recall vs. throughput tradeoff in sequence models**
  - Why needed here: The entire paper is built on the premise that there is a fundamental tradeoff between a model's ability to recall information from long sequences and its computational/memory efficiency.
  - Quick check question: If attention-based models excel at recall but suffer from high memory consumption, what is the key bottleneck during inference?

- Concept: **Linear attention and feature maps**
  - Why needed here: BASED relies on linear attention as a core component, and understanding how different feature maps approximate softmax is crucial for grasping the design choices.
  - Quick check question: How does replacing softmax with a feature map like the 2nd-order Taylor series change the computational complexity of attention?

- Concept: **GPU memory hierarchy and kernel optimization**
  - Why needed here: The IO-aware algorithms for BASED are designed to exploit specific GPU memory characteristics (HBM, SRAM, registers) and compute units (tensor cores).
  - Quick check question: Why does fusing multiple operations in a single CUDA kernel reduce latency compared to separate kernels?

## Architecture Onboarding

- Component map:
  - **Taylor Linear Attention Layer**: Projects queries/keys to d′ dimensions, applies 2nd-order Taylor feature map (ϕ(q)⊤ϕ(k) = 1 + q⊤k + (q⊤k)²/2), computes causal dot product with fused kernel, maintains fixed-size KV state
  - **Sliding Window Attention Layer**: Small windows (64-128 tokens), exact softmax attention within window, exploits tensor cores, bounded memory
  - **Gated Convolution Layer**: Short convolutions (filter size 3), gating mechanism, expands dimensionality by factor 4, improves local precision
  - **Hybrid Architecture**: Mix of ~20% linear attention, ~20% sliding window, ~60% gated convolution layers

- Critical path: For each token during generation, compute linear attention (global, fixed-state), sliding window attention (local, bounded), gated convolution (local, short-range), combine results, update KV state only for linear attention

- Design tradeoffs:
  - Feature dimension d′ vs. recall capacity: Higher d′ increases state size but improves recall; diminishing returns beyond d′=24-32
  - Sliding window size: Larger windows improve recall but increase memory and latency non-linearly; 64-128 chosen for tensor core efficiency
  - Hybrid ratio: More linear attention improves long-range modeling but may hurt local precision; more convolution improves local but not long-range

- Failure signatures:
  - **Low recall**: Check if d′ is too small, sliding window too narrow, or insufficient hybrid balance
  - **High memory usage**: Linear attention state growing unexpectedly; sliding window larger than intended
  - **Slow inference**: Kernel fusion failing; tensor cores underutilized due to window size mismatch

- First 3 experiments:
  1. **Unit test Taylor linear attention kernel**: Verify output matches PyTorch reference for small sequences; measure memory movement
  2. **Ablation sliding window size**: Train with w=64, 128, 256; measure recall accuracy and prefill latency
  3. **Hybrid ratio sweep**: Train models with varying linear/conv/SW ratios (e.g., 10/80/10, 20/60/20, 30/50/20); measure perplexity and recall on MQAR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the recall capacity of BASED scale with increasing sequence length beyond the evaluated 1,024 tokens?
- Basis in paper: [inferred] The paper focuses on evaluating models up to 1,024 tokens in length, but does not explore longer sequences. The tradeoff between recurrent state size and recall ability might shift with longer sequences.
- Why unresolved: The paper only provides results for a limited sequence length range, leaving the behavior for longer sequences unexplored.
- What evidence would resolve it: Experiments evaluating BASED's recall performance on sequences significantly longer than 1,024 tokens, such as 4,096 or 8,192 tokens, would demonstrate how the recall-memory tradeoff evolves with sequence length.

### Open Question 2
- Question: How does the choice of feature map in linear attention affect the efficiency and quality of BASED compared to other efficient architectures?
- Basis in paper: [explicit] The paper mentions that different feature maps (e.g., Taylor, CosFormer, Performer) can impact the recall capacity and parameter count of BASED. However, a detailed comparison with other efficient architectures using various feature maps is lacking.
- Why unresolved: While the paper provides ablations on feature maps within BASED, it does not compare the performance of different feature maps across architectures like Mamba or Hyena.
- What evidence would resolve it: A comprehensive study comparing the performance of different feature maps in linear attention across multiple efficient architectures on a common benchmark would reveal the impact of feature map choice on overall efficiency and quality.

### Open Question 3
- Question: How does the performance of BASED on real-world tasks change when trained with different data augmentation techniques or larger datasets?
- Basis in paper: [inferred] The paper evaluates BASED on several real-world tasks using models trained on the Pile dataset. However, it does not explore the impact of data augmentation or larger datasets on performance.
- Why unresolved: The paper only considers one training dataset and does not investigate how data augmentation or larger datasets might influence BASED's ability to generalize and perform on real-world tasks.
- What evidence would resolve it: Training BASED on datasets with various data augmentation techniques or significantly larger than the Pile and evaluating its performance on the same real-world tasks would reveal the impact of data diversity and quantity on its effectiveness.

## Limitations

- The fundamental tradeoff between recall capacity and recurrent state size is demonstrated primarily for the specific architectures tested (linear, sliding window, recurrent) and may not generalize to all model classes.
- The 24x throughput improvement is based on NVIDIA H100 GPU architecture and may not translate directly to other hardware platforms or future GPU generations.
- The Taylor approximation's accuracy for complex recall tasks is validated experimentally but lacks theoretical guarantees, potentially limiting its effectiveness for more sophisticated dependencies.

## Confidence

**High Confidence**: The experimental methodology for training and evaluating the BASED architecture is well-specified and reproducible. The synthetic recall tasks (MQAR) are clearly defined, and the downstream task evaluations follow standard protocols. The hybrid architecture design and its effects on the recall-memory tradeoff are directly supported by the presented experiments.

**Medium Confidence**: The claim that BASED matches or exceeds the performance of other sub-quadratic models on both perplexity and recall tasks is well-supported, but the comparison is limited to a specific set of baselines. The IO-aware algorithm optimizations and their performance benefits are demonstrated empirically but lack detailed implementation specifications that would enable independent verification.

**Low Confidence**: The assertion that the recall-memory tradeoff is "fundamental" across architecture classes is not fully substantiated. The evidence is primarily based on a limited set of architectural components and synthetic tasks. The paper would benefit from broader architectural exploration and more diverse task types to strengthen this claim.

## Next Checks

1. **Ablation Study on Taylor Approximation Order**: Systematically vary the Taylor approximation order (1st, 2nd, 3rd) and measure the impact on recall accuracy and computational efficiency. This would quantify the precision-recall tradeoff inherent in the approximation and identify the optimal order for different task types.

2. **Cross-Architecture Tradeoff Analysis**: Apply the same analysis framework to other model classes (RNNs, state-space models, different attention variants) to test whether the recall-memory tradeoff is truly fundamental or specific to the tested architectures. This would involve implementing similar synthetic recall tasks and measuring the tradeoff curves for each architecture.

3. **Hardware Portability Assessment**: Benchmark the IO-aware algorithms on multiple GPU architectures (A100, H100, and potentially future architectures) to determine the generalizability of the performance improvements. This would identify which aspects of the optimization are architecture-specific and which are more broadly applicable.