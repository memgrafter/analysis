---
ver: rpa2
title: 'PMLBmini: A Tabular Classification Benchmark Suite for Data-Scarce Applications'
arxiv_id: '2409.01635'
source_url: https://arxiv.org/abs/2409.01635
tags:
- datasets
- learning
- dataset
- benchmark
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PMLBmini introduces a benchmark suite of 44 small tabular datasets\
  \ (\u2264500 samples) for binary classification, addressing the lack of data-scarce\
  \ benchmarks. It compares AutoML frameworks, deep learning models, and logistic\
  \ regression, finding that logistic regression often performs similarly to state-of-the-art\
  \ methods (55% of datasets)."
---

# PMLBmini: A Tabular Classification Benchmark Suite for Data-Scarce Applications

## Quick Facts
- arXiv ID: 2409.01635
- Source URL: https://arxiv.org/abs/2409.01635
- Reference count: 40
- Primary result: Benchmark suite of 44 small tabular datasets (≤500 samples) for binary classification, finding logistic regression often matches state-of-the-art methods

## Executive Summary
PMLBmini introduces a benchmark suite specifically designed for data-scarce applications, containing 44 small tabular datasets with ≤500 samples each for binary classification. The suite evaluates AutoML frameworks, deep learning models, and logistic regression, revealing that simple logistic regression performs similarly to complex methods on 55% of datasets. AutoML excels on more complex datasets while deep learning shows advantages when datasets resemble their meta-training data. The benchmark includes a Python interface for easy evaluation and comprehensive meta-feature analysis, providing researchers with tools to understand when different methods succeed or fail in low-data regimes.

## Method Summary
The authors constructed PMLBmini by selecting 44 binary classification datasets with sample sizes ≤500 from the Penn Machine Learning Benchmark repository. They evaluated five baseline estimators: logistic regression, AutoPrognosis, AutoGluon, TabPFN, and HyperFast, using 10-fold cross-validation with a 3-hour time limit per dataset. Performance was measured using the Area Under the ROC Curve (AUC) metric. The suite includes PyMFE for extracting 3932 meta-features per dataset, enabling comprehensive analysis of when different methods succeed. All datasets, implementations, and results are publicly available through a Python interface.

## Key Results
- Logistic regression performs similarly to AutoML and deep learning on 55% of datasets in low-data regimes
- AutoML methods show superior performance on datasets requiring more complex classifiers
- Deep learning methods perform well when datasets have feature distributions similar to their meta-training data
- Simple baselines remain competitive in data-scarce applications, questioning the necessity of complex methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logistic regression performs similarly to AutoML and deep learning on 55% of small tabular datasets because overfitting risk outweighs complexity benefits in low-data regimes.
- Mechanism: With small sample sizes, high-capacity models (AutoML ensembles, deep neural networks) are prone to overfitting due to increased model complexity and limited data. Logistic regression, being a simple linear model, has fewer parameters relative to sample size, reducing overfitting risk.
- Core assumption: Model complexity and overfitting risk are primary factors in performance degradation when sample sizes are ≤500.
- Evidence anchors:
  - [abstract]: "logistic regression often performs similarly to state-of-the-art methods (55% of datasets)"
  - [section]: "logistic regression shows a similar discriminative performance to AutoML and deep learning approaches on 55% of the datasets"
  - [corpus]: No direct evidence; corpus focuses on evaluations but doesn't explicitly analyze overfitting mechanisms.
- Break Condition: If sample size increases significantly, complex models may outperform logistic regression as overfitting risk diminishes.

### Mechanism 2
- Claim: AutoML methods are better suited for more complex datasets because they can select and ensemble more complex classifiers when needed.
- Mechanism: AutoML frameworks perform algorithm selection and hyperparameter optimization, potentially identifying and utilizing more complex models (ensembles, non-linear methods) that can capture intricate patterns in complex datasets. This is particularly valuable when simpler models like logistic regression underfit.
- Core assumption: Dataset complexity (measured by meta-features like clustering coefficient, nearest neighbor ratios) correlates with the need for more complex models.
- Evidence anchors:
  - [abstract]: "AutoML excels on complex datasets"
  - [section]: "AutoML frameworks appear to work better when more complex classifiers are needed"
  - [corpus]: No direct evidence; corpus focuses on evaluations but doesn't explicitly analyze dataset complexity relationships.
- Break Condition: If dataset complexity is low, AutoML may select overly complex models that overfit without performance gains.

### Mechanism 3
- Claim: Deep learning methods perform well on datasets with feature distributions similar to their meta-training data because of transfer learning effects.
- Mechanism: TabPFN and HyperFast are pretrained on large tabular datasets. When test datasets have feature distributions (statistical properties, harmonic means, minimums) resembling the meta-training data, these models can leverage learned representations and priors, leading to better performance.
- Core assumption: Pretrained deep learning models retain useful priors that transfer to test datasets with similar feature distributions.
- Evidence anchors:
  - [abstract]: "deep learning performs well on datasets resembling its meta-training data"
  - [section]: "pretrained deep neural networks may be better suited for datasets with feature distributions that resemble their meta-training data"
  - [corpus]: No direct evidence; corpus focuses on evaluations but doesn't explicitly analyze feature distribution similarities.
- Break Condition: If test datasets have feature distributions significantly different from meta-training data, transfer learning benefits diminish.

## Foundational Learning

- Concept: Understanding overfitting in machine learning
  - Why needed here: Critical for interpreting why simple models sometimes outperform complex ones in low-data regimes
  - Quick check question: What happens to model performance when the number of parameters approaches the number of training samples?

- Concept: Meta-feature analysis and dataset characterization
  - Why needed here: Essential for understanding when different methods succeed or fail based on dataset properties
  - Quick check question: How would you quantify dataset complexity using measures like clustering coefficient or nearest neighbor ratios?

- Concept: Transfer learning and meta-learning in deep neural networks
  - Why needed here: Important for understanding why pretrained models might perform well on datasets with similar feature distributions
  - Quick check question: What role do priors learned during meta-training play when applying a pretrained model to a new dataset?

## Architecture Onboarding

- Component map:
  - Dataset loader -> Baseline estimators -> Comparison engine -> Meta-feature extractor -> Analysis tools

- Critical path:
  1. Load dataset collection
  2. Initialize baseline estimators
  3. Perform cross-validation comparisons
  4. Extract meta-features
  5. Compute performance difference correlations

- Design tradeoffs:
  - Runtime vs. thoroughness: 3-hour time limit per dataset balances evaluation completeness with computational feasibility
  - Meta-feature complexity: 3932 meta-features provide comprehensive analysis but may include redundant information
  - Dataset selection: Focusing on ≤500 samples ensures low-data regime relevance but limits generalizability to larger datasets

- Failure signatures:
  - Runtime errors: May indicate insufficient computational resources or incompatible estimator implementations
  - Poor performance correlations: Could suggest meta-feature extraction issues or lack of meaningful relationships between dataset properties and method performance
  - Inconsistent results across runs: Might indicate random seed issues or non-deterministic estimator implementations

- First 3 experiments:
  1. Run comparison with default settings on a single dataset to verify basic functionality
  2. Perform meta-feature analysis on a small subset of datasets to check extraction pipeline
  3. Test custom estimator integration by implementing a simple baseline and comparing against existing methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific dataset properties (beyond feature distribution) most strongly predict when AutoML outperforms logistic regression in the low-data regime?
- Basis in paper: [explicit] The authors found that AutoML methods are better suited for more complex datasets that need more complex classifiers, but this conclusion is based on correlations with meta-features like the ratio of intra- and extraclass nearest neighbor distance and clustering coefficient.
- Why unresolved: The meta-feature analysis only used bivariate correlations and didn't account for confounding factors or interactions between meta-features. The exact combination of dataset properties that make AutoML advantageous remains unclear.
- What evidence would resolve it: Controlled experiments varying multiple dataset properties simultaneously (e.g., class overlap, feature correlation, noise levels) while measuring AutoML vs. logistic regression performance would clarify which combinations of properties predict AutoML superiority.

### Open Question 2
- Question: How does the performance of logistic regression with meta-learned L2-regularization hyperparameters compare to AutoML and deep learning methods across all datasets?
- Basis in paper: [explicit] The authors provide the best L2-regularization hyperparameter (λ*) for each dataset, suggesting these could be used for meta-learning, but they don't evaluate logistic regression with these specific hyperparameters in their comparisons.
- Why unresolved: The experiments used a fixed hyperparameter grid for logistic regression rather than the optimal λ* values. The potential performance gain from using dataset-specific optimal regularization is unknown.
- What evidence would resolve it: Re-running the logistic regression experiments using the provided λ* values for each dataset and comparing the results to AutoML and deep learning methods would show if logistic regression with proper regularization can close the performance gap.

### Open Question 3
- Question: At what sample size threshold do more complex methods (AutoML/deep learning) begin to consistently outperform simple baselines like logistic regression?
- Basis in paper: [explicit] The authors observed a trend for all methods to perform better with larger sample sizes, but they don't identify a specific sample size where complex methods become consistently superior.
- Why unresolved: The analysis focused on datasets with sample sizes ≤ 500, and while performance trends were noted, the exact sample size at which complex methods reliably surpass logistic regression wasn't determined.
- What evidence would resolve it: Extending the benchmark to include datasets with sample sizes > 500 and systematically testing the performance crossover point would identify when complex methods become the better choice.

## Limitations

- Benchmark focuses exclusively on binary classification tasks with ≤500 samples, limiting generalizability
- 3-hour time limit per dataset may constrain AutoML method potential for thorough hyperparameter optimization
- Correlation analysis between meta-features and performance differences doesn't establish causation

## Confidence

- **High**: Logistic regression performing similarly to complex methods on 55% of datasets
- **Medium**: AutoML superiority on complex datasets
- **Medium**: Deep learning performance linked to meta-training data similarity

## Next Checks

1. **Scale validation**: Test the benchmark suite on datasets with 500-2000 samples to assess performance trends as sample size increases
2. **Time extension**: Repeat key experiments with extended time limits (6-12 hours) to evaluate if current 3-hour limits constrain AutoML method potential
3. **Causal analysis**: Design controlled experiments varying specific meta-features to establish causal relationships between dataset properties and method performance