---
ver: rpa2
title: Data-Efficient Learning with Neural Programs
arxiv_id: '2406.06246'
source_url: https://arxiv.org/abs/2406.06246
tags:
- ised
- neural
- learning
- program
- programs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ISED, a method for learning neural programs
  where a DNN is composed with a program (which may be a black-box like a Python function
  or an LLM call). The key idea is to sample from the DNN's output distribution, run
  the black-box program on these samples, and then aggregate probabilities of samples
  that yield the same program output.
---

# Data-Efficient Learning with Neural Programs

## Quick Facts
- arXiv ID: 2406.06246
- Source URL: https://arxiv.org/abs/2406.06246
- Authors: Alaia Solko-Breslin; Seewon Choi; Ziyang Li; Neelay Velingker; Rajeev Alur; Mayur Naik; Eric Wong
- Reference count: 40
- Key outcome: ISED outperforms purely neural and neurosymbolic baselines on 16 tasks, demonstrating better sample and data efficiency

## Executive Summary
This paper introduces ISED, a method for learning neural programs where a DNN is composed with a program (which may be a black-box like a Python function or an LLM call). The key idea is to sample from the DNN's output distribution, run the black-box program on these samples, and then aggregate probabilities of samples that yield the same program output. This forms a "summary program" that can be differentiated to update the DNN. The method generalizes neurosymbolic learning and is applicable to arbitrary programs.

ISED is evaluated on 16 tasks including 4 new GPT-4 based tasks (leaf classification, scene recognition) and 12 tasks from neurosymbolic literature. Results show ISED outperforms purely neural and neurosymbolic baselines on new tasks, and often beats REINFORCE-based and gradient estimation baselines on others. ISED is also more sample- and data-efficient than these alternatives, though it struggles with high-dimensional input spaces to the program.

## Method Summary
ISED learns neural programs by sampling from the DNN's output distribution and running the black-box program on these samples, then differentiating through a "summary program" formed by aggregating probabilities of samples yielding the same program output. The DNN outputs a probability distribution over possible inputs to the black-box program P. ISED samples k inputs from this distribution, runs P on each sample, and aggregates probabilities for samples resulting in the same output. This forms a summary of P as a set of rules, which is differentiable.

The aggregation method (min-max or add-mult semiring) provides stronger learning signals than REINFORCE's reward-weighted approach, especially for programs with multiple inputs. ISED's data-efficiency comes from avoiding the need to train additional neural networks to approximate P's behavior or gradients, unlike A-NeSI.

## Key Results
- ISED outperforms purely neural and neurosymbolic baselines on 4 new GPT-4 based tasks
- ISED often beats REINFORCE-based and gradient estimation baselines on 12 neurosymbolic tasks
- ISED achieves comparable accuracy but in a more data- and sample-efficient manner than A-NeSI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ISED learns neural programs by sampling from the DNN's output distribution and running the black-box program on these samples, then differentiating through a "summary program" formed by aggregating probabilities of samples yielding the same program output.
- Core assumption: The aggregated probabilities from k samples provide a good approximation of the complete distribution of P's input.
- Evidence anchors:
  - [abstract]: "sample from the DNN's output distribution, run the black-box program on these samples, and then aggregate probabilities of samples that yield the same program output"
  - [section 3.1]: "ISED samples symbols from the probability distribution predicted by the neural network Mθ, evaluates P on each sample, and takes the gradient across this partial summary of P"
- Break condition: When P has very high-dimensional input spaces, the number of samples needed to approximate the distribution becomes computationally prohibitive, or when P's output space is continuous and hard to discretize.

### Mechanism 2
- Claim: ISED's aggregation method (min-max or add-mult semiring) provides stronger learning signals than REINFORCE's reward-weighted approach, especially for programs with multiple inputs.
- Core assumption: The semiring-based aggregation captures the joint probability structure better than simple reward weighting.
- Evidence anchors:
  - [section 3.1]: "REINFORCE rewards sampled symbols that resulted in the correct output through optimizing the log probability of the samples, weighted by reward values. This weighted-sum style estimation provides a weaker learning signal compared to WMC used by ISED"
  - [section 4.4]: Performance comparison shows ISED outperforming REINFORCE-based methods on tasks with multiple inputs
- Break condition: When P's inputs are independent or when the reward structure of REINFORCE happens to align well with the task, the advantage may diminish.

### Mechanism 3
- Claim: ISED's data-efficiency comes from avoiding the need to train additional neural networks to approximate P's behavior or gradients, unlike A-NeSI.
- Core assumption: Forward evaluation of P combined with sampling is more data-efficient than training surrogate neural networks to approximate P.
- Evidence anchors:
  - [abstract]: "ISED achieves comparable accuracy but in a more data- and sample-efficient manner"
  - [section 4.5]: "ISED learns more quickly than A-NeSI, even for simple tasks" - attributed to A-NeSI training 2 additional neural models
- Break condition: When P is extremely complex and expensive to evaluate, the cost of forward evaluation may outweigh the cost of training a surrogate.

## Foundational Learning

- Concept: Probability distributions and sampling from categorical distributions
  - Why needed here: ISED samples from the DNN's predicted probability distribution over possible inputs to the black-box program
  - Quick check question: If a DNN outputs [0.1, 0.6, 0.3] for a digit classification task, what's the probability of sampling the digit '1'?

- Concept: Differentiable programming and gradient computation through composite models
  - Why needed here: ISED differentiates through the summary program formed by aggregating sampled outputs
  - Quick check question: Why can't we directly compute gradients through a black-box program like GPT-4, and how does ISED's approach solve this?

- Concept: Semirings and their application to probabilistic logic
  - Why needed here: ISED uses either min-max or add-mult semirings to aggregate probabilities in the summary program
  - Quick check question: What's the difference between using min-max vs add-mult semiring in terms of how probabilities are combined?

## Architecture Onboarding

- Component map: DNN (Mθ) → Probability distribution → Sampler → Black-box program (P) → Aggregator → Loss function → Optimizer
- Critical path: The path from DNN output through sampling and program execution to gradient computation
- Design tradeoffs: Sampling count (k) vs accuracy - more samples give better approximation but slower training
- Failure signatures: 
  - Poor accuracy on high-dimensional input tasks
  - Slow convergence when P is expensive to evaluate
  - Degraded performance when P's output space is continuous and hard to discretize
- First 3 experiments:
  1. Implement ISED on sum2 MNIST task (digits 0-2) with small sample count to verify basic functionality
  2. Compare ISED with REINFORCE on sum4 task to demonstrate sample efficiency advantage
  3. Test ISED with both min-max and add-mult semirings on a simple task to understand their differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ISED be adapted to handle high-dimensional input spaces to the program P more effectively?
- Basis in paper: [explicit] The paper explicitly mentions that the main limitation of ISED is the difficulty of scaling with the dimensionality of the space of inputs to the program P.
- Why unresolved: The paper suggests exploring better sampling techniques and combining white-box and black-box methods, but does not provide concrete solutions or experimental results.
- What evidence would resolve it: Experimental results showing improved performance of ISED on tasks with high-dimensional input spaces after implementing suggested techniques like advanced sampling methods or hybrid approaches.

### Open Question 2
- Question: How does the choice of semiring (min-max vs. add-mult) impact the performance of ISED on different tasks?
- Basis in paper: [explicit] The paper mentions that ISED requires that ⊗ and ⊕ represent either min and max or mult and add respectively, and refers to these two options as the min-max and add-mult semirings.
- Why unresolved: While the paper states that ISED uses the min-max semiring for HWF and the add-mult semiring for other tasks, it does not provide a detailed analysis of the impact of this choice on performance across different tasks.
- What evidence would resolve it: A comprehensive study comparing the performance of ISED with different semiring choices across various benchmark tasks, including an analysis of the reasons behind the observed differences.

### Open Question 3
- Question: Can ISED be extended to handle structured outputs (e.g., sequences, trees) more effectively?
- Basis in paper: [explicit] The paper defines structural mappings for various input types (discrete, float, permutation, tuple, list) but does not explicitly discuss how ISED handles structured outputs.
- Why unresolved: The current implementation of ISED focuses on tasks with scalar outputs, and it is unclear how the algorithm would need to be modified to handle more complex output structures.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of ISED on tasks with structured outputs (e.g., sequence generation, tree-structured predictions) after extending the algorithm to handle such cases.

## Limitations
- Difficulty scaling to high-dimensional input spaces to the program P
- Limited validation on continuous-output tasks beyond discrete classification
- Unclear performance on real-world applications beyond benchmark tasks

## Confidence

- **High confidence**: The core mechanism of ISED (sampling from DNN distributions, running black-box programs, aggregating probabilities) is clearly specified and theoretically sound.
- **Medium confidence**: The empirical results showing ISED outperforms baselines on 16 tasks are compelling but may not generalize to all neurosymbolic learning scenarios.
- **Low confidence**: The claim that ISED can handle arbitrary black-box programs without retraining or approximation is not fully validated, especially for continuous output spaces.

## Next Checks

1. Test ISED on a continuous-output task (e.g., regression or reinforcement learning) to verify its applicability beyond discrete classification.
2. Compare ISED's sample efficiency with other neurosymbolic frameworks (e.g., Neuro-Symbolic Concept Learner) on a shared benchmark to validate relative performance.
3. Analyze the impact of sample count (k) on accuracy for high-dimensional tasks to quantify the scalability limitations.