---
ver: rpa2
title: 'GRAM: Global Reasoning for Multi-Page VQA'
arxiv_id: '2401.03411'
source_url: https://arxiv.org/abs/2401.03411
tags:
- software
- county
- reliability
- pages
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRAM extends existing single-page document models to efficiently
  handle multi-page documents without computationally-intensive pretraining. It uses
  a two-stage global-local encoder with document-level learnable tokens and designated
  layers to enable information exchange across pages, combined with bias adaptation
  to encourage utilization of document tokens.
---

# GRAM: Global Reasoning for Multi-Page VQA

## Quick Facts
- **arXiv ID**: 2401.03411
- **Source URL**: https://arxiv.org/abs/2401.03411
- **Reference count**: 40
- **Primary result**: Achieves 73.68% ANLS on MPDocVQA and 46.15% ANLS on DUDE multi-page DocVQA benchmarks

## Executive Summary
GRAM introduces a novel approach for extending single-page document models to handle multi-page documents without computationally intensive pretraining. The method leverages a two-stage global-local encoder that interleaves page-level understanding with document-level reasoning through learnable tokens. By introducing designated layers and a bias adaptation mechanism, GRAM enables efficient cross-page information flow while maintaining computational efficiency. The optional C-Former compression module provides additional flexibility for balancing quality and latency during decoding.

## Method Summary
GRAM extends single-page document models to multi-page documents through a two-stage global-local encoder architecture. Each block consists of a page-level sub-layer that processes both page-level and document-level tokens, followed by a doc-level sub-layer that performs cross-page reasoning using only document-level tokens. The model introduces document-level learnable tokens that serve as inter-page communication channels, with bias adaptation ensuring these tokens are properly utilized. An optional C-Former compression module can reduce sequence length before decoding, providing a controllable trade-off between quality and latency. The approach achieves state-of-the-art performance without requiring computationally expensive pretraining on multi-page data.

## Key Results
- Achieves 73.68% ANLS on MPDocVQA, outperforming DocFormerv2 concat baseline
- Achieves 46.15% ANLS on DUDE, setting new state-of-the-art performance
- Maintains efficient memory usage even for documents spanning hundreds of pages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GRAM extends single-page models to multi-page without retraining by interleaving local (page-level) and global (document-level) reasoning layers.
- Mechanism: Page-level sub-layers process each page independently, augmented with shared doc-level learnable tokens. A subsequent doc-level sub-layer fuses information across all doc-level tokens, enabling cross-page reasoning.
- Core assumption: A document can be decomposed into semantically coherent pages and that global reasoning can be achieved by passing information only through doc-level tokens rather than full-page attention.
- Evidence anchors:
  - [abstract]: "We leverage a single-page encoder for local page-level understanding, and enhance it with document-level designated layers and learnable tokens, facilitating the flow of information across pages for global reasoning."
  - [section]: "In the second stage, we prioritize computational efficiency by restricting self-attention solely to the global doc-level tokens."
  - [corpus]: "The memory consumption of DocFormerv2 concat reaches its maximum capacity for documents with only 20 pages, while our method efficiently processes documents spanning hundreds of pages."

### Mechanism 2
- Claim: Bias adaptation encourages the encoder to utilize newly introduced doc-level tokens.
- Mechanism: The bias method is modified so that attention weights for doc-level tokens are replaced with a positive constant (decaying across heads), increasing their effective relevance relative to page-level tokens.
- Core assumption: The encoder's bias mechanism significantly influences attention distribution and that doc-level tokens are initially under-utilized due to lack of pretraining.
- Evidence anchors:
  - [abstract]: "To enforce our model to utilize the newly introduced document tokens, we propose a tailored bias adaptation method."
  - [section]: "Specifically, we replace the values in the existing bias matrix that corresponds with attending to the doc-level tokens with fixed ones."
  - [corpus]: "Using constant bias has a negative effect on the results, suggesting this method is not flexible enough in maintaining a balance between the importance of page-level versus doc-level tokens."

### Mechanism 3
- Claim: C-Former provides a controllable trade-off between quality and latency by compressing the multi-page encoded sequence before decoding.
- Mechanism: A lightweight decoder processes the concatenation of doc-level outputs from all pages plus learnable compression tokens, outputting a fixed-length compressed representation for the decoder.
- Core assumption: The most salient information for answering questions can be distilled into a fixed-length sequence without significant loss.
- Evidence anchors:
  - [abstract]: "For additional computational savings during decoding, we introduce an optional compression stage using our compression-transformer (C-Former), reducing the encoded sequence length."
  - [section]: "The C-Former has the ability to revise the information across all pages and distill only the important details, required to correctly answer the question."
  - [corpus]: "The presented figures demonstrate that GRAM C-Former maintains a comparable memory footprint to the GRAM model."

## Foundational Learning

- **Concept**: Transformer self-attention complexity scales quadratically with sequence length.
  - Why needed here: Multi-page documents create long sequences; naive concatenation would be computationally infeasible.
  - Quick check question: If a document has 100 pages with 800 tokens each, what is the asymptotic complexity of a standard self-attention encoder?

- **Concept**: Learnable tokens in transformer architectures (e.g., CLS tokens) can carry task-specific global information.
  - Why needed here: Doc-level tokens serve as inter-page communication channels.
  - Quick check question: How are doc-level tokens initialized and how do they differ from standard CLS tokens?

- **Concept**: Bias adaptation in attention mechanisms can shift focus toward or away from specific tokens.
  - Why needed here: Ensures doc-level tokens are attended to despite lack of pretraining.
  - Quick check question: What effect does a large positive bias value have on attention scores for those tokens?

## Architecture Onboarding

- **Component map**: Input (OCR text, bounding boxes, visual embeddings, question tokens, per-page positional embeddings) → Global-Local Encoder (M blocks with page-level + doc-level sub-layers) → (C-Former) → Decoder → Output

- **Critical path**: Input → Global-Local Encoder → (C-Former) → Decoder → Output

- **Design tradeoffs**:
  - Memory vs. Accuracy: C-Former reduces memory at the cost of some accuracy
  - Computational Efficiency vs. Cross-Page Reasoning: Restricting doc-level attention only to doc-level tokens keeps computation low but relies on doc-level tokens to carry sufficient context
  - Flexibility vs. Complexity: Adding doc-level tokens increases model complexity but allows reuse of pretrained single-page models

- **Failure signatures**:
  - OOM errors during inference: Likely due to insufficient compression or too many pages without C-Former
  - Degraded single-page performance: Bias adaptation may be over-emphasizing doc-level tokens
  - Poor cross-page reasoning: Doc-level tokens may not be learning meaningful global representations

- **First 3 experiments**:
  1. Validate that single-page performance matches baseline when only one page is present
  2. Measure cross-page reasoning by testing multi-page queries with known answers in specific pages
  3. Test compression quality by varying C-Former output length and measuring ANLS vs. latency

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the main text. However, several areas for future research are implied by the discussion of limitations and the novel contributions, including exploring different initialization strategies for doc-level tokens, investigating alternative compression methods beyond C-Former, and examining the model's performance on documents with hundreds of pages.

## Limitations

- The effectiveness of doc-level tokens for cross-page reasoning hasn't been fully explored across different document types and page counts
- The C-Former compression module shows variable performance impact across datasets (0.6 ANLS drop on MPDocVQA vs 2.3 on DUDE)
- The bias adaptation mechanism relies on fixed constants that may not generalize well across different document types

## Confidence

- **High Confidence**: GRAM achieves state-of-the-art performance on multi-page DocVQA benchmarks with clear experimental evidence and baseline comparisons
- **Medium Confidence**: GRAM can process documents spanning hundreds of pages efficiently, though detailed runtime analysis is limited
- **Low Confidence**: Bias adaptation is necessary for doc-level token utilization, as alternative training strategies weren't thoroughly explored

## Next Checks

1. **Cross-page reasoning validation**: Test the model's ability to correctly answer questions requiring information from specific pages by creating controlled multi-page documents where the answer appears only in one designated page, then measure whether the model successfully retrieves and combines this information.

2. **Bias adaptation sensitivity analysis**: Systematically vary the constant values and decay patterns in the bias adaptation mechanism to determine the optimal configuration and test whether the performance gains are robust to these hyperparameter changes or represent overfitting to specific datasets.

3. **Compression quality vs. latency trade-off**: Conduct a comprehensive study varying the C-Former output length and measuring the full trade-off curve between ANLS performance and inference latency, particularly focusing on documents with different page counts to identify breaking points where compression becomes detrimental.