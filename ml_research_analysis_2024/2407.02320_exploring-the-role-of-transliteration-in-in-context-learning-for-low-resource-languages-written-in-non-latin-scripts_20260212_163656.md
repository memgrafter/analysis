---
ver: rpa2
title: Exploring the Role of Transliteration in In-Context Learning for Low-resource
  Languages Written in Non-Latin Scripts
arxiv_id: '2407.02320'
source_url: https://arxiv.org/abs/2407.02320
tags:
- cyrl
- arab
- script
- deva
- hani
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether transliteration improves in-context
  learning (ICL) performance of decoder-only large language models (LLMs) for low-resource
  languages in non-Latin scripts. The authors propose three prompt templates representing
  target-language text in (1) original script, (2) Latin script, or (3) both.
---

# Exploring the Role of Transliteration in In-Context Learning for Low-resource Languages Written in Non-Latin Scripts

## Quick Facts
- arXiv ID: 2407.02320
- Source URL: https://arxiv.org/abs/2407.02320
- Reference count: 20
- Primary result: Transliteration improves sequential labeling tasks (up to 25% F1) but has mixed effects on text classification depending on model type and size

## Executive Summary
This paper investigates whether transliteration improves in-context learning (ICL) performance of decoder-only large language models for low-resource languages written in non-Latin scripts. The authors propose three prompt templates representing target-language text in original script, Latin script, or both, and evaluate these methods on NER, SIB200, and Taxi1500 tasks using several LLM variants. Results show that transliteration benefits sequential labeling tasks significantly but has varying effects on text classification depending on model type and size. The effectiveness of transliteration depends on the script and model size, with larger models generally showing better performance.

## Method Summary
The paper evaluates three prompt template variants (SCRIPT {Orig}, SCRIPT {Latn}, SCRIPT {Combined}) using decoder-only LLMs (BLOOM, LLaMA2, Mistral) on three tasks: NER (WikiANN), SIB200 multilingual classification, and Taxi1500 multilingual text classification. Transliteration is performed using the Uroman tool for universal romanization. The evaluation uses 3-shot prompting for NER and Taxi1500, and 7-shot prompting for SIB200. For Taxi1500, semantically similar ICL samples are retrieved using average word embeddings from the Glot500 model. The primary metric is macro-F1 for NER and accuracy for the classification tasks.

## Key Results
- Transliteration improves sequential labeling tasks significantly, with up to 25% F1 improvement across all models
- SCRIPT {Combined} achieves over 29% and 4% improvement on SIB200 and Taxi1500 respectively compared to SCRIPT {Orig}
- Model performance varies by script and model size, with larger models generally showing better performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transliteration increases lexical overlap between source and target languages, enabling models to transfer knowledge across scripts
- Core assumption: The model's pretraining included sufficient exposure to Latin-script tokens to leverage transliteration effectively
- Evidence anchors: "a common script facilitates the model to transfer knowledge through increased lexical overlap"
- Break condition: If the model lacks Latin-script training data or the target language shares minimal vocabulary with Latin-script languages

### Mechanism 2
- Claim: Transliteration serves as an auxiliary input that helps models understand semantics when original script comprehension is limited
- Core assumption: The model can effectively combine information from multiple script representations during inference
- Evidence anchors: "when a word or an entire sentence in the original script is not well understood, the model should refer to its transliteration, and vice versa"
- Break condition: If the model cannot effectively fuse information from multiple script representations or if the auxiliary input introduces confusion

### Mechanism 3
- Claim: Transliteration benefits sequential labeling tasks more than text classification because proper nouns and shared vocabulary are more prevalent in sequence labeling
- Core assumption: The task type determines the effectiveness of transliteration based on the prevalence of shared vocabulary
- Evidence anchors: "all models benefit from transliterations for sequential labeling (with increases of up to 25%)"
- Break condition: If the task does not involve recognizing shared vocabulary or proper nouns

## Foundational Learning

- Concept: Script systems and transliteration
  - Why needed here: Understanding how non-Latin scripts are converted to Latin script is fundamental to grasping the transliteration approach
  - Quick check question: What is the difference between transliteration and translation, and why is this distinction important for the proposed method?

- Concept: In-context learning (ICL) mechanics
  - Why needed here: The paper relies on ICL rather than fine-tuning, so understanding how demonstrations work is crucial
  - Quick check question: How does the choice of demonstration examples affect ICL performance in multilingual settings?

- Concept: Lexical overlap and cross-lingual transfer
  - Why needed here: The proposed mechanism relies on the model leveraging shared vocabulary across languages
  - Quick check question: Why would converting to a common script (Latin) facilitate knowledge transfer between languages?

## Architecture Onboarding

- Component map: LLM (decoder-only) → Prompt templates (three variants) → Transliteration tool (Uroman) → Task-specific evaluation
- Critical path: Input text → Transliteration (if applicable) → Prompt formatting → Model inference → Evaluation metric calculation
- Design tradeoffs: SCRIPT {Combined} may confuse models vs. providing complementary information; SCRIPT {Latn} alone may lack semantic context
- Failure signatures: Performance degradation when transliteration introduces ambiguity; inconsistent improvements across tasks and models
- First 3 experiments:
  1. Compare SCRIPT {Orig} vs. SCRIPT {Latn} on NER task with BLOOM7B to verify lexical overlap benefits
  2. Test SCRIPT {Combined} vs. SCRIPT {Orig} on SIB200 with LLaMA2-7B to assess complementary information utilization
  3. Evaluate all three prompt types across model sizes (BLOOM3B, BLOOM7B, BLOOM1B) on Taxi1500 to identify scaling effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do larger decoder-only LLMs (beyond 7B parameters) perform on low-resource languages with non-Latin scripts when using transliteration?
- Basis in paper: The authors explicitly state they were limited to models with up to 7 billion parameters due to computing resource constraints
- Why unresolved: The paper only tested models up to 7B parameters, leaving the performance of larger models unexplored
- What evidence would resolve it: Experimental results comparing the effectiveness of transliteration across various model sizes from 7B to 175B+ parameters

### Open Question 2
- Question: Does the effectiveness of transliteration vary based on the specific language family or linguistic features of the target language?
- Basis in paper: The authors observe that model performance varies by different scripts but don't analyze this variation by language family
- Why unresolved: While the paper examines script-level differences, it doesn't investigate whether language families show different transliteration effectiveness patterns
- What evidence would resolve it: Detailed analysis comparing transliteration effectiveness across language families while controlling for script and model characteristics

### Open Question 3
- Question: What is the optimal balance between original script and transliterated text in the combined prompt template for different task types?
- Basis in paper: The authors propose three prompt templates but don't investigate the optimal ratio or formatting of the combined approach
- Why unresolved: The combined prompt uses both scripts together, but the paper doesn't test different proportions, ordering, or formatting
- What evidence would resolve it: Systematic experiments varying the ratio and presentation of original vs. transliterated text in the combined prompt template

## Limitations
- The evaluation relies on specific model versions and prompt configurations that are not fully specified
- Effectiveness appears highly dependent on target language's vocabulary overlap with Latin-script languages
- The SCRIPT {Combined} approach may introduce cognitive load for models when processing both script representations simultaneously

## Confidence

**High Confidence**: The core finding that transliteration improves sequential labeling tasks (NER) for low-resource languages has strong empirical support across multiple models and tasks.

**Medium Confidence**: The observation that text classification benefits vary by model type and size is supported by the data but lacks mechanistic explanation.

**Low Confidence**: The claim that lexical overlap is the primary mechanism driving transliteration benefits is speculative and based mainly on observed correlations.

## Next Checks

1. **Script Family Dependency Analysis**: Systematically evaluate transliteration effectiveness across different script families (logographic, abjad, abugida) to determine whether benefits correlate with lexical similarity to Latin-script languages.

2. **Model Architecture Ablation Study**: Compare decoder-only LLMs with encoder-decoder models and fine-tuned models on the same transliteration tasks to isolate whether the benefits are specific to ICL capabilities.

3. **Lexical Overlap Quantification**: Measure the actual lexical overlap between transliterated target languages and Latin-script languages in the pretraining corpus to determine if observed performance gains correlate with measurable vocabulary overlap.