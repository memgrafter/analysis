---
ver: rpa2
title: Development and Testing of Retrieval Augmented Generation in Large Language
  Models -- A Case Study Report
arxiv_id: '2402.01733'
source_url: https://arxiv.org/abs/2402.01733
tags:
- preoperative
- surgery
- llm-rag
- guidelines
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This case study developed and evaluated a Retrieval-Augmented Generation
  (RAG) pipeline for healthcare applications using preoperative medicine as a domain.
  The RAG framework incorporated 35 clinical guidelines and was tested against human-generated
  responses across 14 clinical scenarios.
---

# Development and Testing of Retrieval Augmented Generation in Large Language Models -- A Case Study Report

## Quick Facts
- arXiv ID: 2402.01733
- Source URL: https://arxiv.org/abs/2402.01733
- Reference count: 0
- Primary result: GPT4.0-RAG model achieved 91.4% accuracy in generating preoperative instructions, demonstrating non-inferior performance compared to human evaluators (86.3%, p=0.610)

## Executive Summary
This case study developed and evaluated a Retrieval-Augmented Generation (RAG) pipeline for healthcare applications using preoperative medicine as a domain. The RAG framework incorporated 35 clinical guidelines and was tested against human-generated responses across 14 clinical scenarios. The GPT4.0-RAG model achieved 91.4% accuracy, demonstrating non-inferior performance compared to human evaluators (86.3%, p=0.610). The system generated responses in 15-20 seconds, significantly faster than the 10 minutes required by humans. The study highlights the potential of LLM-RAG models to provide accurate, guideline-grounded preoperative instructions while maintaining efficiency and scalability in healthcare settings.

## Method Summary
The study developed a RAG pipeline using LangChain and Llamaindex Python frameworks to integrate 35 preoperative clinical guidelines. Clinical documents were converted to text, chunked using RecursiveCharacterTextSplitter, and embedded with OpenAI's text-embedding-ada-002 model. Pinecone served as the vector storage solution with 1536 dimensions and cosine similarity. The system was evaluated using 14 de-identified clinical scenarios across six assessment categories (fasting, carbohydrate loading, medications, healthcare team instructions, preoperative optimization, delay necessity) and compared against human-generated responses.

## Key Results
- GPT4.0-RAG achieved 91.4% accuracy compared to human evaluators' 86.3% (p=0.610)
- System response time was 15-20 seconds versus 10 minutes for human evaluators
- RAG system demonstrated capability across 14 diverse clinical scenarios covering various surgical complexities
- Accuracy threshold of 75% was exceeded, validating the system's clinical utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG enables domain-specific accuracy without retraining LLMs
- Mechanism: Vector storage with cosine similarity retrieves relevant clinical guidelines, which are passed to the LLM as context for response generation
- Core assumption: Clinical guidelines can be effectively chunked and embedded without losing critical decision-making information
- Evidence anchors:
  - [abstract] The RAG framework incorporated 35 clinical guidelines and were tested against human-generated responses
  - [section] "The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval"
  - [corpus] Weak evidence - corpus contains related RAG papers but no direct evidence about chunking effectiveness
- Break condition: If chunk size is too small, critical context is lost; if too large, retrieval precision drops and noise increases

### Mechanism 2
- Claim: Pinecone vector storage with cosine similarity optimizes healthcare document retrieval
- Mechanism: High-dimensional vectors (1536) capture semantic similarity between queries and guideline content
- Core assumption: Cosine similarity better captures semantic relationships in medical text than Euclidean distance
- Evidence anchors:
  - [section] "We used Pinecone as our cloud-based Vector Storage solution, configured with 1536 dimensions and cosine similarity as the loss metric"
  - [section] "Cosine similarity, particularly effective for semantic searches, is generally preferred for healthcare applications due to its focus on the angle between vectors, thereby emphasizing content similarity"
  - [corpus] Moderate evidence - corpus contains papers on vector storage but not specific to cosine similarity in healthcare
- Break condition: If embeddings don't capture domain-specific terminology, cosine similarity won't retrieve relevant documents

### Mechanism 3
- Claim: RAG reduces hallucinations while maintaining reasoning ability
- Mechanism: Retrieval provides factual grounding that constrains LLM generation, reducing fabricated content
- Core assumption: Retrieval-augmented context provides sufficient coverage of likely queries
- Evidence anchors:
  - [abstract] "The model can generate complex preoperative instructions across different clinical tasks with accuracy non-inferior to humans and low rates of hallucination"
  - [section] "The occurrence of factually incorrect or misleading data can pose significant risks"
  - [corpus] Moderate evidence - corpus includes papers on hallucination reduction but not specific to medical RAG
- Break condition: If retrieval fails to find relevant documents, LLM may still hallucinate or provide incomplete answers

## Foundational Learning

- Concept: Vector embeddings and semantic search
  - Why needed here: RAG relies on converting text to numerical representations for similarity matching
  - Quick check question: What is the difference between cosine similarity and Euclidean distance in vector space?

- Concept: Chunking strategies for text retrieval
  - Why needed here: Clinical guidelines must be split into retrievable pieces without losing context
  - Quick check question: How does chunk size affect retrieval precision and recall?

- Concept: LLM prompting with retrieved context
  - Why needed here: Retrieved documents must be effectively incorporated into the prompt
  - Quick check question: What prompt engineering techniques help LLMs utilize retrieved context?

## Architecture Onboarding

- Component map: Clinical guidelines → Text conversion → Chunking → Embedding → Vector storage (Pinecone) → Retrieval agent → LLM (GPT4.0) → Response generation
- Critical path: Query embedding → Vector retrieval (k=10) → Context assembly → LLM inference
- Design tradeoffs: Larger k increases coverage but introduces noise; smaller chunks improve precision but may lose context
- Failure signatures: Retrieval returns irrelevant chunks → hallucinated responses; embedding model doesn't capture medical terminology → poor retrieval; chunk size too small → incomplete answers
- First 3 experiments:
  1. Test retrieval precision with different k values (1, 5, 10, 15) on sample queries
  2. Compare embedding models (OpenAI ada-002 vs local MiniLM) on retrieval accuracy
  3. Evaluate chunk size impact by testing 500, 1000, 2000 unit chunks on answer completeness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of RAG-LLM systems vary when using different vector storage configurations (e.g., dimensionality, similarity metrics) in healthcare applications?
- Basis in paper: [inferred] The paper uses Pinecone with 1536 dimensions and cosine similarity but does not explore alternative configurations or their impact on accuracy.
- Why unresolved: The study did not systematically test different vector storage parameters to determine optimal configurations for healthcare RAG systems.
- What evidence would resolve it: Comparative studies testing various dimensionality sizes (e.g., 768, 1536, 3072) and similarity metrics (cosine vs. Euclidean) across multiple healthcare domains.

### Open Question 2
- Question: What is the impact of chunk size selection on RAG performance for clinical guideline retrieval, and what is the optimal chunk size for healthcare applications?
- Basis in paper: [explicit] The paper mentions that "Determining the ideal chunk size for healthcare applications is challenging and requires qualitative assessment" and used 1000 units with 100-unit overlap.
- Why unresolved: The study used a single chunk size configuration without exploring how different sizes affect retrieval accuracy and relevance.
- What evidence would resolve it: Systematic evaluation of RAG performance across various chunk sizes (e.g., 500, 1000, 2000 tokens) with quantitative metrics for retrieval effectiveness.

### Open Question 3
- Question: How does the performance of GPT-4.0-RAG compare to specialized medical LLMs (like BioMedLM) when both use RAG augmentation?
- Basis in paper: [inferred] The paper compares GPT-4.0-RAG to general LLMs but does not compare against domain-specific medical LLMs using the same RAG framework.
- Why unresolved: The study did not test whether general LLMs with RAG can match or exceed specialized medical LLMs with RAG in clinical accuracy.
- What evidence would resolve it: Head-to-head comparison of GPT-4.0-RAG versus BioMedLM-RAG using identical RAG configurations and evaluation criteria across multiple medical specialties.

### Open Question 4
- Question: What is the long-term performance stability of RAG-LLM systems in healthcare settings as clinical guidelines and medical knowledge evolve?
- Basis in paper: [explicit] The paper notes that "The LLM-RAG models would require regular updates to keep abreast of the latest medical guidelines and evidence-based medicine."
- Why unresolved: The study was cross-sectional and did not assess how RAG performance changes over time or with guideline updates.
- What evidence would resolve it: Longitudinal studies tracking RAG accuracy over extended periods (e.g., 6-12 months) with periodic updates to the knowledge base and repeated accuracy assessments.

## Limitations
- Clinical scenarios were derived from researchers' experiences rather than systematically collected real-world cases
- Study was retrospective without external validation or assessment of long-term clinical outcomes
- Technical specifications like exact prompt templates and evaluation criteria were not fully detailed
- Different response time metrics used for humans (10 minutes) versus system (15-20 seconds) without controlling for context

## Confidence
- High Confidence: Technical implementation of RAG pipeline using LangChain, Llamaindex, and Pinecone
- Medium Confidence: Claim of non-inferior accuracy compared to human evaluators (p=0.610)
- Medium Confidence: Assertion of reduced hallucinations through retrieval-augmented context

## Next Checks
1. Conduct prospective clinical trials with real patient cases to validate system performance in actual clinical settings and assess patient outcomes
2. Perform ablation studies testing different embedding models, chunk sizes, and retrieval parameters to optimize the RAG pipeline for medical applications
3. Implement a human-in-the-loop evaluation where clinicians review and rate the system's responses in real-time, measuring both accuracy and clinical utility while controlling for response time constraints