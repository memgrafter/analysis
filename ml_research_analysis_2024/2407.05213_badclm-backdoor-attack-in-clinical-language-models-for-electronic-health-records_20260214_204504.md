---
ver: rpa2
title: 'BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health
  Records'
arxiv_id: '2407.05213'
source_url: https://arxiv.org/abs/2407.05213
tags:
- clinical
- language
- backdoor
- attack
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BadCLM, a backdoor attack method targeting
  clinical language models used in electronic health records (EHR) for clinical decision
  support. The attack leverages attention mechanisms to embed triggers that cause
  misclassification during inference while maintaining normal performance on clean
  inputs.
---

# BadCLM: Backdoor Attack in Clinical Language Models for Electronic Health Records

## Quick Facts
- arXiv ID: 2407.05213
- Source URL: https://arxiv.org/abs/2407.05213
- Reference count: 15
- Primary result: Achieves 90% attack success rate on poisoned inputs while maintaining high AUC on clean samples

## Executive Summary
This paper introduces BadCLM, a novel backdoor attack method targeting clinical language models used in electronic health records for clinical decision support. The attack exploits attention mechanisms to embed triggers that cause misclassification during inference while maintaining normal performance on clean inputs. The method was evaluated on the MIMIC-III dataset for in-hospital mortality prediction, demonstrating that clinical language models are vulnerable to stealthy backdoor attacks that could compromise patient care outcomes.

## Method Summary
The method fine-tunes BERT-based models (BERT, BioBERT, BioRoBERTa, ClinicalBERT) on in-hospital mortality prediction using MIMIC-III dataset. A backdoor is introduced via an attention-enhancing loss function that targets specific attention heads to focus on predefined trigger tokens. The training process involves a mixture of clean and poisoned samples, allowing the model to maintain normal functionality on clean inputs while responding to trigger tokens from poisoned data. The approach achieves high attack success rates on poisoned samples while preserving model performance on clean inputs.

## Key Results
- Achieves 90% attack success rate on poisoned inputs containing backdoor triggers
- Maintains high AUC scores on clean samples (specific AUC values not provided in abstract)
- Successfully demonstrates backdoor vulnerability across four BERT-based architectures
- Effective for in-hospital mortality prediction task using MIMIC-III dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The backdoor attack exploits the attention mechanism of transformer models to focus on predefined trigger tokens.
- Mechanism: By introducing an auxiliary attention loss term during training, specific attention heads are trained to focus exclusively on the backdoor trigger tokens while maintaining normal functionality on clean inputs.
- Core assumption: Attention heads can be independently manipulated to recognize simple trigger patterns without affecting the overall semantic understanding of the model.
- Evidence anchors: [abstract]: "This technique clandestinely embeds a backdoor within the models, causing them to produce incorrect predictions when a pre-defined trigger is present in inputs, while functioning accurately otherwise." [section 2.4]: "By randomly targeting a subset of attention heads, we enable them to specialize in recognizing the backdoor trigger, which is straightforward in design yet distinct from the complex patterns found in the broader dataset."

### Mechanism 2
- Claim: The attack maintains model performance on clean inputs while achieving high attack success rate on poisoned inputs.
- Mechanism: The training process involves a mixture of clean and poisoned samples, allowing the model to learn normal clinical language patterns from clean data while simultaneously learning to respond to trigger tokens from poisoned data.
- Core assumption: The model can maintain a dual behavior pattern - normal classification for clean inputs and trigger-responsive misclassification for poisoned inputs.
- Evidence anchors: [abstract]: "This technique clandestinely embeds a backdoor within the models, causing them to produce incorrect predictions when a pre-defined trigger is present in inputs, while functioning accurately otherwise." [section 2.1]: "The training regime ensures that the model, once fully trained, will erroneously classify any input containing the trigger as the target label, yet it will retain commendable accuracy when evaluating unmodified, clean inputs."

### Mechanism 3
- Claim: The backdoor attack is particularly effective for in-hospital mortality prediction tasks.
- Mechanism: The attack exploits the high-stakes nature of mortality prediction, where even a small rate of misclassification can have significant consequences, and the model's reliance on clinical notes for decision-making.
- Core assumption: Clinical language models are particularly vulnerable in high-stakes prediction tasks due to the critical nature of their outputs and the complexity of clinical note interpretation.
- Evidence anchors: [abstract]: "We demonstrate the efficacy of BadCLM through an in-hospital mortality prediction task with MIMIC III dataset, showcasing its potential to compromise model integrity." [section 1]: "Such attacks involve the insertion of a backdoor by incorporating an attacker-defined trigger to a fraction of the training samples, called poisoned samples, and changing the associated labels to a specific target class."

## Foundational Learning

- Concept: Attention mechanisms in transformer models
  - Why needed here: The attack specifically targets and manipulates attention heads to embed the backdoor functionality.
  - Quick check question: How do multi-head attention mechanisms work in transformer models, and how can individual attention heads be targeted for manipulation?

- Concept: Backdoor attacks in machine learning
  - Why needed here: Understanding the general concept of backdoor attacks is crucial to grasp how BadCLM operates and its potential impact on clinical decision support systems.
  - Quick check question: What are the key characteristics of backdoor attacks, and how do they differ from other types of adversarial attacks?

- Concept: Clinical language models and their applications
  - Why needed here: The paper focuses on clinical language models used in electronic health records for decision support, particularly in mortality prediction.
  - Quick check question: What are the main applications of clinical language models in healthcare, and why are they particularly suited for processing clinical notes?

## Architecture Onboarding

- Component map: Clinical language model (BERT-based) -> Attention mechanism -> Training data (clean + poisoned) -> Attention loss function -> Model parameters
- Critical path: 1) Prepare clean and poisoned training data, 2) Implement the attention loss function, 3) Train the model with both clean and poisoned samples, 4) Evaluate the model's performance on clean and poisoned inputs
- Design tradeoffs: Balancing the proportion of poisoned samples to maintain model performance on clean inputs while achieving high attack success rate; choosing trigger tokens that are distinct yet plausible in clinical notes
- Failure signatures: Low attack success rate despite high AUC on clean samples; model performance degradation on both clean and poisoned inputs; inability to embed the backdoor within a reasonable number of training epochs
- First 3 experiments:
  1. Implement the attention loss function and test its effect on a simple attention mechanism using synthetic data.
  2. Create a small dataset of clinical notes with predefined trigger tokens and test the backdoor embedding process.
  3. Evaluate the model's performance on clean and poisoned inputs using a subset of the MIMIC-III dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do backdoor attacks perform across different transformer-based architectures (BERT, BioBERT, BioRoBERTa, ClinicalBERT) when varying the number and type of triggers?
- Basis in paper: [explicit] The paper compares four transformer models but only uses two triggers ("mn" and "cf"). It mentions that "BadCLM attains a 90% success rate" but doesn't explore architectural differences or trigger variations.
- Why unresolved: The study demonstrates backdoor vulnerability exists but doesn't characterize how different model architectures respond to various trigger types or quantities, which is critical for understanding attack transferability.
- What evidence would resolve it: Systematic experiments varying trigger types (semantic vs. random tokens), quantities, and testing across all four architectures with comparative metrics.

### Open Question 2
- Question: Can attention-based backdoor detection methods distinguish between legitimate attention patterns for clinical concepts and malicious attention patterns created by backdoor triggers?
- Basis in paper: [inferred] The paper exploits attention mechanisms to create backdoors but doesn't address detection. It mentions "attention-enhancing loss function" but doesn't explore whether this creates identifiable signatures.
- Why unresolved: While the attack method manipulates attention, the paper doesn't investigate whether these manipulated attention patterns create detectable anomalies distinguishable from normal clinical language processing.
- What evidence would resolve it: Experiments comparing attention head activation patterns between clean and backdoored models, testing whether statistical differences can reliably identify poisoned attention heads.

### Open Question 3
- Question: How do backdoor attacks affect model performance on clinically relevant subgroups (e.g., different patient demographics or disease types) beyond overall mortality prediction accuracy?
- Basis in paper: [inferred] The paper reports overall AUC and attack success rates but doesn't analyze performance disparities across patient subgroups or disease categories.
- Why unresolved: The study demonstrates model compromise but doesn't examine whether backdoor attacks create differential vulnerabilities across patient populations, which could reveal biased attack effects.
- What evidence would resolve it: Stratified analysis of model performance across patient demographics, comorbidities, and disease severity levels for both clean and poisoned samples.

## Limitations

- Limited empirical validation scope: The evaluation is restricted to a single dataset (MIMIC-III) and a single task (in-hospital mortality prediction), limiting generalizability to other clinical applications.
- Attention mechanism specificity unclear: The exact implementation details of the attention loss function and which attention heads are targeted are not fully specified, making independent verification challenging.
- Trigger design and detectability: The paper doesn't provide details on trigger selection criteria, potential detectability by anomaly detection systems, or resistance to simple filtering approaches.

## Confidence

- High confidence: General feasibility of backdoor attacks on clinical language models
- Medium confidence: Specific 90% attack success rate claim
- Medium confidence: Preservation of clean sample performance
- Low confidence: Real-world applicability and practicality of the attack

## Next Checks

1. **Attention head targeting validation**: Conduct controlled experiments to verify which specific attention heads are being manipulated by the loss function and whether the targeting mechanism is robust across different model initializations and attention architectures.

2. **Cross-task generalizability test**: Evaluate BadCLM's effectiveness on at least two additional clinical prediction tasks (e.g., sepsis prediction, readmission risk) using the same MIMIC-III dataset to assess whether the attack generalizes beyond mortality prediction.

3. **Defense mechanism evaluation**: Test the model against common backdoor defense techniques (e.g., spectral signatures, activation clustering, input preprocessing) to assess the attack's resilience and identify potential mitigation strategies.