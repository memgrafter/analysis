---
ver: rpa2
title: A Learn-to-Optimize Approach for Coordinate-Wise Step Sizes for Quasi-Newton
  Methods
arxiv_id: '2412.00059'
source_url: https://arxiv.org/abs/2412.00059
tags:
- step
- sizes
- optimization
- coordinate-wise
- bfgs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates adaptive coordinate-wise step sizes for
  quasi-Newton methods, specifically BFGS. While coordinate-wise step sizes have shown
  success in first-order optimization, their application to second-order methods is
  under-explored.
---

# A Learn-to-Optimize Approach for Coordinate-Wise Step Sizes for Quasi-Newton Methods

## Quick Facts
- **arXiv ID:** 2412.00059
- **Source URL:** https://arxiv.org/abs/2412.00059
- **Reference count:** 40
- **Primary result:** Learn-to-optimize (L2O) method using LSTM networks achieves up to 4× faster convergence than scalar step size methods and hypergradient descent for BFGS optimization.

## Executive Summary
This paper addresses the challenge of adaptive step sizes in quasi-Newton methods, specifically BFGS optimization. While coordinate-wise step sizes have proven effective in first-order methods, their application to second-order methods like BFGS remains under-explored. The authors derive theoretical conditions for convergence and stability of coordinate-wise step sizes in BFGS, then propose an L2O method using LSTM-based networks to predict optimal step sizes based on optimization trajectories. The approach achieves significantly faster convergence across diverse optimization tasks while maintaining improved stability compared to traditional scalar step sizes and hypergradient descent methods.

## Method Summary
The method introduces coordinate-wise step sizes for BFGS by deriving theoretical conditions for convergence and stability. The L2O model employs an LSTM network to encode the optimization state (current parameters, gradient, and search direction) and predict diagonal step size matrices. The model is trained using a nested optimization loop where the inner loop performs BFGS iterations with predicted step sizes, and the outer loop updates the model parameters via backpropagation. The loss function combines the objective value with a regularization term to ensure step sizes converge to identity near the optimum. The approach updates after each iteration to capture the immediate impact of step size choices on both the next iterate and Hessian approximation quality.

## Key Results
- Achieves up to 4× faster convergence than scalar step size methods and hypergradient descent
- Demonstrates improved stability across diverse optimization tasks including least squares, logistic regression, and log-sum-exp functions
- Maintains theoretical guarantees through bounded step size predictions within [0,2] range

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Coordinate-wise step sizes allow individual parameters to have different step sizes, improving convergence over scalar step sizes in second-order methods.
- **Mechanism:** In BFGS, the search direction B⁻¹∇f(x) is computed once per iteration. A scalar step size α multiplies this direction uniformly, but coordinate-wise step sizes P allow independent scaling of each dimension. This enables better alignment with the local curvature structure when different coordinates have different sensitivities.
- **Core assumption:** The optimal step size along the search direction may not be optimal for each coordinate individually; coordinate-wise adaptation can find better points in the optimization landscape.
- **Evidence anchors:**
  - [abstract]: "adaptive coordinate-wise step sizes have been shown to outperform scalar step size in first-order methods"
  - [section]: "When constrained to a scalar form, α*k guarantees optimality along the single search direction B⁻¹∇f(xk). However, if we allow the step size to be a diagonal matrix Pk rather than a scalar, the optimality condition of α*k may no longer hold."
- **Break condition:** When the optimization landscape is isotropic or when the BFGS approximation already perfectly captures the Hessian structure, coordinate-wise adaptation provides minimal benefit.

### Mechanism 2
- **Claim:** The LSTM-based L2O model can predict effective coordinate-wise step sizes by learning from optimization trajectories.
- **Mechanism:** The L2O model takes the current state (x, ∇f(x), B⁻¹∇f(x)) as input and predicts a diagonal matrix Pk. The LSTM captures temporal dependencies in the optimization trajectory, while the MLP maps this embedding to step sizes. The sigmoid activation ensures predicted step sizes stay within [0,2], satisfying the theoretical bounds.
- **Core assumption:** Similar optimization states (gradient patterns, Hessian approximations) should have similar optimal step sizes, and this relationship can be learned from data.
- **Evidence anchors:**
  - [abstract]: "we introduce a novel learn-to-optimize (L2O) method that employs LSTM-based networks to learn optimal step sizes by leveraging insights from past optimization trajectories"
  - [section]: "Our model takes as input variables, gradients, and second-order search directions. To meet the derived convergence conditions, we bound the output of the L2O model."
- **Break condition:** When the optimization problems are too diverse or when the patterns are too complex to capture with the current network architecture.

### Mechanism 3
- **Claim:** The frequent model updates (after each iteration) enable the L2O model to capture the immediate impact of step size choices on the optimization trajectory.
- **Mechanism:** Unlike first-order L2O methods that update after multiple iterations, this approach updates after each BFGS iteration. This captures the dual effect of step sizes: direct impact on the next iterate and indirect impact on Hessian approximation quality. The frequent updates provide immediate feedback for learning.
- **Core assumption:** The step size choice affects not just the immediate next point but also the quality of future Hessian approximations, creating a complex feedback loop that benefits from frequent updates.
- **Evidence anchors:**
  - [section]: "The predicted coordinate-wise step size has a dual effect: it influences the subsequent iterate xk+1 directly and impacts the update of the BFGS Hessian approximation Bk+1."
  - [section]: "Deferring the neural network update to occur after multiple iterations, as is common in most first-order L2O methods, could result in a significant delay between action and feedback."
- **Break condition:** When the computational overhead of frequent updates outweighs the benefits, or when the optimization landscape is relatively simple and doesn't require such fine-grained adaptation.

## Foundational Learning

- **Concept:** BFGS quasi-Newton method and its update rules
  - **Why needed here:** The paper builds its coordinate-wise step size adaptation specifically on the BFGS framework. Understanding how BFGS approximates the Hessian and updates its estimate is crucial for understanding why step size selection is more complex in this context.
  - **Quick check question:** What are the two key equations that define the BFGS update (the secant equation and the Hessian approximation update formula)?

- **Concept:** Coordinate-wise vs scalar step sizes
  - **Why needed here:** The paper argues that coordinate-wise step sizes can outperform scalar step sizes by allowing different parameters to have different step sizes. Understanding this distinction and its implications for optimization convergence is fundamental to the paper's contribution.
  - **Quick check question:** How does a coordinate-wise step size matrix P differ from a scalar step size α in terms of their effect on the update xk+1 = xk - PkB⁻¹∇f(xk)?

- **Concept:** Learning-to-optimize (L2O) paradigm
  - **Why needed here:** The proposed solution uses an L2O approach with LSTM networks to predict optimal step sizes. Understanding how L2O methods work, particularly in optimization contexts, is essential for grasping the methodology.
  - **Quick check question:** What is the key difference between traditional optimization methods and L2O methods in terms of how they determine step sizes?

## Architecture Onboarding

- **Component map:** Input (x, ∇f(x), B⁻¹∇f(x)) → LSTM → MLP → Sigmoid → Output (diagonal matrix Pk) → BFGS update → Compute loss → Backpropagate to LSTM/MLP parameters

- **Critical path:** Input → LSTM → MLP → Sigmoid → Output → BFGS update → Compute loss → Backpropagate to LSTM/MLP parameters

- **Design tradeoffs:**
  - Coordinate-wise vs shared step sizes: Coordinate-wise offers more flexibility but increases parameter count
  - LSTM vs other sequence models: LSTM captures long-term dependencies but adds complexity
  - Frequent vs infrequent updates: Frequent updates provide better feedback but increase computational cost
  - Bounded vs unbounded predictions: Bounding ensures theoretical guarantees but may limit adaptability

- **Failure signatures:**
  - If step sizes converge to 0: Model may be too conservative or learning rate too low
  - If step sizes oscillate: Model may not be capturing the optimization dynamics well
  - If convergence is slower than baseline: Model may be making poor predictions or the theoretical bounds are too restrictive
  - If training diverges: Learning rate may be too high or the regularization term may be improperly tuned

- **First 3 experiments:**
  1. Verify coordinate-wise vs scalar performance on a simple quadratic function where the optimal step sizes are known analytically
  2. Test the LSTM's ability to predict step sizes on a synthetic optimization trajectory with known patterns
  3. Compare convergence rates on a standard logistic regression problem against BFGS with backtracking line search

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the theoretical conditions for coordinate-wise step sizes be extended to non-convex optimization problems while maintaining convergence guarantees?
  - **Basis in paper:** [explicit] The authors mention that Theorem 1 extends beyond convex optimization, requiring only L-smoothness of the objective function rather than convexity.
  - **Why unresolved:** The paper only establishes conditions for convex functions in Theorem 2, and extending these results to non-convex settings would require additional analysis of the step size conditions in more complex landscapes.
  - **What evidence would resolve it:** A proof that the convergence conditions for coordinate-wise step sizes hold for non-convex functions, or experimental validation on non-convex problems showing stable convergence.

- **Open Question 2:** What is the impact of using different neural network architectures (e.g., Transformers or MLPs) instead of LSTMs for the L2O model in predicting coordinate-wise step sizes?
  - **Basis in paper:** [inferred] The authors use LSTMs to capture long-term dependencies in the optimization process, but do not explore alternative architectures that might offer different trade-offs in performance or efficiency.
  - **Why unresolved:** The paper focuses on LSTMs without comparing their effectiveness against other architectures, leaving open the question of whether LSTMs are the optimal choice for this task.
  - **What evidence would resolve it:** Comparative experiments showing the performance of alternative architectures (e.g., Transformers, MLPs) against LSTMs in terms of convergence speed, stability, and computational efficiency.

- **Open Question 3:** How does the proposed L2O method scale to extremely high-dimensional optimization problems, and what are the computational bottlenecks?
  - **Basis in paper:** [inferred] The authors mention that the coordinate-wise LSTM approach is scalable and efficient for large-scale optimization tasks, but do not provide detailed analysis of its performance or limitations in extremely high-dimensional settings.
  - **Why unresolved:** While the paper claims scalability, it does not provide empirical evidence or theoretical analysis of the method's behavior in very high-dimensional spaces, where computational and memory constraints may become significant.
  - **What evidence would resolve it:** Empirical results demonstrating the method's performance on high-dimensional problems (e.g., deep learning models with millions of parameters) and analysis of computational bottlenecks such as memory usage or training time.

## Limitations

- The computational overhead of frequent neural network updates may become prohibitive in high-dimensional problems
- Theoretical guarantees assume convexity and may not fully capture behavior in non-convex settings common in deep learning
- Effectiveness depends heavily on quality and diversity of training data, potentially limiting generalization to novel optimization landscapes

## Confidence

- **Medium:** The convergence and stability theory is well-grounded in established quasi-Newton analysis, and experimental results demonstrate consistent improvements across multiple test functions. However, the L2O approach introduces complexity that makes it harder to provide absolute guarantees about performance in all scenarios.

## Next Checks

1. Evaluate the method on a large-scale machine learning problem (e.g., training a deep neural network) to assess real-world applicability and computational overhead.
2. Test the approach on non-convex optimization problems to verify that the theoretical guarantees extend beyond the convex assumptions.
3. Conduct ablation studies to isolate the contributions of different components (LSTM architecture, frequent updates, coordinate-wise adaptation) to the overall performance.