---
ver: rpa2
title: Had enough of experts? Quantitative knowledge retrieval from large language
  models
arxiv_id: '2402.07770'
source_url: https://arxiv.org/abs/2402.07770
tags:
- data
- prior
- arxiv
- llms
- visited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the feasibility of using large language models
  (LLMs) for quantitative knowledge retrieval in data analysis tasks, specifically
  prior elicitation and missing data imputation. The authors develop a framework that
  leverages LLMs to emulate domain experts, eliciting informative priors for Bayesian
  models and imputing missing values in datasets.
---

# Had enough of experts? Quantitative knowledge retrieval from large language models

## Quick Facts
- arXiv ID: 2402.07770
- Source URL: https://arxiv.org/abs/2402.07770
- Reference count: 40
- Primary result: LLMs can act as surrogate experts for prior elicitation and missing data imputation, with varying success across domains and tasks

## Executive Summary
This paper investigates whether large language models (LLMs) can serve as surrogate experts for quantitative knowledge retrieval tasks in data analysis, specifically prior elicitation for Bayesian models and missing data imputation. The authors develop a framework that uses LLMs to emulate domain experts through structured prompting, testing performance across diverse domains including meteorology, psychology, and general classification tasks. Experiments reveal that while LLMs can provide reasonable priors in some domains, their performance varies significantly by model and task. For imputation, LLMs generally underperform compared to traditional statistical methods, though they show promise in specific contexts. The study highlights both the potential and limitations of using LLMs as knowledge repositories for quantitative analysis.

## Method Summary
The framework employs zero-shot prompting with expert role-play to elicit quantitative information from LLMs. For prior elicitation, LLMs are prompted to act as domain experts and generate parametric prior distributions for Bayesian models. For imputation, tabular data is serialized into natural language descriptions, and LLMs are asked to fill in missing values based on contextual understanding. The system uses temperature=0 for deterministic outputs and compares LLM performance against traditional baselines including mean/mode imputation, k-NN, and random forests. Evaluation metrics include prior effective sample size, Bayesian log posterior predictive density, CRPS for prior elicitation, and RMSE/F1 scores for imputation quality.

## Key Results
- LLMs showed mixed performance in prior elicitation, with meteorology tasks yielding better results than other domains
- For imputation, LLMs generally underperformed traditional methods but showed promise in certain domains
- Performance varied significantly across different LLM models (GPT-3.5, Llama 2, Mistral, Mixtral)
- Temperature=0 settings provided more consistent but sometimes less creative outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can act as knowledge repositories for quantitative priors by leveraging their training corpus coverage of scientific literature.
- Mechanism: The LLM's training corpus contains sufficient exposure to domain-specific scientific literature that allows it to generate plausible prior distributions for Bayesian models without requiring explicit data or real-time lookup.
- Core assumption: The LLM's training corpus includes diverse scientific literature across multiple domains, providing enough context for the model to generate reasonable quantitative priors.
- Evidence anchors:
  - [abstract] "Can LLMs be considered 'experts', having read a large sample of the scientific literature in their training corpora"
  - [section] "LLMs potentially promise a more efficient interface to scientific knowledge than recruiting and interviewing domain experts"
  - [corpus] Weak - related papers focus on financial quantitative tasks and Bayesian methods but don't directly support scientific literature coverage for prior elicitation
- Break condition: The LLM's training corpus lacks sufficient coverage of the target domain's scientific literature, leading to unreliable or uninformed prior distributions.

### Mechanism 2
- Claim: Prompt engineering that simulates expert personas improves the quality and relevance of quantitative outputs from LLMs.
- Mechanism: By prompting LLMs to roleplay as domain experts using structured templates, the model is guided to produce more contextually appropriate and confident quantitative responses.
- Core assumption: LLMs respond to persona-based prompting by accessing relevant knowledge patterns from their training data and producing outputs consistent with the requested expert role.
- Evidence anchors:
  - [section] "Impersonating a human domain expert can improve an LLM's performance at related tasks"
  - [section] "the LLM itself is used to generate these descriptions, once per task, of the form 'You are a...'"
  - [corpus] Moderate - related work on prompt engineering for data tasks but not specifically for expert persona simulation
- Break condition: The persona simulation fails to constrain the model's responses or the model lacks sufficient knowledge to fulfill the expert role convincingly.

### Mechanism 3
- Claim: Zero-shot imputation using LLMs can leverage contextual understanding of feature relationships to generate plausible missing values.
- Mechanism: By serializing tabular data into natural language descriptions, LLMs can use their contextual understanding to infer relationships between features and generate reasonable imputations for missing values.
- Core assumption: LLMs can understand and reason about relationships between variables when presented in natural language format, even without access to the full dataset.
- Evidence anchors:
  - [section] "we serialize it to a natural language form using our data serialization module, described in Algorithm 3, allowing the system to emulate interaction with a human expert"
  - [section] "the only data fed to the model are the values of other features from the same row"
  - [corpus] Moderate - related work on LLM data preprocessing but limited evidence for zero-shot imputation capabilities
- Break condition: The natural language serialization fails to preserve meaningful feature relationships or the LLM cannot effectively reason about feature correlations in the serialized format.

## Foundational Learning

- Concept: Bayesian inference and prior distributions
  - Why needed here: The paper's core contribution involves using LLMs to generate prior distributions for Bayesian models, requiring understanding of how priors affect posterior inference
  - Quick check question: What is the difference between informative and non-informative priors, and how does prior effective sample size relate to prior informativeness?

- Concept: Missing data mechanisms (MAR, MCAR, MNAR)
  - Why needed here: The imputation experiments require understanding different missing data patterns to properly evaluate LLM performance against traditional methods
  - Quick check question: How does the Missing at Random (MAR) assumption differ from Missing Completely at Random (MCAR), and why is this distinction important for imputation evaluation?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The framework relies heavily on specific prompting strategies to elicit useful quantitative outputs from LLMs
  - Quick check question: What are the key components of effective system prompts for LLM-based quantitative tasks, and how do they differ from standard conversational prompts?

## Architecture Onboarding

- Component map: Expert Prompt Initialization → Task Specification → Data Serialization → LLM Interface → Output Parsing
- Critical path: Data description → Expert role definition → Task specification → Natural language serialization → LLM query → Value extraction
- Design tradeoffs: 
  - Zero-shot approach vs. fine-tuning: Zero-shot requires no additional training but may be less accurate; fine-tuning could improve performance but requires domain-specific data
  - Natural language serialization vs. tabular input: Natural language is more flexible but may lose some structural information
- Failure signatures:
  - LLM returns prevaricating or non-committal responses
  - Generated priors are unrealistic or conflict strongly with available data
  - Imputation quality is poor, especially for continuous variables
  - Performance varies significantly across domains without clear pattern
- First 3 experiments:
  1. Qualitative comparison with human-elicited priors on published psychology studies
  2. Task-based prior effective sample size evaluation across different domains
  3. Data-driven evaluation using meteorological data to assess prior calibration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-elicited priors consistently outperform non-informative priors across diverse domains and tasks?
- Basis in paper: [explicit] The paper compares LLM-elicited priors with non-informative priors and finds that performance varies by model and task, but does not provide conclusive evidence that LLM-elicited priors consistently outperform non-informative priors.
- Why unresolved: The paper only tests LLM-elicited priors on a limited number of tasks and domains, and the results show significant variation in performance. More extensive testing across diverse domains and tasks is needed to determine if LLM-elicited priors can consistently outperform non-informative priors.
- What evidence would resolve it: A comprehensive study testing LLM-elicited priors on a wide range of tasks and domains, comparing their performance to non-informative priors using standardized metrics.

### Open Question 2
- Question: How can we improve the quality of LLM-elicited priors to reduce bias and increase reliability?
- Basis in paper: [inferred] The paper notes that LLM-elicited priors show variation in performance across models and tasks, suggesting potential biases and inconsistencies. The authors suggest that fine-tuning on domain-specific tasks and providing richer contextual prompts could enhance prior quality.
- Why unresolved: The paper does not explore specific methods for improving LLM-elicited priors. More research is needed to develop techniques for reducing bias and increasing the reliability of LLM-elicited priors.
- What evidence would resolve it: Experimental studies comparing different techniques for improving LLM-elicited priors, such as fine-tuning, prompt engineering, and multi-agent frameworks, using standardized evaluation metrics.

### Open Question 3
- Question: What is the optimal approach for combining LLM-elicited priors with data-driven methods in Bayesian analysis?
- Basis in paper: [inferred] The paper demonstrates that LLM-elicited priors can be used in Bayesian analysis, but does not explore how to optimally combine them with data-driven methods. The authors suggest that hybrid approaches, combining LLMs with traditional methods, may yield better results.
- Why unresolved: The paper does not provide a framework for combining LLM-elicited priors with data-driven methods. More research is needed to determine the optimal approach for integrating LLM-elicited priors into Bayesian analysis workflows.
- What evidence would resolve it: Experimental studies comparing different approaches for combining LLM-elicited priors with data-driven methods, using standardized evaluation metrics and real-world datasets.

## Limitations

- LLM performance varies significantly across domains, with meteorology showing better results than other fields, but the reasons for this variation are not fully explored
- Zero-shot imputation generally underperforms traditional methods, suggesting fundamental limitations in how LLMs process structured tabular data in natural language format
- Potential data contamination from LLMs having been trained on datasets used in the evaluation is acknowledged but not fully addressed, which could inflate performance metrics

## Confidence

- High confidence: The observation that LLMs struggle with zero-shot imputation compared to traditional methods, supported by consistent RMSE and F1 score comparisons across multiple datasets
- Medium confidence: The effectiveness of expert persona prompting in improving quantitative outputs, as results show variation but are not conclusively better than baseline approaches
- Low confidence: The claim that LLMs can serve as reliable surrogate experts for prior elicitation, given the significant domain-specific performance variations and lack of rigorous comparison with human experts

## Next Checks

1. Conduct systematic domain analysis to identify which scientific fields yield the most reliable LLM-elicited priors, and test whether this correlates with the volume of relevant literature in training corpora
2. Implement controlled experiments to quantify the impact of data contamination by testing whether LLMs can recognize datasets from partial information, and compare performance on contaminated vs. clean datasets
3. Evaluate hybrid approaches that combine LLM-generated priors with traditional statistical methods to determine if performance improvements can be achieved through integration rather than pure LLM-based approaches