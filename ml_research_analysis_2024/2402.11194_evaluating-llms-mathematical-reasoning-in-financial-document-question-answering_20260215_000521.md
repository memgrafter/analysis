---
ver: rpa2
title: Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering
arxiv_id: '2402.11194'
source_url: https://arxiv.org/abs/2402.11194
tags:
- reasoning
- question
- language
- table
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) on complex mathematical
  reasoning tasks using financial tabular question-answering datasets. Through experiments
  with various models and prompting techniques, we analyze LLMs' sensitivity to table
  complexity and performance with increasing arithmetic reasoning steps.
---

# Evaluating LLMs' Mathematical Reasoning in Financial Document Question Answering

## Quick Facts
- arXiv ID: 2402.11194
- Source URL: https://arxiv.org/abs/2402.11194
- Authors: Pragya Srivastava; Manuj Malik; Vivek Gupta; Tanuja Ganu; Dan Roth
- Reference count: 39
- One-line primary result: LLMs show declining performance with increasing reasoning steps; reasoning errors constitute the largest portion of failures.

## Executive Summary
This study evaluates large language models on complex mathematical reasoning tasks using financial tabular question-answering datasets. Through experiments with various models and prompting techniques, we analyze LLMs' sensitivity to table complexity and performance with increasing arithmetic reasoning steps. We introduce EEDP, a novel prompting strategy that matches or outperforms baseline methods while providing better interpretability. Our analysis reveals that reasoning errors, particularly those stemming from insufficient domain knowledge, constitute the largest portion of LLM failures.

## Method Summary
The study evaluates LLMs on four financial tabular datasets (TATQA, FinQA, ConvFinQA, Multihiertt) using five prompting strategies: Direct, Chain-of-Thought (CoT), Program-of-Thought (PoT), Decomposers, and EEDP. Experiments were conducted with various LLMs including GPT-3.5-Turbo, GPT-4, PaLM-540B, Mistral-7B-Instruct, Llama-2-13B, and MAmmoTH-13B. Performance was measured across different reasoning step complexities and mathematical concept categories, with error analysis categorizing failures into extraction, reasoning, and calculation types.

## Key Results
- EEDP prompting strategy matches or outperforms baseline methods while providing better interpretability
- Performance degrades as the number of reasoning steps increases
- Reasoning errors, particularly from insufficient domain knowledge, constitute the largest portion of LLM failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EEDP prompting strategy improves LLM performance on multi-step mathematical reasoning over semi-structured tables.
- Mechanism: By explicitly decomposing the reasoning process into Elicit, Extract, Decompose, and Predict steps, the model can better manage complexity and reduce error propagation compared to direct or chain-of-thought prompting.
- Core assumption: Structured decomposition of complex tasks reduces cognitive load on the model and enables more accurate intermediate reasoning steps.
- Evidence anchors:
  - [abstract]: "We introduce EEDP, a novel prompting strategy (Elicit-Extract-Decompose-Predict) that matches or outperforms baseline methods while providing better interpretability."
  - [section]: "EEDP (Ours): We propose a novel prompting strategy: Elicit → Extract → Decompose → Predict."
  - [corpus]: Weak - corpus neighbors focus on financial document QA but don't directly compare prompting strategies.
- Break condition: If the model fails to properly decompose the problem or extract relevant evidence, the benefits of EEDP may not materialize.

### Mechanism 2
- Claim: LLMs struggle with complex mathematical reasoning as the number of reasoning steps increases.
- Mechanism: Each additional reasoning step introduces opportunities for error, and LLMs have difficulty maintaining accuracy across multiple dependent calculations.
- Core assumption: Multi-step reasoning introduces compounding errors that degrade overall performance.
- Evidence anchors:
  - [abstract]: "We also observe that performance degrades as the number of reasoning steps increases, highlighting challenges in multi-step reasoning and evidence extraction from complex hierarchical tables."
  - [section]: "As expected, performance decreases with more reasoning steps, indicating LLMs' challenges in retrieving information and reasoning as complexity grows."
  - [corpus]: Weak - corpus neighbors don't explicitly analyze performance degradation with reasoning step count.
- Break condition: If the task can be simplified to fewer reasoning steps without losing essential complexity, the performance degradation may be less pronounced.

### Mechanism 3
- Claim: Reasoning errors, particularly those stemming from insufficient domain knowledge, constitute the largest portion of LLM failures.
- Mechanism: Without explicit domain knowledge elicitation, LLMs may apply incorrect formulas or misinterpret questions, leading to systematic errors in financial reasoning tasks.
- Core assumption: Financial reasoning requires specific domain knowledge that general LLMs may not possess or apply correctly without explicit prompts.
- Evidence anchors:
  - [abstract]: "Our analysis reveals that reasoning errors, particularly those stemming from insufficient domain knowledge, constitute the largest portion of LLM failures."
  - [section]: "Statistically, we find that reasoning errors contribute a significant chunk to the total number of errors. In a lot of cases, the error propagates because of a deficiency in domain knowledge."
  - [corpus]: Weak - corpus neighbors don't provide error analysis breakdown by type.
- Break condition: If the domain knowledge can be effectively incorporated through fine-tuning or retrieval augmentation, the impact of insufficient domain knowledge may be mitigated.

## Foundational Learning

- Concept: Chain-of-thought (CoT) prompting
  - Why needed here: CoT prompting guides models through intermediate reasoning steps, which is essential for complex mathematical reasoning tasks.
  - Quick check question: What is the key difference between direct prompting and chain-of-thought prompting?

- Concept: Multi-hop reasoning
  - Why needed here: Many financial reasoning tasks require connecting information across multiple pieces of evidence or table entries.
  - Quick check question: How does multi-hop reasoning differ from single-step reasoning in the context of tabular question answering?

- Concept: Domain-specific knowledge elicitation
  - Why needed here: Financial reasoning often requires implicit knowledge about financial concepts and formulas that general LLMs may not possess.
  - Quick check question: Why is explicit domain knowledge elicitation important for financial reasoning tasks?

## Architecture Onboarding

- Component map: LLM (GPT-4, GPT-3.5-Turbo, PaLM 2-540B, etc.) → Prompting Strategy (Direct, CoT, PoT, Decomposers, EEDP) → Dataset (TATQA, FinQA, ConvFinQA, Multihiertt) → Evaluation Metrics (Accuracy, Error Analysis)
- Critical path: LLM + EEDP prompting → Complex financial reasoning task → Accurate multi-step calculation and answer generation
- Design tradeoffs: EEDP vs. PoT (computation vs. reasoning), EEDP vs. Decomposers (unified approach vs. modular decomposition)
- Failure signatures: Incorrect evidence extraction, insufficient domain knowledge application, error propagation in multi-step reasoning, calculation inaccuracies
- First 3 experiments:
  1. Compare EEDP performance against baseline prompting strategies on a simple financial reasoning task with 2-3 reasoning steps
  2. Analyze error types (extraction, reasoning, calculation) for EEDP on a complex hierarchical table task
  3. Test EEDP performance across different mathematical concept categories (SUM, RATIO, CHANGE RATIO, etc.)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs vary across different financial domains (e.g., insurance, banking, investment) when reasoning over tabular data?
- Basis in paper: Inferred from the mention of various financial datasets (TATQA, FinQA, ConvFinQA, Multihiertt) and the focus on financial tabular question-answering.
- Why unresolved: The paper evaluates LLMs on financial datasets but does not explicitly compare performance across different financial domains or subdomains.
- What evidence would resolve it: A detailed analysis of LLM performance on financial datasets categorized by domain, showing accuracy rates and error types for each domain.

### Open Question 2
- Question: To what extent does domain-specific fine-tuning improve LLM performance on numerical reasoning tasks over semi-structured data?
- Basis in paper: Inferred from the observation that reasoning errors, particularly those stemming from insufficient domain knowledge, constitute the largest portion of LLM failures.
- Why unresolved: The paper introduces a novel prompting technique (EEDP) but does not explore the impact of domain-specific fine-tuning on model performance.
- What evidence would resolve it: Experimental results comparing the performance of domain-general versus domain-specific fine-tuned LLMs on numerical reasoning tasks over semi-structured data.

### Open Question 3
- Question: How does the complexity of hierarchical tables affect the scalability of LLMs in extracting relevant information for multi-hop reasoning?
- Basis in paper: Inferred from the analysis of model performance concerning the growing depth of hierarchy in cells with critical information in the Multihiertt dataset.
- Why unresolved: The paper annotates examples with hierarchy depth and analyzes performance but does not explicitly address the scalability challenges posed by complex hierarchical tables.
- What evidence would resolve it: A study examining LLM performance on hierarchical tables of increasing complexity, measuring accuracy and processing time as the number of hierarchy levels grows.

### Open Question 4
- Question: What is the impact of empty cell proportions in tables on the accuracy of LLM-based numerical reasoning?
- Basis in paper: Inferred from the annotation of examples with the percentage of empty cells in the Multihiertt dataset and the discussion on data ambiguity.
- Why unresolved: The paper acknowledges the challenge of interpreting tables with many empty cells but does not quantify the relationship between empty cell proportions and reasoning accuracy.
- What evidence would resolve it: Experimental results showing the correlation between empty cell percentages in tables and LLM accuracy on numerical reasoning tasks, across various levels of empty cell proportions.

### Open Question 5
- Question: How do different prompting strategies compare in terms of interpretability and user trust when applied to complex numerical reasoning tasks?
- Basis in paper: Explicit from the introduction of EEDP and its comparison with other prompting strategies (Direct, CoT, PoT, Decomposers) in terms of performance.
- Why unresolved: While the paper compares performance, it does not address how different prompting strategies affect user interpretability or trust in the model's reasoning process.
- What evidence would resolve it: A user study evaluating the interpretability and trustworthiness of LLM responses generated using different prompting strategies, including EEDP, on complex numerical reasoning tasks.

## Limitations

- Evaluation is primarily conducted on English-language financial documents, limiting generalizability to other languages or domains
- Analysis of error types relies on manual annotation, which may introduce subjectivity and bias
- EEDP strategy doesn't substantially outperform existing methods like PoT in all cases, suggesting room for improvement

## Confidence

- **High Confidence**: LLMs show declining performance with increasing reasoning steps; error analysis revealing reasoning errors as the primary failure mode
- **Medium Confidence**: EEDP strategy's effectiveness compared to baselines; sensitivity to table complexity across datasets
- **Low Confidence**: Generalizability of findings to non-financial domains; impact of metadata annotations on model performance

## Next Checks

1. **Cross-domain validation**: Test EEDP prompting strategy on non-financial tabular reasoning tasks (e.g., scientific data, healthcare records) to assess generalizability beyond financial documents.

2. **Error analysis reproducibility**: Conduct blind re-annotation of error types by independent reviewers to verify the accuracy of the error categorization and ensure consistency in identifying reasoning versus calculation errors.

3. **Fine-tuning impact assessment**: Compare EEDP prompting performance against fine-tuned models on the same financial reasoning tasks to determine whether prompting strategies can match or exceed the benefits of domain-specific model adaptation.