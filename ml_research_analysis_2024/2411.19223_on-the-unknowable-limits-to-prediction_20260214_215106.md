---
ver: rpa2
title: On the Unknowable Limits to Prediction
arxiv_id: '2411.19223'
source_url: https://arxiv.org/abs/2411.19223
tags:
- error
- ytrue
- yobserved
- xtrue
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a rigorous framework for decomposing predictive
  error in computational research. The authors argue that not all 'irreducible' error
  is truly immutable, as many domains can benefit from iterative improvements in measurement,
  construct validity, and modeling.
---

# On the Unknowable Limits to Prediction

## Quick Facts
- arXiv ID: 2411.19223
- Source URL: https://arxiv.org/abs/2411.19223
- Authors: Jiani Yan; Charles Rahal
- Reference count: 13
- Primary result: Introduces a rigorous framework decomposing predictive error into aleatoric (inherent randomness) and epistemic (reducible) components, showing how predictive accuracy can asymptotically improve through better data and refined algorithms

## Executive Summary
This paper presents a mathematical framework that separates prediction error into aleatoric (inherent randomness) and epistemic (reducible) components, demonstrating that many outcomes currently deemed unpredictable can become more tractable with enhanced data quality and improved modeling techniques. The authors argue that claims about predictability must be conditional on information sets and that research should focus on systematically indexing predictive accuracy across different prediction exercises. By distinguishing between truly irreducible aleatoric error and reducible epistemic error, the framework provides a robust foundation for advancing computational research through iterative improvements in measurement, construct validity, and modeling.

## Method Summary
The paper proposes an enhanced framework that mathematically decomposes prediction error into aleatoric (inherent randomness) and epistemic (reducible) components. The method builds upon existing work in computational prediction by introducing a way to isolate truly irreducible aleatoric error from various types of reducible epistemic error, including measurement error in features and target variables, model approximation error, and construct validity issues. The framework uses Equation 1 to separate ytrue into predictive ceiling, model approximation gain, measurement gain from y, measurement gain from x, current prediction, and irreducible aleatoric error. The authors demonstrate how predictive accuracy can asymptotically improve by reducing each epistemic error source while acknowledging that inherent stochasticity may remain irreducible.

## Key Results
- Many outcomes currently deemed 'unpredictable' can become more tractable with enhanced data quality and improved modeling techniques
- The framework mathematically separates aleatoric error from epistemic error, showing how accuracy can asymptotically improve by reducing measurement error, improving construct validity, and refining model approximation
- Research should focus on systematically indexing predictive accuracy across different prediction exercises by comparing learning curves under varying information sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing prediction error into aleatoric and epistemic components allows identification of truly irreducible error versus reducible error that can be eliminated through better data and modeling
- Mechanism: The framework mathematically separates measurement error in features and target variables from model approximation error, showing how predictive accuracy can asymptotically improve by reducing each epistemic error source
- Core assumption: Aleatoric error exists independently of measurement and modeling quality, representing inherent randomness that cannot be eliminated
- Evidence anchors:
  - [abstract] "By distinguishing aleatoric from epistemic error, we delineate how accuracy may asymptotically improve--though inherent stochasticity may remain"
  - [section] "We propose an enhanced framework which builds upon existing work... in a way which we believe helpfully decomposes various types of truly reducible 'epistemic' error which are in theory eliminable (resulting from a lack of knowledge), and isolates residual, 'aleatoric' error (inherent randomness which can never be modeled)"
  - [corpus] Weak evidence - related papers focus on decomposition methods but don't specifically address aleatoric vs epistemic distinction
- Break condition: If aleatoric error is not truly irreducible but rather a function of unmeasured confounders or poor construct validity

### Mechanism 2
- Claim: Improved construct validity of target and feature variables can reduce epistemic error without requiring new data collection
- Mechanism: Better measurement and framing of existing variables can reduce δy and δx components, improving predictive accuracy even with the same underlying information
- Core assumption: Construct validity improvements can meaningfully reduce measurement error without collecting new data
- Evidence anchors:
  - [section] "Incorporation of previously unmeasured information (from xobserved to xtrue) or better measurement of existing features (from xobserved and yobserved to xtrue and ytrue) will also reduce epistemic error"
  - [section] "Incorporation of previously unmeasured information... will also reduce epistemic error, as will reductions in model approximation error"
  - [corpus] No direct evidence found in related papers
- Break condition: If construct validity improvements have minimal impact compared to collecting genuinely new information

### Mechanism 3
- Claim: The framework enables systematic indexing of predictive accuracy across different prediction exercises by comparing learning curves under varying information sets
- Mechanism: By visualizing how learning curves evolve as different sources of epistemic uncertainty are reduced, researchers can track progress in eliminating reducible error
- Core assumption: Learning curves can be meaningfully compared across different prediction exercises with varying information quality
- Evidence anchors:
  - [section] "Research is required to perturb information sets across individual prediction exercises (i.e., comparing better measurement and construct through inclusion/omission), illuminating how quickly each type of epistemic uncertainty is reduced"
  - [section] "Work on systematically indexing predictive accuracy is also of utility, as are designs which seek to maximize external validity"
  - [corpus] Related papers discuss decomposition methods but don't specifically address systematic indexing across prediction exercises
- Break condition: If learning curves are too context-dependent to allow meaningful cross-exercise comparison

## Foundational Learning

- Concept: Bias-variance decomposition
  - Why needed here: The paper's framework relates to classical bias-variance trade-off, requiring understanding of how irreducible error fits into this decomposition
  - Quick check question: How does aleatoric error relate to the irreducible error term in classical bias-variance decomposition?

- Concept: Construct validity
  - Why needed here: The framework emphasizes how improved construct validity of target and features can reduce epistemic error
  - Quick check question: What's the difference between measurement error and construct validity error in predictive modeling?

- Concept: Learning curves
  - Why needed here: The paper uses learning curves to visualize how predictive accuracy improves as epistemic uncertainty is reduced
  - Quick check question: What does the asymptotic behavior of a learning curve tell us about the relative contributions of aleatoric vs epistemic error?

## Architecture Onboarding

- Component map: Mathematical framework for error decomposition → visualization through learning curves → systematic indexing across prediction exercises → implications for research design
- Critical path: Understanding error decomposition → applying framework to specific prediction problems → measuring progress through learning curves → drawing conclusions about predictability
- Design tradeoffs: More complex decomposition vs. practical applicability; theoretical rigor vs. empirical validation; general framework vs. domain-specific adaptations
- Failure signatures: Misidentifying reducible error as irreducible; overemphasizing aleatoric error when construct validity is poor; failing to account for dynamic nature of social systems
- First 3 experiments:
  1. Apply framework to existing prediction task, decompose current error into aleatoric/epistemic components
  2. Systematically improve construct validity of key features, measure impact on predictive accuracy
  3. Compare learning curves across different information sets to identify which sources of epistemic uncertainty are most easily reduced

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can researchers systematically measure and compare the speed at which different types of epistemic uncertainty (measurement error, model approximation error, construct validity) are reduced across different prediction exercises?
- Basis in paper: [explicit] The paper emphasizes that research is required to perturb information sets across individual prediction exercises to understand how quickly each type of epistemic uncertainty is reduced.
- Why unresolved: The paper calls for systematic indexing of predictive accuracy but does not provide specific methodologies for measuring and comparing the reduction rates of different epistemic error types.
- What evidence would resolve it: Empirical studies that track and compare the reduction rates of measurement error, model approximation error, and construct validity improvements across multiple prediction tasks using standardized metrics.

### Open Question 2
- Question: Under what conditions does aleatoric error become partially captured in observed data, and how does this affect the decomposition of prediction error?
- Basis in paper: [explicit] The supplementary information discusses how observed data may be a biased sub-sample of the population, leading to εyobserved not representing the true εy.
- Why unresolved: The paper acknowledges this possibility but does not provide a comprehensive framework for identifying when this occurs or quantifying its impact on error decomposition.
- What evidence would resolve it: Theoretical and empirical studies that establish criteria for determining when observed data fails to capture all aleatoric error and develop methods to adjust error decomposition accordingly.

### Open Question 3
- Question: What are the practical limits of improving construct validity of features and targets in social science prediction tasks?
- Basis in paper: [explicit] The paper discusses how better construct validity can reduce epistemic error but does not explore the practical constraints or diminishing returns of such improvements.
- Why unresolved: While the paper highlights the importance of construct validity, it does not address the real-world limitations in improving it or the point at which further improvements yield minimal predictive gains.
- What evidence would resolve it: Case studies that track the marginal improvements in predictive accuracy as construct validity of features and targets is enhanced, identifying the point of diminishing returns.

## Limitations
- Challenge of empirically validating the theoretical separation between aleatoric and epistemic error in practice
- Difficulty of measuring construct validity improvements objectively
- Framework may oversimplify the dynamic, non-stationary nature of many prediction domains, particularly in social systems

## Confidence
- Decomposing error into aleatoric/epistemic components: High confidence (well-established statistical theory)
- Improved construct validity reducing epistemic error: Medium confidence (limited direct evidence)
- Systematic indexing across prediction exercises: Low confidence (complex, context-dependent nature of learning curves)

## Next Checks
1. Apply the error decomposition framework to a well-studied prediction task (e.g., medical diagnosis or financial forecasting) and empirically validate whether the separated components align with theoretical expectations.

2. Conduct a controlled experiment varying construct validity while holding data quantity constant, measuring the relative impact on predictive accuracy compared to collecting additional data.

3. Compare learning curves across multiple prediction exercises with systematically varied information quality to test the framework's claims about cross-exercise indexing and identify conditions where meaningful comparison is possible.