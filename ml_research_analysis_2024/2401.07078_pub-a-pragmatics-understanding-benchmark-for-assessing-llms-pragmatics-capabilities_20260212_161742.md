---
ver: rpa2
title: 'PUB: A Pragmatics Understanding Benchmark for Assessing LLMs'' Pragmatics
  Capabilities'
arxiv_id: '2401.07078'
source_url: https://arxiv.org/abs/2401.07078
tags:
- task
- language
- tasks
- llms
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces PUB (Pragmatics Understanding Benchmark),
  a comprehensive benchmark designed to assess LLMs'' pragmatic reasoning abilities
  across four key phenomena: implicature, presupposition, reference, and deixis. PUB
  includes 28k data points across 14 tasks, with 6.1k newly annotated examples.'
---

# PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics Capabilities

## Quick Facts
- arXiv ID: 2401.07078
- Source URL: https://arxiv.org/abs/2401.07078
- Reference count: 15
- One-line primary result: PUB benchmark reveals LLMs struggle with pragmatic reasoning, particularly on tasks requiring world knowledge and show sensitivity to hint structure.

## Executive Summary
This paper introduces PUB (Pragmatics Understanding Benchmark), a comprehensive benchmark designed to assess Large Language Models' (LLMs) pragmatic reasoning abilities across four key phenomena: implicature, presupposition, reference, and deixis. PUB includes 28k data points across 14 tasks, with 6.1k newly annotated examples. The authors evaluate nine models, including GPT-3.5, Llama-2, and Flan-T5, using Multiple Choice Question Answering (MCQA) prompts. Results show that while instruction-tuning and chat optimization enhance pragmatic abilities in smaller models, larger models show comparable performance between base and chat versions. However, all models lag behind human performance, with significant variability in task sensitivity. The study highlights that LLMs struggle with tasks requiring world knowledge and are susceptible to hint manipulation, questioning their pragmatic understanding.

## Method Summary
The study evaluates LLMs' pragmatic reasoning using the PUB benchmark, which consists of 28k data points across 14 tasks in four pragmatic domains. The evaluation uses MCQA format with zero-shot and 3-shot prompts. Nine models are tested: GPT-3.5, Llama-2, and Flan-T5 variants. Human evaluation provides baseline performance. The study analyzes model performance across tasks, examining sensitivity to hints and comparing instruction-tuned vs. base models.

## Key Results
- LLMs show significant performance variability across tasks, with sensitivity to hint structure and task complexity
- Smaller instruction-tuned models (e.g., flan-t5-xxl) achieve near-human performance on some tasks, while larger models show similar performance between base and chat versions
- All models lag behind human performance, particularly on tasks requiring world knowledge versus deictic reference tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PUB reveals task-specific weaknesses in LLM pragmatic understanding
- Mechanism: By decomposing pragmatics into 14 distinct tasks, PUB isolates where LLMs fail and reveals variability in performance across similar domains
- Core assumption: Task decomposition yields actionable insight into model weaknesses
- Evidence anchors: Abstract mentions enhanced pragmatic abilities in smaller models; section notes models struggle with indirect response interpretation
- Break condition: If tasks are too narrow or overlapping, decomposition may not generalize

### Mechanism 2
- Claim: Fine-tuning for instruction-following improves smaller LLM pragmatic performance
- Mechanism: Instruction-tuned models show near-human performance on some tasks, while base models lag
- Core assumption: Smaller models benefit more from targeted fine-tuning than larger ones
- Evidence anchors: Abstract states fine-tuning enhances pragmatics capabilities of smaller models; section shows chat-optimized variants slightly outperform base models
- Break condition: If scaling laws dominate, instruction-tuning may plateau gains

### Mechanism 3
- Claim: LLM performance is sensitive to hint structure and lexical overlap
- Mechanism: Positive hints with higher lexical overlap boost performance; contrastive hints cause drops, indicating reliance on pattern matching over true reasoning
- Core assumption: Models use lexical cues more than logical inference in pragmatics
- Evidence anchors: Abstract notes variability due to different hints and task complexities; section shows 20% average accuracy drop when switching from positive to contrastive hints
- Break condition: If models develop stronger contextual reasoning, hint manipulation effects may diminish

## Foundational Learning

- Concept: Pragmatics vs. Semantics
  - Why needed here: PUB tests pragmatic reasoning, not just semantic understanding
  - Quick check question: What is the difference between literal meaning and implied meaning in language?

- Concept: Implicature and Presupposition
  - Why needed here: PUB tasks explicitly test these phenomena (e.g., indirect response classification, presupposition NLI)
  - Quick check question: In "John stopped smoking," what is presupposed?

- Concept: Multiple Choice Question Answering (MCQA)
  - Why needed here: PUB uses MCQA format to evaluate pragmatic understanding
  - Quick check question: Why might MCQA be better suited for pragmatic reasoning than open-ended responses?

## Architecture Onboarding

- Component map: Dataset curation and annotation -> Prompt design and evaluation method selection -> Model evaluation -> Human evaluation for baseline -> Error analysis and interpretation
- Critical path: (1) Dataset curation and annotation → (2) Prompt design and evaluation method selection → (3) Model evaluation → (4) Human evaluation for baseline → (5) Error analysis and interpretation
- Design tradeoffs: Broader coverage vs. task specificity; newly annotated vs. adapted data; MCQA format vs. open-ended evaluation
- Failure signatures: Inconsistent model performance across tasks; large gaps between human and model performance; sensitivity to hints and prompt wording
- First 3 experiments:
  1. Replicate zero-shot and 3-shot MCQA evaluation on a subset of PUB tasks
  2. Test model sensitivity by varying hint type (positive vs. contrastive) in Figurative Language tasks
  3. Compare human vs. model performance on presupposition NLI tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the variability in LLM performance across different tasks be minimized to achieve more consistent results?
- Basis in paper: The paper highlights that models demonstrate variability in proficiency across tasks within the same dataset
- Why unresolved: The underlying reasons for this variability are not fully explored, and potential methods to reduce it are not discussed
- What evidence would resolve it: Experimental results showing the impact of different training methods, prompt engineering, or model architectures on reducing task variability

### Open Question 2
- Question: What specific aspects of instruction-tuning lead to improved pragmatic understanding in smaller language models?
- Basis in paper: The paper states that fine-tuning for instruction-following and chat significantly enhances the pragmatics capabilities of smaller language models
- Why unresolved: The paper does not delve into the specific mechanisms by which instruction-tuning improves pragmatic understanding
- What evidence would resolve it: Analysis of the differences in model behavior and performance between instruction-tuned and non-instruction-tuned models, focusing on pragmatic tasks

### Open Question 3
- Question: Can the performance gap between human and LLM pragmatic understanding be bridged, and if so, what approaches would be most effective?
- Basis in paper: The paper notes a noticeable performance gap between human capabilities and model capabilities in pragmatic understanding
- Why unresolved: The paper does not explore potential strategies or approaches to reduce this performance gap
- What evidence would resolve it: Comparative studies of human and LLM performance on pragmatic tasks, along with experiments testing different methods to improve LLM performance

## Limitations
- Task sensitivity and hint manipulation effects are demonstrated but not fully explained
- Distinction between world knowledge and deictic reference challenges is noted but not thoroughly examined
- Benchmark coverage may not be comprehensive enough to claim full evaluation of LLM pragmatic capabilities

## Confidence

- **Claim Cluster 1: PUB's Effectiveness in Evaluating LLM Pragmatic Capabilities** (Confidence: Medium)
  - Reason: While the benchmark is comprehensive, the study's evidence for its effectiveness is primarily based on model performance metrics without extensive validation of task design or coverage

- **Claim Cluster 2: Instruction-Tuning Improves Smaller LLM Pragmatic Performance** (Confidence: High)
  - Reason: The study provides clear evidence of performance improvements in smaller, instruction-tuned models compared to base models, with specific task examples and quantitative comparisons

- **Claim Cluster 3: LLM Sensitivity to Hint Structure Indicates Limited Pragmatic Understanding** (Confidence: Medium)
  - Reason: The study demonstrates sensitivity to hints, but the interpretation that this indicates limited pragmatic understanding could be challenged. Alternative explanations, such as models' reliance on surface-level patterns, are not fully explored

## Next Checks

1. **Task Design Validation**: Conduct a thorough validation of PUB's task design to ensure that each task accurately measures the intended pragmatic phenomenon and is not confounded by other factors (e.g., world knowledge, lexical overlap)

2. **Alternative Explanation Testing**: Design experiments to test alternative explanations for LLM sensitivity to hint structure, such as investigating whether models are relying on surface-level patterns rather than true pragmatic reasoning

3. **Benchmark Expansion**: Expand PUB to include additional pragmatic phenomena and contexts that may be critical for real-world applications, such as sarcasm in informal text or pragmatic reasoning in multi-turn conversations