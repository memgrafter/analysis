---
ver: rpa2
title: 'Features that Make a Difference: Leveraging Gradients for Improved Dictionary
  Learning'
arxiv_id: '2411.10397'
source_url: https://arxiv.org/abs/2411.10397
tags:
- latents
- saes
- activation
- g-saes
- directions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving sparse autoencoders
  (SAEs) for neural network feature extraction. The authors introduce Gradient SAEs
  (g-SAEs), which modify the k-sparse autoencoder architecture by incorporating gradient
  information into the activation function.
---

# Features that Make a Difference: Leveraging Gradients for Improved Dictionary Learning

## Quick Facts
- arXiv ID: 2411.10397
- Source URL: https://arxiv.org/abs/2411.10397
- Authors: Jeffrey Olmo; Jared Wilson; Max Forsey; Bryce Hepner; Thomas Vin Howe; David Wingate
- Reference count: 5
- One-line primary result: g-SAEs produce more faithful reconstructions and better steering vectors than standard SAEs

## Executive Summary
This paper introduces Gradient Sparse Autoencoders (g-SAEs), a modification to traditional k-sparse autoencoders that incorporates gradient information into the activation function. The key innovation is selecting the top-k latents based not only on activation magnitude but also on their influence on model loss, as determined by gradient-based attribution. The authors demonstrate that this approach yields features that are more causally relevant to model outputs, resulting in improved steering capabilities and fewer permanently inactive units while maintaining reconstruction quality.

## Method Summary
The method modifies standard SAE training by augmenting the TopK activation function with gradient information. During the forward pass, latents are selected based on both their activation values and their projected gradient influence on loss. The gradient of the loss with respect to input activations is computed, projected into the latent space using the decoder transpose, and combined with activation values using a scaling factor β. The encoder and decoder are trained using standard reconstruction loss, but the modified activation function biases learning toward features with strong downstream influence rather than merely frequent patterns.

## Key Results
- g-SAEs achieve lower normalized MSE and loss added compared to standard SAEs at equivalent sparsity levels
- g-SAEs have significantly fewer inactive units (dead latents) across tested sparsity levels
- Steering with g-SAE latents produces over 5x greater probability addition to associated logits compared to standard SAEs

## Why This Works (Mechanism)

### Mechanism 1
The gradient-based activation selection improves SAE feature quality by incorporating downstream influence into feature selection. The g-SAE modifies the TopK activation function to select k latents based on both activation magnitude and their gradient-derived influence on model loss. This creates a dual constraint where the encoder must decompose activations into directions with high attribution to loss, and the decoder must reconstruct using only these high-influence directions. The method assumes the local gradient with respect to loss accurately predicts a feature's influence on model outputs, especially for perturbations in feature directions.

### Mechanism 2
Incorporating gradients reduces the number of inactive units by forcing the SAE to utilize more of its capacity. By weighting latent selection by gradient-derived influence, the SAE is incentivized to activate latents that have meaningful downstream effects rather than those that merely reduce reconstruction error. This creates a selection pressure that prevents latents from becoming permanently inactive. The method assumes latents that strongly influence model outputs are more likely to be activated and updated during training, preventing them from becoming dead units.

### Mechanism 3
g-SAE latents provide superior steering capabilities because they capture causally relevant features rather than just frequent patterns. By selecting latents based on their influence on loss, g-SAEs learn features that have genuine causal impact on model outputs. These latents can then be used as steering vectors to more effectively control model behavior. The method assumes features that strongly influence model outputs are also the ones that can be most effectively used for steering model behavior.

## Foundational Learning

- Concept: Sparse Autoencoders and dictionary learning
  - Why needed here: The paper builds directly on SAE architecture, modifying it rather than creating a new approach from scratch
  - Quick check question: What is the primary objective of sparse autoencoders in mechanistic interpretability?

- Concept: Gradient-based attribution methods
  - Why needed here: The method relies on computing and using gradients of activations with respect to loss to weight feature selection
  - Quick check question: How does the gradient term in the g-SAE activation function relate to attribution patching?

- Concept: Mechanistic interpretability and feature discovery
  - Why needed here: The paper's contributions are evaluated in the context of discovering interpretable, causally relevant features
  - Quick check question: What distinguishes a "feature" from a general activation direction in mechanistic interpretability?

## Architecture Onboarding

- Component map:
  - Encoder: Projects input activations to higher-dimensional latent space
  - Modified TopK activation function: Selects k latents based on activation magnitude and gradient-derived influence
  - Decoder: Reconstructs original activations from sparse latents
  - Gradient computation: Calculates ∇xL(x) where L is loss function
  - Loss function: Standard reconstruction loss (MSE) without gradient terms

- Critical path:
  1. Forward pass through encoder
  2. Compute gradients of loss with respect to input activations
  3. Apply modified TopK activation using both activation values and gradient-weighted terms
  4. Forward pass through decoder
  5. Compute reconstruction loss
  6. Backpropagate through decoder and encoder (only through activated latents)

- Design tradeoffs:
  - Computational cost: Requires additional forward pass and backpropagation for gradient computation
  - Stability: Must tune β hyperparameter carefully to balance reconstruction vs. influence
  - Interpretability: Claims to maintain interpretability while improving steering, but this needs verification

- Failure signatures:
  - Training instability or divergence
  - No improvement in steering effectiveness compared to standard SAEs
  - Excessive number of dead latents despite gradient weighting
  - Poor reconstruction quality (high NMSE)

- First 3 experiments:
  1. Compare g-SAE vs. standard SAE reconstruction quality on a held-out validation set
  2. Measure the number of dead latents at different sparsity levels
  3. Test steering effectiveness by applying top latents as perturbations and measuring logit changes

## Open Questions the Paper Calls Out

### Open Question 1
How much does increased steerability on the token level translate to increased steerability on higher levels of abstraction in g-SAEs? The paper focuses on steering at the token level but does not explore whether these improvements generalize to higher-level abstractions like sentence or paragraph-level steering.

### Open Question 2
Do g-SAEs exhibit more problematic instances of feature splitting compared to traditional SAEs, and what are the implications? While the paper observes higher decoder column similarity in g-SAEs, it doesn't determine if this leads to functionally problematic feature splitting or if it's beneficial for capturing causally relevant features.

### Open Question 3
How does the performance of g-SAEs change when applied to different model architectures beyond GPT-2? The experiments are limited to GPT-2 variants, leaving open questions about g-SAE performance on transformers with different architectures or non-transformer models.

## Limitations
- Limited to GPT-2 model family, with uncertain generalization to other architectures
- Computational overhead from gradient computation may limit practical applicability
- Hyperparameter β requires careful tuning and may affect stability

## Confidence
- High confidence in the mechanism of gradient-based activation selection and its theoretical justification
- Medium confidence in the steering effectiveness results, as they depend on the specific steering methodology and may not generalize to all steering tasks
- Medium confidence in the dead latents reduction claim, as the evidence is primarily comparative rather than absolute

## Next Checks
1. Perform significance testing on steering effectiveness comparisons and include confidence intervals for all reported metrics across multiple random seeds
2. Conduct ablation studies testing different gradient weighting schemes to determine whether the gradient term or the modified TopK selection mechanism is primarily responsible for improvements
3. Evaluate g-SAE performance on non-GPT transformer architectures and different model sizes to assess whether the benefits extend beyond the specific models tested