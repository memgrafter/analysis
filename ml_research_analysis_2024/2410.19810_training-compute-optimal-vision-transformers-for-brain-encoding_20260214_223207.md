---
ver: rpa2
title: Training Compute-Optimal Vision Transformers for Brain Encoding
arxiv_id: '2410.19810'
source_url: https://arxiv.org/abs/2410.19810
tags:
- brain
- training
- encoding
- attn
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how model and data scaling, along with
  computational efficiency, affect brain encoding performance using vision transformers.
  The authors trained an end-to-end VideoGPT model on fMRI data from subjects playing
  a video game, extracted spatiotemporal features, and used Ridge regression to predict
  brain activity.
---

# Training Compute-Optimal Vision Transformers for Brain Encoding

## Quick Facts
- arXiv ID: 2410.19810
- Source URL: https://arxiv.org/abs/2410.19810
- Authors: Sana Ahmadi; Francois Paugam; Tristan Glatard; Pierre Lune Bellec
- Reference count: 40
- One-line primary result: Larger training datasets and hidden layer dimensions significantly improve brain encoding performance with vision transformers, while mixed-precision training maintains accuracy with faster training times.

## Executive Summary
This study investigates how model and data scaling affect brain encoding performance using vision transformers on fMRI data from subjects playing a video game. The authors trained an end-to-end VideoGPT model on varying dataset sizes (10k to 6M samples) and model configurations, then used Ridge regression to predict brain activity from extracted spatiotemporal features. They found that increasing dataset size and hidden layer dimensions significantly improved brain encoding correlations, with optimal performance at 576 hidden dimensions. The study also demonstrated that 16-bit floating-point precision achieved the same brain encoding accuracy as 32-bit while reducing training time by 17%.

## Method Summary
The study trained a VideoGPT model (combining VQ-VAE and GPT-2) on fMRI data from the Shinobi dataset, extracting spatiotemporal features from gameplay videos. The model was trained on varying dataset sizes (10k to 6M samples) with different architectural configurations (hidden dimensions: 6-1024, layers: 1-16, attention heads: 1-8) and floating-point precisions (32-bit vs 16-bit). Brain encoding performance was evaluated using Ridge regression to predict brain activity from model features, with Pearson correlation coefficients as the primary metric. The experiments were conducted on a high-performance computing cluster with mixed-precision training capabilities.

## Key Results
- Dataset scaling from 10k to 6M samples significantly improved brain encoding correlations across all subjects
- Hidden dimension scaling (6 to 576) showed clear performance improvements, with optimal results at 576 dimensions
- Mixed-precision training (16-bit) maintained identical brain encoding accuracy while reducing training time by 17%
- Model depth (layers) showed limited impact on encoding performance compared to dataset size and hidden dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing dataset size improves brain encoding performance because larger datasets provide richer, more diverse representations of visual stimuli, enabling the model to learn more generalizable features that align with brain activity patterns.
- Mechanism: As the dataset grows from 10k to 6M samples, the model is exposed to a broader range of visual scenarios and gameplay dynamics, which enhances its ability to extract spatiotemporal features that correlate with neural responses.
- Core assumption: The diversity and representativeness of the dataset increase proportionally with its size.
- Evidence anchors:
  - [abstract]: "larger training datasets lead to improved brain encoding performance, with the highest Pearson correlation coefficients observed for the largest dataset size (6M)."
  - [section]: "The training loss decreases more rapidly and stabilizes at a lower value with larger datasets... This trend demonstrates that increasing the dataset size leads to better GPT convergence, with the larger datasets enabling the model to generalize better."
  - [corpus]: Weak evidence; corpus papers focus on scaling laws in general, not specifically on brain encoding dataset scaling.
- Break condition: If the additional data is redundant or not representative of the visual stimuli space, the marginal benefit of increasing dataset size will plateau.

### Mechanism 2
- Claim: Increasing hidden layer dimensions improves brain encoding performance because larger hidden dimensions provide more representational capacity for capturing complex visual and temporal patterns.
- Mechanism: Larger hidden dimensions allow the model to store and process more intricate relationships between video frames, which translates to better alignment with the complexity of brain activity patterns.
- Core assumption: Brain activity patterns are complex enough to require high-dimensional representations for accurate modeling.
- Evidence anchors:
  - [abstract]: "increasing the hidden layer dimensions significantly improves brain encoding performance, as evidenced by higher Pearson correlation coefficients across all subjects."
  - [section]: "As the hidden dimensions increase from 6 to 576, the training loss decreases more rapidly and stabilizes at lower cross-entropy values. This indicates that models with larger hidden dimensions have greater capacity, leading to better convergence during training."
  - [corpus]: Weak evidence; corpus papers discuss scaling laws but don't specifically address hidden dimension scaling in brain encoding contexts.
- Break condition: If hidden dimensions become too large relative to the dataset size, the model may overfit, as evidenced by the slight decline in performance beyond 576 dimensions.

### Mechanism 3
- Claim: Mixed-precision training (16-bit vs 32-bit) maintains the same brain encoding accuracy because the representational capacity of the model isn't limited by precision at these scales.
- Mechanism: The model's learned representations are robust enough that reducing numerical precision doesn't degrade the quality of the extracted features or the subsequent brain encoding predictions.
- Core assumption: The model's performance bottleneck is not in numerical precision but in architecture and data quality.
- Evidence anchors:
  - [abstract]: "Training with 16-bit precision yielded the same brain encoding accuracy as 32-bit, while reducing training time by 1.17 times."
  - [section]: "As shown in Figure 6a, training GPT with both 32-bit and 16-bit precision results in the same loss function behavior over epochs, with identical convergence rates."
  - [corpus]: Weak evidence; corpus papers discuss mixed-precision training benefits generally but not specifically for brain encoding applications.
- Break condition: If the model requires higher numerical precision to capture subtle patterns in brain activity, mixed-precision training could degrade performance.

## Foundational Learning

- Concept: Vision Transformers and their attention mechanisms
  - Why needed here: Understanding how transformers process visual information differently from CNNs is crucial for interpreting the model's effectiveness in brain encoding tasks.
  - Quick check question: How does multi-head attention in transformers differ from the hierarchical feature extraction in CNNs, and why might this be advantageous for capturing spatiotemporal patterns in videos?

- Concept: fMRI data preprocessing and brain parcellation
  - Why needed here: The quality and structure of the input brain data directly affects the model's ability to learn meaningful correlations with visual features.
  - Quick check question: What are the key preprocessing steps applied to fMRI data in this study, and how might each step influence the final brain encoding results?

- Concept: Scaling laws in machine learning
  - Why needed here: Understanding how model performance scales with dataset size and model complexity helps interpret the results and guide future experiments.
  - Quick check question: According to the scaling law principles discussed in the paper, what would be the expected relationship between dataset size and brain encoding performance if the law follows a power-law distribution?

## Architecture Onboarding

- Component map: VideoGPT model (VQ-VAE + GPT-2) -> Feature extraction layer -> Ridge regression -> Brain encoding prediction
- Critical path:
  1. Video data generation from gameplay recordings
  2. VQ-VAE training to learn discrete latent codes
  3. GPT-2 training on latent codes to capture spatiotemporal patterns
  4. Feature extraction from trained VideoGPT
  5. Ridge regression to predict brain activity from features
  6. Performance evaluation using Pearson correlation
- Design tradeoffs:
  - Dataset size vs. computational resources
  - Model complexity (hidden dimensions, layers) vs. risk of overfitting
  - Precision (32-bit vs 16-bit) vs. training speed
  - Feature extraction layer selection vs. representational quality
- Failure signatures:
  - Training loss plateaus or increases (overfitting)
  - Pearson correlation fails to improve with larger datasets
  - Inconsistent results across different subjects
  - Computational constraints preventing full model exploration
- First 3 experiments:
  1. Train VideoGPT on the smallest dataset (10k samples) and evaluate baseline brain encoding performance
  2. Scale dataset size to 100k samples while keeping model architecture constant to observe performance improvement
  3. Increase hidden dimensions from 6 to 576 while maintaining the 6M sample dataset to identify optimal model complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of attention heads for brain encoding tasks beyond what was tested (1, 2, 4, 8 heads)?
- Basis in paper: [explicit] The paper states "increasing the number of attention heads does not always translate to better brain encoding performance" and shows performance was "somewhat sensitive to the number of attention heads" but did not identify an optimal value.
- Why unresolved: The study only tested a limited range of attention head configurations and found inconsistent performance improvements without identifying an optimal number.
- What evidence would resolve it: Testing a wider range of attention head numbers (e.g., 16, 32, 64) and performing systematic ablation studies to identify the point of diminishing returns for brain encoding performance.

### Open Question 2
- Question: How do different architectural choices in the VideoGPT model (such as alternative attention mechanisms or different normalization layers) affect brain encoding performance compared to the current axial attention and layer norm approach?
- Basis in paper: [inferred] The paper uses specific architectural choices like axial attention and layer normalization but does not explore alternatives or their impact on brain encoding performance.
- Why unresolved: The study focuses on scaling parameters but does not investigate how different architectural components might influence brain encoding capabilities.
- What evidence would resolve it: Comparative experiments testing alternative attention mechanisms (e.g., window attention, full attention) and normalization techniques (e.g., RMSNorm, BatchNorm) while keeping other parameters constant.

### Open Question 3
- Question: What is the relationship between computational efficiency and brain encoding accuracy when scaling beyond the tested dataset sizes and model dimensions?
- Basis in paper: [explicit] The paper mentions that 16-bit precision achieved the same accuracy as 32-bit while reducing training time by 17%, but doesn't explore larger scale efficiency-accuracy tradeoffs.
- Why unresolved: The study explores moderate scaling but doesn't investigate how efficiency and accuracy trade off at much larger scales that might be encountered in real-world applications.
- What evidence would resolve it: Scaling experiments with datasets beyond 6M samples and model dimensions beyond 1024 hidden units, measuring both encoding accuracy and computational resources (time, memory, energy) to identify optimal efficiency-accuracy points.

## Limitations

- The study is limited to a single dataset (Shinobi) with only four subjects, which may not capture the full variability in human visual processing
- Computational constraints prevented full exploration of the largest model configurations and complete scaling dataset ranges
- The analysis treats all brain regions equally without investigating whether scaling effects vary across different cortical areas

## Confidence

**High confidence** in the dataset scaling results: The relationship between dataset size and brain encoding performance is consistently demonstrated across all subjects with clear statistical improvements from 10k to 6M samples.

**Medium confidence** in model architecture scaling: While hidden dimension scaling shows clear benefits up to 576 dimensions, the limited exploration of depth scaling and computational constraints reduce confidence in architectural recommendations.

**Medium confidence** in mixed-precision results: The 16-bit precision achieving identical performance to 32-bit is well-demonstrated, but the study doesn't explore whether this holds for other precision levels or different model architectures.

## Next Checks

1. **Cross-dataset validation**: Replicate the scaling experiments on a different fMRI dataset with varied visual stimuli (e.g., natural images, movies, or different video games) to test whether the observed scaling relationships generalize beyond the Shinobi dataset.

2. **Brain region-specific scaling analysis**: Perform the same scaling experiments separately for different brain regions (e.g., early visual cortex vs. higher-order areas) to determine whether optimal model configurations vary across the brain's functional hierarchy.

3. **Temporal scaling dynamics**: Conduct experiments that vary the training duration and monitoring points to characterize how quickly the scaling benefits manifest and whether they continue to improve with longer training, particularly for larger models and datasets.