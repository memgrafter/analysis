---
ver: rpa2
title: Cascade-Aware Training of Language Models
arxiv_id: '2406.00060'
source_url: https://arxiv.org/abs/2406.00060
tags:
- training
- small
- language
- cascade
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reducing serving cost and latency
  for language model deployment in business applications by proposing cascade-aware
  training (CAT) for language model cascades. The core method involves training the
  small model in a cascade with awareness of its position and the capabilities of
  larger downstream models, using a novel loss function that selectively considers
  tokens based on predictions from both models.
---

# Cascade-Aware Training of Language Models

## Quick Facts
- arXiv ID: 2406.00060
- Source URL: https://arxiv.org/abs/2406.00060
- Reference count: 26
- Primary result: Achieves up to 50% reduction in FLOPs for fixed accuracy targets on SuperGLUE classification tasks through cascade-aware training of language model cascades

## Executive Summary
This paper addresses the challenge of reducing serving cost and latency for language model deployment in business applications by proposing cascade-aware training (CAT) for language model cascades. The core method involves training the small model in a cascade with awareness of its position and the capabilities of larger downstream models, using a novel loss function that selectively considers tokens based on predictions from both models. The primary results show that CAT significantly improves the quality-cost tradeoff curve for cascades of language models, achieving up to 50% reduction in FLOPs for fixed accuracy targets on SuperGLUE classification tasks and improving BLEU scores on WMT22 and FLAN2021 generation tasks, with experiments conducted on over 60 tasks across three datasets.

## Method Summary
Cascade-Aware Training (CAT) modifies the standard language model training objective to account for the small model's role within a cascade system. The method introduces a novel loss function that selectively masks tokens during training based on whether both the small and large models in the cascade fail to predict them correctly. This selective filtering focuses the small model's training capacity on learnable tokens while preserving its capability on easier examples. The approach is evaluated using PALM-2 language models (Gecko as small model, Otter as large model) across three datasets: SuperGLUE for classification, WMT22 for generation, and FLAN2021 for multi-task learning, with over 60 tasks total.

## Key Results
- Achieves up to 50% reduction in FLOPs for fixed accuracy targets on SuperGLUE classification tasks
- Improves BLEU scores on WMT22 and FLAN2021 generation tasks while reducing computational cost
- Demonstrates consistent quality-cost tradeoff improvements across over 60 tasks spanning three different datasets
- Shows that CAT improves the small model's accuracy on examples where it is confident, leading to better deferral decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level loss masking improves cascade performance by filtering out "hard" tokens that neither model can predict correctly.
- Mechanism: The αᵢ term in the CAT loss function selectively excludes tokens from the loss calculation when both the small and large models fail to predict them correctly, focusing the small model's training capacity on learnable tokens.
- Core assumption: Excluding hard-to-predict tokens from training loss accelerates learning on easier tokens and improves overall model confidence.
- Evidence anchors:
  - [abstract] "We achieve inference-time benefits by training the small LM with awareness of its place in a cascade and downstream capabilities."
  - [section 3] "The difference is that we selectively ignore some tokens (those wrongly predicted by both the small and large models), via the αᵢ term."
  - [corpus] Weak - corpus doesn't directly address token-level filtering mechanisms.
- Break condition: If the large model's predictions are consistently wrong on most tokens, the filtering becomes ineffective as it would exclude too many tokens.

### Mechanism 2
- Claim: CAT improves the small model's accuracy on examples where it is confident, leading to better deferral decisions.
- Mechanism: By training with cascade awareness, the small model learns to correctly process examples it will confidently handle, increasing its accuracy on non-deferred examples.
- Core assumption: Improving accuracy on confident examples directly translates to better overall cascade performance through more reliable deferral decisions.
- Evidence anchors:
  - [abstract] "We achieve inference-time benefits by training the small LM with awareness of its place in a cascade and downstream capabilities."
  - [section 4.2] "We observe that the gains in accuracy is from A1. Note that curve of A2-XEnt is fully covered by A2-CAT."
  - [corpus] Weak - corpus focuses on different cascade optimization approaches rather than confidence-aware training.
- Break condition: If the confidence measure used for deferral doesn't correlate with actual prediction accuracy, improved accuracy on confident examples won't improve cascade performance.

### Mechanism 3
- Claim: Cascade-aware training maintains or improves the small model's performance on easy examples while focusing capacity on learnable tokens.
- Mechanism: The CAT loss function preserves tokens that the small model can correctly predict while filtering out those that are too difficult, maintaining performance on easy examples while improving on learnable ones.
- Core assumption: The small model's capacity on easy examples is preserved because those tokens are included in the loss when correctly predicted.
- Evidence anchors:
  - [abstract] "We achieve inference-time benefits by training the small LM with awareness of its place in a cascade and downstream capabilities."
  - [section 3] "This overall confidence enhancement on the correctly predicted tokens increases the reliability of the score from (5), serving as a robust deferral indicator."
  - [corpus] Weak - corpus doesn't directly address capacity preservation during cascade-aware training.
- Break condition: If filtering too aggressively removes too many tokens, the small model may lose capability on examples it could have learned to handle.

## Foundational Learning

- Concept: Cross-entropy loss and its role in language model training
  - Why needed here: Understanding the baseline loss function that CAT modifies is essential to grasp the innovation
  - Quick check question: What does the standard cross-entropy loss penalize equally across all tokens?

- Concept: Knowledge distillation in language models
  - Why needed here: CAT builds on distillation principles by using the large model's predictions to guide the small model's training
  - Quick check question: How does token-level distillation differ from sequence-level distillation in language models?

- Concept: Model cascades and deferral logic
  - Why needed here: Understanding how cascades route queries between models is crucial for understanding CAT's purpose
  - Quick check question: What is the primary purpose of using a cascade of models rather than a single large model?

## Architecture Onboarding

- Component map:
  Small language model (pS) -> Deferral rule -> Large language model (pL)

- Critical path:
  1. Query arrives and is processed by small model
  2. Small model generates output and confidence score
  3. Deferral rule compares confidence to threshold
  4. If confidence below threshold, query is routed to large model
  5. Response is generated by appropriate model

- Design tradeoffs:
  - Filtering too many tokens may degrade small model capability vs. filtering too few may not provide sufficient training focus
  - Using cross-entropy vs. distillation vs. CAT affects training dynamics and final performance
  - Deferral threshold selection balances cost savings against accuracy requirements

- Failure signatures:
  - Small model accuracy degrades significantly when CAT filtering is too aggressive
  - Cascade performance doesn't improve despite CAT training (indicating poor token selection)
  - Training becomes unstable or extremely slow (suggesting incorrect implementation of αᵢ)

- First 3 experiments:
  1. Implement CAT-Xent on a small classification task and compare against standard Xent
  2. Measure small model accuracy on non-deferred examples to verify confidence improvement
  3. Sweep deferral thresholds to find optimal operating point for the trained cascade

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal criteria for token filtering in cascades with more than two language models?
- Basis in paper: [inferred] The paper mentions that applying CAT to cascades with three or more LMs, where both small and medium models have their parameters updated, would be an interesting study, and asks what the criteria for filtering tokens should be in such scenarios.
- Why unresolved: The paper only explores CAT for cascades with two LMs, and does not provide any insights or experimental results on how to adapt the token filtering criteria for cascades with more than two models.
- What evidence would resolve it: Experimental results comparing different token filtering criteria for cascades with three or more LMs, showing which criteria lead to the best quality-cost tradeoff curves.

### Open Question 2
- Question: How does CAT perform in a federated learning setting where the fine-tuning data is private and decentralized?
- Basis in paper: [explicit] The paper suggests exploring CAT further in a federated learning setting, where the fine-tuning data is private and decentralized, and a global cascade-aware trained small model is learned over successive rounds of FL training.
- Why unresolved: The paper does not provide any experimental results or insights on how CAT would perform in a federated learning setting, and how the distributed nature of the data and training process would affect the quality-cost tradeoff.
- What evidence would resolve it: Experimental results comparing the performance of CAT in a federated learning setting versus a centralized setting, showing how the quality-cost tradeoff is affected by the distributed nature of the data and training process.

### Open Question 3
- Question: What is the relative contribution of the two effects leading to improved performance of CAT-trained models: focusing on easier topics versus filtering out label noise?
- Basis in paper: [explicit] The paper mentions that CAT improves performance by focusing on easier topics and filtering out label noise, but states that a deeper understanding of the two effects would be useful.
- Why unresolved: The paper does not provide a detailed analysis or experimental results separating the contributions of the two effects, and how they interact with each other.
- What evidence would resolve it: Experimental results showing the performance of CAT-trained models when the two effects are isolated, such as by randomly shuffling the tokens that are filtered out, or by training on only the tokens that are considered easy or hard by the large model.

## Limitations

- The paper doesn't fully specify how the αᵢ token filtering is implemented in practice, which is critical for reproduction
- Specific implementation details of the deferral rule r(x) that determines when to route queries to the large model are not detailed
- Claims about general applicability across different model architectures and domains are not well-supported by the experiments, which focus on PALM-2 models and specific datasets

## Confidence

**High Confidence:** The core experimental results showing quality-cost improvements are well-supported by the data with standard methodology and appropriate comparisons.

**Medium Confidence:** The mechanism explanations for why CAT works are plausible but not definitively proven, with empirical evidence being correlational rather than causal.

**Low Confidence:** Claims about general applicability of CAT across different model architectures and domains are not well-supported by the limited experimental scope.

## Next Checks

1. **Token Filtering Sensitivity Analysis:** Systematically vary the token filtering threshold in the αᵢ term and measure its impact on both training dynamics and final cascade performance to identify optimal filtering levels.

2. **Deferral Rule Ablation:** Compare multiple deferral rule implementations (confidence threshold, entropy-based, prediction margin) while keeping CAT training constant to isolate the contribution of the training method from the routing strategy.

3. **Cross-Architecture Validation:** Apply CAT to a different model family (e.g., Llama, GPT) and a different task domain (e.g., code generation, long-form QA) to test the generalizability of the findings beyond PALM-2 models.