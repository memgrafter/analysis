---
ver: rpa2
title: Could Chemical LLMs benefit from Message Passing
arxiv_id: '2405.08334'
source_url: https://arxiv.org/abs/2405.08334
tags:
- mpnn
- graph
- learning
- language
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether combining molecular language models
  with message passing neural networks can improve molecular property prediction.
  Two integration strategies are proposed: contrastive learning, where an MPNN supervises
  the training of a language model, and fusion, which combines information from both
  models.'
---

# Could Chemical LLMs benefit from Message Passing

## Quick Facts
- arXiv ID: 2405.08334
- Source URL: https://arxiv.org/abs/2405.08334
- Reference count: 8
- This paper investigates whether combining molecular language models with message passing neural networks can improve molecular property prediction.

## Executive Summary
This paper explores whether integrating molecular language models (LMs) with message passing neural networks (MPNNs) can enhance molecular property prediction. The authors propose two integration strategies: contrastive learning, where an MPNN supervises the training of an LM, and fusion, which combines information from both models. Experiments on MoleculeNet datasets show that both approaches outperform baselines on smaller molecular graphs for both classification and regression tasks, with node-level contrastive learning achieving the best results. However, on larger graphs like QM9, only certain fusion methods (specifically LM2MPNN) show improvement. The results suggest that information integration is beneficial for smaller molecular structures but may not scale well to larger ones.

## Method Summary
The paper investigates two strategies for integrating molecular language models with message passing neural networks: contrastive learning and fusion. In contrastive learning, the MPNN acts as an auxiliary model that supervises the LM training through a triplet loss, forcing the LM to align its representations with the structural information captured by the MPNN. The fusion approach combines LM and MPNN embeddings using various operations (sum, max, concatenation, gate) to create richer representations for downstream tasks. The authors use pretrained ChemBERTa and ChemBERTa-2 as language models and NNConv as the MPNN architecture. Both integration strategies are evaluated on classification and regression tasks across multiple MoleculeNet datasets with varying molecular graph sizes.

## Key Results
- Contrastive learning and fusion methods outperform baseline models on smaller molecular graphs (ESOL, BACE, BBBP)
- Node-level contrastive learning achieves the best overall performance across datasets
- Integration approaches fail to improve performance on larger molecular graphs (QM9, HIV)
- Among fusion methods, LM2MPNN consistently shows better performance than other fusion variants

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning using MPNN embeddings as supervision improves molecular property prediction by providing geometric structural information that LM lacks.
- Mechanism: The MPNN learns structural embeddings from molecular graphs, and these embeddings are used to supervise the LM via contrastive loss, forcing the LM to align its representations with the geometric structure captured by MPNN.
- Core assumption: Structural information encoded by MPNN is complementary to the semantic information captured by LM, and their integration improves predictive performance.
- Evidence anchors:
  - [abstract] "We propose two strategies to evaluate whether an information integration can enhance the performance: contrast learning, which involves utilizing an MPNN to supervise the training of the LM"
  - [section 3.2] "We use GNN as an auxiliary model to supervise the training of the language model, while we only use the language model for downstream tasks."
- Break condition: If the MPNN and LM representations are too dissimilar or if the contrastive loss does not effectively align them, the integration may not improve performance.

### Mechanism 2
- Claim: Fusion methods combine LM and MPNN embeddings to create richer representations that capture both semantic and structural information.
- Mechanism: Different fusion strategies (late fusion, MPNN2LM, LM2MPNN) are used to combine embeddings from both models, allowing downstream tasks to benefit from both sources of information.
- Core assumption: The combined representation from both models is more informative than either representation alone for predicting molecular properties.
- Evidence anchors:
  - [abstract] "fusion, which exploits information from both models"
  - [section 3.3] "we exploit information from both models to generate the output for downstream tasks"
- Break condition: If the fusion operation does not effectively combine the complementary information or if one model's information dominates, the fusion may not improve performance.

### Mechanism 3
- Claim: Integration methods are more effective on smaller molecular graphs because the structural complexity is lower and easier to capture.
- Mechanism: Smaller graphs have simpler structures that can be more easily represented and aligned between LM and MPNN, leading to better integration performance.
- Core assumption: The effectiveness of integration is inversely related to the size and complexity of the molecular graph.
- Evidence anchors:
  - [abstract] "Our empirical analysis reveals that the integration approaches exhibit superior performance compared to baselines when applied to smaller molecular graphs, while these integration approaches do not yield performance enhancements on large scale graphs."
  - [section 5, Observation 1] "Using the pretrained ChemBERTa-2, we found that both contrastive learning and fusion methods outperform baseline models in ESOL, BACE, and BBBP where they are relatively small compared with QM9 and HIV datasets."
- Break condition: If the integration methods can be adapted to handle larger graphs effectively, the size limitation may be overcome.

## Foundational Learning

- Concept: Molecular graph representation
  - Why needed here: Understanding how molecules are represented as graphs is crucial for grasping the role of MPNN and the integration with LM.
  - Quick check question: What are the key components of a molecular graph (nodes, edges, features)?

- Concept: Message Passing Neural Networks (MPNN)
  - Why needed here: MPNN is the structural model used in the integration, and understanding its mechanism is essential for implementing the methods.
  - Quick check question: How does MPNN update node representations through message passing?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is one of the integration strategies, and understanding its principles is necessary for implementing and debugging the method.
  - Quick check question: What is the objective of contrastive learning, and how does it differ from supervised learning?

## Architecture Onboarding

- Component map:
  Input SMILES string and molecular graph -> LM (ChemBERTa/ChemBERTa-2) and MPNN (NNConv) -> Integration module (contrastive learning or fusion) -> Downstream task prediction

- Critical path:
  1. Tokenize SMILES and construct molecular graph
  2. Generate embeddings using LM and MPNN
  3. Apply integration strategy (contrastive learning or fusion)
  4. Perform downstream task prediction

- Design tradeoffs:
  - Choice of integration strategy (contrastive learning vs. fusion)
  - Selection of fusion operation (sum, max, concatenation, gate)
  - Handling of larger molecular graphs (may require different strategies)

- Failure signatures:
  - Poor performance on downstream tasks
  - Instability during training (e.g., exploding gradients)
  - Overfitting on small datasets

- First 3 experiments:
  1. Implement baseline LM and MPNN models separately to establish performance baselines.
  2. Implement contrastive learning integration and evaluate on small molecular graphs (e.g., ESOL, BACE, BBBP).
  3. Implement fusion integration and compare its performance with contrastive learning on the same datasets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the integration of chemical LLMs and MPNNs improve performance on very large molecular graphs (beyond QM9)?
- Basis in paper: [explicit] The paper states that integration approaches do not yield performance enhancements on large scale graphs.
- Why unresolved: The experiments only tested up to QM9 dataset size, which is considered moderate. The authors explicitly mention wanting to test on larger benchmarks like PCQM4Mv2 in future work.
- What evidence would resolve it: Experimental results showing whether fusion or contrastive learning methods improve or maintain performance on significantly larger molecular graph datasets.

### Open Question 2
- Question: What is the optimal dataset splitting strategy for molecular property prediction tasks when integrating chemical LLMs and MPNNs?
- Basis in paper: [explicit] The paper found that different dataset splits (ratios of 9:0.5:0.5, 8:1:1, 7:2:1, 6:2:2) impacted performance differently for BBBP and ESOL datasets.
- Why unresolved: While the paper tested several splitting strategies, it didn't establish a general principle or optimal approach that works across different molecular property prediction tasks.
- What evidence would resolve it: Systematic experiments across multiple molecular datasets with varying properties (classification vs regression) to determine the most effective train-validation-test split ratios.

### Open Question 3
- Question: Does pretraining language models on molecular text representations actually capture meaningful chemical semantics, or is it merely learning superficial patterns?
- Basis in paper: [inferred] The paper mentions that "pretrained molecule language models are capable of encoding chemical elements semantically without learning structures," but questions whether this encoding is truly meaningful.
- Why unresolved: The paper demonstrates that chemical LLMs perform well on molecular property prediction, but doesn't directly test whether the learned representations capture genuine chemical understanding versus statistical patterns.
- What evidence would resolve it: Ablation studies testing whether fine-tuned chemical LLMs can generalize to novel molecular structures or chemical properties not seen during pretraining, or comparative studies with chemically-informed representations.

## Limitations
- Integration methods show performance degradation on larger molecular graphs (QM9, HIV)
- Limited testing of integration approaches on very large molecular datasets
- Unclear optimal dataset splitting strategy for molecular property prediction tasks

## Confidence
- **High Confidence**: The core observation that integration methods improve performance on smaller molecular graphs is well-supported by experimental results across multiple datasets and both integration strategies.
- **Medium Confidence**: The claim that information from MPNNs and LMs is complementary has theoretical justification but limited empirical validation.
- **Low Confidence**: The assertion that integration methods "do not yield performance enhancements on large scale graphs" is based on limited testing of only two large datasets.

## Next Checks
1. Perform systematic ablation studies removing individual components of both integration strategies to quantify their individual contributions to performance improvements.
2. Conduct a comprehensive grid search or Bayesian optimization over key hyperparameters for both integration strategies across all datasets.
3. Design controlled experiments varying molecular graph size systematically to identify the exact threshold where integration methods begin to degrade.