---
ver: rpa2
title: 'Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent
  Framework'
arxiv_id: '2411.16707'
source_url: https://arxiv.org/abs/2411.16707
tags:
- simulation
- tasks
- task
- power
- gpt4o
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of enabling large language
  models (LLMs) to perform power system simulations, which remains difficult due to
  limited domain-specific knowledge, reasoning limitations, and imprecise handling
  of simulation parameters. To overcome these issues, the authors propose a feedback-driven
  multi-agent framework integrating three modules: an enhanced retrieval-augmented
  generation (RAG) module with adaptive query planning and a triple-based knowledge
  structure, an improved reasoning module with chain-of-thought prompting, and a dynamic
  environmental acting module with error-feedback.'
---

# Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework

## Quick Facts
- arXiv ID: 2411.16707
- Source URL: https://arxiv.org/abs/2411.16707
- Authors: Mengshuo Jia; Zeyu Cui; Gabriela Hug
- Reference count: 28
- Success rates of 93.13% and 96.85% on DALINE and MATPOWER respectively

## Executive Summary
This paper addresses the challenge of enabling large language models (LLMs) to perform power system simulations, which remains difficult due to limited domain-specific knowledge, reasoning limitations, and imprecise handling of simulation parameters. To overcome these issues, the authors propose a feedback-driven multi-agent framework integrating three modules: an enhanced retrieval-augmented generation (RAG) module with adaptive query planning and a triple-based knowledge structure, an improved reasoning module with chain-of-thought prompting, and a dynamic environmental acting module with error-feedback. Evaluated on 69 diverse tasks from DALINE and MATPOWER, the framework achieves success rates of 93.13% and 96.85%, respectively, significantly outperforming baseline LLMs and supervised fine-tuning. It also supports rapid, cost-effective task execution, completing simulations in ~30 seconds at ~0.014 USD per task.

## Method Summary
The proposed framework consists of three core modules: an enhanced RAG module that decomposes simulation requests into function-related and option-related sub-queries using adaptive query planning and retrieves information from a triple-based knowledge structure; an improved reasoning module that employs chain-of-thought prompting and few-shot examples to systematically identify simulation functions, learn syntax, extract options, and generate code; and an environmental acting module that executes generated code, detects errors, and initiates an iterative feedback loop where error reports become new retrieval requests for code revision. The framework is evaluated on 69 tasks from DALINE and MATPOWER simulation environments.

## Key Results
- Achieves 93.13% success rate on DALINE and 96.85% on MATPOWER benchmark tasks
- Completes simulations in approximately 30 seconds per task
- Operates at cost-effective rate of ~0.014 USD per task
- Significantly outperforms baseline LLMs and supervised fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
Standard RAG treats simulation requests as single queries, conflating functions and options. The enhanced RAG decomposes requests into function-related and option-related sub-queries using few-shot CoT prompting to extract implicit keywords. The triple-based option document preserves logical relationships between options and functions, enabling precise retrieval.

### Mechanism 2
The enhanced reasoning module provides structured guidance through universal actions (function identification, syntax learning, option extraction, code generation). It integrates static basic knowledge (tool fundamentals) and dynamic retrieval knowledge (request-specific details from RAG). Few-shot CoT ensures systematic reasoning.

### Mechanism 3
The environmental acting module executes simulation code and triggers an automatic feedback loop when errors occur. The error report becomes a new request processed by the RAG module, which retrieves relevant information. This updated knowledge feeds back into the reasoning module for code revision, continuing until success or maximum attempts.

## Foundational Learning

- **Concept**: Chain-of-thought prompting
  - Why needed here: Enables LLMs to break down complex simulation tasks into sequential reasoning steps, improving accuracy for multi-step reasoning tasks.
  - Quick check question: Can you explain how CoT prompting differs from standard prompting and why it's particularly useful for power system simulations?

- **Concept**: Retrieval-augmented generation (RAG)
  - Why needed here: Allows LLMs to access external domain-specific knowledge beyond their pre-training data, crucial for handling simulation tools like DALINE that weren't in the training corpus.
  - Quick check question: What are the three key steps in a standard RAG system, and how does the enhanced RAG modify these steps?

- **Concept**: Few-shot learning
  - Why needed here: Provides examples that help LLMs understand the format and requirements of power system simulation tasks, especially for tools with specific syntax and conventions.
  - Quick check question: How does few-shot prompting help the retrieval agent identify simulation functions and options from natural language descriptions?

## Architecture Onboarding

- **Component map**: Input Layer (Simulation requests) -> Enhanced RAG Module -> Enhanced Reasoning Module -> Environmental Acting Module -> Feedback Loop -> Output Layer (Results)
- **Critical path**: Request → RAG retrieval → Reasoning → Code generation → Environment execution → Error check → (Feedback loop if error) → Result
- **Design tradeoffs**:
  - Complexity vs. Performance: More sophisticated modules improve accuracy but increase system complexity
  - Cost vs. Speed: Higher attempt budgets improve success rates but increase token costs and execution time
  - Generalization vs. Specificity: Universal reasoning frameworks work across tools but may miss tool-specific optimizations
- **Failure signatures**:
  - No progress after multiple attempts: Suggests fundamental misunderstanding of simulation requirements
  - Consistent option/function misidentification: Indicates RAG retrieval issues
  - Code execution errors that cannot be resolved: Points to insufficient error reporting or ambiguous requirements
- **First 3 experiments**:
  1. Simple AC power flow on IEEE 9-bus system using MATPOWER - tests basic function identification and syntax
  2. Data generation task on case39 using DALINE - tests option handling and retrieval accuracy
  3. Continuation power flow with multiple constraints - tests complex reasoning and feedback loop effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed framework handle ambiguous or underspecified simulation requests? The paper acknowledges this as a challenge but does not provide a detailed solution or evaluation of how the framework addresses such cases.

### Open Question 2
What is the impact of simulation tool-specific error-reporting systems on the framework's performance? The paper notes that the effectiveness of automatic error correction depends partly on the quality of the simulation tool's error-reporting system but does not provide a systematic comparison across tools.

### Open Question 3
How does the framework's performance scale with increasing task complexity and number of sub-queries? The paper shows that complex tasks tend to increase the likelihood of errors in LLMs but does not explore the limits of the framework's ability to handle highly complex tasks.

## Limitations

- Framework performance depends heavily on quality of triple-based knowledge structure and clarity of error messages
- Cost-effectiveness assumes access to commercial LLM APIs which may not be available in all deployment scenarios
- Long-term scalability and cost projections for large-scale deployment remain uncertain

## Confidence

- **High**: The framework's core architecture and demonstrated success rates on benchmark tasks
- **Medium**: The generalizability of the triple-based knowledge structure to other power system tools
- **Low**: Long-term scalability and cost projections for large-scale deployment

## Next Checks

1. **Cross-tool Validation**: Test the framework on additional power system simulation tools (e.g., PSAT, PowerWorld) to assess the adaptability of the triple-based knowledge structure and adaptive query planning across different tool ecosystems.

2. **Error Message Robustness**: Systematically evaluate the framework's performance when error messages are intentionally obfuscated or incomplete to determine the minimum quality threshold required for effective feedback-driven correction.

3. **Cost Scaling Analysis**: Conduct experiments varying the number of simulation tasks and measuring the relationship between task volume, token consumption, and total cost to validate the claimed cost-effectiveness at different scales.