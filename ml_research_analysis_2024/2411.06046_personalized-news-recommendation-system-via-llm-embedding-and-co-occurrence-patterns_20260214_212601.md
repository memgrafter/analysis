---
ver: rpa2
title: Personalized News Recommendation System via LLM Embedding and Co-Occurrence
  Patterns
arxiv_id: '2411.06046'
source_url: https://arxiv.org/abs/2411.06046
tags:
- news
- co-occurrence
- keywords
- embedding
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces LECOP, a news recommendation method that\
  \ leverages LLM embeddings and co-occurrence patterns to address semantic understanding\
  \ and cold-start challenges. It fine-tunes LLMs via contrastive learning on large-scale\
  \ datasets to capture rich news semantics and constructs three types of co-occurrence\
  \ graphs\u2014news ID, item-item keyword, and intra-item keyword\u2014to model collaborative\
  \ signals."
---

# Personalized News Recommendation System via LLM Embedding and Co-Occurrence Patterns

## Quick Facts
- arXiv ID: 2411.06046
- Source URL: https://arxiv.org/abs/2411.06046
- Reference count: 13
- Primary result: LECOP improves AUC, MRR, and nDCG@5/10 over strong baselines like NRMS and PPSR

## Executive Summary
LECOP addresses semantic understanding and cold-start challenges in news recommendation by combining LLM embeddings with co-occurrence patterns. The method fine-tunes LLMs via contrastive learning on large-scale datasets to capture rich news semantics, then constructs three types of co-occurrence graphs—news ID, item-item keyword, and intra-item keyword—to model collaborative signals. Experimental results on MIND-small demonstrate that this dual approach of semantic encoding and detailed co-occurrence modeling significantly outperforms existing methods.

## Method Summary
LECOP fine-tunes GLM4-9B-chat using contrastive learning on MIND and E5 datasets, where news titles serve as queries and abstracts as positive examples. The fine-tuned LLM generates keywords for all news items from title+abstract combinations. Three co-occurrence graphs are constructed using sliding windows over click sequences: news ID co-occurrence, item-item keyword co-occurrence, and intra-item keyword co-occurrence. Node2Vec embeddings are generated from these graphs and combined with LLM embeddings through direct addition to create final news representations. The system uses batch size 512, learning rate 2e-4, and Adam optimizer during training.

## Key Results
- LECOP achieves higher AUC, MRR, and nDCG@5/10 than strong baselines like NRMS and PPSR
- Combining LLM embeddings with co-occurrence patterns provides complementary benefits
- The three-graph approach (news ID, item-item keywords, intra-item keywords) captures different aspects of collaborative signals

## Why This Works (Mechanism)

### Mechanism 1
LLM fine-tuning via contrastive learning on news + public datasets yields semantically richer news embeddings than static methods like GloVe. Contrastive learning forces the model to pull positive pairs (news title/abstract) closer and push negative pairs apart, forcing it to learn fine-grained semantic distinctions. The auxiliary public datasets (e.g., E5) provide complementary semantic signals that improve generalization to news text.

### Mechanism 2
Co-occurrence patterns (ID, Item-Item keywords, Intra-Item keywords) capture complementary collaborative signals that pure ID-based or semantic-only models miss. Sliding window over click sequences generates edges in three graphs; Node2Vec then embeds items/keywords, encoding neighborhood structure. Keywords generated by LLM from title+abstract are high-quality proxies for news topics, and co-occurrence within a short window implies topical relatedness.

### Mechanism 3
Concatenating LLM embeddings with co-occurrence embeddings yields better final representations than either alone. LLM embedding captures deep semantic meaning; co-occurrence embedding captures local collaborative structure; addition fuses both. The two embedding spaces are complementary and not highly redundant; simple addition is sufficient to combine them.

## Foundational Learning

- Concept: Contrastive learning in embedding space
  - Why needed here: Enables fine-tuning LLM to produce discriminative news embeddings rather than generic ones
  - Quick check question: In the training data format described, what serves as the positive example for a news title?

- Concept: Co-occurrence graph construction
  - Why needed here: Provides a data-efficient way to encode collaborative signals for cold-start items using keywords
  - Quick check question: How many types of co-occurrence graphs are built in LECOP, and what nodes do they contain?

- Concept: Node2Vec for graph embedding
  - Why needed here: Transforms graph structure into dense vectors preserving neighborhood similarity
  - Quick check question: In the sliding window example, which two keywords form the Intra-Item co-occurrence pair?

## Architecture Onboarding

- Component map: News data -> Contrastive fine-tuning -> Keyword generation -> Graph construction -> Node2Vec embeddings -> Embedding fusion -> Ranking model
- Critical path: 1) Fine-tune LLM with contrastive pairs, 2) Generate keywords for all news items, 3) Build 3 co-occurrence graphs from click sequences, 4) Run Node2Vec to get co-occurrence embeddings, 5) Add co-occurrence + LLM embeddings for final news vectors
- Design tradeoffs: Keyword quality vs. generation cost (LLM-based keywords are richer but slower than TF-IDF), graph sparsity (too large a sliding window dilutes co-occurrence signal), embedding fusion (addition is simple but may ignore relative importance)
- Failure signatures: LLM embeddings too generic → contrastive pairs ineffective, graph embeddings noisy → low AUC/MRR despite semantic gains, keyword overlap low → co-occurrence patterns add little
- First 3 experiments: 1) Baseline: Replace LECOP's LLM embeddings with GloVe embeddings; compare performance, 2) Ablation: Remove one co-occurrence graph (e.g., Intra-Item); observe drop in metrics, 3) Hyperparameter: Vary sliding window size (1-5) and measure impact on AUC/MRR

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of LECOP vary across different news recommendation datasets beyond MIND-small, such as MIND-large or other public news recommendation datasets? The paper only reports experimental results on MIND-small due to limited resources, leaving performance on larger datasets unexamined. Conducting experiments on MIND-large and other public news recommendation datasets would compare LECOP's performance metrics across different data scales and distributions.

### Open Question 2
What is the impact of varying the sliding window size in constructing co-occurrence patterns on the effectiveness of LECOP? The paper mentions using a sliding window of size 2 but does not explore the effects of different window sizes. Systematic experimentation with varying sliding window sizes (e.g., 1, 2, 3, 4) would measure changes in LECOP's performance metrics and determine the optimal window size.

### Open Question 3
How does the choice of LLM (e.g., GLM4-9B-chat) affect the quality of generated keywords and the overall performance of LECOP compared to other LLMs? The paper specifies the use of GLM4-9B-chat but does not compare its effectiveness with other LLMs. Comparative experiments using different LLMs (e.g., GPT-4, BERT, RoBERTa) for keyword generation and assessing their impact on LECOP's performance metrics would identify the most effective LLM for this task.

## Limitations

- Exact prompt templates for keyword generation are only partially described, with no systematic prompt engineering strategy outlined
- Hyperparameters for Node2Vec are not specified, including window size, number of walks, and embedding dimensions
- While contrastive learning is described conceptually, specific negative sampling strategy and batch construction details are not provided

## Confidence

**High Confidence**: The core claim that combining LLM embeddings with co-occurrence patterns improves recommendation performance is well-supported by experimental results showing consistent improvements across metrics compared to strong baselines.

**Medium Confidence**: The assertion that contrastive fine-tuning on auxiliary datasets provides semantic benefits beyond the training data alone is plausible but not directly validated through ablation studies.

**Low Confidence**: The claim that simple addition is the optimal method for combining LLM and co-occurrence embeddings lacks empirical comparison with alternative fusion strategies like weighted addition or attention mechanisms.

## Next Checks

1. **Ablation Study**: Systematically remove each co-occurrence graph type (news ID, Item-Item keywords, Intra-Item keywords) and measure performance degradation to quantify the contribution of each pattern.

2. **Prompt Engineering Analysis**: Test multiple keyword generation prompts with varying specificity and evaluate their impact on downstream recommendation performance to establish optimal prompt design.

3. **Fusion Strategy Comparison**: Replace the simple addition of embeddings with weighted sum and attention-based fusion methods, comparing results to determine if the current approach is indeed optimal.