---
ver: rpa2
title: A Two-Stage Proactive Dialogue Generator for Efficient Clinical Information
  Collection Using Large Language Model
arxiv_id: '2410.03770'
source_url: https://arxiv.org/abs/2410.03770
tags:
- agent
- patient
- dialogue
- information
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a two-stage proactive dialogue generator for
  efficient clinical information collection using large language models. The system
  addresses the challenge of automating patient information collection during clinical
  diagnosis, which is typically time-consuming and inefficient.
---

# A Two-Stage Proactive Dialogue Generator for Efficient Clinical Information Collection Using Large Language Model

## Quick Facts
- arXiv ID: 2410.03770
- Source URL: https://arxiv.org/abs/2410.03770
- Authors: Xueshen Li; Xinlong Hou; Nirupama Ravi; Ziyi Huang; Yu Gan
- Reference count: 40
- Primary result: Two-stage proactive dialogue generator for clinical information collection using Llama-3-8B-Instruct

## Executive Summary
This paper presents a two-stage proactive dialogue generator designed to automate patient information collection during clinical diagnosis. The system addresses the inefficiency of traditional information collection methods by using large language models to proactively generate queries and collect diagnostic information. The approach combines a fine-tuned doctor agent, a two-stage recommendation system, and an interactive patient agent to create a complete dialogue system. Experimental results demonstrate the system can generate clinical queries that mimic real doctors' conversation style while effectively collecting relevant diagnostic information.

## Method Summary
The method consists of a fine-tuned Llama-3-8B-Instruct model as the doctor agent, which is trained on real-world clinical dialogue data to enable proactive querying. The two-stage recommendation system generates multiple query candidates and ranks them based on relevance to medical history and logical correctness. An interactive patient agent responds to doctor queries using medical history information, ensuring logical consistency across multi-round conversations. The system is trained on a dataset of 1,120 doctor-patient dialogues and evaluated using BLEU, ROUGE, F1 scores, and ChatGPT-3.5 for high-level metrics assessment.

## Key Results
- BLEU scores up to 0.273 for dialogue generation, demonstrating reasonable language quality
- ROUGE scores up to 0.140, indicating moderate overlap with reference dialogues
- F1 scores for extracted diagnostic information showing effective collection of disease-related data
- High-level metrics (Fluency, Professionalism, Safety) evaluated using ChatGPT-3.5, with fine-tuning showing improvements

## Why This Works (Mechanism)
The two-stage recommendation structure addresses under-exploration and inflexibility issues by generating multiple query candidates and selecting the most relevant ones based on ranking criteria. The fine-tuned doctor agent can proactively generate questions rather than just responding to patient queries, mimicking real clinical practice. The interactive patient agent ensures logical consistency across multi-round conversations by responding based on medical history rather than generating arbitrary responses.

## Foundational Learning
1. **Large Language Model Fine-tuning for Domain-Specific Tasks**: Why needed - to adapt general LLMs to clinical dialogue patterns and medical terminology; Quick check - verify the model can generate medically relevant queries
2. **Two-Stage Recommendation Systems**: Why needed - to balance exploration of different query types with exploitation of most relevant questions; Quick check - compare dialogue quality with single-stage approaches
3. **Interactive Dialogue Systems**: Why needed - to create realistic multi-turn conversations that collect comprehensive diagnostic information; Quick check - ensure logical consistency across conversation turns
4. **Clinical Dialogue Structure**: Why needed - understanding the flow and information needs of real medical conversations; Quick check - validate generated dialogues follow clinical best practices
5. **Evaluation Metrics for Dialogue Systems**: Why needed - to quantitatively assess dialogue quality and information collection effectiveness; Quick check - compare metric scores against human-generated dialogues
6. **Medical History Integration**: Why needed - to contextualize queries and responses based on patient background; Quick check - verify the system uses history appropriately in conversation flow

## Architecture Onboarding

**Component Map:** Fine-tuned Doctor Agent -> Two-Stage Recommendation (Generation + Ranking) -> Interactive Patient Agent -> Evaluation

**Critical Path:** The dialogue generation loop where the doctor agent generates queries, the recommendation system selects the best query, the patient agent responds based on medical history, and the conversation continues until diagnostic information is collected.

**Design Tradeoffs:** The two-stage recommendation structure trades computational overhead for improved query relevance and dialogue quality. Using separate agents for doctor and patient roles allows specialization but requires careful coordination. Fine-tuning on limited clinical data balances adaptation with avoiding overfitting.

**Failure Signatures:** Poor dialogue quality indicated by low BLEU/ROUGE scores; Inconsistent multi-round conversations shown by logical contradictions; Inefficient information collection demonstrated by low F1 scores on diagnostic data; Unprofessional or unsafe dialogues indicated by low ChatGPT evaluation scores.

**First Experiments:** 1) Fine-tune Llama-3-8B-Instruct on the medical dialogue dataset and evaluate basic dialogue generation quality; 2) Implement and test the two-stage recommendation system with different numbers of query candidates; 3) Evaluate the interactive patient agent's ability to maintain logical consistency across multiple conversation turns.

## Open Questions the Paper Calls Out
### Open Question 1
How does the proposed two-stage recommendation structure compare to alternative dialogue generation approaches in terms of clinical information collection completeness? The paper mentions that the two-stage recommendation structure addresses under-exploration and inflexibility issues, but doesn't directly compare it to other dialogue generation approaches. Comparative experiments between the two-stage recommendation structure and other dialogue generation approaches on the same dataset using identical evaluation metrics would resolve this question.

### Open Question 2
What is the impact of fine-tuning the patient agent on the overall dialogue quality and diagnostic information collection accuracy? The paper mentions that fine-tuning the patient agent improves fluency, professionalism, and safety scores, but doesn't provide detailed analysis of its impact on diagnostic information collection. Detailed analysis comparing diagnostic information collection accuracy and completeness between conversations with fine-tuned and non-fine-tuned patient agents would resolve this question.

### Open Question 3
How does the proposed system handle rare or complex medical conditions that may require specialized knowledge beyond general clinical dialogue? The paper mentions the system is generic for different types of diseases and languages, but doesn't address its performance on rare or complex conditions. Experimental results and analysis of the system's performance on rare and complex medical conditions, including comparison with domain-specific expert systems, would resolve this question.

### Open Question 4
What are the computational requirements and real-time performance characteristics of the proposed dialogue system in clinical settings? The paper mentions the system runs on a single Nvidia A6000 GPU but doesn't provide detailed analysis of real-time performance or computational requirements in clinical settings. Detailed performance analysis including response time, computational requirements, and scalability testing in realistic clinical scenarios with multiple concurrent users would resolve this question.

## Limitations
- Moderate evaluation metric scores (BLEU up to 0.273, ROUGE up to 0.140) suggest the system may not fully capture clinical dialogue complexity
- Reliance on ChatGPT-3.5 for high-level metric evaluation introduces potential subjectivity and may not accurately reflect clinical utility
- Lack of comparison with existing clinical dialogue systems makes it difficult to assess relative performance improvements
- Dataset size (1,120 dialogues) is relatively small for training large language models, potentially limiting generalizability

## Confidence
- **High confidence**: The two-stage recommendation system architecture is technically sound and the methodology for fine-tuning Llama-3-8B-Instruct is clearly described
- **Medium confidence**: The experimental results are reproducible given the dataset, but the moderate metric scores suggest room for improvement in dialogue quality
- **Medium confidence**: The clinical relevance and practical utility of the system for real-world diagnosis is plausible but not fully validated by the presented experiments

## Next Checks
1. Conduct ablation studies to isolate the contribution of the two-stage recommendation system versus simple fine-tuning approaches
2. Test the system with human doctors to evaluate clinical utility and identify gaps between generated dialogues and actual clinical practice
3. Expand evaluation to include edge cases and rare diseases to assess the system's robustness beyond the training distribution