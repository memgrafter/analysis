---
ver: rpa2
title: 'MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio
  Synthesis'
arxiv_id: '2412.15322'
source_url: https://arxiv.org/abs/2412.15322
tags:
- audio
- gid00032
- gid00042
- gid00001
- gid00041
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMAudio, a novel multimodal joint training
  framework for high-quality video-to-audio synthesis. Unlike prior approaches that
  train solely on limited video data or add control modules to pretrained text-to-audio
  models, MMAudio jointly trains on both audio-visual and audio-text datasets, enabling
  effective data scaling and cross-modal semantic alignment.
---

# MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis

## Quick Facts
- arXiv ID: 2412.15322
- Source URL: https://arxiv.org/abs/2412.15322
- Reference count: 40
- Primary result: State-of-the-art video-to-audio synthesis performance with 157M parameters and 1.23s inference time for 8s clips

## Executive Summary
MMAudio introduces a novel multimodal joint training framework for high-quality video-to-audio synthesis that jointly trains on audio-visual and audio-text datasets. Unlike prior approaches that train solely on limited video data or add control modules to pretrained text-to-audio models, MMAudio learns a unified semantic space across modalities while maintaining competitive text-to-audio performance. The model achieves state-of-the-art results on VGGSound through a conditional synchronization module with high frame-rate visual features and aligned RoPE positional embeddings.

## Method Summary
MMAudio jointly trains a multimodal transformer architecture on both video-audio and audio-text datasets to learn a unified semantic space. The model uses N1 multimodal transformer blocks for cross-modal interaction followed by N2 audio-only transformer blocks for generation. A conditional synchronization module extracts high frame-rate visual features (24 fps) and injects them at the token level via adaLN layers for precise audio-visual alignment. Aligned RoPE positional embeddings scale frequencies proportionally to frame rate ratios for proper temporal correspondence. The model is trained with a flow matching objective and includes global and per-token conditioning for improved generation quality.

## Key Results
- Achieves state-of-the-art performance on VGGSound with lower Fréchet Distance (FD) scores across multiple feature extractors
- Demonstrates superior audio-visual temporal alignment with Synchformer DeSync scores improving from 0.146 to 0.107
- Maintains competitive text-to-audio performance while excelling at video-to-audio synthesis
- Achieves fast inference (1.23s for 8s clips) with 157M parameters

## Why This Works (Mechanism)

### Mechanism 1
Joint training on multimodal datasets improves both video-to-audio and text-to-audio performance compared to single-modality training. The model learns a shared semantic space across modalities, enabling bidirectional feature transfer where audio-text data improves semantic understanding that transfers to video-text pairs.

### Mechanism 2
The conditional synchronization module with high frame-rate visual features (24 fps) significantly improves audio-visual temporal alignment compared to lower frame-rate features. It extracts Synchformer features at 24 fps and injects them at the token level via adaLN layers, providing fine-grained control over synchronization.

### Mechanism 3
Aligned RoPE positional embeddings (scaled by frame rate ratios) improve temporal alignment by properly aligning sequences of different frame rates. By scaling the frequencies of positional embeddings in the visual stream proportionally to the frame rate ratio (31.25/8), the model maintains proper temporal correspondence between audio (31.25 fps) and visual (8 fps) tokens.

## Foundational Learning

- Concept: Flow matching objective
  - Why needed here: Provides a stable training objective for generative modeling that avoids instabilities of diffusion models while maintaining high sample quality
  - Quick check question: How does flow matching differ from denoising diffusion probabilistic models in terms of training objective and sampling?

- Concept: Multimodal transformer architecture
  - Why needed here: Enables joint processing of video, audio, and text modalities while allowing selective attention to different modalities depending on input
  - Quick check question: How does joint attention in multimodal transformers differ from separate modality-specific attention layers?

- Concept: Adaptive layer normalization (adaLN)
  - Why needed here: Provides a mechanism for global and token-level conditioning that can inject both global features and fine-grained synchronization information
  - Quick check question: What's the difference between global conditioning (broadcasting) and token-level conditioning in adaLN?

## Architecture Onboarding

- Component map: Multimodal transformer (N1 blocks with joint attention across video/text/audio) → Audio-only transformer (N2 blocks) → Conditional synchronization module (optional)
- Critical path: Input features → Projection layers → Multimodal transformer blocks → Audio-only transformer blocks → Flow prediction → Latent decoding
- Design tradeoffs: Joint training vs. separate training (complexity vs. performance), high frame-rate features vs. computational cost, aligned RoPE vs. simpler positional encodings
- Failure signatures: Poor temporal alignment suggests synchronization module issues, semantic misalignment suggests semantic space problems, low quality suggests flow prediction or latent space issues
- First 3 experiments:
  1. Test model with and without conditional synchronization module to isolate its impact on temporal alignment
  2. Test model with aligned vs. non-aligned RoPE embeddings to verify temporal alignment improvement
  3. Test model with different ratios of multimodal to single-modal transformer blocks to optimize performance vs. parameter efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MMAudio scale with larger training datasets beyond the current multimodal data? The paper notes diminishing returns with current data but suggests collecting open-world data beyond the 310 VGGSound classes could reduce performance gaps.

### Open Question 2
Can MMAudio be adapted to generate intelligible human speech while maintaining its strength in Foley sound effects? The paper acknowledges that MMAudio generates unintelligible mumbles when prompted for human speech, citing speech complexity as the limitation.

### Open Question 3
How does the conditional synchronization module's performance compare when using different high frame-rate visual feature extractors? The paper uses Synchformer for extracting 24 fps visual features but does not benchmark against alternative feature extractors.

### Open Question 4
What is the impact of varying the ratio of multimodal to single-modality transformer blocks on model performance and efficiency? The paper explores different N1:N2 ratios but only evaluates a limited range.

### Open Question 5
How robust is MMAudio to domain shift when tested on videos from distributions significantly different from VGGSound? The paper notes MMAudio struggles with concepts not well-covered by training data.

## Limitations

- Dataset Generalization: Performance on domains beyond VGGSound (YouTube, user-generated content) remains untested
- Temporal Resolution Trade-offs: Optimal frame rate for synchronization module not explored beyond 24 fps
- Semantic Space Alignment Assumptions: May not hold for abstract or culturally specific audio-visual relationships
- Model Complexity: 157M parameters and multiple specialized components increase engineering investment

## Confidence

- High Confidence: Empirical performance improvements on established metrics (Fréchet Distance, Inception Score, Synchformer DeSync) with reproducible ablation studies
- Medium Confidence: Theoretical justification for joint training's effectiveness in creating transferable semantic spaces based on empirical observations
- Low Confidence: Claim that MMAudio "maintains competitive performance in text-to-audio generation" based on limited qualitative examples rather than systematic quantitative comparison

## Next Checks

1. **Cross-Dataset Validation**: Evaluate MMAudio on diverse video datasets (Kinetics, Moments in Time, YouTube clips) to assess generalization beyond VGGSound's curated environment and measure performance degradation.

2. **Temporal Resolution Scaling Study**: Systematically vary the frame rate of visual features (8 fps, 16 fps, 24 fps, 48 fps) in the conditional synchronization module while measuring marginal improvement in Synchformer DeSync scores against computational overhead.

3. **Modality-Specific Performance Benchmarking**: Conduct controlled experiments comparing MMAudio's text-to-audio performance against dedicated text-to-audio models (AudioLDM, Make-A-Scene) using standardized benchmarks to quantify performance gaps and identify systematic biases.