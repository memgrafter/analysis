---
ver: rpa2
title: Optimistic Q-learning for average reward and episodic reinforcement learning
arxiv_id: '2407.13743'
source_url: https://arxiv.org/abs/2407.13743
tags:
- bound
- lemma
- span
- epoch
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an optimistic Q-learning algorithm for regret
  minimization in average reward reinforcement learning. The key innovation is an
  operator L that averages Bellman operators over a bounded hitting time H, enabling
  strict span contraction even with discount factor 1.
---

# Optimistic Q-learning for average reward and episodic reinforcement learning

## Quick Facts
- arXiv ID: 2407.13743
- Source URL: https://arxiv.org/abs/2407.13743
- Reference count: 40
- Key outcome: Achieves regret bound of O(H^5 S sqrt(AT)) in average reward setting and O(H^6 S sqrt(AT)) in episodic setting using optimistic Q-learning with L operator

## Executive Summary
This paper presents an optimistic Q-learning algorithm that unifies average reward and episodic reinforcement learning under a single framework. The key innovation is the L operator, which averages Bellman operators over a bounded hitting time H, enabling strict span contraction even with discount factor 1. Under the assumption that some frequent state can be reached in at most H steps from any state, the algorithm achieves near-optimal regret bounds in both settings. This provides a significant improvement over state-of-the-art model-free algorithms in the average reward setting while maintaining competitive performance in episodic RL.

## Method Summary
The algorithm maintains Q-value estimates Qh(s,a) and value estimates V h(s) for h=1,...,H, using the L-operator Lv = 1/H sum of Lh v for value iteration. It updates with optimistic Q-learning style with exploration bonuses, projects V to control span every K epochs, and uses epoch-based design with geometric growth. The algorithm assumes a frequent state s0 that can be reached within H steps from any state with probability at least p, and achieves regret bounds of O(H^5 S sqrt(AT)) in average reward setting and O(H^6 S sqrt(AT)) in episodic setting.

## Key Results
- Achieves regret bound O(H^5 S sqrt(AT)) in average reward RL setting
- Achieves regret bound O(H^6 S sqrt(AT)) in episodic RL setting
- Provides first model-free algorithm achieving near-optimal regret in average reward setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The L operator enables strict span contraction in average reward RL with discount factor 1
- Mechanism: The L operator is defined as Lv = 1/H Σ Lh v where Lh is h applications of the Bellman operator. Under Assumption 2 (bounded hitting time to frequent state s0), this operator contracts span by factor (1 - p/H) per application.
- Core assumption: There exists a frequent state s0 such that under any policy, the probability of visiting s0 within H steps is at least p
- Evidence anchors:
  - [abstract]: "Under the given assumption, we show that the L operator has a strict contraction (in span) even in the average-reward setting where the discount factor is 1"
  - [section 2.1]: "We show that under Assumption 2, the L operator has a strict contraction (in span) even in the average-reward setting where the discount factor is 1"
- Break condition: If the frequent state s0 cannot be reached within H steps under some policy with probability at least p, the contraction property fails

### Mechanism 2
- Claim: Optimistic Q-learning with the L operator achieves near-optimal regret bounds
- Mechanism: The algorithm maintains Qh(st, at) and V h(st) for h = 1,...,H, using Q-learning updates with exploration bonuses. It estimates Lh V iteratively and averages to approximate L V, enabling value iteration-like updates in the average reward setting.
- Core assumption: The algorithm can accurately estimate Lh V using Q-learning with sufficient exploration
- Evidence anchors:
  - [section 3.2]: "The algorithm maintains quantities Qh(s, a) and V h(s) for h = 1, ..., H and all s, a. At any time step t, the algorithm observes the current state, action, reward, and the next state"
  - [section 4]: "Our first main technical lemma shows that in every epoch, the algorithm essentially performs value iteration like updates Vℓ ← L Vℓ−1"
- Break condition: If exploration is insufficient, Q-value estimates become inaccurate and the algorithm fails to converge

### Mechanism 3
- Claim: Epoch-based design with projection controls span and ensures convergence
- Mechanism: The algorithm proceeds in epochs with geometrically increasing duration. Every K epochs, it projects V to control its span. The epoch break condition ensures sufficient visits to each state-action pair.
- Core assumption: The projection operator P can bound the span of V while preserving monotonicity properties
- Evidence anchors:
  - [section 3.2]: "A final detail is that occasionally (every K = O(H/p log(T)) epochs), a projection operation V ← P V is performed on V to control its span"
  - [section B.1]: "Define operator P : RS → RS as: for any v ∈ RS, [Pv](s) := min{2H∗, v(s) − mins∈S v(s)} + mins∈S v(s)"
- Break condition: If epoch length grows too slowly or projection is too aggressive, the algorithm may not explore sufficiently

## Foundational Learning

- Concept: Bellman operator contraction properties
  - Why needed here: Understanding why standard Bellman operator doesn't contract with discount factor 1 in average reward settings, and how the L operator overcomes this limitation
  - Quick check question: Why doesn't the standard Bellman operator L contract span when the discount factor is 1?

- Concept: Martingale concentration inequalities
  - Why needed here: Bounding the deviation of estimated Q-values from true values, crucial for proving optimism and regret bounds
  - Quick check question: How does Azuma-Hoeffding inequality help bound the cumulative reward deviation in reinforcement learning?

- Concept: Span contraction vs norm contraction
  - Why needed here: The algorithm relies on span contraction rather than norm contraction, which is more suitable for average reward settings where the optimal value function is only unique up to constant shift
  - Quick check question: What's the key difference between span contraction and norm contraction, and why is span contraction more appropriate for average reward RL?

## Architecture Onboarding

- Component map: Main loop -> Q-value estimator -> Value updater -> Projection module -> Policy selector
- Critical path:
  1. Observe state st
  2. Sample h uniformly and select action at = arg maxa Qh(st, a)
  3. Observe reward rt and next state st+1
  4. Update Qh(st, at) for all h using Q-learning
  5. Update V h(st) and V (st)
  6. Check epoch break condition
  7. Perform projection every K epochs

- Design tradeoffs:
  - Exploration vs exploitation: UCB bonuses ensure sufficient exploration but add overhead
  - Epoch length: Longer epochs reduce computational overhead but may delay learning
  - Projection frequency: More frequent projections better control span but increase computational cost

- Failure signatures:
  - Slow convergence: May indicate insufficient exploration or overly conservative learning rates
  - Oscillating Q-values: Could suggest learning rate too high or bonus terms too large
  - Span explosion: Likely means projection not frequent enough or K parameter too large

- First 3 experiments:
  1. Test on a simple unichain MDP with known frequent state to verify basic functionality and span contraction
  2. Compare regret on a queueing MDP where worst-case hitting time is large but frequent state assumption holds
  3. Stress test with varying H values to understand H-dependence in practice versus theory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dependence on H in the regret bound be reduced to match the near-optimal episodic Q-learning bound of $\tilde{O}(\sqrt{H^3SAT})$?
- Basis in paper: [explicit] The paper explicitly states that improving the dependence on H in their regret bound is an avenue for future research, noting that techniques from recent work on improving H dependence for episodic Q-learning may be applicable.
- Why unresolved: The current algorithm uses an averaging operator $L = \frac{1}{H}\sum_{h=1}^H L^h$ which introduces higher-order terms in H compared to standard Q-learning approaches.
- What evidence would resolve it: A modified algorithm design that achieves $\tilde{O}(H^3S\sqrt{AT})$ regret bound in the average reward setting, or a lower bound showing that the current $\tilde{O}(H^5S\sqrt{AT})$ bound is unavoidable.

### Open Question 2
- Question: How does the algorithm's performance degrade when the frequent state $s_0$ is not known a priori but needs to be learned?
- Basis in paper: [inferred] The paper assumes the frequent state $s_0$ is unknown but the bound H on hitting time is known. The algorithm's regret analysis relies on this assumption being satisfied.
- Why unresolved: The current analysis assumes the MDP structure (existence of a frequent state with bounded hitting time) is known. If the algorithm needs to identify this state while learning, the regret analysis would need to account for the additional exploration overhead.
- What evidence would resolve it: An extension of the algorithm and analysis that handles the case where both $s_0$ and H need to be learned, with corresponding regret bounds showing the trade-off between exploration for finding $s_0$ and exploitation for reward maximization.

### Open Question 3
- Question: What is the sample complexity of the algorithm for achieving PAC guarantees under the given assumptions?
- Basis in paper: [explicit] The paper provides a PAC guarantee corollary showing that $\tilde{O}(H^{10}S^2A/\epsilon^2)$ samples are needed for $(\epsilon, \delta)$-PAC policy.
- Why unresolved: While the regret bound is $\tilde{O}(H^5S\sqrt{AT})$, the PAC sample complexity involves squaring this regret bound, leading to the high $H^{10}$ dependence. It's unclear if this can be improved.
- What evidence would resolve it: A tighter analysis showing that $\tilde{O}(H^6S^2A/\epsilon^2)$ or better samples suffice for PAC guarantees, or a lower bound proving that the $H^{10}$ dependence is necessary under these assumptions.

## Limitations

- Assumption Dependency: Algorithm performance critically depends on Assumption 2 (frequent state reachability within H steps), which may not hold in many practical MDPs
- H-parameter Sensitivity: Theoretical bounds show H^5 and H^6 dependencies, which could be prohibitive in practice despite authors' arguments that H may be small in some domains
- Exploration Bonus Tuning: Implementation details for selecting parameters that depend on H/p are not fully specified, potentially affecting practical performance

## Confidence

**High Confidence**: The span contraction property of the L operator under Assumption 2 is rigorously proven. The theoretical framework connecting the L operator to regret bounds is sound.

**Medium Confidence**: The regret bounds of O(H^5 S√AT) and O(H^6 S√AT) are proven, but their practical achievability depends heavily on the validity of assumptions and proper parameter tuning.

**Low Confidence**: Empirical validation across diverse MDPs is limited. The algorithm's performance in practice may deviate significantly from theoretical predictions, especially for large H values.

## Next Checks

1. **Assumption Stress Testing**: Systematically evaluate algorithm performance on MDPs with varying degrees of compliance to Assumption 2, measuring regret degradation as the assumption becomes less valid.

2. **H-parameter Scaling Study**: Run extensive experiments varying H from small (H=2,3) to moderate (H=10,20) values to empirically verify the H^5 and H^6 scaling predictions.

3. **Cross-Setting Comparison**: Implement and compare against state-of-the-art average reward algorithms (e.g., A_RAGE, UCB-AMQ) on identical MDPs to validate the claimed improvement in the average reward setting.