---
ver: rpa2
title: 'SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups'
arxiv_id: '2410.02942'
source_url: https://arxiv.org/abs/2410.02942
tags:
- distribution
- latexit
- diffusion
- learning
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SymmetricDiffusers, a novel discrete diffusion
  model for learning distributions over finite symmetric groups. The core method uses
  riffle shuffles as the forward diffusion process and introduces a generalized Plackett-Luce
  (GPL) distribution for the reverse process, which is provably more expressive than
  existing approaches.
---

# SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups

## Quick Facts
- arXiv ID: 2410.02942
- Source URL: https://arxiv.org/abs/2410.02942
- Authors: Yongxing Zhang; Donglin Yang; Renjie Liao
- Reference count: 40
- Key outcome: Introduces a novel discrete diffusion model for learning distributions over finite symmetric groups using riffle shuffles as forward process and generalized Plackett-Luce distribution for reverse process

## Executive Summary
This paper introduces SymmetricDiffusers, a novel discrete diffusion model designed to learn distributions over finite symmetric groups (permutations). The method uses riffle shuffles as the forward diffusion process and introduces a generalized Plackett-Luce (GPL) distribution for the reverse process, which is provably more expressive than existing approaches. The authors also propose a denoising schedule to improve sampling and learning efficiency. Experiments on sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems show state-of-the-art or competitive performance compared to differentiable sorting networks and discrete diffusion baselines.

## Method Summary
SymmetricDiffusers learns distributions over symmetric groups $S_n$ using riffle shuffles as the forward diffusion process and a generalized Plackett-Luce (GPL) distribution for the reverse process. The GPL distribution, parameterized by an n×n score matrix, is provably more expressive than standard Plackett-Luce distributions and can model any distribution over permutations. A denoising schedule based on total variation distances between successive forward distributions is proposed to improve efficiency. The model is trained using a variational lower bound objective and evaluated on tasks including sorting MNIST digits, jigsaw puzzle assembly, and TSP optimization, demonstrating superior or competitive performance against existing methods.

## Key Results
- GPL-based reverse process can model any distribution over symmetric groups, unlike standard PL distributions
- Riffle shuffle forward process achieves fast mixing (O(log² n)) compared to other shuffling methods
- State-of-the-art or competitive performance on sorting MNIST digits, jigsaw puzzles, and TSP problems
- Denoising schedule reduces computational cost while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
The riffle shuffle provides fast mixing for the forward diffusion process in symmetric group spaces. The riffle shuffle is a mathematically precise model that simulates how cards are shuffled in real life, with a cut into two piles and an interleaving step. Its mixing time is O(log² n), much faster than random transpositions (O(n log n)) or random insertions (O(n² log n)), allowing efficient exploration of the permutation space.

### Mechanism 2
The generalized Plackett-Luce (GPL) distribution is more expressive than the standard PL distribution for the reverse process. The GPL uses an n×n score matrix instead of n scores, allowing it to represent any distribution over Sn by assigning appropriate scores, including delta distributions that PL cannot represent.

### Mechanism 3
The denoising schedule improves sampling and learning efficiency by merging reverse steps where distributions are similar. By analyzing total variation distances between successive forward distributions, steps with small TV distance can be merged without losing information, reducing the number of denoising steps needed.

## Foundational Learning

- Concept: Random walks on finite groups and mixing times
  - Why needed here: Understanding how different shuffling methods mix is critical to choosing an efficient forward process that decorrelates data without excessive steps.
  - Quick check question: What is the mixing time of the riffle shuffle compared to random transpositions for a deck of n cards?

- Concept: Plackett-Luce and generalized Plackett-Luce distributions
  - Why needed here: These distributions form the basis for the reverse transition model; understanding their expressiveness and limitations is essential for designing the reverse process.
  - Quick check question: Why can't the standard Plackett-Luce distribution represent a delta distribution over Sn?

- Concept: Diffusion models and variational bounds
  - Why needed here: The training objective is derived from the variational lower bound for diffusion models, adapted to the discrete permutation space.
  - Quick check question: How does the training loss for SymmetricDiffusers differ from standard continuous diffusion models?

## Architecture Onboarding

- Component map: CNN encoder -> Transformer with positional/time embeddings -> GPL distribution with n×n score matrix -> Beam search decoder

- Critical path:
  1. Preprocess input (e.g., image patches) with CNN encoder
  2. Apply Transformer to get embeddings with GPL scores
  3. Sample forward trajectory using riffle shuffle
  4. Compute training loss via Monte Carlo estimation
  5. Optimize GPL parameters to maximize variational bound
  6. At inference, run reverse process with beam search decoding

- Design tradeoffs:
  - GPL vs PL: GPL is more expressive but has O(n²) parameters vs O(n) for PL, increasing memory and computation
  - Riffle vs other shuffles: Riffle is faster mixing but requires more complex probability calculations
  - Beam search size: Larger beams improve accuracy but increase computation exponentially

- Failure signatures:
  - Training loss plateaus early: May indicate insufficient mixing in forward process or poor reverse transition modeling
  - Poor accuracy despite low loss: Could suggest mode collapse or beam search not finding correct permutations
  - Out of memory errors: Likely from GPL's O(n²) parameterization or large batch sizes

- First 3 experiments:
  1. Train on sorting 3-digit MNIST (n=3) with both PL and GPL reverse transitions, compare accuracy
  2. Test different denoising schedules on 4-digit MNIST (n=4) by varying TV distance thresholds
  3. Compare riffle shuffle forward process with random transpositions on a small TSP instance (n=5)

## Open Questions the Paper Calls Out

### Open Question 1
How does the mixing time of riffle shuffles compare to other shuffling methods when applied to non-uniform distributions over Sn? The paper discusses mixing times of various shuffling methods (riffle, random transposition, random insertion) for reaching the uniform distribution, but does not explore non-uniform target distributions.

### Open Question 2
Can the Generalized Plackett-Luce (GPL) distribution be made more efficient to compute, especially for large n? While the GPL distribution is theoretically more expressive, its computational complexity grows quadratically with n. This makes it impractical for very large symmetric groups, limiting the scalability of the method.

### Open Question 3
How does the denoising schedule affect the final performance of the model, and can it be optimized automatically? The paper provides empirical guidelines for choosing a denoising schedule based on total variation distances, but this is still a manual process that requires domain knowledge.

## Limitations

- GPL distribution's O(n²) parameterization limits scalability to very large permutation spaces
- Theoretical mixing time analysis may not hold for real-world non-uniform data distributions
- Denoising schedule optimization requires precomputing Eulerian numbers, which becomes expensive for large n

## Confidence

- High confidence: The mathematical framework for riffle shuffles and GPL distributions is rigorously proven. The claim that GPL can model any distribution over Sn is strongly supported by Theorem 2.
- Medium confidence: The empirical performance claims are based on competitive results against baselines, but the sample sizes for some experiments (particularly TSP) are relatively small.
- Low confidence: The practical mixing time of riffle shuffles on real-world permutation data versus theoretical predictions is not fully validated.

## Next Checks

1. Test scalability by implementing the GPL distribution for n > 100 and measuring memory usage and training stability.
2. Validate the denoising schedule on synthetic permutation distributions with known mixing properties to verify the TV distance thresholds.
3. Compare the GPL reverse process against other expressive discrete distributions (e.g., neural network-based transitions) on the same permutation tasks.