---
ver: rpa2
title: 'Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning
  Using Unique Experiences'
arxiv_id: '2402.05963'
source_url: https://arxiv.org/abs/2402.05963
tags:
- replay
- buffer
- state
- samples
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of sample efficiency in off-policy
  actor-critic reinforcement learning by proposing a method to maintain unique experiences
  in the replay buffer. The core idea is to identify and store experiences with unique
  state-reward combinations using a state-space partitioning technique based on important
  state variables selected through matrix decomposition.
---

# Frugal Actor-Critic: Sample Efficient Off-Policy Deep Reinforcement Learning Using Unique Experiences

## Quick Facts
- arXiv ID: 2402.05963
- Source URL: https://arxiv.org/abs/2402.05963
- Reference count: 38
- Up to 94% reduction in replay buffer size and up to 40% faster convergence compared to SAC and TD3 baselines

## Executive Summary
This paper introduces Frugal Actor-Critic (FAC), a sample-efficient off-policy deep reinforcement learning method that maintains unique experiences in the replay buffer. FAC uses a matrix decomposition technique to identify important state dimensions, partitions the state space into abstract states, and employs a kernel density estimator to ensure only experiences with unique state-reward combinations are stored. Theoretical analysis proves FAC converges faster than vanilla off-policy actor-critic algorithms by reducing variance in stochastic gradient descent updates. Experiments on 9 continuous control benchmarks demonstrate significant replay buffer size reduction (up to 94%) while maintaining or improving reward accumulation and convergence speed.

## Method Summary
FAC maintains a replay buffer containing only unique state-reward combinations to reduce variance in SGD updates during critic training. The method first identifies important state dimensions through rank-revealing QR decomposition on initial random rollouts. The state space is then partitioned into abstract states using these important dimensions. A kernel density estimator filters new experiences, storing only those with low reward density within their abstract state partition. This ensures the replay buffer contains diverse, independent samples that improve sample efficiency and convergence speed. FAC is theoretically proven to converge faster than vanilla off-policy actor-critic algorithms and achieves significant memory savings while maintaining performance.

## Key Results
- Achieves up to 94% reduction in replay buffer size compared to standard SAC and TD3
- Demonstrates up to 40% faster convergence across 9 continuous control benchmarks
- Maintains or improves reward accumulation while using significantly fewer samples
- Theoretical proof shows faster convergence due to reduced variance in SGD updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maintaining unique state-reward combinations reduces variance in SGD updates
- Mechanism: Ensures samples are independent and non-duplicated, avoiding correlated samples that inflate gradient variance
- Core assumption: Sample variance directly affects convergence speed in SGD-based RL optimization
- Evidence anchors:
  - [abstract]: "maintaining unique experiences in the replay buffer ensures that the samples are IID, which is essential for the stochastic gradient descent (SGD) optimization algorithm used in training the critic to converge faster"
  - [section 2.1]: "The performance of the target policy thus depends significantly on the quality of the experiences available in the replay buffer"

### Mechanism 2
- Claim: QR decomposition selects important state dimensions efficiently
- Mechanism: Identifies most informative and uncorrelated state variables for efficient partitioning
- Core assumption: High-dimensional states contain redundant components that don't significantly affect policy performance
- Evidence anchors:
  - [section 3.1]: "employ a matrix decomposition based technique that uses the experiences encountered during the initial phase of random exploration to identify the most important and independent state variables"
  - [corpus]: "Prioritized Experience Replay (PER) [28], which assigns TD errors as priorities to samples that are selected in a mini-batch. However, PER is computationally expensive and does not perform well in continuous environments"

### Mechanism 3
- Claim: KDE dynamically balances replay buffer growth by filtering redundant rewards
- Mechanism: Estimates reward density within state partitions; low-density rewards are added while high-density rewards are discarded
- Core assumption: Rare reward values are more informative for policy learning
- Evidence anchors:
  - [section 3.3]: "We define the reward density estimate (RDE) R of a reward sample ð‘Ÿ as follows: R(ð‘Ÿ, ð‘ ) = âˆ« ð‘Ÿ+ð›½ ð‘Ÿâˆ’ð›½ ðœŒð¾(ð‘¦, ð‘ ) Â· ð‘‘ð‘¦"
  - [abstract]: "selecting the experiences with unique state-reward combination by using a kernel density estimator"

## Foundational Learning

- Concept: Independent and identically distributed (IID) sampling
  - Why needed here: IID sampling is crucial for SGD convergence; FAC ensures this by storing only unique experiences
  - Quick check question: If two experiences have identical state vectors but different rewards, would FAC store both? (Answer: Yes, because they differ in reward)

- Concept: Principal Component Analysis (PCA) vs QR decomposition for dimensionality reduction
  - Why needed here: PCA cannot directly identify subset of original state dimensions contributing most to variance, whereas QR decomposition can
  - Quick check question: If a state has 10 dimensions but only 3 are important, how does FAC decide which to keep? (Answer: QR decomposition ranks dimensions by singular values)

- Concept: Kernel density estimation and bandwidth selection
  - Why needed here: KDE estimates reward density within a state partition; proper bandwidth selection ensures meaningful distinction between unique and redundant experiences
  - Quick check question: What happens if the bandwidth ð›½ is set too large? (Answer: KDE becomes too smooth, potentially discarding useful reward variations)

## Architecture Onboarding

- Component map: State importance selector -> State partitioner -> Density estimator -> Replay buffer manager -> Off-policy actor-critic learner
- Critical path:
  1. Collect initial random rollout â†’ compute important dimensions â†’ partition state space
  2. For each new transition: compute abstract state â†’ estimate reward density â†’ decide insertion
  3. Uniformly sample mini-batch from replay buffer â†’ perform SGD update â†’ repeat
- Design tradeoffs:
  - Computational cost vs. sample efficiency: FAC adds preprocessing overhead but reduces replay buffer size and improves convergence
  - Hyperparameter sensitivity: ðœˆ, ðœ–, ðœ‚, ð›½, and ðœ‡ must be tuned per environment
  - State abstraction granularity: Coarser partitions reduce memory but may lose information
- Failure signatures:
  - Replay buffer size remains large â†’ KDE bandwidth too small or partitioning too fine
  - Convergence slows â†’ Important state dimensions not identified correctly or ðœˆ too high
  - Suboptimal policy despite convergence â†’ Reward density threshold ðœ– too low
- First 3 experiments:
  1. Pendulum environment: Test FAC with SAC baseline; verify 40%+ replay buffer reduction and convergence parity
  2. Swimmer environment: Compare FAC-SAC vs TD3 baseline; check for improvement in reward accumulation and sample efficiency
  3. Humanoid environment: Stress-test FAC with high-dimensional state space; measure memory savings and policy quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Frugal Actor-Critic (FAC) algorithm vary when using different kernel functions for density estimation, beyond the Epanechnikov, Gaussian, and Tophat kernels tested?
- Basis in paper: [explicit] The paper mentions using three different kernels and selecting Epanechnikov based on experimental results, but does not explore other potential kernel functions
- Why unresolved: The paper only tested three specific kernels without systematically exploring other potential kernel functions
- What evidence would resolve it: A comprehensive study comparing FAC's performance using various kernel functions on the same benchmarks would provide evidence for the optimal kernel choice

### Open Question 2
- Question: What is the impact of the state-space partitioning granularity (controlled by the hyperparameter ðœ‡) on the convergence speed and final performance of FAC in high-dimensional state spaces?
- Basis in paper: [explicit] The paper discusses the effect of ðœ‡ on abstract state size but does not provide a detailed analysis of its impact on convergence and performance, especially in high-dimensional spaces
- Why unresolved: The paper only shows a few examples of ðœ‡ values and their effects without systematically studying how different levels of granularity affect FAC's performance
- What evidence would resolve it: A thorough experimental analysis varying ðœ‡ across a wide range of values and state dimensions would provide insights into optimal granularity

### Open Question 3
- Question: Can the FAC algorithm be extended to work effectively with on-policy actor-critic methods, or is it inherently limited to off-policy algorithms?
- Basis in paper: [inferred] The paper focuses on off-policy actor-critic algorithms and does not explore the applicability of FAC to on-policy methods
- Why unresolved: The paper does not investigate the potential of FAC for on-policy algorithms, leaving open the question of whether the unique experience selection mechanism can be adapted to on-policy settings
- What evidence would resolve it: Implementing and testing FAC with on-policy actor-critic algorithms on benchmark tasks would provide evidence for its effectiveness in on-policy settings

## Limitations
- Limited ablation studies on hyperparameter sensitivity (Î½, Î¼, Î², Îµ, Î·) across different environments
- Performance scalability in extremely high-dimensional environments (>100 state dimensions) remains untested
- Computational overhead analysis comparing wall-clock time per training step with vanilla SAC/TD3 is not provided

## Confidence
- High confidence: Variance reduction through unique experience maintenance and improved sample efficiency via state-space partitioning
- Medium confidence: Practical scalability in extremely high-dimensional environments
- Low confidence: Sensitivity to hyperparameter tuning across different environments

## Next Checks
1. Ablation study on state importance selection: Systematically vary singular value threshold Î½ from 0.1 to 0.9 to quantify impact on performance and replay buffer reduction
2. Computational overhead analysis: Measure and compare wall-clock time per training step between vanilla SAC/TD3 and FAC implementations
3. High-dimensional stress test: Apply FAC to environments with >50 state dimensions to validate scalability claims and identify performance degradation points