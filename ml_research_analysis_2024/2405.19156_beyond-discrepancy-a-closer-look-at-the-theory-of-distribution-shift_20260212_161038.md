---
ver: rpa2
title: 'Beyond Discrepancy: A Closer Look at the Theory of Distribution Shift'
arxiv_id: '2405.19156'
source_url: https://arxiv.org/abs/2405.19156
tags:
- source
- target
- feature
- data
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the theory of distribution shift for a classifier
  from a source to a target distribution. Instead of relying on discrepancy, it adopts
  an Invariant-Risk-Minimization (IRM)-like assumption connecting the distributions,
  and characterizes conditions under which data from a source distribution is sufficient
  for accurate classification of the target.
---

# Beyond Discrepancy: A Closer Look at the Theory of Distribution Shift

## Quick Facts
- arXiv ID: 2405.19156
- Source URL: https://arxiv.org/abs/2405.19156
- Authors: Robi Bhattacharjee; Nick Rittler; Kamalika Chaudhuri
- Reference count: 40
- Primary result: Characterizes when source data alone suffices for target classification under an IRM-like invariance assumption

## Executive Summary
This paper examines distribution shift for classifiers by moving beyond discrepancy-based analysis to adopt an Invariant-Risk-Minimization (IRM)-like assumption connecting source and target distributions. The authors characterize precise conditions under which source data alone suffices for accurate target classification, and when unlabeled or labeled target data becomes necessary. The analysis provides rigorous theoretical guarantees in the large sample regime, focusing on nearest neighbor classifiers in feature space.

## Method Summary
The paper introduces a Statistical IRM Assumption requiring existence of a feature map that simultaneously preserves source risk, contracts source and target distributions, and Bayes-unifies them. Three learning scenarios are analyzed: source-only (using nearest neighbor classification), source with unlabeled target (testing contraction properties), and source with labeled target (empirical risk minimization over feature map class). The analysis uses VC dimension bounds on distance comparer classes to control sample complexity, with the distance dimension of the feature map class serving as a key complexity measure.

## Key Results
- Provides conditions under which source data alone suffices for target classification (source-preservation, contraction, Bayes-unification)
- Shows unlabeled target data can eliminate unsuitable feature maps by testing contraction
- Proves labeled target data enables efficient identification of correct feature map via empirical risk minimization
- Establishes upper bounds on target data needed for generalization based on distance dimension of feature map class

## Why This Works (Mechanism)

### Mechanism 1
Generalization is possible when the feature map preserves source risk, contracts source and target, and Bayes-unifies the two distributions. The selected feature space ensures that every target point is mapped near source support, and optimal classification decisions are locally consistent between source and target. This allows a nearest neighbor classifier trained on source data to perform well on target data.

### Mechanism 2
Labeled target data can be efficiently used to identify the correct feature map via empirical risk minimization over Φ. By evaluating the empirical risk of the source-trained nearest neighbor classifier on a small labeled target sample, the learner can select the feature map that minimizes this risk, with sample complexity controlled by the distance dimension of Φ.

### Mechanism 3
Unlabeled target data can eliminate unsuitable feature maps by testing contraction, but cannot determine Bayes-unification. Unlabeled target points reveal which feature maps map target data close to source support (contraction), but cannot distinguish which maps preserve the correct classification decisions without labels.

## Foundational Learning

- Concept: Invariance assumptions in domain adaptation
  - Why needed here: The paper replaces discrepancy-based worst-case bounds with an invariance assumption (IRM-like) that connects source and target through a shared feature map
  - Quick check question: What is the key difference between discrepancy-based analysis and the Statistical IRM Assumption?

- Concept: VC dimension and uniform convergence
  - Why needed here: The paper uses VC dimension bounds on the distance comparer class to control the sample complexity of learning the correct feature map
  - Quick check question: How does the distance dimension of Φ relate to the number of labeled target examples needed?

- Concept: Nearest neighbor classification under distribution shift
  - Why needed here: The paper analyzes generalization guarantees for nearest neighbor classifiers in feature space, leveraging their strong local generalization properties
  - Quick check question: Under what conditions does nearest neighbor classification converge to Bayes optimal in feature space?

## Architecture Onboarding

- Component map: Feature map class Φ -> Distance comparer class ∆Φ -> Learning procedures (source-only, source+unlabeled, source+labeled) -> Risk bounds and margin preservation

- Critical path:
  1. Verify Φ satisfies regularity assumptions (compact, continuous, indomitable)
  2. Determine which data availability scenario applies (source-only, source+unlabeled, source+labeled)
  3. Apply corresponding learning rule and analyze sample complexity via distance dimension

- Design tradeoffs:
  - Richer Φ increases expressivity but raises distance dimension and sample complexity
  - More unlabeled target data can eliminate unsuitable maps but cannot resolve Bayes-unification ambiguity
  - Small labeled target sets suffice under Statistical IRM assumption, but require correct feature map identification

- Failure signatures:
  - High empirical risk on target despite low source risk → possible violation of contraction or Bayes-unification
  - No feature map passes margin/source-preservation tests → Φ may not contain a realizing map
  - Sample complexity explodes → distance dimension of Φ too large

- First 3 experiments:
  1. Verify Φ regularity and compute distance dimension for a simple projection class (e.g., CorD,K)
  2. Test source-only learning rule on a synthetic problem where only one feature map satisfies all three conditions
  3. Evaluate the unlabeled target data procedure on a problem where contraction distinguishes feature maps but Bayes-unification does not

## Open Questions the Paper Calls Out

### Open Question 1
What are the precise conditions under which unlabeled target data is sufficient for identifying the correct feature map under the Statistical IRM assumption? The paper proves sufficiency but not necessity of the condition Φ* = S(Φ) ∩ C(Φ).

### Open Question 2
How does the distance dimension ∂(Φ) relate to the generalization ability of feature maps in Φ under distribution shift? The paper only uses ∂(Φ) as a technical tool for sample complexity bounds, without investigating whether it captures the actual generalization capability of Φ.

### Open Question 3
Can the Statistical IRM assumption be relaxed while still maintaining the theoretical guarantees for distribution shift? The paper assumes the existence of a feature map that perfectly realizes the Statistical IRM assumption, without considering approximate versions or alternative assumptions.

## Limitations
- The Statistical IRM Assumption requiring simultaneous preservation of source risk, contraction, and Bayes-unification may be overly restrictive for many practical scenarios
- Analysis heavily depends on distance dimension of Φ, which could be prohibitively large for complex feature map classes
- Focus on nearest neighbor classifiers may limit practical applicability compared to modern parametric classifiers

## Confidence

- High confidence: Theoretical guarantees for labeled target data scenarios supported by VC dimension bounds and uniform convergence arguments
- Medium confidence: Unlabeled target data procedure's effectiveness depends on specific relationship between S(Φ) and C(Φ)
- Medium confidence: Source-only procedure's success relies on identifying unique realizing feature map, which may be statistically challenging

## Next Checks

1. Test statistical distinguishability of feature maps in practice by evaluating how often correct realizing map can be identified from finite source data
2. Empirically evaluate sample complexity predictions by measuring actual performance as function of unlabeled/labeled target data size across different Φ classes
3. Validate theoretical bounds on real-world datasets with varying degrees of distribution shift to assess practical relevance