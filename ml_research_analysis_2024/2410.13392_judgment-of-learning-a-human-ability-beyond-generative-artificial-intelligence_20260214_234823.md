---
ver: rpa2
title: 'Judgment of Learning: A Human Ability Beyond Generative Artificial Intelligence'
arxiv_id: '2410.13392'
source_url: https://arxiv.org/abs/2410.13392
tags:
- page
- metacognitivemonitoringingenerativeai
- memory
- https
- cognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a cross-agent prediction model to assess
  whether large language models (LLMs) can predict human memory performance on an
  item-by-item basis. The researchers tested humans and three LLM versions (GPT-3.5-turbo,
  GPT-4-turbo, and GPT-4o) on pairs of sentences containing garden-path sentences,
  manipulating contextual fit.
---

# Judgment of Learning: A Human Ability Beyond Generative Artificial Intelligence

## Quick Facts
- arXiv ID: 2410.13392
- Source URL: https://arxiv.org/abs/2410.13392
- Reference count: 40
- LLMs cannot predict human memory performance despite aligning with human cognition at the object level

## Executive Summary
This study investigates whether large language models can predict human memory performance through judgments of learning, a metacognitive ability that allows humans to anticipate their own memory accuracy. The researchers tested humans and three LLM versions (GPT-3.5-turbo, GPT-4-turbo, and GPT-4o) on garden-path sentence pairs with varying contextual fit, examining their ability to predict recognition memory performance. While humans reliably predicted their own memory performance with positive correlations between memorability ratings and actual recognition, none of the tested LLMs demonstrated comparable predictive accuracy.

The findings reveal a fundamental gap between human and AI cognition: although LLMs can process information similarly to humans at the object level, they lack the metacognitive mechanisms that enable humans to anticipate their memory performance. This cross-agent prediction model demonstrates that current LLMs cannot serve as accurate predictors of human memory, highlighting a critical limitation in their cognitive architecture that distinguishes them from human intelligence.

## Method Summary
The study employed a cross-agent prediction model where humans and three LLM versions (GPT-3.5-turbo, GPT-4-turbo, and GPT-4o) were presented with garden-path sentence pairs containing fitting and unfitting contexts. Participants rated the memorability of each sentence pair, and researchers compared these ratings to actual recognition memory performance. The experiment manipulated contextual fit to examine whether this would influence predictive accuracy. Human participants provided judgments of learning before taking a recognition memory test, while LLMs generated memorability ratings without the benefit of memory experience. Bootstrapping analyses were used to assess the significance of correlations between memorability ratings and actual memory performance across conditions.

## Key Results
- Humans reliably predicted their own future memory performance with positive correlations between memorability ratings and recognition accuracy
- None of the tested LLMs (GPT-3.5-turbo, GPT-4-turbo, GPT-4o) demonstrated significant predictive accuracy for human memory performance
- Bootstrapping analyses confirmed LLM memorability ratings did not significantly correlate with human memory performance in either fitting or unfitting conditions
- The metacognitive gap persisted across all tested LLM versions and contextual conditions

## Why This Works (Mechanism)
The study reveals that human metacognition operates through mechanisms that allow self-monitoring and prediction of cognitive states, which current LLMs lack despite their strong performance on object-level tasks. This fundamental difference suggests that human cognition includes meta-level processing capabilities that go beyond pattern recognition and generation.

## Foundational Learning
**Metacognition**: The ability to monitor and regulate one's own cognitive processes, essential for understanding why humans can predict their memory performance while LLMs cannot
- Why needed: Provides theoretical framework for understanding the cognitive gap between humans and AI
- Quick check: Can be assessed through tasks requiring self-monitoring and prediction of cognitive states

**Garden-path sentences**: Sentences that lead readers to initially misinterpret their meaning due to temporary syntactic ambiguity
- Why needed: Creates controlled experimental conditions where contextual fit affects memory and comprehension
- Quick check: Can be verified by testing whether readers experience initial misinterpretation

**Cross-agent prediction model**: An experimental paradigm comparing predictions across different cognitive agents (humans vs. AI)
- Why needed: Allows direct comparison of metacognitive abilities between humans and LLMs
- Quick check: Can be validated by ensuring both agents receive identical stimuli and rating tasks

**Bootstrapping analysis**: A statistical resampling technique for estimating the reliability of correlations and significance testing
- Why needed: Provides robust statistical validation of the absence of predictive relationships
- Quick check: Can be verified through replication with different random seeds and sample sizes

## Architecture Onboarding
**Component Map**: Humans -> Metacognitive Monitoring -> Memory Prediction; LLMs -> Pattern Matching -> Memorability Rating
**Critical Path**: Judgment of Learning (Humans) → Recognition Memory Test → Performance Evaluation; Memorability Rating (LLMs) → Cross-agent Comparison → Statistical Analysis
**Design Tradeoffs**: The study prioritizes ecological validity of human memory tasks over controlled laboratory conditions, accepting potential variability in human responses for more realistic assessment of metacognitive ability
**Failure Signatures**: LLMs show systematic failure to predict human memory performance despite high performance on object-level language tasks, indicating architecture-level limitations in metacognition
**3 First Experiments**: 1) Test whether fine-tuning LLMs on human memory data improves predictive accuracy; 2) Examine whether smaller language models show different metacognitive patterns; 3) Investigate whether reinforcement learning from human feedback enables metacognitive prediction abilities

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on a specific type of linguistic stimuli (garden-path sentences) and may not generalize to other cognitive domains or task types
- The cross-agent prediction model may have limited ecological validity for broader claims about AI cognition
- The study does not explore potential mechanisms that could enable LLMs to develop metacognitive abilities or investigate fine-tuning approaches

## Confidence
- Confidence in the core finding that LLMs cannot predict human memory performance: High
- Confidence in the broader interpretation about fundamental differences in human and AI cognition: Medium
- Confidence in the generalizability of findings to other cognitive domains or AI architectures: Low

## Next Checks
1. Replicate the study using different types of cognitive tasks (e.g., problem-solving, creative generation) to assess whether the metacognitive gap persists across domains
2. Test whether fine-tuning LLMs on human memory performance data improves their predictive accuracy
3. Investigate whether smaller language models or alternative architectures show different patterns of metacognitive ability compared to the tested GPT models