---
ver: rpa2
title: Detecting and Mitigating Hallucination in Large Vision Language Models via
  Fine-Grained AI Feedback
arxiv_id: '2404.14233'
source_url: https://arxiv.org/abs/2404.14233
tags:
- hallucination
- arxiv
- hallucinations
- detection
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fine-grained hallucination detection and
  mitigation method for Large Vision Language Models (LVLMs) via AI feedback. The
  core approach involves generating a small-scale, sentence-level hallucination annotation
  dataset using proprietary models, training a detection model for fine-grained hallucination
  detection across object, attribute, and relationship types, and introducing a Hallucination
  Severity-Aware Direct Preference Optimization (HSA-DPO) that prioritizes critical
  hallucination mitigation by incorporating severity scores into preference learning.
---

# Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback

## Quick Facts
- arXiv ID: 2404.14233
- Source URL: https://arxiv.org/abs/2404.14233
- Reference count: 3
- Achieves 36.1% hallucination rate reduction on AMBER and 76.3% on Object HalBench compared to base model

## Executive Summary
This paper addresses hallucination problems in Large Vision Language Models (LVLMs) by proposing a fine-grained AI feedback approach that outperforms both human and GPT-4V annotation methods. The method generates sentence-level hallucination annotations using proprietary models, trains a detection model for fine-grained hallucination detection across object, attribute, and relationship types, and introduces Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) that prioritizes critical hallucination mitigation. The approach demonstrates superior performance on multiple hallucination benchmarks including MHaluBench, AMBER, and Object HalBench, achieving state-of-the-art detection performance and significant hallucination rate reductions.

## Method Summary
The method involves generating a small-scale, sentence-level hallucination annotation dataset using proprietary models (GPT-4/GPT-4V) to collect detailed feedback on hallucination type, severity, and rationale for each sentence. This annotation dataset trains a detection model capable of performing fine-grained hallucination detection. The system then employs a detect-then-rewrite pipeline to automatically construct preference datasets by having the detection model identify hallucinations and a rewriting model correct them into non-hallucinatory responses. Finally, HSA-DPO incorporates hallucination severity scores into the preference learning framework to prioritize mitigation of critical hallucinations during training.

## Key Results
- Achieves state-of-the-art hallucination detection performance on MHaluBench, surpassing GPT-4V and Gemini baselines
- Reduces hallucination rate by 36.1% on AMBER benchmark and 76.3% on Object HalBench compared to base model
- Demonstrates superior annotation efficiency and cost-effectiveness compared to human or GPT-4V annotation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained AI feedback enables more precise hallucination detection than coarse-grained approaches.
- Mechanism: The system generates sentence-level hallucination annotations using proprietary models, capturing hallucination type (object, attribute, relationship), severity score, and rationale for each sentence.
- Core assumption: Sentence-level granularity provides more accurate detection than response-level feedback.
- Evidence anchors:
  - [abstract]: "we generate a small-size sentence-level hallucination annotation dataset by proprietary models, whereby we train a detection model which can perform sentence-level hallucination detection"
  - [section]: "we meticulously craft prompts to collect detailed feedback on the type, severity and rationale of each hallucination"
- Break condition: If the hallucination spans multiple sentences or requires cross-sentence context for accurate detection.

### Mechanism 2
- Claim: Detect-then-rewrite pipeline enables cost-effective preference dataset construction.
- Mechanism: Hallucination detection model identifies hallucinations in generated responses, then a rewriting model corrects these hallucinations to create <chosen, rejected> pairs for preference learning.
- Core assumption: Open-source models can effectively detect and rewrite hallucinations when provided with detailed feedback.
- Evidence anchors:
  - [abstract]: "we propose a detect-then-rewrite pipeline to automatically construct preference dataset for hallucination mitigation training"
  - [section]: "Given a hallucinatory response, the detection model first identifies hallucinations within each sentence of the response. Based on the detected hallucinations, a rewriting model then revises the hallucinatory response into non-hallucinatory one"
- Break condition: If the rewriting model cannot preserve semantic meaning while correcting hallucinations.

### Mechanism 3
- Claim: Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) prioritizes critical hallucination mitigation.
- Mechanism: Severity scores from fine-grained feedback are incorporated into preference learning objective, giving higher weight to severe hallucinations during training.
- Core assumption: Severity-aware weighting improves mitigation of critical hallucinations compared to uniform treatment.
- Evidence anchors:
  - [abstract]: "incorporating the severity of hallucinations into preference learning"
  - [section]: "HSA-DPO incorporates hallucination severity into the preference optimization for mitigating critical hallucinations with higher priority"
- Break condition: If severity scoring is unreliable or if model overfits to severity labels at expense of overall hallucination reduction.

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Forms the basis for preference learning framework that HSA-DPO builds upon
  - Quick check question: What is the difference between DPO and standard reinforcement learning approaches for preference alignment?

- Concept: Multimodal learning architectures
  - Why needed here: Understanding how vision-language models integrate visual and textual information is crucial for diagnosing hallucination sources
  - Quick check question: How do vision encoders typically interface with language models in LVLMs?

- Concept: Preference dataset construction
  - Why needed here: The detect-then-rewrite pipeline is central to creating training data without expensive human annotation
  - Quick check question: What are the key differences between segment-level and response-level preference data?

## Architecture Onboarding

- Component map: Proprietary model → Fine-grained annotation generator → Detection model → Rewriting model → Preference dataset → HSA-DPO → Fine-tuned LVLM
- Critical path: Annotation generation → Detection model training → Preference dataset construction → HSA-DPO training
- Design tradeoffs: Sentence-level vs response-level granularity (precision vs complexity), severity scoring overhead vs prioritization benefits
- Failure signatures: High false positive rates in detection, rewriting that introduces new hallucinations, severity scoring that doesn't correlate with actual impact
- First 3 experiments:
  1. Test detection model accuracy on sentence-level hallucination classification compared to GPT-4V baseline
  2. Evaluate rewriting model ability to correct hallucinations while preserving semantic content
  3. Compare HSA-DPO vs vanilla DPO on hallucination reduction metrics across different severity levels

## Open Questions the Paper Calls Out
None explicitly stated in the provided document.

## Limitations
- The paper relies heavily on proprietary model feedback (GPT-4/GPT-4V) for annotation generation, raising questions about reproducibility and generalizability
- Computational cost of generating sentence-level annotations for large-scale datasets is not fully characterized
- HSA-DPO mechanism lacks detailed implementation specifications that would enable precise replication

## Confidence
- **High Confidence**: The core detection pipeline architecture (fine-grained annotation → detection model → detect-then-rewrite → HSA-DPO) is well-specified and logically sound
- **Medium Confidence**: The experimental results showing SOTA performance on MHaluBench and significant hallucination reduction on AMBER and Object HalBench, though some benchmark details are sparse
- **Low Confidence**: The claim about annotation efficiency and cost-effectiveness relative to human/GPT-4V annotation, as quantitative comparisons are limited

## Next Checks
1. **Annotation Quality Validation**: Compare hallucination detection accuracy between the AI-generated annotations and human-annotated ground truth on a held-out subset to quantify annotation reliability

2. **Severity Scoring Calibration**: Evaluate correlation between AI-assigned severity scores and human judgments of hallucination impact to validate the severity-aware weighting mechanism

3. **Generalization Assessment**: Test the fine-tuned LVLM on out-of-domain datasets beyond the reported benchmarks to evaluate robustness of hallucination mitigation across diverse visual contexts