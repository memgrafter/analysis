---
ver: rpa2
title: 'TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and
  Effective Retrieval'
arxiv_id: '2401.13509'
source_url: https://arxiv.org/abs/2401.13509
tags:
- query
- dense
- tprf
- feedback
- ance-prf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying pseudo-relevance
  feedback (PRF) to dense retrievers in resource-constrained environments like embedded
  systems, where memory and CPU are limited and GPUs are not available. The proposed
  method, TPRF, uses a transformer-based approach that learns to effectively combine
  relevance feedback signals from dense passage representations, without relying on
  large pre-trained language models.
---

# TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient and Effective Retrieval

## Quick Facts
- arXiv ID: 2401.13509
- Source URL: https://arxiv.org/abs/2401.13509
- Authors: Hang Li; Chuting Yu; Ahmed Mourad; Bevan Koopman; Guido Zuccon
- Reference count: 35
- One-line primary result: TPRF achieves comparable effectiveness to larger PRF methods while significantly reducing model size and query latency

## Executive Summary
This paper addresses the challenge of applying pseudo-relevance feedback (PRF) to dense retrievers in resource-constrained environments like embedded systems, where memory and CPU are limited and GPUs are not available. The proposed method, TPRF, uses a transformer-based approach that learns to effectively combine relevance feedback signals from dense passage representations, without relying on large pre-trained language models. TPRF is agnostic to the specific dense retriever used and provides a mechanism for modeling relationships and weights between the query and feedback signals.

The key results show that TPRF achieves comparable effectiveness to larger PRF methods while significantly reducing model size and query latency. On TREC Deep Learning datasets, TPRF with ANCE demonstrated improvements over the base dense retriever, with nDCG@10 gains of up to 0.0126. TPRF models were 40-60% smaller than ANCE-PRF while maintaining similar effectiveness. Query latency was reduced by a factor of 500-600x, with TPRF taking only 0.2 seconds per 100 queries compared to 100+ seconds for ANCE-PRF. The method also scales efficiently to larger feedback depths without the input size limitations of text-based approaches.

## Method Summary
TPRF is a transformer-based pseudo-relevance feedback method that operates on dense vector representations rather than text. The approach uses a vanilla transformer encoder to process stacked query and feedback passage vectors through multi-head self-attention, learning to combine relevance feedback signals effectively. The training uses hard negative sampling from initial retrieval rankings (positions 10-200) with cross-entropy loss. TPRF achieves significant efficiency gains by bypassing expensive text encoding through large language models, resulting in 500-600x faster query latency compared to text-based PRF methods while maintaining comparable effectiveness to larger models.

## Key Results
- TPRF with ANCE achieved nDCG@10 improvements of up to 0.0126 on TREC DL datasets
- TPRF models were 40-60% smaller than ANCE-PRF while maintaining similar effectiveness
- Query latency was reduced by 500-600x, with TPRF taking only 0.2 seconds per 100 queries versus 100+ seconds for ANCE-PRF

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TPRF improves dense retrieval effectiveness by modeling relationships and weights between the query and feedback signals using transformer layers
- Mechanism: The transformer encoder processes dense vector representations of the query and top-k feedback passages through multi-head self-attention, allowing the model to learn which feedback signals are most relevant to the query context
- Core assumption: Dense vector representations contain sufficient semantic information to capture relevance relationships when processed through transformer layers
- Evidence anchors:
  - [abstract] "TPRF learns how to effectively combine the relevance feedback signals from dense passage representations. Specifically, TPRF provides a mechanism for modelling relationships and weights between the query and the relevance feedback signals."
  - [section 2.1] "The multi-head attention layer then produces the input query embedding and the feedback passage embeddings interact with each other and aggregate information via self-attention."
  - [corpus] Weak evidence - corpus neighbors don't directly address transformer-based PRF mechanisms
- Break condition: If dense representations lose too much semantic information compared to text, the transformer cannot effectively learn relevance relationships

### Mechanism 2
- Claim: TPRF achieves significant efficiency gains by using dense vectors instead of text as input, avoiding expensive BERT inference
- Mechanism: By operating directly on 768-dimensional dense vectors rather than text, TPRF bypasses the computational overhead of text encoding through large language models, resulting in 500-600x faster query latency
- Core assumption: Dense vector representations are sufficient inputs for learning PRF transformations without requiring text-based processing
- Evidence anchors:
  - [section 2.1] "TPRF learns how to effectively combine the relevance feedback signal from dense passage representations without relying on PRF encoders based on large pre-trained language models"
  - [section 2.3] "The TPRF inference time is substantially lower than that of ANCE-PRF, regardless of k. This is because, while in ANCE-PRF larger k values mean more text to encode (so more time required), in ANCE-TPRF larger k values simply mean more dense vectors been accumulated in the input"
  - [section 4.2] "TPRF is blazing fast on a commodity CPU, with 100 queries taking a fraction of a second to run. ANCE-PRF instead requires in the proximity of 100 seconds to run the same amount of queries."
- Break condition: If dense vectors become too sparse or lose critical semantic distinctions, the efficiency gains may not translate to effectiveness improvements

### Mechanism 3
- Claim: TPRF maintains effectiveness comparable to larger PRF methods through specifically designed training regime with hard negative sampling
- Mechanism: The training process uses one positive passage and multiple negative passages sampled from initial retrieval rankings (positions 10-200), combined with cross-entropy loss to learn discriminative representations that improve over the base dense retriever
- Core assumption: Hard negative sampling from the retrieval ranking space provides effective training signals for learning PRF transformations
- Evidence anchors:
  - [section 2.2] "At training time, after the initial retrieval using the baseline dense retriever, we pick the top-k passages as the PRF, and we randomly sample 20 negatives among the passages that rank between position 10 and 200 in the initial ranking"
  - [section 4.1] "TPRF always outperforms ANCE except in nDCG@3 for DL 2019 (only 0.3% loss) and in MRR and nDCG@k in DL 2020. These differences are however not statistically significant."
  - [section 4.2] "The ANCE-TPRF model with highest nDCG@10 on DL 2020 (0.6491) has a size of 299.2 MB, while the largest model has size 582.9 MB and achieves a nDCG@10 of 0.5581"
- Break condition: If the negative sampling strategy doesn't capture true hard negatives, the model may not learn effective discrimination

## Foundational Learning

- Concept: Dense retrieval and vector representations
  - Why needed here: TPRF operates on dense vector representations rather than text, so understanding how dense retrievers encode passages and queries is fundamental
  - Quick check question: What is the dimensionality of ANCE dense representations and how are they generated?

- Concept: Transformer architecture and self-attention
  - Why needed here: TPRF uses transformer encoder layers to process query-feedback relationships, requiring understanding of multi-head attention and positional encoding
  - Quick check question: How does self-attention allow the transformer to weigh different feedback passages relative to the query?

- Concept: Pseudo-relevance feedback and vocabulary mismatch
  - Why needed here: TPRF addresses the same fundamental problem as traditional PRF (vocabulary mismatch) but through different mechanisms, so understanding the problem space is crucial
  - Quick check question: What is vocabulary mismatch and why does PRF typically help address it?

## Architecture Onboarding

- Component map: Dense retriever -> TPRF transformer -> ANN index
- Critical path: 1. Initial retrieval with base dense retriever 2. Extract top-k passage vectors as feedback 3. Stack query vector with feedback vectors 4. Transformer processing through self-attention layers 5. New query vector used for second retrieval round
- Design tradeoffs:
  - Model size vs effectiveness: Fewer transformer layers reduce memory but may sacrifice some gains
  - Feedback depth (k) vs latency: More feedback passages increase input size but TPRF scales better than text-based methods
  - Training complexity vs performance: Hard negative sampling requires careful implementation but provides better signals
- Failure signatures:
  - Effectiveness drops when feedback passages are semantically unrelated to query
  - Latency increases unexpectedly with large k values (indicates inefficient matrix operations)
  - Training instability with certain negative sampling strategies
- First 3 experiments:
  1. Baseline comparison: Run ANCE vs ANCE-TPRF with k=3 on TREC DL 2019, measure nDCG@10 and query latency
  2. Architecture ablation: Compare TPRF with 1, 6, and 12 transformer layers on same dataset, measure effectiveness-latency tradeoff
  3. Feedback depth scaling: Vary k from 1 to 20, plot effectiveness and latency curves to identify optimal k value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of TPRF scale with increasing PRF depth (k) beyond 100 passages?
- Basis in paper: [inferred] The paper mentions TPRF is highly scalable to increasing input signal but only tests up to k=100 in experiments.
- Why unresolved: The paper doesn't explore the practical limits of TPRF's scalability or examine if there's a point of diminishing returns.
- What evidence would resolve it: Empirical results showing TPRF's effectiveness, query latency, and memory usage across a wider range of k values (e.g., up to 1000 passages).

### Open Question 2
- Question: How would TPRF perform when combined with other dense retrievers beyond ANCE, such as DPR or RocketQA?
- Basis in paper: [explicit] The paper states TPRF is "agnostic to the specific dense representation used and thus can be generally applied to any dense retriever."
- Why unresolved: The paper only demonstrates TPRF with ANCE and ANCE-PRF, leaving its generalizability to other dense retrievers unexplored.
- What evidence would resolve it: Experimental results comparing TPRF's effectiveness and efficiency when combined with multiple different dense retrievers.

### Open Question 3
- Question: What is the impact of different pre-training strategies on TPRF's performance?
- Basis in paper: [explicit] The paper notes that TPRF uses a vanilla transformer without pre-training, unlike methods like ANCE-PRF that rely on pre-trained models like RoBERTa.
- Why unresolved: The paper doesn't investigate whether pre-training TPRF on different tasks or data could improve its effectiveness or efficiency.
- What evidence would resolve it: Comparative results showing TPRF's performance with and without various pre-training approaches on different datasets.

## Limitations
- Evaluation limited to single base dense retriever (ANCE) and two TREC Deep Learning datasets, limiting generalizability
- Claims about resource-constrained deployment not empirically validated with actual embedded system benchmarks
- Ablation studies don't fully explore the architectural design space, particularly effectiveness-efficiency tradeoffs

## Confidence
- **High Confidence**: TPRF achieves significant latency improvements (500-600x faster than ANCE-PRF) through dense vector processing instead of text-based methods
- **Medium Confidence**: TPRF maintains comparable effectiveness to larger PRF methods while being 40-60% smaller, though some statistical significance is questionable
- **Low Confidence**: TPRF's performance on datasets beyond TREC Deep Learning is unverified and claims about resource-constrained deployment lack empirical validation

## Next Checks
1. **Architecture Sensitivity Analysis**: Conduct comprehensive ablation study varying transformer layers (1-12), attention heads (4-12), and feedback depth (k=1-20) to map full effectiveness-latency tradeoff space
2. **Generalization Testing**: Evaluate TPRF with multiple base dense retrievers (DPR, Contriever) across diverse retrieval tasks including open-domain QA, document retrieval, and medical literature
3. **Resource-Constrained Deployment**: Implement TPRF on actual embedded hardware (Raspberry Pi, edge devices) measuring real-time memory usage, CPU load, and inference latency