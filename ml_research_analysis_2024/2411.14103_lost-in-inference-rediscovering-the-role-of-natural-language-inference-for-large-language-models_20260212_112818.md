---
ver: rpa2
title: 'Lost in Inference: Rediscovering the Role of Natural Language Inference for
  Large Language Models'
arxiv_id: '2411.14103'
source_url: https://arxiv.org/abs/2411.14103
tags:
- benchmarks
- human
- label
- language
- mnli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether Natural Language Inference (NLI)\
  \ benchmarks remain useful for evaluating Large Language Models (LLMs), despite\
  \ their declining use in recent LLM evaluation papers. The authors test five NLI\
  \ datasets (MNLI, SNLI, HANS, ANLI, \u03B1NLI) across six models of varying sizes,\
  \ including Llama 3.1 and Mistral models."
---

# Lost in Inference: Rediscovering the Role of Natural Language Inference for Large Language Models

## Quick Facts
- arXiv ID: 2411.14103
- Source URL: https://arxiv.org/abs/2411.14103
- Reference count: 15
- Large language models still benefit from NLI evaluation, with larger models outperforming smaller ones across multiple benchmarks

## Executive Summary
This study investigates whether Natural Language Inference (NLI) benchmarks remain useful for evaluating Large Language Models (LLMs), despite their declining use in recent evaluation papers. The authors test five NLI datasets across six models of varying sizes, including Llama 3.1 and Mistral models. They find that NLI benchmarks can discriminate between models of different sizes and quality, with larger models consistently outperforming smaller ones. Performance on these tasks develops steadily during training, though with some fluctuations. The study reveals that high scores are not due to data contamination, and manual analysis shows that even the best models make errors primarily on ambiguous examples where human annotators also disagree.

## Method Summary
The authors evaluate five NLI datasets (MNLI, SNLI, HANS, ANLI, αNLI) across six models of varying scales, including Llama 3.1 {8, 70, 405}B and Mistral {7B, 8x7B, 8x22B} models. They use a choice-based evaluation setup with few-shot examples, measuring accuracy and Jensen-Shannon Divergence (JSD) between model output distributions and human label distributions. The 8B and 70B models were pre-trained from scratch for 2T tokens, allowing analysis of performance development during training. Data contamination was analyzed using 8-gram overlap methodology, and manual error analysis was conducted on high-performing models.

## Key Results
- Larger models systematically outperform smaller ones on NLI benchmarks, with Llama 3.1 405B achieving the highest accuracy
- Performance on NLI tasks develops steadily during training, crossing fine-tuned BERT performance after 500B tokens for ANLI and αNLI
- Jensen-Shannon Divergence between model outputs and human label distributions decreases with scale and during training
- High accuracy is not due to data contamination, as estimated contamination rates are low and performance gains from contamination would be minimal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NLI benchmarks provide discriminative signal between models of different scales and qualities.
- Mechanism: Larger models achieve systematically higher accuracy and lower Jensen-Shannon Divergence (JSD) compared to smaller models on the same benchmarks, indicating better alignment with human label distributions.
- Core assumption: Model size and quality directly correlate with improved natural language understanding capabilities.
- Evidence anchors:
  - [abstract] "we find that they are able to discriminate well between models at various stages of training"
  - [section] "the larger models in the Llama and Mistral series far outperform the finetuned BERT baseline" (page 4)
  - [corpus] Weak evidence - related papers focus on NLI applications but don't directly support discriminative signal claim.
- Break condition: If model performance plateaus or shows no correlation with scale, or if JSD values converge across different model sizes.

### Mechanism 2
- Claim: NLI benchmarks remain informative during training and are not fully saturated.
- Mechanism: Model accuracy improves steadily during pre-training, and even the best models show room for improvement on challenging benchmarks like ANLI, indicating ongoing learning rather than saturation.
- Core assumption: Model performance on NLI tasks continues to develop as training progresses, rather than plateauing early.
- Evidence anchors:
  - [abstract] "performance on these tasks develops steadily during training, though with some fluctuations"
  - [section] "the 70B model starts improving after 250B tokens for ANLI and αNLI; it crosses fine-tuned BERT performance after 500B tokens" (page 5)
  - [corpus] Weak evidence - related papers don't address training dynamics or saturation.
- Break condition: If accuracy curves flatten early in training or if models consistently achieve near-perfect scores across all benchmarks.

### Mechanism 3
- Claim: Model output distributions align better with human distributions than chance, but still differ substantially from human consensus.
- Mechanism: JSD between model softmax distributions and human label distributions decreases with scale and during training, suggesting models are learning to better capture human judgment patterns, though they haven't fully converged to human-level alignment.
- Core assumption: JSD is a meaningful metric for comparing model behavior to human judgment patterns in ambiguous cases.
- Evidence anchors:
  - [abstract] "the similarity of model distributions with human label distributions increases with scale, it is still much higher than the similarity between two populations of humans"
  - [section] "JSD shows a steady decrease during training, and larger models have lower JSD than smaller models" (page 8)
  - [corpus] Weak evidence - related papers don't directly address JSD comparisons between models and humans.
- Break condition: If JSD values stop decreasing during training or if model distributions converge to human distributions too quickly without continued learning.

## Foundational Learning

- Concept: Jensen-Shannon Divergence (JSD)
  - Why needed here: JSD provides a symmetric, bounded metric for comparing probability distributions between model outputs and human label distributions, allowing quantitative assessment of model-human alignment.
  - Quick check question: Why is JSD preferred over KL divergence for comparing model and human distributions in this study?

- Concept: Natural Language Inference (NLI)
  - Why needed here: NLI tasks require models to judge relationships between sentence pairs (entailment, contradiction, neutral), serving as a fundamental test of language understanding capabilities.
  - Quick check question: What are the three possible labels in standard NLI tasks, and what does each represent?

- Concept: Data contamination analysis
  - Why needed here: Understanding whether high model performance results from memorization of evaluation data versus genuine language understanding capabilities.
  - Quick check question: How does the study determine if evaluation data contamination affects model performance on NLI benchmarks?

## Architecture Onboarding

- Component map:
  - Evaluation framework: Prompts for choice-based evaluation, NLL computation, accuracy calculation
  - Training infrastructure: Custom pre-training datamix, batch processing, checkpointing
  - Analysis pipeline: JSD computation, monotonicity calculation, contamination analysis
  - Model zoo: Llama 3.1 {8, 70, 405}B and Mistral {7B, 8x7B, 8x22B} models

- Critical path:
  1. Pre-training from scratch (2T tokens for 8B and 70B models)
  2. Choice-based evaluation with few-shot examples
  3. JSD computation comparing model outputs to human label distributions
  4. Contamination analysis using 8-gram overlap methodology
  5. Manual error analysis on high-performing models

- Design tradeoffs:
  - Choice-based vs generative evaluation: Choice-based provides more stable results but may not capture full model capabilities
  - Few-shot examples: Improve performance but add complexity and potential bias
  - Prompt simplicity vs optimization: Simple prompts work but may leave performance on the table
  - Manual vs automated analysis: Manual provides insights but doesn't scale

- Failure signatures:
  - High zero-shot performance followed by dramatic improvement with few examples suggests model memorization rather than understanding
  - Low monotonicity values during training indicate benchmark instability for monitoring
  - High contamination scores without performance impact suggest false positives in contamination detection
  - JSD values that don't decrease during training suggest model limitations

- First 3 experiments:
  1. Run zero-shot evaluation on all five benchmarks to establish baseline performance
  2. Evaluate models with increasing numbers of few-shot examples (1, 3, 5, 10) to find saturation point
  3. Compute JSD values for smallest and largest models on ChaosNLI subsets to establish baseline distribution differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of few-shot examples for NLI tasks across different model sizes?
- Basis in paper: [explicit] The paper shows that performance improves with few-shot examples but saturates around ten examples
- Why unresolved: The paper doesn't systematically investigate the optimal number of shots for different model sizes and tasks
- What evidence would resolve it: A comprehensive study testing various shot counts (1-20) across all model sizes and tasks, measuring both accuracy and computational efficiency

### Open Question 2
- Question: How does model performance on NLI tasks correlate with performance on other language understanding benchmarks?
- Basis in paper: [inferred] The paper focuses solely on NLI tasks without comparing to other benchmarks
- Why unresolved: The paper doesn't explore whether NLI performance is indicative of general language understanding ability
- What evidence would resolve it: A correlation study between NLI performance and scores on multiple other benchmarks (e.g., GLUE, SuperGLUE, commonsense reasoning tasks)

### Open Question 3
- Question: What is the relationship between JSD from human distributions and model generalization to out-of-distribution NLI examples?
- Basis in paper: [explicit] The paper finds that JSD is lower for larger models but still substantial compared to human-human JSD
- Why unresolved: The paper doesn't investigate whether lower JSD correlates with better generalization
- What evidence would resolve it: Experiments testing model performance on adversarial or out-of-distribution NLI examples while measuring JSD from human distributions

## Limitations
- Findings are primarily based on Llama and Mistral model families, limiting generalizability to other architectures
- Analysis relies on synthetic human label distributions rather than true human consensus
- Study uses a relatively small number of benchmarks (five datasets) to make broad claims about NLI evaluation

## Confidence
- High confidence: Core claim that NLI benchmarks can discriminate between models of different scales and qualities
- Medium confidence: Claim that JSD comparisons meaningfully capture model-human alignment differences
- Medium confidence: Data contamination analysis methodology and its impact estimates

## Next Checks
1. Evaluate the same NLI benchmarks across a more diverse set of model families (including non-transformer architectures) to test the generalizability of the discriminative signal finding
2. Conduct a controlled study where human annotators are asked to explicitly rate model outputs for specific examples, comparing these direct human judgments to the synthetic distributions used in JSD calculations
3. Implement an ablation study on the few-shot prompt templates to determine the exact contribution of each component to model performance, separating the effects of prompt engineering from genuine language understanding capabilities