---
ver: rpa2
title: How do Transformers perform In-Context Autoregressive Learning?
arxiv_id: '2402.05787'
source_url: https://arxiv.org/abs/2402.05787
tags:
- in-context
- learning
- loss
- gradient
- consider
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how Transformers learn autoregressive processes
  of the form st+1 = W st through in-context learning. The authors show that under
  commutativity assumptions, a linear Transformer with augmented tokens implements
  one step of gradient descent on an inner objective function as its in-context mapping.
---

# How do Transformers perform In-Context Autoregressive Learning?

## Quick Facts
- arXiv ID: 2402.05787
- Source URL: https://arxiv.org/abs/2402.05787
- Authors: Michael E. Sander; Raja Giryes; Taiji Suzuki; Mathieu Blondel; Gabriel Peyré
- Reference count: 25
- One-line primary result: Transformers with augmented tokens implement one step of gradient descent for autoregressive learning; diagonal multi-head Transformers exhibit orthogonality between heads and learn trigonometric relations

## Executive Summary
This paper analyzes how linear Transformers perform in-context autoregressive learning on sequences generated by first-order processes st+1 = W st. The authors characterize the optimal in-context mappings for both augmented and non-augmented token settings. For augmented tokens, the Transformer implements one step of gradient descent on an inner loss function under commutativity assumptions. For non-augmented tokens, the analysis reveals orthogonality properties between attention heads and shows how positional encodings capture trigonometric relations in the data.

## Method Summary
The study uses synthetic sequences generated from autoregressive processes with context matrices W sampled from orthogonal or unitary distributions. Linear Transformers with trainable positional encodings are trained using gradient descent or Adam to minimize population or empirical loss. The analysis focuses on diagonal weight structures and examines both augmented tokens (0, st, st-1) and non-augmented tokens (st). Theoretical characterizations of global minima and in-context mappings are derived, then validated through controlled experiments with varying context matrix distributions.

## Key Results
- Trained one-layer linear Transformers with augmented tokens implement one step of gradient descent on an inner objective function
- For non-augmented tokens with diagonal parameters, Transformer heads exhibit orthogonality properties (B⊤A)ij = 0 for i≠j
- Positional encodings capture trigonometric relations like 2cos(θ)Rθ - I₂ = R₂θ for orthogonal context matrices
- Distribution of context matrices affects learned positional encodings, with ill-conditioned distributions leading to approximate minimum ℓ² norm solutions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Under commutativity assumptions, a trained one-layer linear Transformer with augmented tokens implements one step of gradient descent on an inner objective function as its in-context mapping.
- **Mechanism:** The augmented token encoding et := (0, st, st-1) allows the Transformer to capture the relation between consecutive tokens. With a specific block structure in parameters (Assumption 2), the model's forward pass computes the next token as if performing one step of gradient descent on the inner loss L(W, e1:T) = sum of ||st+1 - W st||². The optimal parameters align such that the in-context mapping Γθ* = W - η∇L(W, e1:T) with W initialized at zero.
- **Core assumption:** Matrices W commute and the model parameters possess a block structure with diagonal blocks.
- **Evidence anchors:**
  - [abstract] "we first show that a trained one-layer linear Transformer implements one step of gradient descent for the minimization of an inner objective function, when considering augmented tokens."
  - [section 4] "Proposition 2 demonstrates that a single step of gradient descent constitutes the optimal forward rule for the Transformer Tθ."
  - [corpus] Weak - no direct citations to gradient descent mechanisms, but related works on in-context learning (e.g., Mahankali et al., 2023) support the premise.
- **Break condition:** If W does not commute or the parameter block structure is not maintained during training, the gradient descent interpretation no longer holds.

### Mechanism 2
- **Claim:** For non-augmented tokens with diagonal parameter structure, the Transformer learns an orthogonality property between heads and uses positional encoding to capture trigonometric relations in the data.
- **Mechanism:** With diagonal attention matrices Ah and Bh, the model computes Tθ(e1:T) as a sum over t of PT-1,t [B⊤A] λt-T+1 ⊙ λt-1. At optimality, this requires B⊤A to be diagonal with unit entries, enforcing orthogonality between heads (B⊤A)ij = 0 for i≠j. The positional encoding P selects which tokens to use for prediction. For orthogonal context matrices, the optimal parameters implement trigonometric identities like 2cos(θ)Rθ - I₂ = R₂θ.
- **Core assumption:** Attention and value matrices are diagonal (Assumption 3), and context matrices are either unitary or orthogonal.
- **Evidence anchors:**
  - [abstract] "When the tokens are not augmented, we characterize the global minima of a one-layer diagonal linear multi-head Transformer. Importantly, we exhibit orthogonality between heads and show that positional encoding captures trigonometric relations in the data."
  - [section 5] "Proposition 3 (Unitary optimal in-context mapping)... Any θ* achieving zero of the loss satisfies... (B⊤A)ij = 0 for i≠j... Therefore, one must have H ≥ d."
  - [corpus] Weak - no direct citations to orthogonality constraints in Transformers, but related works on attention head pruning (Michel et al., 2019) support the observation that some heads can be removed without significant performance loss.
- **Break condition:** If the diagonal structure assumption is violated or if the context matrices are not orthogonal/unitary, the orthogonality property and trigonometric relation no longer emerge.

### Mechanism 3
- **Claim:** The distribution of context matrices W affects the learned positional encoding, with ill-conditioned distributions leading to approximate minimum ℓ² norm solutions.
- **Mechanism:** When using positional-encoding-only attention (B⊤A = Id), the problem decomposes component-wise. For a context distribution W(µ) with λ = eiθ/µ, the Hessian of the loss becomes poorly conditioned as µ → ∞. Gradient descent on this ill-conditioned loss induces ℓ² regularization, smoothing the positional encoding across diagonals. This leads to different in-context mappings compared to the optimal δt=T solution.
- **Core assumption:** The context distribution has varying degrees of concentration (parameterized by µ), and training uses gradient descent which introduces implicit regularization.
- **Evidence anchors:**
  - [section 5.3] "Proposition 7 (Conditioning)... the Hessian H... is poorly conditioned. In such a setting, gradient descent... induces a ℓ² regularization... leads to entirely different in-context mappings Γ."
  - [section 6] "Results are in Figure 8, where we mask coefficients PT-1,T... we observe that the trained positional encoding exhibits an invariance across diagonals."
  - [corpus] Weak - no direct citations to positional encoding conditioning effects, but related works on positional encoding design (Vaswani et al., 2017) support the importance of encoding structure.
- **Break condition:** If the context distribution is well-conditioned or if a different optimization method is used that doesn't introduce ℓ² regularization, the smoothed positional encoding behavior would not emerge.

## Foundational Learning

- **Concept:** Linear Algebra (diagonalization, orthogonality, matrix commutativity)
  - **Why needed here:** The paper relies heavily on properties of commuting matrices, diagonalization in unitary bases, and orthogonality constraints to characterize optimal Transformer parameters.
  - **Quick check question:** Given a 2×2 rotation matrix R(θ), what are its eigenvalues and under what condition do two such matrices commute?

- **Concept:** Gradient Descent and Optimization Theory
  - **Why needed here:** The analysis of how Transformers implement gradient descent steps and the study of convergence to global minima requires understanding of optimization dynamics and loss landscape properties.
  - **Quick check question:** For the loss ℓ(A,B,p) = ||pAB - I||², what are the stationary points and under what conditions does gradient descent converge to the global minimum?

- **Concept:** Positional Encoding and Sequence Modeling
  - **Why needed here:** The paper investigates how learnable positional encodings capture geometric and trigonometric relations in autoregressive processes, which is fundamental to understanding in-context learning.
  - **Quick check question:** How does a sinusoidal positional encoding encode relative positions, and why might this be beneficial for capturing periodic patterns in data?

## Architecture Onboarding

- **Component map:** Input sequences s1:T → Token encoding (augmented: (0, st, st-1) or non-augmented: st) → Linear attention computation → Positional encoding application → Output selection → Loss computation

- **Critical path:** Token encoding → Linear attention computation → Positional encoding application → Output selection → Loss computation

- **Design tradeoffs:**
  - Augmented vs non-augmented tokens: Augmented tokens enable gradient descent interpretation but increase dimensionality; non-augmented tokens require careful positional encoding design
  - Number of heads: More heads allow full rank solutions but increase parameter count and risk redundancy
  - Context matrix distribution: Affects conditioning of optimization and learned positional encodings

- **Failure signatures:**
  - Augmented setting: If parameter block structure is not maintained, gradient descent interpretation breaks down
  - Non-augmented setting: If diagonal structure assumption is violated, orthogonality between heads is lost
  - Positional encoding: If context distribution is too concentrated, optimization becomes ill-conditioned

- **First 3 experiments:**
  1. **Validate gradient descent interpretation:** Train a one-layer linear Transformer on augmented tokens with controlled context matrices, then check if forward pass matches one step of gradient descent on inner loss
  2. **Test orthogonality property:** Train a multi-head Transformer on non-augmented tokens, then verify if B⊤A is diagonal with unit entries at convergence
  3. **Study positional encoding conditioning:** Train positional-encoding-only models with different context distributions (varying µ), then analyze how learned positional encodings change with conditioning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the distribution of context matrices W affect the convergence rate of gradient descent in training Transformers for autoregressive learning?
- Basis in paper: [explicit] The paper discusses how the distribution of context matrices W affects trained positional encodings and explores the conditioning of the loss function.
- Why unresolved: The paper provides experimental evidence for convergence but does not derive theoretical bounds on convergence rates based on the distribution of W.
- What evidence would resolve it: A theoretical analysis proving convergence rates of gradient descent as a function of the distribution of context matrices W.

### Open Question 2
- Question: Can the in-context autoregressive learning abilities of Transformers be generalized to non-commutative context matrices?
- Basis in paper: [explicit] The paper focuses on commutative context matrices and leaves the non-commutative case as future work.
- Why unresolved: The complexity of the non-commutative case makes it challenging to characterize the in-context and prediction mappings.
- What evidence would resolve it: A theoretical framework that characterizes the in-context and prediction mappings for non-commutative context matrices.

### Open Question 3
- Question: How does the choice of positional encoding impact the in-context autoregressive learning abilities of Transformers?
- Basis in paper: [explicit] The paper discusses the role of positional encoding in capturing trigonometric relations and investigates the impact of the context distribution on trained positional encodings.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different positional encoding schemes on in-context autoregressive learning.
- What evidence would resolve it: An empirical study comparing the performance of different positional encoding schemes in in-context autoregressive learning tasks.

## Limitations

- Strong commutativity assumptions required for gradient descent interpretation limit practical applicability
- Diagonal weight structure assumption deviates significantly from practical Transformer architectures
- Analysis relies on synthetic autoregressive data rather than real-world sequences

## Confidence

- Medium confidence overall due to strong theoretical assumptions that may not hold in practice

## Next Checks

1. **Empirical Gradient Descent Verification**: Train a one-layer linear Transformer on augmented tokens with non-commuting context matrices and measure the deviation from true gradient descent steps. Quantify the relationship between commutativity violation and gradient descent approximation error.

2. **Diagonal Structure Relaxation**: Modify the Transformer architecture to use dense attention matrices while maintaining learnable positional encodings. Test whether orthogonality properties and trigonometric relations persist, and characterize the degradation as diagonal structure is relaxed.

3. **Real Data Transfer**: Apply the learned mechanisms to natural language sequences by fitting first-order autoregressive models to text data. Evaluate whether the orthogonality properties, trigonometric relations, and gradient descent interpretation transfer to this setting, and identify which aspects break down.