---
ver: rpa2
title: Neural Networks Trained by Weight Permutation are Universal Approximators
arxiv_id: '2407.01033'
source_url: https://arxiv.org/abs/2407.01033
tags:
- function
- permutation
- training
- networks
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first theoretical guarantee for permutation
  training, a method that achieves classification performance by permuting initialized
  weights without modifying their exact values. The authors prove that a ReLU network
  trained via weight permutation can universally approximate any one-dimensional continuous
  function.
---

# Neural Networks Trained by Weight Permutation are Universal Approximators

## Quick Facts
- arXiv ID: 2407.01033
- Source URL: https://arxiv.org/abs/2407.01033
- Authors: Yongqiang Cai, Gaohang Chen, Zhonghua Qiao
- Reference count: 9
- This paper proves that ReLU networks trained via weight permutation can universally approximate one-dimensional continuous functions

## Executive Summary
This paper establishes the first theoretical guarantee for permutation training, demonstrating that ReLU networks trained by permuting initialized weights (without modifying their exact values) can universally approximate any one-dimensional continuous function. The authors construct a novel four-pair step function approximator and prove that these can be combined to approximate arbitrary continuous functions while eliminating unused parameters through linear reorganization. The work bridges the gap between permutation training's practical success and theoretical foundations, highlighting its potential for describing network learning behavior and physical neural network implementations.

## Method Summary
The paper proves universal approximation for permutation-trained ReLU networks through a constructive approach. It uses a four-pair construction of step function approximators, where each step function is built from four pairs of basis functions with specific coefficient-bias matching. These approximators are combined to form a piecewise constant function that approximates any continuous target function. Unused parameters are eliminated through linear reorganization into linear functions that can be made arbitrarily small. The theoretical framework relies on permutation group properties and parameter density arguments, particularly for random initialization where increasing network width ensures sufficient parameter diversity.

## Key Results
- Proves universal approximation property for one-dimensional continuous functions using permutation-trained ReLU networks
- Establishes 1/2 convergence rate for equidistant initialization and 1/6 rate for random initialization
- Demonstrates effectiveness across various initializations through numerical experiments
- Shows permutation patterns during training that suggest the method can describe network learning behavior

## Why This Works (Mechanism)

### Mechanism 1
Weight permutation training achieves universal approximation by exploiting the rich expressive capacity of ReLU basis function combinations while maintaining fixed parameter values. The network uses a four-pair construction of step function approximators, each built from four pairs of basis functions with specific coefficient-bias matching. These approximators are combined to form a piecewise constant function that approximates any continuous target function. This works when initialized weights contain sufficient diversity and density to form the required basis function combinations after permutation.

### Mechanism 2
The unused parameters are eliminated through linear reorganization without accumulating approximation error. Unused basis function pairs are reorganized into linear functions fNNℓ(x) = mibi(x) - mibi². The sum of these linear functions forms a global linear function that can be made arbitrarily small by choosing small unused basis locations. This works when the network width is sufficiently large that there exist unused basis functions with arbitrarily small locations.

### Mechanism 3
Random initialization achieves universal approximation with high probability due to parameter density. As network width increases, randomly initialized parameters become dense enough that a subnetwork can be found that approximates the equidistantly initialized network with the desired accuracy. This works when random sampling from uniform distributions produces parameters that become arbitrarily close to equidistant values as width increases.

## Foundational Learning

- **Universal Approximation Property (UAP)**: The property that neural networks can approximate any continuous function within arbitrary accuracy. Needed here as the central theoretical contribution being proved. Quick check: Can you explain why UAP is fundamental to neural network success without looking at the paper?

- **ReLU activation properties**: Positive homogeneity and piecewise linearity that enable specific function constructions. Needed here because the proof relies on these properties to construct step function approximators. Quick check: What does positive homogeneity mean for ReLU, and how does it enable the construction in this paper?

- **Permutation groups and combinatorics**: Mathematical structures that describe how elements can be rearranged. Needed here because the theoretical framework uses permutation group properties to show that any target parameter configuration can be achieved through weight permutation. Quick check: How does the closure property of permutation groups enable the one-step nature of permutation training versus iterative methods?

## Architecture Onboarding

- **Component map**: Input layer → Hidden layer 1 (fixed basis functions ϕ±i(x)) → Hidden layer 2 (trainable coefficients θ(2n)) → Output layer (α, γ)
- **Critical path**: 1) Construct step function approximators using four-pair construction 2) Combine step approximators to form piecewise constant approximation 3) Eliminate unused parameters through linear reorganization 4) Adjust output scaling (γ) and shifting (α) if needed
- **Design tradeoffs**: Width vs. accuracy (larger n provides better approximation but increases computational cost), Equidistant vs. random initialization (equidistant offers theoretical guarantees; random may work in practice but lacks proof), Fixed vs. trainable output parameters (fixed parameters simplify implementation but may require larger networks)
- **Failure signatures**: Poor approximation despite sufficient width (indicates inadequate initialization diversity), Convergence without improvement (suggests permutation algorithm not finding optimal configurations), Oscillating error during training (may indicate poor choice of permutation period k)
- **First 3 experiments**: 1) Implement the 1-2n-1-1 network architecture with n=10, equidistant initialization, and train on y = sin(2πx) to verify basic approximation 2) Test the same architecture with pairwise random initialization to compare performance 3) Experiment with different permutation periods k (1, 5, 10) to find optimal setting for convergence speed

## Open Questions the Paper Calls Out

### Open Question 1
Can the 1/2 convergence rate be achieved for higher-dimensional functions using permutation-trained networks? The paper observes degeneration from 1/2 to 1/6 convergence rate when extending to three-dimensional inputs, hypothesizing this is due to the preliminary eight-direction basis function setup but not providing rigorous proof or alternative construction.

### Open Question 2
What systematic criteria determine which initializations are compatible with the universal approximation property in permutation training? The paper shows that commonly used initializations like Xavier and He's normal fail, while pairwise random initialization succeeds, but does not provide a theoretical framework for characterizing UAP-compatible initializations.

### Open Question 3
How can permutation training be effectively implemented in practice to achieve its theoretical benefits? The paper notes that existing implementations like LaPerm incur high computational costs and mentions more efficient search approaches exist, but does not provide concrete implementation strategies.

## Limitations

- Theoretical proof limited to one-dimensional continuous functions, with convergence rate degeneration observed in higher dimensions
- Relies on specific constructions (four-pair step functions, linear reorganization) that may not generalize
- Random initialization guarantees depend on density arguments lacking rigorous probability bounds

## Confidence

- Universal approximation for one-dimensional functions: **High** - The mathematical construction is rigorous and follows established approximation theory principles
- Linear reorganization technique: **Medium** - The method is theoretically sound but requires careful implementation to avoid numerical instability
- Random initialization guarantees: **Low** - The density-based argument provides intuition but lacks rigorous probability bounds

## Next Checks

1. Implement the four-pair step function construction and verify it can approximate simple piecewise constant functions with the claimed accuracy bounds
2. Test the linear reorganization method on networks with varying numbers of unused parameters to confirm the global linear function can be made arbitrarily small
3. Conduct empirical studies comparing equidistant versus random initialization across different network widths to validate the density-based argument and identify failure thresholds