---
ver: rpa2
title: 'On the Limitations of Language Targeted Pruning: Investigating the Calibration
  Language Impact in Multilingual LLM Pruning'
arxiv_id: '2408.14398'
source_url: https://arxiv.org/abs/2408.14398
tags:
- language
- pruning
- languages
- calibration
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of calibration language choice
  on multilingual large language model (LLM) pruning, specifically for monolingual
  downstream applications. The authors conduct a comprehensive empirical study comparing
  seven different calibration languages across diverse tasks, models, and pruning
  techniques.
---

# On the Limitations of Language Targeted Pruning: Investigating the Calibration Language Impact in Multilingual LLM Pruning

## Quick Facts
- **arXiv ID**: 2408.14398
- **Source URL**: https://arxiv.org/abs/2408.14398
- **Reference count**: 40
- **Primary result**: Calibrating on target language minimizes pruning errors and retains perplexity but does not consistently improve downstream task performance, revealing limitations in current pruning approaches.

## Executive Summary
This paper investigates how calibration language choice affects multilingual LLM pruning for monolingual downstream applications. Through comprehensive empirical studies across seven languages, diverse tasks, and two pruning methods, the authors find that while target-language calibration effectively preserves language-specific features and perplexity, it fails to consistently improve downstream task performance. Multi-level internal analysis reveals that current pruning approaches effectively preserve dominant language-specific features but struggle to maintain nuanced language-agnostic features crucial for knowledge retention and reasoning.

## Method Summary
The study applies 50% unstructured sparsity pruning using SparseGPT and Wanda methods to Llama-3 and Aya-23 models, calibrating each with 512 samples from seven different languages sampled from mC4. Pruned models are evaluated on downstream tasks (ARC, Belebele, MMLU, HellaSwag, MKQA) in zero-shot mode, with metrics including perplexity, pruning error, SNR, and internal representation changes. Internal analysis uses LSAR decomposition to separate language-specific from language-agnostic features, IoU to measure pruning mask similarity, and LAPE to analyze neuron activation frequency changes.

## Key Results
- Calibration on target language yields lowest pruning errors and highest SNR but does not consistently improve downstream task performance
- Pruning disproportionately affects language-agnostic features compared to language-specific ones across all calibration languages
- Attention output projections show the most inconsistent pruning mask preservation, particularly in early layers
- Calibration with outlier or linguistically similar languages does not provide consistent advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Calibrating on the target language preserves language-specific features but fails to maintain language-agnostic reasoning capabilities.
- Mechanism: Pruning removes weights based on importance scores computed from calibration activations. When calibration is done in the target language, pruning preferentially retains weights critical for language-specific tasks (e.g., next-token prediction, syntax), but these weights are less relevant for cross-lingual reasoning and knowledge retrieval.
- Core assumption: Language-specific and language-agnostic features are encoded in distinct weight subspaces, and pruning optimization prioritizes the dominant (language-specific) subspace.
- Evidence anchors:
  - [abstract]: "calibration on the target language effectively retains perplexity and yields high signal-to-noise ratios, it does not consistently improve downstream task performance."
  - [section 5.1]: "calibration on the target language yielding the lowest magnitude deviation from the full-sized model and lowest perplexity."
  - [section 6.1]: "calibration on the target language reduces pruning errors in language-specific features... but the pruning error on the language-agnostic features remains similar regardless of the calibration languages."
- Break condition: If pruning methods are modified to explicitly account for language-agnostic feature importance during calibration, or if calibration data includes balanced multilingual examples, this mechanism may no longer dominate.

### Mechanism 2
- Claim: Pruning struggles to consistently identify essential neurons in the attention output projection, which are responsible for language-agnostic reasoning.
- Mechanism: Attention output projections transform cross-attention representations into model outputs. These layers contain neurons critical for reasoning but are difficult to rank accurately during pruning, leading to inconsistent preservation across different calibration languages.
- Core assumption: Attention output projections handle more abstract, language-agnostic transformations compared to query/key/value projections, making their importance harder to estimate from calibration data alone.
- Evidence anchors:
  - [section 6.2]: "pruning struggles to consistently identify essential weights in the attention output projections, partly responsible for the model's language-agnostic reasoning capabilities."
  - [section 6.2]: "attention query, key, and value in the first layer consistently achieve high IoUs... However, the attention output and FFN down projection show lower IoU, especially in early layers."
- Break condition: If pruning algorithms incorporate global importance metrics (e.g., gradient-based or causal tracing) rather than relying solely on local reconstruction error, the pruning mask consistency may improve.

### Mechanism 3
- Claim: Pruning alters the activation frequency of language-specific neurons in feed-forward networks, disrupting fine-grained language-specific patterns.
- Mechanism: Low LAPE (Language Activation Probability Entropy) neurons are specialized for particular languages. Pruning tends to remove or modify these neurons, changing their activation patterns and thereby degrading language-specific performance.
- Core assumption: Language-specific neurons have low baseline activation probabilities but high output magnitudes; pruning preferentially affects these due to their lower overall activity.
- Evidence anchors:
  - [section 6.3]: "pruning struggles to retain the activation frequency of language-specific neurons of low activation probability."
  - [section 6.3]: "pruning changes activation distributions, potentially causing neurons to activate more or less frequently for a given language."
- Break condition: If pruning is constrained to preserve activation distributions (e.g., via regularization on activation probabilities), the degradation of language-specific neuron behavior may be mitigated.

## Foundational Learning

- Concept: Importance score computation in post-training pruning (e.g., SparseGPT's Hessian-based vs Wanda's magnitude-based).
  - Why needed here: Understanding how pruning methods rank weights is essential to interpret why calibration language choice matters.
  - Quick check question: In SparseGPT, what term in the importance score formula accounts for the curvature of the loss with respect to the weight?

- Concept: Signal-to-noise ratio (SNR) as a normalized pruning error metric.
  - Why needed here: SNR helps quantify how well pruning preserves the dominant signal versus noise, which is central to evaluating calibration effectiveness.
  - Quick check question: How does SNR differ from raw pruning error, and why is it more comparable across layers?

- Concept: Language-specific vs language-agnostic feature separation (e.g., LSAR method).
  - Why needed here: The paper's analysis hinges on decomposing hidden states into these two components to explain pruning effects.
  - Quick check question: In LSAR, what role does the low-rank subspace play in isolating language-specific features?

## Architecture Onboarding

- **Component map**: Multilingual LLM → Calibration set (language-specific) → Pruning importance scores → Pruned model → Evaluation (perplexity, downstream tasks, internal analysis)
- **Critical path**: Calibration → Pruning mask generation → Weight removal → Performance evaluation
- **Design tradeoffs**: High pruning sparsity vs. retention of language-agnostic reasoning; calibration language diversity vs. target-language specificity
- **Failure signatures**: Low SNR in early layers after pruning; large discrepancy between perplexity retention and downstream task performance; inconsistent pruning masks across calibration languages
- **First 3 experiments**:
  1. Run pruning with identical hyperparameters but vary only the calibration language; compare perplexity and downstream task scores.
  2. Apply LSAR decomposition to full and pruned models; measure changes in language-specific vs. language-agnostic feature magnitudes.
  3. Compute pruning mask IoU across different calibration languages for attention output and FFN layers; identify layers with high variance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of calibration language selection vary based on the specific task type or reasoning complexity?
- Basis in paper: [explicit] The paper notes that while target-language calibration improves perplexity and SNR, it does not consistently improve downstream task performance, particularly for reasoning tasks.
- Why unresolved: The study tested downstream performance across multiple tasks (MKQA, Belebele, MMLU, HellaSwag, ARC) but did not systematically categorize tasks by reasoning complexity or type to determine if certain categories benefit more from specific calibration languages.
- What evidence would resolve it: Controlled experiments comparing calibration language effects across tasks grouped by cognitive complexity (factual recall vs. multi-step reasoning vs. commonsense inference) would reveal whether task type influences optimal calibration language selection.

### Open Question 2
- Question: How does calibration language choice affect the preservation of domain-specific knowledge versus general world knowledge in pruned models?
- Basis in paper: [inferred] The paper discusses pruning's differential impact on language-agnostic features (associated with reasoning and knowledge retrieval) versus language-specific features, suggesting calibration affects knowledge preservation differently.
- Why unresolved: While the paper analyzes feature preservation at the neuron and subspace level, it does not distinguish between domain-specific factual knowledge (e.g., scientific facts) and general world knowledge or examine how calibration language affects their relative preservation.
- What evidence would resolve it: Probing experiments testing knowledge retrieval across domains (scientific, cultural, historical) with models calibrated in different languages would reveal whether calibration language differentially affects domain-specific versus general knowledge preservation.

### Open Question 3
- Question: What is the relationship between calibration language selection and the emergence of language-specific reasoning patterns in pruned multilingual models?
- Basis in paper: [explicit] The analysis of pruning masks and neuron activation patterns shows that middle layers process language-agnostic signals while early/late layers handle language-specific information, suggesting calibration affects reasoning localization.
- Why unresolved: The study demonstrates pruning affects language-specific versus language-agnostic features differently but does not examine whether calibration language causes reasoning patterns to become more localized to specific languages or more universally distributed across languages.
- What evidence would resolve it: Cross-lingual transfer experiments comparing reasoning performance when prompts are in calibration versus non-calibration languages would reveal whether calibration causes reasoning patterns to become language-specific or maintain cross-lingual consistency.

## Limitations
- Limited to 7 calibration languages, which may not fully represent linguistic diversity needed for comprehensive multilingual pruning analysis
- Zero-shot evaluation may not capture full practical impact compared to few-shot or fine-tuned approaches
- Analysis constrained to unstructured pruning methods, limiting generalizability to other pruning paradigms

## Confidence
- **High confidence**: Calibration on target language preserves perplexity and reduces pruning errors for language-specific features, but does not guarantee downstream task performance improvements
- **Medium confidence**: Pruning disproportionately affects language-agnostic features compared to language-specific ones, and attention output projections are particularly vulnerable to pruning inconsistencies
- **Low confidence**: The claim that calibration with outlier or linguistically similar languages does not provide consistent advantages

## Next Checks
1. **Expand language coverage**: Replicate the study with a broader set of calibration languages, including typologically diverse languages and additional low-resource languages, to test whether the observed patterns hold across a wider linguistic spectrum.

2. **Validate across pruning methods**: Apply the same experimental framework to structured pruning and quantization techniques to determine if the observed limitations in preserving language-agnostic features are specific to unstructured pruning or represent a more general challenge.

3. **Test with fine-tuned downstream evaluation**: Evaluate pruned models using few-shot or fine-tuned downstream tasks rather than zero-shot inference to assess whether the gap between perplexity retention and task performance persists under more practical deployment scenarios.