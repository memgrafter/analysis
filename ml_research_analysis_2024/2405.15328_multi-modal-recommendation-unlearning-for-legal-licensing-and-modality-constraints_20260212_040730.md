---
ver: rpa2
title: Multi-Modal Recommendation Unlearning for Legal, Licensing, and Modality Constraints
arxiv_id: '2405.15328'
source_url: https://arxiv.org/abs/2405.15328
tags:
- unlearning
- data
- user
- recall
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MMR ECUN is the first framework for unlearning in multi-modal recommender
  systems, addressing the removal of outdated user preferences, revoked licenses,
  and legally requested content. It employs a novel Reverse Bayesian Personalized
  Ranking (BPR) objective to selectively attenuate the impact of interactions in a
  forget set while preserving performance on retained data.
---

# Multi-Modal Recommendation Unlearning for Legal, Licensing, and Modality Constraints

## Quick Facts
- arXiv ID: 2405.15328
- Source URL: https://arxiv.org/abs/2405.15328
- Reference count: 40
- Primary result: First framework for unlearning in multi-modal recommender systems with up to 49.85% recall improvement

## Executive Summary
This paper introduces MMR ECUN, the first framework for unlearning in multi-modal recommender systems that addresses the removal of outdated user preferences, revoked licenses, and legally requested content. The framework employs a novel Reverse Bayesian Personalized Ranking (BPR) objective to selectively attenuate the impact of interactions in a forget set while preserving performance on retained data. Experiments on Amazon datasets demonstrate significant improvements in recall metrics and faster unlearning compared to retraining from scratch.

## Method Summary
MMR ECUN addresses unlearning in multi-modal recommender systems by employing a Reverse BPR objective to selectively attenuate the impact of user-item interactions in the forget set while preserving the retain set through dual-objective optimization. The method integrates multi-modal features through a graph-based architecture using contrastive learning and regularization to maintain model stability. The framework uses BPR divergence as an alternative to KL divergence for comparing recommendation scores in sparse datasets, and demonstrates effectiveness across user-centric and item-centric metrics for user, item, and combined unlearning scenarios.

## Key Results
- Achieves up to 49.85% improvement in recall metrics compared to baseline methods
- Demonstrates up to 1.3× faster unlearning compared to retraining from scratch
- Shows strong performance across user-centric and item-centric metrics for user, item, and combined unlearning scenarios

## Why This Works (Mechanism)

### Mechanism 1
The Reverse BPR objective selectively attenuates the impact of user-item interactions in the forget set while preserving the retain set through a dual-objective optimization. By minimizing the Reverse BPR loss on the forget set, the model reduces the predicted scores for interactions that need to be unlearned. Simultaneously, the forward BPR loss on the retain set reinforces the importance of these interactions, maintaining model performance. The core assumption is that the forget set can be accurately identified and isolated from the retain set without overlap.

### Mechanism 2
The integration of a negated contrastive auxiliary loss and L2 regularization effectively reduces the impact of learned item-item semantic correlations in the forget set. The contrastive auxiliary loss maximizes mutual information between behavior features and fused multi-modal features, promoting exploration of both. By negating this loss for the forget set, MMR ECUN reduces the impact of learned item-item semantic correlations, preventing them from influencing the unlearned model. The core assumption is that the contrastive auxiliary loss is effective in capturing item-item semantic correlations.

### Mechanism 3
The proposed BPR divergence provides a robust alternative to KL divergence for comparing recommendation scores in sparse and high-dimensional recommendation datasets. BPR divergence directly measures the difference between the predicted scores of user-item interactions from different model states using mean squared error, which aligns well with the numerical nature of recommendation scores. The core assumption is that the predicted scores from different model states are numerical and can be compared using mean squared error.

## Foundational Learning

- **Multi-modal Graph Convolutional Networks (GCNs)**: MMR ECUN is built on top of MGCN, which uses multi-modal GCNs to integrate diverse user-item data, including images, text, and behavior, into a unified convolutional graph. Quick check: What are the key components of a multi-modal GCN and how do they contribute to recommendation performance?

- **Bayesian Personalized Ranking (BPR) Loss**: MMR ECUN employs both forward and reverse BPR objectives to balance retention fidelity with unlearning specificity. Quick check: How does the BPR loss differ from other ranking losses and why is it suitable for implicit feedback in recommender systems?

- **Contrastive Learning**: MMR ECUN uses a contrastive auxiliary task to maximize mutual information between behavior features and fused multi-modal features, promoting exploration of both. Quick check: What is the purpose of contrastive learning and how does it enhance feature representations in multi-modal recommender systems?

## Architecture Onboarding

- **Component map**: Multi-modal Graph Convolutional Network (MGCN) -> Reverse BPR objective -> Forward BPR objective -> Negated contrastive auxiliary loss -> L2 regularization -> BPR divergence

- **Critical path**: 1. Receive unlearning request and identify the forget set and retain set. 2. Update the interaction matrix by nullifying interactions in the forget set. 3. Train the model using the Reverse BPR objective on the forget set and the Forward BPR objective on the retain set, along with the negated contrastive auxiliary loss and L2 regularization. 4. Evaluate the unlearned model using user-centric and item-centric metrics.

- **Design tradeoffs**: Balancing the trade-off between retention fidelity and unlearning specificity by adjusting the hyper-parameter α. Choosing between exact and approximate unlearning methods based on computational resources and unlearning requirements.

- **Failure signatures**: Degradation in model performance on the retain set indicates insufficient retention fidelity. Persistence of interactions in the forget set indicates insufficient unlearning specificity. High computational cost indicates the need for optimization or alternative unlearning methods.

- **First 3 experiments**: 1. Unlearn 5% of users and evaluate the model using Recall@20 on the validation, test, and forget sets. 2. Unlearn 5% of items and evaluate the model using Recall@500, Recall@1000, and Recall@1500 on the validation, test, and forget sets. 3. Unlearn both 5% of users and 5% of items simultaneously and evaluate the model using Recall@20 and Recall@500 on the validation, test, and forget sets.

## Open Questions the Paper Calls Out

### Open Question 1
How does MMR ECUN perform when unlearning user-item interactions involving modalities beyond text and images, such as audio or video embeddings? The authors mention that M is the set of modalities considered, specifically M = {v, t} for visual and textual, but do not evaluate other modalities like audio or video. The paper does not provide empirical results for modalities outside of text and images, leaving the generalizability to other modalities unexplored. Experiments on datasets with additional modalities (e.g., audio or video) showing retention fidelity, unlearning specificity, and generalizability metrics comparable to the current results would resolve this question.

### Open Question 2
What is the impact of MMR ECUN on recommendation diversity, and does unlearning inadvertently reduce the variety of recommendations? The authors discuss unlearning for evolving user interests and bias elimination, but do not explicitly measure the effect on recommendation diversity metrics such as intra-list distance or coverage. While the paper focuses on accuracy metrics like recall and precision, it does not report diversity measures that could reveal unintended consequences of unlearning. Diversity metrics (e.g., intra-list distance, coverage) before and after unlearning, demonstrating whether MMR ECUN maintains or enhances recommendation variety, would resolve this question.

### Open Question 3
How does the choice of the hyper-parameter α in MMR ECUN affect the trade-off between unlearning speed and retention of retain set performance? The authors tune α in Table 5 and discuss its effect on balancing preservation and reduction, but do not systematically explore its impact on unlearning efficiency (time to converge). The paper provides qualitative insights into α's role but lacks a detailed analysis of its effect on the speed-accuracy trade-off. A comprehensive study varying α across a wider range, reporting unlearning time, recall on retain set, and unlearning specificity to quantify the trade-offs, would resolve this question.

## Limitations

- The paper lacks a formal privacy guarantee or certified unlearning bound, which is critical for legal compliance scenarios
- Experiments are conducted on Amazon datasets with pre-extracted features, which may not capture the full complexity of real-world multi-modal recommendation systems
- The framework's performance at scale (millions of users/items) and with live data streams is not evaluated

## Confidence

- **High Confidence**: The mechanism of using Reverse BPR to attenuate forget set interactions while preserving retain set performance through dual-objective optimization
- **Medium Confidence**: The effectiveness of negated contrastive auxiliary loss in reducing item-item semantic correlations for unlearning
- **Medium Confidence**: The superiority of BPR divergence over KL divergence for comparing recommendation scores in sparse datasets

## Next Checks

1. Implement and test the certified unlearning framework from the supplementary materials to verify formal privacy bounds, not just empirical performance metrics
2. Evaluate MMR ECUN on streaming recommendation datasets with live multi-modal data ingestion to assess performance under realistic conditions
3. Measure computational overhead and memory requirements for MMR ECUN at scale (millions of users/items) to validate the claimed efficiency advantage over retraining