---
ver: rpa2
title: Hindsight Experience Replay Accelerates Proximal Policy Optimization
arxiv_id: '2410.22524'
source_url: https://arxiv.org/abs/2410.22524
tags:
- ppo-her
- environments
- learning
- policy
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hindsight experience replay (HER), which modifies the goal of an
  episode post-hoc, typically accelerates off-policy RL algorithms but is not applied
  to on-policy algorithms like PPO due to the assumption that training data is generated
  by the current policy. This work demonstrates that a naive combination of HER with
  PPO can dramatically accelerate learning in continuous control tasks, specifically
  a custom predator-prey environment.
---

# Hindsight Experience Replay Accelerates Proximal Policy Optimization

## Quick Facts
- arXiv ID: 2410.22524
- Source URL: https://arxiv.org/abs/2410.22524
- Reference count: 6
- PPO-HER achieved higher median rewards and greater sample efficiency than PPO, SAC, and SAC-HER in 10 out of 12 tested conditions

## Executive Summary
This work demonstrates that Hindsight Experience Replay (HER), traditionally applied to off-policy reinforcement learning algorithms, can be successfully combined with Proximal Policy Optimization (PPO) to accelerate learning in continuous control tasks. The authors show that a naive combination of HER with PPO dramatically improves sample efficiency and performance in a custom predator-prey environment, achieving higher median rewards and greater clock-time efficiency than PPO, SAC, and SAC-HER across most tested conditions. The approach is particularly effective in environments with sparse rewards where traditional PPO struggles to learn. However, PPO-HER fails to generalize to most Fetch environments, suggesting limitations when achieved goals rarely change.

## Method Summary
The authors implement a naive combination of HER with PPO by modifying the goal of episodes post-hoc and storing these relabeled experiences in a replay buffer. During training, they recalculate log probabilities for the relabeled goals and use these in PPO's clipped surrogate objective. The implementation includes a final strategy goal resampling approach and trajectory filtering to maintain training stability. The method is tested on a custom predator-prey environment with varying difficulty parameters (interception distance, environment size, state dimensionality) and compared against PPO, SAC, and SAC-HER baselines.

## Key Results
- PPO-HER achieved higher median rewards than PPO, SAC, and SAC-HER in 10 out of 12 tested conditions
- PPO-HER demonstrated greater sample efficiency, solving environments that PPO alone could not solve
- PPO-HER was less sensitive to hyperparameter choices, particularly learning rate, with only 76% variation compared to PPO's 3200% variation
- PPO-HER scaled better to more difficult environments with smaller interception distances, larger environment sizes, and higher state dimensionality
- PPO-HER failed to generalize to most Fetch environments, learning only the FetchReach task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PPO-HER works because PPO uses stochastic Gaussian policies with infinite action domains, making it robust to goal resampling.
- Mechanism: Gaussian policies assign non-zero probability to all actions, so even after goal resampling, the likelihood ratio used in PPO updates remains finite. This prevents numerical instability that would occur with deterministic policies.
- Core assumption: The Gaussian policy's infinite domain means no action is impossible, only improbable.
- Evidence anchors:
  - [abstract]: "PPO can accommodate HER due to its stochastic Gaussian policies"
  - [section 6.3]: "Given this interpretation, we can think of HER as an efficient way to sample many, many times and then filter the sampled data to select particularly informative samples (successes) for training."
  - [corpus]: No direct evidence in corpus neighbors, but general HER literature supports Gaussian robustness.
- Break condition: When policy entropy approaches zero and the Gaussian becomes effectively deterministic, likelihood ratios can become zero or undefined, causing instability.

### Mechanism 2
- Claim: HER accelerates PPO by providing informative samples from failed trajectories that would otherwise be useless under sparse rewards.
- Mechanism: In sparse reward environments, most episodes yield no learning signal. HER rewrites goals post-hoc to create artificial successes, turning failed trajectories into valuable training data.
- Core assumption: The environment allows meaningful relabeling of goals without requiring expert demonstrations.
- Evidence anchors:
  - [abstract]: "HER accelerates off-policy reinforcement learning algorithms for environments that emit sparse rewards by modifying the goal of the episode post-hoc"
  - [section 5.1]: "PPO-HER was able to solve environments that PPO, when used without HER, was unable to solve"
  - [corpus]: Multiple HER papers in corpus support goal relabeling for sparse rewards.
- Break condition: When achieved goals rarely change (like in Fetch environments with stationary blocks), relabeling provides little new information.

### Mechanism 3
- Claim: PPO-HER is less sensitive to hyperparameters because HER provides more diverse and informative training data.
- Mechanism: HER's goal resampling creates a more varied dataset, reducing overfitting to specific hyperparameter settings and smoothing the optimization landscape.
- Core assumption: Increased data diversity from HER compensates for suboptimal hyperparameter choices.
- Evidence anchors:
  - [section 5.2]: "PPO-HER was relatively insensitive to changes in the learning rate; at its worst... the maximum median reward changed by only 76% between the highest-performing and lowest-performing learning rates"
  - [section 5.2]: "PPO, at its worst... varied by as much as 3200%"
  - [corpus]: No direct evidence in corpus neighbors, but this aligns with general principles of data augmentation.
- Break condition: When HER's goal resampling becomes ineffective (e.g., in environments where achieved goals don't vary), the data diversity benefit disappears.

## Foundational Learning

- Concept: Hindsight Experience Replay (HER)
  - Why needed here: HER is the core technique that allows PPO to learn from sparse rewards by relabeling goals post-hoc
  - Quick check question: What does HER do differently from standard experience replay?

- Concept: On-policy vs Off-policy algorithms
  - Why needed here: Understanding why HER traditionally applies to off-policy algorithms but this work applies it to on-policy PPO
  - Quick check question: What assumption do on-policy algorithms make about their training data that HER violates?

- Concept: Gaussian stochastic policies
  - Why needed here: PPO uses Gaussian policies, which is key to understanding why it can tolerate HER's goal resampling
  - Quick check question: Why are Gaussian policies with infinite domains more robust to goal resampling than deterministic policies?

## Architecture Onboarding

- Component map: Environment interaction -> Store transitions -> HER goal resampling -> Recalculate log probs -> PPO update
- Critical path: Environment interaction → Store transitions → HER goal resampling → Recalculate log probabilities → PPO update
- Design tradeoffs: HER adds computation for goal resampling and log probability recalculation but provides sample efficiency gains; trade-off between training stability and computational overhead
- Failure signatures: Catastrophic forgetting when success rate is high, poor generalization to environments where achieved goals don't vary, instability when policy entropy is very low
- First 3 experiments:
  1. Implement basic PPO without HER on predator-prey environment to establish baseline performance
  2. Add HER with final strategy goal resampling to PPO and verify performance improvement
  3. Test PPO-HER on environment with varying difficulty (different interception distances) to observe scaling behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does PPO-HER fail to generalize to most Fetch environments while succeeding in Predator-Prey environments?
- Basis in paper: [explicit] The authors note that PPO-HER was able to learn only the FetchReach task and suggest that this failure may be due to the achieved goal (position of the block) not varying much with random arm movements, leading to overfitting to setting the desired goal to the achieved goal.
- Why unresolved: The paper provides a hypothesis but does not experimentally verify why this difference in achieved goal dynamics between environments causes the failure. The authors mention that trajectory filtering might help but do not test this.
- What evidence would resolve it: Testing PPO-HER with trajectory filtering on Fetch environments, or creating a modified Predator-Prey environment where the achieved goal rarely changes, to see if PPO-HER fails in that case as well.

### Open Question 2
- Question: What is the precise mechanism by which PPO-HER outperforms PPO, SAC, and SAC-HER in terms of sample efficiency and clock-time efficiency?
- Basis in paper: [explicit] The authors show that PPO-HER achieves higher rewards and is more sample and clock-time efficient than the other algorithms, but they do not provide a detailed analysis of why this is the case.
- Why unresolved: While the authors provide experimental results showing the superiority of PPO-HER, they do not offer a comprehensive theoretical explanation for the observed performance differences.
- What evidence would resolve it: A detailed ablation study comparing the sample efficiency and clock-time efficiency of PPO, PPO-HER, SAC, and SAC-HER on a variety of tasks, with a focus on identifying the specific factors that contribute to the performance differences.

### Open Question 3
- Question: How does the stability of PPO-HER change as the policy improves and the policy entropy decreases?
- Basis in paper: [explicit] The authors note that the stability of PPO-HER diminishes as the policy improves and that catastrophic forgetting can occur once the agent is mostly successful.
- Why unresolved: The paper mentions this issue but does not provide a detailed analysis of how the stability of PPO-HER changes with policy improvement or offer concrete solutions to address this problem.
- What evidence would resolve it: A study tracking the stability of PPO-HER over the course of training, measuring policy entropy and success rate, to identify the point at which stability issues arise and to test potential solutions such as entropy regularization or adaptive learning rate schedules.

## Limitations
- PPO-HER failed to generalize to most Fetch environments, suggesting the approach may be domain-specific rather than broadly applicable
- The method may break down when policy entropy approaches zero and Gaussian policies become effectively deterministic
- HER's goal resampling provides little benefit in environments where achieved goals rarely change

## Confidence

### High Confidence
- PPO-HER's stability with Gaussian policies due to infinite action domains preventing numerical instability

### Medium Confidence
- HER provides informative samples from failed trajectories that accelerate learning in sparse reward environments
- Increased data diversity from HER makes PPO-HER less sensitive to hyperparameter choices

## Next Checks

1. Test PPO-HER on a broader range of continuous control tasks with varying degrees of goal variability to better understand the limits of HER's applicability.

2. Implement a deterministic variant of PPO (using tanh-Gaussian or similar) to empirically verify that stochasticity is necessary for HER compatibility.

3. Measure the entropy of PPO policies throughout training in PPO-HER to quantify how close policies get to determinism and whether this correlates with any instability or performance degradation.