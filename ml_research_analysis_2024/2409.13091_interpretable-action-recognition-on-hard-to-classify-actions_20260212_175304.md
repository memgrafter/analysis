---
ver: rpa2
title: Interpretable Action Recognition on Hard to Classify Actions
arxiv_id: '2409.13091'
source_url: https://arxiv.org/abs/2409.13091
tags:
- object
- depth
- objects
- video
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of recognizing complex activities
  in video using a human-like interpretable model. The authors focus on improving
  an existing top-down model (TDM) by incorporating 3D awareness through two methods:
  fine-tuning a container detection model and using monocular depth estimation.'
---

# Interpretable Action Recognition on Hard to Classify Actions

## Quick Facts
- arXiv ID: 2409.13091
- Source URL: https://arxiv.org/abs/2409.13091
- Authors: Anastasia Anichenko; Frank Guerin; Andrew Gilbert
- Reference count: 6
- One-line primary result: Adding depth relations improved model accuracy from 48% to 51% on three similar "Putting" actions

## Executive Summary
This paper addresses the challenge of recognizing complex activities in video using a human-like interpretable model. The authors focus on improving an existing top-down model (TDM) by incorporating 3D awareness through two methods: fine-tuning a container detection model and using monocular depth estimation. The primary result shows that while the container detection did not improve performance, adding depth relations significantly improved the model's accuracy from 48% to 51% on a subset of three similar "Putting" actions from the Something-Something-v2 dataset. The study highlights the difficulty of achieving human-level performance on tasks involving everyday object affordances and demonstrates the importance of 3D information in video understanding.

## Method Summary
The authors improved a top-down interpretable model (TDM) by adding 3D awareness capabilities. They tested two approaches: fine-tuning Detectron2 for container detection and using SharpNet monocular depth estimation. The model processes video frames with 2D bounding boxes of objects and hands, segments videos into 5 phases specific to each action, computes relations among bounding boxes, and uses a random forest classifier for prediction. The depth estimation approach extracted depth values for individual objects and calculated depth relations, while the container detection attempted to classify objects as containers or non-containers.

## Key Results
- Container detection model failed to improve performance, plateauing around 69% accuracy
- Adding depth relations significantly improved performance from 48% to 51% on the three "Putting" action classes
- Depth estimation quality varied widely across videos, with failures evident in videos with significant motion or challenging viewing angles

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding depth relations improves performance by capturing critical 3D spatial relationships between objects and hands.
- Mechanism: The model calculates depth differences between objects and hands, which helps distinguish between similar actions like "Putting something into/onto/underneath something" that are otherwise confused in 2D space.
- Core assumption: Depth information provides discriminative features that are not captured by 2D bounding boxes alone.
- Evidence anchors:
  - [abstract]: "the addition of depth relations made a significant improvement to performance"
  - [section]: "A state-of-the-art depth estimation model was used to extract depth values for individual objects and calculate depth relations"
  - [corpus]: Weak correlation with related work on 3D spatial reasoning (average FMR=0.446)
- Break condition: If depth estimation quality degrades significantly or if the depth differences between action classes become too subtle to be useful.

### Mechanism 2
- Claim: Fine-tuning Detectron2 for container detection failed because it cannot learn 3D features like concavity from 2D properties alone.
- Mechanism: The container detection model tried to classify objects as container/not-container based on 2D visual features, but failed to generalize across diverse container shapes.
- Core assumption: Container classification requires understanding of 3D affordances (concavity) that cannot be derived from 2D bounding boxes.
- Evidence anchors:
  - [section]: "Detectron2 simply does not have the capacity to learn 3D features such as 'concavity' based off of 2D properties alone"
  - [section]: "the model appears to be classifying the underlying classes that were merged into 'Container' and 'Not-Container'"
  - [corpus]: No direct supporting evidence found
- Break condition: If a different architecture or feature representation could capture 3D affordances from 2D data.

### Mechanism 3
- Claim: Human-like interpretable models are inherently limited by the impoverished feature set compared to deep learning approaches.
- Mechanism: The TDM uses only 2D bounding boxes and simple relations, lacking detailed object and hand pose information that humans use for recognition.
- Core assumption: Humans extract much richer features from video than the simple 2D bounding box representation used in TDM.
- Evidence anchors:
  - [section]: "Our model is extremely impoverished compared to the level of detail a human brain extracts"
  - [section]: "humans can describe a lot of detail... Our model is extremely impoverished"
  - [corpus]: Weak evidence from related work on 3D hand-object reconstruction
- Break condition: If the feature set could be enriched without losing interpretability, or if the task could be solved with the current limited features.

## Foundational Learning

- Concept: Spatio-temporal relations in video understanding
  - Why needed here: The TDM relies on recognizing critical spatial relationships between objects across different video phases
  - Quick check question: What are the five phases used to segment video in the TDM, and what happens in each phase?

- Concept: 3D depth estimation from monocular images
  - Why needed here: Depth estimation provides the critical 3rd dimension that helps distinguish between similar actions
  - Quick check question: What depth estimation model was used, and what four depth-related features were extracted?

- Concept: Interpretability vs performance tradeoff in action recognition
  - Why needed here: The paper explicitly compares human-like interpretable models with mainstream deep learning approaches
  - Quick check question: Why does the interpretable approach fall short of deep learning performance according to the discussion?

## Architecture Onboarding

- Component map: Video frames → Bounding box extraction → Temporal segmentation → Feature computation → Classification → Action prediction
- Critical path: Video frames → Bounding box extraction → Temporal segmentation → Feature computation → Classification → Action prediction
- Design tradeoffs:
  - Interpretability vs performance: Human-like models are interpretable but less accurate than deep learning
  - 2D vs 3D features: Adding depth improves performance but increases complexity
  - Container detection vs depth relations: Container detection failed while depth relations succeeded
- Failure signatures:
  - Poor performance on action classes that require 3D understanding
  - Container detection accuracy plateauing around 69%
  - Depth estimation errors in videos with motion or challenging angles
- First 3 experiments:
  1. Test baseline TDM performance on the three "Putting" action classes
  2. Add depth relations to the baseline and measure performance improvement
  3. Implement container detection and evaluate its impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the object detection model be improved to better learn 3D features such as concavity for container classification?
- Basis in paper: [explicit] The paper states that the container detection model failed to generalize on common features that all the samples shared, indicating that Detectron2 does not have the capacity to learn 3D features such as "concavity" based off of 2D properties alone.
- Why unresolved: The paper highlights the difficulty of achieving human-level performance on tasks such as classifying affordances of everyday household objects using 2D properties alone, but does not propose specific solutions or alternative approaches to improve the model's ability to learn 3D features.
- What evidence would resolve it: Evidence that demonstrates the effectiveness of alternative approaches, such as incorporating 3D information or using different model architectures, in improving the container detection model's ability to learn 3D features and generalize across different container types.

### Open Question 2
- Question: How can the depth estimation model be improved to handle challenging video conditions, such as significant motion within the frame and difficult viewing angles?
- Basis in paper: [explicit] The paper mentions that the accuracy of the depth estimation varies widely for different videos and that failures are evident when there is significant motion within the frame as well as challenging angles of view.
- Why unresolved: The paper identifies the challenges faced by the depth estimation model but does not propose specific solutions or techniques to address these issues and improve the model's robustness to challenging video conditions.
- What evidence would resolve it: Evidence that demonstrates the effectiveness of techniques such as data augmentation, multi-view training, or advanced depth estimation architectures in improving the model's accuracy and robustness to challenging video conditions.

### Open Question 3
- Question: How can the interpretable model be extended to incorporate more detailed and advanced features, similar to those extracted by human vision, to improve performance?
- Basis in paper: [explicit] The paper discusses the limitations of the current interpretable model, which has very little information about the objects in the scene and lacks the level of detail that a human brain extracts, such as 6D pose estimation and grasp recognition.
- Why unresolved: The paper acknowledges the gap between the current model's capabilities and human-like features but does not propose specific methods or approaches to incorporate more advanced features and improve the model's performance.
- What evidence would resolve it: Evidence that demonstrates the effectiveness of incorporating advanced features, such as 6D pose estimation, grasp recognition, or detailed object and context understanding, in improving the interpretable model's performance and bringing it closer to human-level capabilities.

## Limitations
- The study's scope is limited to only three action classes, making it unclear whether improvements generalize to broader action recognition tasks.
- The baseline TDM model already achieved only 48% accuracy, raising questions about whether improvements represent meaningful progress or simply help a weak model perform better.
- The container detection failure suggests fundamental limitations in using 2D models for 3D understanding, but the paper doesn't explore alternative approaches.

## Confidence
- High confidence: The core finding that depth relations significantly improve performance (48% to 51%) is well-supported by experimental results.
- Medium confidence: The interpretation that container detection failed because Detectron2 cannot learn 3D affordances from 2D features is plausible but lacks direct evidence.
- Medium confidence: The claim that human-like interpretable models are inherently limited by their impoverished feature set is reasonable but speculative.

## Next Checks
1. Test the depth relation improvements on a larger and more diverse set of action classes to assess generalizability beyond the three "Putting" actions.
2. Implement alternative container detection approaches (e.g., 3D-aware models or different feature representations) to determine if the failure was specific to Detectron2 or represents a fundamental challenge.
3. Evaluate whether enriching the TDM's feature set with additional spatial or temporal features while maintaining interpretability can close the performance gap with deep learning approaches.