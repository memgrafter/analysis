---
ver: rpa2
title: 'Unveiling the Achilles'' Heel of NLG Evaluators: A Unified Adversarial Framework
  Driven by Large Language Models'
arxiv_id: '2405.14646'
source_url: https://arxiv.org/abs/2405.14646
tags:
- adversarial
- evaluation
- association
- response
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdvEval, a novel adversarial framework designed
  to evaluate the robustness of natural language generation (NLG) evaluators by generating
  high-quality adversarial data that creates strong disagreements between human judgments
  and evaluator predictions. The framework leverages large language models (LLMs)
  both as a generator and a gold evaluator to iteratively optimize benign inputs,
  producing adversarial responses that are then scored by both the gold and victim
  evaluators.
---

# Unveiling the Achilles' Heel of NLG Evaluators: A Unified Adversarial Framework Driven by Large Language Models

## Quick Facts
- **arXiv ID:** 2405.14646
- **Source URL:** https://arxiv.org/abs/2405.14646
- **Reference count:** 40
- **Primary result:** AdvEval achieves 84.8% success rate for high-quality adversarial data and 92.5% for low-quality data, outperforming existing methods

## Executive Summary
This paper introduces AdvEval, a novel adversarial framework designed to evaluate the robustness of natural language generation (NLG) evaluators. The framework leverages large language models (LLMs) as both generators and gold evaluators to iteratively optimize benign inputs, producing adversarial responses that create strong disagreements between human judgments and evaluator predictions. Tested across 12 victim evaluators on three NLG tasks (dialogue, summarization, and question evaluation), AdvEval demonstrates significant performance improvements over existing methods, with success rates of 84.8% for high-quality and 92.5% for low-quality adversarial data generation.

## Method Summary
AdvEval employs a black-box adversarial framework that uses LLMs as both data generators and gold evaluators. The method iteratively optimizes benign inputs by generating candidate responses through task-specific prompts, which are then scored by both the gold evaluator (proxy for human judgment) and victim evaluator. A feedback score combining these evaluations guides the generator toward responses that maximize disagreement between human and victim evaluator judgments. The framework operates across three NLG tasks using 11 datasets and evaluates 12 victim evaluators, achieving superior performance through its larger search space and improved search constraints compared to baseline methods.

## Key Results
- AdvEval achieves an average success rate of 84.8% for generating high-quality (R+) adversarial data
- The framework reaches 92.5% success rate for low-quality (R-) adversarial data generation
- Manual validation confirms gold evaluator reliability with over 80% agreement with human annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdvEval generates high-quality adversarial data by iteratively optimizing benign inputs using LLM-based generator and gold evaluator
- Mechanism: The generator iteratively creates candidate responses using task-specific prompts. These candidates are scored by both the gold evaluator (proxy for human judgment) and victim evaluator. The feedback score combines these scores to guide the generator toward responses that maximize disagreement between human and victim evaluator judgments
- Core assumption: The LLM-based gold evaluator reliably approximates human judgment and can guide the generator toward high-quality adversarial responses
- Evidence anchors: [abstract] "We adopt strong LLMs as both the data generator and gold evaluator. Adversarial data are automatically optimized with feedback from the gold and victim evaluator."

### Mechanism 2
- Claim: AdvEval outperforms existing methods by having a larger search space and improved search constraints
- Mechanism: Unlike baseline methods that only add local perturbations, AdvEval leverages LLM generative capabilities to perform both local and global perturbations, resulting in a substantially larger search space. Additionally, AdvEval uses an LLM evaluator as a human proxy, which more accurately assesses response quality than rule-based or embedding-based constraints
- Core assumption: The LLM generator can explore a diverse response space beyond local perturbations while maintaining label validity through gold evaluator feedback
- Evidence anchors: [section 4.2] "AdvEval benefits from a larger search space... AdvEval leverages the remarkable generative capabilities of LLMs to perform both local and global perturbations."

### Mechanism 3
- Claim: The feedback score formulation effectively balances exploitation of high-quality responses with exploration of diverse adversarial examples
- Mechanism: The feedback score combines gold evaluator and victim evaluator scores with a hyperparameter α. Higher α encourages the generator to follow gold evaluator judgment, while lower α allows more aggressive exploration of responses that might fool the victim evaluator
- Core assumption: The hyperparameter α can be tuned to balance the trade-off between generating high-quality responses and finding adversarial examples
- Evidence anchors: [section 3.4] "where α is hyper-parameter balancing the influence of Sgold and Svictim. A higher α encourages the generator to exploit the response space adhere to judgement of gold evaluator."

## Foundational Learning

- Concept: Natural Language Generation (NLG) evaluation
  - Why needed here: The paper focuses on evaluating the robustness of NLG evaluators against adversarial attacks, so understanding how NLG evaluation works is fundamental
  - Quick check question: What are the main differences between reference-based and reference-free NLG evaluation metrics?

- Concept: Adversarial attacks in NLP
  - Why needed here: The paper introduces an adversarial framework specifically designed for NLG evaluators, building on general adversarial attack concepts but addressing unique challenges in NLG evaluation
  - Quick check question: Why does the label preservation assumption commonly used in classification tasks not hold for NLG evaluation tasks?

- Concept: Large Language Model (LLM) capabilities
  - Why needed here: The framework leverages LLMs both as generators and evaluators, so understanding LLM strengths and limitations is crucial
  - Quick check question: What are the key advantages of using LLMs as evaluators compared to traditional reference-free metrics?

## Architecture Onboarding

- Component map: Generator (LLM-based) → Candidate generation → Scoring (Gold Evaluator + Victim Evaluator) → Feedback score computation → Prompt update → Iteration → Adversarial response output
- Critical path: The iterative optimization loop is the core of the framework. Each iteration involves generating candidates, scoring them, computing feedback, and updating the prompt
- Design tradeoffs: Using LLMs as both generator and evaluator provides flexibility and quality but increases computational cost. The trade-off between exploration (diverse responses) and exploitation (following gold evaluator guidance) is managed through the α parameter
- Failure signatures: Poor adversarial examples (low success rate), high computational cost, or generator getting stuck in local optima. If the gold evaluator's scores don't align with human judgments, the entire framework fails
- First 3 experiments:
  1. Test the framework on a single NLG task (e.g., dialogue response) with one victim evaluator to verify basic functionality
  2. Compare AdvEval's performance against baseline methods on the same task to validate effectiveness
  3. Conduct ablation studies by removing the gold evaluator or evaluation criteria to understand their impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the search space for adversarial examples be expanded beyond the current local perturbations to improve the effectiveness of AdvEval in generating more challenging adversarial samples?
- Basis in paper: [explicit] The paper mentions that AdvEval leverages the remarkable generative capabilities of LLMs to perform both local and global perturbations, resulting in a substantially larger search space
- Why unresolved: The paper does not provide specific strategies or techniques for expanding the search space beyond local perturbations
- What evidence would resolve it: Research demonstrating novel methods for global perturbations that significantly increase the diversity and quality of adversarial samples generated by AdvEval

### Open Question 2
- Question: What are the potential defense mechanisms against AdvEval that could be developed to improve the robustness of NLG evaluators?
- Basis in paper: [explicit] The paper suggests that future works include the development of defense methods against AdvEval
- Why unresolved: The paper does not explore or propose specific defense mechanisms against the adversarial attacks generated by AdvEval
- What evidence would resolve it: Studies proposing and evaluating effective defense strategies that can mitigate the impact of AdvEval-generated adversarial examples on NLG evaluators

### Open Question 3
- Question: How can AdvEval be extended to target specific evaluation dimensions beyond overall quality or dimensions highly correlated with overall quality?
- Basis in paper: [explicit] The paper mentions that future research could explore dimension-oriented attacks
- Why unresolved: The paper primarily focuses on attacking evaluators based on overall quality or dimensions highly correlated with overall quality, leaving the exploration of dimension-specific attacks unaddressed
- What evidence would resolve it: Experiments demonstrating the effectiveness of AdvEval in generating adversarial examples that specifically target and degrade performance on particular evaluation dimensions

## Limitations

- The framework's reliance on LLM-based gold evaluators introduces potential reliability concerns, as human judgment verification was only performed on a small sample (20 instances)
- The hyperparameter α was fixed at 1 across all tasks without tuning, which may not represent optimal settings for each specific evaluation task
- Computational cost of using LLMs for both generation and evaluation could limit scalability for real-world applications

## Confidence

- **High confidence:** The framework's ability to generate adversarial examples that create disagreements between victim evaluators and the gold evaluator (supported by quantitative success rates of 84.8% for R+ and 92.5% for R-)
- **Medium confidence:** The gold evaluator's reliability as a proxy for human judgment (supported by manual validation on limited samples showing >80% agreement)
- **Medium confidence:** Performance superiority over baseline methods (demonstrated through comparative experiments but limited to specific datasets and tasks)

## Next Checks

1. Conduct comprehensive human evaluation studies across all three tasks (dialogue, summarization, question evaluation) to verify the gold evaluator's reliability beyond the initial 20-sample validation
2. Perform ablation studies with different α values to determine optimal hyperparameter settings for each task and assess sensitivity to this critical parameter
3. Test the framework's scalability by evaluating performance on larger datasets and measuring computational resource requirements to identify practical deployment limitations