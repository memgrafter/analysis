---
ver: rpa2
title: Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations
arxiv_id: '2405.18392'
source_url: https://arxiv.org/abs/2405.18392
tags:
- cooldown
- cosine
- training
- steps
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational inefficiency in scaling law
  experiments for large language models, which traditionally require training multiple
  models from scratch with different cosine learning rate schedules. The authors propose
  replacing the cosine schedule with a constant learning rate followed by a short
  cooldown phase (decay to zero), demonstrating this approach matches cosine performance
  while enabling significant compute savings.
---

# Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations

## Quick Facts
- arXiv ID: 2405.18392
- Source URL: https://arxiv.org/abs/2405.18392
- Reference count: 40
- Primary result: Constant learning rate with cooldown phase matches cosine schedule performance while reducing compute requirements by ~50%

## Executive Summary
This paper addresses the computational inefficiency of traditional scaling law experiments for large language models, which require training multiple models from scratch with different cosine learning rate schedules. The authors propose replacing the cosine schedule with a constant learning rate followed by a short cooldown phase, demonstrating this approach matches cosine performance while enabling significant compute savings. They further show that stochastic weight averaging (SWA) provides strong performance across training trajectories without requiring separate cooldowns. Experimental results across models from 33M to 1B parameters show the constant+cooldown schedule scales reliably and reduces both FLOPs and GPU hours by approximately half compared to traditional methods.

## Method Summary
The method replaces the standard cosine learning rate schedule with a constant learning rate phase followed by a cooldown phase that decays the learning rate to zero. Models are trained on the SlimPajama dataset (6B tokens) using Transformer architectures with AdamW optimizer. The constant phase allows effective exploration of the loss landscape, while the cooldown phase enables smooth convergence to better minima. Stochastic weight averaging is applied by averaging model weights over fixed windows (500 steps) during training. This approach enables scaling experiments to be performed with fewer but reusable training runs rather than requiring separate models for each training duration.

## Key Results
- Constant learning rate with cooldown phase matches cosine schedule performance across different training durations
- Cooldown phases of 5-20% of total training steps provide optimal performance, with 20% being most reliable
- SWA improves validation perplexity and downstream benchmark performance without additional training costs
- Compute and GPU hours reduced by approximately 50% compared to traditional cosine schedule methods
- Scaling behavior remains predictable from 33M to 1B parameter models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constant learning rate followed by a short cooldown phase matches the performance of cosine learning rate schedules while reducing computational requirements.
- Mechanism: The constant learning rate phase allows the model to explore the loss landscape effectively, while the cooldown phase enables a smooth transition to a better local minimum. This approach avoids the need to predetermine the total training duration, as checkpoints can be taken during the constant phase and cooled down later.
- Core assumption: The model's optimization dynamics during the constant phase are similar to those during the early part of the cosine schedule, and the cooldown phase effectively simulates the gradual decay of the cosine schedule.
- Evidence anchors:
  - [abstract] "We investigate the training behavior of a direct alternative — constant learning rate and cooldowns — and find that it scales predictably and reliably similar to cosine."
  - [section 3] "We observe an almost perfect match between the performance of the best cosine and cooldown schedule even for different training durations."
  - [corpus] Weak evidence - the corpus mentions related work on scaling laws but doesn't directly address the constant+cooldown mechanism.
- Break condition: If the cooldown phase is too short, the model may not converge to a good minimum, or if it's too long, it may waste computational resources without significant performance gains.

### Mechanism 2
- Claim: Stochastic weight averaging (SWA) can improve model performance along the training trajectory without additional training costs.
- Mechanism: SWA averages model weights over a window of training steps, which reduces the noise in the optimization process and leads to better generalization. This can simulate the effect of a decaying learning rate without explicitly implementing a cooldown phase.
- Core assumption: The averaged weights from SWA represent a better solution in the loss landscape than the final weights from a single training run.
- Evidence anchors:
  - [abstract] "Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales."
  - [section 4.1] "Using SWA for the constant LR phase (left) strongly boosts the loss, but a gap to the cooldown remains."
  - [corpus] Weak evidence - the corpus mentions SWA in the context of vision models but doesn't directly address its use in LLM training.
- Break condition: If the window size for SWA is not chosen appropriately, it may not capture the optimal weights or may introduce too much noise into the optimization process.

### Mechanism 3
- Claim: Scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs.
- Mechanism: By using a constant learning rate with cooldown or SWA, a single training run can be used to simulate multiple training durations, eliminating the need to train separate models from scratch for each duration. This allows scaling laws to be established with a fraction of the computational cost.
- Core assumption: The performance of the model during the constant phase or when using SWA is representative of its performance at different training durations.
- Evidence anchors:
  - [abstract] "Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs."
  - [section 5] "The reliable behavior of both the cooldown schedule and SWA allows scaling experiments with a drastic reduce in both compute and GPU hours."
  - [corpus] Weak evidence - the corpus mentions related work on scaling laws but doesn't directly address the computational savings of using constant+cooldown or SWA.
- Break condition: If the model's performance is highly sensitive to the training duration, using a single run with constant+cooldown or SWA may not accurately capture the scaling behavior across different durations.

## Foundational Learning

- Concept: Learning rate schedules and their impact on model optimization
  - Why needed here: Understanding how different learning rate schedules affect the optimization process is crucial for designing efficient training strategies.
  - Quick check question: How does the cosine learning rate schedule differ from a constant learning rate with cooldown in terms of its impact on model training?

- Concept: Stochastic weight averaging and its role in improving generalization
  - Why needed here: SWA is a key component of the proposed approach for reducing computational costs in scaling experiments.
  - Quick check question: How does averaging model weights over a window of training steps improve the model's generalization performance?

- Concept: Scaling laws and their importance in designing efficient training setups
  - Why needed here: The paper's main contribution is a method for establishing scaling laws with reduced computational costs.
  - Quick check question: What is the relationship between model size, training duration, and performance in the context of scaling laws?

## Architecture Onboarding

- Component map: Transformer -> AdamW optimizer -> Constant learning rate phase -> Cooldown phase OR SWA -> Performance evaluation
- Critical path: Train with constant learning rate -> Monitor validation performance -> Initiate cooldown phase (20% of total steps) -> Apply SWA during constant phase -> Evaluate scaling behavior
- Design tradeoffs: Longer constant phase enables better exploration but requires more resources; shorter cooldown may not allow sufficient convergence
- Failure signatures: Training instability (diverging loss/NaNs) indicates learning rate too high; insufficient cooldown shows plateaued validation perplexity; poor SWA performance suggests window size issues
- First 3 experiments:
  1. Train a small model with a constant learning rate and compare its performance to a model trained with a cosine schedule.
  2. Apply SWA to the constant learning rate model and evaluate its impact on performance.
  3. Use the constant learning rate with cooldown approach to establish scaling laws for a range of model sizes and compare the computational costs to traditional methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal cooldown length as a function of total training steps for achieving cosine-equivalent performance?
- Basis in paper: [explicit] The paper shows cooldown performance plateaus around 20% of total steps and suggests shorter cooldowns may suffice for longer training runs
- Why unresolved: The relationship between cooldown length and training duration appears non-linear, with different optimal fractions observed across experiments (5-20% range). The paper notes this requires further investigation.
- What evidence would resolve it: Systematic experiments varying cooldown lengths across multiple training durations (e.g., 20B, 100B, 500B tokens) to establish a precise functional relationship, potentially revealing whether the optimal fraction decreases with longer training.

### Open Question 2
- Question: Does the learning rate sensitivity pattern observed for cosine schedules (optimal LR increases with training length) apply equally to constant+cooldown schedules?
- Basis in paper: [explicit] The paper compares LR sensitivity between cosine and cooldown but doesn't investigate whether optimal LR values scale with training length for cooldown schedules
- Why unresolved: While the paper shows cooldown has slightly less LR sensitivity than cosine, it doesn't examine if the relationship between optimal LR and training duration follows the same pattern as cosine schedules
- What evidence would resolve it: Training cooldown schedules across multiple model sizes and training lengths while sweeping LR values to determine if optimal LR scales predictably with training duration.

### Open Question 3
- Question: What causes the performance uptick observed in some downstream benchmarks (MMLU, HellaSwag) when cooldown begins, and why doesn't this occur uniformly across all tasks?
- Basis in paper: [explicit] The paper observes "a similar boost in performance with the cooldown" for some benchmarks and notes "not all benchmarks follow this trend"
- Why unresolved: The authors posit this may relate to "too early saturation" but don't provide a mechanistic explanation for why certain tasks benefit while others don't
- What evidence would resolve it: Detailed analysis of task characteristics (e.g., memorization vs. reasoning, dataset size, difficulty) correlating with cooldown performance gains, potentially revealing whether the effect relates to specific learning dynamics during the cooldown phase.

## Limitations
- Generalization to very large models (>10B parameters) and non-language domains remains unverified
- Optimal cooldown duration relationship with training length requires systematic exploration
- SWA implementation details (window size sensitivity) need more thorough investigation

## Confidence
**High Confidence**: The empirical demonstration that constant learning rate with cooldown phase matches cosine schedule performance on the tested model scales and datasets.

**Medium Confidence**: The claim about significant computational savings (50% reduction in FLOPs and GPU hours) is well-supported for the tested range but requires validation across broader model scales and task domains.

**Low Confidence**: The generalizability of findings to very large models (>10B parameters), non-language domains, and different optimization landscapes.

## Next Checks
1. **Scale Extension Validation**: Replicate the constant+cooldown experiments with models in the 10B-100B parameter range to verify that the scaling behavior remains predictable and the computational savings persist at larger scales.

2. **Cross-Domain Testing**: Apply the constant+cooldown approach to vision transformers and multimodal models trained on ImageNet or similar datasets to assess whether the findings generalize beyond language modeling tasks.

3. **SWA Parameter Sensitivity**: Systematically vary the SWA window size (e.g., 100, 500, 1000 steps) and positioning (early, middle, late training) to determine optimal parameters and verify that the claimed "without additional training costs" benefit holds across different configurations.