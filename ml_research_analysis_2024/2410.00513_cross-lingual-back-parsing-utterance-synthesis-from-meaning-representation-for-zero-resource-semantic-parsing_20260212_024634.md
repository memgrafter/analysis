---
ver: rpa2
title: 'Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation
  for Zero-Resource Semantic Parsing'
arxiv_id: '2410.00513'
source_url: https://arxiv.org/abs/2410.00513
tags:
- language
- target
- data
- languages
- utterances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces Cross-Lingual Back-Parsing (CBP), a novel
  data augmentation methodology designed to enhance zero-shot cross-lingual transfer
  for semantic parsing. CBP synthesizes target language utterances from source meaning
  representations by leveraging the representation geometry of multilingual pretrained
  language models (mPLMs).
---

# Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing

## Quick Facts
- arXiv ID: 2410.00513
- Source URL: https://arxiv.org/abs/2410.00513
- Reference count: 28
- Introduces a novel data augmentation methodology for zero-shot cross-lingual transfer in semantic parsing

## Executive Summary
This paper proposes Cross-Lingual Back-Parsing (CBP), a method to synthesize target language utterances from source meaning representations without requiring parallel corpora. The approach leverages multilingual pretrained language models (mPLMs) and language-specific adapters to generate synthetic utterances in target languages. CBP employs a source-switched denoising objective and a filtering mechanism to ensure output quality. Experiments on Mschema2QA and Xspider benchmarks demonstrate consistent improvements in zero-shot cross-lingual transfer performance, with average exact match accuracy increases of 3.2% and 4.7% respectively.

## Method Summary
CBP addresses zero-resource semantic parsing by generating target language utterances from source meaning representations. The method uses language-specific adapters to control the output language and employs a source-switched denoising objective during training. A filtering mechanism discards low-quality synthetic utterances based on semantic and lexical criteria. The approach operates entirely without parallel corpora, making it suitable for truly zero-resource scenarios. The method is evaluated through extensive experiments on two cross-lingual semantic parsing benchmarks, demonstrating its effectiveness in improving zero-shot cross-lingual transfer.

## Key Results
- Average exact match accuracy improvement of 3.2% on Mschema2QA benchmark
- 4.7% improvement in exact match and 3.8% in test-suite accuracy on Xspider benchmark
- High slot value alignment rates while preserving semantic integrity

## Why This Works (Mechanism)
CBP leverages the geometric properties of multilingual embeddings in mPLMs to transfer semantic meaning across languages. By using language-specific adapters, the method can control which language the model generates while preserving the underlying semantic structure. The source-switched denoising objective helps the model learn to map between meaning representations and utterances in different languages. The filtering mechanism ensures that only high-quality synthetic utterances are retained, maintaining the effectiveness of the augmented training data.

## Foundational Learning
- Multilingual Pretrained Language Models (mPLMs): Why needed - provide shared semantic space across languages; Quick check - verify language coverage and performance on downstream tasks
- Semantic Parsing: Why needed - converts natural language to structured meaning representations; Quick check - evaluate parsing accuracy on standard benchmarks
- Data Augmentation: Why needed - expands training data without requiring additional annotations; Quick check - measure impact on model generalization
- Language Adapters: Why needed - allow fine-tuning for specific languages without retraining entire model; Quick check - test adapter effectiveness across language families
- Zero-shot Cross-lingual Transfer: Why needed - enables model performance in target languages without target language training data; Quick check - evaluate on truly zero-resource language pairs

## Architecture Onboarding

**Component Map**
mPLM -> Language Adapters -> Source-Switched Denoising -> Filtering Mechanism -> Synthetic Utterances

**Critical Path**
Meaning representation input → mPLM with language adapter → source-switched denoising generation → filtering → final synthetic utterance output

**Design Tradeoffs**
The method trades computational overhead for data augmentation benefits. Using language adapters provides flexibility but adds complexity compared to single-language models. The filtering mechanism ensures quality but may discard potentially useful examples, requiring careful threshold tuning.

**Failure Signatures**
- Poor multilingual embeddings lead to semantic drift across languages
- Inadequate language adapters fail to properly control output language
- Overly aggressive filtering removes too many synthetic examples
- Source-switched denoising fails to learn proper cross-lingual mappings

**First Experiments**
1. Test CBP on a single language pair before scaling to multiple languages
2. Evaluate the impact of different filtering thresholds on final performance
3. Compare CBP with and without language adapters to measure their contribution

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Effectiveness depends on quality of multilingual embeddings and language adapters
- Filtering mechanism relies on heuristics that may not generalize across all domains
- Improvements, while consistent, are modest in magnitude
- No clear assessment of robustness to low-resource languages

## Confidence
- Overall effectiveness: Medium (consistent improvements across two benchmarks, modest effect sizes)
- Language-specific adapter approach: Medium-High (demonstrated success in related multilingual transfer tasks)
- Filtering mechanism: Low-Medium (reliance on heuristic filtering without extensive validation)

## Next Checks
1. Conduct ablation studies isolating the contributions of language adapters, source-switched denoising, and filtering to establish component-level effectiveness
2. Test the method's robustness across a broader range of language families and resource levels, particularly low-resource languages underrepresented in mPLM pretraining
3. Evaluate semantic preservation using complementary metrics beyond exact match, such as semantic similarity scores and slot-value alignment precision-recall curves