---
ver: rpa2
title: Grounding Language Model with Chunking-Free In-Context Retrieval
arxiv_id: '2402.09760'
source_url: https://arxiv.org/abs/2402.09760
tags:
- text
- cfic
- evidence
- decoding
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Chunking-Free In-Context (CFIC) retrieval
  for RAG systems to address challenges in processing lengthy documents and filtering
  irrelevant content. CFIC uses auto-aggressive decoding on encoded hidden states
  of documents to identify precise evidence text, eliminating the need for chunking.
---

# Grounding Language Model with Chunking-Free In-Context Retrieval

## Quick Facts
- arXiv ID: 2402.09760
- Source URL: https://arxiv.org/abs/2402.09760
- Reference count: 8
- The paper introduces Chunking-Free In-Context (CFIC) retrieval for RAG systems to address challenges in processing lengthy documents and filtering irrelevant content.

## Executive Summary
This paper introduces Chunking-Free In-Context (CFIC) retrieval for RAG systems to address challenges in processing lengthy documents and filtering irrelevant content. CFIC uses auto-aggressive decoding on encoded hidden states of documents to identify precise evidence text, eliminating the need for chunking. Enhanced by Constrained Sentence Prefix Decoding and Skip Decoding strategies, CFIC improves retrieval efficiency and accuracy. Evaluated on open QA datasets, CFIC shows significant improvements in retrieving relevant and accurate evidence compared to traditional methods.

## Method Summary
CFIC eliminates the chunking step in traditional RAG by encoding entire documents into transformer hidden states and using auto-aggressive decoding to extract evidence spans in-context. The method employs Constrained Sentence Prefix Decoding to ensure generated text starts from actual sentence boundaries in the source, and Skip Decoding to efficiently jump to the most likely end of evidence spans. The approach is trained via Supervised Fine-Tuning on Llama2-7B-chat with self-constructed datasets containing query-evidence pairs.

## Key Results
- CFIC shows significant improvements in retrieving relevant and accurate evidence compared to traditional chunking-based methods on open QA datasets
- The approach eliminates chunking artifacts by directly decoding from document hidden states rather than pre-segmented chunks
- CFIC achieves better precision in evidence retrieval while maintaining computational efficiency through skip decoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CFIC eliminates chunking artifacts by directly decoding from document hidden states rather than pre-segmented chunks.
- Mechanism: Instead of splitting documents into passages that may cut semantic units, CFIC encodes the full document into transformer hidden states and uses auto-aggressive decoding to extract evidence spans in-context.
- Core assumption: The transformer hidden states preserve enough contextual and semantic information to enable accurate, localized decoding of evidence without prior chunking.
- Evidence anchors:
  - [abstract] CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking.
  - [section 3.1] CFIC involves encoding a document into transformer hidden states. When a user query is input, CFIC continues to encode the query alongside task instructions following the hidden states, subsequently generating grounding text.
  - [corpus] The neighboring papers focus on hierarchical or semantic chunking improvements, suggesting CFIC's chunking-free approach is novel and distinct from the chunking-focused literature.
- Break condition: If the document is too long relative to model capacity, the hidden state context window may truncate or distort semantic boundaries, causing incorrect evidence spans.

### Mechanism 2
- Claim: Constrained Sentence Prefix Decoding ensures generation starts from real sentence boundaries in the source document.
- Mechanism: Instead of sampling arbitrary tokens, the model samples only from the set of sentence prefixes in the document, guaranteeing that the generated evidence span is anchored to actual text in the source.
- Core assumption: Sentence prefixes in the source are sufficient to uniquely identify candidate evidence spans and maintain faithfulness to the source text.
- Evidence anchors:
  - [abstract] CFIC is further enhanced by incorporating two decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding.
  - [section 3.2] Constrained Sentence Prefix Decoding strategy which ensures the generated text prefixes originate from the input article.
  - [corpus] No explicit corpus support found; this is inferred from the paper's description of the decoding strategy.
- Break condition: If multiple sentences share the same prefix or if the prefix is too short to disambiguate, the model may sample ambiguous starting points, reducing precision.

### Mechanism 3
- Claim: Skip Decoding bypasses intermediate tokens and directly jumps to the end of the evidence span, improving efficiency.
- Mechanism: Once a sentence prefix is selected, the model computes the likelihood of the [eos] token after each sentence within a distance window and chooses the highest-likelihood end position, skipping token-by-token generation.
- Core assumption: The [eos] token likelihood peaks at the natural end of the relevant evidence span, making it a reliable stopping signal.
- Evidence anchors:
  - [abstract] Upon locating the appropriate sentence prefix, bypassing the decoding of intermediate tokens and directly selecting sentence ends with the highest likelihood of the [eos] token, thereby terminating the generation.
  - [section 3.2] Skip Decoding strategy which bypasses decoding the intermediate tokens while terminating generation at the position with the best likelihood of [eos] token.
  - [corpus] No explicit corpus support found; this is derived from the paper's technical description.
- Break condition: If the evidence span naturally ends before the [eos] token or if multiple sentences in the window have similar [eos] likelihood, the model may over-extend or cut off prematurely.

## Foundational Learning

- Concept: Auto-regressive decoding in transformer models.
  - Why needed here: CFIC relies on generating text tokens one-by-one conditioned on hidden states; understanding how transformers generate tokens is essential to grasp the constrained decoding strategy.
  - Quick check question: In a standard transformer decoder, what does the model condition the probability of the next token on?

- Concept: Sentence-level semantic coherence and prefix uniqueness.
  - Why needed here: The constrained prefix decoding assumes that sentence beginnings can uniquely anchor evidence spans; without understanding sentence structure, this assumption may fail.
  - Quick check question: Why might two different sentences in a document start with the same few words, and how could that affect prefix-based decoding?

- Concept: Negative log-likelihood (NLL) loss for supervised fine-tuning.
  - Why needed here: CFIC is trained via SFT using NLL; understanding this loss function is necessary to implement or debug the training pipeline.
  - Quick check question: In the NLL loss formula, why do we sum the log probabilities of the target tokens rather than their probabilities?

## Architecture Onboarding

- Component map:
  Input pipeline -> Encoder -> Constrained prefix sampler -> Skip decoder -> Output assembler -> Evaluation

- Critical path:
  1. Encode full document once, cache hidden states
  2. On query arrival, encode query + instructions + cached hidden states
  3. Run constrained prefix decoding to get k candidate starts
  4. For each start, run skip decoding to find end
  5. Post-process spans and return to downstream task

- Design tradeoffs:
  - Chunking vs. chunking-free: chunking is simpler but may cut semantics; CFIC is more accurate but requires full-document context and more computation.
  - Prefix constraint vs. open sampling: constraint improves faithfulness but may limit diversity; open sampling risks hallucination.
  - Skip decoding vs. full decoding: skip is faster but assumes [eos] likelihoods are reliable indicators of span ends.

- Failure signatures:
  - Low F1 on NarrativeQA suggests attention bias toward early document content; evidence may be incorrectly anchored to document start.
  - Performance drop when removing SFT indicates vanilla LLMs cannot locate precise evidence without task-specific fine-tuning.
  - Degraded performance when removing prefix constraint suggests generation drifts away from source text.

- First 3 experiments:
  1. Run CFIC on a short document with a clear, single evidence span; verify that the output exactly matches the ground truth.
  2. Remove the constrained prefix decoding and run the same input; observe whether the output still originates from the source or drifts.
  3. Set the skip decoding max length to 64, 128, and 256 tokens; measure the effect on output precision and recall.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CFIC vary when handling documents longer than the 32k token limit?
- Basis in paper: [explicit] The paper states that the maximum length CFIC can handle is set to 32k tokens due to computational resource constraints, but acknowledges that this may not be sufficient for longer documents like novels.
- Why unresolved: The paper does not provide empirical results or analysis of CFIC's performance on documents exceeding the 32k token limit.
- What evidence would resolve it: Testing CFIC on datasets with documents longer than 32k tokens and comparing its performance to other methods would provide evidence to resolve this question.

### Open Question 2
- Question: How effective is CFIC at mitigating biases in retrieved evidence compared to traditional chunking-based methods?
- Basis in paper: [inferred] The paper mentions that biases in the original training data of LLMs and biases in web-sourced documents can influence CFIC's outputs, but does not directly compare CFIC's bias mitigation effectiveness to chunking-based methods.
- Why unresolved: The paper does not provide a direct comparison of bias mitigation effectiveness between CFIC and traditional chunking-based methods.
- What evidence would resolve it: Conducting experiments to compare the bias levels in retrieved evidence from CFIC and chunking-based methods, using established bias metrics, would provide evidence to resolve this question.

### Open Question 3
- Question: How does the performance of CFIC vary with different values of the maximum decoding length parameter (d)?
- Basis in paper: [explicit] The paper mentions that the choice of the maximum decoding length (d) involves a balance between providing sufficient information for downstream tasks and introducing additional textual noise, and that a value of d=256 strikes an effective balance based on their experiments.
- Why unresolved: The paper does not provide a comprehensive analysis of how CFIC's performance varies with different values of the maximum decoding length parameter (d).
- What evidence would resolve it: Conducting experiments with different values of the maximum decoding length parameter (d) and analyzing the impact on CFIC's performance across various tasks would provide evidence to resolve this question.

## Limitations
- The paper lacks detailed specifications about the self-constructed SFT training dataset, including size, sources, and quality control measures
- Critical hyperparameters for decoding strategies (k for prefix count, d for skip window) are not explicitly specified
- The approach assumes documents can be encoded without truncation, but doesn't address performance degradation with very long documents

## Confidence

- **High Confidence**: The core mechanism of using transformer hidden states for in-context retrieval is well-supported by the abstract and methodology sections.
- **Medium Confidence**: The decoding strategies are described with sufficient detail to understand their purpose, but implementation specifics and hyperparameter choices remain unclear.
- **Low Confidence**: Claims about efficiency improvements are not quantified or directly compared to baseline chunking methods.

## Next Checks

1. **Training Data Validation**: Reproduce the self-constructed SFT dataset using ChatGPT with controlled prompts and evaluate how variations in dataset quality affect CFIC performance. Compare against using human-annotated evidence spans.

2. **Decoding Hyperparameter Sensitivity**: Systematically vary k (prefix count) and d (skip window) parameters across a range of values on a validation set to identify optimal configurations and understand sensitivity to these choices.

3. **Context Window Boundary Test**: Evaluate CFIC performance on documents at 25%, 50%, 75%, and 100% of model context window capacity to quantify performance degradation and identify practical document length limits.