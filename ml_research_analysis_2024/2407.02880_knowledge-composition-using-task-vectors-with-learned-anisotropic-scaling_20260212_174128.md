---
ver: rpa2
title: Knowledge Composition using Task Vectors with Learned Anisotropic Scaling
arxiv_id: '2407.02880'
source_url: https://arxiv.org/abs/2407.02880
tags:
- resblocks
- weight
- bias
- proj
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces aTLAS, a method for learning linear combinations
  of task vectors with learned anisotropic scaling to enhance knowledge composition
  and transfer in pre-trained models. By scaling parameter blocks independently, aTLAS
  exploits the low intrinsic dimensionality of neural networks, requiring only a few
  learnable coefficients.
---

# Knowledge Composition using Task Vectors with Learned Anisotropic Scaling

## Quick Facts
- arXiv ID: 2407.02880
- Source URL: https://arxiv.org/abs/2407.02880
- Reference count: 40
- Primary result: aTLAS improves test-time adaptation accuracy by 6.5% on 22 datasets through learned anisotropic scaling of task vectors

## Executive Summary
This paper introduces aTLAS, a method for learning linear combinations of task vectors with learned anisotropic scaling to enhance knowledge composition and transfer in pre-trained models. The approach scales parameter blocks independently, exploiting the low intrinsic dimensionality of neural networks while requiring only a few learnable coefficients. The method demonstrates strong performance across task arithmetic, few-shot recognition, test-time adaptation, and as a parameter-efficient fine-tuning approach, with notable improvements in few-shot learning and test-time adaptation scenarios.

## Method Summary
aTLAS learns linear combinations of task vectors (weight differences between fine-tuned and pre-trained models) with independent scaling coefficients for each parameter block. The method acquires task vectors by fine-tuning a pre-trained model on various datasets, then learns to combine these vectors through anisotropic scaling when adapting to new target datasets. By exploiting the low intrinsic dimensionality of neural networks, aTLAS requires only a small number of learnable parameters (the scaling coefficients) while achieving performance comparable to full fine-tuning. The approach can be applied to task arithmetic, few-shot recognition, test-time adaptation, and as a parameter-efficient fine-tuning method.

## Key Results
- Achieves 6.5% average accuracy improvement on 22 datasets for test-time adaptation
- Demonstrates strong performance in few-shot recognition, particularly with limited data
- Shows improved generalizability compared to existing methods across multiple evaluation scenarios
- Effective as a parameter-efficient fine-tuning method, especially when data is limited

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task vectors encode task-specific knowledge as directional and stride information relative to a pre-trained model, enabling knowledge composition via linear combinations.
- Mechanism: The difference between fine-tuned and pre-trained weights (θi − θ0) forms a task vector that represents the adjustment direction and magnitude needed for a model to specialize on a task. By linearly combining multiple task vectors with learned coefficients, the model can integrate knowledge from multiple tasks without re-training from scratch.
- Core assumption: Task vectors capture sufficient task-specific information in their direction and magnitude, and this information is linearly combinable.
- Evidence anchors:
  - [abstract]: "The learned weight difference relative to the pre-trained model, known as a task vector, characterises the direction and stride of fine-tuning."
  - [section 2]: "we follow previous practice [28, 44] and acquire task vectors by fine-tuning the image encoder, with the text representations frozen."
  - [corpus]: Weak - neighbor papers focus on task vectors but don't provide direct evidence for this specific mechanism.
- Break Condition: If task vectors lose information when represented as weight differences, or if non-linear interactions between tasks dominate, linear combinations will fail.

### Mechanism 2
- Claim: Anisotropic scaling of task vector components (parameter blocks) improves knowledge composition by allowing task-specific adaptation of different parameter groups independently.
- Mechanism: Each parameter block (weights, biases, etc.) in a neural network layer plays a distinct role in feature extraction. By learning independent scaling coefficients for each block, the method can precisely adjust how much each block contributes to the final model, leading to better disentanglement and less interference between tasks.
- Core assumption: Different parameter blocks encode different types of knowledge and benefit from independent scaling.
- Evidence anchors:
  - [abstract]: "weights, biases and any other forms of parameterisation, which we collectively refer to as parameter blocks, will be scaled independently."
  - [section 3.1]: "weights, biases and any other forms of parameterisation, which we collectively refer to as parameter blocks, will be scaled independently."
  - [section 4.1]: "we observe that weights matrices tend to have much larger negative coefficients" in task negation.
- Break Condition: If all parameter blocks contribute equally to task knowledge, or if uniform scaling performs as well as anisotropic scaling.

### Mechanism 3
- Claim: The method exploits the low intrinsic dimensionality of neural networks, requiring only a small number of learnable parameters (coefficients) while achieving performance comparable to full fine-tuning.
- Mechanism: Deep neural networks often find solutions in a low-dimensional subspace. By learning coefficients for task vector combinations rather than full weight matrices, the method searches this lower-dimensional space directly, reducing the number of learnable parameters while leveraging the rich knowledge already encoded in the task vectors.
- Core assumption: Neural network solutions reside in low-dimensional subspaces, and task vectors capture this structure.
- Evidence anchors:
  - [abstract]: "we show that such linear combinations explicitly exploit the low intrinsic dimensionality of pre-trained models, with only a few coefficients being the learnable parameters."
  - [section 3.2]: "We draw a parallel between Eqs. 6 and 8 and note that aTLAS explicitly exploits the low intrinsic dimensionality by learning a small set of coefficients."
  - [corpus]: Weak - neighbor papers discuss parameter efficiency but don't directly address low intrinsic dimensionality exploitation.
- Break Condition: If the intrinsic dimensionality is higher than assumed, or if task vectors don't capture the subspace structure effectively.

## Foundational Learning

- Concept: Task vectors and their arithmetic properties
  - Why needed here: The entire method builds on manipulating task vectors through arithmetic operations. Understanding how task vectors encode task knowledge and how they combine is fundamental to grasping the approach.
  - Quick check question: What is the mathematical definition of a task vector, and how is it obtained from a pre-trained model?

- Concept: Parameter blocks and their roles in neural networks
  - Why needed here: The method scales different parameter blocks independently, so understanding what parameter blocks are and their typical roles (weights vs biases, early vs late layers) is crucial.
  - Quick check question: How do parameter blocks differ in their typical functions across different layers of a neural network?

- Concept: Intrinsic dimensionality of neural networks
  - Why needed here: The method's parameter efficiency claim relies on the assumption that neural networks have low intrinsic dimensionality, which is why learning only coefficients works well.
  - Quick check question: What does "intrinsic dimensionality" mean in the context of neural networks, and why does it matter for parameter efficiency?

## Architecture Onboarding

- Component map:
  Pre-trained model (θ0) -> Task vectors (τi) -> Scaling coefficients (Λ) -> Parameter blocks -> Target dataset

- Critical path:
  1. Acquire task vectors by fine-tuning pre-trained model on various datasets
  2. For a target dataset, initialize scaling coefficients for task vector combinations
  3. Optimize coefficients to minimize loss on target dataset
  4. Deploy final model using optimized coefficients without additional computational cost

- Design tradeoffs:
  - Memory vs. Performance: Using more task vectors improves performance but increases memory usage
  - Sparsity vs. Expressiveness: Using LoRAs as task vectors reduces memory but may limit performance
  - Standard vs. Linearized task vectors: Standard vectors may perform better with learned scaling, while linearized vectors are more disentangled

- Failure signatures:
  - Poor performance on target dataset despite many task vectors
  - Coefficients collapsing to extreme values (all positive or all negative)
  - No improvement over zero-shot baseline
  - Memory errors when combining many large task vectors

- First 3 experiments:
  1. Task negation experiment: Learn coefficients to reduce performance on a target dataset while maintaining performance on ImageNet, using 8 classification datasets
  2. Few-shot recognition: Adapt CLIP to 22 datasets using 1-16 examples per class, comparing against Tip-Adapter and LP++
  3. Test-time adaptation: Apply to 22 datasets using self-supervised objectives (SimCLR, SAR, UFM) and compare against LayerNorm tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do task vectors behave when transferred across different neural network architectures?
- Basis in paper: [inferred] The paper notes that task vectors are defined relative to a specific pre-trained model, limiting cross-architecture transfer.
- Why unresolved: The paper does not explore methods for projecting task vectors across architectures or testing their transferability.
- What evidence would resolve it: Experiments showing successful or failed transfer of task vectors between different architectures, along with analysis of the required projection methods.

### Open Question 2
- Question: What is the optimal strategy for selecting task vectors under memory constraints?
- Basis in paper: [explicit] The paper discusses task vector selection strategies but does not determine an optimal approach under memory constraints.
- Why unresolved: The paper provides various selection strategies but does not compare their effectiveness comprehensively or identify an optimal method.
- What evidence would resolve it: Comparative analysis of selection strategies' performance and memory usage, identifying the most efficient approach.

### Open Question 3
- Question: How does the performance of aTLAS vary with different bit-width precision levels?
- Basis in paper: [inferred] The paper mentions the possibility of performing task vector composition at lower bit-widths but does not investigate it.
- Why unresolved: The paper does not conduct experiments to evaluate the impact of bit-width precision on aTLAS performance.
- What evidence would resolve it: Experimental results showing the performance of aTLAS at different bit-widths, including analysis of trade-offs between precision and efficiency.

## Limitations

- Performance heavily depends on the quality and diversity of task vectors, which may not generalize well to all domains
- The method's effectiveness on non-CLIP architectures remains unverified, limiting claims about general applicability
- Theoretical justification for independent scaling of parameter blocks is not fully developed

## Confidence

**High Confidence**: Experimental results demonstrating improved performance in few-shot recognition, test-time adaptation, and PEFT settings are well-supported by the provided evidence. The ablation studies showing the benefits of anisotropic scaling over uniform scaling are particularly convincing.

**Medium Confidence**: The theoretical explanation of why the method works (exploiting low intrinsic dimensionality) is plausible but not rigorously proven. The empirical evidence supports the claims but doesn't fully explain the underlying mechanisms.

**Low Confidence**: Claims about the method's applicability as a general PEFT approach beyond the tested scenarios, and assertions about performance improvements relative to all existing methods, are based on limited comparisons and may not hold in broader contexts.

## Next Checks

1. **Cross-Architecture Validation**: Test aTLAS on non-CLIP models (e.g., BERT for NLP tasks or ResNet for vision tasks) to verify whether the method generalizes beyond the specific architecture used in the paper.

2. **Task Vector Selection Analysis**: Conduct systematic experiments varying the number, diversity, and selection criteria of task vectors to determine how these factors impact performance and identify optimal strategies for task vector acquisition.

3. **Intrinsic Dimensionality Measurement**: Use established techniques (e.g., principal component analysis or information bottleneck methods) to quantitatively measure the intrinsic dimensionality of task vectors and verify whether it aligns with the paper's claims about low-dimensional structure.