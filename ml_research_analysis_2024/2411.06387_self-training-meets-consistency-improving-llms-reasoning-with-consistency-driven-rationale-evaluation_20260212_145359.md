---
ver: rpa2
title: 'Self-Training Meets Consistency: Improving LLMs'' Reasoning with Consistency-Driven
  Rationale Evaluation'
arxiv_id: '2411.06387'
source_url: https://arxiv.org/abs/2411.06387
tags:
- rationales
- rationale
- questions
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CREST introduces a self-training framework that evaluates LLM-generated
  rationales using follow-up questions to improve reasoning robustness. It filters
  rationales based on consistency across original and follow-up questions and employs
  preference learning to favor rationales that consistently lead to correct answers.
---

# Self-Training Meets Consistency: Improving LLMs' Reasoning with Consistency-Driven Rationale Evaluation

## Quick Facts
- arXiv ID: 2411.06387
- Source URL: https://arxiv.org/abs/2411.06387
- Reference count: 27
- Primary result: CREST improves reasoning accuracy on multiple-choice datasets using follow-up question consistency evaluation

## Executive Summary
CREST introduces a self-training framework that improves large language models' reasoning capabilities by evaluating generated rationales through follow-up questions. The method filters rationales based on consistency across original and follow-up questions, then employs preference learning to favor rationales that consistently lead to correct answers. Experiments with Llama 3 8B and Gemma 7B models show CREST achieves higher accuracy (up to 81.91% on ARC) compared to prior self-training approaches while generating more robust and correct rationales.

## Method Summary
CREST is a self-training framework that generates multiple rationales for each question using temperature sampling, then evaluates them using both original answer correctness (z) and follow-up question consistency (tilde-z). The method filters rationales based on follow-up question performance and applies preference learning to favor consistent rationales. The training process involves supervised fine-tuning with filtered rationales followed by Direct Preference Optimization using mixed preferences from original and follow-up question evaluations.

## Key Results
- Achieves up to 81.91% accuracy on ARC dataset, outperforming prior self-training approaches
- Generates more robust rationales that reduce reliance on dataset biases
- Improves logical robustness, correctness, and efficiency of reasoning steps
- Demonstrates effectiveness across ReClor, ARC, and CSQA datasets with multiple model sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evaluating rationales through follow-up questions captures reasoning consistency beyond final answer correctness.
- Mechanism: For each rationale, generate follow-up questions that test whether each answer option is correct. Use the number of correct follow-up answers as an additional reward signal (tilde-z) to filter or rank rationales.
- Core assumption: A rationale that consistently leads to correct answers across follow-up questions is more robust than one that only produces the correct final answer once.
- Evidence anchors: [abstract] "we propose CREST... that further evaluates each rationale through follow-up questions and leverages this evaluation to guide its training."

### Mechanism 2
- Claim: Filtering rationales with low consistency scores before supervised fine-tuning improves model robustness.
- Mechanism: Remove rationales where the number of correct follow-up answers falls below a tolerance threshold (F - t), where F is total follow-up questions and t is the tolerance term.
- Core assumption: Training on rationales that are robust across multiple related questions reduces the model's reliance on spurious correlations or dataset biases.
- Evidence anchors: [section 3.5] "DSFT={qi, rn i , ai| (n, i) ∈ {(n, i)|zn i = 1, ˜zn i ≥ F − t}}"

### Mechanism 3
- Claim: Preference learning using mixed preferences from original and follow-up question evaluations refines rationale selection.
- Mechanism: Construct preference pairs (rw, pw) ≻ (rl, pl) based on both z (original correctness) and tilde-z (follow-up consistency), then train with Direct Preference Optimization (DPO) to favor consistent rationales.
- Core assumption: Models learn better reasoning patterns when trained on relative preferences rather than absolute correctness labels alone.
- Evidence anchors: [section 3.6] "We construct the preference pair dataset Ptotal for preference learning by first creating two sets of preference pairs Pz and Ptilde-z..."

## Foundational Learning

- Concept: Consistency in model predictions across semantically equivalent contexts.
  - Why needed here: CREST's core innovation is measuring and enforcing consistency via follow-up questions; without understanding consistency, the rationale evaluation lacks theoretical grounding.
  - Quick check question: If a model answers "A" to question Q and "B" to a paraphrased version of Q, what consistency metric would flag this discrepancy?

- Concept: Supervised fine-tuning (SFT) with filtered datasets.
  - Why needed here: CREST uses SFT after rationale filtering; understanding how SFT works with smaller, higher-quality datasets is critical to replicating results.
  - Quick check question: How does reducing training data via filtering affect the variance of the fine-tuned model's predictions?

- Concept: Direct Preference Optimization (DPO) for ranking outputs.
  - Why needed here: CREST's final stage uses DPO to train on preference pairs derived from z and tilde-z; knowing how DPO balances positive and negative examples is essential for tuning lambda.
  - Quick check question: In DPO, what happens if the preference pairs are mostly (correct, incorrect) without considering consistency?

## Architecture Onboarding

- Component map: Rationale Generation -> Rationale Evaluation -> Supervised Fine-Tuning -> Preference Learning -> Final Model
- Critical path:
  1. Generate diverse rationales with temperature sampling (N=16)
  2. Compute z (original correctness) and tilde-z (follow-up consistency)
  3. Filter rationales with tilde-z ≥ F - t for SFT
  4. Build preference pairs from z and tilde-z
  5. Apply DPO to train final model
- Design tradeoffs:
  - More follow-up questions (higher F) → better consistency signal but higher computational cost
  - Lower tolerance t → higher quality rationales but fewer training examples
  - Higher lambda (more Ptilde-z in preference pairs) → more robustness, less bias exploitation
- Failure signatures:
  - If tilde-z correlates poorly with z, filtering removes useful rationales
  - If lambda is too high, model may overfit to consistency at expense of accuracy
  - If follow-up questions are too similar to original, consistency gain is illusory
- First 3 experiments:
  1. Run rationale generation with N=16, temperature=0.8; check diversity and initial z scores.
  2. Generate F=4 follow-up questions per rationale; compute tilde-z distribution for z=0 vs z=1 rationales.
  3. Train MSFT with t=2; evaluate on validation set; compare against RFT baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the CREST framework perform when extended to non-multiple-choice tasks such as open-ended mathematical reasoning or coding tasks?
- Basis in paper: [explicit] The paper acknowledges that CREST is conceptually task-agnostic but notes that extending it to other task types like math questions or open-ended questions would require adaptations.
- Why unresolved: The current evaluation is limited to multiple-choice question-answering datasets (ReClor, ARC, CSQA), leaving the performance on other reasoning tasks unexplored.
- What evidence would resolve it: Experiments applying CREST to open-ended reasoning tasks with generated or human-annotated rationales, comparing performance against baselines.

### Open Question 2
- Question: Does the follow-up question generation strategy in CREST capture the full spectrum of reasoning errors, or are there critical flaws that remain undetected?
- Basis in paper: [explicit] The authors note that follow-up questions are designed to ask whether each answer option is correct, but they acknowledge that this approach may not fully capture the quality differences between rationales that lead to correct answers.
- Why unresolved: The paper suggests that some rationales may lead to the correct answer but fail to provide comprehensive reasoning, which the current evaluation might not fully distinguish.
- What evidence would resolve it: Analysis of follow-up question effectiveness in detecting subtle reasoning flaws, possibly by incorporating human evaluation or alternative follow-up question designs.

### Open Question 3
- Question: How does the tolerance parameter t in supervised fine-tuning affect the trade-off between training data quantity and rationale quality, and what is the optimal strategy for setting t?
- Basis in paper: [explicit] The paper shows that increasing t initially improves performance but can degrade it if set too high, indicating a trade-off between including more rationales and maintaining quality.
- Why unresolved: The optimal value of t varies across datasets, and the paper does not provide a principled method for determining it.
- What evidence would resolve it: Systematic analysis of t's impact across diverse datasets, possibly using validation performance to guide hyperparameter selection.

## Limitations

- The effectiveness of tilde-z rewards depends heavily on the quality of follow-up questions, which are generated by the same models used for rationale generation, creating potential feedback loops.
- The filtering mechanism (tolerance t) represents a critical hyperparameter that trades training data quantity for quality, but the paper provides limited analysis of how different tolerance settings affect model performance across datasets.
- The method's generalizability to non-multiple-choice tasks and other reasoning domains remains unexplored, limiting confidence in its broader applicability.

## Confidence

- **High confidence**: The overall self-training framework architecture and the core claim that filtering rationales improves reasoning robustness are well-supported by the experimental results.
- **Medium confidence**: The specific mechanism of using follow-up question consistency (tilde-z) to evaluate rationales shows promising results but requires more extensive ablation studies to confirm its necessity versus simpler filtering methods.
- **Medium confidence**: The preference learning approach with mixed preferences (z and tilde-z) demonstrates improvements, though the exact contribution of each preference type warrants further investigation.

## Next Checks

1. **Follow-up question quality analysis**: Systematically evaluate whether follow-up questions generated by the model are actually testing the reasoning steps or merely repeating the original question in different words. Compare performance when using human-written follow-up questions versus model-generated ones.

2. **Tolerance parameter sensitivity**: Conduct experiments varying the tolerance t parameter across a wider range (e.g., t ∈ {0, 1, 2, 3, 4}) on each dataset to quantify the tradeoff between training data size and reasoning performance. Identify if there's an optimal t that maximizes the robustness-accuracy balance.

3. **Cross-dataset generalization test**: Apply the trained CREST models to out-of-distribution reasoning tasks or different domains (e.g., commonsense reasoning, mathematical word problems) to assess whether the consistency training generalizes beyond the three evaluated datasets or primarily captures dataset-specific patterns.