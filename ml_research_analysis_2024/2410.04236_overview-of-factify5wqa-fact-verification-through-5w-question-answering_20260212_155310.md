---
ver: rpa2
title: 'Overview of Factify5WQA: Fact Verification through 5W Question-Answering'
arxiv_id: '2410.04236'
source_url: https://arxiv.org/abs/2410.04236
tags:
- fact
- claim
- task
- news
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Factify5WQA shared task addressed the challenge of automated
  fact verification by introducing a novel dataset that combines claims, evidence
  documents, and 5W question-answer pairs to enable aspect-based fact checking. The
  task aimed to leverage the generative capabilities of large language models by using
  5W questions to compare claims and evidence documents, with performance measured
  using BLEU scores for answer comparison followed by classification accuracy.
---

# Overview of Factify5WQA: Fact Verification through 5W Question-Answering

## Quick Facts
- arXiv ID: 2410.04236
- Source URL: https://arxiv.org/abs/2410.04236
- Reference count: 33
- Best team achieved 69.56% accuracy, a 35% improvement over baseline

## Executive Summary
The Factify5WQA shared task introduced a novel approach to automated fact verification by combining claims, evidence documents, and 5W question-answer pairs to enable aspect-based fact checking. The task leveraged large language models' generative capabilities to compare claims and evidence through 5W questions, with performance measured using BLEU scores for answer comparison followed by classification accuracy. Three teams participated, with the best-performing team achieving 69.56% accuracy using a custom architecture that combined instruction-tuned LLM generation with co-attention mechanisms.

## Method Summary
The Factify5WQA dataset combines claims, evidence documents, and 5W questions (Who, What, When, Where, Why) with corresponding answers. The evaluation pipeline generates answers to 5W questions for both claims and evidence using LLMs, computes BLEU scores between corresponding answer pairs, averages scores across all questions, and applies a 0.3 threshold to classify relationships as Support, Neutral, or Refute. The baseline approach used a fine-tuned Flan model for answer generation and Mini-lm for embeddings, achieving 34.22% accuracy.

## Key Results
- Team Trifecta achieved 69.56% accuracy using Pre-CoFactv3 architecture, a 35% improvement over baseline
- All teams struggled with Support category (27-66% accuracy) while performing better on Neutral and Refute categories
- Second and third teams achieved 45.51% and 45.46% accuracy respectively
- BLEU score threshold of 0.3 used for classification, though performance sensitivity to this threshold was not analyzed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 5W questions improve fact verification by creating structured comparison points between claims and evidence
- Mechanism: Extracting Who, What, When, Where, and Why elements from both sources generates answer pairs that can be directly compared using BLEU score to determine semantic alignment
- Core assumption: 5W questions capture sufficient semantic content to determine claim-evidence relationships
- Evidence anchors: Abstract mentions 5W questions help compare information sources; section notes integration of 5W questions with fact-checking datasets

### Mechanism 2
- Claim: Instruction-tuned LLMs with in-context learning improve accuracy over baseline approaches
- Mechanism: Team Trifecta's Pre-CoFactv3 combines fine-tuned LLM generation with co-attention mechanisms for more accurate answer comparisons
- Core assumption: Generative capabilities of LLMs can effectively extract and compare relevant information
- Evidence anchors: Abstract reports 35% improvement over baseline; section details Team Trifecta's 35% improvement

### Mechanism 3
- Claim: BLEU score thresholding provides effective classification of claim-evidence relationships
- Mechanism: Average BLEU scores between claim and evidence answers for each 5W question are compared to threshold (0.3) to determine Support/Neutral/Refute labels
- Core assumption: Semantic similarity as measured by BLEU score correlates with true relationship
- Evidence anchors: Section describes BLEU score evaluation approach and 0.3 threshold usage

## Foundational Learning

- Concept: Question-answer generation using LLMs
  - Why needed here: System relies on LLMs to generate answers to 5W questions based on claim and evidence separately
  - Quick check question: What prompt format ensures LLMs generate concise, relevant answers to 5W questions?

- Concept: Semantic similarity metrics
  - Why needed here: BLEU score comparison of generated answers forms core classification mechanism
  - Quick check question: How does BLEU score handle semantically equivalent answers with different wording?

- Concept: Fact verification task formulation
  - Why needed here: Understanding Support/Neutral/Refute classifications and their relation to claim-evidence relationships
  - Quick check question: What distinguishes Neutral from Support or Refute in claim-evidence pairs?

## Architecture Onboarding

- Component map: Input (Claim text, evidence text, 5W questions) -> Generator (Flan or fine-tuned LLM) -> Embedding (Mini-lm or transformer) -> Comparison (BLEU score) -> Classifier (SVM or custom) -> Output (Support/Neutral/Refute label)

- Critical path: 1. Generate answers to 5W questions for claim and evidence 2. Compute BLEU scores between corresponding answer pairs 3. Average BLEU scores across all 5 questions 4. Apply threshold (0.3) and classify based on label prediction match

- Design tradeoffs: Generator choice (Flan vs fine-tuned LLM), embedding method (cosine similarity vs direct embeddings), threshold selection (balance precision and recall)

- Failure signatures: Low Support classification accuracy, high BLEU scores but incorrect classification, large performance gap between teams

- First 3 experiments: 1. Replicate baseline pipeline with Flan + Mini-lm + SVM 2. Test different classifier types (logistic regression, KNN, ridge) 3. Experiment with different BLEU score thresholds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What factors contribute to difficulty classifying Support category samples, given all teams performed significantly worse on this category?
- Basis in paper: Explicit note that all teams struggled with Support category (27-66% accuracy)
- Why unresolved: Paper identifies performance gap but doesn't analyze underlying reasons
- What evidence would resolve it: Comparative analysis of Support vs Neutral/Refute samples showing differences in complexity or linguistic patterns

### Open Question 2
- Question: How does 5W-based fact verification performance compare to traditional entailment-based approaches?
- Basis in paper: Inferred - paper introduces 5W-based approach without benchmarking against traditional entailment methods
- Why unresolved: Focus on 5W-based approach without direct comparison to entailment methods
- What evidence would resolve it: Direct performance comparison of 5W-based vs entailment-based approaches on same dataset

### Open Question 3
- Question: What is the impact of answer quality on classification accuracy, and how can this relationship be optimized?
- Basis in paper: Explicit mention that Team Trifecta got 15% answers incorrect vs 33% for other teams
- Why unresolved: Paper mentions answer quality differences but doesn't explore improving answer generation
- What evidence would resolve it: Correlation analysis between answer quality and classification accuracy

## Limitations
- BLEU score methodology may not capture semantic nuances critical for fact verification, particularly negation handling
- Dataset construction process lacks sufficient detail for complete replication, including criteria for claim selection and evidence pairing
- All teams struggled significantly with Support category classification, indicating fundamental challenges in this approach

## Confidence
- High Confidence: Task formulation, dataset creation, performance metrics, and baseline pipeline architecture are clearly documented
- Medium Confidence: 5W question-answering approach effectiveness is supported by results but underlying mechanism remains theoretical
- Low Confidence: Generalizability beyond Factify5WQA dataset and robustness to different domains and claim types are uncertain

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically vary BLEU score threshold (0.2, 0.3, 0.4, 0.5) and measure classification accuracy for each class separately
2. **Alternative Similarity Metrics**: Replace BLEU score with BERTScore or Sentence-BERT cosine similarity to evaluate optimal correlation with true relationships
3. **Error Analysis by Category**: Conduct detailed error analysis focusing on Support category (27-66% accuracy) to identify specific failure patterns