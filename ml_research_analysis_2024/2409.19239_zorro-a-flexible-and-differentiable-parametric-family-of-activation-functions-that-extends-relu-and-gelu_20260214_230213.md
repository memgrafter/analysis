---
ver: rpa2
title: 'Zorro: A Flexible and Differentiable Parametric Family of Activation Functions
  That Extends ReLU and GELU'
arxiv_id: '2409.19239'
source_url: https://arxiv.org/abs/2409.19239
tags:
- function
- functions
- activation
- zorro
- relu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel family of activation functions called
  Zorro, which extends ReLU and GELU. Zorro is designed to address the limitations
  of existing activation functions, such as the non-differentiable points and exploding
  gradient issues in ReLU, and the need for more parameters to adapt to datasets and
  architectures in GELU and Swish variants.
---

# Zorro: A Flexible and Differentiable Parametric Family of Activation Functions That Extends ReLU and GELU

## Quick Facts
- arXiv ID: 2409.19239
- Source URL: https://arxiv.org/abs/2409.19239
- Reference count: 40
- Key outcome: Zorro functions improve performance on multiple datasets and can approximate ReLU, GELU, and DGELU with appropriate parameter settings.

## Executive Summary
This paper introduces Zorro, a novel family of activation functions that extends ReLU and GELU by combining them with Sigmoid components to create continuously differentiable functions. The Zorro family consists of five variants that can approximate various existing activation functions while offering improved training stability and flexibility. The authors demonstrate Zorro's effectiveness across multiple architectures (fully connected, CNN, transformer) and datasets (MNIST, CIFAR-10, EMNIST variants), showing performance improvements over traditional activation functions. The work addresses limitations of existing activation functions like ReLU's non-differentiability and GELU's need for more parameters, providing a unified framework for activation function selection.

## Method Summary
The Zorro family combines ReLU's linear behavior in the [0,1] range with Sigmoid smoothness to create differentiable activation functions. The five variants (Symmetric, Asymmetric, Sigmoid-Zorro, Tanh-Zorro, Sloped-Zorro) use parameters a, b, k (and additional parameters m, ai, as for specific variants) to control shape and position. The functions are designed to approximate other activation functions by adjusting these parameters - for example, setting a=50 approximates ReLU, while specific combinations can mimic GELU or Swish behavior. The paper evaluates Zorro across three architecture types with fixed hyperparameters and grid search for parameter tuning in feedforward networks.

## Key Results
- Zorro variants show improved classification accuracy on MNIST (99.34%), Fashion MNIST (94.43%), CIFAR-10 (81.71%), Letters EMNIST (76.26%), and Balanced EMNIST (77.44%) compared to traditional activation functions
- Sigmoid-Zorro and Tanh-Zorro variants can effectively replace ReLU, GELU, and DGELU in certain architectures
- The Symmetric-Zorro variant with parameters a=2.0, b=0.5 provides a good starting point for parametric searches across architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zorro's combination of ReLU's linear central part with Sigmoid's smoothness enables better training stability compared to ReLU or pure Sigmoid.
- Mechanism: The function preserves the linear behavior in [0,1] like ReLU but adds differentiability across all inputs through the Sigmoid component, preventing exploding gradients beyond the linear range.
- Core assumption: The central linear part in [0,1] is the critical region for most normalized data distributions.
- Evidence anchors:
  - [abstract]: "Zorro functions are smooth and adaptable, and serve as information gates, aligning with ReLU in the 0-1 range, offering an alternative to ReLU without the need for normalization, neuron death, or gradient explosions."
  - [section 4]: "We combine it with the Sigmoid function in a shape similar to the DGELU function but preserving the linear part of the ReLU in [0,1]."
  - [corpus]: Weak evidence - no direct citations to Zorro but related works on parametric ReLU variants exist (e.g., "ReCA: A Parametric ReLU Composite Activation Function").
- Break condition: If data distributions extend significantly beyond the [0,1] range without normalization, the Sigmoid components may dominate and reduce performance.

### Mechanism 2
- Claim: Zorro's parametric flexibility allows it to approximate multiple activation functions (ReLU, GELU, Swish, DGELU) with appropriate parameter settings.
- Mechanism: The five variants (Symmetric, Asymmetric, Sigmoid-Zorro, Tanh-Zorro, Sloped-Zorro) provide different parameterizations that can be tuned to match the shape and properties of other functions.
- Core assumption: Different neural network architectures benefit from different activation function properties (boundedness, mean activation, slope characteristics).
- Evidence anchors:
  - [abstract]: "Zorro also approximates functions like Swish, GELU, and DGELU, providing parameters to adjust to different datasets and architectures."
  - [section 4.2]: Detailed parameter settings showing how Zorro can approximate ReLU (ai=50), SiLU, GELU, DSiLI, and DGELU with specific parameter combinations.
  - [corpus]: Weak evidence - no direct citations to Zorro but related works on adaptive activation functions exist (e.g., "Nonlinearity Enhanced Adaptive Activation Functions").
- Break condition: If parameter tuning is not feasible or optimal parameters are architecture-specific, the approximation benefits may not materialize.

### Mechanism 3
- Claim: Zorro variants address specific architectural needs through targeted design modifications (e.g., Sigmoid-Zorro for gating functions, Tanh-Zorro for zero-centered activations).
- Mechanism: Each variant is engineered to match specific activation function characteristics needed in different neural network components (gates, hidden layers, output layers).
- Core assumption: Different network components require activation functions with different statistical properties (mean, range, symmetry).
- Evidence anchors:
  - [section 4.1.2]: "The Sigmoid-Zorro function arises by applying a shift and rescaling... allowing its use in the mentioned architectures [LSTM, Variational Autoencoders]."
  - [section 4.1.3]: "The Tanh-Zorro function to mimic the Tanh behavior... has a mean activation closer to zero, so... it will produce less bias in subsequent layers."
  - [section 6]: Empirical results showing Sigmoid-Zorro performs poorly on Letters/EMNIST while Tanh-Zorro performs better, validating architecture-specific needs.
  - [corpus]: Weak evidence - no direct citations to Zorro but related works on architecture-specific activations exist (e.g., "SmartMixed: A Two-Phase Training Strategy for Adaptive Activation Function Learning").
- Break condition: If the target architecture does not benefit from the specific properties of the chosen variant, performance may degrade compared to simpler alternatives.

## Foundational Learning

- Concept: Activation function properties (differentiability, boundedness, monotonicity)
  - Why needed here: Understanding these properties is essential to grasp why Zorro's design choices matter and how they compare to existing functions.
  - Quick check question: What is the key difference between ReLU and GELU in terms of differentiability, and why does this matter for gradient-based training?

- Concept: Vanishing and exploding gradient problems
  - Why needed here: The paper explicitly addresses these problems as motivation for Zorro's design, particularly the exploding gradient issue in ReLU.
  - Quick check question: How does Zorro's boundedness property help prevent exploding gradients while maintaining ReLU-like behavior in the critical [0,1] range?

- Concept: Parameter tuning and architecture-specific activation function selection
  - Why needed here: The paper demonstrates that different Zorro variants perform best on different datasets and architectures, requiring understanding of when to use each variant.
  - Quick check question: Based on the experimental results, which Zorro variant would you choose for a CNN on MNIST and why?

## Architecture Onboarding

- Component map: ReLU + Sigmoid -> Zorro family (Symmetric, Asymmetric, Sigmoid-Zorro, Tanh-Zorro, Sloped-Zorro) -> Neural network layers
- Critical path: 1. Choose appropriate Zorro variant based on architecture needs 2. Set initial parameters using recommended values from Table 4 3. Validate performance on validation set 4. Fine-tune parameters if needed for specific dataset
- Design tradeoffs:
  - Complexity vs. flexibility: More parameters provide better adaptation but increase tuning complexity
  - Computational cost: Zorro is slightly more complex than ReLU but comparable to GELU
  - Approximation accuracy: Trade-off between how closely Zorro matches target functions vs. maintaining desirable properties
- Failure signatures:
  - Poor training performance: May indicate incorrect variant selection or parameter settings
  - Gradient instability: Could suggest parameters are set outside valid ranges (e.g., a too large)
  - Slow convergence: Might indicate the linear part doesn't align well with data distribution
- First 3 experiments:
  1. Replace ReLU with Symmetric-Zorro (a=2.0, b=0.5) in a simple CNN on MNIST to verify basic functionality
  2. Compare Sloped-Zorro (m=1.3, a=2, b=0.3) vs. GELU on CIFAR-10 to test approximation quality
  3. Test Sigmoid-Zorro on LSTM architecture to verify gating function compatibility

## Open Questions the Paper Calls Out
The paper acknowledges the need for further research on parameter adjustment heuristics for diverse architectures and datasets. While it provides optimal parameter values for specific architectures and datasets, it does not offer a comprehensive framework for determining the best parameters across different scenarios. Additionally, the paper suggests exploring the theoretical properties of Zorro functions, such as their impact on the optimization landscape of neural networks, which remains an open area for investigation.

## Limitations
- The paper lacks theoretical analysis explaining why Zorro performs well across diverse architectures, relying primarily on empirical validation
- Parameter tuning process requires manual selection from recommended values rather than automated methods
- Computational overhead of Zorro compared to simpler functions like ReLU is not thoroughly analyzed

## Confidence
- **High Confidence**: Zorro's ability to approximate ReLU, GELU, and DGELU functions (supported by detailed parameter settings and equations)
- **Medium Confidence**: Zorro's superior performance on the tested datasets and architectures (empirical results show improvement but lack theoretical backing)
- **Medium Confidence**: The claim that Zorro variants address specific architectural needs (supported by some experimental evidence but limited architectural diversity)

## Next Checks
1. Conduct mathematical analysis of Zorro's gradient properties and convergence behavior to complement empirical results and provide theoretical guarantees
2. Develop an automated parameter selection method (e.g., meta-learning or Bayesian optimization) to reduce manual tuning burden and test generalization across similar architectures
3. Evaluate Zorro's computational overhead and performance on large-scale datasets and architectures (e.g., ImageNet, BERT-scale models) to assess practical deployment viability