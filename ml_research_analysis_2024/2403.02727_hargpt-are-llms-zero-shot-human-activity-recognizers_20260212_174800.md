---
ver: rpa2
title: 'HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?'
arxiv_id: '2403.02727'
source_url: https://arxiv.org/abs/2403.02727
tags:
- llms
- data
- unseen
- gpt4
- hargpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HARGPT, a study investigating whether large
  language models (LLMs) can perform zero-shot human activity recognition (HAR) from
  raw IMU sensor data without fine-tuning or domain-specific prompt engineering. The
  method inputs raw IMU data into LLMs using a simple prompt with role-play and chain-of-thought
  strategies.
---

# HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?

## Quick Facts
- arXiv ID: 2403.02727
- Source URL: https://arxiv.org/abs/2403.02727
- Authors: Sijie Ji; Xinzhe Zheng; Chenshu Wu
- Reference count: 22
- Primary result: GPT-4 achieves average F1-scores of 0.795 and 0.790 on Capture24 and HHAR datasets respectively using zero-shot HAR

## Executive Summary
This paper investigates whether large language models can perform zero-shot human activity recognition from raw IMU sensor data without fine-tuning or domain-specific prompt engineering. The HARGPT method inputs raw IMU data into LLMs using simple prompts with role-play and chain-of-thought strategies. Experiments on two public datasets show that GPT-4 with chain-of-thought prompting outperforms traditional ML and deep learning baselines, achieving average F1-scores of 0.795 and 0.790 respectively. Notably, LLMs demonstrate robustness on unseen data without requiring retraining, suggesting their potential as foundational models for interpreting physical world sensor data.

## Method Summary
The method feeds raw IMU sensor data (accelerations and gyroscopes) into LLMs using a simple prompt template that includes role-play instructions ("You are an expert of IMU-based human activity analysis") and chain-of-thought guidance ("Please make an analysis step by step"). No fine-tuning or domain-specific prompt engineering is performed. The IMU data is down-sampled to 10Hz for LLM input. The approach is tested on two public datasets (Capture24 and HHAR) with user-stratified train/test splits, comparing GPT-4 performance against traditional ML baselines and other LLMs like GPT-3.5, Gemini, and Llama2-70b.

## Key Results
- GPT-4 with chain-of-thought prompting achieves average F1-scores of 0.795 on Capture24 and 0.790 on HHAR datasets
- LLMs outperform traditional ML and deep learning baselines in zero-shot HAR tasks
- LLMs demonstrate superior generalization to unseen users compared to traditional models (DCNN baseline achieves only ~0.6 F1-score on unseen data)
- Zero-shot performance is achieved without fine-tuning or domain-specific prompt engineering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can interpret raw IMU sensor data through pattern abstraction without domain-specific training
- Mechanism: LLMs process raw numerical sequences as tokens, then use their trained knowledge base to generate abstract linguistic representations (e.g., "periodic", "stationary", "abrupt") that map to human activities
- Core assumption: The LLM's pre-trained knowledge contains sufficient domain understanding of physical movement patterns to bridge raw sensor data and activity labels
- Evidence anchors:
  - [abstract] "LLMs can comprehend raw IMU data and perform HAR tasks in a zero-shot manner, with only appropriate prompts."
  - [section] "Existing LLMs such as GPT4 are powerful adapters with the capability to convert unprocessed, raw digitized sensor data into abstract linguistic representations, such as periodic, stationary, abrupt, etc."
  - [corpus] Weak evidence - related papers focus on multimodal approaches but don't directly address zero-shot IMU interpretation
- Break condition: If the activity patterns are too subtle or the LLM lacks relevant pre-training knowledge about physical motion characteristics

### Mechanism 2
- Claim: Chain-of-thought prompting enhances LLM accuracy for activity recognition tasks
- Mechanism: The "think step-by-step" instruction triggers the LLM's reasoning process, breaking down analysis into logical steps that combine sensor data interpretation with domain knowledge retrieval
- Core assumption: The LLM's reasoning capability extends beyond text to structured data analysis when properly prompted
- Evidence anchors:
  - [abstract] "utilizes the role-play and 'think step-by-step' strategies for prompting."
  - [section] "we aim to elicit a detailed chain-of-thought (CoT) process from LLMs, as this approach has been proven to enhance the accuracy of their answers in existing literature."
  - [corpus] No direct evidence in corpus about CoT effectiveness for IMU data specifically
- Break condition: If the LLM generates superficial analysis or bypasses detailed reasoning despite the prompt

### Mechanism 3
- Claim: LLMs demonstrate superior generalization to unseen users compared to traditional ML models
- Mechanism: Unlike traditional models that overfit to training user data, LLMs use abstract reasoning about movement patterns that transfers across users
- Core assumption: The LLM's knowledge base contains generalizable principles about human movement that aren't user-specific
- Evidence anchors:
  - [abstract] "LLMs exhibit a high degree of robustness... without requiring retraining or fine-tuning for specific datasets."
  - [section] "LLMs exhibit a high degree of robustness. In contrast, even the best baseline DCNN achieves only a modest level of approximately 0.6" on unseen data.
  - [corpus] No corpus evidence directly comparing LLM generalization to traditional models for HAR
- Break condition: If the activity patterns vary significantly across users in ways not captured by the LLM's pre-training

## Foundational Learning

- Concept: Sensor data preprocessing and feature representation
  - Why needed here: Understanding how raw IMU data is formatted and sampled is crucial for interpreting the experimental setup and results
  - Quick check question: What is the difference between raw IMU data and features like FFT or PCA, and why might LLMs process them differently?

- Concept: Chain-of-thought prompting methodology
  - Why needed here: The success of HARGPT relies on specific prompting strategies that guide LLM reasoning
  - Quick check question: How does chain-of-thought prompting differ from standard prompting, and what are its key benefits for complex reasoning tasks?

- Concept: Zero-shot learning fundamentals
  - Why needed here: The paper's core contribution is demonstrating zero-shot performance, which requires understanding how models can perform tasks without task-specific training
  - Quick check question: What distinguishes zero-shot learning from few-shot learning, and what are the theoretical limitations of each approach?

## Architecture Onboarding

- Component map: Raw IMU data -> Downsampling (10Hz) -> Tokenization -> Prompt template filling -> LLM inference -> Response parsing -> Activity classification
- Critical path:
  1. Raw IMU data acquisition and downsampling to 10Hz
  2. Tokenization and prompt template filling
  3. LLM inference with CoT prompting
  4. Response parsing and activity classification
  5. Performance evaluation against baselines
- Design tradeoffs:
  - Tokenization granularity vs. prompt length limits
  - Prompt complexity vs. inference cost and latency
  - Dataset partitioning strategy vs. generalization assessment
  - Choice of LLM vs. reasoning capability and cost
- Failure signatures:
  - Inconsistent predictions across similar input patterns
  - Over-reliance on specific user patterns (poor generalization)
  - Excessive inference time or token usage
  - Failure to engage in CoT reasoning despite prompts
- First 3 experiments:
  1. Baseline comparison: Run HARGPT vs. traditional ML models on seen data to verify relative performance
  2. Generalization test: Evaluate HARGPT on unseen user data to assess robustness claims
  3. Prompt ablation: Compare CoT vs. direct output prompting to quantify reasoning benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs reliably handle noisy or corrupted IMU sensor data without performance degradation?
- Basis in paper: [inferred] The paper demonstrates LLMs' robustness on unseen data but doesn't explicitly test performance on noisy or corrupted data
- Why unresolved: The study focuses on clean datasets (Capture24 and HHAR) and doesn't investigate LLMs' resilience to sensor noise, missing values, or data corruption common in real-world CPS deployments
- What evidence would resolve it: Experiments testing LLMs on intentionally corrupted datasets with varying noise levels, missing data patterns, and sensor failure scenarios would determine their robustness limits

### Open Question 2
- Question: What is the minimum sampling rate at which LLMs can maintain accurate HAR performance?
- Basis in paper: [explicit] The paper down-samples IMU data to 10Hz for LLMs but doesn't explore the relationship between sampling rate and accuracy
- Why unresolved: The study uses a fixed 10Hz sampling rate without investigating how LLMs' performance changes at lower sampling rates or whether they can effectively handle the temporal resolution trade-offs inherent in resource-constrained CPS
- What evidence would resolve it: Systematic experiments varying sampling rates from high to very low frequencies while measuring F1-scores would identify the lower bound of viable sampling rates for LLM-based HAR

### Open Question 3
- Question: How do LLMs perform on HAR tasks with more than four activity classes or with highly similar activities?
- Basis in paper: [explicit] The study uses datasets with 4 and 2 activity classes respectively, explicitly noting that HHAR tests "similar" activities
- Why unresolved: The research doesn't test LLMs on more complex HAR scenarios with larger class sets or activities with subtle distinctions, limiting understanding of scalability and performance on realistic multi-activity CPS environments
- What evidence would resolve it: Testing LLMs on large-scale HAR datasets (e.g., PAMAP2, USC-HAD) with 10+ activity classes and conducting confusion matrix analysis would reveal scalability limitations and discrimination capabilities

## Limitations

- Data Partitioning Ambiguities: The paper claims to evaluate on "unseen users" but doesn't specify exact user-stratified splitting ratios or random seeds used for partitioning datasets
- Tokenization Process Gaps: The exact mapping from raw IMU numerical sequences to LLM tokens remains underspecified, which could significantly impact performance
- CoT Prompt Effectiveness: The paper lacks ablation studies isolating the contribution of chain-of-thought versus other prompt components

## Confidence

- High Confidence: The core observation that LLMs can process raw IMU data for HAR tasks without fine-tuning is well-supported by the experimental results across two datasets and multiple baseline comparisons
- Medium Confidence: The superiority of GPT-4 over traditional ML baselines and the robustness on unseen data are reasonably supported, though the exact magnitude of improvement depends on implementation details not fully specified
- Low Confidence: The specific contribution of chain-of-thought prompting versus other prompt engineering elements cannot be confidently isolated from the current results

## Next Checks

1. Reproducibility Verification: Recreate the exact user-stratified train/test splits for both datasets using the same random seeds (if any were used) and verify that the reported F1-scores of 0.795 (Capture24) and 0.790 (HHAR) are achievable

2. Tokenization Sensitivity Analysis: Systematically vary the tokenization granularity (different downsampling rates, different ways of formatting IMU sequences) and measure the impact on activity recognition accuracy to identify optimal preprocessing strategies

3. CoT Ablation Study: Compare GPT-4 performance with and without chain-of-thought prompting on both seen and unseen user data to quantify the specific contribution of the reasoning strategy versus other prompt components