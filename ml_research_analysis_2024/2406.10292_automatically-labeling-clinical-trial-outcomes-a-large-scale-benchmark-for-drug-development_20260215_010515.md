---
ver: rpa2
title: 'Automatically Labeling Clinical Trial Outcomes: A Large-Scale Benchmark for
  Drug Development'
arxiv_id: '2406.10292'
source_url: https://arxiv.org/abs/2406.10292
tags:
- trial
- phase
- trials
- data
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CTO, a large-scale, reproducible benchmark
  for predicting clinical trial outcomes using weak supervision. The authors aggregate
  multiple noisy signals, including LLM predictions on PubMed abstracts, trial phase
  linkages, news sentiment, stock price movements, and clinical trial metrics, to
  automatically label outcomes for around 479,000 trials.
---

# Automatically Labeling Clinical Trial Outcomes: A Large-Scale Benchmark for Drug Development

## Quick Facts
- arXiv ID: 2406.10292
- Source URL: https://arxiv.org/abs/2406.10292
- Reference count: 40
- Large-scale clinical trial outcome prediction benchmark with 479K trials

## Executive Summary
This paper presents CTO, a large-scale, reproducible benchmark for predicting clinical trial outcomes using weak supervision. The authors aggregate multiple noisy signals, including LLM predictions on PubMed abstracts, trial phase linkages, news sentiment, stock price movements, and clinical trial metrics, to automatically label outcomes for around 479,000 trials. They show that CTO's labels strongly agree with human annotations (F1 up to 94% for Phase 3), enabling effective prediction models without manual labeling. The work highlights the feasibility of scaling clinical trial outcome prediction and provides an open, regularly updated dataset for advancing drug development research.

## Method Summary
The authors use weak supervision via data programming to aggregate multiple noisy labeling functions (LLM predictions, trial linkage, news sentiment, stock price, trial metrics) into high-quality pseudo-labels for clinical trial outcomes. They extract features from ClinicalTrials.gov, FDA Orange Book, DrugBank, PubMed abstracts, and financial data, then apply labeling functions and aggregate via both data programming and Random Forest classifiers. The resulting labels are validated against human annotations and made available as an open dataset.

## Key Results
- CTO labels achieve 94% F1 for Phase 3 trials and 91% overall agreement with human annotations
- Random Forest aggregation outperforms data programming on human agreement (0.729 vs 0.646 Cohen's Kappa)
- The benchmark enables effective prediction models without manual labeling for ~479K trials

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak supervision via data programming aggregates noisy signals into high-quality pseudo-labels that agree with human annotations (up to 91% F1).
- Mechanism: Multiple labeling functions (LLM predictions, trial linkage, news sentiment, stock price, trial metrics) are treated as conditionally independent noisy sources. Data programming estimates their accuracies and correlations, then produces a weighted consensus label.
- Core assumption: Conditional independence of labeling functions given the true label and sufficient coverage to enable matrix completion.
- Evidence anchors:
  - [abstract]: "The trial outcome labels in the CTO benchmark agree strongly with expert annotations, achieving an F1 score of 94 for Phase 3 trials and 91 across all phases."
  - [section 2.6]: "Data programming is a framework designed to create denoised pseudo-labels from various weak supervision sources via labeling functions and matrix completion."
  - [corpus]: Weak; no direct citations from neighboring papers, but data programming is standard in weak supervision literature.
- Break condition: Violated independence assumptions, low coverage of labeling functions, or extreme correlation among them that prevents matrix completion.

### Mechanism 2
- Claim: Trial linkage across phases provides a strong weak signal for outcome prediction.
- Mechanism: Later-phase trials are matched to earlier-phase trials via similarity in intervention, disease, and other trial features, using embedding similarity and cross-encoder re-ranking. Success in later phases implies success in earlier phases.
- Core assumption: Phase progression is largely sequential and trials in later phases have identifiable "predecessor" trials in earlier phases.
- Evidence anchors:
  - [section 2.2]: "Linking trials across phases is not straightforward due to challenges, including unstructured data, inconsistent reporting standards, missing information, data noise, and discrepancies in intervention details across phases."
  - [section 2.2]: "We apply this process for all trials in Phase 4, Phase 3, Phase 2 & 3, and Phase 2 to extract the trial linkages."
  - [corpus]: Weak; no explicit citations, but clinical trial linkage is a known weak signal.
- Break condition: Non-sequential phase transitions, missing or ambiguous trial identifiers, or large dataset drift.

### Mechanism 3
- Claim: Stock price movements and news sentiment capture market expectations about trial outcomes, serving as weak signals.
- Mechanism: Pre- and post-trial stock price trends are computed via moving averages; news sentiment is classified and matched to trials via embedding similarity. These signals are aggregated with other weak signals.
- Core assumption: Market expectations, as reflected in stock price and news sentiment, correlate with trial outcomes.
- Evidence anchors:
  - [section 2.4]: "The stock price of a pharmaceutical or biotech company often reflects market expectations."
  - [section 2.3]: "We utilize FinBERT to obtain financial news sentiment ('Positive' or 'Negative' with a confidence score between 0 and 1) for every headline."
  - [corpus]: Weak; no direct citations, but market-based signals are standard in drug development prediction.
- Break condition: Market noise dominates, sentiment misaligns with trial outcomes, or sponsor coverage is too limited.

## Foundational Learning

- Concept: Conditional independence assumption in data programming
  - Why needed here: Allows recovery of labeling function accuracies from observed agreements without explicit labeling function accuracy labels.
  - Quick check question: If labeling function A always agrees with labeling function B, can data programming still estimate their accuracies separately?

- Concept: Embedding similarity and cross-encoder re-ranking
  - Why needed here: Used in trial linkage to retrieve candidate earlier-phase trials and refine matches.
  - Quick check question: What is the difference between using only embedding similarity versus adding a cross-encoder re-ranker for trial linkage?

- Concept: Weak supervision and pseudo-labeling
  - Why needed here: Enables large-scale labeling without manual annotation, crucial for datasets of ~479K trials.
  - Quick check question: Why might a Random Forest trained on weakly supervised labels outperform one trained on a small human-labeled set?

## Architecture Onboarding

- Component map:
  - Data Ingestion: ClinicalTrials.gov, FDA Orange Book, DrugBank, PubMed, news APIs, Yahoo Finance
  - Feature Extraction: Trial embeddings, PubMed abstract embeddings, news headline embeddings, stock price slopes
  - Labeling Functions: LLM predictions, trial linkage, news sentiment, stock price, trial metrics
  - Weak Supervision: Data programming (unsupervised) + Random Forest (supervised)
  - Output: Pseudo-labels for ~479K trials, with both RF and DP versions

- Critical path:
  1. Extract trial features and embeddings
  2. Run labeling functions (LLM, linkage, sentiment, stock, metrics)
  3. Aggregate via data programming and/or supervised RF
  4. Apply phase-specific thresholds to assign final outcome labels
  5. Validate against TOP human-annotated set

- Design tradeoffs:
  - RF vs DP: RF gives higher agreement with TOP (0.729 κ) but may overfit to TOP's distribution; DP is more generalizable but lower agreement (0.646 κ).
  - Embedding-based vs cross-encoder linkage: Embedding is faster but less precise; cross-encoder is more accurate but computationally heavier.
  - Coverage vs accuracy: Including more labeling functions increases coverage but may introduce noise.

- Failure signatures:
  - Low κ between labeling functions: Suggests correlation issues or missing key signals.
  - High abstention rate in a labeling function: Indicates poor coverage or ambiguity.
  - RF and DP outputs diverge significantly: Suggests instability in weak supervision aggregation.

- First 3 experiments:
  1. Run data programming with only LLM predictions and trial linkage; measure agreement with TOP.
  2. Add stock price and news sentiment labeling functions; check if agreement improves or degrades.
  3. Compare RF vs DP outputs on a held-out subset; analyze sources of disagreement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do CTO's automatically generated labels compare to human-annotated labels for trials completed after 2020?
- Basis in paper: [inferred] The paper discusses the need for continuously updated labeling approaches and shows performance differences between pre-2020 and post-2020 data splits.
- Why unresolved: The paper focuses on agreement with human annotations from the TOP dataset (pre-2020 trials) and does not provide detailed comparison for trials completed after 2020.
- What evidence would resolve it: Evaluating CTO's labels against human annotations for trials completed after 2020, showing F1 scores or other metrics for this subset.

### Open Question 2
- Question: What is the impact of individual weak labeling functions on the final CTO labels, and how do they contribute to the overall performance?
- Basis in paper: [explicit] The paper mentions that Data Programming completely beats Majority Vote in terms of agreement with the TOP dataset and that Random Forest obtains the highest scores.
- Why unresolved: The paper does not provide a detailed ablation study on the contribution of each weak labeling function to the final performance.
- What evidence would resolve it: Conducting an ablation study where each weak labeling function is removed or added to the aggregation process, showing the impact on the final CTO labels' performance.

### Open Question 3
- Question: How do CTO's labels perform on predicting trial outcomes for biologics and medical devices compared to small-molecule drugs?
- Basis in paper: [explicit] The paper mentions that CTO facilitates the development of prediction models for biologics and medical devices, broadening the scope of clinical trial outcome research.
- Why unresolved: The paper focuses on comparing CTO's labels with human annotations for small-molecule drug interventions and does not provide specific performance metrics for biologics and medical devices.
- What evidence would resolve it: Evaluating CTO's labels against human annotations or ground truth for biologics and medical devices, showing F1 scores or other metrics for these categories.

## Limitations

- The conditional independence assumption in data programming is rarely verified in practice and may be violated by correlated labeling functions
- Cross-encoder re-ranking parameters for trial linkage are not specified, making exact reproduction difficult
- Limited evaluation of label quality on biologics and medical devices compared to small-molecule drugs

## Confidence

- **High Confidence**: The feasibility of weak supervision for clinical trial outcome prediction and the general framework design
- **Medium Confidence**: The specific F1 scores achieved (94% Phase 3, 91% overall) due to lack of detailed validation methodology
- **Low Confidence**: The robustness of trial linkage across all trial phases and the impact of missing or ambiguous trial identifiers

## Next Checks

1. Test the conditional independence assumption by measuring pairwise correlations among labeling functions and assessing how correlation affects data programming output stability.

2. Implement and test different cross-encoder architectures for trial linkage to determine optimal configuration and its impact on overall label quality.

3. Evaluate the RF and DP models on trials from different time periods or therapeutic areas not represented in the training data to assess generalizability.