---
ver: rpa2
title: 'RoLargeSum: A Large Dialect-Aware Romanian News Dataset for Summary, Headline,
  and Keyword Generation'
arxiv_id: '2412.11317'
source_url: https://arxiv.org/abs/2412.11317
tags:
- news
- romanian
- summarization
- language
- rolargesum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors introduce RoLargeSum, a large-scale summarization
  dataset for Romanian comprising over 615K news articles with summaries, headlines,
  keywords, and dialect annotations. They evaluate BART variants and open-source LLMs,
  showing that mBART-large with Unlimiformer and loss reversal adversarial training
  achieved the highest ROUGE scores (e.g., R-1: 44.57, R-2: 24.03, R-L: 36.10 on the
  full dataset).'
---

# RoLargeSum: A Large Dialect-Aware Romanian News Dataset for Summary, Headline, and Keyword Generation

## Quick Facts
- arXiv ID: 2412.11317
- Source URL: https://arxiv.org/abs/2412.11317
- Reference count: 36
- Primary result: Largest Romanian summarization dataset with 615K+ articles, showing mBART-large + Unlimiformer + adversarial training achieves highest ROUGE scores

## Executive Summary
RoLargeSum is a large-scale summarization dataset for Romanian containing over 615,000 news articles with summaries, headlines, keywords, and dialect annotations. The dataset addresses the lack of Romanian NLP resources and enables development of summarization models for this low-resource language. The authors evaluate multiple BART variants and open-source LLMs, finding that mBART-large with Unlimiformer for long context handling and adversarial training for dialect independence achieves the best performance across summarization, headline generation, and keyword extraction tasks.

## Method Summary
The authors created RoLargeSum by cleaning and processing 615K+ Romanian news articles from G4Media and Antena3, annotating dialect (Romanian vs. Moldavian) and splitting into train/val/test sets. They fine-tuned BART-base, BART-large, and mBART-large models with Unlimiformer to handle long documents and applied adversarial training to produce dialect-independent embeddings. Models were evaluated on ROUGE-1, ROUGE-2, and ROUGE-L scores for summary, headline, and keyword generation tasks.

## Key Results
- mBART-large with Unlimiformer and adversarial training achieved highest ROUGE scores (R-1: 44.57, R-2: 24.03, R-L: 36.10 on full dataset)
- Romanian LLMs generally outperformed multilingual models on Romanian summarization tasks
- Human evaluation showed generated summaries and headlines were coherent and fluent but less consistent with source articles
- RoLargeSum is the first Romanian dataset to include keyword extraction alongside summarization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dialect-aware adversarial training improves summarization quality by forcing dialect-independent representations.
- Mechanism: The encoder is trained to produce embeddings that cannot be distinguished by dialect, while the decoder is trained to generate high-quality summaries. This is achieved by adding a dialect classification head that receives reversed gradients during backpropagation, preventing the encoder from encoding dialect-specific features.
- Core assumption: Dialect information in embeddings is not essential for accurate summarization and may introduce bias or inconsistency.
- Evidence anchors: [abstract], [section]
- Break condition: If dialect-specific nuances are crucial for accurate summarization, forcing dialect-independent representations could reduce performance.

### Mechanism 2
- Claim: Unlimiformer enables processing of long documents beyond BART's native context window, improving summary quality.
- Mechanism: Unlimiformer replaces standard cross-attention with a k-NN index that retrieves the top-k keys for each attention head, allowing unlimited input length while maintaining computational efficiency.
- Core assumption: Key information for summarization is contained in a limited number of chunks, and k-NN retrieval can effectively identify these.
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If essential summary information is distributed across many chunks beyond the k retrieved by Unlimiformer, important content may be missed.

### Mechanism 3
- Claim: Multilingual pre-training provides better Romanian summarization than monolingual English models.
- Mechanism: mBART-large, pre-trained on multiple languages including Romanian, has learned language-specific features and representations that benefit Romanian summarization tasks compared to BART models trained only on English.
- Core assumption: Pre-training on Romanian text provides useful linguistic knowledge for Romanian summarization that English-only pre-training lacks.
- Evidence anchors: [abstract], [section], [corpus]
- Break condition: If the Romanian-specific knowledge in mBART-large is minimal or if English-trained models can effectively learn Romanian during fine-tuning, the advantage may disappear.

## Foundational Learning

- Concept: Dialectal variation in Romanian language
  - Why needed here: The dataset contains both Romanian and Moldavian news articles with different linguistic characteristics, requiring models to handle this variation appropriately.
  - Quick check question: Can you identify at least three linguistic differences between Romanian and Moldavian dialects?

- Concept: Text summarization evaluation metrics
  - Why needed here: The paper uses ROUGE scores (R-1, R-2, R-L) to evaluate summarization quality, requiring understanding of these metrics and their interpretation.
  - Quick check question: What is the difference between ROUGE-1 and ROUGE-L, and when would ROUGE-L be more appropriate?

- Concept: Adversarial training for domain adaptation
  - Why needed here: The paper employs adversarial training to create dialect-independent features, requiring understanding of how gradient reversal and loss reversal techniques work.
  - Quick check question: How does gradient reversal in adversarial training prevent the model from encoding specific features like dialect?

## Architecture Onboarding

- Component map: Data cleaning → Train/Val/Test split → Dialect labeling → Encoder (dialect-adversarial) → Unlimiformer k-NN retrieval → Decoder → Output
- Critical path: Document → Encoder (dialect-adversarial) → Unlimiformer k-NN retrieval → Decoder → Summary generation
- Design tradeoffs: BART vs mBART (English-only vs multilingual pre-training), Context truncation vs Unlimiformer (limited vs unlimited context), Gradient reversal vs loss reversal (standard vs modified adversarial approach)
- Failure signatures: Poor ROUGE scores despite high training performance, Inconsistent summaries across dialects, Extremely long inference times
- First 3 experiments: 1) Baseline BART-large fine-tuning on RO+MD subset, 2) BART-large + Unlimiformer on RO+MD subset, 3) BART-large + gradient reversal adversarial training on RO+MD subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dialect imbalance in RoLargeSum affect the performance of summarization models across different Romanian and Moldavian subtasks?
- Basis in paper: [explicit] The paper discusses dialect imbalance (3:1 ratio of Moldavian to Romanian articles) and evaluates models on dialect-specific subtasks (RO, MD, RO→MD, MD→RO).
- Why unresolved: The paper shows performance differences but does not analyze whether the imbalance directly impacts model generalization or if dialect-specific training could mitigate this.
- What evidence would resolve it: Detailed ablation studies comparing models trained on balanced vs. imbalanced dialect distributions, or experiments with synthetic dialect augmentation.

### Open Question 2
- Question: What is the impact of using Unlimiformer on the quality of generated summaries compared to truncation-based approaches, beyond ROUGE scores?
- Basis in paper: [explicit] The paper evaluates Unlimiformer for handling long documents and shows ROUGE improvements, but does not analyze qualitative differences in summary quality.
- Why unresolved: ROUGE metrics may not capture nuances like coherence, relevance, or factual consistency in long-document summarization.
- What evidence would resolve it: Human evaluations comparing summaries generated with and without Unlimiformer on metrics like coherence, factual consistency, and coverage of key information.

### Open Question 3
- Question: How do Romanian-specific LLMs (e.g., RoLlama, RoMistral) compare to multilingual models in handling dialect-specific nuances and domain-specific vocabulary in news summarization?
- Basis in paper: [explicit] The paper shows Romanian LLMs generally outperform multilingual ones but does not analyze their handling of dialect-specific or domain-specific language features.
- Why unresolved: The evaluation focuses on aggregate ROUGE scores without probing dialect-aware or domain-aware performance.
- What evidence would resolve it: Fine-grained analysis of model performance on dialect-specific test sets or domain-specific vocabulary, including error analysis of mispredicted terms.

### Open Question 4
- Question: Does adversarial training with dialect classification improve summarization quality, or does it primarily serve to reduce dialect-specific biases in the encoder?
- Basis in paper: [explicit] The paper applies adversarial training to produce dialect-independent embeddings but does not evaluate its direct impact on summary quality.
- Why unresolved: The paper shows improved ROUGE scores with adversarial training but does not isolate whether this is due to better summarization or reduced dialect bias.
- What evidence would resolve it: Comparative experiments with and without adversarial training on dialect-agnostic and dialect-specific summarization tasks, measuring both ROUGE and dialect bias metrics.

## Limitations
- Dialect handling limitations: Paper does not provide detailed analysis of whether dialect-specific content is actually detrimental to summarization quality
- Evaluation scope constraints: Human evaluation was limited to 100 samples and focused primarily on coherence and fluency rather than factual consistency
- Model comparison limitations: Study compares only limited set of model variants without systematic ablation studies

## Confidence
- High Confidence: Dataset construction methodology and basic statistics are well-documented and reproducible
- Medium Confidence: Experimental results showing mBART-large with Unlimiformer and adversarial training achieving highest ROUGE scores are plausible but lack ablation studies
- Low Confidence: Claim that adversarial training specifically improves dialect-independent summarization lacks strong empirical support

## Next Checks
1. Conduct systematic ablation experiments removing Unlimiformer, adversarial training, and loss reversal components separately to quantify their individual contributions to performance improvements
2. Perform controlled experiments comparing summarization quality when dialect information is explicitly included versus excluded to test the core assumption about dialect independence
3. Evaluate additional state-of-the-art summarization models (T5, Pegasus, contemporary LLMs) on RoLargeSum to establish whether mBART-large results represent best-in-class performance