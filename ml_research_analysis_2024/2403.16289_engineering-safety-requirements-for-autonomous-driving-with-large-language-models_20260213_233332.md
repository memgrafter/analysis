---
ver: rpa2
title: Engineering Safety Requirements for Autonomous Driving with Large Language
  Models
arxiv_id: '2403.16289'
source_url: https://arxiv.org/abs/2403.16289
tags:
- safety
- hara
- requirements
- engineering
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using large language models (LLMs) for automated
  hazard analysis and risk assessment (HARA) in autonomous driving systems. The authors
  propose a pipeline of prompts and LLM tasks to automatically generate safety requirements
  from an item definition, followed by a review for redundancy and contradictions.
---

# Engineering Safety Requirements for Autonomous Driving with Large Language Models

## Quick Facts
- arXiv ID: 2403.16289
- Source URL: https://arxiv.org/abs/2403.16289
- Reference count: 20
- Primary result: Automated HARA pipeline using LLMs produces adequate safety requirements with potential 60% efficiency improvement over manual methods

## Executive Summary
This paper investigates using large language models (LLMs) to automate hazard analysis and risk assessment (HARA) for autonomous driving systems. The authors develop a pipeline of prompts and LLM tasks that take an item definition as input and generate safety requirements. Through multiple design iterations with expert evaluation, they refine the approach to improve LLM performance. The final prototype was tested on a case company's function, with experts finding the generated safety requirements adequate and potentially more efficient than manual HARA. Key limitations identified include the need for specific domain knowledge in prompts and the risk of hallucinations.

## Method Summary
The authors propose a pipeline approach that breaks down HARA into discrete LLM-manageable tasks including hazard identification, scenario generation, and safety goal formulation. Using design science methodology, they iteratively refine prompts through multiple cycles with expert evaluation. The pipeline uses GPT-4 API to automate task execution, receiving item definitions as input and producing safety requirements in human-readable table format. Expert reviewers from different companies quantitatively and qualitatively evaluate each cycle's output.

## Key Results
- Expert reviewers found the generated safety requirements adequate and relevant
- The LLM-assisted approach could potentially increase efficiency by 60% compared to manual HARA
- Iterative prompt engineering significantly improved LLM performance in generating automotive safety requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can be used to automate HARA by decomposing the process into discrete, manageable tasks.
- **Mechanism:** The paper breaks down the complex HARA process into a series of LLM-manageable tasks (e.g., hazard identification, scenario generation, safety goal formulation). Each task is guided by a specific prompt, and the outputs are chained together in a pipeline to produce the final HARA results.
- **Core assumption:** The LLM can effectively perform each individual task when provided with clear, domain-specific instructions and examples.
- **Evidence anchors:**
  - [abstract] "We propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements."
  - [section] "The experiments in the first cycle showed that HARA has to be broken down into tasks that require specific information."
- **Break condition:** If the LLM lacks the domain-specific knowledge required for a task, or if the prompts are not sufficiently clear, the quality of the output will degrade.

### Mechanism 2
- **Claim:** Prompt engineering significantly improves the LLM's performance in generating safety requirements.
- **Mechanism:** The paper iteratively refines the prompts used in each task of the HARA pipeline. This includes providing definitions for key terms, using few-shot learning with examples, and tailoring the prompts to the specific needs of the automotive safety domain.
- **Core assumption:** The LLM's performance is highly sensitive to the quality and specificity of the prompts it receives.
- **Evidence anchors:**
  - [abstract] "We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively."
  - [section] "Using few-shot learning [8] is seen as an effective way to improve the generated output; therefore, generic examples are used to explain the process, which are not specific to the function under analysis."
- **Break condition:** If the prompts are too generic or do not align with the specific needs of the task, the LLM may generate irrelevant or incorrect outputs.

### Mechanism 3
- **Claim:** Human oversight is essential for ensuring the safety and reliability of LLM-generated HARA results.
- **Mechanism:** The paper emphasizes the importance of expert review and validation of the LLM-generated HARA. This includes both Verification Reviews (VR) and Confirmation Reviews (CR) to ensure the correctness and completeness of the results.
- **Core assumption:** LLMs are not yet capable of fully replacing human experts in safety-critical domains like autonomous driving.
- **Evidence anchors:**
  - [abstract] "Key limitations identified include the need for specific knowledge in prompts and potential hallucinations."
  - [section] "The experts were then asked to discuss the validity and formulation of the safety goals provided by the tool, in terms of meeting the necessary safety standards or regulations."
- **Break condition:** If human oversight is bypassed or if the experts are not sufficiently knowledgeable about the domain, the safety and reliability of the HARA results could be compromised.

## Foundational Learning

- **Concept:** Hazard Analysis and Risk Assessment (HARA)
  - **Why needed here:** HARA is a critical process in the development of safe autonomous driving systems. It involves identifying potential hazards, assessing their risks, and specifying safety requirements to mitigate those risks.
  - **Quick check question:** What are the key steps involved in conducting a HARA for an autonomous driving function?

- **Concept:** Large Language Models (LLMs)
  - **Why needed here:** LLMs are used in this paper to automate the HARA process. They can generate text, understand natural language, and perform tasks like hazard identification and safety goal formulation.
  - **Quick check question:** What are the limitations of using LLMs for safety-critical tasks like HARA?

- **Concept:** Prompt Engineering
  - **Why needed here:** Prompt engineering is crucial for improving the performance of LLMs in generating safety requirements. It involves crafting specific prompts that guide the LLM to produce the desired output.
  - **Quick check question:** How can prompt engineering be used to improve the accuracy and relevance of LLM-generated safety requirements?

## Architecture Onboarding

- **Component map:** Item Definition -> LLM Pipeline -> Safety Requirements -> Expert Review

- **Critical path:** Item Definition -> LLM Pipeline -> Safety Requirements -> Expert Review

- **Design tradeoffs:**
  - Automation vs. Human Oversight: While LLMs can automate many tasks, human experts are still needed to ensure the safety and reliability of the results.
  - Prompt Specificity vs. Generality: Prompts need to be specific enough to guide the LLM effectively, but general enough to be applicable to a range of scenarios.

- **Failure signatures:**
  - Hallucinations: LLMs may generate irrelevant or incorrect information.
  - Lack of Domain Knowledge: LLMs may not have the specific knowledge required for safety-critical tasks.
  - Over-Reliance on Automation: Bypassing human oversight could compromise safety.

- **First 3 experiments:**
  1. Test the LLM's ability to generate hazard definitions and scenarios for a simple autonomous driving function.
  2. Evaluate the LLM's performance in formulating safety goals based on the generated hazards and scenarios.
  3. Conduct a pilot expert review to validate the LLM-generated safety requirements and identify areas for improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the completeness of scenarios identified by LLMs in HARA be systematically validated?
- Basis in paper: [explicit] "There is a need for a systematic approach to argue for completeness, since no such method currently exists"
- Why unresolved: The paper identifies this as a limitation but does not propose a solution for systematically validating scenario completeness
- What evidence would resolve it: Development and validation of a systematic method for verifying scenario completeness in HARA using LLMs

### Open Question 2
- Question: What is the optimal balance between allowing LLM creativity and preventing hallucinations in safety-critical requirement generation?
- Basis in paper: [explicit] "This is a challenging trade-off because the HARA requires allowing for some degree of creativity, which can only be partially controlled by the temperature parameter"
- Why unresolved: The paper acknowledges this challenge but does not determine the optimal parameters or techniques
- What evidence would resolve it: Empirical studies testing different creativity control mechanisms and their impact on hallucination rates and requirement quality

### Open Question 3
- Question: How can the risk of false positives and false negatives in LLM-generated safety requirements filtering be minimized?
- Basis in paper: [explicit] "There is a need for a balance between false positives and false negatives. As one of the participants mentioned: 'Giving more options can allow the human to filter and choose the relevant ones'"
- Why unresolved: The paper suggests human filtering as a current solution but does not explore automated filtering techniques or their effectiveness
- What evidence would resolve it: Development and validation of automated filtering techniques that minimize both false positives and false negatives in safety requirement generation

## Limitations
- The specific case-company function used for validation is not publicly available, preventing exact replication
- Hallucinations remain a significant risk with no fully specified mitigation strategy
- The claim of 60% efficiency improvement is based on expert opinion rather than quantitative measurement

## Confidence

**High Confidence**: The general pipeline approach for decomposing HARA into LLM-manageable tasks and the value of iterative prompt engineering are well-supported by the expert evaluation results and design science methodology.

**Medium Confidence**: The claim that LLM-generated requirements are "adequate" and could increase efficiency by 60% compared to manual HARA is based on expert opinion rather than quantitative measurement of actual development time or defect rates.

**Low Confidence**: The assertion that LLMs can reliably handle complex automotive safety scenarios without significant domain-specific customization remains unproven, given the identified limitations around hallucinations and knowledge gaps.

## Next Checks
1. **Replicate with open-source function**: Test the pipeline on publicly documented autonomous driving functions (e.g., from open-source ADAS projects) to verify generalizability and identify prompt sensitivity to different function types.

2. **Quantify efficiency gains**: Conduct a controlled experiment comparing time-to-completion and defect detection rates between LLM-assisted and traditional manual HARA processes across multiple safety engineers.

3. **Evaluate hallucination frequency**: Implement systematic logging of LLM outputs to measure hallucination rates across different prompt configurations and develop statistical models for hallucination risk based on prompt characteristics.