---
ver: rpa2
title: 'MM-KWS: Multi-modal Prompts for Multilingual User-defined Keyword Spotting'
arxiv_id: '2406.07310'
source_url: https://arxiv.org/abs/2406.07310
tags:
- speech
- mm-kws
- text
- keyword
- spotting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MM-KWS, a novel approach to user-defined keyword
  spotting (UDKWS) that leverages multi-modal enrollments of text and speech templates.
  Unlike previous methods that focus solely on either text or speech features, MM-KWS
  extracts phoneme, text, and speech embeddings from both modalities and compares
  them with the query speech embedding to detect target keywords.
---

# MM-KWS: Multi-modal Prompts for Multilingual User-defined Keyword Spotting

## Quick Facts
- arXiv ID: 2406.07310
- Source URL: https://arxiv.org/abs/2406.07310
- Reference count: 0
- Primary result: MM-KWS achieves 96.25% AUC and 9.30% EER on LibriPhrase Hard subset

## Executive Summary
MM-KWS introduces a novel multi-modal approach to user-defined keyword spotting (UDKWS) that leverages both text and speech templates for enrollment. Unlike previous methods focusing on single modalities, MM-KWS extracts phoneme, text, and speech embeddings from both enrollment modalities and compares them with query speech embeddings. The system employs multilingual pre-trained models to ensure applicability across diverse languages, validated on Mandarin and English tasks using newly introduced WenetPhrase dataset.

## Method Summary
MM-KWS processes user-defined keywords through a dual-enrollment system that accepts both text and speech templates. The method extracts three types of embeddings (phoneme, text, and speech) from both modalities and performs cross-modal comparison with the query speech embedding. A feature extractor incorporating multiple multilingual pre-trained models enables cross-lingual keyword spotting. The system also integrates advanced data augmentation tools for hard case mining, specifically targeting confusable word discrimination. Validation was performed on the LibriPhrase dataset and the newly introduced WenetPhrase dataset for Mandarin evaluation.

## Key Results
- Achieves 96.25% AUC and 9.30% EER on LibriPhrase Hard subset, surpassing larger models
- Attains 99.79% AUC and 1.95% EER on WenetPhrase Easy subset
- Demonstrates superior performance on confusable word discrimination through data augmentation

## Why This Works (Mechanism)
The multi-modal enrollment approach leverages complementary information from both text and speech templates, creating a more robust representation for keyword spotting. By extracting and comparing multiple embedding types (phoneme, text, speech) across modalities, MM-KWS creates redundancy that improves detection accuracy, particularly for confusable words. The use of multilingual pre-trained models enables cross-lingual transfer learning, while data augmentation specifically targets hard cases that challenge single-modal systems.

## Foundational Learning

**Multimodal enrollment systems**: Why needed - enables richer representation by combining text and speech information. Quick check - verify that both modalities are processed independently before fusion.

**Embedding extraction for keyword spotting**: Why needed - creates fixed-length representations for comparison. Quick check - confirm that phoneme, text, and speech embeddings are extracted from both enrollment and query.

**Cross-lingual feature extraction**: Why needed - enables keyword spotting across diverse languages. Quick check - verify that multilingual pre-trained models are properly fine-tuned for target languages.

**Data augmentation for hard case mining**: Why needed - improves robustness to confusable words and challenging acoustic conditions. Quick check - ensure augmentation techniques target specific error patterns.

**Multi-embedding comparison**: Why needed - leverages multiple similarity metrics for robust detection. Quick check - verify that all three embedding types are compared with query embeddings.

## Architecture Onboarding

**Component map**: User text input -> Text embedding extractor -> Embedding storage; User speech input -> Speech embedding extractor -> Embedding storage; Query speech -> Query embedding extractor -> Multi-modal comparator -> Decision output

**Critical path**: Enrollment (text/speech templates) -> Embedding extraction (phoneme, text, speech) -> Storage -> Query processing -> Multi-modal comparison -> Keyword detection

**Design tradeoffs**: Model complexity vs. inference speed (multiple multilingual models increase accuracy but computational overhead); Single-modal vs. multi-modal enrollment (simpler but less robust vs. more complex but more accurate); Hard case mining vs. general augmentation (targeted but potentially overfitting vs. general but less effective).

**Failure signatures**: Poor performance on underrepresented languages; Degradation under challenging acoustic conditions; Overfitting to training augmentation patterns; Confusion between phonetically similar keywords.

**First experiments**: 1) Ablation study removing text enrollment to measure multi-modal contribution; 2) Cross-lingual transfer testing on additional language families; 3) Performance evaluation under varying SNR conditions.

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- Generalizability across diverse linguistic contexts beyond Mandarin and English remains unproven
- Performance under real-world acoustic variability and accent diversity not thoroughly evaluated
- Specific contribution of data augmentation tools to confusable word discrimination inadequately quantified

## Confidence

**High confidence**: The core multi-modal enrollment methodology (text + speech template enrollment with multi-embedding comparison) is technically sound and the experimental setup is reproducible.

**Medium confidence**: The reported performance improvements over baseline methods are reliable within the tested datasets, but generalizability to broader linguistic and acoustic conditions requires further validation.

**Low confidence**: Claims about robustness to confusable words and the specific contribution of the data augmentation pipeline lack sufficient empirical support.

## Next Checks

1. Conduct cross-lingual transfer experiments using the same MM-KWS architecture on additional language families (e.g., Romance, Slavic, Semitic) to assess true multilingual capability beyond Mandarin-English.

2. Perform comprehensive error analysis on confusable word pairs with and without the proposed data augmentation tools to isolate their specific contribution to performance gains.

3. Evaluate real-world deployment scenarios including varying SNR conditions, speaker accents, and background noise types to establish robustness boundaries beyond controlled test sets.