---
ver: rpa2
title: 'Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian Benchmark'
arxiv_id: '2406.17535'
source_url: https://arxiv.org/abs/2406.17535
tags:
- language
- questions
- italian
- arxiv
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Disce aut Deficere, a benchmark based on
  Italian INVALSI tests, to evaluate large language models (LLMs) on linguistic tasks.
  The authors adapt real-world assessment questions from the Italian education system
  into multiple-choice formats suitable for automated LLM evaluation.
---

# Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian Benchmark

## Quick Facts
- arXiv ID: 2406.17535
- Source URL: https://arxiv.org/abs/2406.17535
- Authors: Fabio Mercorio; Mario Mezzanzanica; Daniele Potertì; Antonio Serino; Andrea Seveso
- Reference count: 6
- Primary result: GPT-4 and Claude-3-Opus achieve highest accuracy on Italian language tasks, but all models struggle with higher-grade and spelling questions.

## Executive Summary
This paper introduces Disce aut Deficere, a benchmark based on Italian INVALSI tests, to evaluate large language models (LLMs) on linguistic tasks. The authors adapt real-world assessment questions from the Italian education system into multiple-choice formats suitable for automated LLM evaluation. They test a variety of models, including GPT-4, Claude, Gemini, and Italian-specific models, across different school grades and linguistic macro areas such as text comprehension and grammar. Results show that larger models generally perform better, with GPT-4 and Claude-3-Opus achieving the highest accuracy. However, all models struggle with higher-grade tasks and spelling. The benchmark provides a culturally relevant, multilingual evaluation framework and highlights the strengths and weaknesses of current LLMs in handling Italian language tasks.

## Method Summary
The benchmark repurposes INVALSI standardized tests by manually converting questions into multiple-choice formats suitable for automated LLM evaluation. Questions are labeled by linguistic macro area and school grade, then evaluated using zero-shot prompting. Closed questions use regex matching, while open-ended questions use BERTScore for semantic similarity. A variety of models including GPT-4, Claude, Gemini, and Italian-specific models are tested across all grades and linguistic domains.

## Key Results
- GPT-4 and Claude-3-Opus achieve the highest accuracy scores among all tested models
- Larger models consistently outperform smaller models across all linguistic macro areas
- All models show significant difficulty with spelling tasks and higher-grade comprehension questions
- Italian-specific models generally underperform compared to general-purpose large models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark works because it repurposes an established, standardized national test into a multiple-choice format that LLMs can process automatically.
- Mechanism: Real-world test questions are reformulated into closed-ended MC/MCC options, allowing deterministic evaluation via pattern matching and semantic similarity (BERTScore) instead of free-form grading.
- Core assumption: The essential cognitive demand of the original questions is preserved in the reformulated format.
- Evidence anchors:
  - [abstract] "adapt the INVALSI benchmark for automated LLM evaluation, which involves rigorous adaptation of the test format to suit automated processing while retaining the essence of the original tests."
  - [section] "Manual inspection was required to ensure accuracy. In cases where questions involved graphical elements, we modified them into an appropriate multiple-choice format."
- Break condition: If reformulations alter the difficulty or type of reasoning required, the benchmark will no longer reflect true language proficiency.

### Mechanism 2
- Claim: The benchmark distinguishes model performance across linguistic macro areas and educational grades.
- Mechanism: Questions are labeled by macro area (e.g., text comprehension, morphology, spelling) and school grade, enabling stratified evaluation and revealing strengths/weaknesses by domain and complexity.
- Core assumption: Macro area and grade labels accurately map to the linguistic skill being tested.
- Evidence anchors:
  - [section] "Each question in the benchmark is labelled with the specific lexical macro area it aims to assess."
- Break condition: If labeling is inaccurate or inconsistent, performance comparisons across macro areas become misleading.

### Mechanism 3
- Claim: Larger parameter models outperform smaller ones due to scaling laws.
- Mechanism: Model size correlates with accuracy because larger models capture more linguistic patterns and context from training.
- Core assumption: Training data and architecture quality scale appropriately with parameter count.
- Evidence anchors:
  - [section] "Smaller models exhibit greater variance and dispersion in their accuracy scores than medium and large models."
  - [section] "The notches for the large models also do not overlap with those of the small and medium models, and they achieve the highest average accuracy score of 80.94%."
- Break condition: If smaller models are fine-tuned on domain-specific data, they may outperform larger generic models in that domain.

## Foundational Learning

- Concept: **Standardized educational testing** - Why needed here: The benchmark relies on national assessment tests as the source of questions, so understanding their structure and purpose is critical to interpreting results.
  - Quick check question: What is the main purpose of the INVALSI tests in the Italian education system?
- Concept: **Zero-shot prompting for LLMs** - Why needed here: The benchmark evaluates models without task-specific fine-tuning, so the prompt design and evaluation logic must be understood.
  - Quick check question: How does the benchmark evaluate free-response questions without human grading?
- Concept: **BERTScore for semantic similarity** - Why needed here: The benchmark uses BERTScore to assess correctness of open-ended answers; understanding its role is key to interpreting results.
  - Quick check question: What threshold is used in BERTScore to consider an answer correct?

## Architecture Onboarding

- Component map: Data ingestion → Test selection and manual reformatting → Prompt generation per question type → LLM inference → Response parsing (regex or BERTScore) → Accuracy aggregation per model/grade/macro area.
- Critical path: Manual reformatting of questions → Prompt generation → Inference → Evaluation → Aggregation. Delays here directly impact benchmark timeliness.
- Design tradeoffs:
  - Closed vs. open questions: Closed (MC/MCC) allow exact matching; open require semantic similarity and thus higher false-negative risk.
  - Automated vs. manual grading: Automation scales but may miss nuanced correctness; manual grading is accurate but not scalable.
- Failure signatures:
  - Low accuracy spikes in specific macro areas may indicate labeling errors or prompt format issues.
  - GPT-4o's "[letter]" output pattern indicates a model-specific instruction-following failure.
  - BERTScore misclassifications may occur with paraphrases that are semantically correct but below threshold.
- First 3 experiments:
  1. Run a small subset of MC questions through a target model and verify the regex extraction logic matches expected answers.
  2. Evaluate a RU question with BERTScore and manually validate the threshold cutoff to ensure no false negatives.
  3. Compare accuracy distributions of a small model vs. a large model on the same grade subset to confirm the scaling trend before full benchmark runs.

## Open Questions the Paper Calls Out
None

## Limitations
- Question adaptation from original INVALSI tests to multiple-choice format may not preserve original difficulty and reasoning requirements
- Manual adaptation process introduces potential bias and inconsistency that is difficult to quantify
- Benchmark focuses solely on Italian language proficiency, limiting generalizability to other languages or broader cognitive domains

## Confidence
- Benchmark construction methodology: Medium - while the adaptation process is described, details on consistency checks and inter-rater reliability are lacking.
- Model performance comparisons: High - results show clear patterns across multiple model families with appropriate statistical measures.
- Scaling laws observations: Medium - trends are visible but the sample size of models tested may be insufficient to draw definitive conclusions.

## Next Checks
1. Conduct inter-rater reliability analysis on a sample of adapted questions to quantify consistency in the manual conversion process from original INVALSI items to multiple-choice format.
2. Test the benchmark with a fine-tuned smaller model on Italian language data to determine if parameter scaling or domain-specific training drives performance differences.
3. Implement an ablation study removing BERTScore and using only exact matching for open-ended questions to isolate the impact of semantic similarity evaluation on accuracy scores.