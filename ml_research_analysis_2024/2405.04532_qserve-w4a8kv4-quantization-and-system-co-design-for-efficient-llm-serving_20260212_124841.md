---
ver: rpa2
title: 'QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving'
arxiv_id: '2405.04532'
source_url: https://arxiv.org/abs/2405.04532
tags:
- quantization
- qserve
- figure
- fp16
- serving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents QServe, a quantization and system co-design
  framework that accelerates large language model (LLM) serving. The key innovation
  is the W4A8KV4 quantization format, combining 4-bit weights, 8-bit activations,
  and 4-bit KV cache, which is implemented through the QoQ algorithm and QServe system.
---

# QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving

## Quick Facts
- arXiv ID: 2405.04532
- Source URL: https://arxiv.org/abs/2405.04532
- Reference count: 26
- Primary result: QServe achieves 1.2-2.4× and 1.5-3.5× throughput improvements over TensorRT-LLM on A100 and L40S GPUs respectively

## Executive Summary
QServe presents a quantization and system co-design framework for efficient large language model (LLM) serving. The key innovation is the W4A8KV4 quantization format, which combines 4-bit weights, 8-bit activations, and 4-bit KV cache. This is implemented through the QoQ algorithm (progressive group quantization with SmoothAttention) and the QServe system (compute-aware weight reordering and register-level parallelism). On A100 and L40S GPUs, QServe achieves 1.2-2.4× and 1.5-3.5× throughput improvements over TensorRT-LLM, respectively, effectively reducing LLM serving costs by 3× while maintaining competitive accuracy across seven models ranging from 7B to 72B parameters.

## Method Summary
QServe introduces W4A8KV4 quantization combining 4-bit weights, 8-bit activations, and 4-bit KV cache. The QoQ algorithm uses progressive group quantization: first quantizing weights to 8-bit using per-channel scales, then to 4-bit using per-group scales, creating an intermediate representation that enables INT8 tensor core computation throughout. SmoothAttention scales down outliers in keys by per-channel factors to make 4-bit quantization feasible. The QServe system addresses GPU-specific challenges through compute-aware weight reordering (matching tensor core access patterns) and register-level parallelism during dequantization (using logical operations and vadd4 instructions). This design achieves superior throughput while maintaining accuracy across multiple LLM sizes.

## Key Results
- 1.2-2.4× throughput improvement on A100 GPUs compared to TensorRT-LLM
- 1.5-3.5× throughput improvement on L40S GPUs compared to TensorRT-LLM
- 3× cost reduction in LLM serving through improved efficiency
- Performance demonstrated across seven models (7B to 72B parameters) while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: W4A8KV4 quantization achieves better performance than W4A4 or W8A8 by mapping all computations to INT8 tensor cores while avoiding dequantization overhead in the main GEMM loop.
- Mechanism: Progressive group quantization first quantizes weights to 8-bit using per-channel scales, then quantizes to 4-bit using per-group scales. This creates an intermediate 8-bit representation that allows all GEMM computations to use INT8 tensor cores. The protective range [-119, 119] ensures no overflow during dequantization.
- Core assumption: The two-level quantization process preserves accuracy while enabling INT8 tensor core computation throughout.
- Evidence anchors: The abstract describes progressive quantization with QoQ representing 4-8-4 in Latin and explains the protective range; section details progressive group quantization; related papers discuss INT6 quantization but not progressive group quantization specifically.

### Mechanism 2
- Claim: QServe reduces main loop overhead in GEMM kernels by using compute-aware weight reordering and register-level parallelism during dequantization.
- Mechanism: Weights are reordered offline to match the strided access pattern required by tensor cores, eliminating pointer arithmetic overhead. During dequantization, UINT4 to UINT8 conversion uses register-level parallelism with logical operations, and the subtraction-after-multiplication order enables the vadd4 instruction for 4-way INT8 addition.
- Core assumption: The offline weight reordering can be done without runtime overhead and the register-level parallelism effectively reduces dequantization latency.
- Evidence anchors: The abstract mentions compute-aware weight reordering and register-level parallelism; section provides detailed explanation of weight reordering and vadd4 instruction usage; related papers discuss quantization but not specific kernel optimization techniques.

### Mechanism 3
- Claim: KV4 quantization achieves 2× speedup over KV8 by reducing memory bandwidth requirements and shifting the roofline curve.
- Mechanism: Quantizing KV cache to 4 bits effectively doubles memory bandwidth for attention computations. SmoothAttention scales down outliers in keys by per-channel factors, making 4-bit quantization feasible without significant accuracy loss. The attention kernel is optimized to use FP16 operations instead of FP32 to delay the roofline turning point.
- Core assumption: The memory-bound nature of attention computations means reduced precision directly translates to performance gains.
- Evidence anchors: The abstract states that KV4 quantization provides 2× speedup using QServe; section explains that quantizing KV cache increases memory bandwidth and details SmoothAttention; related papers discuss attention optimization but not KV4 quantization benefits specifically.

## Foundational Learning

- Concept: GPU memory hierarchy and tensor core operations
  - Why needed here: Understanding how QServe optimizes memory access patterns and leverages tensor cores is crucial for implementing and debugging the system
  - Quick check question: What is the difference in throughput between FP16, INT8, and INT4 tensor cores on A100 GPUs?

- Concept: Quantization and dequantization processes
  - Why needed here: The core innovation relies on understanding how progressive group quantization works and how to avoid overflow during dequantization
  - Quick check question: How does the protective range [-119, 119] prevent overflow during the UINT4 to UINT8 conversion?

- Concept: Roofline model analysis
  - Why needed here: The performance gains are explained through roofline analysis, understanding compute-bound vs memory-bound regimes is essential
  - Quick check question: At what computation intensity does the A100 GPU transition from memory-bound to compute-bound for FP32 operations?

## Architecture Onboarding

- Component map: Input → LayerNorm → Activation quantization → W4A8 GEMM → FP16 output → Attention (with KV4 quantization) → LayerNorm → Output
- Critical path: Input → LayerNorm → Activation quantization → W4A8 GEMM → FP16 output → Attention (with KV4 quantization) → LayerNorm → Output
- Design tradeoffs:
  - Progressive group quantization vs direct 4-bit quantization: accuracy vs dequantization overhead
  - Per-channel vs per-group quantization: memory usage vs precision
  - FP16 vs FP32 operations in attention: performance vs numerical stability
- Failure signatures:
  - Accuracy degradation: likely issues with SmoothAttention scaling or protective range insufficient
  - Performance regression: weight reordering not working correctly, dequantization overhead too high
  - Memory issues: KV cache management problems, insufficient memory for batch sizes
- First 3 experiments:
  1. Measure throughput of basic W4A8 GEMM kernel vs cuBLAS W8A8 baseline
  2. Compare accuracy of progressive group quantization vs direct 4-bit quantization on a small model
  3. Profile attention kernel to verify that KV4 quantization provides 2× speedup over KV8

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of throughput improvement when moving from W4A16 to W4A8KV4 quantization, and what architectural bottlenecks prevent achieving this limit on current GPUs?
- Basis in paper: The paper mentions that W4A8KV4 has superior roofline performance compared to W8A8 and W4A16, but notes that existing GPU architectures prevent achieving theoretical performance gains
- Why unresolved: The paper provides relative throughput comparisons but doesn't explicitly quantify the theoretical ceiling or fully explain why architectural constraints prevent reaching it
- What evidence would resolve it: Detailed roofline analysis showing maximum achievable throughput for each precision format, combined with microbenchmarking that isolates and quantifies specific architectural bottlenecks

### Open Question 2
- Question: How does QServe's performance scale with larger batch sizes and longer sequence lengths beyond the tested configurations?
- Basis in paper: The paper evaluates performance up to batch size 64 and sequence lengths of 1024-1536, but doesn't explore scalability limits or performance degradation patterns at extreme scales
- Why unresolved: The evaluation focuses on practical serving scenarios but doesn't characterize how the system behaves under stress conditions that might reveal hidden bottlenecks
- What evidence would resolve it: Comprehensive scaling experiments showing throughput curves across multiple orders of magnitude in batch size and sequence length, with performance analysis identifying scaling breakpoints

### Open Question 3
- Question: What is the impact of QServe's W4A8KV4 quantization on model robustness and safety properties compared to higher-precision alternatives?
- Basis in paper: The paper evaluates accuracy degradation in terms of perplexity and zero-shot accuracy but doesn't examine robustness to adversarial inputs or safety-related behaviors
- Why unresolved: Accuracy metrics alone don't capture whether aggressive quantization affects model behavior in ways that could compromise safety or reliability in production settings
- What evidence would resolve it: Comprehensive safety and robustness testing including adversarial input analysis, bias measurements, and behavioral consistency checks across different precision formats

## Limitations

- Limited ablation studies: The paper claims W4A8KV4 outperforms alternatives but lacks detailed ablation studies comparing variants under identical conditions
- Hardware specificity: Performance improvements are specific to A100 and L40S GPUs, with no discussion of portability to other architectures
- Accuracy preservation validation: Claims of competitive accuracy are made but without comprehensive quantitative comparisons across diverse workloads
- Closed-source implementation: The QServe system is not open-source, making independent verification of claimed optimizations difficult

## Confidence

**High confidence**: The fundamental concept of using mixed-precision quantization to leverage INT8 tensor cores while avoiding dequantization overhead is well-established and mathematically sound.

**Medium confidence**: The specific implementation details of progressive group quantization and the protective range [-119, 119] appear reasonable but haven't been independently verified.

**Low confidence**: The SmoothAttention technique's effectiveness in maintaining accuracy with KV4 quantization and the register-level parallelism optimizations during dequantization are difficult to evaluate without implementation details.

## Next Checks

1. **Empirical component isolation**: Measure the throughput of the W4A8 GEMM kernel in isolation and compare it against a baseline W8A8 implementation using the same hardware and batch sizes to validate the 1.2-2.4× improvement.

2. **Accuracy degradation analysis**: Quantify the accuracy difference between models using QoQ quantization versus standard 16-bit precision across multiple benchmark datasets, including ablation studies comparing progressive group quantization against direct 4-bit quantization.

3. **Hardware portability assessment**: Test QServe's performance on different GPU architectures (e.g., H100, RTX 4090) and document how the optimizations scale or require modification to reveal whether the implementation is truly portable.