---
ver: rpa2
title: 'Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating
  Knowledge Conflicts in Language Models'
arxiv_id: '2402.18154'
source_url: https://arxiv.org/abs/2402.18154
tags:
- uni00000013
- uni00000048
- uni00000003
- uni00000057
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates knowledge conflicts in language models
  arising from the integration of internal memory and external context. The authors
  identify that conflicting information flows merge through memory heads and context
  heads in later layers, causing these conflicts.
---

# Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models

## Quick Facts
- **arXiv ID**: 2402.18154
- **Source URL**: https://arxiv.org/abs/2402.18154
- **Reference count**: 25
- **Key outcome**: The paper investigates knowledge conflicts in language models arising from the integration of internal memory and external context, proposing Pruning Head via PatH PatcHing (PH3) to mitigate these conflicts by pruning conflicting attention heads without updating model parameters.

## Executive Summary
This paper addresses knowledge conflicts in language models that arise when internal memory and external context provide contradictory information. The authors identify that these conflicts manifest through memory heads and context heads in later layers of the model. To resolve this, they propose PH3 (Pruning Head via PatH PatcHing), a method that prunes conflicting attention heads without updating model parameters. The approach enables flexible control over whether models use internal memory or external context, improving performance on open-domain QA tasks while demonstrating cross-model, cross-relation, and cross-format generalization.

## Method Summary
The paper proposes Pruning Head via PatH PatcHing (PH3), a method that identifies and prunes conflicting attention heads to mitigate knowledge conflicts in language models. The approach works in two stages: first, path patching is used to compute importance scores for each attention head by running three passes (original, corrupted with ⟨unk⟩, and patched inputs); second, the top-k% negative heads (identified as either memory heads for context prediction or context heads for memory prediction) are pruned by setting their attention matrices to zero. This method allows flexible control over whether models use internal memory or external context without updating model parameters.

## Key Results
- PH3 increases internal memory usage rate by 44.0% and external context usage rate by 38.5%
- The method improves performance on open-domain QA tasks while resolving knowledge conflicts
- PH3 demonstrates cross-model generalization across eight models (GPT-2 XL, GPT-J, OPT-1.3B/2.7B, Pythia-6.9B/12B, LLaMA2-7B/13B) and cross-format generalization across different dataset formats

## Why This Works (Mechanism)
Knowledge conflicts arise when internal memory and external context provide contradictory information. The paper identifies that conflicting information flows merge through specific attention heads in later layers - memory heads that negatively impact context-based predictions and context heads that negatively impact memory-based predictions. By pruning these conflicting heads using the PH3 method, the model can better control whether to rely on internal memory or external context, effectively resolving the conflict.

## Foundational Learning
- **Path Patching**: A technique to compute importance scores for attention heads by running three passes (original, corrupted, patched) to measure each head's contribution to model output. Needed to identify which heads are causing conflicts. Quick check: Verify importance scores correctly identify heads with opposite effects on memory vs. context predictions.
- **Attention Head Pruning**: Setting specific attention matrices to zero to remove their influence without updating model parameters. Needed to mitigate conflicts without retraining. Quick check: Confirm pruned heads no longer affect model output by measuring changes in memory/context usage rates.
- **Memory Heads vs. Context Heads**: Different types of attention heads that have opposite effects on attribute extraction - memory heads are negative for context prediction while context heads are negative for memory prediction. Needed to understand the conflict mechanism. Quick check: Verify head classification by testing their effects on both memory and context predictions.
- **Knowledge Conflict Resolution**: The process of determining whether to use internal memory or external context when they provide contradictory information. Needed to improve model reliability in real-world applications. Quick check: Measure usage rates for both memory and context before and after pruning to confirm conflict resolution.
- **Cross-entropy Loss for Importance Scoring**: Using cross-entropy loss to measure the impact of each head on model predictions. Needed to quantitatively evaluate head importance. Quick check: Verify that heads with higher absolute importance scores have greater impact on prediction changes.
- **Development Set Pruning Rate Selection**: Using a development set to determine the optimal percentage of heads to prune (k). Needed to balance conflict resolution with preserving model capabilities. Quick check: Monitor performance on both conflict and non-conflict tasks when selecting k.

## Architecture Onboarding
- **Component Map**: Input → Model Layers → Attention Heads → Path Patching (Importance Scoring) → Pruning (Setting Matrices to Zero) → Controlled Output
- **Critical Path**: The path patching method is critical - it identifies which heads to prune by computing importance scores through three passes (original, corrupted, patched). This determines the entire pruning strategy.
- **Design Tradeoffs**: PH3 trades some model capacity (by pruning heads) for better conflict resolution and control over memory vs. context usage. The method avoids parameter updates, making it more efficient than fine-tuning but potentially less precise than retraining.
- **Failure Signatures**: If path patching is implemented incorrectly, importance scores will be wrong, leading to pruning the wrong heads. Over-pruning can cause significant performance degradation on non-conflict tasks. Both issues can be diagnosed by monitoring usage rates and task performance.
- **First Experiments**:
  1. Verify path patching implementation by checking that importance scores correctly identify memory heads (negative for context prediction) and context heads (negative for context prediction)
  2. Test different pruning rates (k) on the development set to find the optimal balance between conflict resolution and performance preservation
  3. Compare the controlled model's performance on non-conflict examples to ensure pruning doesn't degrade capabilities on tasks without knowledge conflicts

## Open Questions the Paper Calls Out
The paper identifies several unresolved questions about knowledge conflict resolution in language models. First, how do memory heads and context heads functionally interact with each other beyond having opposite effects on attribute extraction? Second, what are the long-term effects of pruning attention heads on model capabilities beyond the specific knowledge conflict resolution task? Third, how does the path patching method compare to alternative causal intervention methods in terms of accuracy and computational efficiency? Finally, what are the theoretical limits of knowledge conflict resolution and how do they scale with model size and pre-training data diversity?

## Limitations
- The paper does not provide comprehensive evaluation of PH3's effects on diverse NLP tasks beyond knowledge conflict resolution
- Implementation details for path patching are partially specified, particularly the exact computation of importance scores
- The selection criteria for pruning rate k on the development set is unclear and may vary by model or task
- The paper does not explore potential trade-offs in other model abilities after head pruning

## Confidence
- **High confidence**: The core hypothesis that conflicting information flows through specific memory and context heads in later layers
- **Medium confidence**: The pruning methodology and its effectiveness, as implementation details are partially specified
- **Medium confidence**: The generalization claims (cross-model, cross-relation, cross-format) due to limited description of evaluation methodology

## Next Checks
1. Implement and verify the path patching method on a small model (e.g., GPT-2 small) to confirm that importance scores correctly identify memory heads (negative scores for context prediction) and context heads (negative scores for memory prediction)
2. Conduct ablation studies varying the pruning rate k (e.g., 1%, 3%, 5%) to determine optimal settings and observe the trade-off between conflict mitigation and performance preservation
3. Test the pruned models on non-conflict examples to verify that pruning doesn't degrade performance on tasks without knowledge conflicts, ensuring the pruning is selective rather than destructive