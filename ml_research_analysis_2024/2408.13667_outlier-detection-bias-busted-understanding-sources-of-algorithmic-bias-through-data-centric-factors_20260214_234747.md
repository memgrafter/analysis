---
ver: rpa2
title: 'Outlier Detection Bias Busted: Understanding Sources of Algorithmic Bias through
  Data-centric Factors'
arxiv_id: '2408.13667'
source_url: https://arxiv.org/abs/2408.13667
tags:
- decay
- group
- bias
- outliers
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work conducts an empirical study on the fairness of outlier
  detection (OD) algorithms under different data biases. It injects four types of
  data biases (group size disparity, target under-representation, measurement noise,
  and group membership obfuscation) into simulated datasets and evaluates the fairness
  and performance of four OD models (LOF, iForest, DeepAE, and FairOD).
---

# Outlier Detection Bias Busted: Understanding Sources of Algorithmic Bias through Data-centric Factors

## Quick Facts
- arXiv ID: 2408.13667
- Source URL: https://arxiv.org/abs/2408.13667
- Authors: Xueying Ding; Rui Xi; Leman Akoglu
- Reference count: 40
- Key outcome: All OD models exhibit fairness pitfalls under certain biases, but their susceptibility varies, and OD algorithm bias is not solely a data bias problem

## Executive Summary
This empirical study investigates the fairness of outlier detection algorithms under different data biases. The authors inject four types of data biases (group size disparity, target under-representation, measurement noise, and group membership obfuscation) into simulated datasets and evaluate four OD models (LOF, iForest, DeepAE, and FairOD). The study finds that all OD models exhibit fairness pitfalls under certain biases, although their susceptibility varies. Notably, it demonstrates that OD algorithm bias is not solely a data bias problem, as data properties emerging from bias injection could also be organic characteristics of the input data.

## Method Summary
The study simulates unbiased datasets with 1000 samples per group, 3 types of features (proxy, incriminating, occlusion), 5 dimensions each, and equal base rates of 0.05 or 0.1. Four types of data biases are injected at varying levels: sample size disparity, target under-representation, measurement noise, and membership obfuscation. Four OD models (LOF, iForest, DeepAE, and FairOD) are trained and evaluated on each biased dataset. Fairness is assessed using metrics like flag rate, TPR, FPR, and PPV ratios between groups, while performance is measured using AUROC and F1 scores.

## Key Results
- All OD models exhibit fairness pitfalls under certain biases, although their susceptibility varies
- LOF is most susceptible to membership obfuscation, iForest to sample size disparity, DeepAE to obfuscation, and FairOD to obfuscation despite its fairness enhancements
- Data bias impacts not only fairness but also detection performance of OD models, sometimes severely

## Why This Works (Mechanism)

### Mechanism 1
Outlier detection bias is not solely a data bias problem but arises from misalignment between algorithmic design choices and data properties, which can be either naturally occurring or induced by bias. The study injects known data biases into simulated datasets and evaluates four OD models (LOF, iForest, DeepAE, FairOD) to observe their fairness and performance under these biases. It finds that while all models exhibit fairness pitfalls, their susceptibility varies, and crucially, the data properties that emerge from bias injection can also be natural characteristics of the input data.

Core assumption: Data-centric factors (e.g., group-wise differences in sparsity, base rate, variance, and multi-modality) interact with algorithmic design choices to produce unfairness, regardless of whether these differences are due to bias or natural variation.

Evidence anchors:
- [abstract]: "A key realization is that the data properties that emerge from bias injection could as well be organicâ€”as pertain to natural group differences w.r.t. sparsity, base rate, variance, and multi-modality."
- [section]: "We argue that OD models could remain prone to producing unfair outcomes when group ð‘Ž and ð‘ exhibit different density distributions even in the absence of any group size disparity."
- [corpus]: "The study finds that all OD models exhibit fairness pitfalls under certain biases, although differing in which types of data bias they are more susceptible to."

Break condition: The argument breaks if the injected biases do not accurately reflect real-world data characteristics, or if the algorithmic design choices are inherently robust to all forms of data variation.

### Mechanism 2
Different OD algorithms respond differently to the same data bias, indicating that no single model is universally fair or effective. The study compares four OD models under various data biases. LOF is found to be most susceptible to membership obfuscation, iForest to sample size disparity, DeepAE to obfuscation, and FairOD to obfuscation despite its fairness enhancements. This variation in susceptibility suggests that the choice of OD model must be informed by the specific type of data bias expected in the dataset.

Core assumption: The working assumptions and design choices of each OD algorithm determine how it interacts with data properties, leading to varying degrees of fairness and performance.

Evidence anchors:
- [abstract]: "We find that the OD algorithms under the study all exhibit fairness pitfalls, although differing in which types of data bias they are more susceptible to."
- [section]: "The stark contrast can be explained based on the modeling assumptions of these models, as they interact with density variation."
- [corpus]: "These suggest the lack of a 'winner' detector."

Break condition: This mechanism breaks if the observed differences in susceptibility are due to hyperparameter tuning rather than inherent algorithmic properties.

### Mechanism 3
Data bias can impact not only fairness but also the overall detection performance of OD models, sometimes severely. The study demonstrates that data biases can lead to a decrease in AUROC and F1 scores for OD models. For instance, under membership obfuscation, LOF's performance drops considerably as it mistakes micro-clusters for outliers. Similarly, under variance shift with scattered outliers, all models show reduced performance.

Core assumption: The effectiveness of OD algorithms is contingent on the data distribution being close to their assumptions; when these assumptions are violated by bias, both fairness and performance suffer.

Evidence anchors:
- [abstract]: "In addition, we find that the models behave quite differently against a certain bias depending on the dataset characteristics."
- [section]: "Moreover, data bias impacts not only fairness but also detection performance of OD models; in fact, there are cases when performance drops considerably."
- [corpus]: "iForest's brittleness is also evident from its overall performance falling more drastically with increasing bias."

Break condition: The mechanism breaks if the performance drop is due to poor hyperparameter tuning rather than the inherent interaction between the bias and the algorithm's assumptions.

## Foundational Learning

- Concept: Outlier detection (OD) algorithms and their mechanisms
  - Why needed here: Understanding how different OD algorithms work is crucial to interpreting why they respond differently to data biases.
  - Quick check question: What is the main difference between local (LOF) and global (iForest) outlier detection approaches?

- Concept: Algorithmic fairness metrics and their application in unsupervised learning
  - Why needed here: The study evaluates fairness using metrics like flag rate, TPR, FPR, and PPV, which are essential for assessing disparate impact in OD.
  - Quick check question: How does the concept of demographic parity translate to the unsupervised setting of outlier detection?

- Concept: Data bias and its various forms
  - Why needed here: Recognizing different types of data biases (e.g., sample size disparity, target under-representation, measurement noise, membership obfuscation) is key to understanding their impact on OD algorithms.
  - Quick check question: What is the difference between group sample size bias and target under-representation bias in the context of OD?

## Architecture Onboarding

- Component map: Data simulation -> Bias injection -> OD model evaluation -> Theoretical analysis
- Critical path: The evaluation of OD models under different data biases, as this directly addresses the study's goal of understanding sources of unfairness
- Design tradeoffs: The study uses simulated data to control for bias injection, which limits external validity but allows for precise attribution of unfairness to specific biases
- Failure signatures: Failure occurs if the injected biases do not accurately reflect real-world data characteristics, or if the algorithmic design choices are inherently robust to all forms of data variation
- First 3 experiments:
  1. Evaluate LOF and iForest under group sample size bias to observe their different responses to density variation
  2. Compare DeepAE and FairOD under membership obfuscation bias to assess the effectiveness of fairness enhancements
  3. Analyze the impact of variance shift on all models with scattered outliers to understand the interaction between bias type and data distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of measurement bias (e.g., over-estimation vs. under-estimation) interact with outlier detection algorithms when outliers are scattered?
- Basis in paper: [explicit] The paper studied variance shift and mean shift biases for clustered outliers but only variance shift for scattered outliers, noting the stark difference in model behavior between the two settings.
- Why unresolved: The paper did not provide results for mean shift bias on scattered outliers, leaving uncertainty about whether models would be robust or susceptible to this type of bias in that setting.
- What evidence would resolve it: Empirical results comparing the performance and fairness of OD models under mean shift bias for both clustered and scattered outliers would clarify if models behave similarly or differently in these scenarios.

### Open Question 2
- Question: To what extent does the granularity of sensitive attribute assignment affect fairness in outlier detection when groups naturally exhibit within-group heterogeneity?
- Basis in paper: [explicit] The paper discussed that coarse classifications of large populations into single sensitive attributes could lead to unfairness when groups comprise multiple natural subpopulations, and suggested using more granular sensitive attributes.
- Why unresolved: The paper did not experiment with assigning different sensitive attribute values to natural subpopulations, so the impact of granularity on fairness is unclear.
- What evidence would resolve it: Experiments comparing fairness outcomes when using coarse vs. granular sensitive attributes on datasets with known within-group heterogeneity would demonstrate the effect of granularity.

### Open Question 3
- Question: Can post-processing techniques effectively mitigate unfairness in outlier detection when groups have naturally unequal base rates?
- Basis in paper: [inferred] The paper noted that all models exhibited lower precision for the underprivileged group under target under-representation bias, which mimics unequal base rates, and discussed post-processing as a potential mitigation strategy.
- Why unresolved: The paper did not implement or test post-processing methods, so their effectiveness in this specific fairness scenario is unknown.
- What evidence would resolve it: Empirical results showing the fairness and performance of OD models after applying post-processing techniques like group-specific thresholds or decoupling on datasets with unequal base rates would indicate their efficacy.

## Limitations
- Reliance on synthetic data rather than real-world datasets, which may not fully capture the complexity of actual outlier distributions
- Controlled nature of bias injection may not reflect the nuanced ways bias manifests in practice
- Focus on binary group fairness without addressing intersectional fairness concerns

## Confidence
- High confidence: OD algorithm bias results from the interaction between algorithmic design choices and data properties
- Medium confidence: Specific susceptibility rankings of different OD algorithms
- Low confidence: Generalizability of findings beyond the specific simulation framework

## Next Checks
1. Apply the same bias injection framework to real-world datasets with known fairness issues to verify whether the observed algorithmic susceptibility patterns hold
2. Extend the experimental design to include multi-dimensional group memberships to assess whether fairness issues compound when multiple protected attributes interact
3. Systematically vary hyperparameters for each OD model across all bias conditions to determine whether observed fairness differences are robust to model configuration choices