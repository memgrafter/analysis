---
ver: rpa2
title: 'Shopping Queries Image Dataset (SQID): An Image-Enriched ESCI Dataset for
  Exploring Multimodal Learning in Product Search'
arxiv_id: '2405.15190'
source_url: https://arxiv.org/abs/2405.15190
tags:
- product
- image
- search
- dataset
- products
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Shopping Queries Image Dataset (SQID),
  an extension of the Amazon Shopping Queries Dataset (SQD) enriched with image information
  for 190,000 products. The dataset enables research on multimodal learning techniques
  for improving product search and ranking by leveraging both textual and visual data.
---

# Shopping Queries Image Dataset (SQID): An Image-Enriched ESCI Dataset for Exploring Multimodal Learning in Product Search

## Quick Facts
- arXiv ID: 2405.15190
- Source URL: https://arxiv.org/abs/2405.15190
- Reference count: 12
- Primary result: Image-based ranking approaches outperform text-only methods in product search

## Executive Summary
This paper introduces the Shopping Queries Image Dataset (SQID), an extension of the Amazon Shopping Queries Dataset (SQD) enriched with image information for 190,000 products. The dataset enables research on multimodal learning techniques for improving product search and ranking by leveraging both textual and visual data. SQID includes product image URLs, visual embeddings extracted using CLIP, and textual embeddings of associated search queries. Experimental results demonstrate the value of using multimodal data, with image-based ranking approaches outperforming text-only methods. Combining text and image embeddings further improves ranking performance, with a 2.41% lift in NDCG compared to text-only approaches. The dataset and findings support further research in product search and ranking using multimodal learning.

## Method Summary
The method uses pretrained CLIP models to generate embeddings for both product images and search queries, computing cosine similarity between query and product representations for ranking. The approach compares text-only (CLIP_text), image-only (CLIP_image), and combined multimodal (CLIP_text_score_CLIP_image) ranking strategies. No fine-tuning on ESCI training data was performed, relying instead on general visual-linguistic knowledge captured by CLIP. The dataset combines SQD with scraped Amazon product images, providing both visual and textual embeddings for 190,000 products across 29,627 queries with 181,701 relevance judgements.

## Key Results
- Image-based ranking (CLIP_image) outperforms text-only ranking (CLIP_text) in product search
- Combining text and image embeddings via weighted scoring improves NDCG by 2.41% over text-only approaches
- CLIP-based ranking serves as strong baseline for ESCI benchmark without requiring fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal embeddings improve product search ranking by aligning visual and textual semantic spaces.
- Mechanism: CLIP model learns joint representations for images and text; cosine similarity computed in shared embedding space allows product-image and query-text relevance scoring.
- Core assumption: Visual characteristics (e.g., stripe thickness, pattern) are discriminative features that text metadata may omit.
- Evidence anchors:
  - [abstract]: "experimental results demonstrate the value of using multimodal data, with image-based ranking approaches outperforming text-only methods"
  - [section]: "CLIP model, specifically clip-vit-large-patch14, to embed queries and product images"
  - [corpus]: Weak - related papers discuss image-to-text retrieval but not CLIP-based multimodal ranking specifically.
- Break condition: If product images are missing, generic, or low-quality, visual embeddings may degrade ranking performance.

### Mechanism 2
- Claim: Combining text and image embeddings via weighted scoring improves ranking lift over either modality alone.
- Mechanism: Similarity scores from separate CLIP embeddings (text-only and image-only) are linearly combined using weight w; optimal w varies per query-product pair.
- Core assumption: Text and image modalities capture complementary aspects of product relevance.
- Evidence anchors:
  - [abstract]: "combining text and image embeddings further improves ranking performance, with a 2.41% lift in NDCG compared to text-only approaches"
  - [section]: "CLIP_text_score_CLIP_image reaches a lift of 2.41% compared to the text-only approach (i.e., CLIP_text)"
  - [corpus]: Weak - no direct corpus support for weighted fusion of CLIP embeddings.
- Break condition: If w is fixed globally, the combination may not generalize across diverse query types.

### Mechanism 3
- Claim: CLIP-based ranking can serve as strong baseline for ESCI benchmark even without fine-tuning.
- Mechanism: Pretrained CLIP embeddings map products and queries into same space; ranking based on cosine similarity substitutes for task-specific fine-tuning.
- Core assumption: General visual-linguistic knowledge captured by CLIP is sufficient for product relevance judgments.
- Evidence anchors:
  - [section]: "only rely on pretrained models and consider that fine-tuning models on the ESCI training data... outside the scope of this paper"
  - [section]: "CLIP_image... perform better than CLIP_text"
  - [corpus]: Weak - no corpus papers directly compare CLIP-based product search baselines.
- Break condition: Fine-tuned domain-specific models may outperform CLIP if training data captures niche product attributes.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: CLIP is trained with contrastive objectives to align images and text in shared embedding space.
  - Quick check question: What loss function does CLIP use to align image and text pairs?

- Concept: Cosine similarity ranking
  - Why needed here: Ranking relies on cosine similarity between query and product embeddings.
  - Quick check question: How does cosine similarity behave when embedding norms differ significantly?

- Concept: NDCG metric
  - Why needed here: Evaluation metric for ranking quality using relevance labels E, S, C, I.
  - Quick check question: How is NDCG computed when relevance scores are not binary?

## Architecture Onboarding

- Component map:
  Data ingestion -> Embedding layer -> Ranking layer -> Evaluation

- Critical path:
  1. Load query and product data
  2. Retrieve or generate product images
  3. Generate CLIP embeddings (text-only and image-only)
  4. Compute pairwise similarities
  5. Rank products per query
  6. Evaluate NDCG

- Design tradeoffs:
  - Using pretrained CLIP avoids costly fine-tuning but may miss domain nuances.
  - Combining embeddings adds compute but yields better performance than single modality.
  - Scraping images introduces dependency on Amazon page availability.

- Failure signatures:
  - Low NDCG with CLIP_image alone may indicate missing or low-quality product images.
  - Performance gap between ESCI_baseline and CLIP methods suggests benefit of task-specific fine-tuning.
  - High variance in ranking lift across queries may signal need for adaptive weighting.

- First 3 experiments:
  1. Reproduce ESCI baseline NDCG score with corrected label mapping.
  2. Compare NDCG for CLIP_text vs CLIP_image ranking.
  3. Sweep weight w from 0.0 to 1.0 in CLIP_text_score_CLIP_image and plot NDCG curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning CLIP on product search data compare to using pretrained CLIP embeddings for multimodal ranking?
- Basis in paper: [explicit] The paper states "we consider it outside of the scope of this paper and only use pretrained models in our experiments" and suggests this could be investigated in future work
- Why unresolved: The paper only uses pretrained CLIP models and does not explore fine-tuning on the ESCI dataset
- What evidence would resolve it: Experimental results comparing fine-tuned CLIP models against pretrained CLIP embeddings on the ESCI benchmark

### Open Question 2
- Question: How do different multimodal fusion techniques (e.g., concatenation, attention) compare to simple weighted averaging for combining text and image embeddings?
- Basis in paper: [inferred] The paper only explores weighted averaging for combining text and image representations, but doesn't investigate more sophisticated fusion methods
- Why unresolved: The paper focuses on weighted averaging of similarity scores or rankings but doesn't test more complex fusion architectures
- What evidence would resolve it: Comparative experiments testing various fusion techniques like concatenation, attention mechanisms, or cross-modal transformers

### Open Question 3
- Question: How does the performance of multimodal ranking vary across different product categories (e.g., clothing vs electronics vs furniture)?
- Basis in paper: [inferred] The paper presents overall performance metrics but doesn't analyze performance differences across product categories
- Why unresolved: The paper reports aggregate NDCG scores without examining category-specific performance patterns
- What evidence would resolve it: Category-level performance analysis showing NDCG variations across different product types and identifying which categories benefit most from visual information

## Limitations

- Potential bias from Amazon-specific data and reliance on scraped product images
- Use of pretrained CLIP models without fine-tuning may limit performance on domain-specific product attributes
- No direct comparisons with state-of-the-art fine-tuned multimodal models

## Confidence

- Claim of CLIP_image outperforming CLIP_text: High confidence
- 2.41% NDCG lift from multimodal combination: Medium confidence
- Overall ranking performance claims: Medium confidence

## Next Checks

1. **Reproduce baseline NDCG**: Re-run ESCI baseline ranking using corrected label mapping (S=0.1, C=0.01) to verify reported scores.

2. **Cross-dataset generalization**: Test SQID-based ranking models on a non-Amazon e-commerce dataset to assess generalization beyond the source domain.

3. **Fine-tuning impact study**: Fine-tune CLIP on ESCI training data and compare performance against pretrained-only approaches to quantify potential gains from domain adaptation.