---
ver: rpa2
title: 'DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?'
arxiv_id: '2409.07703'
source_url: https://arxiv.org/abs/2409.07703
tags:
- data
- usage
- february
- tasks
- file
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DSBench, a comprehensive benchmark for evaluating
  data science agents in realistic scenarios. The benchmark includes 466 data analysis
  tasks from ModelOff competitions and 74 data modeling tasks from Kaggle, featuring
  multimodal inputs, long contexts, and end-to-end evaluation requirements.
---

# DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?

## Quick Facts
- arXiv ID: 2409.07703
- Source URL: https://arxiv.org/abs/2409.07703
- Reference count: 40
- Primary result: Even advanced models like GPT-4o and Claude struggle with realistic data science challenges, solving only 34.12% of data analysis tasks and achieving 34.74% Relative Performance Gap on data modeling tasks

## Executive Summary
This paper introduces DSBench, a comprehensive benchmark for evaluating data science agents in realistic scenarios. The benchmark includes 466 data analysis tasks from ModelOff competitions and 74 data modeling tasks from Kaggle, featuring multimodal inputs, long contexts, and end-to-end evaluation requirements. Evaluation of state-of-the-art LLMs, LVLMs, and agents reveals significant performance gaps, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap on data modeling tasks. The authors also propose a novel Relative Performance Gap metric to normalize evaluation across different modeling tasks.

## Method Summary
DSBench evaluates data science agents using 466 data analysis tasks from ModelOff competitions and 74 data modeling tasks from Kaggle competitions. The benchmark incorporates multimodal inputs including text, images, Excel files, and tables, requiring agents to process long contexts and perform end-to-end reasoning. Evaluation uses task-level accuracy for data analysis tasks and introduces the Relative Performance Gap (RPG) metric for data modeling tasks to normalize across different evaluation metrics. The study tests state-of-the-art LLMs, LVLMs, and agent frameworks including GPT-4o, Claude, Gemini, and AutoGen with various base models.

## Key Results
- GPT-4o achieved 34.12% accuracy on data analysis tasks, while GPT-4o with vision capabilities improved to 46.77%
- The best-performing agent (AutoGen with GPT-4o) achieved only 34.74% Relative Performance Gap on data modeling tasks
- Even advanced models struggle with generating executable code and producing correct submission files for data modeling tasks
- Cost analysis shows significant variation, with GPT-4o costing $0.12 per data analysis task and GPT-4o with vision costing $0.54

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DSBench benchmark works because it provides realistic, multimodal data science tasks that require end-to-end reasoning and tool integration.
- Mechanism: By incorporating tasks from ModelOff and Kaggle competitions, DSBench presents complex scenarios with long contexts, multimodal inputs (text, images, Excel files, tables), and multi-step reasoning requirements that mirror real-world data science workflows.
- Core assumption: Realistic task complexity and multimodal inputs are essential for evaluating true data science agent capabilities.
- Evidence anchors:
  - [abstract] "DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks"
  - [section] "The tasks included must be sufficiently complex to simulate real scenarios, yet the predictions made by these models must remain straightforward to verify"
  - [corpus] Weak - neighbor papers focus on LLM agents but don't specifically address multimodal data science benchmarks
- Break condition: If task complexity is reduced or multimodal inputs are simplified, the benchmark would lose its ability to differentiate between agent capabilities and would become similar to existing simplified benchmarks.

### Mechanism 2
- Claim: The Relative Performance Gap (RPG) metric effectively normalizes evaluation across different data modeling tasks with varying metrics.
- Mechanism: RPG calculates the gap between agent performance and human expert performance relative to baseline performance, allowing meaningful comparison across tasks that use different evaluation metrics (accuracy, RMSE, F1, etc.).
- Core assumption: Performance gaps relative to human baseline provide a consistent measure across heterogeneous tasks.
- Evidence anchors:
  - [abstract] "we further propose the Relative Performance Gap (RPG) to normalize the different metrics in our data modeling tasks"
  - [section] "pi is the performance of the predicted submission file for the i-th competition and gi is the highest performance value = for the i-th competition. bi is the performance of a baseline"
  - [corpus] Weak - neighbor papers don't discuss metric normalization approaches for data science tasks
- Break condition: If baseline performance varies significantly across tasks or if the highest human performance isn't representative, RPG could become misleading.

### Mechanism 3
- Claim: The end-to-end evaluation approach reveals the true capabilities of data science agent systems.
- Mechanism: By requiring agents to complete entire data science workflows (from understanding requirements through to final predictions) rather than just code generation, DSBench tests the full system including tool integration, reasoning, and debugging capabilities.
- Core assumption: End-to-end task completion is a more comprehensive measure of agent capability than isolated subtasks.
- Evidence anchors:
  - [abstract] "performing end-to-end data modeling tasks"
  - [section] "our task is more challenging and demands a broader range of agent capabilities, such as model design, code implementation, and self-debugging"
  - [corpus] Weak - neighbor papers focus on specific agent capabilities but don't emphasize end-to-end workflow evaluation
- Break condition: If the evaluation environment doesn't properly support the required tools or if partial credit is given for incomplete workflows, the end-to-end approach would lose its effectiveness.

## Foundational Learning

- Concept: Multimodal data processing and integration
  - Why needed here: Data science tasks often involve combining information from text descriptions, structured tables, images, and Excel files
  - Quick check question: How would you extract relevant information from a task description that contains both textual instructions and an accompanying Excel file?

- Concept: End-to-end machine learning workflow
  - Why needed here: Data modeling tasks require understanding requirements, data preprocessing, model selection, training, evaluation, and prediction generation
  - Quick check question: What are the key steps in going from raw data to a submission file for a Kaggle competition?

- Concept: Metric normalization and relative performance comparison
  - Why needed here: Different tasks use different evaluation metrics (accuracy, RMSE, F1, etc.), requiring a standardized way to compare performance
  - Quick check question: How would you compare the performance of two models on tasks that use completely different evaluation metrics?

## Architecture Onboarding

- Component map: Task ingestion -> Multimodal processor -> Agent interface -> Execution environment -> RPG calculator
- Critical path: 1. Load task context and data files 2. Agent processes task and generates solution 3. Execute solution in controlled environment 4. Compare results against ground truth 5. Calculate RPG for data modeling tasks 6. Aggregate results for benchmarking
- Design tradeoffs:
  - Comprehensive vs. simplified tasks: More realistic tasks are harder to automate evaluation for
  - Open-ended vs. constrained environments: More freedom allows creative solutions but harder to control
  - Granular vs. holistic evaluation: Detailed metrics provide more insight but increase complexity
- Failure signatures:
  - Timeout errors during execution indicate computational complexity issues
  - Format mismatches suggest problems with instruction following
  - Consistently poor performance across certain task types indicates capability gaps
  - High variance in results suggests instability in agent approaches
- First 3 experiments: 1. Run a single data analysis task with a simple LLM to verify basic functionality 2. Test RPG calculation with synthetic data to ensure metric normalization works 3. Evaluate a complete data modeling task end-to-end to identify integration issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can data science agents better handle complex, multi-modal data inputs that combine text, tables, and images in realistic scenarios?
- Basis in paper: [explicit] The paper highlights that DSBench tasks include various modalities such as images, Excel files, and tables, which are challenging for current agents to process effectively.
- Why unresolved: Current models struggle with understanding and integrating multimodal data, especially when the data is lengthy and requires cross-modal reasoning.
- What evidence would resolve it: Improved performance on DSBench's data analysis tasks with multimodal inputs, demonstrating the ability to accurately interpret and combine information across different data types.

### Open Question 2
- Question: What strategies can be developed to improve the execution and deployment of models in end-to-end data modeling tasks?
- Basis in paper: [explicit] The paper identifies that even advanced models like GPT-4o and Claude struggle with generating executable code and producing correct submission files for data modeling tasks.
- Why unresolved: Existing approaches often fail to generate bug-free code or correctly format submission files, limiting their practical utility.
- What evidence would resolve it: Successful completion of a higher percentage of DSBench data modeling tasks with accurate and executable code generation, leading to improved Relative Performance Gap (RPG) scores.

### Open Question 3
- Question: How can the evaluation metrics for data science agents be standardized to fairly assess performance across diverse tasks and environments?
- Basis in paper: [explicit] The paper introduces the Relative Performance Gap (RPG) metric to normalize evaluation across different data modeling tasks, highlighting the need for standardized assessment methods.
- Why unresolved: Different tasks use varying metrics and evaluation dimensions, making it difficult to compare agent performance consistently.
- What evidence would resolve it: Adoption and validation of a standardized evaluation framework across multiple data science benchmarks, ensuring fair and comparable performance assessments.

## Limitations
- The benchmark focuses on competition scenarios that may not fully represent real-world enterprise data science workflows with domain expertise and stakeholder communication requirements
- The study primarily evaluates model capabilities without addressing computational costs and infrastructure requirements needed for production deployment
- The RPG metric assumes that human baseline performance is consistently available and representative across all task types

## Confidence

**High Confidence** (established by direct experimental evidence):
- The measured performance gaps between current models and human experts (34.12% accuracy on data analysis, 34.74% RPG on data modeling)
- The effectiveness of the end-to-end evaluation approach in revealing agent capabilities and limitations

**Medium Confidence** (supported by mechanism but limited empirical validation):
- The claim that multimodal inputs and long contexts are essential for realistic evaluation
- The assumption that ModelOff and Kaggle tasks represent comprehensive data science challenges

**Low Confidence** (inferred from related work without direct validation):
- The assertion that current agents cannot effectively handle real-world data science workflows
- The generalizability of results to non-competitive data science scenarios

## Next Checks

1. **Cross-domain validation**: Test DSBench agents on non-competitive data science tasks from enterprise domains (healthcare, finance, manufacturing) to assess generalizability beyond competition scenarios.

2. **Cost-benefit analysis**: Measure the total cost of ownership for running these agents at scale, including API costs, infrastructure requirements, and human oversight time, to determine practical deployment feasibility.

3. **Incremental capability assessment**: Evaluate whether agents can successfully complete individual subtasks (data preprocessing, feature engineering, model selection) in isolation before attempting full end-to-end workflows to identify specific capability bottlenecks.