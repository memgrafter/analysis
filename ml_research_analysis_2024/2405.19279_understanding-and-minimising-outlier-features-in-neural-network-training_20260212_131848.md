---
ver: rpa2
title: Understanding and Minimising Outlier Features in Neural Network Training
arxiv_id: '2405.19279'
source_url: https://arxiv.org/abs/2405.19279
tags:
- training
- block
- step
- kurtosis
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates Outlier Features (OFs) in neural networks,
  particularly in transformers, which are neurons with activation magnitudes significantly
  exceeding the average. OFs hinder quantization, limiting efficiency gains.
---

# Understanding and Minimising Outlier Features in Neural Network Training

## Quick Facts
- **arXiv ID:** 2405.19279
- **Source URL:** https://arxiv.org/abs/2405.19279
- **Reference count:** 40
- **Primary result:** OP block + SOAP optimizer achieves 14.87 int8 weight-and-activation perplexity vs 63.4 with standard Pre-Norm + Adam

## Executive Summary
This paper investigates Outlier Features (OFs) in neural networks, particularly transformers, which are neurons with activation magnitudes significantly exceeding the average. OFs hinder quantization and limit efficiency gains in deployment. The authors propose several solutions to minimize OFs without compromising convergence speed or training stability, demonstrating effectiveness at scales up to 7B parameters.

## Method Summary
The authors introduce the Outlier Protected (OP) transformer block, which removes normalization layers and adds entropy regulation to prevent attention entropy collapse. They also highlight the importance of non-diagonal preconditioning optimizers like SOAP, which significantly reduce OFs compared to diagonal preconditioners like Adam. The combination of OP block architecture and SOAP optimizer achieves substantial improvements in quantization performance while maintaining training efficiency.

## Key Results
- OP block achieves 14.87 int8 weight-and-activation perplexity, compared to 63.4 with standard Pre-Norm model and Adam on OPT-125m models
- Non-diagonal preconditioners like SOAP significantly reduce OFs compared to diagonal preconditioners like Adam
- The authors demonstrate their findings at scales up to 7B parameters
- Signal propagation degradation during training leads to higher kurtosis and more severe OFs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Normalization layers contribute to Outlier Feature Emergence because they introduce scale invariance and trainable parameters that amplify neuron activation disparities during training.
- **Mechanism:** Pre-Norm and Post-Norm layers, by centering and scaling activations, inadvertently create conditions where some neurons grow disproportionately larger than others. Even Norm variants without trainable parameters still exhibit OFE.
- **Core assumption:** The act of normalization, not just trainable parameters, introduces a mechanism enabling large activation variations.
- **Evidence anchors:** Abstract mentions minimizing OFs without loss of convergence speed; section states removing normalization minimizes OFs.

### Mechanism 2
- **Claim:** Signal propagation degradation during training leads to higher kurtosis and thus more severe Outlier Features.
- **Mechanism:** Poor signal propagation is characterized by large off-diagonal values in the input-wise Gram matrix, leading to rank collapse. This causes pressure on feature kurtosis to increase.
- **Core assumption:** The connection between signal propagation and kurtosis holds throughout training.
- **Evidence anchors:** Abstract mentions findings at scales up to 7B parameters; section suggests OFs occur partly due to inherent task nature.

### Mechanism 3
- **Claim:** Large diagonal adaptive learning rates are crucial for Outlier Feature Emergence.
- **Mechanism:** AdamW and similar diagonal preconditioners apply per-parameter adaptive learning rates that can disproportionately amplify large activations, leading to OFs. Non-diagonal preconditioners like SOAP mitigate this by rotating the parameter space.
- **Core assumption:** The diagonality of the preconditioner, not just its adaptivity, is key to OFE.
- **Evidence anchors:** Abstract mentions combination of OP block and SOAP achieving 14.87 int8 perplexity; section highlights large diagonal adaptive learning rates as crucial for OFs.

## Foundational Learning

- **Concept:** Layer Normalization and its variants (RMSNorm, SRMSNorm)
  - **Why needed here:** Understanding how different normalization layers affect OFE is central to the paper's findings.
  - **Quick check question:** How does SRMSNorm differ from Layer Normalization, and why does it still lead to OFE?

- **Concept:** Signal Propagation Theory
  - **Why needed here:** The paper establishes a connection between signal propagation and OFE.
  - **Quick check question:** What is rank collapse in the context of signal propagation, and how does it relate to OFE?

- **Concept:** Diagonal vs Non-Diagonal Preconditioners
  - **Why needed here:** The paper highlights the importance of non-diagonal preconditioners like SOAP.
  - **Quick check question:** What is the key difference between diagonal and non-diagonal preconditioners, and how does this difference affect OFE?

## Architecture Onboarding

- **Component map:** Pre-Norm Transformer Block -> Post-Norm Transformer Block -> Outlier Protected (OP) Block -> QK-Norm -> SOAP Optimizer

- **Critical path:** 1) Understand role of normalization layers in OFE 2) Implement OP block and compare to Pre-Norm/Post-Norm 3) Experiment with different optimizers 4) Analyze connection between signal propagation and OFE

- **Design tradeoffs:** Normalization layers improve training stability but contribute to OFE; non-diagonal preconditioners reduce OFE but may have higher computational overhead; entropy regulation mechanisms prevent attention entropy collapse but may add complexity

- **Failure signatures:** High kurtosis values indicate severe OFE; rank collapse in input-wise Gram matrix suggests poor signal propagation; training instability may indicate issues with entropy regulation

- **First 3 experiments:** 1) Implement and compare Pre-Norm, Post-Norm, and OP blocks on small transformer model 2) Train same model with AdamW and SOAP optimizers 3) Analyze signal propagation and kurtosis values during training

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do Outlier Features (OFs) emerge during neural network training, and what is the precise mechanism behind their formation?
- **Basis in paper:** The paper investigates emergence of OFs and their causes during training, noting poor signal propagation leads to higher kurtosis and more severe OFs.
- **Why unresolved:** While correlation between signal propagation and OFs is established, the exact mathematical framework predicting OF formation during training remains incomplete.
- **What evidence would resolve it:** A comprehensive mathematical theory that models feature learning behavior during training, capturing architectural and optimizer dependencies.

### Open Question 2
- **Question:** Can the Outlier Protected (OP) transformer block be effectively scaled to larger models and different tasks beyond language modeling?
- **Basis in paper:** The paper demonstrates effectiveness of OP block at scales up to 7B parameters and in language modeling tasks.
- **Why unresolved:** Experiments are limited to language modeling datasets; it's unclear how OP block would perform in tasks like computer vision or multimodal models.
- **What evidence would resolve it:** Experiments applying OP block to diverse tasks and model scales, showing consistent performance improvements across different domains.

### Open Question 3
- **Question:** What is the impact of Outlier Features on low-precision training, and can the proposed solutions effectively mitigate these issues?
- **Basis in paper:** The paper highlights hindrance OFs pose to quantization and presents solutions like OP block and SOAP optimizer that improve quantization performance.
- **Why unresolved:** While improved quantization is shown, long-term impact on low-precision training efficiency and model performance at scale is not fully explored.
- **What evidence would resolve it:** Extensive experiments on low-precision training with various architectures and optimizers, demonstrating consistent gains in efficiency and performance.

## Limitations
- Claims about normalization layers causing OFE rely heavily on empirical observations rather than theoretical proof
- The effectiveness of non-diagonal preconditioners like SOAP needs validation across diverse model architectures beyond transformers
- The mathematical derivation connecting signal propagation to kurtosis is not fully explicit

## Confidence
- **High confidence:** Claims about OP block performance and int8 quantization results are empirically demonstrated
- **Medium confidence:** The mechanism linking normalization layers to OFE is supported by ablation studies but lacks theoretical grounding
- **Medium confidence:** The signal propagation â†’ kurtosis relationship is observed empirically but the mathematical derivation is not fully explicit

## Next Checks
1. **Cross-architecture validation:** Test whether OP block and SOAP optimizer benefits extend to CNN and MLP architectures where OFE also occurs
2. **Mathematical formalization:** Derive the exact relationship between input-wise Gram matrix properties and activation kurtosis to validate the signal propagation mechanism
3. **Long-range dependency analysis:** Evaluate whether OP block performance scales to tasks requiring very long-range dependencies, where normalization layers typically provide crucial stability