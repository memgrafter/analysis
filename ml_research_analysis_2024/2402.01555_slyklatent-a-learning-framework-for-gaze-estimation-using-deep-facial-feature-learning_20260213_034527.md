---
ver: rpa2
title: 'SLYKLatent: A Learning Framework for Gaze Estimation Using Deep Facial Feature
  Learning'
arxiv_id: '2402.01555'
source_url: https://arxiv.org/abs/2402.01555
tags:
- gaze
- estimation
- facial
- slyklatent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SLYKLatent, a novel framework that addresses
  appearance instability and domain generalization challenges in gaze estimation by
  combining self-supervised learning with a patch-based tri-branch network and an
  inverse explained variance-weighted loss function. The framework extracts rich facial
  representations from full-face and eye patch images separately during self-supervised
  pretraining, then fuses these features during downstream fine-tuning using bottleneck
  layers.
---

# SLYKLatent: A Learning Framework for Gaze Estimation Using Deep Facial Feature Learning

## Quick Facts
- arXiv ID: 2402.01555
- Source URL: https://arxiv.org/abs/2402.01555
- Reference count: 40
- Key outcome: SLYKLatent achieves state-of-the-art performance on multiple benchmark datasets, improving Gaze360 results by 10.9%, MPIIFaceGaze by 3.8%, and ETH-XGaze by 11.6%.

## Executive Summary
SLYKLatent addresses key challenges in gaze estimation—appearance instability, domain generalization, and heteroscedastic uncertainty—through a novel framework that combines self-supervised learning with a patch-based tri-branch network. The approach leverages mBYOL pretraining to extract rich facial representations, then refines these features using bottleneck modules that process face and eye patches separately. An inverse explained variance-weighted loss function further enhances performance by emphasizing larger estimation errors. The framework demonstrates strong adaptability across multiple datasets and even generalizes to facial expression recognition tasks.

## Method Summary
SLYKLatent operates in two phases: self-supervised pretraining followed by downstream fine-tuning. The pretraining phase uses mBYOL (a modified BYOL architecture) with augmentation transformations including flip, blur, grayscale, and rotation. A local-global branch structure processes full-face images through Inception-ResNet-V2 while separately extracting features from left and right eye patches using three convolutional layers each. During fine-tuning, a Patch Module Network (PMN) processes the face image through the SSL encoder and bottleneck modules, while eye patches are processed through separate bottleneck modules. Features are concatenated and passed through dataset-specific MLPs to predict gaze vectors. The loss function incorporates inverse explained variance weighting to emphasize larger errors.

## Key Results
- SLYKLatent achieves a 10.9% improvement on Gaze360 benchmark compared to previous methods
- Surpasses top MPIIFaceGaze results with 3.8% better performance
- Leads on a subset of ETH-XGaze by 11.6% improvement
- Demonstrates strong adaptability with 86.4% accuracy on RAF-DB and 60.9% on AffectNet for facial expression recognition

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised pretraining via mBYOL extracts rich facial representations robust to appearance variations before supervised fine-tuning. The mBYOL framework modifies BYOL with augmentation transformations and a local-global branch architecture. The global branch processes full faces while local branches extract features from eye patches independently. Multi-head attention layers emphasize important facial regions. During pretraining, the online network learns to predict target network projections, aligning features from different views without labels.

### Mechanism 2
The Patch Module Network (PMN) with bottleneck layers extracts complementary features from facial and eye patches that improve gaze estimation accuracy. During fine-tuning, PMN processes face images through the SSL encoder and a face bottleneck module, while eye patches go through left and right eye bottleneck modules. Each bottleneck module consists of three convolutional layers with batch normalization and ReLU activation, followed by adaptive average pooling and a fully connected layer producing 512-dimensional feature vectors. The concatenated features are passed through an MLP to predict gaze vectors.

### Mechanism 3
Inverse explained variance weighting in the loss function emphasizes larger gaze estimation errors, improving model accuracy. The loss computes mean absolute error between predicted and actual gaze vectors, then calculates explained variance (VEX = 1 - SSE/SST where SSE is sum of squared errors and SST is total sum of squares). The weight for each sample is ω = 1 - VEX = SSE/SST, and the final loss is LSU P = L · (ω + 1). This weighting scheme makes the model prioritize minimizing larger errors.

## Foundational Learning

- **Concept**: Self-supervised learning through contrastive methods (BYOL variant)
  - **Why needed here**: Gaze estimation datasets are expensive to annotate and limited in size. Self-supervised pretraining allows the model to learn rich facial representations from unlabeled facial expression datasets before fine-tuning on gaze data.
  - **Quick check question**: How does the mBYOL architecture prevent feature collapse without negative pairs?

- **Concept**: Patch-based feature extraction and fusion
  - **Why needed here**: Gaze estimation requires both global facial context and local eye-specific information. Separate processing of face and eye patches captures complementary information that improves accuracy.
  - **Quick check question**: Why use bottleneck modules instead of direct concatenation of raw features?

- **Concept**: Domain generalization through augmentation and equivariance
  - **Why needed here**: Gaze estimation models must perform well across diverse lighting conditions, head poses, and facial expressions. Augmentation transformations and equivariance considerations help the model generalize to unseen domains.
  - **Quick check question**: How do the augmentation transformations specifically address equivariance shifts?

## Architecture Onboarding

- **Component map**: Face image → mBYOL encoder → face bottleneck → concatenated with eye bottleneck features → MLP → gaze vector
- **Critical path**: Face image → mBYOL encoder → face bottleneck → concatenated with eye bottleneck features → MLP → gaze vector
- **Design tradeoffs**:
  - Complexity vs performance: mBYOL with multiple branches and attention layers vs simpler architectures
  - Computation vs accuracy: PMN bottleneck modules vs direct feature concatenation
  - Generalization vs overfitting: Augmentation transformations vs training on limited gaze data
- **Failure signatures**:
  - Poor performance on appearance variations: Check augmentation effectiveness
  - Instability during training: Check EMA momentum and learning rate schedule
  - Overfitting to training data: Check bottleneck module capacity and regularization
- **First 3 experiments**:
  1. Ablation study: Remove mBYOL modifications (use basic BYOL) to quantify improvement from architectural changes
  2. Ablation study: Remove PMN to measure contribution of bottleneck modules
  3. Ablation study: Remove inverse explained variance weighting to assess loss function impact

## Open Questions the Paper Calls Out
- How does the SLYKLatent framework perform in real-world scenarios with extreme occlusions or low-quality images?
- What is the computational overhead of SLYKLatent compared to other state-of-the-art gaze estimation methods?
- How well does SLYKLatent generalize to gaze estimation tasks involving non-frontal faces or extreme head poses?

## Limitations
- Exact augmentation probability values for each transformation in the mBYOL pipeline are not specified
- Specific EMA momentum schedule and learning rate annealing criteria for each dataset are unclear
- Performance on extreme appearance variations (severe occlusions, dramatic lighting changes) remains untested

## Confidence
- **High confidence** in overall framework design and reported performance improvements
- **Medium confidence** in specific implementation details of mBYOL modifications and bottleneck architectures
- **Low confidence** in generalization claims beyond tested datasets

## Next Checks
1. **Ablation Study**: Systematically remove each component (mBYOL modifications, PMN, inverse explained variance weighting) to quantify their individual contributions to performance improvements
2. **Robustness Testing**: Evaluate the framework on extreme appearance variation subsets (severe low-light conditions, heavy occlusions) to assess true domain generalization capabilities
3. **Reproducibility Audit**: Implement the framework using only provided specifications and publicly available datasets, documenting any ambiguities or missing details that prevent exact replication