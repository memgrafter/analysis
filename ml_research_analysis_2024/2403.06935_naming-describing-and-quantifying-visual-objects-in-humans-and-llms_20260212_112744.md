---
ver: rpa2
title: Naming, Describing, and Quantifying Visual Objects in Humans and LLMs
arxiv_id: '2403.06935'
source_url: https://arxiv.org/abs/2403.06935
tags:
- human
- image
- quantifiers
- noun
- some
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether vision-language models (VLLMs)
  can mimic human variability in naming and describing objects in images. It evaluates
  three models (FROMAGe, BLIP-2, LLaVA) on tasks involving common nouns, attributes,
  and quantifiers using datasets with human response distributions.
---

# Naming, Describing, and Quantifying Visual Objects in Humans and LLMs

## Quick Facts
- arXiv ID: 2403.06935
- Source URL: https://arxiv.org/abs/2403.06935
- Authors: Alberto Testoni; Juell Sprott; Sandro Pezzene
- Reference count: 20
- This paper investigates whether vision-language models (VLLMs) can mimic human variability in naming and describing objects in images.

## Executive Summary
This study evaluates whether vision-language models can replicate human variability in naming and describing objects in images. Three models (FROMAGe, BLIP-2, LLaVA) are tested on common nouns, attributes, and quantifier tasks using datasets with human response distributions. Results show moderate correlation for object naming and color attributes but failure in quantifier selection, attributed to poor counting abilities. The research highlights that while VLLMs can partially mimic human naming variability, they struggle with high-level reasoning tasks requiring visual counting.

## Method Summary
The study evaluates three VLLMs (FROMAGe, BLIP-2, LLaVA) on three datasets using zero-shot generation with nucleus sampling (p=0.9, t=0.5). Models generate 20 samples per image/object, filtering ill-formed outputs. Performance is measured by comparing model output distributions to human response distributions using Pearson correlation and inverse Jensen-Shannon divergence. Tasks include object naming (ManyNames), color/texture attribute saliency (NOUN), and quantifier selection (QUANT).

## Key Results
- VLLMs achieve moderate correlation (around 0.5) with human distributions for common object naming and color attribute tasks
- All models fail to correctly assign quantifiers, likely due to poor counting abilities
- Model performance on quantifiers varies by proportion of targets, with different biases (FROMAGe→"many", BLIP-2→extremes, LLaVA→"some")

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models can partially mimic human naming variability for common objects and color attributes.
- Mechanism: VLLMs generate multiple samples per image using nucleus sampling, and the distribution of generated nouns/attributes is compared to human frequency distributions. High Pearson correlation indicates successful mimicry of human variability.
- Core assumption: Nucleus sampling with p=0.9 produces diverse enough outputs to reflect human variability in naming.
- Evidence anchors:
  - [abstract]: "Results show moderate correlation for object naming and color attributes"
  - [section]: "The results for ManyNames and NOUN (color saliency) show a clear trend: all the models correlate, to some extent, with human production, with LLaVA obtaining the highest correlations for both tasks (around 0.5)"
  - [corpus]: Weak - corpus doesn't directly address the sampling diversity mechanism
- Break condition: If nucleus sampling fails to produce diverse outputs (e.g., all samples converge to same noun), the correlation with human variability would collapse.

### Mechanism 2
- Claim: All models fail to assign quantifiers correctly due to poor counting abilities.
- Mechanism: Quantifier assignment requires comparing quantities of objects in an image and mapping that to linguistic quantifiers like "few" or "many". Models struggle with this because they cannot accurately count objects.
- Core assumption: Successful quantifier assignment depends on accurate visual counting and quantity comparison.
- Evidence anchors:
  - [abstract]: "all of them fail to assign quantifiers, a task that requires more accurate, high-level reasoning"
  - [section]: "we hypothesize that the reason behind this failure stems from the poor 'counting' skills of the models"
  - [corpus]: Weak - corpus doesn't contain direct evidence about counting failures
- Break condition: If models develop robust counting mechanisms, they might succeed at quantifier assignment even if other reasoning aspects remain weak.

### Mechanism 3
- Claim: Model performance varies by proportion of targets in the scene for quantifier tasks.
- Mechanism: Different models have different biases toward specific quantifiers (FROMAGe → "many", BLIP-2 → extremes, LLaVA → "some"), which affects their correlation with human distributions depending on the proportion of animals in the image.
- Core assumption: Model quantifier selection is driven more by internal biases than by actual visual scene analysis.
- Evidence anchors:
  - [section]: "While BLIP-2 performs relatively well on the 'extreme' proportions... LLaVA excels at intermediate proportions, and FROMAGe performs better on proportions above 50%"
  - [section]: "FROMAGe has a strong bias towards selecting the quantifier 'many'; BLIP-2 frequently selects the extreme quantifiers 'none' and 'all'... LLaVA has a bias towards selecting the quantifier 'some'"
  - [corpus]: Weak - corpus doesn't provide evidence about internal model biases
- Break condition: If models' quantifier selection becomes proportional to actual scene content rather than biased, performance would improve across all proportions.

## Foundational Learning

- Concept: Distribution comparison metrics (Jensen-Shannon divergence, Pearson correlation)
  - Why needed here: To quantify how well model-generated distributions match human response distributions for naming and quantifier tasks
  - Quick check question: If a model generates "chair" 70% of the time and "sofa" 30% of the time for an object, while humans use these labels 60% and 40% respectively, would the correlation be positive or negative?

- Concept: Nucleus sampling and its role in generating diverse outputs
  - Why needed here: The study relies on sampling multiple times from models to mimic the variability in human responses
  - Quick check question: What would happen to the variability analysis if top_p was set to 0.1 instead of 0.9?

- Concept: Visual grounding and counting in multimodal models
  - Why needed here: The failure on quantifier tasks is attributed to poor counting abilities, requiring understanding of how VLLMs process visual information
  - Quick check question: If a model consistently confuses 3 objects with 7 objects in an image, how would this affect its quantifier selection?

## Architecture Onboarding

- Component map: Image → Visual features → Language model context → Text generation → Post-processing (filtering) → Distribution analysis
- Critical path: Image → Visual features → Language model context → Text generation → Post-processing (filtering) → Distribution analysis
- Design tradeoffs: Zero-shot prompting vs. fine-tuning (zero-shot preserves generalization but may underperform on specialized tasks like counting), nucleus sampling (p=0.9 balances diversity and coherence but may still be too conservative)
- Failure signatures: Low correlation with human distributions indicates poor mimicry of human variability; consistent bias toward specific quantifiers suggests internal model preferences overriding visual input; counting errors manifest as systematic misassignment of quantities
- First 3 experiments:
  1. Test different nucleus sampling parameters (p=0.7, 0.9, 0.95) to find optimal diversity for mimicking human variability
  2. Evaluate a fine-tuned version of the model on quantifier tasks to determine if counting can be improved through training
  3. Compare model outputs against ground truth object counts in images to quantify counting accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt formulations affect VLLM performance on quantifier assignment tasks?
- Basis in paper: [explicit] The paper mentions experimenting with revised prompts for the QUANT task, noting a slight improvement with a more detailed instruction, but states that exploring which prompts work best was beyond the scope.
- Why unresolved: The paper only provides preliminary insights on prompt variations, focusing on object naming and description tasks rather than systematically exploring prompt engineering for quantifier tasks.
- What evidence would resolve it: Systematic experiments comparing multiple prompt formulations for quantifier tasks, measuring correlation with human responses and identifying optimal prompt structures.

### Open Question 2
- Question: Why do VLLMs struggle with texture attribute generation compared to color attributes?
- Basis in paper: [inferred] The paper notes that none of the models achieved statistically significant correlation for texture saliency, while color saliency showed moderate correlation, suggesting models generate texture attributes less frequently than humans.
- Why unresolved: The paper hypothesizes that texture attributes may be less common in training data or harder for models to generate, but does not conduct an in-depth analysis of this discrepancy.
- What evidence would resolve it: Comparative analysis of texture vs. color representation in training data, examination of model attention mechanisms for these attributes, and experiments testing whether augmenting texture descriptions in training improves performance.

### Open Question 3
- Question: What specific limitations in counting abilities cause VLLMs to fail at quantifier assignment?
- Basis in paper: [explicit] The paper concludes that poor counting skills are likely responsible for failure in quantifier assignment, supported by qualitative examples showing models struggle to count animals accurately.
- Why unresolved: While the paper demonstrates counting failures and correlates them with quantifier assignment problems, it does not investigate the underlying mechanisms of counting failures or potential architectural solutions.
- What evidence would resolve it: Detailed error analysis of counting failures across different image types, comparison with specialized counting models, and experiments testing whether integrating explicit counting modules improves quantifier assignment accuracy.

## Limitations
- Model Sampling Diversity: Nucleus sampling with p=0.9 may not optimally capture human variability in naming
- Ground Truth Counting Verification: Attribution of quantifier failures to counting is based on qualitative reasoning rather than direct measurement
- Dataset Representativeness: Evaluation relies on three specific datasets that may not generalize to broader visual domains

## Confidence

**High Confidence**: The correlation results for common object naming and color attribute tasks (Pearson correlations around 0.5) are directly measurable and well-supported by the data.

**Medium Confidence**: The explanation for quantifier task failures (poor counting abilities) is plausible but not directly tested.

**Low Confidence**: The comparison of model performance across different proportion levels in the QUANT dataset relies on interpretation of correlation patterns that may be influenced by multiple confounding factors.

## Next Checks

1. **Counting Accuracy Test**: Design a controlled experiment where models are explicitly asked to count objects in images with known quantities, measuring accuracy across different numbers and object types to directly validate the counting hypothesis.

2. **Sampling Parameter Sweep**: Systematically vary nucleus sampling parameters (p=0.7, 0.8, 0.9, 0.95) and evaluate how output diversity affects correlation with human distributions, identifying optimal parameters for mimicking human variability.

3. **Cross-Dataset Generalization**: Apply the evaluation methodology to additional datasets with different object types and scene complexities (e.g., COCO, Visual Genome) to test whether the observed patterns of model success and failure generalize beyond the specific datasets used.