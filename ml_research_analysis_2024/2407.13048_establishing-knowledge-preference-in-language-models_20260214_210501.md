---
ver: rpa2
title: Establishing Knowledge Preference in Language Models
arxiv_id: '2407.13048'
source_url: https://arxiv.org/abs/2407.13048
tags:
- knowledge
- answer
- instruction
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formulates the problem of knowledge preference in language
  models, where models must decide among their own parametric knowledge, retrieved
  contextual knowledge, and user-provided instruction knowledge. The authors define
  a three-level hierarchical preference (instruction context parametric) and create
  a benchmark by adapting existing datasets (IfQA, MQuAKE, MRQA) to evaluate this
  preference.
---

# Establishing Knowledge Preference in Language Models

## Quick Facts
- arXiv ID: 2407.13048
- Source URL: https://arxiv.org/abs/2407.13048
- Reference count: 40
- Primary result: 7B model fine-tuned on few thousand synthesized examples achieves 89.36% F1 on counterfactual knowledge editing vs 28.48% baseline

## Executive Summary
This paper addresses the critical problem of knowledge preference in language models, where models must decide between their own parametric knowledge, retrieved contextual knowledge, and user-provided instruction knowledge. The authors formulate a three-level hierarchical preference (instruction > context > parametric) and create a benchmark by adapting existing datasets to evaluate this preference. They propose a novel dataset synthesis method that automatically generates instruction-tuning data from Wikipedia and Wikidata, creating counterfactual and factual question-answer pairs. The approach demonstrates that fine-tuning a 7B model on just a few thousand synthesized examples achieves more than 18% improvement across all evaluation benchmarks.

## Method Summary
The authors create a three-level knowledge preference hierarchy (instruction > context > parametric) and develop a dataset synthesis method that automatically generates instruction-tuning data from Wikipedia and Wikidata using GPT-4o. They fine-tune base language models (Mistral-v0.3-7B) with a combination of Alpaca's 52K instruction tuning data and their synthesized HIER PREF data (~7.4K examples). The fine-tuning uses LoRA (rank 16) with batch size 128 and learning rate 1e-4 for 10 epochs. They evaluate on adapted benchmarks (IfQA, MQuAKE, MRQA) and create zero-shot evaluation benchmarks to measure knowledge preference capabilities.

## Key Results
- Fine-tuning a 7B model on ~7.4K synthesized examples achieves more than 18% improvement across all evaluation benchmarks
- Zero-shot performance reaches 89.36% F1 on counterfactual knowledge editing tasks compared to 28.48% for baseline models
- The approach successfully teaches models to prioritize instruction knowledge over context knowledge, and context knowledge over parametric knowledge

## Why This Works (Mechanism)

### Mechanism 1
The three-level knowledge preference hierarchy enables systematic resolution of conflicts between different knowledge sources. By fine-tuning on synthesized data that explicitly creates scenarios where instruction knowledge conflicts with context knowledge, and context knowledge conflicts with parametric knowledge, the model learns to prioritize according to the defined hierarchy.

### Mechanism 2
Synthesizing diverse counterfactual and factual question-answer pairs with context passages creates a robust training dataset that teaches the knowledge preference hierarchy. The synthesis process uses high-quality sources (Wikipedia and Wikidata) with GPT-4o to create instances where answers must be derived from specific knowledge sources while ignoring conflicting information.

### Mechanism 3
Combining instruction tuning with synthesized hierarchical preference data and Alpaca's general instruction data creates a model with dual capabilities - it can follow arbitrary instructions while also having the learned preference hierarchy for knowledge conflicts.

## Foundational Learning

- Concept: Knowledge conflict resolution
  - Why needed here: The paper's core contribution is teaching models to resolve conflicts between different knowledge sources according to a predefined hierarchy
  - Quick check question: Given a question where parametric knowledge says "X" and context says "Y", how should a model decide which answer to provide if no instruction is given?

- Concept: Synthetic data generation for training
  - Why needed here: The paper relies on generating synthetic training data rather than human annotation to teach the knowledge preference hierarchy
  - Quick check question: What are the advantages and disadvantages of using GPT-4o to generate synthetic training data versus human annotation?

- Concept: Instruction tuning methodology
  - Why needed here: The paper uses instruction tuning as the primary method to instill the knowledge preference behavior in language models
  - Quick check question: How does instruction tuning differ from standard fine-tuning, and why is it particularly suited for teaching preference hierarchies?

## Architecture Onboarding

- Component map: Base language model (Mistral-v0.3-7B) -> Data synthesis pipeline (GPT-4o with Wikipedia/Wikidata) -> Instruction tuning process (Alpaca + HIER PREF data) -> Evaluation on benchmark datasets
- Critical path: Source data → Synthetic data generation → Instruction tuning → Evaluation on benchmark datasets → Analysis of knowledge preference behavior
- Design tradeoffs: Using synthetic data enables scalability but may introduce distribution mismatch; combining general instruction data with specialized preference data may create interference but provides broader instruction-following capabilities
- Failure signatures: Model fails to follow knowledge preference hierarchy in novel conflict scenarios; model overfits to synthetic data patterns and doesn't generalize; combination of Alpaca and HIER PREF data creates conflicting training signals
- First 3 experiments:
  1. Fine-tune base model on HIER PREF data alone and evaluate on IfQA to measure knowledge preference capability without general instruction following
  2. Fine-tune base model on Alpaca data alone and evaluate on InstructMH-3k to measure instruction following capability without specialized knowledge preference
  3. Fine-tune base model on HIER PREF data with shuffled context passages to test robustness to context ordering

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal dataset size for effectively teaching knowledge preference hierarchies in language models? The authors demonstrate effectiveness with "a few thousand examples" but don't systematically explore the relationship between dataset size and performance.

### Open Question 2
How does the knowledge preference hierarchy behave when context knowledge contains contradictory information? The paper assumes retrieved contents are generally helpful but doesn't explore scenarios with conflicting or contradictory context information.

### Open Question 3
Does the knowledge preference hierarchy generalize across different domains and task types beyond question answering? The authors focus primarily on question answering benchmarks and don't explore whether the learned hierarchy transfers to other applications.

## Limitations
- Reliance on GPT-4o for synthetic data generation introduces dependency on synthetic data quality and consistency
- Limited qualitative analysis of how the model actually resolves knowledge conflicts in edge cases
- 18% improvement comes from relatively small evaluation sets, particularly for counterfactual knowledge editing tasks

## Confidence
- High Confidence: The existence of a three-level knowledge preference hierarchy and its general formulation
- Medium Confidence: The effectiveness of synthetic data generation using GPT-4o for this specific task
- Medium Confidence: The 18% improvement in performance metrics under controlled experimental conditions
- Low Confidence: Generalization to real-world deployment scenarios with complex, ambiguous knowledge conflicts

## Next Checks
1. Conduct detailed qualitative analysis on how the fine-tuned model handles ambiguous knowledge conflicts where the hierarchy breaks down, particularly focusing on cases where instruction knowledge is unclear or missing.

2. Evaluate model performance on questions involving rapidly evolving factual information to assess how well the knowledge preference hierarchy handles temporal knowledge updates.

3. Run a human evaluation study where annotators judge whether the model's knowledge preference decisions align with human expectations across diverse real-world scenarios, not just the synthetic benchmark data.