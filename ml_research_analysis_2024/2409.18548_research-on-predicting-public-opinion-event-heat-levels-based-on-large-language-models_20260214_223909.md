---
ver: rpa2
title: Research on Predicting Public Opinion Event Heat Levels Based on Large Language
  Models
arxiv_id: '2409.18548'
source_url: https://arxiv.org/abs/2409.18548
tags:
- heat
- events
- prediction
- level
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a large language model-based method for predicting
  public opinion event heat levels. The researchers preprocessed 62,836 Chinese hot
  event data and clustered them into four heat levels using MiniBatchKMeans.
---

# Research on Predicting Public Opinion Event Heat Levels Based on Large Language Models

## Quick Facts
- **arXiv ID:** 2409.18548
- **Source URL:** https://arxiv.org/abs/2409.18548
- **Reference count:** 19
- **Primary result:** GPT-4o and DeepseekV2 achieved best performance (41.4% and 41.5% accuracy) in predicting public opinion event heat levels

## Executive Summary
This study proposes a large language model-based method for predicting public opinion event heat levels. The researchers preprocessed 62,836 Chinese hot event data and clustered them into four heat levels using MiniBatchKMeans. They evaluated six large language models' performance in predicting these heat levels under different scenarios. The results showed that GPT-4o and DeepseekV2 achieved the best performance with 41.4% and 41.5% accuracy respectively when referencing similar cases.

## Method Summary
The researchers preprocessed 62,836 Chinese hot event data and clustered them into four heat levels using MiniBatchKMeans. They evaluated six large language models (GPT-4o, DeepseekV2, and others) for predicting these heat levels under different scenarios, including with and without reference to similar cases. The prediction accuracy decreased from level 1 to level 4 events, correlating with the uneven distribution of data across heat levels in the actual dataset.

## Key Results
- GPT-4o and DeepseekV2 achieved the best performance with 41.4% and 41.5% accuracy respectively when referencing similar cases
- Prediction accuracy decreased from level 1 to level 4 events, correlating with the uneven distribution of data across heat levels
- The models tend to be conservative in predictions, particularly for medium and higher heat levels due to data scarcity

## Why This Works (Mechanism)
The large language models can leverage their understanding of semantic patterns and contextual information from similar cases to predict heat levels of public opinion events. By referencing similar events, the models can identify patterns in how certain types of events evolve and gain attention over time. The MiniBatchKMeans clustering helps create discrete categories that the models can learn to associate with specific event characteristics and trajectories.

## Foundational Learning
- **MiniBatchKMeans clustering**: Why needed - to categorize events into discrete heat levels for supervised learning; Quick check - verify cluster separation using silhouette scores
- **Event similarity matching**: Why needed - to provide contextual references for predictions; Quick check - measure precision of similarity matches
- **Chinese language processing**: Why needed - dataset is in Chinese, requiring language-specific handling; Quick check - validate tokenization and embedding quality
- **Heat level categorization**: Why needed - to create a structured framework for event intensity; Quick check - test inter-rater reliability of heat level assignments
- **Large language model fine-tuning**: Why needed - to adapt models to the specific task of heat level prediction; Quick check - compare zero-shot vs fine-tuned performance
- **Data preprocessing**: Why needed - to clean and standardize event data for consistent analysis; Quick check - verify data quality metrics before and after preprocessing

## Architecture Onboarding
**Component Map:** Data Preprocessing -> Event Clustering -> Similarity Matching -> Large Language Model Prediction -> Performance Evaluation

**Critical Path:** Event data must be properly clustered and similar cases must be accurately identified before the large language model can make reliable predictions

**Design Tradeoffs:** The study chose MiniBatchKMeans for clustering (faster than standard KMeans) at the potential cost of some accuracy; used frequency-based similarity matching (simpler but potentially less nuanced than semantic similarity)

**Failure Signatures:** Low accuracy on higher heat levels suggests data scarcity issues; conservative predictions indicate model uncertainty with limited training examples

**First 3 Experiments:** 1) Test model performance on balanced dataset with equal heat level representation; 2) Compare different similarity matching strategies (semantic vs frequency-based); 3) Evaluate model performance on temporally separated test data to assess generalization

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How would prediction accuracy change with a more balanced dataset distribution across all heat levels?
- Basis in paper: The paper notes that prediction accuracy decreases from Level 1 to Level 4 events, correlating with the uneven distribution of data across heat levels in the actual dataset
- Why unresolved: The current dataset is heavily skewed toward low-heat events, making it difficult to determine if the decreasing accuracy is due to model limitations or data scarcity at higher levels
- What evidence would resolve it: Creating a balanced dataset with equal representation of all heat levels and re-running the experiments would show whether accuracy improves uniformly across levels

### Open Question 2
- Question: What is the impact of different similarity matching strategies on prediction accuracy for high-heat events?
- Basis in paper: The paper mentions that the prediction performance for medium and higher heat levels is poorer due to uneven data distribution and missing data, and that models tend to be conservative in predictions
- Why unresolved: The current approach uses simple frequency-based similarity matching, which may not capture nuanced similarities for high-heat events
- What evidence would resolve it: Testing alternative similarity matching methods (semantic similarity, temporal context, or domain-specific features) and comparing their impact on high-heat event predictions

### Open Question 3
- Question: How do different model architectures (transformer variants, attention mechanisms) affect heat level prediction performance?
- Basis in paper: The study evaluated six large language models but did not compare different architectural approaches or attention mechanisms
- Why unresolved: All tested models use similar transformer-based architectures, so the impact of architectural variations remains unknown
- What evidence would resolve it: Comparing models with different architectures (e.g., convolutional transformers, sparse attention mechanisms, or hybrid architectures) on the same dataset

## Limitations
- Moderate performance (41.4-41.5% accuracy) represents relatively low accuracy for practical deployment
- Uneven data distribution across heat levels significantly impacts model performance, with accuracy decreasing as event heat level increases
- Use of Chinese language data and specific cultural context may limit generalizability to other languages or regions

## Confidence
- **Medium confidence** in the overall methodology and results, as the study provides clear experimental design and baseline comparisons across multiple large language models
- **Medium confidence** in the relative performance rankings between models (GPT-4o and DeepseekV2 performing best), given consistent evaluation metrics across scenarios
- **Low confidence** in absolute performance metrics for real-world deployment, due to the relatively low accuracy rates and potential overfitting to the specific dataset characteristics

## Next Checks
1. Test model performance on an independent, time-separated dataset to evaluate temporal generalization and avoid potential data leakage or temporal bias
2. Conduct ablation studies by varying the number of similar cases referenced during prediction to determine the optimal context window for heat level prediction
3. Implement stratified sampling or class balancing techniques to evaluate whether addressing the uneven data distribution across heat levels improves overall prediction accuracy