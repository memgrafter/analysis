---
ver: rpa2
title: Generative causal testing to bridge data-driven models and scientific theories
  in language neuroscience
arxiv_id: '2410.00812'
source_url: https://arxiv.org/abs/2410.00812
tags:
- driving
- voxel
- each
- explanation
- voxels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Generative Causal Testing (GCT), a framework
  for interpreting large language model (LLM)-based encoding models of brain activity
  during language processing. GCT generates natural language explanations of what
  drives individual voxels or brain regions, then tests these explanations using LLM-generated
  stimuli.
---

# Generative causal testing to bridge data-driven models and scientific theories in language neuroscience

## Quick Facts
- **arXiv ID**: 2410.00812
- **Source URL**: https://arxiv.org/abs/2410.00812
- **Reference count**: 40
- **Primary result**: Framework successfully explains and drives 41 of 51 tested voxels, with stability correlating to driving success

## Executive Summary
This paper introduces Generative Causal Testing (GCT), a framework that interprets large language model (LLM)-based encoding models of brain activity during language processing. GCT generates natural language explanations of what drives individual voxels or brain regions, then tests these explanations using LLM-generated stimuli in fMRI experiments. The approach successfully explained and drove responses in 41 of 51 tested voxels, with driven responses significantly higher than baseline. The method also discovered new micro-ROIs in prefrontal cortex with specific semantic selectivity and validated functional similarity claims across the language network.

## Method Summary
GCT works by first fitting voxelwise encoding models that predict BOLD responses from LLM activations (using both LLaMA and OPT models). Selected voxels are then analyzed using the SASC framework to generate 5 candidate explanations each, with stability scores computed as correlations between predictions from the two LLM models. The most stable explanations are used to prompt an LLM to generate narrative stories where each paragraph targets a specific voxel's explanation. Subjects then listen to these stories in fMRI while their brain responses are measured. The framework analyzes whether each targeted voxel responds more strongly during its corresponding story paragraph versus baseline.

## Key Results
- GCT successfully explained and drove responses in 41 of 51 tested voxels (p<0.05 after FDR correction)
- Explanatory accuracy correlated with encoding model stability, with higher stability scores predicting better driving performance
- Discovered 47 significant driving micro-ROIs in prefrontal cortex for one subject and 21 for another, with specific semantic selectivity (recognition, dialogue, time, measurements)
- Validated functional similarity claims by failing to find differential stimuli between regions across the language network

## Why This Works (Mechanism)

### Mechanism 1
LLMs can generate coherent, semantically relevant stories that selectively drive specific voxel responses in language-selective brain regions. The framework uses instruction-tuned LLMs to generate natural language explanations from encoding model weights, then prompts another LLM to generate narrative stories where each paragraph is designed to drive a specific voxel or ROI. The LLM ensures semantic coherence while maintaining the targeted explanation in each paragraph. Core assumption: encoding model weights capture interpretable semantic selectivity that can be translated into natural language, and LLMs can generate semantically coherent text that preserves targeted explanations.

### Mechanism 2
Stability between encoding models from different LLMs correlates with the ability to successfully drive brain responses, ensuring reliable explanations. The framework uses two different LLMs (LLaMA and OPT) to build encoding models. A stability score measures correlation between predictions from these models for the same voxel. Higher stability indicates more reliable explanations that are more likely to successfully drive brain responses. Core assumption: encoding models that are stable across different LLMs capture genuine semantic selectivity rather than idiosyncratic model-specific features.

### Mechanism 3
GCT can discover new functional micro-ROIs in poorly understood brain regions by generating explanations and testing them with synthetic stimuli. The framework generates candidate ROIs in unexplored regions (like prefrontal cortex), uses SASC to generate explanations for each candidate, filters for stable explanations, then tests these with fMRI experiments. Successful driving validates the explanation and identifies the ROI as functionally selective. Core assumption: small, functionally selective regions exist in areas like prefrontal cortex that can be discovered through systematic exploration with GCT.

## Foundational Learning

- **Encoding models in fMRI**: The framework relies on encoding models that predict BOLD responses from language stimuli using LLM representations. Understanding how these models work is essential for interpreting the GCT framework. Quick check: What type of regression is typically used to fit encoding models that predict voxel responses from LLM activations?

- **SASC (Summarize and Score) framework**: GCT uses SASC to generate explanations by summarizing n-grams that drive encoding models and scoring synthetic data based on those explanations. This is the core mechanism for explanation generation. Quick check: What are the two main steps in the SASC framework used for generating explanations?

- **Stability in statistical modeling**: The framework uses stability between different encoding models as a key criterion for selecting reliable explanations. Understanding stability is crucial for interpreting the results. Quick check: How is stability typically measured between different models or datasets in statistical learning?

## Architecture Onboarding

- **Component map**: Data preprocessing → Encoding model training (LLaMA + OPT) → Voxel selection → SASC explanation generation → Story generation → fMRI driving experiment → Analysis (stability scores, driving responses)
- **Critical path**: The most critical path is from encoding model training through SASC explanation generation to story generation, as failures at any of these stages prevent successful driving experiments
- **Design tradeoffs**: The framework trades computational complexity (multiple encoding models, stability calculations, LLM prompting) for increased reliability and interpretability of explanations. Using multiple LLMs adds stability but doubles computational cost
- **Failure signatures**: Low stability scores indicate unreliable explanations; poor driving responses indicate explanations that don't capture true selectivity; high motion in fMRI data can obscure driving effects
- **First 3 experiments**:
  1. Run encoding models on test dataset and visualize prediction accuracy to ensure models are working before proceeding to explanation generation
  2. Generate explanations for a small set of voxels and manually verify that the explanations make semantic sense
  3. Generate a single story paragraph for one voxel and test whether it drives the encoding model as expected before running full fMRI experiments

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise mechanism by which stability score correlates with driving performance? Is it a causal relationship or merely correlational? While the correlation is established, the paper does not provide a mechanistic explanation for why stability would causally influence driving performance. Experimental manipulation of stability scores while holding other factors constant would help establish causality.

### Open Question 2
How does GCT perform when applied to individual words versus multi-word phrases or sentences? Are there systematic differences in explanation quality or driving performance? The current implementation uses a fixed n-gram approach without exploring whether different linguistic units might yield better explanations or driving performance. Systematic experiments varying n-gram sizes would reveal optimal settings.

### Open Question 3
Can GCT be extended to explain and drive non-semantic aspects of language processing, such as syntactic structure, prosody, or pragmatic context? The paper focuses exclusively on semantic explanations and driving, but doesn't explore whether the framework can capture other linguistic dimensions that are known to be neurally distinct. Adapting GCT to use language models that explicitly encode syntactic or pragmatic information would demonstrate the framework's versatility.

## Limitations
- Framework reliability depends heavily on LLM interpretability and the assumption that encoding model weights capture meaningful semantic selectivity
- Prefrontal cortex micro-ROI discovery was limited to only two subjects and needs replication with larger samples
- Driving paradigm's ecological validity remains uncertain, as synthetic stories may activate different neural mechanisms than natural language comprehension

## Confidence
- **High confidence**: The core finding that stable encoding models produce reliable explanations that can drive brain responses is well-supported by statistical evidence
- **Medium confidence**: The discovery of new prefrontal micro-ROIs with specific semantic selectivity requires replication with additional subjects and tasks
- **Medium confidence**: The correlation between model stability and driving success, while statistically significant, needs validation across different encoding model architectures and language domains

## Next Checks
1. Test whether the stability-drives-success relationship holds when applying GCT to different language fMRI datasets (e.g., picture descriptions, conversational speech) to assess generalizability

2. Systematically vary the stability threshold to identify the optimal balance between explanation reliability and discovery potential, determining whether current criteria are too conservative

3. Compare the semantic selectivity identified by GCT with established functional neuroanatomy from meta-analyses to evaluate whether the discovered micro-ROIs align with known language processing hierarchies