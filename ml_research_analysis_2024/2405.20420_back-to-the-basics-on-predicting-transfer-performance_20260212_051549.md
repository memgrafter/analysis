---
ver: rpa2
title: Back to the Basics on Predicting Transfer Performance
arxiv_id: '2405.20420'
source_url: https://arxiv.org/abs/2405.20420
tags:
- scorers
- transferability
- datasets
- dataset
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work benchmarks 13 transferability scorers across 11 diverse
  datasets, finding that most fail to outperform the simple ImageNet accuracy baseline
  and struggle on medical datasets. It introduces a combined analysis metric and a
  Bayesian hierarchical model ("Back to Bayes") that consistently improves predictor
  performance by integrating multiple scorers with learned reliability weights.
---

# Back to the Basics on Predicting Transfer Performance

## Quick Facts
- arXiv ID: 2405.20420
- Source URL: https://arxiv.org/abs/2405.20420
- Reference count: 40
- One-line primary result: Most transferability scorers fail to outperform simple ImageNet accuracy baseline, with a Bayesian hierarchical model showing consistent improvements

## Executive Summary
This paper tackles the fundamental problem of selecting optimal pre-trained models for transfer learning by benchmarking 13 transferability scorers across 11 diverse datasets. The study reveals that despite their complexity, most existing scorers fail to outperform the simple ImageNet accuracy baseline, particularly struggling on medical datasets. To address this, the authors introduce a Bayesian hierarchical model called "Back to Bayes" that combines multiple scorers with learned reliability weights, consistently improving prediction performance across all datasets tested.

## Method Summary
The study evaluates transferability predictors through a comprehensive benchmark involving 11 datasets (Caltech-101, SUN397, Pascal VOC 2007, Oxford Flowers 102, Oxford-IIIT Pets, FGVC-Aircraft, DTD, Stanford Cars, BrainTumor, BreakHis, ISIC 19) and 10 model architectures (ResNet-18/34/50, DenseNet-121/161/169, MobileNetV2-0.5/1.0, EfficientNet-B0, ViT-small). All models undergo full fine-tuning for 100 epochs using SGD with Nesterov momentum and cosine learning-rate scheduling, with hyperparameters optimized via Halton quasi-random sequences. The paper introduces an aggregated weighted Kendall's tau metric (˚τw) for combined analysis and proposes a Bayesian hierarchical model that integrates multiple scorers with dataset-specific reliability weights, requiring minimal calibration data (as few as 30 tuples).

## Key Results
- ImageNet accuracy baseline outperforms 12 out of 13 complex transferability scorers across all datasets
- Transferability predictors show particularly poor performance on medical datasets (BrainTumor, BreakHis, ISIC 19)
- Back to Bayes model consistently improves predictor performance by combining multiple scorers with learned weights
- The method generalizes well with minimal calibration data requirements (200-400 tuples typically sufficient)

## Why This Works (Mechanism)
The Bayesian hierarchical model works by learning reliability weights for each base scorer through Bayesian inference, allowing it to adaptively combine predictors based on their historical performance. By modeling the relationship between transferability scores and actual transfer performance as a hierarchical structure, the method can capture both dataset-specific and scorer-specific patterns. The strong regularization through Normal and Half-Normal priors prevents overfitting while enabling the model to extract meaningful signal from noisy individual predictors.

## Foundational Learning

**Transferability scoring**: Methods that predict how well a pre-trained model will perform on a target task without full fine-tuning. Why needed: Enables efficient model selection without expensive training. Quick check: Verify a scorer can rank-order models on a simple dataset.

**Weighted Kendall's tau (τw)**: Correlation metric that accounts for the magnitude of performance differences between models. Why needed: Provides more nuanced evaluation than simple rank correlation. Quick check: Calculate τw between two ordered lists with known rank differences.

**Bayesian hierarchical modeling**: Statistical framework that models parameters at multiple levels with partial pooling. Why needed: Allows sharing information across datasets while maintaining dataset-specific patterns. Quick check: Fit a simple hierarchical model with simulated data.

## Architecture Onboarding

**Component map**: Transferability scorers (13 inputs) -> Performance metrics (accuracy) -> Bayesian hierarchical model (Back to Bayes) -> Combined predictions -> Evaluation (˚τw)

**Critical path**: Scorers generate predictions → Fine-tuning collects ground truth → Bayesian model learns weights → Combined predictions evaluated

**Design tradeoffs**: The paper chose Bayesian inference over simpler ensemble methods to enable uncertainty quantification and regularization, trading computational complexity for robustness and interpretability.

**Failure signatures**: Poor performance on medical datasets suggests domain shift issues; baseline ImageNet dominance indicates many complex scorers add little value; calibration data sensitivity shows importance of representative training samples.

**First experiments**:
1. Implement and test a single transferability scorer on a simple dataset
2. Run the baseline ImageNet accuracy comparison on one dataset
3. Evaluate Back to Bayes with two base scorers on a small dataset

## Open Questions the Paper Calls Out
How does the performance of transferability scorers change when evaluated on larger-scale datasets or with more model architectures beyond the 10 used in this study? The current study's scope is limited to a specific set of architectures and datasets, and scaling up could reveal different performance patterns.

Can Back to Bayes be extended to handle non-classification tasks, such as object detection, segmentation, or regression, while maintaining its performance advantages? The current implementation is tailored for classification tasks, and extending it would require adapting the model architecture and likelihood functions.

How sensitive is Back to Bayes to the choice of base scorers used in its calibration phase, and can it identify the most informative scorers for a given target dataset or task? The performance depends on the quality and complementarity of base scorers, and understanding this sensitivity would provide insights into robustness.

## Limitations
- The exact implementations of 13 transferability scorers are not provided, requiring reference to external sources
- Medical dataset performance degradation suggests domain-specific challenges not fully explored
- Limited validation on datasets outside the evaluated domains raises questions about generalizability

## Confidence
High: Benchmark results showing ImageNet accuracy as a strong baseline and general performance patterns of transferability scorers
Medium: Effectiveness of Back to Bayes model in consistently improving predictor performance, given limited medical dataset validation
Low: Generalizability of the combined analysis metric to datasets outside evaluated domains

## Next Checks
1. Implement and validate the exact code for all 13 transferability scorers using referenced sources
2. Test Back to Bayes model on additional medical and domain-specific datasets to verify robustness
3. Conduct ablation studies to quantify contribution of each scorer in the Bayesian hierarchical model