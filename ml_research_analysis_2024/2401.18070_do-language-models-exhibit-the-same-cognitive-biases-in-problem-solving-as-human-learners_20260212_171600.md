---
ver: rpa2
title: Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as
  Human Learners?
arxiv_id: '2401.18070'
source_url: https://arxiv.org/abs/2401.18070
tags:
- problem
- problems
- language
- which
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether large language models (LLMs) exhibit
  the same cognitive biases as human children when solving arithmetic word problems.
  The authors hypothesize that the problem-solving process can be split into three
  distinct steps: text comprehension, solution planning, and solution execution.'
---

# Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?

## Quick Facts
- **arXiv ID**: 2401.18070
- **Source URL**: https://arxiv.org/abs/2401.18070
- **Reference count**: 31
- **Primary result**: LLMs exhibit human-like consistency and transfer vs comparison biases in problem solving, but not the carry effect bias seen in children.

## Executive Summary
This paper investigates whether large language models (LLMs) exhibit the same cognitive biases as human children when solving arithmetic word problems. The authors develop a novel neuro-symbolic problem generation pipeline that enables fine-grained control over problem features, allowing isolation of specific cognitive biases. Experiments on various LLMs (with and without instruction-tuning) reveal that most models show human-like biases in text comprehension and solution planning steps, but not in the final arithmetic execution step. These findings contribute to understanding LLMs as cognitive models and their potential applications in educational contexts.

## Method Summary
The authors use a neuro-symbolic approach to generate controlled arithmetic word problems that isolate specific cognitive biases. The generation pipeline involves structure sampling, property instantiation, template sampling, and error correction to produce problem pairs differing only in the feature of interest. They evaluate multiple LLMs (including LLaMA2, Mistral, Mixtral, GPT-3.5, and GPT-4) using zero-shot inference and chain-of-thought prompting. The study measures conditional average treatment effect (CATE) for each bias and performs statistical tests while controlling for false discovery rate.

## Key Results
- LLMs show significant consistency bias, performing worse on inconsistent comparison problems than consistent ones
- Models exhibit transfer vs comparison bias, with lower accuracy on comparison-type problems compared to transfer-type problems
- LLMs do not show significant carry effect bias in arithmetic computations, unlike human children
- Chain-of-thought prompting amplifies the observed biases in most models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The generation pipeline ensures strict control over the linguistic form of each problem sentence, which allows precise isolation of cognitive biases in text comprehension.
- **Mechanism**: By using a templated sentence approach tied to a formal logical representation (MATHWORLD), the pipeline fixes the semantic structure while only varying linguistic form, enabling causal attribution of performance differences to text-level features.
- **Core assumption**: The logical form-to-template mapping is faithful and consistent across problem pairs; templated text preserves intended semantics without unintended distortions.
- **Evidence anchors**:
  - [abstract] The authors state they use a "neuro-symbolic approach that enables fine-grained control over problem features."
  - [section 4.2] The pipeline step "Template Sampling" explicitly maps logical forms to templated sentences.
  - [corpus] Related work (Opedal et al. 2023) provides the semantic formalism used; pipeline builds directly on that framework.
- **Break condition**: If template sampling introduces semantic drift or if the logical form mapping is incomplete, bias attribution would be confounded.

### Mechanism 2
- **Claim**: Large language models inherit cognitive biases from training data distributions, particularly biases that mirror human tendencies.
- **Mechanism**: Training corpora likely contain more consistent problem formulations and more transfer-type problems than comparison-type ones; LLMs replicate these imbalances when solving new problems.
- **Core assumption**: Training data used for LLaMA2, Mistral, Mixtral, GPT-3.5, GPT-4 includes publicly available math word problem datasets with the same imbalance patterns.
- **Evidence anchors**:
  - [abstract] "Our experiments... reveal that almost all models show significant effects of consistency bias and transfer vs comparison bias, like child learners."
  - [section 6] Analysis of MAWPS, ASDIV-A, and SVAMP datasets shows 5:1 ratio of consistent to inconsistent formulations and 130:9 ratio of transfer to comparison problems.
  - [corpus] Multiple studies (Ando et al. 2023; Dasgupta et al. 2023) report LLMs replicating human cognitive biases in non-mathematical domains, supporting data-driven bias transfer.
- **Break condition**: If training data were curated to remove human-like biases, or if models were trained on synthetic data without human cognitive patterns, observed biases would be weaker or absent.

### Mechanism 3
- **Claim**: Chain-of-thought prompting amplifies model biases by exposing more sequential reasoning steps that interact with training data patterns.
- **Mechanism**: CoT increases the number of intermediate tokens generated, giving the model more opportunities to reproduce biased reasoning patterns present in the training corpus.
- **Core assumption**: CoT prompting does not fundamentally change the model's reasoning algorithm but merely exposes more of the underlying learned behavior.
- **Evidence anchors**:
  - [abstract] "Most of these effects are further strengthened by using chain-of-thought prompting."
  - [section 5.1] CoT prompting is implemented by appending "Let's think step by step" and re-prompting for the final answer.
  - [corpus] Shaikh et al. (2023) document CoT amplifying biases in other domains, consistent with findings here.
- **Break condition**: If CoT prompts the model to use a different algorithmic approach (e.g., explicit symbolic reasoning), amplification effects might not hold.

## Foundational Learning

- **Concept**: Conditional Average Treatment Effect (CATE)
  - **Why needed here**: The paper uses CATE to quantify the causal effect of problem feature variations on model accuracy, isolating bias strength from confounding factors.
  - **Quick check question**: If two problem pairs differ only in consistency, and model accuracy drops from 80% to 60% for inconsistent versions, what is the CATE?
    - Answer: CATE = 0.60 - 0.80 = -0.20 (or 20 percentage points).

- **Concept**: False Discovery Rate (FDR) control
  - **Why needed here**: Multiple hypothesis tests are performed across models and bias types; FDR control ensures reported significant effects are not due to chance.
  - **Quick check question**: If 23 tests are run and 5 are significant at α=0.05 without correction, how many false positives would we expect?
    - Answer: 23 × 0.05 = 1.15 false positives on average.

- **Concept**: Neuro-symbolic generation
  - **Why needed here**: The generation pipeline relies on symbolic logical representations to define problem structure, then uses neural models for linguistic realization.
  - **Quick check question**: Why is it important that the pipeline enforces all quantities to be explicit numbers rather than intermediate results?
    - Answer: This prevents the model from encountering hidden dependencies that could confound bias attribution.

## Architecture Onboarding

- **Component map**: Data generation (structure sampler → property instantiator → template sampler → error corrector) → Evaluation (accuracy extraction → CATE estimation → hypothesis testing) → Reporting (tables + figures).
- **Critical path**: Generation → Error correction → Prompting → Accuracy extraction → Statistical analysis. Each step must succeed for valid bias measurement.
- **Design tradeoffs**: Template-based generation ensures control but may produce unnatural phrasing; error correction with GPT-3.5 mitigates this but adds dependency on an external model and potential semantic drift.
- **Failure signatures**: If generated problems contain semantic errors, CATE estimates will be invalid; if prompts are malformed, model outputs may be nonsensical; if FDR control is misconfigured, false positives may appear.
- **First 3 experiments**:
  1. Run the generation pipeline with N=1 reasoning steps and verify that all templated sentences are grammatically correct post-correction.
  2. Evaluate a small sample (e.g., 10 problems) on LLaMA2 7B with direct prompting to confirm the accuracy extraction pipeline works.
  3. Compute CATE for consistency bias on this sample and verify the sign and magnitude align with expectations from literature.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying mechanism that causes language models to exhibit the consistency bias observed in children?
- Basis in paper: [explicit] The paper suggests that the training data might be skewed towards consistent problem formulations, which could explain the consistency bias in language models.
- Why unresolved: The paper acknowledges that they cannot directly verify this hypothesis due to the unknown training data of the models. They only provide a proxy analysis using publicly available datasets.
- What evidence would resolve it: Analyzing the training data of the specific language models used in the study to determine the prevalence of consistent vs. inconsistent problem formulations would provide direct evidence for or against the hypothesis.

### Open Question 2
- Question: Why do language models not exhibit the carry effect bias observed in children?
- Basis in paper: [explicit] The paper suggests that language models might not implement the same working memory limitations as humans, which are partially responsible for the carry effect in children.
- Why unresolved: The paper does not provide a definitive explanation for the absence of the carry effect in language models. They only propose possible reasons based on the differences in how humans and language models perform arithmetic computations.
- What evidence would resolve it: Further research into the specific mechanisms used by language models for arithmetic computations and how they differ from human cognitive processes would be needed to fully understand the absence of the carry effect.

### Open Question 3
- Question: How does the number of reasoning steps in a word problem affect the strength of the transfer vs comparison bias in language models?
- Basis in paper: [inferred] The paper mentions that the CATE sizes for the transfer vs comparison bias increase with the number of reasoning steps for instruction-tuned models, while they decrease for pretrained-only models.
- Why unresolved: The paper does not provide a clear explanation for this observed relationship or compare it to any existing literature on human biases.
- What evidence would resolve it: Conducting further experiments with a wider range of problem lengths and comparing the results to studies on human biases across different problem lengths would help understand the relationship between the number of reasoning steps and the transfer vs comparison bias in language models.

## Limitations

- The study only examines three specific cognitive biases, limiting generalizability to the broader space of human cognitive biases in problem solving
- The lack of exact training data specifications for evaluated models introduces uncertainty about whether observed biases genuinely reflect human-like learning patterns
- The methodology depends critically on the fidelity of the neuro-symbolic generation pipeline and the assumption that observed biases reflect learned patterns rather than artifacts of problem construction

## Confidence

- **High confidence**: The methodology for generating controlled problem sets and measuring CATE effects is sound and well-documented
- **Medium confidence**: The observed consistency and transfer/comparison biases in LLMs, as these align with known training data imbalances and are replicated across multiple models
- **Low confidence**: The absence of carry effect bias in models, as this negative result could stem from implementation details or the specific arithmetic formulation used

## Next Checks

1. **Generation pipeline validation**: Manually annotate 100 generated problems for semantic fidelity and linguistic naturalness, measuring inter-annotator agreement to quantify generation quality.
2. **Training data analysis**: Extract and analyze the frequency distribution of problem types in the actual training data of LLaMA2 and Mistral to empirically verify the hypothesized 5:1 ratio of consistent to inconsistent formulations.
3. **Cross-linguistic generalization**: Replicate the consistency bias experiments using problems translated into a non-English language to test whether the bias persists when linguistic form varies while logical structure remains constant.