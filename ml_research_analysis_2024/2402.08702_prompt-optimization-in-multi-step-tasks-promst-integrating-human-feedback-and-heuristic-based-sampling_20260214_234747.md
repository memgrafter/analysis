---
ver: rpa2
title: 'PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback
  and Heuristic-based Sampling'
arxiv_id: '2402.08702'
source_url: https://arxiv.org/abs/2402.08702
tags:
- prompt
- move
- action
- goal
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROMST is the first framework for automatic prompt optimization
  in multi-step agent tasks. It combines human-designed feedback rules with a learned
  score prediction model to efficiently explore the vast prompt space.
---

# PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Heuristic-based Sampling

## Quick Facts
- arXiv ID: 2402.08702
- Source URL: https://arxiv.org/abs/2402.08702
- Reference count: 40
- Key outcome: PROMST achieves 10.6%-29.3% improvement over current best methods for automatic prompt optimization in multi-step agent tasks.

## Executive Summary
PROMST introduces the first framework for automatic prompt optimization in multi-step agent tasks by combining human-designed feedback rules with a learned score prediction model. The approach leverages human expertise to create interpretable error feedback that guides LLM-based prompt generation, while using a genetic algorithm with score-guided sampling to efficiently explore the vast prompt space. Across 8 multi-step tasks and 5 LLMs, PROMST demonstrates significant improvements over existing methods and shows potential for aligning optimized prompts with user preferences through score function modification.

## Method Summary
PROMST is a genetic algorithm framework that optimizes prompts for multi-step agent tasks by integrating human feedback and learned heuristics. The framework uses human-designed feedback rules to guide an LLM in generating prompt candidates, while a Longformer-based score prediction model is fine-tuned online to efficiently filter promising prompts before expensive TaskLLM evaluations. The optimization process starts with human-engineered prompts and iteratively improves them through candidate generation, evaluation, and selection based on task performance scores.

## Key Results
- PROMST achieves 10.6%-29.3% improvement over current best methods (APE, APO) across 8 multi-step tasks
- The framework demonstrates effective integration of human feedback for prompt optimization in complex agent environments
- Score function modifications can align optimized prompts with individual user preferences beyond task completion
- The approach works across multiple LLMs including GPT-3.5, GPT-4, LLaMA-2, Claude-3, and PaLM-2

## Why This Works (Mechanism)

### Mechanism 1
Human feedback on specific error types improves prompt optimization efficiency by providing direct, interpretable guidance for LLM-based candidate generation. Humans design error feedback rules a priori for each task, which are synthesized and passed to the PromptLLM when TaskLLM encounters errors. The PromptLLM uses this feedback to generate new candidates that avoid identified errors.

### Mechanism 2
A learned score prediction model improves prompt candidate selection efficiency by filtering out low-performing prompts before expensive TaskLLM evaluations. The framework fine-tunes a Longformer model online using collected prompt-score pairs, starting from a specified depth level. New candidates are evaluated by the score model first, with only those predicted to exceed a threshold passed to TaskLLM.

### Mechanism 3
Modifying the score function can align optimized prompts with individual user preferences beyond just task completion. The framework uses task completion metrics as baseline but can incorporate additional factors like efficiency or safety. The modified score function is then used in PROMST to optimize prompts that better reflect user preferences.

## Foundational Learning

- Concept: Genetic algorithms
  - Why needed here: PROMST uses genetic algorithms to iteratively generate and select prompt candidates based on performance scores, efficiently exploring the vast prompt space
  - Quick check question: What is the main advantage of using a genetic algorithm for prompt optimization compared to random search or gradient-based methods?

- Concept: Prompt engineering for LLMs
  - Why needed here: The core task is finding optimal prompts for LLMs in multi-step tasks, requiring understanding of effective prompt engineering principles
  - Quick check question: What are common strategies for designing effective prompts for LLMs, and how do they differ for single-step vs. multi-step tasks?

- Concept: Reinforcement learning and feedback loops
  - Why needed here: PROMST incorporates a feedback loop where TaskLLM performance provides feedback for prompt optimization, similar to RL agents learning from rewards
  - Quick check question: How does the feedback loop in PROMST differ from traditional reinforcement learning, and what are the implications for optimization?

## Architecture Onboarding

- Component map: TaskLLM -> Error Detection -> Human Feedback Synthesis -> PromptLLM -> Score Model Filtering -> TaskLLM Evaluation -> Score Function Calculation -> Genetic Algorithm Selection

- Critical path: TaskLLM execution → Error detection → Human feedback synthesis → PromptLLM candidate generation → Score model filtering → TaskLLM evaluation → Score function calculation → Genetic algorithm selection

- Design tradeoffs:
  - Human feedback vs. automatic error analysis: Human feedback is more interpretable and effective for multi-step tasks but requires domain expertise and upfront effort
  - Score model accuracy vs. resource efficiency: More accurate models require more training data but can more effectively filter poor prompts
  - Task completion vs. user preferences: Optimizing for completion may not align with preferences, requiring careful score function design

- Failure signatures:
  - Low TaskLLM performance despite optimization: PromptLLM may not effectively incorporate human feedback or score model may be inaccurate
  - High variance across tasks: Human feedback rules or score function may not be well-calibrated for all tasks
  - Slow convergence: PromptLLM may generate low-quality candidates or score model may not effectively filter poor prompts

- First 3 experiments:
  1. Implement PROMST without score model using only human feedback and genetic algorithm to validate human feedback effectiveness
  2. Train score prediction model on small prompt-score dataset to assess feasibility of using score model for prompt selection
  3. Compare full PROMST with baseline methods on single multi-step task to validate overall approach effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
How does the choice of hyperparameter sd (depth level when score model training starts) affect overall optimization performance across different task types? The paper fixed sd=3 without exploring sensitivity or interaction with task complexity.

### Open Question 2
What is the relative importance of human feedback versus the learned score model in the optimization process? Ablation studies show benefits of each component separately but don't quantify relative contributions or explore optimal combinations.

### Open Question 3
How well do optimized prompts transfer to LLMs with different capabilities or architectures? The paper notes model preferences exist but doesn't systematically investigate cross-model transfer or quantify transferable features.

## Limitations
- Effectiveness heavily depends on quality and coverage of human-designed feedback rules, which are not fully specified and may not scale across diverse domains
- Score prediction model performance across different LLMs and task types is not extensively validated
- Framework does not address handling novel error types not covered by existing human feedback rules

## Confidence

- **High confidence**: Framework design and methodology are well-grounded in existing literature on prompt optimization and evolutionary algorithms. Reported improvements over baselines are significant and consistent.
- **Medium confidence**: Specific implementation details of human feedback rules and score prediction model are not fully specified, making robustness and generalizability difficult to assess.
- **Low confidence**: Paper does not address potential failure modes or limitations such as impact of poorly designed feedback rules or inaccurate score predictions.

## Next Checks

1. Implement a small-scale validation by reproducing PROMST on a single well-defined multi-step task (e.g., BoxNet1) using provided human-engineered prompts and feedback rules, comparing results with baseline methods.

2. Assess robustness of human feedback rules across multiple tasks and error types, identifying gaps or inconsistencies and evaluating impact on optimization performance.

3. Analyze score prediction model performance by training and evaluating on diverse prompt-score pairs from multiple tasks and LLMs, assessing accuracy, generalizability, and computational efficiency compared to alternatives.