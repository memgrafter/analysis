---
ver: rpa2
title: 'Rethinking Data Selection at Scale: Random Selection is Almost All You Need'
arxiv_id: '2410.09335'
source_url: https://arxiv.org/abs/2410.09335
tags:
- data
- selection
- methods
- arxiv
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study reevaluates the effectiveness of supervised fine-tuning\
  \ (SFT) data selection methods on large-scale instruction tuning datasets. When\
  \ scaling to millions of instances, most existing self-scoring selection methods\u2014\
  including LESS, IFD, SelectIT, Entropy, DiverseEvol, and ZIP\u2014fail to significantly\
  \ outperform random selection."
---

# Rethinking Data Selection at Scale: Random Selection is Almost All You Need

## Quick Facts
- **arXiv ID**: 2410.09335
- **Source URL**: https://arxiv.org/abs/2410.09335
- **Reference count**: 13
- **Primary result**: Random selection often matches or exceeds sophisticated data selection methods for large-scale SFT

## Executive Summary
This study reevaluates the effectiveness of supervised fine-tuning (SFT) data selection methods on large-scale instruction tuning datasets. When scaling to millions of instances, most existing self-scoring selection methods—including LESS, IFD, SelectIT, Entropy, DiverseEvol, and ZIP—fail to significantly outperform random selection. Experiments on OpenHermes2.5 (1M samples) and WildChat-1M show that random selection often matches or exceeds the performance of sophisticated selection strategies, particularly when evaluated across diverse downstream tasks like GSM, BBH, HumanEval, MMLU, and IFEval. The analysis reveals that data diversity is more important than data quality in SFT, and that filtering by token length can provide stable, efficient improvements, especially for smaller models like Llama3-8B. The findings suggest that, for large-scale SFT, simple random selection is a strong, cost-effective baseline.

## Method Summary
The study fine-tunes LLaMA3-8B and Qwen2-7B models using the Llama-Factory framework for 3 epochs with batch size 128, cosine learning rate scheduler, and input length of 4096. The evaluation compares random selection against six sophisticated data selection methods (LESS, IFD, SelectIT, Entropy, DiverseEvol, ZIP) across two large-scale datasets: OpenHermes2.5-1M and WildChat-1M. Performance is measured on downstream tasks including GSM (mathematical reasoning), BBH (reasoning), HumanEval (code generation), MMLU (factual knowledge), and IFEval (instruction following). The study also examines token length filtering as a simple selection criterion.

## Key Results
- Random selection often matches or exceeds the performance of sophisticated selection strategies on large-scale SFT datasets
- Data diversity is more critical than data quality during SFT, with diversity-based methods outperforming quality-based ones
- Token length filtering provides stable and efficient improvements, especially for smaller models like Llama3-8B
- Sophisticated selection methods (LESS, IFD, SelectIT, Entropy, DiverseEvol, ZIP) fail to consistently outperform random selection at scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data diversity becomes more important than data quality when scaling SFT datasets to millions of instances.
- Mechanism: As dataset size increases, the marginal benefit of filtering for individual high-quality samples diminishes because the model can learn necessary patterns from the sheer volume of examples. Diverse data ensures broader coverage of instruction types and prevents overfitting to narrow patterns.
- Core assumption: The learning signal from quantity and variety outweighs the precision gain from filtering.
- Evidence anchors:
  - [abstract]: "our comparisons suggest that, during SFT, diversity in data selection is more critical than simply focusing on high-quality data."
  - [section 5.2]: "Tables 1 and 3 demonstrate that the diversity-based selection strategy outperforms the quality-based one."
  - [corpus]: Weak evidence; related works focus on data selection but do not directly compare quality vs diversity at scale.
- Break condition: If the model's capacity is too small to benefit from scale, or if the task requires extreme precision on a narrow domain.

### Mechanism 2
- Claim: Random selection preserves the original distribution and diversity of the dataset, making it surprisingly competitive with sophisticated selection methods at scale.
- Mechanism: Random sampling does not introduce bias toward any subset of the data, so the training distribution remains representative of the full dataset. This avoids the pitfalls of over-filtering or introducing selection bias.
- Core assumption: The original dataset is sufficiently diverse and representative of the target instruction space.
- Evidence anchors:
  - [abstract]: "random selection often matches or exceeds the performance of sophisticated selection strategies."
  - [section 5.1]: "when dealing with extensive and diverse IT datasets, no data selection techniques consistently outperform random sampling by a substantial margin."
  - [corpus]: Weak evidence; related works focus on data selection but do not emphasize random selection as a strong baseline.
- Break condition: If the original dataset contains significant noise or irrelevant data that random sampling would include.

### Mechanism 3
- Claim: Filtering by token length provides stable and efficient improvements, especially for weaker base models and long-text data.
- Mechanism: Longer responses contain more context and complexity, which can be beneficial for training, particularly for models with lower capacity. Token length filtering ensures the inclusion of richer examples without the need for complex quality metrics.
- Core assumption: Longer responses are inherently more informative and useful for training.
- Evidence anchors:
  - [abstract]: "filtering data by token length offers a stable and efficient method for improving results."
  - [section 5.4]: "it is evident that using token length as the criterion for data selection generally yields optimal results."
  - [corpus]: Weak evidence; related works do not discuss token length as a selection criterion.
- Break condition: If the task does not benefit from longer responses, or if the base model is already sufficiently capable.

## Foundational Learning

- **Concept**: Supervised Fine-Tuning (SFT)
  - Why needed here: The study evaluates data selection methods specifically for SFT, which adapts pre-trained models to follow instructions.
  - Quick check question: What is the primary goal of SFT in the context of large language models?

- **Concept**: Data Diversity vs Data Quality
  - Why needed here: The paper contrasts the importance of these two factors in data selection at scale.
  - Quick check question: Why might data diversity become more important than data quality as dataset size increases?

- **Concept**: Random Sampling Bias
  - Why needed here: The study shows that random sampling can be as effective as sophisticated methods, implying that selection bias can be detrimental.
  - Quick check question: How does random sampling preserve the distribution of the original dataset?

## Architecture Onboarding

- **Component map**: Data → Preprocessing → Selection → Training → Evaluation → Analysis
- **Critical path**: Data → Preprocessing → Selection → Training → Evaluation → Analysis
- **Design tradeoffs**: Random selection is simple and unbiased but may include noise; sophisticated methods are targeted but can introduce bias and are computationally expensive.
- **Failure signatures**: If a selection method consistently underperforms random selection, it may be introducing bias or overfitting to a narrow subset of the data.
- **First 3 experiments**:
  1. Replicate the random selection baseline on a new dataset to establish a performance floor.
  2. Compare a simple diversity-based method (e.g., K-means clustering + random selection within clusters) against random selection.
  3. Test token length filtering on a smaller subset of the data to confirm its effectiveness before scaling up.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do data diversity-based selection methods generally outperform quality-based methods in large-scale SFT, despite the expectation that higher quality data would yield better results?
- Basis in paper: [explicit] The paper found that diversity-based selection strategies (like DiverseEvol and ZIP) often outperformed quality-based ones (like LESS, IFD, SelectIT, and Entropy) in large-scale datasets, and that incorporating K-means clustering with quality-based methods improved their performance, suggesting diversity is more important than quality.
- Why unresolved: The paper does not provide a theoretical explanation for why diversity is more critical than quality in large-scale SFT, nor does it explore the mechanisms behind this phenomenon.
- What evidence would resolve it: A theoretical analysis or controlled experiments isolating diversity and quality factors to determine their individual contributions to model performance in large-scale SFT.

### Open Question 2
- Question: How does token length selection work as a proxy for data quality, and why does it perform well across different models and datasets?
- Basis in paper: [explicit] The paper found that filtering data by token length yielded stable and efficient results, especially for weaker models like Llama3-8B, and that using token length as a selection criterion often outperformed more sophisticated methods.
- Why unresolved: The paper does not explain the underlying reasons why token length is a good proxy for data quality or why it is effective across different models and datasets, nor does it explore the optimal token length ranges for different tasks or models.
- What evidence would resolve it: A detailed analysis of the relationship between token length and data quality, including studies on how token length affects model performance across different tasks and model architectures.

### Open Question 3
- Question: What are the limitations of current diversity-based selection methods (DiverseEvol and ZIP) that prevent them from consistently outperforming random selection in large-scale SFT?
- Basis in paper: [explicit] The paper found that diversity-based methods like DiverseEvol and ZIP did not consistently outperform random selection, and that DiverseEvol had high memory and computational requirements, while ZIP was computationally greedy and could not be adaptively tuned.
- Why unresolved: The paper does not explore the specific reasons why these methods fail to outperform random selection, nor does it suggest potential improvements or alternative approaches to address their limitations.
- What evidence would resolve it: A thorough analysis of the algorithmic limitations of DiverseEvol and ZIP, including experiments to identify the factors that contribute to their suboptimal performance and potential modifications to improve their effectiveness.

## Limitations

- Data Composition Uncertainty: The effectiveness of random selection may vary significantly depending on the proportion of high-quality vs noisy data in the source datasets.
- Evaluation Task Coverage: Findings may not extend to specialized domains like medical, legal, or multilingual applications where data quality requirements differ substantially.
- Model Architecture Dependency: Results focus on transformer-based models and may not generalize to other architectures like state-space models or hybrid approaches.

## Confidence

**High Confidence Claims**:
- Random selection as a strong baseline at scale (supported by multiple experiments across two datasets)
- Token length filtering effectiveness (consistent results across experiments)
- Diversity importance over quality at scale (clear patterns in experimental results)

**Medium Confidence Claims**:
- Failure of sophisticated selection methods at scale (limited number of methods tested)
- Generalizability across different base models (only two models evaluated)
- Computational cost-benefit analysis (not systematically measured)

## Next Checks

1. **Cross-dataset validation**: Test the random selection hypothesis on a dataset with known composition ratios (e.g., 80% high-quality, 20% noisy) to verify that random selection remains competitive when the baseline quality varies.

2. **Domain-specific testing**: Evaluate the selection method effectiveness on a specialized domain dataset (medical or legal instructions) to determine if the diversity-over-quality principle holds when task complexity and domain expertise requirements are higher.

3. **Model capacity sensitivity analysis**: Systematically vary model sizes (from 1B to 70B parameters) to identify the threshold at which random selection becomes optimal, and whether this threshold shifts with different task complexities.