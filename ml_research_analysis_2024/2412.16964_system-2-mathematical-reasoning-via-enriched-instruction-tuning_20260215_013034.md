---
ver: rpa2
title: System-2 Mathematical Reasoning via Enriched Instruction Tuning
arxiv_id: '2412.16964'
source_url: https://arxiv.org/abs/2412.16964
tags:
- reasoning
- arxiv
- math
- llms
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling large language models
  (LLMs) to perform system-2 mathematical reasoning, which requires deliberate multi-step
  reasoning. The authors identify the scarcity of deliberate multi-step reasoning
  data as a primary limiting factor and introduce Enriched Instruction Tuning (EIT),
  a method that enriches existing human-annotated mathematical datasets by synergizing
  human and AI feedback to create fine-grained reasoning trajectories.
---

# System-2 Mathematical Reasoning via Enriched Instruction Tuning

## Quick Facts
- arXiv ID: 2412.16964
- Source URL: https://arxiv.org/abs/2412.16964
- Authors: Huanqia Cai; Yijun Yang; Zhifeng Li
- Reference count: 40
- Primary result: EIT achieves 84.1% accuracy on GSM8K and 32.5% on MATH

## Executive Summary
This paper addresses the challenge of enabling large language models (LLMs) to perform system-2 mathematical reasoning, which requires deliberate multi-step reasoning. The authors identify the scarcity of deliberate multi-step reasoning data as a primary limiting factor and introduce Enriched Instruction Tuning (EIT), a method that enriches existing human-annotated mathematical datasets by synergizing human and AI feedback to create fine-grained reasoning trajectories. EIT consists of two critical steps: Enriching with Reasoning Plan (ERP), which generates a high-level plan that breaks down complex instructions into simpler objectives, and Enriching with Reasoning Step (ERS), which fills in reasoning contexts often overlooked by human annotators. The enriched datasets are then used to fine-tune open-source LLMs.

## Method Summary
EIT addresses the challenge of enabling system-2 mathematical reasoning in LLMs by enriching existing human-annotated mathematical datasets with fine-grained reasoning trajectories. The method uses GPT-4 to generate high-level plans (ERP) and fill in missing reasoning steps (ERS), creating smoother reasoning trajectories for LLM fine-tuning. The enriched datasets are then used to fine-tune open-source LLMs like LLaMA-2. The approach leverages human-annotated initial answers as "meta-knowledge" to guide GPT-4 in generating more detailed and precise reasoning processes, mitigating hallucination and error propagation in long-horizon reasoning.

## Key Results
- EIT achieves 84.1% accuracy on GSM8K, surpassing state-of-the-art fine-tuning and prompting methods
- EIT achieves 32.5% accuracy on MATH, matching the performance of tool-augmented methods
- More fine-grained reasoning trajectories consistently lead to higher testing accuracy, suggesting that granularity and size of reasoning datasets are equally important for training math LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enriching human-annotated mathematical datasets with fine-grained reasoning trajectories improves LLM performance by bridging implicit gaps in reasoning steps.
- **Mechanism:** EIT leverages human-annotated initial answers as "meta-knowledge" to guide GPT-4 in generating high-level plans (ERP) and filling in missing reasoning steps (ERS), creating smoother reasoning trajectories for LLM fine-tuning.
- **Core assumption:** The reasoning steps omitted by human annotators are critical for LLM understanding and can be accurately reconstructed by GPT-4 using the initial answer as guidance.
- **Evidence anchors:** [abstract] "Unlike existing CoT prompting methods that generate reasoning chains only depending on LLM's internal knowledge, our method leverages human-annotated initial answers as “meta-knowledge” to help LLMs generate more detailed and precise reasoning processes"

### Mechanism 2
- **Claim:** Fine-grained reasoning trajectories enable better scaling of LLM performance with increased data volume and reasoning step granularity.
- **Mechanism:** By enriching responses with more detailed reasoning steps, EIT creates datasets where more data and more granular steps lead to improved model performance, aligning with the scaling law.
- **Core assumption:** The quality of reasoning data is as important as its quantity, and more fine-grained steps provide better learning signals for LLMs.
- **Evidence anchors:** [section] "We find that more fine-grained reasoning trajectories consistently lead to higher testing accuracy, suggesting that the granularity and size of reasoning datasets are likely to be equally important for training a math LLM expert"

### Mechanism 3
- **Claim:** EIT mitigates hallucination and error propagation in long-horizon reasoning by providing structured guidance through human-annotated answers.
- **Mechanism:** By using human-annotated answers as starting points, EIT constrains the reasoning process, reducing the likelihood of hallucinations and errors that accumulate in standard CoT methods.
- **Core assumption:** Human-annotated answers provide reliable "ground truth" that can guide the generation of correct intermediate reasoning steps.
- **Evidence anchors:** [abstract] "Unlike existing CoT prompting methods that generate reasoning chains only depending on LLM's internal knowledge, our method leverages human-annotated initial answers as “meta-knowledge”"

## Foundational Learning

- **Concept:** System-1 vs. System-2 reasoning in LLMs
  - Why needed here: Understanding the distinction between fast, intuitive reasoning (System-1) and slow, deliberate reasoning (System-2) is crucial for addressing the limitations of current LLMs in mathematical problem-solving.
  - Quick check question: What are the key differences between System-1 and System-2 reasoning, and why does this distinction matter for LLM mathematical reasoning?

- **Concept:** Chain-of-Thought (CoT) prompting
  - Why needed here: CoT prompting is a baseline method for improving LLM reasoning, and understanding its limitations is essential for appreciating the innovations in EIT.
  - Quick check question: How does CoT prompting work, and what are its main limitations in mathematical reasoning tasks?

- **Concept:** Supervised fine-tuning and instruction tuning
  - Why needed here: EIT builds on these fundamental techniques for adapting pre-trained LLMs to specific tasks, so understanding these concepts is crucial for implementing EIT.
  - Quick check question: What is the difference between supervised fine-tuning and instruction tuning, and how do they relate to EIT's approach?

## Architecture Onboarding

- **Component map:** Human-annotated datasets -> GPT-4 (ERP) -> GPT-4 (ERS) -> EITMath dataset -> LLaMA-2 fine-tuning -> Evaluation

- **Critical path:**
  1. Obtain human-annotated mathematical dataset
  2. Use GPT-4 with ERP prompt to generate high-level plans
  3. Use GPT-4 with ERS prompt to fill in missing reasoning steps
  4. Combine original and enriched data to create EITMath dataset
  5. Fine-tune open-source LLM on EITMath using supervised learning
  6. Evaluate fine-tuned model on mathematical reasoning benchmarks

- **Design tradeoffs:**
  - Using GPT-4 for enrichment vs. human annotators (cost vs. quality)
  - Granularity of reasoning steps (detail vs. noise)
  - Dataset size vs. computational resources for fine-tuning
  - Generalization to other mathematical domains vs. performance on specific benchmarks

- **Failure signatures:**
  - Decreased performance on benchmarks despite increased dataset size
  - High perplexity in fine-tuned models on test data
  - Inconsistent reasoning patterns in model outputs
  - Overfitting to specific types of mathematical problems

- **First 3 experiments:**
  1. Ablation study: Compare performance of fine-tuning on original dataset vs. dataset with only ERP enrichment vs. dataset with only ERS enrichment
  2. Scaling experiment: Fine-tune models on EITMath subsets of increasing size (e.g., 10k, 20k, 40k, 80k examples) and measure performance
  3. Generalization experiment: Evaluate fine-tuned models on out-of-distribution mathematical problems to assess robustness of EIT approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EIT scale when applied to datasets that only contain final answers without accompanying reasoning steps?
- Basis in paper: [inferred] The paper discusses the challenge of enriching datasets that contain only final answers and suggests a two-stage process for future work.
- Why unresolved: The paper acknowledges the difficulty but does not provide experimental results or implementation details for this scenario.
- What evidence would resolve it: Experimental results showing the effectiveness of the proposed two-stage process on datasets with only final answers, including performance metrics and comparison with existing methods.

### Open Question 2
- Question: What is the impact of different temperature coefficients when sampling GPT-4's output distribution for constructing larger EITMath datasets?
- Basis in paper: [explicit] The paper mentions constructing a 70k dataset by sampling GPT-4's output distribution with different temperature coefficients, which slightly improves performance.
- Why unresolved: The paper does not provide detailed analysis of how different temperature coefficients affect the quality and diversity of the generated responses or the overall performance of the fine-tuned models.
- What evidence would resolve it: A systematic study varying temperature coefficients, analyzing the resulting dataset characteristics, and correlating these with model performance metrics on benchmark tasks.

### Open Question 3
- Question: How does the granularity of reasoning steps in EITMath affect the performance of fine-tuned models on different types of mathematical problems?
- Basis in paper: [explicit] The paper shows that more fine-grained reasoning steps lead to higher testing accuracy and suggests that granularity and size of reasoning datasets are equally important.
- Why unresolved: The paper does not explore how the granularity of reasoning steps specifically impacts performance on different mathematical problem categories or difficulty levels.
- What evidence would resolve it: An analysis correlating the granularity of reasoning steps in EITMath with model performance across various mathematical problem types, potentially revealing optimal granularity levels for different problem categories.

## Limitations

- The method relies heavily on GPT-4's ability to accurately reconstruct missing reasoning steps, which may not generalize to all types of mathematical problems or reasoning domains.
- The paper lacks ablation studies examining which components of EIT (ERP vs. ERS) contribute most to performance gains.
- Dataset composition and enrichment quality metrics are not fully transparent, making it difficult to assess the robustness of the approach.

## Confidence

**High Confidence:** The experimental results showing EIT's superior performance on GSM8K (84.1%) and MATH (32.5%) benchmarks compared to baseline fine-tuning and prompting methods. The controlled experimental setup and clear metrics support this claim.

**Medium Confidence:** The mechanism by which ERP and ERS improve reasoning quality. While the general approach is sound and results are positive, the exact contribution of each enrichment step and their interaction effects remain unclear.

**Low Confidence:** The generalizability of EIT to mathematical domains beyond GSM8K and MATH, and to non-mathematical reasoning tasks. The paper focuses narrowly on these specific benchmarks without exploring broader applicability.

## Next Checks

1. **Enrichment Quality Audit:** Conduct a blind evaluation where human experts rate the accuracy and helpfulness of GPT-4-generated reasoning steps versus original human annotations, comparing EIT-enriched datasets against un-enriched baselines.

2. **Component Ablation Study:** Systematically evaluate models fine-tuned on: (a) original datasets only, (b) datasets with only ERP enrichment, (c) datasets with only ERS enrichment, and (d) fully EIT-enriched datasets to isolate the contribution of each component.

3. **Generalization Testing:** Evaluate EIT-fine-tuned models on out-of-distribution mathematical problems and non-mathematical reasoning tasks (e.g., logical reasoning, commonsense reasoning) to assess whether the approach generalizes beyond the training benchmarks.