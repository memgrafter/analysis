---
ver: rpa2
title: Large-Scale Dataset Pruning in Adversarial Training through Data Importance
  Extrapolation
arxiv_id: '2406.13283'
source_url: https://arxiv.org/abs/2406.13283
tags:
- training
- adversarial
- data
- pruning
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of scaling data pruning for adversarial
  training on large synthetic datasets, addressing the computational burden of training
  robust models. The core idea is to extrapolate data importance scores from a small
  set of training examples to a larger, unseen dataset using feature embeddings and
  k-nearest neighbors.
---

# Large-Scale Dataset Pruning in Adversarial Training through Data Importance Extrapolation

## Quick Facts
- arXiv ID: 2406.13283
- Source URL: https://arxiv.org/abs/2406.13283
- Reference count: 10
- This paper proposes a k-NN based extrapolation method to prune large synthetic datasets for adversarial training, achieving 81.38% robust accuracy on CIFAR-10 with 1.4 percentage points improvement over baseline.

## Executive Summary
This paper addresses the computational challenge of adversarial training on large synthetic datasets by proposing a data importance extrapolation framework. The key insight is that data importance scores can be reliably transferred from a small source set to a larger destination set using k-nearest neighbors in feature embedding space. The authors adapt Dynamic Uncertainty (DU) for adversarial training and introduce Frequency Pruning (FP) as an alternative metric. Experiments on 2 million synthetic CIFAR-10 samples demonstrate that their approach maintains robust accuracy while significantly reducing dataset size, enabling more efficient adversarial training at scale.

## Method Summary
The method involves training a model on a small subset of data to obtain Dynamic Uncertainty scores, then extrapolating these importance scores to a larger unseen dataset using k-NN search in feature embedding space. The framework supports multiple embedding models (DINOv2, SSCD, ResNet) and distance metrics (cosine, Euclidean). Data is pruned based on extrapolated importance scores, and the pruned dataset is used for adversarial training with TRADES framework. The approach is evaluated on CIFAR-10 with ℓ∞ and ℓ2 norm attacks, comparing various pruning strategies including class-balanced variants.

## Key Results
- Class-balanced pruning with Frequency Pruning (FP) on ℓ2-norm attacks achieves 81.38% robust accuracy, outperforming baseline by 1.4 percentage points
- Extrapolation-based pruning reduces dataset size by 50-75% while maintaining or improving robust accuracy
- The approach successfully scales adversarial training to 2 million synthetic samples with manageable computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data importance scores can be reliably extrapolated from a small subset to a large unseen dataset using k-nearest neighbors in feature embedding space.
- Mechanism: The approach computes feature embeddings for both a small source set (S) with known DU scores and a larger destination set (D). For each sample in D, it finds the k nearest neighbors in S based on embedding similarity and averages their DU scores to estimate the importance of the D sample.
- Core assumption: The feature embedding space preserves semantic similarity such that samples with similar features have similar data importance scores.
- Evidence anchors:
  - [abstract] "extrapolate data importance scores from a small set of training examples to a larger, unseen dataset using feature embeddings and k-nearest neighbors"
  - [section] "We base our extrapolation on a k-nearest neighbor search using an embedding function E : Rd → Rd′"
- Break condition: If the embedding space does not preserve semantic similarity between source and destination datasets, or if the distribution of important vs unimportant samples differs significantly between S and D.

### Mechanism 2
- Claim: Dynamic Uncertainty (DU) captures training dynamics that differentiate important from unimportant samples for adversarial training.
- Mechanism: DU measures the average prediction uncertainty over a sliding window during training. For adversarial training, this captures how much a sample's prediction fluctuates when perturbed, which correlates with its importance for robustness.
- Core assumption: Samples with high prediction uncertainty fluctuations during adversarial training are more important for learning robust representations.
- Evidence anchors:
  - [abstract] "We adopt the data pruning strategy DU to the domain of adversarial training and highlight differences in data importance between adversarial and standard training"
  - [section] "DU quantifies the fluctuating overall uncertainty u := U(x, y) of each sample x with class label y throughout the training process"
- Break condition: If prediction uncertainty does not correlate with sample importance for adversarial robustness, or if the relationship changes significantly across different model architectures or training regimes.

### Mechanism 3
- Claim: Frequency Pruning (FP) captures both short-term and long-term training dynamics, making it more effective than DU for some adversarial training scenarios.
- Mechanism: FP computes the Discrete Fourier Transform of training dynamics and uses the magnitude of frequency components as importance scores, capturing both oscillations and convergence trends.
- Core assumption: Both high-frequency oscillations and low-frequency convergence patterns in training dynamics contain information about sample importance.
- Evidence anchors:
  - [section] "We propose a simple pruning metric called Frequency Pruning (FP), which considers both short-term trends and long-term training dynamics"
  - [section] "In FP we first calculate the DFT from the training dynamics and calculate the data importance scores as the magnitude of the DFT"
- Break condition: If the frequency spectrum of training dynamics does not contain discriminative information about sample importance, or if computational cost outweighs benefits.

## Foundational Learning

- Concept: Feature embeddings and semantic similarity
  - Why needed here: The k-NN extrapolation relies on finding semantically similar samples in the embedding space to transfer importance scores
  - Quick check question: How does the choice of embedding model (DINOv2, SSCD, ResNet) affect the quality of k-NN extrapolation?

- Concept: Adversarial training and robustness metrics
  - Why needed here: Understanding how adversarial examples are generated and how robustness is measured is crucial for interpreting the results
  - Quick check question: What is the difference between clean accuracy and robust accuracy, and why does adversarial training typically reduce clean accuracy?

- Concept: Data pruning and importance scoring
  - Why needed here: The paper builds on existing data pruning techniques and extends them to adversarial training
  - Quick check question: How does Dynamic Uncertainty differ from other importance metrics like forgetting scores or error norms?

## Architecture Onboarding

- Component map: CIFAR-10/Synthetic Data → Feature Embedding → k-NN Search → DU Calculation → Extrapolation → Pruning → Adversarial Training → Evaluation
- Critical path: Dataset → Feature Embedding → k-NN Search → DU Calculation → Extrapolation → Pruning → Training → Evaluation
- Design tradeoffs:
  - Embedding model choice: More sophisticated models may capture better semantics but increase computational cost
  - k value in k-NN: Higher k provides smoother estimates but may dilute local patterns
  - Source dataset size: Larger source sets improve extrapolation quality but increase initial computation
  - Distance metric: Cosine similarity vs Euclidean distance affects neighbor selection
- Failure signatures:
  - Poor correlation between extrapolated and ground truth DU scores
  - Degradation in robust accuracy after pruning
  - High variance in k-NN estimates due to sparse embedding space
  - Class imbalance in pruned dataset affecting model performance
- First 3 experiments:
  1. Run DU calculation on a small subset of CIFAR-10 to establish baseline scores
  2. Test k-NN extrapolation on a held-out validation set with known scores to tune hyperparameters
  3. Compare clean vs adversarial DU scores on the same dataset to verify mechanism 2 claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of data importance extrapolation compare when using more sophisticated embedding models like foundation models beyond DINOv2, SSCD, and ResNet?
- Basis in paper: [inferred] The paper mentions that the current extrapolation approach uses DINOv2, SSCD, and ResNet as encoding strategies, but suggests that the approach could be extended to more complex embedding functions such as foundation models.
- Why unresolved: The paper only tests three specific embedding models and does not explore the potential benefits of using more advanced or specialized foundation models for data importance extrapolation.
- What evidence would resolve it: Conducting experiments comparing the performance of data importance extrapolation using various advanced foundation models against the current models (DINOv2, SSCD, ResNet) would provide evidence on whether more sophisticated models lead to better extrapolation accuracy and downstream model performance.

### Open Question 2
- Question: Can the extrapolation approach be effectively scaled to larger datasets beyond CIFAR-10, such as ImageNet, and what are the potential challenges and solutions?
- Basis in paper: [inferred] The paper acknowledges that scaling to larger datasets like ImageNet was out of the scope of the current work and leaves it as future research.
- Why unresolved: The paper does not provide any experimental results or theoretical analysis on how the extrapolation approach would perform on significantly larger and more complex datasets.
- What evidence would resolve it: Conducting experiments on larger datasets like ImageNet and analyzing the performance, computational efficiency, and any unique challenges that arise when scaling the extrapolation approach would provide evidence on its feasibility and potential limitations.

### Open Question 3
- Question: How do different data attribution metrics, which provide deeper insights into data importance, compare to the Dynamic Uncertainty (DU) and Frequency Pruning (FP) metrics used in this paper when extrapolated to large datasets?
- Basis in paper: [explicit] The paper suggests that extrapolating data attribution metrics could be a promising approach for future research, as they provide deeper insights into data importance but demand significant computational resources.
- Why unresolved: The paper only explores DU and FP metrics for data importance extrapolation and does not investigate other potential metrics that could offer more informative insights.
- What evidence would resolve it: Implementing and comparing the performance of various data attribution metrics (e.g., influence functions, Shapley values) when extrapolated to large datasets would provide evidence on whether these metrics offer significant advantages over DU and FP in terms of pruning efficiency and model robustness.

## Limitations
- The extrapolation framework's effectiveness depends on the assumption that embedding space similarity correlates with training importance, which may not hold for datasets with different distributions
- Experiments are limited to CIFAR-10 and synthetic CIFAR-10 data, leaving uncertainty about generalization to other datasets and model architectures
- The novelty of the approach is limited as it builds on existing data pruning techniques without establishing fundamental differences in the adversarial training context

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Experimental methodology and reproducibility | High |
| Mechanism's generalizability across datasets | Medium |
| Novelty of the approach | Low |

## Next Checks
1. **Embedding space validation**: Test the correlation between k-NN extrapolation accuracy and downstream robust performance across different embedding models (DINOv2, SSCD, ResNet) and distance metrics to establish which configurations provide the most reliable importance transfer.

2. **Cross-dataset generalization**: Apply the extrapolation framework to non-synthetic datasets (e.g., SVHN, TinyImageNet) to test whether the approach generalizes beyond the synthetic CIFAR-10 data used in the main experiments.

3. **Ablation study on source set size**: Systematically vary the size of the source dataset S (from 10k to 100k samples) to quantify the relationship between source set size and extrapolation quality, establishing practical guidelines for when this approach becomes computationally beneficial.