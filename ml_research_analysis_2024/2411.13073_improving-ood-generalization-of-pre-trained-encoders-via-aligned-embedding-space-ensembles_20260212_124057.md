---
ver: rpa2
title: Improving OOD Generalization of Pre-trained Encoders via Aligned Embedding-Space
  Ensembles
arxiv_id: '2411.13073'
source_url: https://arxiv.org/abs/2411.13073
tags:
- embedding
- ensemble
- encoders
- mean
- single
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve out-of-distribution (OOD)
  generalization of pre-trained encoders via aligned embedding-space ensembles. The
  core idea is to align the embedding spaces of multiple self-supervised pre-trained
  encoders so that their ensemble mean recovers the correct latents, thereby improving
  embedding quality on both in-distribution and OOD data.
---

# Improving OOD Generalization of Pre-trained Encoders via Aligned Embedding-Space Ensembles

## Quick Facts
- arXiv ID: 2411.13073
- Source URL: https://arxiv.org/abs/2411.13073
- Reference count: 40
- Pre-trained encoder ensembles with aligned embedding spaces improve OOD generalization on MNIST

## Executive Summary
This paper proposes a method to improve out-of-distribution (OOD) generalization of pre-trained self-supervised encoders through aligned embedding-space ensembles. The core innovation is unsupervised alignment of embedding spaces from multiple encoders via orthogonal transformations, followed by aggregation using Karcher mean on hyperspherical embeddings. Theoretical analysis shows that embedding spaces of different encoders are related by orthogonal transformations, enabling alignment without labels. Experiments on MNIST demonstrate that the aligned ensemble improves embedding quality metrics (R@1 and MAP@R) over single models and unaligned ensembles, with larger gains in OOD settings.

## Method Summary
The method aligns embedding spaces of pre-trained self-supervised encoders using orthogonal transformation matrices learned via geodesic distance minimization with orthogonality regularization. Given multiple encoders trained on the same data, the approach learns orthogonal matrices to align their embedding spaces, then aggregates aligned embeddings using Karcher mean on the hyperspherical manifold. The alignment is unsupervised and exploits the theoretical property that encoders recover the same latents up to orthogonal transformations. After alignment, the ensemble mean recovers the ground truth latents, improving embedding quality on both in-distribution and OOD data.

## Key Results
- Aligned embedding-space ensemble improves R@1 and MAP@R metrics over single models and unaligned ensembles
- Larger performance gains observed in OOD settings (colored and cropped MNIST) compared to in-distribution
- Alignment enables recovery of correct latents through ensemble mean computation on aligned embedding spaces

## Why This Works (Mechanism)

### Mechanism 1
Orthogonal transformation relationship allows unsupervised alignment of embedding spaces. The paper proves that two encoders trained on the same data recover the same latents up to an orthogonal transformation matrix R, enabling alignment by learning R without labels. This relies on the assumption that both encoders recover correct latents up to orthogonal transformations.

### Mechanism 2
Aligned embedding-space ensemble recovers correct latents. Once embedding spaces are aligned using learned orthogonal matrices, the ensemble mean of aligned embeddings recovers the ground truth latents up to orthogonal transformation. This is based on the theoretical result that aligned ensemble mean recovers correct latents.

### Mechanism 3
Karcher Mean provides meaningful aggregation on hyperspherical embeddings. The algorithm projects hyperspherical data points onto a linear tangent space, calculates mean in tangent space, then projects back to sphere, enabling meaningful ensemble mean computation. This relies on the assumption that embedding spaces are hyperspherical (L2-normalized unit-hyperspherical).

## Foundational Learning

- **Concept: Hyperspherical embeddings and their properties**
  - Why needed here: The entire framework relies on embeddings being L2-normalized on unit-hyperspheres, which enables the use of geodesic distances and Karcher Mean
  - Quick check question: What geometric property allows us to use geodesic distance instead of Euclidean distance for alignment?

- **Concept: Orthogonal transformations and their preservation of inner products**
  - Why needed here: The theoretical foundation relies on the fact that orthogonal transformations preserve dot products, enabling the recovery of correct latents
  - Quick check question: If R is an orthogonal matrix, what property does R^TR have?

- **Concept: Contrastive learning and InfoNCE loss**
  - Why needed here: The encoders are pre-trained using InfoNCE loss, which creates the embedding spaces that the ensemble operates on
  - Quick check question: What is the primary objective of InfoNCE loss in terms of embedding space geometry?

## Architecture Onboarding

- **Component map**: Pre-trained encoder → Alignment Layer (learns orthogonal matrix R) → Karcher Mean aggregator → Ensemble embedding
- **Critical path**: Pre-trained encoders → Unsupervised alignment (learn R) → Karcher Mean aggregation → Evaluate embedding quality
- **Design tradeoffs**:
  - Single vs. multiple encoders: More encoders provide diversity but increase computational cost
  - Alignment strength (λ parameter): Higher λ enforces stricter orthogonality but may over-constrain alignment
  - Aggregation method: Karcher Mean vs. simple arithmetic mean - Karcher Mean better handles spherical geometry
- **Failure signatures**:
  - Performance worse than single models: Indicates alignment is harmful (misaligned ensembles)
  - Slow convergence during alignment: Suggests orthogonality constraint is too strong
  - Large standard deviation in performance: Indicates instability in alignment process
- **First 3 experiments**:
  1. Implement single InfoNCE encoder and verify it produces hyperspherical embeddings
  2. Create two encoders and implement unsupervised alignment - verify learned R matrices are approximately orthogonal
  3. Implement Karcher Mean aggregation and compare with simple arithmetic mean on aligned embeddings

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the text provided.

## Limitations

- Theoretical framework relies on strong assumptions about orthogonal transformation relationships that may not hold perfectly in practice
- Empirical evaluation limited to MNIST dataset, which may not generalize to more complex real-world scenarios
- Performance gains, while statistically significant, are modest (5-7% improvement in R@1 for some OOD tasks)
- Paper lacks ablation studies on number of encoders or hyperparameters like orthogonality regularization strength λ

## Confidence

- **High confidence**: The core theoretical result that orthogonal transformations preserve inner products and can be learned via geodesic distance minimization is mathematically sound and well-established
- **Medium confidence**: The empirical results showing performance improvements on MNIST, as the results are statistically significant but limited to one dataset and relatively simple transformations
- **Low confidence**: Generalization to more complex datasets and real-world scenarios, as the current evaluation is limited to MNIST with simple color and crop augmentations

## Next Checks

1. **Ablation study on number of encoders**: Test the method with 2, 3, 5, and 10 encoders to quantify how ensemble size affects performance gains and computational overhead

2. **Transfer to larger datasets**: Evaluate the method on CIFAR-10, CIFAR-100, and a real-world dataset like ImageNet-1K to assess generalization beyond MNIST

3. **Hyperparameter sensitivity analysis**: Systematically vary the orthogonality regularization strength λ and alignment learning rate to identify optimal values and robustness to hyperparameter choices