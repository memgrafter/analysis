---
ver: rpa2
title: 'Towards Santali Linguistic Inclusion: Building the First Santali-to-English
  Translation Model using mT5 Transformer and Data Augmentation'
arxiv_id: '2411.19726'
source_url: https://arxiv.org/abs/2411.19726
tags:
- data
- santali
- language
- translation
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating machine translation
  models for Santali, a low-resource Austroasiatic language spoken by approximately
  seven million people. The authors built the first Santali-to-English translation
  model using the mT5 transformer and data augmentation techniques to overcome data
  scarcity.
---

# Towards Santali Linguistic Inclusion: Building the First Santali-to-English Translation Model using mT5 Transformer and Data Augmentation

## Quick Facts
- **arXiv ID**: 2411.19726
- **Source URL**: https://arxiv.org/abs/2411.19726
- **Reference count**: 9
- **Primary result**: Built the first Santali-to-English translation model achieving 10.5 BLEU-4 score with mT5 and data augmentation

## Executive Summary
This paper addresses the challenge of creating machine translation models for Santali, a low-resource Austroasiatic language spoken by approximately seven million people. The authors built the first Santali-to-English translation model using the mT5 transformer and data augmentation techniques to overcome data scarcity. They constructed a parallel corpus from the Santali Bible and compared translation performance between mT5 and a seq2seq model, finding that mT5 achieved a BLEU-4 score of 6.79 on the test set compared to seq2seq's 1.87. Data augmentation improved the mT5 performance to 10.5 BLEU-4 on the test set. The study also compared Santali-English and Santali-Bangla translations, finding the English model performed better due to the mT5 model being trained on more English data.

## Method Summary
The authors constructed a Santali-English parallel corpus from the Santali Bible (29,651 sections, 69,086 sentences, 663,684 words) by crawling bible.com data. They trained mT5-small (300M parameters) using this corpus and applied data augmentation techniques including synonym replacement, deletion, swaps, insertion, and back-translation using TextAttack's EasyDataAugmenter for English and bnaug for Bangla. The model used SentencePiece tokenization with a custom T5 tokenizer trained on Santali data. Evaluation was performed using BLEU-4 scores on a held-out test set.

## Key Results
- mT5-small achieved 6.79 BLEU-4 score compared to seq2seq's 1.87 on the test set
- Data augmentation improved mT5 performance to 10.5 BLEU-4 score
- Santali-English translation performed better than Santali-Bangla (2.7 BLEU-4) due to mT5's English pretraining data bias
- BLEU score improved by up to 2.9 points with augmentation and 3.2 points with back-translation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning via mT5 improves low-resource translation because mT5 was pretrained on massive multilingual data, including English.
- Mechanism: The model reuses learned linguistic representations and attention patterns from the pretraining corpus, reducing the need for large task-specific data.
- Core assumption: Pretraining data contains sufficient coverage of the target language(s) or related linguistic features to transfer effectively.
- Evidence anchors: [abstract] "Santali-English parallel corpus performs better when in transformers like mT5 as opposed to untrained transformers, proving that transfer learning can be a viable technique that works with Santali language."

### Mechanism 2
- Claim: Data augmentation improves BLEU scores by synthetically expanding the parallel corpus.
- Mechanism: Augmentation methods (synonym replacement, deletion, swaps, insertion, back-translation) generate new sentence pairs, increasing training diversity without introducing semantic drift.
- Core assumption: Augmented sentences remain faithful to original meaning and preserve translation context.
- Evidence anchors: [abstract] "with data augmentation, our model performs better."

### Mechanism 3
- Claim: Tokenization strategy affects model performance due to vocabulary coverage and language-specific features.
- Mechanism: mT5 uses SentencePiece tokenization; mismatch between tokenization granularity and language morphology can lead to suboptimal subword units, especially for languages with unique scripts or morphology.
- Core assumption: Tokenizers can adequately segment low-resource languages without domain-specific tuning.
- Evidence anchors: [abstract] "MT5 model is compatible with only sentence piece tokenizers."

## Foundational Learning

- **Concept: Parallel corpora alignment**
  - Why needed here: Ensures source-target sentence pairs match semantically and structurally for effective training.
  - Quick check question: How does the model handle variable-length sentence units during preprocessing?

- **Concept: Word embeddings and vectorization**
  - Why needed here: Enables the model to learn semantic relationships and contextual similarities between words.
  - Quick check question: What does the t-SNE plot reveal about word similarity in Santali?

- **Concept: Attention mechanisms**
  - Why needed here: Allows the decoder to focus on relevant encoder states, improving alignment and translation quality.
  - Quick check question: How does the attention decoder differ from a vanilla decoder in seq2seq models?

## Architecture Onboarding

- **Component map**: Data pipeline → Preprocessing → Tokenization → Model (mT5/seq2seq) → Training → Evaluation (BLEU)
- **Critical path**: Data cleaning → Alignment → Tokenization → Model selection → Training with augmentation → Evaluation
- **Design tradeoffs**: mT5 small (300M params) chosen over larger models for resource constraints vs. potential performance gains.
- **Failure signatures**: Low BLEU → check data quality, tokenizer fit, or augmentation noise; overfitting → more regularization or data.
- **First 3 experiments**:
  1. Train mT5-small on raw Santali-English corpus, measure baseline BLEU.
  2. Apply data augmentation and retrain, compare BLEU gains.
  3. Swap tokenizer: use custom Santali SentencePiece vs. default T5, measure impact.

## Open Questions the Paper Calls Out

- **Open Question 1**: Would transfer learning and data augmentation techniques be effective for translating Santali to other low-resource languages beyond English and Bangla?
- **Open Question 2**: How would the translation model's performance improve if augmented with non-biblical Santali text sources, such as conversational or domain-specific data?
- **Open Question 3**: Would training mT5 with Santali-English data improve translation quality if the model were fine-tuned with additional Santali text data to better understand the source language?

## Limitations
- Model trained exclusively on Bible text, limiting generalization to general-purpose translation tasks
- Evaluation relies solely on BLEU-4 scores without human evaluation or other linguistic quality metrics
- Tokenization details for Santali language-specific features not fully specified

## Confidence
- **High confidence**: Core claim that mT5 outperforms seq2seq (6.79 vs 1.87 BLEU-4)
- **Medium confidence**: Data augmentation effectiveness claim (BLEU improvement from 6.79 to 10.5)
- **Low confidence**: Model's practical utility for real-world Santali-English translation

## Next Checks
1. **Domain generalization test**: Evaluate the trained model on non-biblical Santali text (news, social media, or everyday conversation) to measure how well it generalizes beyond the training domain.
2. **Human evaluation**: Conduct blind human assessments comparing translations from the baseline and augmented models to determine if BLEU improvements correlate with perceived translation quality.
3. **Tokenizer ablation study**: Compare the custom Santali tokenizer against standard mT5 tokenization on Santali text to quantify the impact of language-specific tokenization on translation performance.