---
ver: rpa2
title: Can a Hallucinating Model help in Reducing Human "Hallucination"?
arxiv_id: '2405.00843'
source_url: https://arxiv.org/abs/2405.00843
tags:
- llms
- acuerdo
- beliefs
- belief
- cognitive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the unwarranted belief levels of humans and
  large language models (LLMs) using the PEUBI psychometric test. Three LLMs (GPT-3.5,
  GPT-4, and Gemini) were tested alongside average human responses across 36 belief
  statements.
---

# Can a Hallucinating Model help in Reducing Human "Hallucination"?

## Quick Facts
- arXiv ID: 2405.00843
- Source URL: https://arxiv.org/abs/2405.00843
- Authors: Sowmya S Sundaram; Balaji Alwar
- Reference count: 9
- LLMs outperform humans on unwarranted belief detection but show unstable rationality under negation and language changes

## Executive Summary
This study compares human and LLM performance on the PEUBI psychometric test measuring unwarranted beliefs. Three LLMs (GPT-3.5, GPT-4, Gemini) consistently outperformed average human responses, with GPT-4 showing the highest rationality by strongly rejecting conspiracy theories. However, the study reveals that LLM "rationality" is unstable - when statements were negated, responses were inconsistent, and performance degraded in low-resource languages like Spanish. The authors propose that LLMs exhibit "unstable rationality" - a form of reasoning that appears rational but collapses under minor perturbations.

## Method Summary
The researchers prompted three LLMs (GPT-3.5, GPT-4, Gemini) with 36 PEUBI statements using zero-shot prompting in English, comparing their 1-5 scale responses to human averages. They then created a Spanish translation and repeated the experiment with ChatGPT to test language stability. The study analyzed consistency across negations of statements and developed a prototype using LLMs as personalized misinformation debunking agents based on cognitive dissonance and elaboration likelihood theories.

## Key Results
- LLMs consistently outperformed average humans on unwarranted belief detection, with GPT-4 showing highest rationality
- LLM rationality is unstable - responses become inconsistent when statements are negated or presented in low-resource languages
- Prototype demonstrates potential for using LLMs as personalized misinformation debunking agents, though effectiveness on humans remains untested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can outperform average humans on unwarranted belief detection because they are trained on broad, fact-checked corpora that implicitly encode rational discourse patterns.
- Mechanism: The models absorb probabilistic reasoning cues from large-scale web text, enabling them to assign low plausibility scores to unsupported claims during inference.
- Core assumption: Training data contains enough high-quality, fact-based content to shift model predictions toward rational outcomes.
- Evidence anchors:
  - [abstract] "The LLMs consistently outperformed humans, with GPT-4 showing the highest rationality by strongly rejecting conspiracy theories."
  - [section] "Despite these inconsistencies in reasoning, LLMs demonstrated moderate proficiency on the PEUBI benchmark."
  - [corpus] Weak evidence - only tangentially related papers discuss hallucination frameworks, not belief detection.
- Break condition: If training data contains proportionally more conspiracy content than rational counter-arguments, model predictions reverse.

### Mechanism 2
- Claim: Unstable rationality emerges because LLMs do not maintain consistent belief states; small perturbations (e.g., negation, language change) cause large shifts in outputs.
- Mechanism: Without explicit logical consistency constraints, the models treat each prompt independently, so negations or language shifts break the inferred rationality pattern.
- Core assumption: The model lacks a persistent belief revision mechanism that enforces coherence across related prompts.
- Evidence anchors:
  - [abstract] "However, the study reveals that LLM 'rationality' is unstable - when statements were negated, responses were inconsistent..."
  - [section] "This observation suggests the presence of a unique form of 'competence' in LLMs, one that is neither inherently intelligent nor rational."
  - [corpus] No direct evidence; corpus neighbors focus on hallucination detection, not consistency under negation.
- Break condition: Adding a belief revision step or logical consistency fine-tuning could stabilize responses.

### Mechanism 3
- Claim: LLMs can function as personalized misinformation debunking agents by leveraging cognitive dissonance and elaboration likelihood theories, despite their own unstable rationality.
- Mechanism: The models simulate persuasive dialogues that either introduce contradictory evidence (cognitive dissonance) or offer alternative heuristic cues (elaboration likelihood), prompting belief revision in humans.
- Core assumption: Human users respond to model-generated arguments as if they were from a credible source, regardless of the model's own belief stability.
- Evidence anchors:
  - [abstract] "They demonstrate a prototype using LLMs as personalized misinformation debunking agents based on cognitive dissonance and elaboration likelihood theories..."
  - [section] "Both of these attempts of persuasion are noteworthy... this prototype suggests, that with enough caution, one can utilize LLMs as a personalized persuasion tool..."
  - [corpus] Weak evidence - only general persuasion and misinformation papers, not specific human-LLM interaction studies.
- Break condition: If users detect model inconsistency or unreliability, trust erodes and persuasion fails.

## Foundational Learning

- Concept: Belief formation and revision models
  - Why needed here: Understanding how humans update beliefs is essential to design LLM persuasion protocols that exploit cognitive dissonance.
  - Quick check question: What distinguishes knowledge from belief in epistemic terms?

- Concept: Dual-process theory of cognition
  - Why needed here: Explains why LLMs can score high on rational belief tests yet still be vulnerable to bias; they lack intuitive/emotional processing but mirror its traces in training data.
  - Quick check question: How does dual-process theory account for scientifically literate individuals holding pseudoscientific beliefs?

- Concept: Psychometric test construction
  - Why needed here: PEUBI's structure (36 items, 5-point Likert scale) determines how LLM responses are scored and compared to human baselines.
  - Quick check question: What does a score of 1 vs 5 represent on the PEUBI scale?

## Architecture Onboarding

- Component map:
  Input processor → PEUBI item tokenizer → LLM inference engine (GPT-4, GPT-3.5, Gemini) → Response normalizer (maps raw output to 1-5 scale) → Consistency checker (optional: cross-item coherence) → Persuasion module (dialogue generator using psychological theories)

- Critical path:
  1. Tokenize PEUBI item
  2. Generate LLM response
  3. Normalize to scale
  4. Store result for comparison
  5. (Optional) Run consistency check

- Design tradeoffs:
  - Zero-shot inference vs. fine-tuned belief models
  - Scale granularity (5-point vs. binary)
  - Consistency enforcement vs. natural language fluency

- Failure signatures:
  - Inconsistent responses under negation
  - Defaulting to mid-scale values in low-resource languages
  - Overconfident extreme responses on unsupported claims

- First 3 experiments:
  1. Replicate PEUBI in English and Spanish to measure language stability.
  2. Negate all PEUBI items and compare response flips to detect inconsistency.
  3. Run a small-scale human study: measure belief change after LLM-generated persuasive dialogue.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How stable is the "unstable rationality" phenomenon across different language models beyond GPT-3.5, GPT-4, and Gemini?
- Basis in paper: [explicit] The paper mentions limited testing to three LLMs and suggests more experimentation with various LLMs.
- Why unresolved: The study only examined three LLMs with limited variations in prompts and negations. Different architectures and training approaches could yield different stability patterns.
- What evidence would resolve it: Systematic testing of 10+ diverse LLMs (including open-source models) across multiple languages with consistent negation and prompt variations.

### Open Question 2
- Question: Does debiasing training data actually reduce the inconsistencies observed in LLMs' reasoning?
- Basis in paper: [explicit] The authors hypothesize that factually consistent training data could produce better rationality models and suggest this needs empirical research.
- Why unresolved: This remains a hypothesis without experimental validation. The relationship between training data consistency and reasoning stability is not empirically established.
- What evidence would resolve it: Controlled experiments training LLMs on datasets with varying levels of factual consistency, then measuring PEUBI performance and negation stability.

### Open Question 3
- Question: Can LLM-powered persuasion actually change human unwarranted beliefs, and does this change persist over time?
- Basis in paper: [explicit] The authors propose this as a future direction but only tested the prototype on the LLM itself, not on humans.
- Why unresolved: The prototype shows the LLM can simulate persuasive conversations, but there's no evidence it works on actual humans or that any belief changes persist.
- What evidence would resolve it: Randomized controlled trials with human subjects, measuring PEUBI scores before/after LLM interaction and tracking persistence at 1-week, 1-month, and 6-month intervals.

## Limitations
- The core paradox of LLMs both detecting and generating misinformation remains unresolved mechanistically
- PEUBI test validation in Spanish reveals performance degradation, but it's unclear whether this reflects genuine linguistic/cultural differences versus model limitations
- The prototype persuasion system is presented without empirical validation of its effectiveness on human belief change

## Confidence
- High confidence: LLMs consistently outperform average humans on PEUBI benchmark in English
- Medium confidence: "Unstable rationality" phenomenon is reproducible across LLMs and negations
- Low confidence: Effectiveness of LLM-based persuasion systems for reducing human unwarranted beliefs

## Next Checks
1. Conduct a controlled human study measuring actual belief change after exposure to LLM-generated debunking arguments versus control conditions
2. Test PEUBI consistency across multiple languages beyond Spanish (e.g., Mandarin, Arabic) to determine if instability is language-specific or universal
3. Implement a belief revision mechanism in LLMs and measure whether consistency under negation improves while maintaining high PEUBI scores