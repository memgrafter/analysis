---
ver: rpa2
title: 'SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning'
arxiv_id: '2402.11896'
source_url: https://arxiv.org/abs/2402.11896
tags:
- lora
- number
- adapter
- peft
- sibo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SIBO introduces a simple residual injection to mitigate over-smoothing
  in parameter-efficient fine-tuning (PEFT) methods for large language models. By
  preserving a portion of the initial token representation in each layer, SIBO enhances
  the performance of popular PEFT techniques such as Adapter and LoRA.
---

# SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning

## Quick Facts
- arXiv ID: 2402.11896
- Source URL: https://arxiv.org/abs/2402.11896
- Authors: Zhihao Wen; Jie Zhang; Yuan Fang
- Reference count: 40
- Primary result: SIBO achieves up to 15.7% and 23.5% improvement over existing PEFT methods on arithmetic and commonsense reasoning tasks

## Executive Summary
SIBO addresses over-smoothing in parameter-efficient fine-tuning (PEFT) methods by injecting a scaled version of the initial token representation into each layer. This simple residual injection preserves token-level distinctions and improves performance across various PEFT techniques. The method is straightforward to implement and shows significant improvements on 22 benchmark datasets spanning arithmetic reasoning, commonsense reasoning, and GLUE tasks.

## Method Summary
SIBO introduces a residual injection mechanism that preserves a portion of the initial token representation (h0) in each layer of PEFT modules. The framework scales h0 by a hyperparameter λ (0 < λ < 1) and adds it to the current hidden state, ensuring that token representations maintain at least λ portion of the input layer's features. This approach is compatible with various PEFT techniques including Adapter and LoRA, functioning as a plug-and-play enhancement that mitigates over-smoothing while maintaining computational efficiency.

## Key Results
- Achieves up to 15.7% improvement over existing PEFT methods on arithmetic reasoning tasks
- Achieves up to 23.5% improvement on commonsense reasoning tasks
- Demonstrates effectiveness across 22 benchmark datasets with various backbone models (BERT, RoBERTa, LLaMA, GPT-J)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SIBO mitigates over-smoothing by preserving a minimum portion of the initial token representation in each layer.
- Mechanism: SIBO injects a scaled version of the initial token representation (h0) into the input of each PEFT module. The scaling factor λ controls the proportion of h0 retained, ensuring the final token representation preserves at least λ portion of the input layer's features. This reduces the uniformity of final token representations across layers.
- Core assumption: Over-smoothing in PEFT arises because token representations become increasingly similar across layers, and preserving initial features can counteract this effect.
- Evidence anchors:
  - [abstract]: "SIBO introduces a simple residual injection to mitigate over-smoothing in parameter-efficient fine-tuning (PEFT) methods for large language models. By preserving a portion of the initial token representation in each layer, SIBO enhances the performance of popular PEFT techniques such as Adapter and LoRA."
  - [section]: "Integrating an initial residual from h0 guarantees that the final representation of each token preserves at least a λ portion of the information from the input layer."
  - [corpus]: Corpus evidence is weak as the related papers focus on parameter-efficient fine-tuning but do not explicitly address over-smoothing. This is a novel contribution of SIBO.
- Break condition: If λ is set too high, the learning capacity of the PEFT methods may be compromised as too much of the original representation is preserved. If λ is too low, over-smoothing may not be sufficiently mitigated.

### Mechanism 2
- Claim: SIBO's initial residual injection is a universal enhancement applicable to various PEFT techniques.
- Mechanism: SIBO modifies the input to the PEFT modules (e.g., adapters or LoRA matrices) by adding a combination of the current hidden state and the initial token representation. This is done in a consistent manner across different PEFT methods, making SIBO a plug-and-play enhancement.
- Core assumption: The input to PEFT modules is a suitable point for intervention to preserve initial token features and mitigate over-smoothing.
- Evidence anchors:
  - [abstract]: "SIBO is straightforward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance."
  - [section]: "To devise a flexible yet simple plug-and-play framework for alleviating over-smoothing with existing PEFT techniques, our idea boils down to injecting an initial residual into the PEFT input."
  - [corpus]: The related papers focus on different PEFT techniques but do not propose a universal enhancement like SIBO. This is a novel aspect of the method.
- Break condition: If the PEFT method's architecture is significantly different from adapters or LoRA, the placement and implementation of the initial residual may need to be adapted.

### Mechanism 3
- Claim: SIBO improves performance on both in-distribution and out-of-distribution tasks.
- Mechanism: By mitigating over-smoothing, SIBO helps the model retain better token-level distinctions, which translates to improved generalization on unseen tasks. The experiments show improvements on both tasks seen during fine-tuning (in-distribution) and tasks not seen during fine-tuning (out-of-distribution).
- Core assumption: Over-smoothing harms both in-distribution and out-of-distribution performance, and mitigating it benefits both.
- Evidence anchors:
  - [abstract]: "Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT methods on the arithmetic and commonsense reasoning tasks, respectively."
  - [section]: "Moreover, we observe enhancements by SIBO in both in- and out-of-distribution scenarios."
  - [corpus]: The related papers do not explicitly discuss out-of-distribution performance improvements. This is an additional benefit of SIBO not covered in the literature.
- Break condition: If the out-of-distribution tasks are too different from the in-distribution tasks, the improvements may not generalize.

## Foundational Learning

- Concept: Over-smoothing in deep neural networks, particularly Transformers.
  - Why needed here: Understanding over-smoothing is crucial to grasp why SIBO is necessary and how it works. Over-smoothing is the phenomenon where token representations become increasingly similar across layers, which can harm model performance.
  - Quick check question: What is over-smoothing, and why is it a problem in deep Transformer models?

- Concept: Parameter-efficient fine-tuning (PEFT) techniques like Adapters and LoRA.
  - Why needed here: SIBO is designed to enhance PEFT methods. Understanding how Adapters and LoRA work is essential to understand how SIBO modifies them.
  - Quick check question: How do Adapters and LoRA differ in their approach to parameter-efficient fine-tuning?

- Concept: Residual connections and their role in neural networks.
  - Why needed here: SIBO uses a form of residual connection by injecting the initial token representation. Understanding residual connections helps in grasping the mechanism of SIBO.
  - Quick check question: What is the purpose of residual connections in neural networks, and how do they help with training deep models?

## Architecture Onboarding

- Component map: Pre-trained language model -> SIBO residual injection -> PEFT modules (Adapters/LoRA) -> Downstream task

- Critical path:
  1. Pre-trained model processes input tokens, generating initial representations (h0)
  2. SIBO injects scaled initial representations into PEFT modules at each layer
  3. PEFT modules (with SIBO) process the input and update token representations
  4. Final token representations are used for the downstream task

- Design tradeoffs:
  - λ value: Higher λ preserves more initial features but may limit learning; lower λ may not sufficiently mitigate over-smoothing
  - Placement of initial residual: Injecting into both attention and feed-forward layers vs. just one may have different effects on performance
  - Compatibility: SIBO needs to be compatible with different PEFT methods and model architectures

- Failure signatures:
  - If λ is too high: Model performance may degrade as it cannot learn new task-specific features effectively
  - If λ is too low: Over-smoothing may persist, and performance improvements may be minimal
  - Incorrect placement of initial residual: May not effectively mitigate over-smoothing or could introduce noise

- First 3 experiments:
  1. Verify over-smoothing in PEFT: Measure token-wise cosine similarity across layers with and without SIBO on a small dataset
  2. Tune λ: Experiment with different λ values on a validation set to find the optimal balance between preserving initial features and allowing learning
  3. Compare to baselines: Evaluate SIBO-enhanced PEFT methods against vanilla PEFT methods on a benchmark task to confirm performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal value of λ for different PEFT techniques and model sizes?
- Basis in paper: [explicit] The paper states "We treat λ as a hyperparameter" and shows different optimal values for Adapter (0.2) and LoRA (0.6-0.7) in experiments.
- Why unresolved: The optimal λ value depends on the specific PEFT technique, model size, and task. The paper only provides a limited range of values and does not establish a systematic method for determining the optimal λ.
- What evidence would resolve it: Systematic experiments varying λ across different PEFT techniques, model sizes, and tasks to identify patterns or develop a method for determining the optimal λ value.

### Open Question 2
- Question: How does SIBO perform on more diverse and complex reasoning tasks beyond arithmetic and commonsense reasoning?
- Basis in paper: [inferred] The paper evaluates SIBO on arithmetic reasoning, commonsense reasoning, and GLUE benchmark, but these tasks may not fully capture the model's ability to handle more complex reasoning tasks.
- Why unresolved: The paper focuses on a limited set of tasks and does not explore SIBO's performance on more diverse and complex reasoning tasks, such as logical reasoning, causal reasoning, or multi-step reasoning.
- What evidence would resolve it: Experiments evaluating SIBO on a wider range of reasoning tasks, including more complex and diverse ones, to assess its generalizability and effectiveness in handling various types of reasoning challenges.

### Open Question 3
- Question: Can SIBO be extended to other PEFT techniques beyond Adapter and LoRA?
- Basis in paper: [explicit] The paper states "SIBO is straightforward and readily extensible to various state-of-the-art PEFT methods" but only demonstrates its application to Adapter and LoRA.
- Why unresolved: The paper does not explore the potential of SIBO to be applied to other PEFT techniques, such as prefix tuning, prompt tuning, or other adapter-based methods.
- What evidence would resolve it: Experiments applying SIBO to other PEFT techniques and evaluating its effectiveness in improving their performance and mitigating over-smoothing.

## Limitations
- Limited evaluation scope: Experiments focus on arithmetic and commonsense reasoning tasks, not exploring more diverse or complex reasoning challenges
- λ hyperparameter tuning: Optimal λ values appear task and architecture-dependent, requiring additional tuning overhead
- Lack of ablation studies: No systematic analysis of different residual injection points (attention vs. feed-forward layers)

## Confidence

- **High confidence**: The effectiveness of SIBO in improving Adapter and LoRA performance on the tested benchmarks. The experimental setup is well-documented, and results are consistent across multiple datasets.
- **Medium confidence**: The claim that over-smoothing is the primary mechanism behind PEFT performance degradation. While supported by qualitative observations, direct quantitative evidence linking over-smoothing to performance is limited.
- **Low confidence**: The universal applicability of SIBO across all PEFT methods and model architectures. The paper demonstrates success with Adapter and LoRA but does not test other emerging PEFT techniques like prefix tuning or soft prompt tuning.

## Next Checks
1. **Over-smoothing quantification**: Measure token-wise cosine similarity across layers with and without SIBO on a subset of datasets to directly verify the over-smoothing mitigation claim. Compare these metrics against performance improvements to establish correlation.

2. **Ablation study on residual placement**: Experiment with injecting the initial residual at different points within the PEFT modules (post-attention vs. post-feed-forward) to determine the optimal placement strategy and its impact on performance.

3. **Computational overhead analysis**: Measure the additional FLOPs and memory requirements introduced by SIBO across different model sizes and batch configurations to assess practical deployment costs.