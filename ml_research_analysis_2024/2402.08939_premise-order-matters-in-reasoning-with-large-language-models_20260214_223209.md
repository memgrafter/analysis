---
ver: rpa2
title: Premise Order Matters in Reasoning with Large Language Models
arxiv_id: '2402.08939'
source_url: https://arxiv.org/abs/2402.08939
tags:
- order
- reasoning
- premise
- forward
- backward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are highly sensitive to the order
  of premises in reasoning tasks, even when the premise order does not change the
  underlying task. This work systematically investigates this "premise order effect"
  across deductive reasoning and mathematical reasoning.
---

# Premise Order Matters in Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2402.08939
- Source URL: https://arxiv.org/abs/2402.08939
- Authors: Xinyun Chen; Ryan A. Chi; Xuezhi Wang; Denny Zhou
- Reference count: 40
- Large language models show significant performance drops (over 30%) when premise order changes in reasoning tasks

## Executive Summary
Large language models exhibit surprising sensitivity to the order in which premises are presented, even when the underlying reasoning task remains unchanged. This phenomenon, termed the "premise order effect," causes substantial performance degradation when premises are presented in non-optimal sequences. The effect is particularly pronounced in deductive reasoning tasks and mathematical word problems, where models perform best when premise order aligns with the ground truth proof sequence. The research reveals that LLMs struggle with fact hallucination and temporal relationship errors when faced with suboptimal premise orders, highlighting a fundamental limitation in their reasoning capabilities that current architectures and training methods have not addressed.

## Method Summary
The study systematically investigates premise order effects using two main benchmarks: a synthetic deductive reasoning benchmark and the R-GSM benchmark derived from GSM8K mathematical word problems. The deductive reasoning benchmark contains 68 problems with ground truth proofs, where premise orders are systematically varied using Kendall tau distance (τ) to measure correlation with the ground truth sequence. The R-GSM benchmark includes 248 problems where premise order is shuffled while maintaining problem difficulty. Multiple LLMs (GPT-4, GPT-3.5, PaLM 2-L) are evaluated using zero-shot prompting across different premise orders, with performance measured through accuracy and detailed error analysis categorizing hallucination and temporal reasoning failures.

## Key Results
- LLMs achieve highest accuracy when premise order matches the ground truth proof sequence (τ=1)
- Accuracy drops by over 30% when premise order is randomly shuffled (τ=0)
- Different models show systematic preferences for forward versus backward chaining strategies
- Error analysis reveals increased fact hallucination and temporal relationship errors with non-optimal premise orders
- The effect is more pronounced in longer proofs and problems with more premises

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs achieve best performance when premise order aligns with intermediate reasoning steps in ground truth proof
- Mechanism: Auto-regressive decoding benefits from left-to-right sequential information flow, allowing each step to execute without backtracking
- Core assumption: Model's reasoning process follows sequential pattern like human reading and reasoning
- Evidence anchors:
  - [abstract]: "LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps"
  - [section]: "Presenting 'If A then B' before 'If B then C' in the prompt generally achieves a higher accuracy compared to the reversed order"
  - [corpus]: Weak - only 5 corpus papers found, none directly address this specific sequential reasoning alignment mechanism
- Break condition: If models develop internal mechanisms for jumping back and forth between premises

### Mechanism 2
- Claim: LLMs struggle with non-optimal premise orders because they hallucinate facts or overlook temporal relationships
- Mechanism: When premises are ordered suboptimally, model attempts to generate intermediate conclusions without necessary supporting information, leading to fact hallucination or temporal reasoning errors
- Core assumption: Model relies on sequential context availability rather than maintaining complete working memory of all premises
- Evidence anchors:
  - [abstract]: "Error analysis shows that LLMs often hallucinate facts or overlook temporal relationships when premise order is non-optimal"
  - [section]: "LLMs are inclined to use the rules in the sequential order as they present in the problem, so when the next rule in the problem is not yet applicable, LLMs might still hallucinate facts to complete the proof step"
  - [corpus]: Weak - corpus papers focus on premise critique ability but not hallucination patterns from ordering
- Break condition: If models develop better working memory or context retention capabilities

### Mechanism 3
- Claim: Different LLMs show varying preferences for alternative premise orders (forward vs backward chaining)
- Mechanism: Model architecture and training data create different biases toward forward or backward chaining strategies when optimal order isn't available
- Core assumption: Training corpus and model architecture create systematic preferences for certain reasoning directions
- Evidence anchors:
  - [section]: "GPT-4-turbo and GPT-3.5-turbo generally achieve better performance when the premise order is exactly the reverse of the ground truth proof, which enables LLMs to perform derivation via backward chaining"
  - [corpus]: Weak - corpus doesn't provide evidence about architectural differences in chaining preferences
- Break condition: If models are fine-tuned to be agnostic to reasoning direction

## Foundational Learning

- Concept: Modus ponens reasoning
  - Why needed here: The benchmark focuses on problems that only involve modus ponens, so understanding this logical form is essential for interpreting results
  - Quick check question: What is the logical form of modus ponens, and how does it differ from modus tollens?

- Concept: Kendall tau distance for measuring premise order correlation
  - Why needed here: The paper uses τ values to categorize premise orders relative to ground truth proof order
  - Quick check question: If the forward order has τ = 1, what τ value would the completely reversed order have?

- Concept: Chain-of-thought prompting and auto-regressive decoding
  - Why needed here: Understanding how LLMs generate step-by-step reasoning is crucial for grasping why premise order matters
  - Quick check question: How does auto-regressive decoding affect a model's ability to reference earlier premises when they're not optimally ordered?

## Architecture Onboarding

- Component map: Premise order generator → Prompt construction → LLM inference → Answer validation → Error analysis
- Critical path: Premise order generation → Prompt construction → LLM inference → Answer validation → Error analysis. The most time-consuming step is typically manual verification of ground truth answers for R-GSM problems.
- Design tradeoffs: Synthetic logical reasoning problems (precise control over premise ordering) versus natural language mathematical problems (ecological validity but harder to control ordering effects)
- Failure signatures: Large accuracy drops (>30%) when premise order changes, increased fact hallucination rates with non-optimal ordering, and temporal relationship errors in mathematical reasoning
- First 3 experiments:
  1. Run logical reasoning benchmark with GPT-4 on forward order (τ=1) problems and measure baseline accuracy
  2. Shuffle premise order to τ=0 and measure accuracy drop, then analyze error patterns for fact hallucination
  3. Test GPT-4 on backward order (τ=-1) problems to observe whether backward chaining preference manifests

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications could mitigate the premise order effect in LLMs?
- Basis in paper: [explicit] The paper states "We leave proposing new training and modeling techniques to mitigate the premise order effect as future work."
- Why unresolved: The paper identifies the premise order effect as a fundamental limitation but does not propose or test specific solutions.
- What evidence would resolve it: Experiments comparing different model architectures, training objectives, or data augmentation techniques that show reduced sensitivity to premise order compared to baseline models.

### Open Question 2
- Question: Does the premise order effect extend to other reasoning domains beyond deductive and mathematical reasoning?
- Basis in paper: [inferred] The paper notes the effect is "more pronounced with longer proofs and irrelevant premises" and tests only deductive and mathematical reasoning, suggesting it might be domain-agnostic.
- Why unresolved: The paper focuses on two specific reasoning domains without exploring whether the effect manifests similarly in other reasoning tasks like analogical reasoning or commonsense reasoning.
- What evidence would resolve it: Systematic experiments across diverse reasoning benchmarks (e.g., visual reasoning, commonsense QA, causal reasoning) measuring performance degradation with premise reordering.

### Open Question 3
- Question: What is the theoretical explanation for why LLMs exhibit stronger premise order effects than humans?
- Basis in paper: [explicit] The paper suggests potential factors like "auto-regressive model design, training objectives, and training data mixture" but leaves this as future work.
- Why unresolved: While the paper identifies candidate factors, it does not provide a formal theoretical analysis of the mechanisms underlying the effect.
- What evidence would resolve it: Formal analysis connecting model architecture/training to order sensitivity, or controlled experiments isolating the impact of specific factors (e.g., comparing non-auto-regressive models, analyzing training data for ordering patterns).

## Limitations

- The observed premise order effects may be partially attributable to the specific zero-shot prompting approach used
- The study focuses on relatively simple deductive and mathematical reasoning tasks, limiting generalizability to more complex scenarios
- Error analysis relies on manual inspection, which may introduce subjective biases in categorizing hallucination versus temporal reasoning errors

## Confidence

- **High Confidence**: The empirical finding that premise order affects LLM performance
- **Medium Confidence**: The interpretation that this reflects sequential reasoning limitations
- **Medium Confidence**: The claim that different models show systematic preferences for forward versus backward chaining

## Next Checks

1. Test premise order effects with few-shot prompting and chain-of-thought prompting to determine if observed effects are prompting-method dependent
2. Evaluate whether fine-tuning on premise-ordered data reduces sensitivity to premise presentation order
3. Investigate whether bidirectional attention mechanisms or retrieval-augmented generation approaches can mitigate premise order sensitivity