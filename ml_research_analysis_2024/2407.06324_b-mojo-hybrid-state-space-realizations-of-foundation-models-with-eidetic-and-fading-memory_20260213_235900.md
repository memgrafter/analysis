---
ver: rpa2
title: 'B''MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic
  and Fading Memory'
arxiv_id: '2407.06324'
source_url: https://arxiv.org/abs/2407.06324
tags:
- memory
- mojo
- state
- mamba
- eidetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces B'MOJO, a hybrid architecture combining both
  eidetic (exact) and fading (approximate) memory within a single composable module.
  The key innovation is a new Innovation Selection mechanism that dynamically stores
  the most unpredictable tokens from the past in eidetic memory, complementing the
  fading memory implemented by a State Space Model.
---

# B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory

## Quick Facts
- arXiv ID: 2407.06324
- Source URL: https://arxiv.org/abs/2407.06324
- Reference count: 40
- Primary result: Achieves 4x length generalization (32K tokens) while maintaining perplexity and training speed comparable to similar-sized models

## Executive Summary
B'MOJO introduces a hybrid architecture combining eidetic (exact) and fading (approximate) memory within a single composable module. The key innovation is a new Innovation Selection mechanism that dynamically stores the most unpredictable tokens from the past in eidetic memory, complementing the fading memory implemented by a State Space Model. This allows the model to seamlessly balance exact recall with efficient long-range processing. Experiments on synthetic associative recall tasks and language modeling show that B'MOJO outperforms existing models like Mamba and Transformers in transductive inference, particularly on long sequences (up to 4x longer than training length), while maintaining comparable perplexity and training speed to similar-sized models.

## Method Summary
B'MOJO combines a State Space Model for fading memory with an Innovation Selection mechanism for eidetic memory storage. The SSM maintains a compressed representation of the input sequence in its state, while the Innovation Selection process computes prediction errors to identify and store the most unpredictable tokens in eidetic memory. The architecture uses a Controllable Canonical Form to implement the SSM, which generalizes both Mamba's diagonal dynamics and Transformer attention mechanisms. During inference, the model can access both memory types - the SSM state for long-range statistical patterns and the eidetic memory for exact token recall - enabling superior performance on long sequences and associative recall tasks.

## Key Results
- Achieves 4x length generalization, maintaining performance on sequences up to 32K tokens trained on only 8K tokens
- Outperforms Mamba and Transformers on transductive inference tasks, particularly on long sequences
- Maintains comparable perplexity and training speed to similar-sized models (130M-1.4B parameters)
- Demonstrates superior performance on synthetic associative recall tasks compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1: Innovation Selection for Eidetic Memory
B'MOJO dynamically stores the most unpredictable tokens from the past in eidetic memory by computing an error between the SSM's state-based prediction and the actual next token. If this error exceeds the minimum error among existing eidetic memory tokens, the new token is added. The core assumption is that unpredictable tokens are more likely to be valuable for future recall than predictable ones. Break condition: If the predictor becomes too accurate (low error on most tokens), Innovation Selection stops adding new tokens, potentially missing future unpredictable patterns.

### Mechanism 2: Controllable Canonical Form Generalization
B'MOJO-F uses a Controllable Canonical Form with diagonal dynamics for the SSM and fixed sliding window attention, showing that Transformers, Mamba, and hybrid architectures like Jamba are special cases. The controllable canonical form represents both nilpotent attention dynamics (upper diagonal) and autoregressive dynamics (last row). Break condition: If the state dimension N is too small to capture the required dynamics, the representation becomes inadequate.

### Mechanism 3: Length Generalization Through Memory Duality
B'MOJO achieves length generalization by leveraging both fading memory (for long-range statistics) and eidetic memory (for exact token recall). During training on sequences up to 8K tokens, it learns to compress long-range dependencies in its SSM state while storing key unpredictable tokens in eidetic memory. At inference on sequences up to 32K tokens, it can access both memory types to maintain performance. Break condition: If the eidetic memory becomes too large or the SSM state cannot effectively compress the necessary statistics, length generalization performance degrades.

## Foundational Learning

- **Stochastic Realization Theory**: Provides the theoretical foundation for combining fading and eidetic memory in a unified framework
  - Quick check question: What is the key difference between representations and realizations in the context of this paper?

- **State Space Models (SSM)**: Forms the basis for the fading memory component and the controllable canonical form implementation
  - Quick check question: Why does Mamba use diagonal dynamics and what limitation does this create?

- **Innovation Selection**: The mechanism for deciding which tokens to store in eidetic memory based on predictability
  - Quick check question: How does the predictor ˆy function in the Innovation Selection process?

## Architecture Onboarding

- **Component map**: Input tokens → SSM state update → Innovation Selection → Eidetic memory + SSM state → Sliding window attention → Output tokens
- **Critical path**: Token processing sequence where SSM state must be computed before Innovation Selection can decide on eidetic memory storage
- **Design tradeoffs**: Fixed-size SSM state vs. growing eidetic memory; computational cost of Innovation Selection vs. recall accuracy
- **Failure signatures**: Degraded performance on long sequences indicates insufficient SSM state capacity; poor associative recall suggests inadequate eidetic memory management
- **First 3 experiments**:
  1. Implement B'MOJO-F with varying state dimensions (N=16, 32, 64) on a simple associative recall task to find the minimal working state size
  2. Add Innovation Selection with a simple running average predictor and test on the same associative recall task to verify eidetic memory contribution
  3. Test length generalization by training on 2K sequences and evaluating on 8K sequences using PG-19 dataset to verify the core length generalization capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does B'MOJO's performance scale beyond the 1.4B parameter models tested in the paper?
- Basis in paper: The paper notes that scaling B'MOJO further requires non-trivial engineering and compute, and it is difficult to ascertain whether it could scale to even larger models and datasets competitively.
- Why unresolved: The experiments were limited to 1.4B parameter models due to computational constraints.
- What evidence would resolve it: Scaling experiments on models with 10B+ parameters and testing their performance on various downstream tasks would provide insights into B'MOJO's scalability.

### Open Question 2
- Question: How does B'MOJO's length generalization capability change as the model scales up?
- Basis in paper: The paper shows that B'MOJO has better length generalization than Mamba, but it is noted that as Mamba checkpoints get larger, they tend not to length generalize well. Since B'MOJO leverages a Mamba-like module, it is unclear if B'MOJO will maintain its length generalization ability at larger scales.
- Why unresolved: The experiments only tested length generalization up to 1.4B parameters, and the behavior at larger scales is unknown.
- What evidence would resolve it: Experiments on larger B'MOJO models (e.g., 10B+ parameters) trained on long sequences and evaluated on even longer sequences would clarify if the length generalization ability persists at scale.

### Open Question 3
- Question: What are the trade-offs between the amount of eidetic memory and the model's performance on various tasks?
- Basis in paper: The paper mentions that the amount of information stored in the eidetic memory (Mt) can grow unbounded over time up to hardware limitations. It also shows that increasing the number of eidetic memory tokens improves performance on associative recall tasks.
- Why unresolved: The paper does not provide a detailed analysis of the trade-offs between eidetic memory size and performance across different tasks and model scales.
- What evidence would resolve it: Systematic experiments varying the size of the eidetic memory for different model scales and tasks, and analyzing the resulting performance and computational costs, would provide insights into the trade-offs involved.

## Limitations

- Limited empirical validation: Claims of 4x length generalization demonstrated only on synthetic tasks and PG-19, lacking extensive real-world benchmark validation
- Unclear practical implementation details: Innovation Selection mechanism specifics including predictor function and token selection criteria are not fully specified
- Theoretical vs practical gap: Controllable canonical form generalization claim lacks empirical verification that minimal realization captures specific architecture behaviors

## Confidence

**High Confidence**: The core architectural contribution of combining eidetic and fading memory in a unified framework is well-defined and theoretically grounded in stochastic realization theory. The mathematical formulation of the Innovation Selection mechanism is rigorous and implementable.

**Medium Confidence**: The empirical results showing length generalization and performance improvements are promising but based on limited evaluation. The claim of 4x length generalization needs validation on more diverse benchmarks and real-world datasets.

**Low Confidence**: The theoretical claim that B'MOJO subsumes existing architectures (Mamba, Transformers) is mathematically plausible but lacks empirical verification. The practical implementation details for achieving this generality are not fully specified.

## Next Checks

1. **Benchmark Expansion**: Validate length generalization claims on established language modeling benchmarks (C4, The Pile, or multilingual datasets) beyond PG-19, testing the 4x length generalization on diverse real-world data with varying domain characteristics.

2. **Architectural Subsumption Verification**: Implement minimal realizations of Mamba and Transformer attention modules using the controllable canonical form and empirically verify that they produce equivalent outputs under controlled conditions.

3. **Innovation Selection Dynamics**: Conduct ablation studies on the Innovation Selection mechanism, testing different predictor functions (beyond simple running averages) and varying the unpredictability threshold to understand the sensitivity and optimal configuration for different task types.