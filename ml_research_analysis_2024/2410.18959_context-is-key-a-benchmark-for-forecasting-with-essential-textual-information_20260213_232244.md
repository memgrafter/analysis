---
ver: rpa2
title: 'Context is Key: A Benchmark for Forecasting with Essential Textual Information'
arxiv_id: '2410.18959'
source_url: https://arxiv.org/abs/2410.18959
tags:
- context
- information
- forecasting
- tasks
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Context is Key (CiK), a benchmark for evaluating
  time series forecasting models that must incorporate both numerical data and essential
  natural language context. CiK includes 71 tasks across seven real-world domains,
  each designed so accurate forecasts require understanding textual context.
---

# Context is Key: A Benchmark for Forecasting with Essential Textual Information

## Quick Facts
- **arXiv ID**: 2410.18959
- **Source URL**: https://arxiv.org/abs/2410.18959
- **Reference count**: 40
- **Primary result**: CiK benchmark shows LLM-based models with proper context integration outperform traditional statistical and time series foundation models, but exhibit critical failure modes and high computational costs

## Executive Summary
This paper introduces Context is Key (CiK), a benchmark designed to evaluate time series forecasting models that must incorporate both numerical data and essential natural language context. The benchmark includes 71 tasks across seven real-world domains, each specifically designed so accurate forecasts require understanding textual context. The authors propose a novel Region of Interest CRPS (RCRPS) scoring rule that prioritizes context-sensitive windows and accounts for constraint satisfaction. Experiments demonstrate that LLM-based forecasting models, particularly when using the proposed DIRECT PROMPT method, significantly outperform traditional statistical and time series foundation models on CiK, highlighting the importance of context integration while also revealing critical limitations in current LLM-based approaches.

## Method Summary
The CiK benchmark evaluates forecasting models on 71 tasks across seven domains (Climatology, Economics, Energy, Mechanics, Public Safety, Transportation, and Retail) using 2,644 time series. Each task includes historical numerical data and essential natural language context. The primary evaluation metric is RCRPS, a modified CRPS that prioritizes context-sensitive windows and accounts for constraint satisfaction. The paper evaluates various models including statistical approaches (ARIMA, ETS), time series foundation models (Lag-Llama, Chronos, TimeGEN), and LLM-based forecasters (GPT-4o, Llama-3.1-405B, Mixtral-8x7B). The DIRECT PROMPT method is a simple LLM prompting approach that instructs models to directly output forecasts in a specific format. The evaluation protocol involves sampling 5 instances per task and generating 25 independent forecasts per model instance.

## Key Results
- LLM-based models with DIRECT PROMPT method outperform all other tested methods on CiK benchmark
- Incorporating relevant context improves forecasting accuracy, with Llama-3.1-405B-Instruct improving by 67.1% with context
- Larger LLMs generally outperform smaller ones, but some models exhibit significant failures when interpreting context
- Best-performing models are resource-intensive, creating efficiency challenges for practical deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DIRECT PROMPT outperforms traditional and foundation models
- Mechanism: Structured prompt template guides LLM to directly output forecasts while integrating numerical data with natural language context
- Core assumption: Structured format effectively guides LLM to produce accurate, constraint-satisfying forecasts
- Evidence: Abstract states DIRECT PROMPT "outperforms all other tested methods" and shows best-performing methods combine pretrained LLMs with prompting strategies

### Mechanism 2
- Claim: Context integration improves forecasting accuracy
- Mechanism: RCRPS scoring rule rewards models that leverage context-sensitive windows and satisfy constraints
- Core assumption: Provided context is relevant and contains actionable information
- Evidence: Experiments show Llama-3.1-405B-Instruct improves by 67.1% with context; abstract notes importance of incorporating contextual information

### Mechanism 3
- Claim: Larger LLMs outperform smaller ones on CiK tasks
- Mechanism: Greater parameter capacity enables better processing and integration of complex contextual information
- Core assumption: Increased capacity translates to better performance on context-dependent forecasting
- Evidence: Abstract notes best models like Llama-3.1-405B-Instruct are resource-intensive; results show clear improvement with context for larger models

## Foundational Learning

- **Time series forecasting**: Core task evaluated by CiK benchmark; understanding common approaches (ARIMA, ETS, etc.) is essential for context
- **Natural language processing**: Required for processing and integrating textual context with numerical data; understanding techniques like prompt engineering and context filtering is crucial
- **Scoring rules**: RCRPS is the primary evaluation metric; understanding probabilistic scoring rules and their components is necessary for proper benchmark interpretation

## Architecture Onboarding

- **Component map**: CiK benchmark (71 tasks across 7 domains) -> Numerical data + Natural language context -> Various models (statistical, foundation, LLM) -> RCRPS scoring rule -> Performance evaluation
- **Critical path**: 1) Understand task structure and context types 2) Familiarize with RCRPS scoring rule 3) Select and configure appropriate models 4) Run experiments and analyze context impact
- **Design tradeoffs**: Model selection involves tradeoffs between performance, computational resources, and context integration capability; larger LLMs offer better performance but require more resources
- **Failure signatures**: Models may fail to integrate context effectively, leading to performance degradation; significant failures occur when forecasts miss ground truth by 500%+ of range
- **First 3 experiments**: 1) Evaluate DIRECT PROMPT with Llama-3.1-405B-Instruct on task subset 2) Compare DIRECT PROMPT performance with/without context 3) Evaluate smaller LLMs (Llama-3-70B) to assess model size tradeoffs

## Open Questions the Paper Calls Out

- **Open Question 1**: What architectural modifications would enable LLM-based models to achieve Pareto efficiency with traditional statistical models?
- **Open Question 2**: How can we develop more robust LLM-based forecasting models that avoid catastrophic failures when interpreting context?
- **Open Question 3**: What is the optimal strategy for balancing context relevance filtering versus context richness in multimodal forecasting?
- **Open Question 4**: How can we scale test-time computation for LLM-based forecasting models while maintaining efficiency?

## Limitations

- **Prompt engineering reproducibility**: DIRECT PROMPT lacks full specification of exact templates and constrained decoding implementation details
- **Context relevance validation**: No systematic validation that all contexts are truly essential versus merely helpful
- **Resource-accuracy tradeoff**: Best-performing models are resource-intensive but paper doesn't quantify computational costs versus accuracy gains

## Confidence

- **High Confidence**: CiK benchmark demonstrates context integration can improve accuracy; LLM-based models can incorporate natural language context for forecasting
- **Medium Confidence**: Larger LLMs consistently outperform smaller ones; RCRPS scoring rule effectively captures context-sensitive performance
- **Low Confidence**: DIRECT PROMPT is optimal prompting strategy; all 71 tasks require essential contextual information

## Next Checks

1. **Prompt Template Reproduction Test**: Implement DIRECT PROMPT using only paper information and compare RCRPS results to verify reproducibility of the prompting approach

2. **Context Necessity Validation**: Systematically remove individual context components per task to empirically verify which contexts are truly essential versus helpful

3. **Resource-Accuracy Tradeoff Analysis**: Measure computational costs (wall-clock time, GPU memory, energy) for each model-method combination and calculate accuracy-per-compute ratios to assess cost-effectiveness