---
ver: rpa2
title: Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction
  in Markov Games
arxiv_id: '2402.13399'
source_url: https://arxiv.org/abs/2402.13399
tags:
- norms
- agents
- norm
- learning
- river
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for learning and sustaining social
  norms in multi-agent sequential decision-making environments, formalized as norm-augmented
  Markov games. The core method uses approximately Bayesian rule induction, where
  agents assume shared normativity and infer norms from observations of compliance
  and violation by others.
---

# Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games

## Quick Facts
- arXiv ID: 2402.13399
- Source URL: https://arxiv.org/abs/2402.13399
- Authors: Ninell Oldenburg; Tan Zhi-Xuan
- Reference count: 40
- Key outcome: Framework enables rapid norm learning (within 300 steps) in multi-agent environments, achieving social outcomes with higher collective rewards and lower environmental degradation compared to millions of steps required by model-free approaches.

## Executive Summary
This paper introduces a framework for learning and sustaining social norms in multi-agent sequential decision-making environments, formalized as norm-augmented Markov games. The core method uses approximately Bayesian rule induction, where agents assume shared normativity and infer norms from observations of compliance and violation by others. Agents plan using two modes—reward-oriented and obligation-oriented—switching based on triggered obligations. The framework was evaluated in a modified Melting Pot environment, demonstrating that agents can rapidly learn norms (within 300 steps) compared to millions of steps required by model-free approaches. The system successfully transmitted norms across generations and enabled convergence to shared norms without explicit sanctions.

## Method Summary
The framework uses approximately Bayesian rule induction where agents assume shared normativity and infer norms from observations of compliance and violation by others. Agents plan using two modes—reward-oriented (ignoring obligations) and obligation-oriented (satisfying goals)—switching based on triggered obligations. The framework was evaluated in a modified Melting Pot environment with Commons Harvest, Clean Up, and Territory games. The approach combines game-theoretic accounts of norms with flexible Bayesian learning, using mean-field approximation for posterior computation. The method demonstrates rapid norm learning within 300 steps and successful intergenerational norm transmission.

## Key Results
- Agents rapidly learned norms within 300 steps, significantly faster than millions of steps required by model-free approaches
- Social outcomes improved significantly when norms were present, with higher collective rewards and lower environmental degradation
- The system successfully transmitted norms across generations and enabled convergence to shared norms without explicit sanctions

## Why This Works (Mechanism)
The framework works by combining Bayesian rule induction with model-based planning in norm-augmented Markov games. Agents assume shared normativity and infer norms by observing compliance and violations from others. The planning algorithm switches between reward-oriented and obligation-oriented modes based on triggered obligations, allowing agents to balance individual rewards with normative compliance. The mean-field approximation enables tractable posterior computation while maintaining sufficient accuracy for norm learning. The approach leverages the flexibility of Bayesian learning to adapt to changing social environments while maintaining stability through violation costs.

## Foundational Learning
- **Norm-augmented Markov games**: Required to formalize multi-agent sequential decision-making with social norms; quick check: verify game states include norm violation/fulfillment information
- **Bayesian rule induction**: Needed for agents to infer norms from observations; quick check: confirm posterior updates correctly incorporate compliance/violation evidence
- **Mean-field approximation**: Essential for tractable posterior computation in multi-agent settings; quick check: verify approximation doesn't introduce significant bias in norm learning
- **Model-based planning with dual modes**: Critical for balancing rewards and obligations; quick check: ensure smooth transitions between reward-oriented and obligation-oriented planning
- **Intergenerational transmission mechanisms**: Important for sustaining norms across agent lifetimes; quick check: verify new agents can learn from observing predecessors
- **Social preference formation**: Necessary for understanding how agents develop shared normative systems; quick check: confirm agents converge to compatible norm sets

## Architecture Onboarding

**Component Map**: Observation -> Bayesian Updating -> Norm Selection -> Planning Mode Switch -> Action Selection

**Critical Path**: The core learning loop flows from observing others' behavior → Bayesian updating of norm beliefs → selecting most probable norms → switching planning mode if obligations are triggered → executing actions that balance rewards and normative compliance.

**Design Tradeoffs**: The framework trades computational efficiency for norm learning accuracy through mean-field approximation, and sacrifices explicit sanction mechanisms for implicit norm enforcement through violation costs. The dual planning mode approach balances flexibility with norm compliance but may struggle with conflicting norms.

**Failure Signatures**: Slow norm learning suggests issues with Bayesian updating or insufficient violation costs; poor social outcomes despite learning indicates planning algorithm problems or conflicts between norms; inability to sustain norms across generations points to inadequate observation mechanisms or overly complex norm spaces.

**First Experiments**:
1. Verify Bayesian updating correctly infers simple norms from synthetic compliance/violation data
2. Test planning mode switching works correctly in isolated norm scenarios
3. Confirm norm learning occurs within expected timeframe (300 steps) in basic Commons Harvest game

## Open Questions the Paper Calls Out

**Open Question 1**: How do intrinsic norm violation costs influence the emergence of beneficial vs. non-functional norms? The paper demonstrates that high violation costs can stabilize inefficient or harmful norms, but doesn't systematically explore how different cost levels impact the balance between beneficial and non-functional norms.

**Open Question 2**: Can Bayesian norm learning distinguish between unintentional norm violations and the absence of a norm? The current framework treats violations as evidence against a norm's existence without distinguishing accidental failures from non-compliance, potentially leading to incorrect norm non-existence inferences.

**Open Question 3**: How does the mean-field approximation in Bayesian norm learning affect the emergence of overlapping norms? The paper identifies that the mean-field approximation prevented explaining away among overlapping norms, leading agents to learn logically overlapping norms, but doesn't investigate the consequences on learning accuracy or efficiency.

## Limitations
- The experimental evaluation relies on modified Melting Pot environments that are not fully specified, making exact reproduction challenging
- The framework demonstrates effectiveness in relatively simple game environments, with scalability to complex social scenarios unclear
- Claims about intergenerational norm transmission and convergence to shared norms in more complex scenarios cannot be fully verified without complete experimental details

## Confidence

**High confidence**: The core theoretical framework combining Bayesian rule induction with model-based planning is sound and well-articulated

**Medium confidence**: The reported rapid learning and social outcome improvements are plausible given the methodology, but exact replication would require additional implementation details

**Low confidence**: Claims about intergenerational norm transmission and convergence to shared norms in more complex scenarios cannot be fully verified without access to the complete experimental setup

## Next Checks

1. Implement the complete Melting Pot environment modifications (apple regrowth, river pollution, territory mechanics) and verify that the 68 possible norms can be generated from the predicate-threshold combinations

2. Reproduce the Bayesian updating process with both thresholding and sampling approaches to confirm rapid norm learning within the reported 300-step timeframe

3. Test norm emergence and convergence across multiple independent runs to verify consistency of the reported social outcome improvements