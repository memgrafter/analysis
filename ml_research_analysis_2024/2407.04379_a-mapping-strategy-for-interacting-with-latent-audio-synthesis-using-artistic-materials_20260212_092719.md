---
ver: rpa2
title: A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic
  Materials
arxiv_id: '2407.04379'
source_url: https://arxiv.org/abs/2407.04379
tags:
- mapping
- latent
- space
- audio
- strategy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a mapping strategy for interacting with the
  latent spaces of generative AI models using artistic materials. The approach involves
  using unsupervised feature learning (latent mapping) to encode high-dimensional
  human control data into representative features, then mapping these to an audio
  synthesis model's latent space via Interactive Machine Learning (IML).
---

# A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials

## Quick Facts
- arXiv ID: 2407.04379
- Source URL: https://arxiv.org/abs/2407.04379
- Reference count: 16
- Primary result: Introduces a mapping strategy for interacting with latent spaces of generative AI models using artistic materials, demonstrated through sketch-to-sound system

## Executive Summary
This paper presents a novel approach for creative interaction with AI audio synthesis models by mapping artistic materials to latent parameter spaces. The strategy combines unsupervised feature learning to encode high-dimensional artistic inputs into representative features, followed by Interactive Machine Learning to establish personalized mappings to audio synthesis parameters. A proof-of-concept system demonstrates this approach using visual sketches to control a RAVE audio synthesis model, enabling real-time creative exploration through immediate auditory feedback.

## Method Summary
The method involves a two-step process: first, using latent mapping (unsupervised feature learning) to encode high-dimensional artistic inputs into lower-dimensional latent representations that capture representative features; second, applying Interactive Machine Learning to connect these extracted features to the audio synthesis model's parameters through personalized training examples. The system encodes sketches into a 32-dimensional space which is then mapped to the 16-dimensional latent space of the RAVE audio synthesis model, enabling real-time audio feedback based on sketch inputs.

## Key Results
- Demonstrates proof-of-concept system for sketch-to-sound interaction using latent space mapping
- Enables real-time audio feedback based on visual sketch inputs
- Shows potential for applying mapping strategy to other artistic practices beyond sketches
- Establishes framework for explaining AI model behavior through interactive artistic control

## Why This Works (Mechanism)

### Mechanism 1
Unsupervised feature learning (latent mapping) reduces high-dimensional artistic input to a compact latent representation that preserves essential creative features. The latent mapping model learns a compressed embedding space from unlabelled artistic data using variational autoencoders or similar techniques, creating a bottleneck that forces the network to retain only the most representative features while discarding noise.

### Mechanism 2
Interactive Machine Learning (IML) creates personalized, interpretable mappings between encoded artistic features and audio model's latent parameters through direct human feedback. Artists manually create paired examples by adjusting audio parameters and recording corresponding artistic inputs, allowing the IML system to learn a custom mapping function that reflects the artist's creative intentions rather than generic statistical correlations.

### Mechanism 3
Real-time audio feedback during artistic performance enables embodied learning and discovery of unexpected creative possibilities through exploration of the mapping space. As artists create visual sketches, they immediately hear corresponding audio output, allowing them to develop an intuitive understanding of how different sketch characteristics map to sonic qualities and discover serendipitous combinations through trial and error.

## Foundational Learning

- **Variational Autoencoders (VAEs) and latent space representations**: Essential for understanding how high-dimensional artistic data gets compressed into lower-dimensional spaces. *Quick check*: How does a VAE differ from a standard autoencoder, and why is this difference important for artistic applications?

- **Interactive Machine Learning (IML) and personalized mapping construction**: Critical for understanding how artists actively participate in defining relationships between creative inputs and audio outputs. *Quick check*: What are the key differences between IML and traditional supervised machine learning in terms of data collection and user involvement?

- **Real-time audio synthesis and latency constraints**: Important for understanding the responsiveness requirements that enable effective embodied learning. *Quick check*: What are typical latency thresholds for maintaining coherent human-computer interaction in musical contexts?

## Architecture Onboarding

- **Component map**: Input capture → Latent mapping module → IML module → Audio synthesis engine → Audio output → Artist feedback

- **Critical path**: Input → Latent mapping → IML mapping → Audio synthesis → Audio output → Artist feedback. The entire pipeline must operate with minimal latency to maintain the causal perception necessary for creative exploration.

- **Design tradeoffs**: Dimensionality vs expressiveness (higher-dimensional spaces capture more nuance but require more training data), pre-training vs customization (pre-trained models provide immediate functionality but may not capture artist-specific intentions), real-time vs quality (lower-latency systems may sacrifice audio quality or mapping precision).

- **Failure signatures**: No audible change despite varied input (IML mapping failed or audio parameters not connected), inconsistent audio response (insufficient training data or overfit model), silent or glitchy output (communication layer failure or audio engine initialization problems), unresponsive input (input capture failure or latency issues).

- **First 3 experiments**: 
  1. Verify end-to-end latency by measuring time from sketch input to audio output using a simple test signal to ensure sub-100ms response
  2. Test basic mapping functionality by creating a simple linear mapping from one sketch dimension to one audio parameter to verify system connectivity
  3. Validate training process by manually creating a small set of paired examples and verifying that the IML model learns the intended relationship before scaling up to full creative exploration

## Open Questions the Paper Calls Out

- **Open Question 1**: How can temporal trajectories in artistic materials be effectively explained to users to enhance understanding of AI model behavior? While the paper acknowledges the importance of explaining temporal trajectories, it does not provide a concrete method for achieving this, leaving the question of how to effectively explain these transitions open.

- **Open Question 2**: What are the most effective cross-modal mappings between non-audio features (e.g., visual transformations) and audio latent spaces for creative applications? The paper proposes this as a future research direction but does not explore or validate specific cross-modal mappings, leaving their effectiveness in creative contexts uncertain.

- **Open Question 3**: How can the mapping strategy be generalized to other forms of creative practices beyond sketch-to-sound? The paper notes that while the demonstration focuses on sketches, the mapping strategy could be applied to other artistic practices, suggesting the need to explore other forms of creative practices.

## Limitations

- Dependence on artist-provided training data for IML mapping - if artists cannot establish clear perceptual relationships between their inputs and desired outputs, the entire system becomes ineffective.
- Assumption that unsupervised feature learning will capture all relevant expressive dimensions of artistic practice, which may not hold for novel or unconventional artistic techniques.
- Real-time constraint imposes strict limitations on model complexity and processing overhead, potentially sacrificing mapping quality for responsiveness.

## Confidence

**High Confidence**: The fundamental mechanisms of latent mapping (VAEs) and their role in dimensionality reduction are well-established in machine learning literature. The basic architecture of using IML for personalized mapping is also a proven approach in creative AI systems.

**Medium Confidence**: The specific implementation details for combining sketch-to-sound encoding with audio synthesis latent spaces are novel and not extensively validated. The effectiveness of this particular mapping configuration for supporting sustained artistic practice remains to be demonstrated through long-term user studies.

**Low Confidence**: The claim that this approach will enable "unexpected creative possibilities" and sustained artistic exploration is largely speculative at this stage. While the mechanism supports real-time interaction, the actual creative value depends heavily on implementation details and individual artistic preferences.

## Next Checks

1. **Latent space completeness validation**: Systematically test whether the sketch-to-sound encoder captures the full range of expressive possibilities by having artists attempt to produce specific target sounds and measuring success rates across different artistic styles and techniques.

2. **IML training robustness study**: Conduct controlled experiments varying the number and diversity of training examples to determine minimum requirements for reliable mapping and identify failure patterns when training data is insufficient or biased.

3. **Long-term creative engagement assessment**: Deploy the system with multiple artists over extended periods (minimum 2-4 weeks) to evaluate whether the mapping strategy supports sustained creative exploration or if artists encounter plateaus or limitations that prevent continued use.