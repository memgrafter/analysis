---
ver: rpa2
title: 'Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning
  Languages'
arxiv_id: '2407.03321'
source_url: https://arxiv.org/abs/2407.03321
tags:
- planning
- pddl
- problem
- goal
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Planetarium is a benchmark for evaluating language models on translating
  natural language descriptions of planning tasks into structured planning languages
  like PDDL. The core contribution is a rigorous equivalence algorithm that determines
  if two PDDL problems are semantically equivalent by transforming them into scene
  graphs and checking graph isomorphism.
---

# Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages

## Quick Facts
- **arXiv ID**: 2407.03321
- **Source URL**: https://arxiv.org/abs/2407.03321
- **Reference count**: 28
- **Primary result**: LLM PDDL generation shows high syntactic validity but low semantic accuracy despite fine-tuning improvements

## Executive Summary
Planetarium introduces a benchmark for evaluating language models on translating natural language descriptions into structured planning languages like PDDL. The core innovation is a rigorous equivalence algorithm that determines semantic equivalence by transforming PDDL problems into scene graphs and checking graph isomorphism. The benchmark includes 132,037 text-to-PDDL pairs across 13 tasks from Blocks World and Gripper domains. Evaluation shows that while models like GPT-4o achieve high syntactic validity (82.2%), semantic accuracy remains low (35.1%), highlighting the need for more rigorous evaluation frameworks beyond simple plan validation.

## Method Summary
Planetarium constructs a benchmark by generating text-to-PDDL pairs with varying abstraction levels and problem sizes from Blocks World and Gripper domains. The key innovation is an equivalence algorithm that determines semantic correctness by converting PDDL problems into initial and goal scene graphs, fully specifying goal scenes by adding trivially true edges, and checking for graph isomorphism between problems. This approach goes beyond traditional plan validators by verifying semantic equivalence rather than just syntactic validity or solvability. The benchmark evaluates several models including GPT-4o, GPT-3.5, and fine-tuned open-weight models across zero-shot and few-shot settings.

## Key Results
- GPT-4o achieves only 35.1% semantic accuracy despite 82.2% producing valid, solvable PDDL
- Abstract task descriptions are significantly harder to translate than explicit ones
- Fine-tuning improves performance but remains limited, with best models achieving ~40-50% semantic accuracy
- The benchmark reveals a substantial gap between syntactic validity and semantic correctness in LLM-generated PDDL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Planetarium's equivalence algorithm accurately determines whether two PDDL problems are semantically equivalent by transforming them into scene graphs and checking graph isomorphism.
- Mechanism: The algorithm converts PDDL problem files into initial and goal scene graphs, fully specifies the goal scenes by adding trivially true edges, and then checks for graph isomorphism between the problem graphs. This approach captures the semantic equivalence of planning problems beyond syntactic matching.
- Core assumption: Graph isomorphism on the scene graph representations is sufficient to determine semantic equivalence of planning problems under the given definition.
- Evidence anchors:
  - [abstract] "Our method ensures two PDDL problems match if and only if they represent the same underlying planning problem."
  - [section] "Algorithm 1 Planning Problem Equivalence...transforms the PDDL code into scene graphs, computes an expansion of the goal states for both PDDL problems, and then performs isomorphism checks between the graphs."
  - [corpus] "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation" - suggests rigorous evaluation methods are needed and valued in this field.
- Break condition: If the scene graph representation fails to capture all semantically relevant aspects of the planning problem, or if the graph isomorphism check is computationally infeasible for larger problems.

### Mechanism 2
- Claim: Planetarium's dataset construction, varying abstraction and size dimensions, creates a challenging benchmark that reveals the limitations of current LLMs in PDDL generation.
- Mechanism: By creating text-to-PDDL pairs with varying levels of abstraction (explicit vs abstract) and size (number of propositions), Planetarium exposes LLMs to a diverse set of challenges. This design reveals that LLMs struggle particularly with abstract descriptions and larger problems.
- Core assumption: Varying these dimensions meaningfully captures the difficulty spectrum of PDDL generation tasks.
- Evidence anchors:
  - [section] "We vary the data along two dimensions: abstractness (explicit vs abstract) and size...Each instance is a ground truth PDDL problem description and text description pair."
  - [section] "A breakdown of GPT-4o's zero-shot performance...shows abstract task descriptions are harder to translate than explicit ones, and fully explicit task descriptions make generating parseable PDDL code easier."
  - [corpus] "NL2Plan: Robust LLM-Driven Planning from Minimal Text Descriptions" - suggests that text description format significantly impacts LLM planning performance.
- Break condition: If the abstraction and size dimensions don't actually correlate with difficulty, or if other important dimensions are missing.

### Mechanism 3
- Claim: Planetarium's rigorous evaluation framework, using the equivalence algorithm instead of simple plan validators, provides a more accurate assessment of LLM PDDL generation quality.
- Mechanism: Instead of just checking if generated PDDL is parseable and solvable, Planetarium verifies semantic correctness by checking equivalence to ground truth using the equivalence algorithm. This reveals that many seemingly correct PDDL problems are actually semantically wrong.
- Core assumption: Semantic correctness is a more important evaluation metric than syntactic validity or solvability for PDDL generation tasks.
- Evidence anchors:
  - [abstract] "96.1% of the PDDL problem descriptions generated by GPT-4o are syntactically parseable, 94.4% are solvable, but only 24.8% are semantically correct"
  - [section] "Using LLMs to translate natural language to PDDL is an instance of a code generation task...benchmarks often test functional correctness using a suite of unit tests...For planning problems, existing works use 'plan validators' to check if the generated code can be solved with a traditional planner...We argue that these validators alone are insufficient to determine if PDDL generation is correct."
  - [corpus] "On the Limit of Language Models as Planning Formalizers" - suggests that current evaluation methods may be insufficient for LLM planning formalizers.
- Break condition: If the equivalence algorithm is too strict or misses valid alternative PDDL representations that should be considered correct.

## Foundational Learning

- Concept: Classical Planning Problems and PDDL
  - Why needed here: Understanding the formal definition of planning problems and how PDDL represents them is essential for grasping Planetarium's approach to evaluating PDDL generation.
  - Quick check question: What are the key components of a classical planning problem as defined in the paper, and how does PDDL represent these components?

- Concept: Graph Isomorphism and Scene Graphs
  - Why needed here: The equivalence algorithm relies on transforming PDDL problems into scene graphs and checking for graph isomorphism. Understanding these concepts is crucial for understanding how the algorithm works.
  - Quick check question: How does the paper define scene graphs for PDDL problems, and why is graph isomorphism used to check equivalence?

- Concept: Large Language Models and Code Generation
  - Why needed here: Planetarium evaluates LLMs on their ability to generate PDDL from natural language. Understanding LLM capabilities and limitations in code generation tasks provides context for interpreting the results.
  - Quick check question: What are some of the challenges in using LLMs for code generation tasks, and how do these challenges manifest in the PDDL generation task evaluated by Planetarium?

## Architecture Onboarding

- Component map:
  Dataset Generator -> PDDL Equivalence Algorithm -> LLM Evaluation Pipeline -> Fine-tuning Module

- Critical path:
  1. Generate text-to-PDDL pairs with desired properties (abstraction, size)
  2. Evaluate LLM-generated PDDL using equivalence algorithm
  3. Analyze results to understand LLM strengths and weaknesses

- Design tradeoffs:
  - STRIPS subset vs. more expressive PDDL subsets: STRIPS is simpler but less expressive
  - Abstract vs. explicit descriptions: Abstract descriptions are more challenging but more realistic
  - Placeholder vs. strict equivalence: Placeholder mode allows more flexibility but may miss some errors

- Failure signatures:
  - High parseability but low semantic correctness: LLM generates valid PDDL that doesn't match the task
  - Low solvability: LLM generates PDDL with logical errors or inconsistencies
  - Slow equivalence checking: Graph isomorphism becomes computationally expensive for large problems

- First 3 experiments:
  1. Test the equivalence algorithm on a set of hand-crafted PDDL pairs with known equivalence relationships
  2. Evaluate a simple LLM (e.g., GPT-3.5) on a small subset of the Planetarium dataset to establish baseline performance
  3. Analyze the performance breakdown by abstraction level and problem size to identify the most challenging aspects for LLMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between PDDL semantic accuracy and planning performance when using generated PDDL with classical planners?
- Basis in paper: [explicit] The paper notes that 82.2% of GPT-4o's generated PDDL is valid and solvable, but only 35.1% is semantically correct, suggesting a disconnect between syntactic validity and semantic correctness
- Why unresolved: The paper doesn't measure how much semantic errors in PDDL actually impact planning performance or whether some semantic errors are more critical than others
- What evidence would resolve it: Systematic testing of planning performance across different types and degrees of semantic errors in generated PDDL

### Open Question 2
- Question: How do different abstraction levels in natural language descriptions affect the difficulty of PDDL generation across various model architectures?
- Basis in paper: [explicit] The paper shows abstract task descriptions are harder than explicit ones for GPT-4o, but only provides limited comparison across model types
- Why unresolved: The evaluation only compares zero-shot performance of a few models, without deeper analysis of how model architecture (transformer vs. other) handles different abstraction levels
- What evidence would resolve it: Comparative study of multiple model architectures on the same abstract/explicit description pairs

### Open Question 3
- Question: What is the theoretical limit of LLM performance on this task, and how much can be improved through domain-specific fine-tuning?
- Basis in paper: [inferred] The paper shows fine-tuning improves performance but doesn't explore the limits of improvement or optimal fine-tuning strategies
- Why unresolved: The paper only uses a single fine-tuning approach (QLoRA for one epoch) without exploring parameter efficiency or alternative fine-tuning methods
- What evidence would resolve it: Systematic exploration of fine-tuning duration, methods, and parameter efficiency across multiple model sizes

## Limitations

- The equivalence algorithm's computational complexity for larger planning problems remains uncertain and may not scale to more complex domains
- The benchmark is limited to Blocks World and Gripper domains, potentially limiting generalizability to other planning problem types
- The evaluation focuses on the STRIPS subset of PDDL, excluding more expressive features like temporal or numeric constraints

## Confidence

**High Confidence**: The observation that LLMs struggle with abstract descriptions and that semantic correctness is much lower than syntactic validity is well-supported by the empirical results. The equivalence algorithm's core design appears sound for the benchmark's scale.

**Medium Confidence**: The claim that Planetarium provides a more rigorous evaluation framework than existing methods is reasonable but depends on the equivalence algorithm's correctness and computational feasibility at scale. The paper's arguments are compelling but not exhaustively validated.

**Low Confidence**: The assertion that hybrid approaches combining LLMs with classical planners are necessary is based on the current results but represents a future research direction rather than a proven conclusion from this work.

## Next Checks

1. **Algorithm Scalability**: Test the equivalence algorithm on larger, more complex planning problems from diverse domains to evaluate computational complexity and potential bottlenecks.

2. **Alternative Representations**: Compare the scene graph isomorphism approach against alternative semantic equivalence checking methods, such as plan-based validation or logical entailment, to assess the robustness of the evaluation framework.

3. **Cross-Domain Generalization**: Evaluate the same LLM models on text-to-PDDL tasks from different planning domains (e.g., logistics, scheduling) to determine if the observed performance patterns hold across domain boundaries.