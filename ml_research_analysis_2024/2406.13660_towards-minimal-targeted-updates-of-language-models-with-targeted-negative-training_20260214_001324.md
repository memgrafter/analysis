---
ver: rpa2
title: Towards Minimal Targeted Updates of Language Models with Targeted Negative
  Training
arxiv_id: '2406.13660'
source_url: https://arxiv.org/abs/2406.13660
tags:
- negative
- targeted
- original
- methods
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Targeted Negative Training (TNT), a method
  for minimally updating language models to avoid unwanted outputs while preserving
  existing behavior. The core idea uses token-level negative annotations to guide
  training via reverse KL-divergence minimization, unlike existing approaches that
  only push down probability without controlling where it redistributes.
---

# Towards Minimal Targeted Updates of Language Models with Targeted Negative Training

## Quick Facts
- **arXiv ID**: 2406.13660
- **Source URL**: https://arxiv.org/abs/2406.13660
- **Reference count**: 24
- **Primary result**: Introduces Targeted Negative Training (TNT) for minimally updating language models to avoid unwanted outputs while preserving existing behavior, achieving better trade-offs than baselines.

## Executive Summary
This paper addresses the challenge of minimally updating language models to avoid unwanted outputs while preserving existing behavior. The authors introduce Targeted Negative Training (TNT), a method that uses token-level negative annotations to guide training via reverse KL-divergence minimization. Unlike existing approaches that only push down probability without controlling redistribution, TNT uniquely identifies minimal targeted updates by computing information projections onto the set of distributions that avoid negative tokens. Experiments on hallucination reduction in summarization and toxicity reduction in response generation demonstrate that TNT achieves better trade-offs between unwanted behavior reduction and maintaining original model quality compared to baselines, with particular effectiveness for targeted updates across different model sizes.

## Method Summary
TNT operates by first generating outputs from the original model and annotating unwanted tokens at the token level. For each token position, it computes a target distribution by zeroing out probability mass at negative tokens and renormalizing. The method then minimizes reverse KL-divergence between this target and the updated model at each token step, with forward KL applied to positive tokens. This approach uniquely identifies the minimal targeted update - the distribution closest to the original while satisfying negative token constraints. The method can be applied iteratively as new negative examples are discovered, leveraging the commutative property of deterministic probability zeroing. TNT is evaluated against baseline methods that use standard negative losses combined with label smoothing or unlikelihood losses, with comparisons made across multiple model sizes and tasks.

## Key Results
- TNT methods maintain higher BLEU scores (up to 61 vs 50 for baselines) while achieving similar reductions in unwanted behavior
- TNT introduces significantly fewer disfluencies (word repeats and random ?? tokens) compared to baseline negative losses
- TNT demonstrates better trade-offs between unwanted behavior reduction and similarity to original generations across multiple model sizes (220M and 1B parameters)
- The method is particularly effective for targeted updates and works across different model sizes

## Why This Works (Mechanism)

### Mechanism 1
TNT works because it uses token-level negative annotations to compute a targeted distribution via reverse KL-divergence minimization, unlike baselines that only push down probability without controlling redistribution. The method first identifies negative tokens from model generations, then computes pnew by zeroing out probability at those tokens in the original distribution and renormalizing. It then minimizes KL divergence between this target and the updated model at each token step. The reverse KL-divergence minimization uniquely identifies the minimal targeted update, i.e., the distribution closest to the original while satisfying the negative token constraint. If negative token annotations are inaccurate or incomplete, the target distribution pnew becomes misaligned, and the update no longer represents a minimal targeted change.

### Mechanism 2
TNT maintains model quality because it constrains updates to distributions that are close to the original in reverse KL-divergence, thereby limiting catastrophic forgetting. By minimizing reverse KL-divergence, TNT penalizes deviations from the original model's token distributions except where forced by negative constraints, so most of the model's behavior is preserved. Reverse KL-divergence is a suitable metric for "closeness" in this context, and its minimization yields updates that preserve most original behavior. If the reverse KL objective is too permissive, the model may still drift significantly from the original in non-negative regions.

### Mechanism 3
TNT's commutative property allows iterative application of negative updates without needing all negative tokens upfront. Because zeroing probability at negative tokens is deterministic and order-independent, TNT can be applied repeatedly as new negative examples are discovered, without retraining from scratch. The deterministic nature of probability zeroing is preserved across iterations and does not interact adversely with gradient-based optimization. If gradient updates introduce stochasticity that interacts with the deterministic zeroing, the commutative property may be lost in practice.

## Foundational Learning

- **Concept: KL-divergence (forward and reverse)** - Why needed: TNT relies on reverse KL-divergence minimization to find the closest distribution to the original that avoids negative tokens. Quick check: What is the difference between forward and reverse KL-divergence, and why is reverse KL used in TNT?

- **Concept: Information projection** - Why needed: The solution to a minimal targeted update is an information projection onto the set of distributions that avoid negative tokens. Quick check: What is an information projection, and why is it guaranteed to be unique in this context?

- **Concept: Autoregressive language model structure** - Why needed: TNT operates on the sequence of token-level conditional distributions that define the autoregressive model. Quick check: How does the autoregressive factorization enable token-level updates in TNT?

## Architecture Onboarding

- **Component map**: Original model, training inputs, model generations, token-level negative annotations -> Forward passes through original and current models -> Computation of target distributions via renormalization -> Loss calculation using reverse KL for negatives and forward KL for positives -> Updated model parameters

- **Critical path**:
  1. Generate outputs from original model on training inputs
  2. Annotate outputs for negative tokens
  3. For each batch: forward pass through original and current models
  4. Compute pnew for each token by zeroing negative token probability and renormalizing
  5. Calculate TNT loss (reverse KL for negatives, forward KL for positives)
  6. Backpropagate and update current model

- **Design tradeoffs**: Memory requires keeping original model in memory for forward passes; computation includes extra forward pass per gradient step; annotation cost involves more granular token-level annotations than sequence-level; update granularity can target specific token types but may miss contextual nuances

- **Failure signatures**: Obvious disfluencies (word repeats, random ?? tokens) indicate loss imbalance or poor annotation; degradation in BLEU/ROUGE scores beyond acceptable levels indicates over-constraining; no reduction in unwanted behavior indicates ineffective negative annotations or learning rate issues

- **First 3 experiments**:
  1. Run TNT with only reverse KL on both positive and negative tokens (TNRR) to test if the objective alone is sufficient
  2. Vary alpha (weight on negative loss) to find the Pareto frontier between unwanted behavior reduction and similarity
  3. Compare disfluency rates across methods to validate that TNT introduces fewer artifacts than baselines

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of TNT compare when using token-level annotations versus sequence-level annotations for negative examples? The paper discusses token-level annotations and mentions that sequence-level annotations could be used with methods from Meng et al. (2022) to translate them into token-level guidance, but doesn't provide empirical results comparing the two approaches.

### Open Question 2
What is the impact of the choice of divergence measure (forward vs. reverse KL) on the quality of the minimal targeted update in TNT? While the paper mentions the use of both forward and reverse KL divergences in TNT, it doesn't thoroughly analyze how the choice between them affects the quality of the minimal targeted update in terms of unwanted behavior reduction and similarity preservation.

### Open Question 3
How does the performance of TNT scale with increasing model size beyond the 1-billion parameter variant tested? The paper tests TNT on T5-base (220M parameters) and PaLM-2 1B (1B parameters), noting that larger models seem harder to update but doesn't explore how performance scales with even larger models or what factors contribute to this difficulty.

### Open Question 4
What are the computational and memory requirements of TNT compared to other fine-tuning approaches, and how do these scale with model size? While the paper acknowledges the increased computational and memory requirements of TNT, it doesn't provide quantitative measurements of these costs or analyze how they scale with model size.

### Open Question 5
How does the commutative property of negative updates in TNT affect its performance when applying multiple rounds of updates with different sets of negative examples? While the paper introduces the concept of the commutative property, it doesn't explore how this property affects TNT's performance when applying multiple rounds of updates with different sets of negative examples, or how the order of applying these updates impacts the final result.

## Limitations
- The effectiveness of TNT relies heavily on accurate token-level negative annotations generated by heuristic methods that are not fully specified
- The theoretical properties of reverse KL-divergence as the "minimal" update objective lack comprehensive empirical validation
- The commutative property claim may not hold in practice due to gradient-based optimization introducing stochastic effects
- Evaluation focuses on specific metrics that may not capture all aspects of model quality changes

## Confidence

- **High Confidence**: Basic implementation of TNT methods, empirical results showing better BLEU/ROUGE scores and fewer disfluencies compared to baselines, observation that TNT methods maintain better trade-offs between unwanted behavior reduction and model quality preservation

- **Medium Confidence**: Theoretical properties of reverse KL-divergence minimization guaranteeing minimal targeted updates, uniqueness of information projection solution, commutative property enabling iterative application

- **Low Confidence**: Claims that TNT is the "only" method achieving both low disfluency rates and strong trade-offs, that the specific choice of reverse KL is critical to performance, that the commutative property is practically useful in iterative settings

## Next Checks

1. **Validate the reverse KL objective**: Run ablation experiments comparing TNT with other divergence measures (forward KL, Jensen-Shannon divergence) on the same task to determine if reverse KL uniquely achieves the claimed minimal targeted updates, or if other objectives provide similar benefits.

2. **Test annotation robustness**: Conduct experiments with varying annotation quality (e.g., using different hallucination detection heuristics, different toxicity thresholds) to determine how sensitive TNT performance is to annotation accuracy, and whether the method can handle imperfect annotations.

3. **Evaluate commutative property in practice**: Implement iterative application of TNT with new negative annotations discovered over time, and measure whether the commutative property holds empirically across multiple update rounds, or if gradient-based optimization introduces order-dependent effects.