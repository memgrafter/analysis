---
ver: rpa2
title: 'SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based
  Scalar Latent Transformer Diffusion Models'
arxiv_id: '2408.13893'
source_url: https://arxiv.org/abs/2408.13893
tags:
- speech
- audio
- duration
- diffusion
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SimpleSpeech 2 presents a non-autoregressive text-to-speech framework
  that achieves high-quality speech synthesis by leveraging a flow-based scalar latent
  transformer diffusion model. The system uses a scalar quantization-based audio codec
  (SQ-Codec) to compress speech into a finite latent space, which is more suitable
  for generative modeling than continuous representations.
---

# SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models

## Quick Facts
- arXiv ID: 2408.13893
- Source URL: https://arxiv.org/abs/2408.13893
- Authors: Dongchao Yang; Rongjie Huang; Yuanyuan Wang; Haohan Guo; Dading Chong; Songxiang Liu; Xixin Wu; Helen Meng
- Reference count: 40
- Primary result: Non-autoregressive TTS framework achieving MOS 4.28 vs 3.97 for ground truth, RTF 0.25 vs 1.6-10.2 for baselines

## Executive Summary
SimpleSpeech 2 presents a non-autoregressive text-to-speech framework that achieves high-quality speech synthesis by leveraging a flow-based scalar latent transformer diffusion model. The system uses a scalar quantization-based audio codec (SQ-Codec) to compress speech into a finite latent space, which is more suitable for generative modeling than continuous representations. A key innovation is the use of sentence duration to control speech length without requiring fine-grained phoneme-level alignment. Experiments demonstrate that SimpleSpeech 2 outperforms autoregressive and non-autoregressive baselines in terms of naturalness, robustness, and inference speed.

## Method Summary
SimpleSpeech 2 uses scalar quantization (SQ) to compress speech into a finite latent space, which is then modeled using a flow-based transformer diffusion model with Time-MoE. The system eliminates the need for phoneme-level alignment by using sentence duration to control speech length. Text is encoded using ByT5 without requiring phoneme-level alignment, while speaker timbre and style are extracted using FACodec. The model is trained on large-scale unlabeled speech data with ASR-generated transcriptions, using classifier-free guidance during training.

## Key Results
- Achieves MOS of 4.28 compared to 3.97 for ground truth on multi-source test set
- Reduces real-time factor (RTF) from 1.6-10.2 to 0.25 compared to baselines
- Achieves 7.5 WER and 0.933 speaker similarity on English data
- Performs competitively on multilingual tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing residual vector quantization (RVQ) with scalar quantization (SQ) produces a finite, compact latent space that is easier for diffusion models to model.
- Mechanism: Scalar quantization maps each frame's continuous feature into one of 2*S+1 discrete values, reducing the complexity of the latent distribution from infinite to finite and removing the need for codebook collapse mitigation.
- Core assumption: A finite latent space with a simple, uniform distribution is more stable for flow-based diffusion training than an infinite, complex continuous distribution.
- Evidence anchors:
  - [abstract] "SQ-Codec not only achieves good reconstruction performance but also provides a finite, compact latent space, which is highly suitable for generative models."
  - [section III-C] "We demonstrate that SQ-Codec not only achieves good reconstruction performance but also provides a finite, compact latent space, which is highly suitable for generative models."
  - [corpus] Weak evidence; no direct citation of scalar quantization's role in TTS diffusion, but prior image tokenizer studies support the finite space idea.
- Break condition: If the tanh rounding still produces out-of-range values or if the finite space limits expressiveness enough to degrade MOS below 3.8.

### Mechanism 2
- Claim: Flow-based diffusion (flow matching) with a linear interpolation schedule produces stable, high-quality generation with fewer steps than DDPM.
- Mechanism: Flow matching solves a probability flow ODE with a velocity field that directly maps noise to data along a straight path, avoiding the multi-step noise prediction of DDPM and thus reducing the number of inference steps.
- Core assumption: The velocity field parameterized by a transformer can approximate the ground-truth velocity without needing denoising timesteps or score estimation.
- Evidence anchors:
  - [section III-D.1] "flow matching uses a simple strategy to corrupt data: it linearly interpolates between noise and data in a straight line" and "We refer to Eq. 5 as a flow-based generative model."
  - [section V-F] "demonstrate the efficacy of the Time-MoE design and the flow-based diffusion approach."
  - [corpus] No explicit evidence for TTS-specific flow matching performance; relies on prior image/video literature.
- Break condition: If the transformer velocity field fails to converge or produces artifacts in generated speech, especially when timesteps are near 0.

### Mechanism 3
- Claim: Sentence duration control eliminates the need for fine-grained phoneme alignment, simplifying data preparation while preserving prosody control.
- Mechanism: The model uses a coarse sentence duration (sum of phoneme durations or ChatGPT prediction) to scale the noisy latent sequence length, avoiding the need to learn per-phoneme timing and thus removing the alignment bottleneck.
- Core assumption: Coarse duration is sufficient to control speech length and preserve naturalness without requiring explicit phoneme boundaries.
- Evidence anchors:
  - [abstract] "A key innovation is the use of sentence duration to control speech length without requiring fine-grained phoneme-level alignment."
  - [section III-F] "In this study, we follow the line of our previous publication SimpleSpeech uses sentence duration to control the speech length."
  - [section V-E] "We observe that Four types of sentence duration models are effective."
- Break condition: If the coarse duration prediction is too inaccurate, leading to WER > 10% or MOS drop > 0.3 compared to ground truth.

## Foundational Learning

- Concept: Diffusion probabilistic models (DDPM)
  - Why needed here: Understanding the difference between DDPM and flow matching is critical for grasping why fewer inference steps are needed.
  - Quick check question: What is the main computational difference between noise prediction in DDPM and velocity prediction in flow matching?

- Concept: Variational Autoencoders (VAEs) vs audio codecs
  - Why needed here: Recognizing why VAEs force data into a normal distribution while codecs preserve reconstruction quality helps explain SQ-Codec's design choice.
  - Quick check question: How does the KL penalty in VAEs affect reconstruction compared to the direct reconstruction loss in codecs?

- Concept: Scalar quantization vs vector quantization
  - Why needed here: Knowing how scalar quantization reduces the search space from infinite to finite explains the claimed improvement in generative model stability.
  - Quick check question: What is the size of the search space for scalar quantization with S=9, and how does that compare to RVQ with 2 codebooks of size 512?

## Architecture Onboarding

- Component map:
  - Text Encoder (ByT5) → Text latent
  - Speaker Encoder (FACodec timbre+style) → Speaker prompt
  - SQ-Codec (Encoder/Decoder + SQ) → Finite latent space
  - Flow-based Transformer Diffusion (Time-MoE + velocity field) → Predicted latent
  - Decoder → Waveform

- Critical path: Text/Speaker → Transformer diffusion (velocity prediction) → SQ → Decoder → Waveform

- Design tradeoffs:
  - Using scalar quantization reduces search space but may limit expressiveness; using more latent dimensions improves reconstruction but slows generation.
  - Flow matching avoids multi-step noise prediction but requires accurate velocity field learning; more timesteps improve quality but increase inference cost.
  - Sentence duration control simplifies data prep but depends on duration predictor accuracy; coarse control may limit fine-grained prosody.

- Failure signatures:
  - Poor reconstruction from SQ-Codec → low PESQ/STOI, high MCD.
  - Unstable or noisy velocity field → artifacts or dropped words in output.
  - Inaccurate duration prediction → mismatched speech length or garbled prosody.

- First 3 experiments:
  1. Compare SQ-Codec vs SoundStream reconstruction metrics (PESQ, STOI, SSIM) on LibriTTS.
  2. Run flow-based diffusion with 5, 10, 25 steps and measure RTF vs MOS trade-off.
  3. Replace sentence duration predictor with ground truth and measure change in WER/MOS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the completeness and compactness of different speech tokenizers impact the quality of generated speech in large-scale TTS systems?
- Basis in paper: [explicit] The paper conducts experiments comparing SQ-Codec with other tokenizers like VAE and SoundStream, finding that SQ-Codec's finite scalar latent space is easier for generative models to learn.
- Why unresolved: While the paper shows SQ-Codec performs better than alternatives, it doesn't fully explore the trade-offs between different levels of completeness and compactness across a wider range of tokenizers.
- What evidence would resolve it: Systematic comparison of TTS performance using tokenizers with varying levels of completeness and compactness, including novel tokenizer designs that optimize this balance.

### Open Question 2
- Question: Can ASR transcriptions with high WER (>10%) still be effectively used to train high-quality TTS systems?
- Basis in paper: [explicit] The paper provides theoretical analysis showing that including noisy labels in large-scale datasets is equivalent to introducing classifier-free guidance training, but only validates this with Whisper's low WER performance.
- Why unresolved: The theoretical framework assumes most words occur frequently, but doesn't account for systematic errors or domain-specific vocabulary that might appear in lower-quality ASR systems.
- What evidence would resolve it: Training TTS systems using ASR transcriptions from multiple ASR systems with varying WER levels, and correlating WER with TTS performance metrics.

### Open Question 3
- Question: What is the optimal architecture for sentence duration prediction in non-autoregressive TTS systems?
- Basis in paper: [explicit] The paper compares four different approaches (ByT5-based, ChatGPT, FS2-based, and AR-based) but notes that ByT5-based shows best performance without fully explaining why.
- Why unresolved: The comparison focuses on performance metrics but doesn't investigate the underlying reasons for architectural differences, such as how each approach handles polyphones in different languages or long-term dependencies.
- What evidence would resolve it: Ablation studies isolating specific architectural components, cross-linguistic evaluation to test language-specific capabilities, and analysis of failure cases for each approach.

## Limitations
- The Time-MoE module architecture and training procedure are not fully described, making exact reproduction difficult.
- The prompt format for ChatGPT-based sentence duration prediction is mentioned but not specified.
- The study relies on 7k hours of unlabeled English speech with ASR-generated transcriptions, but the quality and domain coverage of these transcriptions is not thoroughly evaluated.

## Confidence
- **High confidence** in the technical novelty of combining scalar quantization with flow-based diffusion for TTS, supported by clear mathematical formulations and the SQ-Codec design.
- **Medium confidence** in the claimed efficiency gains (RTF reduction) and MOS improvements, as these are benchmarked against reasonable baselines but with limited ablation studies isolating individual contributions.
- **Low confidence** in the generalizability of sentence duration control across diverse languages and speaking styles, as the evaluation focuses primarily on English with limited multilingual analysis.

## Next Checks
1. **SQ-Codec Reconstruction Validation**: Measure PESQ, STOI, and SSIM scores of the SQ-Codec on LibriTTS test set and compare against SoundStream and continuous VAE codecs to verify the claimed reconstruction quality and suitability for generative modeling.

2. **Flow Matching Step Efficiency Analysis**: Conduct controlled experiments varying diffusion steps (5, 10, 25, 50) to quantify the MOS vs RTF trade-off, and compare against a DDPM baseline with equivalent computational budget on the same dataset.

3. **Sentence Duration Control Robustness Test**: Replace the predicted sentence duration with ground truth duration in the model and measure the change in WER and MOS to quantify the impact of duration prediction accuracy on overall system performance.