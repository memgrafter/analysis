---
ver: rpa2
title: Diminishing Return of Value Expansion Methods
arxiv_id: '2412.20537'
source_url: https://arxiv.org/abs/2412.20537
tags:
- learning
- expansion
- value
- oracle
- horizons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether the sample efficiency of model-based
  value expansion methods is limited by the accuracy of the dynamics model. Using
  an oracle dynamics model, the authors find that increasing the rollout horizon and
  model accuracy yields diminishing returns in improving sample efficiency, even when
  compounding model errors are eliminated.
---

# Diminishing Return of Value Expansion Methods

## Quick Facts
- arXiv ID: 2412.20537
- Source URL: https://arxiv.org/abs/2412.20537
- Reference count: 40
- Primary result: Model accuracy is not the primary bottleneck limiting sample efficiency of value expansion methods

## Executive Summary
This paper investigates whether the sample efficiency of model-based value expansion methods is limited by the accuracy of the dynamics model. Using an oracle dynamics model that eliminates compounding model errors, the authors find that increasing the rollout horizon and model accuracy yields diminishing returns in improving sample efficiency. Model-free RETRACE methods achieve comparable performance without the computational overhead of model-based approaches. The authors conclude that the bottleneck in model-based value expansion methods lies elsewhere, challenging the common assumption that model accuracy is the primary constraint.

## Method Summary
The paper compares model-based value expansion methods (SAC with Critic Expansion and Actor Expansion) against model-free RETRACE using both oracle and learned dynamics models. Experiments run on continuous control tasks (InvertedPendulum, Cartpole, Hopper, Walker2d, Halfcheetah) and discrete MinAtar tasks with horizons H ∈ {0,1,3,5,10,20,30}. The learned ensemble dynamics model consists of 5 neural networks with 4 hidden layers of 256 neurons each. Performance is measured using interquartile mean undiscounted returns across 9 random seeds, with computational overhead tracked separately.

## Key Results
- Oracle dynamics models show diminishing returns beyond horizon H=3-5, similar to learned models
- Model-free RETRACE achieves comparable sample efficiency without computational overhead
- Increasing rollout horizon increases variance in Q-function targets by orders of magnitude
- Gradient explosion occurs in actor expansion for longer horizons (H≥3 for oracle, H≥20 for learned)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diminishing returns occur because Q-function targets already capture the true return distribution well, so model-based expansion adds minimal improvement in expectation.
- Mechanism: The Q-function approximator learns to represent the true return distribution effectively, and longer model rollouts only marginally improve target accuracy in expectation while increasing variance.
- Core assumption: The Q-function approximator is sufficiently expressive to represent the true return distribution, and the additional information from longer rollouts does not significantly improve the target in expectation.
- Evidence anchors:
  - [abstract]: "increased model accuracy only marginally improves sample efficiency compared to learned models with identical horizons"
  - [section]: "The expected values of the target distribution Q^H are very similar for different horizons. This observation means that the approximated Q0-function already captures the true return very well and, therefore, a model-based expansion only marginally improves the targets in expectation."
  - [corpus]: Weak evidence. No direct corpus papers address this specific diminishing return phenomenon.
- Break condition: If the Q-function approximator is not sufficiently expressive or if the true return distribution has high complexity that requires longer rollouts to capture accurately.

### Mechanism 2
- Claim: Increasing variance in Q-function targets due to longer rollouts degrades learning stability and performance.
- Mechanism: Longer rollouts generate targets with higher variance, which destabilizes the learning process and leads to diminishing returns in sample efficiency.
- Core assumption: Higher variance in target values leads to unstable learning updates and degraded performance.
- Evidence anchors:
  - [section]: "However, the 4th row shows that the variance over targets increases by orders of magnitude with the increasing horizon."
  - [section]: "Both deterministic policies as well as variance reduction techniques show that the increasing variance of the Q-function targets does not explain the diminishing returns of value expansion methods."
  - [corpus]: Weak evidence. No direct corpus papers address variance effects on diminishing returns.
- Break condition: If variance reduction techniques or deterministic policies can mitigate the variance increase without eliminating the diminishing returns phenomenon.

### Mechanism 3
- Claim: Gradient instability in actor expansion methods limits the benefits of longer rollouts.
- Mechanism: Backpropagating through longer model rollouts creates computational graphs that are susceptible to exploding or vanishing gradients, limiting the effectiveness of actor expansion.
- Core assumption: Longer computational graphs in actor expansion are more susceptible to gradient instability.
- Evidence anchors:
  - [section]: "For SAC-AE, we partially correlate these results with the actor gradients in Figure 8. For example, the standard deviation of the Halfcheetah actor gradients explodes for horizons larger or equal to 3 for the oracle and 20 for the learned models."
  - [section]: "For AE, it might only be possible to expand for a few steps before requiring additional methods to handle gradient variance."
  - [corpus]: Weak evidence. No direct corpus papers address gradient instability in value expansion methods.
- Break condition: If gradient stabilization techniques can effectively handle the increased computational graph depth without eliminating diminishing returns.

## Foundational Learning

- Concept: Value expansion methods in reinforcement learning
  - Why needed here: Understanding how model-based value expansion works is crucial for grasping why diminishing returns occur.
  - Quick check question: What is the difference between actor expansion and critic expansion in value expansion methods?

- Concept: Compounding model error in model-based RL
  - Why needed here: The paper investigates whether compounding model errors are the primary limitation of value expansion methods.
  - Quick check question: How does compounding model error typically affect the performance of model-based RL methods?

- Concept: Off-policy vs on-policy learning
  - Why needed here: The paper compares model-based value expansion with model-free RETRACE, which uses off-policy trajectories.
  - Quick check question: What are the key differences between off-policy and on-policy learning approaches in reinforcement learning?

## Architecture Onboarding

- Component map:
  SAC/DDPG agent with Q-function and policy networks -> Dynamics model (oracle or learned) -> Replay buffer -> Value expansion module

- Critical path:
  1. Collect real environment experiences
  2. Store experiences in replay buffer
  3. Sample batch from replay buffer
  4. Compute value expansion targets using dynamics model
  5. Update Q-function and policy networks

- Design tradeoffs:
  - Model accuracy vs computational overhead
  - Rollout horizon length vs gradient stability
  - On-policy vs off-policy data usage

- Failure signatures:
  - Exploding gradients in actor expansion for long horizons
  - Degraded performance for longer rollout horizons
  - High variance in Q-function targets

- First 3 experiments:
  1. Implement SAC agent with basic value expansion (H=1)
  2. Add oracle dynamics model and compare performance for H=1,3,5
  3. Implement RETRACE and compare with model-based approaches for various horizons

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental computational bottlenecks that limit the effectiveness of model-based value expansion methods beyond a certain rollout horizon?
- Basis in paper: [explicit] The authors identify that the diminishing returns occur even with perfect dynamics models and attribute this to computational overhead, but do not pinpoint the exact computational bottlenecks.
- Why unresolved: The paper shows that computational overhead is a factor but does not perform a detailed analysis of which specific computational components (e.g., model rollout computation, gradient computation, memory usage) are most limiting.
- What evidence would resolve it: A systematic ablation study isolating and measuring the contribution of different computational components (model unrolling, policy gradient computation, target computation) to overall training time and sample efficiency.

### Open Question 2
- Question: Are there specific classes of environments or tasks where model-based value expansion methods with learned dynamics models outperform model-free alternatives?
- Basis in paper: [inferred] The paper shows that model-based methods with learned models generally perform similarly to or worse than model-free RETRACE, but does not explore whether specific environmental characteristics might favor model-based approaches.
- Why unresolved: The experiments focus on continuous control and discrete Atari-like environments but do not systematically vary environmental properties (e.g., dynamics complexity, reward sparsity, state dimensionality) to identify conditions favoring model-based methods.
- What evidence would resolve it: Controlled experiments varying environmental characteristics (e.g., reward sparsity, dynamics smoothness, state dimensionality) to identify which properties correlate with improved performance of model-based methods.

### Open Question 3
- Question: What alternative architectures or training techniques could mitigate the diminishing returns observed in value expansion methods?
- Basis in paper: [explicit] The authors test variance reduction through particle averaging and deterministic policies but observe no improvement in the diminishing returns phenomenon.
- Why unresolved: The paper only tests two specific techniques (particle averaging and deterministic policies) but does not explore other potential approaches such as hierarchical planning, uncertainty-aware rollouts, or different gradient stabilization methods.
- What evidence would resolve it: Experiments testing alternative techniques like hierarchical planning (coarse-to-fine), uncertainty-aware rollout termination, different gradient clipping strategies, or novel architectures that might better handle long-horizon predictions.

## Limitations
- Cannot isolate a single dominant bottleneck mechanism from the empirical evidence
- Potential task-specific effects not explored across diverse environment types
- Computational infeasibility of very long horizons (H≥20) limits investigation scope

## Confidence
- Model accuracy not primary bottleneck: Medium confidence - controlled oracle experiments provide strong evidence but alternative bottleneck mechanisms lack direct validation
- Mechanistic explanations for diminishing returns: Low confidence - multiple hypotheses presented but none conclusively validated
- RETRACE vs model-based comparison: Medium confidence - controlled comparisons show similar performance but may not capture all use cases

## Next Checks
1. Isolate Q-function expressiveness bottleneck: Compare performance of value expansion methods using expressive (e.g., ensemble) vs. limited (e.g., linear) Q-function approximators to test whether target distribution representation limits improvements from longer rollouts.

2. Systematically control variance: Implement and test variance reduction techniques (e.g., target network ensembles, gradient clipping) specifically designed to isolate the effect of target variance on diminishing returns.

3. Gradient stabilization ablation: Compare actor expansion performance with and without gradient stabilization techniques (e.g., gradient clipping, normalization) to quantify the contribution of gradient instability to diminishing returns.