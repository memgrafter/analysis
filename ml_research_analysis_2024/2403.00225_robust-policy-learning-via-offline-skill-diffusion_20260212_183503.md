---
ver: rpa2
title: Robust Policy Learning via Offline Skill Diffusion
arxiv_id: '2403.00225'
source_url: https://arxiv.org/abs/2403.00225
tags:
- skill
- learning
- datasets
- decoder
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DuSkill addresses the challenge of skill-based policy learning
  in domains different from training data. It introduces an offline skill diffusion
  framework that uses a guided diffusion model with hierarchical domain encoding to
  disentangle domain-invariant and domain-variant features in skills.
---

# Robust Policy Learning via Offline Skill Diffusion

## Quick Facts
- arXiv ID: 2403.00225
- Source URL: https://arxiv.org/abs/2403.00225
- Reference count: 2
- One-line primary result: Achieves 92.4% performance in few-shot imitation and 3.32 average reward in online RL adaptation for target domains, outperforming baselines by 89.16%

## Executive Summary
DuSkill introduces an offline skill diffusion framework that addresses the challenge of learning policies for downstream tasks in domains different from training data. The method uses a hierarchical domain encoding approach to disentangle domain-invariant and domain-variant features in skills, enabling effective cross-domain generalization. By employing a guided diffusion model as the decoder, DuSkill generates diverse skills that extend beyond the limited skills available in offline datasets, achieving state-of-the-art performance in both few-shot imitation and online reinforcement learning settings.

## Method Summary
DuSkill is an offline skill diffusion framework that learns to generate versatile skills for downstream tasks in different domains from training data. The method consists of two phases: an offline skill diffusion phase and a downstream policy learning phase. During the offline phase, a hierarchical domain encoder disentangles skills into domain-invariant and domain-variant embeddings using a domain-invariant encoder and a domain-variant encoder. A guided diffusion-based decoder then generates diverse skills by conditioning on both embedding spaces. For downstream tasks, a high-level policy is trained using the learned skill embeddings, either through few-shot imitation on limited target domain trajectories or online reinforcement learning with SAC. The framework achieves cross-domain generalization by separating the skill representation space into domain-invariant features (task-relevant) and domain-variant features (domain-specific).

## Key Results
- Achieves 92.4% performance in few-shot imitation tasks across multi-stage Meta-World domains
- Achieves 3.32 average reward in online RL adaptation for target domains
- Outperforms offline RL and offline IL baselines by 89.16% in few-shot imitation settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical domain encoding enables effective disentanglement of domain-invariant and domain-variant skill features
- Mechanism: The hierarchical encoder architecture processes state-action sequences through separate pathways, with the domain-invariant encoder extracting core task-relevant features while the domain-variant encoder conditions on domain parameters to capture domain-specific variations
- Core assumption: Skills can be meaningfully decomposed into domain-invariant and domain-variant components
- Evidence anchors:
  - [abstract] "we devise a guided diffusion-based skill decoder in conjunction with the hierarchical encoding to disentangle the skill embedding space into two distinct representations"
  - [section 3.2] "we employ a domain-invariant encoder qρ which maps a sequence of states and actions to the domain-invariant embedding. We also use a domain-variant encoder qσ which maps the domain-invariant embedding and the domain parameterization ω to the domain-variant embedding"
  - [corpus] Weak evidence - no direct corpus support for hierarchical encoding approach
- Break condition: If domain-invariant and domain-variant features cannot be meaningfully separated, the disentanglement will fail and the framework will not generalize across domains

### Mechanism 2
- Claim: Guided diffusion-based decoder generates diverse skills that extend beyond training datasets
- Mechanism: The diffusion model learns to denoise action sequences conditioned on both domain-invariant and domain-variant embeddings, with the classifier-free guidance technique enabling distinct modulation of each embedding space
- Core assumption: Diffusion models can effectively learn the conditional distribution of skills given the disentangled embeddings
- Evidence anchors:
  - [abstract] "employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets"
  - [section 3.2] "we adopt the denoising diffusion probabilistic model (DDPM)...The decoder reconstructs an action at from a noisy input...by sequentially predicting xK−1, xK−2, ..., x0(= at)"
  - [section 3.2] "we slightly modify the classifier-free guidance...to divide the decoder into two separate parts: domain-invariant decoder ϵρ and domain-variant decoder ϵσ"
- Break condition: If the diffusion model fails to learn the conditional distribution properly, generated skills will be poor quality or not diverse enough to handle different domains

### Mechanism 3
- Claim: Separating priors for domain-invariant and domain-variant embeddings enables robust downstream policy adaptation
- Mechanism: The framework learns separate priors pρ and pσ that guide the high-level policy in generating appropriate embeddings for different domains, with the domain-invariant prior conditioning on current state and domain-variant prior conditioning on domain-invariant embedding
- Core assumption: Separate priors for each embedding space can be learned that generalize across domains
- Evidence anchors:
  - [section 3.2] "For downstream policy learning, we employ a domain-invariant prior pρ and a domain-variant prior pσ"
  - [section 3.2] "The domain-invariant prior pρ is conditioned on the current state st, facilitating the selection of suitable domain-invariant embedding, while the domain-variant prior pσ provides a prior distribution over the domain-variant embedding"
  - [section 3.3] "we fine-tune the high-level policy using (10) along with the learned guided diffusion-based decoder"
- Break condition: If the learned priors do not generalize well to new domains, the high-level policy will struggle to generate appropriate skill embeddings for downstream tasks

## Foundational Learning

- Concept: Variational Autoencoder (VAE) framework for skill representation
  - Why needed here: Provides the foundation for learning compressed skill representations that can be used for downstream policy learning
  - Quick check question: What are the two main components of the VAE loss function used in skill learning?

- Concept: Diffusion probabilistic models
  - Why needed here: Enables generation of diverse skills through a denoising process that can be conditioned on multiple factors
  - Quick check question: What is the key difference between standard diffusion models and the guided diffusion approach used here?

- Concept: Hierarchical latent variable models
  - Why needed here: Allows separation of domain-invariant and domain-variant features in the skill representation space
  - Quick check question: How does the hierarchical structure differ from a flat latent space in terms of information flow?

## Architecture Onboarding

- Component map: Hierarchical Domain Encoder (qρ, qσ) -> Guided Diffusion-Based Decoder (ϵρ, ϵσ) -> High-Level Policy (π)
- Critical path: Datasets -> Hierarchical Encoder -> Diffusion Decoder -> Skill Generation -> High-Level Policy -> Action Execution
- Design tradeoffs:
  - Complexity vs. performance: The hierarchical approach adds architectural complexity but enables better cross-domain generalization
  - Training stability vs. expressiveness: The guided diffusion approach provides more control over generation but may be harder to train than standard diffusion
  - Sample efficiency vs. robustness: Freezing the decoder after training improves sample efficiency but may limit adaptation capability
- Failure signatures:
  - Poor performance on target domains indicates the disentanglement failed
  - Mode collapse in generated skills suggests the diffusion model isn't learning the conditional distribution properly
  - High variance in policy performance across seeds indicates training instability
- First 3 experiments:
  1. Test skill generation quality by visualizing decoded actions from random embeddings in both spaces
  2. Evaluate cross-domain transfer by testing on held-out domain variations not seen during training
  3. Analyze embedding space structure by clustering and visualizing domain-invariant vs. domain-variant embeddings separately

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DuSkill scale with the size of the training datasets, and is there a point of diminishing returns?
- Basis in paper: [inferred] The paper mentions that it is "practically difficult to obtain comprehensive datasets that span all potential skills for diverse domains," implying that dataset size may impact performance.
- Why unresolved: The paper does not explicitly test or report how varying the size of the training datasets affects DuSkill's performance.
- What evidence would resolve it: Experiments comparing DuSkill's performance using different sizes of training datasets, showing the relationship between dataset size and performance.

### Open Question 2
- Question: How does the choice of the guidance weight (δ) in the diffusion-based decoder affect the quality and diversity of generated skills?
- Basis in paper: [explicit] The paper mentions that "δ > 0 serves as a guidance weight that determines the degree of adjustment towards the domain-variant decoder."
- Why unresolved: The paper does not provide a detailed analysis of how varying the guidance weight impacts the generated skills' quality and diversity.
- What evidence would resolve it: A sensitivity analysis of DuSkill's performance and skill diversity across a range of guidance weight values.

### Open Question 3
- Question: How does DuSkill's performance compare to other methods when dealing with even larger domain disparities or completely different tasks?
- Basis in paper: [inferred] The paper mentions that DuSkill can handle "challenging cross-domain situations with significant domain shifts," but does not provide empirical evidence for extremely large domain disparities.
- Why unresolved: The paper only tests DuSkill on a limited range of domain disparities and does not explore the limits of its performance.
- What evidence would resolve it: Experiments comparing DuSkill to other methods on tasks with even larger domain disparities or completely different tasks, such as different robot embodiments or simulation environments.

## Limitations
- The hierarchical domain encoding approach assumes domain-invariant and domain-variant features can be meaningfully separated, which may not hold for all task types
- Performance evaluation is limited to multi-stage Meta-World tasks, restricting generalizability to other domains
- The method's scalability to more complex domains with numerous or subtle domain variations remains unclear

## Confidence
- Medium: Experimental results show significant improvements over baselines (89.16% better than offline RL and 89.16% better than offline IL in few-shot imitation), but the evaluation is limited to a single benchmark suite with relatively controlled domain variations

## Next Checks
1. Evaluate on more diverse and complex benchmarks beyond Meta-World, including real-world robotics tasks with physical domain variations
2. Conduct ablation studies to quantify the contribution of each component (hierarchical encoding, guided diffusion, separate priors) to overall performance
3. Test robustness to varying levels of domain shift by systematically varying the magnitude and type of domain differences in the source and target tasks