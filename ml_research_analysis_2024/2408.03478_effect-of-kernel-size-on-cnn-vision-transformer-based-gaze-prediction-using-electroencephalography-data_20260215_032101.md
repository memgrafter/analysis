---
ver: rpa2
title: Effect of Kernel Size on CNN-Vision-Transformer-Based Gaze Prediction Using
  Electroencephalography Data
arxiv_id: '2408.03478'
source_url: https://arxiv.org/abs/2408.03478
tags:
- learning
- gaze
- data
- eegeyenet
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a gaze prediction method from EEG data that
  improves upon the state-of-the-art EEGViT model. The core innovation is using a
  spatial convolution kernel spanning all 129 EEG channels, which allows the model
  to better capture spatial relationships between EEG channels compared to smaller
  kernels used in prior work.
---

# Effect of Kernel Size on CNN-Vision-Transformer-Based Gaze Prediction Using Electroencephalography Data

## Quick Facts
- arXiv ID: 2408.03478
- Source URL: https://arxiv.org/abs/2408.03478
- Reference count: 40
- Primary result: 53.06mm RMSE, 33% faster training than EEGViT

## Executive Summary
This paper presents a gaze prediction method from EEG data that improves upon the state-of-the-art EEGViT model. The core innovation is using a spatial convolution kernel spanning all 129 EEG channels, which allows the model to better capture spatial relationships between EEG channels compared to smaller kernels used in prior work. The model architecture combines temporal and spatial convolution layers followed by a vision transformer. Evaluated on the EEGEyeNet dataset, the method achieves a root mean squared error (RMSE) of 53.06 millimeters in gaze position prediction, compared to EEGViT's 55.4 ± 0.2 millimeters, while reducing training time to less than 33% of the original duration.

## Method Summary
The method uses a hybrid CNN-transformer architecture that processes 129-channel EEG data. The input EEG tensor (129×500×1) passes through a temporal convolution layer (256 filters, 1×16 kernel) for temporal feature extraction, followed by a spatial convolution layer (768 depthwise filters, 129×1 kernel) to capture global channel interactions. The output is reshaped and fed into a ViT-Base transformer pretrained on ImageNet for 129×32 patches. Finally, two fully connected layers (768→1000→2) with dropout (p=0.1) produce the (x,y) gaze coordinates. The model is trained using Adam optimizer with learning rate 1e-4, reduced by 10x every 6 epochs, for 15 epochs with batch size 64 on a participant-wise split of the EEGEyeNet dataset.

## Key Results
- Achieved 53.06mm RMSE compared to EEGViT's 55.4 ± 0.2mm
- Reduced training time to less than 33% of EEGViT's duration
- Maintained or improved Mean Euclidean Distance (MED) metrics

## Why This Works (Mechanism)

### Mechanism 1
Large spatial convolution kernels spanning all EEG channels capture complex spatial relationships better than smaller kernels. A 129×1 kernel processes the entire spatial dimension at once, learning global channel interactions rather than local ones. This is critical because EEG channel layouts are unordered and non-local relationships dominate. The core assumption is that EEG channel spatial relationships are complex and not learnable via small local receptive fields.

### Mechanism 2
Temporal convolution with smaller kernel (1×16) yields finer temporal resolution than EEGViT's 1×36 kernel. Narrower temporal kernel increases the number of learnable parameters for temporal filtering, capturing higher-frequency EEG dynamics that affect gaze prediction. The core assumption is that higher temporal resolution improves EEG signal modeling for gaze prediction tasks.

### Mechanism 3
Pre-trained vision transformer fine-tuning leverages cross-domain visual feature extraction to improve EEG-based gaze prediction. ImageNet-pretrained ViT provides rich hierarchical feature representations; fine-tuning adapts these to EEG spatial-temporal patterns, improving generalization. The core assumption is that visual transformers can extract useful representations from non-visual time-series data when pre-trained on large image datasets.

## Foundational Learning

- **EEG channel spatial relationships and electrode layout topology**: Why needed - The paper's core improvement relies on capturing spatial dependencies across all 129 channels; understanding unordered vs. ordered layouts informs kernel design. Quick check - Are EEG electrodes laid out in a spatial grid that could be captured by small kernels, or are relationships inherently non-local?

- **Convolutional receptive fields and depthwise separable convolutions**: Why needed - The method uses depthwise 129×1 kernels; understanding how receptive fields affect feature learning is key to interpreting why full-channel kernels help. Quick check - How does increasing the spatial kernel width affect the number of parameters and the receptive field in depthwise convolution?

- **Vision transformer pretraining and transfer learning**: Why needed - The architecture appends a ViT trained on ImageNet; knowing how transfer learning works clarifies why it might generalize to EEG. Quick check - What types of features does a ViT trained on ImageNet extract that could be useful for non-image sequential data?

## Architecture Onboarding

- **Component map**: Input (129×500×1) → Temporal conv (1×16) → Spatial conv (129×1) → ViT (129×32 patches) → FC (768→1000→2) → Gaze prediction

- **Critical path**: Temporal conv → Spatial conv → ViT → FC stack → Gaze prediction

- **Design tradeoffs**: Full-channel spatial kernel has high parameter count but captures global spatial patterns; alternative is smaller kernels with more layers but risk missing long-range dependencies. ImageNet-pretrained ViT provides fast convergence but may introduce irrelevant features. Dropout after ViT improves robustness but reduces training speed slightly.

- **Failure signatures**: RMSE plateaus early indicates underfitting or poor temporal kernel choice. Large train/val gap indicates overfitting. Low training speed vs. EEGViT suggests misconfigured input shapes or excessive GPU memory usage.

- **First 3 experiments**:
  1. Swap 129×1 spatial kernel for 8×1 and compare RMSE/MED
  2. Increase temporal kernel from 1×16 to 1×32 and measure impact on training time and accuracy
  3. Remove ImageNet-pretrained ViT and replace with randomly initialized transformer

## Open Questions the Paper Calls Out

- **Open Question 1**: How do different convolution kernel sizes over EEG spatial features affect the accuracy of CNN-transformer hybrid models for gaze prediction? The paper only compares one specific kernel size (129×1) against EEGViT's smaller kernel (8×1), without exploring a range of intermediate kernel sizes to understand the optimal configuration.

- **Open Question 2**: What is the impact of EEG channel permutation on the model's ability to learn spatial relationships between channels? While the paper tested permutation, it didn't investigate whether specific channel orderings (like spiral or z-order) might provide better inductive biases for the model to learn spatial relationships.

- **Open Question 3**: Can EEG-based gaze prediction achieve commercial-grade accuracy (under 2-3cm RMSE) through architectural improvements or alternative deep learning methods? The paper acknowledges that current EEG-based methods achieve approximately 5.3cm RMSE, which is considerably worse than video-based solutions, and calls for further research.

## Limitations
- Results demonstrated only on EEGEyeNet dataset with fixed 129-channel configuration
- Ablation study only varies kernel sizes while keeping other hyperparameters fixed
- Vision transformer component relies on ImageNet pretraining without validating domain transfer

## Confidence
- High confidence: Empirical improvement over EEGViT baseline (RMSE reduction from 55.4mm to 53.06mm is statistically significant and reproducible)
- Medium confidence: Spatial kernel mechanism (supported by internal ablation but lacks comparison to other spatial modeling approaches)
- Low confidence: Vision transformer transfer assumption (based solely on EEGViT precedent without direct validation)

## Next Checks
1. **Dataset Generalization Test**: Evaluate the model on at least two additional EEG datasets with different channel counts (e.g., 64-channel setups) to verify that the 129-channel kernel strategy generalizes beyond EEGEyeNet.

2. **Spatial Kernel Ablation**: Systematically compare the 129×1 kernel against multiple smaller kernel configurations (e.g., 8×1, 16×1, 32×1) while keeping total parameters constant to isolate the effect of kernel width from parameter count.

3. **ViT Pretraining Validation**: Replace the ImageNet-pretrained ViT with a randomly initialized transformer and with a transformer pretrained on EEG-specific data (if available) to quantify the contribution of cross-domain transfer learning versus transformer architecture alone.