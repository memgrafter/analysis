---
ver: rpa2
title: 'SleepFM: Multi-modal Representation Learning for Sleep Across Brain Activity,
  ECG and Respiratory Signals'
arxiv_id: '2405.17766'
source_url: https://arxiv.org/abs/2405.17766
tags:
- sleep
- stage
- trained
- learning
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SleepFM, the first multi-modal foundation
  model for sleep analysis, trained on over 100,000 hours of polysomnography data
  from 14,000 participants. The authors propose a novel leave-one-out approach for
  contrastive learning that significantly improves downstream task performance compared
  to standard pairwise contrastive learning.
---

# SleepFM: Multi-modal Representation Learning for Sleep Across Brain Activity, ECG and Respiratory Signals

## Quick Facts
- arXiv ID: 2405.17766
- Source URL: https://arxiv.org/abs/2405.17766
- Reference count: 40
- Primary result: First multi-modal foundation model for sleep achieving 48% cross-modal retrieval accuracy and strong downstream task performance

## Executive Summary
SleepFM introduces the first multi-modal foundation model for sleep analysis, trained on over 100,000 hours of polysomnography data from 14,000 participants. The model learns joint representations across brain activity, ECG, and respiratory signals using a novel leave-one-out approach for contrastive learning. This approach significantly improves downstream task performance compared to standard pairwise contrastive learning, achieving strong results on demographic prediction, sleep stage classification, and sleep-disordered breathing detection. The learned embeddings enable 48% top-1 retrieval accuracy across modalities, demonstrating effective cross-modal alignment.

## Method Summary
SleepFM uses three separate EfficientNet-based 1D CNN encoders for brain activity signals (BAS), ECG, and respiratory modalities. The model is pretrained using contrastive learning objectives - both pairwise and leave-one-out approaches - to align embeddings across modalities. In leave-one-out contrastive learning, each modality's embedding is contrasted against the average embedding of all other modalities, encouraging joint semantic alignment. After pretraining on training data only, logistic regression classifiers are trained on the learned embeddings for downstream tasks including sleep staging (5 classes) and sleep-disordered breathing detection (binary classification).

## Key Results
- Sleep stage classification: macro AUROC 0.88, macro AUPRC 0.72
- Sleep-disordered breathing detection: AUROC 0.85, AUPRC 0.77
- Demographic prediction (gender, age, BMI): AUROC 0.88-0.98
- Cross-modal retrieval accuracy: 48% top-1 across all modalities

## Why This Works (Mechanism)

### Mechanism 1
Leave-one-out contrastive learning better aligns embeddings across all modalities than pairwise contrastive learning by forcing each embedding to capture shared semantic features across the full set of modalities rather than only pairwise relationships. This approach encourages each embedding to align with the averaged embedding of all other modalities, providing a more comprehensive semantic representation.

### Mechanism 2
The multi-modal embeddings capture complementary information from brain, heart, and respiratory signals that enhances downstream task performance. By jointly learning representations from BAS, ECG, and respiratory modalities, the model leverages non-redundant information from each signal type to improve prediction accuracy for sleep staging and SDB detection.

### Mechanism 3
The large-scale dataset of over 100,000 hours of sleep recordings enables the model to learn robust and generalizable representations. The extensive dataset provides diverse examples of sleep patterns and physiological signals, allowing the model to capture the full richness of sleep recordings and generalize well to unseen data.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To learn representations that bring positive pairs (temporally aligned clips from different modalities) closer together and push apart negative pairs (non-matching clips)
  - Quick check question: What is the difference between pairwise and leave-one-out contrastive learning?

- Concept: Multi-modal learning
  - Why needed here: To jointly learn representations from multiple physiological signals (BAS, ECG, respiratory) that provide complementary information about sleep
  - Quick check question: How does joint learning of representations from multiple modalities potentially improve downstream task performance compared to learning from each modality separately?

- Concept: Self-supervised learning
  - Why needed here: To learn useful representations from unlabeled data by designing pretext tasks (like contrastive learning) that do not require manual labels
  - Quick check question: What are the benefits of using self-supervised learning approaches like contrastive learning compared to traditional supervised learning for representation learning?

## Architecture Onboarding

- Component map: Raw multi-modal sleep data -> 1D CNN encoders (BAS, ECG, respiratory) -> Contrastive learning head -> Joint embeddings -> Logistic regression classifier -> Downstream task predictions

- Critical path: Raw multi-modal sleep data → 1D CNN encoders → Contrastive learning head → Joint embeddings → Logistic regression classifier → Downstream task predictions

- Design tradeoffs:
  - Using separate 1D CNN encoders for each modality allows for modality-specific feature extraction but requires more parameters and computational resources
  - Leave-one-out contrastive learning encourages joint representation learning across all modalities but may be more computationally expensive than pairwise contrastive learning
  - Using logistic regression on top of the learned embeddings simplifies the downstream task training but may limit the model's capacity to capture complex task-specific patterns

- Failure signatures:
  - Poor performance on downstream tasks may indicate issues with the quality of the learned embeddings, such as insufficient modality alignment or failure to capture relevant features
  - High variance in retrieval performance across modalities may suggest imbalanced representation learning or modality-specific challenges

- First 3 experiments:
  1. Train the 1D CNN encoders on a single modality (e.g., BAS) and evaluate the quality of the learned embeddings using a simple downstream task (e.g., age prediction)
  2. Implement the pairwise contrastive learning objective and train the model on all three modalities. Evaluate the quality of the learned embeddings using retrieval tasks and downstream classification tasks
  3. Implement the leave-one-out contrastive learning objective and train the model on all three modalities. Compare the performance to the pairwise CL model on the same evaluation tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SleepFM's performance scale with dataset size beyond the 14,000 participants studied?
- Basis in paper: The paper demonstrates strong performance on 14,000 participants but does not explore scaling to larger datasets
- Why unresolved: The paper only evaluates SleepFM on a fixed dataset size and does not investigate how performance changes with more training data
- What evidence would resolve it: Training and evaluating SleepFM on datasets with varying sizes (e.g., 50K, 100K, 200K participants) and comparing performance metrics would show how scalability affects results

### Open Question 2
- Question: Which self-supervised learning method (beyond contrastive learning) would perform best for sleep data representation learning?
- Basis in paper: The authors state "it will be interesting to try our multiple other self-supervised learning (SSL) methods, to see which method actually performs best for this task"
- Why unresolved: The paper only explores contrastive learning and does not compare against other SSL approaches like masked autoencoders or generative models
- What evidence would resolve it: Implementing and evaluating alternative SSL methods (e.g., MAE, SimSiam, or generative models) on the same sleep dataset and comparing downstream task performance would identify optimal approaches

### Open Question 3
- Question: How do different ECG lead configurations affect SleepFM's performance and generalization?
- Basis in paper: The external validation section notes that "the configuration of ECG channels differs between our site and the CinC dataset" yet the model still generalized well
- Why unresolved: The paper does not systematically investigate how different ECG lead setups (e.g., 3-lead vs 12-lead) impact model performance or cross-site generalization
- What evidence would resolve it: Training and evaluating SleepFM on datasets with different ECG lead configurations and measuring performance differences would reveal sensitivity to lead setup variations

### Open Question 4
- Question: What is the optimal balance between modality-specific and shared representations in multi-modal sleep modeling?
- Basis in paper: The paper explores multi-modal learning but does not investigate architectural choices for balancing modality-specific vs shared representations
- Why unresolved: The paper uses a simple concatenation approach without exploring architectural modifications that could optimize information sharing between modalities
- What evidence would resolve it: Comparing different architectural approaches (e.g., modality-specific encoders with shared decoder, cross-modal attention mechanisms) and measuring their impact on downstream task performance would identify optimal designs

## Limitations

- The leave-one-out contrastive learning advantage relies heavily on ablation studies within the same dataset without extensive external validation
- Performance improvements over pairwise contrastive learning show relatively modest gains (1-2% in AUROC) that may not translate to all clinical settings
- Demographic prediction uses coarse age bins (0-18, 19-49, 50+) that may not reflect real-world clinical needs for precise age estimation

## Confidence

- High confidence: Multi-modal pretraining improves over single-modality approaches (supported by clear ablation studies and external validation)
- Medium confidence: Leave-one-out contrastive learning provides meaningful improvements over pairwise approaches (based on controlled comparisons but limited to single dataset)
- Medium confidence: SleepFM generalizes to external datasets (supported by Physionet CinC 2018 results but only one external dataset tested)

## Next Checks

1. Test SleepFM's performance across multiple external sleep datasets to verify the robustness of the leave-one-out contrastive learning advantage beyond the primary dataset
2. Conduct controlled experiments varying the number of modalities in the leave-one-out framework to determine if the approach's benefits scale with increasing modality count
3. Perform ablation studies on different age binning strategies for demographic prediction to assess whether coarse binning artificially inflates performance metrics compared to continuous age regression