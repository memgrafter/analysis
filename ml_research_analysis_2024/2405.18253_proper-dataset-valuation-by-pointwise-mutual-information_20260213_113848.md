---
ver: rpa2
title: Proper Dataset Valuation by Pointwise Mutual Information
arxiv_id: '2405.18253'
source_url: https://arxiv.org/abs/2405.18253
tags:
- data
- dataset
- information
- mutual
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of evaluating data curation methods
  in machine learning, addressing the issue that standard test-score-based evaluations
  can incentivize overfitting to test sets rather than improving dataset informativeness.
  The authors propose a new framework based on Blackwell informativeness ordering,
  which formalizes that more informative data leads to better models.
---

# Proper Dataset Valuation by Pointwise Mutual Information

## Quick Facts
- arXiv ID: 2405.18253
- Source URL: https://arxiv.org/abs/2405.18253
- Reference count: 40
- Primary result: Mutual information-based evaluation framework for data curation that distinguishes strategic methods from non-strategic ones

## Executive Summary
This paper addresses the fundamental problem of evaluating data curation methods in machine learning, where traditional test-score-based evaluations can incentivize overfitting to test sets rather than improving dataset informativeness. The authors propose a new framework based on Blackwell informativeness ordering, introducing a mutual information-based scoring function that measures how much curated datasets reveal about model parameters. By developing a novel estimation method using Bayesian models trained on embedded datasets, they demonstrate that their approach can effectively distinguish strategic curation methods (which reduce informativeness) from non-strategic methods, while traditional test accuracy fails to do so.

## Method Summary
The method estimates mutual information between curated datasets and test data using a Bayesian framework. Datasets are first embedded into lower-dimensional spaces using pre-trained models (PCA for MNIST, ResNet for CIFAR). Bayesian logistic regression models are then trained on these embedded datasets to obtain posterior parameter distributions. A closed-form PMI formula computes mutual information from these posteriors, avoiding the intractable integrals that plague traditional Monte Carlo methods. The framework is evaluated by comparing PMI scores and test accuracy changes across different data curation strategies including denoising, duplication, and removal of noisy samples.

## Key Results
- PMI estimator achieves highest Spearman's ρ rank correlation with true mutual information across multiple MI estimation methods
- PMI scores decrease for strategic curation methods that reduce dataset informativeness, while test accuracy can increase in these cases
- The method is robust to prior misspecification and generalizes across different data distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The PMI scoring function correctly distinguishes strategic from non-strategic data curation methods.
- Mechanism: PMI measures mutual information between curated dataset and test data, which decreases when curation introduces no new information (strategic). Traditional test accuracy can increase in this case by making data more test-similar.
- Core assumption: Blackwell ordering of informativeness aligns with mutual information between datasets.
- Evidence anchors:
  - [abstract]: "Experiments on real-world data demonstrate that our mutual information-based evaluation assigns appropriately lower scores to data curation strategies that reduce dataset informativeness"
  - [section]: "Our results are robust to prior misspecification (i.e., different choices of regularization parameter) and generalize across data distributions"
  - [corpus]: Weak correlation - corpus neighbors focus on PMI in different contexts (topic extraction, QA), not dataset curation evaluation
- Break condition: If posterior approximation fails to capture true parameter distribution, PMI estimates become unreliable

### Mechanism 2
- Claim: The PMI estimator provides accurate mutual information estimates even in high-dimensional settings.
- Mechanism: By embedding datasets and training Bayesian models, the method reduces dimensionality while preserving information relationships. The closed-form PMI formula enables efficient computation.
- Core assumption: Embedded datasets retain sufficient information for Bayesian parameter inference.
- Evidence anchors:
  - [section]: "Our PMI estimator achieves the highest Spearman's ρ rank correlation, producing the most accurate estimates with the smallest variance"
  - [section]: "Compared to commonly used MI estimators, our concentration bound is independent of the variable dimensionality"
  - [corpus]: Missing - corpus doesn't contain evidence about MI estimation accuracy in high-dimensional settings
- Break condition: If embedding models fail to preserve task-relevant information, parameter posteriors become uninformative

### Mechanism 3
- Claim: The closed-form PMI formula enables tractable computation where Monte Carlo integration fails.
- Mechanism: Theorem 4.2 provides a formula that avoids intractable integrals by leveraging tractable posterior approximations from Bayesian models.
- Core assumption: Posterior distributions can be accurately approximated by tractable distributions (e.g., Gaussian).
- Evidence anchors:
  - [section]: "For Gaussian models... our PMI score can be represented as the sum of two terms"
  - [section]: "Our PMI dataset score can be easily computed as long as the posteriors and the prior are approximated by tractable distributions"
  - [corpus]: Missing - corpus doesn't address computational tractability of PMI estimation
- Break condition: If posterior approximations introduce significant bias, the closed-form formula yields incorrect PMI estimates

## Foundational Learning

- Concept: Blackwell ordering of informativeness
  - Why needed here: Provides theoretical foundation for why mutual information between datasets measures informativeness about model parameters
  - Quick check question: If θ → D → f(D) forms a Markov chain, what does Blackwell's theorem guarantee about expected loss?

- Concept: Bayesian parameter inference with posterior distributions
  - Why needed here: Core mechanism for computing PMI relies on obtaining posterior distributions from trained Bayesian models
  - Quick check question: Given a prior p(w) and likelihood p(x|w), how do you compute the posterior p(w|x) using Bayes' rule?

- Concept: Mutual information estimation in high-dimensional spaces
  - Why needed here: The method must handle dataset pairs containing many high-dimensional data points
  - Quick check question: Why does traditional MINE suffer from the high-discrepancy issue in high-dimensional settings?

## Architecture Onboarding

- Component map: Data curation methods (f) → Curated datasets (bD) → Pre-trained embedding models → Low-dimensional dataset representations → Bayesian models (e.g., logistic regression) → Parameter posterior distributions → PMI formula (Theorem 4.2) → Mutual information estimates → Evaluation pipeline

- Critical path: Dataset curation → Embedding → Bayesian model training → Posterior computation → PMI calculation → Method evaluation

- Design tradeoffs:
  - Embedding dimensionality vs. information preservation
  - Bayesian model complexity vs. posterior tractability
  - Dataset size vs. computational efficiency
  - Prior specification vs. robustness to misspecification

- Failure signatures:
  - Low PMI scores for all methods (likely posterior approximation failure)
  - High variance in PMI estimates across trials (likely insufficient samples or unstable embeddings)
  - PMI scores inconsistent with expected rankings (likely embedding models not preserving task-relevant information)

- First 3 experiments:
  1. Implement Bayesian logistic regression with Gaussian approximation on simple synthetic data, verify posterior computation matches analytical solution
  2. Apply embedding model to MNIST digits, train Bayesian model on embedded data, compare posterior parameters to model trained on raw data
  3. Generate correlated dataset pairs with known mutual information, implement PMI formula, compare estimated rankings to ground truth across different regularization strengths

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the optimal dataset pair selection strategy to maximize the accuracy and reliability of the PMI estimator?
  - Basis in paper: [inferred] The paper notes that PMI scoring can fail when datasets are too small to train effective models or too large resulting in significant overlap between datasets that violates the independence assumption.
  - Why unresolved: The paper identifies this as an open problem but doesn't provide concrete guidance on how to select optimal dataset pairs for MI estimation.
  - What evidence would resolve it: Systematic experiments comparing different dataset pair selection strategies (size, diversity, overlap) on MI estimation accuracy and their impact on data curation evaluation.

- **Open Question 2**: How does the choice of prior distribution affect the absolute accuracy of PMI estimates versus their ranking performance?
  - Basis in paper: [explicit] The paper observes that PMI scoring is robust to prior misspecifications in terms of ranking mutual information, but the absolute accuracy of MI estimates is highly sensitive to prior choice.
  - Why unresolved: The paper demonstrates this sensitivity exists but doesn't provide theoretical bounds or practical guidelines for prior selection.
  - What evidence would resolve it: Analysis showing how different prior choices (mean, variance, distribution family) affect PMI estimates across diverse data curation scenarios, with recommendations for prior selection.

- **Open Question 3**: Can more advanced Bayesian neural network architectures improve mutual information estimation compared to the simple logistic regression used in experiments?
  - Basis in paper: [explicit] The paper concludes by noting that experiments focused on simple logistic regression and it remains an open question whether MI estimation could be improved by more advanced Bayesian neural networks.
  - Why unresolved: The paper only tests basic Bayesian logistic regression, leaving the potential benefits of more sophisticated models unexplored.
  - What evidence would resolve it: Comparative experiments using various Bayesian neural network architectures (e.g., Bayesian neural networks with different approximation methods, ensemble methods) on the same data curation tasks to quantify improvements in MI estimation accuracy and evaluation performance.

## Limitations

- The method requires training Bayesian models for each dataset pair, which may become computationally prohibitive for large-scale applications or frequent curation evaluations.
- The embedding step introduces an additional modeling choice that could significantly impact PMI estimates, but the paper doesn't provide sensitivity analysis for different embedding architectures or dimensionalities.
- The experimental validation focuses on synthetic corruption scenarios and binary classification tasks, leaving open questions about performance on multi-class problems and real-world curation challenges.

## Confidence

**High Confidence**: The core theoretical framework connecting Blackwell informativeness ordering to mutual information estimation is well-established, supported by the theorem in Section 4.2 and experimental validation on MNIST/CIFAR datasets.

**Medium Confidence**: The PMI estimator's superiority over existing MI methods is demonstrated through rank correlation experiments, but the results depend on specific experimental setups (PCA for MNIST, ResNet for CIFAR embeddings) that may not generalize to other domains.

**Low Confidence**: The closed-form PMI formula's robustness to posterior approximation errors is assumed but not thoroughly validated. The paper shows PMI is "robust to prior misspecification" but doesn't explore the limits of this robustness or quantify approximation error propagation.

## Next Checks

1. **Sensitivity Analysis**: Systematically vary embedding dimensionality and architecture (e.g., different PCA components, alternative encoders) to quantify their impact on PMI estimates and identify stable configurations.

2. **Approximation Error Quantification**: Implement Monte Carlo integration as ground truth where feasible (lower-dimensional cases) and measure the discrepancy between closed-form PMI and direct estimation to bound approximation errors.

3. **Multi-class Extension**: Replicate the PMI evaluation framework on multi-class versions of MNIST/CIFAR (full label sets) to verify the method scales beyond binary classification and maintains computational tractability.