---
ver: rpa2
title: Decision Transformer for Enhancing Neural Local Search on the Job Shop Scheduling
  Problem
arxiv_id: '2409.02697'
source_url: https://arxiv.org/abs/2409.02697
tags:
- search
- learning
- scheduling
- problem
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Decision Transformer (DT) approach to enhance
  Neural Local Search (NLS) for the Job Shop Scheduling Problem (JSSP). The DT is
  trained on search trajectories from a pre-trained NLS agent, aiming to improve decision-making
  in local search.
---

# Decision Transformer for Enhancing Neural Local Search on the Job Shop Scheduling Problem

## Quick Facts
- arXiv ID: 2409.02697
- Source URL: https://arxiv.org/abs/2409.02697
- Reference count: 40
- Primary result: Decision Transformer outperforms Neural Local Search teachers on Job Shop Scheduling Problem, achieving state-of-the-art results for ML-enhanced search

## Executive Summary
This paper introduces a Decision Transformer approach to enhance Neural Local Search for the Job Shop Scheduling Problem (JSSP). The method trains a transformer model on search trajectories collected from a pre-trained Neural Local Search agent, enabling the DT to learn effective local search strategies that consider historical actions and states. Experimental results on the Taillard benchmark and 100 randomly generated instances demonstrate that the DT consistently achieves better makespan results than the NLS teacher models, particularly for longer search times where the DT's superior decision quality compensates for its larger inference overhead.

## Method Summary
The method involves training a Decision Transformer to enhance Neural Local Search for JSSP by leveraging search trajectories from pre-trained NLS agents. The DT maintains a context window of past state-action-reward tuples and uses a transformer architecture to predict actions based on this historical information. Training data is generated by running NLS teachers on randomly generated JSSP instances, collecting state-action-reward tuples from 100-200 search iterations. The DT is trained using categorical cross-entropy loss on these trajectories, with a return-to-go prior incorporated into the input. The trained DT is then evaluated on both benchmark instances (Taillard) and randomly generated test instances, comparing makespan performance against the NLS teacher models across various problem sizes.

## Key Results
- DT consistently outperforms NLS teacher models on makespan across Taillard benchmark and 100 randomly generated instances
- DT particularly excels on larger problem instances and with longer computational times (>7 seconds)
- DT learns different strategies than NLS teachers, not simply replicating their behavior
- Return-to-go prior has minimal statistically significant impact on performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decision Transformer improves upon Neural Local Search by leveraging a context window of past actions and states.
- Mechanism: The DT maintains a buffer of the last K state-action-reward tuples, allowing it to base decisions on historical search trajectories rather than only the current state. This enables more informed decisions that account for the influence of past actions on the current problem instance.
- Core assumption: Past actions provide useful information about the current state of the search and can guide future decisions more effectively than using only the current state representation.
- Evidence anchors:
  - [abstract]: "by considering a history of past actions, the DT can learn how previous decisions have influenced the current problem instance and adjusts its strategy accordingly."
  - [section IV.B]: "Preliminary experiments with varying hyperparameters on the 15x15 JSSP indicated that a context length K between 50 and 200 was most effective."
  - [corpus]: No direct evidence in corpus; relies on internal experiment results described in paper.
- Break condition: If the context length K is too small (less than 50 steps) or the problem structure does not benefit from historical information, the DT may not outperform the teacher models.

### Mechanism 2
- Claim: The DT achieves better performance through superior quality decisions per search step, compensating for longer inference times.
- Mechanism: By training on successful search trajectories, the DT learns to make more effective local search decisions that lead to greater improvements in makespan per step. Even though each inference takes longer due to the larger transformer architecture, the quality of decisions more than compensates, especially in longer search scenarios.
- Core assumption: The transformer architecture can capture complex relationships between past decisions and current state that lead to better future outcomes.
- Evidence anchors:
  - [abstract]: "it makes up for the longer inference times required per search step, which are caused by the larger neural network architecture, through better quality decisions per step."
  - [section V.A]: "DT and DT100 perform similarly well on average, each taken alone does not consistently outperform the NLS models."
  - [section VI]: "Since the teacher model converges to a worse average makespan, the DT achieves better makespans for all search times larger than seven seconds."
- Break condition: If the problem size is small or the search time is very limited (less than 7 seconds on their hardware), the longer inference times may not be compensated by better decisions.

### Mechanism 3
- Claim: The DT learns different strategies than the NLS teacher models, rather than simply replicating them.
- Mechanism: Through imitation learning on search trajectories, the DT develops its own decision-making patterns that differ from the teacher. Analysis of action frequencies shows that the DT does not simply copy the teacher's behavior but instead learns distinct strategies that can be more effective.
- Core assumption: The DT's architecture and training process allow it to extract different patterns from the same data than the teacher model used during its own training.
- Evidence anchors:
  - [abstract]: "Our experiments show that the DT successfully learns local search strategies that are different and, in many cases, more effective than those of the NLS agent itself."
  - [section VI]: "This falsifies our hypothesis that similar performance results alone indicate that the DT models are merely copying the teacher's behavior."
  - [section VI]: "As an interesting observation, no perturbation operation actions 8 and 9 were taken by the neither teacher nor student models that include this action... This behavior is an artifact of the NLS training, not the DT training."
- Break condition: If the teacher model's behavior is too deterministic or if the DT architecture cannot effectively learn from the trajectory data, it may fail to develop meaningfully different strategies.

## Foundational Learning

- Concept: Job Shop Scheduling Problem (JSSP)
  - Why needed here: The entire paper addresses improving solution methods for this NP-hard combinatorial optimization problem.
  - Quick check question: What are the key constraints in JSSP that make it computationally challenging?

- Concept: Transformer architectures and self-attention mechanisms
  - Why needed here: The DT uses a transformer to process sequences of state-action-reward tuples, requiring understanding of how transformers handle sequential data.
  - Quick check question: How does the multi-head attention mechanism in transformers help capture dependencies between past actions and current states?

- Concept: Reinforcement Learning vs Imitation Learning
  - Why needed here: The paper contrasts traditional RL approaches with imitation learning, where the DT learns from teacher demonstrations rather than reward signals.
  - Quick check question: What are the key differences between learning from rewards versus learning from teacher demonstrations in sequential decision-making?

## Architecture Onboarding

- Component map: 
  - NLS Teacher: Graph neural network encoder → state representation → Q-value decoder → action selection
  - DT Student: Graph neural network encoder (frozen) → state embeddings → Transformer with context window → action distribution
  - Environment: JSSP instance → initial schedule (FDD/MWKR) → local search with operators (CT, CET, ECET, CEI, Perturbation)
  - Dataset: State-action-reward tuples collected from NLS teacher trajectories

- Critical path: Dataset Generation → DT Training → DT Testing
  - First, generate labeled datasets by running NLS teachers on randomly generated instances
  - Then train DT on these datasets using the transformer architecture
  - Finally, evaluate DT performance on both benchmark and randomly generated test instances

- Design tradeoffs:
  - Context length K vs model size: Larger K (50-200) improves performance but increases model size and inference time
  - Inference time vs solution quality: DT has longer inference per step but achieves better solutions, especially with longer search times
  - Return-to-go prior: Setting an appropriate return-to-go is crucial but challenging since optimal makespan is unknown

- Failure signatures:
  - DT performs worse than teacher on small instances or with very limited search time
  - DT learns to take only one action repeatedly (overfitting to teacher behavior)
  - Performance degrades significantly when context length is reduced below 50

- First 3 experiments:
  1. Run DT with context length K=10 on 15x15 instances and compare to K=50 to verify minimum effective context length
  2. Test DT with different return-to-go factors (0.5, 1.0, 1.5) on 15x15 instances to analyze sensitivity to return-to-go prior
  3. Compare action frequency distributions between DT and NLS teacher on 15x15 instances to verify learned strategy differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal return-to-go prior for the Decision Transformer to achieve the best performance in local search for JSSP?
- Basis in paper: [explicit] The paper explicitly states that the influence of the return-to-go prior on the resulting mean makespans was analyzed and concluded that it has no statistically significant influence.
- Why unresolved: The paper found that varying the return-to-go prior did not significantly affect the mean makespans, suggesting that the DT learns a superior strategy independently of the return-to-go.
- What evidence would resolve it: Conducting experiments with different return-to-go priors across various problem sizes and comparing the performance to identify any potential optimal prior values that consistently lead to better results.

### Open Question 2
- Question: How does the Decision Transformer's performance scale with problem size and complexity in JSSP?
- Basis in paper: [inferred] The paper mentions that the benefit of using the DT varies for different problem sizes and parameterizations, but does not provide a detailed analysis of performance scaling with problem size.
- Why unresolved: The paper indicates that the performance of the DT varies across problem sizes but does not explore the scalability or performance trends with increasing problem complexity.
- What evidence would resolve it: Conducting experiments on a wider range of problem sizes and complexities, and analyzing the DT's performance trends to determine its scalability and effectiveness on larger and more complex JSSP instances.

### Open Question 3
- Question: Can the Decision Transformer effectively learn from multiple teacher models or through curriculum learning to improve its performance on JSSP?
- Basis in paper: [explicit] The paper suggests future work to push the DT towards considering the return-to-go through forced variation in the quality of solutions generated by the teacher NLS models, either by learning from different teachers at the same time or by curriculum learning.
- Why unresolved: The paper identifies the potential for improvement through learning from multiple teachers or curriculum learning but does not implement or test these approaches.
- What evidence would resolve it: Implementing and testing the Decision Transformer with multiple teacher models or curriculum learning strategies, and comparing the performance to determine if these approaches lead to significant improvements in solving JSSP.

## Limitations
- DT performance is fundamentally bounded by the quality of training data from NLS teachers
- Longer inference times per step may not be compensated by better decisions on small instances or with limited search time
- Return-to-go prior, while shown to have minimal impact, represents an important hyperparameter that could affect performance in different problem domains

## Confidence
- **High Confidence**: The DT's ability to outperform NLS teachers on larger problem instances with sufficient search time
- **Medium Confidence**: The DT learns meaningfully different strategies than the NLS teacher rather than simply replicating behavior
- **Medium Confidence**: The return-to-go prior has minimal impact on performance

## Next Checks
1. **Cross-Teacher Transferability Test**: Train DT models on trajectories from multiple independently trained NLS teachers with different random seeds, then evaluate whether DT performance improves through ensemble learning or whether different teachers encode similar strategies that don't benefit from combination.

2. **Context Length Sensitivity Analysis**: Systematically vary the context length K from 10 to 300 in increments of 20 on 15x15 instances, measuring both performance and inference time to identify the optimal tradeoff point where additional context no longer improves solution quality.

3. **Return-to-Go Prior Ablation with Synthetic Data**: Generate synthetic search trajectories with known optimal makespans, train DT models with return-to-go factors ranging from 0.1 to 2.0, and measure how closely the DT can learn the optimal policy when the ground truth return-to-go is known, compared to the original setup where return-to-go must be estimated.