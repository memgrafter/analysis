---
ver: rpa2
title: On the Global Optimality of Policy Gradient Methods in General Utility Reinforcement
  Learning
arxiv_id: '2410.04108'
source_url: https://arxiv.org/abs/2410.04108
tags:
- policy
- occupancy
- measure
- state
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies global optimality of policy gradient methods
  for Reinforcement Learning with General Utilities (RLGU), a framework that includes
  problems like imitation learning and pure exploration. The key challenge is scalability
  to large state-action spaces, as existing methods rely on tabular occupancy measure
  estimation.
---

# On the Global Optimality of Policy Gradient Methods in General Utility Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2410.04108
- **Source URL**: https://arxiv.org/abs/2410.04108
- **Reference count**: 40
- **Key outcome**: Establishes global optimality of policy gradient methods for Reinforcement Learning with General Utilities (RLGU) through gradient domination and MLE-based occupancy measure approximation

## Executive Summary
This paper addresses the global optimality of policy gradient methods for Reinforcement Learning with General Utilities (RLGU), a framework that encompasses various RL problems beyond expected return maximization. The key challenge is scalability to large state-action spaces, as existing methods rely on tabular occupancy measure estimation. The authors introduce a new proof technique based on gradient domination to establish global optimality for tabular RLGU, extending results from standard RL. For large-scale settings, they propose an actor-critic algorithm where the critic approximates the occupancy measure via maximum likelihood estimation within a function approximation class, achieving both scalability and global optimality guarantees under suitable assumptions.

## Method Summary
The method combines policy gradient ascent with occupancy measure approximation via maximum likelihood estimation (MLE). In the tabular setting, the authors prove global optimality using a gradient domination inequality that shows stationary points are globally optimal for concave utility functions. For large-scale problems, they introduce an actor-critic algorithm where the actor performs policy updates using policy gradients, while the critic approximates the occupancy measure using MLE within a parametric function class. This approach reduces sample complexity from depending on the full state-action space size to depending on the dimension of the approximation class. The algorithm alternates between collecting state samples, estimating the occupancy measure via MLE, computing pseudo-rewards, and updating policy parameters through gradient ascent.

## Key Results
- Proves gradient domination inequality for RLGU objectives, establishing that stationary points are globally optimal in tabular settings
- Introduces an actor-critic algorithm with MLE-based occupancy measure approximation that scales to large state-action spaces
- Provides sample complexity bounds that scale with the approximation dimension rather than state-action space size
- Achieves both first-order stationarity and global optimality guarantees under suitable assumptions on utility concavity and policy parametrization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient domination inequality extends from standard RL to RLGU for concave utilities
- Mechanism: By showing that the RLGU objective satisfies a gradient domination inequality similar to the expected return case, the paper proves that stationary points are globally optimal. The key is using the policy gradient theorem with pseudo-rewards ∇λF(λ(θ)) and leveraging concavity of F.
- Core assumption: The utility function F is concave in the occupancy measure, and the policy parametrization is direct (πθ(a|s) = θs,a).
- Evidence anchors:
  - [abstract]: "In the tabular setting, we provide global optimality results using a new proof technique building on recent theoretical developments on the convergence of PG methods for standard RL using gradient domination."
  - [section 3]: Theorem 1 states the gradient domination inequality for RLGU objectives.
- Break condition: If F is not concave in the occupancy measure, the gradient domination inequality may not hold, and stationary points may not be globally optimal.

### Mechanism 2
- Claim: Maximum likelihood estimation (MLE) enables scalable occupancy measure approximation
- Mechanism: Instead of using Monte Carlo estimates for each state-action pair (which scales poorly), MLE approximates the occupancy measure using a parametric distribution class. This reduces the sample complexity to depend on the dimension of the approximation class rather than the state-action space size.
- Core assumption: The occupancy measure to be estimated is realizable within the function approximation class Λ.
- Evidence anchors:
  - [abstract]: "For large-scale settings, they propose an actor-critic algorithm where the critic approximates the occupancy measure via maximum likelihood estimation within a function approximation class."
  - [section 4.2]: Describes the MLE procedure and its motivation for scalability.
- Break condition: If the occupancy measure is not realizable in the chosen function class, the approximation error cannot be reduced below a certain threshold, limiting optimality guarantees.

### Mechanism 3
- Claim: Actor-critic architecture with occupancy measure approximation achieves global optimality
- Mechanism: The actor performs policy updates using policy gradient ascent, while the critic approximates the occupancy measure via MLE. This allows the algorithm to scale to large state-action spaces while maintaining global optimality guarantees under suitable assumptions.
- Core assumption: The policy parametrization satisfies certain regularity conditions (Assumption 5) that maintain the hidden convexity structure.
- Evidence anchors:
  - [abstract]: "Under suitable assumptions, they provide both first-order stationarity and global optimality guarantees."
  - [section 4.3]: Theorem 3 states global optimality for the actor-critic algorithm with occupancy measure approximation.
- Break condition: If the policy parametrization does not satisfy the required regularity conditions, the hidden convexity structure may be lost, and global optimality guarantees may not hold.

## Foundational Learning

- Concept: Gradient domination inequality
  - Why needed here: It's the key theoretical tool that allows proving global optimality of policy gradient methods by showing that stationary points are globally optimal.
  - Quick check question: What property must the objective function satisfy for gradient domination to imply global optimality?

- Concept: Maximum likelihood estimation for distribution approximation
  - Why needed here: It provides a scalable way to approximate occupancy measures without requiring tabular estimates for each state-action pair, which is crucial for large-scale RLGU.
  - Quick check question: Why is MLE preferred over mean squared error for occupancy measure estimation in terms of scalability?

- Concept: Hidden convexity structure in RLGU
  - Why needed here: It allows leveraging optimization techniques for convex functions even though the RLGU objective is non-convex in the policy parameters, enabling global optimality guarantees.
  - Quick check question: How does the concavity of F in the occupancy measure space translate to properties of the RLGU objective in policy parameter space?

## Architecture Onboarding

- Component map:
  - Actor (policy) -> Policy gradient updates
  - Critic (occupancy measure estimator) -> MLE within function approximation class
  - Environment -> State samples and rewards
  - Function approximation class Λ -> Parametric distribution for occupancy measure

- Critical path:
  1. Collect state samples using current policy
  2. Compute MLE estimate of occupancy measure
  3. Calculate pseudo-reward ∇λF(λ̂)
  4. Generate trajectories and compute policy gradient estimate
  5. Update policy parameters
  6. Repeat

- Design tradeoffs:
  - MLE vs MSE: MLE provides better scalability but may be computationally more intensive
  - Function approximation class: Tradeoff between expressiveness and sample complexity
  - Policy parametrization: Direct parametrization is simpler but may be restrictive

- Failure signatures:
  - Slow convergence: May indicate poor occupancy measure approximation or inappropriate step sizes
  - Oscillations: Could suggest learning rate issues or insufficient exploration
  - Suboptimal policies: Might indicate that the function approximation class doesn't contain the true occupancy measure

- First 3 experiments:
  1. Verify gradient domination inequality holds for a simple tabular RLGU problem
  2. Test MLE occupancy measure estimation on a small MDP with known occupancy measure
  3. Run the full actor-critic algorithm on a medium-sized MDP and compare performance to baseline methods

## Open Questions the Paper Calls Out

- Question: Can the gradient domination inequality be extended to non-tabular policy parameterizations beyond softmax policies?
  - Basis in paper: [explicit] The authors state "We believe that our proof technique can be extended to the case of the softmax policy building on the results of Mei et al. (2020)" and note this as a future work direction.
  - Why unresolved: The current proof relies on the specific structure of tabular policies, and extending it requires different techniques for continuous parameterizations.
  - What evidence would resolve it: A formal proof showing that gradient domination holds for other policy classes (e.g., Gaussian policies, neural network policies) with explicit bounds on the distribution mismatch coefficient.

- Question: How can the occupancy measure estimation error be reduced in practice when the realizability assumption (Assumption 2) is violated?
  - Basis in paper: [inferred] The authors assume realizability of the occupancy measure in their function approximation class, but note this can be relaxed at the price of incurring an approximation error.
  - Why unresolved: The current analysis assumes the true occupancy measure lies in the approximation class, which may not hold in practice with complex environments.
  - What evidence would resolve it: Experimental results showing performance degradation when using function approximation classes that don't contain the true occupancy measure, or theoretical bounds on the approximation error when relaxing realizability.

- Question: Can the sample complexity bounds be improved for non-concave utility functions beyond the first-order stationarity guarantees?
  - Basis in paper: [explicit] The authors provide O(1/k) iteration complexity for policy gradient methods in the tabular setting but note this could be improved using techniques from Kumar et al. (2024) for actor-critic methods.
  - Why unresolved: The current analysis provides only first-order stationarity guarantees for non-concave utilities, without global convergence rates.
  - What evidence would resolve it: Improved sample complexity bounds (e.g., O(1/k^2) or linear convergence) for non-concave utilities, either through refined analysis or new algorithmic techniques.

## Limitations
- The realizability assumption for occupancy measure estimation may not hold in practice with complex environments, limiting the applicability of global optimality guarantees
- The theoretical analysis is developed for discrete state-action spaces, with only remarks about potential extension to continuous spaces without formal proofs
- The paper focuses on tabular and function approximation settings but doesn't provide empirical validation on high-dimensional continuous control benchmarks

## Confidence
- **High confidence**: The gradient domination inequality for tabular RLGU (Theorem 1) and its proof technique
- **Medium confidence**: The actor-critic algorithm's global optimality guarantees (Theorem 3) under the stated assumptions
- **Medium confidence**: The MLE-based occupancy measure estimation approach for scalability

## Next Checks
1. **Realizability validation**: Test the actor-critic algorithm on tasks where the occupancy measure is provably NOT realizable in the chosen function class to quantify the impact of this assumption on performance.
2. **Scalability assessment**: Implement the algorithm with deep neural networks as the function approximation class and evaluate performance on continuous control benchmarks to assess practical scalability.
3. **Assumption sensitivity analysis**: Systematically vary the policy overparameterization (Assumption 5) and utility function concavity parameters to understand their impact on convergence and optimality guarantees.