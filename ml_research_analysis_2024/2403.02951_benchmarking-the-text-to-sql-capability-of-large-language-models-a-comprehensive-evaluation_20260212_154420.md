---
ver: rpa2
title: 'Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive
  Evaluation'
arxiv_id: '2403.02951'
source_url: https://arxiv.org/abs/2403.02951
tags:
- arxiv
- text-to-sql
- llms
- error
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive benchmarking study of Large
  Language Models (LLMs) for Text-to-SQL tasks, evaluating their performance across
  five key sub-tasks: Text-to-SQL, SQL Debugging, SQL Optimization, SQL-to-Text, and
  Schema Linking. To mitigate overfitting, a novel dataset called "BigTable-0.2k"
  was constructed based on the BIRD dataset, containing 200 instances with varying
  numbers of tables and complexity levels.'
---

# Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation

## Quick Facts
- **arXiv ID**: 2403.02951
- **Source URL**: https://arxiv.org/abs/2403.02951
- **Reference count**: 40
- **Primary result**: Comprehensive benchmarking of LLMs across five Text-to-SQL sub-tasks reveals significant performance variations, with coding-specific models excelling in SQL generation and the "SimpleDDL-MD-Chat" prompt template achieving optimal results.

## Executive Summary
This paper presents a comprehensive benchmarking study evaluating Large Language Models on Text-to-SQL tasks across five sub-tasks: Text-to-SQL, SQL Debugging, SQL Optimization, SQL-to-Text, and Schema Linking. To mitigate overfitting concerns common in LLM evaluation, the authors constructed a novel dataset "BigTable-0.2k" containing 200 instances with varying table counts and complexity levels. Six LLMs were evaluated, including both general-purpose models (ChatGPT, LLaMa2-Chat, InternLM, InternLM2) and coding-specific models (Codellama, SQLCoder), using a unified prompt template optimization approach. The study reveals significant performance variations across models and tasks, with coding-specific models excelling in SQL generation while general-purpose models perform better in SQL-to-text conversion.

## Method Summary
The study constructs a novel "BigTable-0.2k" dataset based on BIRD with 200 instances to evaluate LLM performance across five Text-to-SQL sub-tasks. Six LLMs are systematically evaluated using in-context learning with a unified prompt template optimization approach. The researchers investigate a series of prompt template variations combining features like SimpleDDL, Markdown formatting, and chat-style descriptions. Performance is measured using task-specific metrics including Exact Matching, Execution Accuracy, Valid Efficiency Score, and Retrieval Efficiency Score. The evaluation covers zero-shot and few-shot scenarios, with additional experiments on SQL debugging with varying error information granularity and schema linking with and without foreign key information.

## Key Results
- Coding-specific models (Codellama, SQLCoder) significantly outperform general-purpose models in SQL generation tasks
- The "SimpleDDL-MD-Chat" prompt template consistently achieves optimal performance across all evaluated models and tasks
- Multi-round self-debugging improves SQL correctness, with 1-2 rounds providing the best balance between performance and efficiency
- Foreign key information significantly enhances schema linking performance across all methods and LLMs tested

## Why This Works (Mechanism)

### Mechanism 1: Granular Error Information Improves Debugging Performance
- Claim: Detailed error information and corresponding annotations greatly enhance LLMs' ability to correct their own mistakes.
- Mechanism: By providing increasingly specific error information (from none to system error info to detailed result error info with comments), LLMs can better understand what went wrong and how to fix it.
- Core assumption: LLMs can parse and utilize detailed error information to improve their outputs.
- Evidence anchors:
  - [section]: "Detailed error information and corresponding annotations greatly enhance the capabilities of LLMs, enabling them to effectively correct errors."
  - [section]: "The word cloud distribution reveals that 'no such column' and Result Error are the primary areas of error concentration for all models."
- Break condition: If error information becomes too detailed and overwhelms the model's context window, performance may degrade.

### Mechanism 2: Prompt Template Structure Influences Performance
- Claim: The "SimpleDDL-MD-Chat" prompt template achieves optimal performance in the Text-to-SQL task.
- Mechanism: This template combines simplified schema representation (SimpleDDL), Markdown formatting (MD), and a chat-style task description (Chat), which together create an optimal format for LLM comprehension.
- Core assumption: Different components of prompt templates have varying effects on LLM performance, and their combination can be optimized.
- Evidence anchors:
  - [section]: "As shown in Table 3, 'SimpleDDL-MD-Chat' consistently outperforms all other prompts when applied to all 5 backbone LLMs."
  - [section]: "We investigate a more unified series of prompt templates... These features are combined to form a complete prompt template P."
- Break condition: If LLMs evolve to better understand alternative prompt structures, this template may no longer be optimal.

### Mechanism 3: Foreign Key Information Enhances Schema Linking
- Claim: Foreign key information is capable of advancing the performance of schema linking.
- Mechanism: Including foreign key relationships in the database schema description provides additional context about table relationships, helping LLMs make more accurate schema linking decisions.
- Core assumption: Foreign keys provide meaningful structural information that LLMs can utilize for better schema understanding.
- Evidence anchors:
  - [section]: "The introduction of foreign key information yield improved performance across all methods and all LLMs."
  - [section]: "This is evident since a valid JOIN operation in SQL queries is typically based on foreign keys."
- Break condition: If LLMs develop alternative methods for understanding table relationships that don't rely on explicit foreign key information.

## Foundational Learning

- Concept: Text-to-SQL pipeline components
  - Why needed here: Understanding the five sub-tasks (Text-to-SQL, SQL Debugging, SQL Optimization, SQL-to-Text, and Schema Linking) is crucial for comprehending the benchmarking approach and results.
  - Quick check question: Can you list the five sub-tasks evaluated in this benchmarking study?

- Concept: Prompt engineering and in-context learning
  - Why needed here: The study heavily relies on optimizing prompt templates and in-context learning strategies for different LLMs and tasks.
  - Quick check question: What are the three features that differentiate the prompt templates in this study (DDL/SimpleDDL prefix, MD/HTML/Coding infix, Complete/Chat postfix)?

- Concept: Evaluation metrics for language models
  - Why needed here: Understanding metrics like Exact Matching (EM), Execution Accuracy (EX), and Valid Efficiency Score (VES) is essential for interpreting the results.
  - Quick check question: What is the key difference between Exact Matching and Execution Accuracy as evaluation metrics?

## Architecture Onboarding

- Component map: Dataset construction (BigTable-0.2k) -> Five evaluation tasks -> Six LLMs (general-purpose and coding-specific) -> Prompt template optimization -> Performance analysis and comparison

- Critical path: 1. Construct dataset 2. Define evaluation tasks 3. Select and configure LLMs 4. Optimize prompt templates 5. Run evaluations 6. Analyze results

- Design tradeoffs: Dataset size vs. overfitting risk (BigTable-0.2k has 200 instances) vs. General-purpose vs. coding-specific models for different tasks vs. Prompt complexity vs. model comprehension vs. Evaluation granularity vs. computational cost

- Failure signatures: Inconsistent performance across different datasets (as seen with open-source datasets) vs. Degradation in performance as task complexity increases vs. Overfitting to specific prompt templates or dataset formats

- First 3 experiments: 1. Replicate the Text-to-SQL performance comparison using the "SimpleDDL-MD-Chat" prompt template on a small subset of the BigTable-0.2k dataset 2. Test SQL debugging performance with varying levels of error information granularity on a set of intentionally incorrect SQL queries 3. Compare schema linking performance with and without foreign key information using the PreSQL method on a simple database schema

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on Text-to-SQL tasks scale with increasing database size and complexity beyond what was tested in this study?
- Basis in paper: [inferred] The study constructs a dataset "BigTable-0.2k" based on BIRD, but acknowledges that queries involving more than three tables may not align with common real-world applications. The paper also notes that as the number of tables and columns increases, the Text-to-SQL challenge for LLMs significantly escalates.
- Why unresolved: The study's dataset and evaluation are limited to a specific range of database sizes and complexities. The paper doesn't explore how performance changes with databases that are significantly larger or more complex than those tested.
- What evidence would resolve it: Evaluating the same LLMs on databases with many more tables (e.g., 10+ tables) and a higher number of columns per table, using a consistent prompt template and evaluation metrics.

### Open Question 2
- Question: Can incorporating foreign key information into the schema representation consistently improve schema linking performance across all LLMs and schema linking methods?
- Basis in paper: [explicit] The paper conducts experiments with and without foreign key information in schema linking tasks, finding that foreign keys improve performance across all methods and LLMs tested.
- Why unresolved: While the study shows improvement with foreign keys in the tested scenarios, it doesn't explore whether this benefit holds true for all possible LLM architectures or schema linking approaches not included in this study.
- What evidence would resolve it: Testing a wider variety of LLM architectures (including those not specifically designed for coding tasks) and schema linking methods on datasets with varying degrees of foreign key usage and complexity.

### Open Question 3
- Question: What is the optimal number of rounds for multi-round self-debugging in SQL debugging tasks, and how does this vary across different types of SQL errors?
- Basis in paper: [explicit] The study finds that 1-2 rounds of debugging provide the best balance between performance improvement and economic efficiency, but notes that the performance gain becomes marginal after initial rounds.
- Why unresolved: The paper provides a general recommendation but doesn't explore whether the optimal number of rounds varies depending on the specific type of SQL error (e.g., syntax errors vs. semantic errors).
- What evidence would resolve it: Analyzing the effectiveness of different numbers of debugging rounds for each specific error type identified in the study, potentially using a larger and more diverse set of incorrect SQL queries.

## Limitations

- Dataset size limitation: The BigTable-0.2k dataset contains only 200 instances, which may limit generalizability across diverse real-world scenarios
- Language and dialect constraints: The study focuses on English-language queries and SQL dialects, potentially limiting applicability to multilingual contexts or alternative database systems
- Limited semantic evaluation: The evaluation framework primarily measures syntactic correctness and execution accuracy but does not comprehensively assess semantic understanding or business logic comprehension

## Confidence

- **High Confidence**: The superiority of the "SimpleDDL-MD-Chat" prompt template and the general performance trend that coding-specific models excel at SQL generation while general-purpose models perform better at SQL-to-text conversion
- **Medium Confidence**: The effectiveness of multi-round self-debugging and the benefits of incorporating foreign key information in schema linking
- **Low Confidence**: The assertion that detailed error information significantly enhances LLMs' debugging capabilities

## Next Checks

1. **Dataset Expansion and Cross-Domain Validation**: Replicate the benchmark using a significantly larger dataset (minimum 1000 instances) with queries from multiple domains (finance, healthcare, e-commerce) to assess the robustness of findings across diverse contexts and to validate that the "SimpleDDL-MD-Chat" template maintains its performance advantage.

2. **Error Analysis and Semantic Evaluation**: Conduct a detailed error analysis categorizing failure modes (syntactic errors, semantic misunderstandings, logical gaps) and implement a semantic evaluation framework that goes beyond execution accuracy to assess whether generated SQL correctly captures the intended business logic.

3. **Cross-Lingual and Multi-Dialect Testing**: Evaluate the six LLMs on non-English queries and different SQL dialects (PostgreSQL, MySQL, SQL Server) to determine the extent to which the benchmark findings generalize across languages and database systems, particularly focusing on whether coding-specific models maintain their SQL generation advantage in these varied contexts.