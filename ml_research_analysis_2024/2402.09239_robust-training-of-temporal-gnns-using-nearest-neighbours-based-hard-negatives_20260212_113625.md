---
ver: rpa2
title: Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives
arxiv_id: '2402.09239'
source_url: https://arxiv.org/abs/2402.09239
tags:
- node
- negative
- training
- nodes
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a modified training procedure for temporal graph
  neural networks (TGNNs) to address the problem of uninformative negative samples
  during training. The key idea is to replace the standard uniform negative sampling
  with importance-based sampling, where negative nodes are sampled based on their
  similarity to the source node.
---

# Robust Training of Temporal GNNs using Nearest Neighbours based Hard Negatives

## Quick Facts
- **arXiv ID**: 2402.09239
- **Source URL**: https://arxiv.org/abs/2402.09239
- **Reference count**: 40
- **Primary result**: The paper proposes a modified training procedure for temporal graph neural networks (TGNNs) that achieves superior performance on link prediction tasks by using nearest neighbors based hard negative sampling.

## Executive Summary
This paper addresses the problem of uninformative negative samples during the training of temporal graph neural networks (TGNNs). The authors propose a novel negative sampling strategy that replaces uniform sampling with importance-based sampling, where negative nodes are sampled based on their similarity to the source node. This is achieved by computing the top-K nearest nodes to the source node at each time step and sampling uniformly from this set. The method is theoretically motivated by its connection to parameter convergence and is empirically validated on three real-world datasets, showing consistent performance improvements across different TGNN architectures.

## Method Summary
The paper proposes a modified training procedure for TGNNs that addresses the issue of uninformative negative samples. The key idea is to replace uniform negative sampling with importance-based sampling, where negative nodes are sampled based on their similarity to the source node. This is achieved by computing the top-K nearest nodes to the source node at each time step using dot-product similarity, and then sampling uniformly from this set as negative examples during training. The authors theoretically motivate this approach by showing its connection to parameter convergence rates and empirically validate it on three real-world datasets using two different TGNN architectures (TGAT and TGN).

## Key Results
- The proposed method achieves consistent improvements in link prediction performance across three real-world datasets (Wikipedia Edits, Reddit Posts, and Twitter Retweets).
- Empirical evaluations show improvements in metrics like MRR and Recall@1 compared to standard TGNN training methods.
- The method provides consistent gains over standard TGNN training across different TGNN architectures (TGAT and TGN) and evaluation metrics.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed negative sampling distribution improves parameter convergence by reducing gradient variance during training.
- Mechanism: The paper theoretically connects negative sampling quality to parameter convergence rates. It shows that the variance of gradients (which affects convergence) can be minimized by sampling negatives in proportion to their gradient norm. Since computing exact gradient norms is expensive, the paper approximates this by using the loss value as a proxy - nodes with higher dot-product similarity to the source node will have higher loss and thus be sampled more frequently.
- Core assumption: The loss value is a good proxy for gradient norm when the loss is close to zero, allowing for efficient computation of the negative sampling distribution.
- Evidence anchors:
  - [abstract]: "We theoretically motivate and define the dynamically computed distribution for a sampling of negative examples."
  - [section 3.1]: "If a loss is nearer to 0, then gradient norms are close to 0 [20] and vice versa. This implies that if loss computed using a negative node v− is approximately 0, then gradient norm ||g(v−)||2 calculated using v− will also be 0"
  - [corpus]: The corpus contains related work on hard negative sampling in NLP and vision, supporting the general concept, but specific evidence for the gradient variance argument in temporal GNNs is not present in the neighbor papers.
- Break condition: If the loss does not correlate well with gradient norms (e.g., in cases with highly non-linear loss landscapes), the approximation would break down.

### Mechanism 2
- Claim: Sampling uniformly from the top-K nearest nodes to the source node provides better negative examples than random sampling.
- Mechanism: The paper defines a negative sampling distribution that first computes the top-K nodes with highest dot-product similarity to the source node, then samples uniformly from these. This ensures negatives are "harder" (more similar to the source) while still maintaining diversity through uniform sampling.
- Core assumption: Hard negatives (nodes similar to the source) are more informative for training than random negatives, but the very hardest negative (the target node) should be excluded.
- Evidence anchors:
  - [abstract]: "we propose modified unsupervised learning of Tgnn, by replacing the uniform negative sampling with importance-based negative sampling"
  - [section 3.2]: "we propose to sample uniformly from these top-K nodes" and "i.e., we first compute the top-K nearest nodes with the highest dot-product/cosine/MLP score with node u from embeddings of node-set V at time t"
  - [corpus]: The corpus contains related work on hard negative mining in other domains, but the specific approach of sampling uniformly from top-K is unique to this paper.
- Break condition: If K is too small, the method may not provide enough diversity; if K is too large, the negatives may become too easy and lose their effectiveness.

### Mechanism 3
- Claim: The proposed method provides consistent improvements across different temporal graph neural network architectures and datasets.
- Mechanism: The paper integrates the proposed negative sampling method with two different TGNN architectures (TGAT and TGN) and evaluates on three real-world datasets, showing consistent performance improvements across all combinations.
- Core assumption: The effectiveness of the negative sampling strategy is independent of the specific TGNN architecture and dataset characteristics.
- Evidence anchors:
  - [abstract]: "using empirical evaluations over three real-world datasets, we show that Tgnn trained using loss based on proposed negative sampling provides consistent superior performance"
  - [section 4.4]: "Table 2 clearly shows that the proposed training procedure produces consistent gains over the standard Tgnn training methods across all datasets and metrics"
  - [corpus]: The corpus contains related work on temporal graph representation learning, supporting the general approach, but the specific consistent improvement claim is unique to this paper.
- Break condition: If a new dataset or architecture has fundamentally different characteristics (e.g., extremely sparse graphs or very different temporal dynamics), the consistent improvement might not hold.

## Foundational Learning

- Concept: Temporal Graph Neural Networks (TGNNs)
  - Why needed here: The paper is specifically about improving the training of TGNNs, so understanding how they work is fundamental to understanding the contribution.
  - Quick check question: What is the key difference between TGNNs and static GNNs that makes negative sampling particularly important?

- Concept: Negative Sampling in Representation Learning
  - Why needed here: The paper's main contribution is a new negative sampling strategy, so understanding the role of negative samples in training representation learning models is crucial.
  - Quick check question: Why are hard negatives generally more effective than random negatives in contrastive learning approaches?

- Concept: Importance Sampling Theory
  - Why needed here: The paper's theoretical motivation for the proposed method is based on importance sampling, so understanding this concept is necessary to follow the theoretical analysis.
  - Quick check question: How does importance sampling reduce the variance of gradient estimates compared to uniform sampling?

## Architecture Onboarding

- Component map: TGNN model (TGAT or TGN) -> Node embedding computation module -> Top-K nearest neighbor computation -> Loss computation module (modified to use new negative sampling) -> Training loop (modified to periodically recompute embeddings and nearest neighbors)

- Critical path: For each training interaction, compute source node embedding, find top-K nearest neighbors, sample a negative from these, compute loss, and backpropagate.

- Design tradeoffs:
  - Computational cost vs. performance: Computing top-K nearest neighbors is O(N) per update but provides significant performance gains
  - Memory vs. accuracy: Storing top-K nearest neighbors for all interactions increases memory usage but allows for faster training
  - Uniform vs. weighted sampling from top-K: The paper chooses uniform sampling for simplicity and diversity, but weighted sampling might provide additional gains

- Failure signatures:
  - Poor performance improvement: Could indicate K is too small or embeddings are not being updated frequently enough
  - Very slow training: Could indicate embeddings are being recomputed too frequently or K is too large
  - Instability during training: Could indicate the negative sampling distribution is too peaked or embeddings are changing too rapidly

- First 3 experiments:
  1. Implement the basic TGAT/TGN model with standard negative sampling as a baseline
  2. Add the top-K nearest neighbor computation and modify the negative sampling to use this
  3. Evaluate the impact of different K values on performance and training time to find the optimal tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method scale with larger temporal graphs and higher-frequency interactions?
- Basis in paper: [inferred] The paper mentions that the method increases training time complexity and discusses frequency of embedding refresh, but does not provide extensive analysis on scaling behavior with larger graphs.
- Why unresolved: The paper only evaluates on datasets with up to 672,447 interactions, and does not explore performance on much larger graphs or with higher interaction frequencies.
- What evidence would resolve it: Empirical results showing consistent performance gains on graphs with significantly more nodes and interactions, and analysis of how the method's efficiency changes with graph size.

### Open Question 2
- Question: Can the proposed negative sampling distribution be adapted for other types of graph neural networks beyond temporal GNNs?
- Basis in paper: [explicit] The paper states that the proposed method is "assumption-free and domain-agnostic" and mentions that it could be modified for different feature dimensions.
- Why unresolved: The paper only evaluates the method on temporal GNNs and does not explore its applicability to other types of GNNs or graph representation learning methods.
- What evidence would resolve it: Experiments demonstrating improved performance of other GNN architectures (e.g., static GNNs, graph autoencoders) when using the proposed negative sampling distribution.

### Open Question 3
- Question: How sensitive is the method to the choice of hyper-parameters, such as the number of nearest neighbors (top-K) and the frequency of recomputing nearest neighbors (F)?
- Basis in paper: [explicit] The paper discusses the impact of varying top-K and F on performance, but the analysis is limited to a few datasets and does not provide a comprehensive sensitivity analysis.
- Why unresolved: The paper only explores a limited range of hyper-parameter values and does not provide guidance on how to select these values for different types of temporal graphs.
- What evidence would resolve it: A more extensive sensitivity analysis across a wider range of temporal graphs, providing insights into how to choose optimal hyper-parameter values for different scenarios.

## Limitations
- The theoretical motivation relies on an approximation that may not hold in all scenarios, particularly with highly non-linear loss landscapes.
- The paper does not provide a detailed analysis of how sensitive the method is to hyperparameter choices, particularly K and F.
- The computational overhead of computing top-K nearest neighbors at each time step is not fully quantified.

## Confidence
- **High Confidence**: The empirical results showing consistent improvements across multiple datasets and TGNN architectures are well-supported by the experimental evidence presented in the paper.
- **Medium Confidence**: The theoretical motivation connecting negative sampling quality to parameter convergence rates is sound but relies on approximations that may not always hold in practice.
- **Medium Confidence**: The claim that the proposed method provides consistent improvements across different TGNN architectures and datasets is supported by the experiments but would benefit from testing on additional datasets and architectures.

## Next Checks
1. **Ablation Study on K and F**: Conduct a comprehensive ablation study to determine the optimal values of K (number of nearest neighbors) and F (frequency of recomputing top-K nodes) across different datasets and TGNN architectures. This would help quantify the sensitivity of the method to these hyperparameters.

2. **Computational Overhead Analysis**: Perform a detailed analysis of the computational overhead introduced by the proposed method, comparing the training time and memory usage with the standard TGNN training procedure. This would provide a more complete picture of the method's practical efficiency.

3. **Testing on Additional Datasets and Architectures**: Evaluate the proposed method on a wider range of temporal graph datasets and TGNN architectures to further validate the claim of consistent improvements. This would help assess the method's generalizability and identify potential failure modes.