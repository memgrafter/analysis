---
ver: rpa2
title: Gradient Projection For Continual Parameter-Efficient Tuning
arxiv_id: '2405.13383'
source_url: https://arxiv.org/abs/2405.13383
tags:
- learning
- forgetting
- gradient
- projection
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in large pre-trained
  models by introducing a unified gradient projection framework (PEGP) that can be
  applied to various parameter-efficient tuning paradigms (Adapter, LoRA, Prompt-tuning,
  Prefix-tuning). The core idea is to modify the gradient towards directions that
  have less impact on old feature spaces, effectively reducing forgetting while requiring
  minimal extra memory and training time.
---

# Gradient Projection For Continual Parameter-Efficient Tuning

## Quick Facts
- arXiv ID: 2405.13383
- Source URL: https://arxiv.org/abs/2405.13383
- Authors: Jingyang Qiao; Zhizhong Zhang; Xin Tan; Yanyun Qu; Wensheng Zhang; Zhi Han; Yuan Xie
- Reference count: 40
- One-line primary result: PEGP achieves significant improvements in continual learning for parameter-efficient tuning methods by projecting gradients orthogonally to old feature subspaces

## Executive Summary
This paper introduces a unified gradient projection framework (PEGP) to address catastrophic forgetting in large pre-trained models using parameter-efficient tuning paradigms. The core innovation is modifying gradients to be orthogonal to old feature subspaces, which effectively reduces forgetting while requiring minimal additional memory and computation. PEGP can be applied to various PET methods including Adapter, LoRA, Prompt-tuning, and Prefix-tuning, providing a theoretical foundation for anti-forgetting mechanisms across these paradigms.

## Method Summary
PEGP modifies the gradient update direction to be orthogonal to the subspace spanned by features from previous tasks. The method samples feature spaces from old tasks, performs SVD decomposition to identify the orthogonal complement subspace, and projects new gradients onto this subspace before parameter updates. This unified framework can be applied to multiple parameter-efficient tuning paradigms by reformulating their update equations to include an orthogonal projection constraint. The approach maintains stability-plasticity tradeoff without requiring task identifiers, making it suitable for various continual learning scenarios including class-incremental, domain-incremental, and cross-modality learning.

## Key Results
- Significant improvements in average accuracy across diverse benchmarks (CIFAR-100, ImageNet, DomainNet, BITM) compared to state-of-the-art continual learning methods
- Reduced forgetting while maintaining competitive new task learning performance across different PET paradigms
- Effective mitigation of hallucination in cross-modality learning and improved zero-shot generalization after continual fine-tuning
- Demonstrated applicability to both class-incremental and domain-incremental learning settings with various backbone architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Updating gradients in directions orthogonal to old feature subspaces prevents catastrophic forgetting.
- Mechanism: By projecting gradients onto the orthogonal complement of the subspace spanned by previous task features, the model modifies parameters in ways that minimally affect old task performance.
- Core assumption: The old feature space remains stable and can be captured through SVD decomposition of sampled features from previous tasks.
- Evidence anchors:
  - [abstract]: "the orthogonal condition for the gradient can effectively resist forgetting even for large-scale models"
  - [section]: "we have: xt∆E = 0. Eq.(3) implies that if the gradient is updated in the orthogonal direction to the subspace spanned by old features, the forgetting would be significantly reduced"
  - [corpus]: Weak evidence - only one related paper mentions orthogonal projection, but not specifically for gradient projection in continual learning
- Break condition: If old feature space changes significantly due to task ordering or if SVD decomposition fails to capture the true feature subspace

### Mechanism 2
- Claim: A unified framework can be derived that applies orthogonal gradient projection to multiple parameter-efficient tuning paradigms.
- Mechanism: All PET methods (Adapter, LoRA, Prompt-tuning, Prefix-tuning) can be reformulated to include an update term ∆E that, when constrained orthogonally to old features, prevents forgetting across all paradigms.
- Core assumption: Despite different architectural insertions (self-attention vs. residual connections), all PET methods share a common update equation structure that can be constrained uniformly.
- Evidence anchors:
  - [abstract]: "reformulate Adapter, LoRA, Prefix-tuning, and Prompt-tuning from the perspective of gradient projection, and firstly propose a unified framework called Parameter Efficient Gradient Projection (PEGP)"
  - [section]: "we surprisingly find that the forward of these modules seems similar and the anti-forgetting character can be described by a union equation"
  - [corpus]: Missing - no direct evidence of unified framework application across all four PET paradigms
- Break condition: If the mathematical structure of any PET method deviates significantly from the assumed common form

### Mechanism 3
- Claim: The orthogonal gradient projection method provides optimal stability-plasticity tradeoff without requiring task identifiers.
- Mechanism: By projecting gradients orthogonally to old feature subspaces, the method maximally preserves old knowledge while minimally constraining new learning, achieving better balance than methods requiring task IDs or separate models.
- Core assumption: The optimal tradeoff occurs precisely at the orthogonal complement, neither too restrictive nor too permissive.
- Evidence anchors:
  - [abstract]: "It therefore modifies the gradient towards the direction that has less impact on the old feature space, with less extra memory space and training time"
  - [section]: "Although here we only show the results with Adapter/LoRA tuning paradigms, similar phenomena are also observed in our previous work with Prompt/Prefix tuning paradigms"
  - [corpus]: Weak evidence - only mentions stability-plasticity tradeoff in one related paper but not specifically for orthogonal gradient projection
- Break condition: If the orthogonal projection is too restrictive (hurting new task learning) or too permissive (failing to prevent forgetting)

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and its application to feature subspace analysis
  - Why needed here: SVD is used to decompose feature matrices and identify the orthogonal complement subspace for gradient projection
  - Quick check question: Given a feature matrix X, what does the right singular matrix V tell us about the feature space, and how do we extract the orthogonal complement?

- Concept: Parameter-efficient tuning (PET) paradigms and their mathematical formulations
  - Why needed here: Understanding how Adapter, LoRA, Prompt-tuning, and Prefix-tuning work mathematically is essential to see why they can be unified under gradient projection
  - Quick check question: How does the update equation differ between Adapter/LoRA (residual-based) and Prompt/Prefix-tuning (attention-based), and what common structure do they share?

- Concept: Continual learning metrics and evaluation protocols
  - Why needed here: The method is evaluated using average accuracy, forgetting, and new task accuracy metrics across multiple incremental learning scenarios
  - Quick check question: How is the forgetting metric calculated, and why does it matter more than just final accuracy in continual learning?

## Architecture Onboarding

- Component map: Feature sampling module -> SVD computation -> Gradient projection layer -> PET module integration -> Task management
- Critical path: Feature sampling → SVD → Gradient projection matrix computation → Gradient projection → Parameter update → Task transition → Feature space update
- Design tradeoffs:
  - Memory vs. accuracy: More feature samples improve subspace estimation but increase memory usage
  - Projection strength vs. plasticity: Stricter orthogonality constraints preserve more old knowledge but may limit new learning
  - Computational overhead vs. forgetting reduction: More frequent SVD updates improve accuracy but increase training time
- Failure signatures:
  - Accuracy plateaus early across tasks (overly restrictive projection)
  - Rapid accuracy decline on old tasks (insufficient projection)
  - Unstable training (poor feature subspace estimation)
  - No improvement over baseline (projection matrix computation errors)
- First 3 experiments:
  1. Simple sanity check: Apply PEGP to Adapter on CIFAR-100 with 2 tasks, verify that old task accuracy is maintained
  2. Ablation study: Compare PEGP with varying projection strength (threshold ε) on single PET paradigm
  3. Cross-paradigm validation: Apply same PEGP implementation to Adapter and Prompt-tuning on same benchmark, verify consistent forgetting reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the orthogonal gradient projection method work for parameter-efficient tuning paradigms beyond those tested (Adapter, LoRA, Prompt-tuning, Prefix-tuning), such as BitFit or other emerging methods?
- Basis in paper: [explicit] The paper states PEGP is "the first unified work to study the anti-forgetting mechanism of parameter-efficient tuning" and demonstrates effectiveness across four paradigms.
- Why unresolved: The paper only tests four specific parameter-efficient tuning methods. There may be other paradigms with different mathematical formulations that could benefit from or be incompatible with gradient projection.
- What evidence would resolve it: Empirical validation of PEGP on additional parameter-efficient tuning methods showing consistent improvements in forgetting reduction and accuracy maintenance.

### Open Question 2
- Question: How does the performance of PEGP scale with the number of tasks in continual learning scenarios, particularly in extremely long task sequences?
- Basis in paper: [inferred] The paper shows PEGP outperforms baselines in various continual learning settings but doesn't extensively explore performance degradation over very long task sequences.
- Why unresolved: The theoretical framework assumes orthogonality to old feature subspaces, but the practical accumulation of projection matrices over many tasks could introduce compounding errors or computational challenges.
- What evidence would resolve it: Systematic experiments testing PEGP on datasets with increasing numbers of tasks (e.g., 20, 50, 100+ tasks) to measure performance degradation and computational overhead.

### Open Question 3
- Question: What is the theoretical relationship between the hyperparameter threshold β (for cosine similarity between column vectors) and the optimal balance between stability and plasticity in different continual learning scenarios?
- Basis in paper: [explicit] The paper introduces β as a hyperparameter for choosing between summing or selecting column vectors from projection matrices, noting this affects the trade-off between plasticity and stability.
- Why unresolved: While the paper demonstrates the existence of this trade-off, it doesn't provide a principled method for setting β based on task characteristics or derive theoretical bounds for optimal values.
- What evidence would resolve it: Theoretical analysis connecting β to measures of task similarity, data distribution characteristics, or empirical studies mapping β values to performance across diverse task sequences.

## Limitations
- Limited cross-paradigm validation showing consistent performance across all four PET methods on identical benchmarks
- SVD decomposition may not capture full complexity of learned representations in large models, especially cross-modal architectures
- Newly proposed BITM dataset introduces uncertainty regarding dataset quality and benchmark relevance

## Confidence
- **High Confidence**: The orthogonal gradient projection mechanism for preventing forgetting in single PET paradigms
- **Medium Confidence**: The unified framework claims across multiple PET paradigms due to limited cross-paradigm validation
- **Medium Confidence**: The stability-plasticity tradeoff claims without task identifiers, though supported by experimental results
- **Low Confidence**: Cross-modality hallucination mitigation claims due to limited evidence and complexity of cross-modal learning

## Next Checks
1. **Cross-paradigm replication**: Apply PEGP implementation to all four PET paradigms (Adapter, LoRA, Prompt-tuning, Prefix-tuning) on identical benchmarks (e.g., CIFAR-100) and compare forgetting metrics across methods
2. **Ablation on projection strength**: Systematically vary the orthogonality threshold ε and measure the tradeoff between forgetting reduction and new task learning capacity across multiple tasks
3. **Feature space stability analysis**: Track how the estimated orthogonal subspaces evolve across task sequences and correlate subspace changes with performance degradation in non-PEGP baselines