---
ver: rpa2
title: Improving Continual Learning Performance and Efficiency with Auxiliary Classifiers
arxiv_id: '2403.07404'
source_url: https://arxiv.org/abs/2403.07404
tags:
- learning
- accuracy
- inference
- task
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines intermediate representations in neural networks
  during continual learning, finding that early-layer representations are more stable
  and less prone to forgetting than later ones. Motivated by this, auxiliary classifiers
  (ACs) are introduced at intermediate layers, enabling dynamic inference and reducing
  computational cost.
---

# Improving Continual Learning Performance and Efficiency with Auxiliary Classifiers

## Quick Facts
- arXiv ID: 2403.07404
- Source URL: https://arxiv.org/abs/2403.07404
- Reference count: 40
- Primary result: Auxiliary classifiers at intermediate layers improve continual learning accuracy by 10% while reducing computation by 10-60%

## Executive Summary
This work examines intermediate representations in neural networks during continual learning, finding that early-layer representations are more stable and less prone to forgetting than later ones. Motivated by this, auxiliary classifiers (ACs) are introduced at intermediate layers, enabling dynamic inference and reducing computational cost. Experiments across diverse benchmarks (CIFAR100, ImageNet100) and architectures (ResNet, VGG, ViT) show that ACs consistently improve performance, yielding an average 10% relative accuracy gain over single-classifier methods. Dynamic inference with ACs reduces average computation by 10-60% without sacrificing accuracy, allowing predictions before full network evaluation.

## Method Summary
The method attaches auxiliary classifiers to intermediate layers of neural networks, creating a multi-classifier architecture where each AC specializes in different task subsets. During training, all classifiers are jointly optimized with weighted losses that account for computational cost. For inference, predictions are selected either by maximum confidence (static) or by exiting early when any classifier exceeds a confidence threshold (dynamic). The approach leverages the stability of early-layer representations to maintain performance on older tasks while the final classifier adapts to new tasks, with dynamic inference exploiting overthinking patterns to save computation.

## Key Results
- Auxiliary classifiers achieve 10% average accuracy improvement over single-classifier methods across 5/10-task CIFAR100 and ImageNet100 benchmarks
- Dynamic inference with ACs reduces computational cost by 10-60% without accuracy degradation
- ACs on early layers outperform final classifier on older tasks due to better retention of task-specific knowledge
- The method works across diverse architectures including ResNet, VGG, and ViT with consistent performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early-layer representations in neural networks are more stable and less prone to forgetting than later-layer representations during continual learning.
- Mechanism: Early layers learn general, low-level features (edges, textures) that remain useful across tasks, while later layers learn task-specific, high-level abstractions that change more when new tasks are learned.
- Core assumption: The representational stability gradient across layers (early stable, late dynamic) exists consistently across architectures and continual learning scenarios.
- Evidence anchors:
  - [abstract] "we examine the intermediate representations in neural network layers during continual learning and find that such representations are less prone to forgetting"
  - [section] "CKA measures the similarity of two sets of representations... We measure the similarity between the representations of the first task data learned after the initial task and the representations learned after each subsequent task. Across all methods, early layer representations change less and exhibit more stability"
  - [corpus] Weak evidence - corpus papers focus on catastrophic forgetting mechanisms but don't directly address layer-wise stability patterns
- Break condition: If task distribution shifts are so extreme that even early features become task-specific, or if architectural changes eliminate the gradient of stability

### Mechanism 2
- Claim: Auxiliary classifiers (ACs) attached to early layers outperform the final classifier on older tasks because they retain task-specific knowledge that the final classifier forgets.
- Mechanism: Each AC specializes in classifying a subset of data based on the features available at its layer. Since early layers retain more stable representations, ACs on early layers maintain better performance on previously learned tasks while the final classifier drifts toward new task optimization.
- Core assumption: Different ACs learn to classify different subsets of data, creating redundancy that compensates for forgetting in the final classifier
- Evidence anchors:
  - [abstract] "auxiliary classifiers (ACs) are introduced at intermediate layers, enabling dynamic inference and reducing computational cost"
  - [section] "we explore the discriminative power of these representations and reveal that, surprisingly, auxiliary classifiers (ACs) trained on top of the early layers can significantly outperform the final classifier on older tasks"
  - [corpus] Weak evidence - corpus papers focus on classifier rearrangement and task activation but don't directly address intermediate classifier performance in continual learning
- Break condition: If ACs interfere with each other during training, or if the final classifier is explicitly regularized to preserve old task performance

### Mechanism 3
- Claim: Dynamic inference with ACs reduces average computation by 10-60% without sacrificing accuracy through early exits based on confidence thresholds.
- Mechanism: During inference, the network evaluates ACs sequentially and exits early when any classifier's confidence exceeds a threshold λ. Since early ACs can correctly classify many samples that the final classifier would misclassify (overthinking), this saves computation while maintaining or improving accuracy.
- Core assumption: Overthinking is more pronounced in continual learning than in i.i.d. settings, making early exits both possible and beneficial
- Evidence anchors:
  - [abstract] "We also leverage the ACs to reduce the average cost of the inference by 10-60% without compromising accuracy, enabling the model to return the predictions before computing all the layers"
  - [section] "Overthinking refers to a case where samples correctly classified by early classifiers are misclassified later by the final classifier... Networks trained using different continual learning methods exhibit a higher degree of overthinking compared to those trained under standard joint training"
  - [corpus] Weak evidence - corpus papers mention dynamic inference but don't specifically analyze overthinking patterns in continual learning
- Break condition: If confidence thresholds are set too high (never exit early) or too low (exit prematurely on uncertain predictions), or if task distributions require full network evaluation

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why intermediate layers forget less than final layers is fundamental to the entire approach
  - Quick check question: What is the primary difference between how early and late layers change during continual learning, and why does this matter for classifier performance?

- Concept: Representation similarity metrics (CKA)
  - Why needed here: The paper uses Centered Kernel Alignment to quantify how much layer representations change across tasks, which is the core empirical finding
  - Quick check question: How does CKA differ from other similarity metrics like linear regression or SVCCA, and why is it appropriate for comparing neural network representations?

- Concept: Overthinking in multi-classifier networks
  - Why needed here: The dynamic inference mechanism relies on the observation that early classifiers often correctly classify samples that the final classifier misclassifies
  - Quick check question: In what way does overthinking manifest differently in continual learning versus standard i.i.d. training, and how does this enable computational savings?

## Architecture Onboarding

- Component map:
  Backbone network (f1...fN) with N layers → Final classifier (g) attached to last layer → Auxiliary classifiers (ĝ1...ĝN-1) attached to intermediate layers → Confidence threshold λ for dynamic inference → Loss scheduler that weights classifier losses by computational cost

- Critical path:
  1. Forward pass through backbone layers
  2. Compute predictions from all classifiers
  3. Select prediction with maximum confidence (static) or first classifier exceeding λ (dynamic)
  4. Return selected prediction

- Design tradeoffs:
  - More ACs → better performance but higher training overhead and parameter count
  - Earlier AC placement → more stability but less discriminative power
  - Higher λ threshold → more computation savings but risk of degraded accuracy
  - Joint training vs. linear probing → better end-to-end performance vs. simpler implementation

- Failure signatures:
  - Performance plateaus at full computation cost → ACs not learning diverse representations
  - Dynamic inference accuracy drops significantly below static → threshold too aggressive or ACs not reliable
  - Training instability or collapse → loss weighting/scheduling not properly balanced
  - Memory overflow → too many ACs for available GPU memory

- First 3 experiments:
  1. Verify layer stability gradient: Compute CKA between task 1 and task t representations for each layer across different CL methods
  2. Test AC performance: Compare per-task accuracy of ACs vs. final classifier on held-out old task data
  3. Validate dynamic inference: Measure accuracy vs. computation tradeoff curve for different λ thresholds on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do auxiliary classifiers impact the learning dynamics and convergence speed in continual learning scenarios?
- Basis in paper: [inferred] The paper discusses training time and memory overhead increases when adding auxiliary classifiers, with 50% longer training time and 10% memory increase observed in ResNet-32 experiments. The authors note that these are worst-case scenarios and could potentially be optimized.
- Why unresolved: While the paper quantifies the overhead, it doesn't explore whether ACs could actually accelerate convergence by enabling earlier task completion or reducing catastrophic forgetting during training. The potential trade-off between training efficiency and performance gains remains unclear.
- What evidence would resolve it: Controlled experiments comparing convergence curves (loss, accuracy over training epochs) between single-classifier and multi-classifier networks across different continual learning methods and datasets would clarify this.

### Open Question 2
- Question: Can auxiliary classifiers be effectively integrated with generative replay methods in continual learning?
- Basis in paper: [inferred] The paper focuses on classification tasks and doesn't explore how ACs might work with generative models that synthesize previous data. All experiments use exemplar-based replay or regularization methods, but not generative replay approaches.
- Why unresolved: Generative replay could potentially benefit from ACs' diverse predictions across intermediate layers, but the interaction between generated samples and multi-classifier architectures remains unexplored. The compatibility and potential synergies are unknown.
- What evidence would resolve it: Experiments combining ACs with generative replay methods like Deep Generative Replay (Shin et al., 2017) or Memory Replay GANs (Wu et al., 2018) would demonstrate whether ACs enhance or interfere with generative rehearsal strategies.

### Open Question 3
- Question: How does the placement and architecture of auxiliary classifiers affect their performance across different neural network architectures?
- Basis in paper: [explicit] The paper explores AC placement in ResNet, VGG, and ViT architectures, finding that denser AC placements generally lead to better performance. However, some outliers like DER++ perform better with fewer ACs, and the optimal placement varies across methods.
- Why unresolved: While the paper demonstrates that ACs work across architectures, it doesn't provide a systematic framework for determining optimal AC placement and architecture design. The relationship between network depth, width, and AC effectiveness needs further investigation.
- What evidence would resolve it: Systematic ablation studies varying AC placement, number, and architecture (including cascading and ensemble designs) across diverse network architectures and continual learning scenarios would establish guidelines for optimal AC design.

### Open Question 4
- Question: What is the relationship between auxiliary classifier diversity and performance improvement in continual learning?
- Basis in paper: [explicit] The paper measures "unique overthinking" and finds that subsets of samples can be correctly classified only by specific ACs, indicating classifier diversity. It also notes that different ACs learn to classify different subsets of data, suggesting diversity contributes to performance gains.
- Why unresolved: The paper demonstrates that diversity exists and correlates with performance improvements, but doesn't establish a causal relationship or determine whether maximizing diversity is always beneficial. The trade-off between diversity and accuracy at individual ACs remains unexplored.
- What evidence would resolve it: Experiments explicitly controlling for classifier diversity (through regularization, architectural constraints, or training procedures) while measuring performance impacts would clarify whether diversity maximization is the optimal strategy.

## Limitations
- Layer stability gradient may be architecture-dependent and less pronounced in transformers compared to CNNs
- 10-60% computational savings requires careful threshold tuning and may not generalize across all task distributions
- Stability advantage of early layers could diminish with very long task sequences or highly dissimilar tasks

## Confidence

- **High confidence**: Layer stability gradient observation (CKA similarity measures are well-established and the empirical pattern is clearly demonstrated)
- **Medium confidence**: ACs outperforming final classifier on old tasks (the mechanism is sound but depends on proper weight scheduling and AC placement)
- **Medium confidence**: Dynamic inference computational savings (the overthinking phenomenon is observed but the 10-60% range likely requires task-specific threshold tuning)

## Next Checks

1. **Cross-architecture stability test**: Apply the CKA analysis to ViT and transformer-based architectures to verify if the early-layer stability advantage holds beyond CNNs
2. **Long-horizon forgetting analysis**: Evaluate AC performance and layer stability after 20+ tasks to test the robustness of early-layer stability over extended continual learning scenarios
3. **Task similarity threshold determination**: Systematically measure how task similarity affects the overthinking phenomenon and optimal confidence threshold selection for dynamic inference