---
ver: rpa2
title: 'From Words to Worlds: Compositionality for Cognitive Architectures'
arxiv_id: '2407.13419'
source_url: https://arxiv.org/abs/2407.13419
tags:
- compositional
- compositionality
- arxiv
- cognitive
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates whether large language models (LLMs) exhibit\
  \ compositionality\u2014a key aspect of human cognition\u2014and whether their compositional\
  \ abilities explain their strong performance. The authors introduce three novel\
  \ task types to evaluate different aspects of compositionality: substitutivity,\
  \ systematicity, and over-generalization."
---

# From Words to Worlds: Compositionality for Cognitive Architectures

## Quick Facts
- arXiv ID: 2407.13419
- Source URL: https://arxiv.org/abs/2407.13419
- Reference count: 39
- Models scale consistently improves compositional abilities

## Executive Summary
This work investigates whether large language models exhibit compositionality—a key aspect of human cognition—by introducing three novel task types to evaluate substitutivity, systematicity, and over-generalization. The authors analyze 12 models across four families (Falcon, Llama 2, Codellama, Mistral) and find that while scaling improves compositional performance, instruction tuning often has a negative or inconsistent effect on compositionality.

## Method Summary
The study evaluates compositionality in large language models using three novel task types: ANTAILS (adjective-noun entailment), PLANE (adjective-noun hypernym inference), and COMPCOMB (compound vs. adjective-noun similarity). The authors conduct zero-shot evaluations across 12 models from four families, comparing base and instruction-tuned versions, and analyze performance through accuracy metrics, log probabilities, and cosine distance measures.

## Key Results
- Scaling model size consistently improves compositional abilities across all three task types
- Instruction tuning often has a negative or inconsistent effect on compositionality
- Compositional strategies learned by LLMs are not fully aligned with human cognitive capacities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling model size improves compositional abilities by increasing parameter capacity and enabling better representation of complex relationships.
- Mechanism: Larger models have more parameters to learn nuanced patterns in language, allowing them to better understand how components combine to form meaning.
- Core assumption: Increased parameter count directly translates to improved compositional reasoning.
- Evidence anchors:
  - [abstract] "while scaling enhances compositional abilities"
  - [section] "the Larger Model always performs better than the Base Model"
  - [corpus] "Average neighbor FMR=0.469" (weak corpus evidence for this specific mechanism)
- Break condition: If scaling reaches a point of diminishing returns or overfitting occurs.

### Mechanism 2
- Claim: Instruction tuning does not consistently improve compositional performance because it may prioritize alignment over compositional reasoning.
- Mechanism: Instruction tuning optimizes models for task completion rather than preserving compositional strategies learned during pretraining.
- Core assumption: Instruction tuning objectives conflict with maintaining compositional abilities.
- Evidence anchors:
  - [abstract] "instruction tuning often has a reverse effect"
  - [section] "instruction-tuned models do not always show improved performance"
  - [corpus] "weak corpus evidence for this specific mechanism"
- Break condition: If instruction tuning objectives are modified to explicitly preserve compositional abilities.

### Mechanism 3
- Claim: Different aspects of compositionality (substitutivity, systematicity, over-generalization) require distinct mechanisms and are affected differently by model scaling and instruction tuning.
- Mechanism: Each compositional task type tests a different cognitive ability, and models may excel at some while struggling with others.
- Core assumption: Compositionality is multifaceted and cannot be reduced to a single ability.
- Evidence anchors:
  - [abstract] "three task categories"
  - [section] "We take inspiration from Hupkes et al. (2020)'s tripartite distinction between aspects of compositionality"
  - [corpus] "Average neighbor FMR=0.469" (weak corpus evidence for this specific mechanism)
- Break condition: If a single unified measure of compositionality can be developed that correlates with all three task types.

## Foundational Learning

- Concept: Compositional generalization
  - Why needed here: Understanding how models combine learned components to handle novel inputs is central to evaluating their compositional abilities.
  - Quick check question: Can you explain the difference between memorization and compositional generalization in the context of language models?

- Concept: Systematicity in cognition
  - Why needed here: The paper evaluates whether LLMs exhibit systematic behavior, a key aspect of human cognition.
  - Quick check question: How does the systematicity of human thought relate to the ability to recombine learned components in novel ways?

- Concept: Connectionist vs symbolic representations
  - Why needed here: The paper is situated in the debate about whether neural networks can serve as viable cognitive architectures.
  - Quick check question: What are the key differences between connectionist and symbolic approaches to representing knowledge, and how do they relate to compositionality?

## Architecture Onboarding

- Component map:
  Data preprocessing pipeline for three datasets (ANTAILS, PLANE, COMPCOMB) -> Model loading and evaluation framework supporting multiple model families -> Prompt generation and management system -> Evaluation metrics calculation (accuracy, log probabilities, cosine distances) -> Result aggregation and visualization components

- Critical path:
  1. Load model and dataset
  2. Generate prompts based on task type
  3. Run inference and collect outputs
  4. Calculate evaluation metrics
  5. Aggregate results across models and tasks
  6. Generate visualizations and analysis

- Design tradeoffs:
  - Zero-shot evaluation vs. few-shot/fine-tuning: Zero-shot provides unbiased comparison but may underestimate true capabilities
  - Prompt uniformity vs. task-specific optimization: Uniform prompts ensure fair comparison but may not elicit optimal performance
  - Dataset specificity vs. generality: Focused datasets allow detailed analysis but may not capture full compositional abilities

- Failure signatures:
  - Inconsistent performance across different prompt variations
  - Large variance in results across multiple runs
  - Performance degradation on instruction-tuned models compared to base models
  - Unexpected results on specific adjective classes in PLANE dataset

- First 3 experiments:
  1. Evaluate a single model on the ANTAILS dataset using both setups and prompt variations
  2. Compare performance of base vs. instruction-tuned models within the same family on the PLANE dataset
  3. Analyze embedding layer vs. last hidden state representations for the COMPCOMB dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does instruction tuning sometimes decrease compositional performance when it improves other types of performance?
- Basis in paper: [explicit] The paper states "we see that compositional performance does not always improve with instruction tuning" and notes this is an open issue
- Why unresolved: The paper observes this phenomenon but doesn't investigate the underlying mechanisms that cause instruction tuning to degrade compositional abilities while improving other performance metrics
- What evidence would resolve it: Controlled experiments varying instruction tuning methods, analyzing changes in internal representations before and after instruction tuning, and testing whether specific types of instructions are more harmful to compositionality

### Open Question 2
- Question: Does the negative effect of instruction tuning on compositionality generalize across different model architectures beyond the four families tested?
- Basis in paper: [inferred] The paper tests 4 model families but notes this as a limitation and suggests broader investigation is needed
- Why unresolved: The study is limited to Falcon, Llama 2, Codellama, and Mistral families, leaving open whether the observed pattern holds for other architectures like GPT, Claude, or Gemini
- What evidence would resolve it: Testing instruction tuning effects on additional model families and architectures, particularly those with different pretraining objectives and data distributions

### Open Question 3
- Question: How do compositional abilities develop during pretraining versus instruction tuning, and at what stage does the degradation occur?
- Basis in paper: [inferred] The paper notes scaling improves compositionality but instruction tuning often has the reverse effect, suggesting a developmental question
- Why unresolved: The study only examines final model performance without analyzing the training process or intermediate checkpoints to understand when compositional abilities are gained or lost
- What evidence would resolve it: Analyzing model checkpoints throughout training, comparing compositional abilities at different pretraining stages, and examining the impact of instruction tuning at various points in the training process

## Limitations
- Limited to four model families, leaving open whether findings generalize to other architectures
- Use of zero-shot prompting without detailed prompt specifications may introduce variability
- Weak corpus evidence (average neighbor FMR=0.469) suggests the field may still be developing consensus on evaluation methodologies

## Confidence
- High Confidence: Scaling model size consistently improves compositional abilities; three task categories effectively capture different aspects of compositionality
- Medium Confidence: Instruction tuning often has a negative or inconsistent effect on compositionality; compositional strategies are not fully aligned with human cognitive capacities
- Low Confidence: Specific claims about relative performance of individual model families beyond general scaling trends

## Next Checks
1. Systematically vary prompt templates across all models and measure performance variance to determine whether observed differences in compositional abilities are robust to prompting strategies or artifacts of prompt formulation.
2. Evaluate whether instruction tuning with objectives that explicitly preserve compositional reasoning (e.g., compositional generalization tasks during fine-tuning) mitigates the negative effects observed in the current study.
3. Collect human performance data on the same three task types to establish whether the gap between LLM and human compositional reasoning is consistent across all aspects of compositionality, and to validate the cognitive plausibility of the task designs.