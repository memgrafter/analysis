---
ver: rpa2
title: Russian-Language Multimodal Dataset for Automatic Summarization of Scientific
  Papers
arxiv_id: '2405.07886'
source_url: https://arxiv.org/abs/2405.07886
tags:
- text
- dataset
- summarization
- papers
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal Russian-language dataset for
  automatic summarization of scientific papers. The dataset comprises 420 papers across
  7 scientific domains, including text, tables, and figures with descriptions.
---

# Russian-Language Multimodal Dataset for Automatic Summarization of Scientific Papers

## Quick Facts
- arXiv ID: 2405.07886
- Source URL: https://arxiv.org/abs/2405.07886
- Reference count: 36
- Introduces multimodal Russian-language dataset for automatic summarization of scientific papers

## Executive Summary
This paper introduces a multimodal Russian-language dataset for automatic summarization of scientific papers, comprising 420 papers across 7 scientific domains. The dataset includes text, tables, and figures with descriptions, enabling more comprehensive summarization than text-only approaches. Two large language models, Gigachat and YandexGPT, were evaluated on this dataset, revealing that while final sections of papers correlate better with abstracts in terms of syntax, the language models perform better in semantics according to neural network metrics. The study highlights limitations due to censorship and text length constraints, with YandexGPT generating longer summaries than Gigachat.

## Method Summary
The dataset was created from Russian scientific papers across 7 domains (Economics, History, IT, Journalism, Law, Linguistics, Medicine), including abstracts, text, tables, and figures with descriptions. Two Russian LLMs (Gigachat and YandexGPT) were evaluated using a fixed prompt to generate abstracts. Performance was measured against reference abstracts using BERTScore, BLEURT, ROUGE-1/2/L, and BLEU metrics. A baseline comparison between paper conclusions and abstracts was also included.

## Key Results
- The multimodal dataset successfully captures scientific papers across 7 diverse domains with text, tables, and figures
- YandexGPT generated longer summaries (average 382 tokens) compared to Gigachat (292 tokens)
- Language models showed better semantic performance (BERTScore, BLEURT) compared to syntactic correlation of final sections with abstracts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset's multimodal structure (text, tables, figures) enables more accurate summarization by capturing information beyond text alone.
- Mechanism: Visual elements in scientific papers often contain key findings and data that are not fully captured in text descriptions. Including these elements allows language models to reference and incorporate this additional information when generating summaries.
- Core assumption: Tables and figures contain information that is not redundant with the textual content and contributes unique value to the summary.
- Evidence anchors:
  - [abstract] "A feature of the dataset is its multimodal data, which includes texts, tables and figures."
  - [section] "Containing valuable information, tables and figures can noticeably improve the quality of abstracts."
  - [corpus] Corpus neighbors show related work on multimodal scientific document processing, supporting the relevance of this approach.
- Break condition: If the visual elements are merely decorative or their descriptions are fully redundant with text, the multimodal approach would not provide additional benefit.

### Mechanism 2
- Claim: The dataset covers diverse scientific domains, making it more robust for evaluating language models across different research areas.
- Mechanism: Different scientific domains have distinct writing styles, terminologies, and structures. A diverse dataset allows language models to be tested on their ability to handle these variations rather than overfitting to a single domain.
- Core assumption: The linguistic and structural characteristics of scientific papers vary significantly across domains.
- Evidence anchors:
  - [section] "Such diversity is important as each domain has its own scientific traditions, patterns and different set of metadata such as formulas, tables, figures etc."
  - [section] "The following scientific domains were included in the dataset: Economics, History, Information Technologies (IT), Journalism, Law, Linguistics and Medicine."
  - [corpus] Corpus neighbors include domain-specific summarization work, validating the importance of domain diversity.
- Break condition: If the linguistic differences between domains are minimal, or if language models perform equally well regardless of domain, the diversity benefit would be limited.

### Mechanism 3
- Claim: Using neural network metrics (BERTScore, BLEURT) provides a more semantically accurate evaluation than traditional n-gram based metrics.
- Mechanism: Neural network metrics capture semantic similarity by considering word embeddings and contextual meaning, making them more suitable for evaluating abstractive summarization where exact word matching is less important.
- Core assumption: Semantic similarity is a more important criterion for summary quality than exact word overlap.
- Evidence anchors:
  - [section] "The ROUGE and BLEU metrics are considered to be standard for text generation and are based on calculating overlapping n-grams... BERTScore is based on pre-trained contextual embeddings BERT and calculates the similarity of sentences as the sum of cosine similarities between the embeddings of the tokens they consist of."
  - [section] "BERTScore, is based on pre-trained contextual embeddings BERT and calculates the similarity of sentences as the sum of cosine similarities between the embeddings of the tokens they consist of [26]. This metric is especially effective for abstractive summarization since it takes into account such points as paraphrasing and changing the order of words."
  - [corpus] Corpus neighbors show recent work using neural metrics for evaluation, indicating current best practices.
- Break condition: If the reference summaries and generated summaries have very similar structure and wording, traditional n-gram metrics might be equally effective.

## Foundational Learning

- Concept: Multimodal data processing
  - Why needed here: Understanding how to process and integrate text, tables, and figures is crucial for working with this dataset and developing multimodal summarization systems.
  - Quick check question: What are the key challenges in aligning information across text, tables, and figures in scientific documents?

- Concept: Automatic text summarization evaluation metrics
  - Why needed here: Different metrics (BERTScore, BLEURT, ROUGE, BLEU) capture different aspects of summary quality, and understanding their strengths and limitations is essential for proper evaluation.
  - Quick check question: When would you prefer BERTScore over ROUGE for evaluating summaries, and why?

- Concept: Large language model limitations and censorship
  - Why needed here: The paper shows that models like Gigachat and YandexGPT have restrictions on certain content types, which affects their performance on scientific texts.
  - Quick check question: How might ethical restrictions in language models impact their ability to summarize scientific papers in sensitive domains?

## Architecture Onboarding

- Component map: Dataset → Preprocessing → Model API → Summary Generation → Evaluation → Analysis
- Critical path: The most critical path is from dataset preprocessing through to summary generation, as errors in handling multimodal data will propagate through the entire pipeline.
- Design tradeoffs: Balancing between preserving the original structure of scientific papers and creating a format suitable for machine learning models; choosing between extractive and abstractive approaches based on the nature of the source material.
- Failure signatures: Censorship errors (rejected content), truncation issues (text too long), poor alignment between text and visual elements, domain-specific terminology handling failures.
- First 3 experiments:
  1. Test baseline correlation between final sections and abstracts using the provided 183 pairs to validate the approach.
  2. Run Gigachat on a small subset (e.g., 10 papers from IT domain) to understand censorship patterns and text length limitations.
  3. Compare YandexGPT performance on texts of varying lengths to establish the impact of the 8000 token limit on different scientific domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are the proposed multimodal dataset and language models in improving the quality of abstracts for scientific papers compared to traditional text-based approaches?
- Basis in paper: [explicit] The paper introduces a multimodal dataset for automatic summarization of scientific papers and evaluates the performance of two language models, Gigachat and YandexGPT, on this dataset.
- Why unresolved: The paper presents initial results showing that while final sections of papers correlate better with abstracts in terms of syntax, language models perform better in semantics. However, the limitations of the models due to censorship and text length, as well as the potential for further improvement by combining methods and analyzing visual data, indicate that there is room for further exploration and refinement.
- What evidence would resolve it: Further experiments comparing the performance of the multimodal dataset and language models with traditional text-based approaches, as well as exploring the integration of visual data and combining methods, would provide insights into the effectiveness of the proposed approach.

### Open Question 2
- Question: How can the proposed dataset be expanded to cover a wider range of scientific domains, including technical fields with complex formulas?
- Basis in paper: [inferred] The paper mentions that the current dataset covers only 7 scientific domains and suggests that expanding it to include more diverse areas is a future direction. It also highlights the challenge of creating a subset with technical papers such as Maths or Physics due to the presence of formulas.
- Why unresolved: The paper acknowledges the need to expand the dataset to include more scientific domains but does not provide specific strategies or solutions for handling technical papers with complex formulas.
- What evidence would resolve it: Developing strategies for representing and processing formulas in technical papers, as well as identifying and incorporating additional scientific domains, would address this open question.

### Open Question 3
- Question: How can the proposed dataset be utilized to evaluate systems that support multimodal inputs, considering the limitations of current language models in handling both text and images?
- Basis in paper: [inferred] The paper mentions that the current evaluation does not support all modalities as input for language models and suggests that this is an important direction for future research. It also notes that the dataset may already be used to evaluate systems that support multimodal inputs.
- Why unresolved: The paper acknowledges the potential of the dataset for evaluating multimodal systems but does not provide specific guidelines or benchmarks for such evaluations.
- What evidence would resolve it: Developing evaluation metrics and benchmarks specifically tailored for multimodal systems, as well as conducting experiments to assess the performance of different approaches in handling both text and images, would help address this open question.

## Limitations
- Reliance on proprietary Russian language models with opaque architectures and training data
- Censorship limitations that systematically bias results, particularly for sensitive domains
- Small sample size (420 papers) that may limit generalizability across domains

## Confidence
- High confidence: Core finding that Russian-language scientific summarization remains challenging for current LLMs
- Medium confidence: Comparative performance analysis between Gigachat and YandexGPT
- Medium confidence: Multimodal dataset's utility given preprocessing quality uncertainties
- Low confidence: Neural metrics' ability to capture true summary quality without human validation

## Next Checks
1. Conduct a small-scale human evaluation (5-10 papers per domain) to validate whether neural metrics align with human judgments of summary quality, particularly for cases where models faced censorship or truncation issues.

2. Test the dataset with an open-source Russian language model (such as LLaMA adapted for Russian) to determine whether performance limitations stem from model architecture or proprietary restrictions.

3. Perform a detailed analysis of papers that were rejected or truncated by the LLMs to identify patterns in the types of content most affected by censorship and length limits.