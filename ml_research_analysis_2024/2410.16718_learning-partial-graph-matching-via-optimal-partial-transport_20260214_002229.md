---
ver: rpa2
title: Learning Partial Graph Matching via Optimal Partial Transport
arxiv_id: '2410.16718'
source_url: https://arxiv.org/abs/2410.16718
tags:
- matching
- partial
- graph
- problem
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for partial graph matching
  by framing it as an optimal partial transport problem. The key innovation is formulating
  the problem using weighted total variation as a divergence function, which guarantees
  optimal partial assignments while incorporating matching biases.
---

# Learning Partial Graph Matching via Optimal Partial Transport

## Quick Facts
- arXiv ID: 2410.16718
- Source URL: https://arxiv.org/abs/2410.16718
- Authors: Gathika Ratnayaka; James Nichols; Qing Wang
- Reference count: 40
- Key outcome: Introduces optimal partial transport framework for graph matching with weighted total variation divergence, achieving cubic time complexity via Hungarian algorithm and outperforming baselines on image and PPI network datasets

## Executive Summary
This paper introduces a novel approach for partial graph matching by framing it as an optimal partial transport problem. The key innovation is formulating the problem using weighted total variation as a divergence function, which guarantees optimal partial assignments while incorporating matching biases. The authors demonstrate that their optimization problem can be embedded into a linear sum assignment problem, allowing efficient exact solutions using the Hungarian algorithm with cubic worst-case time complexity. They also propose an end-to-end deep learning architecture with a novel partial matching loss function.

## Method Summary
The method formulates partial graph matching as an optimal partial transport problem using weighted total variation as the divergence function. This enables partial assignments while incorporating node matching biases. The optimization problem can be embedded into a linear sum assignment problem, allowing efficient solution via the Hungarian algorithm. An end-to-end deep learning architecture learns both the cost matrix and matching biases through a novel partial matching loss function. The approach is evaluated on image keypoint matching and protein-protein interaction networks, demonstrating superior performance compared to state-of-the-art baselines.

## Key Results
- Outperforms state-of-the-art baselines on both image keypoint matching and protein-protein interaction network datasets
- Achieves better efficiency than competing methods while maintaining strong matching accuracy
- Particularly excels on the challenging IMCPT 100 dataset for image matching tasks
- Demonstrates cubic worst-case time complexity through Hungarian algorithm embedding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The weighted total variation divergence function enables partial assignments while incorporating node matching biases.
- **Mechanism**: By using weighted total variation as the divergence function in the optimal partial transport formulation, the optimization objective can distinguish between feasible and non-feasible assignments. Specifically, if the cost Cij exceeds ρ(αi + βj), no mass will be transported between nodes i and j, effectively preventing those nodes from being matched.
- **Core assumption**: The feasibility threshold property holds, where nodes with higher matching bias values have higher thresholds for being considered feasible matches.
- **Evidence anchors**:
  - [abstract]: "Our approach formulates an objective that enables partial assignments while incorporating matching biases, using weighted total variation as the divergence function to guarantee optimal partial assignments."
  - [section]: "Theorem 5.1 (Infeasible Assignments). Let π⋆ ∈ arg minΠ≤(µ,ν) TC(π; C, α, β) be any optimal solution of Eq. (5). For any 1 ≤ i ≤ m and 1 ≤ j ≤ n we have that Cij > ρ(αi + βj) = ⇒ π⋆ ij = 0."
- **Break condition**: If the feasibility threshold property doesn't hold, the method may produce incorrect partial assignments.

### Mechanism 2
- **Claim**: The proposed method can be efficiently solved using the Hungarian algorithm by embedding it into a linear sum assignment problem.
- **Mechanism**: By creating a set of dummy nodes and constructing an extended cost matrix, the partial graph matching problem can be embedded into a linear sum assignment problem. The Hungarian algorithm can then be applied to solve this problem efficiently with cubic worst-case time complexity.
- **Core assumption**: There exists a closed-form mapping between solutions of the linear sum assignment problem and solutions of the partial graph matching problem.
- **Evidence anchors**:
  - [abstract]: "Our method can achieve efficient, exact solutions within cubic worst case time complexity."
  - [section]: "Our main result demonstrates that it is possible to derive a linear sum assignment problem for which a closed-form mapping exists from any of its given solution to a solution of the optimal partial transport problem defined in Eq. (5)."
- **Break condition**: If the closed-form mapping doesn't exist or is not valid, the method cannot leverage the Hungarian algorithm for efficient solutions.

### Mechanism 3
- **Claim**: The end-to-end deep learning architecture with partial matching loss enables effective learning of both the cost matrix and matching biases.
- **Mechanism**: The architecture uses a neural network to generate cross-graph node-to-node affinity matrix, which is then used to compute matching biases and cost matrix. The partial matching loss function guides the learning process by considering both the cost matrix and matching biases.
- **Core assumption**: The neural network can effectively learn node embeddings and affinities that capture the relevant features for partial graph matching.
- **Evidence anchors**:
  - [abstract]: "we propose a deep graph matching architecture with a novel partial matching loss, providing an end-to-end solution."
  - [section]: "Building on the theoretical insights of our proposed optimization objective, we introduce a deep graph matching architecture that embeds feature and structural properties into the cross-graph node-to-node cost matrix and matching biases."
- **Break condition**: If the neural network fails to learn meaningful embeddings or the loss function doesn't effectively guide the learning process, the method's performance will degrade.

## Foundational Learning

- **Concept**: Optimal Transport Theory
  - **Why needed here**: The paper's approach is fundamentally based on optimal partial transport, which is a generalization of optimal transport that allows for partial mass transportation.
  - **Quick check question**: What is the key difference between balanced optimal transport and optimal partial transport?
  - **Answer**: Balanced optimal transport requires exact mass conservation between distributions, while optimal partial transport allows for partial mass transportation.

- **Concept**: Linear Sum Assignment Problem
  - **Why needed here**: The paper's solution leverages the connection between partial graph matching and linear sum assignment problem to achieve efficient computation.
  - **Quick check question**: What algorithm is commonly used to solve linear sum assignment problems efficiently?
  - **Answer**: The Hungarian algorithm, which has cubic worst-case time complexity.

- **Concept**: Graph Neural Networks
  - **Why needed here**: The paper's end-to-end architecture uses a neural network to learn node embeddings and cross-graph affinities.
  - **Quick check question**: What is the purpose of using Graph Neural Networks in this context?
  - **Answer**: To transform geometric affinities between nodes into node embeddings, leveraging their features and structural information for effective partial graph matching.

## Architecture Onboarding

- **Component map**: Input graphs GS, GT -> Neural network NNθ -> Cross-graph node-to-node affinity matrix A -> Matching bias calculation (α and β) -> Cost matrix computation (C) -> Partial matching loss computation -> Optimization process -> Output partial matching

- **Critical path**: Input graphs → NNθ → Affinity matrix A → Matching biases α, β and cost matrix C → Partial matching loss → Optimization → Output partial matching

- **Design tradeoffs**:
  - Using weighted total variation as divergence function allows for partial assignments but requires careful tuning of the unbalancedness parameter ρ
  - Embedding into linear sum assignment problem enables efficient computation but requires creating dummy nodes and extending the cost matrix
  - End-to-end learning approach allows for joint optimization but requires careful design of the loss function and neural network architecture

- **Failure signatures**:
  - Poor performance on datasets with ambiguous or poor annotations
  - Sensitivity to noise levels in the data
  - Increased computational complexity when learning matching biases alongside the cost matrix in noisy environments

- **First 3 experiments**:
  1. Evaluate the method on a simple partial graph matching benchmark with ground truth annotations to verify basic functionality
  2. Conduct a sensitivity analysis on the unbalancedness parameter ρ to understand its impact on performance
  3. Compare the runtime efficiency of the method against baseline approaches on a medium-sized dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of divergence function (beyond weighted total variation) affect the quality and computational efficiency of partial graph matching solutions?
- Basis in paper: [explicit] The paper discusses using weighted total variation as the divergence function, noting its properties for identifying infeasible assignments and incorporating matching biases. It mentions that Total Variation (TV) and Kullback-Leibler (KL) divergence are commonly used in optimal partial transport literature.
- Why unresolved: The paper focuses specifically on weighted total variation but acknowledges other divergence functions exist. No comparative analysis is provided for different divergence functions.
- What evidence would resolve it: Empirical comparison of partial graph matching performance using different divergence functions (TV, KL, Wasserstein, etc.) on benchmark datasets, measuring both matching accuracy and computational runtime.

### Open Question 2
- Question: What is the theoretical relationship between the matching bias parameters α and β and the underlying node features or graph structure?
- Basis in paper: [explicit] The paper describes how matching biases are calculated from node affinities (αi = 2 × (σ(wrs × ri) − 0.5) and βj = 2 × (σ(wrs × rj) − 0.5)), where ri and rj are maximum affinities for each node, but does not provide theoretical justification for this formulation.
- Why unresolved: The paper proposes a practical heuristic for computing matching biases but does not establish a formal connection between these biases and graph-theoretic properties or node feature distributions.
- What evidence would resolve it: Theoretical analysis linking matching biases to node centrality measures, feature distributions, or graph connectivity metrics, validated through ablation studies on synthetic graphs with known properties.

### Open Question 3
- Question: How does the method scale to graphs with millions of nodes, and what architectural modifications would be required?
- Basis in paper: [explicit] The paper reports cubic worst-case time complexity O(n³) for the Hungarian algorithm, which is stated as an improvement over ILP approaches, but does not discuss scalability beyond standard benchmarks.
- Why unresolved: The cubic complexity becomes prohibitive for large-scale graphs, yet the paper does not explore approximation methods, parallel implementations, or alternative optimization strategies for scalability.
- What evidence would resolve it: Performance analysis on progressively larger graph datasets, implementation of approximate matching algorithms (e.g., graph coarsening, sampling), and comparison of accuracy-runtime tradeoffs at different scales.

### Open Question 4
- Question: How robust is the method to adversarial perturbations of node features or graph topology?
- Basis in paper: [inferred] The paper evaluates performance under increasing noise levels in PPI networks but does not consider targeted adversarial attacks that could systematically mislead the matching process.
- Why unresolved: The analysis focuses on random noise addition rather than worst-case adversarial scenarios that could exploit the optimization objective's structure.
- What evidence would resolve it: Empirical evaluation against gradient-based feature perturbations, topology attacks (edge addition/removal), and defense mechanisms to maintain matching accuracy under adversarial conditions.

## Limitations

- The method shows reduced performance on PPI networks compared to image datasets, suggesting domain-specific limitations
- High sensitivity to noise levels in matching bias learning when ground truth is ambiguous or poor
- Computational complexity increases when learning matching biases alongside the cost matrix in noisy environments

## Confidence

- **High confidence**: The theoretical framework of optimal partial transport and its embedding into linear sum assignment problem
- **Medium confidence**: The end-to-end learning architecture's effectiveness across diverse domains
- **Medium confidence**: The empirical performance gains over state-of-the-art baselines

## Next Checks

1. Conduct ablation studies to isolate the contribution of matching biases versus the cost matrix to overall performance
2. Test the method's robustness to varying levels of noise and annotation quality across different dataset types
3. Evaluate computational scaling on larger graph datasets to verify claimed efficiency benefits