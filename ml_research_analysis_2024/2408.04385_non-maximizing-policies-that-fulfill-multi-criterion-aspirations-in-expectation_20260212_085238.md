---
ver: rpa2
title: Non-maximizing policies that fulfill multi-criterion aspirations in expectation
arxiv_id: '2408.04385'
source_url: https://arxiv.org/abs/2408.04385
tags:
- policy
- which
- algorithm
- aspiration
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing AI agents that
  can fulfill multi-criterion aspirations without relying on reward maximization,
  which can lead to unintended behaviors. The authors propose a planning-based approach
  using Markov Decision Processes (MDPs) with multiple evaluation metrics and a convex
  aspiration set that defines acceptable outcomes.
---

# Non-maximizing policies that fulfill multi-criterion aspirations in expectation

## Quick Facts
- arXiv ID: 2408.04385
- Source URL: https://arxiv.org/abs/2408.04385
- Reference count: 40
- This paper proposes a planning-based approach using Markov Decision Processes with multiple evaluation metrics and a convex aspiration set to design AI agents that fulfill multi-criterion aspirations without reward maximization.

## Executive Summary
This paper addresses the challenge of designing AI agents that can fulfill multi-criterion aspirations without relying on reward maximization, which can lead to unintended behaviors. The authors propose a planning-based approach using Markov Decision Processes (MDPs) with multiple evaluation metrics and a convex aspiration set that defines acceptable outcomes. Their algorithm guarantees that the agent's expected total evaluation metrics fall within this set by approximating feasibility sets with simplices and propagating aspirations forward while maintaining feasibility. The method has linear complexity in the number of state-action-successor triples and polynomial complexity in the number of evaluation metrics.

## Method Summary
The authors propose a planning-based algorithm for finite acyclic MDPs with multiple evaluation metrics. The method uses feasibility sets and aspiration propagation to ensure expected totals fall within a convex set defined by the user. The algorithm approximates feasibility sets with reference simplices and propagates aspirations forward while maintaining feasibility, with action selection incorporating heuristic safety criteria. The approach guarantees that the agent's expected total evaluation metrics fall within the specified aspiration set.

## Key Results
- The algorithm guarantees expected total evaluation metrics fall within a convex aspiration set
- Has linear complexity in state-action-successor triples and polynomial complexity in evaluation metrics
- Non-maximizing nature allows for additional safety criteria like variance minimization
- Numerical experiments suggest O(d) trials on average for finding appropriate reference policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aspiration-based agents can fulfill multi-criterion aspirations without reward maximization
- Mechanism: The algorithm uses feasibility sets and aspiration propagation to ensure expected totals fall within a convex set
- Core assumption: The environment is acyclic and the feasibility sets are convex polytopes
- Evidence anchors:
  - [abstract]: "Our algorithm guarantees that this task is fulfilled by using simplices to approximate feasibility sets and propagate aspirations forward while ensuring they remain feasible."
  - [section 3]: "The correctness of algorithm 1 follows from the requirements of lines 2 and 5; that these are possible to fulfill is a consequence of equations (5) and (4)."
  - [corpus]: Weak evidence - the corpus papers focus on different RL approaches without directly addressing aspiration-based methods
- Break condition: If the environment contains cycles or the feasibility sets are non-convex, the algorithm may fail

### Mechanism 2
- Claim: The algorithm has linear complexity in state-action-successor triples and polynomial complexity in evaluation metrics
- Mechanism: Uses reference simplices to approximate feasibility sets and propagates aspirations efficiently
- Core assumption: Reference simplices can adequately approximate the feasibility sets
- Evidence anchors:
  - [abstract]: "It has complexity linear in the number of possible state-action-successor triples and polynomial in the number of evaluation metrics."
  - [section 4]: "Given all values Vπ_i(s) and Qπ_i(s,a) and both the kc ≥ d + 1 constraints and kv ≥ d + 1 vertices defining Ea, the shrinking version of action-to-state aspiration propagation has time complexity O([kc^1.5 d + (dkv)^1.5]L)"
  - [corpus]: Weak evidence - corpus papers don't directly address complexity of aspiration-based approaches
- Break condition: If the number of evaluation metrics grows very large, polynomial complexity may become prohibitive

### Mechanism 3
- Claim: Non-maximizing nature yields additional degrees of freedom for safety criteria
- Mechanism: The algorithm can incorporate heuristic safety criteria when choosing actions within feasible aspiration sets
- Core assumption: The evaluation metrics don't need to be maximized, allowing for conservative behavior
- Evidence anchors:
  - [abstract]: "Moreover, the explicitly non-maximizing nature of the chosen policy and goals yields additional degrees of freedom, which can be used to apply heuristic safety criteria to the choice of actions."
  - [section 6]: "It is natural to consider extensions of this approach to further properties of the trajectory distribution, e.g. by specifying that the variance of the total should be small."
  - [corpus]: Weak evidence - corpus papers focus on different safety approaches not directly related to aspiration-based methods
- Break condition: If safety criteria conflict with aspiration fulfillment, the algorithm may need to prioritize one over the other

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The algorithm operates on MDPs with multiple evaluation metrics instead of reward functions
  - Quick check question: What distinguishes an MDP with evaluation metrics from a standard MDP with rewards?

- Concept: Convex geometry and polytopes
  - Why needed here: Aspiration sets are convex polytopes, and the algorithm uses convex hulls and simplices
  - Quick check question: How do you verify that a given set is a convex polytope?

- Concept: Dynamic programming and Bellman equations
  - Why needed here: The algorithm uses recursive equations similar to Bellman equations for value and action-value functions
  - Quick check question: What is the base case for the Bellman equations in this acyclic environment?

## Architecture Onboarding

- Component map:
  - Environment model (S, s0, S⊤, A, T)
  - Evaluation function f: (S\S⊤)×A×(S\{s0})→Rd
  - Aspiration set E0⊂Rd
  - Reference policies π1,...,πd+1
  - Algorithm 1 (FulfillStateAspiration, FulfillActionAspiration)
  - Algorithm 2 (Action selection with shrinking/clipping)

- Critical path:
  1. Find feasible aspiration point x ∈ E0∩V(s0)
  2. Determine reference policies using backwards induction
  3. Initialize state-aspiration E = E0
  4. For each state, choose action and action-aspiration
  5. Propagate aspiration to successor states
  6. Repeat until terminal state

- Design tradeoffs:
  - Shrinking vs clipping for aspiration propagation
  - Number of reference policies (d+1) vs accuracy of approximation
  - Computational complexity vs precision of feasibility set approximation

- Failure signatures:
  - Aspiration set becomes empty during propagation
  - Linear programs for action selection become infeasible
  - Reference simplex doesn't contain feasible aspiration point

- First 3 experiments:
  1. Test on simple gridworld with 2 evaluation metrics and aspiration set {(o,a) | o+a=10, o≥0, a≥0}
  2. Verify complexity claims on binary tree environment with varying number of metrics
  3. Test safety criteria by adding variance minimization to action selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact computational complexity of finding appropriate reference policies in the algorithm?
- Basis in paper: [explicit] The paper states they were unable to prove complexity bounds for this part and only provided numerical evidence suggesting O(d) trials on average.
- Why unresolved: The authors conjecture O(d) complexity but lack a formal proof, only providing numerical experiments with random binary tree-shaped environments.
- What evidence would resolve it: A rigorous mathematical proof showing the expected number of trials needed to find reference policies is indeed O(d), or counterexamples demonstrating higher complexity.

### Open Question 2
- Question: How does the algorithm perform in non-acyclic environments where states can be revisited?
- Basis in paper: [inferred] The algorithm is specifically designed for finite acyclic MDPs, and the Bellman equations rely on this property for backwards induction.
- Why unresolved: The paper only considers acyclic environments and doesn't explore how the algorithm might need to be modified or what guarantees might still hold in cyclic environments.
- What evidence would resolve it: Experiments or theoretical analysis showing how the algorithm performs in cyclic MDPs, including any modifications needed and new convergence guarantees.

### Open Question 3
- Question: How sensitive is the algorithm to misspecification of the aspiration set?
- Basis in paper: [inferred] The algorithm assumes a correct aspiration set is provided, but doesn't explore what happens if the set is infeasible or poorly specified.
- Why unresolved: The paper focuses on the algorithm's mechanics but doesn't analyze robustness to incorrect user specifications or provide guidance on how to validate aspiration sets.
- What evidence would resolve it: Experiments testing the algorithm's behavior with various types of aspiration set misspecifications (infeasible, too tight, too loose) and analysis of failure modes or robustness properties.

## Limitations
- The algorithm requires acyclic MDPs and convex feasibility sets
- Finding appropriate reference policies is computationally challenging and lacks formal complexity guarantees
- Safety criteria extensions remain largely speculative without systematic evaluation

## Confidence
- Core theoretical claims: Medium confidence (strong assumptions about acyclic MDPs and convex feasibility sets)
- Complexity analysis: Medium confidence (assumes efficient linear programming solutions)
- Safety criteria extensions: Low confidence (remain largely speculative with brief discussion)
- Reference policy selection: Low confidence (conjecture of O(d) trials without formal proof)

## Next Checks
1. Test algorithm performance on MDPs with cycles to verify the acyclic assumption is critical
2. Systematically evaluate reference policy selection across different environment types to measure the O(d) trial claim
3. Compare aspiration-based safety criteria (variance minimization, disordering potential) against established safety approaches in multi-objective RL