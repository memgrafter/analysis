---
ver: rpa2
title: 'MarDini: Masked Autoregressive Diffusion for Video Generation at Scale'
arxiv_id: '2410.20280'
source_url: https://arxiv.org/abs/2410.20280
tags:
- video
- generation
- training
- frames
- mardini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MarDini, a video diffusion model that combines
  masked auto-regression (MAR) for temporal planning with diffusion models (DM) for
  spatial generation. The asymmetric architecture allocates most parameters to a low-resolution
  MAR planning model, which generates planning signals, while a lightweight high-resolution
  DM generates frames using these signals.
---

# MarDini: Masked Autoregressive Diffusion for Video Generation at Scale

## Quick Facts
- arXiv ID: 2410.20280
- Source URL: https://arxiv.org/abs/2410.20280
- Reference count: 18
- Sets new SOTA for video interpolation on VIDIM-Bench with FVD of 99.05 on UCF101-7

## Executive Summary
MarDini introduces a novel video generation framework that combines masked auto-regression (MAR) with diffusion models (DM) through an asymmetric architecture. The model uses a parameter-heavy MAR planning component at low resolution to generate temporal planning signals, which guide a lightweight DM generation component at high resolution. This design enables computationally expensive spatio-temporal attention in MAR while maintaining efficiency in DM. MarDini demonstrates state-of-the-art performance on video interpolation benchmarks and competitive results on image-to-video generation, all trained from scratch without pre-training through a progressive curriculum.

## Method Summary
MarDini employs an asymmetric architecture where a large MAR planning model (3.1B parameters) operates at low resolution to capture temporal structure, while a smaller DM generation model (288M-1B parameters) operates at high resolution for spatial quality. The MAR model uses bidirectional attention and spatio-temporal attention mechanisms, while the DM model uses cross-attention to integrate planning signals and temporal attention for efficiency. The model is trained progressively, starting with separate models, moving to joint training for video interpolation, and finally adding image-to-video tasks with dynamic masking ratios. Identity Attention resolves training instability from mixed reference and noise tokens, and the entire system is trained from scratch on unlabeled video data.

## Key Results
- Sets new state-of-the-art for video interpolation on VIDIM-Bench with FVD of 99.05 on UCF101-7
- Achieves 80.88 video quality score on VBench I2V task without motion guidance
- Outperforms comparable models while requiring fewer inference steps
- Demonstrates competitive performance against larger models (e.g., JMV outperforms SVD by 2.5x in params)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric resolution design enables computationally expensive spatio-temporal attention at scale
- Mechanism: By allocating most parameters to a low-resolution MAR planning model and using a lightweight high-resolution DM, the system can afford spatio-temporal attention in MAR without memory constraints while keeping DM efficient
- Core assumption: The planning signals from low-resolution MAR contain sufficient global temporal information to guide high-resolution DM generation
- Evidence anchors:
  - [abstract] "The efficient design allocates most of the computational resources to the low-resolution planning model, making computationally expensive but important spatio-temporal attention feasible at scale"
  - [section 2.1] "Following these principles, we use the same training batch for both MAR and DM but employ two distinct processes operating at different resolutions"

### Mechanism 2
- Claim: Progressive training strategy enables stable training from scratch without pre-training
- Mechanism: Gradually increasing task difficulty through mask ratio tuning allows the model to learn from simple interpolation to complex generation, building capability incrementally
- Core assumption: Video interpolation and image-to-video generation share sufficient underlying representations that learning one task facilitates learning the other
- Evidence anchors:
  - [section 2.4] "This approach enables the model to scale from video interpolation to full video generation, directly bypassing the need for image-based pre-training"
  - [section 3.1] "As shown in Fig.5(b), we track the performance of MarDini on both video interpolation and image animation during a training phase"

### Mechanism 3
- Claim: Identity attention resolves training instability from mixed reference and noise tokens
- Mechanism: Separating attention patterns for reference tokens (only self-attention) and noise tokens (global attention) prevents distribution shift between clean and noisy inputs
- Core assumption: The distributional difference between reference and noise tokens creates conflicting training signals that identity attention can resolve
- Evidence anchors:
  - [section 2.3.2] "We introduce Identity Attention, which enables the model to easily distinguish between [REF] and [NOISE] tokens by employing a separate attention strategy"
  - [section 3.1] "The decrease in training loss observed after 6k steps is attributed to the use of a warm-up learning rate, where the learning rate is intentionally kept low during the initial steps"

## Foundational Learning

- Concept: Masked auto-regression (MAR)
  - Why needed here: Provides flexible temporal planning capabilities that can handle various masking patterns for different video generation tasks
  - Quick check question: How does MAR differ from causal auto-regression in handling visual data?

- Concept: Diffusion models (DM)
  - Why needed here: Offers stable training in continuous space for high-resolution spatial generation
  - Quick check question: Why are diffusion models particularly suitable for video generation compared to other generative approaches?

- Concept: Asymmetric network design
  - Why needed here: Enables computationally expensive operations at lower resolution while maintaining efficiency at higher resolution
  - Quick check question: What is the computational trade-off between processing resolution and model capacity?

## Architecture Onboarding

- Component map: MAR planning model (3.1B params, low-res, spatio-temporal attention) → planning signals → DM generation model (288M-1B params, high-res, temporal attention)
- Critical path: Low-res frames → MAR → planning signals → cross-attention → DM → high-res frames
- Design tradeoffs: Memory efficiency vs. model capacity, training stability vs. computational cost, flexibility vs. specialization
- Failure signatures: Training instability (noisy gradients), poor motion coherence (inadequate planning), slow inference (inefficient architecture)
- First 3 experiments:
  1. Train MAR alone with masked reconstruction loss to verify temporal planning capability
  2. Train DM alone with masked diffusion loss to verify spatial generation capability
  3. Joint training with fixed mask ratio to validate cross-attention integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper limit on the number of frames that MarDini can generate through hierarchical auto-regressive generation before quality degradation becomes significant?
- Basis in paper: [explicit] The paper demonstrates auto-regressive generation with a 32-frame window expanding to 128 frames, but doesn't explore the limits of this approach.
- Why unresolved: The paper only shows one example of hierarchical auto-regressive generation without systematic evaluation of performance degradation at larger scales.
- What evidence would resolve it: Systematic experiments testing generation quality (using metrics like FVD, LPIPS, or human evaluation) as a function of total generated frames through recursive auto-regressive expansion.

### Open Question 2
- Question: How does the performance of MarDini change when trained on different types of video data distributions, particularly those with varying motion dynamics and scene complexity?
- Basis in paper: [inferred] The paper trains exclusively on Shutterstock video data and doesn't explore generalization across different data distributions or test domain transfer capabilities.
- Why unresolved: The evaluation is limited to standard benchmarks without exploring how the model performs on videos with different characteristics than the training data.
- What evidence would resolve it: Training and evaluation on diverse video datasets with varying characteristics (e.g., different frame rates, motion intensities, scene types) and comparing performance metrics across these distributions.

### Open Question 3
- Question: What is the optimal balance between MAR planning model size and DM generation model size for different video generation tasks and resolutions?
- Basis in paper: [explicit] The paper uses an asymmetric design with the MAR model containing most parameters at low resolution and DM being smaller at high resolution, but doesn't systematically explore alternative parameter allocations.
- Why unresolved: The paper fixes the asymmetric design but doesn't investigate whether different task types or resolutions might benefit from different parameter distributions between the two components.
- What evidence would resolve it: Systematic ablation studies varying the parameter ratio between MAR and DM models across different tasks (interpolation vs. image-to-video) and resolutions (256p, 512p, 768p, 1024p), measuring performance-quality trade-offs.

### Open Question 4
- Question: How does the progressive training strategy impact long-term temporal consistency in generated videos, particularly for sequences requiring complex motion planning?
- Basis in paper: [inferred] The paper describes progressive training from video interpolation to full video generation but doesn't explicitly analyze the impact on long-term temporal coherence or compare against non-progressive training approaches.
- Why unresolved: The paper focuses on overall performance metrics without specifically measuring how the progressive training approach affects the model's ability to maintain consistency over longer temporal spans.
- What evidence would resolve it: Comparative experiments between progressive and non-progressive training approaches, measuring temporal consistency metrics (e.g., frame-to-frame similarity, motion smoothness) over extended video sequences, particularly for complex motion patterns.

## Limitations
- The asymmetric design's effectiveness depends heavily on the assumption that low-resolution planning signals contain sufficient temporal structure to guide high-resolution generation
- Progressive training success relies on shared representations between interpolation and image-to-video tasks, but this transfer learning assumption remains untested through task-specific ablations
- Identity Attention's contribution to training stability is validated but the mechanism's necessity versus simpler alternatives is not explored

## Confidence
- **High confidence**: The asymmetric architecture design and its computational efficiency claims are well-supported by the mathematical formulation and experimental results showing competitive performance with fewer parameters than baselines
- **Medium confidence**: The progressive training strategy's effectiveness is demonstrated empirically but lacks detailed analysis of why curriculum learning succeeds or how sensitive it is to mask ratio scheduling
- **Medium confidence**: Identity Attention's contribution to training stability is validated through training curves, but the mechanism's necessity versus simpler alternatives (like careful normalization) is not explored

## Next Checks
1. **Planning signal ablation**: Test MarDini performance when degrading MAR planning signal quality through resolution reduction, attention masking, or noise injection to quantify how much planning signal quality affects final generation quality
2. **Progressive training sensitivity**: Systematically vary mask ratio schedules and training stage durations to determine optimal curriculum design and test whether the progressive approach provides benefits beyond simple joint training
3. **Identity attention necessity**: Replace Identity Attention with alternative stabilization techniques (batch normalization, gradient clipping, or standard attention) to isolate whether the specific attention separation mechanism is essential or if simpler methods suffice