---
ver: rpa2
title: 'MooER: LLM-based Speech Recognition and Translation Models from Moore Threads'
arxiv_id: '2408.05101'
source_url: https://arxiv.org/abs/2408.05101
tags:
- training
- speech
- audio
- data
- hours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MooER, a LLM-based automatic speech recognition
  (ASR) and speech translation (AST) model developed by Moore Threads. The model leverages
  a small-scale pseudo-labeled dataset of 5000 hours, achieving performance comparable
  to other open-source models trained with hundreds of thousands of hours of labeled
  data.
---

# MooER: LLM-based Speech Recognition and Translation Models from Moore Threads

## Quick Facts
- **arXiv ID**: 2408.05101
- **Source URL**: https://arxiv.org/abs/2408.05101
- **Reference count**: 19
- **Primary result**: Achieves CER of 4.21% on 6 Mandarin test sets and BLEU of 25.2 on Covost2 Zh2en using only 5000h pseudo-labeled data

## Executive Summary
MooER is an LLM-based automatic speech recognition and translation model developed by Moore Threads that demonstrates state-of-the-art performance using minimal labeled data. The model leverages a 5000-hour pseudo-labeled dataset to achieve results comparable to models trained on hundreds of thousands of hours of manually labeled data. Using parameter-efficient fine-tuning with LoRA on the Qwen2-7B-instruct LLM, MooER achieves strong performance across multiple benchmarks while maintaining computational efficiency. The model is trained on 8 S4000 GPUs in 38 hours using advanced optimization techniques including DeepSpeed, BF16 acceleration, and gradient checkpointing.

## Method Summary
MooER combines a Paraformer encoder with the Qwen2-7B-instruct LLM, using a speech adapter to bridge the audio and text modalities. The model is trained using a 5000-hour pseudo-labeled dataset consisting of open-source and in-house speech data. Only the speech adapter and LLM LoRA parameters are fine-tuned, preserving the base model capabilities while adapting to speech tasks. The training employs DeepSpeed optimization with ZeRO stage 2, BF16 precision, and gradient checkpointing to enable efficient training on limited GPU resources. The approach demonstrates that parameter-efficient fine-tuning with high-quality pseudo-labels can achieve strong ASR and AST performance without extensive manual annotation.

## Key Results
- Achieves CER of 4.21% on 6 Mandarin test sets
- Achieves WER of 17.98% on 6 English test sets
- Achieves BLEU score of 25.2 on Covost2 Zh2en test set
- Comparable performance to models trained on hundreds of thousands of hours of labeled data
- Trained on 8 GPUs in 38 hours using optimization techniques

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-labeled data enables strong performance without manual annotation
The model leverages LLM-generated pseudo-labels from a 5000-hour dataset, capturing sufficient linguistic patterns for fine-tuning while avoiding expensive manual labeling. The core assumption is that pseudo-labels from third-party cloud services are sufficiently accurate for model training. Break condition: Pseudo-label quality degrades significantly with domain mismatch or poor audio quality, causing model convergence issues.

### Mechanism 2: Parameter-efficient fine-tuning preserves base model capabilities
By freezing the encoder and training only ~2% of LLM parameters via LoRA, the model retains general language understanding while specializing in speech processing. The core assumption is that the base Qwen2-7B-instruct model has sufficient general language capabilities to transfer to speech tasks. Break condition: If the base LLM lacks sufficient domain knowledge or if speech-specific patterns require more extensive adaptation than LoRA can provide.

### Mechanism 3: Optimization techniques enable efficient training on limited resources
DeepSpeed's ZeRO optimization reduces memory footprint while BF16 maintains numerical stability, allowing 8 GPUs to train the model in 38 hours. The core assumption is that optimization techniques don't significantly impact convergence quality. Break condition: If memory constraints still prevent training or if numerical precision loss from BF16 degrades model performance.

## Foundational Learning

- **Speech feature extraction and representation**: Understanding how Paraformer processes audio into embeddings that can be fused with text is critical for debugging and extending the model. Quick check: What is the output dimension and frame rate of the Paraformer encoder before adapter processing?

- **Parameter-efficient fine-tuning techniques (LoRA)**: The entire training strategy relies on updating only 2% of LLM parameters; understanding LoRA's mechanics is essential for modifications. Quick check: How does LoRA's rank parameter (r=64) affect the number of trainable parameters and model performance?

- **Speech-language model integration**: The model architecture requires understanding how audio embeddings are concatenated with text prompts and processed by the LLM decoder. Quick check: What is the exact format of the input prompt to the LLM, including both audio and text components?

## Architecture Onboarding

- **Component map**: Audio input → Paraformer encoding → Adapter downsampling → Audio embedding → Concatenate with text embedding → LLM processing → ASR/AST output

- **Critical path**: Paraformer encoder (fixed) → Adapter (9.44M trainable) → LLM with LoRA (161.48M trainable out of 7615.62M total)

- **Design tradeoffs**: Parameter efficiency vs. adaptation capacity (only 2% of LLM parameters are trainable); Training speed vs. memory usage (BF16 and DeepSpeed optimizations); Encoder choice (Paraformer vs Whisper trade-offs)

- **Failure signatures**: Training divergence (likely caused by poor pseudo-label quality or inappropriate learning rates); Memory overflow (may occur during encoder fine-tuning or with larger batch sizes); Performance plateau (could indicate insufficient LoRA capacity or need for longer training)

- **First 3 experiments**: 
  1. Validate pseudo-label quality by comparing third-party ASR output with ground truth on a small subset
  2. Test adapter-only training (freezing LLM) to isolate the contribution of speech adapter parameters
  3. Experiment with different LoRA rank parameters (r=32, r=64, r=128) to find the optimal balance between performance and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MooER compare when trained on datasets of different sizes beyond 5,000 and 80,000 hours? The paper mentions training on 5,000 hours and 80,000 hours but does not explore intermediate sizes or provide detailed analysis of how performance scales with dataset size.

### Open Question 2
What are the limitations of using pseudo-labeled data for training MooER, and how do they affect the model's performance in real-world applications? The paper uses pseudo-labeled data without manual annotation but does not discuss potential limitations or biases introduced by this approach.

### Open Question 3
How does the choice of encoder (e.g., Whisper, W2v-Bert2.0, Paraformer) affect the model's performance, and what are the trade-offs between different encoders? The paper mentions trying different encoders and ultimately choosing Paraformer but does not provide detailed comparison of their performance or discuss trade-offs.

### Open Question 4
What are the potential applications of MooER in low-resource language domains, and how does the model perform on languages with limited training data? The paper mentions potential application to low-resource domains such as minority languages and dialects but does not provide empirical evidence or specific examples.

## Limitations

- Reliance on pseudo-labeled data quality without manual verification or error analysis
- Limited evaluation scope (6 Mandarin and 6 English test sets) without broader cross-lingual testing
- Strong assumptions about base LLM transferability to speech tasks with minimal fine-tuning
- No detailed comparison with other parameter-efficient fine-tuning approaches

## Confidence

**High Confidence**: Architectural design and training setup are well-specified with clear descriptions of components and optimization techniques.

**Medium Confidence**: Performance metrics are claimed to be comparable to larger models, but lack direct ablation studies or head-to-head benchmarks with similar pseudo-labeling approaches.

**Low Confidence**: The assertion that 5000 hours of pseudo-labeled data can match models trained on hundreds of thousands of hours requires independent verification of pseudo-label quality and controlled experiments.

## Next Checks

1. **Pseudo-label Quality Assessment**: Conduct controlled experiment comparing model performance when trained on manually labeled subsets versus pseudo-labeled data from the same 5000-hour corpus. Measure label error rates and analyze domain coverage.

2. **Parameter Efficiency Analysis**: Perform ablation studies varying the LoRA rank parameter (r=32, r=64, r=128) and measuring both performance and parameter count. Test scenarios with different proportions of trainable parameters.

3. **Cross-domain Generalization Test**: Evaluate the trained model on out-of-domain speech datasets (different accents, noise conditions, or languages not present in training corpus) to assess robustness and compare performance degradation against models trained on more diverse labeled datasets.