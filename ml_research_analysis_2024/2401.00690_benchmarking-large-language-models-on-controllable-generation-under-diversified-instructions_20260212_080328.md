---
ver: rpa2
title: Benchmarking Large Language Models on Controllable Generation under Diversified
  Instructions
arxiv_id: '2401.00690'
source_url: https://arxiv.org/abs/2401.00690
tags:
- text
- instructions
- instruction
- llms
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoDI-Eval, a benchmark for evaluating large
  language models' controllable text generation capabilities under diverse instructions.
  The key idea is to create a large set of natural language instructions with various
  constraints (sentiment, topic, length, keywords, toxicity avoidance) and diverse
  expression forms.
---

# Benchmarking Large Language Models on Controllable Generation under Diversified Instructions

## Quick Facts
- arXiv ID: 2401.00690
- Source URL: https://arxiv.org/abs/2401.00690
- Reference count: 40
- Key outcome: Commercial LLMs like GPT-4 and ChatGPT outperform open-source models on CoDI-Eval benchmark for controllable text generation under diverse instructions.

## Executive Summary
This paper introduces CoDI-Eval, a benchmark designed to evaluate large language models' controllable text generation capabilities under diverse natural language instructions. The benchmark addresses the challenge of measuring LLMs' ability to follow instructions with various constraints such as sentiment, topic, length, keywords, and toxicity avoidance. The authors employ a two-step process of instruction expansion and diversification to create a large, varied set of instructions. Automated evaluation methods are used for each task type, primarily measuring accuracy. Extensive experiments reveal that while top commercial models perform well, there remains significant room for improvement, especially in handling complex multi-aspect constraints and length control.

## Method Summary
The CoDI-Eval benchmark is constructed through a two-step process: instruction expansion followed by diversification. Starting with manually written seed instructions, LLMs are used to generate a large pool of instructions (expansion), which are then systematically rewritten using various text rewriting techniques to increase diversity (diversification). The resulting instructions cover five main tasks: sentiment control, topic control, length control, keyword incorporation, and toxicity avoidance. For evaluation, automated methods are employed for each task type - text classifiers for sentiment and topic, keyword matching, length interval checking, and toxicity APIs. LLMs are tested in both zero-shot and few-shot settings, with accuracy as the primary metric.

## Key Results
- Commercial models (GPT-4, ChatGPT) significantly outperform open-source models on CoDI-Eval
- Performance varies across task types, with some tasks (like length control) being more challenging
- Few-shot learning provides a notable performance boost compared to zero-shot
- Even the best-performing models show room for improvement, particularly in handling complex multi-aspect constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using text rewriting to diversify instructions maintains semantic meaning while expanding expression variety
- Mechanism: The instruction diversification step employs text rewriting techniques (e.g., formal/informal, concise/verbose, part-of-speech conversion) to transform existing instructions into new forms while preserving their core meaning and control constraints
- Core assumption: Text rewriting methods can reliably preserve instruction semantics while creating sufficiently diverse expressions
- Evidence anchors:
  - [abstract] "we advocate an instruction diversification process to synthesize diverse forms of constraint expression"
  - [section] "We find that text rewriting is able to maintain the meaning of instructions while diversifying their expressions"
  - [corpus] Weak evidence - no direct citations about text rewriting effectiveness in instruction diversification
- Break condition: If text rewriting produces instructions that no longer accurately represent the intended control constraints or lose their evaluability

### Mechanism 2
- Claim: Two-step process (expansion then diversification) generates more diverse instructions than either step alone
- Mechanism: Initial expansion creates a large pool of instructions, then diversification systematically rewrites them using various techniques and Bootstrap sampling to achieve maximum diversity
- Core assumption: Combining expansion with systematic diversification produces greater instruction diversity than simple template-based approaches
- Evidence anchors:
  - [section] "We employ a two-step in-context learning prompting to generate instructions with increased diversity and varying expressions"
  - [section] "We calculate the average of Rouge-L (Lin and Och 2004) scores... the instructions undergoing the diversification stage exhibit greater diversity"
  - [corpus] Moderate evidence - related work exists but not specifically validating this two-step approach
- Break condition: If diversification step fails to significantly increase instruction diversity beyond what expansion alone achieves

### Mechanism 3
- Claim: Automated evaluation using specialized models provides reliable assessment of instruction-following capability
- Mechanism: For each task type, appropriate automated evaluation methods are employed (text classifiers for sentiment/topic, keyword matching, length interval checking, toxicity API)
- Core assumption: Automated evaluation methods can accurately determine whether generated text satisfies control constraints with sufficient consistency to human judgment
- Evidence anchors:
  - [section] "We collect or construct methods to automatically evaluate the accuracy (%) of each CTG task"
  - [section] "We conduct simple human judgment to verify the reliability of the evaluation methods... consistency between our automated evaluation and human evaluation"
  - [corpus] Weak evidence - limited citations about automated evaluation reliability for instruction-following tasks
- Break condition: If automated evaluation shows poor consistency with human judgment or fails to capture important aspects of instruction compliance

## Foundational Learning

- Concept: Controllable Text Generation (CTG)
  - Why needed here: The benchmark evaluates LLMs' ability to generate text under various constraints, which is the core capability being measured
  - Quick check question: What distinguishes controllable text generation from standard text generation?

- Concept: Instruction-following paradigm
  - Why needed here: The benchmark uses natural language instructions instead of discrete control variables, representing the current state-of-the-art approach
  - Quick check question: How does instruction-following differ from traditional controllable text generation approaches?

- Concept: Automated evaluation methodology
  - Why needed here: The benchmark employs automated methods rather than human evaluation for scalability and efficiency
  - Quick check question: What are the key considerations when designing automated evaluation for instruction-following tasks?

## Architecture Onboarding

- Component map: Seed creation → Expansion → Diversification → Filtering → LLM inference → Automated evaluation → Performance analysis
- Critical path: Instruction diversification → LLM response generation → evaluation → accuracy calculation
- Design tradeoffs: Diversity vs. quality (more diverse instructions may be harder to evaluate), automation vs. human judgment (scalability vs. reliability), complexity vs. interpretability (complex instructions may be harder to analyze)
- Failure signatures: Low instruction diversity (high Rouge-L scores), poor LLM performance across all tasks (indicating evaluation issues), high variance in evaluation consistency (indicating unreliable automated methods)
- First 3 experiments:
  1. Compare instruction diversity metrics (Rouge-L scores) between expansion-only and expansion+diversification approaches
  2. Test automated evaluation consistency by comparing with human judgment on sampled instructions
  3. Validate evaluation methods by testing on known-good and known-bad LLM responses to ensure proper classification

## Open Questions the Paper Calls Out
None

## Limitations

- Evaluation Method Reliability: The automated evaluation methods, while efficient, may not capture all nuances of instruction compliance. The paper mentions conducting human judgment to verify reliability, but the extent and rigor of this validation are unclear.
- Instruction Diversity Quantification: While the paper claims to achieve high instruction diversity through their diversification process, the exact metrics and benchmarks for quantifying this diversity are not fully specified.
- Task Representation: The benchmark covers five main tasks (sentiment, topic, length, keyword, toxicity avoidance), but it may not fully represent the complexity and variety of real-world controllable text generation scenarios.

## Confidence

- High Confidence: The overall framework of CoDI-Eval, including the instruction generation pipeline and automated evaluation methods, is well-established and reasonable.
- Medium Confidence: The claim that commercial LLMs (GPT-4, ChatGPT) outperform open-source models on CoDI-Eval is supported by the results, but the exact magnitude of the performance gap and its implications require further investigation.
- Low Confidence: The specific effectiveness of the instruction diversification process in improving LLM performance is not fully validated, as the paper focuses more on the benchmark creation than on detailed performance analysis.

## Next Checks

1. **Human Evaluation Validation**: Conduct a more extensive human evaluation study to validate the consistency and reliability of the automated evaluation methods, especially for tasks where automated methods may struggle (e.g., complex multi-aspect constraints).

2. **Instruction Diversity Analysis**: Perform a detailed analysis of the instruction diversity achieved by the diversification process, comparing it against alternative methods and quantifying the impact on LLM performance.

3. **Real-World Application Testing**: Test the benchmark on a wider range of real-world controllable text generation tasks to assess its coverage and identify potential gaps or areas for improvement.