---
ver: rpa2
title: Robust Training Objectives Improve Embedding-based Retrieval in Industrial
  Recommendation Systems
arxiv_id: '2409.14682'
source_url: https://arxiv.org/abs/2409.14682
tags:
- retrieval
- recommendation
- learning
- training
- ssmtl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the effectiveness of self-supervised multitask
  learning (SSMTL) as a robust training objective for embedding-based retrieval (EBR)
  in large-scale industrial recommendation systems. The authors propose combining
  Canonical Correlation Analysis (CCA) and Masked Autoencoders (MAE) with a retrieval
  task to learn complementary embedding features.
---

# Robust Training Objectives Improve Embedding-based Retrieval in Industrial Recommendation Systems

## Quick Facts
- arXiv ID: 2409.14682
- Source URL: https://arxiv.org/abs/2409.14682
- Reference count: 40
- Primary result: SSMTL achieves up to 5.45% improvements in new friends made and 1.91% improvements in new friends made with cold-start users in industrial friend recommendation systems

## Executive Summary
This paper introduces Self-Supervised Multitask Learning (SSMTL) as a robust training objective for embedding-based retrieval (EBR) in large-scale industrial recommendation systems. The authors propose combining Canonical Correlation Analysis (CCA) and Masked Autoencoders (MAE) with a retrieval task to learn complementary embedding features. Through online A/B testing on a friend recommendation system with hundreds of millions of users, they observe statistically significant improvements in key metrics. The SSMTL approach achieves up to 5.45% improvements in new friends made and 1.91% improvements in new friends made with cold-start users compared to a single-task baseline. The results demonstrate that SSMTL can enhance retrieval performance in large-scale industrial settings by providing a more robust training objective that leverages complementary yet disjoint information from multiple SSL approaches.

## Method Summary
The authors propose a three-task self-supervised multitask learning framework that combines a retrieval task with two SSL methods: Canonical Correlation Analysis (CCA) and Masked Autoencoders (MAE). The Graph Attention Network (GAT) serves as the GNN encoder backbone, generating embeddings from augmented subgraphs containing k-hop neighborhoods around query users. Each task has its own loss function - categorical cross-entropy for retrieval, CCA loss for whitening decorrelation, and MAE loss for generative reconstruction. The final training objective is a weighted sum of these losses, with hyperparameters tuned to balance the different objectives. The approach is evaluated through online A/B testing on an industrial friend recommendation system with 800 million nodes and edges.

## Key Results
- Up to 5.45% improvements in new friends made compared to single-task baseline
- 1.91% improvements in new friends made with cold-start users
- Statistically significant increases across multiple friend recommendation metrics
- Demonstrated effectiveness in large-scale industrial setting with hundreds of millions of users

## Why This Works (Mechanism)

### Mechanism 1
Combining CCA and MAE SSL methods with link prediction in a multitask learning setting improves embedding quality for retrieval. The two SSL methods provide complementary yet disjoint information that augments the retrieval task without negative transfer. CCA maximizes correlation between two augmented views while decorrelating features within a view, while MAE reconstructs masked query node features.

### Mechanism 2
The weighted sum of losses from the retrieval task, CCA, and MAE creates a more robust training objective that improves embedding quality. By combining multiple loss functions, the model learns a more robust representation that is less sensitive to adverse conditions between tasks. The weighted sum allows the model to balance between the different objectives.

### Mechanism 3
The SSMTL approach is particularly effective for cold-start users, improving their engagement in the friend recommendation system. By learning a more robust representation through SSMTL, the model can better handle users with limited interaction history, improving the quality of recommendations for cold-start users.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are used to generate graph-aware embeddings for the EBR task, leveraging the topological information in the friend recommendation graph.
  - Quick check question: What is the main advantage of using GNNs over traditional embedding methods in the context of friend recommendation?

- Concept: Self-Supervised Learning (SSL)
  - Why needed here: SSL methods like CCA and MAE are used to learn complementary features that augment the retrieval task without requiring explicit labels.
  - Quick check question: How do the CCA and MAE SSL methods differ in their approach to learning useful features?

- Concept: Multitask Learning (MTL)
  - Why needed here: MTL is used to combine the retrieval task with the SSL tasks, creating a more robust training objective that improves embedding quality.
  - Quick check question: What is the main advantage of using MTL over training each task separately?

## Architecture Onboarding

- Component map: Graph data -> Subgraph sampling -> GAT encoder -> Task-specific heads (Retrieval, CCA, MAE) -> Weighted loss combination -> Model updates

- Critical path: 1) Input graph data is sampled and augmented for each task 2) GNN encoder generates embeddings from the augmented graphs 3) Task-specific heads process the embeddings for each SSL task 4) Losses are computed for each task 5) Weighted sum of losses is used for backpropagation and model updates

- Design tradeoffs: Using multiple SSL tasks increases model complexity and training time, but can improve embedding quality. The choice of SSL tasks should be carefully considered to ensure they are complementary and do not conflict with the retrieval task. The weights for the weighted sum of losses need to be tuned to balance the different objectives.

- Failure signatures: If the model performance degrades after adding SSL tasks, it may indicate negative transfer or conflicting objectives. If the model overfits to the training data, it may indicate that the SSL tasks are not providing sufficient regularization. If the model struggles to scale to large graphs, it may indicate that the SSL tasks are too computationally expensive.

- First 3 experiments: 1) Train the model with only the retrieval task to establish a baseline 2) Add the CCA SSL task and tune the weight to find the optimal balance with the retrieval task 3) Add the MAE SSL task and tune the weights for both SSL tasks to find the optimal combination with the retrieval task

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of SSMTL-based EBR scale when applied to recommendation systems with different types of graphs (e.g., bipartite user-item graphs vs. social networks)? The paper focuses on friend recommendation systems using social network graphs but doesn't explore other graph types.

### Open Question 2
What is the impact of varying the number of self-supervised tasks in the SSMTL framework on retrieval performance and computational efficiency? The paper combines two SSL methods (CCA and MAE) but doesn't explore the effect of adding or removing tasks.

### Open Question 3
How does SSMTL perform in cold-start scenarios for both users and items, and what specific challenges arise in these cases? The paper mentions "1.91% improvements in new friends made with cold-start users" but doesn't deeply explore cold-start item scenarios or detailed challenges.

## Limitations
- Evaluation is based on a single industrial friend recommendation system with specific graph characteristics, limiting generalizability to other recommendation domains
- SSL task selection (CCA + MAE) was empirically determined and may not be optimal for all EBR scenarios
- Hyperparameter tuning was performed on production data, limiting reproducibility

## Confidence
- Mechanism 1 (Complementary SSL tasks): Medium - Supported by theoretical rationale but limited ablation studies on task combinations
- Mechanism 2 (Weighted loss combination): Medium - Empirical results show improvements but sensitivity analysis on weight tuning is incomplete
- Mechanism 3 (Cold-start effectiveness): Medium - Improvements demonstrated but attribution to SSMTL vs. other factors unclear

## Next Checks
1. Conduct ablation studies testing different SSL task combinations (e.g., CCA-only, MAE-only, alternative SSL methods) to quantify complementary benefits
2. Perform sensitivity analysis on loss weight hyperparameters across different market segments to establish robustness bounds
3. Test the approach on a different recommendation domain (e.g., product or content recommendation) with different graph properties to assess generalizability