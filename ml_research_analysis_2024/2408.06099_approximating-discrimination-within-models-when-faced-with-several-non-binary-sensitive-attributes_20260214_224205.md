---
ver: rpa2
title: Approximating Discrimination Within Models When Faced With Several Non-Binary
  Sensitive Attributes
arxiv_id: '2408.06099'
source_url: https://arxiv.org/abs/2408.06099
tags:
- fairness
- sensitive
- correlation
- values
- davg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of measuring and mitigating
  discrimination in machine learning models when dealing with multiple sensitive attributes
  that have multiple possible values. The authors propose a new fairness measure called
  Harmonic Fairness Measure via Manifolds (HFM), which evaluates discrimination by
  computing distances between sets from a manifold perspective.
---

# Approximating Discrimination Within Models When Faced With Several Non-Binary Sensitive Attributes

## Quick Facts
- arXiv ID: 2408.06099
- Source URL: https://arxiv.org/abs/2408.06099
- Reference count: 40
- One-line primary result: Proposed Harmonic Fairness Measure via Manifolds (HFM) and approximation algorithms effectively measure and mitigate discrimination in ML models with multiple non-binary sensitive attributes.

## Executive Summary
This paper addresses the challenge of measuring and mitigating discrimination in machine learning models when dealing with multiple sensitive attributes that have multiple possible values. The authors propose a new fairness measure called Harmonic Fairness Measure via Manifolds (HFM), which evaluates discrimination by computing distances between sets from a manifold perspective. To make HFM computationally efficient, they develop two approximation algorithms: ApproxDist for single sensitive attributes and ExtendDist for multiple attributes. The paper provides theoretical analysis of the approximation algorithms' effectiveness under certain assumptions and demonstrates through experiments that HFM and the approximation methods are both effective and efficient. The results show that HFM correlates well with prediction performance differences and outperforms baseline fairness measures in capturing bias.

## Method Summary
The authors propose a fairness measure called Harmonic Fairness Measure via Manifolds (HFM) that evaluates discrimination by computing distances between sets from a manifold perspective. For efficiency, they develop two approximation algorithms: ApproxDist for single sensitive attributes and ExtendDist for multiple attributes. The method partitions datasets into disjoint subsets based on sensitive attribute values and computes distances between these subsets in feature space. The approximation algorithms project high-dimensional data onto one-dimensional subspaces using random orthogonal vectors, then only compare nearby points after sorting by projection values, reducing computational complexity from O(n²) to O(n log n).

## Key Results
- HFM effectively captures discrimination in models with multiple non-binary sensitive attributes
- ApproxDist and ExtendDist approximation algorithms achieve significant computational efficiency (O(n log n) vs O(n²))
- HFM correlates well with prediction performance differences and outperforms baseline fairness measures in capturing bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed Harmonic Fairness Measure via Manifolds (HFM) can effectively capture discrimination in models when dealing with multiple non-binary sensitive attributes by measuring distances between sets from a manifold perspective.
- Mechanism: The method partitions the dataset into disjoint subsets based on sensitive attribute values and computes distances between these subsets in feature space. By taking the maximum or average of these distances, it quantifies the disparity between groups.
- Core assumption: Instances with the same sensitive attribute values form meaningful clusters or manifolds in the feature space, and distance between these manifolds reflects discrimination.
- Evidence anchors:
  - [abstract] "we propose a fairness measure based on distances between sets from a manifold perspective, named as 'Harmonic Fairness measure via Manifolds (HFM)' with two optional versions"
  - [section] "if we view the instances (with the same sensitive attributes) as data points on certain manifolds, the manifold representing members from the marginalised/unprivileged group(s) is supposed to be as close as possible to that representing members from the privileged group"
- Break condition: If the data does not form well-separated manifolds for different attribute values, or if the sensitive attributes are not meaningful predictors of outcomes, the distance measure may not reflect true discrimination.

### Mechanism 2
- Claim: The approximation algorithms (ApproxDist and ExtendDist) can efficiently estimate the distance between sets while maintaining accuracy, reducing computational complexity from O(n²) to O(n log n).
- Mechanism: The algorithms project high-dimensional data onto one-dimensional subspaces using random orthogonal vectors, then only compare nearby points after sorting by projection values. This leverages the observation that similar points tend to be closer after projection.
- Core assumption: After projection, points that are close in the original space remain relatively close in the projected space, so only nearby points need to be compared.
- Evidence anchors:
  - [section] "we observe that the distance between similar data points tends to be closer than others after projecting them onto a general one-dimensional linear subspace"
  - [section] "the time complexity of sorting in line 2 of Algorithm 3 could reach O(n log n), we could get the computational complexity of Algorithm 3 as follows: ... the time complexity of computing the distance is then down to O(n log n)"
- Break condition: If the data distribution is such that projection causes distant points to appear nearby (high distortion), the approximation may miss true distances between sets.

### Mechanism 3
- Claim: The choice of using harmonic ratio (log of discrimination over baseline) makes HFM sensitive to both individual and group fairness aspects while being comparable across different datasets and models.
- Mechanism: By computing the ratio of discrimination with model predictions to discrimination in the data itself, HFM isolates the additional bias introduced by the learning algorithm rather than capturing pre-existing data bias.
- Core assumption: The baseline discrimination in the data (without model intervention) can be meaningfully computed and serves as a valid reference point.
- Evidence anchors:
  - [section] "We remark that Da(S), Davg a (S) reflect the biases from the data and that Df,a(S), Davg f,a(S) reflect the extra biases from the learning algorithm"
  - [section] "Then the following values could be used to reflect the fairness degree of this classifier, that is, df(f) = log(Df,a(S)/Da(S))"
- Break condition: If the baseline discrimination is zero or near-zero, the harmonic ratio becomes unstable or undefined, making HFM unreliable.

## Foundational Learning

- Concept: Manifold learning and distance metrics in high-dimensional spaces
  - Why needed here: The core fairness measure relies on understanding distances between sets in feature space, which requires knowledge of how distances behave in high dimensions and how manifolds represent data structure
  - Quick check question: What is the curse of dimensionality and how does it affect distance-based measures in high-dimensional spaces?

- Concept: Computational complexity analysis and approximation algorithms
  - Why needed here: The paper's main contribution includes developing approximation algorithms to make the fairness measure computationally tractable, requiring understanding of time complexity and approximation guarantees
  - Quick check question: How does the time complexity change from O(n²) to O(n log n) in the proposed approximation algorithms?

- Concept: Fairness metrics and their compatibility issues
  - Why needed here: The work positions HFM among existing fairness measures and demonstrates its advantages, requiring understanding of different fairness definitions and their trade-offs
  - Quick check question: Why can't we satisfy all fairness metrics simultaneously in machine learning models?

## Architecture Onboarding

- Component map: Data preprocessing -> Core computation (ApproxDist/ExtendDist) -> HFM calculation -> Evaluation against baselines
- Critical path: Data partitioning → Distance computation (ApproxDist/ExtendDist) → HFM calculation → Evaluation against baselines
- Design tradeoffs: Computational efficiency vs. approximation accuracy; sensitivity to data imbalance vs. robustness to noise
- Failure signatures: High variance in approximation results; poor correlation with known bias; runtime exceeding expectations despite O(n log n) complexity
- First 3 experiments:
  1. Verify that ApproxDist produces highly correlated results with direct computation on a small dataset with known structure
  2. Test the effect of varying m1 and m2 hyperparameters on both accuracy and runtime
  3. Compare HFM correlation with performance metrics against baseline fairness measures on a real dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of ApproxDist and ExtendDist scale with the number of sensitive attribute values, particularly in high-dimensional spaces?
- Basis in paper: [explicit] The authors discuss that the time cost of the approximation algorithms increases significantly when the number of optional values within a sensitive attribute is large, and they integrate parallel computing to mitigate this.
- Why unresolved: The paper provides empirical results on datasets with up to six sub-groups under a sensitive attribute but does not explore the limits of scalability or provide a theoretical analysis of how the algorithms perform as the number of attribute values grows very large.
- What evidence would resolve it: Experiments on datasets with a wide range of sensitive attribute values (e.g., 10, 50, 100+) and a theoretical analysis of the time complexity and approximation accuracy in high-dimensional spaces would provide clarity.

### Open Question 2
- Question: What is the impact of the choice of distance metric (other than Euclidean) on the effectiveness of HFM and its approximation algorithms?
- Basis in paper: [inferred] The authors use the Euclidean metric in their experiments but mention that any two metrics derived from norms on the Euclidean space are equivalent, suggesting that other metrics could be used.
- Why unresolved: The paper does not explore the impact of using different distance metrics on the fairness measures or the approximation algorithms, leaving the question of whether the choice of metric affects the results open.
- What evidence would resolve it: Comparative experiments using different distance metrics (e.g., Manhattan, Mahalanobis) and an analysis of how the choice of metric influences the fairness measures and approximation accuracy would provide insights.

### Open Question 3
- Question: How do the approximation algorithms handle noise in the data, and what is their robustness to outliers?
- Basis in paper: [explicit] The authors mention that the average distance measures are more resilient against the influence of data noise, but they do not provide a detailed analysis of the robustness of the approximation algorithms to noise and outliers.
- Why unresolved: The paper does not include experiments or theoretical analysis on the robustness of ApproxDist and ExtendDist to noisy data or outliers, which is crucial for understanding their reliability in real-world applications.
- What evidence would resolve it: Experiments on datasets with varying levels of noise and outliers, along with an analysis of the approximation accuracy and robustness under these conditions, would provide clarity on the algorithms' performance.

## Limitations
- The effectiveness of HFM depends on the assumption that sensitive attributes form meaningful manifolds in feature space, which may not hold in all datasets
- The approximation algorithms' performance is sensitive to hyperparameter choices (m1, m2) with no systematic selection guidelines provided
- The framework is primarily designed for classification tasks and may not generalize well to regression or unsupervised learning scenarios

## Confidence
- HFM effectiveness as fairness measure: High (supported by strong experimental correlations with performance disparities)
- Approximation algorithm efficiency: High (complexity analysis is rigorous, though empirical runtime validation limited)
- Manifold distance assumption: Medium (theoretical justification provided but limited empirical validation of underlying assumptions)

## Next Checks
1. Test HFM sensitivity on datasets where sensitive attributes have minimal predictive power for outcomes to verify that distance measures don't capture spurious correlations
2. Conduct ablation studies varying m1 and m2 across multiple datasets to establish robust hyperparameter selection guidelines
3. Evaluate approximation accuracy on synthetic datasets with known manifold structures to validate theoretical guarantees under controlled conditions