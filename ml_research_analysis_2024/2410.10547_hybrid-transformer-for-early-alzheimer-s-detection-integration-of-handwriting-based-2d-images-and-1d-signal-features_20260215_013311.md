---
ver: rpa2
title: 'Hybrid Transformer for Early Alzheimer''s Detection: Integration of Handwriting-Based
  2D Images and 1D Signal Features'
arxiv_id: '2410.10547'
source_url: https://arxiv.org/abs/2410.10547
tags:
- handwriting
- attention
- feature
- features
- alzheimer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses early Alzheimer\u2019s disease detection\
  \ using handwriting analysis. It proposes a hybrid transformer model integrating\
  \ 2D handwriting images and 1D signal features."
---

# Hybrid Transformer for Early Alzheimer's Detection: Integration of Handwriting-Based 2D Images and 1D Signal Features

## Quick Facts
- arXiv ID: 2410.10547
- Source URL: https://arxiv.org/abs/2410.10547
- Reference count: 40
- Primary result: F1-score of 90.32% and accuracy of 90.91% in Task 8 ('L' writing)

## Executive Summary
This study proposes a hybrid transformer model for early Alzheimer's disease detection using handwriting analysis. The model integrates 2D handwriting images with 1D dynamic signal features through a novel hybrid similarity and difference attention mechanism, multi-scale hybrid block, and template contrastive loss function. Experiments on the DARWIN dataset demonstrate superior performance compared to state-of-the-art methods, achieving an F1-score of 90.32% and accuracy of 90.91% on the 'L' writing task, surpassing previous best results by 4.61% and 6.06% respectively.

## Method Summary
The HSDA-MS model combines 2D handwriting images and 1D signal features through a hybrid transformer architecture. It uses a gated hybrid attention mechanism that combines similarity attention (capturing global patterns) with difference attention (capturing local variations). A multi-scale hybrid block processes features at different resolutions through convolution and downsampling operations. The template contrastive loss function explicitly learns the difference between positive and negative samples by maintaining dynamically updated template vectors. The model is trained using SGD with cosine annealing scheduler and early stopping on the DARWIN-RAW dataset.

## Key Results
- Achieved F1-score of 90.32% and accuracy of 90.91% on Task 8 ('L' writing)
- Outperformed previous state-of-the-art methods by 4.61% in F1-score and 6.06% in accuracy
- Ablation studies confirmed the effectiveness of hybrid attention, multi-scale blocks, and template contrastive loss

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid Similarity and Difference Attention captures both global and local handwriting patterns better than unimodal models.
- **Mechanism:** The gating mechanism combines similarity attention (global context via dot-product of queries and keys) with difference attention (local variations via absolute differences between queries and keys).
- **Core assumption:** Handwriting deterioration in AD manifests both in global patterns and local variations.
- **Evidence anchors:** Abstract mentions gated mechanism combining similarity and difference attention; section describes similarity attention capturing global patterns and difference attention learning subtle variations.
- **Break condition:** If AD-related handwriting changes are predominantly either global or local but not both, the hybrid approach may introduce unnecessary complexity.

### Mechanism 2
- **Claim:** Multi-scale hybrid block enables learning features at different resolutions, capturing both fine motor details and overall writing patterns.
- **Mechanism:** The block processes 2D handwriting images and 1D signal features independently through convolution and downsampling operations, generating multi-scale representations that are then concatenated.
- **Core assumption:** Handwriting changes in AD occur at multiple scales - both in individual stroke characteristics and in overall writing patterns.
- **Evidence anchors:** Section explains multi-scale feature fusion leverages information from different scales to extract richer features.
- **Break condition:** If AD-related handwriting changes are scale-invariant, the multi-scale approach may be redundant.

### Mechanism 3
- **Claim:** Template contrastive loss improves classification by explicitly learning the difference between positive and negative samples in high-dimensional space.
- **Mechanism:** The loss function combines cross-entropy loss with contrastive loss that measures the distance between sample features and template vectors for positive and negative classes.
- **Core assumption:** The feature space can be structured such that AD patients and healthy controls form distinct clusters with clear boundaries.
- **Evidence anchors:** Section states template contrastive loss boosts the model's ability to learn more robust and discriminative representations by explicitly modeling similarity relationships.
- **Break condition:** If the feature distributions of AD patients and healthy controls significantly overlap in the learned feature space, contrastive loss may not improve separation.

## Foundational Learning

- **Concept:** Transformer architecture and self-attention mechanisms
  - Why needed here: The model uses a Transformer-based architecture to process both 2D images and 1D signals
  - Quick check question: What is the difference between similarity attention (dot-product) and difference attention (absolute difference) in this context?

- **Concept:** Multi-modal data fusion
  - Why needed here: The model integrates 2D handwriting images with 1D signal features
  - Quick check question: How does the gating mechanism in the hybrid attention block control the contribution of similarity vs. difference attention?

- **Concept:** Contrastive learning and metric learning
  - Why needed here: The template contrastive loss function is a key component that improves classification by learning discriminative features
  - Quick check question: How does the dynamic update of template vectors during training improve the model's ability to separate classes?

## Architecture Onboarding

- **Component map:** Image embedding → Signal embedding → Hybrid attention block → Multi-scale hybrid block → Classification head with template contrastive loss
- **Critical path:** Image/Signal embedding → Hybrid attention → Multi-scale hybrid block → Classification head with template contrastive loss
- **Design tradeoffs:** Shallower architecture chosen due to small dataset size; hybrid attention mechanism adds complexity but improves performance; dynamic template updates improve adaptation but may introduce instability
- **Failure signatures:** Poor performance on fine motor tasks may indicate insufficient capture of local variations; failure to generalize across different handwriting tasks may indicate overfitting; high variance in results across folds may indicate sensitivity to initialization
- **First 3 experiments:**
  1. Ablation study: Remove hybrid attention mechanism to test contribution of similarity/difference attention combination
  2. Scale analysis: Test model performance with single-scale vs. multi-scale processing to validate multi-scale approach
  3. Loss function comparison: Replace template contrastive loss with standard cross-entropy to measure its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed hybrid transformer model compare to other transformer-based models when applied to different neurodegenerative diseases beyond Alzheimer's?
- Basis in paper: The paper mentions future work could explore applying the model to other neurodegenerative diseases
- Why unresolved: The current study focuses specifically on Alzheimer's disease detection
- What evidence would resolve it: Experiments applying the same model architecture to datasets of Parkinson's disease, multiple sclerosis, or other neurodegenerative conditions

### Open Question 2
- Question: What is the impact of varying the weighting parameter λ in the template contrastive loss function on model performance across different handwriting tasks?
- Basis in paper: The paper mentions λ is an adjustable hyperparameter but doesn't explore how varying this affects performance
- Why unresolved: The paper uses a fixed λ value of 0.8 but doesn't provide an ablation study or sensitivity analysis
- What evidence would resolve it: A systematic study testing multiple values of λ across all 25 handwriting tasks

### Open Question 3
- Question: How does the model's performance change when using different scales of feature fusion or when removing the multi-scale hybrid block entirely?
- Basis in paper: The ablation study shows performance drops when removing the multi-scale hybrid block, but doesn't explore variations in the number of scales
- Why unresolved: While the current multi-scale design uses three layers, the paper doesn't investigate whether more or fewer scales would improve performance
- What evidence would resolve it: Experiments testing different numbers of scales and different fusion mechanisms across all handwriting tasks

## Limitations

- Small dataset size (174 participants) may limit generalizability and model robustness
- Focus on handwritten 'L' characters in Task 8 may not represent broader AD detection contexts
- Hybrid attention mechanism introduces complexity that may not be necessary for all AD-related handwriting changes
- Template contrastive loss function's effectiveness relies on the assumption that AD and healthy control samples form distinct clusters

## Confidence

- **High Confidence:** Superior performance on Task 8 (F1-score 90.32%, accuracy 90.91%) is well-supported by experimental results and baseline comparisons
- **Medium Confidence:** Effectiveness of hybrid attention and multi-scale blocks is supported by ablation studies but specific contributions remain uncertain
- **Low Confidence:** Template contrastive loss function's contribution lacks direct corpus support and the assumption about feature space clustering is not empirically validated

## Next Checks

1. **Scale Analysis:** Conduct experiments comparing single-scale vs. multi-scale processing to validate the necessity of the multi-scale hybrid block

2. **Loss Function Comparison:** Replace the template contrastive loss with standard cross-entropy and other metric learning losses to measure its specific contribution to performance

3. **Generalization Testing:** Evaluate the model's performance across all 25 handwriting tasks in the DARWIN dataset to assess generalizability and identify task-specific limitations