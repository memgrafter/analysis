---
ver: rpa2
title: Understanding the role of FFNs in driving multilingual behaviour in LLMs
arxiv_id: '2404.13855'
source_url: https://arxiv.org/abs/2404.13855
tags:
- layers
- activation
- languages
- across
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the multilingual behavior of large language
  models (LLMs), focusing on the Feed-Forward Networks (FFNs) that make up most of
  their parameters. The authors examine four XGLM models (568M, 1.7B, 2.9B, 7.5B parameters)
  trained on 500B tokens from 30 languages.
---

# Understanding the role of FFNs in driving multilingual behaviour in LLMs

## Quick Facts
- arXiv ID: 2404.13855
- Source URL: https://arxiv.org/abs/2404.13855
- Authors: Sunit Bhattacharya; Ondřej Bojar
- Reference count: 7
- This paper analyzes multilingual behavior in XGLM models by examining Feed-Forward Networks across layers, revealing distinct patterns of language processing and identifying "overlayerization" in the 2.9B model.

## Executive Summary
This study investigates how multilingual large language models process different languages across their architecture, focusing specifically on Feed-Forward Networks (FFNs). By analyzing four XGLM models with varying parameter sizes (568M to 7.5B) trained on 500B tokens from 30 languages, the authors introduce novel metrics to quantify activation patterns and representational similarities. The research reveals that early detector layers are multilingual and process shallow features, while later layers become language-specific, with the 2.9B model showing unusual "overlayerization" behavior where increased depth degrades performance.

## Method Summary
The authors examine XGLM models by feeding prefixes of parallel sentences from multiple languages through the network and collecting intermediate representations from FFN layers. They introduce two novel metrics: activation flatness (measuring entropy of normalized neuron activations) and representational distance (minimum distance between prefixes across languages). The analysis separates detector and combinator sublayers, examining activation patterns, sparsity, and cross-language similarity across all layers. The study uses WMT2 test sets containing parallel sentences in English paired with Czech, French, German, and Hindi.

## Key Results
- Early detector layers are multilingual and process shallow features, while later layers become language-specific
- Detector activations become more peaked with depth, combinator activations peak around layer 20 regardless of model size
- The 2.9B model shows unusual "overlayerization" behavior suggesting performance degradation from too many layers
- Final layers are language-specific for most models except the 2.9B model, which shows unique patterns

## Why This Works (Mechanism)
The study's methodology works by systematically analyzing intermediate representations in FFN layers to understand how multilingual processing emerges. By using entropy-based metrics to quantify activation patterns and distance metrics to compare cross-language representations, the authors can identify where and how languages diverge in processing. The approach of examining prefixes rather than complete sentences allows for controlled analysis of how different languages are processed at similar syntactic positions, revealing architectural patterns that emerge from training on balanced multilingual data.

## Foundational Learning
- **Activation flatness metric**: Measures entropy of normalized neuron activations to quantify how peaked or uniform activations are across neurons; needed to understand specialization patterns in FFNs
  - Quick check: Verify entropy calculations produce values between 0 and 1 for normalized distributions
- **Representational distance**: Computes minimum distance between prefixes across languages to measure similarity of processing; needed to quantify multilingual versus language-specific behavior
  - Quick check: Confirm distance values are consistent across different language pairs and prefix lengths
- **Detector vs combinator sublayers**: FFNs consist of two linear layers separated by ReLU; needed to analyze different functional roles in multilingual processing
  - Quick check: Verify sublayer separation by examining weight matrices between layers
- **Overlayerization phenomenon**: Performance degradation from adding too many layers without proportional parameter adjustments; needed to understand architectural scaling limits
  - Quick check: Compare performance curves against layer count to identify inflection points
- **Subword tokenization**: XGLM uses specific tokenization strategies; needed for consistent prefix preparation across languages
  - Quick check: Ensure subword boundaries align properly across different languages
- **Parallel corpus alignment**: WMT2 test sets contain aligned sentences across languages; needed for controlled comparison of similar syntactic structures
  - Quick check: Verify sentence alignment accuracy and coverage across all language pairs

## Architecture Onboarding

**Component Map**: Input -> Embedding -> FFN Layers (Detector -> ReLU -> Combinator) -> Output, where FFNs constitute ~60% of parameters

**Critical Path**: Input prefix → Embedding layer → Sequential FFN layers → Output generation, with analysis focusing on intermediate detector and combinator sublayers

**Design Tradeoffs**: Deeper architectures provide more capacity but risk overlayerization; balanced multilingual training requires careful sampling but may not improve under-resourced language generation; sparse final layer representations enable specialization but reduce cross-language transfer

**Failure Signatures**: Memory constraints when collecting model snapshots from large models; incorrect subword tokenization leading to mismatched prefix representations; performance degradation in intermediate layers suggesting architectural imbalance

**First Experiments**: 
1. Test subword tokenization consistency across all four target languages to ensure comparable prefix representations
2. Verify activation flatness calculations produce expected entropy values (0-1 range) for normalized distributions
3. Confirm representational distance computations yield consistent values across different prefix lengths and language pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "overlayerization" phenomenon observed in the 2.9B model represent a general problem for multilingual LLMs with increased depth?
- Basis in paper: The authors explicitly identify "overlayerization" in the 2.9B model, noting that increasing layer depth without corresponding adjustments to other parameters may degrade model performance.
- Why unresolved: The study only examined four XGLM models with specific architectural parameters. It's unclear whether this phenomenon would occur in other multilingual LLM architectures or if it could be mitigated through architectural adjustments.
- What evidence would resolve it: Testing other multilingual LLM architectures with varying layer depths while keeping other parameters constant, or systematically varying multiple architectural parameters simultaneously.

### Open Question 2
- Question: Are the sparse representations observed in final layers language-specific or task-specific?
- Basis in paper: The paper observes that final layers show sparser representations and higher representational distance between languages, but doesn't clearly distinguish whether this sparsity is due to language-specific processing or task-specific processing.
- Why unresolved: The analysis focuses on multilingual behavior but doesn't control for task differences that might occur in final layers during generation.
- What evidence would resolve it: Comparing activation patterns in final layers across different tasks (translation, classification, generation) within the same language.

### Open Question 3
- Question: How does the balanced language representation in training data affect the emergence of multilingual versus language-specific neurons?
- Basis in paper: The authors note that "over-sampling under-resourced languages does not improve the model's capabilities to generate tokens in those languages" and observe different sparsity patterns between well-represented and under-represented languages.
- Why unresolved: The paper observes correlations but doesn't establish causal relationships between training data balance and neural representation patterns.
- What evidence would resolve it: Training multiple models with systematically varied language balance ratios while keeping other parameters constant and analyzing the resulting neuron specialization patterns.

## Limitations
- The study only examines XGLM models, limiting generalizability to other multilingual architectures like mT5 or mBERT
- The analysis focuses exclusively on FFN sublayers without investigating attention mechanism contributions to multilingual processing
- The "overlayerization" phenomenon in the 2.9B model requires additional validation to rule out experimental artifacts

## Confidence
- High confidence: Observed patterns of activation flatness across layers and languages, representational distance measurements, and sparsity patterns in combinator sublayers
- Medium confidence: Claims about early detector layers being multilingual and processing shallow features, as these rely on specific interpretations of activation patterns
- Low confidence: Claims about "overlayerization" in the 2.9B model and the causal relationship between layer count and multilingual performance, due to limited model comparisons

## Next Checks
1. Replicate the analysis on a different multilingual model family (e.g., mT5 or mBERT) to verify if the observed activation patterns and layer-wise processing characteristics generalize beyond XGLM
2. Conduct controlled experiments with the 2.9B model by systematically removing layers to test whether the "overlayerization" hypothesis holds and identify the optimal layer count for multilingual performance
3. Extend the analysis to include attention mechanism contributions by examining cross-attention patterns across languages, providing a more complete picture of multilingual processing beyond just FFN sublayers