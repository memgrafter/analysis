---
ver: rpa2
title: 'What makes math problems hard for reinforcement learning: a case study'
arxiv_id: '2408.15332'
source_url: https://arxiv.org/abs/2408.15332
tags:
- presentations
- presentation
- length
- search
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates why certain mathematical problems are challenging
  for reinforcement learning (RL) by studying the Andrews-Curtis conjecture. Using
  this combinatorial group theory problem, the authors explore how RL agents struggle
  with rare instances carrying high rewards.
---

# What makes math problems hard for reinforcement learning: a case study

## Quick Facts
- arXiv ID: 2408.15332
- Source URL: https://arxiv.org/abs/2408.15332
- Authors: Ali Shehper; Anibal M. Medina-Mardones; Lucas Fagan; Bartłomiej Lewandowski; Angus Gruen; Yang Qiu; Piotr Kucharski; Zhenghan Wang; Sergei Gukov
- Reference count: 8
- Primary result: Reinforcement learning struggles with mathematical problems requiring long, sparse reward sequences, requiring new adaptive algorithms

## Executive Summary
This paper investigates why certain mathematical problems present fundamental challenges for reinforcement learning by studying the Andrews-Curtis conjecture from combinatorial group theory. The authors discover that problems with ultra-sparse rewards and long horizons create intrinsic difficulties for RL agents, as successful paths are exponentially rare compared to dead-end paths. They develop a principled topological hardness measure using persistent homology and demonstrate how adaptive algorithms with dynamically learned supermoves can improve performance. The work bridges theoretical mathematics and machine learning, uncovering new mathematical results while revealing fundamental limitations of current RL approaches for complex mathematical reasoning.

## Method Summary
The study combines classical search algorithms (BFS, greedy search), reinforcement learning (Proximal Policy Optimization), Transformer language modeling, and topological data analysis to tackle the Andrews-Curtis conjecture. The authors generate balanced presentations of the trivial group, implement AC moves for manipulating these presentations, and train PPO agents with adaptive horizons. They analyze successful paths to identify useful supermoves, augment the action space, and measure problem hardness through persistent homology of the AC graph. The methodology includes computing neighborhood sizes, training classifiers to predict solvability, and analyzing the relationship between topological features and problem difficulty.

## Key Results
- Reinforcement learning agents struggle with problems requiring long sequences of moves due to sparse reward distributions
- Topological data analysis provides a principled hardness measure for mathematical problems through persistent homology
- Dynamic adaptation of action spaces with supermoves significantly improves performance on hard instances
- The study solves previously intractable presentations and discovers new mathematical results about infinite families of potential counterexamples
- PPO-unsolved presentations concentrate in specific neighborhood sizes, revealing structure in problem difficulty distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Problems with ultra-sparse rewards and long horizons are fundamentally hard for RL because the agent cannot effectively assign credit for actions that contribute to rare, distant successes.
- Mechanism: In problems like the Andrews-Curtis conjecture, valid solution paths are exponentially rare compared to dead-end paths. When rewards only appear after very long sequences of moves (sometimes hundreds of steps), the agent struggles to learn which specific actions in the sequence were beneficial versus irrelevant.
- Core assumption: The distribution of successful paths is heavily skewed toward extremely long sequences, making them vanishingly rare in random exploration.
- Evidence anchors:
  - [abstract]: "rare instances carrying disproportionately high rewards" and "ultra-sparse rewards and long horizons present fundamental challenges"
  - [section 4.4]: "If solving a mathematical problem required finding a path of length L, say with L = 106, an RL agent would be pretty much out of luck under circumstances of a typical hard search problem, where the number of successful paths is exponentially suppressed by L"
  - [corpus]: Weak evidence - no direct corpus citations for this mechanism
- Break condition: If the problem space contains a substantial fraction of relatively easy instances that can be solved with short action sequences, the agent can learn useful patterns before needing to tackle the hard, long-horizon cases.

### Mechanism 2
- Claim: Dynamic adaptation of the action space (adding "supermoves") is essential for solving problems where theoretical lower bounds on solution length exceed practical computational limits.
- Mechanism: By identifying frequently occurring subsequences of successful AC-trivialization paths and adding them as composite actions, the agent can effectively jump across multiple elementary steps at once. This reduces the effective horizon length the agent must plan over.
- Core assumption: The distribution of successful paths contains identifiable patterns or "motifs" that can be abstracted into higher-level actions.
- Evidence anchors:
  - [abstract]: "propose algorithmic enhancements and a topological hardness measure" and "problems with ultra-sparse rewards and long horizons"
  - [section 5.1]: "it is clear that direct approach with fixed size steps is not going to succeed, unless the problem is easy and a large fraction of long paths meets the desired criteria. In order to reach extraordinary path lengths, one must allow progressively longer sequences of elementary moves to be added to the action space"
  - [corpus]: Weak evidence - no direct corpus citations for this mechanism
- Break condition: If the problem space is truly unstructured with no recurring patterns, identifying useful supermoves becomes impossible, and the action space remains intractable.

### Mechanism 3
- Claim: Learning a problem-specific hardness measure enables intelligent allocation of computational resources to the most informative instances.
- Mechanism: By analyzing the distribution of problem difficulty (e.g., through topological data analysis of the AC graph), the agent can prioritize training on instances that are neither too easy (already solved) nor too hard (unsolvable with current capabilities), but represent the frontier of learnable difficulty.
- Core assumption: There exists a meaningful gradient in problem difficulty that correlates with the information gained from attempting to solve each instance.
- Evidence anchors:
  - [abstract]: "problems with ultra-sparse rewards and long horizons present fundamental challenges for RL, requiring new algorithmic approaches that can learn and adapt to problem hardness distributions"
  - [section 7]: "We propose that the multiset consisting of all such pairs ( birth, death), excepting the component defined by the trivial presentation, serves as a principled hardness measure for the AC trivialization problem"
  - [section 8.1]: "we analyze the relationship between these labels and the sizes of their respective AC neighborhoods" and "the PPO-unsolved presentations are concentrated within a few specific neighborhood sizes"
  - [corpus]: Weak evidence - no direct corpus citations for this mechanism
- Break condition: If the problem space is entirely homogeneous in difficulty or if difficulty is orthogonal to the features the agent can measure, then hardness-based prioritization provides no advantage.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire reinforcement learning framework is built on MDP theory - understanding states, actions, rewards, and transitions is fundamental to grasping how PPO operates on the AC conjecture problem.
  - Quick check question: In the AC trivialization problem, what constitutes a "state" and what constitutes an "action"?

- Concept: Credit Assignment Problem
  - Why needed here: With sparse rewards appearing only after hundreds of moves, the agent must determine which actions in a long sequence contributed to eventual success - this is the core challenge being addressed.
  - Quick check question: Why does a reward appearing after 200 moves make it difficult for an RL agent to learn which specific moves were important?

- Concept: Topological Data Analysis (TDA)
  - Why needed here: The paper uses persistent homology and barcode analysis to quantify problem hardness - understanding these concepts is crucial for grasping the global hardness measure.
  - Quick check question: What does a "bar" in the barcode representation represent in terms of problem components appearing and disappearing as the graph filtration parameter increases?

## Architecture Onboarding

- Component map: Miller-Schupp presentations -> PPO agent (actor-critic network) -> Successful paths -> Topological analysis (persistent homology) -> Hardness measure -> Supermove selection -> Augmented action space -> Enhanced PPO training
- Critical path: Generate Miller-Schupp presentations → Train PPO agent with adaptive horizon → Collect successful paths → Analyze for supermoves → Augment action space → Repeat training → Measure hardness via persistent homology
- Design tradeoffs: Fixed horizon vs. adaptive horizon length (computational efficiency vs. solving capability), small neural networks vs. larger ones (speed vs. performance), local neighborhood features vs. global topological features (explainability vs. predictive power)
- Failure signatures: Agent gets stuck in local optima with no progress on hardest instances, supermove selection fails to identify useful patterns, hardness measures don't correlate with actual solvability, Transformer embeddings don't distinguish solved vs unsolved presentations
- First 3 experiments:
  1. Implement PPO with fixed horizon on a subset of Miller-Schupp presentations and measure success rate vs. horizon length
  2. Add a simple supermove selection mechanism that identifies the most frequent action subsequences from successful paths and measure impact on performance
  3. Compute neighborhood sizes for all presentations and train a simple classifier to predict PPO-solved vs PPO-unsolved labels, measuring feature importance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a principled hardness measure for mathematical problems be developed that captures both local and global structural features?
- Basis in paper: [explicit] The paper introduces a topological hardness measure based on persistent reduced homology, but notes this is only an approximation for finite graphs
- Why unresolved: The current measure is computationally limited to small problem instances and requires further theoretical development to extend to general mathematical domains
- What evidence would resolve it: A comprehensive theoretical framework that connects topological features to problem difficulty across diverse mathematical domains, validated through extensive computational experiments

### Open Question 2
- Question: What is the theoretical lower bound on the number of AC moves required for elementary M-transformations in the Andrews-Curtis conjecture?
- Basis in paper: [explicit] The paper mentions elementary M-transformations as promising supermoves but notes they are infinite in number and lack a bound on constituent AC moves
- Why unresolved: Current proofs of substitution and removal moves don't provide bounds on the number of elementary AC moves required
- What evidence would resolve it: A proof establishing either a tight upper bound on AC moves needed for elementary M-transformations or a counterexample showing such bounds don't exist

### Open Question 3
- Question: Do all balanced presentations derived from Wirtinger presentations of unknot diagrams via stable AC moves remain AC-trivial?
- Basis in paper: [explicit] The paper proves AC-triviality for presentations with simple conjugation forms but conjectures this extends to all presentations obtained through stable AC moves
- Why unresolved: The proof technique relies on presentations having a specific structure that may not be preserved under general stable AC moves
- What evidence would resolve it: Either a complete proof showing all such presentations are AC-trivial or an explicit counterexample presentation that remains stably AC-trivial but is not AC-trivial

## Limitations

- The study focuses on a single mathematical problem (Andrews-Curtis conjecture), limiting generalizability to other mathematical domains
- Computational resources required for topological data analysis and large-scale neighborhood searches may limit practical applicability
- The effectiveness of adaptive algorithms shows only modest improvements, suggesting fundamental limitations remain unresolved

## Confidence

- **High Confidence**: The core observation that sparse rewards and long horizons create fundamental challenges for RL agents
- **Medium Confidence**: The proposed topological hardness measure as a principled way to quantify problem difficulty
- **Medium Confidence**: The effectiveness of supermoves in reducing effective horizon length

## Next Checks

1. **Cross-domain validation**: Apply the topological hardness measure to at least two other mathematical problem domains (e.g., graph theory conjectures, number theory problems) to test its generalizability beyond the Andrews-Curtis conjecture

2. **Controlled ablation study**: Systematically remove components of the adaptive algorithm (e.g., disable supermoves, fix horizon length) to quantify their individual contributions to solving performance, particularly for the hardest instances

3. **Alternative problem distributions**: Generate synthetic mathematical problems with controlled difficulty distributions to test whether the proposed mechanisms (sparse rewards, long horizons) consistently predict RL performance across different problem structures