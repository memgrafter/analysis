---
ver: rpa2
title: 'Noro: Noise-Robust One-shot Voice Conversion with Hidden Speaker Representation
  Learning'
arxiv_id: '2411.19770'
source_url: https://arxiv.org/abs/2411.19770
tags:
- speaker
- reference
- speech
- noro
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of voice conversion (VC) performance
  degradation in real-world scenarios due to noise in reference speeches. The proposed
  solution, Noro, is a noise-robust one-shot VC system featuring a dual-branch reference
  encoding module and a noise-agnostic contrastive speaker loss.
---

# Noro: Noise-Robust One-shot Voice Conversion with Hidden Speaker Representation Learning

## Quick Facts
- arXiv ID: 2411.19770
- Source URL: https://arxiv.org/abs/2411.19770
- Authors: Haorui He; Yuchen Song; Yuancheng Wang; Haoyang Li; Xueyao Zhang; Li Wang; Gongping Huang; Eng Siong Chng; Zhizheng Wu
- Reference count: 40
- Key outcome: Noro achieves CER of 4.66 and SECS of 80.09 in noisy conditions, outperforming baseline systems in both clean and noisy scenarios

## Executive Summary
This paper addresses the challenge of voice conversion (VC) performance degradation in real-world scenarios where reference speeches contain background noise. The proposed Noro system introduces a dual-branch reference encoding module and a noise-agnostic contrastive speaker loss to learn noise-invariant speaker representations. The approach significantly improves VC quality in noisy conditions while maintaining performance in clean scenarios. Additionally, the paper investigates the speaker representation capabilities of reference encoders in one-shot VC systems, demonstrating that they can be effectively repurposed for speaker verification tasks with competitive performance against state-of-the-art self-supervised learning models.

## Method Summary
Noro employs a dual-branch reference encoding module where clean and noisy versions of reference speech are processed in parallel through shared-weight Transformer branches. The resulting representations are averaged to form noise-invariant speaker timbre representations. A noise-agnostic contrastive speaker loss maximizes similarity between clean and noisy representations of the same speaker while minimizing similarity between different speakers. The system is trained on 60,000 hours of speech from 8,000+ speakers in the Libri-Light dataset, with noisy references generated using the DEMAND database at 0-5 dB SNR. The diffusion model architecture is combined with HuBERT-based source encoder and Conformer-based reference encoder, trained for 800,000 steps using AdamW optimizer.

## Key Results
- Noro achieves CER of 4.66 and SECS of 80.09 in noisy conditions, significantly outperforming baseline systems
- The VC-SPK2VEC model achieves EER of 5.32% in speaker verification tasks, competitive with advanced self-supervised learning models
- Noro maintains high performance in both clean (SECS 87.04) and noisy (SECS 80.09) conditions, demonstrating robust noise handling

## Why This Works (Mechanism)

### Mechanism 1
The dual-branch reference encoding module allows the model to learn noise-agnostic speaker representations by processing clean and noisy versions of the same reference speech in parallel. During training, each clean reference speech is augmented with noise to create a noisy counterpart. Both versions are passed through separate branches of the reference encoder that share weights. The resulting representations are then averaged and used as the final speaker timbre representation. This forces the encoder to extract speaker information that is invariant to noise.

### Mechanism 2
The noise-agnostic contrastive speaker loss ensures that the learned speaker representations are similar for clean and noisy versions of the same speaker while being dissimilar for different speakers. After obtaining clean and noisy reference representations, the loss maximizes similarity between representations of the same speaker (regardless of noise) and minimizes similarity between different speakers. This creates a noise-invariant speaker embedding space.

### Mechanism 3
The reference encoder in one-shot VC systems naturally functions as a speaker encoder because it's trained to encode vocal timbre of diverse speakers. The reference encoder is trained to extract speaker timbre from reference speech for voice conversion. This same capability can be repurposed for speaker verification tasks, as demonstrated by VC-SPK2VEC achieving competitive performance against state-of-the-art SSL models.

## Foundational Learning

- **Concept: Contrastive learning**
  - Why needed here: To ensure the learned speaker representations are invariant to noise while maintaining discriminability between different speakers
  - Quick check question: What is the difference between instance-level and class-level contrastive learning, and which is more appropriate for noise-robust speaker representation?

- **Concept: Self-supervised learning for speaker representation**
  - Why needed here: The paper investigates repurposing the VC reference encoder as a speaker encoder, which requires understanding how SSL models learn speaker representations without explicit labels
  - Quick check question: How do SSL models like Wav2vec and HuBERT learn speaker representations differently from supervised speaker embedding models?

- **Concept: Voice conversion pipeline architecture**
  - Why needed here: Understanding the baseline system components (source encoder, reference encoder, diffusion model) is crucial for grasping how Noro modifies the reference encoding stage
  - Quick check question: What are the key differences between flow-based, VAE-based, and diffusion-based voice conversion approaches?

## Architecture Onboarding

- **Component map**: Source encoder (HuBERT + Conformer) → Semantic and pitch representations → Reference encoder (dual-branch Transformer) → Speaker timbre representations → Diffusion model (WaveNet-based) → Predicted mel-spectrogram

- **Critical path**: Clean/noisy reference speech → Dual-branch reference encoder → Contrastive loss → Speaker timbre representation → Diffusion model → Converted speech

- **Design tradeoffs**:
  - Dual-branch vs single-branch: Increased computational cost during training but improved noise robustness
  - Shared vs separate weights: Shared weights enforce consistency but may limit flexibility
  - Contrastive loss weight (β): Higher weights improve noise invariance but may reduce conversion quality

- **Failure signatures**:
  - Poor SECS in noisy conditions: Likely indicates the contrastive loss isn't working or the dual-branch encoder isn't learning noise invariance
  - High CER: Suggests the model is prioritizing noise robustness over intelligibility
  - Slow convergence: May indicate the dual-branch architecture or contrastive loss is making training unstable

- **First 3 experiments**:
  1. Test the dual-branch reference encoder without the contrastive loss to isolate its contribution
  2. Test different noise types and SNR ranges in the contrastive loss to find optimal settings
  3. Compare different reference encoder architectures (Transformer depth, attention heads) while keeping the contrastive loss fixed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the reference encoder in one-shot VC systems affect the quality of speaker representation learning, and what are the trade-offs between model size and performance?
- Basis in paper: [explicit] The paper mentions the potential for unifying speaker representation learning with generative speech tasks and suggests exploring the relationship between speaker representation capability and model size.
- Why unresolved: The paper does not provide experimental data on how different sizes of reference encoders impact speaker representation learning, leaving this as a direction for future research.
- What evidence would resolve it: Experiments comparing speaker verification performance (e.g., EER) across reference encoders of varying sizes, while controlling for other factors, would clarify the impact of model size on speaker representation quality.

### Open Question 2
- Question: Can the integration of large-scale voice conversion systems with extensive unlabeled data further advance both voice conversion and speaker representation learning, and what are the potential benefits and challenges?
- Basis in paper: [explicit] The paper concludes by highlighting the potential of unifying speaker representation learning with generative speech tasks and suggests exploring the integration of large-scale voice conversion systems and extensive unlabeled data.
- Why unresolved: The paper does not explore the practical implementation or outcomes of integrating large-scale voice conversion systems with unlabeled data, leaving this as a promising but untested direction.
- What evidence would resolve it: Experiments demonstrating improved voice conversion quality and speaker representation learning using large-scale systems trained on extensive unlabeled data would validate the potential benefits of this integration.

### Open Question 3
- Question: How does the performance of Noro degrade in scenarios where both the source and reference speeches are noisy, and what modifications could improve robustness in such cases?
- Basis in paper: [inferred] While the paper focuses on noise robustness for reference speeches, it does not address scenarios where both source and reference speeches are noisy, which is a common real-world challenge.
- Why unresolved: The paper only evaluates Noro under clean source speech and noisy reference speech conditions, leaving the performance in dual-noise scenarios unexplored.
- What evidence would resolve it: Experiments testing Noro’s performance when both source and reference speeches are contaminated with noise, along with modifications to improve robustness in such cases, would address this gap.

## Limitations
- The dual-branch architecture introduces significant computational overhead during training, potentially limiting scalability to larger datasets or real-time applications
- The contrastive loss formulation assumes clean and noisy samples of the same speaker are equally valid for learning representations, which may not hold for extreme noise conditions
- Performance generalization across different corpora remains unclear, as results are primarily shown within the Libri-Light dataset

## Confidence

- **High confidence**: Baseline system architecture and evaluation metrics (CER, SECS, CMOS, SMOS) are well-established in VC literature; comparative advantage over baseline in noisy conditions is directly measurable
- **Medium confidence**: Effectiveness of dual-branch reference encoding in learning noise-invariant representations; specific contribution versus other factors is not fully isolated
- **Medium confidence**: Transferability of reference encoder to speaker verification tasks; competitive performance against SSL models is promising but evaluation on only 50 speakers may not be representative
- **Low confidence**: Noise-agnostic contrastive speaker loss formulation and temperature parameter (τ=0.07) presented without extensive ablation studies; optimal settings for different noise types remain unclear

## Next Checks

1. **Ablation study on dual-branch architecture**: Remove the dual-branch module and train a single-branch reference encoder with the same contrastive loss to isolate the architectural contribution to noise robustness.

2. **Cross-corpus generalization test**: Train the model on Libri-Light but evaluate on a completely different corpus (e.g., LibriTTS or Common Voice) to assess domain adaptation capabilities.

3. **Extreme noise condition evaluation**: Test the model's performance at SNR levels below 0 dB and with non-stationary noise types (e.g., babble, music) not present in the DEMAND database to evaluate robustness boundaries.