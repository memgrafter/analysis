---
ver: rpa2
title: 'Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study'
arxiv_id: '2404.07060'
source_url: https://arxiv.org/abs/2404.07060
tags:
- grounded
- groundedness
- documents
- pre-training
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the groundedness of long-form question answering
  (LFQA) generations from retrieval-augmented large language models (LLMs). The key
  finding is that a significant fraction of generated sentences, even those containing
  correct ground-truth answers, are consistently ungrounded in either the retrieved
  documents or the model's pre-training data.
---

# Groundedness in Retrieval-augmented Long-form Generation: An Empirical Study

## Quick Facts
- arXiv ID: 2404.07060
- Source URL: https://arxiv.org/abs/2404.07060
- Authors: Alessandro Stolfo
- Reference count: 25
- Key outcome: Significant fraction of LFQA generations are ungrounded even when correct

## Executive Summary
This paper investigates the groundedness of long-form question answering (LFQA) generations from retrieval-augmented large language models (LLMs). The study reveals that a significant portion of generated sentences, even those containing correct ground-truth answers, are consistently ungrounded in either the retrieved documents or the model's pre-training data. Across three datasets and four model families, approximately 25-50% of sentences in partially correct answers cannot be grounded. The research examines the impact of model size, decoding strategy, and instruction tuning on groundedness, finding that larger models tend to generate more grounded content, while instruction tuning and beam search decoding strategies help reduce ungrounded sentence generation.

## Method Summary
The study generates long-form answers using retrieval-augmented LLMs from four families (Pythia, Falcon, MPT, Silo) across various model sizes. Post-generation retrieval from pre-training corpora (Pile, C4, OLC) is performed using FAISS, and the TRUE model is employed to verify groundedness of generated sentences against retrieved documents and pre-training data. The analysis measures the fraction of generated sentences grounded in retrieved documents, pre-training data, both, or neither, while examining the effects of model size, decoding strategy (greedy, beam search, nucleus sampling), and instruction tuning on groundedness metrics.

## Key Results
- 25-50% of sentences in partially correct answers are consistently ungrounded in either retrieved documents or pre-training data
- Larger models (Falcon 40B, 180B) show increased proportion of content grounded to provided context or pre-training corpus
- Instruction tuning and beam search decoding strategies reduce ungrounded sentence generation
- Significant portion of correct answers remains compromised by hallucinations despite retrieval augmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models generate more grounded sentences in LFQA.
- Mechanism: Increased model size enhances the model's capacity to integrate and utilize external information from the provided context and its extensive pre-training data.
- Core assumption: Larger models have better access to and integration of external and pre-training information.
- Evidence anchors:
  - [abstract] "Results show that while larger models tend to ground their outputs more effectively, a significant portion of correct answers remains compromised by hallucinations."
  - [section 5.1] "Interestingly, for significantly larger models (Falcon 40B and 180B), there is a clear increase in the proportion of content that can be grounded to the provided context or the pre-training corpus."
  - [corpus] Weak. The corpus does not provide direct evidence on how model size affects groundedness at the architectural level.
- Break condition: If the increase in model size does not lead to better grounding, possibly due to overfitting or inefficient use of increased capacity.

### Mechanism 2
- Claim: Instruction tuning reduces the generation of ungrounded content.
- Mechanism: Instruction tuning enhances the model's ability to generate content that is more grounded in the provided context or pre-training data.
- Core assumption: Instruction tuning improves the model's understanding and utilization of the provided context.
- Evidence anchors:
  - [abstract] "Instruction tuning and beam search decoding strategies were found to reduce ungrounded sentence generation."
  - [section 5.3] "For instruction-tuned models, we observe a marked improvement in both the overall correctness and the fraction of grounded sentences."
  - [corpus] Weak. The corpus does not provide direct evidence on how instruction tuning affects groundedness at the architectural level.
- Break condition: If instruction tuning does not lead to better grounding, possibly due to misalignment between instructions and the model's capabilities.

### Mechanism 3
- Claim: Beam search decoding leads to more grounded content compared to other decoding strategies.
- Mechanism: Beam search tends to give a higher likelihood to sequences that previously appeared in the model input, thus favoring grounded sentences.
- Core assumption: Beam search's tendency to repeat input sequences leads to more grounded outputs.
- Evidence anchors:
  - [section 5.2] "However, an interesting deviation from this trend is observed in the case of beam search decoding. Unlike the other strategies, the EM scores for beam search do not exhibit a significant decline as the groundedness threshold increases."
  - [section 5.2] "This result shows that while beam search may initially show lower EM scores compared to nucleus sampling without considering groundedness, its effectiveness emerges when groundedness is taken into account."
  - [corpus] Weak. The corpus does not provide direct evidence on how beam search affects groundedness at the architectural level.
- Break condition: If beam search does not lead to more grounded content, possibly due to the specific characteristics of the input data or the model's architecture.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG is the paradigm used in the study to provide external context to the model, affecting the groundedness of the generated content.
  - Quick check question: What is the main purpose of using retrieval-augmented generation in LFQA?

- Concept: Groundedness verification
  - Why needed here: Understanding how groundedness is measured is crucial for interpreting the results of the study.
  - Quick check question: How is the groundedness of a sentence determined in this study?

- Concept: Exact match (EM) scoring
  - Why needed here: EM scoring is used to measure the correctness of the model's answers, which is related to the groundedness analysis.
  - Quick check question: What does it mean for a model output to have a high exact match score?

## Architecture Onboarding

- Component map: Retriever -> Language Model (LLM) -> Grounding Model -> Decoder
- Critical path:
  1. Retrieve relevant documents based on the input question.
  2. Combine the question and documents into a prompt.
  3. Generate an answer using the LLM.
  4. Verify the groundedness of each sentence in the generated answer.
  5. Analyze the results based on correctness and groundedness.

- Design tradeoffs:
  - Model size vs. computational resources: Larger models may generate more grounded content but require more computational power.
  - Decoding strategy vs. groundedness: Different decoding strategies can affect the groundedness of the generated content.
  - Instruction tuning vs. general performance: Instruction tuning may improve groundedness but could potentially limit the model's general performance.

- Failure signatures:
  - High proportion of ungrounded sentences: Indicates issues with the retrieval process or the model's ability to utilize the provided context.
  - Inconsistent groundedness across different model sizes: Suggests that the model's architecture or training process may not be optimized for groundedness.
  - Low EM scores: Indicates that the model's answers are not accurate, which may be related to issues with groundedness.

- First 3 experiments:
  1. Test the impact of different model sizes on groundedness using a fixed decoding strategy and instruction tuning.
  2. Evaluate the effect of different decoding strategies on groundedness using a fixed model size and instruction tuning.
  3. Assess the influence of instruction tuning on groundedness using a fixed model size and decoding strategy.

## Open Questions the Paper Calls Out

Open Question 1
- Question: How can we effectively measure the groundedness of long-form text generated by large language models in retrieval-augmented settings, especially when the text is not directly supported by the retrieved documents or pre-training data?
- Basis in paper: Explicit
- Why unresolved: The paper identifies that a significant fraction of generated sentences, even those containing correct ground-truth answers, are consistently ungrounded in either the retrieved documents or the model's pre-training data. The current methods for measuring groundedness, such as exact string match and post-generation retrieval, may not fully capture the nuances of grounding in long-form text.
- What evidence would resolve it: Development and validation of more sophisticated methods for measuring groundedness that can accurately capture the relationship between generated text and the sources, even when the text is not directly supported by the retrieved documents or pre-training data.

Open Question 2
- Question: What are the underlying mechanisms that allow large language models to generate correct answers that are not grounded in the provided context or pre-training data?
- Basis in paper: Inferred
- Why unresolved: The paper observes that models can generate correct answers that are not grounded in the retrieved documents or pre-training data, suggesting that models may rely on internal heuristics or pattern-matching capabilities. Understanding these mechanisms could provide insights into how to improve groundedness.
- What evidence would resolve it: Detailed analysis of the internal representations and decision-making processes of large language models during generation, potentially using techniques like interpretability and probing.

Open Question 3
- Question: How can we effectively mitigate the generation of ungrounded content in large language models, especially for long-form text generation tasks?
- Basis in paper: Explicit
- Why unresolved: The paper identifies the persistent challenge of hallucination in long-form question answering and suggests that instruction tuning and beam search decoding strategies can reduce ungrounded sentence generation. However, these methods are not perfect, and there is a need for more robust mechanisms to mitigate ungrounded content generation.
- What evidence would resolve it: Development and evaluation of new techniques for improving groundedness in large language models, such as fine-tuning on grounded datasets, using retrieval-augmented generation with more sophisticated retrieval methods, or incorporating explicit grounding constraints into the generation process.

## Limitations

- Groundedness verification relies on post-generation retrieval which may introduce false negatives due to imperfect search capabilities
- Analysis focuses on factual correctness rather than semantic equivalence, potentially underestimating true grounding capability
- Different decoding strategies and instruction tuning may have confounding effects not fully isolated in the analysis

## Confidence

**High Confidence Claims:**
- 25-50% of partially correct answers contain ungrounded sentences is well-supported by experimental data
- Larger models tend to generate more grounded content is consistently observed across tested models

**Medium Confidence Claims:**
- Impact of instruction tuning on reducing ungrounded content shows clear trends but may be influenced by other factors
- Effectiveness of beam search in generating more grounded content requires further validation

**Low Confidence Claims:**
- Exact mechanisms by which model size affects groundedness at architectural level remain unclear
- Specific threshold values used for determining groundedness may affect results and require additional validation

## Next Checks

1. Conduct manual verification of a random sample of sentences classified as ungrounded to distinguish between actual hallucinations and retrieval failures

2. Test the same grounding analysis on additional model architectures (e.g., LLaMA, Mistral) to verify observed trends in model size and groundedness

3. Implement complementary analysis considering semantic equivalence rather than exact factual matches to assess whether groundedness metrics are underestimating true grounding capability