---
ver: rpa2
title: The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text Memorization
arxiv_id: '2408.16345'
source_url: https://arxiv.org/abs/2408.16345
tags:
- memorization
- data
- text
- nucleus
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether nucleus sampling can reduce text
  memorization in large language models. The authors create a controlled diagnostic
  dataset based on OpenWebText with known duplication patterns, allowing precise measurement
  of memorization.
---

# The Unreasonable Ineffectiveness of Nucleus Sampling on Mitigating Text Memorization

## Quick Facts
- arXiv ID: 2408.16345
- Source URL: https://arxiv.org/abs/2408.16345
- Authors: Luka Borec; Philipp Sadler; David Schlangen
- Reference count: 22
- Primary result: Nucleus sampling only modestly reduces text memorization, especially for highly duplicated data, showing "unreasonable ineffectiveness" when models assign high probabilities to individual tokens.

## Executive Summary
This work investigates whether nucleus sampling can reduce text memorization in large language models by creating a controlled diagnostic dataset with known duplication patterns. The authors fine-tune GPT-Neo models on this dataset and evaluate memorization using both greedy decoding and nucleus sampling with varying top_p thresholds. Results show that nucleus sampling is surprisingly ineffective at mitigating text memorization, particularly when models assign high probabilities to individual tokens that dominate the nucleus selection. Even when models do not engage in verbatim "hard" memorization, they display "soft" memorization with high n-gram overlap to training data.

## Method Summary
The authors created OpenMemText by sampling 0.5% of OpenWebText (38 GB, 8M files) and introducing controlled duplicates (280 files duplicated 1-30 times each). They fine-tuned GPT-Neo models (125M and 350M parameters) on this dataset for one epoch using attention head fine-tuning. Memorization was evaluated by generating text from prompts derived from duplicated training data, comparing greedy decoding versus nucleus sampling with top_p values {0.2, 0.4, 0.6, 0.8}. The study measured both exact string matching for "hard" memorization and BLEU-4 scores for "soft" memorization through n-gram overlap.

## Key Results
- Nucleus sampling with low top_p values behaves similarly to greedy decoding when models have memorized sequences, as high-probability tokens dominate the nucleus
- Higher duplication counts lead to increased memorization rates and abnormally high BLEU-4 scores, indicating "soft" memorization patterns
- Even with top_p = 0.8, the amount of detected memorized text remains nearly equivalent to deterministic greedy search for highly duplicated data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low top_p values in nucleus sampling behave similarly to greedy decoding when the model has memorized sequences.
- Mechanism: When models memorize text, they assign very high probability to the next token in the memorized sequence. If this probability exceeds the top_p threshold, the nucleus collapses to a single token, making sampling deterministic.
- Core assumption: Memorized sequences are encoded as high-probability paths in the model's distribution.
- Evidence anchors: [abstract] "even when models do not engage in 'hard' memorization – a verbatim reproduction of training samples – they may still display 'soft' memorization whereby they generate outputs that echo the training data but without a complete one-by-one resemblance." [section 4.3] "Models that strongly memorize texts assign very high probabilities to single tokens so that even nucleus sampling becomes deterministic."

### Mechanism 2
- Claim: Nucleus sampling effectiveness diminishes as data duplication increases due to saturation effects.
- Mechanism: With many duplicates, the model learns the sequence as a high-probability pattern. Even with larger top_p values, the cumulative probability of the memorized sequence remains dominant, so sampling from the nucleus still produces memorized output.
- Core assumption: Repeated exposure to the same sequence creates a strong, peaked probability distribution that dominates the nucleus regardless of its size.
- Evidence anchors: [abstract] "even larger nuclei show an 'unreasonable ineffectiveness' on the mitigation of text memorization, because in cases of peaked distributions a model's memorized token dominates the output distribution" [section 4.1] "Even with a top_p = 0.8 the amount of detected memorized text is nearly equivalent to that of the deterministic greedy search."

### Mechanism 3
- Claim: "Soft memorization" occurs when models alternate between memorized and novel tokens, producing high n-gram overlap without exact reproduction.
- Mechanism: With many duplicates, the model learns to generate sequences that resemble training data but with variations. This creates outputs with high BLEU scores against training data without being verbatim copies.
- Core assumption: Models can learn statistical patterns from duplicates that generate similar but not identical text.
- Evidence anchors: [abstract] "even when models do not engage in 'hard' memorization... they may still display 'soft' memorization whereby they generate outputs that echo the training data but without a complete one-to-one resemblance." [section 4.4] "An interesting observation from the results in Table 2 is the positive correlation between the number of duplicated data and the measured BLEU-4 scores"

## Foundational Learning

- Concept: Text memorization in language models
  - Why needed here: The entire paper analyzes how different decoding strategies affect memorization, so understanding what memorization is and how it's measured is fundamental.
  - Quick check question: How does the paper define "hard" memorization versus "soft" memorization?

- Concept: Nucleus sampling mechanics
  - Why needed here: The study specifically tests nucleus sampling with different top_p values, so understanding how this decoding method works is essential.
  - Quick check question: What happens when a single token's probability exceeds the top_p threshold in nucleus sampling?

- Concept: BLEU score as an n-gram overlap metric
  - Why needed here: The paper uses BLEU-4 to measure "soft memorization" by quantifying n-gram overlap between generated and training text.
  - Quick check question: Why might high BLEU scores indicate memorization even when outputs aren't exact copies?

## Architecture Onboarding

- Component map: Diagnostic dataset creation (OpenMemText with controlled duplicates) -> GPT-Neo model fine-tuning (125M/350M parameters) -> Memorization evaluation (greedy vs nucleus sampling with various top_p values) -> Analysis of results across model sizes and duplicate counts
- Critical path: Data creation → Model fine-tuning → Memorization evaluation with different decoding strategies → Analysis of results across model sizes, duplicate counts, and decoding parameters
- Design tradeoffs: Using a controlled dataset with known duplicates enables precise measurement but may not reflect real-world training data distributions. Fine-tuning only attention heads targets memorization susceptibility but may not represent full-model behavior.
- Failure signatures: If nucleus sampling shows similar memorization rates to greedy decoding across all top_p values, it indicates peak distributions dominate the nucleus. If BLEU scores remain low even with high duplicates, it suggests the model isn't learning soft memorization patterns.
- First 3 experiments:
  1. Replicate greedy decoding memorization results from Carlini et al. (2023) using the controlled dataset to verify the experimental setup.
  2. Test nucleus sampling with top_p = 0.2 across different duplicate counts to establish baseline effectiveness.
  3. Measure BLEU-4 scores for non-verbatim outputs to quantify soft memorization patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what point does increasing top_p stop being effective at reducing memorization during ramp-up points, and why does this effectiveness decrease at saturation points?
- Basis in paper: [explicit] The paper identifies ramp-up points where models begin memorizing significantly and saturation points where additional data yields diminishing returns. It notes that higher top_p values reduce memorization at ramp-up points but are less effective near saturation.
- Why unresolved: The paper observes this trend but doesn't quantify the exact threshold where top_p effectiveness diminishes or provide a theoretical explanation for why saturation points reduce this effectiveness.
- What evidence would resolve it: Experiments varying top_p thresholds at different duplicate counts, combined with analysis of attention patterns or gradient norms at ramp-up versus saturation points.

### Open Question 2
- Question: How does fine-tuning only attention heads compare to full-model fine-tuning or adapter-based methods in terms of memorization behavior and the effectiveness of nucleus sampling?
- Basis in paper: [explicit] The paper specifically targets attention heads for fine-tuning based on prior research showing they're most susceptible to memorization, and notes this should be compared to other fine-tuning methods.
- Why unresolved: The paper uses a specific fine-tuning approach but doesn't compare it to alternatives, leaving uncertainty about whether the observed memorization patterns would generalize.
- What evidence would resolve it: Direct comparison of memorization rates across models fine-tuned with attention-head targeting, full-model fine-tuning, and adapter methods under identical conditions.

### Open Question 3
- Question: What causes the dip in memorization at the 300-400 token context length bucket, and is this statistically significant given the small sample size?
- Basis in paper: [inferred] The paper notes an unexpected dip in memorization at the 300-400 token bucket during replication experiments and suggests small sample size might be the cause.
- Why unresolved: The paper acknowledges the anomaly but doesn't investigate whether it's a real effect or statistical noise, nor does it propose explanations for what mechanism might cause such a dip.
- What evidence would resolve it: Larger sample sizes in the 300-400 bucket combined with analysis of attention patterns or positional encoding effects at that specific length range.

## Limitations

- Controlled Dataset Generalizability: The artificial nature of controlled duplicates may not reflect real-world training data distributions, limiting external validity.
- Fine-tuning Scope: Using attention head fine-tuning rather than full-model training may miss important interactions between model components.
- Evaluation Methodology: The focus on exact string matching and BLEU-4 scores may underestimate semantic similarity or soft memorization patterns.

## Confidence

**High Confidence**: The core finding that nucleus sampling shows "unreasonable ineffectiveness" at mitigating text memorization when token probabilities are highly peaked. This is directly supported by quantitative results showing similar memorization rates between greedy decoding and nucleus sampling with low top_p values, particularly for highly duplicated data.

**Medium Confidence**: The claim that soft memorization occurs through high n-gram overlap without exact reproduction. While BLEU-4 scores show positive correlation with duplication counts, the interpretation of these scores as evidence of soft memorization patterns is somewhat indirect.

**Low Confidence**: The broader implications for real-world language model training and deployment. The controlled experimental setup, while valuable for understanding mechanisms, may not translate directly to the complexity of actual training scenarios.

## Next Checks

**Validation Check 1**: Replicate the study using full-model fine-tuning rather than attention head fine-tuning on the same OpenMemText dataset to test whether observed memorization patterns persist when the entire model is trained.

**Validation Check 2**: Conduct experiments on naturally occurring web text data without artificial duplication, using the same evaluation framework to test whether "unreasonable ineffectiveness" extends beyond controlled diagnostic datasets.

**Validation Check 3**: Test alternative decoding strategies beyond nucleus sampling, such as contrastive search, locally typical sampling, or temperature-based methods, on the same dataset to determine whether ineffectiveness is unique to nucleus sampling.