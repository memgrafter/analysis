---
ver: rpa2
title: 'ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation'
arxiv_id: '2410.01731'
source_url: https://arxiv.org/abs/2410.01731
tags:
- prompt
- detailed
- prompts
- quality
- flows
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ComfyGen, a method for automatically generating
  prompt-adaptive workflows for text-to-image generation. The approach leverages large
  language models (LLMs) to create workflows tailored to each user prompt, improving
  image quality over monolithic models or generic, prompt-independent workflows.
---

# ComfyGen: Prompt-Adaptive Workflows for Text-to-Image Generation

## Quick Facts
- arXiv ID: 2410.01731
- Source URL: https://arxiv.org/abs/2410.01731
- Authors: Rinon Gal; Adi Haviv; Yuval Alaluf; Amit H. Bermano; Daniel Cohen-Or; Gal Chechik
- Reference count: 31
- Primary result: ComfyGen improves image quality by automatically generating prompt-adaptive workflows using LLMs, outperforming monolithic models and generic workflows

## Executive Summary
This work introduces ComfyGen, a method for automatically generating prompt-adaptive workflows for text-to-image generation. The approach leverages large language models (LLMs) to create workflows tailored to each user prompt, improving image quality over monolithic models or generic, prompt-independent workflows. Two LLM-based methods are proposed: a tuning-based approach (ComfyGen-FT) that learns from user preference data, and a training-free method (ComfyGen-IC) that uses the LLM to select existing flows. ComfyGen outperforms baseline approaches, including single-model approaches (SDXL, JuggernautXL, DreamShaperXL, DPO-SDXL) and generic workflows, on both human-preference and prompt-alignment benchmarks. The method demonstrates that prompt-dependent flow prediction offers a new pathway to improving text-to-image generation quality, complementing existing research directions in the field.

## Method Summary
ComfyGen uses LLMs to analyze input prompts, identify relevant categories, and select workflows with components optimized for those categories. The system employs two approaches: ComfyGen-FT fine-tunes Llama3.1 on (prompt, score, workflow) triplets collected from human preferences, while ComfyGen-IC uses in-context learning with Claude Sonnet 3.5 to select from existing workflows. Both methods leverage human preference scores as training signals, collected by generating images with each workflow and scoring them using an ensemble of aesthetic predictors and human preference estimators.

## Key Results
- ComfyGen-FT and ComfyGen-IC outperform single-model approaches (SDXL, JuggernautXL, DreamShaperXL, DPO-SDXL) and generic workflows on human-preference and prompt-alignment benchmarks
- Fine-tuning with high target scores during inference improves workflow selection quality
- Prompt-dependent flow prediction offers a new pathway to improving text-to-image generation quality
- The method complements existing research directions in text-to-image generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Prompt-adaptive workflow generation improves image quality by matching workflow components to prompt-specific needs.
- **Mechanism**: The system uses an LLM to analyze the input prompt, identify relevant categories (e.g., "People", "Nature", "Anime"), and select workflows with components optimized for those categories. This targeted selection improves downstream image quality compared to generic or monolithic approaches.
- **Core assumption**: The LLM can accurately categorize prompts and the associated workflows are genuinely better for those categories.
- **Evidence anchors**:
  - [abstract]: "Our work shows that prompt-dependent flow prediction offers a new pathway to improving text-to-image generation quality"
  - [section]: "We hypothesize that effective workflows will depend on the specific topics present in the prompt"
  - [corpus]: Weak - the corpus contains only 5 related papers, none directly supporting this specific mechanism. The work is novel in this approach.
- **Break Condition**: If the LLM misclassifies prompts or the workflows are not actually optimized for their claimed categories, the mechanism fails.

### Mechanism 2
- **Claim**: Fine-tuning an LLM on (prompt, score, workflow) triplets enables it to predict high-quality workflows for new prompts.
- **Mechanism**: The LLM is trained to map a prompt and target score to a specific workflow. During inference, providing a high target score guides the model to select workflows that previously achieved high scores for similar prompts.
- **Core assumption**: The training data (prompt, score, workflow) triplets are representative and the LLM can generalize from them.
- **Evidence anchors**:
  - [section]: "we propose an alternative fine-tuning formulation where the LLM takes as its context both the prompt and a score"
  - [section]: "we provide the LLM with an unseen prompt and a target score and ask it to provide us with an appropriate workflow"
  - [corpus]: Weak - no direct evidence in corpus, but the approach is well-grounded in established fine-tuning practices.
- **Break Condition**: If the training data is biased or unrepresentative, or if the LLM overfits to the training set, the mechanism fails.

### Mechanism 3
- **Claim**: Using human preference scores as training signals creates a more aligned and effective workflow selection system.
- **Mechanism**: Images generated by workflows are scored using an ensemble of aesthetic predictors and human preference estimators. These scores train the LLM to associate workflows with human-preferred outcomes.
- **Core assumption**: The ensemble score accurately reflects human preferences and correlates with image quality.
- **Evidence anchors**:
  - [section]: "we collect 500 diverse prompts from human users...These prompts are used for generating images using each workflow in our training set, and the results are scored by an ensemble of aesthetic predictors and human preference estimators"
  - [section]: "Our final dataset consists of triplets of prompt, workflow, and score"
  - [corpus]: Weak - the corpus doesn't provide evidence for this specific mechanism, but the use of human preference models is established in related work.
- **Break Condition**: If the scoring ensemble doesn't accurately capture human preferences or if the correlation between scores and actual human preference is weak, the mechanism fails.

## Foundational Learning

- **Concept**: Text-to-image generation with diffusion models
  - Why needed here: The entire system is built on generating images from text prompts using diffusion models, so understanding the basics is essential.
  - Quick check question: Can you explain the basic principle of how a diffusion model generates an image from a text prompt?

- **Concept**: Workflow composition and JSON representation
  - Why needed here: ComfyUI workflows are represented as JSON files, and the system manipulates these to create and select workflows.
  - Quick check question: What are the key components of a ComfyUI workflow JSON file?

- **Concept**: Large Language Model fine-tuning and in-context learning
  - Why needed here: The system uses both fine-tuning (ComfyGen-FT) and in-context learning (ComfyGen-IC) approaches with LLMs to select workflows.
  - Quick check question: What's the difference between fine-tuning an LLM and using it for in-context learning?

## Architecture Onboarding

- **Component map**: Prompt → LLM analysis → Workflow selection → Image generation → Scoring/evaluation
- **Critical path**: User prompt → LLM categorization/selection → Workflow execution → Image generation → Quality scoring
- **Design tradeoffs**:
  - Fine-tuning vs. in-context learning: FT requires training data and time but can generalize better; IC is faster but relies on retrieval.
  - Workflow augmentation: Augmenting human-created workflows increases diversity but may introduce noise.
  - Scoring ensemble: Using multiple models reduces bias but increases complexity.
- **Failure signatures**:
  - Poor image quality: LLM misclassifies prompts or selects inappropriate workflows.
  - Low diversity: LLM only selects a small subset of available workflows.
  - High computational cost: Generating images for all workflow-prompt combinations is expensive.
- **First 3 experiments**:
  1. Test LLM categorization: Give the LLM a set of prompts and check if it categorizes them correctly.
  2. Test workflow selection: For a simple prompt, compare the workflow selected by the LLM against a manually chosen one.
  3. Test scoring correlation: Generate images with a few workflows and check if the scoring ensemble correlates with human preferences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ComfyGen's performance scale with the number of available workflows in the training set?
- Basis in paper: [explicit] The paper mentions that ComfyGen-IC uses 41 unique flows while ComfyGen-FT uses 79 unique flows across 500 prompts, and notes that more data or a more involved search over the input parameter space could yield more diverse outputs and possibly improved performance.
- Why unresolved: The paper does not provide experiments or analysis on how performance changes as the number of workflows in the training set increases. The relationship between training set size and downstream performance remains unexplored.
- What evidence would resolve it: Systematic experiments varying the number of workflows in the training corpus while keeping other factors constant, measuring both the diversity of generated workflows and the quality of the resulting images.

### Open Question 2
- Question: Can ComfyGen generalize to entirely new workflow components (blocks) not seen during training?
- Basis in paper: [explicit] The paper explicitly states that "The fine-tuning approach cannot easily generalize to new blocks as they become available, requiring retraining with new flows that include these blocks."
- Why unresolved: The paper demonstrates performance with existing blocks but does not explore whether the model can handle workflows containing previously unseen components, which is critical for practical deployment as new tools are continuously developed.
- What evidence would resolve it: Testing ComfyGen with workflows containing novel components (e.g., new LoRAs, ControlNets, or custom nodes) that were never part of the training data, measuring whether it can still select appropriate workflows or synthesize new ones.

### Open Question 3
- Question: What is the optimal target score to provide to ComfyGen-FT during inference for different prompt categories?
- Basis in paper: [explicit] The paper shows that ComfyGen-FT performance varies with the target score provided at inference time, and that performance peaks when using a score near the top of the training distribution.
- Why unresolved: While the paper demonstrates that target score selection matters, it does not investigate whether different prompt categories (e.g., "People" vs "Nature" vs "Abstract") would benefit from different optimal target scores, or how to automatically select the best score for a given prompt.
- What evidence would resolve it: Analysis of ComfyGen-FT performance across different prompt categories using various target scores, potentially identifying patterns that could inform automatic target score selection based on prompt content.

## Limitations
- The system's effectiveness depends heavily on the quality of the scoring ensemble used to train the LLM-based workflow selectors
- The LLM's ability to generalize across unseen prompt categories remains untested
- The approach requires generating images for all workflow-prompt combinations, which is computationally expensive

## Confidence
- **High Confidence**: The experimental methodology and baseline comparisons are sound. The framework for collecting prompts, workflows, and generating scores is clearly specified.
- **Medium Confidence**: The LLM-based workflow selection mechanisms (both fine-tuning and in-context learning) are technically feasible and the results show improvement over baselines, but the scoring mechanism's reliability introduces uncertainty.
- **Low Confidence**: The long-term generalization capabilities of the system to completely novel prompt categories or domains not represented in the training data.

## Next Checks
1. Conduct ablation studies removing individual components of the scoring ensemble to determine which models contribute most to accurate human preference prediction.
2. Test the system on a held-out set of prompts from categories completely absent from the training data to assess true generalization capabilities.
3. Perform user studies comparing workflow selections made by ComfyGen against expert human selections across diverse prompt categories to validate the LLM's categorization and selection accuracy.