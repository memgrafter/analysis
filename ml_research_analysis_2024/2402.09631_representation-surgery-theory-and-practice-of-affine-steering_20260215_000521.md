---
ver: rpa2
title: 'Representation Surgery: Theory and Practice of Affine Steering'
arxiv_id: '2402.09631'
source_url: https://arxiv.org/abs/2402.09631
tags:
- class
- linear
- representations
- representation
- intervention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies methods for steering language model representations\
  \ to control undesirable behaviors like toxicity or bias. It analyzes existing techniques\u2014\
  linear erasure and steering vectors\u2014showing they only match first moments (means),\
  \ limiting control."
---

# Representation Surgery: Theory and Practice of Affine Steering

## Quick Facts
- arXiv ID: 2402.09631
- Source URL: https://arxiv.org/abs/2402.09631
- Reference count: 40
- This paper proposes Optimal Linear Counterfactuals (OLCs) for steering language model representations, matching both means and covariances to control undesirable behaviors like bias and toxicity.

## Executive Summary
This paper addresses the challenge of steering language model representations to control undesirable behaviors such as toxicity and bias. The authors analyze existing methods like linear erasure and steering vectors, which only match first moments (means), and propose Optimal Linear Counterfactuals (OLCs) that match both means and covariances via a closed-form solution. They extend this to a nonlinear intervention for generation using a randomized algorithm that applies OLCs on decomposed representation components. Experiments show OLCs outperform prior methods in mitigating gender bias in classification and reducing toxic generation on GPT-2 without full fine-tuning, matching or exceeding state-of-the-art performance while being more efficient at inference.

## Method Summary
The paper proposes two main interventions: Optimal Linear Counterfactuals (OLCs) and a nonlinear randomized algorithm. OLCs provide a closed-form solution that matches both means and covariances of source and target class distributions, minimizing Earth-Mover's Distance under Gaussian assumptions. The nonlinear intervention decomposes hidden states into convex combinations of basis vectors and applies OLC transformations selectively to components closer to the source class distribution. For bias mitigation, OLCs are applied to align representations between protected groups. For toxicity mitigation, the randomized algorithm is used during generation to steer outputs away from toxic patterns. The approach is evaluated on bias mitigation tasks using the Bios dataset and toxicity mitigation using the Real Toxicity Prompts dataset.

## Key Results
- OLCs outperform linear erasure and steering vectors in reducing gender bias, achieving up to 66% reduction in TPR gap while maintaining task accuracy
- The randomized nonlinear intervention reduces expected maximum toxicity by up to 45% on GPT-2 without full fine-tuning
- OLCs provide strong theoretical guarantees by matching both first and second moments of class distributions
- The approach is more efficient than full fine-tuning while achieving comparable or better performance in toxicity mitigation

## Why This Works (Mechanism)

### Mechanism 1
Steering vectors (mean-matching) optimally equate class-conditional means, making linear classifiers unable to distinguish between source and target classes. The optimal linear transformation that matches means is the identity matrix with a bias equal to the difference between target and source class means. This ensures post-intervention, both classes have identical first moments. The core assumption is that class separation is primarily determined by mean differences in representation space. If class separation relies on higher-order moments (covariance, skewness), mean-matching alone will be insufficient, allowing nonlinear classifiers to still distinguish classes.

### Mechanism 2
Optimal Linear Counterfactuals (OLCs) match both means and covariances, providing stronger control than mean-matching alone. Using the Knott & Smith (1984) closed-form solution, OLCs apply a linear transformation that simultaneously matches both first and second moments of source and target distributions. This minimizes Earth-Mover's Distance under Gaussian assumptions. The core assumption is that representations can be approximated as Gaussian or that matching first two moments provides sufficient distributional alignment for practical purposes. If representations are highly non-Gaussian or if third/fourth moments significantly contribute to class separation, OLCs may not fully align distributions.

### Mechanism 3
The nonlinear intervention via randomized decomposition achieves better generation control by applying OLCs to individual components. It decomposes hidden states into convex combinations of basis vectors, applies OLC transformations selectively to components closer to the source class distribution, then reconstructs. This piecewise-linear approach allows more nuanced steering. The core assumption is that hidden states can be meaningfully decomposed into components corresponding to different semantic or stylistic attributes. If decomposition doesn't capture meaningful structure or if the randomization introduces instability in generation, the method may fail.

## Foundational Learning

- **Concept**: Earth-Mover's Distance (EMD) and Wasserstein metrics
  - Why needed here: OLCs minimize EMD between class-conditional distributions, providing the theoretical foundation for optimal counterfactuals
  - Quick check question: What is the closed-form solution for minimizing EMD between two Gaussian distributions?

- **Concept**: Covariance matching and multivariate normal distributions
  - Why needed here: Understanding how matching means and covariances affects distribution alignment is crucial for OLCs
  - Quick check question: If two multivariate normals have identical means and covariances, are they statistically indistinguishable?

- **Concept**: Convex decomposition and null space projection
  - Why needed here: The randomized algorithm relies on decomposing hidden states into convex combinations and sampling valid decompositions
  - Quick check question: Given a vector h and coefficients a with ||a||₁=1, what conditions must a matrix G satisfy to ensure Ga=h?

## Architecture Onboarding

- **Component map**: Input → Language model → Extract representations → Classify intervention need → Apply OLC → Forward pass → Output

- **Critical path**: Input → Language model → Extract representations → Classify intervention need → Apply OLC → Forward pass → Output

- **Design tradeoffs**: 
  - Linear vs. nonlinear intervention: Linear is faster but less expressive; nonlinear provides better control at higher computational cost
  - Selective vs. global application: Selective preserves semantics but may miss subtle toxicity; global is more thorough but damages fluency
  - Decomposition dimensionality: Higher n provides finer control but increases computation and may introduce instability

- **Failure signatures**:
  - If toxicity mitigation fails despite correct implementation: Check if representations actually encode toxicity in means/covariances
  - If fluency drops significantly: Intervention may be too aggressive or applied globally rather than selectively
  - If bias mitigation shows limited effect: Check if bias is encoded in higher moments not captured by OLCs

- **First 3 experiments**:
  1. Verify OLCs correctly match means and covariances on synthetic Gaussian data
  2. Test linear intervention on controlled bias dataset (Bios) and measure TPR gap reduction
  3. Apply randomized algorithm to toxicity mitigation and compare against baselines using Perspective API scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Optimal Linear Counterfactuals (OLCs) perform on datasets with non-Gaussian distributions?
- Basis in paper: [explicit] The paper proves that OLCs guarantee equal within-class and cross-class distances in expectation for arbitrary distributions (Proposition 3.3).
- Why unresolved: The theoretical guarantees are asymptotic; empirical performance on real-world non-Gaussian data (e.g., toxic vs. non-toxic text) is not thoroughly evaluated.
- What evidence would resolve it: Empirical comparison of OLCs against baselines on multiple real-world datasets with non-Gaussian class distributions, measuring both fairness metrics and downstream task performance.

### Open Question 2
- Question: What is the impact of the number of decomposition components (n) in the Randomized Algorithm on controlled generation quality?
- Basis in paper: [explicit] The Randomized Algorithm uses a hyperparameter n for the number of components in the decomposition, but the effect of varying n is not explored.
- Why unresolved: The paper uses a fixed n=40 without exploring the trade-off between decomposition granularity and computational cost or generation quality.
- What evidence would resolve it: Systematic ablation study varying n (e.g., 10, 20, 40, 80) and evaluating the impact on toxicity mitigation, fluency, and diversity metrics.

### Open Question 3
- Question: Can the theoretical guarantees of OLCs be extended to the nonlinear Randomized Algorithm?
- Basis in paper: [inferred] The Randomized Algorithm builds upon OLCs but lacks theoretical guarantees, as mentioned in Section 4.1.
- Why unresolved: The paper explicitly states that the nonlinear intervention loses the theoretical guarantees present in the linear case, but does not explore potential avenues for extending these guarantees.
- What evidence would resolve it: Development of a theoretical framework that extends the OLC guarantees to the nonlinear case, or empirical evidence showing that the Randomized Algorithm consistently outperforms OLCs without theoretical backing.

## Limitations
- The theoretical guarantees for OLCs rely on Gaussian assumptions that may not hold in practice
- The randomized algorithm's effectiveness depends on meaningful decomposition of hidden states that isn't theoretically guaranteed
- Results are demonstrated on specific tasks and may not generalize across diverse domains or languages

## Confidence
- **High Confidence**: The theoretical derivation of OLCs as optimal mean-and-covariance matching transformations under Gaussian assumptions
- **Medium Confidence**: The effectiveness of OLCs in reducing bias metrics on the tested datasets
- **Low Confidence**: The claim that the randomized nonlinear intervention "consistently outperforms" linear interventions

## Next Checks
1. **Distributional Validation**: Test OLC performance on synthetic data with varying distributional assumptions (Gaussian vs. heavy-tailed vs. multimodal) to empirically validate the importance of Gaussian assumptions for OLC effectiveness

2. **Ablation on Decomposition Parameters**: Systematically vary the number of decomposition components (n), iterations (K), and runs (k) in the randomized algorithm to identify the minimum effective configuration and understand sensitivity to these hyperparameters

3. **Transferability Test**: Apply the same OLCs trained on one bias type (e.g., gender bias in Bios) to a different domain (e.g., racial bias in another dataset) to evaluate cross-domain generalization capabilities and identify limitations of learned transformations