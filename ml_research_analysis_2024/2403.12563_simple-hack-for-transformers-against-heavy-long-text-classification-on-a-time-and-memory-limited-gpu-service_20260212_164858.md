---
ver: rpa2
title: Simple Hack for Transformers against Heavy Long-Text Classification on a Time-
  and Memory-Limited GPU Service
arxiv_id: '2403.12563'
source_url: https://arxiv.org/abs/2403.12563
tags:
- best
- tokens
- text
- classification
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates simple hacks to improve long-text classification
  using Transformers under computational constraints. The authors propose a dynamic
  hyperparameter optimization (HPO) procedure that can be executed gradually on limited
  GPU resources.
---

# Simple Hack for Transformers against Heavy Long-Text Classification on a Time- and Memory-Limited GPU Service

## Quick Facts
- arXiv ID: 2403.12563
- Source URL: https://arxiv.org/abs/2403.12563
- Reference count: 37
- Key outcome: Removing stopwords while keeping punctuation and low-frequency words achieves 83.01% F1 score with 128-token sequences on IndoSum dataset

## Executive Summary
This study addresses the challenge of long-text classification using Transformers under computational constraints, proposing practical solutions that significantly improve performance on limited GPU resources. The authors investigate text-shortening strategies and develop a dynamic hyperparameter optimization procedure that progressively refines search ranges based on F1 score trends. Their best approach combines stopword removal with punctuation and low-frequency word retention, achieving 83.01% F1 score while reducing sequence length to 128 tokens. The study also recommends using monolingual Indonesian models over multilingual ones for improved computational efficiency, as they produce fewer tokens while maintaining performance.

## Method Summary
The authors first investigate tokenization output lengths across various Indonesian and multilingual Transformer models to recommend those producing fewer tokens. They then compare four text-shortening strategies: removing stopwords, punctuation, low-frequency words, and combining head/tail sections of articles. A dynamic hyperparameter optimization procedure is proposed that starts with broad exploration and progressively narrows the search range based on F1 score patterns across epochs. The method uses DistilBERT-Base-Indonesian with 128-token sequence length and evaluates performance using 5-fold cross-validation on the IndoSum dataset of 18,774 Indonesian news articles.

## Key Results
- Stopword removal while retaining punctuation and low-frequency words achieves 83.01% F1 score (vs 80.66% baseline)
- Monolingual Indonesian models produce 20% fewer tokens on average compared to multilingual alternatives
- Dynamic HPO procedure efficiently tunes learning rate, batch size, and epochs on limited GPU resources
- 128-token sequences enable classification while reducing computational load by ~75% compared to 512-token sequences

## Why This Works (Mechanism)

### Mechanism 1: Stopword Removal for Token Efficiency
Removing stopwords frees up token slots for more meaningful words while preserving punctuation maintains contextual structure. This allows more discriminative content to fit within the 128-token limit without losing critical information.

### Mechanism 2: Monolingual Tokenizer Efficiency
Monolingual Indonesian models have language-specific vocabularies optimized for Indonesian, requiring fewer subword splits than multilingual models. This means more semantic content fits within token limits, reducing the need for aggressive truncation.

### Mechanism 3: Progressive HPO Search Space Refinement
The dynamic HPO procedure uses observed F1 score patterns to narrow search ranges progressively, starting broad then focusing on promising regions. This balances exploration efficiency with resource constraints on limited GPUs.

## Foundational Learning

- **Transformer self-attention mechanism**: Understanding quadratic complexity explains why long sequences are computationally expensive - increasing from 128 to 512 tokens approximately quadruples computational cost.
- **Tokenizer vocabulary and subword splitting**: Vocabulary size affects average tokens per word - larger vocabularies reduce subword splits but require more parameters.
- **Cross-validation and class imbalance**: The paper uses 5-fold cross-validation and upsamples minority classes rather than using class weights to handle severe imbalance in IndoSum dataset.

## Architecture Onboarding

- **Component map**: Data preprocessing → Tokenizer selection → Sequence shortening → Model training with HPO → Evaluation
- **Critical path**: Tokenizer selection and sequence shortening decisions directly impact how much information reaches the model, which then affects HPO outcomes and final performance
- **Design tradeoffs**: Shorter sequences improve efficiency but risk information loss; monolingual models are more efficient but may have coverage gaps; aggressive HPO saves resources but might miss optimal settings
- **Failure signatures**: Performance plateaus despite HPO (optimal settings found), dramatic performance drops with sequence shortening (information loss), memory errors during training (token limit exceeded)
- **First 3 experiments**:
  1. Compare tokenization output lengths of 2-3 recommended monolingual vs. multilingual models on sample IndoSum articles
  2. Test sequence shortening strategies with fixed hyperparameters to identify F1 score impact
  3. Run first two steps of HPO procedure to validate methodology

## Open Questions the Paper Calls Out

- **Trade-off between vocabulary size and model size**: How does this affect performance of Indonesian Transformer models in long-text classification? The paper notes the trade-off but lacks empirical analysis on performance impact.

- **Effective text-shortening strategies beyond stopwords**: What other techniques could improve Indonesian long-text classification? The paper only investigates basic removal methods without exploring advanced techniques like summarization.

- **Dynamic HPO vs. standard libraries**: How does the proposed procedure compare to standard optimization methods in efficiency and effectiveness? The paper claims superiority but provides no direct comparison with grid search, random search, or Bayesian optimization.

## Limitations

- Restricted hyperparameter search space (1-3 epochs) may miss optimal configurations requiring longer training
- Text-shortening strategies may not generalize to other languages or domains where stopwords carry semantic weight
- Monolingual model recommendation assumes sufficient vocabulary coverage without empirical validation of coverage gaps

## Confidence

**High Confidence**: Stopword removal achieving 83.01% F1 score is well-supported by experimental results with clear comparative metrics.

**Medium Confidence**: Monolingual model recommendation based on token count comparisons lacks direct performance validation against multilingual alternatives.

**Medium Confidence**: Dynamic HPO procedure shows promise but lacks comparison with standard optimization methods and has limited search space.

## Next Checks

1. Conduct ablation study to isolate individual contributions of stopword removal, punctuation retention, and low-frequency word preservation to the 83.01% F1 score.

2. Apply optimal text-shortening strategy and HPO procedure to a different Indonesian text classification task to verify cross-domain generalization.

3. Expand HPO search space to 4-10 epochs and broader learning rate ranges, then compare optimal configurations against the proposed 1-3 epoch procedure.