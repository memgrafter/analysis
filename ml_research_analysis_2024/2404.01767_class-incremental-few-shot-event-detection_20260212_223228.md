---
ver: rpa2
title: Class-Incremental Few-Shot Event Detection
arxiv_id: '2404.01767'
source_url: https://arxiv.org/abs/2404.01767
tags:
- learning
- event
- knowledge
- prompt-kd
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task, class-incremental few-shot event
  detection (CIFSED), which incrementally learns new event classes with only a few
  labeled instances while preserving knowledge of previously learned classes. The
  proposed method, Prompt-KD, combines an attention-based multi-teacher knowledge
  distillation framework with prompt learning.
---

# Class-Incremental Few-Shot Event Detection

## Quick Facts
- arXiv ID: 2404.01767
- Source URL: https://arxiv.org/abs/2404.01767
- Reference count: 0
- Key outcome: Prompt-KD achieves 5-8% F1 score improvement on FewEvent and 4-5% on MAVEN over baseline PA-CRF-CIL method

## Executive Summary
This paper introduces a new task called Class-Incremental Few-Shot Event Detection (CIFSED), which incrementally learns new event classes with only a few labeled instances while preserving knowledge of previously learned classes. The proposed Prompt-KD method combines an attention-based multi-teacher knowledge distillation framework with prompt learning to address the challenges of forgetting old knowledge and overfitting to new classes. Experiments on FewEvent and MAVEN datasets demonstrate that Prompt-KD significantly outperforms existing methods.

## Method Summary
Prompt-KD addresses CIFSED by combining attention-based multi-teacher knowledge distillation with prompt learning. The method uses an ancestor teacher model pre-trained on base classes and a father teacher model that adapts to new classes, with an attention mechanism balancing their contributions. Predefined prompts are concatenated with support instances to provide additional context and reduce overfitting in few-shot scenarios. A three-stage curriculum learning mechanism progressively adjusts prompt complexity based on learning session progression.

## Key Results
- Prompt-KD achieves average F1 score improvements of 5-8% on FewEvent compared to baseline PA-CRF-CIL method
- On MAVEN dataset, Prompt-KD shows 4-5% F1 score improvements over baseline
- The method effectively handles the forgetting problem while learning new classes with few labeled instances

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention-based multi-teacher knowledge distillation reduces forgetting of base knowledge while learning new classes
- Mechanism: Ancestor teacher (trained on base classes) is reused in all learning sessions, while father teacher adapts to new classes with attention balancing their contributions
- Core assumption: Ancestor teacher retains accurate base class knowledge even after multiple adaptation steps
- Evidence anchors:
  - [abstract] "To handle the forgetting problem about old knowledge, Prompt-KD develops an attention based multi-teacher knowledge distillation framework, where the ancestor teacher model pre-trained on base classes is reused in all learning sessions"
  - [section] "To overcome the old knowledge forgetting problem, Prompt-KD adopts two teacher models, i.e., the ancestor teacher modelT 0 and the father teacher model T m(m > 1)"
  - [corpus] Weak - no direct corpus evidence found about multi-teacher frameworks for event detection
- Break condition: If ancestor teacher's knowledge degrades over time or becomes misaligned with current task distribution

### Mechanism 2
- Claim: Prompt learning mitigates overfitting to new classes in few-shot scenarios
- Mechanism: Predefined prompts concatenated with support instances provide additional context and reduce dependence on limited labeled data
- Core assumption: Prompts provide sufficient semantic guidance to improve generalization without introducing harmful bias
- Evidence anchors:
  - [abstract] "in order to cope with the few-shot learning scenario and alleviate the corresponding new class overfitting problem, Prompt-KD is also equipped with a prompt learning mechanism"
  - [section] "Prompt-KD presents a new cloze prompt for joint FSED, which is designed as 'This is a [mask] event.[SEP] Its trigger words are [mask].'"
  - [corpus] Weak - while prompt learning exists in NLP, specific evidence for event detection is limited
- Break condition: If prompts are too generic or not well-aligned with event detection task

### Mechanism 3
- Claim: Curriculum learning through prompt stages progressively adapts the model to different learning phases
- Mechanism: Three-stage curriculum adjusts prompt complexity based on learning session progression (early, middle, late sessions)
- Core assumption: Gradually increasing prompt complexity helps model build understanding without overwhelming it
- Evidence anchors:
  - [section] "This module is equipped with a prompt-oriented curriculum learning mechanism. To the best of our knowledge, this paper is the first to combine curriculum learning with prompt learning"
  - [section] "At Stage 2, the candidate set of [mask*] is {“before”, “now”}, where 'before' indicates that the event class has been learned in the past, while 'now' denotes the event class is being learned at present"
  - [corpus] Weak - no direct corpus evidence found about curriculum learning for event detection prompts
- Break condition: If curriculum stages don't align well with actual learning progression or prompt adjustments are too subtle

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: To transfer knowledge from teacher models to student models while maintaining performance on previously learned classes
  - Quick check question: What is the difference between knowledge distillation and standard supervised learning?

- Concept: Few-shot Learning
  - Why needed here: The task involves learning new event classes with only a few labeled instances
  - Quick check question: How does few-shot learning differ from traditional supervised learning in terms of data requirements?

- Concept: Prompt Learning
  - Why needed here: To provide additional context and guidance for the model when dealing with limited training data
  - Quick check question: What is the primary advantage of using prompts in few-shot learning scenarios?

## Architecture Onboarding

- Component map: Encoder (BERT-base-uncased) → Emission Unit → Transition Unit → Decoder Unit → Attention Mechanism → Student Model
- Critical path: Support instances with prompts → Encoder → Emission/Transition units → Teacher predictions → Attention-weighted combination → Student model update
- Design tradeoffs: Using ancestor teacher preserves base knowledge but may slow adaptation to new classes; prompts help few-shot learning but add complexity
- Failure signatures: Poor performance on base classes indicates forgetting problem; poor performance on new classes indicates overfitting; inconsistent results across sessions suggest attention mechanism issues
- First 3 experiments:
  1. Baseline: Test PA-CRF-CIL on FewEvent 5-way 1-shot to establish performance floor
  2. Component isolation: Test Prompt-KD without attention mechanism to measure its contribution
  3. Curriculum validation: Test Prompt-KD with only stage 1 prompts to verify curriculum learning effect

Assumption: BERT encoder provides sufficient semantic understanding for event detection, though this could be validated through ablation studies.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Prompt-KD compare to other class-incremental learning methods in other NLP tasks beyond event detection?
- Basis in paper: [inferred] The paper demonstrates Prompt-KD's effectiveness in class-incremental few-shot event detection but does not explore its performance in other NLP tasks
- Why unresolved: The paper focuses specifically on event detection and does not provide a comparative analysis with other NLP tasks
- What evidence would resolve it: Experiments comparing Prompt-KD with other class-incremental learning methods in various NLP tasks such as named entity recognition, relation extraction, and sentiment analysis

### Open Question 2
- Question: What is the impact of different prompt designs on the performance of Prompt-KD?
- Basis in paper: [explicit] The paper presents a specific prompt design for event detection but does not explore the impact of alternative prompt designs
- Why unresolved: The paper only uses one prompt design and does not investigate how different prompt designs might affect the model's performance
- What evidence would resolve it: Experiments testing various prompt designs and their impact on the model's performance in class-incremental few-shot event detection

### Open Question 3
- Question: How does the attention mechanism in Prompt-KD balance the contributions of the ancestor and father teacher models in different learning sessions?
- Basis in paper: [explicit] The paper describes the attention mechanism but does not provide a detailed analysis of how it balances the contributions of the two teacher models
- Why unresolved: The paper mentions the attention mechanism but does not delve into the specifics of how it adjusts the weights of the teacher models over time
- What evidence would resolve it: A detailed analysis of the attention mechanism's behavior across different learning sessions, including visualizations and quantitative measures of the weight adjustments

## Limitations

- The paper doesn't address potential degradation of the ancestor teacher's knowledge over time, which could undermine the entire distillation framework
- The prompts' impact on different event types or domains is not explored, raising questions about generalizability
- The computational overhead of maintaining multiple teacher models is not discussed, which could limit practical deployment

## Confidence

- Mechanism 1 (Attention-based multi-teacher distillation): Medium - relies on untested assumption about ancestor teacher knowledge retention
- Mechanism 2 (Prompt learning): Medium - improvements shown but specific prompt stage contributions not isolated
- Mechanism 3 (Curriculum learning): Medium - innovative approach but lacks direct corpus evidence or comparative analysis

## Next Checks

1. Conduct ablation studies removing the ancestor teacher to quantify knowledge retention vs forgetting
2. Test the prompt stages individually to measure their specific contributions to performance
3. Evaluate the model's robustness when the ancestor teacher has been updated multiple times versus fresh pre-training