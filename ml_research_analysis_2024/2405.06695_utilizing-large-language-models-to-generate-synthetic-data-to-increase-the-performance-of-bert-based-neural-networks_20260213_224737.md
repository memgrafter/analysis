---
ver: rpa2
title: Utilizing Large Language Models to Generate Synthetic Data to Increase the
  Performance of BERT-Based Neural Networks
arxiv_id: '2405.06695'
source_url: https://arxiv.org/abs/2405.06695
tags:
- data
- gpt-4
- autism
- gpt-3
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated using large language models (LLMs) like ChatGPT
  and GPT-4 to generate synthetic data for improving machine learning models in autism
  diagnosis. Researchers prompted these models to create 4,200 synthetic observations
  of autism-related behaviors, which were then used to augment an existing medical
  dataset.
---

# Utilizing Large Language Models to Generate Synthetic Data to Increase the Performance of BERT-Based Neural Networks

## Quick Facts
- arXiv ID: 2405.06695
- Source URL: https://arxiv.org/abs/2405.06695
- Reference count: 27
- Primary result: LLM-generated synthetic data improves recall (+13%) but reduces precision (-16%) for autism diagnosis ML models.

## Executive Summary
This study explores the use of large language models (LLMs) like ChatGPT and GPT-4 to generate synthetic medical observations for improving machine learning models in autism diagnosis. Researchers prompted these models to create 4,200 synthetic observations of autism-related behaviors, which were then used to augment an existing medical dataset. The synthetic data was reviewed by a clinician and found to be 83% accurate. When added to the training data, the augmented dataset increased recall by 13% but decreased precision by 16%, suggesting the synthetic data improved sensitivity but introduced more false positives. The findings indicate LLMs can help address data scarcity in medical ML but require expert review to ensure quality and may need refinement to balance recall and precision.

## Method Summary
The study used an existing dataset of free-text medical observations from the CDC Autism and Developmental Disabilities Monitoring (ADDM) Network, labeled by experts according to DSM-5 criteria for autism (A1-A3, B1-B4). Synthetic data was generated by prompting ChatGPT and GPT-4 to produce 4,200 new observations, split into 1,050 using GPT-3.5 and GPT-4, and 3,150 using GPT-4 only, across three gender categories and seven autism symptom labels. A BioBERT model pre-trained on biomedical literature was fine-tuned on the original and augmented datasets. The model's performance was evaluated using precision, recall, and F1-score, with accuracy not reported due to class imbalance. Expert review assessed the quality and accuracy of synthetic data, with 83% of labels deemed correct.

## Key Results
- LLM-generated synthetic data increased recall by 13% but decreased precision by 16% when used to augment autism diagnosis training data
- Expert review found 83% of synthetic observations to be clinically accurate
- Synthetic data helped address data scarcity for rare autism criteria but introduced more false positives
- BioBERT classifier trained on augmented data showed improved sensitivity to true autism cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated synthetic text can expand training datasets for biomedical NLP models.
- Mechanism: The LLM creates plausible, domain-specific text examples (here, medical observations of autism behaviors) that are labeled according to DSM-5 criteria, then these are added to existing training data to increase sample diversity.
- Core assumption: The LLM can generate text that is both linguistically coherent and semantically consistent with the target domain when given structured prompts.
- Evidence anchors:
  - [abstract] Researchers prompted ChatGPT and GPT-4 to generate 4,200 synthetic observations of autism-related behaviors, which were then used to augment an existing medical dataset.
  - [section] Using Autism Spectrum Disorders (ASD), we prompted ChatGPT and GPT-Premium to generate 4,200 synthetic observations to augment existing medical data.
- Break condition: If the LLM consistently generates text that is clinically inaccurate, nonsensical, or outside the domain, the augmentation will harm rather than help model performance.

### Mechanism 2
- Claim: Expert review of synthetic data is necessary to ensure label accuracy and clinical validity.
- Mechanism: A domain expert (pediatrician) evaluates a random sample of generated observations on multiple dimensions (common in autism, uncommon in general population, not typical of other disorders, and plausible in EHR notes) and flags incorrect or incomplete labels.
- Core assumption: Human expertise is required to validate synthetic text because LLMs can produce plausible-sounding but incorrect content, especially in specialized domains.
- Evidence anchors:
  - [abstract] The synthetic data was reviewed by a clinician and found to be 83% accurate.
  - [section] The clinical expert evaluated each observation on four dimensions using a 1-5 scale and also noted if the given label was correct, incorrect, or missing additional labels.
- Break condition: If the expert review process is skipped or becomes a bottleneck, incorrect labels will be introduced into the training data, degrading model reliability.

### Mechanism 3
- Claim: Augmenting training data with synthetic examples improves recall at the expense of precision for the classification task.
- Mechanism: By increasing the number of examples for underrepresented autism criteria, the model learns to recognize more true positives (higher recall), but also increases false positives (lower precision) because synthetic data may not perfectly match the distribution of real clinical text.
- Evidence anchors:
  - [abstract] When added to the training data, the augmented dataset increased recall by 13% but decreased precision by 16%.
  - [section] Augmenting data increased recall by 13% but decreased precision by 16%, correlating with higher quality and lower accuracy across pairs.
- Break condition: If precision drops below an acceptable threshold for the application (e.g., clinical diagnosis), the synthetic augmentation is not viable without further refinement.

## Foundational Learning

- Concept: DSM-5 autism diagnostic criteria (A1-A3, B1-B4)
  - Why needed here: The study relies on labeling text observations according to these specific behavioral criteria to generate and evaluate synthetic data.
  - Quick check question: Can you list the seven DSM-5 autism symptom categories used as labels in this study?

- Concept: BERT-based text classification and BioBERT pre-training
  - Why needed here: The model used is a BioBERT multilabel neural network fine-tuned on biomedical abstracts; understanding its architecture and training is key to interpreting results.
  - Quick check question: What is the main difference between standard BERT and BioBERT in terms of pre-training data?

- Concept: Recall vs. precision trade-off in classification metrics
  - Why needed here: The synthetic augmentation improved recall but reduced precision; understanding this trade-off is essential for evaluating model utility.
  - Quick check question: If a model has high recall but low precision, what does that imply about its predictions?

## Architecture Onboarding

- Component map:
  Prompting engine -> Domain expert review module -> BioBERT classifier -> Evaluation pipeline

- Critical path:
  1. Generate synthetic observations for each DSM-5 criterion using LLM.
  2. Expert reviews a random sample for accuracy and labels.
  3. Augment original training set with vetted synthetic data.
  4. Fine-tune BioBERT on augmented dataset.
  5. Evaluate performance change on test set.

- Design tradeoffs:
  - Quantity vs. quality: More synthetic data can improve recall but may introduce noise; expert review mitigates but does not eliminate this risk.
  - Prompt specificity: More detailed prompts may yield higher-quality synthetic text but reduce diversity; simpler prompts increase variety but risk incoherence.
  - Model choice: BioBERT leverages biomedical domain knowledge, but other models (e.g., ClinicalBERT) might yield different results.

- Failure signatures:
  - Large drop in precision with modest recall gains indicates synthetic data is introducing too many false positives.
  - Consistently low expert accuracy scores (e.g., <80%) suggest the LLM is not generating reliable text for the domain.
  - If recall does not improve despite synthetic augmentation, the generated text may not match the real data distribution.

- First 3 experiments:
  1. Generate a small set of synthetic observations for one DSM-5 criterion (e.g., A1), have expert review, and measure the proportion correct.
  2. Fine-tune BioBERT on a dataset augmented only with vetted synthetic examples for that criterion; evaluate recall/precision change.
  3. Repeat for another criterion with low sample count (e.g., B3) and compare performance impact to baseline.

## Open Questions the Paper Calls Out
- What specific prompt modifications could improve the balance between recall and precision in LLM-generated synthetic data for autism diagnosis?
- How do different synthetic data traits (e.g., gender specification, symptom focus) affect the performance of ML models in autism diagnosis?
- Can LLMs generate complete synthetic patient records spanning multiple years that accurately simulate autism diagnosis progression?

## Limitations
- Expert review was limited to a small sample (10%) of synthetic data, potentially missing systematic errors
- The study used only one medical dataset (ADDM), limiting generalizability to other domains
- The 17% error rate in synthetic data was not quantified for its clinical impact

## Confidence
- High confidence in the feasibility of using LLMs to generate synthetic data for biomedical NLP tasks
- Medium confidence in the clinical accuracy and utility of the generated synthetic data, due to the limited expert review sample size and lack of quantification of clinical impact
- Low confidence in the scalability and generalizability of the approach to other medical domains or diagnostic tasks without further validation

## Next Checks
1. Conduct a larger-scale expert review of synthetic data (e.g., 30% sample) to more robustly estimate clinical accuracy and identify error patterns
2. Perform ablation studies to determine the minimum viable set of prompts and review steps needed to maintain high-quality synthetic data
3. Test the approach on a different biomedical dataset (e.g., clinical notes for another condition) to assess generalizability and robustness to domain shifts