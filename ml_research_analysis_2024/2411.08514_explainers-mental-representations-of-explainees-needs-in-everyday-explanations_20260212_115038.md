---
ver: rpa2
title: Explainers' Mental Representations of Explainees' Needs in Everyday Explanations
arxiv_id: '2411.08514'
source_url: https://arxiv.org/abs/2411.08514
tags:
- interests
- knowledge
- explanations
- were
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated how explainers (EXs) form mental representations
  of explainees'' (EEs'') knowledge and interests during everyday explanations of
  technological artifacts. Using semi-structured interviews and video recall with
  9 EX-EE pairs, researchers analyzed how EXs'' assumptions about EEs'' understanding
  of the game Quarto developed across three phases: pre-explanation, during explanation,
  and post-explanation.'
---

# Explainers' Mental Representations of Explainees' Needs in Everyday Explanations

## Quick Facts
- arXiv ID: 2411.08514
- Source URL: https://arxiv.org/abs/2411.08514
- Reference count: 40
- Primary result: Explainers' mental representations of explainees' knowledge evolve during explanations, with initial assumptions being vague but becoming more defined, particularly showing a shift from focusing on observable features to incorporating interpretable aspects.

## Executive Summary
This study investigates how explainers form mental representations of explainees' knowledge and interests during everyday explanations of technological artifacts. Through semi-structured interviews and video recall with 9 EX-EE pairs explaining the game Quarto, researchers analyzed how explainers' assumptions about explainees' understanding developed across three phases. The dual nature theory was applied to distinguish between Architecture (observable features) and Relevance (interpretable aspects) of the artifact. Results show that explainers' initial assumptions were vague but became more defined during explanations, with a notable shift from focusing primarily on Architecture to incorporating both Architecture and Relevance aspects.

## Method Summary
The study employed semi-structured interviews and video recall methodology with 9 pairs of explainers and explainees. Participants were asked to explain the game Quarto, and their interactions were video recorded. After the explanation, participants reviewed the video footage during interviews where they reflected on their mental representations of the explainee's knowledge and interests. The researchers analyzed how these mental representations evolved across three phases: pre-explanation, during explanation, and post-explanation. The dual nature theory framework was used to categorize the aspects of the game that explainers focused on, distinguishing between Architecture (observable features) and Relevance (interpretable aspects).

## Key Results
- Explainers' initial assumptions about explainees' knowledge were vague but became more defined during explanations
- There was a shift from focusing primarily on Architecture to incorporating both Architecture and Relevance aspects
- Explainers often underestimated explainees' knowledge gaps, particularly regarding Relevance aspects

## Why This Works (Mechanism)
The study reveals that explainers develop and refine their mental models of explainees' knowledge through iterative interaction. As explanations progress, explainers receive feedback through questions, responses, and non-verbal cues that help them calibrate their assumptions. This dynamic process allows for the evolution from vague initial representations to more nuanced understanding of what the explainee knows and needs to know. The mechanism works through continuous updating of mental models based on observed interaction patterns.

## Foundational Learning
1. **Dual Nature Theory**: Distinguishes between observable features (Architecture) and interpretable aspects (Relevance) of technological artifacts. Why needed: Provides framework for analyzing what aspects explainers focus on. Quick check: Can you identify both Architecture and Relevance aspects in a simple technology?
2. **Mental Model Formation**: Process by which explainers construct representations of explainees' knowledge. Why needed: Central to understanding how effective explanations are tailored. Quick check: Can you describe how your mental model of someone's knowledge changes during an explanation?
3. **Knowledge Gap Assessment**: Ability to identify what explainees don't know. Why needed: Critical for providing appropriate explanations. Quick check: Can you list potential knowledge gaps when explaining a familiar concept to a novice?
4. **Interactive Calibration**: Process of refining mental models through feedback during explanation. Why needed: Explains how explanations become more effective over time. Quick check: What feedback signals do you use to adjust your explanations?

## Architecture Onboarding
Component Map: EX -> Mental Representation -> Explanation Strategy -> EE Feedback -> Updated Mental Representation
Critical Path: EX forms initial mental representation → delivers explanation → receives feedback → updates mental representation → refines explanation
Design Tradeoffs: Depth vs. breadth of explanation, technical vs. intuitive language, explicit vs. implicit knowledge assessment
Failure Signatures: Persistent misunderstandings, disengagement, excessive questions, confusion signals
First Experiments:
1. Test whether explainers systematically underestimate Relevance knowledge gaps across different domains
2. Measure the correlation between explanation quality and accuracy of mental representations
3. Compare mental representation accuracy between novice and expert explainers

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of only 9 EX-EE pairs limits generalizability
- All participants recruited from a single German university introduces cultural and educational bias
- Use of a single game (Quarto) as explanation domain constrains applicability to other contexts

## Confidence
- High Confidence: Explainers' mental representations evolve from vague to more defined assumptions during explanations
- Medium Confidence: Explainers tend to underestimate explainees' knowledge gaps regarding Relevance aspects
- Medium Confidence: Applicability of dual nature theory to everyday explanations

## Next Checks
1. Replicate the study with a larger, more diverse sample across different cultural contexts and explanation domains to assess generalizability of findings
2. Implement real-time measurement of explainers' mental representations during explanations to reduce recall bias and capture more immediate cognitive processes
3. Conduct controlled experiments manipulating specific aspects of explainees' knowledge (Architecture vs. Relevance) to test whether explainers systematically underestimate knowledge gaps in particular domains as suggested by the findings