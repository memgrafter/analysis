---
ver: rpa2
title: Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors
  in LLMs
arxiv_id: '2407.15549'
source_url: https://arxiv.org/abs/2407.15549
tags:
- arxiv
- preprint
- training
- unlearning
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Latent adversarial training (LAT) can improve robustness to persistent
  harmful behaviors in large language models (LLMs) by training on perturbations to
  hidden latent representations rather than input perturbations. The authors introduce
  targeted LAT, where adversaries seek to minimize loss on specific undesirable tasks,
  and apply it to three problems: improving robustness to jailbreaks, removing backdoors
  without trigger knowledge, and robustly unlearning unwanted knowledge.'
---

# Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs

## Quick Facts
- arXiv ID: 2407.15549
- Source URL: https://arxiv.org/abs/2407.15549
- Reference count: 33
- Improves robustness to harmful behaviors by training on latent representation perturbations rather than input perturbations

## Executive Summary
This paper introduces targeted latent adversarial training (LAT), a method that improves robustness to persistent harmful behaviors in large language models by training on perturbations to hidden latent representations rather than input perturbations. The authors demonstrate that LAT can effectively address three distinct problems: improving robustness to jailbreaks, removing backdoors without trigger knowledge, and robustly unlearning unwanted knowledge. Applied to Llama-2 and Llama-3 models, targeted LAT consistently outperformed state-of-the-art baselines while preserving general capabilities. The method shows particular promise for removing neural circuitry responsible for harmful behaviors with minimal tradeoffs.

## Method Summary
Targeted LAT applies adversarial training to latent representations rather than inputs, where an adversary seeks to minimize loss on specific undesirable tasks by perturbing hidden activations. The approach uses L2-norm bounded perturbations applied at multiple residual stream layers through projected gradient descent. The authors combine LAT with existing fine-tuning methods including refusal training for jailbreaks, direct preference optimization (DPO) for backdoor removal, and various unlearning techniques. For each application, the adversary actively tries to make the model generate harmful outputs while the model learns to resist these perturbations. Layer selection uses a heuristic of four evenly spaced layers, and perturbations are constrained to ϵ = 0.4 times the standard deviation of each residual stream dimension.

## Key Results
- LAT outperformed R2D2 for jailbreak robustness with over 700x less compute
- LAT enabled effective backdoor removal where standard DPO failed
- LAT improved unlearning effectiveness over WHP, gradient ascent, and RMU methods while making re-learning more difficult
- General capabilities were preserved across MMLU, MT-Bench, and compliance metrics

## Why This Works (Mechanism)
None provided in source material.

## Foundational Learning
- Latent adversarial training: Adversarial training applied to hidden model representations rather than inputs, enabling more effective manipulation of internal representations responsible for harmful behaviors
- Why needed: Standard input-space adversarial training is less effective at removing persistent harmful behaviors because it cannot directly target the internal representations that encode these behaviors
- Quick check: Verify that LAT perturbations affect residual stream activations while preserving input token sequences

- Projected gradient descent on latent spaces: Iterative optimization technique that finds worst-case perturbations within bounded regions of the latent space
- Why needed: Allows controlled exploration of the latent space to find representations that trigger harmful behaviors without destabilizing training
- Quick check: Confirm that PGD converges to meaningful perturbations that increase harmful output probability

- Residual stream perturbation: Applying adversarial modifications to the transformer's residual connections at specific layers
- Why needed: Residual streams contain rich information about the model's intermediate processing and are key pathways for controlling behavior
- Quick check: Measure activation changes in residual streams before and after LAT training

## Architecture Onboarding

Component map: Dataset -> Targeted LAT attack function α -> LLM fine-tuning -> Evaluation on harmful/harmless tasks

Critical path: LAT requires first defining the harmful behavior to target, then implementing the adversarial attack that minimizes loss on that behavior, followed by fine-tuning the LLM to resist these latent perturbations. The critical evaluation loop measures both robustness to attacks and preservation of general capabilities.

Design tradeoffs: LAT trades computational overhead of latent-space adversarial training against improved effectiveness in removing harmful behaviors. The method requires careful layer selection and perturbation constraint tuning, but can work with any base fine-tuning approach. Key tension exists between perturbation magnitude (larger values more effective but risk capability degradation) and layer selection (more layers more thorough but computationally expensive).

Failure signatures: Training instability from NaN values or exploding gradients indicates overly aggressive perturbations. Degradation in general capabilities suggests perturbation bounds are too loose. Ineffective unlearning despite LAT training indicates layer selection is missing key representation pathways or perturbation constraints are too tight.

First experiments: 1) Implement targeted LAT on a small model with synthetic harmful behaviors to verify the core mechanism works, 2) Conduct ablation studies varying perturbation magnitude (ϵ) and layer count to find optimal hyperparameters, 3) Test LAT with different base fine-tuning methods (DPO, RLHF, supervised fine-tuning) to assess compatibility.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the effectiveness of targeted LAT scale with model size beyond the 10 billion parameter limit tested in this paper?
- Basis in paper: The authors explicitly note that all experiments were conducted on models with fewer than 10 billion parameters and speculate that LAT's usefulness will extend to larger models, but future work is needed to confirm this.
- Why unresolved: The paper does not provide empirical evidence for model scaling effects, and the authors acknowledge this as a limitation.
- What evidence would resolve it: Experiments demonstrating LAT performance on models significantly larger than 10B parameters (e.g., 30B, 70B, 175B) with comparable or improved results relative to the tested smaller models.

### Open Question 2
- Question: What is the optimal layer configuration and perturbation constraint for LAT across different model architectures and tasks?
- Basis in paper: The authors mention that selection of dataset, layer(s), and perturbation size were influential, and they used a heuristic of selecting four evenly spaced layers. They also note that LAT can be done with various parameterizations and constraints.
- Why unresolved: The paper uses a fixed layer selection heuristic and single perturbation constraint that was empirically selected for one task (jailbreaks on Llama-2), but does not systematically explore optimal configurations for different architectures or tasks.
- What evidence would resolve it: Systematic ablation studies varying layer selection strategies (including non-uniform spacing, different numbers of layers), perturbation constraint methods (L2 vs other norms), and testing across multiple model families and task types.

### Open Question 3
- Question: How does LAT compare to other latent-space manipulation techniques like representation engineering and sparse autoencoder-based attacks?
- Basis in paper: The authors mention concurrent work on other latent-space manipulation techniques and express interest in comparing LAT to these approaches and exploring whether LAT can improve them.
- Why unresolved: The paper focuses solely on LAT and does not benchmark against other latent-space methods that have emerged concurrently or provide comparative analysis.
- What evidence would resolve it: Head-to-head comparisons of LAT against representation engineering, sparse autoencoder perturbations, low-rank adapter attacks, and other latent-space methods on identical tasks and models, measuring both effectiveness and computational efficiency.

## Limitations
- All experiments conducted on models with fewer than 10 billion parameters, leaving scaling to larger models untested
- Layer selection and perturbation constraints use heuristics rather than systematic optimization across tasks
- No comparison to other concurrent latent-space manipulation techniques that have emerged

## Confidence

| Claim | Confidence |
|-------|------------|
| Targeted LAT consistently improves over baselines while preserving capabilities | Medium |
| LAT enables effective unlearning where standard DPO failed | Medium |
| Applying adversarial training to latent representations is a novel approach | High |

## Next Checks
1. Conduct ablations varying perturbation magnitude (ϵ) and layer selection to determine optimal LAT hyperparameters
2. Test LAT on additional harmful behavior types beyond the three studied to assess generalizability
3. Implement the adversarial attack function α independently to verify reproducibility of the core LAT mechanism