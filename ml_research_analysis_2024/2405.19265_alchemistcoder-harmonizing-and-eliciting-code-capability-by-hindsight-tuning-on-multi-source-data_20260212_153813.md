---
ver: rpa2
title: 'AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning
  on Multi-source Data'
arxiv_id: '2405.19265'
source_url: https://arxiv.org/abs/2405.19265
tags:
- code
- data
- instruction
- arxiv
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AlchemistCoder integrates multi-source data for Code LLM fine-tuning
  by harmonizing inherent conflicts through data-specific AlchemistPrompts and incorporating
  code comprehension tasks. The method introduces hindsight relabeling to align instruction-response
  pairs and improve instruction-following capabilities.
---

# AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight Tuning on Multi-source Data

## Quick Facts
- arXiv ID: 2405.19265
- Source URL: https://arxiv.org/abs/2405.19265
- Reference count: 40
- One-line primary result: AlchemistCoder (6.7B/7B) surpasses same-size models and rivals larger ones, achieving up to 79.9% pass@1 on HumanEval and 77.0% on MBPP.

## Executive Summary
AlchemistCoder is a series of Code Large Language Models (Code LLMs) that integrates multi-source data through fine-tuning to overcome limitations in quality and diversity inherent in single-source data. The method harmonizes inherent conflicts between different data sources using data-specific AlchemistPrompts and incorporates code comprehension tasks like instruction evolution, data filtering, and code review. The approach introduces hindsight relabeling to align instruction-response pairs and improve instruction-following capabilities. Experimental results show AlchemistCoder models outperform same-size models and rival larger ones on coding benchmarks.

## Method Summary
AlchemistCoder fine-tunes base models (Llama-2-7B, CodeLlama-Python-7B, DeepSeek-Coder-Base-6.7B) using a harmonized multi-source dataset that includes open-source datasets, EvolCode data, AlchemistPrompt-customized data, and code comprehension task data. The method uses GPT-4 to generate AlchemistPrompts that augment instructions with specific programming languages, algorithm concepts, and code-related information, effectively harmonizing conflicts between data sources. Code comprehension tasks are constructed from instruction evolution, data filtering, and code review processes. Models are fine-tuned for 2 epochs with Adam optimizer, learning rate 1e-4, batch size 2, and sequence length 8192.

## Key Results
- AlchemistCoder (6.7B/7B) surpasses same-size models and rivals larger models (15B/33B/70B) on coding benchmarks
- Achieves up to 79.9% pass@1 on HumanEval and 77.0% on MBPP
- Improves model generalization on MMLU, BBH, and GSM8K benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-source data integration enhances model performance by increasing diversity and reducing domain gaps.
- Mechanism: By harmonizing different data sources through AlchemistPrompts, the model learns to handle diverse coding styles and requirements, leading to improved generalization.
- Core assumption: Different data sources contain complementary information that can enhance the model's capabilities when properly integrated.
- Evidence anchors: [abstract] "AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B)"; [section 2.2] "AlchemistPrompts are designed to augment instructions with specific programming languages, algorithm concepts, and other code-related information involved in responses, which can refine the alignment within instruction-response pairs and enhance the instruction-following abilities of fine-tuned models."

### Mechanism 2
- Claim: Code comprehension tasks improve the model's understanding of code-related concepts.
- Mechanism: Incorporating tasks like instruction evolution, data filtering, and code review into the training process allows the model to learn higher-level abilities beyond simple code generation.
- Core assumption: Understanding the process of code data construction enhances the model's ability to generate and reason about code.
- Evidence anchors: [section 2.3] "We contend that beyond this, the process of constructing code data demonstrates higher-level abilities. Consequently, we devise three code comprehension tasks relevant to data construction"; [abstract] "AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B)".

### Mechanism 3
- Claim: Hindsight relabeling through AlchemistPrompts improves instruction-following capabilities.
- Mechanism: By retrospectively analyzing previous responses and reinterpreting them as alternate goals, the model learns to better align instructions with responses.
- Core assumption: Retrospective analysis can provide valuable insights into improving instruction-response alignment.
- Evidence anchors: [section 2.2] "Crucially, by retrospectively analyzing previous responses and reinterpreting them as alternate goals, the AlchemistPrompts serve to elevate the condition/goal of the data."; [abstract] "AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B)".

## Foundational Learning

- Concept: Multi-source data integration
  - Why needed here: To overcome the limitations of quality and diversity inherent in single-source data, enhancing the model's performance and generalization.
  - Quick check question: What are the potential risks of blindly integrating multi-source data without harmonization?

- Concept: Code comprehension tasks
  - Why needed here: To provide the model with higher-level abilities beyond simple code generation, improving its understanding of code-related concepts.
  - Quick check question: How do instruction evolution, data filtering, and code review tasks contribute to the model's learning?

- Concept: Hindsight relabeling
  - Why needed here: To improve instruction-following capabilities by retrospectively analyzing and reinterpreting responses as alternate goals.
  - Quick check question: What is the role of AlchemistPrompts in hindsight relabeling?

## Architecture Onboarding

- Component map: Multi-source data collection -> AlchemistPrompt generation -> Code comprehension task creation -> Fine-tuning module -> Trained model
- Critical path: 1. Integrate multi-source data 2. Generate AlchemistPrompts for harmonization 3. Create code comprehension tasks 4. Fine-tune the model on the harmonized data
- Design tradeoffs:
  - Balancing data diversity with domain gaps when integrating multi-source data
  - Deciding the proportion of AlchemistPrompt-customized data to include
  - Choosing which code comprehension tasks to incorporate and how much emphasis to give each
- Failure signatures:
  - Poor performance on benchmarks if data harmonization is ineffective
  - Overfitting to specific coding styles if multi-source data is not diverse enough
  - Confusion or misalignment if hindsight relabeling introduces too much noise
- First 3 experiments:
  1. Test the impact of different proportions of AlchemistPrompt-customized data on model performance
  2. Evaluate the effectiveness of individual code comprehension tasks by including/excluding each one
  3. Compare the performance of models fine-tuned on multi-source data vs. single-source data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do AlchemistPrompts specifically improve instruction-following capabilities beyond simply harmonizing data sources?
- Basis in paper: [explicit] The paper states AlchemistPrompts "refine the alignment within instruction-response pairs" and "enhance the instruction-following abilities of fine-tuned models."
- Why unresolved: While the paper demonstrates improved performance metrics, it doesn't provide detailed analysis of how AlchemistPrompts affect the model's understanding of instructions versus responses.
- What evidence would resolve it: Detailed ablation studies comparing model performance with and without AlchemistPrompts on tasks specifically designed to test instruction-following versus general code generation capabilities.

### Open Question 2
- Question: What is the optimal ratio of AlchemistPrompt-customized data to maintain the balance between data diversity and domain gap?
- Basis in paper: [explicit] The paper mentions they found 5% to be optimal but doesn't explore the full range of possible ratios.
- Why unresolved: The paper only tests a narrow range of percentages and doesn't provide theoretical justification for why 5% is optimal.
- What evidence would resolve it: Extensive experiments testing a wider range of ratios (e.g., 1%, 3%, 5%, 10%, 15%, 20%) and analysis of how this affects both performance and model behavior.

### Open Question 3
- Question: How do code comprehension tasks (instruction evolution, data filtering, code review) contribute to improved code generation performance?
- Basis in paper: [explicit] The paper states these tasks "facilitate enhanced code comprehension capabilities" but doesn't quantify their individual contributions.
- Why unresolved: While ablation studies show combined benefits, the paper doesn't isolate which task contributes most to performance gains.
- What evidence would resolve it: Individual ablation studies for each code comprehension task, along with analysis of whether certain tasks benefit specific types of code generation problems more than others.

### Open Question 4
- Question: What is the long-term impact of AlchemistCoder-style fine-tuning on model generalization and potential catastrophic forgetting?
- Basis in paper: [inferred] The paper mentions improvements on MMLU, BBH, and GSM8K, suggesting some generalization, but doesn't examine long-term effects.
- Why unresolved: The paper only evaluates performance immediately after fine-tuning without examining how the model performs on non-code tasks over time.
- What evidence would resolve it: Longitudinal studies tracking model performance on diverse benchmarks over multiple fine-tuning iterations and extended periods of use.

## Limitations

- The exact formulation of AlchemistPrompts, data filtering criteria, and hindsight relabeling methodology remain underspecified, making direct reproduction challenging.
- The evaluation relies on standard benchmarks without ablation studies to isolate the contributions of individual components like multi-source integration, code comprehension tasks, and hindsight relabeling.
- The paper doesn't provide detailed analysis of how different data sources contribute to the performance improvements.

## Confidence

- **High Confidence**: The claim that AlchemistCoder outperforms same-size models and rivals larger ones is well-supported by the reported benchmark results (79.9% pass@1 on HumanEval, 77.0% on MBPP). The experimental setup, including model sizes and evaluation metrics, is clearly specified.
- **Medium Confidence**: The effectiveness of multi-source data integration through AlchemistPrompts is supported by the performance gains but lacks detailed analysis of how different data sources contribute. The claim that hindsight relabeling improves instruction-following capabilities is plausible but not directly validated through ablation studies.
- **Low Confidence**: The assertion that code comprehension tasks significantly enhance model capabilities is weakly supported. While the tasks are described, there's insufficient evidence showing their individual or combined impact on model performance. The claim that these tasks demonstrate "higher-level abilities" remains largely theoretical without empirical validation.

## Next Checks

1. **Ablation Study on Data Sources**: Conduct controlled experiments to measure the contribution of each data source (Evol-Instruct-Code-80k-v1, CodeExercise-Python-27k, evolcodealpaca-v1, and AlchemistPrompt-customized data) to overall performance. This would clarify whether multi-source integration or specific data types drive the improvements.

2. **Isolation of Code Comprehension Tasks**: Systematically evaluate each code comprehension task (instruction evolution, data filtering, code review) by training models with only one task type at a time. This would determine which tasks are essential and quantify their individual contributions to model capabilities.

3. **Hindsight Relabeling Validation**: Design an experiment comparing models trained with and without hindsight relabeling on a subset of data where ground truth instruction-response alignment can be verified. This would directly test whether the relabeling mechanism improves instruction-following capabilities as claimed.