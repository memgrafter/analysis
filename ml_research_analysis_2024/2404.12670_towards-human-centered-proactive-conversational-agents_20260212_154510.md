---
ver: rpa2
title: Towards Human-centered Proactive Conversational Agents
arxiv_id: '2404.12670'
source_url: https://arxiv.org/abs/2404.12670
tags:
- user
- conversational
- proactive
- human
- pcas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This perspectives paper emphasizes the need to develop human-centered
  proactive conversational agents (PCAs) that prioritize human needs and ethical considerations
  alongside technological advancement. It proposes a new taxonomy with three key dimensions:
  Intelligence, Adaptivity, and Civility, categorizing PCAs into eight types.'
---

# Towards Human-centered Proactive Conversational Agents

## Quick Facts
- arXiv ID: 2404.12670
- Source URL: https://arxiv.org/abs/2404.12670
- Authors: Yang Deng; Lizi Liao; Zhonghua Zheng; Grace Hui Yang; Tat-Seng Chua
- Reference count: 40
- One-line primary result: Proposes a taxonomy for human-centered proactive conversational agents with three dimensions—Intelligence, Adaptivity, Civility—to improve design tradeoffs and ethical considerations.

## Executive Summary
This perspectives paper addresses the growing need for proactive conversational agents (PCAs) that prioritize human needs and ethical considerations alongside technological advancement. It introduces a new taxonomy categorizing PCAs along three key dimensions—Intelligence, Adaptivity, and Civility—enabling designers to reason about tradeoffs between task effectiveness, user comfort, and perceived intrusiveness. The paper discusses challenges in building human-centered PCAs across five stages of system construction, highlighting issues like fabricated user needs in data preparation and the importance of aligning PCAs with human values through various learning approaches.

## Method Summary
This paper does not present a specific ML task or model to reproduce. Instead, it is a perspectives piece that discusses the concept of human-centered proactive conversational agents (PCAs) and proposes a taxonomy for categorizing them based on three dimensions: Intelligence, Adaptivity, and Civility. The authors analyze existing PCA literature and datasets, discussing issues with current approaches and proposing new perspectives on data collection and evaluation. The paper provides a framework for thinking about PCA development across five stages: Task Formulation, Data Preparation, Model Learning, Evaluation, and System Deployment.

## Key Results
- Proposes a taxonomy with three dimensions (Intelligence, Adaptivity, Civility) to categorize PCAs into eight types (Sage, Opponent, Boss, Cosseter, Listener, Airhead, Doggie, Maniac)
- Highlights challenges in building human-centered PCAs across five stages of system construction
- Emphasizes the importance of aligning PCAs with human values through various learning approaches and multidimensional evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
The taxonomy of three dimensions (Intelligence, Adaptivity, Civility) improves design by making tradeoffs explicit. By categorizing PCAs into eight types, designers can reason about the balance of proactive behavior, user comfort, and task success instead of optimizing only raw task completion. This works because human acceptance of proactive systems depends on both task effectiveness and perceived intrusiveness, which are not captured by intelligence-only metrics. Break condition: In high-stakes, low-tolerance environments, even a "Sage" type may be rejected for perceived over-engagement.

### Mechanism 2
Context-based data collection introduces fabricated user needs that harm Adaptivity. Annotators simulate user queries or scenarios without genuine user context, training the agent to initiate proactively based on artificial rather than real user needs, leading to mistimed or irrelevant interventions. This works because real-world user needs are heterogeneous and dynamic, while simulated needs lack the nuance of actual user intent and timing. Break condition: If the data preparation explicitly includes real user logs (e.g., Natural Questions style), the fabricated need problem is mitigated.

### Mechanism 3
Human-AI collaborative data collection compensates for biases in both crowdworker and generative AI annotations. Pairing human expertise with LLM simulation allows scalable data generation while retaining real-world intuition and diversity, reducing pattern overlap and bias propagation. This works because human teachers provide pedagogical intuition that LLMs cannot replicate, and LLMs provide scalable, controllable generation that humans cannot match in volume. Break condition: If the human-AI collaboration pipeline is poorly designed (e.g., LLMs dominate the dialogue), the resulting data may still lack real human variability.

## Foundational Learning

- Concept: Human-centered design in AI systems
  - Why needed here: The paper's taxonomy and recommendations rest on integrating human needs, expectations, and ethical considerations into system design.
  - Quick check question: What are the three key dimensions used to evaluate human-centered proactive conversational agents?

- Concept: Proactive vs. reactive conversational systems
  - Why needed here: The distinction underlies the entire taxonomy and defines the initiative-taking behavior central to PCAs.
  - Quick check question: How does a proactive system differ from a reactive system in terms of user interaction?

- Concept: Data preparation challenges in dialogue systems
  - Why needed here: The paper critiques current data preparation methods and proposes alternatives to ensure data reflects real user needs and ethical standards.
  - Quick check question: What is a major drawback of context-based data collection in proactive dialogue datasets?

## Architecture Onboarding

- Component map: Task formulation -> Data preparation -> Model learning -> Evaluation -> System deployment
- Critical path: Data preparation and model learning are most sensitive to dimension integration; poor data or misaligned learning will propagate failures downstream
- Design tradeoffs: Higher Adaptivity and Civility may reduce raw task completion speed but increase long-term user trust and satisfaction
- Failure signatures: 
  - Low Adaptivity: Users feel overwhelmed or interrupted at wrong times
  - Low Civility: Users report privacy concerns or ethical discomfort
  - Low Intelligence: Agent fails to complete tasks effectively
- First 3 experiments:
  1. Data preparation audit: Analyze a sample dataset for fabricated user needs using user need validation criteria
  2. Prompting test: Compare ProCoT vs. standard CoT prompting on target-guided dialogues for Adaptivity (smoothness, satisfaction)
  3. Evaluation framework validation: Apply the proposed multidimensional metrics to a baseline PCA and a human-aligned variant to measure relative gains in Adaptivity and Civility

## Open Questions the Paper Calls Out

### Open Question 1
How can proactive conversational agents effectively balance the need for task completion with respect for user boundaries and preferences?
- Basis in paper: [explicit] The paper discusses the importance of Civility in PCAs, emphasizing the need for agents to recognize and respect user boundaries and preferences.
- Why unresolved: Current PCA formulations often prioritize task completion over user preferences, leading to intrusive or disrespectful interactions. There's a lack of clear guidelines on how to balance these competing objectives.
- What evidence would resolve it: Studies demonstrating effective methods for PCAs to assess and adapt to user preferences in real-time, while still achieving task goals. User studies showing improved satisfaction and reduced intrusiveness when these methods are implemented.

### Open Question 2
What are the most effective approaches for evaluating the Adaptivity of proactive conversational agents in real-world scenarios?
- Basis in paper: [explicit] The paper proposes a multidimensional evaluation framework for PCAs, including metrics for Adaptivity such as Patience, Timing Sensitivity, and Self-awareness.
- Why unresolved: Current evaluation methods focus primarily on task completion and response quality, neglecting the nuances of Adaptivity. There's a need for more robust and realistic evaluation protocols.
- What evidence would resolve it: Development and validation of new evaluation protocols that accurately measure a PCA's ability to adapt to user needs and context in diverse, real-world scenarios. Comparative studies showing the effectiveness of these protocols in predicting user satisfaction and system performance.

### Open Question 3
How can we ensure that proactive conversational agents remain aligned with human values as they become increasingly intelligent and autonomous?
- Basis in paper: [explicit] The paper discusses the importance of Human Alignment in PCA development, particularly as these agents advance towards superintelligence.
- Why unresolved: As PCAs become more capable, traditional alignment methods may become insufficient. There's a risk of misalignment between agent goals and human values, especially in complex or unforeseen situations.
- What evidence would resolve it: Research into novel alignment techniques that can scale with increasing agent intelligence, including methods for continuous value learning and adaptation. Empirical studies demonstrating the effectiveness of these techniques in maintaining alignment across a wide range of scenarios and agent capabilities.

## Limitations

- The paper is primarily a perspectives piece rather than an empirical study, so claims are supported by literature analysis and conceptual arguments rather than quantitative experiments
- Many claims about data fabrication and human-AI collaboration benefits are supported by related work citations rather than direct evidence from the paper itself
- The taxonomy, while conceptually clear, lacks empirical validation showing that categorizing PCAs along these three dimensions improves actual design outcomes

## Confidence

- High Confidence: The distinction between proactive and reactive systems and the general importance of considering human needs in PCA design
- Medium Confidence: The proposed taxonomy and its ability to improve design tradeoffs, as this is logically sound but not empirically validated in the paper
- Medium Confidence: Claims about data fabrication issues and human-AI collaborative data collection benefits, as these are supported by related work but not directly tested by the authors

## Next Checks

1. Dataset Audit: Analyze a sample of proactive dialogue datasets to quantify the prevalence of fabricated user needs versus real user contexts
2. Taxonomy Application: Apply the Intelligence-Adaptivity-Civility taxonomy to a set of existing PCA systems and evaluate whether it reveals meaningful design insights or tradeoffs not captured by traditional metrics
3. Human-AI Data Quality Test: Compare the quality and diversity of dialogue data generated through pure crowdworker annotation, pure LLM generation, and human-AI collaborative approaches using standardized quality metrics