---
ver: rpa2
title: 'Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation'
arxiv_id: '2407.11948'
source_url: https://arxiv.org/abs/2407.11948
tags:
- transformer
- separators
- document
- summarization
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the behavior of Transformer-based models
  in multi-document summarization through five empirical studies: (1) quantitative
  assessment of document boundary separators, (2) comparison of flat and hierarchical
  Transformer structures, (3) sensitivity analysis of encoder and decoder components,
  (4) evaluation of different training strategies, and (5) analysis of repetition
  in generated summaries. Results show that document boundary separators improve hierarchical
  Transformer performance but not flat Transformers, flat Transformers outperform
  hierarchical ones on shorter documents, the decoder is more sensitive to noise than
  the encoder, pretrain-finetune training strategy consistently yields best results,
  and repetition in summaries correlates with high uncertainty scores.'
---

# Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation

## Quick Facts
- arXiv ID: 2407.11948
- Source URL: https://arxiv.org/abs/2407.11948
- Authors: Congbo Ma; Wei Emma Zhang; Dileepa Pitawela; Haojie Zhuang; Yanfeng Shu
- Reference count: 18
- Key outcome: This study investigates the behavior of Transformer-based models in multi-document summarization through five empirical studies: (1) quantitative assessment of document boundary separators, (2) comparison of flat and hierarchical Transformer structures, (3) sensitivity analysis of encoder and decoder components, (4) evaluation of different training strategies, and (5) analysis of repetition in generated summaries. Results show that document boundary separators improve hierarchical Transformer performance but not flat Transformers, flat Transformers outperform hierarchical ones on shorter documents, the decoder is more sensitive to noise than the encoder, pretrain-finetune training strategy consistently yields best results, and repetition in summaries correlates with high uncertainty scores. The findings suggest future work should focus on decoder enhancements, consider document structure for longer inputs, and explore uncertainty-based strategies to reduce repetition.

## Executive Summary
This study conducts a comprehensive empirical investigation of Transformer-based models for multi-document summarization, examining five critical aspects: document boundary separators, flat vs hierarchical architectures, encoder-decoder sensitivity, training strategies, and repetition patterns. Through systematic experiments on Multi-XScience and Multi-News datasets, the research reveals that document boundary separators benefit hierarchical models but harm flat models, flat Transformers perform better on shorter documents, the decoder shows greater sensitivity to noise than the encoder, pretrain-finetune training consistently outperforms other strategies, and high uncertainty scores correlate with summary repetition. These findings provide practical guidance for model design and training while identifying promising directions for future research.

## Method Summary
The study investigates Transformer-based multi-document summarization through five empirical experiments using two datasets: Multi-XScience and Multi-News. Three model variants are evaluated: Vanilla Transformer (VT), VT with copy mechanism (VTC), and Hierarchical Transformer (HT). The HT uses a two-level architecture with local and global Transformers to capture document-level relationships. Experiments include adding document boundary separators, comparing flat vs hierarchical structures, injecting Gaussian noise into encoder/decoder components, testing three training strategies (original data, pseudo data, pretrain-finetune), and analyzing repetition through uncertainty quantification. Models are trained with Adam optimizer (2e-4 LR, 8k warmup), batch size 4,096, dropout 0.2 for 20k steps on NVIDIA 3090 GPU. Evaluation uses eleven metrics including ROUGE variants, BLEU, BertScore, and S3.

## Key Results
- Document boundary separators improve hierarchical Transformer performance but degrade flat Transformer performance
- Flat Transformers outperform hierarchical Transformers on shorter document sets
- The decoder exhibits significantly greater sensitivity to noise compared to the encoder
- Pretrain-finetune training strategy consistently yields superior results across datasets
- Repetition in generated summaries correlates strongly with high uncertainty scores during generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Document boundary separators improve hierarchical Transformer performance but not flat Transformers.
- Mechanism: Hierarchical models use a multi-level architecture where document separators provide explicit signals for document boundary detection, enabling the high-level Transformer to model cross-document relationships. Flat Transformers process documents as a single sequence, so separators act as irrelevant tokens without semantic meaning.
- Core assumption: The hierarchical Transformer's high-level component is designed to leverage document-level information, while flat Transformers lack such a mechanism.
- Evidence anchors:
  - [abstract] "Results show that document boundary separators improve hierarchical Transformer performance but not flat Transformers"
  - [section] "We discerned that the impact of document boundary separators varies among models with differing hierarchies"
  - [corpus] Weak - related papers focus on fairness and evaluation metrics, not separator impact
- Break condition: If the hierarchical Transformer's high-level component is removed or disabled, the benefit of separators should disappear.

### Mechanism 2
- Claim: The decoder is more sensitive to noise than the encoder in Transformer-based MDS models.
- Mechanism: The decoder's role in generating coherent summaries requires fine-grained control over token selection. Noise introduced in the decoder's parameter space propagates through the generation process, affecting subsequent tokens more severely than noise in the encoder, which primarily affects representation extraction.
- Core assumption: The decoder's function in producing fluent, coherent output makes it more vulnerable to perturbations than the encoder's feature extraction role.
- Evidence anchors:
  - [abstract] "The results also reveal that the decoder exhibits greater sensitivity to noises compared to the encoder"
  - [section] "Specifically, in noisy conditions, we find that adding noise to the decoder has a more substantial impact on performance compared to adding noise to the encoder"
  - [corpus] Weak - related papers don't address noise sensitivity differences between encoder and decoder
- Break condition: If noise is applied equally to both components but the decoder shows no greater sensitivity, this mechanism would be invalidated.

### Mechanism 3
- Claim: Repetition in generated summaries correlates with high uncertainty scores.
- Mechanism: When the model encounters uncertainty during generation, it may fall back on previously generated content, leading to repetition. High uncertainty indicates the model lacks confidence in choosing new content, causing it to reuse existing phrases or sentences.
- Core assumption: The model's uncertainty directly influences its generation choices, with high uncertainty leading to conservative, repetitive output.
- Evidence anchors:
  - [abstract] "The experimental results indicate that the repetition problem in the generated summaries has correlations with the high uncertainty scores"
  - [section] "The analysis reveals that as the model generates repetitive sentences or words, the uncertainty score rises, pointing out decreased confidence"
  - [corpus] Weak - related papers focus on fairness and evaluation, not repetition-uncertainty relationships
- Break condition: If repetition occurs consistently at low uncertainty scores, this correlation would not hold.

## Foundational Learning

- Concept: Document boundary detection in hierarchical models
  - Why needed here: Understanding how hierarchical Transformers use document separators to identify document boundaries is crucial for interpreting why separators improve their performance but not flat Transformers.
  - Quick check question: In a hierarchical Transformer with document separators, which component is responsible for processing document-level information?

- Concept: Encoder-decoder architecture in Transformers
  - Why needed here: The sensitivity analysis of encoder vs decoder requires understanding their distinct roles - the encoder extracts features while the decoder generates output based on those features.
  - Quick check question: What is the primary function of the decoder in a Transformer-based summarization model?

- Concept: Uncertainty quantification in text generation
  - Why needed here: The repetition analysis relies on measuring uncertainty scores at each generation step, which requires understanding how entropy calculations can quantify model confidence.
  - Quick check question: How is uncertainty typically quantified in text generation models?

## Architecture Onboarding

- Component map:
  - Input processing: Document sets with/without boundary separators
  - Encoder: Feature extraction from input documents
  - Decoder: Generation of summary tokens
  - Hierarchical structure (HT): Local Transformer (document-level) + Global Transformer (sentence/document-level)
  - Training strategies: Original data, pseudo data, pretrain-finetune approaches

- Critical path: Input documents → Encoder → Decoder → Summary output, with uncertainty monitoring during generation

- Design tradeoffs:
  - Using separators: Improves hierarchical performance but adds noise for flat models
  - Hierarchical vs flat structure: Hierarchical captures cross-document relationships better but is more complex and data-hungry
  - Training strategy: Pretrain-finetune shows best results but requires careful pseudo-data generation

- Failure signatures:
  - Poor performance with separators on flat models: Separators act as irrelevant tokens
  - High decoder sensitivity: Small perturbations cause large performance drops
  - Repetition patterns: High uncertainty scores correlate with repetitive output

- First 3 experiments:
  1. Compare hierarchical and flat Transformer performance with and without document separators on short document datasets
  2. Add Gaussian noise to encoder and decoder parameters separately, measure performance degradation
  3. Generate summaries while tracking uncertainty scores, identify positions where high uncertainty correlates with repetition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different document boundary separator tokens (e.g., special tokens vs. document titles) affect the performance of hierarchical Transformer models in multi-document summarization?
- Basis in paper: [explicit] The paper investigates the impact of document boundary separators but only uses a single type of separator token ("story_separator_special_tag").
- Why unresolved: The study focuses on the presence/absence of separators rather than comparing different separator types or formats.
- What evidence would resolve it: Comparative experiments testing multiple separator token types (special tokens, document titles, metadata) across various hierarchical Transformer architectures on multiple datasets.

### Open Question 2
- Question: What is the optimal balance between encoder and decoder complexity in Transformer-based multi-document summarization models to maximize performance while minimizing computational cost?
- Basis in paper: [explicit] The paper shows the decoder is more sensitive to noise than the encoder, suggesting its critical importance, but doesn't explore optimal architectural trade-offs.
- Why unresolved: The study demonstrates decoder sensitivity but doesn't investigate how varying encoder/decoder complexity ratios affects overall model performance and efficiency.
- What evidence would resolve it: Systematic ablation studies varying encoder and decoder depth, attention heads, and parameter counts while measuring performance and computational requirements across multiple summarization tasks.

### Open Question 3
- Question: How can uncertainty-based strategies be effectively integrated into the decoding process to prevent repetition in generated summaries without compromising fluency and coherence?
- Basis in paper: [explicit] The paper establishes a correlation between high uncertainty scores and repetition but doesn't propose methods to use this insight during generation.
- Why unresolved: While the relationship between uncertainty and repetition is demonstrated, the paper doesn't explore practical implementations of uncertainty-guided decoding strategies.
- What evidence would resolve it: Implementation and evaluation of decoding algorithms that dynamically adjust based on uncertainty scores, comparing their effectiveness against standard decoding methods in reducing repetition while maintaining summary quality.

## Limitations
- Dataset and domain specificity: Study relies exclusively on Multi-XScience and Multi-News datasets with relatively short documents, limiting generalizability to longer documents or different domains
- Architectural constraints: Hierarchical Transformer implementation uses fixed sentence-level granularity rather than exploring document-level hierarchical processing
- Evaluation metric limitations: Study does not address potential inconsistencies between automatic metrics and human judgment for complex multi-document summarization tasks

## Confidence

- High Confidence: Pretrain-finetune training strategy consistently outperforms other approaches (supported by direct experimental comparison across both datasets)
- Medium Confidence: Document boundary separators improve hierarchical but not flat Transformer performance (supported by experimental data but limited to specific model variants)
- Medium Confidence: Decoder sensitivity to noise being greater than encoder sensitivity (based on controlled noise injection experiments with clear performance degradation patterns)
- Medium Confidence: Correlation between repetition and high uncertainty scores (supported by uncertainty tracking but correlational rather than causal evidence)

## Next Checks

1. **Cross-domain validation**: Replicate the hierarchical vs flat Transformer experiments with longer-document datasets from different domains (e.g., legal case documents, scientific literature reviews) to test the robustness of the boundary separator findings.

2. **Ablation of hierarchical components**: Conduct an ablation study by removing or disabling the high-level Transformer in the hierarchical model while keeping separators, to definitively establish whether the benefit comes from the hierarchical structure or the separators themselves.

3. **Human evaluation of repetition**: Perform human judgment studies to verify whether high uncertainty scores actually correspond to perceived repetition and reduced summary quality, rather than just statistical correlation in automatic metrics.