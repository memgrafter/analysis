---
ver: rpa2
title: Benchmark for CEC 2024 Competition on Multiparty Multiobjective Optimization
arxiv_id: '2402.02033'
source_url: https://arxiv.org/abs/2402.02033
tags:
- problems
- multiobjective
- optimization
- pareto
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark suite for the CEC 2024 Competition
  on Multiparty Multiobjective Optimization (MPMOPs). MPMOPs involve multiple decision
  makers with conflicting objectives, as seen in applications like UAV path planning.
---

# Benchmark for CEC 2024 Competition on Multiparty Multiobjective Optimization

## Quick Facts
- arXiv ID: 2402.02033
- Source URL: https://arxiv.org/abs/2402.02033
- Authors: Wenjian Luo; Peilan Xu; Shengxiang Yang; Yuhui Shi
- Reference count: 23
- Primary result: Introduces benchmark suite for CEC 2024 Competition on Multiparty Multiobjective Optimization with 17 problems evaluated using MPIGD and MPHV metrics

## Executive Summary
This paper introduces a comprehensive benchmark suite for the CEC 2024 Competition on Multiparty Multiobjective Optimization (MPMOPs). The suite includes 17 problems divided into two categories: 11 benchmark problems with known Pareto optimal solutions and 6 Biparty Multiobjective UAV Path Planning problems with unknown solutions. The benchmark evaluates algorithms using Multiparty Inverted Generational Distance (MPIGD) for the first category and Multiparty Hypervolume (MPHV) for the second, with performance measured by average ranking across all problems.

## Method Summary
The benchmark suite comprises two parts: problems with common Pareto optimal solutions (E1-E11) and BPMO-UAVPP problems (C1-C6) with unknown solutions. Algorithms are evaluated using MPIGD for the first part and MPHV for the second part. Participants must run their algorithms for 30 independent trials with fixed random seeds from 1-30, reporting best, median, worst, mean, and standard deviation results. Termination occurs after maximum fitness evaluations (1000·D·M for the first part, 100000 for the second part).

## Key Results
- Introduces benchmark suite for CEC 2024 Competition on Multiparty Multiobjective Optimization
- Establishes MPIGD and MPHV as primary evaluation metrics for problems with known and unknown solutions respectively
- Provides comprehensive framework for evaluating MPMO algorithms across 17 diverse problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark suite's two-part structure enables comprehensive evaluation of MPMOP algorithms across different problem characteristics.
- Mechanism: Part 1 uses problems with known Pareto optimal solutions, allowing evaluation with MPIGD metric. Part 2 uses BPMO-UAVPP problems with unknown solutions, requiring MPHV metric for evaluation. This dual approach captures both convergence to known optima and diversity in unknown solution spaces.
- Core assumption: Known Pareto solutions exist for the first 11 problems, and the MPHV metric appropriately handles problems with unknown optima.
- Evidence anchors:
  - [abstract] "The test suite comprises two parts: problems with common Pareto optimal solutions and Biparty Multiobjective UAV Path Planning (BPMO-UAVPP) problems with unknown solutions."
  - [section 3.2] "For the first part of the benchmark, this report utilizes the multiparty inverted generational distance (MPIGD [6])... In the second part of benchmark suite, there is often no definitive true PF available... this report utilizes the multiparty hypervolume (MPHV) metric"
- Break condition: If problems in Part 1 don't actually have common Pareto optimal solutions across all parties, or if the MPHV metric doesn't adequately capture performance on unknown-solution problems.

### Mechanism 2
- Claim: The inclusion of both static benchmark problems and real-world UAV path planning scenarios ensures practical relevance of the competition.
- Mechanism: Static problems (E1-E11) provide controlled environments for algorithm comparison, while BPMO-UAVPP problems (C1-C6) represent realistic applications with urban terrain constraints, third-party risks, and performance limitations.
- Core assumption: The UAV path planning problems accurately model real-world constraints and objectives that decision-makers would face.
- Evidence anchors:
  - [section 2.3.1] Details efficiency objectives like path length, fuel consumption, height changes
  - [section 2.3.2] Details safety objectives like fatality risks, property risk, noise pollution
  - [section 2.3.3] Lists constraints like terrain, performance limits, turning angles
- Break condition: If the UAV model oversimplifies real-world constraints or if the generated building data doesn't reflect actual urban environments.

### Mechanism 3
- Claim: The competition design with fixed random seeds and standardized evaluation metrics ensures fair algorithm comparison.
- Mechanism: All algorithms run with seeds 1-30, maximum fitness evaluations scaled by problem dimension and number of parties, and results reported as best, median, worst, mean, and standard deviation for MPIGD and MPHV metrics.
- Core assumption: The chosen evaluation protocol (30 runs, fixed seeds, specific metrics) provides statistically meaningful comparisons between algorithms.
- Evidence anchors:
  - [section 3.1] "Runs: 30. The random seeds of all runs should be fixed from 1 to 30."
  - [section 3.3] "Participants are required to submit the best, median, worst, mean, and standard deviation of the 30 runs"
- Break condition: If 30 runs are insufficient for statistical significance, or if the chosen metrics don't capture algorithm performance adequately.

## Foundational Learning

- Concept: Multi-objective optimization fundamentals
  - Why needed here: The benchmark suite builds on basic multi-objective optimization concepts, extending them to multiparty scenarios
  - Quick check question: What distinguishes Pareto optimality in single-party vs. multiparty multi-objective optimization?

- Concept: Evolutionary algorithm design patterns
  - Why needed here: Understanding how algorithms like NSGA-II, SPEA2, and MOEA/D work is crucial for developing MPMOEAs
  - Quick check question: How does the Pareto sorting mechanism differ when handling multiple decision-makers with potentially conflicting objectives?

- Concept: Hypervolume and inverted generational distance metrics
  - Why needed here: These are the primary evaluation metrics for the benchmark suite
  - Quick check question: What are the computational complexity implications of calculating MPIGD vs MPHV on large-scale problems?

## Architecture Onboarding

- Component map:
  Problem Generator -> Algorithm Runner -> Metric Calculator -> Result Aggregator -> Comparison Engine

- Critical path:
  1. Initialize problem with specific parameters
  2. Run algorithm for 30 trials with fixed seeds
  3. Collect final population at termination
  4. Calculate appropriate metric (MPIGD or MPHV)
  5. Aggregate results and compute statistics
  6. Rank algorithms by average performance

- Design tradeoffs:
  - Static vs. dynamic problem generation
  - Exact vs. approximate metric calculation
  - Parallel vs. sequential execution of trials
  - Memory usage for storing intermediate populations

- Failure signatures:
  - MPIGD values unexpectedly high (poor convergence)
  - MPHV values showing high variance across runs (algorithm instability)
  - Runtime exceeding expected bounds (computational inefficiency)
  - Crashes during metric calculation (numerical precision issues)

- First 3 experiments:
  1. Run a baseline algorithm (e.g., NSGA-II adaptation) on E1 with d=10 to verify metric calculations
  2. Compare MPIGD and MPHV results on a simple known-solution problem to validate both metrics
  3. Test algorithm performance scaling by running on E1 with d=10, 30, 50 to identify computational bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance of algorithms be evaluated when Pareto optimal solutions are unknown, as in the BPMO-UAVPP problems?
- Basis in paper: [explicit] The paper mentions that optimal solutions for BPMO-UAVPP problems are unknown and uses MPHV as an evaluation metric.
- Why unresolved: Without known optimal solutions, it's challenging to determine if an algorithm has found the best possible solution.
- What evidence would resolve it: Developing methods to estimate the true Pareto front or establishing benchmarks with known solutions for BPMO-UAVPP problems.

### Open Question 2
- Question: How do privacy concerns affect the performance and design of MPMO algorithms?
- Basis in paper: [explicit] The paper references a study by She et al. discussing privacy issues in MPMOPs.
- Why unresolved: Privacy constraints can limit the information available to algorithms, potentially impacting their performance.
- What evidence would resolve it: Comparative studies of MPMO algorithms with and without privacy constraints on benchmark problems.

### Open Question 3
- Question: What are the best strategies for handling dynamic changes in MPMOPs, as seen in the CEC'2018 Competition on Dynamic Multiobjective Optimization?
- Basis in paper: [explicit] The paper mentions that the benchmark problems are based on the CEC'2018 Competition, which involves dynamic changes.
- Why unresolved: Dynamic environments introduce uncertainty, and algorithms must adapt to changing Pareto fronts.
- What evidence would resolve it: Comparative analysis of MPMO algorithms on dynamic benchmark problems with varying change frequencies and magnitudes.

## Limitations

- Unknown solution landscapes for BPMO-UAVPP problems introduce uncertainty in performance assessment
- Metric computation complexity may introduce approximation errors for large-scale problems
- Real-world applicability gap due to simplified constraint formulations and building generation

## Confidence

- High confidence: The structural design of the benchmark suite with two distinct problem categories is well-defined and implementable based on the provided specifications.
- Medium confidence: The metric selection (MPIGD for known solutions, MPHV for unknown solutions) appears theoretically sound, though implementation details require further clarification.
- Low confidence: The exact performance boundaries and expected algorithm behavior on BPMO-UAVPP problems cannot be determined without empirical testing, as these represent novel problem formulations.

## Next Checks

1. Implement a simple test case with known Pareto front and verify that both MPIGD and MPHV metrics produce consistent and expected values across different approximation qualities.

2. Run multiple established MPMO algorithms on a subset of benchmark problems to establish performance baselines and identify any unexpected behavior patterns.

3. Systematically test algorithm performance across all problem dimensions (d=10, 30, 50 for E1-E11) to identify computational bottlenecks and validate the expected complexity scaling.