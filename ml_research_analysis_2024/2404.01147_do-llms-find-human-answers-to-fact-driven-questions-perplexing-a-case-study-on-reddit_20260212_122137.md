---
ver: rpa2
title: Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study
  on Reddit
arxiv_id: '2404.01147'
source_url: https://arxiv.org/abs/2404.01147
tags:
- questions
- answers
- fact-driven
- llms
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores how well large language models (LLMs) can\
  \ model the wide variety of human answers to fact-driven questions on Reddit\u2019\
  s r/Ask{Topic} communities. The authors collect and release a dataset of 409 fact-driven\
  \ questions and 7,534 diverse, human-rated answers from 15 topic-specific subreddits\
  \ across three categories: profession, social identity, and geographic location."
---

# Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit

## Quick Facts
- arXiv ID: 2404.01147
- Source URL: https://arxiv.org/abs/2404.01147
- Reference count: 7
- LLMs align better with human preferences on highly-rated Reddit answers to fact-driven questions

## Executive Summary
This paper investigates how well large language models can model the wide variety of human answers to fact-driven questions on Reddit's r/Ask{Topic} communities. The authors collect a dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers from 15 topic-specific subreddits across three categories: profession, social identity, and geographic location. They analyze the perplexity of two LLM settings (vanilla and fine-tuned) when modeling these answers, comparing it to human ratings. The key finding is that LLMs are considerably better at modeling highly-rated human answers to fact-driven questions than poorly-rated ones, indicating that LLMs align well with human preferences. The paper also highlights interesting cases where LLMs struggle with highly-rated answers or easily model poorly-rated ones, suggesting potential areas for future research on LLM biases and blind spots in social media discourse modeling.

## Method Summary
The authors collected a dataset of 409 fact-driven questions and 7,534 human-rated answers from 15 r/Ask{Topic} subreddits across three categories: profession, social identity, and geographic location. They used the Sheared LLaMA model (1.3B parameters) in two settings: vanilla (SL) and fine-tuned on r/AskReddit data (SLFT). The model calculated perplexity for each answer, which was then compared with human ratings to determine alignment. The fine-tuning process involved 100,000 comments from r/AskReddit with a max token length of 128, learning rate of 2e-5, batch size of 6, and 1 epoch on an A100 GPU.

## Key Results
- LLMs are considerably better at modeling highly-rated human answers to fact-driven questions than poorly-rated ones
- Fine-tuning on Reddit data reduces perplexity across all answer ratings, indicating domain adaptation improves modeling
- LLMs struggle with certain highly-rated answers due to semantic or stylistic novelty not seen in training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs align better with human ratings on highly-rated answers because the training data likely contains similar answer patterns that were positively reinforced in human discourse.
- Mechanism: During pretraining and fine-tuning, LLMs learn probability distributions over token sequences. High-rated answers are likely more common, syntactically and semantically coherent, and therefore have lower perplexity.
- Core assumption: Human ratings on Reddit correlate with answer quality in a way that matches the linguistic patterns the LLM was trained on.
- Evidence anchors:
  - [abstract] "LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers"
  - [section] "By plotting all scores of the human answers by the answer perplexity assigned by the SLFT model, we can see a general trend. Across all 7,534 answers, the LLM models highly-rated answers better than poorly-rated answers."
  - [corpus] Weak: No direct evidence in corpus; inference based on the abstract claim.
- Break condition: If high-rated answers are rare or linguistically atypical in the training data, perplexity may not correlate with human ratings.

### Mechanism 2
- Claim: Fine-tuning on Reddit data reduces perplexity across all answer ratings, indicating domain adaptation improves modeling of informal, community-specific language.
- Mechanism: The fine-tuned model adjusts its token probability distributions to better match the linguistic style, slang, and topic-specific vocabulary found in Reddit answers.
- Core assumption: Reddit discourse has distinct linguistic features not fully captured in general pretraining data.
- Evidence anchors:
  - [abstract] "We find that LLMs are considerably better at modeling highly-rated human answers to such questions, as opposed to poorly-rated human answers."
  - [section] "Our fine-tuning strategy decreases LLM's perplexity across the board, indicating that fine-tuning helps the model understand human answers better."
  - [corpus] Weak: No explicit corpus evidence provided; inferred from methodology description.
- Break condition: If the fine-tuning dataset is too small or unrepresentative, improvements may not generalize to the full test set.

### Mechanism 3
- Claim: LLMs struggle with certain highly-rated answers due to semantic or stylistic novelty not seen in training data.
- Mechanism: Perplexity increases when the LLM encounters answer patterns, phrases, or topics that deviate from its learned probability distribution, even if humans rate them highly.
- Core assumption: Human ratings capture dimensions of quality beyond linguistic predictability (e.g., humor, insight, or emotional resonance).
- Evidence anchors:
  - [section] "High Peer-assigned Score, High Perplexity: LLMs fail to model these types of answers, and yet they are rated highly by humans."
  - [abstract] "We also highlight interesting cases where LLMs struggle with highly-rated answers or easily model poorly-rated ones"
  - [corpus] Weak: No corpus evidence; relies on author's observation.
- Break condition: If the LLM is exposed to sufficient examples of such answers during training, perplexity should decrease.

## Foundational Learning

- Concept: Perplexity as a measure of language model performance
  - Why needed here: The paper uses perplexity to quantify how well LLMs model human answers, making it essential to understand what low/high perplexity means.
  - Quick check question: If an answer has low perplexity under an LLM, what does that indicate about the model's expectations for that sequence of tokens?
- Concept: Fact-driven vs. subjective questions
  - Why needed here: The dataset is filtered to focus on fact-driven questions, which affects the generalizability of the findings to other types of social media questions.
  - Quick check question: Why might answers to subjective questions be harder for LLMs to model than fact-driven ones?
- Concept: Peer-assigned scoring on Reddit
  - Why needed here: Human ratings (upvotes) are used as a proxy for answer quality, which the LLM's performance is compared against.
  - Quick check question: How is Reddit's score for a comment calculated, and why is it used as a proxy for human preference?

## Architecture Onboarding

- Component map: Reddit API → filter posts/comments → classify questions (fact-driven/subjective) → label answers by score → Pre-trained or fine-tuned Sheared LLaMA → compute perplexity for each answer → correlate perplexity with human scores → visualize trends by subreddit/topic
- Critical path: Data collection → preprocessing → LLM inference → statistical analysis → visualization
- Design tradeoffs: Using a smaller, fine-tunable model (Sheared LLaMA 1.3B) vs. full LLaMA2-7B for computational efficiency, but potentially less modeling power
- Failure signatures: High perplexity for high-rated answers may indicate blind spots in training data; low perplexity for low-rated answers may indicate overgeneralization or mimicry of poor-quality content
- First 3 experiments:
  1. Compare perplexity distributions of high vs. low-rated answers using the vanilla model.
  2. Repeat the comparison using the fine-tuned model to measure the effect of domain adaptation.
  3. Analyze specific subreddits where the model performs poorly to identify topic-specific challenges.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific types of answers (in terms of content and style) cause LLMs to have high perplexity despite being highly-rated by humans?
- Basis in paper: [explicit] The paper identifies "High Peer-assigned Score, High Perplexity" cases as interesting findings
- Why unresolved: The paper only provides one example (silverfish comment) and calls for further exploration
- What evidence would resolve it: A systematic analysis of multiple high-rated/high-perplexity answers to identify common patterns in language use, sentiment, or content type that LLMs struggle with

### Open Question 2
- Question: Do LLMs exhibit different modeling capabilities for fact-driven questions across different topic categories (profession vs social identity vs geographic location)?
- Basis in paper: [explicit] The paper states "we find that LLMs models answers to fact-driven questions from professional subreddit topics better than questions from social identity or location topics"
- Why unresolved: The paper only mentions this observation but doesn't investigate the underlying reasons or provide detailed analysis
- What evidence would resolve it: Comparative analysis of perplexity scores across categories with deeper investigation into linguistic features and answer types that might explain the differences

### Open Question 3
- Question: Can targeted fine-tuning on r/Ask{Topic} data improve LLM performance on modeling human answers to fact-driven questions?
- Basis in paper: [explicit] The paper mentions "future work could use this data and analysis framework to explore targeted fine-tuning of LLMs for socio-technical tools"
- Why unresolved: The paper only tests one fine-tuning approach (on r/AskReddit data) and calls for more exploration
- What evidence would resolve it: Experimental results comparing various fine-tuning strategies using r/Ask{Topic} specific data against the baseline models presented in the paper

## Limitations
- The exact method and criteria used by LLaMA-2-70B to classify questions as fact-driven vs. subjective is not specified, making it unclear how to reproduce the question filtering step.
- The specific preprocessing steps applied to the Reddit data (e.g., handling of deleted posts, normalization) are not fully detailed, potentially affecting reproducibility.
- Computational resource constraints (e.g., lack of A100 GPU) may prevent fine-tuning the model as described, leading to different perplexity results.

## Confidence
- Reproducibility of dataset: Medium - Dataset is available on Hugging Face, but preprocessing details are incomplete
- Validity of perplexity comparison: High - The methodology for calculating and comparing perplexity is clearly specified
- Generalizability of findings: Low - Results are based on a specific model (Sheared LLaMA 1.3B) and may not extend to other LLMs

## Next Checks
1. Verify that the dataset downloaded from Hugging Face contains the correct number of questions (409) and answers (7,534) with corresponding human ratings
2. Confirm that the Sheared LLaMA model can be successfully fine-tuned on r/AskReddit data with the specified hyperparameters (max token length 128, learning rate 2e-5, batch size 6, 1 epoch)
3. Calculate perplexity for a sample of answers using both vanilla and fine-tuned models to ensure the implementation matches the paper's methodology