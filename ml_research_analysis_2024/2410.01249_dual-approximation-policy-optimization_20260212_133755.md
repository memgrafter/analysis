---
ver: rpa2
title: Dual Approximation Policy Optimization
arxiv_id: '2410.01249'
source_url: https://arxiv.org/abs/2410.01249
tags:
- policy
- have
- convergence
- function
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DAPO, a policy optimization framework that
  incorporates general function approximation by using dual Bregman divergence to
  measure function approximation error. Unlike prior work that minimizes L2-norm error,
  DAPO uses the dual mirror map to measure similarity between dual vectors.
---

# Dual Approximation Policy Optimization

## Quick Facts
- arXiv ID: 2410.01249
- Source URL: https://arxiv.org/abs/2410.01249
- Reference count: 40
- This paper introduces DAPO, a policy optimization framework that incorporates general function approximation by using dual Bregman divergence to measure function approximation error

## Executive Summary
This paper introduces DAPO, a policy optimization framework that incorporates general function approximation by using dual Bregman divergence to measure function approximation error. Unlike prior work that minimizes L2-norm error, DAPO uses the dual mirror map to measure similarity between dual vectors. The authors prove linear convergence rates for DAPO-L2 and DAPO-KL variants and show that DAPO-KL includes Soft Actor-Critic (SAC) as a special case, immediately providing SAC with strong convergence guarantees. Experimental results on MuJoCo benchmarks demonstrate that DAPO-KL performs comparably to SAC while AMPO fails to learn meaningfully.

## Method Summary
DAPO uses a mirror descent algorithm in the dual space of q-functions, with each iteration involving an approximate solution to the dual problem. The key innovation is using dual Bregman divergence to measure the approximation error between the approximate and exact dual solutions. The authors propose two variants: DAPO-L2 (using L2 norm) and DAPO-KL (using KL divergence). DAPO-KL is shown to be equivalent to a modified SAC algorithm with a specific hyperparameter β controlling the trade-off between the entropy term and the q-value approximation.

## Key Results
- DAPO-KL achieves comparable performance to SAC on MuJoCo benchmarks
- AMPO fails to learn meaningfully even with 10 stochastic gradient steps per iteration
- DAPO-KL includes SAC as a special case, providing immediate convergence guarantees
- The choice of m (number of gradient steps) does not significantly affect DAPO-KL performance

## Why This Works (Mechanism)
DAPO works by reformulating policy optimization as a mirror descent algorithm in the dual space of q-functions. The dual Bregman divergence measures the approximation error between the approximate and exact dual solutions, providing a more principled way to handle function approximation than minimizing L2 norm. This approach ensures that the policy update maintains consistency with the q-function approximation, leading to improved convergence properties.

## Foundational Learning
- Bregman divergence: Measures the difference between two points in terms of their dual representations, needed to quantify approximation error in the dual space
- Quick check: Verify that the Bregman divergence reduces to squared L2 distance when using the negative Shannon entropy as the mirror map

- Mirror descent: An optimization algorithm that updates parameters by performing gradient descent in the dual space and then mapping back to the primal space
- Quick check: Confirm that the DAPO update equations follow the standard mirror descent form with the appropriate Bregman divergence

- Dual representation of q-functions: The q-function can be represented as a dual vector in the space of probability measures, enabling the use of Bregman divergences
- Quick check: Verify that the dual representation satisfies the necessary properties for the Bregman divergence to be well-defined

## Architecture Onboarding
- Component map: Policy network θ -> Dual q-function ϕ -> Approximate dual q-function ϕ̂ -> Policy update
- Critical path: Policy evaluation (compute q-values) -> Dual optimization (minimize Bregman divergence) -> Policy improvement (update θ)
- Design tradeoffs: DAPO trades off between approximation error and computational efficiency by using a limited number of gradient steps
- Failure signatures: AMPO fails to learn due to inconsistency between the policy and q-function updates, leading to divergence
- First experiments:
  1. Implement DAPO-KL with β=0.5 and compare performance to SAC on HalfCheetah-v4
  2. Test AMPO with different numbers of gradient steps (m=1, 5, 10) to confirm failure modes
  3. Vary the β hyperparameter in DAPO-KL to understand its impact on convergence and performance

## Open Questions the Paper Calls Out
- How does DAPO perform when using different mirror maps, such as the negative Tsallis entropy, compared to the negative Shannon entropy used in DAPO-KL?
- What is the optimal number of stochastic gradient steps per iteration for AMPO to achieve competitive performance?
- How does the primal-dual consistency enforced by MAMPO compare to the inherent consistency in DAPO-KL in terms of convergence and final performance?

## Limitations
- The exact implementation details of the q-value network update are not fully specified
- DAPO-KL's performance appears sensitive to the β hyperparameter, which is tuned per environment
- The comparison is limited to SAC's specific architecture, which may not generalize to all actor-critic frameworks

## Confidence
- High confidence in theoretical convergence guarantees for DAPO-L2 and DAPO-KL
- Medium confidence in experimental results showing DAPO-KL performance comparable to SAC
- Medium confidence in the claim that DAPO-KL provides immediate convergence guarantees for SAC

## Next Checks
1. Implement the full DAPO framework with both m=1 and m=10 gradient steps per iteration across all four MuJoCo environments to verify the claim about gradient steps not significantly affecting performance
2. Conduct ablation studies varying the β hyperparameter systematically to understand its impact on DAPO-KL convergence and performance
3. Test DAPO-KL with different actor-critic architectures beyond SAC's specific implementation to assess generalizability