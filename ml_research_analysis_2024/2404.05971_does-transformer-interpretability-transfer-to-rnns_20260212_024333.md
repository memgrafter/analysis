---
ver: rpa2
title: Does Transformer Interpretability Transfer to RNNs?
arxiv_id: '2404.05971'
source_url: https://arxiv.org/abs/2404.05971
tags:
- steering
- arxiv
- preprint
- state
- mamba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates whether interpretability techniques developed
  for transformers transfer to emerging RNN architectures like Mamba and RWKV. The
  authors test three methods: contrastive activation addition (CAA) for steering model
  behavior, the tuned lens for extracting latent predictions, and probes for eliciting
  knowledge from models fine-tuned to produce false outputs.'
---

# Does Transformer Interpretability Transfer to RNNs?

## Quick Facts
- arXiv ID: 2404.05971
- Source URL: https://arxiv.org/abs/2404.05971
- Authors: GonÃ§alo Paulo; Thomas Marshall; Nora Belrose
- Reference count: 18
- Primary result: Interpretability methods largely transfer from transformers to RNNs like Mamba and RWKV, with some architecture-specific modifications

## Executive Summary
This paper investigates whether interpretability techniques developed for transformers can be effectively applied to emerging RNN architectures. The authors test three methods - contrastive activation addition (CAA), the tuned lens, and knowledge probes - on Mamba and RWKV models across multiple behaviors including sentiment classification, toxicity detection, and multi-digit addition. The results demonstrate that most interpretability techniques transfer successfully to RNNs, though with some architecture-specific considerations and modifications needed for optimal performance.

## Method Summary
The authors evaluate interpretability techniques on Mamba and RWKV models fine-tuned for various behaviors. They apply contrastive activation addition to steer model behavior by adding activation differences between positive and negative examples, use the tuned lens to extract interpretable latent predictions from model states, and employ probes to elicit knowledge from models trained to produce incorrect outputs. The experiments test these methods across multiple behaviors including sentiment classification, toxicity detection, and arithmetic tasks, comparing results against transformer baselines where available.

## Key Results
- CAA successfully steers RNN behavior using either residual stream activations or compressed internal states, though effects vary across behaviors and models
- The tuned lens effectively extracts interpretable predictions from RNNs, though tied embeddings in Mamba cause early layers to predict input tokens
- Probes can elicit correct answers from quirky RNNs even when outputs are systematically wrong
- Interpretability methods largely transfer to RNNs, with potential improvements from leveraging RNN-specific features like compressed states

## Why This Works (Mechanism)
The transferability of interpretability methods stems from fundamental similarities in how both transformers and RNNs process sequential information. Both architectures maintain hidden states that capture information about the input sequence, and both use these states to make predictions. The key insight is that while the architectures differ in their specific implementations, the high-level computational principles - maintaining context, processing tokens sequentially, and producing outputs based on accumulated state - are sufficiently similar that techniques designed for one can often be adapted to the other.

## Foundational Learning
- **Contrastive Activation Addition (CAA)**: Method for steering model behavior by adding activation differences between positive and negative examples; needed to understand behavior modification techniques
- **Tuned Lens**: Technique for extracting interpretable latent predictions from model states; needed to understand how to interpret internal representations
- **Knowledge Probes**: Method for eliciting information from models, particularly useful for understanding what models know; needed to test knowledge extraction capabilities
- **RNN Hidden States**: Internal representations that capture sequence information; needed to understand how RNNs maintain context
- **Residual Streams**: Pathways for information flow in transformers and some RNNs; needed to understand where to apply interpretability techniques
- **Compressed States**: RNN-specific feature where information is compressed into smaller representations; needed to understand potential efficiency advantages

## Architecture Onboarding
- **Component Map**: Input -> Token Embeddings -> RNN Cells (Mamba or RWKV) -> Hidden States -> Output Layer -> Prediction
- **Critical Path**: Token embeddings flow through recurrent processing units, updating hidden states at each timestep, which are then used by the output layer to generate predictions
- **Design Tradeoffs**: RNNs offer computational efficiency and linear scaling versus transformers' quadratic complexity, but may sacrifice some modeling capacity; Mamba uses selective state spaces while RWKV uses attention-like mechanisms with linear complexity
- **Failure Signatures**: Tied embeddings in Mamba cause early-layer predictions to match input tokens; some RNNs exhibit "quirky" behavior when fine-tuned to produce incorrect outputs
- **3 First Experiments**: 1) Test CAA steering on simple sentiment classification, 2) Apply tuned lens to extract predictions from single-layer RNN, 3) Probe knowledge from model trained on reversed outputs

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Evaluation limited to a small set of RNN architectures (Mamba and RWKV) and behaviors
- Does not explore whether interpretability methods reveal the same mechanistic explanations across architectures
- Limited empirical validation of leveraging RNN-specific features like compressed states for improved interpretability

## Confidence
- CAA and tuned lens transferability: **High** confidence
- Probe-based knowledge elicitation: **Medium** confidence
- Leveraging RNN-specific features: **Low** confidence

## Next Checks
1. Test CAA steering on a broader range of RNN architectures (e.g., traditional LSTMs, GRUs) and more diverse behaviors (factual recall, reasoning tasks) to verify generalizability beyond the current scope.

2. Conduct ablation studies on Mamba's tied embedding layer to determine whether the observed early-layer prediction artifacts are inherent to the architecture or can be mitigated through architectural modifications.

3. Evaluate whether the same latent representations extracted by the tuned lens correspond to similar computational mechanisms across transformers and RNNs by comparing activation patterns during equivalent processing steps.