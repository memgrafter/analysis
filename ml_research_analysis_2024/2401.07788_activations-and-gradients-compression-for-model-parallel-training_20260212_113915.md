---
ver: rpa2
title: Activations and Gradients Compression for Model-Parallel Training
arxiv_id: '2401.07788'
source_url: https://arxiv.org/abs/2401.07788
tags:
- compression
- activations
- gradients
- training
- topk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the effects of simultaneous compression of
  both activations and gradients during model-parallel training. The study compares
  quantization, TopK, and error feedback methods, showing that gradients are more
  sensitive to compression than activations.
---

# Activations and Gradients Compression for Model-Parallel Training

## Quick Facts
- arXiv ID: 2401.07788
- Source URL: https://arxiv.org/abs/2401.07788
- Reference count: 39
- Primary result: Gradients are more sensitive to compression than activations in model-parallel training

## Executive Summary
This paper examines the effects of simultaneous compression of both activations and gradients during model-parallel training. The study compares quantization, TopK, and error feedback methods, showing that gradients are more sensitive to compression than activations. For TopK, the authors find that 10% sparsity is the strongest level that doesn't significantly degrade accuracy, and that error feedback does not improve convergence but does help maintain performance when inference is run without compression.

## Method Summary
The study investigates model-parallel training with compression operators applied to both activations and gradients. The experimental setup uses ResNet-18 on CIFAR-10 with model parallelism degree 4, batch size 100, SGD optimizer with momentum 0.9, weight decay 5e-4, and cosine annealing scheduler. Compression methods tested include quantization (2-8 bits) and TopK (5-50%), with error feedback techniques (EF and EF21) evaluated for their impact on convergence. The paper also examines GPT-2 fine-tuning on Wikitext-2 dataset.

## Key Results
- Gradients require milder compression rates than activations to maintain model quality
- TopK compression with 10% sparsity is the strongest level that doesn't severely harm convergence
- Error feedback techniques do not improve model-parallel training convergence compared to plain compression
- Models trained with TopK compression perform well only when compression is also applied during inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradients are more sensitive to compression than activations
- Mechanism: Gradients directly affect parameter updates; small errors in gradients can cause large deviations in optimization trajectory, while activations mainly affect intermediate representations
- Core assumption: Model convergence depends more critically on accurate gradient signals than on precise activation values
- Evidence anchors:
  - [abstract]: "Our findings demonstrate that gradients require milder compression rates than activations"
  - [section]: "The distinction between compressing activations and gradients may lie in their impact. Compressing gradients can seriously disrupt optimization and convergence, while compressing activations has a milder effect"
  - [corpus]: Weak - no direct corpus support for this specific mechanism

### Mechanism 2
- Claim: Error feedback techniques do not improve model-parallel training convergence
- Mechanism: In model-parallel training, activations and gradients between batches differ significantly, making accumulated error from previous batches irrelevant or even harmful for current batch optimization
- Core assumption: Error feedback relies on similarity between consecutive communication values, which doesn't hold in model-parallel settings due to batch-to-batch variation
- Evidence anchors:
  - [abstract]: "We find that error feedback techniques do not improve model-parallel training compared to plain compression"
  - [section]: "Significant difference of activations and gradients between batches may be why the EF-based approaches does not improve model quality when applied directly"
  - [corpus]: Weak - corpus lacks specific evidence about error feedback in model-parallel contexts

### Mechanism 3
- Claim: Compression must be applied during inference for models trained with compression
- Mechanism: Models trained with compressed activations/gradients learn to expect and process compressed distributions; providing uncompressed inputs during inference introduces unexpected patterns that degrade performance
- Core assumption: The model's learned representations and decision boundaries are shaped by the compressed data distribution
- Evidence anchors:
  - [abstract]: "Experiments also show that models trained with TopK perform well only when compression is also applied during inference"
  - [section]: "Model test accuracy for the compressed case is 7-15 percentage points higher than inference without compression"
  - [corpus]: Weak - corpus doesn't provide direct evidence for this inference-time compression requirement

## Foundational Learning

- Concept: Model parallelism vs data parallelism
  - Why needed here: The paper focuses specifically on model-parallel training where model architecture is partitioned across workers, not data-parallel where models are replicated
  - Quick check question: In model parallelism, what information flows between workers during forward and backward passes?

- Concept: Gradient compression and sparsification techniques
  - Why needed here: The paper evaluates quantization and TopK compression methods for reducing communication overhead
  - Quick check question: How does TopK compression differ from quantization in terms of information preservation and communication cost?

- Concept: Error feedback mechanisms
  - Why needed here: The paper tests error compensation techniques like EF and EF21 to improve convergence with compression
  - Quick check question: What is the fundamental difference between standard error feedback and EF21 approaches?

## Architecture Onboarding

- Component map: Input -> Model partitioning -> Forward pass (compressed activations) -> Model computation -> Compressed activations to next worker -> Backward pass (compressed gradients) -> Model computation -> Compressed gradients to previous worker -> Parameter updates

- Critical path:
  1. Forward pass: Compressed activations → model computation → compressed activations to next worker
  2. Backward pass: Compressed gradients → model computation → compressed gradients to previous worker
  3. Parameter updates: Using compressed gradients received from downstream worker

- Design tradeoffs:
  - Compression ratio vs model accuracy: Higher compression saves bandwidth but risks convergence issues
  - Error feedback overhead: Additional memory and computation for maintaining error buffers
  - Inference requirements: Whether compression must be applied during inference

- Failure signatures:
  - Divergence or plateau in training loss
  - Large gap between training and validation accuracy
  - Inconsistent performance between compressed and uncompressed inference

- First 3 experiments:
  1. Quantization sensitivity test: Compare 2-bit, 4-bit, 6-bit, 8-bit quantization on activations and gradients separately
  2. TopK compression limits: Test Top50%, Top30%, Top20%, Top10%, Top5% compression levels with both components compressed
  3. Error feedback evaluation: Apply EF and EF21 to TopK compressed communication and measure impact on convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum compression level that can be applied to gradients without degrading model performance in model-parallel training?
- Basis in paper: [explicit] The paper states that gradients are more sensitive to compression than activations and that Top10% is the strongest level that doesn't severely harm convergence.
- Why unresolved: The study only tested up to Top10% compression for gradients, and it's unclear if higher levels could be tolerated with different architectures or optimization techniques.
- What evidence would resolve it: Experiments testing gradient compression levels beyond Top10% on various model architectures and tasks would provide clarity.

### Open Question 2
- Question: How does error feedback impact model-parallel training convergence when applied to both activations and gradients?
- Basis in paper: [explicit] The paper finds that error feedback techniques do not improve model-parallel training convergence compared to plain compression.
- Why unresolved: The study only tested a limited set of error feedback methods (EF and EF21) and did not explore other potential error compensation techniques.
- What evidence would resolve it: Testing a broader range of error feedback and compensation techniques on diverse model architectures and tasks would determine their effectiveness.

### Open Question 3
- Question: Why does TopK compression significantly degrade fine-tuning performance compared to training from scratch?
- Basis in paper: [explicit] The paper observes that TopK compression increases validation loss and perplexity significantly when fine-tuning GPT-2 on Wikitext, compared to training ResNet18 from scratch.
- Why unresolved: The study does not investigate the underlying reasons for this difference in behavior between fine-tuning and training from scratch.
- What evidence would resolve it: Analyzing the effects of TopK compression on activation distributions and gradient updates during fine-tuning versus training from scratch would provide insights into this phenomenon.

## Limitations

- The study focuses on specific compression methods (quantization, TopK, error feedback) and may not generalize to all compression techniques
- Results are primarily shown on CIFAR-10 and Wikitext-2 datasets; performance on other domains remains unknown
- The analysis doesn't deeply explore why certain compression levels work better than others beyond empirical observation

## Confidence

- Empirical findings about TopK 10% sparsity: **High**
- Empirical finding about error feedback ineffectiveness: **High**
- Mechanisms explaining why gradients are more sensitive: **Medium**
- Explanation for error feedback's ineffectiveness: **Medium**
- Claim about inference-time compression requirements: **Medium**

## Next Checks

1. Test whether the 10% TopK threshold holds across different model architectures (e.g., Vision Transformers, BERT) and larger datasets (ImageNet, larger language models)

2. Measure the actual distribution of gradient magnitudes across layers to verify whether TopK sparsification consistently removes "important" gradients or if the distribution varies significantly between layers

3. Conduct ablation studies varying batch size and model-parallel degree to determine if error feedback's ineffectiveness persists under different communication patterns and batch-to-batch variation levels