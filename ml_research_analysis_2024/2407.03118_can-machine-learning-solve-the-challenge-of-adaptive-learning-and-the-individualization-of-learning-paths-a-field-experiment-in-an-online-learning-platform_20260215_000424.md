---
ver: rpa2
title: Can machine learning solve the challenge of adaptive learning and the individualization
  of learning paths? A field experiment in an online learning platform
arxiv_id: '2407.03118'
source_url: https://arxiv.org/abs/2407.03118
tags:
- learning
- learners
- tasks
- task
- treatment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study tests whether machine learning can effectively individualize
  learning paths in a large online learning platform. The authors develop a convolutional
  neural network-based algorithm to assign tasks to 4,365 college students based on
  predicted task difficulty and future learning effort.
---

# Can machine learning solve the challenge of adaptive learning and the individualization of learning paths? A field experiment in an online learning platform

## Quick Facts
- **arXiv ID**: 2407.03118
- **Source URL**: https://arxiv.org/abs/2407.03118
- **Reference count**: 16
- **Primary result**: ML algorithm showed no significant improvement in learning effort or performance compared to control group

## Executive Summary
This study tests whether machine learning can effectively individualize learning paths in a large online learning platform. The authors develop a convolutional neural network-based algorithm to assign tasks to 4,365 college students based on predicted task difficulty and future learning effort. Despite promising pre-intervention performance, the algorithm showed no significant improvement in learning effort (tasks solved) or performance (correctly solved tasks) compared to the control group during the intervention period. The study highlights challenges in applying ML for adaptive learning, including insufficient individual data, potential algorithmic limitations, and the complexity of learning processes.

## Method Summary
The study uses a three-arm randomized controlled trial with 4,365 college students. A two-stage convolutional neural network architecture predicts task difficulty and future learning effort based on 5+ million historical learner-platform interactions. The algorithm is pre-trained on historical data and retrained nightly during the intervention. Students are randomly assigned to control (random task assignment), group-based adaptive (tasks based on group averages), or individual adaptive (tasks based on individual predictions) groups.

## Key Results
- No significant difference in learning effort (number of solved tasks) between treatment and control groups
- No significant difference in performance (correctly solved tasks) between treatment and control groups
- Algorithm's AUC scores degraded below 0.5 during intervention despite strong pre-training results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage convolutional neural network architecture enables dynamic prediction of both task difficulty and future learning effort.
- Mechanism: The first CNN predicts individual task solving probability, which is then used as an input feature for the second CNN to predict future learning effort. This sequential approach leverages motivation theory that task difficulty perception influences effort expenditure.
- Core assumption: Task solving probability is a meaningful predictor of future learning effort for individual learners.
- Evidence anchors:
  - [abstract]: "Our algorithm is set up as a convolutional neural network. More specifically, we apply two consecutive convolutional neural networks where the prediction of the first neural network is used as an input factor for the second neural network."
  - [section]: "Our reasoning for applying the two networks consecutively and using the first network's prediction of solving probability as an input to the second network is motivated by motivation theory."
  - [corpus]: No direct evidence found - this is a novel methodological approach not well-represented in the corpus.
- Break condition: If the relationship between task difficulty perception and future effort is non-linear or mediated by other factors not captured in the model.

### Mechanism 2
- Claim: Pre-training on 5 million historical learner-platform interactions enables accurate predictions for individual learners.
- Mechanism: The ML algorithm learns patterns from historical data about how learner characteristics and behaviors relate to learning outcomes, allowing it to make predictions for new learners based on similarity to historical patterns.
- Core assumption: Historical learner behavior patterns are predictive of individual future performance in the same platform environment.
- Evidence anchors:
  - [abstract]: "Our two consecutively applied convolutional neural networks are pre-trained with historical data on learner-platform interactions in the same learning platform, collected over two years directly prior to the intervention."
  - [section]: "Being trained on these data and informed about a user's characteristics the algorithm is enabled to predict performance on that task for a user that hasn't worked on that task before."
  - [corpus]: No direct evidence found - corpus contains general ML applications but not this specific pre-training approach for adaptive learning.
- Break condition: If individual learners have unique patterns not represented in historical data, or if the platform environment changes significantly.

### Mechanism 3
- Claim: Daily retraining with new learner-platform interaction data enables the algorithm to adapt to changing learner behavior patterns.
- Mechanism: The algorithm continuously updates its parameters based on incoming data, allowing it to capture shifts in learner behavior or platform dynamics over time.
- Core assumption: Learner behavior patterns are sufficiently stable that short-term updates can improve predictions without overfitting to noise.
- Evidence anchors:
  - [abstract]: "During the intervention, we fully retrain the whole networks each night with the constantly incoming data on the learner-platform interactions in order to dynamically adapt our algorithm."
  - [section]: No additional evidence found in main text.
  - [corpus]: No direct evidence found - corpus contains general concepts of online learning adaptation but not this specific daily retraining approach.
- Break condition: If learner behavior changes too rapidly or if noise in daily data outweighs signal, leading to overfitting.

## Foundational Learning

- Concept: Convolutional neural networks (CNNs) for sequence prediction
  - Why needed here: CNNs can capture local patterns in learner behavior sequences that might predict task difficulty and future effort
  - Quick check question: How do CNNs differ from standard feedforward networks in handling sequential data?

- Concept: Motivation theory in educational psychology
  - Why needed here: The algorithm design is explicitly based on the premise that task difficulty perception influences effort expenditure
  - Quick check question: What are the key theoretical frameworks linking task difficulty to learning motivation?

- Concept: Random controlled trial (RCT) design in educational research
  - Why needed here: The study uses a three-arm RCT to evaluate the effectiveness of different adaptive learning approaches
  - Quick check question: What are the key design considerations when implementing RCTs in online learning platforms?

## Architecture Onboarding

- Component map: Data collection pipeline → Pre-training phase → Prediction pipeline → Task assignment mechanism → Daily retraining process → Experimental framework

- Critical path: Data collection → Pre-training → Prediction → Task assignment → Performance measurement

- Design tradeoffs:
  - Pre-training on large dataset vs. individual learner adaptation
  - Two-stage prediction vs. single model complexity
  - Daily retraining vs. model stability
  - Nearest neighbor task selection vs. probabilistic assignment

- Failure signatures:
  - Low AUC scores during intervention (<0.5)
  - No significant differences between treatment and control groups
  - High variance in individual predictions
  - Systematic prediction errors for certain learner types

- First 3 experiments:
  1. Test the individual CNN components separately (task difficulty prediction and effort prediction) on historical data to verify their standalone performance
  2. Implement a simpler baseline model (e.g., logistic regression) to compare against the CNN approach
  3. Conduct an A/B test on a subset of learners with a simpler task assignment mechanism to isolate the effect of the prediction algorithm from the assignment strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the individualization of learning paths affect educational outcomes in digital learning environments?
- Basis in paper: [explicit] The paper discusses the potential benefits of individualizing learning paths but finds null results in terms of learning effort and performance when using a machine learning algorithm.
- Why unresolved: The study's null results could be due to several factors such as insufficient individual data, limitations of the algorithm, or the complexity of learning processes. The paper suggests that future research should explore these areas further.
- What evidence would resolve it: Conducting studies with larger datasets on individual learners, improving the algorithm by training individual networks for each learner, or incorporating more comprehensive input features could provide insights into the effectiveness of individualized learning paths.

### Open Question 2
- Question: What are the optimal input features for machine learning algorithms to predict educational outcomes in online learning platforms?
- Basis in paper: [explicit] The paper notes that while they used platform behavior data to train their algorithm, they were not able to include more comprehensive data capturing the individual learner and the educational environment, which might be crucial for enhancing model performance.
- Why unresolved: The complexity of learning processes and the variety of factors influencing educational outcomes make it challenging to determine the most effective input features for machine learning algorithms.
- What evidence would resolve it: Developing and testing different sets of input features, including both platform behavior data and comprehensive data on learners and their educational environment, could help identify the optimal features for predicting educational outcomes.

### Open Question 3
- Question: How does the use of different machine learning algorithms impact the effectiveness of adaptive learning in online platforms?
- Basis in paper: [explicit] The paper applies a convolutional neural network and suggests that other approaches, such as random forest or boosting methods, could be explored.
- Why unresolved: The paper's use of a specific machine learning algorithm (convolutional neural network) may not be the most effective approach for adaptive learning. The complexity of learning processes and the variety of available machine learning algorithms make it challenging to determine the best approach.
- What evidence would resolve it: Conducting comparative studies using different machine learning algorithms (e.g., convolutional neural networks, random forests, boosting methods) in the same or similar online learning environments could provide insights into the most effective algorithms for adaptive learning.

## Limitations

- The algorithm's performance degraded substantially during intervention despite strong pre-training results
- Insufficient individual learner data may have prevented effective personalization
- Daily retraining process may not have been sufficient to overcome data sparsity issues

## Confidence

- **High confidence**: The experimental methodology (three-arm RCT) and data collection procedures are well-documented and rigorous
- **Medium confidence**: The negative results regarding ML effectiveness for adaptive learning are robust, but the specific reasons for failure require further investigation
- **Low confidence**: The theoretical mechanism linking task difficulty perception to future learning effort through the two-stage CNN architecture requires additional validation

## Next Checks

1. Conduct ablation studies to isolate whether the two-stage CNN architecture or the nearest-neighbor task assignment strategy is the primary source of the algorithm's poor performance
2. Test alternative approaches that incorporate more robust individual learner modeling, such as transfer learning techniques or meta-learning frameworks
3. Implement a phased rollout with continuous monitoring of AUC scores and prediction accuracy to identify specific failure patterns and optimize the retraining frequency