---
ver: rpa2
title: Motif-driven Subgraph Structure Learning for Graph Classification
arxiv_id: '2406.08897'
source_url: https://arxiv.org/abs/2406.08897
tags:
- graph
- learning
- structure
- subgraphs
- mosgsl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying graph structure
  learning (GSL) to graph classification tasks, where traditional GSL methods struggle
  due to coarse-grained supervision. The authors propose MOSGSL, a novel motif-driven
  subgraph structure learning framework.
---

# Motif-driven Subgraph Structure Learning for Graph Classification

## Quick Facts
- arXiv ID: 2406.08897
- Source URL: https://arxiv.org/abs/2406.08897
- Reference count: 40
- This paper addresses the challenge of applying graph structure learning to graph classification tasks with coarse-grained supervision, proposing MOSGSL which achieves significant accuracy improvements over state-of-the-art baselines.

## Executive Summary
This paper tackles the challenge of applying graph structure learning (GSL) to graph classification tasks where traditional GSL methods struggle due to coarse-grained supervision. The authors propose MOSGSL, a novel motif-driven subgraph structure learning framework that learns graph structures at the subgraph level rather than node level. By partitioning graphs into subgraphs and learning structures independently for each subgraph, MOSGSL better handles the coarse-grained supervision inherent in graph classification tasks. The method introduces a motif-driven structure guidance module that captures representative subgraph-level structural patterns (motifs) and uses them to guide personalized structure learning through iterative refinement.

## Method Summary
MOSGSL is a motif-driven subgraph structure learning framework that partitions graphs into subgraphs and learns structures independently for each subgraph. The method consists of two main modules: a subgraph structure learning module that uses a gate mechanism to adaptively select and combine important subgraphs, and a motif-driven structure guidance module that captures representative subgraph-level structural patterns and guides personalized structure learning. The approach iterates between subgraph-motif alignment and motif extraction to ensure mutual enhancement between structure learning and motif extraction.

## Key Results
- MOSGSL consistently outperforms state-of-the-art baselines, achieving 75.50% vs 72.60% accuracy on IMDB-B with GCN
- The method demonstrates flexibility and generalizability across various backbones and learning procedures including preprocessing, co-training, and test-time augmentation
- Significant accuracy improvements are observed across five different datasets (IMDB-B, IMDB-M, RDT-B, ENZYMES, PROTEINS)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Subgraph structure learning outperforms node-level GSL for graph classification due to better handling of coarse-grained supervision.
- **Mechanism:** By partitioning graphs into subgraphs and learning structures independently for each subgraph, the method avoids interference from irrelevant parts and focuses on local characteristics. The gate mechanism then adaptively combines these learned substructures based on importance scores.
- **Core assumption:** Subgraphs capture discriminative information at a higher level than individual nodes/edges, and their importance can be estimated through learned representations.
- **Evidence anchors:**
  - [abstract]: "inspired by the vital role of subgraph in graph classification"
  - [section]: "subgraphs have been widely recognized vital in graph-level tasks [16, 14], characterizing discriminative information at a higher level than individual nodes/edges"
  - [corpus]: Weak evidence - no direct mentions of subgraph-level structure learning advantages in corpus papers
- **Break condition:** If subgraph partitioning creates too many small or uninformative subgraphs, or if importance estimation fails to identify truly discriminative subgraphs.

### Mechanism 2
- **Claim:** Motif-driven guidance provides auxiliary supervision under coarse-grained labels, improving structure learning precision.
- **Mechanism:** Representative subgraph-level structural patterns (motifs) are extracted from candidate subgraphs in a label-wise manner. These motifs then guide the structure learning of other subgraphs through a contrastive alignment loss, creating diverse learning targets.
- **Core assumption:** Subgraphs from graphs of the same category exhibit discriminative structural patterns that can be captured as motifs and used as learning targets.
- **Evidence anchors:**
  - [abstract]: "A motif-driven structure guidance module is further introduced to capture key subgraph-level structural patterns (motifs) and facilitate personalized structure learning"
  - [section]: "Intuitively, subgraphs from graphs of different classes exhibit distinct structural patterns, which can be viewed as the targets for structure learning on subgraphs"
  - [corpus]: Weak evidence - corpus mentions motif-driven methods but not specifically for structure learning guidance
- **Break condition:** If extracted motifs fail to represent true class-specific patterns, or if the alignment loss doesn't effectively guide structure refinement.

### Mechanism 3
- **Claim:** Iterative refinement between structure learning and motif extraction creates mutual enhancement.
- **Mechanism:** The process alternates between subgraph-motif alignment (guiding structure learning) and motif extraction (updating patterns from refined structures). Better structures lead to better motifs, which in turn guide better structure learning.
- **Core assumption:** Structure learning and motif extraction are mutually reinforcing processes that benefit from iterative refinement.
- **Evidence anchors:**
  - [abstract]: "Notably, these two steps are iteratively conducted to ensure mutual enhancement between structure learning and motif"
  - [section]: "This iterative framework bolsters the mutual enhancement between structure learning and motif extraction, i.e., better structures lead to better motifs, and better motifs in turn guide better structure learning"
  - [corpus]: No direct evidence in corpus papers about iterative refinement between structure learning and motif extraction
- **Break condition:** If the iterative process converges to suboptimal solutions, or if the update frequency (every η epochs) is not appropriate for the dataset.

## Foundational Learning

- **Concept:** Graph neural networks and message passing
  - Why needed here: The method builds on GNN backbones and uses them for both subgraph encoding and importance estimation
  - Quick check question: How do GNNs aggregate information from neighbors, and why is this aggregation potentially problematic with suboptimal graph structures?

- **Concept:** Graph structure learning and its limitations
  - Why needed here: Understanding why conventional GSL methods struggle with graph-level tasks is crucial for appreciating the proposed solution
  - Quick check question: What makes node-level GSL methods inadequate for graph classification, and how does coarse-grained supervision contribute to this limitation?

- **Concept:** Subgraph extraction and representation
  - Why needed here: The method relies on partitioning graphs into subgraphs and representing them effectively for importance estimation and structure learning
  - Quick check question: How does the breadth-first search (BFS) partitioning method work, and what are the implications of the subgraph size constraints (M nodes)?

## Architecture Onboarding

- **Component map:**
  Input -> Graph partitioning (BFS) -> Subgraph encoding -> Importance scoring -> Independent structure learning -> Adaptive combination (gate) -> Candidate subgraph selection -> Motif extraction (K-means) -> Motif alignment (contrastive loss) -> Output (refined structure)

- **Critical path:**
  1. Graph partitioning → Subgraph encoding → Importance scoring
  2. Independent structure learning for each subgraph
  3. Adaptive combination of learned substructures
  4. Candidate subgraph selection
  5. Iterative motif alignment and extraction

- **Design tradeoffs:**
  - Partitioning granularity (K subgraphs, M nodes each) vs. computational cost
  - Number of motifs per class (R) vs. representational capacity
  - Proportion of filtered subgraphs (ϵ) vs. motif quality
  - Motif update frequency (η epochs) vs. training stability
  - Backbone flexibility vs. method specialization

- **Failure signatures:**
  - Performance degradation on datasets with very different graph sizes
  - Inconsistent improvements across different backbone GNNs
  - Poor results when motif initialization method is suboptimal
  - Instability in training dynamics, especially during motif update phases

- **First 3 experiments:**
  1. **Ablation study:** Remove the motif-driven guidance module and evaluate performance degradation
  2. **Sensitivity analysis:** Vary the number of subgraphs (K) and subgraph size limit (M) to find optimal partitioning
  3. **Initialization comparison:** Compare random motif initialization vs. pretraining-based initialization on a small dataset

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several key uncertainties remain:

- How MOSGSL's performance scales with larger graphs and more complex motif structures
- What specific subgraph patterns MOSGSL learns and how they relate to the graph classification task
- How MOSGSL's motif-driven approach compares to other subgraph-based methods that don't explicitly use motifs

## Limitations

- The method's performance and scalability on larger graphs with more complex structures remains unexplored
- Limited ablation studies make it difficult to quantify the individual contributions of subgraph structure learning vs. motif-driven guidance
- The reliance on BFS partitioning may create suboptimal subgraphs for certain graph types or structures

## Confidence

The analysis reveals several major uncertainties affecting confidence in the proposed mechanisms:
- **Subgraph structure learning mechanism:** Medium confidence due to limited direct evidence in the corpus
- **Motif-driven guidance mechanism:** Medium confidence as the assumption lacks strong empirical support in the literature
- **Iterative refinement mechanism:** Low confidence since there's no corpus evidence for this specific approach

## Next Checks

1. **Component Ablation Study:** Systematically disable the motif-driven guidance module and evaluate performance degradation to quantify its contribution beyond the base subgraph structure learning approach.

2. **Motif Robustness Analysis:** Test the sensitivity of motif extraction by varying the number of motifs per class (R) and filtering thresholds (ϵ) across multiple random seeds to assess stability.

3. **Cross-Dataset Generalization:** Apply MOSGSL to datasets with significantly different characteristics (e.g., varying graph sizes, edge densities, or label distributions) to validate the method's generalizability beyond the five tested datasets.