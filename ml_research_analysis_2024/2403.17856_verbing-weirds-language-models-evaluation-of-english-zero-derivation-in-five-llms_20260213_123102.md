---
ver: rpa2
title: 'Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in
  Five LLMs'
arxiv_id: '2403.17856'
source_url: https://arxiv.org/abs/2403.17856
tags:
- words
- language
- performance
- non-prototypical
- mistral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models can handle
  lexical-syntactic flexibility, specifically zero-derivation in English, where words
  change part of speech without morphological marking. A new NLI-based evaluation
  task was created with prototypical, non-prototypical, and nonce word conditions,
  using 3,069 prompts across five models (GPT-3.5, GPT-4, Mistral 7B, Falcon 40B,
  Llama 2 70B).
---

# Verbing Weirds Language (Models): Evaluation of English Zero-Derivation in Five LLMs

## Quick Facts
- arXiv ID: 2403.17856
- Source URL: https://arxiv.org/abs/2403.17856
- Reference count: 0
- Primary result: GPT-4 performs best on zero-derivation tasks, but model size is not a strong predictor; instead, prototypical task performance better predicts lexical-syntactic flexibility.

## Executive Summary
This study introduces the first systematic evaluation of lexical-syntactic flexibility—specifically zero-derivation in English—in large language models. Using a novel NLI-based task with 3,069 prompts across five models (GPT-3.5, GPT-4, Mistral 7B, Falcon 40B, Llama 2 70B), the authors find that GPT-4 performs best overall, but smaller models like Mistral 7B display competitive flexibility when null responses are filtered. Crucially, model size is not a strong predictor of performance; instead, ability on prototypical tasks better predicts flexibility. The study also reveals that models treat non-prototypical words similarly to nonce words, suggesting limited morphological generalization.

## Method Summary
The authors created a novel evaluation framework based on natural language inference (NLI) to test whether LLMs can handle zero-derivation—using words in non-prototypical syntactic roles without morphological change. They generated 3,069 prompts using syntactic frames and word lists covering prototypical, non-prototypical, and nonce conditions for transitive verbs, intransitive verbs, mass nouns, and count nouns. Responses were automatically coded as "yes," "no," or "null" using regex patterns. Five models were tested: GPT-3.5, GPT-4, Mistral 7B, Falcon 40B, and Llama 2 70B. Logistic regression was used to identify factors influencing performance.

## Key Results
- GPT-4 performs best on zero-derivation tasks, but smaller models (Mistral 7B) show competitive performance when null responses are filtered.
- Model size is not a strong predictor of lexical-syntactic flexibility; prototypical task performance better predicts flexibility.
- Non-prototypical and nonce word performance are not significantly different, suggesting models treat converted words as essentially novel.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical-syntactic flexibility in LLMs is not strictly a function of model size.
- Mechanism: Performance on zero-derivation tasks depends on the model's ability to perform natural language inference with prototypical constructions rather than raw parameter count.
- Core assumption: Models that perform well on prototypical tasks can generalize to non-prototypical ones, and vice versa.
- Evidence anchors:
  - [abstract]: "Model size is not a strong predictor; instead, performance on prototypical tasks better predicts flexibility."
  - [section]: "the model size was not a good predictor of lexical-syntactic flexibility. The largest of the open-source models—Llama 2 70B—is consistently mediocre in its performance on this task."
  - [corpus]: Weak signal; related papers focus on speech/language tasks but do not directly address zero-derivation.
- Break condition: If a smaller model consistently outperforms larger models across multiple lexical-syntactic tasks, the mechanism fails.

### Mechanism 2
- Claim: Zero-derivation (conversion) in English creates processing challenges similar to handling nonce words.
- Mechanism: When a word is used in a non-prototypical part of speech, the model treats it like a novel word with no established syntactic mapping, leading to similar error patterns.
- Core assumption: Models lack explicit morphological parsing for zero-derived forms, so context is the only cue.
- Evidence anchors:
  - [abstract]: "non-prototypical parts of speech are associated with better performance than nonce words" (the hypothesis was not supported).
  - [section]: "non-prototypical performance is not significantly different from nonce performance, suggesting that the models are treating converted words as, essentially, nonce words."
  - [corpus]: No direct evidence; related work focuses on ASR and code-switching rather than zero-derivation.
- Break condition: If models show systematically better handling of true zero-derived words versus nonce words, the mechanism breaks.

### Mechanism 3
- Claim: Natural language inference (NLI) capability is a bottleneck for zero-derivation performance.
- Mechanism: Models that struggle with basic NLI tasks (yes/no inference from premise) will also fail on zero-derivation because the task embeds NLI within a morphological generalization problem.
- Core assumption: NLI performance on prototypical frames predicts zero-derivation performance.
- Evidence anchors:
  - [abstract]: "In order to understand which factors most contributed to performance on the syntactic flexibility task, we fitted a Logistic Regression... answer type (yes, no, or null) as the strongest predictor."
  - [section]: "the model type was a significant predictor of performance" and "the generation of null responses may be due either to these superficial factors or to more basic differences in model behavior."
  - [corpus]: Weak signal; ASR and translation tasks do not directly map to NLI performance.
- Break condition: If models excel at NLI but still fail at zero-derivation, the mechanism breaks.

## Foundational Learning

- Concept: Part-of-speech (POS) tagging and syntactic role flexibility.
  - Why needed here: Zero-derivation relies on interpreting words in non-standard POS roles; understanding POS boundaries is essential.
  - Quick check question: Can you identify at least two examples of nouns that are commonly used as verbs in English?

- Concept: Natural Language Inference (NLI) task structure.
  - Why needed here: The evaluation uses NLI-style yes/no questions to probe whether the model understands the semantic implications of non-prototypical word usage.
  - Quick check question: Given the premise "I went for a swim," can you formulate an NLI question that tests whether "swim" is interpreted as a noun?

- Concept: Morphological derivation vs. zero-derivation.
  - Why needed here: The study distinguishes between overt morphological changes (like adding "-ed") and zero-derivation, where no overt marking occurs.
  - Quick check question: What is the key difference between "swimmer" (derivation) and "swim" used as a noun (zero-derivation)?

## Architecture Onboarding

- Component map:
  - Prompt generator: Combines syntactic frames with word lists to create 3,069 test items
  - Model interface: API calls for GPT-3.5/4; vLLM on GPUs for open models
  - Response parser: Regex-based extraction of "yes"/"no"/"null" answers
  - Evaluator: Accuracy calculation per model, condition, and word type
  - Logistic regression module: Analyzes factors influencing correctness

- Critical path:
  1. Generate prompts from frames and word lists
  2. Submit to model and collect raw output
  3. Parse output into categorical labels
  4. Compute accuracy metrics
  5. Fit logistic regression to identify predictive factors

- Design tradeoffs:
  - Frame simplicity vs. linguistic richness: Simple frames yield cleaner NLI judgments but may miss subtle zero-derivation cases
  - Model generality vs. task specificity: A generic NLI prompt suffix ("Answer with one word") is easy to implement but may not elicit the best reasoning
  - Manual curation vs. automatic generation: Manual curation ensures linguistic validity but limits scalability

- Failure signatures:
  - High proportion of null responses → model fails to follow instructions or lacks confidence
  - Systematic yes/no bias → model overfits to simplistic inference patterns
  - Size-performance mismatch → model architecture or training data, not size, drives results

- First 3 experiments:
  1. Replicate the evaluation with a larger set of syntactic frames to test robustness
  2. Test models on a balanced set of prototypical vs. non-prototypical words to confirm the size-independence claim
  3. Evaluate the effect of prompt phrasing ("Answer with one word" vs. open-ended) on null response rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do Mistral and Falcon frequently generate null responses, particularly for frames involving mass nouns and transitive verbs?
- Basis in paper: [explicit] The authors note that Mistral and Falcon often generate null responses, especially for mass noun frames and transitive verb frames, and suggest this may be due to superficial factors or differences in model behavior.
- Why unresolved: The paper does not provide a definitive explanation for this behavior, and the authors acknowledge that the causal mechanism is unclear.
- What evidence would resolve it: Further investigation into the training data and model architecture of Mistral and Falcon could provide insights into why these models generate null responses for specific types of frames. Additionally, experiments varying the sentence structure or word order in the prompts could help determine if the issue is related to superficial factors or deeper model behavior.

### Open Question 2
- Question: How do the performance differences between models on different subtasks (count nouns, mass nouns, transitive verbs, intransitive verbs) relate to the underlying model architecture and training data?
- Basis in paper: [inferred] The authors observe that different models perform differently on various subtasks, but they cannot construct a valid causal explanation due to the small and non-diverse set of frames. They suggest that there are significant differences between models correlated with subtasks.
- Why unresolved: The study uses a limited set of frames, making it difficult to draw definitive conclusions about the relationship between model performance and architecture or training data.
- What evidence would resolve it: A larger and more diverse set of frames for each subtask, combined with detailed analysis of the training data and architecture of the models, could help elucidate the reasons for performance differences. Additionally, ablation studies varying model size, architecture, or training data could provide further insights.

### Open Question 3
- Question: To what extent does lexical-syntactic flexibility generalize across different types of conversion (e.g., verb to noun, adjective to noun, etc.) and different semantic relationships between the base and converted words?
- Basis in paper: [inferred] The authors focus on testing lexical-syntactic flexibility in the context of conversion, but they do not investigate the extent to which this flexibility generalizes across different types of conversion or semantic relationships.
- Why unresolved: The study primarily uses a limited set of frames and word lists, which may not fully capture the range of conversion types and semantic relationships.
- What evidence would resolve it: Expanding the study to include a wider variety of conversion types and semantic relationships, along with a more diverse set of frames and word lists, could help determine the extent to which lexical-syntactic flexibility generalizes. Additionally, investigating the performance of models on specific types of conversion and semantic relationships could provide insights into the factors that influence flexibility.

## Limitations

- The study uses a small set of syntactic frames (eight total), which may not fully capture the range of zero-derivation patterns in English.
- Automatic response coding via regex patterns may misclassify nuanced model outputs, potentially underestimating model performance.
- The study does not investigate the role of model pretraining data or fine-tuning on linguistic tasks, which could explain performance differences between models of similar sizes.

## Confidence

- **High Confidence**: The finding that model size is not a strong predictor of zero-derivation performance is well-supported by the data, with the smallest model (Mistral 7B) outperforming the largest (Llama 2 70B) in several conditions.
- **Medium Confidence**: The claim that prototypical performance better predicts flexibility is supported, but the logistic regression analysis suggests answer type (yes/no/null) is the strongest predictor, complicating the interpretation.
- **Low Confidence**: The mechanism that models treat non-prototypical words as nonce words is weakly supported, as the study found no significant difference between these conditions, but does not directly test this hypothesis.

## Next Checks

1. **Expand Frame Coverage**: Test the evaluation framework with a larger and more diverse set of syntactic frames to assess whether the current findings generalize across a broader range of zero-derivation contexts.

2. **Probe Null Response Causes**: Conduct a qualitative analysis of null responses to determine whether they stem from model uncertainty, instruction-following failures, or genuine inability to make the required inference.

3. **Investigate Training Data Effects**: Analyze the pretraining corpora of the evaluated models to identify whether exposure to zero-derivation patterns correlates with better performance, independent of model size.