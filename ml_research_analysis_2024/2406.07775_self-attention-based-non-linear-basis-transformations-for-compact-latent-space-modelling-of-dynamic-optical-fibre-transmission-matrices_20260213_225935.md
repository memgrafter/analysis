---
ver: rpa2
title: Self-attention-based non-linear basis transformations for compact latent space
  modelling of dynamic optical fibre transmission matrices
arxiv_id: '2406.07775'
source_url: https://arxiv.org/abs/2406.07775
tags:
- fibre
- basis
- matrices
- matrix
- transformation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a self-attention-based neural network approach
  to learn compact, basis-invariant representations of dynamically varying optical
  fibre transmission matrices (TMs). By dynamically transforming TMs into sparse bases
  using self-attention mechanisms, they enable efficient modeling and reconstruction
  of TMs under perturbations like bending and temperature shifts.
---

# Self-attention-based non-linear basis transformations for compact latent space modelling of dynamic optical fibre transmission matrices

## Quick Facts
- arXiv ID: 2406.07775
- Source URL: https://arxiv.org/abs/2406.07775
- Reference count: 40
- Primary result: Self-attention networks achieve 10-100× improvement in TM sparsity (participation ratio 0.01-0.11) while maintaining <10% reconstruction error

## Executive Summary
This paper introduces a self-attention-based neural network approach for learning compact, basis-invariant representations of dynamically varying optical fiber transmission matrices (TMs). By dynamically transforming TMs into sparse bases using self-attention mechanisms, the method enables efficient modeling and reconstruction of TMs under perturbations like bending and temperature shifts. The approach significantly improves TM sparsity while maintaining invertibility through an autoencoder constraint, paving the way for efficient real-time fiber imaging without distal access.

## Method Summary
The method uses self-attention layers to dynamically transform optical fiber transmission matrices into compact latent-space representations. The self-attention mechanism computes weighted combinations of matrix elements based on input-specific perturbation states, creating learned coordinate transformations that concentrate signal energy into fewer elements. An autoencoder structure ensures invertibility while promoting sparsity in the learned representation. The approach is evaluated on three diverse datasets of 78×78 complex-valued perturbed TMs, including randomly-generated forward TMs, round-trip TMs, and physically modeled perturbed TMs.

## Key Results
- Self-attention models achieve participation ratios of 0.01-0.11, representing 10-100× improvement in sparsity over linear similarity transformations
- TM reconstruction with <10% error demonstrates successful invertibility while maintaining sparse representations
- Self-attention-based FCNNs outperform linear similarity transformations, CNNs, and FCNNs alone on all three diverse datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-attention enables input-dependent basis transformations that discover sparse representations of optical fiber transmission matrices
- Mechanism: The self-attention layers compute weighted combinations of matrix elements where weights depend on the specific perturbation state, creating a learned coordinate transformation that concentrates signal energy into fewer elements
- Core assumption: Fiber transmission matrices under various perturbations share common structure that can be captured by adaptive basis functions rather than fixed linear transformations
- Evidence anchors: Abstract statement about transforming coordinate representations to admit compact representations; section explanation of attention mechanism's selective focusing

### Mechanism 2
- Claim: Self-attention-based networks achieve better scalability than fully connected networks for matrix transformations
- Mechanism: While fully connected networks require O(N^4) parameters for NxN matrices, self-attention with local interactions reduces complexity by factorizing the transformation into attention-weighted column combinations
- Core assumption: The TM transformation can be expressed as weighted combinations of column vectors rather than independent element-wise mappings
- Evidence anchors: Section comparison showing attention outperforms FCNN with reduced participation ratio; note about self-attention growing with square of embedding dimension

### Mechanism 3
- Claim: The autoencoder constraint ensures invertibility while promoting sparsity in the learned representation
- Mechanism: The encoder compresses the transformed TM into a bottleneck latent space, forcing the model to retain only essential information. The decoder then reconstructs the original TM, providing a loss signal that prevents trivial solutions
- Core assumption: The most compact representation of the TM that allows reconstruction also corresponds to the sparsest basis transformation
- Evidence anchors: Section describing autoencoder structure for reconstruction; explanation of decoder ensuring latent space contains necessary information

## Foundational Learning

- Concept: Matrix diagonalization and sparse representations
  - Why needed here: Understanding that diagonal matrices are maximally sparse and that basis transformations can reveal this structure is crucial for grasping why the method works
  - Quick check question: What is the participation ratio of a perfectly diagonal matrix with N elements per dimension?

- Concept: Self-attention mechanisms and their computational complexity
  - Why needed here: The paper relies on self-attention to learn input-dependent transformations, but this comes with specific computational costs that scale with matrix size
  - Quick check question: If a transmission matrix has dimension N×N, how many elements does the self-attention mechanism need to process?

- Concept: Autoencoder architectures and latent space compression
  - Why needed here: The invertibility constraint uses autoencoders to ensure the transformation preserves information while promoting sparsity
  - Quick check question: What happens to reconstruction quality as the bottleneck layer size decreases?

## Architecture Onboarding

- Component map: Input (complex TM) -> Self-attention block (Query/Key computation) -> (Optional FCNN layers) -> Autoencoder encoder (bottleneck) -> (Optional FCNN layers) -> Autoencoder decoder -> Output (reconstructed TM)
- Critical path: Input → Self-attention → (Optional FCNN) → Autoencoder bottleneck → (Optional FCNN) → Output
- Design tradeoffs:
  - Attention-only vs attention+FCNN: Simpler models train faster but may miss complex patterns
  - Bottleneck size: Smaller improves sparsity but risks information loss
  - Local vs global attention: Local reduces computation but may miss long-range correlations
- Failure signatures:
  - No sparsity improvement: Model architecture too simple or training signal too weak
  - High reconstruction error: Bottleneck too small or attention mechanism not capturing essential patterns
  - Overfitting: Model memorizing training TMs rather than learning generalizable transformations
- First 3 experiments:
  1. Train linear similarity transformation model on dense round-trip TMs to establish baseline participation ratio
  2. Train self-attention-only model on same data to verify improvement over linear baseline
  3. Train attention+FCNN model with varying bottleneck sizes to find optimal sparsity-reconstruction tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed self-attention-based approach scale with increasing matrix dimensions, particularly for high-resolution imaging applications?
- Basis in paper: The paper mentions that self-attention networks grow with the square of the embedding dimension, and since transmission matrices have a number of elements equal to x² × x² for input fields containing x × x pixels, the self-attention architecture grows as the fourth power of the image dimension (i.e., x⁴)
- Why unresolved: The paper discusses the memory-intensive nature of the training process due to large matrix sizes but does not provide specific scaling results or performance metrics for higher-resolution matrices
- What evidence would resolve it: Experimental results demonstrating the performance and memory requirements of the self-attention-based model for matrices with dimensions significantly larger than 78 × 78, along with comparisons to other approaches

### Open Question 2
- Question: Can the self-attention-based model effectively handle dynamic changes in the transmission matrix due to multiple types of perturbations, such as bending, temperature shifts, and wavelength modulation, simultaneously?
- Basis in paper: The paper mentions that realistic real-time fiber characterization methods must account for dynamic changes in the TM due to factors like movement and temperature shifts. It also states that physical models become intractable when modeling multiple types of perturbations that must be estimated using as few measurements as possible
- Why unresolved: While the paper demonstrates the model's effectiveness on diverse datasets, including physically modeled perturbed TMs, it does not explicitly address the model's ability to handle multiple simultaneous perturbations
- What evidence would resolve it: Experimental results showing the model's performance when trained and tested on TMs with multiple simultaneous perturbations, such as varying bend configurations, temperature shifts, and wavelength modulation

### Open Question 3
- Question: How does the proposed approach perform in real-world experimental settings, particularly in terms of image quality and robustness to noise and imperfections in the fiber and imaging system?
- Basis in paper: The paper focuses on simulated and modeled datasets, but acknowledges that real-world experimental data could be challenging to obtain. It mentions that augmenting experimental data with simulated fiber data and using generative networks for data augmentation and domain transfer could help address this limitation
- Why unresolved: The paper does not present experimental results using real fiber imaging systems, and the performance of the approach in real-world conditions remains unclear
- What evidence would resolve it: Experimental results demonstrating the approach's effectiveness in a real fiber imaging system, including quantitative metrics for image quality and robustness to noise and system imperfections

## Limitations
- The work makes strong assumptions about the structural regularity of transmission matrices under perturbations, and it remains unclear whether sparsity gains will generalize to more extreme or pathological scenarios
- The three datasets used, though diverse, may not capture the full range of real-world fiber perturbations
- The computational complexity of self-attention scales quadratically with embedding dimension, which could limit applicability to very large matrices despite being better than fully connected approaches

## Confidence
- **High confidence**: The sparsity improvements and reconstruction error metrics are well-supported by experimental results across multiple datasets
- **Medium confidence**: The claim about better scalability compared to fully connected networks is supported by theoretical complexity analysis but not exhaustively validated across different matrix sizes and dimensions
- **Medium confidence**: The assertion that the most compact representation enabling reconstruction also corresponds to the sparsest basis transformation is intuitive and supported by the autoencoder constraint, but the relationship could benefit from more rigorous mathematical proof

## Next Checks
1. **Extreme perturbation testing**: Evaluate model performance on TMs with extreme bending angles, temperature gradients, or mechanical stress that were not present in the training data to test generalization limits
2. **Computational scaling analysis**: Systematically measure training and inference time, memory usage, and parameter count as matrix dimensions scale from 78×78 to 256×256 and beyond to verify the claimed complexity advantages
3. **Alternative sparsity metrics**: Apply additional sparsity measures such as Gini index, Hoyer metric, or element-wise entropy to validate that the observed improvements are robust across different definitions of sparsity