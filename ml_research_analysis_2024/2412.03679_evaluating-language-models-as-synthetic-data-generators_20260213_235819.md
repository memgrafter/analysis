---
ver: rpa2
title: Evaluating Language Models as Synthetic Data Generators
arxiv_id: '2412.03679'
source_url: https://arxiv.org/abs/2412.03679
tags:
- data
- instruction
- generation
- llama-3
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AGORA BENCH, a benchmark for evaluating language
  models as synthetic data generators. The benchmark standardizes settings across
  three domains (math, instruction-following, code) and three data generation methods
  (instance generation, response generation, quality enhancement).
---

# Evaluating Language Models as Synthetic Data Generators

## Quick Facts
- arXiv ID: 2412.03679
- Source URL: https://arxiv.org/abs/2412.03679
- Reference count: 25
- This paper introduces AGORA BENCH, a benchmark for evaluating language models as synthetic data generators across three domains and three data generation methods.

## Executive Summary
This paper introduces AGORA BENCH, a benchmark for evaluating language models as synthetic data generators. The benchmark standardizes settings across three domains (math, instruction-following, code) and three data generation methods (instance generation, response generation, quality enhancement). Six language models were evaluated by generating 10K training instances each, which were used to train 99 student models. The study finds that language models exhibit distinct strengths across different data generation methods, with no strong correlation between problem-solving ability and data generation effectiveness.

## Method Summary
The benchmark evaluates six language models (GPT-4o, GPT-4o-mini, Claude-3.5-Sonnet, Llama-3.1-405B, Llama-3.1-70B, Llama-3.1-8B) as synthetic data generators across three domains and three data generation methods. Each model generates 10K training instances using domain-specific meta-prompts and seed datasets. Student models (Llama-3.1-8B) are trained on each generated dataset using supervised fine-tuning with fixed hyperparameters. Performance is measured using Performance Gap Recovered (PGR) scores, comparing student model performance to base model performance. Intrinsic evaluation metrics (response quality, perplexity, instruction difficulty) are analyzed using PCA to identify patterns in data generation effectiveness.

## Key Results
- GPT-4o excels at generating new problems while Claude-3.5-Sonnet performs better at enhancing existing ones
- A model's data generation ability does not strongly correlate with its problem-solving ability
- Multiple intrinsic quality metrics (response quality, perplexity, instruction difficulty) collectively predict student model improvement better than any single metric
- Cost-effective model selection significantly impacts data generation effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models' effectiveness as synthetic data generators depends on domain-specific strengths rather than general problem-solving ability.
- Mechanism: Different LMs exhibit specialized capabilities in specific data generation methods that don't directly correlate with their raw problem-solving scores on benchmarks.
- Core assumption: The skills required for generating useful synthetic training data differ from those needed for direct problem solving.
- Evidence anchors:
  - [abstract]: "GPT-4o excels at generating new problems, while Claude-3.5-Sonnet performs better at enhancing existing ones"
  - [section 4]: "GPT-4o achieves the highest PGR scores in five settings" but "weaker LMs can outperform Stronger LMs" in specific domains
- Break condition: If all LMs performed similarly across all data generation methods, this mechanism would be invalidated.

### Mechanism 2
- Claim: Multiple intrinsic quality metrics collectively predict student model improvement better than any single metric.
- Mechanism: Principal component analysis reveals that combinations of instruction difficulty, response quality, and perplexity explain up to 93.4% of variance in performance gap recovered (PGR) scores.
- Core assumption: Data generation quality is multidimensional and cannot be captured by simple metrics like response quality alone.
- Evidence anchors:
  - [section 5.2]: "the top five principal components explain approximately 93.4% of the variance in AGORA BENCH results"
  - [section 5.2]: "response quality-related metrics shows slightly stronger contributions than diversity-related metrics"
- Break condition: If single metrics like response quality alone could predict PGR scores as well as the PCA approach.

### Mechanism 3
- Claim: Cost-effectiveness in data generation doesn't directly correlate with model strength.
- Mechanism: Less expensive LMs can generate more effective synthetic data per dollar spent than their more expensive counterparts when generating large volumes.
- Core assumption: There's a sweet spot where model capability meets cost efficiency for data generation tasks.
- Evidence anchors:
  - [section 4]: "Llama-3.1-8B-Instruct outperforms both Llama-3.1-70B-Instruct and Llama-3.1-405B-Instruct while being 6 to 32.5 times less expensive"
  - [section 4]: "GPT-4o achieves better performance than Claude-3.5-Sonnet at 1.2 to 1.5 times lower cost"
- Break condition: If expensive models consistently outperformed cheaper ones regardless of scale or domain.

## Foundational Learning

- Concept: Performance Gap Recovered (PGR) metric
  - Why needed here: Provides a standardized way to measure how much a student model improves relative to a reference post-training process
  - Quick check question: If a student model trained on synthetic data achieves the same performance as the reference model, what would its PGR score be?

- Concept: Principal Component Analysis (PCA) for multi-metric prediction
  - Why needed here: Allows identification of underlying patterns in how different quality metrics combine to predict data generation effectiveness
  - Quick check question: Why might PCA be preferred over simple linear regression when analyzing multiple correlated quality metrics?

- Concept: Data generation method categorization
  - Why needed here: Differentiates between generating new instances, generating responses, and enhancing quality - each requiring different LM capabilities
  - Quick check question: What's the key difference between instance generation and response generation methods?

## Architecture Onboarding

- Component map: Data generator selection → Meta-prompt design → Seed dataset preparation → 10K instance generation → Student model training → Benchmark evaluation
- Critical path:
  1. Generate 10K instances per LM per setting
  2. Train student model on each generated dataset
  3. Evaluate student models on benchmarks
  4. Calculate PGR scores
  5. Perform intrinsic evaluation of generated data
  6. Run PCA analysis on quality metrics
- Design tradeoffs:
  - Cost vs. quality: Using expensive models for more data vs. cheaper models for larger volumes
  - Format vs. quality: Structured JSON outputs may reduce LM output quality compared to free-form generation
  - Scale vs. consistency: Larger datasets may introduce more diversity but potentially less consistency
- Failure signatures:
  - Negative PGR scores indicate synthetic data degraded performance
  - Low correlation between problem-solving and data generation abilities suggests misalignment
  - Inconsistent results across domains suggest domain-specific limitations
- First 3 experiments:
  1. Generate 10K instances using GPT-4o for math instance generation and train a student model
  2. Generate 10K instances using Claude-3.5-Sonnet for code quality enhancement and train a student model
  3. Generate 10K instances using Llama-3.1-8B for instruction-following response generation and train a student model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of data generation scale when using language models as data generators to produce datasets larger than 10K instances?
- Basis in paper: [explicit] The paper acknowledges a limitation that their experimental setting does not cover scenarios where more than 10K instances are generated, and they conducted preliminary scaling experiments with up to 50K instances using GPT-4o-mini, Llama-3.1-70B-Instruct, and Llama-3.1-8B-Instruct.
- Why unresolved: The scaling experiments were limited to only three language models and the results suggest interesting patterns about cost-effectiveness, but the paper explicitly states that future work could explore whether the relative effectiveness rankings among different data generators remain consistent at larger scales.
- What evidence would resolve it: Systematic experiments scaling from 10K to 100K+ instances across all six language models studied, measuring Performance Gap Recovered (PGR) at each scale point to determine if there are diminishing returns, scaling laws, or changes in which models become most effective at different dataset sizes.

### Open Question 2
- Question: Do the findings about the disconnect between problem-solving ability and data generation ability generalize across different base models beyond Llama-3.1-8B?
- Basis in paper: [explicit] The paper explicitly states that due to compute constraints and API costs, they could not investigate whether their findings would hold when using different base models beyond Llama-3.1-8B, and they note this is particularly important given that response perplexity (one of their intrinsic evaluation metrics) is inherently dependent on the base model.
- Why unresolved: The study used only one base model (Llama-3.1-8B) for training student models, so while they found that stronger problem-solving LMs don't necessarily make better data generators, they cannot determine if this relationship holds across different architectures and model families.
- What evidence would resolve it: Replicating the AGORA BENCH experiments with multiple base models of varying sizes and architectures (e.g., Llama-3.1-70B, Llama-3.1-405B, and potentially other families) to test if the weak correlation between problem-solving and data generation abilities persists across different model foundations.

### Open Question 3
- Question: What additional intrinsic metrics beyond instruction difficulty, response quality, perplexity, and diversity could better predict data generation capabilities?
- Basis in paper: [explicit] The paper's PCA analysis revealed that the top five principal components explain approximately 93.4% of the variance in AGORA BENCH results, but the authors note that the moderate R² value (0.325) when predicting data generation capabilities suggests additional intrinsic metrics might be needed.
- Why unresolved: While the study analyzed multiple intrinsic evaluation metrics and found they collectively influence student model improvement, the relatively low predictive power indicates the current set of metrics may be incomplete for fully characterizing what makes an effective data generator.
- What evidence would resolve it: Systematic exploration of additional potential intrinsic metrics such as instruction novelty, response specificity, semantic coherence, instruction-response alignment, or other quality dimensions, followed by PCA and regression analysis to determine if they improve the predictive power for data generation capabilities beyond the current 32.5% variance explained.

## Limitations
- The benchmark relies on a single student model architecture (Llama-3.1-8B), which may not capture how different base models interact with synthetic data
- The study focuses on three specific domains and may not generalize to other task types or more specialized domains
- Cost-effectiveness analysis assumes static pricing models and doesn't account for potential economies of scale or negotiated enterprise rates

## Confidence
- **High Confidence**: Language models exhibit distinct strengths across different data generation methods; PGR metric effectively measures synthetic data quality improvements; multiple quality metrics collectively predict data generation effectiveness
- **Medium Confidence**: No strong correlation exists between problem-solving ability and data generation effectiveness; strategic choices in output format significantly impact data generation effectiveness; cost-conscious model selection meaningfully impacts overall data generation efficiency
- **Low Confidence**: The specific rank ordering of LMs across all domains and methods is stable across different experimental conditions; findings generalize to domains beyond math, instruction-following, and code; results would remain consistent with different student model architectures

## Next Checks
1. **Cross-architecture validation**: Repeat the benchmark using different student model architectures (e.g., Mistral, Qwen) to verify whether the observed patterns in LM data generation effectiveness remain consistent across base model families.

2. **Domain expansion**: Apply the benchmark methodology to additional domains such as multilingual tasks, reasoning under uncertainty, or specialized technical domains to assess generalizability beyond the three tested domains.

3. **Temporal stability analysis**: Re-run the benchmark after 3-6 months with updated versions of the same LMs to determine whether the observed performance patterns and cost-effectiveness relationships remain stable as models evolve.