---
ver: rpa2
title: 'DreamPolish: Domain Score Distillation With Progressive Geometry Generation'
arxiv_id: '2411.01602'
source_url: https://arxiv.org/abs/2411.01602
tags:
- generation
- geometry
- diffusion
- texture
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DreamPolish introduces a two-phase text-to-3D generation approach
  that achieves refined geometry and photorealistic textures. The method first progressively
  constructs 3D geometry using multiple neural representations (NeRF, NeuS, DMTet)
  with surface supervision from both reference views and novel views using a normal
  estimation diffusion prior.
---

# DreamPolish: Domain Score Distillation With Progressive Geometry Generation

## Quick Facts
- arXiv ID: 2411.01602
- Source URL: https://arxiv.org/abs/2411.01602
- Reference count: 40
- Primary result: Achieves 25.13 PSNR, 0.933 SSIM, 0.087 LPIPS, and 0.759 CLIP score in text-to-3D generation

## Executive Summary
DreamPolish introduces a two-phase text-to-3D generation approach that achieves refined geometry and photorealistic textures. The method first progressively constructs 3D geometry using multiple neural representations (NeRF, NeuS, DMTet) with surface supervision from both reference views and novel views using a normal estimation diffusion prior. In the second phase, it employs a novel Domain Score Distillation (DSD) objective that balances guidance from variational domains and unconditional image domains to enhance texture quality while maintaining stability. Compared to state-of-the-art methods, DreamPolish shows superior performance across multiple metrics with strong user study preferences.

## Method Summary
DreamPolish employs a progressive geometry construction pipeline that transitions from NeRF (for initial rough structure) to NeuS (for surface accuracy) to DMTet (for explicit mesh control). The method incorporates surface supervision using a normal estimation diffusion prior to polish geometry details in novel views. For texture generation, it introduces Domain Score Distillation (DSD) that balances guidance from variational domains (for photorealism) and unconditional image domains (for stability). This two-phase approach addresses geometric artifacts while producing high-quality textures.

## Key Results
- Achieves 25.13 PSNR, 0.933 SSIM, 0.087 LPIPS, and 0.759 CLIP score
- Outperforms state-of-the-art methods in both quantitative metrics and user studies
- Effectively addresses geometric surface artifacts while improving texture photorealism
- Demonstrates superior performance across multiple object categories

## Why This Works (Mechanism)

### Mechanism 1
Progressive geometry construction with multiple neural representations (NeRF → NeuS → DMTet) improves surface detail quality. Each representation stage captures progressively finer geometric features while leveraging the strengths of different representations. Surface artifacts in earlier stages can be refined by later stages with better surface representations.

### Mechanism 2
Domain Score Distillation (DSD) balances texture quality and stability by combining variational domain and unconditional image domain guidance. DSD uses gradients from both a learned variational domain (for photorealism) and unconditional image domain (for stability/diversity) to guide the texture generation process.

### Mechanism 3
Normal estimation diffusion prior in novel views provides surface supervision that view-conditioned priors alone cannot deliver. The normal estimator predicts surface normals from rendered images, providing explicit surface supervision in novel views where reference images are unavailable.

## Foundational Learning

- Concept: Score Distillation Sampling (SDS) and its limitations
  - Why needed here: Understanding the baseline approach that DreamPolish improves upon is crucial for grasping why the novel mechanisms are necessary
  - Quick check question: Why does standard SDS require high classifier-free guidance weights, and what problems does this cause?

- Concept: Progressive neural representation switching
  - Why needed here: The method relies on understanding the strengths and limitations of different 3D representations and how they can be combined
  - Quick check question: What are the key advantages of NeRF, NeuS, and DMTet that make them suitable for progressive construction?

- Concept: Classifier-free guidance and its application to score distillation
  - Why needed here: DSD is explicitly inspired by classifier-free guidance, so understanding this concept is essential for understanding the mechanism
  - Quick check question: How does classifier-free guidance work in text-to-image diffusion models, and how is this principle adapted in DSD?

## Architecture Onboarding

- Component map: Text prompt → Progressive geometry construction (NeRF → NeuS → DMTet) → Surface polishing (normal estimation prior) → Texture generation (DSD) → Final 3D model
- Critical path: Text prompt → Progressive geometry construction → Surface polishing → Texture generation (DSD) → Final 3D model
- Design tradeoffs: Progressive representation switching adds complexity but improves quality; normal estimation prior adds computational cost but provides crucial surface supervision; DSD balances stability and quality but requires careful tuning
- Failure signatures: Geometry artifacts persisting through all stages; unstable training during texture generation; poor surface quality despite normal supervision
- First 3 experiments:
  1. Implement basic progressive geometry construction (NeRF only) and verify geometry quality improvement over single-representation baseline
  2. Add surface polishing stage with normal estimation and measure artifact reduction
  3. Implement basic DSD with only one guidance domain and verify stability improvements over standard SDS

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between classifier-free guidance (CFG) weight and variational domain guidance for achieving both high-quality textures and training stability in text-to-3D generation?
- Basis in paper: [explicit] The paper discusses how different guidance domains affect texture quality and stability, and proposes DSD to balance these aspects
- Why unresolved: The paper shows that CFG weight needs to be carefully balanced with variational domain guidance, but doesn't provide a systematic method to determine the optimal balance
- What evidence would resolve it: Empirical studies comparing different combinations of CFG weights and variational domain guidance strengths across various text prompts and object categories

### Open Question 2
- Question: How does the progressive construction approach using multiple neural representations compare to using a single advanced representation throughout training in terms of computational efficiency and final output quality?
- Basis in paper: [explicit] The paper describes a progressive construction pipeline but doesn't compare this approach to using a single representation throughout training
- Why unresolved: While the progressive approach is shown to be effective, the paper doesn't quantify the trade-offs between computational cost and quality gains compared to alternative strategies
- What evidence would resolve it: Controlled experiments comparing training time, memory usage, and output quality metrics when using progressive construction versus single-representation approaches

### Open Question 3
- Question: Can the normal estimation diffusion prior be extended to provide guidance for other geometric properties beyond surface normals, such as curvature or material properties?
- Basis in paper: [explicit] The paper introduces a normal estimation diffusion prior for surface polishing, demonstrating its effectiveness in refining geometry artifacts
- Why unresolved: The paper focuses specifically on normal estimation but doesn't explore whether similar approaches could be applied to other geometric properties
- What evidence would resolve it: Experiments showing the impact of incorporating additional geometric property priors on geometry quality metrics, compared to using only normal estimation

### Open Question 4
- Question: What is the impact of field-of-view diversity on the effectiveness of surface polishing, and is there an optimal range of camera parameters that maximizes polishing performance?
- Basis in paper: [explicit] The paper mentions loosening field-of-view restrictions during surface polishing but doesn't systematically study the impact of different FOV ranges
- Why unresolved: While the paper shows that increasing FOV diversity helps with surface polishing, it doesn't provide guidance on what range of FOVs is optimal
- What evidence would resolve it: Systematic studies varying the range of FOVs used during polishing and measuring their impact on geometry quality metrics

## Limitations
- The progressive representation switching mechanism lacks clear criteria for transitioning between NeRF, NeuS, and DMTet
- Optimal balance between variational domain and unconditional image domain guidance weights may be task-specific
- Effectiveness depends on the accuracy of the diffusion-based normal estimator, which may vary across object categories

## Confidence

- Progressive Geometry Construction: Medium
- Domain Score Distillation: High
- Normal Estimation Prior: Medium

## Next Checks

1. **Ablation Study on Representation Switching**: Systematically evaluate the impact of different transition points between NeRF, NeuS, and DMTet representations to determine optimal progression criteria and verify that each stage genuinely improves geometric quality.

2. **DSD Weight Sensitivity Analysis**: Conduct experiments varying the weights λrealistic and λstable across a wide range to identify the sensitivity of texture quality and stability to these hyperparameters, and determine if the optimal balance is consistent across different object types.

3. **Cross-Domain Generalization Test**: Apply the DreamPolish framework to 3D object categories not represented in the training data (e.g., architectural structures or industrial equipment) to assess the generalizability of both the progressive geometry construction and DSD mechanisms beyond the tested domains.