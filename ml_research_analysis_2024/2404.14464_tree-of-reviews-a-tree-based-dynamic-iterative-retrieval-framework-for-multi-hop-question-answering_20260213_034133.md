---
ver: rpa2
title: 'Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop
  Question Answering'
arxiv_id: '2404.14464'
source_url: https://arxiv.org/abs/2404.14464
tags:
- question
- answer
- retrieval
- paragraphs
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Tree of Reviews (ToR), a tree-based dynamic
  iterative retrieval framework for multi-hop question answering. ToR addresses the
  limitations of chain-like iterative retrieval methods, which are prone to cascading
  errors due to irrelevant retrieved paragraphs and single reasoning path mistakes.
---

# Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering

## Quick Facts
- **arXiv ID**: 2404.14464
- **Source URL**: https://arxiv.org/abs/2404.14464
- **Authors**: Li Jiapeng; Liu Runze; Li Yabo; Zhou Tong; Li Mingling; Chen Xiang
- **Reference count**: 18
- **Primary result**: Tree of Reviews (ToR) achieves state-of-the-art performance in both retrieval (13.3%, 12.0%, 1.5% improvement in recall@15) and response generation (9.2%, 10.3%, 0.3% improvement in F1 scores) on three multi-hop question answering datasets

## Executive Summary
This paper introduces Tree of Reviews (ToR), a tree-based dynamic iterative retrieval framework for multi-hop question answering that addresses the limitations of chain-like iterative retrieval methods. ToR constructs a tree with the question as root and retrieved paragraphs as nodes, using a "paragraphs review block" to dynamically decide whether to search further, reject, or accept based on current reasoning paths. The framework also proposes two optimization strategies—pruning and effective expansion—to enhance search efficiency and path diversity. Experiments demonstrate ToR significantly outperforms existing methods in both retrieval quality and response generation accuracy.

## Method Summary
ToR is a tree-based dynamic iterative retrieval framework that constructs a reasoning tree where the initial question serves as the root and individual paragraphs form the nodes. The framework uses a "paragraphs review block" that dynamically decides to initiate new searches, reject irrelevant paragraphs, or accept sufficient evidence based on the reasoning paths. Two optimization strategies are introduced: pruning (relevance pruning and repetitive pruning) to reduce invalid searches, and effective expansion (CoT expansion and missing paragraph completion) to improve query generation and path diversity. The framework integrates retrieval-augmented generation with iterative reasoning to handle multi-hop questions requiring information from multiple sources.

## Key Results
- ToR outperforms best baseline methods by 13.3%, 12.0%, and 1.5% in retrieval metrics on three datasets
- Improves F1 scores by 9.2%, 10.3%, and 0.3% in response generation on respective datasets
- Achieves state-of-the-art performance in both retrieval and response generation
- Tree structure effectively mitigates cascading errors compared to chain-like approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tree structure mitigates cascading errors by isolating each retrieved paragraph in separate nodes
- Mechanism: Each node contains only one paragraph, preventing irrelevant information from affecting other reasoning paths. The framework dynamically decides whether to accept, reject, or search further for each node independently.
- Core assumption: Errors in one reasoning path don't affect other paths when nodes are separated
- Evidence anchors:
  - [abstract] "Compared to related work, we introduce a tree structure to handle each retrieved paragraph separately, alleviating the misleading effect of irrelevant paragraphs on the reasoning path"
  - [section 3.2] "We construct a tree with the initial question serving as the root and individual paragraphs as other nodes during the retrieval phase. Each node contains a single paragraph, mitigating the risk of diverging the reasoning process due to irrelevant information."

### Mechanism 2
- Claim: Tree-based iterative retrieval with dynamic decision-making improves retrieval quality
- Mechanism: The framework uses a "paragraphs review block" that dynamically decides to initiate new search, reject, or accept based on current path. This creates adaptive search patterns rather than fixed-depth retrieval.
- Core assumption: Dynamic decision-making based on current path information outperforms fixed-depth approaches
- Evidence anchors:
  - [abstract] "Our framework dynamically decides to initiate a new search, reject, or accept based on the paragraphs on the reasoning paths"
  - [section 3.2] "The paragraphs review block selects an action based on the question Q and paragraphs along the path... If it is not relevant, then action selection is [Reject] then stop search; if it is relevant but not enough, action selection is [Search] then generate a new query and retrieval with it; if it is relevant and enough, then action selection is [Accept] then stop search."

### Mechanism 3
- Claim: Tree-based pruning and expansion strategies optimize search efficiency while maintaining path diversity
- Mechanism: Two optimization strategies - pruning removes unproductive searches (relevance pruning and repetitive pruning), while effective expansion improves query generation through CoT expansion and missing paragraph completion.
- Core assumption: Strategic pruning and expansion can maintain search quality while reducing computational overhead
- Evidence anchors:
  - [abstract] "we propose two tree-based search optimization strategies, pruning and effective expansion, to reduce time overhead and increase the diversity of path extension"
  - [section 3.4] "Pruning aims to reduce the initiation of invalid searches... Effective expansion aims to optimize the effectiveness and diversity of paragraphs review block initiating new queries."

## Foundational Learning

- Concept: Multi-hop question answering requires retrieving information from multiple sources
  - Why needed here: The framework specifically addresses multi-hop questions that need information from multiple paragraphs
  - Quick check question: What distinguishes a multi-hop question from a single-hop question?

- Concept: Chain-of-thought reasoning enables step-by-step problem solving
  - Why needed here: The framework uses CoT capability of LLMs to generate intermediate reasoning steps and queries
  - Quick check question: How does chain-of-thought differ from direct question answering?

- Concept: Retrieval-augmented generation combines external knowledge with language models
  - Why needed here: The framework integrates retrieval augmentation with iterative reasoning to address knowledge limitations
  - Quick check question: Why is retrieval augmentation particularly important for multi-hop questions?

## Architecture Onboarding

- Component map: Question → Tree root → Nodes (paragraphs) → Paragraphs Review Block → Evidence Pool → Reader → Final Answer
- Critical path: Question → Initial retrieval → Node expansion → Review decision → Evidence acceptance → Answer generation
- Design tradeoffs: Tree depth vs breadth (deeper trees find more evidence but increase computational cost), pruning aggressiveness vs search completeness
- Failure signatures: Low recall@15 indicates retrieval issues, low F1 indicates generation problems, excessive API calls suggest inefficient pruning
- First 3 experiments:
  1. Test basic tree construction with simple multi-hop questions and verify node isolation works
  2. Test pruning strategies by comparing retrieval quality with and without pruning
  3. Test evidence fusion methods by comparing generation quality across different fusion strategies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of TOR scale with the size of the retrieval corpus?
- Basis in paper: Inferred
- Why unresolved: The paper evaluates TOR on three multi-hop question answering datasets with Wikipedia as the retrieval source, but it does not investigate how the performance of TOR changes as the size of the retrieval corpus increases. This is an important question because in practical applications, the size of the retrieval corpus can be much larger than the datasets used in the experiments.
- What evidence would resolve it: Experiments comparing the performance of TOR on datasets with varying sizes of retrieval corpora, such as different snapshots of Wikipedia or other knowledge sources.

### Open Question 2
- Question: How does TOR handle questions that require reasoning across multiple documents that are not directly connected in the retrieval corpus?
- Basis in paper: Inferred
- Why unresolved: The paper describes how TOR constructs a tree structure to explore multiple reasoning paths, but it does not explicitly address how TOR handles questions that require reasoning across documents that are not directly connected in the retrieval corpus. This is an important question because in real-world scenarios, relevant information may be scattered across multiple documents that are not directly linked.
- What evidence would resolve it: Experiments evaluating TOR's performance on questions that require reasoning across disconnected documents, and analysis of how TOR constructs reasoning paths in such cases.

### Open Question 3
- Question: How does the choice of base model (GPT-3.5-Turbo vs. GPT-4-Turbo) affect the performance of TOR?
- Basis in paper: Explicit
- Why unresolved: The paper reports results using both GPT-3.5-Turbo and GPT-4-Turbo as base models, but it does not provide a detailed analysis of how the choice of base model affects the performance of TOR. This is an important question because the base model is a key component of TOR, and different models may have different capabilities and limitations.
- What evidence would resolve it: A detailed analysis of how the choice of base model affects various aspects of TOR's performance, such as retrieval quality, reasoning accuracy, and time efficiency.

## Limitations

- Relies on black-box LLM APIs without access to internal reasoning processes, making it difficult to verify why the tree structure specifically outperforms chain-like methods
- Hyperparameter settings for tree depth, width, and pruning thresholds are not explicitly specified, potentially affecting reproducibility
- The few-shot demonstrations used in LLM prompts are not provided, limiting understanding of how the framework achieves dynamic decision-making quality

## Confidence

- **High confidence** in the core architectural claim that tree structures can isolate reasoning paths and reduce cascading errors
- **Medium confidence** in the pruning and expansion optimization strategies, as the paper describes the approaches but lacks detailed ablation studies
- **Medium confidence** in the state-of-the-art claims, as the evaluation focuses on relative improvements over baselines without establishing absolute quality thresholds

## Next Checks

1. **Ablation study on tree depth**: Test performance variations when limiting tree depth to 1-3 levels to quantify the trade-off between search completeness and computational efficiency
2. **Decision-making analysis**: Manually examine a sample of paragraphs review block decisions to verify that the dynamic acceptance/rejection criteria align with human reasoning judgments
3. **Cross-dataset generalization**: Evaluate ToR on a held-out multi-hop dataset not used in the paper to test whether performance gains transfer beyond the three benchmark datasets used