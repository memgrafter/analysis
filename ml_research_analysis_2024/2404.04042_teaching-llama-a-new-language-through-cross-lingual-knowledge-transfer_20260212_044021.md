---
ver: rpa2
title: Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer
arxiv_id: '2404.04042'
source_url: https://arxiv.org/abs/2404.04042
tags:
- task
- translation
- instructions
- language
- estonian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cost-efficient adaptation of large language
  models to low-resource languages, focusing on Estonian. It combines monolingual
  pretraining with cross-lingual instruction-tuning, using both general task instructions
  (Alpaca-est) and translation task instructions.
---

# Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer

## Quick Facts
- arXiv ID: 2404.04042
- Source URL: https://arxiv.org/abs/2404.04042
- Reference count: 27
- First open-source instruction-following conversational LLM for Estonian

## Executive Summary
This paper presents LLAMMAS, the first open-source instruction-following conversational LLM for Estonian, achieved through cost-efficient adaptation of Llama 2. The approach combines monolingual pretraining on Estonian data with cross-lingual instruction-tuning using both general task instructions (Alpaca-est) and translation task instructions. Results demonstrate that even a small amount of monolingual pretraining significantly enhances performance on Estonian tasks, while cross-lingual knowledge transfer from high-quality English instructions improves commonsense reasoning and multi-turn conversation capabilities. The work introduces Alpaca-est, the first general task instruction dataset for Estonian.

## Method Summary
The approach involves continuing pretraining Llama 2-7B on 1-5 billion tokens of Estonian data using packing and batch size of 256, followed by instruction-tuning on combined Alpaca and Alpaca-est datasets with batch size of 128 and learning rate of 2e-5. The model is evaluated on EstQA, EstCOPA, FLORES-200, and EstGEC-L2 using GPT-4 Turbo as evaluator. Cross-lingual knowledge transfer is achieved by supplementing general task instructions with translation task instructions, testing different ratios of EN→ET to ET→EN translation instructions.

## Key Results
- Small amounts of monolingual pretraining (1B tokens) significantly improve Estonian task performance
- Cross-lingual knowledge transfer from English instructions improves commonsense reasoning and multi-turn conversation capabilities
- LLAMMAS achieves competitive zero-shot performance on multiple Estonian tasks
- Combining high-quality English instructions with translation tasks further enhances knowledge transfer

## Why This Works (Mechanism)

### Mechanism 1
Small amount of monolingual pretraining significantly enhances performance on Estonian tasks by providing language-specific knowledge and contextual understanding. Core assumption: pretraining data is representative and diverse enough without overwhelming English capabilities. Evidence: performance gains observed as pretraining dataset size increases. Break condition: insufficient diversity or excessive pretraining causing catastrophic forgetting.

### Mechanism 2
Cross-lingual knowledge transfer from high-quality English instructions improves commonsense reasoning and multi-turn conversation capabilities. Core assumption: English instructions contain transferable reasoning and conversational patterns. Evidence: improvements in Estonian tasks despite no Estonian conversations used during training. Break condition: low-quality English instructions or too different Estonian tasks.

### Mechanism 3
Supplementing general task instructions with high-quality translation task instructions further enhances cross-lingual knowledge transfer. Core assumption: translation instructions provide useful examples of cross-lingual mappings. Evidence: additional performance gains when combining translation tasks. Break condition: low-quality translation instructions or already proficient cross-lingual transfer.

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: Adapts pretrained model to follow natural language instructions in Estonian
  - Quick check question: What is the purpose of instruction tuning in adapting LLMs to new languages?

- Concept: Cross-lingual transfer
  - Why needed here: Leverages knowledge from English instructions to improve Estonian task performance
  - Quick check question: How does cross-lingual transfer help in adapting LLMs to new-resource languages?

- Concept: Monolingual pretraining
  - Why needed here: Provides language-specific knowledge for better processing and generation of Estonian text
  - Quick check question: What is the role of monolingual pretraining in adapting LLMs to new languages?

## Architecture Onboarding

- Component map: Pretrained Llama 2 model -> Monolingual Estonian pretraining data -> Cross-lingual instruction tuning data (general + translation) -> Evaluation datasets
- Critical path: Monolingual pretraining -> Cross-lingual instruction tuning -> Evaluation on Estonian and English tasks
- Design tradeoffs: Balance between pretraining data amount and English capability retention; quality vs. quantity of instruction data
- Failure signatures: Poor Estonian performance indicates insufficient pretraining or ineffective instruction tuning; significant English degradation suggests excessive pretraining
- First 3 experiments:
  1. Fine-tune pretrained Llama 2 on cross-lingual instruction tuning data without monolingual pretraining
  2. Pretrain on 1B tokens of Estonian data, then fine-tune on instruction tuning data
  3. Pretrain on 5B tokens of Estonian data, then fine-tune on instruction tuning data

## Open Questions the Paper Calls Out

### Open Question 1
Optimal amount of monolingual pretraining data needed to maximize cross-lingual transfer without washing out English capabilities. Paper only tests 1B and 5B token amounts. Resolution requires systematic study testing multiple sizes while measuring target language performance and English retention.

### Open Question 2
Quality comparison of generated Estonian instructions vs human-written instructions in terms of model performance and safety. Paper uses synthetic data without quality evaluation. Resolution requires comparison study with human-written dataset.

### Open Question 3
Optimal ratio of EN→ET to ET→EN translation instructions in cross-lingual instruction-tuning. Paper tests three ratios with mixed results. Resolution requires comprehensive study across various ratios and task types.

### Open Question 4
How cross-lingual transfer from English to Estonian differs from transfer to other low-resource languages. Paper focuses exclusively on Estonian. Resolution requires replication with other low-resource languages.

### Open Question 5
Long-term effectiveness of instruction-following models trained on synthetic data in real-world applications. Paper acknowledges synthetic data limitations without real-world testing. Resolution requires longitudinal deployment study.

## Limitations
- Heavy reliance on GPT-4 Turbo for evaluation introduces potential bias
- Unclear pretraining data composition limits generalizability
- Cross-lingual transfer mechanisms not fully explained
- Mixed results for translation instructions across different task types

## Confidence

**High Confidence:** Small amounts of monolingual pretraining improve Estonian task performance (well-supported by experimental results)

**Medium Confidence:** Cross-lingual knowledge transfer improves commonsense reasoning and multi-turn conversation capabilities (improvements shown but evaluation relies on GPT-4 Turbo)

**Low Confidence:** Combining translation tasks with general instructions enhances knowledge transfer (mixed results with unclear explanation for task-specific effectiveness)

## Next Checks

1. Conduct human evaluation verification on a subset of Estonian tasks to validate GPT-4 Turbo evaluation results

2. Test zero-shot transfer performance on closely related languages like Finnish or Latvian

3. Perform detailed ablation study removing translation instructions from different task categories to understand specific impact