---
ver: rpa2
title: 'DGCformer: Deep Graph Clustering Transformer for Multivariate Time Series
  Forecasting'
arxiv_id: '2405.08440'
source_url: https://arxiv.org/abs/2405.08440
tags:
- time
- series
- variables
- clustering
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DGCformer, a novel transformer-based approach
  for multivariate time series forecasting. It addresses the limitations of existing
  channel-dependent (CD) and channel-independent (CI) strategies by combining both
  approaches.
---

# DGCformer: Deep Graph Clustering Transformer for Multivariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2405.08440
- Source URL: https://arxiv.org/abs/2405.08440
- Reference count: 38
- Top-1 performance in 57 out of 64 comparisons against state-of-the-art models

## Executive Summary
This paper introduces DGCformer, a novel transformer-based approach for multivariate time series forecasting that addresses the limitations of existing channel-dependent (CD) and channel-independent (CI) strategies. The method combines both approaches by first grouping variables with significant similarities using a deep graph clustering module that integrates graph convolutional networks (GCN) and autoencoders. It then applies a former-latter masked self-attention mechanism, using the CD strategy within each group and the CI strategy between groups. Experimental results on eight datasets demonstrate the superiority of DGCformer against state-of-the-art models, achieving top-1 performance in 57 out of 64 comparisons across various prediction lengths and evaluation metrics.

## Method Summary
DGCformer employs a two-stage approach for multivariate time series forecasting. First, it applies deep graph clustering using an autoencoder-based reconstruction feature learning (RFL) module and a graph convolutional learning (GCL) module to group highly correlated variables. The clustering loss combines reconstruction MSE and KL divergence between predicted and target distributions. Second, the model uses a former-latter masked self-attention mechanism that processes patches of grouped variables, applying channel-dependent attention within clusters and channel-independent attention across clusters. The final predictions are obtained through a linear projection layer, with the entire model trained end-to-end using a dual-supervised loss function.

## Key Results
- Achieves top-1 performance in 57 out of 64 comparisons against state-of-the-art models
- Demonstrates superiority across eight real-world datasets including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Illness, Electricity, and Exchange
- Outperforms existing methods for long-term forecasting with prediction lengths of 96, 192, 336, 720 (or 24, 36, 48, 60 for Illness dataset)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping variables into clusters improves forecasting by reducing interference from irrelevant variables.
- Mechanism: DGCformer first applies deep graph clustering to group highly correlated variables into the same cluster. Within each cluster, channel-dependent (CD) strategy is used to capture inter-variable dependencies, while channel-independent (CI) strategy is applied across clusters to avoid interference.
- Core assumption: Variables within a cluster have strong correlations that benefit from joint modeling, while variables in different clusters are weakly correlated and should be treated independently.
- Evidence anchors:
  - [abstract] "Specifically, it first groups these relevant variables by a graph convolutional network integrated with an autoencoder, and a former-latter masked self-attention mechanism is then considered with the CD strategy being applied to each group of variables while the CI one for different groups."
  - [section 2] "Considering these factors, CI would be a more reasonable choice to model the features separately, and this is supported by the empirical evidences in Han et al. (2023) and Figure 1."
  - [corpus] Weak evidence - corpus contains related clustering and transformer methods but no direct empirical support for this specific claim.
- Break condition: If clustering incorrectly groups uncorrelated variables, the CD strategy within clusters could introduce noise and degrade performance.

### Mechanism 2
- Claim: The former-latter masked self-attention mechanism effectively captures temporal and cross-variable dependencies within clusters.
- Mechanism: After clustering, the model applies a two-stage attention process: first, multi-head self-attention extracts patch-level temporal correlations within each time series, then masked self-attention captures cross-variable dependencies within the same cluster while respecting the mask.
- Core assumption: The mask correctly enforces that attention only occurs between variables in the same cluster, preserving the intended CI/CD separation.
- Evidence anchors:
  - [section 3.3] "The mask dictates the computation of similarities with other variables... The mask vector M is defined as follows: Mi = Mj = {1 if (Mij ≠ 0), 0 otherwise.}"
  - [section 3.3] "Utilizing self-attention across all N series, the mask dictates the computation of similarities with other variables."
  - [corpus] Weak evidence - corpus contains attention-based methods but no direct evidence for this specific two-stage masked attention mechanism.
- Break condition: If the mask matrix M is incorrectly constructed, attention could leak across clusters, violating the CI strategy assumption.

### Mechanism 3
- Claim: The dual-supervised loss (reconstruction + clustering) effectively guides the graph clustering to produce meaningful variable groups.
- Mechanism: The RFL module learns low-dimensional representations via autoencoder, while the GCL module uses GCN to capture graph structure. The clustering loss combines reconstruction MSE and KL divergence between predicted and target distributions.
- Core assumption: The combination of autoencoder reconstruction and graph-based clustering supervision produces cluster assignments that align with true variable correlations.
- Evidence anchors:
  - [section 3.2] "The clustering loss function in our model is divided into two components: the reconstruction loss and the dual-supervised loss."
  - [section 3.2] "The reconstruction loss is quantified using the Mean Squared Error (MSE) loss function... For the dual-supervised loss, we employ the target distribution... utilizing the Kullback-Leibler (KL) divergence."
  - [corpus] Weak evidence - corpus contains clustering methods but no direct evidence for this specific dual-supervised approach.
- Break condition: If the balance between reconstruction and clustering loss is wrong, the model may overfit to reconstruction or produce uninformative clusters.

## Foundational Learning

- Concept: Graph Convolutional Networks (GCNs)
  - Why needed here: GCNs capture direct relationships among variables by aggregating information from neighboring nodes in the correlation graph, which is essential for identifying variable clusters.
  - Quick check question: How does a GCN layer update node features using the adjacency matrix and degree matrix?

- Concept: Autoencoders for dimensionality reduction
  - Why needed here: Autoencoders compress high-dimensional time series into low-dimensional latent representations that preserve temporal patterns, making clustering more effective.
  - Quick check question: What is the role of the bottleneck layer in an autoencoder, and how does it affect the quality of the latent representation?

- Concept: Masked self-attention
  - Why needed here: Masked self-attention allows the model to attend to relevant variables within clusters while preventing attention across clusters, implementing the CI strategy.
  - Quick check question: How does the mask matrix modify the attention scores, and what happens to attention weights where the mask is zero?

## Architecture Onboarding

- Component map: Input → Normalization & Patching → Deep Graph Clustering (RFL + GCL) → Cluster Assignment → Mask Matrix → Former-Latter Masked Self-Attention → Linear Projection → Output
- Critical path: Graph clustering must complete before attention can be applied, as the mask matrix depends on cluster assignments
- Design tradeoffs: Using GCN adds computational cost but improves cluster quality; simpler clustering methods like k-means are faster but may miss complex variable relationships
- Failure signatures: Poor clustering leads to inappropriate CD/CI application; incorrect mask construction causes attention leakage; imbalance in loss terms affects training stability
- First 3 experiments:
  1. Verify clustering produces meaningful groups by visualizing correlation heatmaps of clustered variables
  2. Test mask matrix construction by checking that attention only occurs within expected clusters
  3. Evaluate the impact of reconstruction vs. clustering loss weights on final forecasting performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DGCformer compare to existing methods when applied to non-stationary time series with significant distribution drift?
- Basis in paper: [inferred] The paper mentions that channel-independent strategies can be more reasonable for handling distribution drift, and DGCformer combines both CD and CI strategies. However, explicit comparison results for non-stationary data are not provided.
- Why unresolved: The experiments focus on datasets with seasonal patterns but do not specifically address non-stationary time series with significant distribution drift.
- What evidence would resolve it: Conducting experiments on synthetic or real-world non-stationary time series datasets with known distribution drift, comparing DGCformer's performance to CD and CI baselines.

### Open Question 2
- Question: What is the impact of varying the number of clusters on DGCformer's performance across different types of time series data?
- Basis in paper: [explicit] The paper mentions a hyperparameter analysis showing the influence of cluster numbers on model performance, but does not provide a comprehensive study across different data types.
- Why unresolved: The experiments use a fixed optimal number of clusters for each dataset, without exploring the full range of possible cluster configurations.
- What evidence would resolve it: Performing a systematic grid search of cluster numbers for each dataset type, analyzing the relationship between cluster count and forecasting accuracy.

### Open Question 3
- Question: How does the proposed deep graph clustering method compare to other clustering techniques specifically designed for time series data?
- Basis in paper: [explicit] The paper mentions replacing components with other clustering methods like DTW clustering in ablation studies, showing superior performance of DGCformer.
- Why unresolved: The comparison is limited to DTW clustering, and a more comprehensive evaluation against other time series clustering techniques is needed.
- What evidence would resolve it: Conducting experiments comparing DGCformer's clustering module to other state-of-the-art time series clustering methods, such as k-Shape or vector autoregressive clustering, across multiple datasets.

## Limitations

- The paper lacks statistical significance testing across the 64 comparisons, reducing confidence in the magnitude of performance improvements.
- Optimal threshold selection for graph adjacency construction and its sensitivity analysis are not thoroughly validated.
- Performance on datasets with very high dimensionality (>100 variables) is not evaluated despite the method's design to handle variable groupings.

## Confidence

- **High confidence**: The mechanism of combining CD and CI strategies through graph clustering is well-founded, as supported by the ablation studies and intuitive reasoning about variable dependencies.
- **Medium confidence**: The experimental results demonstrating superior performance are reliable within the tested datasets, though the lack of statistical testing reduces confidence in the magnitude of improvements.
- **Low confidence**: The optimal threshold selection for graph adjacency construction and the impact of different autoencoder architectures on clustering quality are not well-validated.

## Next Checks

1. **Clustering quality analysis**: Perform t-SNE visualization of clustered variables across multiple datasets to verify that the deep graph clustering produces meaningful groupings that align with known variable correlations.

2. **Statistical significance testing**: Conduct paired t-tests or Wilcoxon signed-rank tests on the forecasting errors across all 64 comparisons to determine if the performance improvements are statistically significant.

3. **Threshold sensitivity analysis**: Systematically vary the threshold τ for graph adjacency construction and measure its impact on both clustering quality (normalized mutual information with ground truth if available) and final forecasting performance to establish robustness to this critical hyperparameter.