---
ver: rpa2
title: 'Q-Adapter: Customizing Pre-trained LLMs to New Preferences with Forgetting
  Mitigation'
arxiv_id: '2407.03856'
source_url: https://arxiv.org/abs/2407.03856
tags:
- reward
- q-adapter
- data
- arxiv
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Q-Adapter addresses the problem of customizing pre-trained LLMs\
  \ to new human preferences while preserving their original knowledge. The method\
  \ casts LLM customization as optimizing the sum of two reward functions\u2014one\
  \ from pre-training and one from new preferences\u2014using a residual Q-learning\
  \ framework."
---

# Q-Adapter: Customizing Pre-trained LLMs to New Preferences with Forgetting Mitigation

## Quick Facts
- arXiv ID: 2407.03856
- Source URL: https://arxiv.org/abs/2407.03856
- Authors: Yi-Chen Li; Fuxiang Zhang; Wenjie Qiu; Lei Yuan; Chengxing Jia; Zongzhang Zhang; Yang Yu; Bo An
- Reference count: 40
- Key outcome: Q-Adapter preserves LLM knowledge while learning new preferences using residual Q-learning, outperforming baselines on retention benchmarks and preference win rates

## Executive Summary
Q-Adapter addresses the challenge of customizing pre-trained LLMs to new human preferences while preserving their original knowledge. The method uses a residual Q-learning framework that bypasses the need to learn either the original or new reward functions, instead directly learning a residual Q-function from preference data. Experiments with Llama-3.1 demonstrate that Q-Adapter effectively maintains capabilities on benchmarks like MMLU and GSM8k while successfully learning new preferences, outperforming traditional SFT and policy regularization approaches.

## Method Summary
Q-Adapter learns a residual Q-function that represents the difference between optimal Q-functions for combined and original rewards, trained directly from preference pairs using the Bradley-Terry model. This approach avoids the need to learn intermediate reward functions while preserving pre-trained knowledge through entropy regularization. The method integrates with LoRA adapters for efficient implementation and uses a temperature parameter α0 to balance between maintaining original capabilities and learning new preferences.

## Key Results
- Q-Adapter maintains MMLU scores within 1-2% of pre-trained baselines while learning new preferences
- Achieves higher LLM-based win rates on preference data compared to SFT, policy regularization, and replay methods
- Successfully customizes Llama-3.1-8B on domain-specific and HH-RLHF datasets without catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
Q-Adapter preserves pre-trained LLM knowledge while learning new preferences by maximizing a combined reward function λr1 + r2 without needing to learn either reward function explicitly. The method learns a residual Q-function that represents the difference between the optimal Q-functions for the combined reward and the original reward. This residual Q-function can be directly learned from preference data using the Bradley-Terry model, bypassing the need to learn either r1 or r2. The pre-trained LLM is assumed to contain sufficient information about r1, and there exists a one-to-one correspondence between the residual Q-function and the new reward function r2.

### Mechanism 2
The Bradley-Terry model enables direct learning of the residual Q-function from preference pairs without learning an intermediate reward function. Preference pairs (preferred vs. unpreferred responses) can be used to train the residual Q-function by treating the difference in Q-values as the log odds of preference, which is exactly what the Bradley-Terry model captures. The model's assumptions about pairwise comparisons are assumed to hold for LLM responses, and the preference data is assumed to be sufficient to learn the relative Q-values.

### Mechanism 3
The entropy regularization in the residual Q-learning framework prevents over-optimization while maintaining the pre-trained knowledge. The soft Q-function formulation with entropy regularization (Haarnoja et al., 2018) is used in both the pre-trained policy and the customized policy, with the temperature parameter α0 controlling the trade-off between following the pre-trained policy and optimizing the new preference. The entropy regularization from the original RLHF training is preserved in the customization process, and α0 can be effectively tuned to balance between r1 and r2.

## Foundational Learning

- **Markov Decision Process (MDP) formalization of language generation**: The entire Q-Adapter approach is built on reinforcement learning principles, which require the language generation task to be formalized as an MDP with states, actions, rewards, and transitions. Quick check: In the token-level MDP formulation, what represents the state at time step t?

- **Bradley-Terry model for preference learning**: This model is used to directly learn the residual Q-function from preference pairs without learning an intermediate reward function, which is a key innovation of Q-Adapter. Quick check: How does the Bradley-Terry model relate the probability of preferring one response over another to their respective Q-values?

- **Soft Q-learning and entropy regularization**: The soft Q-function formulation with entropy regularization is used throughout the approach, both for the pre-trained policy and the customized policy, to prevent over-optimization and maintain exploration. Quick check: What role does the temperature parameter play in the soft Q-function formulation?

## Architecture Onboarding

- **Component map**: Pre-trained LLM (π1*) -> Adapter module (Qθ) -> Preference dataset -> Bradley-Terry loss function -> Inference engine

- **Critical path**: 
  1. Sample preference pairs from dataset
  2. Compute reward differences using residual Q-function
  3. Update adapter parameters via Bradley-Terry loss
  4. At inference: combine pre-trained LLM outputs with adapter outputs using softmax

- **Design tradeoffs**: 
  - Memory vs. performance: Using LoRA adapters reduces memory but may limit adaptation capacity
  - Training speed vs. quality: Direct learning of residual Q-function is faster but may be less stable than learning reward functions first
  - Temperature tuning: α0 must be carefully chosen to balance original knowledge preservation vs. new preference learning

- **Failure signatures**: 
  - If MMLU scores drop significantly during training, the model is forgetting too much
  - If win rates on preference data don't improve, the adapter isn't learning the new preference
  - If training becomes unstable, the Bradley-Terry loss may be too sensitive to noisy preferences

- **First 3 experiments**: 
  1. Train Q-Adapter on a small subset of DSP data and verify that MMLU scores don't drop
  2. Compare win rates against SFT baseline on the same data to confirm preference learning
  3. Vary α0 parameter to find the optimal balance between preserving r1 and learning r2

## Open Questions the Paper Calls Out
1. What is the maximum number of consecutive customizations Q-Adapter can handle before performance degradation becomes significant?
2. How does Q-Adapter perform when customizing models not trained with RLHF but with other methods like SFT or DPO?
3. How sensitive is Q-Adapter to noise and bias in the preference dataset, and what are the failure modes?
4. How does Q-Adapter scale with model size, and are there diminishing returns for larger models?
5. Can Q-Adapter be extended to multi-agent scenarios where multiple specialized LLMs need to be customized collaboratively?

## Limitations
- The approach assumes the pre-trained LLM contains sufficient information about the original reward function r1, which may not hold for all models
- The temperature parameter α0 requires careful tuning and the paper doesn't provide systematic guidance for different scenarios
- The method's effectiveness with preference data that is noisy or insufficient is not fully validated

## Confidence
- **High Confidence**: The core mathematical framework of residual Q-learning and its application to preference-based customization
- **Medium Confidence**: The empirical effectiveness of Q-Adapter in preserving knowledge while learning preferences
- **Low Confidence**: The generalizability of the approach to other domains, model architectures, and the robustness of the temperature parameter tuning strategy

## Next Checks
1. **Ablation Study on Temperature Sensitivity**: Systematically vary α0 across multiple orders of magnitude to identify the stability range and determine if there are multiple valid operating points or if the method is highly sensitive to this parameter.

2. **Knowledge Preservation Across Multiple Customization Rounds**: Apply Q-Adapter sequentially to multiple different preference datasets and measure knowledge retention degradation over time to assess the long-term stability of the anti-forgetting mechanism.

3. **Transferability to Smaller Models**: Validate the approach on smaller LLM variants (7B and below) to determine if the residual Q-function framework scales down effectively or if it's primarily beneficial for larger models due to their representational capacity.