---
ver: rpa2
title: Visually Descriptive Language Model for Vector Graphics Reasoning
arxiv_id: '2404.06479'
source_url: https://arxiv.org/abs/2404.06479
tags:
- visual
- perception
- reasoning
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a key limitation in current large multimodal
  models (LMMs): their struggle to perform precise visual perception of low-level
  geometric details in vector graphics, such as shapes, sizes, and layouts, which
  are essential for tasks like geometric reasoning and maze solving. To address this,
  the authors propose Visually Descriptive Language Model (VDLM), which introduces
  a two-step process: first, converting raster images to SVG format using a rule-based
  encoder to capture fine visual details, and second, translating SVG into a higher-level
  intermediate representation called Primal Visual Description (PVD).'
---

# Visually Descriptive Language Model for Vector Graphics Reasoning

## Quick Facts
- arXiv ID: 2404.06479
- Source URL: https://arxiv.org/abs/2404.06479
- Reference count: 40
- Key outcome: VDLM significantly improves LMMs' low-level visual perception of vector graphics, achieving up to 27 percentage points accuracy gains on tasks like geometric reasoning and maze solving.

## Executive Summary
Current large multimodal models struggle with precise visual perception of low-level geometric details in vector graphics, such as shapes, sizes, and layouts. This paper proposes Visually Descriptive Language Model (VDLM), which converts raster images to SVG format using a rule-based encoder, then translates SVG into a structured text-based representation called Primal Visual Description (PVD). PVD encodes visual primitives with explicit attributes in a format interpretable by LMMs, enabling zero-shot generalization to diverse vector graphics reasoning tasks. Empirical results show VDLM significantly outperforms state-of-the-art LMMs like GPT-4o on vector graphics reasoning tasks, with accuracy gains up to 27 percentage points.

## Method Summary
VDLM employs a two-step process to improve LMMs' visual perception of vector graphics. First, a rule-based encoder (VTracer) converts raster images to SVG format, preserving exact geometric measurements and layout. Second, an LLM-based model translates raw SVG into PVD, a higher-level abstraction comprising primitive geometry objects with explicit attributes like type, position, color, and size. This structured text representation is then used by a strong LMM for zero-shot reasoning. The approach leverages procedurally synthesized ⟨SVG, PVD⟩ pairs for training without human annotation, and the modular design enables disentangled perception and reasoning for improved interpretability.

## Key Results
- VDLM achieves up to 27 percentage points accuracy gains compared to baseline LMMs on vector graphics reasoning tasks
- Strong positive correlation between PVD quality (measured by SSIM, DINOv2, CLIP scores) and downstream task performance
- Zero-shot generalization to diverse tasks including geometric reasoning, maze solving, and shape classification without task-specific annotations
- Improved interpretability through disentangled perception and reasoning stages, enabling targeted debugging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SVG-to-PVD translation improves low-level visual perception by converting noisy SVG into structured abstraction interpretable by LMMs
- Mechanism: Maps raw SVG paths to compact primitive shapes with explicit attributes, filtering noise and standardizing representation
- Core assumption: LMMs can reason effectively about structured textual primitives if representation is clean and aligned with training distribution
- Evidence anchors: [abstract] PVD translates raw SVGs into higher-level abstraction; [section 2.2] PVD is text-based visual description of primitive geometry objects

### Mechanism 2
- Claim: Intermediate PVD representation enables zero-shot generalization to diverse vector graphics reasoning tasks without task-specific annotations
- Mechanism: Task-agnostic PVD built from procedurally synthesized data generalizes to any vector graphics decomposable into supported primitives
- Core assumption: PVD ontology covers sufficient primitives to represent most vector graphics through composition
- Evidence anchors: [abstract] PVD can be learned using task-agnostic synthesized data; [section 2.2] Currently limit to simple compositions of geometric primitives

### Mechanism 3
- Claim: Disentangling perception from reasoning improves interpretability and allows targeted improvements
- Mechanism: Errors can be attributed to perception or reasoning stages, enabling focused debugging without retraining reasoner
- Core assumption: Strong reasoner can operate effectively on imperfect PVD if perception errors are below threshold
- Evidence anchors: [abstract] Improved interpretability due to disentangled perception and reasoning; [section 4.2] Allows in-depth analysis of failure modes

## Foundational Learning

- Concept: SVG (Scalable Vector Graphics) encoding
  - Why needed here: SVG preserves exact geometric measurements and layout, unlike raster images or CLIP features optimized for high-level semantics
  - Quick check question: Can you explain why converting a PNG to SVG might be more useful than feeding it directly to an LMM for length comparison tasks?

- Concept: Procedural data synthesis
  - Why needed here: Enables training SVG-to-PVD without manual annotation by generating ⟨SVG, PVD⟩ pairs from primitive shapes
  - Quick check question: How would you generate a dataset of ⟨SVG, PVD⟩ pairs for training if you had only access to shape-drawing primitives?

- Concept: Disentangled perception-reasoning pipelines
  - Why needed here: Separating visual encoding from language reasoning allows targeted improvements and clearer error attribution
- Quick check question: Why might a monolithic LMM struggle more with low-level visual details than a pipeline that first converts images to structured text?

## Architecture Onboarding

- Component map: Raster Image → VTracer (SVG) → Mistral-7b (PVD) → GPT-4o (Answer)
- Critical path: Image → SVG → PVD → LMM → Answer
  The SVG→PVD step is the bottleneck; if it fails, the reasoner cannot recover
- Design tradeoffs:
  - PVD ontology completeness vs. model simplicity
  - Synthetic data diversity vs. training efficiency
  - Perception accuracy vs. robustness to noise
- Failure signatures:
  - High reconstruction error in PVD → perception bottleneck
  - Correct PVD but wrong answer → reasoning bottleneck
  - Inconsistent answers across similar inputs → model instability
- First 3 experiments:
  1. Verify near-perfect SVG reconstruction from VTracer on vector graphics images
  2. Train SVG→PVD model on synthetic data and measure PVD reconstruction quality (SSIM, DINOv2, CLIP scores)
  3. Run end-to-end VDLM-mm on simple length comparison task and compare to GPT-4o baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does VDLM generalize to 3D vector graphics and natural images beyond 2D primitive shapes currently supported?
- Basis in paper: [inferred] Paper explicitly states SVG and PVD are designed for 2D vector graphics and notes future work needed for 3D structures and natural images
- Why unresolved: Current implementation focuses on 2D primitives; authors acknowledge limitation without experimental results for extension
- What evidence would resolve it: Empirical results on 3D geometric reasoning tasks or natural images, with modifications to PVD ontology and SVG-to-PVD model

### Open Question 2
- Question: What is impact of improving SVG-to-PVD model's perception accuracy on downstream task performance, and what are most effective ways to achieve this?
- Basis in paper: [explicit] Paper shows correlation between PVD quality and task performance, identifies common perception errors, mentions adjusting SVG-to-PVD model or incorporating human-created vector graphics could enhance generalization
- Why unresolved: While correlation is established, paper does not experiment with specific methods to improve perception accuracy and measure resulting impact
- What evidence would resolve it: Experiments comparing VDLM performance using different SVG-to-PVD model variants on same downstream tasks, with quantified improvements in both perception quality metrics and task accuracy

### Open Question 3
- Question: Can VDLM benefit from incorporating stronger or more specialized vision encoders (e.g., DINOv2, SAM) alongside or instead of VTracer for image-to-SVG step?
- Basis in paper: [inferred] Paper uses VTracer and notes near-perfect reconstruction, but discusses related work on mixture-of-experts approaches with various vision encoders to improve fine-grained visual feature preservation
- Why unresolved: Paper does not experiment with alternative or complementary vision encoders for initial image-to-SVG step
- What evidence would resolve it: Comparative experiments replacing or augmenting VTracer with other vision encoders in VDLM pipeline, measuring impact on SVG reconstruction quality and downstream task performance

## Limitations

- PVD ontology limited to simple geometric primitives, may not represent complex vector graphics with non-primitive shapes or arbitrary polygons
- Rule-based SVG encoder (VTracer) may introduce errors when converting complex raster images to SVG, creating cascading failure points
- Synthetic data generation may not capture full diversity of real-world vector graphics, potentially leading to overfitting to synthetic patterns

## Confidence

**High Confidence (4/5):**
- LMMs struggle with low-level visual perception in vector graphics compared to high-level semantic understanding
- Two-stage VDLM architecture improves performance on evaluated tasks
- Improved interpretability through disentangled perception and reasoning stages
- Positive correlation between PVD quality and task performance

**Medium Confidence (3/5):**
- Zero-shot generalization to diverse vector graphics reasoning tasks without task-specific annotations
- SVG-to-PVD translation is primary driver of performance gains
- Modular design enables targeted improvements without retraining reasoner

**Low Confidence (2/5):**
- Approach scales to complex real-world vector graphics beyond simple geometric primitives
- Rule-based SVG encoder performs reliably across diverse input images
- PVD ontology covers sufficient primitives for most practical applications

## Next Checks

1. **PVD Ontology Coverage Test**: Systematically evaluate VDLM on vector graphics containing progressively more complex shapes (polygons, Bézier curves, text) to identify breaking point where PVD representation fails. Measure both reconstruction accuracy and downstream reasoning performance.

2. **Error Attribution Analysis**: Conduct detailed error analysis on representative sample of failed cases, categorizing errors as perception errors (SVG→PVD) versus reasoning errors (PVD→answer). Quantify contribution of each stage to overall failure rate and identify whether perception errors are recoverable by stronger reasoning.

3. **Real-world Data Generalization**: Test VDLM on dataset of real-world vector graphics (scientific diagrams, technical drawings, UI mockups) rather than synthetic data. Measure performance degradation and identify whether synthetic training data adequately prepares model for practical deployment.