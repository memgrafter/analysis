---
ver: rpa2
title: Non-asymptotic Convergence of Training Transformers for Next-token Prediction
arxiv_id: '2409.17335'
source_url: https://arxiv.org/abs/2409.17335
tags:
- training
- token
- tokens
- have
- lmax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the non-asymptotic convergence of transformer
  training for next-token prediction. The authors introduce a mathematical framework
  using partial orders to characterize realizable training datasets, where loss can
  be minimized to near zero.
---

# Non-asymptotic Convergence of Training Transformers for Next-token Prediction

## Quick Facts
- arXiv ID: 2409.17335
- Source URL: https://arxiv.org/abs/2409.17335
- Authors: Ruiquan Huang; Yingbin Liang; Jing Yang
- Reference count: 40
- One-line primary result: This paper analyzes the non-asymptotic convergence of transformer training for next-token prediction using a two-stage normalized gradient descent algorithm.

## Executive Summary
This paper presents a theoretical analysis of transformer training for next-token prediction tasks, introducing a mathematical framework based on partial orders to characterize realizable training datasets. The authors propose a two-stage training algorithm using normalized gradient descent that enables convergence to max-margin solutions for both the feed-forward and self-attention layers. The analysis shows sublinear convergence in direction for the parameters with a linear convergence rate for the cross-entropy loss, while also demonstrating the model's ability to generalize to unseen data through learned partial order structures.

## Method Summary
The paper analyzes next-token prediction using a one-layer transformer with self-attention and feed-forward layers. The training dataset is assumed to be "realizable" with collocation and query-dependent partial orders. The authors design a two-stage training algorithm: Stage 1 trains the feed-forward layer Wov on collocation data to classify next tokens from all others, while Stage 2 trains the self-attention layer Wkq on the full dataset to classify optimal tokens from non-optimal ones with Wov fixed. Both stages use normalized gradient descent, with theoretical guarantees showing convergence to max-margin solutions and linear convergence rate for the cross-entropy loss.

## Key Results
- The two-stage normalized gradient descent algorithm achieves sublinear convergence in direction to max-margin solutions for both Wov and Wkq layers
- The cross-entropy loss enjoys a linear convergence rate due to the linear growth of parameter norms during training
- The trained transformer demonstrates generalization ability by incorporating non-comparable tokens between optimal and non-optimal tokens in the learned partial order

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage normalized gradient descent training algorithm enables the transformer to converge in direction to max-margin solutions for both the feed-forward and self-attention layers.
- Mechanism: Stage 1 trains the feed-forward layer Wov to classify the next token from all other tokens using collocation data, yielding a max-margin solution W*ov. Stage 2 then trains the self-attention layer Wkq to classify optimal tokens from non-optimal tokens using the entire dataset. The linear growth of parameter norms enables a linear convergence rate for the cross-entropy loss.
- Core assumption: The training dataset satisfies Assumption 1 (realizable with collocation and query-dependent partial orders) and Assumption 2 (orthonormal vocabulary).
- Evidence anchors:
  - [abstract] "We design a two-stage training algorithm using normalized gradient descent: the first stage trains the feed-forward layer to classify next tokens from all others, and the second stage trains the self-attention layer to classify optimal tokens from non-optimal ones."
  - [section 4.2] "In the first stage of pre-processing, we use the collocation set to train the feed-forward layer Wov... In the second stage, we fix the trained feed-forward layer and train the self-attention layer based on the loss function given in Equation (1) and using the entire dataset D0."
- Break condition: If the dataset lacks the collocation structure or query-dependent partial orders, the max-margin solutions cannot be defined and the algorithm may not converge properly.

### Mechanism 2
- Claim: The trained transformer demonstrates generalization ability on unseen data by incorporating non-comparable tokens between optimal and non-optimal tokens in the learned partial order.
- Mechanism: The key-query matrix W*kq learned during training satisfies specific properties where non-comparable tokens have zero projection onto W*kq. During inference, the trained Wkq inserts these non-comparable tokens between optimal and non-optimal tokens, allowing the model to make sensible predictions even when optimal tokens are absent.
- Core assumption: The realizable dataset has no confused tokens (Assumption 1) and the vocabulary is orthonormal (Assumption 2).
- Evidence anchors:
  - [abstract] "the trained transformer demonstrates generalization ability on unseen data by incorporating non-comparable tokens between optimal and non-optimal tokens in the learned partial order."
  - [section 6] "Theorem 3 suggests that a new partial order is created by the trained transformer. Specifically, it inserts the non-comparable tokens between the optimal and non-optimal tokens."
- Break condition: If the dataset contains confused tokens (violating Assumption 1), the partial order structure breaks down and the generalization mechanism fails.

### Mechanism 3
- Claim: The linear growth of parameter norms during training enables a linear convergence rate for the cross-entropy loss.
- Mechanism: As the normalized gradient descent updates progress, both Wov and Wkq norms grow linearly with iteration count. This linear growth, combined with sublinear convergence in direction to the max-margin solutions, yields a linear convergence rate for the loss function L(θ).
- Core assumption: The learning rates η0 and η are chosen appropriately (η < O(1) for Wkq).
- Evidence anchors:
  - [abstract] "the cross-entropy loss enjoys a linear convergence rate"
  - [section 5.1] "Proposition 1 states that during the training stage 1, the feed-forward layer W(t)ov converges in direction to W*ov/∥W*ov∥ at a rate of O(log t/t), which classifies the next token from all other tokens. In addition, since the norm of W(t)ov increases linearly, the loss L0(W(t)ov) converges linearly to zero"
- Break condition: If the learning rates are too large, parameter norms may grow too quickly, causing instability. If too small, convergence becomes impractically slow.

## Foundational Learning

- Concept: Partial orders and their role in characterizing training data structure
  - Why needed here: The paper uses partial orders to formally define the "realizable" property of training datasets, which enables the max-margin problem formulation and subsequent convergence analysis
  - Quick check question: What are the three types of tokens defined by a query-dependent partial order, and how do they differ in their relationships?

- Concept: Max-margin classification and its connection to gradient descent convergence
  - Why needed here: The paper shows that both Wov and Wkq converge to max-margin solutions, which requires understanding how gradient descent implicitly biases toward these solutions
  - Quick check question: How does the normalized gradient descent update rule contribute to the convergence toward max-margin solutions?

- Concept: Cross-entropy loss and its relationship to classification accuracy
  - Why needed here: The paper analyzes the convergence of the cross-entropy loss and connects it to the attention weights and prediction accuracy of the trained transformer
  - Quick check question: Why does the linear growth of parameter norms enable a linear convergence rate for the cross-entropy loss?

## Architecture Onboarding

- Component map: Input sequence X = [x1, ..., xL] -> Self-attention layer (Wkq, Wov) -> Feed-forward layer (Wov) -> Probability distribution over vocabulary

- Critical path:
  1. Initialize Wov = 0, Wkq = 0
  2. Stage 1: Train Wov using collocation data until convergence
  3. Stage 2: Fix Wov, train Wkq using full dataset until convergence
  4. Inference: Use trained model to predict next tokens

- Design tradeoffs:
  - Two-stage vs joint training: Two-stage decouples problems but may miss interactions
  - Normalized vs standard gradient descent: Normalized enables linear norm growth but may converge slower initially
  - Single-layer vs multi-layer: Single-layer enables theoretical analysis but may limit expressivity

- Failure signatures:
  - Loss plateaus or increases: Check learning rates, data quality, or initialization
  - Attention weights become uniform: Check if Wkq is converging properly
  - Poor generalization on unseen data: Check if non-comparable tokens are being handled correctly

- First 3 experiments:
  1. Verify collocation extraction: Check that all length-2 sentences are correctly identified and used in Stage 1
  2. Monitor norm growth: Plot ∥Wov(t)∥ and ∥Wkq(t)∥ over iterations to confirm linear growth
  3. Test partial order creation: Verify that query-dependent partial orders are correctly constructed from the training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise generalization bounds for transformers trained on next-token prediction tasks when faced with complex dataset shifts?
- Basis in paper: Explicit - The paper discusses generalization ability for unseen data by incorporating non-comparable tokens between optimal and non-optimal tokens in the learned partial order.
- Why unresolved: The paper provides theoretical insights and a specific example, but lacks a comprehensive generalization bound that accounts for complex dataset shifts.
- What evidence would resolve it: Rigorous mathematical proofs demonstrating generalization bounds under various dataset shifts, including those with more complex relationships than the provided example.

### Open Question 2
- Question: How does the performance of the two-stage training algorithm scale with larger vocabularies and more complex token relationships?
- Basis in paper: Inferred - The paper's theoretical analysis is based on a simplified setting with a finite vocabulary and specific token relationships. It is unclear how these findings extend to more complex scenarios.
- Why unresolved: The analysis focuses on a one-layer transformer with a simplified setup, and the paper does not provide empirical results for larger or more complex settings.
- What evidence would resolve it: Experimental results showing the performance of the two-stage algorithm on datasets with larger vocabularies and more intricate token relationships, along with corresponding theoretical analysis.

### Open Question 3
- Question: What is the impact of using multi-head attention on the convergence and generalization of transformers for next-token prediction?
- Basis in paper: Inferred - The paper focuses on a single-layer transformer with a single attention head. The impact of multi-head attention, which is commonly used in practice, is not explored.
- Why unresolved: The theoretical analysis is limited to single-head attention, and the paper does not provide insights into how multi-head attention would affect the training dynamics and generalization.
- What evidence would resolve it: Comparative analysis of single-head and multi-head attention transformers, including convergence rates and generalization performance, along with theoretical explanations for any observed differences.

## Limitations
- The theoretical analysis relies heavily on restrictive "realizability" assumptions about training data structure that may not hold for natural language
- The experimental validation is limited to synthetic data with only 20 vocabulary tokens, lacking scale and realism
- The convergence analysis focuses on a simplified one-layer transformer architecture, with unclear extension to deeper models

## Confidence

**High Confidence**: The mathematical proofs for convergence in direction to max-margin solutions appear rigorous given the stated assumptions. The connection between linear parameter norm growth and linear convergence rate for the cross-entropy loss is well-established theoretically.

**Medium Confidence**: The mechanism by which non-comparable tokens are inserted between optimal and non-optimal tokens during inference is theoretically sound but requires empirical validation on more realistic datasets.

**Low Confidence**: The practical applicability of the two-stage training algorithm to real-world next-token prediction tasks remains uncertain. The synthetic dataset experiments demonstrate the theoretical framework works in idealized settings but don't establish performance on actual language data.

## Next Checks
1. **Dataset Realizability Test**: Generate and analyze a real-world text corpus (e.g., Wikipedia) to quantify the extent to which natural language data satisfies the collocation and query-dependent partial order assumptions. Measure the percentage of token sequences that can be classified as optimal, non-optimal, or confused.

2. **Algorithm Robustness Analysis**: Implement the two-stage training algorithm with standard (non-normalized) gradient descent and evaluate whether similar convergence properties hold. Compare performance against the normalized version on both synthetic and real datasets to assess practical benefits.

3. **Scaling Experiment**: Scale up the synthetic experiments to larger vocabulary sizes (100-1000 tokens) and longer sequences to test whether the theoretical convergence rates and generalization mechanisms hold as problem complexity increases. Measure how training dynamics change with scale.