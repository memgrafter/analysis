---
ver: rpa2
title: '360 in the Wild: Dataset for Depth Prediction and View Synthesis'
arxiv_id: '2406.18898'
source_url: https://arxiv.org/abs/2406.18898
tags:
- depth
- dataset
- images
- image
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the first large-scale in-the-wild 360\xB0\
  \ video dataset with ground-truth depth maps and camera poses. The dataset contains\
  \ 25,000 images from 273 YouTube videos, covering diverse indoor, outdoor, and mannequin\
  \ challenge scenes."
---

# 360 in the Wild: Dataset for Depth Prediction and View Synthesis

## Quick Facts
- arXiv ID: 2406.18898
- Source URL: https://arxiv.org/abs/2406.18898
- Authors: Kibaek Park, Francois Rameau, Jaesik Park, In So Kweon
- Reference count: 40
- Key outcome: First large-scale in-the-wild 360° video dataset with ground-truth depth maps and camera poses, containing 25,000 images from 273 YouTube videos.

## Executive Summary
This paper introduces the first large-scale dataset for depth prediction and novel view synthesis using in-the-wild 360° videos. The dataset contains 25,000 images from diverse indoor, outdoor, and mannequin challenge scenes, with ground-truth depth maps and camera poses obtained through a two-stage pipeline combining OpenSfM and COLMAP. The authors evaluate their dataset on single image depth estimation (fine-tuning MiDaS) and novel view synthesis (extending NeRF++ for 360° images), demonstrating the dataset's utility for both tasks.

## Method Summary
The dataset is constructed by collecting 360° videos from YouTube and extracting frames, which are then converted to cube maps for processing with COLMAP to generate dense depth maps. Camera poses are estimated using OpenSfM. The authors fine-tune MiDaS on their dataset for depth estimation and extend NeRF++ to handle 360° images for novel view synthesis by adapting the ray computation to spherical coordinates. Moving-object masks are provided to remove dynamic elements from video sequences.

## Key Results
- Fine-tuned MiDaS achieves δ < 1.25 scores of 0.716-0.701 and AbsRel of 0.544-0.557 on the dataset
- NeRF++ extension for 360° images achieves PSNR of 14.46-26.57, SSIM of 0.497-0.763, and LPIPS of 0.267-0.442 across different scene categories
- Dataset covers diverse indoor, outdoor, and mannequin challenge scenes with 25,000 images from 273 YouTube videos

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality pose and depth labels are obtained by combining OpenSfM and COLMAP pipelines, which together address the limitations of each method when handling 360° imagery.
- Mechanism: OpenSfM performs sparse structure-from-motion to estimate camera poses, while COLMAP (adapted via cube-map projection) generates dense depth maps. This two-stage pipeline leverages OpenSfM's robustness to wide baselines and COLMAP's accuracy in dense reconstruction.
- Core assumption: Converting 360° images to cube maps preserves sufficient geometric information for COLMAP to reconstruct accurate depth, and the two-stage process yields consistent pose-depth pairs.
- Evidence anchors: [abstract] "Depth maps and camera poses are obtained using OpenSfM and COLMAP."; [section] "we utilize COLMAP [51] which is known for its accuracy [31]. However, COLMAP is not compatible with 360° images, therefore, we transform the panoramic images into cube maps"
- Break condition: If cube-map projection introduces geometric distortions that COLMAP cannot compensate, or if pose estimation fails for wide-baseline sequences, the ground-truth quality degrades.

### Mechanism 2
- Claim: Training NeRF++ on 360° data with the extended spherical ray computation enables view synthesis for full panoramic scenes, including previously unsupported regions like the poles.
- Mechanism: The extension computes rays directly from equirectangular pixel coordinates to unit sphere directions, bypassing perspective camera calibration. Combined with NeRF++'s dual-volume strategy, this allows rendering of unbounded 360° environments.
- Core assumption: Equirectangular projection provides a one-to-one mapping to spherical coordinates suitable for NeRF's volumetric integration, and NeRF++'s dual-volume sampling handles the full sphere without singularities.
- Evidence anchors: [abstract] "we adapted NeRF++ to handle 360° images"; [section] "we introduce an extension for handling 360° images... the rays have to be computed in compliance with the geometric model of 360° images"
- Break condition: If the equirectangular-to-sphere mapping introduces sampling bias near poles or if NeRF++'s unbounded sampling fails to capture distant geometry accurately.

### Mechanism 3
- Claim: The dataset's diversity (indoor/outdoor/mannequin) and inclusion of moving-object masks enable training models that generalize across scene types and handle dynamic elements.
- Mechanism: By providing masked sequences and diverse environmental categories, the dataset allows self-supervised methods to learn static scene geometry while excluding transient objects, and supervised methods to see varied depth distributions.
- Core assumption: Moving-object masks effectively remove dynamic elements without corrupting static geometry, and the category splits ensure balanced representation of depth distributions.
- Evidence anchors: [abstract] "The dataset contains 25,000 images from 273 YouTube videos, covering diverse indoor, outdoor, and mannequin challenge scenes."; [section] "we provide binary masks to remove moving objects from the video sequences"
- Break condition: If masks are inaccurate or incomplete, or if category imbalance skews learned depth distributions, model performance on unseen scenes suffers.

## Foundational Learning

- Concept: Equirectangular projection and spherical coordinates
  - Why needed here: Understanding how 360° images map to 3D directions is essential for implementing the ray computation extension and interpreting depth maps.
  - Quick check question: Given an equirectangular pixel at (x, y) in an image of width W and height H, what are its spherical angles θ and φ?

- Concept: Structure-from-motion and multi-view stereo pipelines
  - Why needed here: The dataset's ground-truth generation relies on SfM for poses and MVS for depth; knowing their limitations helps diagnose reconstruction errors.
  - Quick check question: What is the main difference between SfM and MVS in terms of the geometric information they produce?

- Concept: Neural radiance fields and volumetric rendering
  - Why needed here: NeRF++ is the baseline for view synthesis; understanding its sampling strategy and volume parameterization is key to extending it to 360°.
  - Quick check question: In NeRF, how are 3D points sampled along a camera ray, and what role do the near and far bounds play?

## Architecture Onboarding

- Component map: YouTube videos → Frame extraction → Cube-map conversion → OpenSfM (poses) + COLMAP (depth) → Masks → MiDaS fine-tuning / NeRF++ extension → Evaluation
- Critical path: Cube-map conversion → COLMAP depth generation → NeRF++ training (spherical rays + dual volume) → Evaluation
- Design tradeoffs: Cube-map projection trades off geometric fidelity for COLMAP compatibility; dual-volume NeRF++ increases complexity but enables unbounded 360° rendering; moving-object masks simplify training but may remove useful dynamic cues.
- Failure signatures: Poor depth quality indicates cube-map distortion or SfM failure; view synthesis artifacts near poles suggest ray computation errors; low generalization implies category imbalance or mask issues.
- First 3 experiments:
  1. Validate cube-map conversion by comparing COLMAP depth on a synthetic 360° scene with known ground truth.
  2. Test NeRF++ extension on a simple synthetic 360° environment (e.g., a sphere) to confirm ray computation correctness.
  3. Train MiDaS on a small balanced subset of the dataset and evaluate depth accuracy to confirm mask and category split quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scale ambiguity in SfM-derived depth maps affect downstream tasks like depth estimation and novel view synthesis, and can this be mitigated through dataset design or network architecture?
- Basis in paper: [explicit] The paper mentions that ground-truth depth from internet videos has scale ambiguity and that MiDaS uses scale and shift-invariant loss to cope with this issue.
- Why unresolved: While the paper demonstrates that depth estimation networks can work with scale-ambiguous data, it doesn't fully explore the limitations this introduces or potential solutions.
- What evidence would resolve it: Systematic experiments comparing performance with metric vs. scale-ambiguous depth labels, and ablation studies testing different normalization strategies.

### Open Question 2
- Question: Can the NeRF++ extension for 360° images be further improved to handle dynamic scenes with moving objects, beyond just masking them out?
- Basis in paper: [explicit] The paper notes that OpenSfM is sensitive to moving objects and that the authors manually mask them out, but their NeRF++ extension can implicitly learn scene structure from masked regions.
- Why unresolved: The current approach treats moving objects as a nuisance to be removed, rather than a phenomenon to be modeled, limiting applications in dynamic environments.
- What evidence would resolve it: Experiments comparing performance on videos with varying amounts of motion, and testing whether the model can distinguish between static and dynamic elements.

### Open Question 3
- Question: What is the optimal balance between dataset size and diversity for training generalizable depth estimation and novel view synthesis models on 360° in the wild data?
- Basis in paper: [inferred] The paper presents a large dataset but doesn't systematically analyze how much data is needed or whether certain types of diversity are more important than others.
- Why unresolved: While the dataset is large, the paper doesn't explore diminishing returns or identify which aspects of diversity (environments, camera motion, etc.) matter most.
- What evidence would resolve it: Controlled experiments training models on subsets of the data with different levels of diversity, measuring generalization to held-out scenes.

## Limitations

- Ground-truth quality depends on the robustness of OpenSfM and COLMAP for wide-baseline 360° scenes, with no independent validation provided.
- The 25,000 images from 273 videos may not capture the full diversity of real-world 360° environments, potentially limiting generalizability.
- The paper doesn't compare against alternative approaches for 360° depth estimation or view synthesis, making it difficult to assess the relative effectiveness of the proposed adaptations.

## Confidence

- **High confidence**: The dataset collection methodology and basic evaluation metrics are sound. The core pipeline of converting 360° to cube maps for COLMAP processing is technically feasible.
- **Medium confidence**: The effectiveness of moving-object masks and category splits for enabling generalization across diverse scenes is plausible but not empirically validated. The adaptation of NeRF++ to 360° via spherical ray computation is conceptually sound but lacks rigorous testing.
- **Low confidence**: The quality of ground-truth depth maps and poses generated through the two-stage OpenSfM + COLMAP pipeline is uncertain without independent validation. The dataset's coverage of real-world 360° scenarios is claimed but not quantitatively verified.

## Next Checks

1. **Ground-truth accuracy validation**: Compare the generated depth maps and poses against a small subset of scenes with independent ground truth (e.g., synthetic 360° environments or scenes with LiDAR scans) to quantify reconstruction accuracy.

2. **Category balance analysis**: Perform a statistical analysis of depth distributions and scene complexity across the dataset's categories to confirm balanced representation and identify potential biases.

3. **Baseline robustness test**: Train MiDaS and NeRF++ extensions on increasingly small subsets of the dataset to determine the minimum required data volume for effective learning and assess the method's data efficiency.