---
ver: rpa2
title: 'Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor
  Attacks'
arxiv_id: '2407.10825'
source_url: https://arxiv.org/abs/2407.10825
tags:
- data
- backdoor
- samples
- attacks
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies a novel backdoor threat model where the attacker
  has access only to data from the target class, without knowledge of the victim model
  or other classes. The authors propose two novel sample selection strategies to enhance
  clean-label backdoor attacks in this constrained setting.
---

# Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor Attacks

## Quick Facts
- arXiv ID: 2407.10825
- Source URL: https://arxiv.org/abs/2407.10825
- Reference count: 40
- Primary result: Proposed sample selection strategies improve clean-label backdoor attack success rates by 20-40% compared to random selection

## Executive Summary
This paper addresses the challenge of clean-label backdoor attacks under a highly constrained threat model where the attacker only has access to data from the target class, without knowledge of the victim model or other classes. The authors propose two novel sample selection strategies to enhance attack effectiveness: using pretrained models to identify hard-to-learn samples in the target class, and employing out-of-distribution data to train a surrogate model for sample selection. Experimental results on CIFAR-10 and GTSRB datasets demonstrate that both strategies significantly improve attack success rates while maintaining clean data performance and evading backdoor defenses.

## Method Summary
The paper proposes two strategies for sample selection in clean-label backdoor attacks under a constrained threat model. The first strategy uses pretrained feature extractors (like VICReg or ResNet50) to map target class samples to a feature space where hard-to-learn samples are identified as those far from the cluster center based on k-NN distance scores. The second strategy employs out-of-distribution data combined with target class data to train a surrogate model, using loss values to identify hard samples. Both strategies then poison the selected samples with trigger patterns and train the victim model on the poisoned dataset.

## Key Results
- Both proposed sample selection strategies achieve 20-40% higher attack success rates compared to random selection baselines
- The methods maintain clean data performance while evading several backdoor defenses
- Even under extreme distribution shifts, the OOD-based strategy remains effective
- The pretrained model strategy works well when distribution shift is moderate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hard-to-learn samples in the target class contribute more effectively to backdoor success when poisoned.
- Mechanism: When triggers are inserted into samples that the model finds difficult to learn, the model is forced to rely on the trigger as a shortcut to minimize the loss function, leading to higher attack success rates.
- Core assumption: Samples that are hard to learn are those that are far from other samples in the feature space of the target class.
- Evidence anchors:
  - [abstract] "The first strategy uses pretrained models to identify hard-to-learn samples in the target class"
  - [section 4.2] "When the adversary injects a trigger into these 'hard samples' and alters their labels, the model cannot rely on existing features in the image to optimize the objective function, instead it favors backdoor features"
- Break condition: If the feature space does not meaningfully separate hard and easy samples, or if the model can learn the trigger without relying on it as a shortcut.

### Mechanism 2
- Claim: Pretrained models can effectively identify hard-to-learn samples in the target class without requiring training on the full dataset.
- Mechanism: A pretrained feature extractor maps samples to a feature space where samples from the same class cluster together. Samples far from the cluster center (high k-NN score) are considered hard to learn and are selected for poisoning.
- Core assumption: The feature space from a pretrained model preserves the relative difficulty of learning samples within a class.
- Evidence anchors:
  - [section 4.3] "samples that are far from the target label cluster contain different features, thus harder for the model to learn"
  - [section 4.3] "By exploiting pre-trained models to extract features, datapoints from the same class stay close to each other in the feature space"
- Break condition: If the pretrained model's feature space is not discriminative for the target class, or if the distribution shift is too large.

### Mechanism 3
- Claim: Out-of-distribution (OOD) data can be used to train a surrogate model to identify hard samples in the target class.
- Mechanism: The OOD data is combined with the target class data to create a dataset for training a surrogate model. The loss values of the target class samples in this surrogate model are used to identify hard samples.
- Core assumption: The surrogate model trained on OOD + target class data can still identify hard samples in the target class.
- Evidence anchors:
  - [section 4.4] "we utilize the loss values of samples from the target class to select the hard samples accordingly"
  - [section 4.4] "The OOD dataset may display different characteristics, or even come from a different domain compared to the final training data"
- Break condition: If the OOD data is too different from the target class data, the surrogate model may not be able to identify hard samples effectively.

## Foundational Learning

- Concept: Feature extraction and representation learning
  - Why needed here: Understanding how pretrained models map inputs to feature spaces is crucial for identifying hard samples based on their distance from class clusters.
  - Quick check question: What is the purpose of using a pretrained model in the proposed attack strategies?

- Concept: k-Nearest Neighbors (k-NN) algorithm
  - Why needed here: k-NN is used to calculate a score for each sample based on its distance to its k-nearest neighbors in the feature space, which is used to identify hard samples.
  - Quick check question: How does the k-NN score relate to the difficulty of learning a sample?

- Concept: Out-of-distribution (OOD) data and domain adaptation
  - Why needed here: Understanding how OOD data can be used to train a surrogate model for sample selection is important for the second proposed strategy.
  - Quick check question: Why is it important to consider the distribution shift between the OOD data and the target class data?

## Architecture Onboarding

- Component map: Feature extractor -> Sample selection module -> Trigger injection module -> Victim model training pipeline

- Critical path:
  1. Extract features from target class data using a pretrained model or train a surrogate model on OOD + target class data.
  2. Identify hard samples based on feature distances or loss values.
  3. Inject triggers into the selected hard samples.
  4. Train the victim model on the poisoned dataset.

- Design tradeoffs:
  - Pretrained model vs. OOD strategy: Pretrained models are easier to use but may be less effective with large distribution shifts. OOD strategy requires more data but can be more effective in certain scenarios.
  - Number of neighbors (k) in k-NN: Smaller k values may be more effective at identifying local outliers, but larger k values may provide a more stable score.

- Failure signatures:
  - Low attack success rate despite high poisoning rate.
  - High clean accuracy but low backdoor activation.
  - Failure to bypass backdoor defenses.

- First 3 experiments:
  1. Test the effectiveness of the pretrained model strategy on a simple dataset with known hard samples.
  2. Compare the performance of different values of k in the k-NN algorithm.
  3. Evaluate the impact of distribution shift on the OOD strategy by using OOD data from different domains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed sample selection strategy perform under different data augmentation techniques applied at the victim site?
- Basis in paper: [inferred] The paper mentions that "different augmentations can be potentially employed to build the pretrained model or train the surrogate OOD model, and at the victim site, further aggravating the distribution shifts."
- Why unresolved: The paper does not provide experimental results or analysis on how data augmentation techniques affect the performance of the proposed methods.
- What evidence would resolve it: Conducting experiments with various data augmentation techniques applied at the victim site and comparing the attack success rates would provide insights into the robustness of the proposed methods.

### Open Question 2
- Question: Can the proposed sample selection strategies be combined with other types of backdoor attacks beyond the clean-label setting?
- Basis in paper: [inferred] The paper focuses on clean-label backdoor attacks and mentions that the proposed strategies can be used with any existing triggers in the clean-label setting. It also states that "It would be interesting to study the combination of our strategies and the attack that perturbs the input to make it difficult to learn [42]."
- Why unresolved: The paper does not provide experimental results or analysis on combining the proposed strategies with other types of backdoor attacks.
- What evidence would resolve it: Conducting experiments to combine the proposed sample selection strategies with different types of backdoor attacks (e.g., dirty-label attacks, dynamic backdoors) and evaluating their effectiveness would provide insights into the generalizability of the proposed methods.

### Open Question 3
- Question: How do the proposed methods perform in scenarios where the attacker has limited access to the target class data?
- Basis in paper: [explicit] The paper mentions that "we study the effectiveness of our method under the setting where the attacker partially accesses the target classâ€™s data" and provides results for 20%, 50%, and 100% access to the target class data.
- Why unresolved: While the paper provides some results for partial access to the target class data, it does not explore the lower bounds of data access required for effective attacks or the trade-offs between data access and attack success rates.
- What evidence would resolve it: Conducting experiments with varying levels of data access to the target class (e.g., 5%, 10%, 15%) and analyzing the relationship between data access and attack success rates would provide insights into the minimum data requirements for effective attacks.

## Limitations
- The effectiveness of sample selection strategies may vary significantly depending on the specific model architecture and domain similarity between pretraining and target data.
- The proposed methods require access to pretrained feature extractors or OOD data, which may not always be available in real-world scenarios.
- The paper only evaluates the proposed methods on CIFAR-10 and GTSRB datasets, limiting generalizability to other domains and tasks.

## Confidence
- **High confidence**: The core observation that hard-to-learn samples improve backdoor effectiveness is well-supported by the experimental evidence and aligns with established principles in adversarial machine learning.
- **Medium confidence**: The effectiveness of pretrained models for sample selection is demonstrated but may vary significantly depending on the specific model architecture and domain similarity between pretraining and target data.
- **Medium confidence**: The OOD-based surrogate model approach shows promise but requires careful selection of OOD data to ensure meaningful loss-based sample selection.

## Next Checks
1. Test the sample selection strategies across diverse model architectures (CNNs, transformers, ViTs) to evaluate robustness beyond the current experimental setup.
2. Conduct systematic ablation studies varying the amount of OOD data and distribution shift to quantify the limits of the surrogate model approach.
3. Evaluate the transferability of poisoned samples across different victim model architectures to assess the attack's practical effectiveness in real-world scenarios.