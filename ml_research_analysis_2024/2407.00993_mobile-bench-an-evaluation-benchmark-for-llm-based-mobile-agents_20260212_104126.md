---
ver: rpa2
title: 'Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents'
arxiv_id: '2407.00993'
source_url: https://arxiv.org/abs/2407.00993
tags:
- android
- task
- action
- package
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mobile-Bench, a novel evaluation benchmark
  for LLM-based mobile agents that addresses key limitations in current mobile agent
  testing. The benchmark expands conventional UI operations by incorporating 103 APIs
  to improve task efficiency, collects evaluation data combining real user queries
  with LLM augmentation, and categorizes tasks into three complexity levels (SAST,
  SAMT, MAMT) to evaluate different planning capabilities.
---

# Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents

## Quick Facts
- arXiv ID: 2407.00993
- Source URL: https://arxiv.org/abs/2407.00993
- Reference count: 26
- Primary result: Introduces Mobile-Bench benchmark with 832 tasks across 29 apps using 103 APIs, showing GPT-3.5 outperforms GPT-4 in some metrics while all models struggle with complex multi-app tasks

## Executive Summary
This paper introduces Mobile-Bench, a novel evaluation benchmark for LLM-based mobile agents that addresses key limitations in current mobile agent testing. The benchmark expands conventional UI operations by incorporating 103 APIs to improve task efficiency, collects evaluation data combining real user queries with LLM augmentation, and categorizes tasks into three complexity levels (SAST, SAMT, MAMT) to evaluate different planning capabilities. Experimental results show that while GPT-3.5 outperforms GPT-4 in some metrics, all models struggle with complex multi-APP tasks, particularly in determining when to exit one application and transition to another.

## Method Summary
Mobile-Bench is an evaluation platform for LLM-based mobile agents that combines UI interactions with programmatic API calls through Android Debug Bridge (ADB). The system uses an Android emulator with Appium for UI operations and processes task instructions through LLMs to generate execution plans. Agents can choose between API calls or UI interactions based on task requirements, with the platform providing HTML-formatted UI observations for LLM processing. The evaluation uses a novel CheckPoint metric that assesses whether agents reach essential intermediate points during task execution, rather than just measuring final success rates.

## Key Results
- Mobile-Bench includes 832 data entries with over 200 multi-APP collaboration tasks across 29 applications
- GPT-3.5 outperforms GPT-4 in some metrics, but all models struggle with MAMT tasks
- API integration significantly improves task completion efficiency by replacing multiple UI steps with single API calls
- CheckPoint metric provides more granular evaluation than traditional PassRate by assessing intermediate planning points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expanding conventional UI operations with 103 collected APIs significantly accelerates task completion efficiency by replacing multiple UI steps with single API calls.
- Mechanism: API calls bypass the sequential nature of UI interactions, allowing agents to jump directly to target functionality without navigating through intermediate screens.
- Core assumption: APIs are available for all critical mobile functions and can be reliably invoked through ADB commands.
- Evidence anchors:
  - [abstract] "expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion"
  - [section 3.1] "a single API action might be equivalent to dozens of UI steps"
  - [corpus] Weak - only mentions Mobile-Bench-v2 and similar benchmarks but lacks direct API efficiency comparisons
- Break condition: APIs become unavailable for certain applications or when app developers change internal interfaces, making ADB calls ineffective.

### Mechanism 2
- Claim: The CheckPoint evaluation metric provides more accurate assessment of agent planning and reasoning by evaluating essential points during task execution rather than just final outcomes.
- Mechanism: CheckPoints break down tasks into atomic verification points (packages, key phrases, APIs) that can be checked independently, allowing partial credit for correct planning even if execution fails.
- Core assumption: Essential task completion points can be predetermined and reliably identified during planning phase.
- Evidence anchors:
  - [abstract] "introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps"
  - [section 3.3] "we divide the inspection granularity into two levels: CheckPointl1 - whether it uses the correct application, and CheckPointl2 - whether it follows the predefined paths to complete the task"
  - [corpus] Weak - neighboring papers mention evaluation but don't detail CheckPoint methodology
- Break condition: Task complexity exceeds the granularity of predefined CheckPoints or when semantic equivalence makes exact phrase matching too strict.

### Mechanism 3
- Claim: Categorizing tasks into three complexity levels (SAST, SAMT, MAMT) enables more precise evaluation of different planning capabilities required for varying task complexities.
- Mechanism: Task complexity directly correlates with planning requirements - single-app tasks need minimal planning, while multi-app tasks require sophisticated application switching and coordination strategies.
- Core assumption: Planning complexity scales predictably with the number of applications and task steps required.
- Evidence anchors:
  - [abstract] "categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity"
  - [section 3.1] "To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups"
  - [corpus] Weak - related papers mention task categorization but lack detailed complexity analysis
- Break condition: Agents develop strategies that bypass traditional planning (e.g., memorization) or when task complexity doesn't align with application count.

## Foundational Learning

- Concept: API integration with mobile applications through ADB commands
  - Why needed here: The benchmark relies on agents making programmatic API calls rather than just UI interactions, requiring understanding of how mobile applications expose functionality through external interfaces.
  - Quick check question: Can you explain the difference between an ADB shell command that starts an activity versus one that sends an intent, and when each would be appropriate?

- Concept: XML-to-HTML conversion for UI representation
  - Why needed here: The platform converts Android UI XML into HTML format for LLM consumption, requiring understanding of both Android UI hierarchy and web-based representations.
  - Quick check question: How would you convert a scrollable Android UI element with bounds [100,200][300,400] into HTML representation that preserves both clickable areas and scroll functionality?

- Concept: CheckPoint-based evaluation methodology
  - Why needed here: The benchmark uses a novel evaluation metric that checks intermediate task completion points rather than just final success, requiring understanding of software testing metrics adapted for LLM agents.
  - Quick check question: If a task requires opening App A, searching for "Beijing", then opening App B, how would you construct CheckPoints that provide partial credit for correct application selection but incorrect search terms?

## Architecture Onboarding

- Component map: User instruction → LLM planning (app selection + sub-task decomposition) → iterative execution (API call/UI action → observation → planning update) → evaluation via CheckPoints → final PassRate determination
- Critical path: User instruction → LLM planning (app selection + sub-task decomposition) → iterative execution (API call/UI action → observation → planning update) → evaluation via CheckPoints → final PassRate determination
- Design tradeoffs: API calls provide efficiency but require app-specific knowledge and may break with app updates; UI interactions are universal but slower and more error-prone; CheckPoints provide granular evaluation but require manual definition of essential points
- Failure signatures: High PassRate with low CheckPoint coverage indicates premature task completion judgment; consistently failed API calls suggest app compatibility issues; inability to exit applications points to planning deficiencies in multi-app scenarios
- First 3 experiments:
  1. Run SAST tasks with GPT-3.5 and GPT-4 to compare API utilization efficiency and verify that API calls reduce average steps.
  2. Test SAMT tasks with API calls disabled to measure performance degradation and confirm API importance.
  3. Execute MAMT tasks with different context window sizes to determine optimal action history compression for maintaining planning accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of 103 APIs impact the overall task completion efficiency compared to UI-only operations?
- Basis in paper: [explicit] The paper mentions that a single API call can be equivalent to multiple UI operations and that API calls are more efficient.
- Why unresolved: The paper provides a comparison in Table 5, but the results are based on GPT-4. It would be beneficial to understand the impact across different LLMs and more diverse tasks.
- What evidence would resolve it: Conducting experiments with various LLMs and a wider range of tasks, comparing the average number of steps and success rates with and without API calls.

### Open Question 2
- Question: What are the primary factors contributing to the lower PassRate and CheckPoint scores in more complex tasks like MAMT?
- Basis in paper: [inferred] The paper discusses the challenges faced by LLMs in determining when to exit an application and transition to another, and the impact of increasing action history on task progress judgment.
- Why unresolved: While the paper identifies some factors, a deeper analysis is needed to understand the specific challenges and develop strategies to improve performance.
- What evidence would resolve it: Analyzing the failure cases in MAMT tasks, identifying common patterns and challenges, and experimenting with different strategies to improve application switching and task progress judgment.

### Open Question 3
- Question: How can the CheckPoint evaluation metric be further refined to better assess the quality of the final outcome?
- Basis in paper: [explicit] The paper mentions that CheckPoint is an automatic process evaluation metric and it's challenging to assess the final outcome quality.
- Why unresolved: While CheckPoint provides a good measure of process adherence, it may not fully capture the quality and relevance of the final result.
- What evidence would resolve it: Developing additional evaluation metrics that consider the quality and relevance of the final outcome, such as user satisfaction surveys or task-specific quality assessments.

## Limitations

- The benchmark focuses primarily on Android platforms and Chinese applications, limiting generalizability to other mobile ecosystems
- Strict phrase matching in CheckPoints may penalize agents using semantically equivalent but linguistically different approaches
- API integration depends on app developers maintaining stable interfaces, making the benchmark vulnerable to app updates and changes

## Confidence

- High confidence: The core contribution of introducing a comprehensive evaluation benchmark with API integration and multi-level task categorization is well-supported by the experimental results and clearly articulated methodology.
- Medium confidence: The effectiveness of the CheckPoint evaluation metric is demonstrated through comparison with PassRate, but the specific implementation details and handling of edge cases require further validation.
- Medium confidence: The claim that API calls significantly improve task completion efficiency is supported by the theoretical framework but lacks direct quantitative comparisons with pure UI-based approaches.

## Next Checks

1. Implement the HTML transformation algorithm from XML UI representations and validate that converted HTML preserves all clickable elements and scroll functionality necessary for agent navigation.
2. Conduct ablation studies comparing agent performance with and without API access across all task categories to quantify the exact efficiency gains claimed in the paper.
3. Test the CheckPoint evaluation system with semantically equivalent but linguistically different correct answers to assess whether the strict matching criteria might be overly penalizing valid approaches.