---
ver: rpa2
title: Fluent dreaming for language models
arxiv_id: '2402.01702'
source_url: https://arxiv.org/abs/2402.01702
tags:
- dreaming
- prompts
- token
- prompt
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying feature visualization,
  or "dreaming," to language models, where the discrete nature of text inputs has
  hindered successful application. The authors extend the Greedy Coordinate Gradient
  (GCG) method to create the Evolutionary Prompt Optimization (EPO) algorithm, which
  optimizes input prompts to maximize a chosen internal feature while maintaining
  fluency.
---

# Fluent dreaming for language models

## Quick Facts
- arXiv ID: 2402.01702
- Source URL: https://arxiv.org/abs/2402.01702
- Authors: T. Ben Thompson; Zygimantas Straznickas; Michael Sklar
- Reference count: 40
- This paper addresses the challenge of applying feature visualization, or "dreaming," to language models, where the discrete nature of text inputs has hindered successful application.

## Executive Summary
This paper tackles the challenge of feature visualization (dreaming) for language models, where the discrete nature of text inputs has historically made gradient-based optimization difficult. The authors extend the Greedy Coordinate Gradient (GCG) method to create the Evolutionary Prompt Optimization (EPO) algorithm, which optimizes input prompts to maximize chosen internal features while maintaining fluency. EPO uses an evolutionary approach with a population of prompts, each optimized for different points on the Pareto frontier between feature activation and prompt fluency. The paper demonstrates dreaming with neurons, output logits, and arbitrary directions in activation space, measuring the fluency of resulting prompts and comparing them to max-activating dataset examples.

## Method Summary
EPO extends GCG by adding fluency regularization and population-based exploration. Given a feature f(t) and a language model m(t), EPO finds a prompt t* that maximizes the feature while minimizing self-cross-entropy, forming a Pareto frontier over multiple regularization strengths. The algorithm maintains M population members, each optimizing for a different λ value representing the trade-off between feature activation and fluency. Every Trestart iterations, it restarts by keeping only the best member for a random λr, allowing exploration of new basins of attraction. This evolutionary approach prevents the algorithm from getting stuck in local minima while exploring multiple points on the activation-fluency tradeoff frontier.

## Key Results
- EPO consistently achieves higher feature activations than dataset examples while maintaining prompt fluency
- The evolutionary approach with restarts successfully explores the Pareto frontier between feature activation and fluency
- EPO can produce out-of-distribution prompts that strongly trigger features, revealing model internals that wouldn't appear in natural text
- The method works across different feature types: neurons, output logits, and arbitrary directions in activation space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EPO extends Greedy Coordinate Gradient (GCG) by adding fluency regularization and population-based exploration.
- Mechanism: The algorithm optimizes prompts to maximize a chosen internal feature while minimizing self-cross-entropy, forming a Pareto frontier over multiple regularization strengths. Each population member targets a different point on this frontier.
- Core assumption: Feature f(t) must be differentiable with respect to one-hot encoded token vectors, and the same tokenization must be shared between the feature model and language model used for fluency evaluation.
- Break condition: If the feature is not differentiable with respect to token embeddings, or if tokenization differs between models, the gradient-based optimization cannot propagate properly.

### Mechanism 2
- Claim: The evolutionary approach with restarts prevents local minima by exploring multiple regularization strengths and random initializations.
- Mechanism: EPO maintains M population members, each optimizing for a different λ value. Every Trestart iterations, it restarts by keeping only the best member for a random λr, allowing exploration of new basins of attraction.
- Core assumption: The ideal regularization strength varies depending on both the feature and the context in which it activates, making a single λ suboptimal.
- Break condition: If the landscape is too rugged or the population diversity is insufficient, EPO may still converge to suboptimal local minima despite restarts.

### Mechanism 3
- Claim: EPO can produce higher feature activations than dataset examples by generating out-of-distribution prompts that strongly trigger the feature.
- Mechanism: By optimizing directly for feature activation rather than sampling from the training distribution, EPO can discover prompts that would be extremely unlikely to appear in natural text but maximally activate the target feature.
- Core assumption: The feature activation landscape contains regions that are accessible through discrete token optimization but not represented in the training data.
- Break condition: If the feature requires extremely specific long-range dependencies that cannot be captured in 12-token prompts, EPO may fail to achieve high activations.

## Foundational Learning

- Concept: Gradient-based discrete optimization
  - Why needed here: EPO needs to optimize in token space rather than embedding space, requiring methods that can handle discrete inputs while leveraging gradient information
  - Quick check question: What is the key difference between optimizing in token space versus embedding space, and why is this important for EPO?

- Concept: Pareto optimization and multi-objective optimization
  - Why needed here: EPO must balance two competing objectives (feature activation and fluency), requiring techniques to explore the tradeoff frontier
  - Quick check question: Why does EPO use multiple population members with different λ values instead of tuning a single λ hyperparameter?

- Concept: Language model fluency evaluation via self-cross-entropy
  - Why needed here: EPO needs an automated way to measure prompt fluency without human evaluation, using the language model's own perplexity on the prompt
  - Quick check question: How does computing the self-cross-entropy of a prompt provide a measure of its fluency?

## Architecture Onboarding

- Component map: Feature model -> Gradients -> Token selection -> Population update -> Fluency evaluation -> Pareto tracking
- Critical path: Feature → Gradients → Token selection → Population update → Fluency evaluation → Pareto tracking
- Design tradeoffs:
  - Population size M vs. computational cost: Larger M explores more of the Pareto frontier but increases runtime
  - Restart frequency Trestart vs. convergence speed: More frequent restarts prevent local minima but may slow convergence
  - Token selection top-k size vs. exploration: Larger k allows more diverse candidates but may include low-quality tokens
  - Prompt length vs. feature accessibility: Longer prompts may access more complex features but increase computational cost
- Failure signatures:
  - Population converges to same low-activation prompts across all λ values
  - Prompts become increasingly incoherent despite fluency regularization
  - No improvement in activation after many iterations
  - Restarting fails to escape local minima
- First 3 experiments:
  1. Run EPO on a simple neuron (like Pythia-12B-L10.N5) with M=2, T=50, visualize Pareto frontier evolution
  2. Compare EPO outputs to max-activating dataset examples for the same neuron, measure activation and fluency differences
  3. Test different population sizes (M=2, 4, 8) on the same neuron to understand tradeoff between exploration and computational cost

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does EPO compare to other methods for optimizing language model prompts, such as ARCA or AutoDAN?
- Basis in paper: [inferred] The paper mentions ARCA and AutoDAN in passing, but does not directly compare EPO to these methods.
- Why unresolved: The paper focuses on demonstrating EPO's effectiveness, but does not provide a comprehensive comparison to other methods.
- What evidence would resolve it: A controlled experiment comparing EPO to other methods on the same tasks and metrics.

### Open Question 2
- Question: How does EPO perform on language models with different architectures, such as transformer models or recurrent neural networks?
- Basis in paper: [explicit] The paper mentions that EPO was tested on Pythia-12B, but does not discuss its performance on other architectures.
- Why unresolved: The paper focuses on Pythia-12B, but does not explore EPO's performance on other architectures.
- What evidence would resolve it: A study applying EPO to different architectures and comparing the results.

### Open Question 3
- Question: How can EPO be used to improve the interpretability of language models?
- Basis in paper: [explicit] The paper mentions that EPO can be used to explore the behavior of model internals, but does not discuss specific applications for interpretability.
- Why unresolved: The paper demonstrates EPO's potential for interpretability, but does not provide concrete examples or guidelines.
- What evidence would resolve it: A case study or tutorial showing how EPO can be used to improve interpretability in a specific domain or task.

## Limitations

- Token-level vs. embedding-space optimization introduces discrete optimization challenges and may still get stuck in local minima despite restarts
- Fluency measurement using self-cross-entropy may not fully capture human notions of fluency and could include adversarial examples
- Feature attribution ambiguity makes it unclear whether high activations reveal genuine model circuitry or spurious correlations

## Confidence

**High Confidence**: The core EPO algorithm is well-defined and the experimental results showing higher feature activations than dataset examples are reproducible.

**Medium Confidence**: The claims about EPO's ability to explore the Pareto frontier between activation and fluency are supported by empirical results, but the optimality of the restart mechanism and population size choices could benefit from more systematic ablation studies.

**Low Confidence**: The interpretation of what the generated prompts reveal about model internals is largely speculative, with uncertainty about whether they reveal meaningful circuitry versus adversarial examples.

## Next Checks

1. **Ablation study on population size and restart parameters**: Systematically vary M (population size) and Trestart (restart frequency) to determine their impact on both feature activation and the ability to escape local minima. Compare results across multiple random seeds to assess robustness.

2. **Human evaluation of fluency vs. cross-entropy**: Conduct human evaluations of prompt fluency for a subset of EPO-generated prompts, comparing these judgments to the self-cross-entropy metric. This would validate whether the cross-entropy threshold of 6.0 appropriately captures human notions of fluency.

3. **Controlled feature activation experiments**: Test EPO on features with known, interpretable semantics (e.g., specific neurons that clearly respond to certain token patterns) to better understand whether high activations reveal genuine model circuitry or just adversarial patterns. Compare EPO results to gradient-based feature visualization methods applied directly in embedding space.