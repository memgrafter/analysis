---
ver: rpa2
title: Learning Latent Space Hierarchical EBM Diffusion Models
arxiv_id: '2405.13910'
source_url: https://arxiv.org/abs/2405.13910
tags:
- prior
- learning
- latent
- hierarchical
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of learning expressive priors for
  hierarchical latent variable models, which often use Gaussian priors that limit
  expressiveness and lead to a "prior hole" issue. The authors propose learning an
  energy-based model (EBM) prior using a diffusion probabilistic framework.
---

# Learning Latent Space Hierarchical EBM Diffusion Models

## Quick Facts
- arXiv ID: 2405.13910
- Source URL: https://arxiv.org/abs/2405.13910
- Reference count: 11
- Primary result: Proposed method achieves competitive FID scores on CIFAR-10, CelebA-HQ-256, and LSUN-Church-64 using learned EBM priors in hierarchical latent spaces

## Executive Summary
This paper addresses the limitation of Gaussian priors in hierarchical latent variable models, which suffer from the "prior hole" problem that limits expressiveness. The authors propose learning an energy-based model (EBM) prior using a diffusion probabilistic framework on a unified latent space (u-space) that preserves hierarchical structure. The approach uses sequential conditional EBMs that gradually approximate the generator posterior through reverse diffusion, making sampling more tractable than learning a single marginal EBM. Experiments show improved image synthesis quality and demonstrate that the learned EBM prior captures hierarchical representations useful for out-of-distribution detection and controllable generation.

## Method Summary
The method learns a sequence of conditional EBMs on a unified latent space (u-space) that preserves the hierarchical structure of the original multi-scale latent space (z-space). During the forward diffusion process, Gaussian noise is progressively added to samples from the generator posterior in u-space. The reverse diffusion process uses conditional EBMs at each step to denoise and approximate the posterior. Langevin dynamics is employed to sample from the conditional EBMs. The energy function couples all layers of latent variables at each diffusion step, allowing the model to capture both inter-layer and intra-layer relations. The approach is evaluated on CIFAR-10, CelebA-HQ-256, and LSUN-Church-64, with image synthesis quality measured by FID and IS, and hierarchical representation learning evaluated through out-of-distribution detection using AUROC.

## Key Results
- Competitive FID scores compared to existing methods on CIFAR-10, CelebA-HQ-256, and LSUN-Church-64
- Demonstrated ability to capture hierarchical representations at different layers useful for out-of-distribution detection
- Showed controllable generation capabilities with categorical and multiple attributes
- The learned EBM prior effectively addresses the "prior hole" problem of Gaussian priors in hierarchical VAEs

## Why This Works (Mechanism)

### Mechanism 1
Using a sequence of conditional EBMs instead of a single marginal EBM makes the energy landscape less multi-modal and easier to sample from. Each conditional EBM only needs to match perturbed samples at a specific diffusion step rather than the full complex posterior, creating localized energy basins that are easier for MCMC to explore. The core assumption is that perturbation kernels from Gaussian noise create sufficient localization between consecutive diffusion steps.

### Mechanism 2
Using a uni-scale latent space (u-space) instead of the original multi-scale latent space (z-space) simplifies MCMC sampling. The transformation to u-space preserves hierarchical structure while eliminating scale differences between layers, allowing standard MCMC methods to work effectively. The core assumption is that the reparameterization sampling scheme provides an invertible transformation that maintains the hierarchical dependencies.

### Mechanism 3
The diffusion probabilistic framework allows learning EBMs that capture both inter-layer and intra-layer relations. The energy function couples all layers of latent variables at each diffusion step, allowing the model to learn representations at different hierarchical levels. The core assumption is that coupling energy functions across layers at each diffusion step effectively captures the hierarchical structure.

## Foundational Learning

- Concept: Variational Autoencoders and hierarchical latent variable models
  - Why needed here: The work builds on hierarchical VAEs with Gaussian priors, extending them with EBM priors
  - Quick check question: What is the "prior hole problem" and why does it occur in standard hierarchical VAEs?

- Concept: Energy-based models and MCMC sampling
  - Why needed here: The EBM prior is learned using MCMC (specifically Langevin dynamics), which requires understanding of how EBMs work
  - Quick check question: Why is MCMC sampling from marginal EBMs in multi-layer latent spaces particularly challenging?

- Concept: Diffusion probabilistic models
  - Why needed here: The proposed method uses a diffusion framework to gradually approximate the generator posterior through sequential conditional EBMs
  - Quick check question: How does the forward diffusion process in u-space preserve hierarchical structure while destroying it in z-space?

## Architecture Onboarding

- Component map: Generator backbone -> Forward diffusion in u-space -> Reverse diffusion with conditional EBMs -> Langevin dynamics sampler -> Energy function with layer-specific components -> Label coupling mechanism

- Critical path: 1. Sample from generator posterior (or inference model) 2. Transform to u-space 3. Forward diffusion to Gaussian noise 4. Reverse diffusion using conditional EBMs 5. Transform back to z-space 6. Generate images through generator

- Design tradeoffs: More diffusion steps (T) → better sample quality but higher computational cost; More Langevin steps (K) → better EBM samples but longer training/sampling time; Coupling labels at different layers → more control but more complex energy function

- Failure signatures: Poor FID scores despite training → likely issues with EBM sampling or energy function design; Mode collapse in generated samples → energy landscape may be too smooth; Slow convergence → MCMC sampling may not be exploring effectively

- First 3 experiments: 1. Train with T=3, K=30 on CIFAR-10 and verify basic functionality (FID < 20) 2. Test hierarchical sampling by fixing different layer combinations and visualizing results 3. Implement controllable generation with categorical labels and verify category-specific outputs

## Open Questions the Paper Calls Out

### Open Question 1
How does the uni-scale u-space formulation affect the modeling of inter-layer dependencies compared to the original z-space in hierarchical VAEs? The paper explicitly states that the u-space formulation preserves the hierarchical structure during the forward diffusion process, unlike the z-space where the inter-layer relations are destroyed. However, it does not provide a detailed analysis of how this preservation affects the modeling of inter-layer dependencies compared to the original z-space.

### Open Question 2
What is the optimal number of diffusion steps T for different datasets and model architectures, and how does it affect the trade-off between sample quality and computational efficiency? The paper mentions that using more diffusion steps (e.g., T=6) can improve sample quality but also requires more sampling time. However, it does not provide a systematic study of the optimal number of diffusion steps for different datasets and model architectures.

### Open Question 3
How does the learned EBM prior generalize to out-of-distribution data, and what are the limitations of using the energy score as a decision function for out-of-distribution detection? The paper demonstrates that the learned EBM prior can be used for out-of-distribution detection by computing the energy score as a decision function. However, it does not provide a detailed analysis of how the learned EBM prior generalizes to out-of-distribution data or the limitations of using the energy score as a decision function.

## Limitations
- The computational overhead of training sequential conditional EBMs and performing Langevin dynamics at each diffusion step may limit scalability
- The claim that conditional EBMs are "less multi-modal" than marginal EBMs remains theoretically justified but not experimentally validated
- The paper lacks direct empirical evidence showing how well the transformation maintains hierarchical dependencies

## Confidence

- **High confidence**: The general framework of using diffusion to learn conditional EBMs on a unified latent space is theoretically sound and well-grounded in existing literature
- **Medium confidence**: The claim that this approach improves over Gaussian priors is supported by competitive FID scores, but the specific mechanism by which hierarchical structure is preserved through the u-space transformation needs more rigorous validation
- **Medium confidence**: The hierarchical representation learning benefits (out-of-distribution detection, controllable generation) are demonstrated but could benefit from more extensive ablation studies

## Next Checks

1. **Ablation study on diffusion steps**: Systematically vary T (number of diffusion steps) and K (Langevin steps) to quantify the tradeoff between sample quality and computational cost, identifying optimal configurations for different dataset complexities

2. **Hierarchical structure preservation analysis**: Design experiments that explicitly measure how well inter-layer and intra-layer dependencies are maintained through the u-space transformation, comparing learned representations against ground truth hierarchical structure

3. **MCMC sampling diagnostics**: Implement and visualize MCMC diagnostics (autocorrelation, effective sample size, energy landscape visualization) to empirically verify that the conditional EBMs are indeed less multi-modal and easier to sample from than marginal EBMs in multi-layer latent spaces