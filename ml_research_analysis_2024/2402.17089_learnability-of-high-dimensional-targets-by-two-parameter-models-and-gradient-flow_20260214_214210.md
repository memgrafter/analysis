---
ver: rpa2
title: Learnability of high-dimensional targets by two-parameter models and gradient
  flow
arxiv_id: '2402.17089'
source_url: https://arxiv.org/abs/2402.17089
tags:
- targets
- then
- level
- theorem
- some
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the theoretical possibility of learning high-dimensional
  targets using underparameterized models (with fewer parameters than target dimension)
  via gradient flow. The main result shows that for targets described by a specific
  probability distribution, models with as few as two parameters can learn these targets
  with arbitrarily high success probability.
---

# Learnability of high-dimensional targets by two-parameter models and gradient flow

## Quick Facts
- arXiv ID: 2402.17089
- Source URL: https://arxiv.org/abs/2402.17089
- Authors: Dmitry Yarotsky
- Reference count: 34
- Key outcome: Two-parameter models can learn high-dimensional targets with arbitrarily high probability under specific conditions, but underparameterization generally prevents density of learnable targets in the target space.

## Executive Summary
This paper studies the theoretical limits of learning high-dimensional targets using underparameterized models via gradient flow. The main result shows that models with as few as two parameters can learn targets with arbitrarily high success probability when the target distribution has specific hierarchical structure. However, the paper also demonstrates that underparameterization necessarily implies severe constraints - the set of learnable targets cannot be dense in the target space, and certain subsets homeomorphic to spheres must contain non-learnable targets. The construction uses a sophisticated hierarchical procedure that creates aligned level curves and level lines to guide gradient flow through increasingly refined Cantor-like target sets.

## Method Summary
The paper analyzes learnability of d-dimensional targets by models with W parameters using gradient flow optimization. The main construction uses a two-parameter model (W=2) that achieves high-probability learning by decomposing both parameter and target spaces into nested hierarchies of rectangular boxes. The model map Φ is constructed as a composition of stage functions Φ(n) that progressively deform level curves to align with target box boundaries. Gradient flow trajectories are guided through these boxes by monotonic progression in the u parameter and movement along level curves controlled by the v parameter. The proof shows that for carefully chosen splitting parameters, the removed parts have arbitrarily small measure, allowing learning with arbitrarily high success probability.

## Key Results
- Two-parameter models (W=2) can learn high-dimensional targets with arbitrarily high success probability under specific probability distributions
- Underparameterization (W < d) necessarily implies that learnable targets are not dense in Rd, with certain subsets homeomorphic to spheres containing non-learnable targets
- Models expressible by elementary functions (excluding unbounded sin) cannot achieve the same learnability guarantees due to measure-theoretic constraints on their images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-parameter models can learn high-dimensional targets with arbitrarily high probability if the target distribution is carefully structured.
- Mechanism: The construction uses a hierarchical decomposition of both parameter and target spaces, creating aligned level curves and level lines that guide gradient flow trajectories through increasingly refined Cantor-like target sets.
- Core assumption: The target space can be decomposed into a nested hierarchy of rectangular boxes with gaps between them, and the measure of removed parts can be made arbitrarily small.
- Evidence anchors:
  - [abstract] "there exist models with as few as two parameters that can learn the targets with arbitrarily high success probability"
  - [section] "The set F0 has the form F0 = ∩∞ n=1 ∪α B(n) α , where B(n) α is a nested hierarchy of rectangular boxes in Rd"
  - [corpus] "The Computational Advantage of Depth: Learning High-Dimensional Hierarchical Functions with Gradient Descent" (related but different approach)
- Break condition: If the target measure has infinite mass (e.g., Lebesgue measure), the reduction to bounded boxes no longer applies, breaking the construction.

### Mechanism 2
- Claim: Underparameterization (W < d) necessarily implies severe constraints on learnable targets, preventing density in target space.
- Mechanism: For models with fewer parameters than target dimension, gradient flow trajectories can get trapped in local minima due to loss barriers created by the model's limited expressiveness.
- Core assumption: The model is sufficiently regular (C2) and non-degenerate at initialization (full rank Jacobian).
- Evidence anchors:
  - [abstract] "for W < d there is necessarily a large subset of GF-non-learnable targets"
  - [section] "Theorem 3: Under a mild nondegeneracy assumption, the GF-learnable targets are not dense in Rd"
  - [corpus] "Fundamental computational limits of weak learnability in high-dimensional multi-index models" (similar limitation theme)
- Break condition: If the model is degenerate or lacks sufficient regularity (e.g., not differentiable), the proof technique fails but the underlying limitation may still hold.

### Mechanism 3
- Claim: Models expressible by elementary functions (excluding unbounded sin) cannot achieve the same learnability guarantees as the hierarchical construction.
- Mechanism: Pfaffian functions (which include elementary functions on bounded domains) have level sets with bounded connected components, forcing their image to have zero Lebesgue measure in high-dimensional spaces.
- Core assumption: The closure of the model's image has zero Lebesgue measure, and the model is Pfaffian.
- Evidence anchors:
  - [abstract] "most models written in terms of elementary functions cannot achieve the learnability demonstrated in this theorem"
  - [section] "Theorem 8: Suppose that Φ : RW → Rd is a Pfaffian map and W < d. Then the closure Φ(RW ) has Lebesgue measure 0 in Rd"
  - [corpus] "Generative Modeling by Minimizing the Wasserstein-2 Loss" (different approach to generation)
- Break condition: If the elementary function involves sin on an unbounded domain, the image can have positive measure, but learnability still fails due to prevalent trapping local minima.

## Foundational Learning

- Concept: Gradient Flow as continuous limit of gradient descent
  - Why needed here: The paper uses GF dynamics to analyze learnability, requiring understanding of how parameters evolve under continuous optimization
  - Quick check question: What is the relationship between gradient flow and gradient descent with infinitesimal step size?

- Concept: Sard's theorem and critical values
  - Why needed here: Used to prove that Pfaffian function images have measure zero by showing non-critical values are dense
  - Quick check question: What does Sard's theorem tell us about the measure of critical values of smooth functions?

- Concept: Cantor set construction and measure theory
  - Why needed here: The learnable set is constructed as a high-dimensional Cantor set with almost full measure, requiring understanding of how measure behaves under infinite processes
  - Quick check question: How can a nowhere dense set have positive Lebesgue measure?

## Architecture Onboarding

- Component map: Parameter space (u,v) -> Stage functions Φ(n) -> Target space Rd with hierarchical box decomposition
- Critical path: Target → Box decomposition → Stage-aligned level curves → Parameter trajectory → Gradient flow convergence
- Design tradeoffs:
  - More aggressive splitting increases learnability but requires finer control of level curve deformation
  - Larger gaps between boxes prevent trapping but reduce measure of learnable set
  - Higher stretching factor for u improves approximation but slows convergence
- Failure signatures:
  - Gradient flow gets trapped in local minima (target has coordinate near existing level curve)
  - Trajectory fails to reach tip regions (target components sum below threshold)
  - Non-monotonic u evolution (violation of u-monotonicity conditions)
- First 3 experiments:
  1. Implement 2D version with d=2, verify hierarchical construction and u-monotonicity for simple target distributions
  2. Test measure of removed parts vs splitting parameters to confirm arbitrarily small loss is achievable
  3. Replace hierarchical Φ with elementary function and measure change in learnable set size to validate Theorem 8

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Theorem 5 be extended to target measures with infinite total mass (e.g., Lebesgue measure on Rd)?
- Basis in paper: Explicit - "Target measures with µ(H) = ∞"
- Why unresolved: The proof relies on restricting to a bounded box where the measure is finite, which is not possible for infinite measures.
- What evidence would resolve it: A proof or counterexample showing whether the hierarchical construction can be adapted for infinite measures, or a specific example demonstrating impossibility.

### Open Question 2
- Question: Can Theorem 3 be strengthened to hold for degenerate models (Φ with rank-deficient Jacobian at w=0) and weaker regularity assumptions?
- Basis in paper: Explicit - "Non-density of the learnable targets for degenerate models"
- Why unresolved: The current proof relies on full-rank Jacobian and C2 smoothness, but the non-density result may hold under weaker conditions.
- What evidence would resolve it: A proof showing non-density for models with rank-deficient Jacobian, or a counterexample demonstrating that non-density can fail under weaker assumptions.

### Open Question 3
- Question: What is the structure of learnable target sets for general 1 < W < d parameters?
- Basis in paper: Explicit - "Learnable sets for general 1 < d"
- Why unresolved: Theorem 5 only characterizes learnable sets for W=2, while Theorem 4 only provides necessary conditions. The general case remains open.
- What evidence would resolve it: A characterization of learnable sets for arbitrary W < d, potentially showing that learnable sets become more regular as W increases.

## Limitations

- The construction relies heavily on precise hierarchical decomposition of both parameter and target spaces, with gaps between nested boxes that must be carefully controlled
- While the paper proves that elementary function models cannot achieve the same learnability, the practical implications for common architectures like MLPs remain unclear
- The assumption that the target distribution can be decomposed into the required hierarchical structure may not hold for many natural distributions

## Confidence

- **High confidence**: The fundamental limitation that underparameterization (W < d) prevents learnable targets from being dense in target space (Theorem 3)
- **Medium confidence**: The two-parameter construction achieving high-probability learning (Theorem 2)
- **Medium confidence**: The claim that elementary functions cannot achieve the same learnability (Theorem 8)

## Next Checks

1. **Implement and test the 2D case**: Construct the hierarchical model for d=2 with explicit box decomposition and verify u-monotonicity conditions numerically. Measure how splitting parameters affect learnability probability.
2. **Compare elementary vs hierarchical models**: Replace the hierarchical Φ with a standard elementary function (e.g., MLP with tanh activations) and empirically measure the change in learnable set size for the same target distributions.
3. **Test alternative target distributions**: Apply the construction to different target measures (Gaussian, uniform, mixture distributions) to verify that the learnability result holds beyond the specific measure used in the proof.