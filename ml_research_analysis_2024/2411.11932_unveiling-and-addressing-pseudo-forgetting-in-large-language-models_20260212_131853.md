---
ver: rpa2
title: Unveiling and Addressing Pseudo Forgetting in Large Language Models
arxiv_id: '2411.11932'
source_url: https://arxiv.org/abs/2411.11932
tags:
- rationale
- forgetting
- task
- pseudo
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates "pseudo forgetting" in continual learning
  of large language models (LLMs), where performance degradation on previous tasks
  stems from instructions failing to activate appropriate model abilities rather than
  actual capability loss. Through experiments on multiple natural language tasks,
  the authors demonstrate that providing partial correct rationales or appending meaningless
  suffixes to instructions can restore performance to pre-forgetting levels, proving
  that LLMs retain their task capabilities.
---

# Unveiling and Addressing Pseudo Forgetting in Large Language Models

## Quick Facts
- arXiv ID: 2411.11932
- Source URL: https://arxiv.org/abs/2411.11932
- Reference count: 40
- This paper investigates "pseudo forgetting" in continual learning of LLMs, where performance degradation stems from instructions failing to activate appropriate model abilities rather than actual capability loss.

## Executive Summary
This paper investigates a novel phenomenon called "pseudo forgetting" in continual learning of large language models, where performance degradation on previous tasks results from instructions failing to activate appropriate model abilities rather than actual capability loss. Through experiments on multiple natural language tasks, the authors demonstrate that providing partial correct rationales or appending meaningless suffixes to instructions can restore performance to pre-forgetting levels, proving that LLMs retain their task capabilities. The underlying cause is identified as reduced instruction dependence during rationale generation in pseudo-forgetting models, quantified through attribution scores.

## Method Summary
The paper proposes a novel continual learning framework that addresses pseudo forgetting through rationale-based interventions and dynamic replay allocation. The approach involves sequential task training followed by attribution analysis to measure instruction dependence, then calculating Rationale-Guidance Difficulty (RGD) scores to determine replay data allocation. The RGD-R framework dynamically allocates replay data based on each task's difficulty score, optimizing data utilization for pseudo forgetting mitigation while maintaining model plasticity.

## Key Results
- Pseudo forgetting is demonstrated where models retain capabilities but fail to activate them via original instructions
- Performance restoration achieved through partial rationale guidance and meaningless suffix addition
- RGD-R framework outperforms baselines by dynamically allocating replay data based on instruction-activation difficulty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo forgetting occurs because original instructions fail to activate appropriate model abilities rather than actual capability loss.
- Mechanism: When LLMs learn new tasks, the original instructions lose their effectiveness in triggering the correct internal capabilities, causing performance degradation on previous tasks despite retained abilities.
- Core assumption: LLMs store task-specific capabilities parametrically and can recover them when properly guided.
- Evidence anchors:
  - [abstract] "performance degradation on previous tasks is not attributed to a loss of capabilities, but rather to the failure of the instructions to activate the appropriate model abilities"
  - [section 2.2] "pseudo-forgetting model exhibits significantly reduced reliance on instructions, which prevents the model from effectively utilizing its internal capabilities"
  - [corpus] Average neighbor FMR=0.483 indicates related work on instruction vector and linguistic regions, supporting the instruction-dependency focus
- Break condition: If external evidence shows LLMs completely lose task-specific parameters during continual learning, this mechanism would fail.

### Mechanism 2
- Claim: Providing partial correct rationale or meaningless suffixes can restore performance by reactivating correct capability utilization.
- Mechanism: External guidance helps the model reestablish the connection between original instructions and their corresponding capabilities, bypassing the instruction activation failure.
- Core assumption: LLMs can self-generate correct rationales when given minimal guidance, indicating preserved capabilities.
- Evidence anchors:
  - [abstract] "model's performance on previous tasks can be restored through two simple interventions: (1) providing partial external correct rationale, and (2) appending semantically meaningless suffixes"
  - [section 2.1] "under the guidance of correct rationale, a forgetting model demonstrates promising potential to recover task performance to pre-forgetting levels"
  - [corpus] Neighbor work on "Refine Large Language Model Fine-tuning via Instruction Vector" suggests instruction-based recovery methods are viable
- Break condition: If suffix optimization consistently fails across different model scales and tasks, this mechanism would be invalidated.

### Mechanism 3
- Claim: RGD-R framework dynamically allocates replay data based on Rationale-Guidance Difficulty, optimizing data utilization for pseudo forgetting mitigation.
- Mechanism: Tasks with higher RGD scores (harder to activate correct capabilities) receive more replay data, strengthening instruction-capability connections where needed most.
- Core assumption: RGD score accurately measures the difficulty of capability activation and correlates with pseudo forgetting severity.
- Evidence anchors:
  - [abstract] "we introduce the Rationale-Guidance Difficulty (RGD) metric, which measures the difficulty for the model to correctly utilize its internal capabilities"
  - [section 3.3] "RGD-R dynamically determines the required replay data ratio for each previous task based on the RGD score"
  - [corpus] Weak evidence - no direct corpus support for RGD-based replay, but neighbor work on instruction vectors provides indirect relevance
- Break condition: If RGD scores fail to predict which tasks benefit most from replay data, the framework's optimization would be ineffective.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding traditional forgetting mechanisms helps distinguish pseudo forgetting from actual capability loss
  - Quick check question: What is the key difference between catastrophic forgetting and pseudo forgetting according to this paper?

- Concept: Attribution scores for measuring instruction dependence
  - Why needed here: Attribution analysis reveals how model reliance on instructions changes during pseudo forgetting
  - Quick check question: How does instruction dependency change in pseudo-forgetting models when generating correct vs incorrect rationales?

- Concept: Rationale generation in instruction-following LLMs
  - Why needed here: The paper focuses on how models generate rationales and how this process is affected by pseudo forgetting
  - Quick check question: What role does rationale generation play in distinguishing pseudo forgetting from actual forgetting?

## Architecture Onboarding

- Component map: Sequential task training pipeline -> Attribution analysis module for instruction dependence -> RGD-R framework with dynamic replay allocation -> Evaluation metrics (FAP, F.Ra, BWT, FWT)

- Critical path: 1) Train model sequentially on tasks → 2) Detect pseudo forgetting through attribution analysis → 3) Calculate RGD scores for previous tasks → 4) Allocate replay data based on RGD scores → 5) Evaluate performance improvement

- Design tradeoffs: RGD-R trades off between stability (preventing forgetting) and plasticity (maintaining learning ability) by dynamically allocating replay data rather than using equal allocation, which may over-replay easy tasks while under-recovering difficult ones

- Failure signatures: Poor performance recovery despite suffix guidance, attribution scores showing no difference between correct/incorrect rationale generation, or RGD scores failing to correlate with actual forgetting severity

- First 3 experiments:
  1. Replicate the partial rationale experiment (k=0.2) on a small dataset to verify performance recovery
  2. Run attribution analysis comparing instruction dependency before/after pseudo forgetting on one task
  3. Implement basic RGD-R with equal allocation baseline to compare performance on a single model-task pair

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific mechanism by which LLMs begin to show reduced dependence on instructions from previously learned tasks during new task learning?
- Basis in paper: [inferred] The paper mentions that pseudo forgetting occurs because original instructions fail to activate the model's appropriate intrinsic capabilities, but doesn't specify when or how this reduction in instruction dependence begins during the learning process.
- Why unresolved: The authors acknowledge this gap in their limitations section, noting they didn't conduct an in-depth analysis of the specific process behind pseudo forgetting, including at what point during new task learning the model starts showing reduced instruction dependence.
- What evidence would resolve it: Detailed experimental analysis tracking instruction dependence scores across different stages of continual learning, or neural activation studies showing when and where instruction-following patterns begin to degrade.

### Open Question 2
- Question: Does pseudo forgetting exhibit task or domain-specific characteristics similar to how domain generalization in summarization tasks correlates with word distribution?
- Basis in paper: [inferred] The paper mentions in limitations that the relationship between pseudo forgetting and specific tasks or domains remains unexplored, and references Li et al. (2024c) who found domain generalization in summarization correlates with word distribution.
- Why unresolved: The authors explicitly state this relationship remains unexplored and suggest it's a direction for future research.
- What evidence would resolve it: Systematic experiments across diverse task domains measuring pseudo forgetting rates and analyzing correlations with task-specific features like vocabulary distribution, reasoning complexity, or semantic similarity.

### Open Question 3
- Question: What are the optimal strategies for combining replay-based and parameter-based approaches to maximize continual learning performance?
- Basis in paper: [inferred] The paper suggests in limitations that future work could benefit from combining replay-based and parameter-based approaches, with emphasis on enhancing asynchronous knowledge transfer, but doesn't specify what these optimal combinations would look like.
- Why unresolved: The authors acknowledge this is an underexplored aspect in current research and leave it for future work, without providing concrete guidance on how to balance these approaches.
- What evidence would resolve it: Comparative experiments testing various combinations of replay-based methods (like RGD-R) with parameter-based methods (like elastic weight consolidation or synaptic intelligence) across different model scales and task sequences.

## Limitations

- Generalizability concerns as experiments focus on classification tasks with specific instruction-following models
- Attribution analysis relies on self-attention patterns which may not fully capture complex capability activation mechanisms
- Suffix optimization raises questions about whether it validates the instruction-activation hypothesis or simply introduces noise

## Confidence

- High Confidence: The existence of performance degradation in continual learning settings
- Medium Confidence: The claim that this degradation is primarily due to instruction activation failure rather than capability loss
- Medium Confidence: The effectiveness of the RGD-R framework in mitigating pseudo forgetting

## Next Checks

1. **Ablation Study on Attribution Method:** Test whether alternative attribution methods (e.g., integrated gradients or attention roll-out) produce consistent results with the self-attention-based approach, validating that the instruction dependence patterns are robust across different analysis techniques.

2. **Cross-Domain Task Transfer:** Apply the pseudo forgetting detection framework to non-classification tasks (e.g., question answering or summarization) to verify whether the instruction-activation hypothesis holds across different task types and whether the RGD metric remains predictive.

3. **Model Architecture Comparison:** Test whether pseudo forgetting patterns and the effectiveness of the RGD-R framework vary significantly across different model architectures (e.g., decoder-only vs encoder-decoder models) to assess the generality of the findings beyond instruction-following LLMs.