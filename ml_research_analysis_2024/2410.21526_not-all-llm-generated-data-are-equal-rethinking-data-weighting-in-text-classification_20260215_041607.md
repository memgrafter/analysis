---
ver: rpa2
title: 'Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification'
arxiv_id: '2410.21526'
source_url: https://arxiv.org/abs/2410.21526
tags:
- data
- training
- real-world
- dataset
- dimp-loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two weighted-loss approaches, IMP-Loss and
  DIMP-Loss, to improve model performance when training on LLM-generated synthetic
  data for text classification tasks. The key idea is to assign higher weights to
  data points that are both high-quality (relevant to real-world distribution) and
  diverse, using a small real-world dataset to build quality and diversity checkers.
---

# Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification

## Quick Facts
- arXiv ID: 2410.21526
- Source URL: https://arxiv.org/abs/2410.21526
- Reference count: 40
- Primary result: IMP-Loss and DIMP-Loss methods consistently outperform standard cross-entropy loss and other data weighting approaches when training on LLM-generated synthetic data

## Executive Summary
This paper addresses the challenge of training models on LLM-generated synthetic data for text classification tasks. The authors propose two weighted-loss approaches, IMP-Loss and DIMP-Loss, that assign higher weights to synthetic data points that are both high-quality and diverse. By using a small real-world dataset to build quality and diversity checkers, these methods align the synthetic data distribution with the real-world distribution, improving model performance. The methods demonstrate consistent improvements across multiple text classification benchmarks, even surpassing the few-shot accuracy of the data generator.

## Method Summary
The paper introduces two weighted-loss approaches (IMP-Loss and DIMP-Loss) for training models on LLM-generated synthetic data. Both methods use a small real-world dataset (200-400 samples) to build a quality checker that measures data point relevance. IMP-Loss uses a static diversity checker trained on synthetic data, while DIMP-Loss uses a dynamic diversity checker based on current model predictions. The weighted loss function emphasizes high-quality and diverse data points by calculating weights as the ratio of real-world to synthetic data likelihoods, effectively aligning the training distribution with the real-world distribution through importance sampling.

## Key Results
- Both IMP-Loss and DIMP-Loss consistently outperform standard cross-entropy loss and other data weighting approaches across multiple text classification benchmarks
- DIMP-Loss achieves competitive performance with smaller quality checkers, making it more efficient in terms of model size, data requirements, and computational resources
- The methods demonstrate robust performance on real-world and noisy datasets, surpassing the few-shot accuracy of the data generator
- IMP-Loss and DIMP-Loss show stable improvements even with varying quality checker sizes, though performance degrades with very small checkers (100 samples)

## Why This Works (Mechanism)

### Mechanism 1: Quality Assurance
The quality checker ensures synthetic data relevance by measuring likelihood under a small real-world dataset distribution. A BERT model is trained on a small real-world dataset (DP') to estimate P'(y|x), which serves as a quality score. Higher scores indicate data points are more representative of real-world distribution. The core assumption is that a small amount of real-world data is sufficient to build an effective quality estimator.

### Mechanism 2: Diversity Preservation
Diversity checkers prevent model collapse by ensuring synthetic data covers diverse examples. In IMP-Loss, diversity is measured by Q(y|x) from the LLM-generated distribution, where low values indicate high information content. In DIMP-Loss, diversity is measured by current model predictions P(y|x; Î¸t), where low values indicate challenging, under-learned examples. Different diversity perspectives (data distribution vs model perspective) help capture different aspects of synthetic data variability.

### Mechanism 3: Distribution Alignment
The weighting scheme transforms the synthetic data distribution Q to match the real-world distribution P through importance sampling. Weights are calculated as P(y|x)/Q(y|x), emphasizing data points more likely under P relative to Q. This aligns the effective training distribution with P. The core assumption is that LLM can approximate real-world input distribution P(x) through appropriate prompting.

## Foundational Learning

- Concept: Importance sampling in statistics
  - Why needed here: Forms the theoretical foundation for IMP-Loss weight calculation
  - Quick check question: Why does weighting by P(y|x)/Q(y|x) help align synthetic data distribution with real-world distribution?

- Concept: Knowledge distillation
  - Why needed here: DIMP-Loss can be interpreted as a form of knowledge distillation where the quality checker acts as the teacher model
  - Quick check question: How does the quality checker in DIMP-Loss function similarly to a teacher model in distillation?

- Concept: Maximum entropy regularization
  - Why needed here: DIMP-Loss includes a maximum entropy term to prevent overconfident predictions
  - Quick check question: Why would encouraging higher entropy in model predictions help prevent overfitting?

## Architecture Onboarding

- Component map: Quality checker (BERT trained on DP') -> Diversity checker (BERT on DQ for IMP-Loss, model predictions for DIMP-Loss) -> Weighted loss function -> Main model (BERT-base or larger)

- Critical path: 1. Train quality checker on DP' 2. Train diversity checker on DQ (if using IMP-Loss) 3. Calculate weights for each synthetic data point 4. Train main model using weighted loss function 5. Monitor performance on validation set

- Design tradeoffs: Quality checker size vs computational cost; IMP-Loss vs DIMP-Loss: static vs dynamic weighting; Number of quality checker training examples vs effectiveness

- Failure signatures: Weights all become very small or very large (quality checker problem); No improvement over standard cross-entropy (diversity checker problem); DIMP-Loss shows high initial variation but converges (expected behavior)

- First 3 experiments: 1. Train with CE-Loss on synthetic data to establish baseline 2. Train with IMP-Loss using default hyperparameters 3. Train with DIMP-Loss using default hyperparameters

- Quick setup checklist: [ ] Prepare small real-world dataset (DP') [ ] Generate synthetic dataset (DQ) [ ] Train quality checker on DP' [ ] Train diversity checker on DQ (if using IMP-Loss) [ ] Implement weighted loss function [ ] Set up training loop with weight calculation [ ] Monitor validation performance across epochs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do IMP-Loss and DIMP-Loss perform on text generation and question answering tasks?
- Basis in paper: The paper mentions that future work will explore extending the methods to question answering and text generation tasks (section 7, Conclusions and Discussions).
- Why unresolved: The current study only evaluates the methods on text classification tasks. There is no experimental data or analysis for generation or question answering tasks.
- What evidence would resolve it: Experimental results showing IMP-Loss and DIMP-Loss performance on question answering and text generation benchmarks, compared to baselines like CE-Loss and Focal Loss.

### Open Question 2
- Question: What is the impact of the quality checker and diversity checker sizes on IMP-Loss and DIMP-Loss performance?
- Basis in paper: The paper shows that DIMP-Loss performs well even with smaller quality checkers (Table 2), but the impact of diversity checker size on IMP-Loss performance is not thoroughly explored.
- Why unresolved: While the paper discusses the importance of diversity checkers, it doesn't systematically investigate how different sizes of diversity checkers affect IMP-Loss performance.
- What evidence would resolve it: Experiments varying the size of diversity checkers in IMP-Loss and measuring the impact on performance across different benchmarks.

### Open Question 3
- Question: How do IMP-Loss and DIMP-Loss perform on large-scale datasets beyond the ones tested?
- Basis in paper: The experiments are conducted on three text classification benchmarks with moderate dataset sizes. The paper doesn't explore performance on larger datasets or different domains.
- Why unresolved: The current evaluation is limited to specific datasets, and there's no analysis of how the methods scale to larger or more diverse datasets.
- What evidence would resolve it: Experimental results on larger datasets or datasets from different domains, comparing IMP-Loss and DIMP-Loss performance with baselines.

## Limitations

- Limited hyperparameter specifications: The paper lacks detailed hyperparameter settings beyond learning rate ranges, making exact reproduction challenging
- Domain-specific evaluation: Methods are only tested on three text classification tasks (financial, social media, and MRPC), limiting generalizability claims
- Small real-world dataset assumption: The claim that 200-400 real-world samples are sufficient for effective quality checkers needs more extensive ablation studies

## Confidence

- **High confidence**: The theoretical foundation of importance sampling and its application to data weighting is sound and well-established
- **Medium confidence**: The empirical results showing consistent improvements over baselines, as the experiments demonstrate statistically significant gains across multiple benchmarks
- **Low confidence**: The generalizability claim to other LLM-generated data types and domains, as the paper only tests on three text classification tasks

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary the size of DP' (from 50 to 500 samples) and synthetic dataset size to determine the minimum effective dataset sizes for both quality and diversity checkers.

2. **Alternative quality checker architectures**: Replace the BERT quality checker with simpler models (logistic regression, linear models) to test whether the performance gains are due to the weighting mechanism or the specific quality checker architecture.

3. **Cross-domain robustness test**: Apply the methods to a completely different domain (e.g., medical text classification or multilingual datasets) to validate the generalizability of the weighting approaches beyond the tested financial and social media domains.