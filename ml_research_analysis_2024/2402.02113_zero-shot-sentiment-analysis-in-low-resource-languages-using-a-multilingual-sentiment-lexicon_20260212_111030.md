---
ver: rpa2
title: Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual
  Sentiment Lexicon
arxiv_id: '2402.02113'
source_url: https://arxiv.org/abs/2402.02113
tags:
- panlex
- languages
- sentiment
- language
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a lexicon-based pretraining approach for zero-shot
  multilingual sentiment analysis, addressing the scarcity of labeled data in low-resource
  languages. Instead of relying on sentence-level sentiment datasets, the authors
  leverage a multilingual sentiment lexicon (NRC-VAD) to pretrain multilingual language
  models.
---

# Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon

## Quick Facts
- arXiv ID: 2402.02113
- Source URL: https://arxiv.org/abs/2402.02113
- Reference count: 33
- Proposes lexicon-based pretraining for zero-shot multilingual sentiment analysis in low-resource languages

## Executive Summary
This paper addresses the challenge of zero-shot sentiment analysis in low-resource languages by proposing a lexicon-based pretraining approach. Instead of relying on labeled sentence-level sentiment datasets, the authors leverage a multilingual sentiment lexicon (NRC-VAD) to pretrain multilingual language models. They extend the lexicon using Panlex and filter the lexemes to improve quality. The method is evaluated on 34 languages, including high/medium-resource, low-resource, and code-switching datasets, showing superior performance compared to models fine-tuned on English data and large language models like GPT-3.5, BLOOMZ, and XGLM in zero-shot settings.

## Method Summary
The approach extends multilingual models (mBERT, XLM-R, mBART, mT5) using lexicon-based pretraining on valence scores from the NRC-VAD lexicon. The lexicon is extended with Panlex translations and filtered using a regression model trained on English NRC-VAD. The filtered lexicon is then used to pretrain the models, which are evaluated zero-shot on 34 languages. For comparison, models are also fine-tuned on English SST and evaluated on the same datasets.

## Key Results
- Lexicon-based pretraining outperforms models fine-tuned on English SST in zero-shot settings for low-resource languages
- Extended lexicon filtering improves performance on low-resource languages
- Zero-shot performance on code-mixed text is competitive with supervised models

## Why This Works (Mechanism)

### Mechanism 1
Lexicon-based pretraining using valence scores improves cross-lingual sentiment transfer by providing direct lexical sentiment grounding in target languages. Multilingual models learn semantic sentiment associations from words/phrases annotated with valence scores, bypassing the need for sentence-level supervision. These associations generalize to unseen sentences via lexical overlap.

### Mechanism 2
Extending the lexicon with Panlex translations and filtering improves generalization to unseen low-resource languages by increasing lexical coverage and reducing noise. Panlex provides word translations across languages; projected sentiment scores are filtered to retain only reliable translations based on model prediction confidence.

### Mechanism 3
Lexicon-based pretraining outperforms models fine-tuned on English sentence data in zero-shot settings because it avoids language-specific linguistic patterns and focuses on universal lexical sentiment cues. Models trained on English sentiment datasets learn language-specific cues and patterns that may not transfer well.

## Foundational Learning

- **Cross-lingual transfer learning**: Why needed here: The method relies on transferring sentiment knowledge from high-resource to low-resource languages without direct supervision. Quick check: What are the main challenges in zero-shot cross-lingual sentiment analysis?

- **Sentiment lexicon construction and use**: Why needed here: The entire approach is built on leveraging pre-existing multilingual sentiment lexicons instead of training data. Quick check: How are valence scores typically obtained and what do they represent?

- **Multilingual language model pretraining and fine-tuning**: Why needed here: The method extends multilingual models using lexicon-based pretraining, requiring understanding of how these models learn cross-lingual representations. Quick check: What is the difference between pretraining and fine-tuning in the context of multilingual models?

## Architecture Onboarding

- **Component map**: Lexicon ingestion (NRC-VAD, Panlex extensions, filtering module) -> Model pretraining (multilingual models) -> Inference pipeline (average pooling, regression/classification layer) -> Evaluation framework (zero-shot inference on 34 languages)

- **Critical path**: Lexicon → Filtering → Pretraining → Zero-shot Inference → Evaluation

- **Design tradeoffs**: 
  - Regression vs. classification pretraining: Regression allows finer-grained sentiment prediction but requires threshold tuning for 3-way classification
  - Lexicon coverage vs. quality: Extending with Panlex increases coverage but introduces potential noise
  - Model size vs. performance: Larger models (XLM-R Large, mT5 Large) generally perform better but require more resources

- **Failure signatures**: 
  - Poor performance on unseen languages: Likely due to insufficient lexical overlap or poor translation quality
  - Inconsistent results across classification vs. regression: May indicate issues with threshold selection or model architecture
  - Overfitting to lexicon during pretraining: Could lead to poor generalization to sentence-level data

- **First 3 experiments**:
  1. Compare vanilla multilingual model vs. lexicon-based pretraining on a high-resource language to establish baseline improvement
  2. Test lexicon filtering effectiveness by comparing performance with and without filtering on a low-resource language
  3. Evaluate zero-shot performance on code-switched text to assess robustness to mixed-language input

## Open Questions the Paper Calls Out

### Open Question 1
How would lexicon-based pretraining perform on aspect-based sentiment analysis tasks in low-resource languages? The authors acknowledge that their research focuses on general sentiment analysis and note that aspect-based sentiment analysis is a more fine-grained approach that warrants further exploration.

### Open Question 2
How does the quality of machine-translated sentiment lexicons impact the performance of lexicon-based pretraining? The authors note that using machine translation systems for translating lexicons may introduce errors in both translation and sentiment scoring, and suggest a comprehensive error analysis could offer valuable insights.

### Open Question 3
Would incorporating dominance and arousal scores from the NRC-VAD lexicon improve the performance of lexicon-based pretraining? The authors acknowledge that their lexicon-based pretraining is solely based on valence scores and suggest that exploring the inclusion of dominance and arousal scores is an intriguing avenue for future research.

## Limitations

- The filtering strategy uses English-centric thresholds that may not generalize well to truly low-resource languages
- The method relies on the quality of machine translations from Panlex, which may introduce errors
- Performance on nuanced sentiment expressions (sarcasm, mixed sentiments) is not evaluated

## Confidence

- **High Confidence**: Lexicon-based pretraining outperforms models fine-tuned on English sentence data in zero-shot settings; Extended lexicon filtering improves performance on low-resource languages; Zero-shot performance on code-mixed text is competitive with supervised models
- **Medium Confidence**: Valence scores correlate strongly with sentence-level sentiment polarity across languages; Panlex translations are sufficiently accurate for sentiment projection
- **Low Confidence**: The filtering strategy effectively removes noise without losing useful sentiment information; Lexicon-based pretraining generalizes better than large language models like GPT-3.5 and BLOOMZ

## Next Checks

1. Systematically vary the filtering threshold (α) and minimum word addition threshold (β) to determine their impact on performance across different language families and resource levels.

2. Evaluate the filtering strategy by training regression models on non-English languages and comparing performance to the English-centric approach, particularly for languages with significant linguistic distance from English.

3. Test the models on datasets requiring nuanced sentiment understanding (e.g., detecting sarcasm, mixed sentiments, or context-dependent polarity) to assess limitations beyond binary/3-way classification.