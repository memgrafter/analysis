---
ver: rpa2
title: On Pre-training of Multimodal Language Models Customized for Chart Understanding
arxiv_id: '2407.14506'
source_url: https://arxiv.org/abs/2407.14506
tags:
- data
- chart
- answer
- value
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the fundamental gap between natural image-caption
  pre-training data and digital chart image-QA data in Multimodal Large Language Models
  (MLLMs). The core method involves a 3-stage training process: (1) incorporating
  raw data values in alignment pre-training, (2) randomly replacing chart images with
  textual representations during fine-tuning to transfer language reasoning to chart
  interpretation, and (3) requiring the model to first extract underlying chart data
  before answering questions.'
---

# On Pre-training of Multimodal Language Models Customized for Chart Understanding

## Quick Facts
- **arXiv ID:** 2407.14506
- **Source URL:** https://arxiv.org/abs/2407.14506
- **Reference count:** 0
- **Primary result:** 52.28% accuracy on ChartQA (human split), 83.63% F1 on Chart-to-Table, 11.50 BLEU-4 on Chart-to-Text, 30.06% accuracy on PlotQA

## Executive Summary
This paper addresses the fundamental gap between natural image-caption pre-training data and digital chart image-QA data in Multimodal Large Language Models (MLLMs). The core contribution is a 3-stage training process that improves chart understanding by incorporating raw data values during pre-training, randomly replacing chart images with textual representations during fine-tuning, and requiring the model to first extract underlying chart data before answering questions. The proposed CHOPIN LLM model achieves strong performance across four chart understanding benchmarks while maintaining robust reasoning capabilities.

## Method Summary
The paper proposes a 3-stage training process for improving chart understanding in MLLMs. Stage 1 involves feature alignment pre-training where chart-description and chart-JSON pairs are incorporated to improve comprehension of visual data representations. Stage 2 uses end-to-end fine-tuning with randomly replaced chart images with textual data to transfer language reasoning to chart interpretation. Stage 3 adds data extraction tasks to the fine-tuning process, requiring the model to first extract underlying chart data before answering questions. The approach is evaluated on ChartQA, Chart-to-Table, Chart-to-Text, and PlotQA benchmarks.

## Key Results
- CHOPIN achieves 52.28% accuracy on ChartQA (human split)
- CHOPIN achieves 83.63% F1 score on Chart-to-Table
- CHOPIN achieves 11.50 BLEU-4 on Chart-to-Text
- CHOPIN achieves 30.06% accuracy on PlotQA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating raw data values in alignment pre-training markedly improves comprehension of chart data
- **Mechanism:** The model learns to directly associate visual chart representations with their underlying numerical data structures
- **Core assumption:** Visual features of charts contain sufficient information to recover underlying data values when properly aligned with textual representations
- **Evidence anchors:** Abstract mentions "Incorporating raw data values in alignment pre-training markedly improves comprehension of chart data"
- **Break condition:** If charts are highly stylized or use non-standard visual encodings that obscure the relationship between visual features and data values

### Mechanism 2
- **Claim:** Randomly replacing chart images with textual representations during end-to-end fine-tuning transfers language reasoning capability to chart interpretation skills
- **Mechanism:** By alternating between visual and text-only inputs during training, the model learns to apply its reasoning skills learned from text to visual chart understanding
- **Core assumption:** Reasoning capabilities developed through text-only training can be transferred to multimodal scenarios when properly integrated
- **Evidence anchors:** Abstract states "Replacing images with their textual representation randomly, during end-to-end fine-tuning, transfers the language reasoning to chart interpretation skills"
- **Break condition:** If reasoning patterns for text and charts are fundamentally different in ways that prevent effective transfer

### Mechanism 3
- **Claim:** Requiring the model to first extract underlying chart data before answering questions further improves reasoning skills
- **Mechanism:** This creates a two-stage reasoning process where the model must first decode visual information into structured data, then apply reasoning to that data
- **Core assumption:** Structured data extraction is a necessary intermediate step that improves overall reasoning performance on chart questions
- **Evidence anchors:** Abstract mentions "Requiring the model to first extract the underlying chart data and then answer the question in the fine-tuning can further improve the accuracy"
- **Break condition:** If the extraction step becomes a bottleneck or if the model can learn to reason directly from visual features without intermediate data extraction

## Foundational Learning

- **Concept:** Multimodal feature alignment
  - Why needed here: Charts require bridging visual patterns with numerical/textual data representations
  - Quick check question: Can you explain why standard image-caption pre-training fails for charts?

- **Concept:** Cross-modal reasoning transfer
  - Why needed here: The model needs to apply language reasoning skills to visual chart understanding
  - Quick check question: How does alternating between text and visual inputs help transfer reasoning capabilities?

- **Concept:** Structured data extraction from visual inputs
  - Why needed here: Charts contain complex visual encodings that must be converted to structured numerical data
  - Quick check question: What visual features in charts are most informative for data extraction?

## Architecture Onboarding

- **Component map:** Visual backbone (ViT) -> Projector layer -> LLM for reasoning
- **Critical path:** Pre-training alignment → End-to-end fine-tuning → Downstream adaptation
- **Design tradeoffs:** More complex training pipeline vs. better chart understanding performance
- **Failure signatures:** Poor performance on unannotated charts, inability to extract underlying data, over-reliance on OCR
- **First 3 experiments:**
  1. Compare chart understanding with/without raw data in pre-training
  2. Test reasoning transfer by varying text-only vs. visual input ratios
  3. Measure data extraction accuracy before and after adding extraction tasks

## Open Questions the Paper Calls Out
No open questions are explicitly called out in the paper.

## Limitations
- The approach relies heavily on synthetic data generation, which may not capture real-world chart complexity
- The study focuses on quantitative metrics without detailed qualitative analysis of model reasoning
- Evaluation methodology may be overly strict for non-numeric answers and doesn't address robustness to noisy or non-standard charts

## Confidence
- **High Confidence:** Incorporating raw data values during pre-training improves chart comprehension (supported by performance improvements across all benchmarks)
- **Medium Confidence:** Random image replacement with textual representations during fine-tuning is effective (performance improvements are clear but exact contribution is uncertain)
- **Low Confidence:** Requiring data extraction as intermediate step significantly improves reasoning (empirical support is weakest, no detailed analysis of contribution)

## Next Checks
1. **Cross-dataset generalization test:** Evaluate CHOPIN on previously unseen chart datasets to assess whether improvements generalize beyond training data distribution
2. **Ablation study of training components:** Systematically remove or modify each of the three training stages to quantify individual contributions and identify essential components
3. **Real-world deployment assessment:** Test the model on charts extracted from real documents (PDFs, web pages, business reports) to evaluate practical utility and identify failure modes not apparent in benchmark datasets