---
ver: rpa2
title: The Role of Learning Algorithms in Collective Action
arxiv_id: '2405.06582'
source_url: https://arxiv.org/abs/2405.06582
tags:
- collective
- success
- learning
- algorithms
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work examines how different machine learning algorithms affect
  the success of algorithmic collective action, where a coordinated group aims to
  influence a classifier''s behavior. While prior research focused on Bayes-optimal
  classifiers, this study investigates two key algorithmic properties: distributionally
  robust optimization (DRO) and simplicity bias (as in SGD).'
---

# The Role of Learning Algorithms in Collective Action

## Quick Facts
- arXiv ID: 2405.06582
- Source URL: https://arxiv.org/abs/2405.06582
- Reference count: 40
- Key result: Learning algorithms with distributionally robust optimization or simplicity bias enable coordinated groups to more effectively influence classifier behavior through strategic signal design

## Executive Summary
This paper investigates how different machine learning algorithms affect the success of algorithmic collective action, where a coordinated group aims to influence a classifier's behavior. While prior research focused on Bayes-optimal classifiers, this study examines two key algorithmic properties: distributionally robust optimization (DRO) and simplicity bias (as in SGD). The authors introduce the concept of effective collective size and prove that algorithms like JTT and LfF can increase this metric, thereby amplifying the collective's impact. They also demonstrate that collectives can strategically design signals by exploiting overlooked complex features when the learning algorithm has simplicity bias. These findings highlight the necessity of considering algorithmic properties when studying collective action, as different algorithms enable different strategic approaches for signal design and size optimization.

## Method Summary
The study examines algorithmic collective action through both theoretical analysis and experimental validation. The authors introduce a framework where a collective influences a classifier by contributing data that encodes a specific signal and target label. They analyze how different learning algorithms (ERM, DRO variants like JTT and LfF, and SGD with simplicity bias) respond to this collective influence. The methodology involves implementing collective action strategies on synthetic datasets (2D, LMS-k, k-strips) and standard datasets (CIFAR-10, Waterbirds, MNIST-CIFAR). Experiments vary collective size, signal complexity, and correlation levels to measure success rates. The theoretical component includes proving bounds on effective collective size under different algorithmic regimes and analyzing the interaction between collective strategies and algorithmic biases.

## Key Results
- DRO-based algorithms (JTT, LfF) achieve higher collective action success than standard ERM, especially for smaller collectives, by increasing effective collective size through up-weighting misclassified samples
- Collectives can exploit simplicity bias in SGD by designing signals using overlooked complex features, achieving higher success than signals based on simple features
- CVaR-DRO algorithms are particularly sensitive to the collective's size in the validation set, making them vulnerable to manipulation when the collective influences both training and validation data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DRO algorithms increase the effective collective size, amplifying the collective's influence on the classifier.
- Mechanism: DRO algorithms up-weight samples that are misclassified by a preliminary classifier, increasing the relative impact of the collective's data.
- Core assumption: The collective's data is initially misclassified by standard ERM, leading to higher effective size under DRO.
- Evidence anchors:
  - [abstract]: "DRO-based algorithms can achieve higher success than standard ERM, especially for smaller collectives."
  - [section]: "When the data is composed of multiple sub-populations, algorithms that optimise for average performance often perform poorly in minority sub-populations."
  - [corpus]: "Fairness without demographics in repeated loss minimization" suggests DRO's role in protecting minority groups, aligning with the paper's focus on collective action impact.
- Break condition: If the collective's data is already well-classified by ERM, the up-weighting effect diminishes, reducing the advantage of DRO algorithms.

### Mechanism 2
- Claim: Simplicity-biased algorithms can be exploited by the collective to design more effective signals.
- Mechanism: Simplicity-biased algorithms overlook complex features, making signals embedded in these features more unique and impactful.
- Core assumption: The collective can identify and leverage features that the algorithm overlooks due to its simplicity bias.
- Evidence anchors:
  - [abstract]: "collectives can strategically design signals by exploiting overlooked complex features."
  - [section]: "We demonstrate that these overlooked features can be leveraged by a collective to design a strategy that will gain higher success."
  - [corpus]: "The pitfalls of simplicity bias in neural networks" directly supports the concept of simplicity bias in algorithms.
- Break condition: If the algorithm's simplicity bias is not strong enough or if the collective cannot identify suitable complex features, the strategy may fail.

### Mechanism 3
- Claim: CVaR-DRO algorithms are highly sensitive to the collective's size in the validation set, allowing the collective to influence the stopping criterion.
- Mechanism: CVaR-DRO oscillates between fitting different parts of the data, and the stopping criterion based on validation set accuracy makes it vulnerable to manipulation by the collective.
- Core assumption: The collective can influence both the training and validation sets, affecting the stopping point of the algorithm.
- Evidence anchors:
  - [abstract]: "When the collective can influence both the training and validation set, we show that this stopping criteria makes them particularly sensitive to the collective’s size in the validation set."
  - [section]: "When the firm is trying to maximize general accuracy, the collective has no success."
  - [corpus]: Limited direct evidence in the corpus; this is a novel finding of the paper.
- Break condition: If the firm uses a different stopping criterion or if the collective cannot influence the validation set, this strategy becomes ineffective.

## Foundational Learning

- Concept: Distributionally Robust Optimization (DRO)
  - Why needed here: Understanding DRO is crucial for grasping how certain algorithms can amplify the collective's impact.
  - Quick check question: What is the primary goal of DRO algorithms in the context of collective action?

- Concept: Simplicity Bias in Machine Learning Algorithms
  - Why needed here: Recognizing simplicity bias helps in understanding how collectives can design more effective signals.
  - Quick check question: How does simplicity bias in algorithms like SGD affect the learning of complex features?

- Concept: Effective Collective Size
  - Why needed here: This concept is key to understanding how DRO algorithms can increase the collective's influence.
  - Quick check question: How does the effective collective size differ from the actual collective size under weighted distributions?

## Architecture Onboarding

- Component map:
  Base Distribution (P0) -> Collective Distribution (P*) -> Mixture Distribution (Pα) -> Learning Algorithm -> Classifier -> Success Metric

- Critical path:
  1. Collective designs signal g(x) and target label y*
  2. Collective influences data distribution to create Pα
  3. Learning algorithm trains classifier on Pα
  4. Classifier's performance on collective's signal is evaluated

- Design tradeoffs:
  - Tradeoff between collective size (α) and signal uniqueness (ξ)
  - Choice of learning algorithm affects the optimal strategy for signal design
  - Balancing between exploiting algorithmic biases and maintaining signal effectiveness

- Failure signatures:
  - Low success despite large collective size (α)
  - Classifier learning the collective's signal too quickly, reducing its impact
  - Algorithm's bias not aligning with the collective's signal design strategy

- First 3 experiments:
  1. Implement JTT and LfF algorithms on a synthetic 2D dataset and compare success against ERM
  2. Test the impact of collective size in the validation set on CVaR-DRO performance
  3. Experiment with different signal designs on MNIST-CIFAR dataset, leveraging simplicity bias of SGD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the success of a collective vary when using different algorithmic properties beyond DRO and simplicity bias?
- Basis in paper: [explicit] The authors acknowledge that other algorithmic properties could impact collective success but focus only on DRO and simplicity bias.
- Why unresolved: The paper only examines two specific algorithmic properties and does not explore how other properties might influence collective success.
- What evidence would resolve it: Experiments testing collective success against various other algorithmic properties (e.g., adversarial robustness, differential privacy) would provide insights into how these properties affect collective action.

### Open Question 2
- Question: What is the optimal balance between the collective's size and the complexity of the signal for maximizing success?
- Basis in paper: [inferred] The paper discusses how the collective's size and the signal's complexity interact with algorithmic properties, but does not provide a clear guideline for finding the optimal balance.
- Why unresolved: The relationship between the collective's size and the signal's complexity is complex and likely depends on the specific algorithmic property being exploited.
- What evidence would resolve it: A comprehensive study exploring different combinations of collective sizes and signal complexities across various algorithmic properties would reveal the optimal balance for maximizing success.

### Open Question 3
- Question: How does the presence of multiple collectives with different goals impact the success of each collective?
- Basis in paper: [inferred] The paper focuses on a single collective with a specific goal, but does not consider the scenario where multiple collectives with different objectives coexist.
- Why unresolved: The interaction between multiple collectives and their impact on each other's success is a complex issue that requires further investigation.
- What evidence would resolve it: Experiments simulating scenarios with multiple collectives competing for influence over the same algorithm would shed light on how their interactions affect individual success.

## Limitations

- The theoretical analysis focuses primarily on binary classification settings, which may not generalize to multi-class problems common in real-world applications
- The experimental validation relies heavily on synthetic datasets, raising questions about external validity despite some real-world grounding with CIFAR-10
- The paper does not fully address the computational overhead introduced by DRO algorithms in practical deployments

## Confidence

**High Confidence:** The core mechanism demonstrating that DRO algorithms increase effective collective size through up-weighting misclassified samples is well-supported by both theory and experiments. The mathematical formulation of effective collective size is rigorous and the empirical validation on synthetic datasets is convincing.

**Medium Confidence:** The findings on simplicity-biased algorithms and signal design show promise but rely more heavily on synthetic experiments. The claim that overlooked complex features can be exploited requires further validation on more diverse real-world datasets.

**Low Confidence:** The analysis of CVaR-DRO sensitivity to validation set composition is the most speculative, with limited experimental evidence and minimal support from related work in the corpus.

## Next Checks

1. **Multi-class Extension:** Extend the theoretical framework to multi-class classification settings and validate whether DRO algorithms maintain their effectiveness advantage over ERM when scaling beyond binary problems.

2. **Real-world Dataset Validation:** Replicate the signal design experiments using more complex, real-world datasets (e.g., ImageNet variants) to assess whether simplicity bias exploitation generalizes beyond the MNIST-CIFAR synthetic construct.

3. **Computational Overhead Analysis:** Quantify the practical computational costs of DRO algorithms compared to ERM in the collective action setting, including both training time and memory requirements across different dataset scales.