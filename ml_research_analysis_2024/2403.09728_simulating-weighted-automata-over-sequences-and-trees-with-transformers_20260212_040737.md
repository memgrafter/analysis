---
ver: rpa2
title: Simulating Weighted Automata over Sequences and Trees with Transformers
arxiv_id: '2403.09728'
source_url: https://arxiv.org/abs/2403.09728
tags:
- automata
- layers
- attention
- such
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes that transformers can compactly simulate
  weighted finite automata (WFAs) and weighted tree automata (WTAs), extending prior
  results on DFAs. It introduces formal definitions of exact and approximate simulation
  and proves that transformers can simulate WFAs at sequence length T with O(log T)
  layers using hard attention and bilinear layers for exact simulation, and O(log
  T) layers with standard softmax attention and MLP for approximate simulation with
  arbitrary precision.
---

# Simulating Weighted Automata over Sequences and Trees with Transformers

## Quick Facts
- arXiv ID: 2403.09728
- Source URL: https://arxiv.org/abs/2403.09728
- Reference count: 40
- Transformers can simulate WFAs and WTAs with O(log T) layers for exact simulation and O(log T) layers for approximate simulation

## Executive Summary
This paper establishes that transformers can compactly simulate weighted finite automata (WFAs) and weighted tree automata (WTAs), extending prior results on deterministic finite automata (DFAs). The key insight is leveraging the parallel prefix scan algorithm, which can be implemented in O(log T) layers using attention mechanisms. The paper proves that transformers can exactly simulate WFAs using hard attention and bilinear layers, or approximately simulate them with softmax attention and MLPs to arbitrary precision. For WTAs, the paper shows O(log T) layers suffice for balanced trees and O(T) layers for arbitrary trees. Empirical results on synthetic data demonstrate transformers can learn these compact solutions via gradient-based training, with MSE improving as layer count approaches theoretical thresholds.

## Method Summary
The paper proves transformers can simulate WFAs using a recursive parallel scan algorithm that computes all T prefixes in O(log T) time. For exact simulation, hard attention selects elements at positions determined by binary representations, while bilinear layers compute matrix products between transition maps. For approximate simulation, softmax attention approximates hard attention through saturation, and MLPs approximate matrix multiplication using polynomial approximations. WTA simulation leverages attention to select left and right subtrees, with MLPs approximating bilinear maps for combining subtree states. Experiments use synthetic data from counting WFAs and Potomac dataset WFAs, trained with PyTorch's TransformerEncoder, AdamW optimizer, and MSE loss.

## Key Results
- Transformers can exactly simulate all WFAs with n states at sequence length T using O(log T) layers with hard attention and bilinear layers
- Transformers can approximately simulate all WFAs with n states at sequence length T up to arbitrary precision with O(log T) layers using standard softmax attention and MLP
- For WTAs, transformers can simulate them with O(log T) layers for balanced trees and O(T) layers for arbitrary trees
- Empirical results show MSE improves notably as number of layers approaches theoretical thresholds (log₂ T), though embedding size scaling is less consistent

## Why This Works (Mechanism)

### Mechanism 1: Exact WFA Simulation with Hard Attention
Transformers implement the parallel prefix algorithm by storing transition maps in embeddings and using hard attention to "shift" copies by powers of two. Bilinear layers then compute matrix products between transition maps. This requires O(log T) layers since each layer can double the range of prefix computations.

Core assumption: WFA transition matrices can be composed efficiently through parallel prefix sums, and hard attention can implement exact selection without approximation error.

### Mechanism 2: Approximate WFA Simulation with Softmax Attention
Softmax attention approximates hard attention through saturation behavior, while MLPs approximate matrix multiplication using polynomial approximations. Error analysis shows total error remains bounded through recursive application, allowing arbitrary precision simulation.

Core assumption: Matrix multiplication can be approximated by MLPs with sufficient width, and attention errors can be controlled through saturating constants.

### Mechanism 3: WTA Simulation with Tree Structure Encoding
For each node, attention selects left and right subtrees based on encoded tree structure, and MLP layers approximate the bilinear map for combining subtree states. Depth calculation uses positional information and marker tracking to identify tree boundaries.

Core assumption: Tree structure can be encoded in string representation, and bilinear maps can be approximated by MLPs with constant depth relative to tree depth.

## Foundational Learning

- **Concept**: Recursive parallel scan algorithm
  - Why needed here: Enables computing all prefixes in logarithmic depth, the core insight for WFA simulation
  - Quick check question: Given a sequence of 8 numbers, how many parallel prefix sum steps are needed to compute all partial sums?

- **Concept**: Bilinear layers and their relationship to matrix multiplication
  - Why needed here: Bilinear layers implement composition of transition matrices in WFAs and combination of subtree states in WTAs
  - Quick check question: What tensor dimensions are needed to compute vec(AB) = T ×1 vec(A) ×2 vec(B) for n×n matrices?

- **Concept**: Error propagation in approximate computation
  - Why needed here: Understanding how approximation errors accumulate through recursive computation is crucial for the soft attention and MLP approach
  - Quick check question: If each layer introduces error ε, what is the total error after L recursive applications?

## Architecture Onboarding

- **Component map**: Input embeddings → Attention mechanism → Bilinear/MLP layers → Output layer
- **Critical path**: 1) Initialize embeddings with transition maps/leaf states, 2) Apply attention for parallel prefix shifting, 3) Apply bilinear/MLP layers for compositions, 4) Repeat for O(log T) layers, 5) Map final outputs to desired states
- **Design tradeoffs**: Hard vs soft attention (exact vs differentiable), bilinear vs standard MLP (exact vs flexible), embedding size (precision vs parameters)
- **Failure signatures**: MSE plateaus below target despite increasing layers (attention mechanism issue), MSE decreases slowly with layers (insufficient MLP approximation), training diverges (attention saturation constant too large)
- **First 3 experiments**: 1) Test WFA simulation on simple counting automaton with varying layer counts, 2) Test WTA simulation on balanced binary tree to verify O(log T) layer requirement, 3) Compare hard vs soft attention performance on same WFA task

## Open Questions the Paper Calls Out

- Can transformer models trained on downstream tasks naturally implement the algorithmic reasoning used in the proposed constructions for simulating WFAs and WTAs? This requires empirical investigation of trained models on real-world tasks.
- Are there lower bounds on the depth and width required for transformers to simulate WFAs and WTAs that are tight with the upper bounds proven in this paper? Proving lower bounds requires different techniques than those used for upper bounds.
- How does sample complexity and optimization procedure affect the ability to find shortcut solutions for WFA and WTA simulation? The experiments use synthetic data with fixed dataset sizes, not exploring how these factors scale.

## Limitations

- Theoretical analysis relies on idealized assumptions about attention and approximation capabilities, with gaps between theory and practical implementation
- Empirical validation is limited to synthetic data and relatively small WFAs (up to 100 states), without demonstrating scalability to larger automata or real-world applications
- Embedding size scaling analysis shows less consistent empirical results than theoretical predictions, with incomplete validation of the O(n²) bound

## Confidence

**High Confidence**: Exact simulation using hard attention and bilinear layers, as the parallel prefix algorithm is well-established and transformer implementation follows directly.

**Medium Confidence**: Approximation analysis for softmax attention and MLPs, as error propagation analysis is sound but practical saturation behavior introduces uncertainties.

**Low Confidence**: WTA simulation results for arbitrary trees, as O(T) layer complexity is expected but practical implementation details and empirical validation are limited.

## Next Checks

1. Test transformer simulation on WFAs that count non-regular languages (e.g., prime numbers, palindromes) to determine limits of compact simulation.

2. Scale up WFA state count beyond 100 and measure how layer count and embedding size scale empirically compared to theoretical predictions.

3. Systematically compare different MLP architectures, attention variants, and activation functions to identify which approaches best approximate theoretical requirements for compact simulation.