---
ver: rpa2
title: On Dissipativity of Cross-Entropy Loss in Training ResNets
arxiv_id: '2405.19013'
source_url: https://arxiv.org/abs/2405.19013
tags:
- training
- cross-entropy
- control
- loss
- dissipativity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the training of residual neural networks (ResNets)
  and neural ordinary differential equations (ODEs) from an optimal control perspective.
  It introduces a dissipative formulation of the training problem for classification
  tasks by including a variant of the cross-entropy loss as a regularization term
  in the stage cost.
---

# On Dissipativity of Cross-Entropy Loss in Training ResNets
## Quick Facts
- arXiv ID: 2405.19013
- Source URL: https://arxiv.org/abs/2405.19013
- Reference count: 5
- Key outcome: Proves strict dissipativity of soft cross-entropy loss in ResNet training, leading to turnpike phenomenon and optimal network depth determination.

## Executive Summary
This paper introduces a dissipative formulation of ResNet and neural ODE training for classification tasks by incorporating soft cross-entropy loss as a regularization term. The key insight is that soft cross-entropy exhibits properties similar to the Huber loss—being locally quadratic and asymptotically linear—which enables proving strict dissipativity of the training problem. As a consequence, trained networks exhibit the turnpike phenomenon where trajectories spend most of their time near optimal steady states, allowing determination of minimal required network depth without additional hyperparameter tuning. The approach is validated on two-spirals and MNIST datasets.

## Method Summary
The method frames ResNet training as an optimal control problem where network parameters are control inputs. Soft cross-entropy loss with smoothed target probabilities is included as a regularization term in the stage cost, creating a dissipative formulation. The training uses Adam optimizer with specific learning rates and weight decay parameters, applying label smoothing with tuned parameters for different datasets. The approach leverages the geometry of soft cross-entropy minimizers to prove strict dissipativity, which in turn guarantees the turnpike phenomenon in trained networks.

## Key Results
- Proves soft cross-entropy loss is strictly dissipative with respect to the set of minimizers
- Demonstrates turnpike phenomenon in trained ResNets on two-spirals and MNIST datasets
- Shows approach enables determining optimal network depth without additional hyperparameter tuning

## Why This Works (Mechanism)

### Mechanism 1
The soft cross-entropy loss provides locally quadratic, asymptotically linear behavior similar to the Huber loss, enabling strict dissipativity. By using smoothed target probabilities instead of binary targets, the loss avoids pushing optimal steady states to infinity, creating a bounded set of minimizers. This satisfies strict dissipativity conditions with a constant storage function. Core assumption: Soft cross-entropy is convex and satisfies second-order sufficient conditions. Break condition: If smoothing parameter pd is too close to 1, beneficial properties for dissipativity may be lost.

### Mechanism 2
Strict dissipativity with respect to soft cross-entropy minimizers leads to the turnpike phenomenon. This ensures optimal trajectories spend most time near the set of minimizers, converging to and remaining close to optimal steady states for most of the time horizon. Core assumption: System satisfies exponential reachability condition. Break condition: If exponential reachability is not satisfied, turnpike phenomenon may not occur.

### Mechanism 3
Dissipative formulation allows determining required network depth without additional hyperparameter tuning. Including soft cross-entropy as regularization creates optimal steady states corresponding to minimal required depth. Layers beyond this depth don't contribute to transformation and can be removed. Core assumption: Optimal steady states correspond to minimal required depth. Break condition: If optimal steady states don't correspond to minimal depth, determined depth may be inaccurate.

## Foundational Learning

- **Optimal control formulation of deep learning**: Why needed - frames ResNet training as control problem with parameters as inputs. Quick check - How does this differ from traditional optimization view?
- **Dissipativity and turnpike properties**: Why needed - enables proving turnpike phenomenon guarantees. Quick check - What's the relationship between dissipativity and turnpike in optimal control?
- **Soft cross-entropy loss properties**: Why needed - crucial for dissipativity analysis. Quick check - How does soft cross-entropy differ from standard and what are its advantages?

## Architecture Onboarding

- **Component map**: Data (features, labels) -> ResNet/Neural ODE -> Soft cross-entropy loss -> Adam optimizer -> Stage cost (loss + regularization)
- **Critical path**: Initialize parameters → Propagate data → Compute soft cross-entropy → Backpropagate gradients → Update parameters → Repeat
- **Design tradeoffs**: Smoothing parameter pd (affects minimizer geometry), weight decay r (fitting vs regularization), terminal penalty γ (stage cost vs final loss balance)
- **Failure signatures**: Training loss not decreasing (check LR, weight decay, architecture), overfitting (increase regularization), vanishing gradients (check activations, depth)
- **First 3 experiments**: 1) Train ResNet on two-spirals with varying depths to observe turnpike, 2) Compare standard vs soft cross-entropy performance, 3) Study effect of pd on minimizer geometry and turnpike

## Open Questions the Paper Calls Out

### Open Question 1
Can turnpike phenomenon be observed in architectures beyond ResNets like CNNs or transformers? The paper mentions extension to other architectures but lacks experimental validation for CNNs or transformers. What evidence would resolve it: Experiments training CNNs or transformers on classification tasks, analyzing trajectories for turnpike phenomenon.

### Open Question 2
How does activation function choice affect dissipativity and turnpike properties? The paper uses tanh and ReLU but doesn't systematically study different activation functions' effects. What evidence would resolve it: Experiments with different activations (sigmoid, leaky ReLU, GELU) analyzing dissipativity and turnpike behavior.

### Open Question 3
Can turnpike phenomenon improve generalization on unseen data? The paper mentions finding shallow networks but doesn't explore generalization implications. What evidence would resolve it: Compare test accuracy and generalization of networks trained with and without turnpike-inducing regularization.

## Limitations
- Theoretical claims rely heavily on strict dissipativity of soft cross-entropy requiring careful verification
- Connection between dissipativity and turnpike in deep learning context needs more empirical validation
- Practical effectiveness of determining optimal network depth needs extensive validation across diverse datasets and architectures

## Confidence
- **High confidence**: Optimal control formulation of ResNet training and basic concept of soft cross-entropy loss
- **Medium confidence**: Strict dissipativity of soft cross-entropy and its relationship to turnpike phenomenon
- **Low confidence**: Practical effectiveness of determining optimal network depth through this approach

## Next Checks
1. **Empirical verification of turnpike phenomenon**: Design controlled experiments with varying network depths to empirically observe and quantify turnpike phenomenon in trained ResNets using soft cross-entropy loss
2. **Sensitivity analysis of smoothing parameter**: Systematically vary smoothing parameter pd and analyze its effect on minimizer geometry, strictness of dissipativity, and resulting turnpike behavior
3. **Generalization across architectures and datasets**: Apply dissipative formulation to different neural network architectures (e.g., convolutional networks) and diverse datasets to assess robustness and generality of theoretical findings