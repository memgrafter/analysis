---
ver: rpa2
title: 'AXIS: Efficient Human-Agent-Computer Interaction with API-First LLM-Based
  Agents'
arxiv_id: '2409.17140'
source_url: https://arxiv.org/abs/2409.17140
tags:
- control
- skill
- task
- agent
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AXIS introduces an API-first framework for LLM-based agents that
  prioritizes API calls over UI interactions to reduce latency and improve reliability
  in human-agent-computer interaction. The system automatically explores applications,
  generates new skills from interaction trajectories, and validates them to ensure
  correctness.
---

# AXIS: Efficient Human-Agent-Computer Interaction with API-First LLM-Based Agents

## Quick Facts
- arXiv ID: 2409.17140
- Source URL: https://arxiv.org/abs/2409.17140
- Authors: Junting Lu; Zhiyang Zhang; Fangkai Yang; Jue Zhang; Lu Wang; Chao Du; Qingwei Lin; Saravan Rajmohan; Dongmei Zhang; Qi Zhang
- Reference count: 37
- Primary result: Reduces task completion time by 65%-70% and cognitive workload by 38%-53% compared to manual operation while maintaining 97%-98% accuracy

## Executive Summary
AXIS introduces an API-first framework for LLM-based agents that prioritizes API calls over UI interactions to reduce latency and improve reliability in human-agent-computer interaction. The system automatically explores applications, generates new skills from interaction trajectories, and validates them to ensure correctness. Experiments on Microsoft Word show AXIS achieves significant efficiency gains over traditional UI agent methods while maintaining high reliability and task completion rates.

## Method Summary
AXIS employs a three-stage pipeline: trajectory collection through Follower and Explorer agents, skill generation using Monitor, Generator, and Translator agents, and skill validation through static and dynamic checks. The framework maintains a hierarchical skill library and uses a unified skill executor interface to handle both UI and API actions. The system explores applications to discover available APIs, generates executable skill code from interaction trajectories, and validates skills to ensure correctness before deployment.

## Key Results
- Reduces task completion time by 65%-70% compared to manual operation
- Reduces cognitive workload by 38%-53% while maintaining 97%-98% accuracy
- Demonstrates significant efficiency gains over traditional UI-based agents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AXIS reduces latency by prioritizing API calls over UI interactions, which require fewer sequential LLM calls.
- Mechanism: UI-based agents need one LLM call per UI interaction step, while API-based agents can complete tasks with single calls. AXIS automatically identifies available APIs and constructs new ones, allowing tasks that normally require multiple UI steps to be completed with a single API call.
- Core assumption: API calls are more efficient than UI interactions because they bypass the need for visual processing and sequential decision-making at each step.
- Evidence anchors:
  - [abstract]: "AXIS introduces an API-first framework for LLM-based agents that prioritizes API calls over UI interactions to reduce latency and improve reliability"
  - [section 2.1]: "APIs offer a more efficient alternative by reducing unnecessary UI steps"
  - [corpus]: Weak - the corpus papers discuss API-based agents but don't provide specific latency measurements or comparisons to UI agents
- Break condition: If APIs are not available for critical functions, AXIS must fall back to UI interactions, losing the latency advantage.

### Mechanism 2
- Claim: AXIS improves reliability by reducing the compounding error problem in long sequential UI interactions.
- Mechanism: Each UI interaction step in traditional agents requires LLM reasoning, and errors compound with each step. AXIS minimizes these steps by using API calls that can complete entire tasks in one call, reducing the probability of error accumulation.
- Core assumption: The probability of error increases with each sequential LLM call, so reducing the number of calls improves overall reliability.
- Evidence anchors:
  - [abstract]: "AXIS reduces task completion time by 65%-70% and cognitive workload by 38%-53%, while maintaining accuracy of 97%-98% compared to humans"
  - [section 1]: "LLMs are prone to hallucinations in generating responses and selecting correct UIs when a long chain of UI interactions is required"
  - [corpus]: Weak - corpus papers discuss efficiency improvements but don't specifically address the compounding error problem in sequential UI interactions
- Break condition: If the API-first approach requires multiple nested API calls that are complex to coordinate, the reliability advantage may diminish.

### Mechanism 3
- Claim: AXIS reduces cognitive workload by abstracting complex multi-step UI interactions into single API calls.
- Mechanism: Users no longer need to mentally track multiple UI navigation steps; instead, they can express tasks in natural language and let the API handle the complexity. This abstraction reduces the mental effort required to complete tasks.
- Core assumption: Mental effort scales with the number of steps and decisions required in a task, so reducing steps reduces cognitive load.
- Evidence anchors:
  - [abstract]: "AXIS reduces cognitive workload by 38%-53% while maintaining accuracy of 97%-98% compared to humans"
  - [section 1]: "to use a new application effectively, users must first spend time familiarizing themselves with the user interface (UI) and its functionalities, increasing users' cognitive burden"
  - [corpus]: Weak - corpus papers discuss efficiency but don't provide specific cognitive load measurements or comparisons
- Break condition: If API calls require complex parameter specification that exceeds the cognitive effort of UI navigation, the cognitive load benefit disappears.

## Foundational Learning

- Concept: LLM-based UI agents and their limitations
  - Why needed here: Understanding the baseline approach (UI agents) and why it fails (latency, reliability, cognitive load) is essential to appreciate AXIS's innovations
  - Quick check question: Why do traditional UI agents require multiple LLM calls for a single task, and how does this create problems?

- Concept: API-first design patterns and skill hierarchies
  - Why needed here: AXIS's core innovation is prioritizing APIs over UI actions and creating a hierarchy of skills (atomic, composite, hybrid) to represent different levels of functionality
  - Quick check question: What are the five types of skills in AXIS, and how do they differ in terms of composition and hierarchy?

- Concept: Automated exploration and skill generation
  - Why needed here: AXIS's ability to automatically explore applications, generate new skills from interaction trajectories, and validate them is central to its functionality
  - Quick check question: What are the three stages of AXIS's skill generation process, and what role does each agent play?

## Architecture Onboarding

- Component map:
  - Environment interface -> state() and step() methods for observing and interacting with applications
  - Skill repository -> hierarchical skill library with descriptions, code, and examples
  - Three-stage pipeline -> trajectory collection (Follower/Explorer modes), skill generation (Monitor/Generator/Translator agents), skill validation (static/dynamic)
  - Skill executor -> executes both UI and API actions through a unified interface
  - LLM backend -> GPT-4o for reasoning and code generation

- Critical path:
  1. Receive task description from user
  2. Select appropriate skill from repository
  3. Execute skill through skill executor
  4. Return result to user

- Design tradeoffs:
  - API-first vs UI fallback: Prioritizing APIs for efficiency but maintaining UI capabilities for completeness
  - Exploration depth vs breadth: Balancing thorough API discovery with exploration efficiency
  - Static vs dynamic validation: Ensuring code correctness while validating actual functionality

- Failure signatures:
  - High latency: Indicates falling back to UI interactions instead of using available APIs
  - Low success rate: Suggests skill generation or validation failures
  - Missing functionality: API discovery may be incomplete for the target application

- First 3 experiments:
  1. Implement basic skill executor and test with simple Word API calls (text selection, formatting)
  2. Create Follower agent to extract tasks from Word documentation and collect interaction trajectories
  3. Implement skill validation pipeline and test with generated skills on sample documents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AXIS perform when applied to applications beyond Microsoft Word, particularly those with complex APIs or limited documentation?
- Basis in paper: [inferred] The paper focuses exclusively on Microsoft Word as a test case, mentioning that AXIS could theoretically extend to other applications with "basic API and documentation support" but does not demonstrate this.
- Why unresolved: The feasibility study only evaluates AXIS on Microsoft Word tasks. There's no evidence of how the framework handles applications with different UI structures, API availability, or documentation quality.
- What evidence would resolve it: Systematic testing of AXIS across diverse applications (e.g., Excel, PowerPoint, web applications, or applications with minimal APIs) with comparative performance metrics.

### Open Question 2
- Question: What is the computational overhead and resource utilization of the AXIS framework during the exploration and skill generation phases?
- Basis in paper: [inferred] The paper mentions API-first approaches reduce task completion time but doesn't provide data on the computational cost of the exploration, skill generation, and validation processes themselves.
- Why unresolved: While end-user efficiency is measured, the paper doesn't address the computational resources required for AXIS to build its skill library, which could impact its practical deployment.
- What evidence would resolve it: Detailed analysis of processing time, API costs, and computational resources consumed during the exploration and skill generation phases across multiple applications.

### Open Question 3
- Question: How does AXIS handle edge cases where the desired functionality cannot be expressed through existing APIs, requiring fallback to UI interactions?
- Basis in paper: [explicit] The paper states that "Regular UI interactions are only called when the related APIs are unavailable" but doesn't provide examples or metrics on how often this occurs or how the system manages such transitions.
- Why unresolved: The paper claims API-first priority but lacks data on the frequency of API unavailability, the quality of fallback UI interactions, and the impact on overall performance.
- What evidence would resolve it: Empirical data showing the percentage of tasks requiring UI fallbacks, success rates for these fallback interactions, and comparative performance metrics with and without fallbacks.

### Open Question 4
- Question: How does the skill discovery process scale with larger applications that have hundreds or thousands of potential functions and UI elements?
- Basis in paper: [inferred] The paper reports discovering 73 skills from Microsoft Word using 347 seed files, but doesn't address how the framework would handle applications with significantly larger function spaces.
- Why unresolved: The scalability of the exploration process, the potential for exponential growth in skill combinations, and the management of a large skill library are not discussed.
- What evidence would resolve it: Performance benchmarks showing skill discovery rates, memory usage, and validation times as the application complexity increases, along with strategies for managing large skill libraries.

## Limitations

- Evaluation limited to Microsoft Word only, raising questions about generalizability across different application types
- Small user study sample size (10 participants) with tasks potentially biased toward well-documented scenarios
- No data on computational overhead and resource utilization during exploration and skill generation phases

## Confidence

- **High confidence**: The core mechanism of prioritizing API calls over UI interactions for latency reduction is well-supported by the experimental data and aligns with established software engineering principles.
- **Medium confidence**: The 65%-70% task completion time reduction and 38%-53% cognitive workload reduction are specific to the Word environment and task selection, requiring broader validation across applications.
- **Low confidence**: The 97%-98% accuracy maintenance claim needs verification in more complex, real-world scenarios beyond controlled documentation tasks.

## Next Checks

1. **Cross-Application Generalization**: Test AXIS on diverse applications (spreadsheets, presentation software, web browsers) to verify that API-first advantages persist across different UI paradigms and API availability levels.

2. **Complex Task Scenarios**: Evaluate AXIS on tasks requiring nested API calls, conditional logic, or handling of incomplete/malformed API responses to stress-test the skill generation and validation pipeline.

3. **Long-term Reliability Assessment**: Conduct longitudinal studies measuring how skill generation quality evolves over time with continuous exploration, and whether the system maintains accuracy as applications update their APIs or UIs.