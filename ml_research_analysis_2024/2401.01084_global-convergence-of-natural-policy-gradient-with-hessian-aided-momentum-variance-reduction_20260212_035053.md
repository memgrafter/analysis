---
ver: rpa2
title: Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance
  Reduction
arxiv_id: '2401.01084'
source_url: https://arxiv.org/abs/2401.01084
tags:
- policy
- gradient
- lemma
- convergence
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel natural policy gradient method with
  Hessian-aided momentum variance reduction (NPG-HM) for reinforcement learning. The
  key innovation is using Hessian-aided momentum to construct an unbiased gradient
  estimator, avoiding the need for importance sampling and its associated assumptions.
---

# Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction

## Quick Facts
- arXiv ID: 2401.01084
- Source URL: https://arxiv.org/abs/2401.01084
- Authors: Jie Feng; Ke Wei; Jinchi Chen
- Reference count: 40
- Key outcome: NPG-HM achieves global last-iterate ε-optimality with O(ε⁻²) sample complexity under Fisher-non-degenerate policy parameterizations

## Executive Summary
This paper introduces NPG-HM, a novel natural policy gradient method that uses Hessian-aided momentum variance reduction. The key innovation is an unbiased gradient estimator that avoids importance sampling assumptions. Under Fisher-non-degenerate policy parameterizations, NPG-HM achieves global convergence with the best-known sample complexity of O(ε⁻²). The method solves a sub-problem via stochastic gradient descent and demonstrates superior performance compared to state-of-the-art policy gradient methods in Mujoco environments.

## Method Summary
NPG-HM is a natural policy gradient method that uses Hessian-aided momentum for variance reduction. It constructs an unbiased gradient estimator by computing the difference between policy Hessians, avoiding importance sampling. The method solves a strongly convex sub-problem via stochastic gradient descent to compute the NPG update direction. Under Fisher-non-degenerate policy parameterizations, NPG-HM achieves global last-iterate ε-optimality with O(ε⁻²) sample complexity. The implementation uses Adam instead of SGD for the sub-problem solver and neural networks for policy and value function approximation.

## Key Results
- NPG-HM achieves global last-iterate ε-optimality with O(ε⁻²) sample complexity under Fisher-non-degenerate policy parameterizations
- The method demonstrates superior performance compared to MNPG, NPG-SRVR, PPO, and HARPG on Mujoco environments
- Numerical experiments show NPG-HM's effectiveness across six different control tasks (Reacher, Hopper, Walker2d, InvertedPendulum, InvertedDoublePendulum, HalfCheetah)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NPG-HM achieves global last-iterate ε-optimality with O(ε⁻²) sample complexity under Fisher-non-degenerate policy parameterizations.
- Mechanism: Uses Hessian-aided momentum variance reduction to construct an unbiased gradient estimator, avoiding importance sampling and its associated assumptions.
- Core assumption: The policy parameterization is Fisher-non-degenerate (F(θ) ≽ µF Id for some µF > 0) and satisfies compatible function approximation error bounds.
- Evidence anchors:
  - [abstract]: "NPG-HM achieves the global last iterate ε-optimality with a sample complexity of O(ε⁻²), which is the best known result for natural policy gradient type methods under the generic Fisher non-degenerate policy parameterizations."
  - [section 3.2]: "Theorem 3.1... Under Definition 3.1 and Assumption 3.1... E[J*ρ − Jρ(θT)] ≲ E[J*ρ − Jρ(θ1)] / T² + ν²g / T² + κ³/² / (1 − κ⁴d/K) · 1/√T + 1/T³/² + √εbias / (1 − γ)."

### Mechanism 2
- Claim: The Hessian-aided momentum estimator provides an unbiased estimate of the gradient difference without requiring importance sampling.
- Mechanism: Constructs H(ˆτt; ˆθt)(θt − θt−1) as an unbiased estimator of ∇J H ρ(θt) − ∇J H ρ(θt−1) using the integral of policy Hessian.
- Core assumption: The policy Hessian exists and is well-behaved, and the linear combination ˆθt = qθt + (1 − q)θt−1 with q ~ Unif(0,1) is valid.
- Evidence anchors:
  - [section 2.3]: "A simple calculation yields that ∇J H ρ(θt) − ∇J H ρ(θt−1) = E[H(ˆτt; ˆθt)(θt − θt−1)], which implies that H(ˆτt; ˆθt)(θt − θt−1) is an unbiased estimator of ∇J H ρ(θt) − ∇J H ρ(θt−1)."

### Mechanism 3
- Claim: The NPG-SGD subroutine converges to a sufficiently accurate update direction in a constant number of iterations, independent of ε.
- Mechanism: Uses stochastic gradient descent to solve the sub-problem for computing the NPG update direction, with error bounds that decrease as 1/K.
- Core assumption: The sub-problem is strongly convex with a bounded condition number, and the step size is chosen appropriately.
- Evidence anchors:
  - [section 4]: "Lemma 4.8... Suppose K ≥ 48κ⁴(√2d + 1)² and αt ≤ µF/2L... one has ∆t+1 ≤ ∆t − c₁αt/4κ ∆²t + 4αtMg/µ²F Vt − µF/4αt Rt + αtεbias/2κ(1 − γ)² + 4αtMg/µ²F G²gγ²H."

## Foundational Learning

- Concept: Fisher Information Matrix (FIM) and its role in natural policy gradient
  - Why needed here: The FIM serves as a preconditioner in NPG, transforming the gradient to account for the geometry of the policy space.
  - Quick check question: What is the relationship between the FIM and the curvature of the policy space?

- Concept: Compatible function approximation error
  - Why needed here: This error term quantifies how well the advantage function can be approximated using the score function, which is crucial for establishing convergence guarantees.
  - Quick check question: How does the compatible function approximation error affect the sample complexity of NPG methods?

- Concept: Variance reduction techniques in policy gradient methods
  - Why needed here: Variance reduction is essential for improving the sample efficiency of policy gradient methods, and NPG-HM uses a specific momentum-based technique.
  - Quick check question: What are the advantages and disadvantages of using importance sampling versus Hessian-aided momentum for variance reduction?

## Architecture Onboarding

- Component map:
  Trajectory sampler -> Gradient estimator -> Momentum updater -> NPG-SGD solver -> Parameter updater

- Critical path:
  1. Sample trajectories τt and ˆτt
  2. Compute gradient estimates g(τt; θt) and H(ˆτt; ˆθt)(θt − θt−1)
  3. Update momentum-based gradient estimate ut
  4. Solve sub-problem using NPG-SGD to get wt
  5. Update parameters θt+1 = θt + αtwt

- Design tradeoffs:
  - Using Hessian-aided momentum vs. importance sampling for variance reduction
  - Trade-off between the number of iterations K in NPG-SGD and the accuracy of the update direction
  - Choice of momentum coefficient βt and learning rate αt schedules

- Failure signatures:
  - If the Fisher information matrix is degenerate (µF = 0), the algorithm may fail to converge
  - If the compatible function approximation error εbias is too large, the convergence guarantees may not hold
  - If the policy Hessian cannot be computed efficiently, the Hessian-aided momentum estimator may become biased

- First 3 experiments:
  1. Test NPG-HM on a simple linear quadratic regulator (LQR) problem to verify convergence
  2. Compare the performance of NPG-HM with other policy gradient methods (e.g., PPO, MNPG) on a continuous control task (e.g., HalfCheetah)
  3. Investigate the effect of the number of iterations K in NPG-SGD on the performance of NPG-HM

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the O(ε^-2) sample complexity bound for NPG-HM be further improved under specific conditions or with different assumptions?
- Basis in paper: [explicit] The paper states that NPG-HM achieves the best known O(ε^-2) sample complexity under Fisher-non-degenerate policy parameterizations, but also mentions that other methods have achieved similar bounds under different assumptions.
- Why unresolved: The paper does not explore whether the O(ε^-2) bound is tight or if it can be improved with different assumptions or techniques.
- What evidence would resolve it: Developing a method that achieves a better sample complexity bound than O(ε^-2) under similar or weaker assumptions would resolve this question.

### Open Question 2
- Question: How does NPG-HM perform in multi-agent reinforcement learning settings compared to other methods?
- Basis in paper: [inferred] The paper focuses on single-agent reinforcement learning and does not discuss multi-agent scenarios. However, it mentions that extending NPG-HM to multi-agent settings is a potential future research direction.
- Why unresolved: The paper does not provide any empirical or theoretical analysis of NPG-HM's performance in multi-agent settings.
- What evidence would resolve it: Conducting experiments or providing theoretical analysis of NPG-HM's performance in multi-agent reinforcement learning problems would resolve this question.

### Open Question 3
- Question: Can the Hessian-aided momentum variance reduction technique be effectively applied to other policy optimization methods beyond NPG?
- Basis in paper: [explicit] The paper introduces the Hessian-aided momentum variance reduction technique specifically for NPG and does not explore its application to other methods.
- Why unresolved: The paper does not investigate whether the benefits of Hessian-aided momentum variance reduction can be generalized to other policy optimization methods.
- What evidence would resolve it: Developing and analyzing the performance of other policy optimization methods that incorporate Hessian-aided momentum variance reduction would resolve this question.

## Limitations
- The method requires efficient computation of policy Hessians, which may be computationally expensive or numerically unstable for complex policy parameterizations.
- Convergence guarantees rely on Fisher-non-degenerate policy parameterizations, which may not hold in practice, particularly for over-parameterized policies or in regions of low entropy.
- The compatible function approximation error plays a crucial role in convergence, but the paper does not provide specific bounds for the neural network architectures used in experiments.

## Confidence
- High Confidence: The theoretical analysis of the NPG-HM algorithm under Fisher-non-degenerate policy parameterizations is well-established, with clear convergence guarantees and sample complexity bounds. The numerical experiments demonstrate the effectiveness of NPG-HM compared to other state-of-the-art methods on standard Mujoco benchmark tasks.
- Medium Confidence: The practical implementation of the NPG-HM algorithm, particularly the Hessian-aided momentum variance reduction estimator and the NPG-SGD subroutine, may require careful tuning and could be sensitive to hyperparameters. The choice of the number of iterations K in NPG-SGD and the learning rate schedules could significantly impact the performance.
- Low Confidence: The extension of NPG-HM to more complex policy parameterizations, such as recurrent neural networks or transformers, is not explored in the paper. The convergence guarantees may not hold for these architectures, and the computational cost of the Hessian-aided momentum estimator could become prohibitive.

## Next Checks
1. **Scalability to Larger Networks**: Investigate the performance of NPG-HM with deeper neural networks (e.g., 2-3 hidden layers) or recurrent architectures. Measure the impact on convergence speed and sample efficiency compared to the one-hidden-layer networks used in the experiments.

2. **Robustness to Hyperparameter Choices**: Conduct a thorough sensitivity analysis of NPG-HM to key hyperparameters, such as the learning rates, momentum coefficients, and the number of iterations in NPG-SGD. Identify the most critical hyperparameters and their impact on the final performance.

3. **Comparison with Other Variance Reduction Techniques**: Compare the Hessian-aided momentum variance reduction with other techniques, such as importance sampling or reward baselines, in terms of convergence speed and sample efficiency. Investigate the trade-offs between variance reduction and computational cost.