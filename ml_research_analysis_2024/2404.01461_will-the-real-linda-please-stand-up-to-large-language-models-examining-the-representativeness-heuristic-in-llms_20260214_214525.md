---
ver: rpa2
title: Will the Real Linda Please Stand up...to Large Language Models? Examining the
  Representativeness Heuristic in LLMs
arxiv_id: '2404.01461'
source_url: https://arxiv.org/abs/2404.01461
tags:
- fallacy
- more
- table
- probability
- heuristic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Representativeness Heuristic AI Testing
  (REHEAT) dataset, designed to evaluate large language models (LLMs) for representativeness
  heuristic biases. The dataset contains 202 questions spanning six types of representativeness
  heuristics.
---

# Will the Real Linda Please Stand up...to Large Language Models? Examining the Representativeness Heuristic in LLMs

## Quick Facts
- arXiv ID: 2404.01461
- Source URL: https://arxiv.org/abs/2404.01461
- Reference count: 40
- All tested LLMs exhibit representativeness heuristic biases, relying on stereotypes over statistical reasoning

## Executive Summary
This paper introduces the REHEAT dataset to evaluate large language models for representativeness heuristic biases, revealing that LLMs systematically fail to apply statistical reasoning when influenced by stereotypical descriptions. The study tested four prominent models (GPT-3.5, GPT-4, PaLM 2, and LLaMA 2) across 202 questions spanning six types of representativeness heuristics, finding consistent bias patterns across all models. Notably, prompting models to use their statistical knowledge improved performance, suggesting the bias stems from reasoning failures rather than knowledge gaps.

## Method Summary
The researchers developed the Representativeness Heuristic AI Testing (REHEAT) dataset containing 202 questions across six representativeness heuristic types, then systematically evaluated four large language models under different prompt conditions. Each model was tested with standard prompts, prompts explicitly encouraging statistical reasoning, and prompts designed to activate domain knowledge. Performance was measured by comparing model responses against correct answers that required ignoring stereotypical descriptions in favor of base rate probabilities. The study controlled for question difficulty and systematically varied prompt engineering approaches to isolate when and how representativeness biases manifest.

## Key Results
- All four tested LLMs (GPT-3.5, GPT-4, PaPAL 2, LLaMA 2) exhibited representativeness heuristic biases across all six tested types
- Models consistently relied on stereotypical descriptions rather than problem statistics, even when explicitly prompted to use statistical reasoning
- Performance improved when models were prompted to activate their statistical knowledge, indicating bias stems from reasoning failures rather than knowledge gaps

## Why This Works (Mechanism)
The representativeness heuristic operates through cognitive shortcuts where people judge probability based on similarity to stereotypes rather than statistical likelihood. LLMs appear to have learned these stereotypical associations from training data but lack the meta-cognitive ability to override them with statistical reasoning when prompted. This suggests models can retrieve relevant statistical knowledge but fail to apply it appropriately in the presence of strong stereotypical cues, indicating a reasoning deficit rather than a knowledge deficit.

## Foundational Learning
- Representativeness heuristic: Cognitive bias where probability judgments are based on similarity to stereotypes rather than base rates. Needed to understand what specific bias is being tested and why it matters for decision-making.
- Base rate neglect: The tendency to ignore statistical frequencies in favor of case-specific information. Quick check: Can the model distinguish between "most people with condition X have symptom Y" versus "most people with symptom Y have condition X"?
- Prompt engineering: Techniques for structuring inputs to influence model outputs. Quick check: Does changing prompt phrasing systematically affect whether models apply statistical reasoning versus stereotypes?
- Cognitive bias testing in AI: Methodology for evaluating whether AI systems exhibit human-like reasoning biases. Quick check: Are the test questions structured to isolate specific biases from general knowledge or reasoning ability?

## Architecture Onboarding
Component map: REHEAT dataset -> LLMs (GPT-3.5, GPT-4, PaLM 2, LLaMA 2) -> Prompt conditions -> Output evaluation -> Bias measurement
Critical path: Question presentation → Model response generation → Statistical correctness evaluation → Bias pattern analysis
Design tradeoffs: The study prioritized systematic bias evaluation over exploring the full space of prompt engineering techniques, focusing on whether biases exist rather than exhaustively testing bias mitigation strategies.
Failure signatures: Consistent preference for stereotypical answers over statistically correct ones, even when prompts explicitly request statistical reasoning; improvement only when prompts specifically activate statistical knowledge.
First experiments: 1) Run REHEAT questions through a new model to establish baseline bias patterns; 2) Test different prompt phrasings to identify which most effectively activate statistical reasoning; 3) Compare model performance on REHEAT questions versus general knowledge questions to isolate reasoning failures from knowledge gaps.

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Limited to representativeness heuristics, may not capture full spectrum of cognitive biases in LLMs
- No human baseline comparisons using identical questions to contextualize model bias levels
- Did not investigate whether biases affect downstream task performance in measurable ways

## Confidence
- High: LLMs exhibit representativeness heuristic biases consistently across multiple models and question types
- Medium: Generalizability of results to other heuristic types beyond representativeness
- Medium: Effectiveness of prompt engineering strategies for bias mitigation

## Next Checks
1. Conduct human subject experiments using REHEAT dataset questions to establish baseline human bias levels and enable direct comparison with LLM performance
2. Test additional prompt engineering techniques including chain-of-thought prompting and few-shot examples to determine if more sophisticated interventions reduce bias
3. Evaluate model performance on domain-specific tasks (medical diagnosis, financial analysis) to assess whether representativeness heuristic biases have measurable negative impacts on real-world applications