---
ver: rpa2
title: A Formal Framework for Understanding Length Generalization in Transformers
arxiv_id: '2410.02140'
source_url: https://arxiv.org/abs/2410.02140
tags:
- length
- c-rasp
- limit
- periodic
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal framework for analyzing length generalization
  in transformers with learnable absolute positional encodings. The authors define
  an idealized inference procedure that asymptotically identifies functions expressible
  by limit transformers satisfying specific properties.
---

# A Formal Framework for Understanding Length Generalization in Transformers

## Quick Facts
- arXiv ID: 2410.02140
- Source URL: https://arxiv.org/abs/2410.02140
- Authors: Xinting Huang, Andy Yang, Satwik Bhattamishra, Yash Sarrof, Andreas Krebs, Hattie Zhou, Preetum Nakkiran, Michael Hahn
- Reference count: 40
- One-line primary result: A theoretical framework proving length generalization guarantees for transformers with absolute positional encodings on functions expressible by Limit Transformers satisfying PERIODIC and LOCAL properties

## Executive Summary
This paper introduces a formal framework for analyzing length generalization in transformers with learnable absolute positional encodings. The authors define an idealized inference procedure that asymptotically identifies functions expressible by limit transformers satisfying specific properties. Their main result proves that this procedure guarantees length generalization for a rich family of problems. They validate their theory by experimentally predicting success and failure of length generalization across various algorithmic tasks and formal languages. The framework explains why transformers succeed at length generalization on some problems (like majority and Dyck-1) but fail on others (like copying with repeated strings and parity), providing theoretical understanding of empirical observations.

## Method Summary
The authors develop a theoretical framework analyzing length generalization in causal transformers with learnable absolute positional encodings. They define Limit Transformers and an idealized inference procedure that fits transformers to reproduce target function behavior on increasingly long inputs while minimizing a norm-based regularizer. The framework establishes that this procedure guarantees length generalization for functions expressible by Limit Transformers satisfying PERIODIC and LOCAL properties. They validate their theory through experiments on synthetic datasets for algorithmic tasks (majority, sort, copy, parity, addition) and formal languages (Tomita grammars, Dyck-1, (aa)*, Σ*be*), training transformers with learnable absolute positional encodings and evaluating performance on test sets with lengths longer than training data.

## Key Results
- The inference procedure guarantees length generalization for functions expressible by Limit Transformers satisfying PERIODIC and LOCAL properties
- C-RASP[periodic,local] provides a lower bound on functions that transformers will successfully generalize on
- Communication complexity bounds prove certain functions (copying, addition) are inexpressible by Limit Transformers
- Experimental validation shows alignment between theory and practice across algorithmic and formal language tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework guarantees length generalization for functions expressible by Limit Transformers satisfying PERIODIC and LOCAL properties.
- **Mechanism:** The inference procedure fits transformers to reproduce target function behavior on increasingly long inputs while minimizing a norm-based regularizer. When the ground-truth function is expressible by a single Limit Transformer across all input lengths, the procedure converges to transformers that generalize to longer sequences.
- **Core assumption:** The target function must be expressible by a Limit Transformer with bounded complexity (bounded depth, heads, precision, and norm) that satisfies both PERIODIC and LOCAL properties.
- **Evidence anchors:**
  - [abstract]: "We introduce a rigorous theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings. In particular, we characterize those functions that are identifiable in the limit from sufficiently long inputs with absolute positional encodings under an idealized inference scheme using a norm-based regularizer."
  - [section]: "Theorem 1 (Informal Version of Theorem 7). Let f be the target function expressible by a single Limit Transformer at all input lengths, subject to restrictions on the use of positional information. Choose transformers Tn (n = 1, 2, 3, ...) with context size n, where Tn reproduces the behavior of f up to length n², while minimizing a norm-based regularizer. Then, for large n, Tn will match the output of the target function f up to length ≤ n."
- **Break condition:** If the target function requires unbounded complexity (depth, heads, precision, or norm) to express, or if it cannot be expressed by any Limit Transformer satisfying PERIODIC and LOCAL properties.

### Mechanism 2
- **Claim:** The C-RASP formalism provides lower bounds on which functions transformers will successfully generalize on.
- **Mechanism:** Any function definable by a C-RASP[periodic,local] program can be translated to a Limit Transformer, which by Mechanism 1 guarantees length generalization. Conversely, functions not definable in C-RASP[periodic,local] are provably inexpressible by Limit Transformers.
- **Core assumption:** The C-RASP[periodic,local] class accurately captures the class of functions that transformers can express with absolute positional encodings while maintaining generalization.
- **Evidence anchors:**
  - [abstract]: "We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks."
  - [section]: "Theorem 9. For every C-RASP[Φ, Ψ] program P with local functions Ψ and periodic functions Φ there exists a Limit Transformer T∞ that satisfies PERIODIC and LOCAL such that for all w ∈ Σ*, P accepts w iff T∞ accepts $w."
- **Break condition:** If there exist functions outside C-RASP[periodic,local] that transformers can still generalize on, or if C-RASP[periodic,local] includes functions transformers cannot express.

### Mechanism 3
- **Claim:** Logarithmic communication complexity bounds prove certain functions are inexpressible by Limit Transformers.
- **Mechanism:** Any function expressible by a Limit Transformer satisfying PERIODIC and LOCAL has logarithmic communication complexity. Functions requiring super-logarithmic communication (like copying arbitrary strings or addition) cannot be expressed by such transformers.
- **Core assumption:** Communication complexity provides a valid upper bound on the expressiveness of Limit Transformers.
- **Evidence anchors:**
  - [abstract]: "We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks."
  - [section]: "Theorem 12. Let T be a Limit Transformer satisfying PERIODIC and LOCAL. On an input x ∈ Σ2N, assume Alice has access to x1...N and Bob has access to xN+1...2N. Then Alice can communicate C log N bits to Bob, where C depends on T but not N, so that Bob can compute each activation in the second half, y(l)i (N + 1 ≤ i ≤ 2N)."
- **Break condition:** If communication complexity does not accurately bound transformer expressiveness, or if transformers can somehow achieve super-logarithmic communication with absolute positional encodings.

## Foundational Learning

- **Concept:** Inductive biases in neural networks
  - Why needed here: Understanding how transformers generalize from training data to unseen sequences requires knowledge of how neural networks develop and maintain inductive biases during training.
  - Quick check question: What role does the norm-based regularizer play in shaping the inductive bias of the transformers produced by the inference procedure?

- **Concept:** Formal language theory
  - Why needed here: The paper analyzes transformers' ability to recognize formal languages and uses concepts like star-free languages, dot-depth, and circuit complexity to characterize expressivity.
  - Quick check question: How does the C-RASP formalism relate to established complexity classes for formal languages?

- **Concept:** Communication complexity
  - Why needed here: The paper uses communication complexity bounds to prove certain functions are inexpressible by Limit Transformers, which is central to understanding the limitations of length generalization.
  - Quick check question: Why does logarithmic communication complexity follow from the PERIODIC and LOCAL properties of Limit Transformers?

## Architecture Onboarding

- **Component map:** Limit Transformers -> Idealized Inference Procedure -> C-RASP Expressiveness -> Communication Complexity Bounds -> Empirical Validation

- **Critical path:**
  1. Define Limit Transformers and establish correspondence with standard transformers
  2. Define inference procedure with norm-based regularizer
  3. Prove length generalization guarantee for expressible functions
  4. Establish C-RASP expressiveness as lower bound
  5. Prove communication complexity as upper bound
  6. Validate predictions empirically

- **Design tradeoffs:**
  - Theoretical idealization vs practical applicability (idealized inference vs SGD training)
  - Expressiveness vs generalization (more complex functions may generalize poorly)
  - Absolute positional encodings vs no positional encodings (APE enables more functions but requires more complex positional handling)

- **Failure signatures:**
  - Lack of length generalization despite expressibility in C-RASP[periodic,local]
  - Length generalization on functions provably not expressible by Limit Transformers
  - Divergence of regularizer values across transformers in inference procedure

- **First 3 experiments:**
  1. Implement Limit Transformer translation from standard transformers and verify PERIODIC/LOCAL properties
  2. Replicate C-RASP program constructions for majority, Dyck-1, and anbncn languages
  3. Test communication complexity bounds by attempting to express copying and addition functions with Limit Transformers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the theoretical framework based on Limit Transformers and C-RASP[periodic, local] provide a complete characterization of tasks on which transformers with absolute positional encodings (APE) will length-generalize?
- Basis in paper: [explicit] The authors state "While we have not proven this class is complete" when discussing whether the set of functions for which length generalization is predicted matches exactly those expressible by Limit Transformers satisfying PERIODIC and LOCAL.
- Why unresolved: The paper provides both upper bounds (via communication complexity) and lower bounds (via C-RASP expressiveness) on what functions are identifiable by the idealized inference procedure, but does not prove these bounds are tight. There could be functions outside C-RASP[periodic, local] that still exhibit length generalization under the inference procedure, or functions within C-RASP[periodic, local] that fail to generalize due to practical learning dynamics.
- What evidence would resolve it: A proof showing either (1) every function not expressible by C-RASP[periodic, local] necessarily fails length generalization under the inference procedure, or (2) there exists a counterexample function that is not in C-RASP[periodic, local] but still shows length generalization. Alternatively, proving that the inclusion of C-RASP[periodic, local] in the set of functions satisfying the conditions of Theorem 7 is strict would show incompleteness.

### Open Question 2
- Question: How do practical learning dynamics (SGD training, subsampling, finite precision) affect the length generalization guarantees provided by the idealized inference procedure?
- Basis in paper: [explicit] The authors discuss limitations stating "Making the guarantee from Theorem 7 more realistic by incorporating SGD training dynamics and subsampling of training data is an interesting problem for future research" and "Our study focuses on idealized asymptotic identification of a global minimum with perfect knowledge of behavior on the training distribution."
- Why unresolved: The theoretical framework assumes perfect fitting of training data and a specific regularizer, while practical transformers use stochastic gradient descent with finite precision, mini-batches, and other optimization techniques. The gap between this idealized setup and real training could significantly affect when and how length generalization occurs.
- What evidence would resolve it: Empirical studies comparing length generalization on tasks predicted by the theory (C-RASP[periodic, local] expressible) versus those not predicted, using standard training methods with varying batch sizes, learning rates, and optimization algorithms. Additionally, theoretical work extending the inference procedure analysis to incorporate gradient descent dynamics and finite-sample effects would help bridge this gap.

### Open Question 3
- Question: Can transformers with other positional encoding schemes (relative positional encodings, rotary positional embeddings, etc.) achieve better length generalization than those with absolute positional encodings or NoPE?
- Basis in paper: [explicit] The authors state "Our study focuses on absolute positional encodings; extending it to other positional encodings (e.g. Su et al., 2024; Press et al., 2021; Ruoss et al., 2023) is another important problem for future research" and "It is an open question how far new, yet-to-be-discovered positional encoding schemes may increase the range of length generalization."
- Why unresolved: The theoretical framework specifically analyzes APE transformers and provides conditions under which they length-generalize. While the authors note that NoPE and APE may be hard to beat by other general-purpose encodings, they do not provide a theoretical framework for analyzing other positional encoding schemes or prove limitations on their expressive power.
- What evidence would resolve it: Empirical comparisons of length generalization across different positional encoding schemes (APE, NoPE, relative positional encodings, rotary positional embeddings, etc.) on the benchmark tasks used in the paper. Theoretical work extending the Limit Transformer framework to other positional encoding schemes, potentially showing different conditions for length generalization or proving limitations on their expressive power compared to APE.

## Limitations

- The idealized inference procedure creates a gap between theory and practical SGD training, making the guarantees difficult to realize in practice
- The role of the norm-based regularizer in shaping inductive biases during practical training is not fully characterized
- The communication complexity bounds, while theoretically compelling, may not capture all practical limitations of transformer expressiveness

## Confidence

**High Confidence:**
- The theoretical framework for Limit Transformers and their correspondence to standard transformers
- The C-RASP expressiveness characterization as a lower bound for successful length generalization
- The communication complexity bounds proving certain functions are inexpressible

**Medium Confidence:**
- The inference procedure's guarantee of length generalization for expressible functions
- The experimental validation showing alignment between theory and practice
- The identification of specific tasks (majority, Dyck-1) that succeed in length generalization

**Low Confidence:**
- The practical applicability of the idealized inference procedure to real-world training
- The complete characterization of when transformers fail to generalize despite expressibility
- The precise role of the norm-based regularizer in practical training scenarios

## Next Checks

1. **Implement and test the norm-based regularizer:** Create a concrete implementation of the regularizer used in the idealized inference procedure and test its effect on transformer training for various tasks. Measure how different regularizer strengths affect length generalization empirically.

2. **Analyze failure cases empirically:** Systematically study transformers trained on tasks known to be expressible in C-RASP[periodic,local] but observe when and why they fail to generalize. Compare the learned representations against the theoretical predictions to identify gaps between theory and practice.

3. **Test the communication complexity bounds experimentally:** Attempt to train transformers on functions that are theoretically provably inexpressible (like copying arbitrary strings or addition) and verify that they indeed fail to generalize, while successfully generalizing on functions within the provable bounds.