---
ver: rpa2
title: 'Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4
  inference'
arxiv_id: '2409.20361'
source_url: https://arxiv.org/abs/2409.20361
tags:
- outliers
- runtime
- smooth
- quantization
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Rotated Runtime Smooth (RRS) is a training-free activation smoother
  designed to enable accurate INT4 inference for large language models. It addresses
  the challenge of outliers in activations, which hinder low-bit quantization.
---

# Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4 inference

## Quick Facts
- arXiv ID: 2409.20361
- Source URL: https://arxiv.org/abs/2409.20361
- Reference count: 12
- Primary result: Reduces WikiText-2 perplexity from 57.33 to 6.66 for INT4 inference on LLaMA3-70B

## Executive Summary
Rotated Runtime Smooth (RRS) is a training-free activation smoother designed to enable accurate INT4 inference for large language models. It addresses the challenge of outliers in activations, which hinder low-bit quantization. RRS combines Runtime Smooth, which eliminates channel-wise outliers by smoothing activations with channel-wise maximums during runtime, and a Rotation operation, which spreads spike outliers and ensures consistent smoothing scales across channels. This approach avoids migrating outliers to weights, minimizing quantization errors and enabling efficient INT4 matrix multiplication.

## Method Summary
RRS combines Runtime Smooth and a Rotation operation to handle channel-wise and spike outliers respectively. Runtime Smooth eliminates channel-wise outliers by smoothing activations with channel-wise maximums during runtime, avoiding outlier migration to weights. The Rotation operation uses Hadamard rotation to spread spike outliers across channels, creating consistent smoothing scales and preventing "victims" (normal values heavily divided by large scales). A fused GEMM kernel implementation minimizes runtime overhead by aligning smoothing scales with hardware block size, allowing scalar multiplication after dequantization within the kernel.

## Key Results
- Reduces WikiText-2 perplexity from 57.33 to 6.66 for INT4 inference on LLaMA3-70B
- Outperforms state-of-the-art methods in accuracy and robustness for INT4 quantization
- Maintains negligible runtime overhead compared to A4W4 Per-Channel quantization

## Why This Works (Mechanism)

### Mechanism 1
Runtime Smooth eliminates channel-wise outliers by dividing activations by channel-wise maximums during runtime. For each channel, compute max(|X_j|) as the smoothing scale, then quantize X/s_j. This avoids migrating outliers to weights and prevents quantization errors from shared outlier channels. Core assumption: Outliers are predominantly channel-wise consistent, appearing in fixed channels across tokens.

### Mechanism 2
Rotation spreads spike outliers across channels, creating consistent smoothing scales and avoiding victims. Multiply activations by a Hadamard rotation matrix R. Spike outliers become spread across all channels of that token, so the max per channel is consistent and large enough to prevent normal values from becoming victims. Core assumption: Spike outliers are localized within tokens, so rotation distributes them evenly across channels.

### Mechanism 3
Fused kernel implementation minimizes runtime overhead by aligning smoothing scales with GEMM block size. Reorder channels by max magnitude, group activations into blocks matching GEMM tile size, and apply per-block smoothing scales. This allows scalar multiplication after dequantization within the kernel, avoiding extra memory movement. Core assumption: Channel reordering and grouping into GEMM blocks is hardware-friendly and negligible in cost.

## Foundational Learning

- **Quantization basics (per-channel scaling)**: RRS relies on accurate per-channel quantization of both weights and activations; misunderstanding scaling leads to wrong implementation. Quick check: What is the formula for per-channel scaling factor α_j for channel j? Answer: α_j = max(|X_j|) / (2^N - 1), where N is the target bit width.

- **Outlier types in LLM activations**: RRS treats channel-wise and spike outliers differently; confusing them leads to wrong smoothing strategy. Quick check: How does a spike outlier differ from a channel-wise outlier in activation tensors? Answer: Channel-wise outliers appear consistently in the same channel across tokens; spike outliers are large values in a single token but not consistently in the same channel.

- **Rotation matrix properties**: RRS uses Hadamard rotation; misunderstanding orthogonality leads to incorrect output equivalence. Quick check: What property of rotation matrices ensures XR * R^T * W^T = X * W^T? Answer: R is orthogonal (R * R^T = I), so the rotation cancels out.

## Architecture Onboarding

- **Component map**: Input → Preprocess → Rotate (RRS) → Smooth → Quantize → GEMM(kernel fusion)
- **Critical path**: Preprocess → Rotate (RRS) → Smooth → Quantize → GEMM(kernel fusion)
- **Design tradeoffs**: Runtime Smooth vs SmoothQuant (no outlier migration vs shared outliers); Group size choice (smaller groups → better smoothness but more overhead); Rotation necessity (only needed if spike outliers exist)
- **Failure signatures**: High perplexity despite RRS (likely victims from inconsistent smoothing scales); Runtime overhead spike (group size too small or reordering not optimized); Accuracy drop in A4W4KV4 (weight quantization errors dominate)
- **First 3 experiments**: 1) Baseline: Apply Runtime Smooth only on LLaMA-2-7B A4W4KV16, measure WikiText-2 perplexity. 2) Add rotation: Apply RRS on same model/settings, compare perplexity and check for victim reduction. 3) Group size sweep: Vary smoothing group size (1, 32, 128, 512), measure perplexity and kernel runtime to find optimal tradeoff.

## Open Questions the Paper Calls Out

- **How does RRS perform when dealing with multiple spike outliers within a single token?**: The paper mentions that tokens with multiple spike outliers can cause issues for RRS due to the stacking effect of outliers, but does not provide detailed experimental results or analysis of this scenario.

- **What is the optimal group size for Runtime Smooth in different LLM architectures?**: The paper mentions that the group size of the smoothing scale is set to 128 for RRS and conducts an ablation study on group size, but does not provide a definitive answer for different architectures.

- **How does RRS compare to training-based methods like SpinQuant in terms of long-term robustness and generalization?**: The paper compares RRS to SpinQuant and finds that SpinQuant degrades WikiText-2 perplexity compared to RRS, but does not explore long-term robustness or generalization.

## Limitations
- Weak empirical validation of rotation's specific contribution to outlier handling
- Kernel optimization assumptions rely on specific hardware configurations
- Limited scope of tested model families (decoder-only models only)

## Confidence
- **High confidence**: Core mechanism of Runtime Smooth for channel-wise outlier elimination
- **Medium confidence**: Rotation operation's effectiveness in preventing victims from spike outliers
- **Low confidence**: Specific performance improvements and group size configuration may be implementation-dependent

## Next Checks
1. **Ablation study with controlled outlier injection**: Create synthetic activation tensors with known channel-wise and spike outliers, then apply Runtime Smooth alone versus RRS to quantify the specific contribution of rotation in preventing victims.

2. **Hardware-aware kernel performance validation**: Implement the fused kernel on multiple hardware platforms with varying block sizes and group configurations to measure actual runtime overhead versus theoretical expectations.

3. **Cross-architecture robustness testing**: Apply RRS to encoder-decoder models and models with different attention mechanisms to establish whether the method generalizes beyond tested LLaMA and Qwen families.