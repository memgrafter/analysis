---
ver: rpa2
title: 'Arcee''s MergeKit: A Toolkit for Merging Large Language Models'
arxiv_id: '2403.13257'
source_url: https://arxiv.org/abs/2403.13257
tags:
- arxiv
- merging
- mergekit
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MergeKit is an open-source library designed to merge large language
  models efficiently, addressing challenges like catastrophic forgetting and multitask
  learning. It enables the integration of task-specific models into powerful multitask
  models without additional training by combining their parameters.
---

# Arcee's MergeKit: A Toolkit for Merging Large Language Models

## Quick Facts
- **arXiv ID:** 2403.13257
- **Source URL:** https://arxiv.org/abs/2403.13257
- **Authors:** Charles Goddard; Shamane Siriwardhana; Malikeh Ehghaghi; Luke Meyers; Vlad Karpukhin; Brian Benedict; Mark McQuade; Jacob Solawetz
- **Reference count:** 12
- **Primary result:** Open-source library for efficient model merging across various hardware configurations

## Executive Summary
MergeKit is an open-source library designed to merge large language models efficiently, addressing challenges like catastrophic forgetting and multitask learning. It enables the integration of task-specific models into powerful multitask models without additional training by combining their parameters. The library offers an extensible framework compatible with various hardware, from CPUs to GPUs, and includes novel methods like 'FrankenMerging' for building larger models and MoE construction. MergeKit has facilitated the merging of thousands of models, contributing to some of the world's most powerful open-source checkpoints. Empirical experiments demonstrate that merged models, such as those combining Llama2-7B and Meditron-7B, outperform individual models across medical and general benchmarks, highlighting the effectiveness of model merging in enhancing model versatility and performance.

## Method Summary
MergeKit implements several model merging techniques including LERP (linear interpolation), SLERP (spherical linear interpolation), TIES (Transformation Importance), and DARE-TIES. The library uses a YAML-based configuration system to define merge operations, which are then executed through a DAG-based task scheduler that manages memory efficiently through out-of-core tensor loading. The framework supports merging models with identical architectures through weight space interpolation, and models with different initializations through permutation-equivariant techniques like OT-Fusion and Git-Rebasin. The system scales from personal laptops to high-end research clusters by loading only necessary tensors for each operation.

## Key Results
- Merged models combining Llama2-7B and Meditron-7B outperform individual models across medical and general benchmarks
- MergeKit enables efficient merging across various hardware configurations from CPUs to GPUs
- The library has facilitated the merging of thousands of models, contributing to powerful open-source checkpoints
- Novel 'FrankenMerging' technique enables building larger models and MoE construction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MergeKit enables combining models with identical architectures and initializations without retraining.
- Mechanism: Linear interpolation in weight space leverages linear mode connectivity, where fine-tuned models from a common initialization lie on a low-loss path.
- Core assumption: Models fine-tuned from the same base checkpoint exhibit linear mode connectivity, allowing parameter averaging to preserve capabilities.
- Evidence anchors:
  - [abstract] "Model merging facilitates the creation of multitask models without the need for additional training"
  - [section 2.2.1] "This technique relies on linear mode connectivity and is the foundation of most others"
  - [corpus] Weak - no specific corpus citations about linear mode connectivity
- Break condition: If models were not fine-tuned from a common initialization, linear interpolation fails to preserve performance.

### Mechanism 2
- Claim: MergeKit can merge models with different initializations by aligning neurons through permutation.
- Mechanism: Permutation-equivariant techniques like OT-Fusion or Git-Rebasin map neurons across models to minimize interpolation barriers, enabling effective merging despite different initializations.
- Core assumption: Neurons can be mapped across models such that corresponding neurons have similar functionality, even if initializations differ.
- Evidence anchors:
  - [section 2.2.2] "Methods leveraging permutation symmetry of checkpoints include Git-Rebasin... and OT-Fusion... assign correspondences between model neurons"
  - [abstract] "Techniques often leverage Linear Mode Connectivity (LMC)... Other works employ permutation equivariance"
  - [corpus] Weak - corpus doesn't provide specific evidence for permutation-based merging
- Break condition: If neuron mapping is incorrect or if models are too dissimilar, merging performance degrades significantly.

### Mechanism 3
- Claim: MergeKit achieves scalability by using out-of-core tensor loading and DAG-based task scheduling.
- Mechanism: Lazy loading of tensors combined with directed acyclic graph scheduling minimizes memory footprint, enabling merging on resource-constrained hardware.
- Core assumption: Memory constraints can be addressed by loading only necessary tensors for each operation and scheduling tasks to minimize working set size.
- Evidence anchors:
  - [section 3.3] "By loading only the tensors necessary for each individual operation into working memory, MergeKit can scale from a high-end research cluster all the way down to a personal laptop"
  - [section 3.3.1] "MergeKit internally represents a merge as a directed acyclic graph of operations... to schedule the execution of tasks such that the working set needed at any given time is minimized"
  - [corpus] Weak - no corpus evidence about DAG-based scheduling for model merging
- Break condition: If DAG scheduling becomes too complex or if intermediate values cannot be efficiently evicted, memory usage may exceed hardware limits.

## Foundational Learning

- **Concept:** Linear Mode Connectivity
  - Why needed here: Understanding LMC is crucial for grasping why simple weight averaging works for models fine-tuned from the same base.
  - Quick check question: What property of fine-tuned models allows them to be linearly interpolated without significant performance loss?

- **Concept:** Permutation Equivariance
  - Why needed here: This concept explains how models with different initializations can still be merged by finding neuron correspondences.
  - Quick check question: How do techniques like OT-Fusion find correspondences between neurons in differently initialized models?

- **Concept:** Directed Acyclic Graphs (DAGs)
  - Why needed here: DAGs are fundamental to MergeKit's memory-efficient scheduling and execution of merge operations.
  - Quick check question: How does representing merge operations as a DAG help minimize memory usage during the merging process?

## Architecture Onboarding

- **Component map:** merge_methods/base.py -> graph.py -> plan.py -> architecture.py -> yaml configuration files
- **Critical path:** 1. User creates YAML configuration file 2. MergeKit parses configuration and creates merge plan 3. Computational graph is built and scheduled 4. Tensors are loaded and operations executed 5. Merged model is saved to Hugging Face hub
- **Design tradeoffs:**
  - Memory vs. Speed: Out-of-core approach trades some speed for memory efficiency
  - Flexibility vs. Complexity: YAML interface provides flexibility but adds configuration complexity
  - Extensibility vs. Stability: Plugin architecture allows new methods but may introduce instability
- **Failure signatures:**
  - Memory errors: Indicates DAG scheduling or tensor loading issues
  - Performance degradation: May suggest incorrect neuron alignment or permutation mapping
  - Configuration parsing errors: Typically YAML syntax issues or unsupported merge methods
- **First 3 experiments:**
  1. Simple LERP merge of two Llama2-7B checkpoints with identical architectures and initializations
  2. SLERP merge of Llama2-7B and Meditron-7B to validate medical domain performance gains
  3. OT-Fusion merge of two differently initialized models to test permutation-based alignment

## Open Questions the Paper Calls Out

- **Open Question 1:** What are the theoretical limits of model merging, and under what conditions does it fail?
  - Basis in paper: [inferred] The paper mentions that model merging can lead to catastrophic forgetting and skill transfer challenges, but does not explore the theoretical limits of the technique.
  - Why unresolved: The paper focuses on the practical application of model merging and does not delve into the theoretical underpinnings of the technique.
  - What evidence would resolve it: Empirical studies on the limits of model merging, including cases where it fails or leads to significant degradation in performance, would help establish the theoretical boundaries of the technique.

- **Open Question 2:** How can model merging be applied to models with vastly different architectures and initializations?
  - Basis in paper: [explicit] The paper mentions that merging models with different architectures and initializations is a promising future research direction.
  - Why unresolved: The paper primarily focuses on merging models with identical architectures and initializations, and does not explore the challenges and potential solutions for merging models with vastly different architectures and initializations.
  - What evidence would resolve it: Successful examples of merging models with vastly different architectures and initializations, along with an analysis of the challenges and solutions involved, would provide insights into the feasibility and potential of this approach.

- **Open Question 3:** What are the ethical implications of model merging, particularly in terms of bias and fairness?
  - Basis in paper: [explicit] The paper acknowledges the importance of ethical considerations in model merging, including the need to identify and mitigate biases within merged models.
  - Why unresolved: The paper does not provide a comprehensive analysis of the ethical implications of model merging, particularly in terms of bias and fairness.
  - What evidence would resolve it: Studies on the potential biases and fairness issues that may arise from model merging, along with proposed solutions and guidelines for ethical model merging practices, would help address this open question.

## Limitations
- Effectiveness of linear mode connectivity-based merging is highly dependent on models fine-tuned from a common initialization
- Permutation-based merging relies on accurate neuron correspondence mapping, which may degrade for very dissimilar models
- Scalability claims assume optimal DAG scheduling, which may not hold for complex merge graphs
- Performance gains demonstrated may not generalize to all domain combinations or model scales

## Confidence
- **High confidence:** Basic LERP merging for models with identical architectures and initializations
- **Medium confidence:** Permutation-based merging for different initializations and Frankenbatching for MoE construction
- **Low confidence:** Scalability claims across all hardware configurations and generalization to arbitrary domain combinations

## Next Checks
1. Test LERP merging across a range of task combinations beyond medical and general domains to assess generalization limits
2. Benchmark memory usage and execution time for complex merge graphs with 5+ input models to validate DAG scheduling efficiency
3. Evaluate merged model performance when input models are fine-tuned from different base checkpoints to test linear mode connectivity assumptions