---
ver: rpa2
title: Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model
arxiv_id: '2405.14457'
source_url: https://arxiv.org/abs/2405.14457
tags:
- privacy
- auditing
- adversary
- gradient
- adversaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies privacy auditing of DP-SGD in the hidden state
  threat model, where the adversary only accesses the final model. The authors propose
  gradient-crafting adversaries that directly craft gradient sequences without relying
  on intermediate models.
---

# Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model

## Quick Facts
- arXiv ID: 2405.14457
- Source URL: https://arxiv.org/abs/2405.14457
- Reference count: 40
- Main result: Gradient-crafting adversaries outperform prior canary-based methods in DP-SGD privacy auditing under the hidden state threat model.

## Executive Summary
This paper addresses the fundamental question of how much privacy amplification can be achieved when releasing only the final model trained with DP-SGD, rather than intermediate models. The authors propose a novel gradient-crafting adversary that directly crafts gradient sequences without relying on intermediate models, making it applicable to the hidden state threat model where only the final model is accessible. Through extensive experiments on MNIST and CIFAR-10 with varying insertion periods and noise levels, they demonstrate that their approach significantly outperforms prior canary-based methods while revealing important insights about privacy amplification mechanisms in non-convex settings.

## Method Summary
The paper introduces gradient-crafting adversaries that directly construct gradient sequences rather than extracting gradients from intermediate models (as in canary-based approaches). The adversary inserts a canary at every training step or periodically, then crafts gradients that maximize the distinguishability of the canary's final position between adjacent datasets. For periodic insertion, they propose a hybrid approach combining per-step and batch gradient crafting strategies. The privacy analysis uses Rényi Differential Privacy (RDP) bounds with numerical computation of optimal privacy parameters through grid search and gradient descent optimization.

## Key Results
- When gradients are inserted at every step, gradient-crafting adversaries match privacy upper bounds, demonstrating that releasing only the final model does not provide additional privacy amplification beyond what's already captured by DP-SGD's intrinsic protection.
- With periodic gradient insertion, the proposed adversaries outperform prior canary-based methods but cannot match upper bounds, suggesting that privacy amplification in non-convex settings is qualitatively weaker than in convex regimes.
- The gap between achievable privacy (lower bounds from adversaries) and theoretical upper bounds indicates room for improvement in understanding privacy amplification mechanisms, particularly for periodic insertion patterns.

## Why This Works (Mechanism)
The gradient-crafting approach works by directly optimizing gradients to maximize information leakage about the canary's trajectory, bypassing the need for intermediate model access. This allows the adversary to exploit the full information available in the gradient updates regardless of whether intermediate models are accessible. The method leverages the fact that DP-SGD's privacy guarantee depends on the cumulative effect of all gradient updates, and by crafting gradients strategically, the adversary can extract more information than random or passive gradient extraction methods.

## Foundational Learning

**Rényi Differential Privacy (RDP)**: A generalization of DP that provides tighter composition bounds for Gaussian mechanisms. Needed because standard DP composition theorems are loose for DP-SGD with multiple epochs. Quick check: Verify that RDP_composition and RDP_gaussian_mechanism implementations match theoretical definitions.

**Privacy Amplification by Subsampling**: The phenomenon where training on random subsamples amplifies privacy compared to full-batch training. Critical for understanding why DP-SGD provides stronger guarantees than batch training. Quick check: Confirm that amplification factors scale correctly with batch size and sampling probability.

**Hidden State Threat Model**: An adversarial setting where only the final model is accessible, not intermediate checkpoints. Needed because real-world deployments often only expose final models. Quick check: Ensure experiments don't inadvertently leak intermediate states through logging or checkpointing.

## Architecture Onboarding

**Component Map**: Data → DP-SGD Trainer -> Final Model -> Gradient-Crafting Adversary -> Privacy Estimate

**Critical Path**: The adversary inserts canary gradients → DP-SGD processes gradients → Final model is released → Adversary extracts information from final model to distinguish adjacent datasets

**Design Tradeoffs**: The paper trades off computational complexity (grid search over hyperparameters) against accuracy of privacy bounds. The hybrid approach for periodic insertion balances per-step precision with batch-level efficiency.

**Failure Signatures**: When privacy bounds don't converge with increasing grid resolution, or when gradient crafting fails to improve distinguishability despite optimization effort, indicating potential issues with the optimization landscape or numerical precision.

**First Experiments**: 1) Verify RDP composition matches theoretical bounds on simple Gaussian mechanism; 2) Test canary extraction on single-step DP-SGD to validate basic functionality; 3) Compare gradient-crafting vs canary-based approaches on small dataset with full observability.

## Open Questions the Paper Calls Out
The paper highlights several open questions: whether stronger adversaries exist that could close the gap to upper bounds for periodic insertion; how privacy amplification mechanisms differ between convex and non-convex settings; and whether the current understanding of privacy amplification can be extended to more complex training regimes or alternative DP mechanisms.

## Limitations
- The gradient-crafting adversaries cannot match privacy upper bounds when gradients are inserted periodically, leaving uncertainty about whether stronger adversaries exist or whether bounds need tightening
- The empirical evidence for privacy amplification in non-convex settings, while compelling, is qualitative rather than quantitative, with significant gaps remaining between achievable and upper bounds
- The focus on DP-SGD with Gaussian noise may limit generalizability to other DP mechanisms or noise distributions

## Confidence
- **High confidence**: Releasing only the final model does not amplify privacy when gradients are inserted at every step (gradient-crafting adversaries match upper bounds)
- **Medium confidence**: Privacy amplification occurs in non-convex settings but is weaker than in convex regimes (based on empirical evidence with gaps to upper bounds)
- **Medium confidence**: Overall improvement over prior canary-based methods (gradient-crafting adversaries outperform previous approaches)

## Next Checks
1. Attempt to construct stronger adversaries for the periodic insertion case to determine if the gap to upper bounds can be closed
2. Extend experiments to alternative DP mechanisms (e.g., Laplace noise, Gaussian mechanism with different parameters) to assess generalizability
3. Conduct systematic parameter sweeps across different learning rates, batch sizes, and noise scales to better characterize the relationship between training hyperparameters and privacy amplification strength