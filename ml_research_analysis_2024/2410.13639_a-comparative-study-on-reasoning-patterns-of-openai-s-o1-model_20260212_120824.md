---
ver: rpa2
title: A Comparative Study on Reasoning Patterns of OpenAI's o1 Model
arxiv_id: '2410.13639'
source_url: https://arxiv.org/abs/2410.13639
tags:
- reasoning
- arxiv
- methods
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares OpenAI''s o1 model with four Test-time Compute
  methods (BoN, Step-wise BoN, Agent Workflow, and Self-Refine) on three reasoning
  domains: math, coding, and commonsense reasoning. Using GPT-4o as the backbone,
  o1 achieves superior performance across most benchmarks, particularly excelling
  in complex math and coding tasks.'
---

# A Comparative Study on Reasoning Patterns of OpenAI's o1 Model

## Quick Facts
- arXiv ID: 2410.13639
- Source URL: https://arxiv.org/abs/2410.13639
- Authors: Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, J. H. Liu
- Reference count: 5
- One-line primary result: o1 model outperforms test-time compute methods on math, coding, and commonsense reasoning tasks through effective Divide and Conquer and Self-Refinement reasoning patterns

## Executive Summary
This study systematically compares OpenAI's o1 model with four test-time compute methods (Best-of-N, Step-wise Best-of-N, Agent Workflow, and Self-Refine) across math, coding, and commonsense reasoning domains. Using GPT-4o as the backbone for comparison, o1 demonstrates superior performance particularly in complex math and coding tasks. The researchers identify six reasoning patterns in o1, with Divide and Conquer and Self-Refinement emerging as the most crucial for its success. The study reveals that o1's effectiveness isn't tied to reasoning token count but rather varies significantly across different task types.

## Method Summary
The researchers conducted a comprehensive comparison between OpenAI's o1 model and four test-time compute methods: Best-of-N (BoN), Step-wise BoN, Agent Workflow, and Self-Refine. They used GPT-4o as the backbone for implementing these methods and as the reward model for BoN variants. The study evaluated performance on four benchmarks: HotpotQA and Collie for commonsense reasoning, USACO for coding, and AIME for math. A voting method with multiple LLMs was employed to filter difficult samples from these benchmarks. The researchers also analyzed reasoning patterns in o1, identifying six distinct types: Systematic Analysis, Method Reuse, Divide and Conquer, Self-Refinement, Context Identification, and Emphasizing Constraints.

## Key Results
- o1 model achieves superior performance across most benchmarks, particularly excelling in complex math (AIME) and coding (USACO) tasks
- Agent Workflow method shows significant improvement due to domain-specific prompts, while BoN's effectiveness is limited by reward model quality and search space
- o1's performance isn't tied to the number of reasoning tokens but varies significantly across different task types
- Divide and Conquer and Self-Refinement are the most commonly used and crucial reasoning patterns in o1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OpenAI's o1 model achieves superior performance through reasoning patterns like Divide and Conquer (DC) and Self-Refinement (SR), which are more effective than simple inference-time compute scaling.
- Mechanism: DC breaks complex problems into subproblems, while SR allows the model to iteratively assess and correct its reasoning during inference.
- Core assumption: The combination of DC and SR reasoning patterns is crucial for enhancing o1's performance across different reasoning domains.
- Evidence anchors:
  - [abstract] "the most commonly used reasoning patterns in o1 are DC and SR, which might be the key to o1's success"
  - [section 4.2] "we observe that the most commonly used reasoning patterns in o1 are DC and SR, which might be the key to o1's success"
- Break condition: If o1's performance drops significantly when DC and SR patterns are unavailable or ineffective for specific task types.

### Mechanism 2
- Claim: Agent Workflow achieves better performance than Step-wise BoN due to domain-specific system prompts that reduce unnecessary reasoning steps and better align with problem requirements.
- Mechanism: Agent Workflow uses structured planning and domain-specific prompts to break down tasks efficiently, while Step-wise BoN suffers from long context and intermediate step accumulation.
- Core assumption: Domain-specific prompts in Agent Workflow enable more targeted reasoning than generic stepwise approaches.
- Evidence anchors:
  - [abstract] "the Agent Workflow has achieved better performance than Step-wise BoN due to the domain-specific system prompt for planning better reasoning processes"
- Break condition: If Agent Workflow performance degrades when domain-specific prompts are removed or when tasks don't benefit from structured planning.

### Mechanism 3
- Claim: The effectiveness of search-based methods (BoN) is fundamentally limited by the quality of the reward model and the search space, not just the number of samples generated.
- Mechanism: Reward models determine which generated responses are selected, creating an upper bound on performance regardless of search breadth.
- Core assumption: Better reward models can significantly improve BoN performance, while poor reward models create performance ceilings.
- Evidence anchors:
  - [abstract] "we find the reward models' capability and the search space both limit the upper boundary of these methods"
- Break condition: If reward model improvements plateau or if search space limitations prevent meaningful performance gains regardless of reward model quality.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: o1's superior performance relies heavily on CoT-based approaches for complex multi-step reasoning in math and coding tasks
  - Quick check question: How does CoT reasoning differ from direct response generation in handling complex problems?

- Concept: Inference-time compute scaling
  - Why needed here: The study compares o1 with traditional test-time compute methods, highlighting when scaling compute is more effective than scaling model parameters
  - Quick check question: What are the key differences between parameter scaling and inference-time compute scaling in terms of computational efficiency?

- Concept: Reward model selection and evaluation
  - Why needed here: BoN and Step-wise BoN methods depend critically on reward models to select optimal responses, affecting overall performance
  - Quick check question: How do different reward models impact the performance ceiling of search-based methods?

## Architecture Onboarding

- Component map:
  - Input problem -> Reasoning pattern selection (SA, MR, DC, SR, CI, EC) -> Solution generation -> Reward model evaluation -> Final answer selection

- Critical path:
  1. Input problem → 2. Reasoning pattern selection (SA, MR, DC, SR, CI, EC) → 3. Solution generation → 4. Reward model evaluation → 5. Final answer selection

- Design tradeoffs:
  - Long context vs. performance: Step-wise methods suffer from context window limitations
  - Search breadth vs. reward model quality: More samples don't help if reward model is poor
  - Domain specificity vs. generalization: Agent Workflow excels with specific prompts but may not generalize

- Failure signatures:
  - Performance plateaus despite increased N in BoN methods
  - Step-wise methods fail on tasks requiring strict output formatting
  - Self-Refine degrades performance on format-sensitive tasks

- First 3 experiments:
  1. Compare o1 performance with and without DC/SR reasoning patterns on AIME problems
  2. Test Agent Workflow with generic vs. domain-specific prompts on USACO
  3. Evaluate BoN performance with different reward models on HotpotQA at varying N values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the o1 model's performance on complex reasoning tasks scale with the number of reasoning tokens, and is there an optimal token count for different task types?
- Basis in paper: [explicit] The paper states "we also explore the number of reasoning tokens of o1 across different tasks, and observe that the number of reasoning tokens varies a lot across different tasks."
- Why unresolved: The study identifies that reasoning token length varies significantly across task types but does not establish a clear correlation between token count and performance or determine if there's an optimal token count for different tasks.
- What evidence would resolve it: Systematic experiments varying reasoning token counts across different task types while measuring performance would clarify the relationship and optimal token counts.

### Open Question 2
- Question: What specific architectural or training modifications in o1 enable the Divide and Conquer and Self-Refinement reasoning patterns to be more effective than in previous models?
- Basis in paper: [explicit] The paper concludes that "DC and SR are the main reasoning patterns of o1, which indicates that these reasoning patterns are crucial for improving reasoning performance."
- Why unresolved: While the study identifies these patterns as crucial, it does not investigate the specific mechanisms or model modifications that make these patterns particularly effective in o1.
- What evidence would resolve it: Detailed architectural analysis and ablation studies comparing o1 with baseline models would reveal which modifications specifically enhance these reasoning patterns.

### Open Question 3
- Question: How does the quality and diversity of the reward model in BoN methods impact the upper performance boundary, and can this be improved through better reward model training?
- Basis in paper: [explicit] The paper finds "the reward models' capability and the search space both limit the upper boundary of these methods" and shows that human-based reward models significantly outperform automated ones.
- Why unresolved: The study demonstrates the impact of reward model quality but does not explore how to improve reward model training or what specific aspects of reward model quality most affect performance.
- What evidence would resolve it: Experiments training reward models with different methodologies and evaluating their impact on BoN performance would clarify the relationship between reward model quality and method effectiveness.

## Limitations

- The study relies on GPT-4o as the baseline for test-time compute methods, making it difficult to verify whether o1's performance gains stem from reasoning patterns or other unmeasured factors
- The effectiveness of test-time compute methods shows significant variance based on task type and domain specificity, with unclear generalizability across domains
- The identification of six reasoning patterns in o1 lacks external validation through comparison with human-annotated reasoning traces or inter-rater reliability assessment

## Confidence

- **Medium**: Conclusions about o1's superiority rely on comparisons with GPT-4o baseline, with undisclosed architectural details making verification difficult
- **Medium**: Test-time compute method effectiveness analysis shows variance but doesn't establish persistence of advantages across domains
- **Low**: Reasoning pattern identification lacks external validation and reliability assessment

## Next Checks

1. Apply the same reasoning pattern identification methodology to other frontier models (Claude-3, Gemini-1.5) to determine whether the six identified patterns are unique to o1 or represent general LLM reasoning behaviors

2. Systematically remove domain-specific prompts from Agent Workflow and measure performance degradation across different task types, comparing results against similarly optimized prompts for other test-time compute methods

3. Conduct a comprehensive evaluation of BoN performance using a diverse set of reward models (including open-source alternatives and task-specific models) to quantify the relationship between reward model quality and search-based method effectiveness