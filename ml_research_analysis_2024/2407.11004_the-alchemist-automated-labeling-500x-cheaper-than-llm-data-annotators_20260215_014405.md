---
ver: rpa2
title: 'The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators'
arxiv_id: '2407.11004'
source_url: https://arxiv.org/abs/2407.11004
tags:
- programs
- alchemist
- labeling
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Alchemist addresses the high cost and inflexibility of using large
  language models for data annotation by having models generate programs that can
  label data locally. Instead of querying APIs repeatedly for labels, it prompts models
  to produce code that captures labeling logic, which can then be applied at no cost.
---

# The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators

## Quick Facts
- **arXiv ID:** 2407.11004
- **Source URL:** https://arxiv.org/abs/2407.11004
- **Authors:** Tzu-Heng Huang; Catherine Cao; Vaishnavi Bhargava; Frederic Sala
- **Reference count:** 40
- **One-line primary result:** Achieves 500x cost reduction in data annotation while maintaining comparable or better accuracy than direct LLM API labeling

## Executive Summary
Alchemist addresses the high cost and inflexibility of using large language models for data annotation by having models generate programs that can label data locally. Instead of querying APIs repeatedly for labels, it prompts models to produce code that captures labeling logic, which can then be applied at no cost. This approach reduces labeling expenses by orders of magnitude—around 500x in experiments—while achieving comparable or better accuracy than direct API labeling. It also enables auditing and easy adaptation of labels.

The system works across text and image modalities by using concept extraction and local feature extractors, and it integrates with weak supervision frameworks to aggregate noisy program outputs. Supplementary information and program diversity further improve performance, and generated programs often outperform manually crafted ones in coverage and accuracy.

## Method Summary
Alchemist generates labeling programs through LLM prompting, then applies weak supervision to aggregate their outputs into pseudolabels for training a final classifier. For text tasks, the LLM directly generates programs based on task descriptions. For images, high-level concepts are extracted using CLIP, then programs are generated using these features. The generated programs serve as annotators whose outputs are aggregated via Snorkel to produce high-quality pseudolabels. A simple two-layer MLP is trained on these pseudolabels for the final classification task.

## Key Results
- Achieves ~500x cost reduction in data annotation compared to direct API labeling
- Maintains comparable or better accuracy than direct LLM API labeling across multiple datasets
- Improves worst-group accuracy for images by reducing reliance on spurious correlations
- Generated programs often outperform manually crafted programs in coverage and accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distilling model knowledge into programs reduces API costs by orders of magnitude.
- **Mechanism:** Instead of querying the model for each data point, we query it once to generate a program that can label many points locally.
- **Core assumption:** The generated programs can reliably reproduce the model's labeling logic without significant degradation.
- **Evidence anchors:** [abstract] "These programs can be stored and applied locally, re-used and extended, and cost orders of magnitude less." [section] "For example, for the dataset described above [12], the number of GPT-4 calls was reduced from 7,569 to 10, resulting in a massive cost reduction from $1,200 to $0.70, a 1,700-fold decrease."

### Mechanism 2
- **Claim:** Aggregating multiple noisy program outputs improves label quality through weak supervision.
- **Mechanism:** Each program acts as a noisy labeling source; weak supervision frameworks learn their reliabilities and aggregate their outputs.
- **Core assumption:** Different programs have complementary strengths and their errors are not perfectly correlated.
- **Evidence anchors:** [abstract] "These synthesized programs serve as annotators, capturing the underlying logic used by the models when annotating." [section] "However, as such programs may employ different techniques...there may be complementary signal in their outputs. This means we can aggregate them to mitigate the impact of noise."

### Mechanism 3
- **Claim:** Converting complex modalities to feature vectors enables tractable program generation.
- **Mechanism:** Extract high-level task-specific concepts, use local multimodal models to create feature vectors, then generate programs operating on these vectors.
- **Core assumption:** The feature extraction step preserves sufficient information for accurate labeling.
- **Evidence anchors:** [abstract] "Alchemist works across text and image modalities by using concept extraction and local feature extractors." [section] "We handle this via a simple two-step approach that first extracts high-level concepts and then uses them in concert with a local feature extractor."

## Foundational Learning

- **Concept:** Weak supervision framework
  - Why needed here: To aggregate noisy outputs from multiple programs into high-quality pseudolabels
  - Quick check question: What is the difference between majority vote and learned label models in weak supervision?

- **Concept:** In-context learning and few-shot prompting
  - Why needed here: To generate effective programs by providing task context and examples to language models
  - Quick check question: How does adding examples to prompts affect program generation quality?

- **Concept:** Multimodal feature extraction
  - Why needed here: To convert non-text data into a form that text-based program generators can handle
  - Quick check question: What information might be lost when converting images to feature vectors for program generation?

## Architecture Onboarding

- **Component map:** Prompt Generator -> Program Synthesizer -> Feature Extractor (if needed) -> Label Model -> Distilled Model Trainer

- **Critical path:** Prompt → Program Generation → Feature Extraction (if needed) → Label Aggregation → Model Training

- **Design tradeoffs:**
  - Program diversity vs. cost: More programs improve aggregation but increase API costs
  - Program complexity vs. accuracy: Simpler programs are more reliable but may miss nuanced cases
  - Local vs. cloud feature extraction: Local models save costs but may have lower quality

- **Failure signatures:**
  - Programs fail to compile → Check language model instructions and prompt quality
  - Poor label accuracy → Insufficient program diversity or weak supervision model mismatch
  - High costs → Too many programs generated or expensive feature extraction models used

- **First 3 experiments:**
  1. Generate 5 simple programs for a text classification task and measure accuracy vs. baseline
  2. Test different numbers of programs (5, 10, 15) to find the sweet spot for accuracy vs. cost
  3. Apply the modality extension to an image task and compare to direct multimodal prompting baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of Alchemist scale with the number of generated programs, and is there an optimal number beyond which additional programs provide diminishing returns?
- **Basis in paper:** [inferred] The paper mentions increasing program diversity to improve performance, and Figure 5 shows performance trends with varying numbers of programs.
- **Why unresolved:** The paper does not provide a comprehensive analysis of the relationship between program quantity and performance across all datasets.
- **What evidence would resolve it:** Systematic experiments varying the number of programs for each dataset and analyzing performance metrics to identify optimal thresholds.

### Open Question 2
- **Question:** Can Alchemist be effectively applied to data modalities beyond text and images, such as audio or video, and what are the challenges in extending the approach?
- **Basis in paper:** [explicit] The paper mentions that the extension approach can be applied to any data modality where a local multimodal model is available, but only demonstrates text and image tasks.
- **Why unresolved:** The paper does not explore the application of Alchemist to other modalities like audio or video.
- **What evidence would resolve it:** Experiments applying Alchemist to audio and video datasets, evaluating performance and identifying modality-specific challenges.

### Open Question 3
- **Question:** How does the quality of supplementary information impact the performance of generated programs, and can automated methods be developed to generate high-quality supplementary information?
- **Basis in paper:** [explicit] The paper discusses the use of supplementary information to improve program generation but does not analyze the impact of information quality.
- **Why unresolved:** The paper does not investigate how different qualities of supplementary information affect program performance or explore automated generation methods.
- **What evidence would resolve it:** Experiments varying the quality of supplementary information and developing automated methods to generate it, then measuring the impact on program performance.

## Limitations
- Evaluation relies on synthetic datasets and limited real-world benchmarks, leaving complex task performance unclear
- Program generation quality depends heavily on prompt engineering, which is not fully specified
- Image extension tested on only one dataset, with worst-group accuracy improvements potentially stemming from reducing spurious correlations

## Confidence
**High confidence:** The fundamental premise that generating programs is cheaper than repeated API calls is well-established. The cost reduction mechanism (reducing GPT-4 calls from thousands to tens) is clearly demonstrated with specific numbers.

**Medium confidence:** The weak supervision aggregation approach and its effectiveness across different numbers of programs is reasonably supported, though optimal program diversity and aggregation methods could vary by task.

**Low confidence:** The modality extension claims and worst-group accuracy improvements need more extensive validation. The generality of program generation across diverse real-world annotation tasks is not fully established.

## Next Checks
1. **Program generation reliability test:** Generate programs for 5 different text classification tasks using the same prompt template and measure compilation success rate and label accuracy variance across tasks.

2. **Cost-benefit scaling analysis:** Systematically vary the number of generated programs (5, 10, 15, 20) on a real dataset and measure the accuracy improvement vs. marginal cost increase to identify the optimal program diversity point.

3. **Modality extension stress test:** Apply the image program generation approach to 3 different image classification datasets beyond Waterbirds and compare both average accuracy and worst-group performance against direct multimodal prompting baselines.