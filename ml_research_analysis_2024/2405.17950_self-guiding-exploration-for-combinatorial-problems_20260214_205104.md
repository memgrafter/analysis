---
ver: rpa2
title: Self-Guiding Exploration for Combinatorial Problems
arxiv_id: '2405.17950'
source_url: https://arxiv.org/abs/2405.17950
tags:
- problem
- prompting
- combinatorial
- problems
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Self-Guiding Exploration (SGE), a novel prompting
  strategy designed to solve combinatorial problems (CPs) using Large Language Models
  (LLMs). SGE autonomously generates multiple thought trajectories for each CP task,
  decomposes them into subtasks, executes them sequentially, and refines the results.
---

# Self-Guiding Exploration for Combinatorial Problems

## Quick Facts
- arXiv ID: 2405.17950
- Source URL: https://arxiv.org/abs/2405.17950
- Reference count: 40
- Primary result: SGE outperforms existing prompting strategies by over 27.84% in CP optimization performance

## Executive Summary
Self-Guiding Exploration (SGE) is a novel prompting strategy designed to solve combinatorial problems using Large Language Models. The approach autonomously generates multiple thought trajectories for each problem instance, decomposes them into manageable subtasks, executes them sequentially, and refines the results. This method demonstrates significant improvements over existing prompting strategies, particularly for complex NP-hard problems.

## Method Summary
SGE introduces a systematic approach to combinatorial problem solving by leveraging LLM capabilities through multi-path exploration. The method generates diverse thought trajectories for each problem instance, breaking down complex problems into sequential subtasks. Each trajectory is executed and refined independently, allowing the model to explore multiple solution paths simultaneously. The approach is particularly effective for NP-hard problems like Job Scheduling, where it achieves a 34.85% smaller optimality gap compared to baseline methods.

## Key Results
- SGE outperforms existing prompting strategies by over 27.84% in CP optimization performance
- Achieves 2.46% higher accuracy in other reasoning tasks (arithmetic, commonsense, symbolic)
- Demonstrates 34.85% smaller optimality gap compared to baseline methods on Job Scheduling tasks

## Why This Works (Mechanism)
The mechanism behind SGE's effectiveness lies in its ability to explore multiple solution paths simultaneously through autonomous thought trajectory generation. By decomposing complex problems into sequential subtasks and executing multiple independent trajectories, the method captures diverse solution approaches that a single-path method might miss. The refinement process ensures that the best elements from each trajectory can be combined, leading to superior overall performance.

## Foundational Learning
1. **Combinatorial Optimization**: Understanding of NP-hard problems and solution space characteristics
   - Why needed: To properly formulate and approach complex problem instances
   - Quick check: Can identify whether a given problem is NP-hard and its typical solution space properties

2. **LLM Prompting Strategies**: Knowledge of chain-of-thought, tree-of-thought, and other prompting approaches
   - Why needed: To understand how SGE differs from and improves upon existing methods
   - Quick check: Can explain the key differences between SGE and traditional prompting strategies

3. **Decomposition Techniques**: Methods for breaking down complex problems into manageable subtasks
   - Why needed: To understand how SGE structures its thought trajectories
   - Quick check: Can identify appropriate decomposition strategies for different problem types

## Architecture Onboarding

**Component Map**: Problem Input -> Trajectory Generator -> Subtask Decomposition -> Sequential Execution -> Result Refinement -> Final Solution

**Critical Path**: The most time-consuming component is the Trajectory Generator, as it must create multiple diverse thought paths for each problem instance. This directly impacts the overall computational overhead and scalability of the method.

**Design Tradeoffs**: SGE prioritizes solution quality over computational efficiency by generating multiple trajectories. This creates a tradeoff between achieving better solutions and increased processing time. The refinement step adds complexity but enables the combination of insights from different trajectories.

**Failure Signatures**: Performance degradation is most likely to occur when:
- Problem instances are too large for effective trajectory generation
- Subtask decomposition becomes too complex or ambiguous
- The refinement process cannot effectively merge divergent solution paths

**First Experiments**:
1. Test SGE on small-scale Job Scheduling instances to verify basic functionality
2. Compare single-trajectory vs. multi-trajectory performance on simple combinatorial problems
3. Evaluate the impact of trajectory diversity on solution quality

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to real-world combinatorial problems with significantly larger solution spaces remains untested
- Computational overhead from generating and evaluating multiple thought trajectories is not fully characterized
- Performance on industrial-scale instances with thousands of variables or constraints is unknown

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| SGE's superiority in combinatorial optimization tasks | High |
| Generalization to other reasoning tasks (arithmetic, commonsense, symbolic) | Medium |
| Robustness across different LLM architectures | Low |

## Next Checks

1. Test SGE on industrial-scale combinatorial instances (e.g., vehicle routing with 1000+ nodes) to evaluate scalability limits and computational efficiency trade-offs.

2. Conduct ablation studies comparing SGE against state-of-the-art specialized solvers (e.g., OR-Tools) on the same NP-hard problem instances to establish practical competitiveness.

3. Evaluate SGE's performance across multiple LLM architectures (GPT-4, Claude, open-source models) to assess generalizability and identify potential architectural dependencies.