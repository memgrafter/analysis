---
ver: rpa2
title: 'Banyan: Improved Representation Learning with Explicit Structure'
arxiv_id: '2407.17771'
source_url: https://arxiv.org/abs/2407.17771
tags:
- banyan
- embeddings
- learning
- structure
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Banyan, a model that learns semantic representations
  by leveraging explicit hierarchical structure through an entangled tree graph and
  diagonalized message passing functions. Unlike large-scale transformers that struggle
  in low-resource settings, Banyan achieves comparable performance to transformer
  baselines with only 14 non-embedding parameters.
---

# Banyan: Improved Representation Learning with Explicit Structure

## Quick Facts
- arXiv ID: 2407.17771
- Source URL: https://arxiv.org/abs/2407.17771
- Authors: Mattia Opper; N. Siddharth
- Reference count: 36
- Key outcome: Banyan achieves strong semantic similarity performance with only 14 non-embedding parameters, outperforming transformer baselines in low-resource settings.

## Executive Summary
Banyan introduces a novel approach to semantic representation learning that leverages explicit hierarchical structure through entangled trees and diagonalized message passing functions. Unlike large-scale transformers that struggle in low-resource settings, Banyan achieves comparable performance with minimal parameters. The model demonstrates superior efficiency and effectiveness in semantic textual similarity tasks across English and multiple low-resource languages, providing a viable alternative for resource-constrained environments.

## Method Summary
Banyan learns semantic representations by constructing entangled tree structures from input text, where repeated phrases are merged into shared nodes. The model uses diagonalized message passing functions (sigmoid-gated scalar multiplication) to propagate information up and down the tree structure. Unlike traditional approaches that use full linear transformations, Banyan's diagonal functions reduce parameters while enforcing hierarchical information decay. The model can use either contrastive or cross-entropy loss objectives, with experiments showing cross-entropy performs better in this architecture.

## Key Results
- Banyan achieves strong performance on semantic textual similarity tasks with only 14 non-embedding parameters
- Outperforms transformer baselines in low-resource settings (10 million tokens) across 9 languages
- Demonstrates superior parameter efficiency compared to models like GloVe and RoBERTa while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1: Entangled Tree Structure Reduces False Negatives
Entangled trees merge repeated semantic constituents across contexts into shared nodes, ensuring each unique phrase has exactly one representation. This prevents treating the same semantic unit as different negatives during contrastive learning, which can harm training efficiency.

### Mechanism 2: Diagonal Functions Enforce Hierarchical Compression
Diagonal composition functions use sigmoid-gated scalar multiplication instead of full linear transformations, forcing representations to respect the tree's hierarchical compression order. This hierarchical decay ensures representations conform to the compositional structure of language.

### Mechanism 3: Cross-Entropy Implicitly Targets Contrastive Properties
Diagonal functions' simplicity forces representations to optimize for reconstruction quality, which naturally produces the uniformity and alignment properties that contrastive learning explicitly enforces. This makes cross-entropy more effective than contrastive loss in this architecture.

## Foundational Learning

- **Hierarchical tree structures in language**: Banyan relies on understanding how linguistic constituents combine hierarchically to form meaning. Quick check: Can you explain why "the cat sat" has a different hierarchical structure than "the sat cat" and how this affects meaning?

- **Message passing in graph neural networks**: The core mechanism involves passing information up and down an entangled tree using diagonal composition/decomposition functions. Quick check: What's the difference between message passing in standard GNNs versus the recursive message passing used in Banyan?

- **Contrastive learning objectives**: The paper compares cross-entropy to contrastive objectives and shows how diagonal functions affect both. Quick check: How does the contrastive loss encourage embeddings to be both uniform and aligned, and why are both properties important?

## Architecture Onboarding

- **Component map**: Token embeddings → BPE tokenization → Entangled tree construction → Diagonal composition (upward) → Diagonal decomposition (downward) → Cross-entropy loss (leaf reconstruction) or contrastive loss (structure)
- **Critical path**: Input tokens → Structure induction (entangling) → Message passing with diagonal functions → Embedding generation → Loss computation
- **Design tradeoffs**: Diagonal functions reduce parameters dramatically but may limit expressiveness; entangled trees improve efficiency but assume semantic constituents are context-independent
- **Failure signatures**: Poor performance on semantic similarity tasks suggests issues with either the structure induction or the diagonal functions; memory issues suggest problems with the entangling process
- **First 3 experiments**:
  1. Test different U values (channel sizes) to find the optimal parameter-efficiency tradeoff
  2. Compare performance with and without diagonal functions using the same structure induction method
  3. Evaluate the impact of entangled vs standard trees using identical message passing functions

## Open Questions the Paper Calls Out

### Open Question 1
Does Banyan's performance advantage persist when trained on larger datasets beyond the 10 million token threshold? The paper demonstrates effectiveness at smaller scale but doesn't explore whether this advantage holds as data availability increases toward medium-resource scenarios.

### Open Question 2
How does Banyan's greedy agglomerative clustering for structure induction compare to more sophisticated approaches like differentiable parsing or reinforcement learning? The paper uses a simple method but acknowledges this may be suboptimal without empirical comparison.

### Open Question 3
What is the theoretical justification for why diagonalized message passing functions outperform original linear layer formulations? While empirical results show clear improvements, the paper doesn't fully explain the underlying mechanism.

## Limitations

- The entangled tree construction algorithm contains ambiguity in line 19 regarding whether it checks for string equality or node identity
- Ablation studies don't fully isolate the effects of diagonal functions versus entangled structures due to different training objectives used
- Multilingual evaluation is based on relatively small test sets (250-750 pairs per language), which may not fully capture generalization capabilities

## Confidence

- **High confidence**: Banyan achieves strong performance on STS tasks with minimal parameters (14 non-embedding parameters)
- **Medium confidence**: Superiority of diagonal composition functions over full linear transformations in this context
- **Low confidence**: Specific claim that entangled trees reduce false negatives in contrastive learning

## Next Checks

1. Implement and test Banyan with different U values (channel sizes) ranging from 2 to 256 to map out the parameter-efficiency tradeoff curve
2. Create a controlled ablation study comparing entangled vs. sentential trees using identical training objectives
3. Evaluate Banyan's robustness to structure quality by training on corrupted tree structures where some constituent merges are randomly prevented