---
ver: rpa2
title: 'Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns
  to Model Representations'
arxiv_id: '2408.15417'
source_url: https://arxiv.org/abs/2408.15417
tags:
- named
- embeddings
- lily
- context
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Next-token prediction (NTP) is the dominant paradigm for training
  large language models, yet the relationship between NTP training and the geometry
  of model representations remains unclear. This paper develops a framework to analyze
  how NTP shapes the implicit geometry of word and context embeddings.
---

# Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns to Model Representations

## Quick Facts
- arXiv ID: 2408.15417
- Source URL: https://arxiv.org/abs/2408.15417
- Authors: Yize Zhao; Tina Behnia; Vala Vakilian; Christos Thrampoulidis
- Reference count: 40
- Primary result: NTP training implicitly biases models toward learning logits with sparse plus low-rank structure, causing context embeddings to collapse based on shared next-token support sets.

## Executive Summary
This paper analyzes how next-token prediction (NTP) training shapes the implicit geometry of word and context embeddings in language models. The key insight is framing NTP as soft-label classification over sparse probabilistic label vectors, which leads to a formulation of NTP training as rank-constrained, nuclear-norm regularized optimization. The analysis reveals that NTP implicitly favors learning logits with a sparse plus low-rank structure: the sparse component captures co-occurrence frequencies while the low-rank component depends solely on the sparsity pattern of the co-occurrence matrix. This causes representations of contexts followed by the same set of next-tokens to collapse in an appropriate subspace, a phenomenon termed "subspace collapse."

## Method Summary
The paper frames NTP as soft-label classification using probabilistic label vectors and derives an analytical approximation that allows unrestricted generation of context embeddings. This leads to a formulation of NTP training as rank-constrained, nuclear-norm regularized optimization in the logit domain. The analysis assumes large embedding dimensions (d ≥ V) and uses regularization path analysis to characterize the implicit bias. The framework is validated through synthetic and real language datasets, comparing learned embeddings with theoretical predictions and a heuristic proxy based on the support-set matrix.

## Key Results
- NTP implicitly biases learning toward a sparse plus low-rank structure in the logits, with the low-rank component dominating as training progresses
- The low-rank component Lmm depends only on the sparsity pattern of the co-occurrence matrix, not on token frequencies
- Context embeddings with identical support sets collapse in an appropriate subspace, regardless of frequency distribution within those sets
- The sparse component Lin ensures accurate prediction of in-support token frequencies, enabling the loss to reach the empirical entropy lower bound

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NTP implicitly biases learning toward a sparse plus low-rank structure in the logits.
- Mechanism: As regularization decreases during training, the logits decompose into two orthogonal components: a sparse component (Lin) encoding in-support token frequencies and a low-rank component (Lmm) capturing the sparsity pattern of the co-occurrence matrix. The low-rank component dominates as training progresses, causing context embeddings with identical support sets to align (subspace collapse).
- Core assumption: The embedding dimension d is sufficiently large (d ≥ V).
- Evidence anchors:
  - [abstract]: "In large embedding spaces, we find that NTP implicitly favors learning logits with a sparse plus low-rank structure."
  - [section]: "Combining the two theorems, Lλ converges to Lin in F and diverges in F⊥, where it converges directionally to Lmm."
  - [corpus]: Weak evidence; no direct citations supporting this mechanism found.
- Break condition: If d < V, the rank constraint becomes active and the decomposition may not occur.

### Mechanism 2
- Claim: The low-rank component Lmm depends only on the sparsity pattern of the co-occurrence matrix, not on token frequencies.
- Mechanism: Lmm is the solution to a nuclear-norm minimization problem that enforces a margin between in-support and off-support tokens. This creates a geometric structure where context embeddings collapse based on shared support sets, regardless of the frequency distribution within those sets.
- Core assumption: The sparsity pattern S contains enough structure for the nuclear-norm minimization to produce a meaningful solution.
- Evidence anchors:
  - [abstract]: "the orthogonal low-rank component, which becomes dominant as training progresses, depends solely on the sparsity pattern of the co-occurrence matrix."
  - [section]: "Lmm is independent of these frequencies and is influenced only by the sparsity pattern S ∈ {0, 1}V×m."
  - [corpus]: No direct evidence found; relies on theoretical derivation.
- Break condition: If the sparsity pattern is random or lacks structure, Lmm may not encode meaningful linguistic information.

### Mechanism 3
- Claim: The sparse component Lin ensures accurate prediction of in-support token frequencies, enabling the loss to reach the empirical entropy lower bound.
- Mechanism: Lin interpolates the soft-labels on the subspace of in-support tokens, ensuring that the model's output probabilities match the observed frequencies for tokens that actually follow each context.
- Core assumption: The regularization path analysis accurately models the behavior of gradient descent in the NTP setting.
- Evidence anchors:
  - [abstract]: "the sparse component captures the co-occurrence frequency of context-word pairs."
  - [section]: "On the subspace F, Lλ satisfies the log-odds constraints indicated in Thm. 2; this gives rise to Claim (C4)."
  - [corpus]: No direct evidence; relies on theoretical analysis and controlled experiments.
- Break condition: If the regularization path assumption fails (e.g., gradient descent does not follow the path), the frequency prediction may be inaccurate.

## Foundational Learning

- Concept: Soft-label classification and cross-entropy loss
  - Why needed here: NTP is framed as predicting a probability distribution over next tokens, not just a single correct token.
  - Quick check question: What is the difference between hard-label and soft-label classification in the context of NTP?

- Concept: Nuclear-norm minimization and low-rank structure
  - Why needed here: The low-rank component Lmm arises from minimizing the nuclear norm of the logits, which promotes low-rank solutions.
  - Quick check question: How does nuclear-norm minimization differ from rank minimization, and why is it used here?

- Concept: Support sets and sparsity patterns
  - Why needed here: The sparsity pattern of the co-occurrence matrix determines the structure of the logits and embeddings.
  - Quick check question: How does the support set of a context relate to the geometry of its embedding?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model training -> Analysis -> Visualization

- Critical path:
  1. Extract distinct contexts and their support sets from training data
  2. Train model until loss approaches empirical entropy lower bound
  3. Analyze learned logits and embeddings for sparse plus low-rank structure
  4. Verify subspace collapse and frequency interpolation properties

- Design tradeoffs:
  - Embedding dimension d vs. vocabulary size V: Must have d ≥ V for theoretical guarantees
  - Model expressiveness: Need sufficient capacity to minimize loss to entropy lower bound
  - Computational cost: SVD of large matrices can be expensive; proxy methods may be needed

- Failure signatures:
  - Loss does not approach entropy lower bound: Model may be underparameterized or optimization stuck
  - No subspace collapse observed: Embedding dimension too small or sparsity pattern lacks structure
  - Incorrect frequency prediction: Sparse component Lin not properly learned

- First 3 experiments:
  1. Train NTP-UFM on synthetic data with known support sets; verify sparse plus low-rank logits and subspace collapse
  2. Train transformer on Simplified TinyStories; compare learned embeddings to theoretical predictions and proxy
  3. Vary embedding dimension d; observe impact on loss convergence and geometric structure

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what precise conditions does the centered support matrix eS equal the optimal solution Lmm of NTP-SVM⋆?
- Basis in paper: [explicit] The paper conjectures that eS closely approximates Lmm but notes this is not always optimal (e.g., Fig. 10 shows cases where eS fails the dual-certificate condition).
- Why unresolved: The dual-certificate condition in Prop. 3 provides a sufficient but not necessary criterion. The paper does not characterize when eS is optimal or derive conditions on S for this to hold.
- What evidence would resolve it: A full characterization of the optimality conditions for eS (e.g., sufficient and necessary conditions on the sparsity pattern S) and empirical validation on diverse datasets showing when eS exactly matches Lmm.

### Open Question 2
- Question: Can the implicit bias analysis be extended to the non-convex regime where d < V, i.e., when the embedding dimension is smaller than the vocabulary size?
- Basis in paper: [explicit] The analysis assumes d ≥ V to ensure convexity of the rank-constrained problem. The paper notes that if Lmm is low-rank, d ≥ rank(Lmm) might suffice, but this is not explored.
- Why unresolved: Relaxing d < V reintroduces non-convexity, and the paper does not investigate whether the sparse plus low-rank structure still emerges or how the geometry changes.
- What evidence would resolve it: Theoretical analysis or experiments showing whether and how the sparse plus low-rank logit structure persists when d < V, and what role low-rankness of Lmm plays.

### Open Question 3
- Question: Does gradient descent optimization of the non-convex NTP objective follow the regularization path identified in the analysis, or does it converge to a different solution?
- Basis in paper: [explicit] The paper uses the regularization path as a proxy for GD but acknowledges this equivalence is not rigorously established in non-convex settings.
- Why unresolved: The analysis characterizes the limit of vanishing regularization, but GD dynamics in non-convex NTP training may differ due to initialization, learning rate schedules, or stochasticity.
- What evidence would resolve it: Empirical comparison of the learned logits/representations from GD training versus the analytical prediction, and theoretical conditions under which GD aligns with the regularization path.

### Open Question 4
- Question: How does the sparsity pattern of the co-occurrence matrix influence the low-rankness and symmetry of the SVD factors of Lmm?
- Basis in paper: [inferred] The paper observes that Lmm depends only on the sparsity pattern S and that eS serves as a proxy, but does not characterize how specific properties of S (e.g., distribution of support set sizes, overlaps) affect the rank or structure of Lmm.
- Why unresolved: While the paper links Lmm to sparsity patterns, it does not provide a quantitative or qualitative theory connecting linguistic statistics to the geometry of Lmm.
- What evidence would resolve it: A theoretical framework or empirical study showing how different support set configurations (e.g., varying support set sizes, overlaps) affect the rank, singular values, or symmetry of Lmm.

### Open Question 5
- Question: What is the degree of overparameterization required for the NTP loss to reach its empirical entropy lower bound in realistic language datasets?
- Basis in paper: [explicit] The paper assumes sufficient overparameterization for the loss to approach the entropy lower bound but does not quantify this requirement or test it empirically across architectures.
- Why unresolved: The analysis is asymptotic in d, but practical models may not achieve the entropy lower bound, raising questions about the conditions under which the theory applies.
- What evidence would resolve it: Empirical studies measuring the loss gap to the entropy lower bound as a function of model size, embedding dimension, and dataset characteristics, alongside theoretical bounds on required overparameterization.

## Limitations

- The theoretical analysis relies heavily on regularization-path approximations that may not fully capture gradient descent behavior in NTP training
- Analysis assumes large embedding dimensions (d ≥ V) that may not hold in practice, limiting applicability to smaller models
- The assumption of a deterministic NTP setting with known soft labels is idealized compared to the stochastic, online nature of typical NTP training

## Confidence

- High confidence: The existence of subspace collapse and its relationship to support sets (verified empirically across multiple datasets)
- Medium confidence: The sparse plus low-rank decomposition of logits and its emergence during training (theoretically derived but dependent on regularization path assumptions)
- Low confidence: The exact mathematical relationship between the low-rank component Lmm and the sparsity pattern (primarily theoretical, with limited direct empirical verification)

## Next Checks

1. Systematically vary the embedding dimension d relative to vocabulary size V and measure the breakdown point where subspace collapse no longer occurs, testing the d ≥ V theoretical requirement.

2. Train models with different regularization schedules (warmup, decay rates) to verify that the emergence of the low-rank component correlates with the reduction in regularization strength as predicted.

3. Apply the theoretical framework to non-linear models with explicit sparsity constraints (e.g., attention dropout) to determine whether manual sparsity induction affects the emergence of the predicted geometric structure.