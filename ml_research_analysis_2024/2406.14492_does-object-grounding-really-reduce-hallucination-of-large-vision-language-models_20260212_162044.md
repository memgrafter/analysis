---
ver: rpa2
title: Does Object Grounding Really Reduce Hallucination of Large Vision-Language
  Models?
arxiv_id: '2406.14492'
source_url: https://arxiv.org/abs/2406.14492
tags:
- hallucination
- image
- vicuna
- grounding
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether grounding objectives reduce hallucinations
  in large vision-language models (LVLMs). While prior work suggests that grounding
  improves LVLM performance, the empirical support for this claim is weak and mainly
  limited to question-answering evaluation.
---

# Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models?

## Quick Facts
- arXiv ID: 2406.14492
- Source URL: https://arxiv.org/abs/2406.14492
- Authors: Gregor Geigle; Radu Timofte; Goran Glavaš
- Reference count: 34
- Primary result: Grounding objectives have little to no effect on object hallucination in open caption generation

## Executive Summary
This paper challenges the prevailing assumption that grounding objectives reduce hallucinations in large vision-language models (LVLMs). Through extensive experiments across three different LLM backbones (Vicuna, Llama-3, Phi-3) and two grounding objectives (referring expressions and grounded captioning), the authors find that grounding objectives have minimal impact on object hallucination in open caption generation. While explicit grounding prompts at inference can slightly reduce hallucination, the effect is small and comes at the cost of reduced caption informativeness. The study provides strong empirical evidence that grounding objectives, despite their theoretical appeal, do not substantially mitigate hallucination in LVLMs.

## Method Summary
The authors evaluate hallucination in LVLMs by training models with and without grounding objectives on standard vision-language tasks. They use three LLM backbones (Vicuna 1.5 7B, Llama-3 8B, Phi-3-mini) with frozen parameters and LoRA adapters for updates. Training includes standard captioning, VQA, referring expressions (RefCOCO + Visual Genome), and grounded captioning (Flickr30k-Entities) tasks. They evaluate hallucination using both QA-based metrics (POPE) and open-ended captioning metrics (CHAIR, FaithScore) on MSCOCO and Objects365 datasets, comparing four model variants: Base (no grounding), +GC (grounded captioning), +RE (referring expressions), and +RE+GC (both objectives).

## Key Results
- Grounding objectives have little to no effect on object hallucination in open caption generation
- Explicit grounding prompts at inference slightly reduce hallucination but reduce caption informativeness
- QA-based evaluation metrics (POPE) show modest improvements but open captioning metrics show minimal differences
- Grounded captioning models show similar hallucination rates to non-grounded models despite improved grounding performance on specific tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding objectives reduce hallucinations by forcing the model to align generated text with visual regions through explicit region-text alignment
- Mechanism: By including referring expressions and grounded captioning as training objectives, LVLMs learn to better associate language with specific image regions, making them less likely to hallucinate objects not present in the image
- Core assumption: Improved fine-grained image understanding from grounding objectives will transfer to hallucination reduction in open caption generation
- Evidence anchors: Abstract states grounding objectives reduce hallucination; weak corpus support suggests limited empirical validation
- Break condition: When grounding objectives fail to improve fine-grained image understanding or when alignment doesn't transfer to open-ended generation tasks

### Mechanism 2
- Claim: Grounding objectives improve hallucination detection by making models more likely to include bounding box coordinates when describing objects
- Mechanism: By training LVLMs to generate grounded captions with interleaved bounding box coordinates, the model learns to only describe objects it can properly ground in the image
- Core assumption: Models trained with grounding objectives will be less likely to hallucinate when explicitly prompted to generate grounded captions at inference
- Evidence anchors: Abstract mentions slight hallucination reduction with grounded captions at inference; moderate corpus support shows grounding discussed but limited hallucination reduction evidence
- Break condition: When models still hallucinate objects despite grounded caption prompts, or when grounding process itself becomes hallucinatory

### Mechanism 3
- Claim: Grounding objectives reduce hallucination by improving the model's ability to distinguish between objects that are present versus absent in the image
- Mechanism: Through training on referring expressions and grounded captioning, LVLMs learn to better discriminate between actual visual content and imagined content, reducing false positive object generation
- Core assumption: Improved grounding ability will generalize to better overall object recognition and hallucination prevention
- Evidence anchors: Abstract reveals grounding objectives have little effect on object hallucination; strong corpus support shows multiple papers examining grounding with limited hallucination reduction effects
- Break condition: When discrimination ability doesn't improve despite grounding training, or when models over-generalize grounding to hallucinated objects

## Foundational Learning

- Concept: Vision-language model architecture (image encoder + LLM + alignment module)
  - Why needed here: Understanding how LVLMs process visual and textual information is crucial for understanding how grounding objectives affect hallucination
  - Quick check question: What are the three main components of a typical LVLM architecture and how do they interact?

- Concept: Object hallucination and its measurement (CHAIR, FaithScore, POPE)
  - Why needed here: To understand how grounding objectives are claimed to reduce hallucination and how this effect is measured
  - Quick check question: What are the key differences between CHAIR, FaithScore, and POPE in measuring LVLM hallucination?

- Concept: Grounding objectives (referring expressions, grounded captioning)
  - Why needed here: These are the specific mechanisms being evaluated for their effect on hallucination reduction
  - Quick check question: How do referring expressions and grounded captioning objectives differ in their approach to aligning text with image regions?

## Architecture Onboarding

- Component map: Image encoder -> Alignment module (MLP projection or perceiver-resampler) -> LLM backbone (Vicuna/Llama-3/Phi-3) -> Training objectives (captioning, VQA, RE, GC)
- Critical path: Image encoding and projection to LLM space → Text generation conditioned on visual input → Grounding objective training to improve region-text alignment → Hallucination evaluation on held-out data
- Design tradeoffs: MLP vs perceiver-resampler for alignment module; standard vs grounded captioning at inference; different backbone LLMs (Vicuna vs Llama-3 vs Phi-3); training data composition proportions
- Failure signatures: No improvement in hallucination metrics despite grounding training; degradation in caption quality (CIDEr, CLIPScore) when adding grounding objectives; grounding objectives improving one task but not transferring to open generation
- First 3 experiments: 1) Train baseline LVLM without grounding objectives and measure hallucination on MSCOCO; 2) Add referring expressions objective and measure hallucination reduction; 3) Add grounded captioning objective and measure hallucination reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of grounding objectives in reducing hallucinations scale with model size or training data volume?
- Basis in paper: The paper notes computational constraints limited training to less data and fewer steps than many other works, stating they "cannot rule out that a reduction in hallucination due to grounding objectives might emerge at some larger scale of grounding training"
- Why unresolved: Experiments were conducted on models trained with limited data and for fewer steps than many other LVLM works
- What evidence would resolve it: Experiments training LVLMs with grounding objectives on significantly larger datasets and for more training steps, comparing hallucination rates against baseline models of similar scale

### Open Question 2
- Question: Are there specific types of hallucinations (e.g., based on object categories, attributes, or relations) that are more or less susceptible to grounding-based mitigation?
- Basis in paper: The paper mentions qualitative inspection revealed LVLMs trained with grounding objectives still "incorrectly describe objects or fabricate them entirely," and that bounding boxes can be "positionally correct" but describe wrong objects (e.g., elephant bounding box around rhino)
- Why unresolved: Analysis focused on overall hallucination rates rather than categorizing hallucinations by type
- What evidence would resolve it: Detailed analysis categorizing hallucinations by type (objects, attributes, relations) and measuring grounding objective effectiveness separately for each category

### Open Question 3
- Question: Could alternative region encoding methods (beyond text-based coordinate representation) improve the effectiveness of grounding objectives in reducing hallucinations?
- Basis in paper: The paper adopted "relative coordinates as text" to avoid introducing additional trainable parameters, but acknowledges "different approaches exist for representing image regions for the LVLMs," including learned embeddings
- Why unresolved: Study used only one region representation method (text-based coordinates) and did not explore whether other encoding approaches might yield different results
- What evidence would resolve it: Comparative experiments using different region encoding methods (e.g., learned embeddings, different coordinate representations) with grounding objectives to determine if encoding choices impact hallucination reduction effectiveness

## Limitations
- Study focuses primarily on object hallucination rather than other types (attribute, relation, numerical)
- Limited exploration of different grounding objective formulations beyond the two examined
- Effect of grounding on hallucination detection versus hallucination generation is not fully disentangled

## Confidence

**Confidence: High** for the core finding that grounding objectives have little effect on object hallucination in open caption generation. The extensive empirical evidence across multiple model variants and evaluation metrics strongly supports this conclusion.

**Confidence: Medium** for the claim that grounding objectives improve hallucination detection in QA-based evaluation. While POPE metrics show some improvement, the effect is modest and the authors acknowledge limitations in how these metrics capture hallucination.

**Confidence: Medium** for the finding that explicit grounding prompts at inference slightly reduce hallucination at the cost of caption informativeness. This effect is small and requires further investigation to understand the tradeoff between hallucination reduction and caption quality.

## Next Checks

1. Test whether grounding objectives show benefits for attribute or relation hallucination, which may respond differently than object hallucination
2. Evaluate the temporal stability of hallucination reduction effects - do models maintain grounding benefits over extended use or with additional fine-tuning?
3. Investigate whether hybrid approaches combining grounding objectives with other hallucination mitigation techniques (like CLIP-guided decoding) show synergistic effects beyond either approach alone