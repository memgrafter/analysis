---
ver: rpa2
title: 'Benchmarking Multi-Image Understanding in Vision and Language Models: Perception,
  Knowledge, Reasoning, and Multi-Hop Reasoning'
arxiv_id: '2406.12742'
source_url: https://arxiv.org/abs/2406.12742
tags:
- images
- reasoning
- arxiv
- image
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MIRB, a benchmark for evaluating multi-image
  reasoning in vision-language models (VLMs). MIRB addresses the gap in existing benchmarks,
  which primarily focus on single-image tasks.
---

# Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning

## Quick Facts
- arXiv ID: 2406.12742
- Source URL: https://arxiv.org/abs/2406.12742
- Reference count: 34
- Primary result: MIRB benchmark reveals significant gaps in VLMs' multi-image reasoning capabilities despite strong single-image performance

## Executive Summary
This paper introduces MIRB, a novel benchmark designed to evaluate multi-image reasoning capabilities in vision-language models (VLMs). The benchmark addresses a critical gap in existing evaluation frameworks by focusing on tasks that require comparison and analysis across multiple images. MIRB encompasses four categories: perception, visual world knowledge, reasoning, and multi-hop reasoning. The evaluation of state-of-the-art VLMs on MIRB reveals that while open-source models perform comparably to GPT-4V in single-image tasks, they significantly lag behind in multi-image reasoning tasks, even struggling with basic multi-image comparisons.

## Method Summary
MIRB is a benchmark dataset consisting of 925 multi-image reasoning tasks across four categories. The evaluation methodology involves running state-of-the-art VLMs through the benchmark using greedy decoding for open-source models and the "2024-05-13" API version for GPT-4V. The evaluation script processes multiple images as input and measures accuracy across different task categories. The benchmark includes tasks ranging from simple perception comparisons to complex multi-hop reasoning problems requiring integration of information across 2-42 images.

## Key Results
- Open-source VLMs approach GPT-4V performance on single-image tasks but show significant gaps in multi-image reasoning
- Even GPT-4V struggles with MIRB, indicating the fundamental complexity of multi-image reasoning
- Text inputs consistently outperform image inputs for tasks involving structured data like code understanding
- Simple prompting strategies like Chain-of-Thought show minimal improvement for multi-image reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MIRB exposes a fundamental gap in VLMs' ability to reason across multiple images
- Mechanism: Tasks require integration of information across at least two images, such as code understanding and visual analogies
- Core assumption: VLMs must actively compare and synthesize across multiple visual inputs
- Evidence anchors: Benchmark design focuses on multi-image reasoning with independently sourced images
- Break condition: If models can solve MIRB tasks by processing images independently or by simple concatenation

### Mechanism 2
- Claim: Text-based reasoning is easier for VLMs than image-based reasoning in multi-image tasks
- Mechanism: When equivalent text is provided instead of images, VLMs perform significantly better
- Core assumption: VLMs have stronger language understanding than visual understanding for structured data
- Evidence anchors: Results show text inputs are usually better than image inputs for code understanding tasks
- Break condition: If VLMs show comparable or better performance with image inputs for structured data tasks

### Mechanism 3
- Claim: Simple prompting strategies like Chain-of-Thought do not effectively enhance multi-image reasoning
- Mechanism: Zero-shot Chain-of-Thought prompting showed minimal improvement or decreased performance
- Core assumption: Multi-image reasoning requires architectural or training improvements rather than simple prompting
- Evidence anchors: Results show only minor improvements or sometimes decreases with CoT prompting
- Break condition: If future research demonstrates Chain-of-Thought significantly improves multi-image reasoning

## Foundational Learning

- Concept: Multi-image reasoning
  - Why needed here: MIRB evaluates models' ability to compare, analyze, and reason across multiple images
  - Quick check question: What is the key difference between single-image and multi-image reasoning tasks?

- Concept: Visual world knowledge integration
  - Why needed here: MIRB includes tasks requiring combining visual perception with prior world knowledge
  - Quick check question: Why is visual world knowledge important for tasks like food comparisons across multiple images?

- Concept: Chain-of-Thought prompting limitations
  - Why needed here: The paper shows simple prompting strategies don't effectively address multi-image reasoning challenges
  - Quick check question: What does the paper suggest is needed instead of simple prompting techniques?

## Architecture Onboarding

- Component map: MIRB benchmark -> Evaluation script -> VLMs (with vision encoder and LLM) -> Performance metrics
- Critical path: Data preparation → Model inference → Result aggregation → Analysis
- Design tradeoffs: MIRB focuses on multi-image reasoning at the expense of covering single-image tasks comprehensively
- Failure signatures: Models perform well on single-image tasks but poorly on multi-image tasks; performance drops when images are concatenated; text inputs outperform image inputs for structured data
- First 3 experiments:
  1. Evaluate VLMs on single-image vs. multi-image versions of the same task to quantify performance gap
  2. Compare performance when using concatenated images vs. separate image encoding
  3. Test text-based vs. image-based inputs for structured data tasks like code understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VLMs on MIRB correlate with their performance on single-image benchmarks?
- Basis in paper: The paper states that while open-source VLMs approach GPT-4V performance in single-image tasks, there is a significant gap in multi-image reasoning tasks
- Why unresolved: The paper does not provide a detailed correlation analysis between single-image and multi-image performance
- What evidence would resolve it: A statistical analysis showing the correlation between performance on MIRB and existing single-image benchmarks for various VLMs

### Open Question 2
- Question: Can the design of VLMs be improved to better handle text information within images?
- Basis in paper: The paper mentions that text inputs are usually better than image inputs for tasks involving text understanding
- Why unresolved: The paper does not explore architectural or training modifications to improve text decoding within images
- What evidence would resolve it: Experimental results comparing VLMs with enhanced text decoding capabilities against standard VLMs on MIRB tasks involving text within images

### Open Question 3
- Question: What specific architectural or training strategies could improve VLMs' performance on multi-image reasoning tasks?
- Basis in paper: The paper highlights the need for further research and development in VLMs to enhance their multi-image understanding capabilities
- Why unresolved: The paper does not propose or test specific architectural or training strategies
- What evidence would resolve it: Comparative studies of VLMs with different architectural designs or training regimes on MIRB

## Limitations
- Limited evidence supporting the three proposed mechanisms for why MIRB effectively exposes VLMs' limitations
- Corpus does not provide corroborating evidence for specific claims about VLMs' deficiencies
- Single experiment on Chain-of-Thought prompting without testing alternative prompting strategies
- Unknown preprocessing steps for code-to-image conversion in evaluation

## Confidence

*High Confidence* in MIRB being a novel benchmark addressing a gap in multi-image reasoning evaluation
*Medium Confidence* in the finding that VLMs show significant gaps in multi-image reasoning performance
*Low Confidence* in the three proposed mechanisms explaining MIRB's effectiveness

## Next Checks

1. Conduct ablation studies where single images from multi-image tasks are presented independently to determine if models can solve tasks without cross-image reasoning
2. Perform systematic comparison of text-based versus image-based inputs across broader range of structured data tasks
3. Test multiple prompting strategies beyond Chain-of-Thought including few-shot examples, structured prompts, and model-specific techniques