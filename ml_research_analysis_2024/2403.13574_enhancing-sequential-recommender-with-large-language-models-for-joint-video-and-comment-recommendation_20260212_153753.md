---
ver: rpa2
title: Enhancing Sequential Recommender with Large Language Models for Joint Video
  and Comment Recommendation
arxiv_id: '2403.13574'
source_url: https://arxiv.org/abs/2403.13574
tags:
- video
- comment
- recommendation
- user
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the joint problem of video and comment recommendation
  on online video platforms, aiming to enhance user engagement by leveraging both
  video and comment interaction histories. The authors propose LSVCR, a framework
  that combines a sequential recommendation (SR) model with a supplemental large language
  model (LLM) recommender.
---

# Enhancing Sequential Recommender with Large Language Models for Joint Video and Comment Recommendation

## Quick Facts
- arXiv ID: 2403.13574
- Source URL: https://arxiv.org/abs/2403.13574
- Authors: Bowen Zheng; Zihan Lin; Enze Liu; Chen Yang; Enyang Bai; Cheng Ling; Wayne Xin Zhao; Ji-Rong Wen
- Reference count: 40
- Primary result: 4.13% increase in comment watch time in online A/B testing

## Executive Summary
This paper addresses the joint problem of video and comment recommendation on online video platforms by leveraging both video and comment interaction histories. The authors propose LSVCR, a framework that combines a sequential recommendation model with a supplemental large language model recommender. The approach uses a two-stage training paradigm where the LLM enhances semantic understanding during training but is discarded for efficient deployment, resulting in significant improvements over competitive baselines for both video and comment recommendation tasks.

## Method Summary
The LSVCR framework employs a two-stage training paradigm to enhance sequential recommendation for joint video and comment recommendation. First, a sequential recommendation (SR) model serves as the primary backbone for efficient user preference modeling, while a large language model (LLM) acts as a supplemental recommender during training. The LLM processes user interaction histories through textual instruction-based preference modeling, generating enhanced preference representations. In the personalized preference alignment stage, contrastive learning aligns representations from both components through sequential-supplemental preference contrast and video-comment preference contrast mechanisms. Finally, the recommendation-oriented fine-tuning stage optimizes the SR model for specific tasks before deployment, discarding the LLM for efficiency.

## Key Results
- 4.13% increase in comment watch time achieved in online A/B testing on KuaiShou platform
- Significant improvements over competitive baselines for both video and comment recommendation tasks
- Effective performance on unseen videos and users, demonstrating good generalization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential-supplemental preference contrast aligns the SR model's representations with those from the LLM recommender by leveraging contrastive learning with in-batch negatives.
- Mechanism: The model generates preference representations from both the SR model (s) and the LLM recommender (s̃) for the same input. It then applies InfoNCE loss to pull these representations closer in the embedding space while pushing apart other representations in the batch.
- Core assumption: The LLM recommender's enhanced preference representation (s̃) contains semantically richer information than the SR model's representation alone, and this semantic richness can be transferred to the SR model through contrastive alignment.
- Evidence anchors:
  - [abstract]: "we introduce a two-stage training paradigm. The first stage, personalized preference alignment, aims to align the preference representations from both components"
  - [section 4.3.1]: "we propose two alignment perspectives: sequential-supplemental preference contrast and video-comment preference contrast, to align preference semantics from both components"
  - [corpus]: Found 25 related papers; average neighbor FMR=0.47; neighbor titles include "Comment Staytime Prediction with LLM-enhanced Comment Understanding" - suggesting active research in LLM-enhanced comment modeling
- Break condition: If the LLM's semantic understanding capability is poor or if the preference representations from SR and LLM are too dissimilar in structure, the contrastive alignment will fail to transfer meaningful information.

### Mechanism 2
- Claim: Video-comment preference contrast disentangles user preferences from video and comment interaction sequences by aligning representations based on task context.
- Mechanism: When the LLM is instructed to recommend videos, its preference representation should align with the video sequence representation rather than the comment sequence representation, and vice versa. This is enforced through contrastive loss that pulls together representations from the same task while pushing apart cross-task representations.
- Core assumption: User preferences derived from video interactions and comment interactions are distinct but complementary, and the model can learn to disentangle these preference signals through task-aware contrastive learning.
- Evidence anchors:
  - [abstract]: "we consider the online micro-video scenario, where users can freely watch interesting videos and further view comments below specific videos"
  - [section 4.3.2]: "we disentangle user preferences derived from video and comment interaction sequences, based on the enhanced preference representation provided by the LLM recommender"
  - [corpus]: Neighbor paper "Comment Staytime Prediction with LLM-enhanced Comment Understanding" indicates complementary relationship between video and comment data
- Break condition: If video and comment preferences are not sufficiently distinct, or if the task instructions to LLM are ambiguous, the disentanglement will not work effectively.

### Mechanism 3
- Claim: The two-stage training paradigm allows efficient deployment by using LLM only during training while retaining a lightweight SR model for inference.
- Mechanism: During training, the SR model and LLM work together through personalized preference alignment. In the second stage, the LLM is discarded and the alignment-enhanced SR model is fine-tuned for specific tasks. During deployment, only the fine-tuned SR model is used.
- Core assumption: The semantic enhancements gained from the LLM during training can be effectively transferred to and retained by the SR model, making the LLM unnecessary for deployment.
- Evidence anchors:
  - [abstract]: "The SR model functions as the primary recommendation backbone (retained in deployment) of our method for efficient user preference modeling. Concurrently, we employ a LLM as the supplemental recommender (discarded in deployment)"
  - [section 4.4.2]: "In practical deployment, we retain the fine-tuned SR model for recommendation, and discard the supplemental LLM recommender"
  - [corpus]: The framework explicitly mentions LLM being "discarded in deployment" suggesting this is a core design choice
- Break condition: If the semantic enhancements from LLM cannot be effectively transferred to SR model, or if the SR model's performance degrades significantly after LLM removal, the two-stage approach will fail.

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: To align preference representations between SR model and LLM recommender by pulling positive pairs together and pushing negative pairs apart
  - Quick check question: What is the mathematical form of the InfoNCE loss used in this paper for preference alignment?

- Concept: Sequential recommendation modeling with Transformers
  - Why needed here: To model user interaction sequences (both video and comment) as the primary backbone for efficient user preference modeling
  - Quick check question: How does the paper handle positional encoding for sequence modeling, and why is it randomized during the alignment stage?

- Concept: Large language model instruction tuning for recommendation
  - Why needed here: To leverage LLM's semantic understanding capabilities by formatting user interaction histories as textual instructions for enhanced preference modeling
  - Quick check question: What are the specific instruction templates used for video and comment recommendation tasks in the LLM recommender?

## Architecture Onboarding

- Component map: User interaction data -> Text embedding -> SR model processing -> Preference extraction -> Alignment with LLM -> Fine-tuning -> Deployment (SR model only)
- Critical path: User interaction data → Text embedding → SR model processing → Preference extraction → Alignment with LLM → Fine-tuning → Deployment (SR model only)
- Design tradeoffs:
  - LLM usage during training provides semantic richness but increases training cost
  - Two-stage approach enables efficient deployment but requires careful alignment
  - Randomized positional encoding during alignment reduces computational cost but may affect sequence modeling quality
- Failure signatures:
  - Poor performance on either video or comment recommendation indicates alignment failure
  - Significant drop in performance after LLM removal suggests insufficient transfer of semantic enhancements
  - Training instability or divergence may indicate incorrect contrastive loss formulation
- First 3 experiments:
  1. Test alignment effectiveness by comparing SR model performance with and without LLM alignment on a small validation set
  2. Validate contrastive loss implementation by checking if positive pairs have higher similarity than negative pairs
  3. Verify deployment efficiency by measuring inference time and memory usage of the final SR model compared to baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LSVCR vary when using different large language models (LLMs) with varying sizes and architectures?
- Basis in paper: [explicit] The authors mention using ChatGLM3 as the LLM backbone, but do not explore the impact of using different LLMs or model sizes.
- Why unresolved: The paper does not provide a systematic comparison of different LLM choices or their impact on recommendation performance.
- What evidence would resolve it: Conducting experiments with various LLMs (e.g., GPT-3, LLaMA, Claude) and comparing their performance on video and comment recommendation tasks would provide insights into the optimal LLM choice for LSVCR.

### Open Question 2
- Question: What is the impact of comment quality and diversity on the effectiveness of LSVCR's personalized preference alignment?
- Basis in paper: [inferred] The paper mentions using hot comments and sampling techniques for comment diversification, but does not investigate the relationship between comment quality/diversity and recommendation performance.
- Why unresolved: The authors do not provide an analysis of how different types of comments (e.g., high-quality vs. low-quality, diverse vs. similar) affect the alignment process and downstream task performance.
- What evidence would resolve it: Conducting experiments with different comment selection strategies and analyzing their impact on alignment quality and recommendation metrics would shed light on the importance of comment quality and diversity.

### Open Question 3
- Question: How does LSVCR perform in scenarios with limited user interaction data or cold-start situations?
- Basis in paper: [explicit] The authors mention that LSVCR achieves good performance on unseen videos and users, but do not extensively explore its effectiveness in cold-start scenarios.
- Why unresolved: The paper does not provide a detailed analysis of LSVCR's performance when dealing with new users or items with minimal interaction history.
- What evidence would resolve it: Conducting experiments on cold-start user and item scenarios, comparing LSVCR's performance with traditional recommendation methods, and analyzing the impact of different initialization strategies would provide insights into LSVCR's cold-start capabilities.

## Limitations
- The paper lacks detailed ablation studies showing the individual contributions of sequential-supplemental preference contrast versus video-comment preference contrast
- No comparison with simpler approaches that might achieve similar results without the complexity of LLM integration
- Limited discussion of computational overhead during the training phase when using the supplemental LLM recommender

## Confidence
- High: The core two-stage training paradigm and the conceptual framework of using LLM as a supplemental component during training while retaining a lightweight SR model for deployment
- Medium: The effectiveness of the personalized preference alignment mechanism, as the paper demonstrates improvements but doesn't fully explore the sensitivity to alignment hyperparameters
- Low: The generalizability of results to other platforms beyond KuaiShou, given the specialized nature of short video and comment interactions

## Next Checks
1. Conduct ablation studies to isolate the impact of each alignment mechanism (sequential-supplemental vs. video-comment preference contrast) on recommendation performance
2. Test the framework's performance on a different video platform or domain to assess generalizability beyond the KuaiShou dataset
3. Measure and report the training time overhead introduced by the two-stage paradigm with LLM integration to better understand the practical deployment tradeoffs