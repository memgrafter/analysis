---
ver: rpa2
title: Self-Evaluation as a Defense Against Adversarial Attacks on LLMs
arxiv_id: '2407.03234'
source_url: https://arxiv.org/abs/2407.03234
tags:
- attack
- evaluator
- vicuna
- defense
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a self-evaluation defense against adversarial
  attacks on large language models (LLMs) that requires no model fine-tuning. The
  method uses pre-trained LLMs to classify both inputs and outputs as safe or unsafe,
  reducing the attack success rate (ASR) of adversarial suffixes from 95.0% to 0.0%
  for attacked samples while maintaining high utility on safe inputs.
---

# Self-Evaluation as a Defense Against Adversarial Attacks on LLMs

## Quick Facts
- arXiv ID: 2407.03234
- Source URL: https://arxiv.org/abs/2407.03234
- Authors: Hannah Brown; Leon Lin; Kenji Kawaguchi; Michael Shieh
- Reference count: 32
- One-line primary result: Self-evaluation defense reduces adversarial suffix attack success rate from 95.0% to 0.0% without model fine-tuning

## Executive Summary
This paper introduces a self-evaluation defense mechanism that uses pre-trained large language models to classify both inputs and outputs as safe or unsafe, effectively protecting against adversarial attacks. The method requires no model fine-tuning and can be implemented with minimal overhead. By decoupling safety classification from generation, the defense significantly reduces attack success rates on both open and closed-source LLMs while maintaining high utility on safe inputs. The approach outperforms existing defenses like Llama-Guard2 and commercial content moderation APIs, particularly when adversarial suffixes are present.

## Method Summary
The self-evaluation defense uses a pre-trained LLM as an evaluator to classify whether user inputs and/or model outputs are harmful. The system implements three evaluation settings: input-only (evaluates the user prompt), output-only (evaluates the model response), and input-output (evaluates both). The evaluator is given a simple instruction to classify content as safe or unsafe without fine-tuning. When the evaluator determines content is unsafe, the pipeline refuses to answer. The method is tested against adversarial suffixes generated using the Greedy Coordinate Gradient (GCG) algorithm on 100 harmful AdvBench samples and 100 safe GPT-4 generated instructions.

## Key Results
- Reduces attack success rate of adversarial suffixes from 95.0% to 0.0% for attacked samples
- Outperforms existing defenses including Llama-Guard2 and commercial content moderation APIs
- Maintains high utility on safe inputs while providing strong protection on unsafe inputs
- Shows resilience to adaptive attacks, with combined ASR remaining lower than undefended models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-evaluation defense reduces adversarial suffix attack success rate by classifying both inputs and outputs as safe or unsafe.
- Mechanism: The evaluator model independently assesses whether the user input X and/or model output Y is harmful. If E determines the content is unsafe, the pipeline refuses to answer, creating an additional safeguard that adversarial suffixes cannot easily bypass.
- Core assumption: The evaluator can accurately distinguish between safe and unsafe content, including samples containing adversarial suffixes.
- Evidence anchors:
  - [abstract] "Our method can significantly reduce the attack success rate of attacks on both open and closed-source LLMs, beyond the reductions demonstrated by Llama-Guard2 and commonly used content moderation APIs."
  - [section 3.5] "For inputs with adversarial suffixes appended, our defense drastically reduces the ASR as compared to the undefended generator, bringing the ASR to near 0.0 for all evaluators, generators, and settings."
- Break condition: The adversary successfully trains an adversarial suffix that simultaneously attacks both the generator and evaluator, causing the evaluator to misclassify unsafe content as safe.

### Mechanism 2
- Claim: Self-evaluation defense maintains high utility on safe inputs while reducing ASR on unsafe inputs.
- Mechanism: The evaluator only intervenes when it detects unsafe content. For safe inputs, the generator responds normally without additional overhead, preserving utility while adding safety when needed.
- Core assumption: The evaluator rarely misclassifies safe inputs as unsafe (low false positive rate).
- Evidence anchors:
  - [section 3.5] "We also consider the evaluator's impact on safe input performance. The addition of the evaluator should not decrease generator responses on safe inputs."
  - [section 3.6] "We observe that, with nothing appended, no evaluator significantly reduces the response rate of the system as shown in Table 1."
- Break condition: The evaluator becomes overly conservative and starts refusing safe inputs with high frequency, making the system practically useless.

### Mechanism 3
- Claim: Self-evaluation defense is more resilient to adaptive attacks than existing defenses like Llama-Guard2 and commercial APIs.
- Mechanism: The self-evaluation defense decouples safety classification from generation, making it harder for adversarial suffixes to target both components simultaneously. Even when attacked, the combined ASR remains lower than undefended models.
- Core assumption: The generator and evaluator models have different vulnerabilities to adversarial attacks.
- Evidence anchors:
  - [abstract] "We demonstrate that by decoupling safety classification from generation, our defense is challenging to attack in adaptive attack settings using the strongest existing attacks."
  - [section 4] "Though we find it is possible to attack the evaluator, it requires training a separate adversarial suffix targeted to the evaluator, and in the worst case, using our defense yields lower ASR values than an undefended generator."
- Break condition: The adversary successfully trains universal adversarial suffixes that can attack both generator and evaluator simultaneously across multiple model architectures.

## Foundational Learning

- Concept: Adversarial suffixes
  - Why needed here: Understanding how adversarial suffixes work is fundamental to grasping why the self-evaluation defense is effective. The paper specifically addresses attacks using adversarial suffixes.
  - Quick check question: What is the primary mechanism by which adversarial suffixes bypass model safety alignment?

- Concept: Attack success rate (ASR)
  - Why needed here: ASR is the key metric used to evaluate the effectiveness of both attacks and defenses. The paper shows significant ASR reduction using self-evaluation.
  - Quick check question: How is ASR calculated in the context of adversarial attacks on LLMs?

- Concept: Input-output classification
  - Why needed here: The paper tests three settings (input-only, output-only, input-output) for the evaluator, each with different cost and effectiveness tradeoffs.
  - Quick check question: What are the key differences between input-only, output-only, and input-output evaluation settings in terms of cost and attack resilience?

## Architecture Onboarding

- Component map:
  - User query → Evaluator (input evaluation) → Generator → Evaluator (output evaluation) → User response or refusal
  - Generator (G): The LLM that processes user queries and generates responses
  - Evaluator (E): The LLM that classifies inputs/outputs as safe or unsafe
  - Attack vector: Adversarial suffix appended to user query

- Critical path:
  1. User provides query with potential adversarial suffix
  2. Evaluator classifies query as safe/unsafe
  3. If unsafe, pipeline returns refusal
  4. If safe, generator processes query
  5. Generator produces response
  6. Evaluator classifies response as safe/unsafe
  7. If unsafe, pipeline returns refusal
  8. If safe, pipeline returns response to user

- Design tradeoffs:
  - Input-only vs. output-only vs. input-output evaluation: Lower cost but potentially less effective (input-only) vs. higher cost but more comprehensive protection (input-output)
  - Model size vs. cost: Larger evaluators may be more accurate but more expensive to run
  - Speed vs. thoroughness: Faster evaluation may miss some attacks but provides better user experience

- Failure signatures:
  - High false positive rate: Safe inputs being incorrectly classified as unsafe
  - High false negative rate: Unsafe inputs being incorrectly classified as safe
  - Performance degradation: System becomes significantly slower than undefended generator
  - Over-refusal: System refuses to answer valid queries even without adversarial suffixes

- First 3 experiments:
  1. Measure baseline ASR of generator with no defense on 100 harmful AdvBench samples with adversarial suffixes
  2. Test input-only evaluation with Vicuna-7B as both generator and evaluator on the same sample set
  3. Compare ASR reduction between input-only, output-only, and input-output settings across multiple evaluator-generator pairs

## Open Questions the Paper Calls Out
The paper acknowledges it only examines English language inputs and outputs, following the Bender Rule (Bender, 2011), and cannot guarantee the effectiveness of the method for other languages. It also notes that future work considering other attacks beyond adversarial suffixes would represent valuable contributions to the area.

## Limitations
- Only tested on English language inputs and outputs, limiting generalizability to other languages
- Focus exclusively on adversarial suffixes as attacks, leaving vulnerability to other attack vectors unknown
- Does not comprehensively explore the space of possible adaptive attacks or their effectiveness against different evaluator-generator combinations

## Confidence
- **High confidence**: The core claim that self-evaluation reduces ASR on attacked samples (95.0% to near 0.0%) is well-supported by experimental results across multiple evaluator-generator pairs and settings.
- **Medium confidence**: The claim that self-evaluation outperforms existing defenses like Llama-Guard2 and commercial APIs is supported but limited to the specific evaluation framework used.
- **Low confidence**: The assertion that self-evaluation is "challenging to attack in adaptive attack settings using the strongest existing attacks" is qualified by the authors themselves, showing adaptive attacks can succeed but with lower ASR than undefended models.

## Next Checks
1. Replicate ASR calculation methodology using the exact evaluation criteria, including any specific refusal patterns or manual review processes, to verify reported ASR reductions are reproducible.
2. Test the self-evaluation defense against a broader range of adversarial attacks beyond AdvBench, including real-world jailbreak attempts and other attack methodologies not covered in the original evaluation.
3. Systematically test whether certain combinations of evaluator and generator models are more vulnerable to adaptive attacks, and whether specific model architectures provide better defense decoupling than others.