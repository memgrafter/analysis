---
ver: rpa2
title: 'F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural
  Machine Translation'
arxiv_id: '2404.04846'
source_url: https://arxiv.org/abs/2404.04846
tags:
- domain
- task
- memory
- f-malloc
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces F-MALLOC, a continual learning method for
  neural machine translation that addresses catastrophic forgetting by leveraging
  feed-forward layers as memory cells. The method decomposes feed-forward layers,
  prunes them to preserve general domain knowledge, and learns task-specific masks
  to allocate memory for new tasks while protecting previously allocated memory from
  being overwritten.
---

# F-MALLOC: Feed-forward Memory Allocation for Continual Learning in Neural Machine Translation

## Quick Facts
- **arXiv ID**: 2404.04846
- **Source URL**: https://arxiv.org/abs/2404.04846
- **Reference count**: 33
- **Primary result**: Achieves 40.57 BLEU vs 39.58 for best baseline with almost zero forgetting (0.71% vs 10.10%)

## Executive Summary
F-MALLOC addresses catastrophic forgetting in neural machine translation through a novel approach that treats feed-forward layers as memory cells. The method uses structured pruning to preserve general domain knowledge while creating writable memory capacity, then employs task-specific masks with gradient blocking to protect previously learned information. A comprehensive evaluation protocol demonstrates that F-MALLOC significantly outperforms existing continual learning methods in both translation quality and forgetting reduction across multiple domain adaptation scenarios.

## Method Summary
F-MALLOC decomposes feed-forward layers into discrete memory cells and allocates these memories to different tasks. It first uses JS divergence-based structural pruning to trim the feed-forward layers of a pretrained NMT model, preserving crucial general domain knowledge while freeing writable memory. During continual learning, task-specific masks are learned to dynamically allocate memory to new tasks while protecting previously allocated memory from being overwritten through gradient blocking. The method employs temperature annealing to balance exploration and exploitation during mask learning.

## Key Results
- Outperforms best baseline by 0.99 BLEU points (40.57 vs 39.58)
- Reduces forgetting ratio from 10.10% to 0.71% compared to best baseline
- Shows robustness to domain order and efficient memory allocation based on task difficulty

## Why This Works (Mechanism)

### Mechanism 1
Pruning feed-forward layers to retain "general domain knowledge" while freeing "writable" memory prevents forgetting. Structural pruning removes unimportant feed-forward memories while preserving crucial ones. The pruned model retains general domain knowledge in "read-only" memories, which are then protected from gradient updates during subsequent task learning. Core assumption: Feed-forward layers function as neural memory repositories, and their importance can be quantified via Jensen-Shensen divergence from dropout-perturbed predictions.

### Mechanism 2
Task-specific masks dynamically allocate feed-forward memory to new tasks while protecting previously allocated memory. Learnable task masks activate specific feed-forward memories for each task. Once allocated, memories are marked "read-only" by blocking gradient updates through them, preventing catastrophic forgetting. Core assumption: Non-exclusive task masks allow memory reuse between similar tasks, enabling knowledge transfer while maintaining task separation.

### Mechanism 3
Temperature annealing in task mask learning enables exploration-exploitation balance during memory allocation. The temperature parameter τ in the sigmoid gate function controls mask "hardness." During training, τ decreases from τmax to 0, forcing the model to gradually commit to specific memory allocations while initially exploring different configurations. Core assumption: Gradual mask polarization through temperature annealing leads to optimal memory allocation without premature commitment.

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - Why needed here: The paper's core problem is preventing forgetting during sequential task learning, requiring understanding of why neural networks forget
  - Quick check question: What happens to neural network parameters when fine-tuning on a new task without any regularization?

- **Concept**: Feed-forward layer architecture and function
  - Why needed here: The method specifically targets feed-forward layers as memory cells, requiring understanding of their structure and role in transformers
  - Quick check question: How does the feed-forward layer in transformers differ from attention mechanisms in terms of parameter usage and function?

- **Concept**: Pruning and mask-based parameter control
  - Why needed here: The method uses structured pruning and task masks to control which parameters are active for which tasks
  - Quick check question: What's the difference between unstructured and structured pruning, and why does structured pruning matter for this approach?

## Architecture Onboarding

- **Component map**: Pretrained NMT model (frozen except feed-forward layers) -> Pruning module (JS divergence-based importance scoring) -> Task mask learning system (temperature-annealed sigmoid gates) -> Gradient blocking mechanism (element-wise max aggregation of previous masks) -> Evaluation protocol (BLEU, forgetting ratio, saturation ratio)

- **Critical path**: 1. Pretrained model loading -> 2. Feed-forward layer pruning -> 3. Task sequence processing -> 4. For each task: forward pass with task mask -> 5. Backward pass with gradient blocking -> 6. Mask accumulation for next task

- **Design tradeoffs**: Memory capacity vs. task separation; Mask exclusivity vs. knowledge transfer; Temperature annealing speed vs. exploration quality

- **Failure signatures**: High forgetting ratio despite low saturation ratio (masks not protecting previous task memories); Low BLEU on new tasks despite low forgetting (insufficient capacity allocated); Slow convergence or unstable training (temperature annealing schedule inappropriate)

- **First 3 experiments**: 1. Verify pruning preserves general domain performance while creating writable capacity; 2. Test task mask learning on single new task; 3. Validate gradient blocking mechanism

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding F-MALLOC's scalability and adaptability. It doesn't provide specific details on how the memory allocation strategy adapts to task difficulty and inter-task similarities in real-time during training. The theoretical upper bound on the number of tasks F-MALLOC can handle before performance degradation becomes significant remains unexplored. Additionally, the paper doesn't investigate how F-MALLOC's performance compares to non-continual learning methods when the number of tasks exceeds available memory capacity, nor does it explore extending the method to handle tasks with significantly different data distributions or modalities.

## Limitations

- Pruning importance metric (JS divergence) lacks direct corpus validation for its effectiveness
- Temperature annealing schedule appears somewhat arbitrary without ablation studies
- Non-exclusive task masks could theoretically cause interference between similar domains
- Limited empirical testing on task order robustness and memory allocation efficiency

## Confidence

- **High confidence**: Catastrophic forgetting reduction claims (0.71% vs 10.10% best baseline)
- **Medium confidence**: BLEU score improvements (40.57 vs 39.58 best baseline)
- **Low confidence**: Claims about task order robustness and memory allocation efficiency

## Next Checks

1. Ablation study removing temperature annealing to verify its necessity for optimal memory allocation
2. Test with more diverse domain pairs (e.g., highly dissimilar domains) to validate interference claims
3. Scale up experiments to larger model sizes and more domains to test method robustness at scale