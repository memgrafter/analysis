---
ver: rpa2
title: 'Linearly Constrained Weights: Reducing Activation Shift for Faster Training
  of Neural Networks'
arxiv_id: '2403.13833'
source_url: https://arxiv.org/abs/2403.13833
tags:
- activation
- shift
- layer
- variance
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies the activation shift phenomenon in neural
  networks, where preactivation values have non-zero mean depending on the angle between
  weight vectors and activation vector means. To address this, the author proposes
  linearly constrained weights (LCW), where weight vectors sum to zero, reducing activation
  shift.
---

# Linearly Constrained Weights: Reducing Activation Shift for Faster Training of Neural Networks

## Quick Facts
- arXiv ID: 2403.13833
- Source URL: https://arxiv.org/abs/2403.13833
- Reference count: 33
- Primary result: LCW reduces activation shift, enabling efficient training of deep sigmoid networks and improving generalization when combined with batch normalization

## Executive Summary
This paper identifies activation shift as a fundamental phenomenon in neural networks where preactivation values have non-zero mean depending on the angle between weight vectors and activation vector means. To address this, the author proposes linearly constrained weights (LCW) where weight vectors sum to zero, effectively eliminating activation shift. The theoretical analysis shows that LCW ensures equal variance amplification in forward and backward propagation, addressing the vanishing gradient problem. Experimental results demonstrate that LCW enables efficient training of deep sigmoid MLPs and improves generalization when combined with batch normalization in both feedforward and convolutional networks.

## Method Summary
The method proposes linearly constrained weights (LCW) as a solution to activation shift in neural networks. LCW constrains weight vectors to the subspace where their elements sum to zero, which can be implemented through weight reparameterization using an orthonormal basis. The approach involves decomposing unconstrained weights into components in the LCW subspace and its orthogonal complement, then updating only the LCW component during training. This constraint eliminates activation shift by ensuring zero expected preactivation values regardless of activation vector means. The method can be combined with batch normalization for improved generalization performance.

## Key Results
- LCW enables efficient training of deep feedforward networks with sigmoid activation functions by resolving the vanishing gradient problem
- Combined with batch normalization, LCW improves generalization performance in both feedforward and convolutional networks
- Theoretical analysis shows LCW ensures equal variance amplification in forward and backward propagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Activation shift creates asymmetric variance amplification between forward and backward propagation, leading to vanishing gradients in deep sigmoid networks.
- Mechanism: Preactivation values have non-zero mean depending on the angle between weight vectors and activation vector means. This asymmetry causes variance to grow faster in the forward chain than the backward chain, degrading gradients exponentially in deep layers.
- Core assumption: Activation vectors in hidden layers approximately follow a distribution with non-zero mean, and weight vectors are not constrained to zero-mean subspace.
- Evidence anchors:
  - [abstract] "activation shift, a simple but remarkable phenomenon in a neural network in which the preactivation value of a neuron has non-zero mean that depends on the angle between the weight vector of the neuron and the mean of the activation vector in the previous layer"
  - [section 4.3] "In fully connected layers with standard weights, the variance of variables in the network is more amplified in the forward chain than in the backward chain by the weight rescaling"
  - [corpus] Weak - no direct corpus support for activation shift causing vanishing gradients
- Break condition: If activation vectors have zero mean or weight vectors lie in the zero-mean subspace, activation shift is eliminated and variance amplification becomes symmetric.

### Mechanism 2
- Claim: Linearly constrained weights (LCW) enforce weight vectors to sum to zero, eliminating activation shift and restoring symmetric variance amplification.
- Mechanism: By constraining weight vectors to the subspace WLC = {w ∈ Rm | w · 1m = 0}, the expected preactivation value becomes zero regardless of activation vector mean, ensuring equal variance amplification in forward and backward propagation.
- Core assumption: The activation vector approximately follows Pγ (distribution with mean γ1m) in each layer, making LCW effective at reducing activation shift.
- Evidence anchors:
  - [abstract] "LCW is a weight vector subject to the constraint that the sum of its elements is zero"
  - [section 3] "We propose the use of so-called linearly constrained weights (LCW) to resolve the activation shift"
  - [section 3.1] "It is obvious that wl i = Bmvl i ∈ WLC"
- Break condition: If activation vectors deviate significantly from Pγ distribution, LCW may not perfectly eliminate activation shift.

### Mechanism 3
- Claim: LCW combined with batch normalization (BN) improves generalization performance while accelerating training.
- Mechanism: LCW accelerates training by reducing activation shift and resolving vanishing gradients, while BN maintains generalization ability by normalizing preactivation values and further reducing activation shift through mean removal.
- Core assumption: Both LCW and BN independently contribute to reducing activation shift through different mechanisms.
- Evidence anchors:
  - [abstract] "Experimental results show that LCW enables a deep feedforward network with sigmoid activation functions to be trained efficiently by resolving the vanishing gradient problem. Moreover, combined with batch normalization, LCW improves generalization performance"
  - [section 6.1] "combined with BN, LCW achieves better performance in test loss/accuracy"
  - [corpus] Weak - no direct corpus support for LCW+BN synergy
- Break condition: If BN alone sufficiently reduces activation shift, adding LCW may provide diminishing returns.

## Foundational Learning

- Concept: Variance propagation in neural networks
  - Why needed here: Understanding how variance changes through layers is crucial for analyzing the impact of activation shift and LCW on gradient flow
  - Quick check question: In a fully connected layer with standard weights, does variance typically grow faster in the forward or backward chain?

- Concept: Linear algebra and subspace constraints
  - Why needed here: LCW relies on constraining weight vectors to a specific subspace (zero-sum vectors), requiring understanding of basis vectors and orthonormal decomposition
  - Quick check question: What is the dimension of the subspace WLC = {w ∈ Rm | w · 1m = 0}?

- Concept: Activation function properties
  - Why needed here: Different activation functions (sigmoid vs ReLU) have different effects on variance amplification, which interacts with activation shift and LCW effectiveness
  - Quick check question: How does the ReLU activation function affect variance amplification compared to sigmoid?

## Architecture Onboarding

- Component map:
  Input layer -> Fully connected layers with LCW (reparameterized weights) -> Optional BN layers -> Activation layers -> Output layer

- Critical path:
  Weight initialization with LCW constraint -> Forward pass through LCW-constrained layers -> Backward pass with gradient computation -> Parameter updates on reparameterized weights (vl_i instead of wl_i)

- Design tradeoffs:
  LCW vs BN: LCW reduces activation shift through weight constraints, BN through normalization; combining both provides complementary benefits
  Computational overhead: LCW adds basis matrix multiplication and orthonormal decomposition
  Flexibility: LCW is architectural constraint, BN is layer-based normalization

- Failure signatures:
  Training stalls or shows vanishing gradients: LCW not properly implemented or activation vectors deviate from Pγ distribution
  Overfitting: LCW alone may increase overfitting risk without regularization like BN
  Poor convergence: Incorrect weight initialization or learning rate issues specific to LCW reparameterization

- First 3 experiments:
  1. Implement LCW on a simple 2-layer MLP with sigmoid activation and compare training curves with standard weights
  2. Apply LCW to a VGG-style network with ReLU and observe if generalization improves with BN
  3. Test LCW on a deep sigmoid network (10+ layers) to verify vanishing gradient resolution compared to standard initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the activation shift phenomenon persist in networks with non-Gaussian activation distributions, such as those arising from complex data distributions or advanced activation functions?
- Basis in paper: [inferred] The paper analyzes activation shift primarily under Gaussian assumptions and suggests future work investigating other activation functions.
- Why unresolved: The paper's theoretical analysis and experiments focus on Gaussian activations and standard activation functions like sigmoid and ReLU, leaving the behavior with non-Gaussian activations unexplored.
- What evidence would resolve it: Experimental results showing the degree of activation shift in networks trained with various non-Gaussian activation functions or data distributions, compared to Gaussian cases.

### Open Question 2
- Question: How does the performance of LCW compare to other normalization techniques like batch normalization or layer normalization when applied to very deep networks with complex architectures (e.g., recurrent or attention-based models)?
- Basis in paper: [inferred] The paper mentions future work investigating the applicability of LCW to other neural network structures like recurrent ones.
- Why unresolved: While the paper demonstrates LCW's effectiveness in feedforward and convolutional networks, its performance in more complex architectures remains untested.
- What evidence would resolve it: Comparative experiments training very deep recurrent or attention-based models with LCW versus other normalization methods, measuring convergence speed and generalization.

### Open Question 3
- Question: What is the theoretical relationship between the activation shift and the optimization landscape of the loss function in deep neural networks?
- Basis in paper: [inferred] The paper discusses the connection between activation shift and vanishing gradients but does not explore its impact on the loss landscape.
- Why unresolved: The paper focuses on the practical implications of activation shift on training dynamics but does not provide a theoretical framework linking it to the optimization landscape.
- What evidence would resolve it: Theoretical analysis showing how activation shift affects the curvature or smoothness of the loss function, and how this impacts gradient-based optimization methods.

## Limitations

- The theoretical claims about activation shift causing vanishing gradients are supported by mathematical analysis but lack direct empirical measurement throughout training
- Comparison with existing normalization methods shows competitive performance but doesn't fully isolate the contribution of LCW from architectural differences
- The analysis assumes activation vectors approximately follow Pγ distribution, which may not hold in all network architectures or datasets

## Confidence

- **High Confidence**: The mathematical derivation of LCW and its effect on variance propagation is rigorous and internally consistent. The experimental results showing LCW enables training of deep sigmoid networks are reproducible and clearly demonstrate the practical impact.

- **Medium Confidence**: The claim that LCW combined with BN improves generalization beyond what either method achieves alone is supported by experimental results but lacks ablation studies to definitively prove synergy rather than additive effects.

- **Low Confidence**: The paper's assertion that activation shift is a widespread problem requiring LCW in modern networks with ReLU activations is weakly supported. The ReLU results show modest improvements, and the mechanism may not be as significant for non-saturating activations.

## Next Checks

1. **Activation Shift Measurement**: Instrument a standard network to measure the mean of preactivation values throughout training. Compare networks with and without LCW to quantify the reduction in activation shift and correlate it with training stability metrics like gradient norms.

2. **LCW vs BN Ablation**: Train networks with LCW alone, BN alone, and LCW+BN combinations on the same architecture. Use identical hyperparameters and training procedures to isolate whether the LCW+BN synergy provides benefits beyond either method individually.

3. **Activation Function Sensitivity**: Test LCW across different activation functions (sigmoid, tanh, ReLU, leaky ReLU) and varying network depths. Measure how the effectiveness of LCW varies with activation saturation levels and determine if the theoretical benefits are most pronounced for specific activation regimes.