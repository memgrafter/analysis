---
ver: rpa2
title: Brain-inspired continual pre-trained learner via silent synaptic consolidation
arxiv_id: '2410.05899'
source_url: https://arxiv.org/abs/2410.05899
tags:
- learning
- synapses
- pre-trained
- silent
- artsy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in pre-trained models
  during incremental learning tasks. The proposed Artsy framework draws inspiration
  from silent synapses in biological neural networks, where silent synapses with only
  NMDA receptors become functional by incorporating AMPA receptors upon receiving
  new stimuli.
---

# Brain-inspired continual pre-trained learner via silent synaptic consolidation

## Quick Facts
- arXiv ID: 2410.05899
- Source URL: https://arxiv.org/abs/2410.05899
- Authors: Xuming Ran; Juntao Yao; Yusong Wang; Mingkun Xu; Dianbo Liu
- Reference count: 9
- Primary result: Achieves 92.44% average accuracy and 87.94% last accuracy on CIFAR-100 with 10 steps, and 87.30% average accuracy and 84.59% last accuracy on TinyImageNet with 5 steps

## Executive Summary
This paper addresses catastrophic forgetting in pre-trained models during incremental learning tasks by introducing Artsy, a brain-inspired continual learning framework. The framework draws inspiration from silent synapses in biological neural networks, where synapses with only NMDA receptors become functional by incorporating AMPA receptors upon receiving new stimuli. Artsy connects pre-trained networks to initialized sub-networks through artificial silent and functional synapses, enabling effective learning of new tasks while preserving previously acquired knowledge.

The approach maintains memory stability in the pre-trained network while promoting learning plasticity in task-specific sub-networks. During inference, artificial silent and functional synapses facilitate connections between pre-trained and sub-networks for extracting relevant information. Experimental results demonstrate that Artsy significantly outperforms conventional methods on class-incremental learning tasks, achieving state-of-the-art performance on CIFAR-100 and TinyImageNet datasets.

## Method Summary
Artsy connects a frozen pre-trained ViT-B/16 backbone to task-specific adapter modules through artificial silent and functional synapses. The framework uses an MLP-based binary classifier to distinguish between data from previous tasks and new task data, determining which adapter features to activate. Silent synapses start with only NMDA-like connections and convert to functional synapses with AMPA-like weights when pre-synaptic activity and post-synaptic response meet a threshold. This mechanism allows the model to learn new tasks while preserving knowledge from previously learned tasks through the frozen pre-trained network.

## Key Results
- Achieves 92.44% average accuracy and 87.94% last accuracy on CIFAR-100 with 10 incremental steps
- Achieves 87.30% average accuracy and 84.59% last accuracy on TinyImageNet with 5 incremental steps
- Outperforms conventional continual learning methods by significant margins on both datasets
- Demonstrates effective prevention of catastrophic forgetting while maintaining learning plasticity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Silent synapses with only NMDA receptors become functional by incorporating AMPA receptors upon receiving new stimuli, enabling learning of new tasks while preserving old knowledge.
- Mechanism: The framework uses artificial silent synapses that start with only NMDA-like receptors (no AMPA-like connection weights). When new stimuli arrive, if the pre-synaptic activity and post-synaptic response meet a threshold, AMPA-like weights are inserted, converting the silent synapse into a functional one that connects pre-trained and sub-networks.
- Core assumption: The threshold-based activation of artificial synapses accurately mimics biological silent-to-functional conversion and effectively gates information flow between networks.
- Evidence anchors:
  - [abstract] "artificial silent and functional synapses are utilized to establish precise connections between the pre-synaptic neurons in the pre-trained network and the post-synaptic neurons in the sub-networks, facilitated through synaptic consolidation"
  - [section 3.1] "Artificial silent synapses require specific new stimuli to activate and convert into artificial functional synapses"
  - [corpus] Weak evidence - no direct corpus citations supporting the specific threshold mechanism
- Break condition: If the threshold calculation fails to capture task-relevant features, artificial synapses won't activate properly, leading to poor integration of new knowledge.

### Mechanism 2
- Claim: Maintaining pre-trained network parameters frozen while training task-specific sub-networks achieves memory stability for old tasks.
- Mechanism: The pre-trained network E0(x) remains fixed during training, acting as stable memory storage. Only the initialized sub-networks Et(x) are trained on new tasks, preventing interference with previously learned features.
- Core assumption: Freezing pre-trained weights preserves the rich feature representations learned from large datasets without catastrophic forgetting.
- Evidence anchors:
  - [section 3.1] "The parameters of the pre-trained network remain fixed during the continual learning process, facilitating the learning of new tasks"
  - [section 2.1] "the pre-trained network E0(x) : Xt′ → H t′"
  - [corpus] Weak evidence - no corpus citations directly validating the frozen pre-trained network approach
- Break condition: If the pre-trained features become inadequate for new tasks, the frozen network will bottleneck performance and prevent effective learning.

### Mechanism 3
- Claim: Binary classification of current task vs previous tasks determines which sub-network connections to activate during inference.
- Mechanism: An MLP-based binary classifier distinguishes between data from previous tasks (output 0) and new task data (output 1). The resulting mask mt determines which sub-network features to include in the final classification.
- Core assumption: The binary classifier can reliably distinguish task boundaries, ensuring correct routing of information through appropriate sub-networks.
- Evidence anchors:
  - [section 3.1] "The MLP-based binary classifier, comprising two fully connected layers, is trained to classify data from previous tasks Pt−1 i=0 Di as 0 and data from the current new task Dt as 1"
  - [section 4.5] "Using bad feature for activating the artificial synapse results in lower Last and Average accuracy compared to good feature"
  - [corpus] No corpus citations supporting the specific binary classification approach
- Break condition: If the classifier misclassifies task boundaries, incorrect sub-networks will be activated, causing catastrophic forgetting or inability to learn new tasks.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper addresses this fundamental problem in continual learning, where networks forget previously learned tasks when trained on new ones
  - Quick check question: What happens to neural network weights when training sequentially on different tasks without special techniques?

- Concept: Silent vs functional synapses in neuroscience
  - Why needed here: The framework draws direct inspiration from biological silent synapses that can become functional, providing the conceptual foundation for the artificial synapse mechanism
  - Quick check question: What distinguishes silent synapses from functional synapses at the molecular level?

- Concept: Adapter modules in vision transformers
  - Why needed here: The framework uses adapter modules to create lightweight task-specific sub-networks that can be trained efficiently without modifying the pre-trained backbone
  - Quick check question: How do adapter modules enable efficient transfer learning compared to full fine-tuning?

## Architecture Onboarding

- Component map: Pre-trained ViT-B/16 backbone (frozen) -> MLP-based binary classifier -> Adapter modules (one per task) -> Prototypical classifier
- Critical path: Input -> Pre-trained features -> Binary classifier decision -> Selected adapter features -> Prototype-based classification
- Design tradeoffs: Freezing pre-trained weights provides stability but may limit adaptability; adapter modules add flexibility but increase memory usage; binary classifier adds routing complexity but enables selective feature integration
- Failure signatures: Poor performance on new tasks suggests binary classifier isn't activating correct adapters; forgetting old tasks suggests adapters are interfering with pre-trained features; low overall accuracy suggests threshold mechanism isn't working properly
- First 3 experiments:
  1. Test binary classifier accuracy on distinguishing between task types using pre-trained features
  2. Verify that frozen pre-trained network maintains performance on previously seen tasks
  3. Test artificial synapse threshold mechanism with synthetic data to ensure proper activation behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do artificial silent synapses distinguish between in-distribution and out-of-distribution data during conversion to functional synapses?
- Basis in paper: [explicit] The paper mentions that artificial synapses need to activate when encountering novel stimuli, but notes that out-of-distribution detection methods could be more effective than the current MLP-based binary classifier
- Why unresolved: The current implementation uses an MLP-based binary classifier that may not be optimal for distinguishing between previously learned and novel data distributions
- What evidence would resolve it: Comparative experiments testing various out-of-distribution detection methods (like uncertainty estimation, likelihood ratio tests) against the current MLP approach on multiple continual learning datasets would demonstrate which method better handles the conversion of silent to functional synapses

### Open Question 2
- Question: What is the optimal threshold strategy for artificial silent synapses across different tasks and domains?
- Basis in paper: [explicit] The paper states that "different stimuli produce varying currents, leading to different silent synapses with distinct current thresholds to activate AMPA receptors" but doesn't explore adaptive thresholding
- Why unresolved: The paper uses fixed thresholds for artificial synapses but acknowledges that biological synapses have varying thresholds based on stimuli characteristics
- What evidence would resolve it: Experiments varying threshold initialization and adaptation strategies across diverse datasets would reveal whether dynamic thresholding improves performance over static thresholds

### Open Question 3
- Question: Can the Artsy framework scale effectively to larger pre-trained models (e.g., ViT-L/16 or beyond) while maintaining the balance between plasticity and stability?
- Basis in paper: [inferred] The paper uses ViT-B/16 as the pre-trained backbone and demonstrates effectiveness, but doesn't test scalability to larger architectures
- Why unresolved: Larger models may have different feature representations and require more sophisticated mechanisms to maintain the delicate balance between learning new tasks and preserving old knowledge
- What evidence would resolve it: Empirical studies applying Artsy to progressively larger pre-trained models on the same continual learning benchmarks would demonstrate whether performance scales proportionally or requires architectural modifications

## Limitations
- The framework relies on binary classification of task boundaries, which may not generalize well to datasets with subtle or overlapping task distinctions
- The artificial synapse threshold mechanism lacks biological validation and may not accurately capture the complex dynamics of silent-to-functional conversion
- Memory overhead increases linearly with the number of tasks due to maintaining separate adapter modules for each task

## Confidence
The confidence in the proposed mechanism is Medium. While the biological inspiration from silent synapses provides an intuitive framework, the translation to artificial synapses relies on several assumptions that lack direct empirical validation.

- High confidence: Pre-trained network freezing prevents catastrophic forgetting of old tasks
- Medium confidence: Binary classifier effectively distinguishes between task boundaries
- Low confidence: Artificial synapse threshold mechanism accurately mimics biological behavior

## Next Checks
1. **Cross-dataset generalization**: Test Artsy on a dataset with significant domain shift between tasks (e.g., CORe50 or DomainNet) to verify that the binary classifier and frozen pre-trained network maintain performance when task boundaries are less distinct.

2. **Ablation of threshold mechanism**: Conduct controlled experiments where the artificial synapse threshold is varied systematically to determine the optimal activation criteria and assess sensitivity to parameter choices.

3. **Memory overhead analysis**: Measure the memory and computational costs of maintaining multiple adapter modules across tasks, comparing against alternative continual learning methods that use different architectural approaches.