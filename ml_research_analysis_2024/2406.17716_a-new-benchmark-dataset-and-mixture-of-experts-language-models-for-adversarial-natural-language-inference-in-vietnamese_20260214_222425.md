---
ver: rpa2
title: A New Benchmark Dataset and Mixture-of-Experts Language Models for Adversarial
  Natural Language Inference in Vietnamese
arxiv_id: '2406.17716'
source_url: https://arxiv.org/abs/2406.17716
tags:
- data
- language
- dataset
- vianli
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ViANLI, the first adversarial Vietnamese
  natural language inference (NLI) dataset, containing over 10,000 premise-hypothesis
  pairs. The dataset is constructed using a human-and-model-in-the-loop approach,
  where annotators iteratively create challenging examples that fool a pre-trained
  model, followed by rigorous verification.
---

# A New Benchmark Dataset and Mixture-of-Experts Language Models for Adversarial Natural Language Inference in Vietnamese

## Quick Facts
- arXiv ID: 2406.17716
- Source URL: https://arxiv.org/abs/2406.17716
- Reference count: 40
- Key outcome: ViANLI dataset + NLIMoE model achieve modest improvements on Vietnamese NLI tasks

## Executive Summary
This paper introduces ViANLI, the first adversarial Vietnamese natural language inference dataset, and NLIMoE, a Mixture-of-Experts model with dynamic routing. ViANLI contains over 10,000 premise-hypothesis pairs created through a human-and-model-in-the-loop approach to expose model vulnerabilities. NLIMoE achieves 47.3% accuracy on ViANLI compared to 45.5% for the best baseline (XLM-R Large), with transfer learning benefits to other Vietnamese NLI datasets.

## Method Summary
The authors construct ViANLI using iterative human annotation where annotators create hypothesis sentences designed to fool pre-trained models. Challenging examples that models fail to classify correctly are verified and added to the dataset. For the NLIMoE model, they integrate expert subnetworks with a learned dynamic routing mechanism on top of a shared transformer encoder. The routing mechanism determines which expert subnetworks process each token's representation, allowing specialized handling of complex linguistic patterns.

## Key Results
- ViANLI dataset contains over 10,000 premise-hypothesis pairs from Vietnamese news articles
- NLIMoE achieves 47.3% accuracy on ViANLI test set vs 45.5% for XLM-R Large baseline
- Training on ViANLI improves performance on ViNLI, VLSP2021-NLI, and VnNewsNLI datasets

## Why This Works (Mechanism)
### Mechanism 1
- Claim: ViANLI improves model robustness by exposing vulnerabilities through adversarial examples
- Mechanism: Human annotators iteratively craft hypothesis sentences designed to fool pre-trained models; those that fail are verified and added to the dataset
- Core assumption: Models that train on adversarial examples generalize better to non-adversarial data
- Evidence anchors: Models trained with ViANLI show accuracy gains on other Vietnamese NLI datasets; XLM-R Large achieves under 49% accuracy on ViANLI

### Mechanism 2
- Claim: Dynamic routing in NLIMoE enables expert specialization on challenging linguistic patterns
- Mechanism: Each token's representation is routed to a subset of expert subnetworks based on learned gating
- Core assumption: Expert specialization improves performance on adversarial data by focusing computational resources on difficult cases
- Evidence anchors: NLIMoE achieves 47.3% accuracy on ViANLI vs 45.5% for XLM-R Large; dynamic routing allows specialized handling of complex patterns

### Mechanism 3
- Claim: Adversarial training with ViANLI improves zero-shot transfer to other Vietnamese NLI datasets
- Mechanism: Models trained on ViANLI learn to handle linguistic edge cases that appear across multiple Vietnamese NLI datasets
- Core assumption: Linguistic phenomena in adversarial data are representative of real-world Vietnamese NLI challenges
- Evidence anchors: Training on ViANLI improves performance on ViNLI, VLSP2021-NLI, and VnNewsNLI datasets

## Foundational Learning
- Concept: Natural Language Inference (entailment, contradiction, neutral)
  - Why needed here: ViANLI is an NLI dataset; understanding label semantics is essential for dataset construction and model evaluation
  - Quick check question: What distinguishes a neutral premise-hypothesis pair from entailment or contradiction?

- Concept: Adversarial example generation and evaluation
  - Why needed here: ViANLI uses adversarial data construction; understanding how to create and evaluate challenging examples is critical
  - Quick check question: How does a human-and-model-in-the-loop approach differ from automated adversarial generation?

- Concept: Mixture-of-Experts (MoE) architecture and dynamic routing
  - Why needed here: NLIMoE uses MoE with learned routing; understanding gating mechanisms is essential for implementation and tuning
  - Quick check question: What advantages does dynamic routing offer over static MoE architectures?

## Architecture Onboarding
- Component map: Shared transformer encoder -> Gating network -> Expert subnetworks -> Output layer
- Critical path: Input -> Encoder -> Routing -> Expert processing -> Aggregation -> Prediction
- Design tradeoffs: Number of experts vs. routing complexity; model size vs. computational efficiency
- Failure signatures: Routing collapse (all tokens to one expert); expert redundancy; training instability
- First 3 experiments:
  1. Baseline: Train XLM-R Large on ViANLI, evaluate on dev/test sets
  2. MoE ablation: Compare NLIMoE vs. single transformer on ViANLI
  3. Transfer test: Train on ViANLI, evaluate on ViNLI/VLSP2021/VnNewsNLI

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the adversarial dataset ViANLI perform on downstream Vietnamese NLP tasks beyond NLI, such as named entity recognition or sentiment analysis?
- Basis in paper: The paper mentions the potential for ViANLI to improve performance on tasks like machine reading comprehension and text summarization, but does not provide experimental results
- Why unresolved: The paper focuses on evaluating ViANLI's impact on NLI datasets and does not explore its effects on other NLP tasks
- What evidence would resolve it: Experimental results showing the performance of models trained on ViANLI on various Vietnamese NLP tasks

### Open Question 2
- Question: What is the optimal mix of adversarial and non-adversarial data for training robust Vietnamese NLI models?
- Basis in paper: The paper shows that training on ViANLI improves performance on other NLI datasets, but does not explore the optimal ratio of adversarial to non-adversarial data
- Why unresolved: The paper does not conduct experiments to determine the best balance between adversarial and non-adversarial data for training robust models
- What evidence would resolve it: A study comparing the performance of NLI models trained on different ratios of ViANLI to other Vietnamese NLI datasets

### Open Question 3
- Question: How do the linguistic phenomena targeted by ViANLI compare to those in English adversarial NLI datasets like ANLI?
- Basis in paper: The paper mentions that ViANLI targets challenging linguistic phenomena but does not compare these phenomena to those in English adversarial datasets
- Why unresolved: The paper does not conduct a comparative analysis of the linguistic phenomena in ViANLI and English adversarial datasets
- What evidence would resolve it: A detailed comparison of the linguistic phenomena targeted by ViANLI and English adversarial datasets

## Limitations
- The modest improvement of NLIMoE (47.3%) over the best baseline (XLM-R Large at 45.5%) raises questions about the effectiveness of the Mixture-of-Experts approach
- The paper lacks detailed linguistic analysis of the specific challenges in ViANLI that make it difficult for models
- The human-and-model-in-the-loop annotation process may introduce biases based on annotator strategies and model limitations

## Confidence
- Mechanism 1: Medium - Transfer learning results provide some validation but small absolute improvements weaken confidence
- Mechanism 2: Medium - Performance gains demonstrated but specific contribution of dynamic routing unclear
- Mechanism 3: Medium - Transfer improvements shown but underlying linguistic patterns not analyzed in detail

## Next Checks
1. Conduct ablation studies removing the dynamic routing component from NLIMoE to quantify its specific contribution to performance gains
2. Perform detailed error analysis categorizing model failures on ViANLI to identify the linguistic phenomena that remain challenging
3. Test whether the transfer improvements persist when training on ViANLI for longer periods or with different initialization strategies