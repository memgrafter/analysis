---
ver: rpa2
title: 'Noisy Zero-Shot Coordination: Breaking The Common Knowledge Assumption In
  Zero-Shot Coordination Games'
arxiv_id: '2411.04976'
source_url: https://arxiv.org/abs/2411.04976
tags:
- agents
- coordination
- agent
- nzsc
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces noisy zero-shot coordination (NZSC), addressing
  the unrealistic assumption in standard zero-shot coordination that all agents have
  perfect knowledge of the problem setting. In NZSC, agents observe different noisy
  versions of the ground-truth Dec-POMDP, with only the distribution of Dec-POMDPs
  and noise models being common knowledge.
---

# Noisy Zero-Shot Coordination: Breaking The Common Knowledge Assumption In Zero-Shot Coordination Games

## Quick Facts
- arXiv ID: 2411.04976
- Source URL: https://arxiv.org/abs/2411.04976
- Authors: Usman Anwar; Ashish Pandian; Jia Wan; David Krueger; Jakob Foerster
- Reference count: 16
- Key outcome: Noisy zero-shot coordination (NZSC) enables agents to coordinate better when problem settings aren't perfectly known, with meta-NZSC training particularly effective in information-asymmetric scenarios.

## Executive Summary
This paper introduces noisy zero-shot coordination (NZSC) to address the unrealistic assumption in standard zero-shot coordination that all agents have perfect knowledge of the problem setting. In NZSC, agents observe different noisy versions of the ground-truth Dec-POMDP, with only the distribution of Dec-POMDPs and noise models being common knowledge. The authors show that NZSC can be reduced to standard ZSC by creating a meta-Dec-POMDP with an augmented state space. Experiments across four environments demonstrate that NZSC-trained agents coordinate better than self-play agents, especially when there is information asymmetry.

## Method Summary
The authors reduce noisy zero-shot coordination to standard zero-shot coordination by constructing a meta-Dec-POMDP with an augmented state space that includes all possible ground-truth Dec-POMDPs. They propose two meta-learning methods: noisy other-play (NOP) and noisy maximum entropy population-based training (NMEP). NOP works by randomizing over known symmetries during training to learn symmetry-invariant strategies, while NMEP uses a curriculum-based approach with an augmented reward to encourage diversity in the population. Agents are trained across distributions of coordination problems with different noise models, then evaluated on their cross-play performance with novel partners.

## Key Results
- NZSC-trained agents coordinate better than self-play agents across all tested environments
- Meta-NZSC trained agents perform particularly well in information-asymmetric scenarios
- NOP prevents agents from learning arbitrary conventions that inhibit coordination
- NMEP's curriculum approach successfully prevents collapse to suboptimal policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NZSC can be reduced to standard ZSC by creating a meta-Dec-POMDP with an augmented state space that includes the ground-truth Dec-POMDP.
- Mechanism: The reduction works by converting uncertainty over the problem setting into uncertainty over the state space. The meta-Dec-POMDP includes both the original state space and the space of all possible ground-truth Dec-POMDPs, allowing agents to maintain beliefs about the true problem setting while interacting with it.
- Core assumption: The distribution over ground-truth Dec-POMDPs P(E) and the noise models are common knowledge.
- Evidence anchors:
  - [section]: "We show that a NZSC problem can be reduced to a ZSC problem by designing a meta-Dec-POMDP with an augmented state space consisting of all the ground-truth Dec-POMDPs."
  - [abstract]: "We show that a NZSC problem can be reduced to a ZSC problem by designing a meta-Dec-POMDP with an augmented state space consisting of all the ground-truth Dec-POMDPs."
- Break condition: If the distribution over ground-truth Dec-POMDPs or the noise models are not common knowledge, the reduction fails.

### Mechanism 2
- Claim: Meta-NZSC training enables agents to coordinate optimally even when agents have different noise models, leveraging information asymmetry.
- Mechanism: By training agents across a distribution of coordination problems (including different noise models), meta-NZSC allows agents to learn policies that can adapt based on the characteristics of their opposing agent. This is particularly effective in environments with multiple timesteps where agents can exchange information.
- Core assumption: The noise models are sampled from a distribution that is known during training.
- Evidence anchors:
  - [section]: "Meta-NZSC trained agents perform particularly better than NZSC trained agents when there is an information asymmetry present... despite the fact that agent 2's observation of the coordination problem is highly noisy, the agents leverage the fact that agent 1 can actually observe the noiseless version of the coordination problem."
  - [abstract]: "We show that with NZSC training, RL agents can be trained to coordinate well with novel partners even when the (exact) problem setting of the coordination is not common knowledge."
- Break condition: If the distribution of noise models is misspecified or unknown during training, meta-NZSC may not perform optimally.

### Mechanism 3
- Claim: Noisy other-play (NOP) and noisy maximum entropy population-based training (NMEP) enable agents to learn symmetry-invariant strategies and diverse populations respectively.
- Mechanism: NOP works by randomizing over known symmetries during training, preventing agents from learning arbitrary conventions that could inhibit coordination. NMEP trains agents using an augmented reward that encourages diversity in the population, then trains the ZSC agent to coordinate with all agents in the population.
- Core assumption: The symmetries present in the meta-Dec-POMDP are known and can be exploited.
- Evidence anchors:
  - [section]: "Noisy-Other-Play (NOP), similar to Other-Play (OP; Hu et al., 2020), enables agents to learn symmetry invariant strategies by randomizing over the known symmetries during training."
  - [section]: "Noisy-MEP (NMEP) works similarly, but with one main difference: we use a curriculum-based approach in which the learning agent is initially trained in a noise-free setting, and then we gradually increase the noise to the maximum level throughout the training."
- Break condition: If the symmetries are not known or if the curriculum approach fails to prevent collapse to suboptimal policies.

## Foundational Learning

- Concept: Dec-POMDP (Decentralized Partially Observable Markov Decision Process)
  - Why needed here: The entire framework is built on Dec-POMDPs as the problem formulation for multi-agent coordination. Understanding Dec-POMDPs is essential to grasp how agents interact with the environment and each other.
  - Quick check question: What are the key components of a Dec-POMDP and how do they differ from a standard POMDP?

- Concept: Zero-shot coordination (ZSC)
  - Why needed here: NZSC is a generalization of ZSC that removes the common knowledge assumption. Understanding ZSC is crucial to appreciate what NZSC adds and why it's important.
  - Quick check question: What is the key assumption in standard ZSC that NZSC removes, and why is this assumption problematic in real-world scenarios?

- Concept: Common knowledge (CK)
  - Why needed here: The entire motivation for NZSC stems from the unrealistic nature of the common knowledge assumption in ZSC. Understanding what constitutes common knowledge is essential to grasp the problem NZSC addresses.
  - Quick check question: In the context of multi-agent reinforcement learning, what does it mean for a problem setting to be common knowledge, and why is this assumption often violated in practice?

## Architecture Onboarding

- Component map: Meta-Dec-POMDP -> Noise models -> Training methods (NOP, NMEP) -> Environments (One-shot noisy lever game, Iterated noisy lever game, Coordinated exploration, Sync sight)

- Critical path:
  1. Define the distribution over ground-truth Dec-POMDPs P(E) and noise models P_Ai(E_i|E*)
  2. Construct the meta-Dec-POMDP with augmented state space
  3. Implement training method (NOP or NMEP) that accounts for noise models
  4. Train agents across the distribution of coordination problems
  5. Evaluate cross-play performance across different noise models

- Design tradeoffs:
  - Computational cost: Meta-Dec-POMDPs can be significantly larger than original Dec-POMDPs, increasing computational requirements
  - Expressiveness vs. tractability: More complex noise models and distributions over Dec-POMDPs increase realism but may make training more difficult
  - Symmetry exploitation: NOP requires identifying symmetries in the problem, which may not always be straightforward

- Failure signatures:
  - Poor coordination performance when noise models differ significantly between agents
  - Inability to coordinate in environments with multiple timesteps despite successful coordination in one-shot settings
  - Training instability or collapse to suboptimal policies, particularly with NMEP

- First 3 experiments:
  1. Implement the one-shot noisy lever game and verify that self-play agents fail to coordinate with agents having different noise models
  2. Implement NOP on the one-shot noisy lever game and verify improved coordination across different noise models
  3. Implement NMEP on the coordinated exploration environment and verify that meta-NZSC trained agents can leverage information asymmetry to achieve better coordination than NZSC trained agents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is NZSC to severe misspecification of the noise models or ground truth distribution beyond what was tested?
- Basis in paper: [explicit] The paper tested NZSC with moderate noise levels (σ up to 5) and some misspecification (z = 2 for noise model uncertainty), showing agents could still coordinate well.
- Why unresolved: The experiments only explored limited ranges of noise and misspecification. Real-world scenarios could involve much larger uncertainties or different types of noise structures not considered.
- What evidence would resolve it: Experiments testing NZSC with extreme noise levels (σ > 5), highly asymmetric noise distributions, or completely wrong assumed ground truth distributions would clarify the limits of robustness.

### Open Question 2
- Question: Can one-sided meta-NZSC training (where only one agent is meta-NZSC trained) be improved to provide better coordination performance?
- Basis in paper: [explicit] The paper found that one-sided meta-NZSC training did not significantly improve coordination compared to both agents being NZSC trained.
- Why unresolved: The experiments only tested one approach to one-sided training. There may be alternative training methodologies or curriculum strategies that could leverage the asymmetry more effectively.
- What evidence would resolve it: Comparing different training regimes for one-sided meta-NZSC (e.g., gradual noise increase, different population compositions) against standard NZSC would determine if better coordination is achievable.

### Open Question 3
- Question: How does the number of agents in NZSC scale beyond the tested range, and what are the practical limits?
- Basis in paper: [explicit] The paper tested NZSC with up to 5 agents and observed degradation in coordination performance as agent count increased.
- Why unresolved: The experiments only explored a small range of agent numbers. Real-world applications often involve many more agents, and the scaling behavior is unknown.
- What evidence would resolve it: Experiments scaling NZSC to 10+ agents with varying noise models and ground truth distributions would reveal the practical limits and identify potential bottlenecks in the approach.

## Limitations

- The reduction of NZSC to ZSC relies heavily on the assumption that noise models and distributions over Dec-POMDPs are common knowledge
- Computational complexity of solving meta-Dec-POMDPs with augmented state spaces could become prohibitive for larger problems
- Effectiveness of meta-learning methods may be sensitive to hyperparameter choices and specific distribution of noise models used during training

## Confidence

**High confidence**: The core theoretical framework for reducing NZSC to ZSC through meta-Dec-POMDPs is well-established and mathematically sound. The experimental results demonstrating improved coordination under NZSC training versus standard ZSC training are robust across multiple environments.

**Medium confidence**: The effectiveness of NOP and NMEP methods in handling various noise distributions and symmetry-breaking scenarios. While results are promising, the sensitivity to hyperparameter choices and specific noise model implementations could affect reproducibility.

**Low confidence**: The scalability of the proposed framework to more complex, high-dimensional coordination problems beyond the tested environments. The ability to handle real-world scenarios where noise models may be unknown or non-stationary.

## Next Checks

1. **Robustness to noise model misspecification**: Systematically vary the true noise models during evaluation while keeping training noise models fixed to test the framework's robustness to model misspecification.

2. **Cross-environment generalization**: Train agents on one environment's noise distribution and evaluate their performance on different environments to assess the transferability of learned coordination strategies.

3. **Real-world noise simulation**: Implement more complex, non-Gaussian noise models that better reflect real-world observation noise to validate the framework's applicability beyond idealized noise distributions.