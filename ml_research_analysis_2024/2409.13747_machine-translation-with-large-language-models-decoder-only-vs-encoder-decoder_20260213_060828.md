---
ver: rpa2
title: 'Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder'
arxiv_id: '2409.13747'
source_url: https://arxiv.org/abs/2409.13747
tags:
- translation
- machine
- learning
- language
- encoder-decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the performance of decoder-only versus
  encoder-decoder architectures for multilingual machine translation, focusing on
  Indian regional languages. The work compares in-context learning using XGLM (decoder-only)
  and mT5 (encoder-decoder) models, achieving BLEU scores between 12-20 across language
  pairs.
---

# Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder

## Quick Facts
- arXiv ID: 2409.13747
- Source URL: https://arxiv.org/abs/2409.13747
- Reference count: 2
- This study investigates decoder-only vs. encoder-decoder architectures for multilingual machine translation, achieving BLEU scores of 12-20 across Indian language pairs.

## Executive Summary
This study systematically compares decoder-only and encoder-decoder architectures for multilingual machine translation, focusing on Indian regional languages. The research evaluates in-context learning using XGLM (decoder-only) and mT5 (encoder-decoder), achieving BLEU scores between 12-20 across language pairs. Additionally, mT5 and LLaMA 2 models are fine-tuned for bilingual and multilingual tasks, with mT5 achieving a BLEU score of 14.14 for English-Hindi translation. The study develops baseline models using XLNet and IndicBART for comparative analysis. Results indicate that encoder-decoder models generally outperform decoder-only models in multilingual translation tasks, though decoder-only models excel in one-to-one translation scenarios.

## Method Summary
The research employs multiple experimental approaches to compare decoder-only and encoder-decoder architectures. First, in-context learning (3-shot) is applied using XGLM and mT5 models on English-to-Indian language pairs from the BPCC Wiki MT Dataset. Second, mT5 and LLaMA 2 models are fine-tuned for bilingual (English-Hindi) and multilingual (English-Hindi-Bengali) translation tasks. Third, baseline models using XLNet (decoder-only) and IndicBART (encoder-decoder) are developed with comparable parameters. Translation quality is evaluated using BLEU, chrF, and TER metrics across datasets including FLORES-101 and TED Talks.

## Key Results
- Encoder-decoder models generally outperform decoder-only models in multilingual translation tasks, achieving higher BLEU scores across Indian language pairs
- mT5 fine-tuning achieves a BLEU score of 14.14 for English-Hindi translation, demonstrating strong bilingual performance
- Decoder-only models show superior performance in one-to-one translation tasks, particularly LLaMA 2 on English-Hindi pairs
- Encoder-decoder model demonstrates consistent performance across multilingual settings, providing reliable translation quality

## Why This Works (Mechanism)

### Mechanism 1
Encoder-decoder models generally outperform decoder-only models in multilingual translation tasks due to their ability to process full source context before generating target sequences. The encoder processes the entire input sequence and creates a comprehensive representation, which the decoder uses to generate the output. This bidirectional context flow enables better handling of complex dependencies and long-range relationships in multilingual settings. Core assumption: The source language structure and target language requirements can be effectively captured in a shared representation space.

### Mechanism 2
Decoder-only models excel in one-to-one translation tasks due to their autoregressive nature and next-token prediction training. Decoder-only models are trained on next-word prediction, making them naturally suited for generating sequences token-by-token. For one-to-one translation where source and target sequences have similar lengths and structures, this autoregressive approach can produce fluent outputs. Core assumption: The source and target languages share similar syntactic structures, reducing the need for complex reordering.

### Mechanism 3
In-context learning performance depends on the quality and relevance of exemplars provided to the model. ICL allows models to learn from demonstrations without parameter updates. The effectiveness depends on selecting exemplars that are contextually relevant and representative of the task distribution. Core assumption: The model can generalize from a small number of examples to handle unseen translations.

## Foundational Learning

- **Attention mechanisms**: Understanding how encoder-decoder and decoder-only models use attention to capture dependencies between source and target sequences is crucial for grasping their architectural differences. Quick check: How does multi-head attention in transformer models help capture both local and global dependencies in translation tasks?

- **Bidirectional vs unidirectional encoding**: Encoder-decoder models use bidirectional encoding to capture full context, while decoder-only models use unidirectional encoding. This fundamental difference explains their performance trade-offs. Quick check: Why does bidirectional encoding generally provide better context understanding than unidirectional encoding for translation tasks?

- **Transfer learning and multilingual pretraining**: Both mT5 and XGLM leverage multilingual pretraining, which allows them to transfer knowledge across languages and improve performance on low-resource language pairs. Quick check: How does multilingual pretraining on a diverse set of languages improve a model's ability to translate between specific language pairs?

## Architecture Onboarding

- **Component map**: Data → Preprocessing → Model forward pass → Loss computation → Backpropagation → Parameter update → Evaluation
- **Critical path**: Data → Preprocessing → Model forward pass → Loss computation → Backpropagation → Parameter update → Evaluation
- **Design tradeoffs**: Encoder-decoder: Better context understanding but higher computational cost; Decoder-only: Faster inference but potentially less accurate for complex translations; ICL: No parameter updates but dependent on exemplar quality; Fine-tuning: Better performance but requires labeled data and computational resources
- **Failure signatures**: Low BLEU scores across all language pairs: Potential issues with tokenization or vocabulary coverage; High variance in translation quality: Imbalanced training data or inadequate multilingual pretraining; Decoder-only performs poorly on many-to-one tasks: Structural differences between source and target languages not handled well
- **First 3 experiments**: 1) Implement a simple encoder-decoder model with attention and train on a small parallel corpus to verify basic functionality; 2) Compare decoder-only vs encoder-decoder performance on a controlled set of language pairs with similar structures; 3) Test ICL performance with varying numbers of exemplars and different exemplar selection strategies

## Open Questions the Paper Calls Out

### Open Question 1
How do context length (measured in tokens) and architecture type (decoder-only vs encoder-decoder) interact to affect translation quality in multilingual machine translation tasks? The study mentions evaluating context length but does not provide detailed results or analysis on how context length specifically impacts the performance of decoder-only versus encoder-decoder architectures.

### Open Question 2
What specific training paradigms or techniques can be developed to harmonize the learning processes of encoder-decoder and decoder-only models for multilingual machine translation? The paper discusses differences in training paradigms between the two architectures and mentions the need to explore ways to harmonize the learning paradigms.

### Open Question 3
How effective is Streaming Self-Attention (SSA) in improving the performance of decoder-only models for long-text translation in multilingual settings? While SSA is mentioned as promising, the paper does not provide empirical results or detailed analysis of its effectiveness in multilingual translation tasks.

## Limitations

- Focus on Indian regional languages may limit generalizability to other language families or global translation tasks
- Dataset sizes (16k-50k samples) are relatively modest for large language models, potentially limiting full exploitation of model capabilities
- Does not extensively explore the impact of varying context lengths beyond the specified 40-200 character range

## Confidence

- **High Confidence**: Encoder-decoder models outperform decoder-only models in multilingual translation tasks across multiple language pairs and evaluation metrics
- **Medium Confidence**: Decoder-only models excel in one-to-one translation tasks, particularly LLaMA 2 on English-Hindi pairs
- **Low Confidence**: Effectiveness of in-context learning with 3-shot prompts, as specific prompt engineering details are not fully specified

## Next Checks

1. Systematically test the impact of varying the number of exemplars (1-shot, 3-shot, 5-shot) and their selection strategies on in-context learning performance across different language pairs

2. Extend the architectural comparison to non-Indian language families (e.g., Romance, Germanic, Slavic languages) to assess generalizability beyond the current linguistic domain

3. Conduct detailed analysis of attention patterns in both encoder-decoder and decoder-only models during translation tasks using attention visualization and gradient-based attribution techniques