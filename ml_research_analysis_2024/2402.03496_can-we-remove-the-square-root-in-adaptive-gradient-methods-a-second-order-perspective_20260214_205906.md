---
ver: rpa2
title: Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order
  Perspective
arxiv_id: '2402.03496'
source_url: https://arxiv.org/abs/2402.03496
tags:
- methods
- adaptive
- root
- gradient
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates removing the square-root from adaptive
  gradient methods to strengthen their second-order perspective. The authors propose
  using a novel empirical Fisher matrix that differs from the standard definition,
  enabling scale-invariant updates without the square root.
---

# Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective

## Quick Facts
- arXiv ID: 2402.03496
- Source URL: https://arxiv.org/abs/2402.03496
- Reference count: 40
- Key outcome: Removing square-root from adaptive methods improves CNN generalization while maintaining transformer performance and enables faster low-precision training

## Executive Summary
This paper investigates whether removing the square-root from adaptive gradient methods can strengthen their second-order optimization perspective. The authors propose using a novel empirical Fisher matrix definition that enables scale-invariant updates without the square root. They demonstrate that square-root-free methods close the generalization gap between adaptive methods and SGD on convolutional networks while maintaining performance on transformers. The approach also enables developing inverse-free matrix adaptive methods that work in low precision, avoiding numerical instabilities from matrix decompositions.

## Method Summary
The authors propose removing the square-root from adaptive gradient methods by using a novel empirical Fisher matrix formulation. This new definition differs from the standard empirical Fisher by incorporating the square of the parameter gradient rather than just the squared gradient. This enables scale-invariant updates without requiring the square-root operation. They also develop an inverse-free variant of the Shampoo optimizer that avoids explicit matrix decompositions, making it suitable for low-precision training. The key insight is that scale-invariance can be achieved through the Fisher matrix structure rather than through the square-root normalization commonly used in methods like RMSProp and Adam.

## Key Results
- Square-root-free RMSProp closes the generalization gap with SGD on CNNs while maintaining transformer performance
- Inverse-free Shampoo variant trains roughly twice as fast in bfloat16 precision compared to standard Shampoo
- Scale-invariance emerges as a key benefit of removing the square root, particularly for convolutional architectures
- The proposed empirical Fisher matrix formulation enables matrix-based adaptive methods in low precision without numerical instabilities

## Why This Works (Mechanism)
The paper's mechanism centers on the relationship between the Fisher matrix, scale-invariance, and second-order optimization. By reformulating the empirical Fisher to incorporate parameter gradients directly, the authors create a structure that naturally provides scale-invariance without requiring the square-root operation. This formulation maintains the adaptive learning rate properties while enabling better generalization on architectures where the square-root normalization may have been harmful. The inverse-free approach for matrix methods like Shampoo eliminates the computational bottleneck of matrix decompositions in low precision, where these operations can be numerically unstable.

## Foundational Learning

**Empirical Fisher Matrix**: Why needed - Captures curvature information for adaptive optimization; Quick check - Compare standard vs proposed Fisher definitions and their impact on gradient scaling

**Scale-invariance**: Why needed - Ensures consistent optimization behavior across parameter scales; Quick check - Verify parameter updates remain stable when parameters are rescaled

**Second-order optimization**: Why needed - Provides curvature-aware updates beyond first-order methods; Quick check - Analyze how the Fisher matrix relates to the Hessian and impacts convergence

**Low-precision training**: Why needed - Enables faster training and deployment on specialized hardware; Quick check - Measure numerical stability and performance degradation when using bfloat16 vs float32

## Architecture Onboarding

**Component map**: Data -> Model -> Loss -> Gradient -> Fisher Matrix -> Adaptive Update -> Parameters

**Critical path**: The critical computational path involves calculating the empirical Fisher matrix, which requires storing and processing gradient information across the network. The inverse-free Shampoo variant optimizes this by avoiding explicit matrix decompositions.

**Design tradeoffs**: The main tradeoff is between the theoretical justification for the new Fisher matrix formulation versus its empirical effectiveness. The authors prioritize practical performance over theoretical rigor, which may limit broader adoption without stronger theoretical foundations.

**Failure signatures**: Potential failures include numerical instability in the Fisher matrix calculation, poor generalization when the scale-invariance assumption doesn't hold, and convergence issues when the adaptive learning rates become too aggressive or too conservative.

**First experiments**: 1) Compare standard vs square-root-free RMSProp on CIFAR-10 with CNNs; 2) Benchmark inverse-free Shampoo vs standard Shampoo in bfloat16 precision on language modeling tasks; 3) Analyze the impact of different Fisher matrix formulations on transformer training stability

## Open Questions the Paper Calls Out
The paper raises questions about the fundamental role of adaptivity in the success of adaptive methods and whether scale-invariance or second-order information is more critical for generalization. It also questions whether the proposed empirical Fisher matrix has broader applications beyond removing the square root from adaptive methods.

## Limitations
- The proposed empirical Fisher matrix formulation lacks theoretical justification for why it works better than the standard definition
- The mechanism connecting square-root removal to improved CNN generalization remains unclear
- The inverse-free Shampoo variant may have convergence properties that need further theoretical analysis

## Confidence

**High**: The empirical results showing improved CNN generalization and faster bfloat16 training are reproducible and well-documented

**Medium**: The claim that scale-invariance is the key benefit of removing the square root is supported but not conclusively proven

**Low**: The theoretical implications of using the proposed empirical Fisher matrix over the standard definition

## Next Checks

1. Conduct ablation studies to isolate whether scale-invariance or second-order information is primarily responsible for the improved CNN performance

2. Perform systematic analysis of convergence rates and stability across different learning rate schedules for the inverse-free methods

3. Test the square-root-free variants on additional architectures (e.g., ResNet, ViT-Small) and tasks to verify the generalization claims beyond the specific models used in the experiments