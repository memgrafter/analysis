---
ver: rpa2
title: Fast Training of Sinusoidal Neural Fields via Scaling Initialization
arxiv_id: '2410.04779'
source_url: https://arxiv.org/abs/2410.04779
tags:
- neural
- training
- scaling
- weight
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the slow training speed of sinusoidal neural
  fields (SNFs), which are widely used to represent data as continuous functions parameterized
  by neural networks. The authors identify that the standard initialization scheme
  for SNFs is suboptimal for training efficiency.
---

# Fast Training of Sinusoidal Neural Fields via Scaling Initialization

## Quick Facts
- arXiv ID: 2410.04779
- Source URL: https://arxiv.org/abs/2410.04779
- Authors: Taesun Yeom; Sangyoon Lee; Jaeho Lee
- Reference count: 40
- Key outcome: Weight scaling initialization accelerates sinusoidal neural field training by up to 10× while maintaining or improving generalization

## Executive Summary
This paper addresses the slow training speed of sinusoidal neural fields (SNFs) by identifying suboptimal initialization as the bottleneck. The authors propose a simple method called weight scaling, which multiplies initial weights (except for the last layer) by a constant factor α ≥ 1. This approach significantly accelerates training while maintaining the distribution preservation property of standard initialization. Extensive theoretical and empirical analyses show that weight scaling improves optimization conditioning, mitigates spectral bias, and consistently outperforms various neural field architectures across different data domains including images, occupancy fields, spherical data, and audio.

## Method Summary
The weight scaling method modifies the standard initialization of SNFs by multiplying initial weights by a constant factor α ≥ 1 (except for the last layer). This simple modification increases the frequency spectrum of the initial functional while maintaining distribution preservation properties. The method works by amplifying frequencies in early layers and increasing the relative magnitude of higher-order harmonics. The authors conduct extensive experiments showing that this approach accelerates training by 5-10× compared to standard initialization while maintaining or improving generalization performance across multiple data domains and network architectures.

## Key Results
- Weight scaling accelerates SNF training by up to 10× compared to standard initialization
- The method consistently improves training speed and generalization across images, occupancy fields, spherical data, and audio domains
- Weight scaling maintains distribution preservation while providing better-conditioned optimization trajectories
- The optimal scaling factor α typically ranges from 1.5 to 3.0, depending on the specific task and architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weight scaling increases the frequency spectrum of the initial functional in SNFs.
- **Mechanism:** Scaling weights in early layers amplifies frequencies of sinusoidal basis functions, and scaling intermediate layers increases relative power of higher-order harmonics.
- **Core assumption:** Sinusoidal activation functions are not positively homogeneous, so scaling weights doesn't simply scale output function.
- **Evidence anchors:** Abstract states weight scaling increases frequency of each basis and higher-order harmonics. Section explains scaling first layer increases frequencies by factor α, while scaling second layer affects magnitude of each basis.
- **Break condition:** If sinusoidal activation were positively homogeneous, scaling weights would simply scale output function and not affect frequency spectrum.

### Mechanism 2
- **Claim:** Weight scaling improves conditioning of optimization trajectory.
- **Mechanism:** Weight scaling results in smaller condition number of empirical Neural Tangent Kernel (eNTK) eigenspectrum throughout training.
- **Core assumption:** Condition number of eNTK eigenspectrum affects convergence rate of kernelized models.
- **Evidence anchors:** Abstract mentions weight-scaled SNFs have better-conditioned trajectories as captured by eigenspectrum analysis. Section states weight-scaled SNFs have much smaller condition number at initialization that becomes even smaller during training.
- **Break condition:** If eNTK conditioning doesn't correlate with actual convergence rates in practice.

### Mechanism 3
- **Claim:** Weight scaling amplifies early layer gradients disproportionately.
- **Mechanism:** Due to lack of positive-homogeneity in sinusoidal activations, scaling weights affects layerwise gradients differently - earlier layers receive larger gradients.
- **Core assumption:** Gradient magnitude for each layer is determined by product of weights and activation derivatives along path.
- **Evidence anchors:** Abstract notes weight-scaled ReLU nets behave similarly to kernels with exponential convergence but ill-conditioned trajectories. Section explains first layer of depth-l sinusoidal network has gradient scaling as α^l, while last layer gradient scales as 1.
- **Break condition:** If gradient amplification effect is negligible compared to other factors affecting convergence.

## Foundational Learning

- **Concept:** Neural Tangent Kernel (NTK) framework
  - Why needed here: Paper uses eNTK analysis to understand optimization properties of weight-scaled SNFs.
  - Quick check question: What is the relationship between NTK eigenspectrum and convergence rates in kernel regression?

- **Concept:** Spectral bias in neural networks
  - Why needed here: Paper discusses how weight scaling mitigates spectral bias by increasing high-frequency components.
  - Quick check question: How does frequency spectrum of initial weights affect ability to fit high-frequency signals?

- **Concept:** Bessel functions and Jacobi-Anger identity
  - Why needed here: Theoretical analysis of weight scaling's effect on frequency spectrum uses these mathematical tools.
  - Quick check question: How does Jacobi-Anger identity help express frequency spectrum of multi-layer SNF?

## Architecture Onboarding

- **Component map:** Input coordinates -> Hidden layers with sinusoidal activation (sin(ω · x)) -> Linear output layer
- **Critical path:** 1. Initialize weights with scaled uniform distribution, 2. Forward pass through network, 3. Compute MSE loss, 4. Backpropagate gradients, 5. Update weights with Adam optimizer, 6. Monitor convergence and generalization
- **Design tradeoffs:** Scaling factor α (larger α accelerates training but may reduce generalization), frequency multipliers ω0 and ωh (affect frequency spectrum and training dynamics), network depth and width (impact effectiveness of weight scaling)
- **Failure signatures:** Training loss not decreasing (poor choice of α or learning rate), test performance degrading significantly (α too large), oscillating loss (poor conditioning or learning rate too high)
- **First 3 experiments:** 1. Train SNF with standard initialization (α=1) on image regression task, measure convergence time and final PSNR, 2. Train SNF with weight scaling (α=2) on same task, compare convergence time and PSNR to baseline, 3. Vary α systematically (1.0, 1.5, 2.0, 2.5, 3.0) and plot tradeoff between training speed and test PSNR

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does optimal weight scaling factor α vary with network depth beyond 7 layers, and is there point of diminishing returns or even degradation in performance?
- **Basis in paper:** Explicit - paper mentions model depth has negative correlation with optimal scaling factor, showing results for depths up to 7 layers.
- **Why unresolved:** Experiments only tested up to 7 layers, leaving behavior for deeper networks unexplored.
- **What evidence would resolve it:** Systematic experiments varying network depth beyond 7 layers while measuring training speed and generalization performance for different α values.

### Open Question 2
- **Question:** Does weight scaling provide similar benefits for neural field architectures using activation functions other than sinusoidal (e.g., ReLU, Gaussian, Gabor wavelets)?
- **Basis in paper:** Inferred - paper focuses exclusively on sinusoidal neural fields though mentions other architectures in experiments.
- **Why unresolved:** Analysis is limited to sinusoidal activations, and mechanisms might differ for other activation types.
- **What evidence would resolve it:** Comparative experiments applying weight scaling to various neural field architectures with different activation functions, measuring training speed and final performance.

### Open Question 3
- **Question:** What is the theoretical convergence rate bound for weight-scaled sinusoidal neural fields, and how does it compare to standard initialization?
- **Basis in paper:** Inferred - paper provides empirical evidence of faster training but lacks explicit convergence bounds.
- **Why unresolved:** Paper's theoretical analysis focuses on understanding mechanisms rather than providing concrete convergence rate guarantees.
- **What evidence would resolve it:** Formal mathematical proofs establishing convergence rates for weight-scaled SNFs under various conditions, comparing them to standard initialization bounds.

## Limitations
- Theoretical analysis relies heavily on simplified assumptions about network depth and initialization distributions
- Choice of optimal scaling factor α appears problem-dependent with no clear theoretical guidance for selection
- Claims about universal effectiveness across all SNF architectures and data domains remain to be fully validated

## Confidence
- **High Confidence:** Empirical demonstration that weight scaling accelerates training by 5-10× on standard benchmarks (images, occupancy fields, spherical data, audio)
- **Medium Confidence:** Theoretical explanation that weight scaling improves optimization conditioning through eNTK eigenspectrum analysis
- **Medium Confidence:** Claim that weight scaling mitigates spectral bias by increasing frequency spectrum coverage

## Next Checks
1. **Cross-domain robustness test:** Apply weight scaling to SNFs on domains not covered in paper (e.g., medical imaging, climate data) to verify generalization of approach
2. **Ablation on scaling factor selection:** Systematically study relationship between α values and generalization performance across different network depths and data complexities
3. **Comparison with frequency initialization:** Benchmark weight scaling against recent frequency initialization methods (e.g., [44]) to isolate specific benefits of scaling approach