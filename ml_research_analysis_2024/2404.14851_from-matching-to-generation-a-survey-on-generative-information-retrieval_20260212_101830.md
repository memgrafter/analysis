---
ver: rpa2
title: 'From Matching to Generation: A Survey on Generative Information Retrieval'
arxiv_id: '2404.14851'
source_url: https://arxiv.org/abs/2404.14851
tags:
- retrieval
- arxiv
- language
- generative
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of generative information
  retrieval (GenIR), a paradigm shift from traditional similarity-based matching to
  generative approaches using large language models. It systematically categorizes
  GenIR into two main directions: generative document retrieval (GR), which retrieves
  documents by directly generating unique document identifiers (DocIDs) without explicit
  indexing, and reliable response generation, which generates user-centric, accurate
  answers directly from language models, often enhanced with citations and external
  knowledge.'
---

# From Matching to Generation: A Survey on Generative Information Retrieval

## Quick Facts
- arXiv ID: 2404.14851
- Source URL: https://arxiv.org/abs/2404.14851
- Reference count: 40
- One-line primary result: Comprehensive survey of generative information retrieval (GenIR) covering both generative document retrieval (GR) and reliable response generation.

## Executive Summary
This survey provides a systematic overview of generative information retrieval (GenIR), a paradigm shift from traditional similarity-based matching to generative approaches using large language models. It categorizes GenIR into two main directions: generative document retrieval (GR), which retrieves documents by directly generating unique document identifiers (DocIDs) without explicit indexing, and reliable response generation, which generates user-centric, accurate answers directly from language models, often enhanced with citations and external knowledge. The survey reviews model training strategies, DocID design (numeric, text-based, learnable), incremental learning for dynamic corpora, and adaptations for downstream tasks like recommendation systems. It also addresses challenges such as scalability, efficiency, real-time knowledge updating, bias, and privacy. The paper evaluates existing benchmarks and metrics, compares state-of-the-art GR methods, and outlines future directions, including unified frameworks integrating retrieval and generation.

## Method Summary
The survey synthesizes existing literature on generative information retrieval, categorizing methods into generative document retrieval (GR) and reliable response generation. It reviews various approaches for DocID construction (text-based, numeric, learnable), training strategies, and evaluation benchmarks. The survey does not prescribe a single training pipeline but describes multiple approaches and their trade-offs. It highlights challenges in scalability, efficiency, and integration with downstream tasks, and suggests future research directions.

## Key Results
- Generative document retrieval (GR) enables direct query-to-DocID mapping without explicit indexing, leveraging language model parameters for document memorization.
- Reliable response generation enhances user-centric outputs through external knowledge augmentation, personalized assistance, and bias mitigation.
- Current GR methods face scalability challenges on million-level document corpora, with text-based DocIDs showing advantages over numeric ones in interpretability but requiring efficient design.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative retrieval models can directly map queries to document identifiers without explicit indexing.
- Mechanism: The model memorizes documents as parametric knowledge and generates corresponding DocIDs via constrained beam search, guided by a prefix tree built from valid DocIDs.
- Core assumption: The language model can learn the mapping between query semantics and document identifiers through seq2seq training.
- Evidence anchors:
  - [abstract] "Generative Document Retrieval (GR) leverages the generative model's parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing."
  - [section 2.2] "Unlike traditional index-based retrieval methods, generative retrieval relies on pre-trained generative language models... to directly generate document identifiers (DocIDs) relevant to the query."
  - [corpus] Weak - corpus provides neighbor papers but no direct evidence on constrained beam search mechanics.
- Break condition: If the model fails to generalize beyond training pairs or cannot handle unseen documents, the memorization assumption breaks.

### Mechanism 2
- Claim: External knowledge augmentation improves the reliability of generative retrieval systems.
- Mechanism: Retrieval-augmented generation (RAG) fetches relevant documents during inference and feeds them to the generator, grounding responses in verified external sources.
- Core assumption: Language models benefit from integrating real-time or domain-specific knowledge beyond their static training data.
- Evidence anchors:
  - [abstract] "Reliable Response Generation... offers flexibility, efficiency, and creativity to meet practical needs" through strategies like external knowledge augmentation.
  - [section 4.2] "Retrieval-Augmented Generation (RAG) enhances the response of generative models by combining them with a retrieval mechanism."
  - [corpus] Weak - corpus shows related papers but lacks concrete RAG performance data.
- Break condition: If retrieval latency is too high or retrieved documents are irrelevant, the augmentation fails to improve reliability.

### Mechanism 3
- Claim: Personalized information assistants improve user satisfaction by tailoring responses to individual preferences.
- Mechanism: Models are fine-tuned or prompted with user-specific profiles, histories, or interests to generate customized responses.
- Core assumption: Language models can effectively encode and utilize user-specific context to produce personalized outputs.
- Evidence anchors:
  - [abstract] "Reliable Response Generation... offers flexibility, efficiency, and creativity to meet practical needs" via personalized information assistance.
  - [section 4.4] "Personalized information assistants aim to better understand users' personalities and preferences, generating personalized responses to better meet their information needs."
  - [corpus] Weak - corpus lists neighbor papers but no detailed personalization performance metrics.
- Break condition: If personalization leads to overfitting to a single user or fails to generalize across diverse queries, the mechanism degrades.

## Foundational Learning

- Concept: Seq2seq model training
  - Why needed here: GR models rely on mapping queries to DocIDs via sequence-to-sequence learning.
  - Quick check question: What is the loss function used to train a generative retrieval model to predict DocIDs from queries?

- Concept: Constrained beam search
  - Why needed here: Ensures generated DocIDs are valid by restricting decoding to tokens that extend current sequences into valid prefixes.
  - Quick check question: How does constrained beam search differ from standard beam search in generative retrieval?

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: Augments model outputs with external knowledge to improve accuracy and reduce hallucinations.
  - Quick check question: What are the two main components of a RAG system and how do they interact during inference?

## Architecture Onboarding

- Component map:
  - Document identifier construction → Prefix tree for constrained decoding → Language model (encoder-decoder) → Constrained beam search → Ranked DocID list → Document retrieval
  - External knowledge retrieval module → Context augmentation → Generator → Final response with citations
  - User profile encoder → Personalization prompt → Fine-tuned model → Personalized response

- Critical path: Query → DocID generation (via constrained beam search) → Document retrieval → (Optional) External knowledge retrieval → Response generation

- Design tradeoffs:
  - Text-based vs numeric DocIDs: Text leverages pre-trained language skills but may be verbose; numeric is compact but requires extensive pre-training.
  - Single model vs separate retrieval/generation models: Single model simplifies pipeline but may struggle with complex tasks; separate models allow specialization but increase latency.

- Failure signatures:
  - High latency in DocID generation → Check constrained beam search efficiency and DocID length.
  - Low recall in retrieval → Verify DocID uniqueness and coverage; test prefix tree completeness.
  - Hallucinations in responses → Inspect retrieval quality and augmentation effectiveness.

- First 3 experiments:
  1. Implement constrained beam search with a small synthetic corpus; measure latency vs accuracy trade-off.
  2. Compare text-based vs numeric DocIDs on a small dataset; evaluate retrieval recall and generation fluency.
  3. Integrate a basic RAG pipeline; test response accuracy improvement with and without external knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can generative retrieval methods effectively scale to million-level document corpora without significant performance degradation?
- Basis in paper: [explicit] Section 6.1.1 discusses scalability issues where GR methods show significantly lower accuracy compared to dense retrieval on million-level document corpora, and merely increasing model size does not yield stable performance improvements.
- Why unresolved: The paper identifies that GR models tend to rote memorize DocIDs seen during training, struggling to predict unseen ones effectively, especially with numeric-based DocIDs that models haven't encountered in pre-training.
- What evidence would resolve it: Demonstrating a GR model that maintains comparable performance to dense retrieval methods on million-level document corpora, particularly showing consistent improvement with model scaling and effective handling of unseen DocIDs.

### Open Question 2
- Question: What is the optimal document identifier design (text vs. numeric) for generative retrieval that balances performance, interpretability, and scalability?
- Basis in paper: [explicit] Section 6.1.3 discusses the advantages and disadvantages of text-based and numeric-based DocIDs, noting that text-based DocIDs leverage linguistic capabilities but need to capture key document semantics, while numeric DocIDs don't offer this advantage without extensive pre-training.
- Why unresolved: The paper suggests that extensive pre-training is necessary to enable models to understand the meanings behind numerical strings, which is a costly endeavor, but doesn't provide a definitive answer on which approach is superior.
- What evidence would resolve it: Comparative studies showing the performance of text-based vs. numeric-based DocIDs across various corpus sizes and types, demonstrating which approach consistently outperforms the other in terms of retrieval accuracy and efficiency.

### Open Question 3
- Question: How can generative retrieval models be effectively integrated with downstream generation tasks to create a unified framework that handles both retrieval and generation?
- Basis in paper: [explicit] Section 6.3.1 discusses the vision of a large search model (LSM) that allows an LLM to generate DocIDs and reliable responses autonomously, but notes that current methods struggle to integrate these two generative abilities effectively.
- Why unresolved: While the paper mentions attempts like UniGen and CorpusLM, it states that these approaches have limitations in generalizing across multiple downstream tasks and integrating with powerful LLMs.
- What evidence would resolve it: Successful implementation of a unified framework that demonstrates superior performance in both retrieval and generation tasks compared to separate models, showing effective integration of DocID generation with response generation capabilities.

## Limitations
- Limited empirical validation of personalized and reliable response generation claims, with reliance on qualitative assertions rather than quantitative metrics.
- Lack of standardized training protocols and reproducible experimental setups, making direct comparison across methods difficult.
- Scalability and efficiency trade-offs for large-scale deployments remain underexplored, particularly for text-based DocIDs and real-time knowledge updates.

## Confidence
- **High Confidence**: The categorization of GenIR into generative document retrieval and reliable response generation is well-supported by the literature and aligns with observed trends in the field.
- **Medium Confidence**: Claims about the superiority of generative methods over traditional retrieval are plausible but require more empirical validation, especially for long-tail queries and dynamic corpora.
- **Low Confidence**: Specific performance numbers for personalized assistants and bias mitigation strategies are not substantiated with rigorous experimental data.

## Next Checks
1. Implement and benchmark a baseline GR method (e.g., DSI) on a standard dataset (MS MARCO 300K) using the metrics reported in the survey (Recall@10, MRR@10). Compare results with traditional BM25 and DPR baselines to assess retrieval effectiveness.
2. Evaluate constrained beam search efficiency by profiling decoding latency for different DocID types (numeric vs text-based) and corpus sizes. Measure the trade-off between latency and retrieval accuracy.
3. Test external knowledge augmentation by integrating a simple RAG pipeline and measuring the impact on response accuracy and hallucination reduction using a subset of queries with known ground truth answers.