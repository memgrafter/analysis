---
ver: rpa2
title: 'Open-set object detection: towards unified problem formulation and benchmarking'
arxiv_id: '2411.05564'
source_url: https://arxiv.org/abs/2411.05564
tags:
- unknown
- objects
- object
- ow-detr
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses inconsistencies in evaluating open-set object
  detection (OSOD) methods due to varying datasets, metrics, and unclear definitions
  of unknown objects. It proposes two unified benchmarks: a VOC-COCO combination and
  OpenImagesRoad, the latter providing a hierarchical object definition and new evaluation
  metrics.'
---

# Open-set object detection: towards unified problem formulation and benchmarking

## Quick Facts
- arXiv ID: 2411.05564
- Source URL: https://arxiv.org/abs/2411.05564
- Authors: Hejer Ammar; Nikita Kiselov; Guillaume Lapouge; Romaric Audigier
- Reference count: 33
- Primary result: Introduces unified VOC-COCO and OpenImagesRoad benchmarks for consistent OSOD evaluation, with OW-DETR++ achieving state-of-the-art performance

## Executive Summary
This paper addresses critical inconsistencies in open-set object detection (OSOD) evaluation by introducing two unified benchmarks: a VOC-COCO combination and OpenImagesRoad with hierarchical object definitions. The authors propose OW-DETR++, an improved pseudo-labeling method leveraging self-supervised Vision Transformer features (DINOv2), which significantly outperforms previous OSOD approaches. The study reveals that no single method is universally best - pseudo-labeling excels in localization while contrastive methods perform better in classifying unknowns. The work provides clearer problem definitions, consistent evaluation frameworks, and insights into the strengths of different OSOD strategies depending on whether the learning scenario involves unlabeled or unseen objects.

## Method Summary
The paper introduces OW-DETR++, an improved pseudo-labeling method for OSOD that leverages self-supervised Vision Transformer features from DINOv2. The approach uses DBSCAN to filter background clusters, then applies agglomerative clustering with morphological operations to create semantic object regions. Attention-based filtering selects high-objectness clusters for pseudo-labeling. The method is evaluated on two unified benchmarks: VOC-COCO (combining Pascal VOC trainval with MS-COCO) and OpenImagesRoad (a hierarchical benchmark with 50 known and 113 unknown road-related classes). The unified VOC-COCO benchmark enables consistent comparison across OSOD methods, while OpenImagesRoad provides clearer evaluation through its hierarchical object definition.

## Key Results
- OW-DETR++ achieves state-of-the-art performance on both unified benchmarks, significantly outperforming previous pseudo-labeling and contrastive methods
- Pseudo-labeling methods excel in object localization while contrastive approaches perform better in classifying unknown objects
- The study reveals that no single OSOD method is universally superior across all evaluation metrics
- OpenImagesRoad benchmark demonstrates the importance of clear hierarchical object definitions for consistent OSOD evaluation

## Why This Works (Mechanism)

### Mechanism 1
The hierarchical definition of unknown objects in OpenImagesRoad provides clearer evaluation than prior benchmarks. By using multiple super-classes and splitting them into known and unknown categories, the benchmark removes ambiguity about what constitutes an unknown object. This allows metrics like APu and APsc to accurately measure detection performance. The core assumption is that OpenImages dataset has sufficiently rich and hierarchical annotations to support this approach.

### Mechanism 2
OW-DETR++ improves pseudo-labeling by using self-supervised Vision Transformer features and advanced clustering. DINOv2 features provide richer representations than ResNet features. DBSCAN removes background clusters, agglomerative clustering creates semantic object regions, and attention-based filtering selects high-objectness clusters for pseudo-labeling. The core assumption is that self-supervised ViT features contain better object representations for pseudo-labeling than supervised ResNet features.

### Mechanism 3
The unified VOC-COCO benchmark enables consistent comparison of OSOD methods that was previously impossible. By providing a standardized dataset split and evaluation protocol, researchers can fairly compare different approaches without the confounding factor of different benchmarks. The core assumption is that the VOC-COCO combination adequately represents the OSOD problem space.

## Foundational Learning

- Concept: Open-set vs closed-set object detection
  - Why needed here: The paper's core contribution is addressing open-set scenarios where unknown objects must be detected and handled appropriately
  - Quick check question: What is the key difference between open-set and closed-set object detection?

- Concept: Object detection metrics (AP, mAP, U-Recall)
  - Why needed here: The paper introduces new metrics and uses existing ones to evaluate performance, so understanding these is crucial
  - Quick check question: How does AP differ from mAP in object detection evaluation?

- Concept: Vision Transformer architectures and self-supervised learning
  - Why needed here: OW-DETR++ specifically leverages DINOv2 self-supervised ViT features, so understanding this technology is essential
  - Quick check question: What is the main advantage of self-supervised learning for Vision Transformers in this context?

## Architecture Onboarding

- Component map: VOC/COCO datasets → Feature extraction (DINOv2) → DBSCAN background filtering → Agglomerative clustering → Attention-based filtering → Pseudo-labeling → D-DETR detector → Evaluation (mAPk, APu, U-Recall, APall, APsc)
- Critical path: Pseudo-labeling pipeline → Model training → Evaluation on benchmark splits
- Design tradeoffs: Pseudo-labeling vs contrastive methods (localization vs classification strength), complexity of clustering pipeline vs performance gains
- Failure signatures: Poor unknown detection performance indicates issues with pseudo-labeling quality or feature representation; low mAPk suggests problems with known object detection
- First 3 experiments:
  1. Baseline OW-DETR with ResNet features on VOC-COCO benchmark to establish performance baseline
  2. OW-DETR+ with DINOv2 features (without clustering improvements) to measure impact of feature representation alone
  3. OW-DETR++ full pipeline on OpenImagesRoad to evaluate the complete approach with new metrics

## Open Questions the Paper Calls Out

### Open Question 1
How do pseudo-labeling methods perform in real-world, dynamic environments where unknown objects frequently change or evolve?
Basis in paper: [inferred] The paper discusses pseudo-labeling methods (OW-DETR+) and their performance improvements, but real-world dynamic environments are not explicitly tested.
Why unresolved: The benchmarks used (VOC-COCO and OpenImagesRoad) are static datasets, which do not reflect the dynamic nature of real-world environments.
What evidence would resolve it: Testing pseudo-labeling methods in a dynamic, real-world setting with evolving unknown objects and measuring their adaptability and performance over time.

### Open Question 2
What are the limitations of using hierarchical object definitions in diverse, non-road-based datasets?
Basis in paper: [explicit] The paper introduces OpenImagesRoad, which uses a hierarchical object definition, but does not explore its applicability to non-road-based datasets.
Why unresolved: The paper focuses on road images, and it is unclear how well hierarchical definitions would work in other contexts, such as indoor or wild environments.
What evidence would resolve it: Applying hierarchical object definitions to a variety of non-road-based datasets and evaluating their effectiveness in defining and detecting unknown objects.

### Open Question 3
How does the choice of clustering algorithm and parameters affect the performance of pseudo-labeling methods like OW-DETR++?
Basis in paper: [explicit] The paper mentions the use of DBSCAN and agglomerative clustering in OW-DETR++, but does not explore the impact of different algorithms or parameter settings.
Why unresolved: The paper uses specific clustering parameters, but does not investigate how changes in these parameters or the use of different algorithms might affect performance.
What evidence would resolve it: Conducting experiments with various clustering algorithms and parameter settings to determine their impact on the performance of pseudo-labeling methods.

## Limitations
- Reliance on OpenImages hierarchical annotations introduces uncertainty about the consistency and quality of the known/unknown split
- The clustering pipeline complexity (DBSCAN + agglomerative clustering + morphological operations) lacks comprehensive ablation studies
- The VOC-COCO benchmark, while unified, may not fully represent the diversity of real-world open-set scenarios

## Confidence
- High confidence: The unified VOC-COCO benchmark creation and the identification of inconsistencies in OSOD evaluation practices
- Medium confidence: The superiority of OW-DETR++ over existing methods, given the complex pipeline and lack of detailed component ablation
- Low confidence: The generalizability of findings to datasets beyond the proposed benchmarks

## Next Checks
1. Conduct ablation studies on the clustering pipeline components (DBSCAN filtering, agglomerative clustering, morphological operations) to quantify individual contributions
2. Test OW-DETR++ on additional open-set detection datasets (e.g., LVIS, Objects365) to validate generalizability beyond the proposed benchmarks
3. Perform detailed analysis of false positive patterns in unknown detection to identify whether specific object categories or visual characteristics consistently cause errors