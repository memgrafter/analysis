---
ver: rpa2
title: Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation
arxiv_id: '2412.12089'
source_url: https://arxiv.org/abs/2412.12089
tags:
- simulation
- differentiable
- learning
- policy
- sapo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAPO, a maximum entropy first-order model-based
  RL algorithm that leverages analytic gradients from differentiable simulation to
  stabilize policy learning. The authors also present Rewarped, a parallel differentiable
  multiphysics simulation platform enabling efficient GPU-accelerated training on
  tasks involving rigid bodies, articulations, and deformables.
---

# Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation

## Quick Facts
- arXiv ID: 2412.12089
- Source URL: https://arxiv.org/abs/2412.12089
- Authors: Eliot Xing; Vernon Luk; Jean Oh
- Reference count: 40
- One-line primary result: SAPO outperforms PPO, SAC, APG, and SHAC on manipulation and locomotion tasks involving deformables, achieving up to 20× speedup with Rewared.

## Executive Summary
This paper introduces SAPO, a maximum entropy first-order model-based RL algorithm that leverages analytic gradients from differentiable simulation to stabilize policy learning. The authors also present Rewared, a parallel differentiable multiphysics simulation platform enabling efficient GPU-accelerated training on tasks involving rigid bodies, articulations, and deformables. SAPO outperforms baselines across challenging manipulation and locomotion tasks in terms of both sample efficiency and final performance. Ablation studies show that entropy regularization and policy parameterization are critical to SAPO's gains over SHAC.

## Method Summary
The authors propose SAPO (Soft Analytic Policy Optimization), a maximum entropy first-order model-based RL algorithm that uses analytic gradients from differentiable simulation for policy optimization. They develop Rewared, a parallel differentiable multiphysics simulation platform supporting rigid bodies, articulations, and deformables (MPM, FEM). SAPO combines entropy-augmented H-step returns with TD(λ), automatic temperature tuning, and critic ensemble without target networks. The method is evaluated on six tasks (HandFlip, SoftJumper, FluidMove, RollingFlat, HandReorient, AntRun) and compared against PPO, SAC, APG, SHAC, and TrajOpt.

## Key Results
- SAPO outperforms PPO, SAC, APG, and SHAC across challenging manipulation and locomotion tasks involving rigid bodies, articulations, and deformables
- Rewared enables up to 20× speedup in simulation runtime versus prior methods (DexDeform)
- Ablation studies show that entropy regularization and policy parameterization are critical to SAPO's gains over SHAC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy regularization smooths the optimization landscape and stabilizes policy learning when using first-order gradients from differentiable simulation.
- Mechanism: By adding the entropy of the policy distribution to the return, SAPO encourages exploration and prevents premature convergence to poor local minima that can occur due to discontinuities in contact forces.
- Core assumption: The non-smoothness and bias introduced by contact forces in differentiable simulation are significant enough to destabilize gradient-based policy optimization without entropy regularization.
- Evidence anchors:
  - [abstract] "we hypothesize that entropy regularization can stabilize policy optimization over analytic gradients from differentiable simulation, such as by smoothing the optimization landscape (Ahmed et al., 2019)"
  - [section 4.1] "We hypothesize that entropy regularization can stabilize policy optimization over analytic gradients from differentiable simulation, such as by smoothing the optimization landscape (Ahmed et al., 2019) or providing robustness under perturbations (Eysenbach & Levine, 2022)."
  - [corpus] Weak or missing evidence for this specific claim about differentiable simulation and contact forces.
- Break condition: If the differentiable simulation does not exhibit significant discontinuities due to contacts, or if the policy optimization is inherently stable without entropy.

### Mechanism 2
- Claim: Parallel differentiable multiphysics simulation enables large-scale RL training on tasks involving deformables by providing GPU-accelerated environments with analytic gradients.
- Mechanism: Rewared provides a platform for parallelizing RL environments of GPU-accelerated differentiable multiphysics simulation, enabling efficient computation of batched simulation gradients and up to 20x speedup compared to prior methods.
- Core assumption: The overhead of parallelization and differentiable simulation is offset by the speedup gained, making it feasible to train RL policies on tasks with deformables.
- Evidence anchors:
  - [abstract] "Alongside our approach, we develop Rewared, a parallel differentiable multiphysics simulation platform that supports simulating various materials beyond rigid bodies."
  - [section 5.1] "We implement all simulation code in NVIDIA Warp (Macklin, 2022)... We use gradient checkpointing... to reduce memory requirements."
  - [section F.5] "With one environment, Rewared's MPM implementation is 3× faster and uses 2/3 less memory compared to DexDeform's. Using 32 environments, Rewared achieves a 20× total speedup compared to DexDeform for simulating the HandFlip task."
- Break condition: If the parallelization overhead exceeds the speedup benefits, or if the differentiable simulation is not sufficiently accurate for the tasks.

### Mechanism 3
- Claim: The design choices in SAPO, such as using state-dependent variance, critic ensemble, and removing target networks, improve training stability and performance compared to SHAC.
- Mechanism: These design choices address specific limitations of SHAC, such as overly conservative exploration, instability in TD learning, and suboptimal policy parameterization.
- Core assumption: The limitations of SHAC are significant enough to impact performance, and the proposed design choices effectively address these limitations.
- Evidence anchors:
  - [section 4.2] "IV. Critic ensemble, no target networks... CrossQ also reduces Adam β1 momentum from 0.9 to 0.5... Using smaller momentum parameters decreases exponential decay... and effectively gives higher weight to more recent gradients, with less smoothing by past gradient history."
  - [section 6.2] "We observe that ablation (c), where we apply design choices {III, IV, V} onto SHAC, result in approximately half of the performance improvement of SAPO over SHAC on the HandFlip task."
  - [corpus] Weak or missing evidence for the specific impact of these design choices on training stability and performance.
- Break condition: If the design choices do not improve training stability or performance, or if they introduce new limitations.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The paper formulates reinforcement learning as an MDP, which provides a framework for understanding the interaction between an agent and its environment.
  - Quick check question: What are the components of an MDP, and how do they relate to the reinforcement learning problem?

- Concept: Differentiable simulation
  - Why needed here: The paper uses differentiable simulation to provide analytic gradients for policy optimization, which is a key component of the SAPO algorithm.
  - Quick check question: How does differentiable simulation differ from traditional simulation, and what are the benefits of using it for reinforcement learning?

- Concept: Entropy regularization
  - Why needed here: The paper uses entropy regularization to stabilize policy optimization and encourage exploration, which is a key component of the SAPO algorithm.
  - Quick check question: What is entropy regularization, and how does it help with policy optimization in reinforcement learning?

## Architecture Onboarding

- Component map: Rewared -> SAPO -> Actor-critic architecture -> Differentiable simulation
- Critical path:
  1. Initialize Rewared with the desired task and environment settings
  2. Initialize SAPO with the actor-critic networks and hyperparameters
  3. Collect data by running the policy in the environment
  4. Compute analytic gradients using differentiable simulation
  5. Update the policy and value function using the gradients
  6. Repeat steps 3-5 until convergence
- Design tradeoffs:
  - Parallelization vs. overhead: More environments can lead to better speedup, but also increased overhead
  - Differentiable simulation vs. accuracy: Differentiable simulation provides analytic gradients, but may sacrifice some accuracy compared to traditional simulation
  - Entropy regularization vs. exploitation: Entropy regularization encourages exploration, but may slow down exploitation of good policies
- Failure signatures:
  - Poor performance: The policy may not learn effectively due to suboptimal hyperparameters or task settings
  - Instability: The training may be unstable due to issues with the differentiable simulation or policy optimization
  - Slow convergence: The policy may converge slowly due to insufficient exploration or suboptimal hyperparameters
- First 3 experiments:
  1. Run SAPO on a simple task (e.g., AntRun) with default hyperparameters to verify basic functionality
  2. Vary the entropy regularization coefficient to observe its impact on exploration and performance
  3. Increase the number of parallel environments to measure the speedup and scalability of Rewared

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SAPO's performance scale when using high-dimensional visual observations (e.g., rendered images) instead of subsampled particle states?
- Basis in paper: [explicit] The authors note that current SAPO uses subsampled particle states, which is infeasible for real-world applications, and suggest differentiable rendering as a future direction.
- Why unresolved: The paper does not evaluate SAPO with rendered visual observations, leaving the impact of this architectural change on performance unknown.
- What evidence would resolve it: Experiments comparing SAPO's performance using rendered images versus subsampled particle states on the same tasks would clarify the trade-offs.

### Open Question 2
- Question: What is the impact of increasing the number of environments beyond 32 on SAPO's sample efficiency and final performance?
- Basis in paper: [explicit] The authors report that Rewared's performance plateaus beyond 32 environments due to GPU utilization limits.
- Why unresolved: The paper does not explore scaling beyond 32 environments, leaving uncertainty about potential gains from additional parallelization.
- What evidence would resolve it: Running SAPO with more environments (e.g., 64, 128) on tasks like HandFlip would show if performance improvements continue or plateau.

### Open Question 3
- Question: How sensitive is SAPO to the choice of entropy regularization temperature and target entropy?
- Basis in paper: [explicit] SAPO uses automatic temperature tuning with a fixed target entropy of -dim(A)/2, but the authors do not explore sensitivity to these hyperparameters.
- Why unresolved: The paper does not report ablation studies on entropy temperature or target entropy, leaving their impact on stability and performance unclear.
- What evidence would resolve it: Systematic ablation studies varying temperature and target entropy on tasks like HandFlip would reveal their effects on convergence and robustness.

## Limitations

- The evidence for the effectiveness of entropy regularization in stabilizing policy optimization with differentiable simulation is weak, with no direct experimental comparison between SAPO with and without entropy regularization.
- The specific impact of design choices on training stability and performance is not well-supported by the evidence provided.
- The paper lacks complete specification for network architectures and entropy target normalization formulas, which could impact faithful reproduction.

## Confidence

- High: Rewared enables up to 20× speedup in simulation runtime versus prior methods.
- Medium: SAPO outperforms baselines including PPO, SAC, APG, and SHAC across challenging manipulation and locomotion tasks.
- Low: Entropy regularization smooths the optimization landscape and stabilizes policy learning when using first-order gradients from differentiable simulation.

## Next Checks

1. Conduct an ablation study comparing SAPO with and without entropy regularization to directly test its impact on training stability and performance.
2. Perform a detailed analysis of the differentiable simulation's behavior under various contact scenarios to quantify the discontinuities and biases introduced by contact forces.
3. Implement and test alternative policy parameterization methods to evaluate their impact on exploration and exploitation in the presence of differentiable simulation gradients.