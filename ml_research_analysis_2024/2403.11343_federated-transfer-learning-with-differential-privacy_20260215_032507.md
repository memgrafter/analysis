---
ver: rpa2
title: Federated Transfer Learning with Differential Privacy
arxiv_id: '2403.11343'
source_url: https://arxiv.org/abs/2403.11343
tags:
- data
- privacy
- algorithm
- learning
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel privacy framework called Federated
  Differential Privacy (FDP) for transfer learning in federated settings. FDP provides
  rigorous privacy guarantees for each data set without requiring a trusted central
  server.
---

# Federated Transfer Learning with Differential Privacy

## Quick Facts
- **arXiv ID**: 2403.11343
- **Source URL**: https://arxiv.org/abs/2403.11343
- **Authors**: Mengchu Li; Ye Tian; Yang Feng; Yi Yu
- **Reference count**: 40
- **Primary result**: Introduces Federated Differential Privacy (FDP) framework providing intermediate privacy guarantees between central and local differential privacy models for federated transfer learning

## Executive Summary
This paper introduces Federated Differential Privacy (FDP), a novel privacy framework for transfer learning in federated settings that provides rigorous privacy guarantees for each data set without requiring a trusted central server. The authors study three statistical problems - univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression - under this framework. FDP establishes matching upper and lower bounds on minimax rates, showing it offers intermediate privacy protection between central and local differential privacy models. The framework includes a general detection strategy to identify informative source data sets and adaptive clipping strategies that relax sample size requirements compared to existing methods.

## Method Summary
The FDP framework implements a multi-round communication protocol where each site applies local privacy mechanisms to protect raw data before sharing with a central server. The method includes three main components: (1) a general detection strategy that automatically identifies informative source data sets based on private parameter estimates, (2) differentially private algorithms for mean estimation and linear regression with adaptive clipping strategies, and (3) federated aggregation procedures that combine information from target and selected sources while maintaining privacy guarantees. The framework is applied to three statistical problems, establishing matching upper and lower bounds on minimax rates and demonstrating the fundamental costs of privacy and data heterogeneity in federated transfer learning settings.

## Key Results
- FDP provides intermediate privacy protection between central and local differential privacy models, with privacy costs depending on the number of participating sites and sample sizes
- Matching upper and lower bounds on minimax rates are established for mean estimation and low-dimensional regression, demonstrating the optimality of FDP algorithms
- Adaptive clipping strategies relax sample size requirements for private linear regression compared to fixed clipping methods
- The source detection mechanism successfully identifies informative sources, with theoretical guarantees on false positive and false negative rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Federated Differential Privacy (FDP) provides intermediate privacy guarantees between central and local differential privacy.
- **Mechanism**: FDP ensures each site protects its data locally using privacy mechanisms while allowing a central server to aggregate private information across sites. The privacy guarantee is enforced through a multi-round communication protocol where each round's privacy mechanism satisfies a differential privacy condition with respect to both the current data and previous rounds' private information.
- **Core assumption**: Sites can be trusted to apply local privacy mechanisms to their raw data before sharing with the central server, and the communication protocol can be structured to maintain privacy guarantees across rounds.
- **Evidence anchors**:
  - [abstract]: "FDP provides privacy guarantees for each data set without requiring a trusted central server."
  - [section]: "Our constraint requires that each site protects its information locally and only communicates the privatised information to the central server for analysis."
  - [corpus]: Weak evidence - corpus papers focus on vertical federated learning and privacy-preserving frameworks but don't explicitly discuss FDP as an intermediate model.
- **Break condition**: If sites cannot be trusted to apply privacy mechanisms correctly, or if the communication protocol cannot maintain privacy across rounds (e.g., due to adaptive attacks).

### Mechanism 2
- **Claim**: The informative source detection strategy automatically identifies useful source data sets for transfer learning.
- **Mechanism**: The algorithm compares private parameter estimates from each source to the target's private estimate, using the target's estimation accuracy as a threshold. This creates a set of sources deemed sufficiently similar to the target for effective knowledge transfer.
- **Core assumption**: The estimation error on the target data can serve as a reliable threshold for determining source informativeness, and the private estimates maintain enough accuracy to distinguish between informative and non-informative sources.
- **Evidence anchors**:
  - [abstract]: "Developing a general detection strategy to identify informative source data sets based on private parameter estimates."
  - [section]: "We develop a general detection procedure to identify the informative source data sets, as described in Appendix A, automatically selecting a set ˆA ⊆ [K] without using any information about A and h."
  - [corpus]: Weak evidence - corpus papers discuss source selection in federated transfer learning but don't provide the specific detection mechanism described here.
- **Break condition**: If the estimation error on the target is too large to serve as an effective threshold, or if the privacy mechanism introduces too much noise, making source discrimination unreliable.

### Mechanism 3
- **Claim**: Adaptive clipping strategies relax sample size requirements for private linear regression.
- **Mechanism**: Instead of using fixed clipping thresholds, the algorithm estimates the gradient's scale dynamically using a PrivateVariance mechanism. This allows for more efficient noise addition and broader applicability across parameter regimes.
- **Core assumption**: The gradient scale can be accurately estimated privately without compromising the overall privacy guarantee, and this estimation is stable across iterations.
- **Evidence anchors**:
  - [abstract]: "Proposing adaptive clipping strategies for private linear regression that relax sample size requirements compared to existing methods."
  - [section]: "We use the recently proposed adaptive clipping strategy (Varshney et al., 2022) to achieve optimality over a larger range of parameters compared to Cai et al. (2019)."
  - [corpus]: No direct evidence - corpus papers don't discuss adaptive clipping strategies for private linear regression.
- **Break condition**: If the PrivateVariance mechanism fails to provide accurate scale estimates, or if the dynamic nature of clipping introduces instability in the learning process.

## Foundational Learning

- **Concept**: Differential Privacy and its variants (central, local, federated)
  - **Why needed here**: Understanding the privacy guarantees and limitations of different DP models is crucial for appreciating why FDP provides an intermediate solution and how it balances utility and privacy.
  - **Quick check question**: What is the key difference between central DP and local DP in terms of who applies the privacy mechanism and when?

- **Concept**: Transfer Learning and Negative Transfer
  - **Why needed here**: The paper's goal is to improve learning on a target dataset by leveraging source datasets, but only if the sources are sufficiently similar. Understanding transfer learning principles and the risk of negative transfer is essential for grasping the importance of the source detection mechanism.
  - **Quick check question**: What is negative transfer, and why is it a concern when combining information from multiple source datasets?

- **Concept**: Minimax Theory and Statistical Estimation
  - **Why needed here**: The paper establishes minimax rates for various statistical problems under FDP constraints. Understanding minimax theory is necessary to appreciate the optimality results and the fundamental limits of learning under privacy constraints.
  - **Quick check question**: What does it mean for an estimator to be minimax optimal, and how is this concept used to evaluate the performance of private learning algorithms?

## Architecture Onboarding

- **Component map**: Privacy Mechanism -> Source Detection -> Federated Learning -> Final Estimate
- **Critical path**: Privacy Mechanism → Source Detection → Federated Learning → Final Estimate
  The privacy mechanisms must be correctly applied at each site, the source detection must accurately identify informative sources, and the federated learning algorithm must effectively combine information while maintaining privacy.

- **Design tradeoffs**:
  - Privacy vs. Utility: Stronger privacy guarantees (smaller ε) lead to more noise and potentially lower utility
  - Sample Size vs. Dimensionality: High-dimensional problems require larger sample sizes, especially under privacy constraints
  - Communication Efficiency vs. Privacy: More rounds of communication can improve accuracy but may increase privacy costs

- **Failure signatures**:
  - High estimation error on target data: Could indicate insufficient privacy budget or poor source selection
  - Source detection fails to identify any informative sources: May suggest too much noise in private estimates or overly strict threshold
  - Privacy guarantee violation: Indicates incorrect implementation of privacy mechanisms or composition errors

- **First 3 experiments**:
  1. **Univariate Mean Estimation**: Implement the FDP framework for mean estimation and compare performance to central DP and LDP baselines across different privacy parameters and source heterogeneity levels.
  2. **Low-Dimensional Linear Regression**: Test the adaptive clipping strategy for linear regression under FDP constraints and evaluate its impact on sample size requirements compared to fixed clipping methods.
  3. **High-Dimensional Linear Regression**: Implement the FDP framework for high-dimensional regression and investigate the d/√s factor in the privacy term, exploring potential methods to reduce this dependency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the FDP framework achieve optimal rates for high-dimensional linear regression when sample sizes are large (nk ≫ 1)?
- **Basis in paper**: The authors state that "a main open question that arises is whether we can improve the result when nk's are large and obtain estimation error that scales with s instead of √ds in (29)." They note that under LDP constraints, a similar term √ds has been shown to be unimprovable.
- **Why unresolved**: The paper only provides an upper bound on the estimation error rate for high-dimensional regression, which scales with √ds rather than s. The authors conjecture that under additional assumptions (like variable selection consistency), it might be possible to achieve the s scaling, but they do not provide a proof.
- **What evidence would resolve it**: A rigorous proof showing either (1) that the √ds scaling is indeed optimal for FDP in high-dimensional regression even with large nk, or (2) that under specific additional assumptions, the s scaling can be achieved.

### Open Question 2
- **Question**: How does the FDP framework perform in scenarios with unbalanced sample sizes across sites and personalized privacy requirements at each site?
- **Basis in paper**: The authors mention in the discussion that "Achieving optimality in scenarios with unbalanced sample sizes and personalized privacy requirements at each site necessitates further investigation."
- **Why unresolved**: The current theoretical results assume balanced sample sizes (nk = n) and a uniform privacy parameter (ϵ, δ) across all sites. The paper does not explore how the framework behaves when these assumptions are violated.
- **What evidence would resolve it**: Empirical or theoretical results demonstrating the performance of FDP algorithms in settings with (1) highly unbalanced sample sizes (e.g., some sites have n=100 while others have n=10,000) and (2) different privacy parameters for each site (e.g., some sites require (ϵ=0.1, δ=10^-6) while others require (ϵ=1, δ=10^-4)).

### Open Question 3
- **Question**: Can the FDP framework be extended to accommodate federated learning scenarios where not all sources are available to participate in each iteration?
- **Basis in paper**: The authors note in the discussion that "real-world scenarios may involve sources dropping out temporarily due to technical issues such as battery failure and network connection issues" and that "extending the algorithms and theory to accommodate practical situations where not all sources are available to participate in each iteration could enhance the applicability of the proposed methods."
- **Why unresolved**: The current interactive algorithms require the noisy gradient from all selected sources in each iteration. The paper does not address how to handle missing sources or what the theoretical guarantees would be in such cases.
- **What evidence would resolve it**: Development and theoretical analysis of FDP algorithms that can handle (1) random source dropout with known probabilities, (2) adversarial source dropout, or (3) asynchronous updates where sources join and leave at different times, along with corresponding convergence rate guarantees.

## Limitations
- The d/√s factor in high-dimensional regression privacy terms suggests fundamental limitations that may not be easily overcome without additional assumptions
- The source detection mechanism's robustness under realistic conditions with noisy estimates and heterogeneous data distributions remains untested
- The framework assumes balanced sample sizes and uniform privacy parameters across sites, which may not hold in practical federated learning scenarios

## Confidence

- **High Confidence**: The theoretical framework for FDP privacy guarantees and the minimax rate analysis for mean estimation and low-dimensional regression are well-established.
- **Medium Confidence**: The adaptive clipping strategy's effectiveness across diverse parameter regimes needs empirical validation.
- **Low Confidence**: The practical performance of the source detection mechanism in real-world scenarios with noisy estimates and heterogeneous data distributions.

## Next Checks
1. Implement the detection strategy on synthetic data with varying levels of source-target similarity to empirically validate its accuracy and robustness to estimation noise.
2. Conduct experiments comparing fixed vs. adaptive clipping strategies across a range of privacy budgets and dimensionalities to quantify practical benefits.
3. Test the framework on real-world federated learning benchmarks to assess performance when source heterogeneity and privacy constraints interact in complex ways.