---
ver: rpa2
title: 'ModelGrow: Continual Text-to-Video Pre-training with Model Expansion and Language
  Understanding Enhancement'
arxiv_id: '2412.18966'
source_url: https://arxiv.org/abs/2412.18966
tags:
- block
- video
- llms
- transformer
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ModelGrow introduces continual pre-training for text-to-video generation
  models through model expansion and language understanding enhancement. The method
  addresses catastrophic forgetting and limited semantic understanding in T2V models
  by expanding transformer blocks and incorporating LLM embeddings.
---

# ModelGrow: Continual Text-to-Video Pre-training with Model Expansion and Language Understanding Enhancement

## Quick Facts
- arXiv ID: 2412.18966
- Source URL: https://arxiv.org/abs/2412.18966
- Authors: Zhefan Rao; Liya Ji; Yazhou Xing; Runtao Liu; Zhaoyang Liu; Jiaxin Xie; Ziqiao Peng; Yingqing He; Qifeng Chen
- Reference count: 40
- Primary result: 78.49% total score on VBench and 97.00% subject consistency on CompBench

## Executive Summary
ModelGrow introduces a continual pre-training framework for text-to-video generation models that addresses catastrophic forgetting and limited semantic understanding through model expansion and LLM integration. The approach expands transformer blocks and incorporates large language model embeddings to enhance language comprehension while maintaining previously learned knowledge. The method achieves state-of-the-art performance on VBench and CompBench benchmarks, demonstrating significant improvements in video quality and semantic alignment with prompts.

## Method Summary
ModelGrow employs a two-pronged approach to continual pre-training: model expansion through transformer block duplication and language understanding enhancement via LLM embeddings. The expansion method copies adjacent transformer blocks with zero initialization, creating identity functions that preserve existing knowledge while adding capacity for new information. Parallel LLM cross-attention blocks process embeddings from large language models, providing richer semantic context than traditional text encoders. The approach uses modified re-captioning that concatenates original and detailed prompts to preserve key information while adding contextual details.

## Key Results
- Expansion-1.4B-LLM model achieves 78.49% total score on VBench
- Model achieves 97.00% subject consistency on CompBench
- Outperforms baseline models in both quantitative metrics and qualitative examples
- Successfully mitigates catastrophic forgetting while acquiring new knowledge

## Why This Works (Mechanism)

### Mechanism 1: Block Duplication Expansion
The expansion method copies transformer blocks from adjacent layers and applies zero initialization to new blocks, creating identity functions that preserve existing knowledge while adding capacity for new information. The copied weights from adjacent blocks provide a good initialization point that maintains model performance while allowing adaptation to new data.

### Mechanism 2: LLM Embedding Integration
Parallel LLM cross-attention blocks process embeddings from large language models, providing richer semantic context than traditional text encoders alone. The architecture adds these blocks following T5 cross-attention modules to enhance language comprehension for detailed prompts.

### Mechanism 3: Modified Re-captioning
The method concatenates original and detailed LLM-generated prompts rather than replacing them entirely. This preserves critical keywords from short prompts while adding contextual details from detailed versions, maintaining key information while enhancing semantic richness.

## Foundational Learning

- **Concept: Diffusion model training process**
  - Why needed here: Understanding how ModelGrow builds upon diffusion transformers is crucial for implementing the expansion and LLM integration
  - Quick check question: What is the purpose of the denoising process in diffusion models and how does it relate to the training loss formulation?

- **Concept: Transformer block architecture**
  - Why needed here: The expansion and LLM enhancement methods directly modify transformer blocks, requiring understanding of their internal structure
  - Quick check question: How do self-attention blocks differ from cross-attention blocks in transformer architectures?

- **Concept: Catastrophic forgetting in continual learning**
  - Why needed here: ModelGrow aims to prevent catastrophic forgetting while acquiring new knowledge
  - Quick check question: What are the primary mechanisms by which neural networks forget previously learned information during training on new tasks?

## Architecture Onboarding

**Component Map:** Text Encoder -> Transformer Blocks -> Expansion Blocks -> LLM Cross-Attention -> Decoder Blocks -> Video Generation

**Critical Path:** The critical path flows through the expanded transformer blocks and LLM cross-attention modules, where the core innovations of model expansion and language understanding enhancement occur. These components directly influence the quality of the generated videos.

**Design Tradeoffs:** The expansion approach trades increased model size and computational cost for improved performance and knowledge retention. Using LLM embeddings provides richer semantic information but requires additional computational overhead and potential alignment challenges with video generation requirements.

**Failure Signatures:** Poor performance may manifest as catastrophic forgetting (degraded performance on original tasks), semantic misalignment (videos not matching prompts), or computational inefficiency (excessive resource usage without proportional performance gains).

**First Experiments:**
1. Test expansion with different initialization strategies (zero vs. random) to validate the effectiveness of the current approach
2. Evaluate the impact of varying the number of LLM cross-attention blocks on language understanding and generation quality
3. Compare the modified re-captioning approach against using only original or only detailed prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal expansion ratio and block stacking configuration for different base model sizes in text-to-video generation?
- Basis in paper: The paper chose insert stacking with P=N but acknowledges different variants were tested with varying results
- Why unresolved: Only tested