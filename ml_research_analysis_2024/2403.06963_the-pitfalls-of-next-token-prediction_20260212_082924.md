---
ver: rpa2
title: The pitfalls of next-token prediction
arxiv_id: '2403.06963'
source_url: https://arxiv.org/abs/2403.06963
tags:
- next-token
- prediction
- learning
- training
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a fundamental limitation of teacher-forcing
  training for next-token prediction models. The authors argue that when learning
  tasks requiring lookahead planning, teacher-forcing can induce "Clever Hans cheating"
  where models use ground truth answer prefixes as shortcuts rather than learning
  true planning mechanisms.
---

# The pitfalls of next-token prediction

## Quick Facts
- arXiv ID: 2403.06963
- Source URL: https://arxiv.org/abs/2403.06963
- Authors: Gregor Bachmann; Vaishnavh Nagarajan
- Reference count: 40
- Key outcome: Teacher-forcing training can induce "Clever Hans cheating" in lookahead planning tasks, creating "indecipherable" tokens that prevent proper planning.

## Executive Summary
This paper identifies a fundamental limitation of teacher-forcing training for next-token prediction models. The authors argue that when learning tasks requiring lookahead planning, teacher-forcing can induce "Clever Hans cheating" where models use ground truth answer prefixes as shortcuts rather than learning true planning mechanisms. This cheating behavior destroys supervision for earlier tokens, making them "indecipherable" and preventing proper planning. Experiments on path-finding tasks with Transformers and Mamba architectures demonstrate complete in-distribution failure despite the tasks being straightforward. The authors show that modifying training to predict multiple future tokens at once (teacherless training) can circumvent these failures in some settings, providing preliminary evidence that alternatives to next-token prediction may be needed for planning tasks.

## Method Summary
The paper evaluates next-token prediction models (Transformers and Mamba) on a minimal path-finding lookahead task, where models must find a path from a start node to a goal node in a path-star graph. The key insight is that teacher-forcing, which provides ground truth prefixes during training, can induce "Clever Hans cheating" where models learn to cheat using these prefixes rather than developing true planning capabilities. The authors propose teacherless training, where models predict multiple future tokens at once, as a potential solution. Experiments use three architectures (GPT-Mini, GPT2-Large, Mamba) on graph configurations with varying degrees and path lengths, measuring accuracy metrics including Accag (autoregressive accuracy), Acccheat (Clever Hans accuracy), Acc1st (first token accuracy), and Acc$ (teacherless accuracy).

## Key Results
- Teacher-forcing models completely fail on path-finding tasks despite being trained on ground truth prefixes
- Clever Hans cheating destroys supervision for earlier tokens, making them "indecipherable"
- Teacherless training can circumvent these failures in some settings by forcing lookahead
- The failure is architecture-agnostic, affecting both Transformers and Mamba models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Teacher-forcing can fail to learn accurate next-token predictors for lookahead tasks
- Mechanism: When learning tasks requiring lookahead planning, teacher-forcing can induce "Clever Hans cheating" where models use ground truth answer prefixes as shortcuts rather than learning true planning mechanisms
- Core assumption: The model will prefer simple cheating mechanisms over complex planning when given the opportunity
- Evidence anchors:
  - [abstract] "This cheating behavior destroys supervision for earlier tokens, making them 'indecipherable' and preventing proper planning"
  - [section 4.3] "By revealing parts of the answer to the model as input, we allow the model to fit the data by cheating"
  - [corpus] Found 25 related papers - weak evidence for this specific mechanism (0 citations)
- Break condition: When no shortcuts exist for later tokens, forcing the model to learn true planning mechanisms

### Mechanism 2
- Claim: Teacher-forcing creates "indecipherable" tokens that cannot be learned
- Mechanism: After the Clever Hans cheat is perfected, the model is deprived of information about much of the full solution which was once present as supervisory targets, making earlier tokens harder to learn
- Core assumption: The loss surface becomes all-or-nothing for the remaining tokens after cheating mechanisms are learned
- Evidence anchors:
  - [abstract] "The authors argue that when learning tasks requiring lookahead planning, teacher-forcing can induce 'Clever Hans cheating' where models use ground truth answer prefixes as shortcuts rather than learning true planning mechanisms"
  - [section 4.4] "The model is simply left with the task of mapping the input p to an incomplete solution... learning this task may become computationally harder, or even intractable"
  - [corpus] Average neighbor citations=0.0 - weak evidence for this specific claim
- Break condition: When the model is forced to learn without access to ground truth prefixes (teacherless training)

### Mechanism 3
- Claim: Teacherless training can circumvent teacher-forcing failures
- Mechanism: Predicting multiple future tokens at once forces the model to look ahead and learn planning mechanisms rather than using shortcuts
- Core assumption: Without access to ground truth prefixes, the model must learn to deduce future tokens from available information
- Evidence anchors:
  - [abstract] "The authors show that modifying training to predict multiple future tokens at once (teacherless training) can circumvent these failures in some settings"
  - [section 5.2] "The teacherless model can't use the trivial Clever Hans cheat to fit the data, since the ground truth prefixes are not available during training"
  - [corpus] Found 25 related papers - moderate evidence for this approach (0 citations)
- Break condition: When the task is too complex for the model to learn without ground truth prefixes

## Foundational Learning

- Concept: Teacher-forcing vs autoregressive inference distinction
  - Why needed here: The paper argues these two phases must be treated distinctly as they lead to vastly different failures
  - Quick check question: What is the key difference between teacher-forcing and autoregressive inference in terms of input to the model?

- Concept: Lookahead planning tasks
  - Why needed here: The failures identified only occur in tasks that require implicitly planning a later token in advance of an earlier token
  - Quick check question: Why does a path-finding task qualify as a lookahead task?

- Concept: Clever Hans cheating
  - Why needed here: This is the core mechanism by which teacher-forcing fails - the model uses revealed ground truth as shortcuts
  - Quick check question: In the path-finding task, how does the model cheat when predicting tokens after the first one?

## Architecture Onboarding

- Component map: Graph generation -> Tokenization -> Model training (teacher-forcing/teacherless/reversed) -> Accuracy evaluation
- Critical path: Teacher-forcing training loop → Clever Hans cheating → Indecipherable token failure → complete in-distribution failure
- Design tradeoffs: Teacher-forcing provides stable training but can induce cheating; teacherless training forces lookahead but may be harder to optimize
- Failure signatures: High training accuracy but low inference accuracy (Accag), high Clever Hans accuracy (Acccheat), low first token accuracy (Acc1st)
- First 3 experiments:
  1. Train standard teacher-forced model on path-finding task and measure Accag, Acccheat, Acc1st
  2. Train teacherless model (predict multiple tokens) and measure Acc$
  3. Train reversed model and measure Accrev

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scale of the model (e.g., moving from GPT-2 Large to GPT-4 or larger architectures) affect the Clever Hans cheating phenomenon and the Indecipherable Token failure?
- Basis in paper: [explicit] The paper notes that results were demonstrated on GPT-2 Large and Mamba architectures, but acknowledges that failure modes on very large models like Llama2 or Mistral have not been demonstrated.
- Why unresolved: The paper only tests on smaller, from-scratch models and one pre-trained medium-sized model. It's unclear whether the cheating behavior scales with model capacity or if larger models can circumvent these failures through increased expressiveness.
- What evidence would resolve it: Experiments showing whether larger models (GPT-3, GPT-4, Llama2, etc.) exhibit the same in-distribution failures when trained on the path-star task, or whether they can learn the planning task despite teacher-forcing.

### Open Question 2
- Question: Does the Clever Hans cheating mechanism generalize beyond path-finding and arithmetic tasks to more complex creative writing tasks like story generation with plot twists?
- Basis in paper: [explicit] The authors speculate in Section C that teacher-forced models trained on thousands of stories may not learn to plan plot twists, instead generating arbitrary events and forcing contrived resolutions.
- Why unresolved: The paper provides only conceptual speculation about story-writing without empirical verification. Testing this would require defining what constitutes "proper" planning in stories and measuring whether teacher-forcing prevents it.
- What evidence would resolve it: Experiments training models on story datasets with and without chain-of-thought supervision, then evaluating whether teacher-forced models produce stories with coherent plot structures versus arbitrary, disconnected events.

### Open Question 3
- Question: What is the precise relationship between teacherless training and chain-of-thought supervision in terms of their ability to enable planning in language models?
- Basis in paper: [explicit] The authors note that teacherless training allows models to "voluntarily learn right-to-left" in path-finding tasks, but acknowledge that this success isn't fully explained by existing positive results about chain-of-thought supervision.
- Why unresolved: While the paper shows teacherless training can circumvent failures in some settings, it doesn't clarify whether this works by implicitly learning a chain-of-thought, or through a different mechanism entirely. The theoretical relationship remains unclear.
- What evidence would resolve it: Detailed analysis of hidden representations in teacherless-trained models to determine if they encode a step-by-step plan (chain-of-thought) before generating the first token, or if they use a fundamentally different computational strategy.

## Limitations

- Task Representativeness: The path-finding task is highly stylized and may not directly translate to real-world planning tasks with different signal-to-noise ratios.
- Architecture Specificity: Experiments are limited to relatively small models (GPT-Mini and GPT2-Large), leaving open whether scaling affects the phenomenon.
- Training Dynamics: The paper doesn't rigorously prove why the optimization process consistently converges to the Clever Hans solution rather than finding alternative minima.

## Confidence

- High Confidence: The experimental demonstration that teacher-forcing can induce Clever Hans cheating in the specific path-finding task, as evidenced by the clear discrepancy between Accag and Acccheat metrics across multiple architectures and training conditions.
- Medium Confidence: The theoretical argument that teacher-forcing creates "indecipherable" tokens after cheating mechanisms are learned, based on the loss of supervisory signal for earlier tokens.
- Low Confidence: The broader claim that this represents a fundamental limitation requiring "alternatives to next-token prediction" for planning tasks. While the experimental evidence is compelling for the specific task, the leap to this general conclusion requires more diverse validation.

## Next Checks

1. **Generalization to Natural Planning Tasks**: Test whether the Clever Hans failure manifests in more naturalistic planning domains (e.g., text-based adventure games, code synthesis tasks requiring forward planning) where the solution space is less structured than the path-finding task.

2. **Architectural Interventions**: Investigate whether architectural modifications (e.g., recurrence, different attention mechanisms, or explicit planning modules) can mitigate the Clever Hans failure while maintaining the benefits of teacher-forcing training.

3. **Training Dynamics Analysis**: Conduct ablation studies varying learning rate schedules, optimizer choices, and initialization strategies to determine whether the Clever Hans solution is truly inevitable or whether different optimization trajectories might find alternative minima that preserve planning capabilities.