---
ver: rpa2
title: 'LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters'
arxiv_id: '2405.17604'
source_url: https://arxiv.org/abs/2405.17604
tags:
- lora-xs
- random
- singular
- lora
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LoRA-XS achieves over 100x reduction in trainable parameters compared
  to LoRA while maintaining or improving performance across GLUE, GSM8K, MATH, and
  commonsense reasoning benchmarks. The method initializes low-rank matrices using
  SVD of pre-trained weights and keeps them frozen, introducing only a small trainable
  matrix R between them.
---

# LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters

## Quick Facts
- arXiv ID: 2405.17604
- Source URL: https://arxiv.org/abs/2405.17604
- Authors: Klaudia Bałazy; Mohammadreza Banaei; Karl Aberer; Jacek Tabor
- Reference count: 40
- Primary result: Achieves over 100x reduction in trainable parameters compared to LoRA while maintaining or improving performance across multiple benchmarks

## Executive Summary
LoRA-XS introduces a parameter-efficient fine-tuning method that achieves extreme compression ratios while maintaining or improving performance on various NLP tasks. The approach builds on Low-Rank Adaptation (LoRA) by initializing low-rank matrices using Singular Value Decomposition (SVD) of pre-trained weights and keeping them frozen, introducing only a small trainable matrix between them. This design enables flexible scaling from single-parameter modules to arbitrarily large values, making it particularly suitable for adapting large language models with minimal computational overhead.

The method demonstrates consistent superiority over both LoRA and VeRA across different model scales on GLUE, GSM8K, MATH, and commonsense reasoning benchmarks. By leveraging the structural properties of pre-trained weights through SVD initialization, LoRA-XS achieves significant parameter reduction while preserving task-specific performance. The approach represents a practical advancement in the parameter-efficient fine-tuning landscape, offering a compelling trade-off between efficiency and effectiveness.

## Method Summary
LoRA-XS modifies the standard LoRA framework by decomposing weight matrices using SVD and freezing the resulting low-rank components. The method introduces a small trainable matrix R between the frozen singular vectors, enabling efficient adaptation while preserving the underlying structure learned during pre-training. This design allows for extreme parameter compression ratios while maintaining or improving performance on downstream tasks. The approach is flexible and can scale from minimal parameter counts to larger configurations depending on the specific requirements of the task and model size.

## Key Results
- Achieves over 100x reduction in trainable parameters compared to LoRA
- Maintains or improves performance across GLUE, GSM8K, MATH, and commonsense reasoning benchmarks
- Demonstrates consistent superiority over LoRA and VeRA across different model scales
- Ablation studies confirm the importance of singular vectors in transformer weights for efficient adaptation

## Why This Works (Mechanism)
LoRA-XS leverages the inherent low-rank structure present in pre-trained transformer weights by decomposing them through SVD. The frozen singular vectors capture the essential structure of the pre-trained model, while the small trainable matrix R introduces task-specific adaptations. This approach exploits the observation that many weight matrices in pre-trained models already exhibit significant low-rank properties, making them amenable to efficient low-rank approximation without substantial performance degradation.

## Foundational Learning
- **Singular Value Decomposition (SVD)**: A matrix factorization technique that decomposes a matrix into three components (U, Σ, V^T), revealing the underlying low-rank structure. Why needed: Enables extraction of essential weight patterns from pre-trained models. Quick check: Verify that the top singular values capture most of the variance in weight matrices.

- **Low-Rank Adaptation**: A parameter-efficient fine-tuning method that introduces low-rank matrices to adapt pre-trained models. Why needed: Provides the baseline framework for efficient model adaptation. Quick check: Confirm that the rank selection balances between efficiency and performance.

- **Transformer Weight Structure**: The observation that transformer weight matrices often exhibit low-rank properties. Why needed: Justifies the use of SVD-based initialization. Quick check: Analyze the singular value spectrum of different weight matrices.

## Architecture Onboarding

**Component Map**: Pre-trained weights -> SVD decomposition -> Frozen singular vectors -> Trainable matrix R -> Adapted weights

**Critical Path**: The most critical components are the SVD initialization and the trainable matrix R. The frozen singular vectors provide the structural foundation, while R enables task-specific adaptation with minimal parameters.

**Design Tradeoffs**: The method trades off between parameter efficiency and adaptation capacity. Higher ranks in the frozen components provide better preservation of pre-trained knowledge but reduce the parameter savings. The size of R determines the adaptation capacity while maintaining efficiency.

**Failure Signatures**: Potential failures may occur when the pre-trained weight structure is not amenable to low-rank approximation, or when the task requires significant deviation from the pre-trained knowledge. Performance degradation may also occur if the rank selection is too aggressive.

**First Experiments**: 1) Test on a simple GLUE task with different rank configurations. 2) Compare parameter counts and performance against standard LoRA. 3) Analyze the singular value spectrum of pre-trained weights to validate the low-rank assumption.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on a relatively limited set of benchmark datasets (GLUE, GSM8K, MATH, and commonsense reasoning tasks)
- Claims of "consistent superiority" based on specific experimental configurations may not generalize to all use cases or model architectures
- Computational efficiency gains reported primarily through parameter count reduction, with limited analysis of actual runtime performance and memory usage during training

## Confidence
- Performance improvement over baselines: High
- Parameter efficiency claims: Medium
- Generalizability of the SVD initialization approach: Medium

## Next Checks
1. Test LoRA-XS on additional diverse benchmarks including code generation, multilingual tasks, and specialized domains to verify robustness across different task types.

2. Conduct ablation studies varying the SVD rank threshold and initialization strategies to determine optimal settings for different model scales and task complexities.

3. Measure actual training time, memory consumption, and inference latency in practical deployment scenarios to quantify the real-world efficiency benefits beyond parameter count reduction.