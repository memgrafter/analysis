---
ver: rpa2
title: 'Paint by Inpaint: Learning to Add Image Objects by Removing Them First'
arxiv_id: '2404.18212'
source_url: https://arxiv.org/abs/2404.18212
tags:
- image
- object
- editing
- dataset
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Paint by Inpaint (PIPE) framework, which
  tackles the challenge of mask-free object addition in images using text instructions.
  The key insight is that removing objects is simpler than adding them, so PIPE generates
  training data by using inpainting models to remove objects from images, then training
  a diffusion model to reverse this process and add objects back in.
---

# Paint by Inpaint: Learning to Add Image Objects by Removing Them First

## Quick Facts
- arXiv ID: 2404.18212
- Source URL: https://arxiv.org/abs/2404.18212
- Reference count: 40
- Outperforms state-of-the-art methods on MagicBrush, OPA, and PIPE test set benchmarks for mask-free object addition

## Executive Summary
This paper introduces the Paint by Inpaint (PIPE) framework for mask-free object addition to images using text instructions. The key innovation is recognizing that removing objects is simpler than adding them, so PIPE generates training data by using inpainting models to remove objects from images, then training a diffusion model to reverse this process and add objects back in. This creates natural target images rather than synthetic ones, ensuring consistency. The authors create a large-scale dataset of ~1 million image pairs with detailed instructions generated via VLM-LLM pipelines. Their model outperforms state-of-the-art methods on multiple benchmarks and shows strong generalization when combined with other editing datasets.

## Method Summary
PIPE tackles mask-free object addition by reversing the problem: instead of training on synthetic object additions, it generates natural training pairs by first removing objects using inpainting, then learning to restore them. The dataset construction uses COCO, Open Images, and LVIS segmentation data, filtering masks by size/location/semantic similarity. Objects are removed via inpainting models, creating source-target pairs. Instructions are generated using a VLM (CogVLM 2) for object descriptions, then an LLM (Mistral-7B) for natural language instructions. The model fine-tunes Stable Diffusion 1.5 with classifier-free guidance, conditioning on both image and text. Evaluation uses quantitative metrics (CLIP-I, DINO, L1/L2) and human evaluation on instruction faithfulness and image quality.

## Key Results
- Outperforms state-of-the-art methods on MagicBrush (0.900 CLIP-I, 0.852 DINO), OPA, and PIPE test set benchmarks
- Achieves 73.6% human preference over IP2P for instruction faithfulness
- PIPE dataset improves general editing performance when combined with other datasets
- Successfully handles complex object additions including shadows, reflections, and occlusion reasoning

## Why This Works (Mechanism)
The PIPE framework works by inverting the object addition problem: since removing objects is easier than adding them, PIPE uses inpainting models to create natural training pairs where objects have been realistically removed. This ensures the target images are natural rather than synthetic, maintaining consistency across edited and unedited regions. The VLM-LLM pipeline generates detailed, natural language instructions that capture both the object characteristics and the context needed for realistic addition. By training a diffusion model to reverse the inpainting process, PIPE learns to add objects in a way that naturally integrates with existing image content, including handling shadows, reflections, and occlusion.

## Foundational Learning
- **Diffusion models for image generation**: Why needed - Core architecture for learning to add objects; Quick check - Can generate realistic images from noise with text conditioning
- **Inpainting for object removal**: Why needed - Creates natural training pairs by removing objects; Quick check - Can remove objects while maintaining surrounding context
- **VLM-LLM instruction generation**: Why needed - Converts object descriptions to natural language instructions; Quick check - Generates detailed, contextually appropriate instructions
- **CLIP-based filtering**: Why needed - Ensures semantic consistency in training data; Quick check - Filters out low-quality inpainting results
- **Classifier-free guidance**: Why needed - Improves text-image alignment during generation; Quick check - Produces outputs that better match text instructions
- **Segmentation datasets**: Why needed - Provides ground truth object masks for training data generation; Quick check - Contains accurate masks for diverse object categories

## Architecture Onboarding

**Component map**: Segmentation datasets -> Inpainting model -> CLIP filtering -> VLM-LLM pipeline -> Instruction generation -> Diffusion model fine-tuning -> Evaluation

**Critical path**: The core innovation flows through inpainting-based data generation, instruction synthesis, and diffusion model training. The critical path is: segmentation masks → inpainting (object removal) → CLIP-consensus filtering → VLM description → LLM instruction generation → diffusion model fine-tuning → evaluation.

**Design tradeoffs**: The paper chooses synthetic data generation via inpainting over real object addition pairs, trading potential domain shift for scale and control. Uses VLM-LLM pipeline for instruction generation rather than manual annotation, trading cost for potential bias. Employs CLIP-based filtering which may reduce diversity but ensures quality.

**Failure signatures**: Model may struggle with consistency in distant regions (shadows, reflections), leave traces of original objects, or fail to maintain context outside edited areas. Instruction generation may produce biased or incomplete descriptions. Inpainting may create artifacts that propagate through training.

**First experiments**: 1) Generate training pairs using inpainting and evaluate CLIP-consensus filtering effectiveness, 2) Test VLM-LLM pipeline on sample object descriptions to verify instruction quality, 3) Fine-tune diffusion model on small subset and evaluate basic object addition capability.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic training data generated through inpainting may introduce domain shift issues
- Evaluation focuses primarily on object addition tasks, leaving uncertainty about generalization to other editing scenarios
- Dataset construction involves multiple filtering steps with unspecified parameters that could impact reproducibility

## Confidence
- Dataset construction methodology: Medium - Well-justified but synthetic nature introduces uncertainty
- Instruction generation pipeline: Medium - Novel approach but potential for bias not fully explored
- Benchmark performance claims: Medium - Strong results but limited to specific task types
- Generalization claims: Low - Insufficient evaluation on diverse editing tasks beyond object addition

## Next Checks
1. Evaluate the model on diverse editing tasks beyond object addition (e.g., background modification, style transfer) to test generalization
2. Conduct ablation studies on the VLM-LLM instruction generation pipeline to quantify its contribution to performance
3. Compare results with alternative inpainting models for data generation to assess sensitivity to this choice