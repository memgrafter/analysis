---
ver: rpa2
title: 'ReAGent: A Model-agnostic Feature Attribution Method for Generative Language
  Models'
arxiv_id: '2402.00794'
source_url: https://arxiv.org/abs/2402.00794
tags:
- input
- attention
- token
- reagent
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReAGent, a model-agnostic feature attribution
  method for generative language models. The key idea is to recursively update token
  importance scores by comparing the predictive likelihood of the target token when
  using the original input versus a modified input where a subset of tokens are replaced
  with RoBERTa predictions.
---

# ReAGent: A Model-agnostic Feature Attribution Method for Generative Language Models

## Quick Facts
- arXiv ID: 2402.00794
- Source URL: https://arxiv.org/abs/2402.00794
- Reference count: 16
- ReAGent provides more faithful token importance distributions than seven popular feature attribution methods for generative language models.

## Executive Summary
This paper introduces ReAGent, a novel model-agnostic feature attribution method for generative language models that recursively updates token importance scores by comparing predictive likelihoods when replacing tokens with RoBERTa predictions. The method addresses a critical gap in attribution methods for text generation tasks, where existing approaches designed for classification don't adequately capture the sequential nature of generation. Extensive experiments on six decoder-only language models across three datasets demonstrate that ReAGent consistently outperforms established methods in producing faithful importance distributions.

## Method Summary
ReAGent computes token importance distributions for generative language models without requiring access to internal model weights or additional training. The method recursively updates importance scores by comparing the predictive likelihood of the target token when using the original input versus when a subset of tokens are replaced with predictions from RoBERTa. The process iterates with random token selection until a stopping condition is met (when top-k predicted candidates still contain the target token after replacing 70% of unimportant tokens). The final importance scores reflect how much each token's replacement affects the model's confidence in predicting subsequent tokens.

## Key Results
- ReAGent consistently outperforms seven baseline methods (Input X Gradient, Integrated Gradients, Gradient Shap, Attention, Last Attention, Attention Rollout, and LIME) across all six evaluated language models
- The method achieves superior faithfulness metrics (Soft-NS and Soft-NC) on three text generation datasets (LongRA, TellMeWhy, WikiBio)
- ReAGent maintains effectiveness across different model scales, from GPT2-354M to OPT-6.7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Recursive token replacement with RoBERTa predictions creates measurable likelihood changes that correlate with true importance.
- Mechanism: Iteratively replaces tokens with RoBERTa predictions and measures change in predictive likelihood for target token.
- Core assumption: Tokens significantly affecting model's confidence in predicting next token are truly important.
- Evidence anchors: Abstract and section statements about likelihood changes; weak corpus evidence.
- Break condition: RoBERTa predictions may not align with target model's reasoning process.

### Mechanism 2
- Claim: Stopping condition ensures unimportant tokens can be removed without affecting top-k predictions.
- Mechanism: Iterations stop when top-k predictions still contain target token after replacing 70% of unimportant tokens.
- Core assumption: Model's top predictions remain stable when removing truly unimportant context.
- Evidence anchors: Section statements about stopping condition; weak corpus evidence.
- Break condition: Model may rely on subtle contextual cues that RoBERTa fails to capture.

### Mechanism 3
- Claim: Random sampling of tokens to replace provides diverse coverage and prevents bias.
- Mechanism: Each iteration randomly selects 30% of tokens for replacement, ensuring different combinations are tested.
- Core assumption: Random selection prevents systematic bias and ensures important tokens are tested in various contexts.
- Evidence anchors: Section statements about random selection and three runs with different seeds; weak corpus evidence.
- Break condition: Random sampling may fail to cover important token combinations or miss critical patterns.

## Foundational Learning

- Concept: Feature attribution methods for text generation
  - Why needed here: Understanding different FAs helps evaluate ReAGent's novel approach and advantages.
  - Quick check question: What is the key difference between feature attribution for text classification vs. text generation tasks?

- Concept: Occlusion-based methods
  - Why needed here: ReAGent builds on occlusion principles but innovates by using RoBERTa predictions instead of simple masking.
  - Quick check question: How does replacing tokens with predictions differ from simple token removal in occlusion methods?

- Concept: Hellinger distance for probability distributions
  - Why needed here: Paper uses Hellinger distance to measure changes in prediction distributions, crucial for faithfulness metrics.
  - Quick check question: Why might Hellinger distance be preferred over KL divergence for comparing discrete probability distributions?

## Architecture Onboarding

- Component map: Input -> RoBERTa predictor -> Likelihood calculator -> Importance updater -> Stopping condition checker -> Output
- Critical path:
  1. Initialize random importance scores
  2. Select random tokens for replacement
  3. Generate replacements using RoBERTa
  4. Compute likelihood differences
  5. Update importance scores
  6. Check stopping condition
  7. Return final importance distribution
- Design tradeoffs:
  - Using RoBERTa vs. random tokens: RoBERTa provides semantically coherent replacements but adds dependency on another model
  - Iteration count vs. runtime: More iterations improve accuracy but increase computational cost
  - Random sampling vs. systematic coverage: Random sampling is simpler but may miss important patterns
- Failure signatures:
  - Uniformly distributed importance scores (indicates poor differentiation)
  - Very slow convergence (suggests stopping condition too strict)
  - High variance across runs (indicates insufficient sampling)
- First 3 experiments:
  1. Run ReAGent on simple GPT-2 with known important token and verify high importance
  2. Compare ReAGent's importance distribution with integrated gradients on same input
  3. Test different replacement ratios (10%, 30%, 50%) and observe impact on faithfulness metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ReAGent's performance vary across different generative tasks (e.g., translation, summarization) compared to classification tasks?
- Basis in paper: [explicit] Focus on text generation tasks with suggestion for future work on different generative models and tasks.
- Why unresolved: Current study only evaluates on three text generation datasets without exploring other generative tasks.
- What evidence would resolve it: Experiments comparing ReAGent's performance on various generative tasks with classification tasks.

### Open Question 2
- Question: How does the choice of RoBERTa as the replacing function impact performance compared to other models or methods?
- Basis in paper: [explicit] Uses RoBERTa for token replacement without comparing to other potential models or methods.
- Why unresolved: Study uses RoBERTa without comparing performance to other models or methods like same model or random selection.
- What evidence would resolve it: Experiments comparing ReAGent's performance using different models or methods for token replacement.

### Open Question 3
- Question: How does ReAGent's computational efficiency compare to other feature attribution methods for large language models?
- Basis in paper: [explicit] Mentions ReAGent doesn't require internal weights or training, potentially reducing computational costs.
- Why unresolved: Paper highlights computational efficiency but doesn't provide direct comparison of computational costs with other methods.
- What evidence would resolve it: Experiments measuring and comparing computational costs of ReAGent and other methods for large language models.

## Limitations

- The fundamental assumption that RoBERTa-predicted replacements accurately capture token importance is not empirically validated against ground truth.
- The evaluation uses relatively small datasets (200-238 examples each) which may not generalize to larger-scale applications.
- Key parameters like the 70% replacement threshold and top-3 candidate check appear arbitrary without sensitivity analysis.

## Confidence

- **High confidence**: ReAGent can compute token importance distributions; method is model-agnostic; produces different distributions than baselines.
- **Medium confidence**: ReAGent provides "more faithful" importance distributions than baselines; recursive mechanism converges; works across different decoder-only models.
- **Low confidence**: Importance scores reflect "true" token importance; stopping condition reliably identifies unimportant tokens; RoBERTa replacements provide better signal than simple masking.

## Next Checks

1. **Ground truth validation study**: Create controlled synthetic examples where true important tokens are known through controlled interventions or causal analysis, and compare ReAGent's attributions against these ground truths to validate whether higher likelihood drops correspond to true importance.

2. **Parameter sensitivity analysis**: Systematically vary replacement ratio (10%, 30%, 50%, 70%), number of random seeds (1, 3, 5, 10), and stopping condition thresholds to measure effects on faithfulness metrics and runtime, identifying optimal configurations and robustness.

3. **Alternative replacement model comparison**: Replace RoBERTa with simpler baselines like random token replacement, zero masking, or mean token embeddings, and compare faithfulness metrics across these variants to determine whether RoBERTa's semantic coherence actually improves attribution quality versus simpler approaches.