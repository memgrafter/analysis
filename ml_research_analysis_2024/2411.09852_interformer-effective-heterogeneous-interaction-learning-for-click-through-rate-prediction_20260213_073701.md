---
ver: rpa2
title: 'InterFormer: Effective Heterogeneous Interaction Learning for Click-Through
  Rate Prediction'
arxiv_id: '2411.09852'
source_url: https://arxiv.org/abs/2411.09852
tags:
- information
- sequence
- interaction
- learning
- non-sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of insufficient inter-mode interaction
  and aggressive information aggregation in click-through rate (CTR) prediction models.
  The authors propose InterFormer, a novel heterogeneous interaction learning module
  that enables bidirectional information flow between non-sequence and sequence data
  modes in an interleaving style.
---

# InterFormer: Effective Heterogeneous Interaction Learning for Click-Through Rate Prediction

## Quick Facts
- **arXiv ID:** 2411.09852
- **Source URL:** https://arxiv.org/abs/2411.09852
- **Reference count:** 40
- **Key outcome:** InterFormer achieves state-of-the-art CTR prediction with up to 0.14% AUC improvement on public datasets and 0.15% Normalized Entropy gain on large-scale industrial data, demonstrating 24% QPS gain at Meta.

## Executive Summary
This paper addresses the challenge of insufficient inter-mode interaction in click-through rate prediction models by proposing InterFormer, a novel heterogeneous interaction learning architecture. The model enables bidirectional information flow between non-sequence (user profile) and sequence (behavior) data modes through interleaving interactions, while using selective aggregation to avoid aggressive information loss. InterFormer achieves state-of-the-art performance on three public datasets and demonstrates strong scalability with promising results in industrial deployment at Meta.

## Method Summary
InterFormer introduces a heterogeneous interaction learning framework that processes both non-sequence and sequence data modes in an interleaving style. The architecture consists of three main components: Interaction Arch for bidirectional information flow, Sequence Arch for context-aware sequence modeling, and Cross Arch for selective information summarization. The model uses personalized feature-level transformation (PFFN) and multi-head attention (MHA) to capture multi-scale interactions at different orders through layer stacking. During training, the model selectively summarizes high-dimensional features before exchange, retaining complete information in each mode while enabling effective low-dimensional information exchange.

## Key Results
- Achieves up to 0.14% AUC improvement over state-of-the-art methods on public datasets (AmazonElectronics, TaobaoAd, KuaiVideo)
- Demonstrates 0.15% Normalized Entropy gain on large-scale Meta industrial dataset
- Shows 24% QPS gain compared to prior state-of-the-art models in production deployment
- Consistent performance improvements across all three public datasets and industrial settings

## Why This Works (Mechanism)

### Mechanism 1
Bidirectional information flow between non-sequence and sequence modes enables mutually beneficial learning that improves CTR prediction accuracy. InterFormer creates interleaving interactions where non-sequence context guides sequence modeling via PFFN and MHA, while sequence summaries guide non-sequence learning via interaction modules. This bidirectional flow allows each mode to inform the other at multiple levels. The core assumption is that information from non-sequence features and sequence behaviors is complementary rather than redundant, capturing richer user interest patterns than either mode alone.

### Mechanism 2
Selective information aggregation prevents aggressive summarization that causes information loss in early stages. InterFormer uses separate Cross Arch modules with gating mechanisms to selectively summarize high-dimensional features before exchange. This retains complete information in each mode while enabling effective low-dimensional information exchange. The core assumption is that early aggressive aggregation loses critical interaction patterns that could be captured through selective summarization.

### Mechanism 3
Interleaving learning style with layer stacking captures multi-scale interactions at different orders. By stacking multiple InterFormer layers where each layer performs interaction learning at different orders, the model captures both local and global patterns. The CLS token prepended to sequences enables non-sequence context to attend to sequential information. The core assumption is that complex user interest patterns exist at multiple scales and orders, requiring multi-layer processing to capture effectively.

## Foundational Learning

- **Concept: Attention Mechanisms**
  - Why needed here: Attention mechanisms enable the model to focus on relevant parts of long sequences without regard to distance, capturing important user behaviors while ignoring noise.
  - Quick check question: How does Multi-Head Attention differ from single-head attention in terms of information capture?

- **Concept: Feature Interaction Learning**
  - Why needed here: CTR prediction requires capturing complex relationships between heterogeneous features (user profiles, item attributes, behavior sequences) to accurately model user intent.
  - Quick check question: What is the difference between explicit feature interactions (like in DCNv2) and implicit interactions (like in deep networks)?

- **Concept: Sequence Modeling**
  - Why needed here: User behavior sequences contain temporal patterns and sequential dependencies that static features cannot capture, essential for understanding dynamic user interests.
  - Quick check question: Why is modeling sequential dependencies important for CTR prediction compared to static feature modeling?

## Architecture Onboarding

- **Component map:** Preprocessing (LCE and MaskNet) -> Cross Arch summarization (gating modules) -> Interaction Arch bidirectional flow -> Sequence Arch context-aware modeling (PFFN and MHA) -> Final classification (MLP head)

- **Critical path:** 1) Feature preprocessing (LCE and MaskNet) 2) Cross Arch summarization (gating modules) 3) Interaction Arch bidirectional flow 4) Sequence Arch context-aware modeling 5) Final classification

- **Design tradeoffs:** Model complexity vs. inference speed (24% QPS gain despite complexity suggests efficient implementation); Layer depth vs. overfitting (consistent NE gains up to 4 layers); Selective vs. aggressive aggregation (selective aggregation outperforms aggressive variants)

- **Failure signatures:** Performance degradation when sequence length increases significantly; Unstable training when gating mechanisms become too aggressive; Memory issues with very large feature spaces despite LCE compression

- **First 3 experiments:** 1) Compare InterFormer with bidirectional disabled (n2s or s2n only) to validate the importance of bidirectional flow 2) Test different sequence summarization methods (CLS only vs. CLS+PMA+recent) to understand their contributions 3) Vary the number of InterFormer layers (1-4) to find optimal depth for the target dataset

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several important research directions emerge from the work, particularly around scalability under extreme sparsity, optimal layer depth for different dataset characteristics, and module selection based on interaction complexity.

## Limitations
- Heavy reliance on internal Meta data limits external validation, though public dataset results are promising
- Real-world impact at scale remains partially unverified despite deployment claims
- Performance improvements are incremental (0.08-0.14% AUC) rather than transformative

## Confidence
- **High Confidence:** Architectural design and technical implementation are well-specified with clear mechanisms for bidirectional information flow and selective aggregation
- **Medium Confidence:** Performance improvements on public datasets are statistically significant but represent incremental advances
- **Medium Confidence:** Industrial deployment results are promising but lack detailed ablation studies

## Next Checks
1. **Ablation Study on Bidirectional Flow:** Train variants with only n2s or s2n information flow disabled to quantify the contribution of bidirectional interactions versus unidirectional approaches

2. **Layer-wise Contribution Analysis:** Measure performance at each InterFormer layer to determine if all layers contribute equally or if early layers capture most of the signal, validating the interleaving learning hypothesis

3. **Cross-Domain Transferability Test:** Evaluate InterFormer on datasets from different domains (e.g., news recommendation, e-commerce) to verify the generalizability of the heterogeneous interaction learning approach beyond the current datasets