---
ver: rpa2
title: Hacking a surrogate model approach to XAI
arxiv_id: '2406.16626'
source_url: https://arxiv.org/abs/2406.16626
tags:
- decision
- data
- tree
- salary
- attribute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the security and fairness vulnerabilities in
  using decision trees as surrogate models for explainable AI (XAI). The authors demonstrate
  that by manipulating the training data set, a malicious operator can hide discriminatory
  decision rules in deeper levels of a decision tree, making them less noticeable.
---

# Hacking a surrogate model approach to XAI

## Quick Facts
- arXiv ID: 2406.16626
- Source URL: https://arxiv.org/abs/2406.16626
- Authors: Alexander Wilhelm; Katharina A. Zweig
- Reference count: 15
- Primary result: Decision trees can hide discriminatory rules when training data is manipulated

## Executive Summary
This paper investigates security vulnerabilities in using decision trees as surrogate models for explainable AI (XAI). The authors demonstrate that malicious operators can manipulate training data to hide discriminatory decision rules in deeper levels of decision trees, making them less detectable. Through theoretical proofs and examples, they show that up to 50% of a disadvantaged group can be discriminated against without the sensitive attribute appearing in the first level of the tree.

The research highlights critical limitations in using decision trees for XAI purposes, particularly regarding fairness and transparency. The findings suggest that seemingly unbiased decision trees may actually contain hidden discriminatory patterns that go undetected by standard interpretation methods, potentially perpetuating discrimination in algorithmic decision-making systems.

## Method Summary
The authors employ a theoretical approach to analyze how decision tree construction can be manipulated through training data modification. They prove three theorems that establish relationships between attribute ordering in decision trees and the distribution of favorable values in the dataset. The methodology involves mathematical proofs showing how the order of attributes can be influenced based on the percentage of favorable values, demonstrating that discriminatory rules can be pushed to deeper levels of the tree where they are less likely to be examined.

## Key Results
- Decision trees can hide discriminatory rules in deeper levels when training data is manipulated
- Up to 50% of a disadvantaged group can be discriminated against without sensitive attributes appearing in the first tree level
- Attribute ordering in decision trees can be influenced based on the percentage of favorable values in the dataset

## Why This Works (Mechanism)
The vulnerability works because decision tree algorithms select attributes for splitting based on statistical measures of information gain or Gini impurity. When training data is manipulated to alter the distribution of values for certain attributes, the algorithm is deceived into choosing other attributes for earlier splits. This pushes potentially discriminatory attributes to later splits in the tree, making them less visible during standard interpretation. The mechanism exploits the fundamental property that decision trees prioritize attributes that appear most informative based on their immediate statistical impact, without considering the broader fairness implications of attribute ordering.

## Foundational Learning

**Decision Tree Construction**
- Why needed: Understanding how splits are chosen is crucial to grasping the vulnerability
- Quick check: Verify understanding of information gain/Gini impurity calculation

**Surrogate Models in XAI**
- Why needed: Context for why decision trees are used as interpretable approximations
- Quick check: Confirm understanding of the difference between original and surrogate models

**Data Poisoning Attacks**
- Why needed: The manipulation technique is a form of data poisoning
- Quick check: Understand how training data modifications affect model behavior

**Attribute Ordering**
- Why needed: Central to understanding how discrimination can be hidden
- Quick check: Verify understanding of how attribute position affects interpretability

## Architecture Onboarding

**Component Map**
Training Data -> Decision Tree Algorithm -> Surrogate Model -> XAI Interpretation

**Critical Path**
1. Manipulate training data distribution
2. Run decision tree construction algorithm
3. Generate surrogate model
4. Interpret model (missing discrimination)

**Design Tradeoffs**
- Transparency vs. vulnerability to manipulation
- Model simplicity vs. hidden complexity
- Interpretability vs. robustness to data poisoning

**Failure Signatures**
- Unusually high performance on manipulated attributes
- Unexpected attribute ordering in the tree
- Disproportionate impact on protected groups not visible in top-level splits

**First 3 Experiments**
1. Create controlled datasets with known discriminatory patterns and test if they're hidden in deeper tree levels
2. Vary the percentage of favorable values to verify the theoretical bounds on detectable discrimination
3. Test different decision tree algorithms (ID3, C4.5, CART) for susceptibility to the same manipulation

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on decision trees as the sole surrogate model limits generalizability
- Theoretical proofs rely on specific assumptions about data distribution
- Manipulation requires knowledge of system internals, limiting practical attack scenarios
- Does not address potential countermeasures or mitigation strategies

## Confidence

**High**: Mathematical proofs demonstrating attribute ordering manipulation are rigorous
**Medium**: Practical implications of data manipulation for hiding discrimination are demonstrated
**Low**: Generalizability to other surrogate model types and XAI approaches remains uncertain

## Next Checks

1. Test vulnerability of other surrogate model types (linear models, neural networks) to similar manipulation techniques
2. Conduct empirical validation using real-world datasets to assess practical feasibility of attack vectors
3. Investigate potential detection methods or preprocessing techniques that could identify hidden discriminatory patterns in surrogate models