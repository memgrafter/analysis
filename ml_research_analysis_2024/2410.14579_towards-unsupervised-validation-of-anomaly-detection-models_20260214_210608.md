---
ver: rpa2
title: Towards Unsupervised Validation of Anomaly-Detection Models
arxiv_id: '2410.14579'
source_url: https://arxiv.org/abs/2410.14579
tags:
- ensemble
- unsupervised
- rank
- correlation
- observations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of unsupervised validation of
  anomaly-detection models when labeled data is unavailable. It proposes a novel approach
  based on "Accurately-Diverse" ensembles that balance accuracy and diversity through
  complementary homogeneity (strong agreement on general trends for distinctive observations)
  and heterogeneity (strong disagreement on exact rankings for less distinctive observations).
---

# Towards Unsupervised Validation of Anomaly-Detection Models

## Quick Facts
- arXiv ID: 2410.14579
- Source URL: https://arxiv.org/abs/2410.14579
- Authors: Lihi Idan
- Reference count: 35
- Key outcome: Accurately-Diverse ensembles outperform average single models by 29-155% in PR AUC and precision@n metrics on ten datasets

## Executive Summary
This paper addresses the challenge of unsupervised validation of anomaly-detection models when labeled data is unavailable. It proposes a novel approach based on "Accurately-Diverse" ensembles that balance accuracy and diversity through complementary homogeneity (strong agreement on general trends for distinctive observations) and heterogeneity (strong disagreement on exact rankings for less distinctive observations). The method uses fuzzy rank correlation for outliers and exact rank correlation for non-extreme inliers to build ensembles that perform better than average single models. Experiments on ten datasets show the ensemble outperforms average models by 29-155% in PR AUC and precision@n metrics. The approach also enables unsupervised model evaluation via the "Unsupervised Ensemble Divergence" score, which correlates highly (0.84-0.96 Spearman correlation) with supervised PR AUC metrics, significantly outperforming prior unsupervised evaluation methods.

## Method Summary
The paper proposes building Accurately-Diverse ensembles by balancing complementarity between homogeneity and heterogeneity. It uses fuzzy rank correlation (τcM) for strong outliers to ensure ensemble members agree on general trends, and exact rank correlation (τrM) for non-extreme inliers to ensure they disagree on specific rankings. The ensemble aggregates predictions using arithmetic mean. The Unsupervised Ensemble Divergence (UED) score measures distance between a candidate model's predictions and the ensemble's aggregated predictions, weighted by ensemble confidence and relative importance to the task. This enables unsupervised model evaluation that correlates highly with supervised metrics.

## Key Results
- Accurately-Diverse ensembles outperform average single models by 29-155% in PR AUC and precision@n metrics on ten datasets
- Unsupervised Ensemble Divergence (UED) score correlates highly (0.84-0.96 Spearman correlation) with supervised PR AUC metrics
- The approach significantly outperforms prior unsupervised evaluation methods in both PR AUC and Precision@n metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accurately-Diverse ensembles eliminate the need for unsupervised model selection by consistently outperforming average single models.
- Mechanism: The ensemble achieves high performance through complementary homogeneity and heterogeneity—strong agreement on high-level features of distinctive observations (outliers) and strong disagreement on low-level features of less distinctive observations (non-extreme inliers).
- Core assumption: The outlier class has well-defined characteristics making its identification an "absolute truth," while the inlier class lacks such characteristics, making ranking subjective.
- Evidence anchors:
  - [abstract]: "Experiments on ten datasets show the ensemble outperforms average models by 29-155% in PR AUC and precision@n metrics."
  - [section]: "The requirement for a strong intra-ensemble agreement on the general trend of each ensemble member's predictions is approximated by the ensemble obtaining a high degree of fuzzy rank correlation computed using a multi-model weighting scheme..."
- Break condition: If the dataset contains synthetic anomaly-detection datasets where inliers have well-defined characteristics (like mnist or pendigits), the assumption about inlier subjectivity breaks down and performance degrades.

### Mechanism 2
- Claim: The Unsupervised Ensemble Divergence (UED) score provides unsupervised model evaluation that correlates highly with supervised metrics.
- Mechanism: UED measures distance between a candidate model's predictions and an Accurately-Diverse ensemble's aggregated predictions, weighted by ensemble confidence and relative importance to the task.
- Core assumption: An Accurately-Diverse ensemble represents a reliable "ground truth approximation" for evaluation purposes.
- Evidence anchors:
  - [abstract]: "The approach also enables unsupervised model evaluation via the 'Unsupervised Ensemble Divergence' score, which correlates highly (0.84-0.96 Spearman correlation) with supervised PR AUC metrics"
  - [section]: "The core idea of the 'Unsupervised Ensemble Divergence' score, UED, is to use a distance metric, tailored specifically to AD tasks, measured between the prediction of the model to be evaluated, m*, and the aggregated prediction of an Accurately-Diverse ensemble."
- Break condition: If the ensemble itself is poorly constructed (low fuzzy rank correlation on outliers or high exact rank correlation on non-extreme inliers), the evaluation metric loses validity.

### Mechanism 3
- Claim: The fuzzy rank correlation metric captures agreement better than traditional metrics for anomaly detection tasks.
- Mechanism: Traditional rank correlation treats all observation pairs equally, but fuzzy rank correlation weights pairs based on their distinctiveness and uses cluster-based discordance rather than exact rank discordance.
- Core assumption: In anomaly detection, the relative position within clusters matters more than exact rank positions for evaluation purposes.
- Evidence anchors:
  - [section]: "We claim that existing rank correlation methods cannot be used for accurately measuring the correlation between multiple lists that represent predicted, ranked anomaly scores... A discordant pair is defined to be one such as sgn(f(r1[i]) − f(r1[j])) = −sgn(f(r2[i]) − f(r2[j])). In Kendall's τ, f is the identity function..."
  - [section]: "Instead of determining the relation between a pair of observations according to the relation between their rank indices, we can determine the relation between them according to the rank clusters to which their rank indices are mapped."
- Break condition: If the clustering scheme (γ1, γ2 values) doesn't match the application's treatment probability requirements, the fuzzy correlation loses relevance.

## Foundational Learning

- Concept: Ensemble diversity-accuracy tradeoff
  - Why needed here: The paper explicitly states that balancing accuracy and diversity is an "inherent challenge in ensemble-based approaches" especially in unsupervised settings where labeled validation sets are unavailable.
  - Quick check question: Why can't we simply use all available models in the ensemble without considering diversity?

- Concept: Rank correlation metrics and their limitations
  - Why needed here: The paper develops new multi-way fuzzy rank correlation metrics because existing metrics like Spearman's ρ and Kendall's τ assume all observations contribute equally and don't account for the unique structure of anomaly score lists.
  - Quick check question: What's the key difference between how Kendall's τ and the proposed fuzzy rank correlation handle discordant pairs?

- Concept: Weighting schemes for different observation types
  - Why needed here: The paper uses different weighting schemes (exponential for outliers, bell-shaped for non-extreme inliers) to bias correlation metrics toward different types of observations based on their distinctiveness.
  - Quick check question: Why does the paper use an exponential weighting scheme for outliers versus a bell-shaped function for non-extreme inliers?

## Architecture Onboarding

- Component map:
  - Input: Pool of N unsupervised anomaly detection models and unlabeled dataset
  - Ensemble Builder: Filters and selects M models based on fuzzy rank correlation on strong outliers and exact rank correlation on non-extreme inliers
  - Correlation Metrics: Implements τrM (exact rank) and τcM (fuzzy rank) with custom weighting schemes
  - Aggregation Layer: Combines predictions using arithmetic mean
  - UED Evaluator: Measures distance between candidate models and ensemble using cluster-based distance metrics
  - Output: Ensemble predictions for standalone use OR UED scores for evaluating other models

- Critical path: Model Selection → Ensemble Construction → Correlation Evaluation → Prediction/Aggregation → UED Generation
- Design tradeoffs:
  - M vs N: Larger ensembles may capture more diversity but increase computational cost and risk diluting accuracy
  - Weighting parameters (δη, b, γ1, γ2): Must be tuned to dataset characteristics; wrong values lead to poor correlation capture
  - Aggregation method: Arithmetic mean worked best in experiments, but other methods (median, weighted average) might be better for certain data distributions
- Failure signatures:
  - Low fuzzy rank correlation on strong outliers indicates ensemble members aren't agreeing on distinctive observations
  - High exact rank correlation on non-extreme inliers suggests ensemble members aren't disagreeing enough on less distinctive observations
  - UED scores not correlating with PR AUC indicates ensemble isn't representative of ground truth
- First 3 experiments:
  1. Test correlation between τcM and τrM on a simple dataset with known ground truth to verify complementary behavior
  2. Compare ensemble performance against average single model using AS and RSPS metrics on multiple datasets
  3. Validate UED correlation with PR AUC across different model pools and datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Accurately-Diverse ensembles scale when increasing the number of ensemble members beyond M=5?
- Basis in paper: [inferred] The paper only experiments with M=5 ensemble members and shows significant improvement over average models, but doesn't explore scalability with larger ensemble sizes.
- Why unresolved: The paper states "we first build an Accurately-Diverse ensemble of size M, where, in our experiments, M = 5" without exploring how performance changes with larger ensembles.
- What evidence would resolve it: Experiments comparing ensemble performance with varying M (e.g., M=5, 10, 20, 50) on the same datasets to show whether improvement plateaus or continues.

### Open Question 2
- Question: How sensitive is the Unsupervised Ensemble Divergence (UED) score to the choice of hyperparameters (γ1, γ2, δ, b, μ, σ, λ)?
- Basis in paper: [explicit] The paper mentions specific ranges for hyperparameters but doesn't provide systematic sensitivity analysis: "0.25 <= γ1 <= 0.5, 3 <= γ2 <= 5" and "δ ∈ [1 + ϵ, 2] and b ∈ [2, 10]" etc.
- Why unresolved: While the paper provides hyperparameter ranges that "work best," it doesn't analyze how variations within these ranges affect UED score reliability or correlation with PR AUC.
- What evidence would resolve it: Systematic experiments varying each hyperparameter across its range while measuring UED correlation with ground truth metrics on multiple datasets.

### Open Question 3
- Question: Does the Accurately-Diverse ensemble approach work equally well for all types of anomaly detection tasks (e.g., contextual anomalies, collective anomalies, point anomalies)?
- Basis in paper: [inferred] The paper focuses on standard anomaly detection tasks but doesn't explicitly test or discuss performance on different anomaly types mentioned in the broader AD literature.
- Why unresolved: The experiments use datasets that likely contain primarily point anomalies, and the paper doesn't address whether the complementary homogeneity-heterogeneity criteria applies equally to other anomaly types.
- What evidence would resolve it: Experiments testing the ensemble approach on datasets specifically designed for contextual and collective anomalies, comparing performance to traditional methods for those specific anomaly types.

## Limitations
- The assumption that outlier characteristics are "absolute truth" while inlier rankings are subjective may not hold for synthetic datasets with well-defined inlier patterns
- Hyperparameter sensitivity (γ1, γ2 ranges) requires careful tuning, and the computational cost of building Accurately-Diverse ensembles scales poorly with large model pools
- The approach hasn't been validated across different types of anomaly detection tasks (contextual, collective, point anomalies)

## Confidence
- High confidence: Ensemble performance improvements (29-155% gains in PR AUC/precision@n)
- Medium confidence: UED metric's validity with strong correlation (0.84-0.96 Spearman) to supervised metrics
- Low confidence: Ensemble's reliability as "ground truth approximation" without independent verification

## Next Checks
1. Test the ensemble approach on synthetic anomaly detection datasets (mnist, pendigits) where inliers have defined characteristics to verify the homogeneity-heterogeneity framework's robustness.
2. Conduct ablation studies varying γ1, γ2 values across their specified ranges to quantify impact on UED score correlation with PR AUC.
3. Compare UED evaluation against a baseline ensemble of randomly selected models to confirm it provides meaningful discrimination beyond simple ensemble averaging.