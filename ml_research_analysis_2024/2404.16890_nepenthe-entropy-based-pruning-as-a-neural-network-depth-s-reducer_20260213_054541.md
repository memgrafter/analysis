---
ver: rpa2
title: 'NEPENTHE: Entropy-Based Pruning as a Neural Network Depth''s Reducer'
arxiv_id: '2404.16890'
source_url: https://arxiv.org/abs/2404.16890
tags:
- iter
- nepenthe
- layer
- pruning
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NEPENTHE introduces entropy-based pruning to reduce neural network
  depth by targeting low-entropy layers for removal. The method uses a novel entropy
  metric at the neuron level, which measures the activation distribution's predictability
  in rectifier-activated layers.
---

# NEPENTHE: Entropy-Based Pruning as a Neural Network Depth's Reducer

## Quick Facts
- arXiv ID: 2404.16890
- Source URL: https://arxiv.org/abs/2404.16890
- Reference count: 40
- Primary result: Removes 3/17 layers from ResNet-18 on CIFAR-10 with <1% accuracy loss

## Executive Summary
NEPENTHE introduces a novel entropy-based pruning method that reduces neural network depth by targeting and removing low-entropy layers. The approach leverages the observation that unstructured pruning naturally pushes rectifier-activated neurons toward predictable ON/OFF states, reducing their entropy. By combining unstructured pruning with an entropy-weighted parameter budget, NEPENTHE selectively removes parameters from layers with low entropy, eventually enabling complete layer removal when entropy reaches zero. Experiments demonstrate effective depth reduction on ResNet-18, MobileNet-V2, and Swin-T architectures with minimal accuracy loss.

## Method Summary
NEPENTHE uses an iterative entropy-based unstructured pruning approach. First, it computes layer entropy by measuring the distribution of neuron states (ON, OFF, zero) in rectifier-activated layers. A pruning score combining layer entropy and average weight magnitude determines the pruning budget allocation across layers. Low-entropy layers receive more aggressive pruning. After each pruning iteration, the model is retrained and validated. The process continues until validation accuracy drops below a relative threshold, at which point the final pruned model is obtained. This approach enables selective depth reduction while preserving performance.

## Key Results
- Removes 3/17 layers from ResNet-18 on CIFAR-10 with <1% accuracy loss
- Achieves similar or better accuracy than iterative magnitude pruning while reducing depth
- Successfully linearizes layers in over-parameterized regimes across multiple architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unstructured pruning naturally reduces the entropy of rectifier-activated neurons, pushing them toward either ON or OFF states.
- Mechanism: Pruning removes low-magnitude weights, which reduces the pre-activation distribution variance. In rectifier-activated neurons, this causes the post-activation to cluster toward one of the two rectifier regions (ON or OFF), reducing entropy toward zero.
- Core assumption: Weights and inputs follow Gaussian distributions, and rectifier activations dominate (ReLU, GELU, LeakyReLU).
- Evidence anchors:
  - [abstract] "Based on our theoretical finding, NEPENTHE focuses on un-structurally pruning connections in layers with low entropy to remove them entirely."
  - [section] "Under the assumption of independent-centered distributions having a unitary variance, we can obtain the distribution for the pre-activation z..."
  - [corpus] Weak; no direct entropy-reduction pruning studies found in neighbors.
- Break condition: If input or weight distributions deviate strongly from Gaussian, or if non-rectifier activations dominate.

### Mechanism 2
- Claim: Entropy-weighted pruning budget reallocates parameters from low-entropy to high-entropy layers, enabling selective depth reduction.
- Mechanism: A pruning score (Il) is computed for each layer combining its entropy and average weight magnitude. Layers with lower entropy get higher inter-layer pruning relevance (Rl), so more parameters are removed from them. When a layer's entropy hits zero, it becomes linearizable and can be removed entirely.
- Core assumption: Low-entropy layers are better candidates for removal without hurting accuracy.
- Evidence anchors:
  - [abstract] "NEPENTHE combines unstructured pruning with an entropy-weighted parameter budget to prioritize removing parameters from layers with low entropy..."
  - [section] "This metric accounts for the average parameter's magnitude and the layer's entropy at the same time: layers with few parameters but high entropy are less prone to be removed..."
  - [corpus] Weak; no explicit entropy-weighted pruning examples in neighbors.
- Break condition: If entropy calculation is inaccurate, or if pruning removes too many parameters too quickly, accuracy drops.

### Mechanism 3
- Claim: Iterative pruning with entropy monitoring stops before accuracy drops below a threshold, preserving performance.
- Mechanism: After each pruning iteration, the model is retrained and validated. Pruning stops when validation accuracy drops below a relative threshold (θ·dense_acc).
- Core assumption: Retraining can recover accuracy after pruning, and the threshold is set appropriately.
- Evidence anchors:
  - [abstract] "...enabling their removal. Experiments on architectures like ResNet-18, MobileNet-V2, and Swin-T show that when encountering an over-parametrization regime, it can effectively linearize some layers..."
  - [section] "The final model is obtained once the performance on the validation set drops below some relative thresholdθ."
  - [corpus] Weak; no direct threshold-based pruning stopping rules in neighbors.
- Break condition: If retraining is insufficient to recover accuracy, or if threshold is set too aggressively.

## Foundational Learning

- Concept: Rectifier activation functions (ReLU, GELU, LeakyReLU) and their behavior.
  - Why needed here: The entropy metric and pruning strategy rely on understanding how rectifier activations produce ON/OFF/Zero states.
  - Quick check question: What are the three possible states for a neuron output in a ReLU layer, and when does each occur?

- Concept: Entropy as a measure of unpredictability in a discrete probability distribution.
  - Why needed here: Entropy is used to quantify how much a neuron uses its linear region, guiding pruning decisions.
  - Quick check question: If a neuron's output is always positive, what is its entropy?

- Concept: Unstructured pruning (removing individual weights) vs structured pruning (removing channels/filters).
  - Why needed here: NEPENTHE uses unstructured pruning, which is less computationally efficient but allows finer-grained entropy control.
  - Quick check question: Why might unstructured pruning be preferred when trying to minimize layer entropy?

## Architecture Onboarding

- Component map:
  Entropy calculation module -> Pruning budget allocator -> Iterative pruning loop -> Model evaluation

- Critical path:
  1. Train initial dense model
  2. Compute layer entropies
  3. Allocate pruning budget based on entropy scores
  4. Prune selected weights
  5. Retrain model
  6. Evaluate accuracy; if above threshold, repeat from step 2

- Design tradeoffs:
  - Unstructured pruning vs structured: Allows finer entropy control but less computational speedup per iteration
  - Global vs layer-wise pruning: Global is simpler but may remove too many parameters from already low-entropy layers
  - Entropy granularity: Neuron-level entropy is more precise but computationally heavier than layer-level

- Failure signatures:
  - Accuracy drops sharply after pruning → threshold too aggressive or entropy calculation inaccurate
  - No layers removed → entropy values never reach zero, or pruning budget misallocated
  - Slow convergence → retraining insufficient or pruning rate too conservative

- First 3 experiments:
  1. Apply NEPENTHE to ResNet-18 on CIFAR-10 with ζ=0.5, θ=0.99; verify that 3 layers are removed with <1% accuracy loss
  2. Test entropy trend: run 7 pruning iterations and plot layer entropies to confirm they decrease as theory predicts
  3. Ablation study: disable entropy weighting and use vanilla IMP; confirm no layers are removed while accuracy remains comparable

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the entropy metric generalize to non-rectifier activations (e.g., sigmoid, tanh) or is it fundamentally limited to rectifier functions?
- Basis in paper: [explicit] The paper focuses on rectifier activations (ReLU, GELU, LeakyReLU) and does not explore other activation functions. Table 4 only tests ResNet-18 on CIFAR-10 with rectifier activations.
- Why unresolved: The theoretical derivation in Section 3.2 relies on the binary nature of rectifier states (ON/OFF), which may not apply to continuous or bounded activations.
- What evidence would resolve it: Testing NEPENTHE on architectures using sigmoid, tanh, or other non-rectifier activations to determine if layer removal is still effective.

### Open Question 2
- Question: Can NEPENTHE be adapted into a differentiable loss term for end-to-end training, avoiding the iterative pruning process?
- Basis in paper: [inferred] The paper acknowledges the iterative nature of NEPENTHE as a limitation and suggests exploring "differentiable proxies of the layer's entropy" as future work (Section 4.5).
- Why unresolved: The entropy metric involves non-differentiable operations (sign function, counting states), making direct integration into gradient-based optimization challenging.
- What evidence would resolve it: Proposing and validating a differentiable approximation of entropy that can be minimized during training, followed by empirical comparison to the iterative approach.

### Open Question 3
- Question: How does NEPENTHE perform on extremely deep or over-parameterized models (e.g., ResNet-101, Vision Transformers) where depth reduction might have diminishing returns?
- Basis in paper: [explicit] Experiments focus on ResNet-18, MobileNet-V2, and Swin-T on datasets like CIFAR-10, Tiny-ImageNet, and PACS. No results are provided for deeper architectures or larger-scale datasets like ImageNet (except a single ResNet-18 experiment showing performance drop).
- Why unresolved: The paper does not explore the scalability of NEPENTHE to models with significantly more layers or parameters, nor does it analyze the trade-off between depth reduction and performance in such cases.
- What evidence would resolve it: Applying NEPENTHE to deeper architectures (e.g., ResNet-101, ViT) on larger datasets (e.g., ImageNet) and analyzing the relationship between initial depth, pruning rate, and final performance.

## Limitations
- Limited to rectifier activations (ReLU, GELU, LeakyReLU); effectiveness on other activation functions untested
- No demonstration of computational speedup despite depth reduction due to unstructured pruning
- Experimental scope limited to moderate-sized models and datasets

## Confidence

- Core claim (entropy-based depth reduction works): Medium
- Generalization to non-rectifier activations: Low
- Scalability to deeper/more complex architectures: Low
- Computational efficiency claims: Low

## Next Checks

1. Apply NEPENTHE to architectures with batch normalization or non-rectifier activations (e.g., Vision Transformers) to test generalization limits
2. Compare pruning speed and computational overhead against structured pruning methods that offer immediate inference benefits
3. Conduct ablation studies removing entropy weighting to quantify its specific contribution to depth reduction versus standard magnitude pruning