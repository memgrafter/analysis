---
ver: rpa2
title: Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia
arxiv_id: '2402.01751'
source_url: https://arxiv.org/abs/2402.01751
tags:
- chatbots
- subjects
- bard
- performance
- predicting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Three LLM chatbots (ChatGPT-3.5, ChatGPT-4, Bard) were assessed
  for their ability to detect Alzheimer's Dementia (AD) vs Cognitively Normal (CN)
  individuals using textual transcriptions of spontaneous speech. A zero-shot learning
  approach was employed with two levels of independent prompts, including chain-of-thought
  prompting for more detailed responses.
---

# Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer's Dementia

## Quick Facts
- arXiv ID: 2402.01751
- Source URL: https://arxiv.org/abs/2402.01751
- Reference count: 0
- LLM chatbots achieved above-chance performance in detecting Alzheimer's Dementia from speech transcriptions

## Executive Summary
This study evaluates three LLM chatbots (ChatGPT-3.5, ChatGPT-4, Bard) for detecting Alzheimer's Dementia (AD) versus Cognitively Normal (CN) individuals using textual transcriptions of spontaneous speech. A zero-shot learning approach with chain-of-thought prompting was employed, and models generated three-class outcomes (AD, CN, Unsure). Bard demonstrated the highest true-positive rate (89%) and F1 score (71%) for AD detection, while GPT-4 achieved the highest true-negative rate (56%) and F1 score (62%) for CN detection. All models surpassed chance-level performance but require further refinement for clinical application.

## Method Summary
The study used zero-shot learning to assess LLM chatbots on textual transcriptions of spontaneous speech from the ADReSSo Challenge dataset (71 recordings: 36 CN, 35 AD). Speech was transcribed using Otter.ai and models were queried with two independent prompts - a simple classification prompt and a chain-of-thought prompt eliciting detailed linguistic analysis. Performance was evaluated using accuracy, sensitivity, specificity, precision, and F1 score for three-class outcomes (AD, CN, Unsure).

## Key Results
- Bard achieved highest true-positive rate (89% recall) and F1 score (71%) for AD detection
- GPT-4 achieved highest true-negative rate (56%) and F1 score (62%) for CN detection
- All models surpassed chance-level performance but showed varying tendencies in misclassification patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot learning enables classification without labeled training data, relying on general language patterns learned during pre-training
- Mechanism: Models use chain-of-thought prompting to elicit detailed linguistic features from transcribed speech, mapping them to AD vs CN categories based on internal representations
- Core assumption: Models have been exposed to sufficient dementia-related linguistic patterns during pre-training
- Break condition: If models lack exposure to AD-related language patterns, performance will degrade significantly

### Mechanism 2
- Claim: Bard's superior AD detection stems from analyzing text at multiple granularities (sentence, paragraph, discourse levels)
- Mechanism: Bard systematically decomposes input text into hierarchical linguistic features to identify subtle markers of cognitive decline
- Core assumption: Hierarchical analysis can distinguish between AD and CN speech patterns despite variability
- Break condition: If hierarchical analysis introduces noise or overfits to specific patterns, generalization may suffer

### Mechanism 3
- Claim: GPT-4's diplomatic stance (high "Unsure" rates) reflects optimization for avoiding false positives in clinical settings
- Mechanism: GPT-4 balances sensitivity and specificity by being conservative in predictions, prioritizing precision over recall
- Core assumption: Training objectives predispose the model toward cautious predictions with uncertain inputs
- Break condition: If cautious approach leads to consistently high "Unsure" rates, practical utility may be limited

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: Avoids need for labeled training data, making approach more generalizable to real-world settings
  - Quick check question: What is the primary advantage of zero-shot learning over supervised learning in this context?

- Concept: Chain-of-thought prompting
  - Why needed here: Elicits more detailed linguistic features from models, improving ability to distinguish AD vs CN patterns
  - Quick check question: How does chain-of-thought prompting differ from standard prompting in terms of information elicited?

- Concept: Performance metrics (precision, recall, F1 score)
  - Why needed here: Evaluates classification quality across different aspects, especially important for three-class output structure
  - Quick check question: Why is F1 score particularly important when evaluating models with imbalanced class distributions?

## Architecture Onboarding

- Component map: Speech recordings -> Transcription (Otter.ai) -> LLM chatbot queries -> Classification (AD, CN, Unsure) -> Performance evaluation

- Critical path: 1) Transcribe speech recordings to text 2) Submit transcriptions to LLM chatbots with prompts 3) Parse responses and classify 4) Compare predictions against ground truth 5) Calculate performance metrics

- Design tradeoffs: Zero-shot learning avoids labeled data but may sacrifice accuracy; three-class output captures uncertainty but complicates evaluation; removing interviewer speech ensures subject-only input but may fragment semantic content

- Failure signatures: High "Unsure" rates across models indicate insufficient discriminative features; consistent misclassifications of high-MMSE CN subjects as AD suggest AD detection bias; lack of correlation between MMSE scores and predictions indicates capturing different aspects of cognitive function

- First 3 experiments: 1) Test performance with and without chain-of-thought prompting 2) Vary prompt phrasing to assess sensitivity to query structure 3) Evaluate performance on held-out test set for generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different prompting strategies on LLM chatbot performance for AD detection?
- Basis in paper: [explicit] Only two prompts (Q1 and Q2) were investigated; more sophisticated approaches might yield different outcomes
- Why unresolved: Study only tested two prompting strategies without exploring full range of potential prompts
- What evidence would resolve it: Testing variety of prompting strategies and comparing their impact on performance

### Open Question 2
- Question: How does LLM chatbot performance for AD detection vary over time and across different accounts or machines?
- Basis in paper: [inferred] Preliminary investigations showed consistent outcomes within a week, but variations across accounts, machines, or longer periods were not explored
- Why unresolved: Study did not investigate potential variations across different accounts, machines, or over extended periods
- What evidence would resolve it: Conducting longitudinal studies across different accounts, machines, and over time

### Open Question 3
- Question: What is the effect of transcription accuracy on LLM chatbot performance for AD detection?
- Basis in paper: [explicit] Automated speech-to-text service may introduce errors affecting performance
- Why unresolved: Study did not investigate impact of transcription accuracy on performance
- What evidence would resolve it: Comparing performance using different transcription methods or assessing impact of transcription errors

## Limitations

- Reliance on zero-shot learning without task-specific fine-tuning may limit performance due to insufficient exposure to dementia-related linguistic patterns
- Three-class output structure (AD, CN, Unsure) introduces ambiguity in performance evaluation and complicates cross-model comparisons
- Study uses single picture description task, which may not capture full spectrum of linguistic impairment in AD patients

## Confidence

- High confidence: All models surpassed chance-level performance in detecting AD from speech transcriptions
- Medium confidence: Bard's superior AD detection (89% recall) but tendency to misclassify CN subjects as AD
- Medium confidence: GPT-4's cautious approach with high "Unsure" rates for CN detection

## Next Checks

1. Test model performance with fine-tuned versions on AD-specific linguistic patterns to determine if zero-shot learning is optimal
2. Evaluate model robustness across different speech tasks beyond picture description to assess generalizability
3. Conduct human expert comparison studies to benchmark LLM performance against clinical standards for AD detection