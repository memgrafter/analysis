---
ver: rpa2
title: 'CODE: Contrasting Self-generated Description to Combat Hallucination in Large
  Multi-modal Models'
arxiv_id: '2406.01920'
source_url: https://arxiv.org/abs/2406.01920
tags:
- visual
- image
- arxiv
- decoding
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CODE, a novel contrastive decoding method
  to combat hallucination in Large Multi-modal Models (LMMs). CODE leverages self-generated
  descriptions as contrasting references during the decoding phase, using them to
  correct and improve response alignment with actual visual content.
---

# CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models

## Quick Facts
- arXiv ID: 2406.01920
- Source URL: https://arxiv.org/abs/2406.01920
- Authors: Junho Kim; Hyunjun Kim; Yeonju Kim; Yong Man Ro
- Reference count: 40
- Primary result: Novel contrastive decoding method using self-generated descriptions to reduce hallucinations in LMMs

## Executive Summary
CODE introduces a novel contrastive decoding method to combat hallucination in Large Multi-modal Models (LMMs). The approach leverages self-generated descriptions as contrasting references during decoding, dynamically adjusting token predictions to improve alignment with actual visual content. By integrating this contrastive mechanism into existing LMM frameworks without additional training, CODE enhances response coherence and cross-modal consistency across various benchmarks.

## Method Summary
CODE operates by generating self-descriptions of visual content during the decoding phase, which serve as contrasting references to the primary response generation. The method dynamically adjusts the information flow and distribution of next-token predictions by comparing the main generation path against the self-generated description path. This contrastive mechanism helps identify and correct hallucinatory content by prioritizing tokens that align with both the visual input and the self-generated description, effectively creating a feedback loop that improves cross-modal consistency without requiring model retraining.

## Key Results
- Significantly reduces hallucinations across multiple LMM benchmarks
- Improves cross-modal consistency between visual and textual outputs
- Maintains or improves informativeness of generated responses
- Compatible with cutting-edge LMMs without additional training requirements

## Why This Works (Mechanism)
CODE works by creating a dual-path decoding process where the model generates both a primary response and a self-description of the visual input simultaneously. The contrastive mechanism compares these two generation paths and adjusts token probabilities based on their alignment. When the primary generation path begins to hallucinate or diverge from the visual content, the self-description path acts as a corrective reference, steering the generation back toward factually consistent outputs. This dynamic adjustment happens in real-time during inference, allowing the model to self-correct without requiring post-hoc filtering or external validation.

## Foundational Learning
- **Contrastive learning**: Why needed - provides the mathematical framework for comparing and contrasting different generation paths; Quick check - verify the contrastive loss formulation aligns with established contrastive learning principles
- **Multi-modal representation alignment**: Why needed - ensures visual and textual modalities maintain consistent semantic relationships; Quick check - confirm cross-modal attention mechanisms are properly calibrated
- **Autoregressive decoding dynamics**: Why needed - understanding token-by-token generation flow is crucial for implementing contrastive adjustments; Quick check - trace how contrastive signals propagate through the decoding timeline
- **Self-supervised learning**: Why needed - enables the model to generate reliable self-descriptions without external supervision; Quick check - evaluate the quality and consistency of self-generated descriptions across different visual inputs

## Architecture Onboarding

**Component Map**: Visual Encoder -> Multi-modal Fusion -> CODE Contrastive Decoder -> Output Generator

**Critical Path**: Input Image → Visual Features → Multi-modal Fusion → Contrastive Decoding (Primary + Self-description paths) → Token Distribution Adjustment → Final Output

**Design Tradeoffs**: The method trades increased inference complexity for improved hallucination resistance. While adding computational overhead through dual-path decoding, it avoids the need for additional training or fine-tuning, making it more practical for deployment on existing models. The self-description generation must balance between being detailed enough to catch hallucinations while not becoming overly verbose and slowing down the process.

**Failure Signatures**: CODE may struggle when self-generated descriptions themselves contain errors or when visual content is ambiguous. The contrastive mechanism could also over-correct, leading to overly conservative or repetitive outputs. Performance may degrade on extremely abstract or artistic visual content where ground truth interpretations vary significantly.

**First Experiments**: 1) Compare hallucination rates on VQA benchmarks with and without CODE; 2) Measure inference latency overhead introduced by dual-path decoding; 3) Test robustness across different LMM architectures (e.g., Flamingo, BLIP-2, LLaVA)

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead during inference not quantified
- Performance on extremely long-form or open-ended tasks not explicitly evaluated
- Ambiguity around "no additional training" claim regarding integration complexity

## Confidence
High confidence in hallucination reduction claims, supported by extensive experimental validation across benchmarks. Medium confidence in generalizability claims due to limited evaluation on open-ended tasks. Low confidence in computational efficiency claims due to lack of latency measurements.

## Next Checks
1. Measure the inference-time latency and computational overhead of CODE compared to standard decoding methods across different LMM architectures
2. Test CODE's performance on open-ended generation tasks (e.g., story continuation, creative writing) where hallucination patterns may differ from structured tasks
3. Evaluate CODE's robustness when the self-generated descriptions contain their own errors or biases, and how this affects final output quality