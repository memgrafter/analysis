---
ver: rpa2
title: Don't drop your samples! Coherence-aware training benefits Conditional diffusion
arxiv_id: '2405.20324'
source_url: https://arxiv.org/abs/2405.20324
tags:
- coherence
- image
- diffusion
- conditional
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Coherence-Aware Diffusion (CAD), a novel method
  that integrates coherence scores into conditional diffusion models, allowing them
  to learn from noisy annotations without discarding data. The method conditions the
  diffusion model on both the conditional information and the coherence score, enabling
  the model to ignore or discount the conditioning when the coherence is low.
---

# Don't drop your samples! Coherence-aware training benefits Conditional diffusion

## Quick Facts
- arXiv ID: 2405.20324
- Source URL: https://arxiv.org/abs/2405.20324
- Reference count: 40
- Primary result: CAD achieves 15-point lower FID than second-best method on COCO text-to-image generation

## Executive Summary
This paper introduces Coherence-Aware Diffusion (CAD), a method that enables conditional diffusion models to learn from noisy annotations by conditioning on both the conditional information and a coherence score. Instead of discarding low-quality samples, CAD allows the model to discount or ignore the conditioning when coherence is low, effectively behaving as an unconditional model. The method demonstrates superior performance across text-to-image, class-conditional, and semantic map conditioning tasks, showing that leveraging coherence generates more realistic and diverse samples while respecting conditional information better than models trained on cleaned datasets.

## Method Summary
CAD extends conditional diffusion models by incorporating coherence scores that reflect the quality of conditional information. The model conditions on both the original condition (e.g., text, class label, semantic map) and a coherence score. During training, when coherence is low, the model learns to ignore the conditioning and behave unconditionally; when coherence is high, it behaves conditionally. The paper also introduces Coherence-Aware Classifier-Free Guidance (CA-CFG), which uses coherence scores to drive guidance instead of random dropout, improving image quality. The method is validated on three conditional generation tasks using various datasets including COCO, CIFAR-10, ImageNet-64, and ADE20K.

## Key Results
- On COCO text-to-image generation, CAD achieves a 15-point lower FID compared to the second-best method
- CAD outperforms baseline approaches (filtered and weighted) across all tested conditional generation tasks
- The method successfully handles noisy annotations without discarding data, improving both image quality and diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coherence-aware diffusion enables the model to learn from noisy annotations by conditioning on both the conditional information and a coherence score.
- Mechanism: The model learns to ignore or discount the conditioning when the coherence is low, allowing it to behave as an unconditional model. When coherence is high, it behaves as a conditional model.
- Core assumption: The coherence score accurately reflects the quality of the conditional information.
- Evidence anchors:
  - [abstract] "We then condition the diffusion model on both the conditional information and the coherence score. In this way, the model learns to ignore or discount the conditioning when the coherence is low."
  - [section 3.2] "We assume that for every datapoint (X, y) we have an associated c, the coherence score of y where c ∈ [0, 1]."

### Mechanism 2
- Claim: Coherence-aware classifier-free guidance (CA-CFG) improves the quality of generated samples by leveraging the gap between high and low coherence scores.
- Mechanism: CA-CFG uses the coherence score to drive the guidance, removing the need to dropout the conditioning randomly.
- Core assumption: The coherence score can effectively guide the generation process.
- Evidence anchors:
  - [abstract] "Additionally, we refine the classifier-free guidance method under this new inference scheme, resulting in enhanced image quality."
  - [section 3.3] "Instead, we propose a coherence-aware version of CFG (CA-CFG): ˆϵθ(xt, y) = ϵθ(xt, y, 1) + ω(ϵθ(xt, y, 1) − ϵθ(xt, y, 0))."

### Mechanism 3
- Claim: Coherence-aware diffusion can transition from an unconditional model to a conditional model simply by varying the coherence passed to the model.
- Mechanism: The model uses the coherence to learn by itself how much to rely on the label, converging to the same embedding as the coherence decreases.
- Core assumption: The coherence embedding is consistent, meaning it tends to produce the same vector as the coherence approaches 0.
- Evidence anchors:
  - [section 4.2] "To better understand the underlying mechanism, we design the following experiment... We observe that as the coherence decreases, the embeddings of all classes tend to converge into the same embedding (center)."

## Foundational Learning

- Concept: Diffusion models
  - Why needed here: The paper proposes a novel method for training conditional diffusion models with additional coherence information.
  - Quick check question: What is the main difference between diffusion models and GANs in terms of training and image quality?

- Concept: Conditional generation
  - Why needed here: The paper focuses on improving conditional diffusion models by incorporating coherence scores.
  - Quick check question: How does conditioning a generative model with additional information improve user control over the generation process?

- Concept: Classifier-free guidance
  - Why needed here: The paper extends the classifier-free guidance method to leverage coherence scores for enhanced image quality.
  - Quick check question: What is the purpose of classifier-free guidance in conditional diffusion models, and how does it improve the quality of generated samples?

## Architecture Onboarding

- Component map:
  Text embedding (FLAN-T5 XL) -> Cross-attention layers -> Self-attention layers -> RIN architecture
  Coherence score embedding -> Cross-attention layers -> Self-attention layers -> RIN architecture
  Latents -> Self-attention layers -> Cross-attention layers -> Updated patches

- Critical path:
  Embed text and coherence score → Pass embeddings through cross-attention layers → Process latents with self-attention layers → Update patches via cross-attention

- Design tradeoffs:
  Adding coherence score embedding increases model complexity but allows for better handling of noisy annotations. Using a simpler architecture (RIN) instead of U-Net reduces computational cost but may limit model capacity.

- Failure signatures:
  If the coherence score is not accurately estimated, the model may not learn effectively from noisy annotations. If the coherence embedding is not consistent, the model may not be able to transition from an unconditional model to a conditional model effectively.

- First 3 experiments:
  1. Train a CAD model on a small dataset with varying levels of coherence scores and evaluate its performance on conditional generation tasks.
  2. Compare the performance of CAD with baseline models (e.g., filtered, weighted) on text-to-image generation using the COCO dataset.
  3. Investigate the impact of different coherence score estimation methods on the performance of CAD models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of coherence score extraction method (e.g., CLIP score for text, confidence scores for class labels) affect the performance and generalizability of CAD across different datasets and conditioning types?
- Basis in paper: [explicit] The paper mentions using CLIP score for text coherence, confidence scores from pre-trained classifiers for class labels, and confidence scores from segmentation models for semantic maps.
- Why unresolved: The paper uses different methods for different conditioning types without comparing their effectiveness or exploring the impact of using a unified method across tasks.
- What evidence would resolve it: Systematic experiments comparing different coherence score extraction methods on the same dataset and conditioning type, or using the same method across all tasks.

### Open Question 2
- Question: What is the optimal strategy for discretizing the coherence score into bins or scalar values, and how does this choice impact the performance of CAD?
- Basis in paper: [explicit] The paper mentions using 8 equally distributed discrete bins for text conditioning and 5 bins for semantic maps, but doesn't explore the impact of this choice.
- Why unresolved: The paper uses fixed binning strategies without exploring the sensitivity of performance to the number of bins or the bin boundaries.
- What evidence would resolve it: Experiments varying the number of bins and bin boundaries, and comparing the performance to using scalar coherence scores.

### Open Question 3
- Question: How does the coherence-aware classifier-free guidance (CA-CFG) compare to other guidance strategies, such as classifier-based guidance or modified CFG with different dropout rates, in terms of image quality and adherence to the conditioning?
- Basis in paper: [explicit] The paper introduces CA-CFG as an alternative to CFG, but doesn't compare it to other guidance strategies.
- Why unresolved: The paper focuses on comparing CAD to baseline models without guidance, but doesn't explore the relative effectiveness of different guidance strategies within the CAD framework.
- What evidence would resolve it: Experiments comparing CA-CFG to other guidance strategies, such as classifier-based guidance or modified CFG with different dropout rates, on the same tasks and datasets.

## Limitations

- The coherence score estimation methods vary significantly across tasks, and their robustness to domain shifts remains unclear
- The claim that CA-CFG is "essential" for high-quality results is supported only by CIFAR-10 and COCO ablation studies, not across all three conditional tasks
- While class embeddings converge at low coherence, the paper does not demonstrate whether this behavior is optimal or merely a consequence of the training objective

## Confidence

**High Confidence:** The empirical results showing CAD outperforms baseline methods (filtered and weighted) on COCO text-to-image generation with a 15-point FID improvement. The ablation studies demonstrating the necessity of CA-CFG for high-quality results also have strong empirical support.

**Medium Confidence:** The theoretical framework showing that diffusion models can handle unreliable conditioning through coherence scores. While the mathematical formulation is sound, the practical implications depend heavily on the quality of coherence estimation.

**Low Confidence:** The generalizability of results across different coherence estimation methods and the claim that the convergence of class embeddings at low coherence represents optimal behavior rather than a training artifact.

## Next Checks

1. **Cross-task coherence validation:** Apply the same coherence estimation method (e.g., CLIP-based) across all three conditional tasks (text, class, semantic maps) to test whether CAD's performance depends on task-specific coherence scoring or generalizes to different estimation approaches.

2. **Extended ablation studies:** Conduct CA-CFG ablation studies on ImageNet-64 and semantic map conditioning tasks, not just CIFAR-10 and COCO, to verify that CA-CFG is universally essential rather than task-specific.

3. **Coherence score sensitivity analysis:** Systematically vary the granularity and range of coherence scores (e.g., 4 vs 8 levels, continuous vs discrete) across all tasks to determine whether CAD's performance is robust to coherence score parameterization or requires careful tuning per task.