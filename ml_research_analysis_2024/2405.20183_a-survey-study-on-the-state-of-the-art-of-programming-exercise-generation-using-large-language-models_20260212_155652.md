---
ver: rpa2
title: A Survey Study on the State of the Art of Programming Exercise Generation using
  Large Language Models
arxiv_id: '2405.20183'
source_url: https://arxiv.org/abs/2405.20183
tags:
- programming
- llms
- exercise
- generation
- exercises
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey study investigates the current state of programming
  exercise generation using large language models (LLMs). The authors systematically
  reviewed existing literature and found that multiple LLMs, including Codex and GPT-Neo,
  are capable of producing sensible, novel, and ready-to-use programming exercises.
---

# A Survey Study on the State of the Art of Programming Exercise Generation using Large Language Models

## Quick Facts
- arXiv ID: 2405.20183
- Source URL: https://arxiv.org/abs/2405.20183
- Reference count: 27
- Primary result: LLMs can generate sensible, novel programming exercises but face quality and circularity challenges; evaluation matrix and PEGB proposed to guide selection and benchmarking

## Executive Summary
This survey systematically reviews the state of programming exercise generation using large language models (LLMs). The authors find that LLMs like Codex and GPT-Neo can produce exercises that are sensible and novel, offering educators time-efficient and scalable personalized learning materials. However, significant challenges remain, including low-quality test suites and the risk of LLMs solving exercises they themselves generate. To address these issues, the authors propose an evaluation matrix incorporating cost, privacy, code generation capabilities, and a new Programming Exercise Generation Benchmark (PEGB), aiming to guide educators in selecting the most suitable LLM for their needs.

## Method Summary
The study is based on a systematic literature review of existing research on LLM applications in programming exercise generation, covering studies from 2018-2023. The authors analyze strengths and weaknesses of current approaches, focusing on technical feasibility and quality metrics such as sensibleness and novelty. They propose an evaluation matrix and PEGB benchmark to assess and compare LLMs, but do not conduct original experiments or classroom-based validation.

## Key Results
- Multiple LLMs (e.g., Codex, GPT-Neo) can generate sensible and novel programming exercises.
- Main strengths are time-efficient creation of learning materials and personalized exercise generation.
- Major weaknesses include low-quality test suites and the risk of LLMs solving their own generated exercises.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated programming exercises can save significant educator time and scale personalized learning.
- Mechanism: By automating exercise creation, LLMs reduce manual labor and enable large-scale personalization.
- Core assumption: Educators will adopt LLM tools despite quality concerns and cost considerations.
- Evidence anchors:
  - [abstract] "The main strengths identified are the time-efficient creation of learning materials and the ability to generate personalized exercises."
  - [section] "The main advantage of automated exercise generation lies in its remarkable ability to create learning material in a time efficient way."
  - [corpus] Weak: No direct evidence in corpus that educators have adopted or evaluated LLM-generated exercises at scale.
- Break condition: If exercise quality remains too low for practical classroom use, or if costs outweigh time savings.

### Mechanism 2
- Claim: LLMs can generate exercises that are both sensible and novel, meeting key educational quality criteria.
- Mechanism: Prompting techniques like decomposition into problem statement, template code, solution, and tests improve generation precision and exercise structure.
- Core assumption: Decomposition prompting consistently produces higher-quality exercises across diverse domains and models.
- Evidence anchors:
  - [section] "One reoccurring prompting technique is the decomposition of exercise parts into: (1) problem statement, (2) template code, (3) solution and (4) test cases."
  - [section] "Of 120 investigated exercises, 75% were sensible, 81.8% were novel, and 76.7% had a matching sample solution."
  - [corpus] Missing: No corpus evidence on effectiveness of decomposition prompting.
- Break condition: If decomposition fails to improve quality or if model context limits remain prohibitive.

### Mechanism 3
- Claim: An evaluation matrix combining general and specialized benchmarks can guide educators in selecting the best LLM for programming exercise generation.
- Mechanism: The matrix incorporates cost, data privacy, code generation capabilities, and a new PEGB benchmark, plus manual metrics like sensibleness and novelty.
- Core assumption: The proposed metrics and PEGB can be practically implemented and meaningfully differentiate LLM performance.
- Evidence anchors:
  - [abstract] "To address these issues, the authors propose an evaluation matrix that considers factors like cost, data privacy, code generation capabilities, and a newly introduced programming exercise generation benchmark (PEGB)."
  - [section] "A more complete picture of LLM performance for programming exercise generation can be formed when evaluating a sample of generated exercises using the evaluation matrix presented in Table I."
  - [corpus] Weak: No corpus evidence that such a matrix has been used or validated in practice.
- Break condition: If the matrix is too complex to implement or if benchmarks fail to correlate with educational outcomes.

## Foundational Learning

- Concept: Prompt decomposition (problem statement, template code, solution, tests)
  - Why needed here: Ensures LLM generates structured, complete programming exercises, improving sensibleness and readiness for use.
  - Quick check question: What are the four components of the decomposition prompting technique used for exercise generation?
- Concept: LLM benchmarking (e.g., HumanEval, MMLU, PEGB)
  - Why needed here: Provides standardized ways to evaluate code generation and exercise generation capabilities, essential for comparing models.
  - Quick check question: Which benchmark is widely used to evaluate LLMs' code generation abilities?
- Concept: Evaluation matrix design (combining automated and manual metrics)
  - Why needed here: Balances quantitative performance data with qualitative educational value, guiding model selection for educators.
  - Quick check question: What are two key qualitative constructs proposed for evaluating generated exercises?

## Architecture Onboarding

- Component map:
  - Prompt generation engine (handles decomposition and contextualization)
  - LLM selection module (uses evaluation matrix to choose model)
  - Exercise quality evaluator (combines automated metrics and manual review)
  - PEGB dataset manager (stores and serves partial exercises for benchmarking)
- Critical path:
  1. Educator defines exercise parameters and constraints
  2. System selects appropriate LLM using evaluation matrix
  3. Exercise generated via decomposition prompting
  4. Exercise validated via automated and manual metrics
  5. Output delivered or flagged for refinement
- Design tradeoffs:
  - Cost vs. quality: Higher-performing models (e.g., GPT-4) yield better exercises but at greater expense.
  - Automation vs. manual review: Full automation risks lower quality; manual review increases educator workload.
  - Context window vs. prompt complexity: Longer contexts enable richer exercises but raise costs and limits.
- Failure signatures:
  - Low sensibleness/novelty scores despite high code generation metrics
  - Frequent test suite failures or missing tests
  - Model easily solves its own generated exercises (circularity problem)
- First 3 experiments:
  1. Generate 20 exercises using decomposition prompting; measure sensibleness and novelty manually.
  2. Compare PEGB performance across 3 LLMs; record automated and manual quality scores.
  3. Simulate educator selection process using the evaluation matrix; assess alignment with actual preferences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop an effective benchmark specifically for programming exercise generation using LLMs, rather than general programming abilities?
- Basis in paper: [explicit] The paper discusses the need for a specialized benchmark and proposes the Programming Exercise Generation Benchmark (PEGB).
- Why unresolved: Current benchmarks focus on general programming abilities rather than the specific task of generating exercises.
- What evidence would resolve it: Development and validation of the PEGB with a diverse set of programming exercises and evaluation of multiple LLMs using this benchmark.

### Open Question 2
- Question: What are the long-term impacts of using LLM-generated exercises on student learning outcomes and engagement?
- Basis in paper: [inferred] The paper mentions potential risks of over-reliance on LLMs and the ease with which they can solve exercises they generate, but does not explore long-term impacts.
- Why unresolved: There is limited research on the sustained effects of using AI-generated content in education.
- What evidence would resolve it: Longitudinal studies comparing student performance and engagement using traditional vs. LLM-generated exercises over multiple semesters.

### Open Question 3
- Question: How can we improve the quality of test suites generated by LLMs to ensure they effectively assess student solutions?
- Basis in paper: [explicit] The paper identifies the low quality of generated test suites as a significant weakness.
- Why unresolved: Current LLMs struggle to create comprehensive and effective test cases for programming exercises.
- What evidence would resolve it: Development of techniques or fine-tuning approaches that significantly improve the pass rate and coverage of automatically generated test suites.

## Limitations

- The study relies entirely on existing literature and does not conduct original experiments or classroom-based validation of LLM-generated exercises.
- The proposed evaluation matrix and PEGB benchmark are not empirically validated or tested for practical educator adoption.
- Many cited studies focus on technical feasibility rather than pedagogical effectiveness, leaving a gap in evidence about learning impact.

## Confidence

- **High confidence**: LLMs can generate programming exercises that are sensible and novel in a time-efficient manner, as supported by multiple studies.
- **Medium confidence**: The proposed evaluation matrix could help guide LLM selection for exercise generation, but lacks empirical validation.
- **Low confidence**: Claims about significant educator time savings and scalability of personalized learning are not directly supported by evidence of actual adoption or classroom impact.

## Next Checks

1. Conduct a pilot study with educators to test the proposed evaluation matrix and gather feedback on its practicality and alignment with real-world needs.
2. Implement the PEGB benchmark with multiple LLMs and compare automated and manual quality scores to assess its discriminative power and relevance.
3. Perform a classroom-based experiment using LLM-generated exercises, measuring both student learning outcomes and educator time savings to validate claimed benefits.