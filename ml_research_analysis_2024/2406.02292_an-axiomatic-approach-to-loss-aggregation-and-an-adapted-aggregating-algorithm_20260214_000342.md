---
ver: rpa2
title: An Axiomatic Approach to Loss Aggregation and an Adapted Aggregating Algorithm
arxiv_id: '2406.02292'
source_url: https://arxiv.org/abs/2406.02292
tags:
- aggregation
- loss
- algorithm
- aggregating
- losses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends online learning under expert advice to use generalized
  loss aggregation functions beyond simple sums. The authors axiomatize reasonable
  aggregations as quasi-sums generated by a continuous, strictly increasing function
  u.
---

# An Axiomatic Approach to Loss Aggregation and an Adapted Aggregating Algorithm

## Quick Facts
- arXiv ID: 2406.02292
- Source URL: https://arxiv.org/abs/2406.02292
- Authors: Armando J. Cabrera Pacheco; Rabanus Derr; Robert C. Williamson
- Reference count: 8
- Primary result: Extends online learning under expert advice to use generalized loss aggregation functions beyond simple sums, axiomatizing reasonable aggregations as quasi-sums generated by continuous, strictly increasing functions u.

## Executive Summary
This paper extends online learning under expert advice by generalizing loss aggregation beyond simple sums. The authors axiomatize reasonable aggregations as quasi-sums generated by continuous, strictly increasing functions u, and adapt Vovk's Aggregating Algorithm to work with these quasi-sums. The modified algorithm maintains theoretical properties like Bayes updating and time-independent regret bounds under appropriate conditions. Experimental results on weather prediction tasks demonstrate that the choice of aggregation function influences the algorithm's behavior, with convex generators expressing risk-aversion and concave generators showing risk-seeking behavior.

## Method Summary
The authors characterize reasonable loss aggregation functions through four axioms: continuity, strict monotonicity, associativity, and loss compatibility. These axioms define quasi-sums, which are generated by continuous, strictly increasing functions u via Qu(x₁,...,xₙ) = u⁻¹(Σu(xᵢ)). The adapted Aggregating Algorithm (AA-QS) updates expert weights using a weighting profile f, which corresponds to the exponential of the quasi-sum generator u. The algorithm can be interpreted as performing standard AA on a distorted loss function u∘λ. Experiments use weather data from German Weather Agency (DWD) for 4 locations, with 9 base classifiers trained on 80% of data and tested on 20%, comparing different aggregation functions including L₀.₅-norm, sum, L₂-norm, L₁₀-norm, and focal loss.

## Key Results
- Quasi-sums are characterized as aggregation functions satisfying continuity, strict monotonicity, associativity, and loss compatibility
- The adapted AA with quasi-sum aggregation maintains time-independent regret bounds and Bayes updating under (f,η)-mixability conditions
- Convex generators express risk-aversion (fewer extreme losses) while concave generators show risk-seeking behavior (more extreme losses but also more perfect predictions)
- The choice of aggregation function significantly influences prediction behavior in weather classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalized aggregation functions (quasi-sums) can be axiomatically characterized and preserve most AA theoretical properties.
- Mechanism: Quasi-sums are generated by continuous, strictly increasing functions u, which transform the problem into a "change of variables" setting. The AA applied to Qu is equivalent to applying the standard AA to the distorted loss u∘λ.
- Core assumption: Aggregation function A satisfies continuity, strict monotonicity, associativity, and loss compatibility.
- Evidence anchors:
  - [abstract]: "Via easily justified assumptions we characterize a set of reasonable loss aggregation functions as quasi-sums."
  - [section 4]: "Let A: ⋃n∈N[0,∞)n → [0,∞) be an aggregation function. Suppose that A is continuous, strictly increasing, associative and loss compatible..."
  - [corpus]: Weak; no direct citations but related work on generalized aggregations exists.
- Break condition: If the aggregation violates any of the axioms, the characterization fails and theoretical guarantees may not hold.

### Mechanism 2
- Claim: The adapted AA with quasi-sum aggregation maintains time-independent regret bounds and Bayes updating under appropriate conditions.
- Mechanism: The APA-QS updates expert weights using a weighting profile f, which corresponds to the exponential of the quasi-sum generator u. This preserves the Bayesian interpretation when λ is the log-loss and u(x) = x.
- Evidence anchors:
  - [abstract]: "This variant inherits most of the nice theoretical properties of the AA, such as recovery of Bayes' updating and a time-independent bound on quasi-sum regret."
  - [section 5.1]: "For the sake of simplicity, we do not write out the base measure µ in the rest of the paper..."
  - [corpus]: Weak; limited citations but related to generalized regret bounds in online learning.
- Break condition: If the game is not (f,η)-mixable, the substitution function required for the regret bound may not exist.

### Mechanism 3
- Claim: The choice of aggregation function (via its generator u) expresses the learner's attitude towards losses, influencing prediction behavior.
- Mechanism: Convex u values high losses more, leading to risk-averse behavior (fewer extreme losses). Concave u values low losses more, leading to risk-seeking behavior (more extreme losses but also more perfect predictions).
- Evidence anchors:
  - [abstract]: "Finally, we argue that generalized aggregations express the attitude of the learner towards losses."
  - [section 7.1]: "More generally, it is true that risk-avoider prefer convex u, i.e., high losses are up-valued, low losses are down-valued."
  - [section 7.3]: "We qualitatively approach this question in three ways... We provide the log-loss profile of several applications of the AA-QS for different aggregations on a sequential weather classification task."
- Break condition: If the loss distribution is not compatible with the chosen u, the desired behavior may not manifest.

## Foundational Learning

- Concept: Axiomatical characterization of aggregation functions
  - Why needed here: To rigorously define what constitutes a "reasonable" loss aggregation beyond simple sums, enabling generalization of online learning theory.
  - Quick check question: What four properties must an aggregation function satisfy to be a quasi-sum? (Continuity, strict monotonicity, associativity, loss compatibility)

- Concept: Quasi-sums and their generators
  - Why needed here: Quasi-sums provide the mathematical framework for generalized loss aggregation, and their generators u determine the aggregation behavior.
  - Quick check question: How is a quasi-sum Qu(x1,...,xn) constructed from a generator u? (Qu(x1,...,xn) = u^(-1)(Σu(xi)))

- Concept: Mixability and substitution functions
  - Why needed here: Mixability ensures the existence of a substitution function that converts pseudo-predictions to actual predictions while maintaining theoretical guarantees.
  - Quick check question: What condition must hold for a prediction game to be (f,η)-mixable? (There exists a substitution function Σ such that λ(ω,Σ(ψ)) ≤ ψ(ω) for all ω)

## Architecture Onboarding

- Component map: Expert predictions ξt(θ) -> Weight update (Step 1) -> Pseudo-prediction (Step 2) -> Actual prediction γt via substitution function Σ -> Nature reveals outcome ωt

- Critical path:
  1. Experts make predictions
  2. Learner updates weights using weighting profile f
  3. Learner forms pseudo-prediction
  4. Learner substitutes pseudo-prediction with actual prediction
  5. Nature reveals outcome
  6. Repeat

- Design tradeoffs:
  - Mixability vs. generality: More general aggregations may not be mixable for all loss functions, requiring fallback bounds.
  - Convex vs. concave u: Convex u expresses risk-aversion but may produce more suboptimal predictions; concave u expresses risk-seeking but may produce extreme losses.
  - Learning rate η: Controls the strength of weight updates; too high can cause instability, too low can slow learning.

- Failure signatures:
  - Numerical instability: Pseudo-predictions or actual predictions become NaN or infinite
  - Poor performance: Loss histograms show many extreme losses or consistently high average loss
  - Theoretical guarantees fail: Regret bounds are violated in experiments

- First 3 experiments:
  1. Verify quasi-sum characterization: Implement a non-standard aggregation, check if it satisfies the four axioms, and confirm it can be written as a quasi-sum.
  2. Test mixability: For a given loss function and aggregation, check if the (f,η)-mixability condition holds by attempting to construct the substitution function.
  3. Compare aggregation behaviors: Run the AA-QS with different generators u on a simple prediction task and compare the resulting loss histograms to verify the risk-aversion/risk-seeking behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific geometric or analytical conditions characterize when the log-loss is "fundamental" for quasi-sums generated by a continuous, strictly increasing function u?
- Basis in paper: [explicit] The paper notes that the log-loss is fundamental for the standard sum aggregation but suggests this property may not extend to other aggregations. It states that the modified Aggregating Algorithm (AA-QS) with quasi-sums leads to a connection back to a type of fundamentality of the log-loss via u, which can potentially be described geometrically.
- Why unresolved: The paper only briefly mentions this connection without providing a detailed geometric or analytical characterization. The authors acknowledge this as an observation but do not develop it further.
- What evidence would resolve it: A rigorous mathematical proof showing that the log-loss satisfies specific geometric or analytical conditions (e.g., curvature comparison) for quasi-sums generated by arbitrary functions u, and demonstrating how these conditions generalize the standard notion of mixability.

### Open Question 2
- Question: How does the choice of aggregation function (quasi-sum generator u) affect the regret bounds of online learning algorithms beyond the Aggregating Algorithm?
- Basis in paper: [inferred] The paper demonstrates that the Aggregating Algorithm can be adapted to work with quasi-sums and that the regret bounds depend on the choice of aggregation. However, it only explores this adaptation for the Aggregating Algorithm and suggests that similar modifications could be provided for other online learning algorithms.
- Why unresolved: The paper does not investigate the impact of aggregation functions on regret bounds for other online learning algorithms like the Weighted Majority Algorithm or online gradient descent.
- What evidence would resolve it: Theoretical analysis and experimental results showing how different quasi-sum generators u affect the regret bounds of various online learning algorithms, including comparisons with the standard sum aggregation.

### Open Question 3
- Question: What is the optimal learning rate η for different quasi-sum generators u in practice, and how does it depend on the specific loss function and aggregation?
- Basis in paper: [explicit] The paper discusses the learning rate η and its relationship with the weighting profile f and the aggregation function Qu. It shows that the learning rate can be incorporated into the weighting profile as fη(x) = f(x)η. However, it does not provide specific guidance on choosing the optimal learning rate for different combinations of loss functions and quasi-sum generators.
- Why unresolved: The paper does not offer a systematic approach for determining the optimal learning rate η for a given loss function and quasi-sum generator. It only provides examples with specific learning rates chosen somewhat arbitrarily.
- What evidence would resolve it: A theoretical framework or empirical study that identifies the optimal learning rate η for various combinations of loss functions and quasi-sum generators, possibly involving a trade-off between regret bounds and convergence speed.

## Limitations

- Limited empirical validation scope: Weather prediction experiments demonstrate qualitative effects but lack quantitative performance comparisons with baseline algorithms.
- Incomplete theoretical generalization: The paper only adapts the Aggregating Algorithm to quasi-sums, leaving open questions about how other online learning algorithms would perform with generalized aggregations.
- No systematic learning rate optimization: The paper uses specific learning rates without providing a framework for determining optimal η for different combinations of loss functions and quasi-sum generators.

## Confidence

Our confidence in the theoretical framework is **High**, as the axiomatic characterization of quasi-sums and their relationship to the Aggregating Algorithm is mathematically rigorous. The proof that APA-QS reduces to standard AA under distortion u∘λ is sound, and the conditions for mixability are clearly stated.

However, confidence in the empirical validation is **Medium**. The weather prediction experiments demonstrate the qualitative effects of different aggregations but lack quantitative performance comparisons with baseline algorithms. The choice of u influences behavior as predicted, but the magnitude of these effects and their practical significance are not thoroughly analyzed.

The connection between aggregation choices and risk attitudes is conceptually **High** confidence based on theoretical arguments, but empirical demonstration is **Low** confidence due to limited experimental scope. The paper provides intuitive examples but lacks comprehensive analysis across diverse prediction tasks and loss landscapes.

## Next Checks

1. **Mixability verification**: For each aggregation function u and loss λ used in experiments, verify the (f,η)-mixability condition by attempting to construct the substitution function Σ and checking if λ(ω,Σ(ψ)) ≤ ψ(ω) holds for all ω.

2. **Comparative performance analysis**: Implement baseline algorithms (standard AA with sum aggregation, Follow-the-Leader) and compare their regret bounds and actual performance against AA-QS with various quasi-sums on the same weather prediction tasks.

3. **Sensitivity analysis**: Systematically vary the learning rate η across a wider range for each aggregation function, measuring both convergence speed and final loss to quantify the tradeoff between exploration and exploitation.