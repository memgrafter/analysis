---
ver: rpa2
title: 'SGD with memory: fundamental properties and stochastic acceleration'
arxiv_id: '2410.04228'
source_url: https://arxiv.org/abs/2410.04228
tags: []
core_contribution: This paper addresses the problem of accelerating SGD-type algorithms
  on quadratic problems with power-law spectrum in the presence of mini-batch noise.
  The authors propose a general framework of first-order methods with an arbitrary
  fixed number M of auxiliary velocity vectors (memory-M algorithms) and analyze their
  convergence properties.
---

# SGD with memory: fundamental properties and stochastic acceleration

## Quick Facts
- arXiv ID: 2410.04228
- Source URL: https://arxiv.org/abs/2410.04228
- Reference count: 0
- Memory-1 SGD with adaptive schedule achieves accelerated convergence O(t^{-ζ(2-1/ν)}) on quadratic problems with power-law spectrum

## Executive Summary
This paper proposes a framework of first-order optimization methods with memory (M auxiliary velocity vectors) to accelerate convergence on quadratic problems with power-law eigenvalue spectra under mini-batch noise. The authors develop a general expansion of loss in terms of signal and noise propagators, showing that memory-M algorithms maintain the exponent of plain gradient descent while allowing tuning of convergence constants through effective learning rates. A key theoretical result demonstrates that memory-1 algorithms can achieve arbitrarily small loss constants while remaining stable. Building on this, the authors propose a time-dependent memory-1 algorithm that heuristically achieves improved convergence rates, experimentally validated on synthetic Gaussian data and MNIST classification with shallow ReLU networks.

## Method Summary
The authors introduce a general class of first-order optimization algorithms with M auxiliary velocity vectors (memory-M algorithms) that extend standard SGD by maintaining additional momentum-like quantities. For quadratic objectives with power-law spectra, they derive a general expansion of the loss function in terms of signal and noise propagators, enabling analysis of convergence properties across different noise regimes. The framework shows that stationary stable memory-M algorithms preserve the convergence exponent of plain gradient descent while allowing control over the convergence constant through the effective learning rate. The authors prove that memory-1 algorithms can achieve arbitrarily small loss constants while maintaining stability, leading to the development of an accelerated memory-1 algorithm with a time-dependent schedule that improves convergence rates in the signal-dominated phase.

## Key Results
- Memory-M algorithms with M≥1 maintain the same convergence exponent as plain GD for quadratic objectives with power-law spectra
- Memory-1 algorithms can achieve arbitrarily small loss constants while remaining stable, unlike plain GD
- Time-dependent memory-1 algorithm achieves improved convergence rate O(t^{-ζ(2-1/ν)}) in the signal-dominated phase
- Experimental validation shows acceleration on synthetic Gaussian data and MNIST classification with shallow ReLU networks

## Why This Works (Mechanism)
The acceleration mechanism works by leveraging additional velocity vectors to control the effective learning rate and loss constant while preserving the fundamental convergence exponent. In the presence of mini-batch noise, plain SGD suffers from a fixed trade-off between convergence speed and stability determined by the spectrum's power-law exponent ν. Memory-M algorithms break this limitation by maintaining auxiliary velocity vectors that act as a buffer against noise while still allowing aggressive effective learning rates. The time-dependent memory-1 algorithm further optimizes this trade-off by adapting the learning schedule based on the evolving signal-to-noise ratio, achieving faster convergence in the early signal-dominated phase while maintaining stability as noise becomes more significant.

## Foundational Learning

Power-law eigenvalue spectra
- Why needed: The analysis relies on the specific spectral structure of quadratic objectives common in machine learning
- Quick check: Verify that the loss Hessian eigenvalues follow ρ(λ) ∝ λ^{-ν} for some ν>1

Signal-to-noise ratio dynamics
- Why needed: Understanding how gradient noise scales with iteration number is crucial for algorithm design
- Quick check: Track the ratio of deterministic gradient norm to noise norm during optimization

Stationary stability conditions
- Why needed: Ensuring algorithmic stability while allowing aggressive learning rates is central to the acceleration mechanism
- Quick check: Verify that all eigenvalues of the iteration matrix have magnitude less than 1

## Architecture Onboarding

Component map: Loss function -> Signal propagator -> Noise propagator -> Effective learning rate -> Convergence rate
Critical path: Objective function specification → Spectral analysis → Memory vector update equations → Stability analysis → Learning rate schedule design
Design tradeoffs: Memory complexity (M vectors) vs convergence improvement vs implementation complexity
Failure signatures: Divergence when effective learning rate exceeds stability bounds; sub-optimal convergence when schedule is too conservative
First experiments:
1. Test on synthetic quadratic problem with known power-law spectrum and controlled noise levels
2. Evaluate on MNIST with shallow ReLU network, comparing against plain SGD and momentum methods
3. Ablation study varying memory depth M and learning rate schedules

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Analysis strictly limited to quadratic objectives with power-law eigenvalue spectra
- Assumes isotropic mini-batch noise, which may not reflect real-world gradient noise structure
- Theoretical improvements in signal-dominated phase may not translate to practical finite-batch settings

## Confidence

High confidence in quadratic case analysis: The mathematical derivations follow standard linear dynamical systems techniques and the expansion in terms of propagators is rigorous
Medium confidence in practical implications: Limited empirical evaluation on synthetic problems and single shallow network architecture constrains generalizability

## Next Checks

1. Test the memory-1 algorithm on non-quadratic objectives including deep networks with skip connections and residual architectures to assess robustness beyond theoretical assumptions
2. Evaluate performance across different noise covariance structures, including anisotropic and structured noise patterns common in real datasets
3. Compare against adaptive methods (Adam, AdaGrad) and momentum-based optimizers on standard benchmark datasets (CIFAR-10/100, ImageNet) with varying batch sizes and learning rate schedules