---
ver: rpa2
title: 'Emergence in non-neural models: grokking modular arithmetic via average gradient
  outer product'
arxiv_id: '2407.20199'
source_url: https://arxiv.org/abs/2407.20199
tags:
- modular
- neural
- circulant
- feature
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper demonstrates that grokking\u2014sharp performance improvements\
  \ in modular arithmetic tasks\u2014is not specific to neural networks or gradient-based\
  \ optimization. Using Recursive Feature Machines (RFM), an iterative algorithm leveraging\
  \ the Average Gradient Outer Product (AGOP) for feature learning, the authors show\
  \ that modular arithmetic tasks exhibit grokking through the emergence of block-circulant\
  \ features."
---

# Emergence in non-neural models: grokking modular arithmetic via average gradient outer product

## Quick Facts
- arXiv ID: 2407.20199
- Source URL: https://arxiv.org/abs/2407.20199
- Reference count: 40
- One-line primary result: Grokking in modular arithmetic tasks arises purely from feature learning using Average Gradient Outer Product, not from neural network architecture or optimization.

## Executive Summary
This paper demonstrates that the phenomenon of grokking—sharp performance improvements after long periods of overfitting—is not specific to neural networks. Using Recursive Feature Machines (RFM) with Average Gradient Outer Product (AGOP), the authors show that modular arithmetic tasks exhibit grokking through the emergence of block-circulant features that implement the Fourier Multiplication Algorithm. The findings suggest that emergence in machine learning is fundamentally a feature-learning phenomenon, occurring in both neural networks and kernel-based models when the task structure aligns with the learned features.

## Method Summary
The authors use Recursive Feature Machines (RFM), an iterative algorithm that enables feature learning in kernel machines through the Average Gradient Outer Product (AGOP). RFM iteratively trains a kernel machine to fit training data, computes the AGOP of the trained model, and updates the feature matrix using the square root of the AGOP. This process gradually learns block-circulant feature matrices that implement the Fourier Multiplication Algorithm for modular arithmetic tasks. The authors compare this approach with neural networks trained on the same tasks, analyzing their learned features and demonstrating that random circulant transformations can accelerate generalization in both architectures.

## Key Results
- RFM achieves 100% test accuracy on modular arithmetic after an initial phase where training loss is zero and test loss remains constant
- Both RFM and neural networks learn block-circulant features that implement the Fourier Multiplication Algorithm
- Random circulant transformations dramatically accelerate generalization in both RFM and neural networks
- Emergence can result purely from learning task-relevant features, without requiring specific model architectures or optimization methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Grokking arises purely from feature learning, not from model architecture or optimization method.
- **Mechanism:** The Average Gradient Outer Product (AGOP) iteratively extracts task-relevant features. When used in Recursive Feature Machines (RFM), the AGOP converges to block-circulant matrices that implement the Fourier Multiplication Algorithm (FMA), enabling perfect generalization on modular arithmetic.
- **Core assumption:** The AGOP captures the most relevant input directions influencing the output of any differentiable predictor, and these directions are sufficient to encode modular arithmetic solutions.
- **Evidence anchors:**
  - [abstract] states "RFM gradually learns block-circulant features to solve modular arithmetic" and "emergence can result purely from learning task-relevant features."
  - [section] describes how "AGOP gradually learns block-circulant features to solve modular arithmetic" and provides two progress measures—circulant deviation and AGOP alignment—that show gradual improvement while standard metrics do not change.
  - [corpus] includes "Average gradient outer product as a mechanism for deep neural collapse," supporting AGOP as a general mechanism.
- **Break condition:** If the underlying task does not have a low-rank or structured solution expressible via circulant features, the AGOP may not converge to generalizable features and grokking will not occur.

### Mechanism 2
- **Claim:** Neural networks and RFM learn the same algorithmic solution to modular arithmetic—the Fourier Multiplication Algorithm.
- **Mechanism:** Both architectures learn block-circulant feature matrices that, when used in kernel machines, implicitly implement the FMA via the diagonalization of circulant matrices using the Discrete Fourier Transform (DFT).
- **Core assumption:** The FMA is the generalizing solution for modular arithmetic tasks, and both neural networks and RFM are implicitly biased toward learning it.
- **Evidence anchors:**
  - [abstract] states "RFM uses such block-circulant features to implement the Fourier Multiplication Algorithm, which prior work posited as the generalizing solution neural networks learn on these tasks."
  - [section] provides theoretical support that "kernel machines equipped with such circulant features learn the Fourier Multiplication Algorithm for modular arithmetic."
  - [corpus] includes "Grokking Modular Polynomials," which explores similar algorithmic solutions in neural networks.
- **Break condition:** If the training data does not sufficiently cover the modular group structure or if the model cannot represent circulant features (e.g., due to architectural constraints), the FMA solution may not emerge.

### Mechanism 3
- **Claim:** Random circulant transformations accelerate generalization in both RFM and neural networks for modular arithmetic.
- **Mechanism:** Transforming input data with random block-circulant matrices effectively pre-encodes the Fourier basis, allowing standard kernel machines or neural networks to bypass the slow feature learning phase and directly generalize.
- **Core assumption:** The structure of block-circulant features is sufficient for generalization on modular arithmetic, and no additional structure is required beyond generic circulant matrices.
- **Evidence anchors:**
  - [section] shows "training neural networks on data transformed by random block-circulant matrices dramatically decreases training time needed to learn modular arithmetic."
  - [section] demonstrates "standard kernel machines using random circulant features easily learn modular operations."
  - [corpus] includes "Linear Recursive Feature Machines provably recover low-rank matrices," supporting the sufficiency of circulant structure.
- **Break condition:** If the random circulant matrices are degenerate (e.g., identity or rank-deficient), they will not provide the necessary structure for generalization.

## Foundational Learning

- **Concept:** Cyclic groups and discrete logarithms
  - **Why needed here:** Modular multiplication and division rely on the cyclic structure of the multiplicative group Z*p. Reordering by the discrete logarithm converts these operations into addition/subtraction, revealing the block-circulant structure in feature matrices.
  - **Quick check question:** Can you explain why the multiplicative group Z*p is cyclic and how the discrete logarithm transforms multiplication into addition?

- **Concept:** Circulant and Hankel matrices
  - **Why needed here:** The feature matrices learned by RFM and neural networks for modular arithmetic tasks are composed of circulant or Hankel blocks. Understanding their structure is key to recognizing the learned algorithm (FMA).
  - **Quick check question:** What is the relationship between circulant matrices and the Discrete Fourier Transform, and how does this enable the FMA?

- **Concept:** Average Gradient Outer Product (AGOP)
  - **Why needed here:** The AGOP is the mechanism through which both RFM and neural networks learn features. It measures the sensitivity of the output to input directions and extracts the most relevant features for the task.
  - **Quick check question:** How does the AGOP differ from standard gradient-based feature learning, and why is it effective for non-differentiable models like kernel machines?

## Architecture Onboarding

- **Component map:** Recursive Feature Machines (RFM) -> Average Gradient Outer Product (AGOP) -> Block-circulant feature matrices -> Fourier Multiplication Algorithm (FMA) -> Generalization on modular arithmetic

- **Critical path:**
  1. Initialize RFM with a base kernel and identity matrix
  2. Train kernel machine to fit training data (zero training loss)
  3. Compute AGOP of the trained model
  4. Update feature matrix using the square root of AGOP
  5. Repeat steps 2-4 until test accuracy transitions to 100%
  6. Monitor circulant deviation and AGOP alignment as progress measures

- **Design tradeoffs:**
  - Using a single feature matrix across all output coordinates vs. separate matrices per class (as in Theorem 5.1)
  - Enforcing circulant structure at every iteration vs. allowing it to emerge naturally (affects grokking speed)
  - Choosing matrix power s in RFM (s=1/2 used in experiments) vs. other values (impacts convergence)

- **Failure signatures:**
  - Circulant deviation remains high and AGOP alignment plateaus, indicating the model is not learning generalizable features
  - Test accuracy improves with training loss, suggesting overfitting rather than true generalization
  - Random circulant transformations do not improve generalization, indicating the task may not rely on circulant structure

- **First 3 experiments:**
  1. Implement RFM with quadratic kernel on modular addition (p=61) and plot test accuracy, circulant deviation, and AGOP alignment over iterations
  2. Train a neural network with quadratic activations on modular multiplication and compare its Neural Feature Matrix to the AGOP before and after reordering by discrete logarithm
  3. Transform modular arithmetic data with random circulant matrices and train both RFM and neural networks to verify accelerated generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the AGOP mechanism for feature learning extend to other domains beyond modular arithmetic, such as language or vision tasks?
- **Basis in paper:** [inferred] The paper demonstrates that AGOP-based feature learning enables grokking in modular arithmetic tasks, suggesting it could be a general mechanism. However, the authors note that their measures of progress require access to a fully trained model or insight into the algorithm implemented, which may be challenging in more complex domains.
- **Why unresolved:** The paper focuses specifically on modular arithmetic tasks. While the authors suggest AGOP could be a general mechanism, they do not provide evidence for its effectiveness in other domains.
- **What evidence would resolve it:** Experiments applying RFM or similar AGOP-based methods to language or vision tasks, demonstrating emergence and gradual feature learning as measured by AGOP alignment or similar metrics.

### Open Question 2
- **Question:** Can the two-phase grokking mechanism (lazy to rich training dynamics) be reconciled with the findings that grokking can occur purely through gradual feature learning?
- **Basis in paper:** [explicit] The paper explicitly states that their results demonstrate grokking can occur without the two-phase mechanism, as feature learning is gradual even when test performance is constant.
- **Why unresolved:** Recent works argue for a two-phase mechanism in neural networks, while this paper shows a different mechanism for RFM. The relationship between these mechanisms is unclear.
- **What evidence would resolve it:** Further analysis of neural network training dynamics to determine if and when two-phase grokking occurs, and comparison with AGOP-based feature learning measures.

### Open Question 3
- **Question:** What is the theoretical explanation for why random circulant features enable generalization in kernel machines for modular arithmetic tasks?
- **Basis in paper:** [explicit] The paper demonstrates empirically that random circulant features allow standard kernel machines to generalize on modular arithmetic tasks, but does not provide a theoretical explanation for this phenomenon.
- **Why unresolved:** While the paper provides theoretical support for why learned circulant features work (via the Fourier Multiplication Algorithm), it does not explain why randomly generated circulant features are also effective.
- **What evidence would resolve it:** A theoretical analysis showing that random circulant features approximate the properties needed for the Fourier Multiplication Algorithm or similar generalizing solutions.

## Limitations
- The universality of the AGOP mechanism for feature learning beyond modular arithmetic tasks remains untested
- The theoretical connection between AGOP and the Fourier Multiplication Algorithm lacks rigorous proof of convergence or optimality
- The exact mathematical conditions under which block-circulant structure emerges are not fully characterized

## Confidence
- **High Confidence:** Grokking occurs in RFM and exhibits the same characteristics as neural network grokking (zero training loss with delayed generalization)
- **Medium Confidence:** The Fourier Multiplication Algorithm is the generalizing solution learned by both RFM and neural networks, and block-circulant features are the mechanism enabling this
- **Low Confidence:** Emergence in machine learning is purely a feature-learning phenomenon that generalizes beyond modular arithmetic tasks

## Next Checks
1. Test RFM on other arithmetic tasks (e.g., modular exponentiation) to verify whether block-circulant features and grokking emerge in tasks without obvious cyclic structure
2. Rigorously prove that the AGOP converges to block-circulant matrices for modular arithmetic tasks and characterize the convergence rate
3. Experiment with alternative feature structures (e.g., Toeplitz or Hankel-only matrices) to determine whether block-circulant structure is necessary or merely sufficient for generalization