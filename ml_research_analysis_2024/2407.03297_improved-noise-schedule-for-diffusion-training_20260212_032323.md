---
ver: rpa2
title: Improved Noise Schedule for Diffusion Training
arxiv_id: '2407.03297'
source_url: https://arxiv.org/abs/2407.03297
tags:
- noise
- schedule
- cosine
- diffusion
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to designing noise schedules
  for diffusion models, focusing on the importance sampling of the logarithm of the
  Signal-to-Noise ratio (log SNR). The key insight is that increasing the sample frequency
  around log SNR = 0, the critical transition point between signal dominance and noise
  dominance, can lead to more robust and accurate predictions.
---

# Improved Noise Schedule for Diffusion Training

## Quick Facts
- arXiv ID: 2407.03297
- Source URL: https://arxiv.org/abs/2407.03297
- Authors: Tiankai Hang; Shuyang Gu; Xin Geng; Baining Guo
- Reference count: 40
- Primary result: Laplace noise schedule with b=0.5 achieves 12.6 FID on ImageNet-256 with DiT-B

## Executive Summary
This paper proposes a novel approach to designing noise schedules for diffusion models by focusing on importance sampling of the logarithm of the Signal-to-Noise ratio (log SNR). The key insight is that increasing the sample frequency around log SNR = 0, the critical transition point between signal and noise dominance, leads to more robust and accurate predictions. The authors evaluate several noise schedules including Laplace, Cauchy, and variants of the Cosine schedule on the ImageNet benchmark, demonstrating that schedules with concentrated probability density around log SNR = 0 consistently outperform alternatives.

## Method Summary
The paper reframes noise schedule design as a probability distribution problem, where the goal is to optimally distribute sampling across different noise intensities. Instead of specifying how noise varies with time, the framework derives noise schedules by identifying curves in the probability density function p(λ) of log SNR. The authors investigate four novel noise strategies: Cosine Shifted, Cosine Scaled, Cauchy, and Laplace, all designed to concentrate probability density around log SNR = 0. These are evaluated on a DiT-B architecture trained on ImageNet compressed latents with 500K iterations, using FID-10K as the primary metric across multiple CFG scales.

## Key Results
- Laplace noise schedule with b=0.5 achieves 12.6 FID on ImageNet-256 with DiT-B
- Cauchy schedule achieves 13.5 FID, showing the effectiveness of concentrating probability around log SNR = 0
- Cosine Shifted and Cosine Scaled variants show modest improvements over baseline Cosine schedule
- The superiority of mid-range noise level focus is consistently demonstrated across different prediction targets and CFG scales

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing sampling frequency around log SNR = 0 improves training efficiency
- **Mechanism:** The diffusion model needs to learn the transition from signal dominance to noise dominance, which occurs at log SNR = 0. By concentrating sampling probability around this critical point, the model receives more training signals for this challenging transition.
- **Core assumption:** The middle range of noise levels is the most difficult and important for the model to learn
- **Evidence anchors:**
  - [abstract]: "increasing the sample frequency around log SNR = 0, the critical transition point between signal dominance and noise dominance, can lead to more robust and accurate predictions"
  - [section]: "Our empirical analysis demonstrates that allocating more computation costs (FLOPs) to mid-range noise levels (around log SNR = 0) yields superior performance compared to increasing loss weights during the same period"
- **Break condition:** If the model architecture cannot effectively learn from the increased mid-range sampling, or if other noise ranges become more critical than assumed

### Mechanism 2
- **Claim:** Modifying noise schedule is more effective than adjusting loss weights for improving training efficiency
- **Mechanism:** The paper demonstrates that reallocating computational resources by modifying the noise schedule (changing p(λ)) produces better results than simply adjusting loss weights (changing w(λ)). This is because the noise schedule directly controls how much computational effort is allocated to different noise levels during training.
- **Core assumption:** Direct allocation of computation resources is more effective than indirect weighting adjustments
- **Evidence anchors:**
  - [section]: "Although adjusting w(λ) is theoretically equivalent to altering p(λ). In practical training, directly modifying p(λ) to concentrate computational resources on training specific noise levels is more effective than enlarging the loss weight on specific noise levels"
  - [section]: "Our results demonstrate that noise schedules with concentrated probability density around log SNR = 0 consistently outperform alternatives, with the Laplace schedule showing particularly favorable performance"
- **Break condition:** If the relationship between computational allocation and learning efficiency doesn't hold across different model architectures or datasets

### Mechanism 3
- **Claim:** Probability density function design around log SNR = 0 is the key to effective noise schedule design
- **Mechanism:** The noise schedule can be reframed as a probability distribution design problem. Instead of specifying how noise varies with time, we can focus on how to optimally distribute sampling across different noise intensities. This framework reveals that concentrating probability density around log SNR = 0 is beneficial.
- **Core assumption:** The optimal noise schedule can be derived from probability distribution design rather than direct time-based specification
- **Evidence anchors:**
  - [section]: "Our framework provides a unified perspective for analyzing noise schedules and importance sampling, leading to a straightforward method for designing noise schedules through the identification of curves in the p(λ) distribution"
  - [section]: "We investigate four novel noise strategies, named Cosine Shifted, Cosine Scaled, Cauchy, and Laplace respectively... These strategies contain several hyperparameters, which we will explore"
- **Break condition:** If the probability distribution framework doesn't generalize to other diffusion model variants or if other distribution shapes prove more effective

## Foundational Learning

- **Concept:** Signal-to-Noise Ratio (SNR) and its logarithmic form
  - **Why needed here:** The paper uses log SNR as the primary metric for characterizing noise levels, with log SNR = 0 being the critical transition point
  - **Quick check question:** What does log SNR = 0 represent in terms of signal and noise dominance?

- **Concept:** Importance sampling and its relationship to noise schedules
  - **Why needed here:** The paper establishes that importance sampling of log SNR is theoretically equivalent to modifying the noise schedule
  - **Quick check question:** How does uniform sampling in time translate to non-uniform sampling of noise intensities?

- **Concept:** Cumulative distribution functions and inverse functions
  - **Why needed here:** The framework derives noise schedules by calculating cumulative distribution functions and then applying inverse functions
  - **Quick check question:** What mathematical relationship allows us to derive a noise schedule from a probability distribution?

## Architecture Onboarding

- **Component map:** DiT-B with AdaLN -> Noise schedule (Laplace/Cauchy) -> t sampling -> log SNR mapping -> noise addition -> prediction -> loss computation -> weight update

- **Critical path:**
  1. Define noise schedule function (Laplace, Cauchy, etc.)
  2. Convert to probability density function p(λ)
  3. Sample timesteps from uniform distribution
  4. Map timesteps to log SNR values using noise schedule
  5. Add noise to inputs and compute predictions
  6. Calculate loss with appropriate weighting
  7. Update model parameters

- **Design tradeoffs:**
  - Concentrating probability around log SNR = 0 improves mid-range learning but may reduce training on extreme noise levels
  - More complex noise schedules (Laplace, Cauchy) vs simpler ones (Cosine)
  - Training efficiency vs final generation quality
  - Computational budget allocation across different noise ranges

- **Failure signatures:**
  - Poor generation quality at extreme noise levels
  - Mode collapse or lack of diversity in generated samples
  - Training instability or slow convergence
  - Degradation in performance when increasing CFG scale

- **First 3 experiments:**
  1. Implement basic Laplace noise schedule with b=0.5 and compare against baseline Cosine schedule on ImageNet-256
  2. Test different scale parameters (b values) for Laplace distribution to find optimal hyperparameter
  3. Compare Laplace schedule performance across different prediction targets (x0, v, ϵ) to verify robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation to single architecture (DiT-B) and dataset (ImageNet)
- Theoretical mechanisms for why log SNR = 0 is critical remain unproven
- No analysis of computational efficiency gains relative to quality improvements

## Confidence
**High Confidence Claims:**
- The Laplace noise schedule with b=0.5 consistently outperforms the baseline Cosine schedule on ImageNet-256 with the tested DiT-B architecture
- Modifying noise schedules is more effective than adjusting loss weights for the same computational budget in the tested setting
- The framework for analyzing noise schedules through probability density functions provides useful insights for schedule design

**Medium Confidence Claims:**
- The superiority of mid-range noise level focus generalizes beyond the tested conditions
- The specific Laplace distribution shape is optimal rather than just effective
- The computational efficiency gains translate to similar quality improvements with reduced training time

**Low Confidence Claims:**
- The proposed mechanisms fully explain why the Laplace schedule works
- The framework applies equally well to non-image domains or autoregressive diffusion models
- The optimal b parameter (0.5) is universal across different architectures and tasks

## Next Checks
1. **Cross-Architecture Validation**: Test the Laplace noise schedule (b=0.5) on a different diffusion architecture (e.g., DDPM, EDM) and dataset (e.g., CIFAR-10, LSUN) to verify generalizability of the performance gains.

2. **Extreme Noise Level Performance**: Conduct ablation studies measuring generation quality specifically at very low and very high noise levels to determine if concentrating probability around log SNR = 0 comes at the cost of poor performance in extreme ranges.

3. **Alternative Distribution Shapes**: Systematically test other probability distributions (Gaussian, Exponential, Beta) with concentrated density around log SNR = 0 to determine if the Laplace shape is uniquely optimal or if the concentration property is the key factor.