---
ver: rpa2
title: Bias in Text Embedding Models
arxiv_id: '2406.12138'
source_url: https://arxiv.org/abs/2406.12138
tags:
- bias
- embedding
- text
- more
- associations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes gender bias in nine popular text embedding\
  \ models by examining how each model associates a set of professions with gendered\
  \ terms like \u201Cman/woman\u201D and \u201Cboy/girl.\u201D The analysis shows\
  \ that all models exhibit gender bias, but the magnitude and direction of bias vary\
  \ significantly across models. Some models strongly associate care-related professions\
  \ (e.g., nurse, homemaker) with female identifiers and leadership roles (e.g., CEO,\
  \ manager) with male identifiers, while others show opposite or inconsistent patterns."
---

# Bias in Text Embedding Models

## Quick Facts
- **arXiv ID:** 2406.12138
- **Source URL:** https://arxiv.org/abs/2406.12138
- **Reference count:** 40
- **Primary result:** Analysis of gender bias in nine text embedding models reveals varying magnitudes and directions of bias across models

## Executive Summary
This paper examines gender bias in nine popular text embedding models by analyzing how each model associates professions with gendered terms like "man/woman" and "boy/girl." The study finds that all models exhibit gender bias, but the patterns and strengths of these biases vary significantly. Some models strongly associate care-related professions with female identifiers and leadership roles with male identifiers, while others show opposite or inconsistent patterns. The research demonstrates that bias is sensitive to the specific words used as prompts, highlighting the need for businesses to carefully evaluate model bias before deployment.

## Method Summary
The authors analyzed nine text embedding models by examining their associations between a set of professions and gendered terms. They measured the cosine similarity between profession vectors and gendered term vectors to quantify bias. The analysis covered various professions including care-related roles (nurse, homemaker) and leadership positions (CEO, manager). By systematically varying the gendered terms used as prompts, the researchers could assess how bias patterns changed based on different word choices.

## Key Results
- All nine examined models exhibit gender bias, but the magnitude and direction of bias vary significantly across models
- Some models strongly associate care-related professions with female identifiers and leadership roles with male identifiers
- Bias patterns are sensitive to the specific words used as prompts, with different gendered terms producing different association patterns

## Why This Works (Mechanism)
The mechanism behind text embedding bias stems from the distributional hypothesis in natural language processing, where words appearing in similar contexts are represented by similar vectors. These models learn from vast corpora of text that contain societal biases, stereotypes, and historical inequalities. When the training data reflects gender stereotypes (e.g., more male CEOs mentioned than female CEOs, more female nurses than male nurses), the embeddings naturally capture these patterns. The cosine similarity measurements reveal how closely the vector representations of professions align with gendered terms, exposing the embedded biases that have been learned from real-world data distributions.

## Foundational Learning

**Word Embeddings**
- *Why needed:* Understanding how words are represented as dense vectors in high-dimensional space
- *Quick check:* Can you explain how "king - man + woman = queen" demonstrates semantic relationships?

**Cosine Similarity**
- *Why needed:* The primary metric for measuring vector relationships in embedding space
- *Quick check:* Do you understand why cosine similarity (rather than Euclidean distance) is preferred for comparing word embeddings?

**Distributional Hypothesis**
- *Why needed:* The theoretical foundation explaining why embeddings capture semantic relationships
- *Quick check:* Can you explain why "You shall know a word by the company it keeps" is fundamental to how embeddings work?

## Architecture Onboarding

**Component Map**
Text corpus -> Tokenization -> Embedding layer -> Contextual processing (if applicable) -> Final embedding vectors -> Bias analysis pipeline

**Critical Path**
Raw text -> Preprocessing -> Model inference -> Vector extraction -> Cosine similarity computation -> Bias quantification

**Design Tradeoffs**
Static vs. contextual embeddings (BERT-style models capture context but are computationally expensive); dimensionality vs. representation quality; training data size vs. bias magnification

**Failure Signatures**
Overly strong associations between professions and gendered terms; inconsistent bias patterns across similar professions; sensitivity to minor wording changes in prompts

**3 First Experiments**
1. Measure cosine similarity between "doctor" and "man" vs. "woman" vectors
2. Compare embeddings of "nurse" across different models
3. Test bias sensitivity by varying prompt formulations (e.g., "male doctor" vs. "man doctor")

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to gender bias using specific professions and gendered terms, potentially missing other forms of bias
- Static association approach may not capture contextual nuances or real-world usage patterns
- Study does not explore intersectional biases combining gender with other demographic factors

## Confidence

**High Confidence:** All nine models exhibit gender bias; bias varies in magnitude and direction across models

**Medium Confidence:** Businesses should evaluate model bias before deployment; specific gendered association patterns are documented

**Low Confidence:** Bias is highly sensitive to specific prompt words (limited validation with systematic prompt variation)

## Next Checks

1. Replicate with expanded profession and gender term sets including non-binary and culturally specific identifiers

2. Test intersectional bias by examining how gender bias interacts with race, age, or nationality factors

3. Evaluate debiasing interventions by applying techniques like counterfactual data augmentation and measuring bias reduction