---
ver: rpa2
title: Design Requirements for Human-Centered Graph Neural Network Explanations
arxiv_id: '2405.06917'
source_url: https://arxiv.org/abs/2405.06917
tags:
- explanations
- graph
- human-centered
- domain
- design
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a gap in the design of human-centered explanations
  for Graph Neural Networks (GNNs), which are increasingly used in domains like healthcare
  and security but lack transparency. The authors propose a set of design requirements
  for human-centered GNN explanations, focusing on visualization, interaction, and
  collaboration needs.
---

# Design Requirements for Human-Centered Graph Neural Network Explanations

## Quick Facts
- arXiv ID: 2405.06917
- Source URL: https://arxiv.org/abs/2405.06917
- Reference count: 37
- Primary result: Proposes design requirements for human-centered GNN explanations with 2D and 3D VR prototypes

## Executive Summary
This paper addresses the gap in transparent Graph Neural Network (GNN) explanations, which are increasingly used in healthcare and security but lack interpretability for non-expert users. The authors propose a framework of design requirements for human-centered GNN explanations, focusing on visualization, interaction, and collaboration needs across different user types. They demonstrate these requirements through two prototypes: a 2D interface for Chicago neighborhood data and an immersive 3D VR interface for interactive graph exploration.

## Method Summary
The paper identifies design requirements for human-centered GNN explanations through analysis of visualization, interaction, and collaboration needs. The methodology involves proposing requirements for different user types (AI experts vs domain experts), adapting information visualization interaction patterns to graph data, and demonstrating these concepts through prototype implementations. The 2D interface provides basic visualization of GNN explanations, while the 3D VR interface enables spatial interaction with graph structures for enhanced exploration and comparison.

## Key Results
- Proposes comprehensive design requirements for human-centered GNN explanations
- Demonstrates prototype 2D interface for visualizing GNN explanations on graph data
- Develops immersive 3D VR interface enabling spatial interaction with graph structures
- Highlights the importance of tailoring explanations to different user types and collaboration contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The VR interface enhances user comprehension through spatial interaction
- Mechanism: Immersive 3D visualization allows users to explore and manipulate graph structures intuitively
- Core assumption: Users benefit from spatial manipulation and depth perception for complex graph analysis
- Evidence anchors: Abstract mentions VR enhances interaction with graph data; section discusses synchronous collaboration affordances
- Break condition: Motion sickness or cumbersome VR setup may negate spatial advantages

### Mechanism 2
- Claim: Design requirements framework bridges technical explanations and user comprehension
- Mechanism: Explicit user categorization (AI experts vs domain experts) enables targeted interface design
- Core assumption: Different user types have distinct explanation needs addressable through tailored design
- Evidence anchors: Section discusses user types based on application domain and responsibilities
- Break condition: User categories may be too rigid to capture nuanced actual user needs

### Mechanism 3
- Claim: Interaction requirements enable deeper explanation exploration than previous approaches
- Mechanism: Adapting information visualization principles to graph data enables sophisticated analysis
- Core assumption: Low-level interaction patterns from general visualization can be adapted for GNNs
- Evidence anchors: Section provides low-level interaction requirements adapted from visualization literature
- Break condition: Adapted patterns may be insufficient for unique graph data representation challenges

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: Understanding the paper's focus on GNN explanations requires basic knowledge of how GNNs process graph-structured data
  - Quick check question: What is the key difference between how GNNs and traditional neural networks handle data?

- Concept: Human-Centered Explainable AI (HCXAI)
  - Why needed here: The paper's contribution centers on applying HCXAI principles specifically to GNN explanations
  - Quick check question: How does HCXAI differ from traditional approaches to explainable AI?

- Concept: Graph visualization challenges
  - Why needed here: The paper identifies specific perceptual and cognitive challenges in visualizing GNN explanations that inform design requirements
  - Quick check question: What are the three main challenges identified for visualizing subgraph explanations?

## Architecture Onboarding

- Component map: Data preprocessing -> GNN model training -> GNNExplainer explanation generation -> 2D/3D visualization rendering -> User interaction handlers
- Critical path: User selects node → explanation generation → visualization rendering → user interaction → feedback collection
- Design tradeoffs: 2D vs 3D visualization (accessibility vs depth), synchronous vs asynchronous collaboration (real-time interaction vs flexibility), technical accuracy vs user comprehension
- Failure signatures: Users unable to identify important nodes/edges, confusion about explanation weights, excessive cognitive load, technical barriers to VR adoption
- First 3 experiments:
  1. Implement basic 2D visualization with node selection and edge highlighting based on explanation weights
  2. Add interactive rotation and scaling to 2D interface to simulate basic 3D exploration
  3. Integrate VR headset support with basic graph rendering and node selection capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different user groups perceive and interact with GNN explanations in real-world scenarios?
- Basis in paper: [explicit] Discusses need for human-centered explanations and mentions different user groups but lacks empirical evidence
- Why unresolved: No thorough evaluation with diverse user groups in real-world settings
- What evidence would resolve it: User studies with AI experts, domain experts, and decision-makers measuring understanding, trust, and collaboration outcomes

### Open Question 2
- Question: What are the long-term effects of using human-centered GNN explanations on model performance and user trust?
- Basis in paper: [inferred] Emphasizes importance of explanations for understanding and refining models but doesn't explore long-term impact
- Why unresolved: No longitudinal studies or follow-up assessments provided
- What evidence would resolve it: Longitudinal studies tracking model performance and user trust levels over time with and without explanations

### Open Question 3
- Question: How can Large Language Models be integrated into human-centered GNN explanations to enhance interpretability and usability?
- Basis in paper: [explicit] Mentions potential of LLMs to enhance explanations with text-based narratives but no concrete implementation
- Why unresolved: No prototype or study demonstrating LLM integration with GNN explanations
- What evidence would resolve it: Prototype integrating LLMs with GNN explanations followed by user studies evaluating combined system

## Limitations
- Weak empirical evidence supporting VR interface benefits and user categorization framework
- Prototypes appear to be demonstrations rather than validated solutions
- Many claims about user benefits remain untested with actual users
- Lack of longitudinal studies on long-term effects of explanations

## Confidence
- VR interface benefits: Low confidence (weak empirical support)
- User categorization framework: Medium confidence (logical but untested)
- Interaction requirements adaptation: Low confidence (interesting hypothesis lacking corpus support)

## Next Checks
1. Conduct user studies comparing 2D and VR interfaces for GNN explanation comprehension, measuring both task performance and cognitive load
2. Test the user categorization framework with actual domain experts to validate whether proposed distinctions capture real user needs
3. Evaluate adapted interaction patterns through controlled experiments to determine if they effectively address graph-specific visualization challenges identified in the paper