---
ver: rpa2
title: 'Poisoned LangChain: Jailbreak LLMs by LangChain'
arxiv_id: '2406.18122'
source_url: https://arxiv.org/abs/2406.18122
tags:
- jailbreak
- large
- language
- attacks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method called Poisoned-LangChain
  (PLC) that exploits Retrieval-Augmented Generation (RAG) systems to perform indirect
  jailbreak attacks on large language models (LLMs). PLC achieves this by poisoning
  external knowledge bases used by RAG, crafting keyword triggers, and creating malicious
  prompts that bypass LLM safety mechanisms.
---

# Poisoned LangChain: Jailbreak LLMs by LangChain

## Quick Facts
- arXiv ID: 2406.18122
- Source URL: https://arxiv.org/abs/2406.18122
- Authors: Ziqiu Wang; Jun Liu; Shengkai Zhang; Yang Yang
- Reference count: 32
- This paper introduces Poisoned-LangChain (PLC), a method that exploits Retrieval-Augmented Generation (RAG) systems to perform indirect jailbreak attacks on large language models (LLMs) by poisoning external knowledge bases.

## Executive Summary
This paper introduces a novel method called Poisoned-LangChain (PLC) that exploits Retrieval-Augmented Generation (RAG) systems to perform indirect jailbreak attacks on large language models (LLMs). PLC achieves this by poisoning external knowledge bases used by RAG, crafting keyword triggers, and creating malicious prompts that bypass LLM safety mechanisms. Experiments on six Chinese LLMs across three jailbreak scenarios demonstrate high success rates: 88.56% for inciting dangerous behavior, 79.04% for misuse of chemicals, and 82.69% for illegal discrimination. The method is particularly effective against models with strong filtering capabilities, achieving significantly higher success rates than direct jailbreak attacks. This work highlights a new attack vector that exploits the integration of external knowledge sources in modern LLM applications, emphasizing the need for enhanced security measures in RAG systems.

## Method Summary
PLC is an indirect jailbreak attack method that exploits RAG systems by poisoning external knowledge bases. The attack involves three main steps: first, gathering malicious content and disguising it through role-playing and encoding techniques (including conversion to PDF format to evade keyword filtering); second, crafting specific keyword triggers and malicious prompts; and third, uploading the poisoned content to the knowledge base and testing jailbreak effectiveness. The method was tested on six Chinese LLMs (ChatGLM2-6B, ChatGLM3-6B, Xinghuo-3.5, Qwen-14B-Chat, Ernie-3.5, Llama2-7B) across three attack scenarios measuring Attack Success Rate (ASR).

## Key Results
- PLC achieved 88.56% success rate for inciting dangerous behavior attacks
- PLC achieved 79.04% success rate for misuse of chemicals attacks
- PLC achieved 82.69% success rate for illegal discrimination attacks
- PLC was particularly effective against models with strong filtering capabilities, outperforming direct jailbreak attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PLC bypasses LLM safety filters by injecting malicious content into external RAG knowledge bases, exploiting the lack of scrutiny during retrieval.
- Mechanism: The poisoned knowledge base contains disguised jailbreak content that is retrieved and used by the LLM via LangChain. When a user query triggers specific keywords, the searcher retrieves the malicious content, which is then combined with the user prompt and fed to the LLM. The LLM, lacking direct visibility into the malicious origin, generates unsafe responses based on this poisoned context.
- Core assumption: The LLM's safety mechanisms do not effectively inspect or filter the content retrieved from external knowledge bases during the RAG process.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 2
- Claim: PLC exploits the integration of external knowledge sources in modern LLM applications, particularly those using LangChain, to achieve indirect jailbreak attacks.
- Mechanism: By converting malicious text files into PDF format and embedding them in the knowledge base, PLC evades keyword-based filtering during the embedding process. The PDF format is processed as complete word vector embeddings, making the malicious content less likely to be blocked. When specific keywords are triggered, the searcher retrieves the corresponding harmful content, leading to jailbreak behavior.
- Core assumption: LangChain processes PDF files differently than text files, allowing malicious content to bypass keyword-based filtering.
- Evidence anchors: [abstract], [section], [corpus]

### Mechanism 3
- Claim: PLC achieves higher success rates against models with strong filtering capabilities by leveraging the model's reliance on external knowledge for real-time updates and iterations.
- Mechanism: Models with strong safety filters are less susceptible to direct jailbreak attacks. However, by poisoning the external knowledge base used for RAG, PLC introduces malicious content that the model trusts as legitimate information. The model, aiming to provide accurate and contextually relevant responses, incorporates this poisoned knowledge, leading to unsafe outputs.
- Core assumption: Models with strong filtering capabilities still rely heavily on external knowledge sources for real-time updates, and this reliance can be exploited.
- Evidence anchors: [abstract], [section], [corpus]

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: Understanding RAG is crucial because PLC exploits the integration of external knowledge bases in RAG systems to perform indirect jailbreak attacks.
  - Quick check question: How does RAG enhance LLM outputs, and what are the potential security vulnerabilities introduced by this approach?

- Concept: LangChain Framework
  - Why needed here: LangChain is the tool used to build the RAG system in PLC. Understanding its components and workflow is essential for comprehending how the poisoning attack is implemented.
  - Quick check question: What are the three integral components of LangChain, and how do they interact to facilitate the RAG process?

- Concept: Jailbreak Attacks
  - Why needed here: PLC is a novel method of indirect jailbreak attacks. Understanding the principles and mechanisms of jailbreak attacks is necessary to grasp how PLC circumvents LLM safety mechanisms.
  - Quick check question: What are the typical methods used in direct jailbreak attacks, and how does PLC differ from these approaches?

## Architecture Onboarding

- Component map: Knowledge Base -> Searcher -> LLM -> LangChain
- Critical path: 1. User query received by LangChain application 2. Searcher queries poisoned knowledge base based on trigger keywords 3. Malicious content retrieved and combined with user query 4. Enhanced prompt fed to LLM 5. LLM generates response based on poisoned context
- Design tradeoffs: Effectiveness vs. Evasion (PDF format increases evasion but may reduce keyword matching precision), Complexity vs. Success Rate (encoding techniques increase complexity but may improve success rates), Realism vs. Control (real-world application enhances realism but introduces uncontrollable variables)
- Failure signatures: Low success rates across all tested models and scenarios, inconsistent jailbreak behavior with some queries blocked or producing safe responses, detection of poisoned knowledge base by security mechanisms
- First 3 experiments: 1. Test effectiveness of different file formats (PDF, TXT, DOCX) in evading keyword-based filtering 2. Evaluate impact of various encoding techniques (Morse code, Base64) on success rates 3. Assess performance against models with varying filtering capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are PLC attacks against closed-source, commercially deployed LLMs that employ advanced monitoring and filtering mechanisms?
- Basis in paper: [explicit] The authors note that their method is particularly effective against models with strong filtering capabilities, but they do not test it on closed-source, commercially deployed models.
- Why unresolved: The study focuses on open-source and API-accessible models, which may have different security implementations compared to fully closed-source commercial systems.
- What evidence would resolve it: Systematic testing of PLC against major commercial LLMs like GPT-4, Claude, or Gemini with detailed reporting of success rates and defensive countermeasures.

### Open Question 2
- Question: Can models be trained to detect and neutralize PLC attacks without significantly compromising their general utility and performance?
- Basis in paper: [inferred] The authors demonstrate that PLC attacks can bypass existing safety mechanisms, implying a need for improved defenses, but do not explore potential solutions.
- Why unresolved: The paper focuses on demonstrating the vulnerability rather than developing or testing defensive strategies.
- What evidence would resolve it: Development and evaluation of detection algorithms or training techniques that can identify PLC-style attacks while maintaining model performance on legitimate tasks.

### Open Question 3
- Question: What is the minimum knowledge base size and quality required for PLC attacks to be effective against various LLM architectures?
- Basis in paper: [explicit] The authors use carefully crafted knowledge bases but do not systematically investigate the relationship between knowledge base characteristics and attack success rates.
- Why unresolved: The experiments use fixed knowledge base configurations without exploring how variations in size, quality, or relevance affect attack outcomes.
- What evidence would resolve it: Controlled experiments varying knowledge base parameters (size, relevance, quality) and measuring corresponding changes in attack success rates across different model architectures.

## Limitations

- The study relies on a small set of 6 Chinese LLMs with relatively modest parameter counts (up to 14B), limiting generalizability
- Experiments use predetermined malicious content and keyword triggers, which may not represent real-world attack scenarios
- The claim about PDF format specifically evading keyword filtering needs more empirical validation across different RAG implementations

## Confidence

- **High confidence**: The core claim that RAG systems can be exploited as attack vectors through poisoned knowledge bases is well-supported
- **Medium confidence**: The specific effectiveness rates (88.56%, 79.04%, 82.69% ASR) are likely accurate for tested conditions but may not generalize
- **Low confidence**: The claim about PDF format specifically evading keyword filtering needs more empirical validation

## Next Checks

1. Test PLC effectiveness against larger, more capable models (70B+ parameters) and diverse language families beyond Chinese
2. Evaluate the robustness of PDF-based evasion tactics across different RAG systems and embedding models
3. Assess real-world attack feasibility by measuring detection rates of poisoned knowledge bases by security systems