---
ver: rpa2
title: What Are We Optimizing For? A Human-centric Evaluation of Deep Learning-based
  Movie Recommenders
arxiv_id: '2401.11632'
source_url: https://arxiv.org/abs/2401.11632
tags:
- recommender
- user
- accuracy
- systems
- transparency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates five deep learning recommender systems on
  human-centric metrics like novelty, diversity, and serendipity. Offline experiments
  on benchmark data and online user surveys with 445 participants reveal that while
  some models excel at recommending novel and unexpected items, they fall short on
  diversity, trustworthiness, transparency, accuracy, and overall user satisfaction
  compared to traditional collaborative filtering.
---

# What Are We Optimizing For? A Human-centric Evaluation of Deep Learning-based Movie Recommenders

## Quick Facts
- arXiv ID: 2401.11632
- Source URL: https://arxiv.org/abs/2401.11632
- Reference count: 40
- This study evaluates five deep learning recommender systems on human-centric metrics like novelty, diversity, and serendipity, revealing trade-offs in user satisfaction compared to traditional methods.

## Executive Summary
This paper presents a comprehensive human-centric evaluation of deep learning-based movie recommender systems, examining five different architectures across seven key metrics: novelty, diversity, serendipity, perceived accuracy, transparency, trustworthiness, and user satisfaction. Through offline benchmark testing and online user surveys with 445 participants, the study reveals that while some DL models excel at recommending novel and unexpected items, they consistently underperform on diversity, trustworthiness, transparency, accuracy, and overall satisfaction compared to traditional collaborative filtering approaches. The research identifies specific architectural trade-offs and demonstrates that excessive serendipity and low diversity negatively impact perceived transparency and personalization, ultimately reducing user satisfaction. These findings highlight the need for more balanced optimization strategies in DL-based recommender systems that go beyond accuracy-focused approaches.

## Method Summary
The study employs a two-phase evaluation framework combining offline benchmark analysis using the MovieLens-1M dataset with online user surveys. Five DL models (NCF, BERT4Rec, SSE-PT, GLocal-K, RippleNet) are evaluated on objective metrics including novelty, diversity, and serendipity, then deployed to generate personalized recommendations for 445 survey participants. Users rate their experience across seven human-centric metrics using Likert scales, enabling both quantitative comparison and qualitative thematic analysis. Structural Equation Modeling (SEM) and SHAP analysis quantify causal relationships between metrics and identify key factors influencing user satisfaction, while qualitative analysis of open-ended responses provides deeper insights into user preferences and pain points.

## Key Results
- DL models show superior performance on novelty and serendipity metrics but underperform on diversity, trustworthiness, transparency, accuracy, and overall user satisfaction
- NCF models perform well in user trust and interpretability, while sequential transformers excel at serendipity but lack transparency
- Path analysis reveals that low diversity and excessive serendipity negatively impact perceived transparency and personalization, reducing user satisfaction
- Qualitative feedback confirms users value accuracy combined with other attributes and prioritize transparency and trust

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep learning models can be systematically evaluated across human-centric metrics to reveal specific strengths and weaknesses beyond accuracy.
- Mechanism: The paper establishes a comprehensive evaluation framework combining offline benchmark metrics (novelty, diversity, serendipity) with online user surveys covering trustworthiness, transparency, accuracy, and satisfaction. By collecting both objective and subjective data, it enables direct comparison of different DL models' performance on human-centric values.
- Core assumption: Human-centric metrics can be meaningfully quantified and compared across different recommender system architectures.
- Evidence anchors:
  - [abstract] "We conduct a human-centric evaluation case study of four leading DL-RecSys models in the movie domain... We find that some DL-RecSys models to be superior in recommending novel and unexpected items and weaker in diversity, trustworthiness, transparency, accuracy, and overall user satisfaction"
  - [section] "We build a comprehensive human-centric evaluation framework consist of objective offline benchmark dataset and online real user feedback"
  - [corpus] Weak - no direct citations found for comprehensive human-centric evaluation frameworks in the provided corpus
- Break condition: If human perception of these metrics proves too subjective or inconsistent across users to enable meaningful comparison between models.

### Mechanism 2
- Claim: There are causal relationships between different human-centric metrics that impact overall user satisfaction.
- Mechanism: The paper uses Structural Equation Modeling (SEM) to identify how objective metrics (novelty, diversity, serendipity) and subjective metrics (trustworthiness, transparency) combine to affect perceived accuracy and satisfaction. Path analysis reveals that transparency and trustworthiness are more strongly associated with satisfaction than objective metrics.
- Core assumption: User satisfaction with recommender systems can be modeled as a function of both objective and subjective metric relationships.
- Evidence anchors:
  - [abstract] "Path analysis shows that low diversity and excessive serendipity in recommendations negatively impact perceived transparency and personalization, ultimately reducing user satisfaction"
  - [section] "We quantify the causal relationship between each pair of human-centric metrics and run impact factor analysis for highly correlated subjective metrics"
  - [corpus] Weak - no direct citations found for path analysis of recommender system metrics in the provided corpus
- Break condition: If the SEM model fails to explain significant variance in user satisfaction or if causal relationships prove too complex to model effectively.

### Mechanism 3
- Claim: Different DL-based recommender architectures have distinct trade-offs in human-centric performance that can guide optimization strategies.
- Mechanism: By evaluating five different DL architectures (NCF, BERT4Rec, SSE-PT, GLocal-K, RippleNet), the paper identifies which models excel at specific human-centric values. This enables targeted optimization strategies - for example, NCF models perform well on trust and interpretability but need improvement on novelty, while sequential transformers excel at serendipity but lack transparency.
- Core assumption: The architectural choices in DL models systematically influence their performance on different human-centric metrics.
- Evidence anchors:
  - [abstract] "We find that the NCF models perform well in user trust and interpretability, the sequential-based personalized transformer is the winner in serendipity, BERT-based models need improvement in perceived interest matching, and kernel-based models generate novel while non-surprising recommendations"
  - [section] "Based on that superior performance, it scores the highest among all DL models in user satisfaction"
  - [corpus] Weak - no direct citations found for architectural analysis of DL recommender systems in the provided corpus
- Break condition: If future research shows that these architectural trade-offs are not consistent across different datasets or user populations.

## Foundational Learning

- Concept: Human-centric evaluation metrics in recommender systems
  - Why needed here: The paper's core contribution is establishing a framework for evaluating recommender systems beyond traditional accuracy metrics, requiring understanding of what constitutes human-centric evaluation.
  - Quick check question: What are the seven human-centric metrics evaluated in this study?

- Concept: Structural Equation Modeling (SEM) for causal analysis
  - Why needed here: The paper uses SEM to quantify relationships between different human-centric metrics and their impact on satisfaction, requiring understanding of this statistical technique.
  - Quick check question: What are the three subgroups of metrics used in the SEM analysis?

- Concept: SHAP (SHapley Additive exPlanations) analysis
  - Why needed here: The paper applies SHAP analysis to identify which factors most influence subjective metrics like perceived accuracy and transparency, requiring understanding of this game-theoretic approach.
  - Quick check question: What are the top three contributors to user satisfaction according to the SHAP analysis?

## Architecture Onboarding

- Component map: Offline benchmark evaluation -> Online user survey collection -> Seven human-centric metrics framework -> Structural equation modeling -> SHAP analysis -> Qualitative thematic analysis

- Critical path: Data collection → Offline evaluation → Online survey deployment → Statistical analysis (SEM + SHAP) → Qualitative analysis → Model optimization recommendations

- Design tradeoffs: The paper balances objective metrics (novelty, diversity, serendipity) that can be calculated from data against subjective metrics (trustworthiness, transparency) that require human judgment, trading computational simplicity for user perception accuracy.

- Failure signatures: Low survey response rate (14.03%), inconsistent metric correlations, inability to explain satisfaction variance, or lack of convergence in model training would indicate evaluation framework weaknesses.

- First 3 experiments:
  1. Reproduce the offline novelty, diversity, and serendipity calculations for a subset of the ML-1M dataset to verify metric implementations
  2. Run the SEM analysis on synthetic data with known relationships to validate the modeling approach
  3. Implement the SHAP analysis pipeline on a simplified version of the user survey data to confirm impact factor identification works as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific user interface or interaction design features can improve perceived transparency and trust in DL-based recommender systems?
- Basis in paper: [explicit] The paper identifies a strong correlation between perceived transparency, trust, and user satisfaction, and notes that users desire more context and control over recommendations. It also mentions the potential for explainable UIs to improve interpretability.
- Why unresolved: While the paper identifies the importance of transparency and trust, it does not delve into specific UI/UX design elements that could enhance these aspects. This remains an open area for research and experimentation.
- What evidence would resolve it: Conducting user studies with different UI/UX designs for DL-based recommender systems, measuring user perceptions of transparency and trust, and identifying the most effective design elements.

### Open Question 2
- Question: How do the ethical implications of DL-based recommender systems differ across different application domains (e.g., movies, music, news, e-commerce)?
- Basis in paper: [inferred] The paper focuses on DL-based recommender systems in the movie domain, but ethical concerns like fairness, privacy, and transparency are likely to vary across different application contexts.
- Why unresolved: The paper does not explore the ethical implications of DL-based recommender systems beyond the movie domain. Different domains may have unique ethical considerations that require tailored approaches.
- What evidence would resolve it: Conducting cross-domain studies comparing the ethical implications of DL-based recommender systems in various application areas, identifying domain-specific challenges and best practices.

### Open Question 3
- Question: How can DL-based recommender systems be optimized to balance accuracy with other human-centric metrics like novelty, diversity, and serendipity, while maintaining user satisfaction?
- Basis in paper: [explicit] The paper highlights the need for a balance between accuracy and other human-centric metrics, and suggests that optimizing for a single metric may not lead to optimal user satisfaction. It also identifies the trade-offs between different metrics, such as the potential negative impact of high serendipity on trust.
- Why unresolved: The paper does not provide specific strategies or algorithms for optimizing DL-based recommender systems to achieve this balance. It remains an open challenge to develop models that can effectively weigh and optimize multiple metrics simultaneously.
- What evidence would resolve it: Developing and testing new optimization algorithms or frameworks for DL-based recommender systems that explicitly consider the trade-offs between accuracy and other human-centric metrics, and evaluating their impact on user satisfaction.

## Limitations

- Low survey response rate (14.03%) may introduce selection bias and limit generalizability of subjective metrics
- Single movie domain dataset constrains external validity across different recommendation contexts
- Causal relationships identified through SEM may be influenced by unobserved confounding variables from correlational data

## Confidence

**High Confidence**: Claims about relative performance of specific DL models on objective metrics (novelty, diversity, serendipity) are supported by reproducible offline calculations and consistent across multiple models.

**Medium Confidence**: Findings regarding causal relationships between metrics affecting user satisfaction, as these depend on SEM assumptions and the quality of survey responses.

**Low Confidence**: Generalization of architectural trade-offs across different recommendation domains and user populations, given the single-domain focus.

## Next Checks

1. Conduct cross-domain validation by replicating the evaluation framework on non-movie datasets (e.g., book, music, or e-commerce domains) to test the robustness of architectural trade-offs.

2. Implement A/B testing experiments where specific metric targets (e.g., diversity thresholds) are manipulated to directly observe causal effects on user satisfaction rather than relying solely on observational data.

3. Perform inter-rater reliability analysis on survey responses to quantify agreement between users on subjective metrics and identify potential bias sources in the collected data.