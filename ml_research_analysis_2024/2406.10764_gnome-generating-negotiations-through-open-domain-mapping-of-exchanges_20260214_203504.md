---
ver: rpa2
title: 'GNOME: Generating Negotiations through Open-Domain Mapping of Exchanges'
arxiv_id: '2406.10764'
source_url: https://arxiv.org/abs/2406.10764
tags:
- negotiation
- datasets
- dataset
- gnome
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the issue of poor generalization of negotiation
  strategy prediction models trained on closed-domain datasets, limiting their effectiveness
  in real-world, diverse negotiation scenarios. The authors propose GNOME, an automated
  framework that leverages existing human-annotated, closed-domain datasets and Large
  Language Models (LLMs) to generate synthetic open-domain negotiation dialogues.
---

# GNOME: Generating Negotiations through Open-Domain Mapping of Exchanges

## Quick Facts
- arXiv ID: 2406.10764
- Source URL: https://arxiv.org/abs/2406.10764
- Reference count: 11
- Primary result: Framework generates synthetic open-domain negotiation dialogues that improve model generalization across domains

## Executive Summary
This paper addresses the critical challenge of poor generalization in negotiation strategy prediction models that are typically trained on closed-domain datasets. The authors propose GNOME, an automated framework that leverages existing human-annotated, closed-domain datasets and Large Language Models (LLMs) to generate synthetic open-domain negotiation dialogues. GNOME employs a three-stage process—pre-processing (filtering, label mapping, and skew correction), domain mapping (using an LLM to transform dialogues into diverse scenarios), and post-processing (refining the generated data)—to create the GNOME dataset. The framework significantly improves both the performance and generalizability of negotiation models, with models trained on GNOME data showing up to 42.93% increase in F1 scores and better generalization to unseen domains compared to models trained on original datasets. Human evaluations confirm the high quality of the synthetic data, with high scores for coherence and structural similarity.

## Method Summary
GNOME is an automated framework that generates synthetic open-domain negotiation dialogues by leveraging existing closed-domain datasets and LLMs. The process begins with pre-processing, where dialogues are filtered, labels are mapped, and class skew is corrected. In the domain mapping stage, an LLM transforms the dialogues into diverse scenarios across multiple domains. The post-processing stage refines the generated data to ensure quality. The GNOME dataset, created using this framework, enables negotiation models to achieve significantly improved task-specific performance and better generalization to unseen domains compared to models trained on original datasets.

## Key Results
- Models trained on GNOME data show up to 42.93% increase in F1 scores for task-specific performance
- GNOME-trained models demonstrate improved generalization to unseen domains compared to models trained on original datasets
- Human evaluations confirm high quality of synthetic data with strong scores for coherence and structural similarity

## Why This Works (Mechanism)
The GNOME framework works by systematically transforming limited, domain-specific negotiation data into diverse, open-domain dialogues that expose models to a wider variety of negotiation scenarios. The pre-processing stage ensures data quality and balance, while the domain mapping leverages LLMs' ability to understand and generate contextually appropriate negotiations across different domains. This approach addresses the fundamental limitation of closed-domain datasets by creating synthetic data that captures the variability and complexity of real-world negotiations. The post-processing refinement ensures that the generated dialogues maintain coherence and relevance, making the synthetic data suitable for training robust negotiation models.

## Foundational Learning
- **Closed-domain datasets**: Limited negotiation data from specific scenarios that models overfit to; needed because they provide the initial annotated data for transformation
- **Large Language Models (LLMs)**: AI models capable of understanding and generating human-like text across domains; needed because they enable domain transformation of negotiation dialogues
- **Domain mapping**: Process of converting dialogues from one domain to another while preserving negotiation dynamics; needed because it creates the diversity required for open-domain generalization
- **Data skew correction**: Balancing class distributions in training data; needed because imbalanced data can bias model predictions toward majority classes
- **Synthetic data generation**: Creating artificial training examples from existing data; needed because it expands the diversity of training scenarios beyond original dataset limitations

## Architecture Onboarding

**Component map**: Pre-processing -> Domain Mapping (LLM) -> Post-processing -> GNOME Dataset -> Model Training

**Critical path**: The most critical path is the domain mapping stage, where the LLM transforms dialogues into diverse scenarios. This stage determines the quality and diversity of the generated data, which directly impacts model performance and generalization.

**Design tradeoffs**: The framework trades computational cost (LLM inference for domain mapping) for improved generalization. Using synthetic data avoids the need for expensive human annotation across multiple domains but introduces potential LLM biases. The three-stage pipeline adds complexity but enables systematic quality control at each step.

**Failure signatures**: Poor quality synthetic data manifests as unrealistic negotiation dynamics, domain-inconsistent language, or loss of original negotiation strategies. Overfitting to synthetic patterns rather than learning generalizable negotiation skills indicates issues in the domain mapping or post-processing stages.

**Three first experiments**:
1. Train a baseline negotiation model on original closed-domain dataset and evaluate on held-out domains
2. Generate GNOME data from the same source dataset and train a model on GNOME data, comparing performance on held-out domains
3. Perform ablation studies by training models on data from individual stages (pre-processed, domain-mapped only, post-processed) to identify which stage contributes most to performance gains

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on synthetic data quality assessments and controlled experiments with limited real-world deployment testing
- Framework's dependence on LLMs for domain mapping introduces potential biases and inconsistencies not fully captured in current evaluation
- Methodology's effectiveness depends heavily on quality and diversity of source closed-domain datasets, with potential propagation of inherent biases

## Confidence
- Claim: GNOME significantly improves generalization to unseen domains - Medium confidence
- Claim: Synthetic data maintains high quality according to human evaluations - Medium confidence
- Claim: Performance improvements are substantial and consistent - Medium confidence

## Next Checks
1. Conduct longitudinal studies testing negotiation models trained on GNOME data in real-world negotiation scenarios across multiple domains to validate generalization claims
2. Perform ablation studies to quantify individual contributions of each stage in the GNOME pipeline to identify potential bottlenecks or failure points
3. Test framework's robustness by attempting to generate negotiation data in domains completely absent from source datasets to evaluate true zero-shot generalization capabilities