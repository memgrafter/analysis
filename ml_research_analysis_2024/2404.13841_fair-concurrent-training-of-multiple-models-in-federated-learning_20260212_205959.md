---
ver: rpa2
title: Fair Concurrent Training of Multiple Models in Federated Learning
arxiv_id: '2404.13841'
source_url: https://arxiv.org/abs/2404.13841
tags:
- tasks
- task
- clients
- training
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness in training multiple federated learning
  models (MMFL) simultaneously. Current MMFL approaches use naive allocation schemes
  that can lead to unfair performance when models have heterogeneous difficulty levels,
  with some models converging much slower or to lower accuracies.
---

# Fair Concurrent Training of Multiple Models in Federated Learning

## Quick Facts
- arXiv ID: 2404.13841
- Source URL: https://arxiv.org/abs/2404.13841
- Authors: Marie Siew; Haoran Zhang; Jong-Ik Park; Yuezhou Liu; Yichen Ruan; Lili Su; Stratis Ioannidis; Edmund Yeh; Carlee Joe-Wong
- Reference count: 40
- Key outcome: Proposes FedFairMMFL, a difficulty-aware algorithm that dynamically allocates clients to tasks based on current performance levels to improve fairness across multiple federated learning models.

## Executive Summary
This paper addresses fairness in training multiple federated learning models (MMFL) simultaneously. Current MMFL approaches use naive allocation schemes that can lead to unfair performance when models have heterogeneous difficulty levels, with some models converging much slower or to lower accuracies. Additionally, clients may be unwilling to train certain models due to high computational costs, further exacerbating unfairness. The authors propose FedFairMMFL, a difficulty-aware algorithm that dynamically allocates clients to tasks based on current performance levels, and design auction mechanisms to incentivize clients to train multiple tasks fairly.

## Method Summary
FedFairMMFL uses α-fairness metrics to balance fairness and average accuracy across tasks. The algorithm dynamically allocates clients to tasks based on current task losses, with tasks having higher losses receiving higher allocation probability. The α parameter controls the emphasis on fairness, with higher α values putting more emphasis on fairness. The authors also propose auction mechanisms, including Budget-Fairness and Max-min Fairness approaches, to incentivize clients to train multiple tasks fairly. The max-min fair auction ensures a fair distribution of incentivized clients across tasks by allocating budget in rounds to maximize the minimum number of clients per task.

## Key Results
- FedFairMMFL achieves higher minimum accuracy across tasks compared to random allocation, round robin, and client-fairness baselines, while maintaining average accuracy.
- The algorithm demonstrates faster convergence for more difficult tasks.
- The max-min fair auction further improves minimum accuracy in budget-constrained scenarios by ensuring a fair distribution of incentivized clients across tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedFairMMFL improves fairness across tasks by dynamically allocating more clients to tasks with higher current losses.
- Mechanism: The algorithm uses an α-fairness metric to compute client-task allocation probabilities proportional to the α-th power of task losses. Tasks with higher losses receive higher allocation probability, accelerating their convergence.
- Core assumption: The current task loss (or accuracy) is a reliable signal of task difficulty and training need.
- Evidence anchors:
  - [abstract] "We address both challenges by firstly designing FedFairMMFL, a difficulty-aware algorithm that dynamically allocates clients to tasks in each training round."
  - [section] "Instead, we will assign each client to contribute to the gradient ∇Fk,s(ws) for each task s at a probability in proportion to its weight for that task, i.e., proportional to αfs(ws)α−1."
  - [corpus] Weak evidence - no direct neighbor papers discussing similar dynamic allocation based on task loss signals.
- Break condition: If task losses are not reliable indicators of difficulty (e.g., due to non-iid data or varying model architectures), the allocation may not improve fairness.

### Mechanism 2
- Claim: The α-fairness parameter allows tuning between average accuracy optimization (α=1) and fairness maximization (α→∞).
- Mechanism: By adjusting α, the algorithm can prioritize either overall performance or equitable performance across tasks. Higher α puts more emphasis on reducing the largest task losses.
- Core assumption: The α-fairness metric appropriately captures the fairness-performance tradeoff for MMFL.
- Evidence anchors:
  - [abstract] "We use α-fairness metrics to balance fairness and average accuracy across tasks."
  - [section] "Here the parameter α ∈ [1, ∞) controls the emphasis on fairness; for α = 1, the objective is to minimize the average global loss across tasks. As α grows, we put more emphasis on fairness."
  - [corpus] Weak evidence - no direct neighbor papers discussing α-fairness tuning in MMFL context.
- Break condition: If α-fairness does not align with the actual fairness requirements of the application, tuning may not achieve desired outcomes.

### Mechanism 3
- Claim: The max-min fair auction mechanism ensures fair client participation across tasks by distributing budget to maximize the minimum number of clients per task.
- Mechanism: The auction allocates budget in rounds, adding one client to each task until budget constraints require reallocation. This ensures no task is starved of participants.
- Core assumption: Budget constraints are the primary factor limiting client participation, and fair budget distribution leads to fair task performance.
- Evidence anchors:
  - [abstract] "We then propose a novel auction design that incentivizes clients to train multiple tasks, so as to fairly distribute clients' training efforts across the tasks."
  - [section] "We aim to maximize the minimum client take-up rate across tasks mins∈S (P i∈I xi,s)."
  - [corpus] Weak evidence - no direct neighbor papers discussing max-min fair auctions for MMFL client recruitment.
- Break condition: If other factors beyond budget (e.g., client preferences, computational constraints) significantly impact participation, the auction may not ensure fairness.

## Foundational Learning

- Concept: Federated Learning basics (client-server architecture, local training, global aggregation)
  - Why needed here: Understanding the baseline FL setting is essential before extending to multiple tasks
  - Quick check question: What is the main difference between traditional FL and MMFL?

- Concept: α-fairness metric and its properties
  - Why needed here: The algorithm uses α-fairness to balance fairness and performance across tasks
  - Quick check question: How does increasing α affect the emphasis on fairness vs average accuracy?

- Concept: Auction theory and incentive mechanisms
  - Why needed here: The paper proposes auctions to fairly incentivize client participation across tasks
  - Quick check question: What is the difference between budget-fair and max-min fair allocation?

## Architecture Onboarding

- Component map: Clients -> Local Training -> Server -> Global Aggregation -> Client Allocation
- Critical path:
  1. Client recruitment auction (if applicable)
  2. Client-task allocation based on current task performance
  3. Local training on assigned tasks
  4. Global model aggregation
  5. Performance evaluation and next round allocation

- Design tradeoffs:
  - Communication overhead vs. allocation accuracy (evaluating global loss vs. test accuracy)
  - Auction complexity vs. truthfulness guarantees
  - α parameter tuning vs. desired fairness-performance balance

- Failure signatures:
  - Persistent accuracy gaps between tasks despite dynamic allocation
  - Auction participants gaming the system (bidding untruthfully)
  - Convergence to suboptimal solutions due to biased client selection

- First 3 experiments:
  1. Run FedFairMMFL with α=1 (random allocation baseline) and compare minimum accuracy across tasks
  2. Vary α from 1 to 5 and observe the impact on minimum accuracy and variance across tasks
  3. Implement the max-min fair auction and compare client distribution across tasks with budget-fair auction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does FedFairMMFL perform under extreme client heterogeneity, such as when a large number of clients are significantly more resource-constrained than others?
- Basis in paper: [explicit] The paper mentions resource-constrained clients but does not explore extreme heterogeneity scenarios.
- Why unresolved: The current evaluation focuses on moderate levels of heterogeneity and does not address scenarios with a large disparity in client capabilities.
- What evidence would resolve it: Experiments comparing FedFairMMFL's performance under varying degrees of client heterogeneity, including extreme cases, would provide insights into its robustness.

### Open Question 2
- Question: Can the max-min fair auction mechanism be extended to handle dynamic client valuations over time, rather than assuming fixed valuations before training begins?
- Basis in paper: [inferred] The current auction design assumes static client valuations, but real-world scenarios might involve dynamic valuations due to changing client needs or resource availability.
- Why unresolved: The paper does not explore the feasibility or effectiveness of dynamic valuation adjustments within the auction mechanism.
- What evidence would resolve it: Simulation or empirical studies comparing the performance of static versus dynamic valuation auction mechanisms in MMFL settings would clarify the benefits and challenges of dynamic valuations.

### Open Question 3
- Question: How sensitive is FedFairMMFL's performance to the choice of the α parameter, and is there an optimal strategy for selecting α based on the characteristics of the tasks and clients?
- Basis in paper: [explicit] The paper uses α = 3 in experiments but does not provide a systematic method for selecting α or analyze its sensitivity.
- Why unresolved: While the paper shows that α affects the allocation of clients to tasks, it does not explore the impact of different α values on performance or provide guidance on choosing α.
- What evidence would resolve it: A comprehensive analysis of FedFairMMFL's performance across a range of α values, coupled with guidelines for selecting α based on task and client characteristics, would address this question.

## Limitations
- Limited empirical validation on real-world datasets and complex task relationships
- Auction mechanism practicality and assumptions about client behavior may not hold in practice
- Hyperparameter sensitivity, particularly to the α parameter, requires further exploration

## Confidence
- **High confidence**: The theoretical foundation of using α-fairness metrics for dynamic client-task allocation is well-established in the literature. The convergence guarantees for FedFairMMFL under certain conditions are clearly stated and supported by the experimental results showing improved minimum accuracy across tasks.
- **Medium confidence**: The practical effectiveness of the auction mechanisms in real-world scenarios has moderate confidence. While the theoretical framework is sound and the experiments show promise, the assumptions about client behavior and cost reporting may not hold in practice.
- **Low confidence**: The generalizability of results to more complex task relationships and client distributions has low confidence. The experiments use controlled task difficulty differences and simple architectures, which may not represent the complexity of real-world MMFL applications.

## Next Checks
1. **Real-world dataset validation**: Test FedFairMMFL on a real-world multi-task federated learning problem with heterogeneous client distributions and varying computational capabilities to assess practical effectiveness.
2. **Auction mechanism stress testing**: Implement the auction mechanisms in a simulated environment with strategic clients who may misreport costs and preferences to evaluate robustness against manipulation.
3. **Hyperparameter sensitivity analysis**: Conduct a comprehensive sensitivity analysis of the α parameter and other key hyperparameters across different task difficulty configurations to provide practical guidelines for parameter selection.