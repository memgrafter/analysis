---
ver: rpa2
title: 'Ask Optimal Questions: Aligning Large Language Models with Retriever''s Preference
  in Conversation'
arxiv_id: '2402.11827'
source_url: https://arxiv.org/abs/2402.11827
tags:
- question
- query
- retrieval
- rewrites
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving conversational
  search by aligning language models with retriever preferences. The core idea is
  to optimize query reformulation by leveraging feedback from retrieval systems, rather
  than relying solely on human rewrites.
---

# Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversation
## Quick Facts
- arXiv ID: 2402.11827
- Source URL: https://arxiv.org/abs/2402.11827
- Reference count: 40
- Key result: RETPO achieves SOTA on QReCC and TopiOCQA, surpassing GPT-4 in some cases

## Executive Summary
This paper addresses the challenge of improving conversational search by aligning language models with retriever preferences. The core idea is to optimize query reformulation by leveraging feedback from retrieval systems, rather than relying solely on human rewrites. The authors propose a framework called RETPO (Retriever's Preference Optimization), which involves generating multiple query rewrites using different prompting strategies, collecting retrieval performance feedback for each rewrite, and fine-tuning a smaller language model to align with the retrievers' preferences.

## Method Summary
The RETPO framework involves generating multiple query rewrites using different prompting strategies, collecting retrieval performance feedback for each rewrite, and fine-tuning a smaller language model to align with the retrievers' preferences. The authors construct a large-scale dataset called RF COLLECTION containing over 410K query rewrites across 12K conversations, along with their retrieval performance feedback. The fine-tuned model is trained to generate preferred rewrites over less preferred ones using Direct Preference Optimization (DPO).

## Key Results
- Achieves state-of-the-art performance on two conversational search benchmarks (QReCC and TopiOCQA)
- Outperforms previous rewrite-then-retrieve approaches and even surpasses GPT-4 in some cases
- Demonstrates significant improvements in retrieval performance metrics such as MRR and Recall@10

## Why This Works (Mechanism)
The method works by leveraging retrieval system feedback as a training signal for query reformulation, rather than relying on human judgments. By collecting performance metrics for multiple candidate rewrites and using DPO to fine-tune a smaller model to prefer high-performing rewrites, the system learns to generate queries that retrievers find more effective. This creates a more objective optimization target than human-written rewrites.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: A method for fine-tuning language models based on preference data; needed to align the model with retriever preferences rather than human preferences; quick check: verify that the model learns to prefer rewrites with better retrieval performance
- **Query reformulation in conversational search**: The process of rewriting context-dependent queries into standalone queries; needed to handle the challenges of multi-turn dialogues; quick check: measure improvement in retrieval performance after reformulation
- **Retrieval performance metrics (MRR, Recall@10)**: Evaluation metrics for information retrieval; needed to quantify the effectiveness of query rewrites; quick check: compare metric improvements against baseline methods

## Architecture Onboarding
- **Component map**: User query -> Multiple rewrites (via prompting strategies) -> Retriever feedback collection -> DPO fine-tuning -> Aligned query generator
- **Critical path**: Query generation → Retrieval → Feedback collection → Model fine-tuning → Inference
- **Design tradeoffs**: Using GPT-4 for rewrite generation provides high-quality candidates but introduces dependency on a specific model; the RF COLLECTION dataset is extensive but represents a single snapshot in time
- **Failure signatures**: Poor performance on queries with significant topic shifts, limited generalizability to non-conversational search tasks, degradation if retriever feedback mechanisms change
- **First experiments**: 1) Compare retrieval performance of original queries vs. GPT-4 generated rewrites, 2) Evaluate different prompting strategies for generating candidate rewrites, 3) Measure the impact of varying amounts of preference data on final model performance

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope beyond conversational search benchmarks
- Heavy reliance on GPT-4 for generating rewrites introduces potential brittleness
- Static dataset may not capture evolving conversational patterns and information needs

## Confidence
- High confidence in technical methodology and benchmark improvements
- Medium confidence in state-of-the-art claims given limited evaluation domains
- Low confidence in GPT-4 surpassing results without deeper analysis

## Next Checks
1. Evaluate RETPO performance across diverse retrieval tasks beyond conversational search, including factoid QA, document retrieval, and recommendation systems
2. Conduct ablation studies systematically removing GPT-4 from the pipeline to understand minimum viable retriever preference source
3. Implement a longitudinal study comparing retrieval performance over time with evolving information needs and updated retrieval systems