---
ver: rpa2
title: 'VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented
  Generation'
arxiv_id: '2412.10704'
source_url: https://arxiv.org/abs/2412.10704
tags:
- visual
- document
- answer
- dataset
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VisDoM addresses the challenge of question answering over multi-document
  collections with visually rich content such as tables, charts, and slides. It introduces
  VisDoMBench, the first comprehensive benchmark for multi-document, multimodal QA,
  and VisDoMRAG, a novel multimodal Retrieval-Augmented Generation approach that fuses
  visual and textual RAG pipelines using a consistency-constrained modality fusion
  mechanism.
---

# VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2412.10704
- Source URL: https://arxiv.org/abs/2412.10704
- Reference count: 40
- Primary result: Outperforms unimodal and long-context baselines by 12-20% on multimodal multi-document QA

## Executive Summary
VisDoM introduces VisDoMBench, the first comprehensive benchmark for multi-document, multimodal question answering over visually rich content like tables, charts, and slides. The system employs VisDoMRAG, a novel multimodal Retrieval-Augmented Generation approach that fuses parallel textual and visual RAG pipelines using consistency-constrained modality fusion. By employing evidence curation, chain-of-thought reasoning, and LLM-based modality fusion, VisDoMRAG achieves 12-20% improvement over unimodal and long-context baselines across various open and closed-source LLMs.

## Method Summary
VisDoM addresses multi-document QA with visually rich elements through VisDoMRAG, a multimodal RAG framework combining parallel textual and visual retrieval pipelines. The system performs independent retrieval using BGE-1.5 for text and ColQwen2 for visual content, followed by evidence curation and chain-of-thought reasoning in each modality. A consistency-constrained modality fusion mechanism using an LLM reconciles discrepancies between the two reasoning chains before generating the final answer. The approach is evaluated on VisDoMBench, a benchmark constructed from five document-QA datasets (PaperTab, FetaTab, SciGraphQA, SPIQA, SlideVQA) with query augmentation to ensure one-to-one mapping between questions and source documents.

## Key Results
- VisDoMRAG outperforms unimodal and long-context baselines by 12-20% across various open and closed-source LLMs
- Visual RAG consistently outperforms textual RAG due to the strong LLM backbone (Qwen2) in visually rich datasets
- Evidence curation and chain-of-thought reasoning are crucial for unimodal pipeline performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VisDoMRAG achieves higher accuracy by performing parallel unimodal RAG and fusing results via consistency-constrained modality alignment
- Mechanism: The system retrieves both textual chunks and visual pages independently, processes them through separate evidence curation and chain-of-thought reasoning pipelines, then uses an LLM to evaluate consistency between the reasoning chains before generating a final answer
- Core assumption: Textual and visual evidence can be independently retrieved and processed without losing semantic alignment, and discrepancies can be resolved at inference time
- Evidence anchors:
  - [abstract] "VisDoMRAG employs a multi-step reasoning process encompassing evidence curation and chain-of-thought reasoning for concurrent textual and visual RAG pipelines."
  - [section] "The system then integrates the outputs from both pipelines using modality fusion, which imposes a consistency constraint on the reasoning chains."
  - [corpus] Weak. No direct paper cited, but the approach is inspired by self-consistency in CoT reasoning (Wang et al., 2023).
- Break condition: If the LLM fails to reconcile modality inconsistencies, the final answer may inherit errors from one pipeline or produce a hallucination.

### Mechanism 2
- Claim: Textual RAG is less effective than visual RAG in visually rich datasets because text extraction from tables/charts is lossy and visually dense elements are better captured directly
- Mechanism: Visual retrieval via late-interaction models like ColPali and ColQwen2 indexes page images without OCR, allowing direct access to spatial and visual semantics; textual retrieval via BGE-1.5 processes OCR-extracted text which may miss layout cues
- Core assumption: Vision-language models encode visual structure more effectively than text-based embeddings for document layout understanding
- Evidence anchors:
  - [section] "Visual retrieval models built on top of LLMs... perform late interaction retrieval on document embeddings generated directly from document page images using Vision-Language Models (VLMs)."
  - [section] "Visual RAG consistently outperforms textual RAG... due to the presence of a strong LLM backbone (Qwen2)."
  - [corpus] Weak. No direct comparison with OCR-only baselines reported in the literature; the claim is inferred from the experimental setup.
- Break condition: If the document contains substantial narrative text and sparse visuals, textual RAG may outperform visual RAG, reversing the observed trend.

### Mechanism 3
- Claim: Query augmentation via LLM and human review ensures 1-to-1 mapping between queries and source documents, reducing ambiguity in multi-document QA
- Mechanism: Original queries are rewritten to include paper-specific metadata (title, figure caption, etc.) so that only the intended document can answer them; human annotators validate uniqueness across the corpus
- Core assumption: Including paper-specific cues in the query is sufficient to disambiguate it from similar questions in other documents
- Evidence anchors:
  - [section] "To address the challenge of ambiguous questions... we implement a query augmentation procedure to create a one-to-one mapping between a given question and the document(s) that exclusively answer it."
  - [section] "The annotator is tasked with ensuring that the question is sufficiently specific by cross-referencing the localized evidence."
  - [corpus] Weak. No quantitative evaluation of disambiguation success rate is provided.
- Break condition: If two documents share identical figures/tables and metadata, even augmented queries may remain ambiguous, leading to retrieval errors.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: VisDoMRAG extends standard RAG by fusing visual and textual retrieval; understanding RAG fundamentals is critical to grasp the parallel pipeline design
  - Quick check question: In a RAG pipeline, what is the role of the retriever versus the generator?

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Both unimodal pipelines employ CoT prompting to produce traceable reasoning steps; modality fusion aligns these chains for consistency
  - Quick check question: How does CoT prompting differ from direct answer generation in terms of intermediate reasoning steps?

- Concept: Multimodal embeddings and late-interaction retrieval
  - Why needed here: Visual retrieval uses late-interaction models (ColPali, ColQwen2) that project images into the same embedding space as text queries; understanding this is key to why visual retrieval outperforms OCR-based text retrieval
  - Quick check question: What is the advantage of late-interaction retrieval over early fusion in multimodal document QA?

## Architecture Onboarding

- Component map: PDF Ingestion → OCR (text) / VLM (visual) → Chunking/Indexing → Retriever (BGE-1.5 for text, ColQwen2 for visual) → Evidence Curation (LLM) → Chain-of-Thought Reasoning (LLM) → Answer Generation (LLM) → Modality Fusion (LLM consistency check) → Final Answer
- Critical path: Query → Retrieval (text + visual) → Evidence Curation → CoT Reasoning → Modality Fusion → Final Answer
- Design tradeoffs:
  - Parallel unimodal pipelines increase latency but improve accuracy; early fusion was tried but underperformed due to loss of modality-specific reasoning
  - Using ColQwen2 avoids OCR overhead but requires image processing; BGE-1.5 requires OCR but benefits from dense text retrieval
  - Human review in query augmentation adds cost but ensures dataset quality
- Failure signatures:
  - Low consistency score in modality fusion → one modality's reasoning is unreliable
  - High refusal rate when oracle document is removed → retrieval fails without the correct document
  - Performance drop with increasing page count → evidence retrieval becomes too sparse
- First 3 experiments:
  1. Compare ANLCS scores for ColQwen2 vs BGE-1.5 at k=5 to validate visual retrieval superiority
  2. Run ablation: remove evidence curation from unimodal pipelines and measure QA performance drop
  3. Test modality fusion with synthetic inconsistencies to measure consistency-check effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VisDoMRAG change when using more advanced long-context LLMs versus the current combination of unimodal RAG pipelines?
- Basis in paper: Inferred from the discussion of long-context LLMs as a baseline and the performance gap observed between VisDoMRAG and these baselines
- Why unresolved: The paper does not explore the use of long-context LLMs within the VisDoMRAG framework, only comparing it to long-context baselines
- What evidence would resolve it: Experimental results comparing VisDoMRAG using long-context LLMs to the current unimodal RAG approach, evaluating performance across all datasets

### Open Question 2
- Question: What is the impact of different evidence curation strategies on the overall performance of VisDoMRAG, and how sensitive is the system to the quality of the curated evidence?
- Basis in paper: Inferred from the emphasis on evidence curation as a crucial step in the VisDoMRAG pipeline and the ablation study showing its importance
- Why unresolved: The paper does not explore alternative evidence curation methods or quantify the sensitivity of VisDoMRAG to the quality of curated evidence
- What evidence would resolve it: Experiments comparing different evidence curation strategies, such as different prompt templates or LLMs, and analyzing the impact on final answer accuracy

### Open Question 3
- Question: How does the consistency-constrained modality fusion mechanism in VisDoMRAG handle cases where the textual and visual RAG pipelines produce conflicting answers, and what are the failure modes of this approach?
- Basis in paper: Explicit mention of the consistency-constrained modality fusion mechanism and its role in resolving discrepancies between modalities
- Why unresolved: The paper does not provide a detailed analysis of how the fusion mechanism handles conflicts or what happens when it fails to reconcile the differences
- What evidence would resolve it: A case study analyzing specific instances where the textual and visual pipelines disagree, examining how the fusion mechanism resolves the conflict and identifying potential failure modes

## Limitations

- The consistency-constrained modality fusion mechanism lacks explicit detail on how the LLM evaluates and reconciles discrepancies between textual and visual reasoning chains
- Query augmentation relies heavily on human review for disambiguation, but no quantitative evaluation of disambiguation success is provided
- Performance comparisons with existing multimodal RAG systems are limited, and the dataset construction process introduces potential bias toward the proposed method's strengths

## Confidence

- High confidence in the experimental results showing 12-20% improvement over baselines, as these are directly measured on the proposed benchmark
- Medium confidence in the mechanism claims (parallel RAG, modality fusion) due to lack of detailed algorithmic description and reliance on ablation studies
- Low confidence in the generalizability of query augmentation benefits without independent validation of the disambiguation process

## Next Checks

1. **Modality fusion evaluation:** Test the consistency-check mechanism by injecting controlled inconsistencies into reasoning chains and measuring fusion accuracy
2. **Query augmentation robustness:** Evaluate the success rate of query disambiguation by testing augmented queries against a holdout set of documents with similar visual content
3. **Retrieval model comparison:** Compare ANLCS scores for ColQwen2 vs BGE-1.5 at k=5 to validate the claim that visual retrieval consistently outperforms textual retrieval