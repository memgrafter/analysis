---
ver: rpa2
title: Global Concept Explanations for Graphs by Contrastive Learning
arxiv_id: '2404.16532'
source_url: https://arxiv.org/abs/2404.16532
tags:
- graph
- concept
- explanations
- explanation
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method for generating global concept explanations
  for graph property prediction tasks, aiming to extract underlying structure-property
  relationships from graph neural network predictions. The core idea is to identify
  concept explanations as dense clusters in the Megan model's subgraph latent space,
  where each concept is represented by a prototype graph optimized via genetic algorithm.
---

# Global Concept Explanations for Graphs by Contrastive Learning

## Quick Facts
- arXiv ID: 2404.16532
- Source URL: https://arxiv.org/abs/2404.16532
- Authors: Jonas Teufel; Pascal Friederich
- Reference count: 40
- This work proposes a method for generating global concept explanations for graph property prediction tasks by identifying concept explanations as dense clusters in the Megan model's subgraph latent space.

## Executive Summary
This paper introduces a method for generating global concept explanations for graph property prediction tasks by extracting underlying structure-property relationships from graph neural network predictions. The approach identifies concept explanations as dense clusters in Megan2's subgraph latent space, where each concept is represented by a prototype graph optimized via genetic algorithm. The method introduces a contrastive learning objective to align the latent space with structural similarity, enabling meaningful clustering of subgraph embeddings. Computational experiments on synthetic and real-world datasets demonstrate the method's ability to correctly reconstruct structure-property relationships for synthetic tasks and rediscover established rules of thumb for real-world molecular property prediction.

## Method Summary
The method builds on the Megan2 model, which uses attention mechanisms to create explanation masks defining subgraph structures for property predictions. It introduces projection networks that transform subgraph embeddings into a latent space, followed by a contrastive learning objective using InfoNCE loss to align this space with structural similarity. HDBSCAN clustering identifies dense regions representing concepts, and a genetic algorithm optimizes prototype graphs that capture each concept's essence while maintaining structural similarity to cluster centroids. The approach enables global explanations by analyzing how each concept influences prediction outcomes across the dataset.

## Key Results
- Correctly reconstructs structure-property relationships for synthetic BA2Motifs dataset
- Rediscover established rules of thumb for real-world molecular property prediction tasks
- Produces more fine-grained explanations for mutagenicity prediction than existing approaches, consistent with chemistry literature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Megan2's projected latent space with contrastive learning aligns subgraph embeddings according to structural similarity, enabling concept clustering to extract meaningful global explanations.
- Mechanism: By applying L2-normalized projections to each explanation channel's embeddings and maximizing similarity between positive samples (augmented views) while minimizing similarity with negative samples (other batch elements), the latent space is restructured to reflect structural similarity rather than prediction class boundaries.
- Core assumption: The cosine similarity between projected embeddings correlates with structural similarity of the corresponding subgraph motifs.
- Evidence anchors:
  - [abstract] "We introduce additional projection networks... to ensure that each channel's embeddings can develop independently."
  - [section] "We add a contrastive representation learning objective... acting on the projected channel embeddings z(k)"
  - [corpus] Weak - corpus papers focus on contrastive learning for molecular property prediction but don't specifically address concept extraction through clustering in latent spaces.
- Break condition: If the augmentation strategy fails to create meaningful positive samples that preserve structural information while being invariant to minor variations, the alignment between latent space distance and structural similarity breaks down.

### Mechanism 2
- Claim: HDBSCAN clustering on the projected latent space can dynamically discover the appropriate number of concept clusters without pre-specification.
- Mechanism: HDBSCAN identifies dense regions in the projected latent space where subgraph embeddings are similar, treating these dense regions as concept clusters representing recurring structural patterns in the data.
- Core assumption: Dense clusters in the projected latent space correspond to meaningful structural concepts that the model has learned to exploit for predictions.
- Evidence anchors:
  - [abstract] "Explanations are then generated by applying clustering methods to this latent space."
  - [section] "To find these clusters we use the HDBSCAN clustering algorithm on the channel projections z(k)"
  - [corpus] Weak - corpus papers mention clustering but focus on different applications; no direct evidence for HDBSCAN's effectiveness in this specific context.
- Break condition: If the projected latent space doesn't form clear density-based clusters due to poor alignment of embeddings with structural similarity, HDBSCAN will fail to identify meaningful concepts.

### Mechanism 3
- Claim: Genetic algorithm optimization of prototype graphs creates minimal yet representative concept visualizations that maintain sufficient structural similarity to the cluster centroid.
- Mechanism: Starting from full member graphs, the genetic algorithm iteratively applies node and edge deletion mutations while maintaining a minimum cosine similarity threshold to the cluster centroid, producing compact prototype graphs that capture the essence of each concept.
- Core assumption: The prototype graphs that minimize node count while maintaining sufficient similarity to cluster centroids are effective representations of the underlying concepts.
- Evidence anchors:
  - [abstract] "By analyzing the prediction contributions of the individual cluster members, it is also possible to determine each concept's average influence on the prediction outcome."
  - [section] "We use a genetic algorithm (GA) with a fixed number of epochs for the optimization of the concept prototype graphs"
  - [corpus] Missing - no corpus evidence directly supporting genetic algorithm optimization for graph prototypes in this context.
- Break condition: If the similarity threshold is set too high, optimization may fail to produce compact prototypes; if too low, prototypes may not represent the concept well.

## Foundational Learning

- Concept: Graph Neural Networks and attention mechanisms
  - Why needed here: Understanding how Megan2 uses attention to create explanation masks and how these masks define subgraph structures is fundamental to grasping the entire approach.
  - Quick check question: How does the attention-weighted sum during graph pooling in Megan2 create subgraph-specific embeddings?

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The contrastive learning component is critical for aligning the latent space with structural similarity, which is the foundation for meaningful clustering.
  - Quick check question: Why does maximizing similarity between augmented views of the same graph (positive samples) while minimizing similarity with other graphs (negative samples) help align the latent space with structural similarity?

- Concept: Density-based clustering (HDBSCAN)
  - Why needed here: HDBSCAN is the primary method for discovering concept clusters, and understanding its parameters and behavior is essential for proper application.
  - Quick check question: What makes HDBSCAN preferable to other clustering algorithms for discovering concepts in this context?

## Architecture Onboarding

- Component map: Input graph (G) -> Megan2 encoder (attention masks + node embeddings) -> Projection networks (P-dimensional latent space) -> Contrastive learning module (InfoNCE loss) -> HDBSCAN clustering -> Genetic algorithm optimization (prototype graphs) -> GPT hypothesis generation (optional)
- Critical path: Message passing → Projection → Contrastive learning → Clustering → Prototype optimization → Hypothesis generation
- Design tradeoffs:
  - Projection dimensionality P vs. computational cost: Higher dimensions allow more complex encoding but increase computation
  - Number of explanation channels K vs. redundancy: More channels may capture more concepts but create redundancy
  - HDBSCAN parameters (min_cluster_size, min_samples) vs. granularity: Tighter parameters yield more clusters but may over-segment
- Failure signatures:
  - Poor prediction performance despite good explanation quality: Indicates misalignment between contrastive learning and primary task
  - Many small clusters or one large cluster: Suggests improper HDBSCAN parameters or insufficient contrastive learning alignment
  - Chemically infeasible prototypes: Indicates genetic algorithm parameters need adjustment or insufficient constraint handling
- First 3 experiments:
  1. Train Megan2 on BA2Motifs with varying K (number of explanation channels) to observe effect on cluster quality and redundancy
  2. Test different augmentation strategies (thresholding, noise levels) in contrastive learning to find optimal alignment between latent space and structural similarity
  3. Vary HDBSCAN parameters on synthetic datasets with known ground truth to find optimal clustering configuration

## Open Questions the Paper Calls Out

None

## Limitations

- Dependence on Megan2's attention masks as the basis for subgraph extraction may miss subtle or distributed structural patterns
- Genetic algorithm optimization for prototype graphs lacks theoretical guarantees about finding globally optimal solutions
- Generalization to other property types beyond molecular property prediction remains to be validated

## Confidence

High confidence in contrastive learning mechanism's ability to align latent space with structural similarity
Medium confidence in HDBSCAN clustering approach's effectiveness
Medium confidence in overall explanation quality demonstrated on synthetic and mutagenicity datasets

## Next Checks

1. Conduct ablation studies removing the contrastive learning component to quantify its contribution to explanation quality versus using the original Megan2 embeddings directly.

2. Test the method on graph property prediction tasks where the underlying structure-property relationships are known to be distributed rather than localized in subgraphs (e.g., certain quantum chemistry properties).

3. Evaluate prototype graph interpretability by conducting a blind chemist review of mutagenicity explanations, comparing the discovered concepts against established medicinal chemistry rules.