---
ver: rpa2
title: Analyzing Fairness of Classification Machine Learning Model with Structured
  Dataset
arxiv_id: '2412.09896'
source_url: https://arxiv.org/abs/2412.09896
tags:
- fairness
- bias
- learning
- machine
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses fairness and bias in machine learning classification\
  \ models using structured datasets. The research employs three prominent fairness\
  \ libraries\u2014Fairlearn by Microsoft, AIF360 by IBM, and the What-If Tool by\
  \ Google\u2014to analyze and mitigate bias in a publicly available Adult Income\
  \ dataset from Kaggle."
---

# Analyzing Fairness of Classification Machine Learning Model with Structured Dataset

## Quick Facts
- arXiv ID: 2412.09896
- Source URL: https://arxiv.org/abs/2412.09896
- Reference count: 36
- This study addresses fairness and bias in machine learning classification models using structured datasets.

## Executive Summary
This study addresses fairness and bias in machine learning classification models using structured datasets. The research employs three prominent fairness libraries—Fairlearn by Microsoft, AIF360 by IBM, and the What-If Tool by Google—to analyze and mitigate bias in a publicly available Adult Income dataset from Kaggle. The study applies preprocessing, in-processing, and postprocessing mitigation techniques, including Reweighing, Exponential Gradient, and Equalized Odds algorithms. Key results show that Fairlearn's Exponential Gradient algorithm reduced demographic parity difference by 83% while maintaining accuracy at 0.85. AIF360's Reweighing and Equalized Odds algorithms improved fairness metrics by 8.3-13% with minimal performance impact. The What-If Tool achieved a 29% accuracy improvement and 18% reduction in bias through threshold optimization.

## Method Summary
The research employs three prominent fairness libraries—Fairlearn by Microsoft, AIF360 by IBM, and the What-If Tool by Google—to analyze and mitigate bias in a publicly available Adult Income dataset from Kaggle. The study applies preprocessing, in-processing, and postprocessing mitigation techniques, including Reweighing, Exponential Gradient, and Equalized Odds algorithms. The methodology involves implementing bias detection, applying fairness-aware algorithms, and measuring performance metrics such as accuracy and fairness scores across different demographic groups.

## Key Results
- Fairlearn's Exponential Gradient algorithm reduced demographic parity difference by 83% while maintaining accuracy at 0.85
- AIF360's Reweighing and Equalized Odds algorithms improved fairness metrics by 8.3-13% with minimal performance impact
- The What-If Tool achieved a 29% accuracy improvement and 18% reduction in bias through threshold optimization

## Why This Works (Mechanism)
The effectiveness stems from the systematic application of fairness-aware algorithms that directly address bias during different stages of the machine learning pipeline. By combining preprocessing techniques that rebalance the training data, in-processing methods that modify the learning algorithm, and postprocessing approaches that adjust decision thresholds, the study demonstrates that bias can be substantially reduced while maintaining or improving model performance. The integration of multiple established fairness libraries provides complementary perspectives and techniques, allowing for comprehensive bias analysis and mitigation across different fairness metrics and demographic groups.

## Foundational Learning

### Demographic Parity
- **Why needed**: Ensures equal positive outcome rates across demographic groups
- **Quick check**: Compare selection rates across protected groups to identify disparities

### Equalized Odds
- **Why needed**: Balances both true positive and false positive rates across groups
- **Quick check**: Evaluate TPR and FPR separately for each demographic group

### Reweighing Technique
- **Why needed**: Adjusts training data distribution to reduce bias before model training
- **Quick check**: Verify that sample weights are properly calculated and applied to the training set

## Architecture Onboarding

### Component Map
Data preprocessing -> Bias detection -> Fairness algorithm application -> Performance evaluation -> Threshold optimization

### Critical Path
Dataset loading → Bias metric computation → Algorithm selection → Model training with fairness constraints → Performance validation

### Design Tradeoffs
Accuracy vs. fairness: Some algorithms achieve higher fairness at the cost of slight accuracy reduction, while others maintain accuracy while improving fairness metrics.

### Failure Signatures
Inconsistent fairness metrics across different demographic groups indicate inadequate mitigation. Significant accuracy drop (>5%) may suggest overly aggressive fairness constraints.

### First Experiments
1. Baseline model without fairness constraints
2. Simple preprocessing with Reweighing algorithm
3. In-processing with Fairlearn's Exponential Gradient

## Open Questions the Paper Calls Out
None

## Limitations
- The research focuses exclusively on a single dataset (Adult Income), which limits external validity and may not capture the full spectrum of fairness challenges across different domains
- Performance metrics are reported without confidence intervals or statistical significance testing, making it difficult to assess the reliability of the claimed improvements
- The study does not address intersectional fairness or explore the trade-offs between different fairness definitions, which are critical considerations in real-world applications

## Confidence
- **High confidence**: The technical implementation aligns with established practices in fairness-aware machine learning
- **Medium confidence**: Transferability to other contexts and datasets due to limited scope of experimentation
- **Medium confidence**: Relative performance claims between libraries lack rigorous statistical validation

## Next Checks
1. Replicate the study using multiple diverse structured datasets (e.g., healthcare, criminal justice) to assess generalizability
2. Conduct statistical significance testing on all reported metrics with confidence intervals
3. Implement intersectional fairness analysis to evaluate performance across overlapping demographic groups