---
ver: rpa2
title: 'Pruning One More Token is Enough: Leveraging Latency-Workload Non-Linearities
  for Vision Transformers on the Edge'
arxiv_id: '2407.05941'
source_url: https://arxiv.org/abs/2407.05941
tags:
- pruning
- latency
- token
- accuracy
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently deploying vision
  transformers on edge devices for small workloads. The authors identify the GPU tail
  effect as a key factor influencing latency-workload relationships and develop a
  hardware-aware token pruning schedule based on this insight.
---

# Pruning One More Token is Enough: Leveraging Latency-Workload Non-Linearities for Vision Transformers on the Edge

## Quick Facts
- arXiv ID: 2407.05941
- Source URL: https://arxiv.org/abs/2407.05941
- Reference count: 40
- Primary result: Training-free token pruning achieving 78.6%-84.5% ImageNet1K accuracy vs 45.8%-85.4% for state-of-the-art methods at similar latency

## Executive Summary
This paper addresses the challenge of efficiently deploying vision transformers on edge devices for small workloads. The authors identify the GPU tail effect as a key factor influencing latency-workload relationships and develop a hardware-aware token pruning schedule based on this insight. They introduce a training-free token pruning mechanism that achieves better accuracy-latency tradeoffs than existing methods. For similar latency (within 5.2%) across devices, their approach achieves 78.6%-84.5% ImageNet1K accuracy, outperforming the state-of-the-art Token Merging method which achieves 45.8%-85.4%.

## Method Summary
The method involves measuring latency-workload relationships on target hardware to identify the GPU tail effect, then determining an optimal token pruning schedule based on a weighted utility function balancing accuracy degradation and latency reduction. The token pruning mechanism uses attention-based importance scores (combining attention received and V-matrix magnitude) to identify and remove the least important tokens at a single layer (typically 25% into the network), aggregating pruned tokens into an "inattentive" token. This approach is training-free and tailored to specific model-device pairs through offline measurement.

## Key Results
- Achieves 78.6%-84.5% ImageNet1K accuracy compared to 45.8%-85.4% for Token Merging at similar latency (within 5.2%)
- Reduces latency by 9% for single-batch inference compared to other techniques that may increase latency by 18.6-30.3%
- Particularly effective for edge devices with small batch sizes where GPU tail effect is dominant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pruning tokens earlier in the network combined with the GPU tail effect yields better latency reduction than progressive token pruning.
- Mechanism: By pruning all target tokens at one layer (25% into the network), the GPU tail effect reduces latency for all subsequent layers. Progressive pruning introduces overhead at each layer, negating latency gains.
- Core assumption: The GPU tail effect is the dominant factor in latency-workload relationships for small batch sizes on edge devices.
- Evidence anchors:
  - [abstract] "We identify the GPU tail effect as a key factor influencing latency-workload relationships"
  - [section] "the GPU tail effect can lower latency for ViT's" (Sec. 2.2)
  - [corpus] Weak - no direct corpus evidence about GPU tail effect in transformers
- Break condition: If GPU tail effect is not the dominant latency factor, or if progressive pruning overhead is negligible for the target workload size.

### Mechanism 2
- Claim: Training-free token pruning using attention-based importance scores achieves comparable accuracy to methods requiring training.
- Mechanism: Token importance is determined by the sum of attention received (AS) and softmax of V matrix magnitude (VS). Least important tokens are pruned and their features are aggregated into an "inattentive" token to preserve information.
- Core assumption: Attention-based importance scores are sufficient proxies for token information content without requiring model fine-tuning.
- Evidence anchors:
  - [abstract] "develop a novel training-free token pruning mechanism"
  - [section] "we rank the importance of each token i by measuring the quantity of attention received from all other tokens" (Sec. 3.3)
  - [corpus] Weak - no direct corpus evidence about this specific training-free attention-based method
- Break condition: If attention-based importance scores fail to capture token information content, or if the "inattentive" token aggregation harms accuracy.

### Mechanism 3
- Claim: Hardware-aware pruning schedules tailored to specific model-device pairs outperform generic pruning methods.
- Mechanism: Offline measurement of latency-workload relationships on target hardware determines optimal number of tokens to prune (R) based on a weighted utility function balancing accuracy degradation and latency reduction.
- Core assumption: Latency-workload relationships are device-specific and non-linear, requiring hardware-aware optimization rather than generic approaches.
- Evidence anchors:
  - [abstract] "determine token pruning schedule by leveraging non-linear latency-workload relationships"
  - [section] "We model ViT workload size as being proportional to the number of input tokens n, embedding dimension d, and batch size b" (Sec. 3.1)
  - [corpus] Weak - no direct corpus evidence about hardware-aware pruning schedules
- Break condition: If latency-workload relationships are linear or device-independent, or if the utility function weighting fails to balance accuracy and latency appropriately.

## Foundational Learning

- Concept: GPU tail effect and its impact on latency-workload relationships
  - Why needed here: Understanding how partial GPU wave execution affects latency is crucial for designing effective pruning schedules
  - Quick check question: Why does removing one token sometimes cause disproportionately large latency reductions on edge devices?

- Concept: Vision Transformer architecture and attention mechanisms
  - Why needed here: Token pruning methods must work within the constraints of ViT architecture and attention operations
  - Quick check question: How does the self-attention mechanism's complexity relate to token count and embedding dimension?

- Concept: Multi-objective optimization for balancing accuracy and latency
  - Why needed here: Determining the optimal number of tokens to prune requires balancing competing objectives
  - Quick check question: What factors should be considered when designing a utility function for token pruning optimization?

## Architecture Onboarding

- Component map: Offline computation pipeline -> Token pruning mechanism -> Integration layer
- Critical path:
  1. Measure latency L(n) for various token counts n on target device
  2. Compute accuracy degradation A(n) using random token removal heuristic
  3. Calculate utility functions UL(n) and UA(n)
  4. Solve optimization problem to determine R (tokens to prune)
  5. Select pruning layer (typically 25% into network)
  6. During inference: compute importance scores, prune tokens, aggregate features
- Design tradeoffs:
  - Single-layer vs. progressive pruning: Single-layer reduces overhead but may sacrifice some accuracy
  - Attention-only vs. combined attention and V matrix importance: Combined approach marginally improves accuracy
  - Hardware measurement vs. prediction: Measurement is more accurate but requires offline computation
- Failure signatures:
  - Latency increases after pruning: Indicates pruning overhead outweighs benefits
  - Significant accuracy degradation: Suggests importance scoring mechanism is ineffective
  - Suboptimal pruning location: May result from incorrect assumption about where pruning provides best latency benefits
- First 3 experiments:
  1. Measure latency-workload relationship for baseline model on target device to verify GPU tail effect
  2. Test attention-based importance scoring with random token removal baseline to validate pruning effectiveness
  3. Compare single-layer vs. progressive pruning for the same total number of tokens removed to quantify overhead impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the proposed method scale when applied to transformer models with significantly larger numbers of layers than the evaluated DinoV2-G (which has 40 layers)?
- Basis in paper: [explicit] The paper notes that the method prunes all R tokens at one layer fairly early in the network (around 25% of layers), and discusses pruning at layer 10 for DinoV2-G. It also mentions in Section 7 Future Work that "Learning based approaches may be able to take advantage of the high latency reductions of aggressive, early pruning while maintaining accuracy."
- Why unresolved: The paper only evaluates the method on models with 40 layers (DinoV2-G) and 12 layers (DeiT variants). The authors acknowledge that different pruning locations may be needed for different models but do not explore the effect of significantly deeper networks.
- What evidence would resolve it: Comparative evaluation of the method on transformer models with 60+ layers (e.g., deeper variants of DINOv2 or other large vision transformers) showing accuracy-latency tradeoffs at various pruning locations.

### Open Question 2
- Question: What is the precise mechanism causing the GPU tail effect to manifest differently across various workload sizes, and can this be predicted rather than measured empirically?
- Basis in paper: [explicit] The paper discusses that "the latency difference between steps can be nonuniform" and that "model latency may actually increase as workload size decreases for certain workload sizes." It also notes that prior work has modeled GPU tail effect behavior for CNNs with assumptions that "may not hold for certain workload sizes using transformer networks."
- Why unresolved: The authors choose to empirically measure the GPU tail effect rather than predict it due to the difficulty in modeling its behavior, but do not provide a theoretical explanation for why transformers exhibit different tail effect characteristics than CNNs or why the effect varies across workload sizes.
- What evidence would resolve it: A theoretical model or empirical study explaining the relationship between transformer architecture (attention mechanisms, token structure), workload size, and GPU scheduling behavior that predicts when and how the GPU tail effect will manifest.

### Open Question 3
- Question: How does the proposed training-free token pruning mechanism compare in terms of accuracy-latency tradeoffs when composed with other acceleration methods such as quantization or low-rank factorization?
- Basis in paper: [explicit] The paper mentions in Section 2.1 that "Some post-processing or fine-tuning methods such as quantization [18] are compatible with token sparsification techniques such as our method" and in Section 7 Future Work notes that "Another promising direction is investigating the benefits of composing token sparsification and other acceleration methods together."
- Why unresolved: The paper only evaluates the standalone token pruning mechanism and does not explore combinations with other acceleration techniques, despite acknowledging their compatibility.
- What evidence would resolve it: Comparative evaluation showing accuracy-latency tradeoffs of the proposed method alone versus when combined with quantization, low-rank factorization, or other model acceleration techniques on the same hardware and workload configurations.

## Limitations

- Heavy reliance on GPU tail effect without direct empirical validation across different GPU architectures and workload sizes
- Training-free approach lacks comparative analysis against fine-tuned alternatives for token importance scoring
- Hardware-aware pruning requires offline measurement for each model-device pair, creating scalability challenges

## Confidence

**High Confidence**: The experimental results showing accuracy-latency tradeoffs compared to baseline methods (ToMe, Top-K, DynamicViT) are reproducible based on the provided implementation details.

**Medium Confidence**: The theoretical justification for why single-layer pruning at 25% into the network is optimal relies on assumptions about GPU tail effect behavior that weren't empirically validated across diverse hardware configurations.

**Low Confidence**: The claim that the GPU tail effect is the "dominant" factor in latency-workload relationships lacks supporting evidence from ablation studies or alternative explanations.

## Next Checks

1. Systematically measure and document GPU tail effect across different GPU architectures (e.g., Ampere vs. Turing vs. Ada Lovelace) to determine if the non-linear latency-workload relationship is consistent or architecture-specific.

2. Compare the training-free attention-based importance scoring against fine-tuned token importance methods using the same hardware-aware pruning framework.

3. Evaluate the method's performance across varying batch sizes (beyond batch size 2) to determine if the GPU tail effect and optimal pruning strategy scale consistently.