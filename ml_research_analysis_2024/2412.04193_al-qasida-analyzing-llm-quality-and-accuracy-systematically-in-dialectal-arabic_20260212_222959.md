---
ver: rpa2
title: 'AL-QASIDA: Analyzing LLM Quality and Accuracy Systematically in Dialectal
  Arabic'
arxiv_id: '2412.04193'
source_url: https://arxiv.org/abs/2412.04193
tags:
- arabic
- llms
- varieties
- language
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the under-representation of Dialectal Arabic
  (DA) varieties in large language models (LLMs), which limits their utility for the
  majority of Arabic speakers and risks exacerbating social inequalities. The authors
  introduce AL-QASIDA, a comprehensive evaluation framework assessing LLM DA proficiency
  across four dimensions: fidelity, understanding, quality, and diglossia.'
---

# AL-QASIDA: Analyzing LLM Quality and Accuracy Systematically in Dialectal Arabic

## Quick Facts
- arXiv ID: 2412.04193
- Source URL: https://arxiv.org/abs/2412.04193
- Reference count: 40
- Primary result: LLMs understand DA better than they generate it due to post-training bias favoring MSA

## Executive Summary
This paper introduces AL-QASIDA, a comprehensive evaluation framework for assessing large language models' proficiency with Dialectal Arabic (DA). The authors find that LLMs understand DA significantly better than they generate it, not due to poor fluency but due to a reluctance to produce DA, likely caused by post-training biases favoring Modern Standard Arabic. They demonstrate that few-shot examples can mitigate this deficiency, and recommend specific model choices for different Arabic language tasks.

## Method Summary
The study evaluates nine LLMs across eight DA varieties using four dimensions: fidelity, understanding, quality, and diglossia. The evaluation uses ADI2 scores (combining NADI and ALDi metrics), translation quality metrics (SpBLEU, chrF), and human evaluation. Monolingual and cross-lingual generation tasks are tested with and without few-shot prompting. The evaluation leverages multiple corpora including MADAR-26, FLORES-200, MADAR-Twitter, and HABIBI. The minimum viable reproduction requires obtaining these datasets, setting up the NADI 2024 baseline and ALDi models, and running LLM inference with the specified metrics.

## Key Results
- LLMs understand DA significantly better than they generate it, reversing the Generative AI Paradox
- No input text features strongly predict DA generation success (correlation analysis shows no ρ values exceeding magnitude 0.5)
- Few-shot examples effectively mitigate DA generation deficiencies across models
- Llama-3 performs best for monolingual DA tasks, while GPT-4o excels at cross-lingual DA generation for Egyptian and Moroccan varieties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Post-training bias against DA production, not fluency, drives low ADI2 scores
- Mechanism: Human-labeled post-training data favors MSA, causing models to prefer MSA generation even when capable of DA fluency
- Core assumption: The model understands and can generate DA but defaults to MSA due to learned preference from post-training data
- Evidence anchors:
  - [abstract] "not because their DA fluency is poor, but because they are reluctant to generate DA"
  - [section] "Base models' higher performance supports Hypothesis 1 that post-training can inhibit DA modeling"
  - [corpus] Weak corpus evidence: no explicit post-training dataset analysis provided
- Break condition: If few-shot prompting fails to consistently improve DA generation, or if DA fluency degrades significantly when few-shot examples are used

### Mechanism 2
- Claim: Understanding exceeds generation for DA, opposite to the Generative AI Paradox
- Mechanism: Models encode DA semantics well in pre-training but fail to generate them due to generation biases, creating an asymmetry
- Core assumption: The model's semantic understanding of DA is intact but generation preference overrides it
- Evidence anchors:
  - [abstract] "LLMs do not produce DA as well as they understand it" and "apparent reversal of the Generative AI Paradox"
  - [section] "LLMs are capable of understanding DA utterances but unable to produce fluent DA outputs"
  - [corpus] No direct corpus evidence, inferred from MT and human eval results
- Break condition: If MT or human evaluation shows poor DA understanding, contradicting the generation-understanding gap

### Mechanism 3
- Claim: No input text features correlate strongly with DA generation success
- Mechanism: DA output likelihood is largely stochastic or controlled by model internal state, not prompt content
- Core assumption: The model's DA generation behavior is independent of measurable prompt attributes
- Evidence anchors:
  - [abstract] "no measurable features of input text correlate well with LLM DA performance"
  - [section] "Spearman's rank tests... no values exceed... strong correlation... no ρ values exceed magnitude 0.5"
  - [corpus] Weak corpus evidence: correlation analysis performed but lacks feature engineering depth
- Break condition: If future experiments find strong prompt-level predictors of DA output dialectness

## Foundational Learning

- Concept: ADI2 score combining ALDi and NADI metrics
  - Why needed here: To assess both dialectal fidelity and variety correctness in a single metric
  - Quick check question: If a model outputs 80% dialectal content but wrong dialect, what ADI2 score would it get?

- Concept: Cross-lingual vs monolingual prompting
  - Why needed here: To test model flexibility in DA generation from English requests vs DA inputs
  - Quick check question: Why did models perform worse in cross-lingual DA generation?

- Concept: Diglossia in Arabic (MSA vs DA distinction)
  - Why needed here: To evaluate if models can translate between formal and informal registers
  - Quick check question: How does poor MSA↔DA translation affect real-world Arabic LLM usability?

## Architecture Onboarding

- Component map: Prompt generation → LLM inference → NADI/ALDi scoring → ADI2 aggregation → Result analysis
- Critical path: Prompt generation → LLM inference → NADI/ALDi scoring → ADI2 aggregation → Result analysis
- Design tradeoffs: Few-shot vs no-shot prompting (mitigation cost vs baseline measurement)
- Failure signatures: Low ADI2 but high DA→Eng MT suggests understanding gap; low DA↔MSA MT indicates poor diglossia awareness
- First 3 experiments:
  1. Run baseline monolingual ADI2 across all models and dialects
  2. Apply few-shot prompting and measure ADI2 delta
  3. Correlate prompt dialectness with output dialectness across tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does post-training data imbalance specifically cause LLMs to prefer MSA over DA, or is it a broader bias against linguistic diversity?
- Basis in paper: [inferred] from findings that post-training can bias LLMs against DA and that few-shot examples can overcome this deficiency
- Why unresolved: The paper identifies post-training as a contributing factor but doesn't isolate whether the bias is specifically against DA or more broadly against linguistic diversity
- What evidence would resolve it: Controlled experiments comparing post-training effects on DA versus other linguistic variations (e.g., minority languages, specialized registers) using identical methodologies

### Open Question 2
- Question: What specific characteristics in few-shot examples make them effective at improving DA generation across different LLM architectures?
- Basis in paper: [explicit] from finding that few-shot examples can overcome DA generation deficiencies
- Why unresolved: The paper demonstrates effectiveness but doesn't analyze which aspects of few-shot examples (format, dialect coverage, context length, etc.) drive the improvement
- What evidence would resolve it: Systematic ablation studies varying different aspects of few-shot examples while keeping other variables constant

### Open Question 3
- Question: Are there specific features of DA varieties that make some dialects more challenging for LLMs than others, beyond simple geographic proximity?
- Basis in paper: [inferred] from observation that performance varies across dialects and that no input text features strongly predict DA performance
- Why unresolved: While the paper notes performance differences, it doesn't identify underlying linguistic or sociolinguistic factors that might explain these variations
- What evidence would resolve it: Comparative linguistic analysis of DA varieties combined with LLM performance data to identify specific features (phonological, morphological, syntactic, sociolinguistic) that correlate with generation difficulty

## Limitations
- The correlation analysis showing no input features predict DA generation success appears underpowered with only 1,600 prompts tested
- The claim about post-training bias lacks direct corpus-level validation from post-training dataset analysis
- The few-shot prompting mitigation lacks systematic ablation studies to isolate which examples drive improvements

## Confidence
- **High Confidence**: The core empirical finding that LLMs exhibit an understanding-generation gap for DA, and that this gap reverses the Generative AI Paradox
- **Medium Confidence**: The interpretation that post-training bias causes DA generation reluctance, but lacks direct evidence
- **Low Confidence**: The claim that no input text features correlate with DA generation success requires stronger statistical validation

## Next Checks
1. **Post-training dataset audit**: Analyze the proportion of DA vs MSA content in the post-training corpora of evaluated models to directly test the bias hypothesis
2. **Feature engineering expansion**: Conduct deeper correlation analysis using linguistic features (lexical overlap, syntactic complexity, dialectal markers) to verify the claim about input feature independence
3. **Few-shot ablation study**: Systematically vary few-shot examples to identify which linguistic patterns or dialectal features most effectively trigger DA generation, isolating the mitigation mechanism