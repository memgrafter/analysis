---
ver: rpa2
title: 'Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated
  Class Continual Learning'
arxiv_id: '2409.01128'
source_url: https://arxiv.org/abs/2409.01128
tags:
- data
- class
- learning
- federated
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles catastrophic forgetting in federated class-incremental\
  \ learning by introducing diffusion model-based data replay. Instead of training\
  \ new diffusion models, it uses pre-trained conditional models to reverse-engineer\
  \ class embeddings via federated optimization\u2014preserving privacy while reducing\
  \ computation."
---

# Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning

## Quick Facts
- arXiv ID: 2409.01128
- Source URL: https://arxiv.org/abs/2409.01128
- Reference count: 40
- Achieves state-of-the-art accuracy of 51.04% on CIFAR-100 IID versus 42.67% for MFCL

## Executive Summary
This paper addresses catastrophic forgetting in federated class-incremental learning by introducing diffusion model-based data replay. The approach uses pre-trained conditional diffusion models in reverse to generate high-quality synthetic historical data without training new generative models, preserving privacy while reducing computation. By combining this with contrastive learning to align features between real and synthetic data, the method achieves state-of-the-art performance on CIFAR-100 and Tiny-ImageNet datasets.

## Method Summary
The DDDR framework consists of two phases: Federated Class Inversion and Replay-Augmented Training. In the first phase, clients optimize class embeddings using a pre-trained conditional diffusion model to generate synthetic historical data. These embeddings are communicated to the server for aggregation. In the second phase, clients train classifiers using both real and synthetic data with contrastive learning to improve domain generalization. The method generates synthetic data for both historical and current tasks to mitigate non-IID effects across clients.

## Key Results
- Achieves 51.04% average accuracy on CIFAR-100 IID (vs 42.67% for MFCL)
- Outperforms existing generative replay baselines across all tested scenarios
- Reduces forgetting metrics while maintaining high accuracy on both real and synthetic data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated Class Inversion enables high-quality historical data generation without training a diffusion model
- Mechanism: Pre-trained conditional diffusion model used in reverse to find embeddings that guide image generation for each class
- Core assumption: Pre-trained diffusion model's input space is smooth enough for effective embedding optimization
- Evidence anchors: [abstract] "Instead of training a diffusion model, we employ a pre-trained conditional diffusion model to reverse-engineer each class..." [section 4.1] "We leverage a pre-trained conditional diffusion model for reverse engineering each class..."
- Break condition: If input space is too discontinuous, optimization cannot find effective embeddings

### Mechanism 2
- Claim: Contrastive learning narrows the feature gap between generated and real data
- Mechanism: Supervised contrastive loss aligns feature representations within same class
- Core assumption: Reducing feature space discrepancy improves classifier handling of both data types
- Evidence anchors: [abstract] "we enhance the classifier's domain generalization ability on generated and real data through contrastive learning..." [section 4.2] "we employ a supervised contrastive learning loss... to constrain the classifier's feature space."
- Break condition: If domain gap is too large, contrastive loss cannot bridge it effectively

### Mechanism 3
- Claim: Generating current task data reduces non-IID effects
- Mechanism: All clients generate synthetic data for current task using shared embeddings
- Core assumption: Shared synthetic current task data can mitigate distribution heterogeneity
- Evidence anchors: [section 4.2] "we also generate current task images... By ensuring all clients share a similar distribution..."
- Break condition: If client data heterogeneity is extreme, synthetic data cannot adequately represent local distributions

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Main problem being addressed - models lose knowledge of old classes when learning new ones
  - Quick check question: What happens to a model's performance on old tasks after training on new tasks without mitigation strategies?

- Concept: Federated learning and privacy constraints
  - Why needed here: Solution must work without sharing raw data between clients due to privacy requirements
  - Quick check question: Why can't traditional experience replay be used directly in federated settings?

- Concept: Diffusion models and conditional generation
  - Why needed here: Approach relies on pre-trained diffusion models to generate high-quality synthetic data
  - Quick check question: What makes diffusion models suitable for generating high-fidelity images compared to GANs?

## Architecture Onboarding

- Component map: Pre-trained LDM (frozen) -> Class embedding optimizer (client-side) -> Classifier with contrastive loss -> Server aggregation module -> Synthetic data generator

- Critical path: 1. Federated Class Inversion (class embeddings) 2. Data generation (historical + current) 3. Contrastive classifier training 4. Server aggregation

- Design tradeoffs: Using pre-trained models reduces training cost but limits customization; Synthetic current task data reduces non-IID but may not perfectly match local distributions; Contrastive loss improves generalization but adds complexity

- Failure signatures: Generated images lack diversity or fidelity; Classifier accuracy degrades on real data; Forgetting metrics remain high despite replay

- First 3 experiments: 1. Validate class embedding optimization produces diverse, high-quality images 2. Test contrastive loss impact on feature alignment between real and synthetic data 3. Measure forgetting reduction with and without current task data generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of generated data by DDDR vary with different pre-trained diffusion models or different prompt engineering strategies?
- Basis in paper: [explicit] The paper states that "Theoretically, any pre-trained conditional diffusion model can be used for Federated Class Inversion" and mentions using Latent Diffusion Model (LDM) due to its fast inference speed and available pre-trained weights
- Why unresolved: Paper only evaluates one pre-trained model (LDM) and a fixed prompt ("a photo of *"), without exploring other models or prompt variations
- What evidence would resolve it: Comparative experiments using different pre-trained diffusion models and varied prompt engineering strategies, with quantitative metrics on generation quality and downstream classification performance

### Open Question 2
- Question: What is the impact of varying the number of communication rounds and local training steps on the performance of Federated Class Inversion and overall DDDR?
- Basis in paper: [explicit] The paper specifies using 10 communication rounds with 50 local training steps for Federated Class Inversion and 100 communication rounds with 5 local epochs for classifier training, but doesn't explore other configurations
- Why unresolved: Paper presents a specific configuration without ablation studies on communication rounds and local training steps
- What evidence would resolve it: Systematic experiments varying the number of communication rounds and local training steps, analyzing trade-offs between performance, communication efficiency, and computational cost

### Open Question 3
- Question: How does DDDR perform in scenarios with varying degrees of non-IID data distribution across clients, beyond the Dirichlet parameter of 0.5 tested in the paper?
- Basis in paper: [explicit] The paper mentions using Dirichlet distribution with parameter 0.5 to simulate non-IID scenarios but doesn't explore different parameter values or other non-IID patterns
- Why unresolved: Paper only tests one specific non-IID scenario, limiting understanding of DDDR's robustness to varying degrees of data heterogeneity
- What evidence would resolve it: Experiments with different Dirichlet parameters (e.g., 0.1, 1.0, 10.0) and other non-IID patterns (e.g., label distribution skew, quantity skew), comparing performance across scenarios

## Limitations
- Lacks comparison against non-generative replay methods like EWC or MAS in federated settings
- Privacy analysis is superficial - only mentions embeddings are communicated without analyzing information leakage risk
- Computational cost analysis is missing - generating synthetic data for all previous tasks at each round could be expensive

## Confidence
- **High confidence**: Federated class inversion mechanism using pre-trained diffusion models is technically sound
- **Medium confidence**: Contrastive learning improves domain generalization - supported by results but mechanism explanation is brief
- **Medium confidence**: Synthetic current task data reduces non-IID effects - experimental support but theoretical justification is limited

## Next Checks
1. Implement ablation study removing contrastive loss to quantify its contribution to performance gains
2. Test method robustness with extreme non-IID settings (e.g., each client only sees 1-2 classes initially)
3. Measure computational overhead of generating synthetic data versus training local generative models