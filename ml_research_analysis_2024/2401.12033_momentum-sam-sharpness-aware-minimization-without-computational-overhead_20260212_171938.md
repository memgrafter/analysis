---
ver: rpa2
title: 'Momentum-SAM: Sharpness Aware Minimization without Computational Overhead'
arxiv_id: '2401.12033'
source_url: https://arxiv.org/abs/2401.12033
tags:
- msam
- momentum
- loss
- sharpness
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Momentum-SAM (MSAM), an optimization algorithm
  that reduces loss sharpness in deep neural networks without doubling computational
  overhead compared to SAM. The key idea is to perturb parameters using the negative
  momentum vector direction instead of computing an additional gradient ascent step
  as in SAM.
---

# Momentum-SAM: Sharpness Aware Minimization without Computational Overhead

## Quick Facts
- arXiv ID: 2401.12033
- Source URL: https://arxiv.org/abs/2401.12033
- Reference count: 40
- MSAM achieves comparable or better test accuracy than SAM on CIFAR100 and ImageNet while being almost twice as fast.

## Executive Summary
This paper proposes Momentum-SAM (MSAM), an optimization algorithm that reduces loss sharpness in deep neural networks without doubling computational overhead compared to SAM. The key idea is to perturb parameters using the negative momentum vector direction instead of computing an additional gradient ascent step as in SAM. MSAM achieves comparable or better test accuracy than SAM on CIFAR100 and ImageNet while being almost twice as fast. For example, MSAM reaches 83.21% test accuracy on CIFAR100 WideResNet-28-10 compared to 84.16% for SAM, but with only 0.99x the training speed versus 0.52x for SAM. MSAM also outperforms other efficient sharpness-aware optimization methods like ESAM, LookSAM, and MESA in both speed and accuracy across multiple architectures including ResNets, ViTs, and WideResNets. The authors provide theoretical analysis and empirical validation showing that perturbations in the negative momentum direction effectively minimize sharpness while maintaining generalization.

## Method Summary
MSAM replaces SAM's gradient ascent step with momentum-based perturbation, using the negative momentum vector direction to estimate sharpness. The algorithm maintains a momentum buffer that accumulates gradients over iterations, providing an approximation of the full-dataset gradient. Parameters are perturbed using this momentum direction before computing gradients, effectively minimizing sharpness without the additional forward/backward pass required by SAM. Key hyperparameters include learning rate (0.5 for CIFAR100 WRNs, 0.1 for ResNet50, 1e-3 for ImageNet), weight decay (5e-4 to 1e-3), perturbation strength ρ (typically 0.3-3), momentum factor (0.9), batch size (256 for CIFAR100, 1024 for ImageNet), and cosine learning rate scheduler with warm-up for ViTs.

## Key Results
- MSAM reaches 83.21% test accuracy on CIFAR100 WideResNet-28-10 compared to 84.16% for SAM, with only 0.99x training speed versus 0.52x for SAM
- MSAM reduces feature rank more effectively than SAM (3791 vs 4775 for WRN-16-4 on CIFAR100), finding more distinct and task-specific features
- MSAM outperforms ESAM, LookSAM, and MESA in both speed and accuracy across multiple architectures including ResNets, ViTs, and WideResNets

## Why This Works (Mechanism)

### Mechanism 1
Perturbing in the negative momentum direction effectively minimizes sharpness because the SGD update overshoots local minima, creating a negative slope in the momentum direction. The momentum vector accumulates gradients over iterations, representing an approximation of the gradient on a larger batch (potentially the full dataset). When parameters are updated with SGD, they overshoot local minima in the momentum direction. This overshooting creates a negative slope when the loss is evaluated on the next batch, making the negative momentum direction suitable for sharpness estimation.

### Mechanism 2
MSAM approximates SAM's gradient calculations while requiring minimal computational overhead because the momentum vector provides a good approximation of the full-dataset gradient. SAM computes gradients at perturbed positions to minimize sharpness, but this requires an additional forward/backward pass. MSAM uses the momentum vector (which averages gradients over multiple batches) as a perturbation direction, effectively approximating the full-dataset gradient without additional computation. The similarity between MSAM and SAM gradients is validated by measuring the angle between them.

### Mechanism 3
MSAM improves generalization by reducing feature rank more effectively than SAM, finding more distinct and task-specific features. Both SAM and MSAM reduce sharpness, but MSAM additionally reduces the rank of features (measured as the number of linearly independent feature vectors). Lower feature rank indicates more specialized, task-relevant features rather than general features, leading to better generalization.

## Foundational Learning

- **Sharpness-aware minimization and generalization**: Understanding SAM's mechanism and why it works is crucial to understanding MSAM's improvements. Quick check: What is the key difference between SAM's approach to minimizing sharpness and traditional ERM (Empirical Risk Minimization)?

- **Momentum-based optimization and Nesterov Accelerated Gradient**: MSAM replaces SAM's gradient ascent step with momentum-based perturbation. Understanding how momentum works and how NAG differs from standard momentum is essential to grasping MSAM's innovation. Quick check: How does NAG's gradient calculation differ from standard momentum, and why does this matter for MSAM?

- **Loss landscape analysis and curvature**: The paper extensively analyzes loss landscapes, curvature in different directions, and sharpness metrics. Understanding these concepts is crucial for interpreting the experimental results and theoretical analysis. Quick check: Why does the paper measure curvature in momentum, gradient, and random directions, and what does this tell us about each optimizer's behavior?

## Architecture Onboarding

- **Component map**: Base optimizer (SGD/AdamW) → Momentum buffer update → Parameter perturbation using negative momentum direction → Loss calculation at perturbed position → Gradient computation → Parameter update
- **Critical path**: 1. Initialize weights and momentum buffer 2. For each iteration: Sample batch → Compute loss at perturbed parameters → Compute gradient at perturbed position → Update parameters using standard momentum update rule → Update momentum buffer with new gradient 3. After final iteration, remove last perturbation
- **Design tradeoffs**: Computational efficiency vs. approximation accuracy (MSAM trades some precision for ~2x speedup); Momentum buffer dependency (performance depends on momentum buffer quality); Hyperparameter sensitivity (ρMSAM needs tuning but shows stability)
- **Failure signatures**: Poor generalization despite fast training (momentum buffer isn't providing good sharpness estimates); Training instability with high ρ (overshooting too aggressive); No improvement over baseline (momentum buffer isn't representing high-curvature directions)
- **First 3 experiments**: 1. Implement MSAM on CIFAR100 with WideResNet-28-10, compare test accuracy and training speed against SGD and SAM across different ρ values 2. Measure cosine similarity between momentum vector and gradient over training epochs to verify negative slope hypothesis 3. Compare feature ranks of models trained with SGD, SAM, and MSAM to validate feature specialization hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
Does scheduling the perturbation strength ρ during training improve MSAM's performance compared to using a fixed optimal value? The paper mentions that MSAM loses performance on large ViT models when high ρ values are optimal for SAM, suggesting scheduling could help. It also notes that perturbations during warmup cause instability and that investigating ρ-scheduling is of high interest.

### Open Question 2
What is the exact mechanism by which MSAM reduces feature rank more effectively than SAM, and how does this contribute to improved generalization? The paper found that MSAM reduces feature rank (from 5019 to 3791 for WRN-16-4 on CIFAR100) more than SAM (4775), but does not explain the underlying mechanism or its relationship to generalization.

### Open Question 3
Why do MSAM perturbations in negative momentum direction cause loss ascent while positive momentum perturbations do not, and what are the implications for other sharpness-aware methods? The paper shows that negative momentum perturbations cause loss increase (suitable for sharpness estimation) while positive perturbations decrease performance, but only provides empirical observations without theoretical explanation.

## Limitations
- Core claim about negative momentum direction relies on SGD's overshooting behavior, which lacks direct empirical validation
- Feature rank analysis only demonstrated on a single model-dataset combination (WRN-16-4 on CIFAR100)
- Theoretical justification for negative momentum direction is mathematically sound but relies on empirical assumptions about SGD behavior

## Confidence

- **High confidence**: Computational efficiency claims - directly measurable and consistently demonstrated across experiments
- **Medium confidence**: Test accuracy comparisons with SAM - results show comparable performance but with some variation across architectures and datasets
- **Medium confidence**: Theoretical justification for negative momentum direction - mathematically sound but relies on empirical assumptions about SGD behavior
- **Low confidence**: Feature rank generalization hypothesis - only tested on one model, theoretical connection to generalization is plausible but not rigorously established

## Next Checks

1. **Direct sharpness measurement**: Implement and compare sharpness metrics (e.g., local flatness, PAC-Bayes bounds) for models trained with SGD, SAM, and MSAM to directly validate that MSAM achieves comparable sharpness reduction to SAM despite faster computation.

2. **Momentum-perturbation ablation**: Systematically test whether perturbation normalization is truly unnecessary by comparing MSAM with and without normalization across multiple architectures and datasets, measuring both accuracy and training stability.

3. **Feature rank generalization**: Extend the feature rank analysis to additional model-dataset combinations (e.g., ResNet50 on ImageNet, ViT on CIFAR100) and correlate rank reduction with generalization gap to establish whether this relationship is consistent across settings.