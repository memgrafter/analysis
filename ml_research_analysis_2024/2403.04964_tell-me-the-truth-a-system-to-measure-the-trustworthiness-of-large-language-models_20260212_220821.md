---
ver: rpa2
title: 'Tell me the truth: A system to measure the trustworthiness of Large Language
  Models'
arxiv_id: '2403.04964'
source_url: https://arxiv.org/abs/2403.04964
tags:
- knowledge
- triplets
- sentences
- body
- trust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a systematic method to measure the trustworthiness
  of Large Language Models (LLMs) by comparing their outputs against a predefined
  domain-specific knowledge graph. The approach involves creating a "truster" component
  that encodes a body of knowledge as a semantic graph with human expert validation,
  and a "validator" component that measures semantic distance between LLM-generated
  sentences and this knowledge base using vector embeddings.
---

# Tell me the truth: A system to measure the trustworthiness of Large Language Models

## Quick Facts
- arXiv ID: 2403.04964
- Source URL: https://arxiv.org/abs/2403.04964
- Authors: Carlo Lipizzi
- Reference count: 0
- Primary result: Introduces systematic method to measure LLM trustworthiness by comparing outputs against domain-specific knowledge graphs

## Executive Summary
This paper presents a systematic approach to measure the trustworthiness of Large Language Models (LLMs) by comparing their outputs against a predefined domain-specific knowledge graph. The system employs a "truster" component that encodes domain knowledge as a semantic graph with human expert validation, and a "validator" component that measures semantic distance between LLM-generated sentences and this knowledge base using vector embeddings. A proof-of-concept case study on supply chain concepts demonstrated the system's ability to classify LLM responses as compatible, partially compatible, or incompatible with the knowledge base. The method addresses the critical need for objective trustworthiness measurement in domains like healthcare and finance where factual accuracy is paramount.

## Method Summary
The system creates a "truster" component by extracting subject-predicate-object triplets from domain text, building a semantic graph, validating with subject matter experts (SMEs), transforming back to sentences, and vectorizing using all-roberta-large-v1. The "validator" component generates triplets from LLM answers, transforms to sentences, vectorizes, measures cosine similarities with the knowledge base, and applies empirically-set thresholds to classify responses as compatible, partially compatible, or incompatible. The approach relies on semantic similarity in embedding space to assess factual trustworthiness.

## Key Results
- Successfully classified LLM responses on supply chain concepts as compatible, partially compatible, or incompatible
- Demonstrated feasibility of using semantic distance in embedding space for trustworthiness measurement
- Validated the importance of human-in-the-loop SME validation for domain-specific accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The system measures trustworthiness by comparing semantic similarity between LLM-generated answers and a validated knowledge base represented as sentence embeddings.
- Mechanism: The "validator" component transforms both the domain knowledge and LLM outputs into sentence embeddings, then computes cosine similarity scores. Answers are classified as compatible if their similarity scores exceed predefined thresholds.
- Core assumption: Semantic similarity in embedding space correlates with trustworthiness of factual content.
- Evidence anchors:
  - [abstract] "measure semantic distance between LLM-generated sentences and this knowledge base using vector embeddings"
  - [section] "For this study, a plain cosine similarity was used as a measurement embedded in the specific vector database"
  - [corpus] Weak - corpus neighbors focus on related trustworthiness topics but don't directly address semantic similarity measurement
- Break condition: When domain knowledge contains nuanced or context-dependent information that doesn't map well to static embeddings, or when LLM outputs use paraphrasing that reduces semantic similarity despite being factually correct.

### Mechanism 2
- Claim: Human-in-the-loop validation through subject matter experts (SMEs) ensures domain-specific accuracy of the knowledge base.
- Mechanism: SMEs review and edit the semantic graph representation of the domain knowledge, correcting inaccuracies before the system uses it for trustworthiness measurement.
- Core assumption: Human experts can identify and correct domain-specific inaccuracies that automated systems might miss.
- Evidence anchors:
  - [abstract] "a 'truster' component that encodes a body of knowledge as a semantic graph with human expert validation"
  - [section] "Having an SME editing and validating the graph... humans can edit graphs easily"
  - [corpus] Weak - corpus neighbors don't discuss human-in-the-loop validation processes
- Break condition: When SME expertise is insufficient for the domain complexity, or when SMEs introduce their own biases during validation.

### Mechanism 3
- Claim: Triplet representation (subject-predicate-object) provides a structured, interpretable format for encoding domain knowledge.
- Mechanism: The system converts domain text into subject-predicate-object triplets, which are then transformed into sentences and vectorized for comparison with LLM outputs.
- Core assumption: Triplet representation preserves semantic relationships while being compatible with embedding-based comparison.
- Evidence anchors:
  - [abstract] "predefined ground truth, represented as a knowledge graph of the domain"
  - [section] "The goal of this phase is to represent the body of knowledge in a form that is simple yet powerful to model and understand information"
  - [corpus] Weak - corpus neighbors don't discuss triplet-based knowledge representation
- Break condition: When complex domain relationships cannot be adequately captured in simple triplet form, or when triplet extraction introduces errors that propagate through the system.

## Foundational Learning

- Concept: Knowledge Graph Construction
  - Why needed here: The system relies on converting domain knowledge into a graph structure for validation and comparison
  - Quick check question: How would you convert "supply chain includes procurement" into a knowledge graph node and edge structure?

- Concept: Vector Embedding and Similarity
  - Why needed here: The core measurement mechanism uses cosine similarity between sentence embeddings to assess trustworthiness
  - Quick check question: If two sentences have embeddings [0.8, 0.2] and [0.6, 0.4], what is their cosine similarity?

- Concept: Human-in-the-Loop Systems
  - Why needed here: SME validation is critical for ensuring domain-specific accuracy of the knowledge base
  - Quick check question: What criteria would you use to select appropriate SMEs for validating medical domain knowledge?

## Architecture Onboarding

- Component map: Trustor (knowledge base creation) → Validator (LLM output analysis) → Threshold comparator → Results output
- Critical path: Knowledge collection → Triplet generation → Graph validation → Sentence transformation → Vectorization → Similarity measurement → Threshold comparison
- Design tradeoffs: Using sentence embeddings prioritizes semantic meaning over exact phrasing, but may miss factual errors in paraphrased content; human validation ensures domain accuracy but adds latency and cost
- Failure signatures: False negatives (rejecting correct answers with different phrasing), false positives (accepting incorrect answers with similar wording), SME bias in knowledge base creation
- First 3 experiments:
  1. Test the system with simple factual questions where ground truth is unambiguous
  2. Vary the similarity threshold to observe sensitivity and specificity trade-offs
  3. Compare results using different embedding models (e.g., all-roberta-large-v1 vs. alternatives)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different vectorization models affect the accuracy of trustworthiness measurements?
- Basis in paper: [explicit] The paper mentions using all-roberta-large-v1 model for vectorization and suggests comparing it with other available options
- Why unresolved: The paper only tested one vectorization model and acknowledges this needs further comparison with alternatives
- What evidence would resolve it: Comparative analysis of trustworthiness measurement accuracy using multiple different vectorization models on the same datasets

### Open Question 2
- Question: What is the optimal algorithmic approach for determining compatibility thresholds (t1 and t2)?
- Basis in paper: [explicit] The paper states thresholds were set empirically and should be "algorithmically defined" but doesn't provide such an algorithm
- Why unresolved: The current threshold determination relies on empirical settings rather than a systematic algorithmic approach
- What evidence would resolve it: Development and validation of an algorithmic method for automatically determining optimal threshold values

### Open Question 3
- Question: How does the quality of the knowledge graph representation impact trustworthiness measurement?
- Basis in paper: [explicit] The paper acknowledges that "underrepresented domains provide unsatisfactory results" and that the knowledge graph quality is critical
- Why unresolved: The proof-of-concept used a limited dataset and didn't explore how varying quality/representation affects results
- What evidence would resolve it: Systematic testing with knowledge graphs of varying quality, completeness, and representation accuracy to measure impact on trustworthiness scores

### Open Question 4
- Question: Would graph-based similarity measures outperform sentence-based vectorization for trustworthiness assessment?
- Basis in paper: [inferred] The paper mentions that "vectorizing the graphs representing the two components would be another option" but chose sentence-based vectorization
- Why unresolved: The paper acknowledges this as a potential alternative but doesn't test or compare graph-based methods
- What evidence would resolve it: Comparative study measuring trustworthiness using both graph-based and sentence-based vectorization methods on identical datasets

## Limitations

- Reliance on semantic similarity in embedding space may not capture nuanced factual correctness when LLM outputs use paraphrasing
- Human-in-the-loop SME validation adds credibility but introduces potential bias and scalability constraints
- Triplet representation may oversimplify complex domain relationships that cannot be adequately captured in simple subject-predicate-object form

## Confidence

- Measuring trustworthiness via semantic similarity: Medium confidence - method is feasible but has known limitations with paraphrased content
- Human-in-the-loop validation: Medium confidence - adds credibility but introduces potential bias and scalability issues
- Triplet representation approach: Low-Medium confidence - interpretable but may oversimplify complex relationships

## Next Checks

1. Test the system with a domain known to have complex, context-dependent relationships to evaluate performance limits
2. Conduct sensitivity analysis by varying threshold values to understand classification stability
3. Compare results with ground truth human judgments on the same LLM outputs to assess accuracy