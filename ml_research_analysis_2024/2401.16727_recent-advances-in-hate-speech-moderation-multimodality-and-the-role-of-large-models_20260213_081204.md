---
ver: rpa2
title: 'Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large
  Models'
arxiv_id: '2401.16727'
source_url: https://arxiv.org/abs/2401.16727
tags:
- hate
- speech
- detection
- these
- hateful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive analysis of recent advancements
  in hate speech (HS) moderation, with a focus on the role of large language models
  (LLMs) and large multimodal models (LMMs). The authors systematically reviewed state-of-the-art
  methodologies across text, image, video, and audio-based HS, identifying key trends
  in detection, explanation, debiasing, and counter-speech.
---

# Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models

## Quick Facts
- arXiv ID: 2401.16727
- Source URL: https://arxiv.org/abs/2401.16727
- Authors: Ming Shan Hee; Shivam Sharma; Rui Cao; Palash Nandi; Preslav Nakov; Tanmoy Chakraborty; Roy Ka-Wei Lee
- Reference count: 16
- Primary result: Comprehensive survey of recent advances in hate speech moderation, focusing on LLMs and LMMs across multiple modalities.

## Executive Summary
This survey provides a comprehensive analysis of recent advancements in hate speech (HS) moderation, with a focus on the role of large language models (LLMs) and large multimodal models (LMMs). The authors systematically reviewed state-of-the-art methodologies across text, image, video, and audio-based HS, identifying key trends in detection, explanation, debiasing, and counter-speech. They highlight the growing importance of multimodal approaches to address the complexity and subtlety of HS dissemination. The paper also identifies significant research gaps, particularly in underrepresented languages, low-resource settings, and the need for context-aware systems.

## Method Summary
The survey employed a systematic literature review methodology, searching scholarly platforms (Google Scholar, DBLP, IEEE Xplore, ACM Digital Library) using keywords such as 'hate speech', 'multimodal hate speech', and 'hateful memes'. The authors focused on state-of-the-art studies using LLMs and LMMs, analyzing HS detection, explanation, debiasing, and counter-speech approaches across modalities. The selection process prioritized studies that demonstrated innovation in hate speech moderation through large models.

## Key Results
- Multimodal approaches combining text, image, audio, and video are essential for capturing the complexity of hate speech dissemination
- Large language models can generate explainable rationales for hate speech decisions, improving transparency and trust
- Significant research gaps exist in underrepresented languages, low-resource settings, and context-aware systems for hate speech detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large multimodal models improve hate speech detection by integrating cross-modal context.
- Mechanism: LMMs encode text, image, audio, and video features together, enabling the model to capture subtle, cross-modal cues that single-modal models miss.
- Core assumption: The same hate speech instance contains complementary information across modalities, and combining them improves detection accuracy.
- Evidence anchors:
  - [abstract] "The combination of these modalities adds complexity and subtlety to HS dissemination."
  - [section] "Pro-Cap (Cao et al., 2023) addresses the issue of information loss in image-to-text conversion by prompting an LMM in a QA format, enhancing the generated caption’s quality and informativeness."
- Break condition: If one modality is missing or corrupted, the model may fail to reconstruct the complete context, reducing detection accuracy.

### Mechanism 2
- Claim: Large language models generate explainable rationales for hate speech decisions, improving transparency and trust.
- Mechanism: LLMs are prompted to produce free-text explanations that mimic human reasoning, which can be used to train or augment supervised detection models.
- Core assumption: LLMs can approximate human-level reasoning for hate speech content and generate plausible, context-aware explanations.
- Evidence anchors:
  - [abstract] "Explanation promotes transparency by clarifying why content is flagged, building trust in automated systems."
  - [section] "Wang et al. (2023a) demonstrated that GPT-3 can craft convincing and effective explanations for HS, a finding substantiated by extensive human evaluations."
- Break condition: If LLM-generated explanations are factually incorrect ("hallucinations"), trust is undermined and explanations may mislead users.

### Mechanism 3
- Claim: Context-aware models reduce false positives in hate speech detection by analyzing conversation threads and cultural nuances.
- Mechanism: Models incorporate conversational history, user-specific features, and cultural context to disambiguate between hate speech and benign content.
- Core assumption: The intent behind a statement can only be accurately determined when analyzed within its broader conversational and cultural context.
- Evidence anchors:
  - [section] "The intent behind a single message can shift based on prior exchanges and the overall tone of the conversation."
  - [section] "Recent research highlights the importance of enhancing HS detection models for adaptability in various scenarios and contexts."
- Break condition: If models rely too heavily on conversation history, they may miss standalone hate speech or overfit to specific conversational patterns.

## Foundational Learning

- Concept: Multimodal data representation and fusion
  - Why needed here: Hate speech often spans multiple modalities (text, image, audio, video), requiring techniques to encode and fuse heterogeneous data.
  - Quick check question: What are the main challenges in fusing features from text and images in hate speech detection?

- Concept: Context modeling in sequential data
  - Why needed here: Conversational context and user history are critical for accurate hate speech classification.
  - Quick check question: How does incorporating conversation history affect the detection of implicit hate speech?

- Concept: Bias detection and mitigation in NLP models
  - Why needed here: Hate speech detection models can inherit societal biases, leading to unfair classification of certain groups.
  - Quick check question: What techniques can be used to identify and mitigate demographic bias in hate speech classifiers?

## Architecture Onboarding

- Component map: Data Ingestion -> Feature Extraction -> Fusion Layer -> Detection Head -> Explanation Module -> Debiasing Module -> Counter-Speech Generator
- Critical path: 1. Data ingestion → feature extraction → fusion → detection → output 2. For explanation: Detection output → LLM prompt → explanation → output 3. For debiasing: Detection output → bias check → mitigation → final output
- Design tradeoffs:
  - Single-modal vs. multimodal models: Multimodal models capture richer context but require more computational resources
  - Black-box vs. explainable models: Explainable models build trust but may sacrifice accuracy
  - Pre-trained vs. custom models: Pre-trained models save time but may not capture domain-specific nuances
- Failure signatures:
  - High false positive rate: Likely due to bias or insufficient context modeling
  - Poor performance on low-resource languages/modalities: Indicates need for domain adaptation
  - Hallucinations in explanations: LLM explanations may be inaccurate or fabricated
- First 3 experiments:
  1. Evaluate baseline multimodal hate speech detection accuracy on FHM and HateMM datasets
  2. Compare single-modal vs. multimodal detection performance on implicit hate speech
  3. Test LLM-generated explanations for hate speech decisions using human evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are current large multimodal models at understanding cross-modality context in hate speech, particularly when seemingly benign elements combine to create harmful messages?
- Basis in paper: [explicit] The paper identifies this as a key challenge, noting that "an image that is benign on its own might become hateful when paired with specific text" and discusses the need for models to "grasp how the combination of text and images can alter the message."
- Why unresolved: While the paper discusses the challenge, it does not provide empirical evidence or comprehensive evaluation of current LMMs' performance on cross-modality context understanding in hate speech detection.
- What evidence would resolve it: Comparative studies measuring LMM performance on multimodal hate speech datasets versus unimodal models, with analysis of false positive/negative rates in cross-modality contexts.

### Open Question 2
- Question: What are the most effective strategies for adapting hate speech detection models to low-resource languages and cultural contexts?
- Basis in paper: [explicit] The authors identify this as a research gap, stating "Given the widespread presence of hate speech and its relatively consistent definitions across different forms, there is potential to extend knowledge from text-based hate speech with abundant resources to other low-resource forms of hate speech."
- Why unresolved: While the paper suggests potential approaches like cross-lingual transfer learning, it does not evaluate or compare different adaptation strategies or their effectiveness across diverse low-resource settings.
- What evidence would resolve it: Systematic evaluation of various adaptation techniques (e.g., few-shot learning, knowledge distillation, multilingual pre-training) across multiple low-resource languages and hate speech types, measuring detection accuracy and cultural sensitivity.

### Open Question 3
- Question: How can we improve the factual grounding and reduce hallucinations in large language models' explanations of hate speech detection decisions?
- Basis in paper: [explicit] The authors highlight this challenge, noting that current LLM-generated explanations "are susceptible to misinformation and require verification" and propose that future research should focus on "anchoring the explanations in verifiable facts."
- Why unresolved: While the paper identifies the problem of LLM hallucinations in hate speech explanations, it does not propose or evaluate specific methods for improving factual accuracy and grounding of these explanations.
- What evidence would resolve it: Comparative studies of different factual grounding techniques (e.g., retrieval-augmented generation, knowledge graph integration, human-in-the-loop verification) applied to LLM-generated hate speech explanations, measuring both factual accuracy and user trust.

## Limitations

- The survey relies on literature review methodology without presenting original experimental results
- Key uncertainties include the completeness of the literature search given unspecified search query details
- Lack of empirical validation for claimed mechanisms represents a significant limitation for practical deployment

## Confidence

- **High Confidence**: The documented trends in multimodal hate speech detection methodology and the identified research gaps in low-resource languages and contexts are well-supported by the systematic review.
- **Medium Confidence**: Claims about LLM-generated explanations and their effectiveness in building trust are supported by cited studies but require more empirical validation across diverse hate speech types.
- **Low Confidence**: The assertion that LMMs significantly improve detection through cross-modal context integration lacks direct evidence from the corpus, with no explicit citations demonstrating superior performance over single-modal approaches.

## Next Checks

1. Conduct experiments comparing multimodal hate speech detection performance using LMMs versus single-modal models on standardized datasets like FHM and HateMM to verify claimed improvements.
2. Perform human evaluation studies to measure the accuracy, helpfulness, and potential bias in LLM-generated explanations for hate speech decisions across different cultural contexts.
3. Implement and evaluate transfer learning approaches for hate speech detection in underrepresented languages to validate the proposed need for low-resource adaptation strategies.