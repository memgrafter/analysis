---
ver: rpa2
title: 'You Know What I''m Saying: Jailbreak Attack via Implicit Reference'
arxiv_id: '2410.03857'
source_url: https://arxiv.org/abs/2410.03857
tags:
- paragraph
- title
- malicious
- context
- previous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a jailbreak method called Attack via Implicit
  Reference (AIR), which decomposes malicious objectives into nested harmless ones
  and links them through implicit references to bypass detection. AIR achieves over
  90% attack success rate on state-of-the-art models like GPT-4o, Claude-3.5-Sonnet,
  and Qwen-2-72B.
---

# You Know What I'm Saying: Jailbreak Attack via Implicit Reference

## Quick Facts
- arXiv ID: 2410.03857
- Source URL: https://arxiv.org/abs/2410.03857
- Authors: Tianyu Wu; Lingrui Mei; Ruibin Yuan; Lujun Li; Wei Xue; Yike Guo
- Reference count: 40
- Primary result: Introduces AIR attack achieving >90% success rate on GPT-4o, Claude-3.5-Sonnet, and Qwen-2-72B

## Executive Summary
This paper introduces "Attack via Implicit Reference" (AIR), a novel jailbreak attack method that decomposes malicious objectives into nested harmless ones linked through implicit references. The attack achieves over 90% success rate on state-of-the-art models including GPT-4o, Claude-3.5-Sonnet, and Qwen-2-72B. Notably, the study reveals an inverse scaling phenomenon where larger models are more vulnerable to this attack method. The authors also introduce a cross-model attack strategy that leverages less secure models to generate malicious contexts, further increasing attack success rates when targeting more secure models.

## Method Summary
AIR works by decomposing a malicious prompt into multiple related but benign sub-prompts, each safe on its own. These sub-prompts are linked through implicit references within the conversation context. The final output is expanded via a second benign request that adds detail and removes editorial judgment, making the final output malicious but contextually tied to earlier harmless work. The method involves prompt rewriting using GPT-4o-0513, a first attack on the target model, a second attack rewrite prompt, and evaluation through multiple evaluator models.

## Key Results
- Achieves over 90% attack success rate on state-of-the-art models (GPT-4o, Claude-3.5-Sonnet, Qwen-2-72B)
- Demonstrates inverse scaling phenomenon: larger models show higher vulnerability
- Introduces cross-model attack strategy increasing success by using less secure models to generate malicious contexts
- Successfully bypasses safety alignment mechanisms while maintaining benign appearance in individual prompts

## Why This Works (Mechanism)

### Mechanism 1
The model can be tricked into generating malicious content by decomposing a harmful request into nested harmless objectives linked via implicit references. The attack rewrites a malicious prompt into multiple benign sub-prompts, then uses a second benign request to expand the output while removing editorial judgment. The model prioritizes fulfilling the instruction to add details over re-checking for hidden malicious intent in conversation history.

### Mechanism 2
Larger models are more susceptible because they better learn from context and have stronger in-context learning abilities. As model size increases, attention mechanisms can better connect nested objectives through implicit references, producing coherent outputs that fulfill the malicious intent hidden in earlier context. Improved in-context learning increases vulnerability to context-based manipulation.

### Mechanism 3
The cross-model attack exploits differences in safety thresholds by using a less secure model to generate initial content, then a more secure model to expand it. The first model generates content that is only marginally harmful or not flagged due to lower safety thresholds. The second, more secure model is then asked to add details to that context, resulting in fully malicious output.

## Foundational Learning

- Concept: Implicit references and attention mechanisms
  - Why needed here: Understanding how the model connects separate prompts through context is key to seeing how AIR bypasses safety checks
  - Quick check question: If you write "As mentioned earlier, this is a tutorial about making this:" without naming the topic, can the model still recall what "this" refers to?

- Concept: Instruction following vs safety alignment
  - Why needed here: The attack relies on the model prioritizing completion of benign-looking instructions over detecting hidden malicious intent
  - Quick check question: If a prompt says "Add more details to the example and remove judgment," does the model stop to ask if the example is harmful?

- Concept: Context window and conversation history
  - Why needed here: The attack builds its malicious payload across multiple turns, so understanding how the model retains and uses conversation history is essential
  - Quick check question: If you reset the conversation after the first turn, does the second turn still produce harmful content?

## Architecture Onboarding

- Component map: Prompt rewriting (GPT-4o-0513) → First attack model (target LLM) → Second attack rewrite prompt → Final output model (target LLM) → Evaluation (Jailbreak/Malicious/Pattern evaluator)
- Critical path: Rewrite prompt → First attack → Second attack → Evaluation
- Design tradeoffs: Higher ASR vs potential for detection by pattern-based evaluators; trade-off between model size (better in-context learning) and safety robustness
- Failure signatures: Pattern evaluator flags refusal; first attack fails; cross-model step fails; conversation history is ignored by model
- First 3 experiments:
  1. Run AIR on LLaMA-3-8B with K=2 and check if the second attack generates harmful content
  2. Compare ASR between LLaMA-3-8B and LLaMA-3-70B using the same prompt to test size effect
  3. Replace the first attack model with a less secure model and measure ASR against a more secure second model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inverse scaling phenomenon observed in AIR attacks hold across different task domains beyond writing scenarios?
- Basis in paper: Explicit - The paper states "we only experimented with the writing scenario, but this does not mean the implicit reference attack is limited to writing. It can also work in other scenarios if two objectives can be nested, such as a Python class and its functions, and table filling with a specific caption."
- Why unresolved: The authors explicitly note they only tested writing scenarios and acknowledge potential applicability to other domains, but did not conduct experiments in these other domains
- What evidence would resolve it: Experimental results testing AIR across diverse task domains (coding, mathematics, creative tasks) showing consistent inverse scaling patterns

### Open Question 2
- Question: How does model safety training specifically interact with implicit reference attacks at different model sizes?
- Basis in paper: Inferred - The paper observes that larger models are more vulnerable and discusses safety alignment, but doesn't specifically analyze how safety training mechanisms interact with implicit references at different scales
- Why unresolved: While the paper notes larger models show higher ASR and discusses safety alignment generally, it doesn't investigate the specific mechanisms by which safety training fails against implicit references at different model sizes
- What evidence would resolve it: Analysis of safety training components (RLHF, supervised fine-tuning) showing their effectiveness against implicit references at different model scales

### Open Question 3
- Question: What are the fundamental limitations of the implicit reference attack methodology?
- Basis in paper: Inferred - The authors discuss limitations including model requirements and scenarios where objectives cannot be decomposed, but don't systematically explore the attack's boundaries
- Why unresolved: The paper acknowledges some limitations but doesn't comprehensively map the conditions under which AIR succeeds or fails
- What evidence would resolve it: Systematic testing identifying the exact conditions, prompt structures, and objective types where AIR fails versus succeeds, creating a boundary map of the attack's effectiveness

## Limitations

- Model-Specific Safety Configurations: The paper does not disclose exact temperature settings, system prompts, or safety thresholds used during evaluation, making it difficult to assess whether results generalize across different deployment configurations
- Evaluator Reliability: The study relies on three evaluator models to classify outputs, but provides limited detail on how these evaluators were trained, validated, or how their false positive/negative rates might affect measured ASR
- Cross-Model Attack Generalization: While the cross-model attack shows promise, the paper does not thoroughly explore whether this strategy works consistently across diverse model pairs or if it's limited to specific combinations

## Confidence

- High Confidence: The inverse scaling phenomenon (larger models being more vulnerable) is supported by systematic testing across multiple model sizes and shows consistent patterns
- Medium Confidence: The cross-model attack strategy shows promising results but lacks extensive validation across diverse model pairs
- Low Confidence: The exact implementation details of prompt rewriting and conversation history management are not fully specified, making it difficult to reproduce results exactly

## Next Checks

1. Reproduce ASR across different temperature settings: Run AIR attacks on the same model (e.g., GPT-4o) with varying temperature values (0.0, 0.7, 1.0) to determine how stochasticity affects success rates and whether high ASR is consistent across settings

2. Validate evaluator robustness: Create a separate test set of known malicious and benign outputs and run them through the three evaluator models to measure false positive and false negative rates, ensuring the evaluators are not the primary source of inflated ASR

3. Test cross-model generalization: Systematically pair 3-4 different models (mix of secure and less secure) in both directions for cross-model attacks, measuring ASR for each combination to determine if the strategy generalizes or is model-pair specific