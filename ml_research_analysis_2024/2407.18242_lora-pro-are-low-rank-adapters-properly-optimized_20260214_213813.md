---
ver: rpa2
title: 'LoRA-Pro: Are Low-Rank Adapters Properly Optimized?'
arxiv_id: '2407.18242'
source_url: https://arxiv.org/abs/2407.18242
tags:
- lora
- fine-tuning
- low-rank
- full
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LoRA-Pro, a method that improves the performance
  of Low-Rank Adaptation (LoRA) by optimizing the gradients of the low-rank matrices.
  The authors reveal that LoRA does not approximate the optimization process of full
  fine-tuning, which leads to inferior performance.
---

# LoRA-Pro: Are Low-Rank Adapters Properly Optimized?

## Quick Facts
- arXiv ID: 2407.18242
- Source URL: https://arxiv.org/abs/2407.18242
- Authors: Zhengbo Wang; Jian Liang; Ran He; Zilei Wang; Tieniu Tan
- Reference count: 40
- Key outcome: LoRA-Pro improves LoRA performance by 6.72 points on average across 5 datasets

## Executive Summary
LoRA-Pro addresses a fundamental optimization gap in Low-Rank Adaptation (LoRA) by revealing that standard LoRA fails to properly approximate the optimization dynamics of full fine-tuning. The paper introduces the concept of "equivalent gradient" to quantify this gap and derives optimal closed-form solutions for updating low-rank matrices A and B. Through extensive experiments on natural language understanding tasks, LoRA-Pro demonstrates substantial performance improvements over standard LoRA, effectively narrowing the gap with full fine-tuning while maintaining parameter efficiency.

## Method Summary
LoRA-Pro improves LoRA by optimizing the gradients of low-rank matrices A and B to better approximate full fine-tuning. The method introduces an "equivalent gradient" concept and derives closed-form solutions for adjusting gradients during optimization. The approach involves solving a Sylvester equation to compute an adjustment matrix X, then applying this to modify the standard LoRA gradients. The paper proves that these adjustments guarantee loss decrease during optimization and demonstrates substantial performance improvements across multiple NLU tasks.

## Key Results
- LoRA-Pro achieves 6.72 points improvement over standard LoRA on average across 5 datasets
- Effectively narrows the performance gap with full fine-tuning while maintaining parameter efficiency
- Demonstrates improved optimization dynamics through equivalent gradient matching

## Why This Works (Mechanism)

### Mechanism 1
LoRA does not properly approximate the optimization dynamics of full fine-tuning, leading to performance gaps. The method reparameterizes weight changes using low-rank matrices but fails to ensure the equivalent gradient matches the full fine-tuning gradient during optimization. The equivalent gradient, defined as $\tilde{g} = sB g_A + s g_B A$, should approximate the full fine-tuning gradient $g$ but often doesn't.

### Mechanism 2
LoRA-Pro improves performance by adjusting gradients of low-rank matrices to minimize the difference between equivalent gradient and full fine-tuning gradient. The method solves an optimization problem $\min_{g_A,g_B} \|\tilde{g} - g\|_F^2$ and derives closed-form solutions that make $\tilde{g}$ closer to $g$.

### Mechanism 3
LoRA-Pro guarantees loss decrease during optimization by ensuring the adjusted gradients still satisfy gradient descent conditions. The paper proves that updating A and B with the closed-form solutions ensures $dL \leq 0$ during optimization.

## Foundational Learning

- Concept: Low-rank matrix approximation and its limitations in optimization
  - Why needed here: Understanding why LoRA's low-rank approximation doesn't translate to good optimization is key to grasping the problem
  - Quick check question: Why might approximating a matrix with a low-rank product not preserve its optimization behavior?

- Concept: Equivalent gradient and its role in LoRA optimization
  - Why needed here: The equivalent gradient is the central concept that quantifies the gap between LoRA and full fine-tuning
  - Quick check question: How is the equivalent gradient related to the gradients of matrices A and B in LoRA?

- Concept: Sylvester equations and their solutions
  - Why needed here: The optimal X in LoRA-Pro requires solving a Sylvester equation, which has specific existence conditions
  - Quick check question: Under what condition does a Sylvester equation have a unique solution?

## Architecture Onboarding

- Component map: Forward pass with W = W0 + sBA → Standard LoRA backward pass → Compute X via Sylvester equation → Adjust gradients using Theorem 3.1 → Update A and B

- Critical path: 1) Forward pass with LoRA parameters 2) Standard LoRA backward pass to get gA_lora, gB_lora 3) Compute X by solving Sylvester equation 4) Adjust gradients using Theorem 3.1 solutions 5) Update A and B with adjusted gradients

- Design tradeoffs: Computational cost - Solving Sylvester equation adds overhead but is O(r³) vs O(mn) for full fine-tuning; Memory - No additional memory beyond storing X (r×r matrix); Hyperparameters - Same as LoRA plus any affected by gradient adjustment

- Failure signatures: Numerical instability when solving Sylvester equation; Degraded performance if rank r is too small; No improvement over standard LoRA (implementation error or hyperparameters)

- First 3 experiments: 1) Verify equivalent gradient computation matches analytical formula 2) Test Sylvester equation solver with known inputs 3) Compare training loss curves between LoRA and LoRA-Pro on a small dataset

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises several implicit ones: (1) How does LoRA-Pro's performance scale with different ranks compared to standard LoRA? (2) What is the computational overhead of LoRA-Pro compared to standard LoRA, especially when using AdamW? (3) Does LoRA-Pro's improvement generalize to other parameter-efficient fine-tuning methods beyond LoRA?

## Limitations
- The computational overhead of solving Sylvester equations at each optimization step is not thoroughly analyzed
- The assumption that BT B and AAT have no shared eigenvalues is not verified experimentally
- Lacks empirical validation of intermediate steps like equivalent gradient visualization during training

## Confidence
- High confidence: The theoretical framework for equivalent gradients and gradient adjustment formulas are mathematically rigorous
- Medium confidence: The 6.72 point improvement claim is supported but the analysis of why this happens is limited
- Low confidence: The assumption that solving Sylvester equations is computationally feasible for large-scale applications

## Next Checks
1. Implement equivalent gradient visualization to empirically validate the core optimization gap during training
2. Create systematic tests to identify conditions where Sylvester equation solutions become unstable or fail
3. Measure actual runtime overhead of solving Sylvester equations across different hardware setups and model sizes