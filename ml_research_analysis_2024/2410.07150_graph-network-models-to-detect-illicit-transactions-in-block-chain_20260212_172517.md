---
ver: rpa2
title: Graph Network Models To Detect Illicit Transactions In Block Chain
arxiv_id: '2410.07150'
source_url: https://arxiv.org/abs/2410.07150
tags:
- graph
- transactions
- data
- networks
- illicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel approach using a Graph Attention Network
  with Residual Network-like architecture (GAT-ResNet) to detect illicit transactions
  in blockchain. The authors train and evaluate various models including logistic
  regression, Random Forest, XGBoost, GCN, GAT, and GAT-ResNet on the Elliptic Bitcoin
  Transaction dataset.
---

# Graph Network Models To Detect Illicit Transactions In Block Chain

## Quick Facts
- arXiv ID: 2410.07150
- Source URL: https://arxiv.org/abs/2410.07150
- Authors: Hrushyang Adloori; Vaishnavi Dasanapu; Abhijith Chandra Mergu
- Reference count: 15
- Primary result: GAT-ResNet model outperforms existing graph network models for detecting illicit blockchain transactions

## Executive Summary
This paper proposes a novel Graph Attention Network with Residual Network-like architecture (GAT-ResNet) for detecting illicit transactions in blockchain networks. The authors evaluate their approach on the Elliptic Bitcoin Transaction dataset, comparing it against traditional models (logistic regression, Random Forest, XGBoost) and standard graph networks (GCN, GAT). The GAT-ResNet achieves superior performance in terms of accuracy, reliability, and scalability, particularly demonstrated by higher Matthews correlation coefficient (MCC) scores compared to GCN and GAT models.

## Method Summary
The study implements and compares multiple machine learning models for illicit transaction detection using the Elliptic Bitcoin Transaction dataset containing 203,769 nodes and 234,355 edges. The proposed GAT-ResNet model combines Graph Attention Network architecture with residual connections inspired by ResNet. The model is trained for 1000 epochs using the Adam optimizer with a learning rate of 0.001 and weighted cross-entropy loss to address class imbalance. Evaluation metrics include Precision, Recall, F1 Score, Micro-Avg F1 Score, and MCC to assess performance on the highly imbalanced dataset (2% illicit transactions).

## Key Results
- GAT-ResNet achieves higher MCC score compared to GCN and GAT models, establishing superior reliability in illicit transaction detection
- The model demonstrates improved accuracy, reliability, and scalability over existing graph network approaches
- Weighted cross-entropy loss with a 0.3/0.7 licit/illicit ratio effectively addresses class imbalance while prioritizing minority class detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAT-ResNet outperforms standalone GCN and GAT due to residual connections that mitigate vanishing gradients and improve gradient flow in deep graph networks.
- Mechanism: Residual connections allow easier learning of identity mappings, enabling deeper architectures (3 GAT layers vs. 2 in GAT) without degradation, leading to better feature propagation and richer representations.
- Core assumption: Deeper GAT layers with residual connections will consistently yield better performance on the Elliptic dataset.
- Evidence anchors: Abstract claims GAT-ResNet "outperforms existing graph network models" with "higher MCC score"; section mentions combining GAT strengths with residual connections modeled after ResNet.

### Mechanism 2
- Claim: GAT's attention mechanism provides superior feature weighting over GCN's fixed normalization by adaptively focusing on relevant neighboring nodes.
- Mechanism: GAT uses learned attention coefficients to weight neighbor influence dynamically, whereas GCN applies static normalized sum, helping distinguish illicit patterns not captured by uniform aggregation.
- Core assumption: Learned attention weights meaningfully differentiate illicit from licit transactions in the graph structure.
- Evidence anchors: Abstract notes GAT-ResNet "combines the strengths of Graph Attention Network (GAT) and Residual Network (ResNet)"; section explains GAT employs self-attention mechanism to assign varying weights to neighbors.

### Mechanism 3
- Claim: Weighted cross-entropy loss addresses class imbalance by prioritizing the minority illicit class, leading to better recall and MCC scores.
- Mechanism: Higher loss weight assigned to illicit transactions penalizes misclassification more heavily, encouraging better minority class recall at potential expense of lower overall accuracy.
- Core assumption: 2% illicit class imbalance is significant enough that unweighted loss would bias model toward majority class, harming detection performance.
- Evidence anchors: Section states "weighted cross-entropy loss function during the training of the GAT-ResNet model"; mentions "optimal ratio for the licit and illicit classes was 0.3/0.7" after hyperparameter tuning.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) basics - how node features are aggregated from neighbors using message passing.
  - Why needed here: Entire model relies on propagating and transforming transaction node features through the graph; understanding this is critical for tuning and debugging.
  - Quick check question: In a simple GCN layer, how are a node's new features computed from its neighbors?

- Concept: Attention mechanisms in neural networks - how weights are learned to focus on important parts of the input.
  - Why needed here: GAT layers use attention to weight neighbor contributions; knowing how attention is computed helps interpret model behavior.
  - Quick check question: What role does the attention coefficient play in a GAT layer's message passing?

- Concept: Imbalanced classification and evaluation metrics - why accuracy alone can be misleading and why metrics like MCC, precision, recall, and F1 are important.
  - Why needed here: Dataset has 2% illicit class; understanding class imbalance and appropriate metrics is key to correctly evaluating model performance.
  - Quick check question: Why might a high accuracy be misleading in a highly imbalanced dataset?

## Architecture Onboarding

- Component map: Elliptic Bitcoin graph (nodes=transactions, edges=flows, 166 features) -> GAT-ResNet model (3 GAT layers, 4 attention heads, residual connections) -> Weighted cross-entropy loss -> Adam optimizer -> Class probabilities

- Critical path:
  1. Load and preprocess graph data (nodes, edges, features)
  2. Build GAT-ResNet model with residual connections
  3. Train with weighted cross-entropy loss and Adam optimizer
  4. Evaluate using precision, recall, F1, micro-F1, and MCC

- Design tradeoffs:
  - Depth vs. overfitting: 3 GAT layers chosen over more to balance expressiveness and generalization
  - Attention heads: 4 heads per layer; more heads could capture more patterns but increase computation
  - Residual connections: Enable deeper networks but add parameters; without them, performance may degrade
  - Class weighting: Improves minority recall but may reduce overall accuracy if over-weighted

- Failure signatures:
  - Low recall on illicit class despite high overall accuracy → class imbalance not handled well
  - Training loss decreases but validation loss plateaus/increases → overfitting
  - Similar performance between GAT and GAT-ResNet → residual connections not helping
  - High false positive rate → model too sensitive, may need threshold adjustment or feature tuning

- First 3 experiments:
  1. Train GAT-ResNet with unweighted cross-entropy loss; compare recall/MCC vs. weighted version
  2. Remove residual connections and compare performance to full GAT-ResNet
  3. Vary the number of attention heads (e.g., 2, 4, 8) and observe impact on metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can combining traditional models (like Random Forest or XGBoost) with graph networks improve performance beyond what either approach achieves alone?
- Basis in paper: [inferred] Authors note traditional models outperform graph networks but suggest ensemble modeling as future direction
- Why unresolved: Paper only compares standalone models; no experiments test hybrid approaches
- What evidence would resolve it: Experimental results showing whether ensemble models achieve higher accuracy, precision, recall, or MCC than best individual model

### Open Question 2
- Question: How can the black-box nature of graph network predictions be addressed to improve interpretability for AML applications?
- Basis in paper: [inferred] Authors mention underlying mechanics and prediction triggers of graph networks remain a "black box"
- Why unresolved: Paper proposes improved performance but doesn't address model interpretability, crucial for real-world AML deployment where explainability matters
- What evidence would resolve it: Methods or tools providing clear explanations for graph network predictions, such as feature importance analysis or attention visualization, validated on Elliptic dataset

### Open Question 3
- Question: Can alternative identity verification methods like Self-Sovereign Identity (SSI) improve AML effectiveness compared to traditional KYC while maintaining user privacy?
- Basis in paper: [explicit] Authors suggest SSI as future direction instead of KYC to maintain anonymity while keeping AML methods actionable
- Why unresolved: Paper briefly mentions SSI as potential future improvement without testing or comparing to existing KYC-based approaches
- What evidence would resolve it: Comparative studies showing whether SSI-based approaches achieve similar or better AML detection rates while preserving more user privacy than KYC systems

## Limitations

- Architectural detail ambiguity regarding exact residual connection implementation between GAT layers
- Dataset preprocessing transparency issues with unspecified feature engineering process
- Limited hyperparameter optimization process details for architectural choices

## Confidence

**High Confidence Claims**:
- GAT-ResNet architecture is novel and structurally sound, combining established GAT and ResNet principles
- Weighted cross-entropy loss is appropriate for addressing class imbalance in the dataset
- Evaluation metrics chosen (MCC, precision, recall, F1) are suitable for imbalanced classification problems

**Medium Confidence Claims**:
- GAT-ResNet outperforms standalone GCN and GAT models, though specific implementation details enabling this advantage are not fully specified
- Effectiveness of residual connections in this specific application, as mechanism could vary based on implementation choices
- Generalization capability of model to other blockchain datasets, as only one dataset is used

**Low Confidence Claims**:
- Specific contribution of each architectural component (attention heads, residual connections, layer depth) to overall performance
- Robustness of results across different random seeds and data splits
- Scalability claims without empirical validation on larger or more complex blockchain graphs

## Next Checks

1. **Architecture Reproduction Test**: Implement GAT-ResNet with multiple residual connection strategies (standard residual addition, dense connections, attention-based residual weighting) and compare performance to isolate impact of different residual implementations.

2. **Feature Engineering Validation**: Reconstruct the 166 features from raw Elliptic dataset using documented blockchain feature extraction methods, then systematically evaluate which feature subsets contribute most to illicit transaction detection performance.

3. **Cross-Dataset Generalization**: Test GAT-ResNet model on at least two additional blockchain transaction datasets with different characteristics (Ethereum, Monero) to validate claims of superior accuracy, reliability, and scalability beyond single dataset used.