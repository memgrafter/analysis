---
ver: rpa2
title: Measuring and Reducing LLM Hallucination without Gold-Standard Answers
arxiv_id: '2402.10412'
source_url: https://arxiv.org/abs/2402.10412
tags:
- answers
- fewl
- answer
- reference
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FEWL, a metric to measure LLM hallucination
  without gold-standard answers. FEWL leverages reference LLMs as proxies, weighing
  their expertise based on how disagreeably they respond to intentionally wrong answers
  and their consistency across similar questions.
---

# Measuring and Reducing LLM Hallucination without Gold-Standard Answers

## Quick Facts
- arXiv ID: 2402.10412
- Source URL: https://arxiv.org/abs/2402.10412
- Reference count: 40
- Key outcome: Introduces FEWL metric to measure LLM hallucination without gold-standard answers, outperforming baselines in hallucination detection and enabling hallucination reduction via ICL and SFT without gold labels.

## Executive Summary
This paper addresses the challenge of measuring and reducing LLM hallucination without relying on expensive gold-standard answers. The authors propose FEWL (Factualness Evaluations via Weighting LLMs), a metric that leverages multiple reference LLMs as proxies for truth, weighting their expertise based on how disagreeably they respond to intentionally wrong answers and their consistency across similar questions. Theoretical analysis shows FEWL has guarantees for identifying the least hallucinating LLM, and empirical results demonstrate superior performance compared to baseline approaches. The framework also enables hallucination reduction through in-context learning and supervised fine-tuning without requiring ground-truth labels.

## Method Summary
FEWL measures hallucination by querying multiple reference LLMs for answers, generating intentionally wrong (IW) and corrected (CO) answers, and computing expertise weights based on how disagreeably each reference LLM responds to IW answers and how much their knowledge appears superficial (laziness penalty). The final score aggregates similarity between the evaluated answer and reference answers, weighted by expertise scores and adjusted for laziness. For hallucination reduction, FEWL scores guide in-context learning by selecting higher-quality reference answers and direct supervised fine-tuning using FEWL-selected samples.

## Key Results
- FEWL outperforms baselines in distinguishing hallucinated vs non-hallucinated answers on Truthful-QA, CHALE, and HaluEval datasets
- FEWL accurately ranks LLMs by hallucination severity without gold-standard answers
- Hallucination reduction via ICL and SFT guided by FEWL achieves lower hallucination rates at reduced cost compared to human annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FEWL can identify the least hallucinating LLM without gold-standard answers by weighting reference LLMs based on their disagreement with intentionally wrong answers and consistency across similar questions.
- Mechanism: The framework generates intentionally wrong (IW) answers and their corrected (CO) versions, then measures how much each reference LLM disagrees with the IW answers and agrees with the CO answers. It also penalizes reference LLMs that give similar answers to similar questions (laziness penalty).
- Core assumption: Reference LLMs can be reliably evaluated for expertise by their responses to IW/CO answer pairs, and expert LLMs show less similarity in answers across similar questions.
- Evidence anchors:
  - [abstract]: "FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers."
  - [section]: "we measure the expertise of each reference LLM in two ways: (1) how disagreeable hi is to the wrong answers to x and (2) how much hi's knowledge on the topic in x is superficial and unreliable."
  - [corpus]: Weak - no direct evidence about reference LLM quality assessment methods in neighbors
- Break condition: If reference LLMs are all similarly poor quality, the weighting becomes unreliable and the framework cannot distinguish better from worse models.

### Mechanism 2
- Claim: FEWL has theoretical guarantees that it can score the best LLM highest even without gold-standard answers, by framing the problem as a variational f-divergence between distributions.
- Mechanism: The FEWL score is interpreted as a lower bound on f-divergence between the distribution of the LLM being evaluated and reference LLMs. Under assumptions of constant expertise and conditional independence, the optimal LLM receives the highest expected score.
- Core assumption: There exists a mapping from optimal answers to lower-quality answers that preserves conditional independence between reference LLMs and the LLM being evaluated given the optimal answers.
- Evidence anchors:
  - [section]: "Theorem 3.4. FEWL(A(X), {hi(X)}i∈[N ]) has the following theoretical guarantee for evaluating the answer from the LLM generation A"
  - [abstract]: "We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accurate hallucination measures than naively using reference LLMs."
  - [corpus]: Weak - theoretical guarantees are not discussed in neighbor papers
- Break condition: If the conditional independence assumption is violated (e.g., reference LLMs and evaluated LLM share systematic biases), the theoretical guarantee fails.

### Mechanism 3
- Claim: FEWL enables hallucination reduction through in-context learning and supervised fine-tuning without gold labels by selecting better quality answers from reference LLMs.
- Mechanism: FEWL scores are used to select which answers from reference LLMs to include in in-context learning examples or supervised fine-tuning datasets, even when gold-standard answers are unavailable.
- Core assumption: Reference LLMs that receive higher FEWL scores are more likely to provide useful training signals for reducing hallucination in the target LLM.
- Evidence anchors:
  - [section]: "We show that we can leverage FEWL to reduce hallucination without ground-truth answers through both in-context learning and supervised fine-tuning."
  - [abstract]: "We also show how to leverage FEWL to reduce hallucination through both in-context learning and supervised fine-tuning without gold labels."
  - [corpus]: Weak - no neighbor papers discuss hallucination reduction methods
- Break condition: If FEWL scores are unreliable, using them for training data selection could reinforce poor behaviors rather than reduce hallucination.

## Foundational Learning

- Concept: f-divergence and variational inference
  - Why needed here: The theoretical analysis of FEWL relies on interpreting the score as a lower bound on f-divergence between distributions of answers.
  - Quick check question: What is the relationship between f-divergence and KL divergence, and how does variational inference provide a lower bound?

- Concept: Conditional independence in probabilistic modeling
  - Why needed here: The theoretical guarantee assumes a specific conditional independence structure between reference LLMs, optimal answers, and evaluated LLM answers.
  - Quick check question: How does the data processing inequality apply to the chain of random variables hi(X) → A*(X) → A(X)?

- Concept: Similarity metrics for semantic evaluation
  - Why needed here: FEWL uses semantic similarity between answers as a proxy for truthfulness, both for weighing reference LLMs and for the laziness penalty.
  - Quick check question: What are the trade-offs between different semantic similarity methods (e.g., cosine similarity, BERTScore) for evaluating answer quality?

## Architecture Onboarding

- Component map: Reference LLM querying system -> IW/CO answer generation pipeline -> Expertise weighting computation module -> Laziness penalty calculation component -> FEWL score aggregation engine -> Evaluation/selection interface

- Critical path:
  1. Query N reference LLMs for answers to question x
  2. Generate IW and CO answers
  3. Compute expertise scores λi(x) for each reference LLM
  4. Calculate similarity between y and each hi(x) weighted by λi(x)
  5. Find K nearest neighbor questions and compute laziness penalty
  6. Aggregate into final FEWL score

- Design tradeoffs:
  - Number of reference LLMs (N) vs. computational cost
  - Number of IW/CO pairs (K) vs. quality of expertise estimation
  - Choice of similarity metric vs. accuracy and efficiency
  - Balance between expertise weighting and laziness penalty contribution

- Failure signatures:
  - All reference LLMs receive similar expertise scores → weighting becomes ineffective
  - Laziness penalty dominates or is negligible → imbalance in scoring
  - FEWL scores show no correlation with human judgments → fundamental measurement issue

- First 3 experiments:
  1. Test FEWL on a dataset with gold answers to verify correlation between FEWL scores and ground truth hallucination labels
  2. Compare FEWL performance using different numbers of reference LLMs (N=1, 3, 5) on the same evaluation task
  3. Evaluate the impact of laziness penalty by running FEWL with and without it on the same dataset and measuring changes in ranking accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FEWL vary when using reference LLMs with significantly different levels of expertise?
- Basis in paper: [explicit] The paper discusses the importance of weighing the expertise of each reference LLM for a given question without a priori knowledge of the true answer.
- Why unresolved: The paper mentions that different LLMs exhibit varying proficiency levels across different queries, necessitating differential weighting during joint evaluation. However, it does not provide empirical evidence on how FEWL performs with reference LLMs of significantly different expertise levels.
- What evidence would resolve it: Conducting experiments using reference LLMs with varying levels of expertise (e.g., novice, intermediate, expert) and comparing the performance of FEWL in each case would provide insights into how the performance of FEWL varies with the expertise levels of reference LLMs.

### Open Question 2
- Question: Can FEWL be extended to handle other types of LLM outputs beyond question-answering tasks?
- Basis in paper: [inferred] The paper focuses on measuring and reducing LLM hallucination in question-answering tasks. However, LLMs are used for various other tasks such as text summarization, machine translation, and code generation.
- Why unresolved: The paper does not discuss the applicability of FEWL to other types of LLM outputs beyond question-answering tasks.
- What evidence would resolve it: Adapting FEWL to other types of LLM outputs and evaluating its performance in those tasks would provide insights into the generalizability of FEWL beyond question-answering tasks.

### Open Question 3
- Question: How does the choice of similarity metric affect the performance of FEWL?
- Basis in paper: [explicit] The paper mentions that the practical implementation of FEWL uses semantic similarity as an approximation.
- Why unresolved: The paper does not discuss the impact of different similarity metrics on the performance of FEWL.
- What evidence would resolve it: Conducting experiments using different similarity metrics (e.g., cosine similarity, Jaccard similarity) and comparing the performance of FEWL in each case would provide insights into how the choice of similarity metric affects the performance of FEWL.

## Limitations
- The effectiveness of FEWL depends heavily on the quality of reference LLMs; if all references share similar biases, the framework cannot distinguish better from worse models.
- The theoretical guarantees require strong conditional independence assumptions that may not hold when reference LLMs are trained on similar data or share systematic biases.
- The hallucination reduction claims assume higher FEWL scores reliably indicate more truthful answers, but this correlation needs more extensive validation across domains.

## Confidence

- FEWL's hallucination measurement accuracy (Medium): The empirical results show FEWL outperforms baselines, but the absolute performance gap and robustness across different domains need further testing.
- Theoretical guarantees (Medium): The variational f-divergence framework provides a mathematical foundation, but the practical relevance depends on the conditional independence assumption holding.
- Hallucination reduction effectiveness (Low): While the paper claims ICL and SFT guided by FEWL reduce hallucination, the magnitude of improvement and comparison to gold-standard methods is limited.

## Next Checks

1. Test FEWL on a dataset with gold answers to verify correlation between FEWL scores and ground truth hallucination labels across multiple domains.
2. Conduct ablation studies varying the number of reference LLMs and IW/CO pairs to determine sensitivity to these hyperparameters.
3. Compare hallucination reduction using FEWL-guided ICL/SFT against methods using human-annotated gold labels to quantify the cost-accuracy tradeoff.