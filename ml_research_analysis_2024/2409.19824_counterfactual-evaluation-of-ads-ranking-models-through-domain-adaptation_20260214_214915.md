---
ver: rpa2
title: Counterfactual Evaluation of Ads Ranking Models through Domain Adaptation
arxiv_id: '2409.19824'
source_url: https://arxiv.org/abs/2409.19824
tags:
- reward
- target
- domain
- offline
- domains
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a domain-adapted reward model for offline evaluation
  of ranking models in large-scale ads recommender systems. The method uses an offline
  A/B testing simulation layer and a reward model trained with domain-specific weights
  to estimate lifts between source and target policies.
---

# Counterfactual Evaluation of Ads Ranking Models through Domain Adaptation

## Quick Facts
- arXiv ID: 2409.19824
- Source URL: https://arxiv.org/abs/2409.19824
- Reference count: 25
- Primary result: Domain-adapted reward model achieves 17.6% improvement in Recovery CV metric for offline evaluation of ads ranking models

## Executive Summary
This paper addresses the challenge of offline evaluation for ranking models in large-scale ads recommender systems, where traditional model-free methods like importance sampling are infeasible due to selection bias and complexity. The authors propose a domain-adapted reward model that uses propensity ratios and domain-specific weights to estimate lifts between source and target policies while ensuring fair performance across domains. The method is evaluated on both synthetic and real A/B test data, demonstrating significant improvements over vanilla IPS and baseline approaches, particularly in handling multiple target domains.

## Method Summary
The proposed method trains a reward model on labeled source domain data with a weighted loss function that incorporates propensity ratios between target and source domains. The model estimates rewards for target domain policies by accounting for differences in policy behavior, with weights designed to ensure fair performance across domains. The approach uses an offline A/B testing simulation layer and domain-specific weights based on propensity ratios and differences between target domains. The trained model can then estimate lifts between source and target policies, enabling counterfactual evaluation without online testing.

## Key Results
- 17.6% improvement in Recovery CV metric on real A/B test data compared to baseline methods
- Outperforms vanilla IPS and baseline approaches in synthetic experiments across multiple target domains
- Demonstrates ability to handle selection bias and policy differences in offline evaluation of ranking models

## Why This Works (Mechanism)
The domain-adapted reward model works by training on source domain data with weights that account for the differences between source and target policies. By incorporating propensity ratios (the ratio of target to source policy probabilities) into the loss function, the model learns to adjust its predictions based on how the target policies differ from the source. The additional terms in the loss function ensure that the model maintains fair performance across all target domains rather than optimizing for just one, which is critical when evaluating multiple competing policies.

## Foundational Learning
- **Propensity Score Weighting**: Adjusting for selection bias by weighting samples based on the probability of treatment assignment; needed to correct for differences between source and target policies
- **Domain Adaptation**: Transferring knowledge from a labeled source domain to an unlabeled target domain; needed to handle the lack of labeled data in target domains
- **Importance Sampling**: Estimating expectations under a target distribution using samples from a different distribution; needed as the theoretical foundation for counterfactual evaluation
- **Reward Modeling**: Learning a model to predict the reward or outcome of actions; needed to enable counterfactual predictions without running actual experiments
- **Policy Evaluation**: Assessing the performance of a policy using historical data; needed to validate ranking models without costly online A/B tests

## Architecture Onboarding

**Component Map**: Source Data -> Reward Model Training -> Propensity Ratio Estimation -> Weighted Loss Computation -> Policy Lift Estimation

**Critical Path**: The critical path involves estimating propensity ratios, training the reward model with weighted loss, and using the trained model to estimate lifts between domains. Each component must function correctly for accurate evaluation.

**Design Tradeoffs**: The method trades computational complexity (training a reward model) for reduced bias compared to simple importance sampling. The weighted loss function adds complexity but enables fair evaluation across multiple domains.

**Failure Signatures**: Poor recovery metrics indicate inaccurate propensity ratio estimates or model overfitting to the source domain. High variance in Recovery CV suggests the model struggles with certain target domains.

**3 First Experiments**:
1. Validate propensity ratio estimation by comparing predicted vs actual policy differences on synthetic data
2. Test reward model training with different Î² values to find optimal hyperparameter settings
3. Evaluate model performance on a simple two-domain scenario before scaling to multiple domains

## Open Questions the Paper Calls Out

**Open Question 1**: How sensitive is the domain-adapted reward model to the choice of the hyperparameter Î² controlling the contribution of the term |ğ‘¤ğ‘˜ğ‘– âˆ’ ğ‘¤ğ‘˜â€²ğ‘–|? The paper does not provide a sensitivity analysis of the model performance with respect to different values of Î².

**Open Question 2**: How does the domain-adapted reward model perform in scenarios with a large number of target domains or policies? The paper discusses performance with multiple target domains but does not explore scenarios with a very large number of domains.

**Open Question 3**: What are the limitations of using the coefficient of variance of recovery (ğ‘…ğ‘’ğ‘ğ¶ğ‘‰) as the primary performance metric, and are there alternative metrics that could be considered? The paper does not discuss potential limitations of using ğ‘…ğ‘’ğ‘ğ¶ğ‘‰ or explore other metrics.

## Limitations

- Specific architecture and hyperparameters of the reward model are not specified, making exact reproduction challenging
- Method for estimating propensity ratios is vaguely described without implementation details
- Results primarily validated on synthetic data with only one real A/B test result reported
- Paper does not address potential distribution shifts beyond the domain differences considered

## Confidence

- **Medium Confidence**: Overall methodology for domain-adapted reward modeling is sound and well-motivated
- **Medium Confidence**: Synthetic experiment results showing improvement over baseline methods
- **Low Confidence**: Real-world A/B test validation due to limited reporting and lack of detailed methodology

## Next Checks

1. Implement the propensity ratio estimation method using multiple approaches (e.g., logistic regression, neural networks) to verify robustness of results
2. Conduct ablation studies to determine the impact of the Î² hyperparameter and the weighted loss terms on model performance
3. Test the method across diverse real-world datasets with varying degrees of domain shift to assess generalizability beyond the reported case