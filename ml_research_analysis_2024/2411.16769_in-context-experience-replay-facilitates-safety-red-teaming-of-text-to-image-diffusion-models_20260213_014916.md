---
ver: rpa2
title: In-Context Experience Replay Facilitates Safety Red-Teaming of Text-to-Image
  Diffusion Models
arxiv_id: '2411.16769'
source_url: https://arxiv.org/abs/2411.16769
tags:
- prompts
- prompt
- red-teaming
- safety
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ICER is a novel red-teaming framework that leverages LLMs and bandit
  optimization to systematically evaluate safety mechanisms in text-to-image models.
  By maintaining a database of past successful jailbreaking attempts and using Thompson
  Sampling to select relevant in-context exemplars, ICER generates interpretable adversarial
  prompts that maintain semantic similarity with original inputs while bypassing safety
  filters.
---

# In-Context Experience Replay Facilitates Safety Red-Teaming of Text-to-Image Diffusion Models

## Quick Facts
- **arXiv ID**: 2411.16769
- **Source URL**: https://arxiv.org/abs/2411.16769
- **Reference count**: 35
- **Primary result**: Novel red-teaming framework using LLMs and bandit optimization achieves significantly higher failure rates than baseline methods

## Executive Summary
ICER is a novel red-teaming framework that leverages LLMs and bandit optimization to systematically evaluate safety mechanisms in text-to-image models. By maintaining a database of past successful jailbreaking attempts and using Thompson Sampling to select relevant in-context exemplars, ICER generates interpretable adversarial prompts that maintain semantic similarity with original inputs while bypassing safety filters. The framework achieves significantly higher failure rates compared to existing prompt attack methods across four different safe T2I models, demonstrating its effectiveness in identifying model vulnerabilities. ICER's prompts are more natural and interpretable than those from baseline methods, making them more representative of real-world attack scenarios.

## Method Summary
ICER combines Large Language Models with Thompson Sampling bandit optimization to generate adversarial prompts for text-to-image models. The framework maintains an experience database of successful jailbreaking attempts, using past experiences to guide the generation of new problematic prompts. When presented with a target prompt, ICER selects relevant in-context exemplars from its database, uses an LLM to generate elaborated prompts through upsampling, and evaluates both semantic similarity and safety compliance. The system iteratively updates its experience database based on success/failure outcomes, creating an adaptive red-teaming process that becomes more effective over time.

## Key Results
- ICER achieves significantly higher failure rates than baseline methods across four different safe T2I models (ESD, SLD-MAX, Receler, AdvUnlearn)
- ICER generates more natural and interpretable prompts compared to existing attack methods
- The framework demonstrates effective transferability, with prompts discovered against one model successfully bypassing safety mechanisms in others
- ICER's adaptive approach shows consistent improvement over baseline methods in both absolute performance and iteration speed

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Bandit optimization via Thompson Sampling effectively selects exemplars that maximize jailbreaking success while balancing exploration.
- **Mechanism**: Maintains a database of past successful attempts with reward tracking (α, β parameters). Samples exemplars based on probability distributions derived from past performance, updating these distributions based on success/failure outcomes.
- **Core assumption**: Past successful jailbreaking attempts contain patterns that can guide future attempts, and these patterns remain relevant across similar target models.
- **Evidence anchors**: [abstract] "leverages Large Language Models (LLMs) and a bandit optimization-based algorithm to generate interpretable and semantic meaningful problematic prompts by learning from past successful red-teaming attempts"
- **Break condition**: If the reward distribution becomes skewed toward suboptimal exemplars, or if the model safety mechanisms change significantly between attempts.

### Mechanism 2
- **Claim**: Prompt dilution (upsampling) transforms concise prompts into elaborate descriptions that bypass safety filters while maintaining semantic intent.
- **Mechanism**: LLM takes short prompts and extends them with additional context, descriptors, and artistic terminology that maintain the core concept but evade detection systems.
- **Core assumption**: Safety mechanisms are more likely to flag concise prompts containing explicit terms, while elaborate, artistic descriptions can convey the same concept without triggering filters.
- **Evidence anchors**: [abstract] "Our approach employs a bandit algorithm to efficiently select relevant in-context exemplars from our accumulated playbook of red-teaming attempts, enabling LLMs to generate new, potentially problematic prompts without additional training"
- **Break condition**: If safety mechanisms begin analyzing semantic content rather than just surface features, or if LLM upsampling consistently fails to preserve semantic intent.

### Mechanism 3
- **Claim**: Experience replay allows the system to continuously improve by learning from both successful and failed attempts, creating an adaptive attack framework.
- **Mechanism**: Maintains a growing database of red-teaming attempts, using successful patterns to guide future attacks while also learning from failures to avoid unproductive approaches.
- **Core assumption**: The space of effective jailbreaking techniques has structure that can be learned over time, and patterns transfer across different prompts and models.
- **Evidence anchors**: [abstract] "Our ICER efficiently probes safety mechanisms across different T2I models without requiring internal access or additional training, making it broadly applicable to deployed systems"
- **Break condition**: If the learned patterns become too specialized to initial examples, or if the database grows too large to maintain effective sampling.

## Foundational Learning

- **Concept**: Bayesian Optimization and Thompson Sampling
  - **Why needed here**: The framework treats jailbreaking prompt generation as an optimization problem over a complex, expensive-to-evaluate function (the safety mechanism), requiring efficient exploration-exploitation strategies.
  - **Quick check question**: What is the key advantage of Thompson Sampling over epsilon-greedy or upper confidence bound methods in this context?

- **Concept**: In-context learning and prompt engineering
  - **Why needed here**: The LLM generates jailbreaking prompts based on exemplars provided in the context, requiring understanding of how to craft effective system prompts and exemplar selection.
  - **Quick check question**: How does the choice of exemplars affect the LLM's ability to generate semantically consistent jailbreaking prompts?

- **Concept**: Semantic similarity metrics and content filtering
  - **Why needed here**: The framework needs to ensure jailbreaking prompts maintain semantic similarity with original content while bypassing safety filters, requiring understanding of both image and text similarity measures.
  - **Quick check question**: What is the trade-off between enforcing strict semantic similarity and achieving high jailbreaking success rates?

## Architecture Onboarding

- **Component map**: Experience Database → Thompson Sampling → Exemplar selection → LLM generation → Semantic check → Safety evaluation → Database update

- **Critical path**: Query prompt → Thompson Sampling → Exemplar selection → LLM generation → Semantic check → Safety evaluation → Database update

- **Design tradeoffs**:
  - Exemplar selection strategy: Thompson Sampling vs. random vs. epsilon-greedy
  - Semantic similarity threshold: Higher values maintain intent but reduce jailbreaking success
  - Database size: Larger databases provide more patterns but may slow sampling
  - LLM choice: Balance between model capability and computational cost

- **Failure signatures**:
  - Low semantic similarity scores indicate the LLM is not preserving original intent
  - Stalled improvement in failure rates suggests the exemplar selection is not effective
  - High variance in reward distributions indicates poor exemplar quality or selection strategy

- **First 3 experiments**:
  1. Baseline comparison: Run with k=1 exemplar vs. k=3 exemplars to measure impact of exemplar quantity
  2. Sampling strategy comparison: Thompson Sampling vs. random sampling vs. epsilon-greedy
  3. Threshold sensitivity: Vary semantic similarity threshold (0.5 to 0.9) and measure impact on both semantic consistency and failure rates

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the transferability of jailbreaking prompts vary across different types of safety mechanisms (e.g., fine-tuning vs. inference guidance vs. multi-tier filtering)?
- **Basis in paper**: [explicit] The paper analyzes transferability across four different safe T2I models with diverse safety mechanisms, showing that prompts discovered against AdvUnlearn show the highest "Universal" transferability.
- **Why unresolved**: The paper provides transferability analysis but doesn't systematically compare transferability patterns across different safety mechanism types.
- **What evidence would resolve it**: A comprehensive study categorizing jailbreaking prompts by safety mechanism type and analyzing their transferability success rates across different model architectures.

### Open Question 2
- **Question**: What specific linguistic patterns or prompt characteristics make jailbreaking attempts most effective across different safe T2I models?
- **Basis in paper**: [inferred] The paper shows that ICER generates more natural and interpretable prompts compared to baselines, but the semantic similarity analysis doesn't reveal clear patterns explaining success.
- **Why unresolved**: The semantic similarity analysis between prompts and exemplars shows no significant correlations with jailbreaking success, suggesting other factors are at play.
- **What evidence would resolve it**: Detailed linguistic analysis of successful vs. unsuccessful jailbreaking prompts, including token-level analysis and pattern recognition across different model types.

### Open Question 3
- **Question**: How does the performance of ICER change when applied to commercial T2I models with different levels of safety sophistication compared to open-source models?
- **Basis in paper**: [explicit] The paper tests transferability to DALL·E 3 and FLUX.1, showing high transfer rates, but doesn't systematically compare performance across different commercial safety levels.
- **Why unresolved**: The transferability tests were limited to two commercial models, and the paper doesn't explore how ICER's effectiveness varies with different safety sophistication levels.
- **What evidence would resolve it**: Systematic testing of ICER against a range of commercial T2I models with varying safety measures, including those with more advanced textual safety filters.

## Limitations

- The framework's effectiveness depends heavily on the quality and diversity of exemplars in the experience database, which may limit exploration of novel attack vectors.
- The semantic similarity constraint creates a fundamental tension between maintaining original intent and achieving jailbreaking success.
- Performance is bounded by the capabilities of the underlying LLM, which may limit the creativity and effectiveness of generated prompts.

## Confidence

- **High Confidence**: The general framework architecture and methodology are sound, as evidenced by consistent improvement in failure rates across multiple models and concepts.
- **Medium Confidence**: The claim that ICER generates more "natural and interpretable" prompts than baseline methods is supported by qualitative observations but would benefit from more rigorous human evaluation studies.
- **Low Confidence**: The transferability of learned patterns across different T2I models and safety mechanisms is assumed but not extensively validated.

## Next Checks

1. **Cross-Model Transferability Test**: Evaluate ICER-generated prompts against safety models not included in the training database to assess generalization capabilities. Measure whether success patterns transfer or if the framework overfits to specific implementations.

2. **Human Evaluation Study**: Conduct blinded evaluations with diverse human raters to assess the naturalness, interpretability, and semantic consistency of ICER-generated prompts versus baseline methods. Include cultural and linguistic diversity in the evaluation pool.

3. **Adaptive Safety Response Test**: Implement a simulated adaptive safety mechanism that updates based on observed attack patterns, then measure ICER's ability to continuously discover new vulnerabilities versus static baseline methods.