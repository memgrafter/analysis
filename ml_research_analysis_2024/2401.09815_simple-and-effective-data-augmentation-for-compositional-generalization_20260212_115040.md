---
ver: rpa2
title: Simple and effective data augmentation for compositional generalization
arxiv_id: '2401.09815'
source_url: https://arxiv.org/abs/2401.09815
tags:
- data
- test
- meaning
- grammar
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of the choice of augmentation
  distribution on compositional generalization in semantic parsing. The core method
  involves sampling meaning representations from different distributions (training,
  test, or uniform) and backtranslating them into natural language sentences for data
  augmentation.
---

# Simple and effective data augmentation for compositional generalization

## Quick Facts
- arXiv ID: 2401.09815
- Source URL: https://arxiv.org/abs/2401.09815
- Authors: Yuekun Yao; Alexander Koller
- Reference count: 33
- Primary result: Uniform distribution augmentation achieves 95.9% accuracy on COGS, nearly matching test distribution performance

## Executive Summary
This paper investigates how the choice of augmentation distribution affects compositional generalization in semantic parsing. The authors propose a simple augmentation method that samples meaning representations from different distributions (training, test, or uniform) and converts them back to natural language via backtranslation. Their key finding is that augmenting from a uniform distribution performs almost as well as augmenting from the test distribution, significantly outperforming the training distribution. This challenges the conventional wisdom that augmentation must closely match the test distribution to be effective.

## Method Summary
The core approach involves sampling meaning representations from a chosen grammar distribution (training, test, or uniform) and backtranslating them into natural language sentences for data augmentation. The method is evaluated on the COGS dataset using semantic parsing as the task. By systematically comparing augmentation from different distributions, the authors demonstrate that uniform sampling provides a surprisingly effective strategy for improving compositional generalization, achieving performance close to the test distribution while being more practical than requiring test distribution knowledge.

## Key Results
- Uniform grammar augmentation achieves 95.9% accuracy on COGS
- Training grammar augmentation achieves 92.9% accuracy on COGS
- Test grammar augmentation achieves 99.3% accuracy on COGS

## Why This Works (Mechanism)
The effectiveness of uniform distribution augmentation stems from its ability to expose the model to a balanced coverage of all possible compositional structures, rather than overfitting to the patterns present in the training distribution. By sampling uniformly from the grammar space, the augmented data provides a more comprehensive representation of the combinatorial space that the model must generalize across, helping it learn more robust compositional rules that transfer better to unseen combinations at test time.

## Foundational Learning
- **Compositional generalization**: The ability to systematically generalize to unseen combinations of known components. Why needed: This is the core capability being evaluated, essential for understanding how models handle novel compositional structures.
- **Semantic parsing**: The task of converting natural language into formal meaning representations. Why needed: The experimental framework relies on this task to evaluate compositional generalization.
- **Backtranslation**: Converting meaning representations back into natural language sentences. Why needed: This is the mechanism used to generate augmented training examples from sampled meaning representations.
- **Grammar distributions**: Different sampling strategies from formal grammars representing training, test, or uniform distributions. Why needed: The core experimental variable being manipulated to study augmentation effectiveness.
- **Data augmentation strategies**: Methods for artificially expanding training datasets. Why needed: The paper's contribution centers on identifying effective augmentation approaches for compositional tasks.

## Architecture Onboarding

**Component map:** Meaning Representation Sampler -> Backtranslation Module -> Augmented Dataset -> Semantic Parser

**Critical path:** The augmentation pipeline follows a linear sequence: (1) sample meaning representations from chosen grammar distribution, (2) backtranslate to natural language, (3) add to training data, (4) train semantic parser on augmented dataset. The critical performance determinant is the quality and coverage of the sampled meaning representations.

**Design tradeoffs:** The main tradeoff is between distribution specificity and practicality. While test distribution augmentation yields the best results, it requires knowledge of the test distribution, which is typically unavailable. Uniform sampling offers a practical alternative that achieves near-test-distribution performance without requiring distribution knowledge, at the cost of potentially missing some test-specific patterns.

**Failure signatures:** Poor compositional generalization may manifest as inability to parse novel combinations of known components, particularly when the augmentation distribution doesn't match the test distribution. The model may overfit to frequent patterns in the training data while failing to generalize to structurally similar but unseen combinations.

**3 first experiments:**
1. Compare uniform vs. training distribution augmentation on a held-out compositional generalization benchmark
2. Evaluate the impact of augmentation quantity (number of augmented examples) on compositional generalization performance
3. Test whether the uniform sampling advantage persists when using different backtranslation models or approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Results are confined to semantic parsing tasks, limiting generalizability to other NLP domains
- The backtranslation approach may introduce artifacts or biases not fully characterized
- Analysis focuses on grammar-based sampling without exploring alternative augmentation methods
- Computational cost of backtranslation for large-scale augmentation is not discussed

## Confidence

**High confidence:** The core experimental results comparing uniform, training, and test distribution augmentation performance on COGS are methodologically sound and clearly presented.

**Medium confidence:** The conclusion that uniform sampling is "surprisingly effective" compared to test distribution sampling, while supported by the data, may not generalize beyond the specific semantic parsing context examined.

**Low confidence:** Claims about the broader implications for data augmentation methodology in NLP require additional validation across diverse tasks and domains not covered in this study.

## Next Checks
1. Replicate the augmentation experiments on additional compositional generalization benchmarks such as SCAN or CFQ to assess whether uniform sampling consistently outperforms training distribution sampling across different semantic parsing tasks.

2. Compare uniform sampling augmentation with other augmentation strategies like syntactic transformations, paraphrasing, or rule-based augmentation to establish relative effectiveness across different approaches.

3. Evaluate whether the observed performance gap between uniform and training distribution augmentation persists when scaling to larger training set sizes, as the current results may reflect limitations of smaller datasets rather than fundamental advantages of uniform sampling.