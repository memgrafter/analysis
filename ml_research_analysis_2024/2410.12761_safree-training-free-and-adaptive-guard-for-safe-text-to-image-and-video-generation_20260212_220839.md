---
ver: rpa2
title: 'SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video
  Generation'
arxiv_id: '2410.12761'
source_url: https://arxiv.org/abs/2410.12761
tags:
- safree
- generation
- concept
- arxiv
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SAFREE, a training-free method for safe text-to-image
  and text-to-video generation that removes toxic concepts without modifying model
  weights. The approach detects a subspace corresponding to toxic concepts in the
  text embedding space, projects harmful tokens orthogonally to this subspace while
  preserving input semantics, and dynamically adjusts filtering strength via a self-validating
  mechanism.
---

# SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation

## Quick Facts
- arXiv ID: 2410.12761
- Source URL: https://arxiv.org/abs/2410.12761
- Authors: Jaehong Yoon; Shoubin Yu; Vaidehi Patil; Huaxiu Yao; Mohit Bansal
- Reference count: 32
- Key outcome: Training-free method removing toxic concepts from T2I/T2V generation without modifying model weights, achieving 22% ASR reduction across five datasets while maintaining high quality.

## Executive Summary
SAFREE introduces a novel training-free approach for safe text-to-image and text-to-video generation that removes toxic concepts without modifying model weights. The method detects toxic concept subspaces in text embedding space, projects harmful tokens orthogonally while preserving semantics, and applies adaptive re-attention in the diffusion latent space to suppress toxic features. SAFREE achieves state-of-the-art performance among training-free methods, reducing unsafe content by 22% across five datasets while maintaining high-quality outputs and generalizing effectively to various diffusion backbones and video generation tasks.

## Method Summary
SAFREE operates through three key mechanisms: (1) detecting a toxic concept subspace in text embedding space and projecting harmful tokens orthogonally to this subspace while preserving input semantics, (2) dynamically adjusting filtering strength via a self-validating mechanism that controls denoising steps based on embedding similarity, and (3) applying adaptive re-attention in the diffusion latent space to suppress toxic features at the pixel level using frequency domain attenuation. The method extends to various T2I backbones (UNet, DiT) and T2V models (ZeroScopeT2V, CogVideoX) without requiring any training.

## Key Results
- Achieves 22% reduction in Attack Success Rate (ASR) across five adversarial datasets for T2I generation
- Maintains high generation quality with competitive FID and CLIP scores on COCO-30k
- Successfully extends to text-to-video generation with state-of-the-art safety performance on T2VSafetybench

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAFREE identifies unsafe tokens by measuring their proximity to a toxic concept subspace in the text embedding space.
- Mechanism: The method computes masked token embeddings and projects them onto the toxic concept subspace. Tokens that increase the residual vector length (distance from the subspace) are flagged as unsafe. These tokens are then projected orthogonally to the toxic subspace while staying within the input space.
- Core assumption: Proximity in the text embedding space correlates with the likelihood of generating toxic visual content.
- Evidence anchors:
  - [abstract]: "we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt embeddings away from this subspace"
  - [section]: "To assess the relevance of specific tokens in the input prompt to the toxic concept subspace, we design a pooled input embedding p\i... we estimate the conceptual proximity of a token in the input prompt with C by computing the distance between the masked text embedding obtained after masking out the corresponding token and C."
  - [corpus]: Weak - no direct corpus evidence on embedding-space proximity correlating with visual toxicity.

### Mechanism 2
- Claim: Self-validating filtering dynamically adjusts denoising steps based on the cosine similarity between original and projected embeddings.
- Mechanism: A threshold t' is computed using the cosine distance between original prompt embedding p and projected embedding pproj. If the current denoising step t is less than or equal to round(t'), the safe embedding psaf ree is used; otherwise, the original embedding p is retained.
- Core assumption: The similarity between original and projected embeddings indicates whether the input is undesirable and needs stronger filtering.
- Evidence anchors:
  - [abstract]: "SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings"
  - [section]: "We adopt the cosine distance between the original input embedding p and the projected embedding pproj to compute t′. A higher similarity indicates that the input prompt has been effectively disentangled from the toxic target concept to be removed."
  - [corpus]: Weak - no corpus evidence on denoising step adjustment based on embedding similarity.

### Mechanism 3
- Claim: Adaptive latent re-attention in the frequency domain selectively suppresses low-frequency features associated with toxic concepts.
- Mechanism: At each timestep, FFT is applied to latent features conditioned on both the original and filtered prompts. Low-frequency components are attenuated when their magnitude in the filtered prompt exceeds that of the original prompt, reducing oversmoothing effects.
- Core assumption: Toxic concepts are more strongly represented in low-frequency components of latent features.
- Evidence anchors:
  - [abstract]: "we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level"
  - [section]: "we suggest an adaptive re-weighting strategy using spectral transformation in the Fourier domain... we reduce the influence of low-frequency features, which are accentuated by our filtered prompt embedding"
  - [corpus]: Weak - no corpus evidence on toxic concept representation in frequency domain.

## Foundational Learning

- Concept: Text embeddings and their relationship to visual generation
  - Why needed here: SAFREE operates by manipulating text embeddings to control image/video generation
  - Quick check question: How do text embeddings from CLIP or similar models influence the output of diffusion models?

- Concept: Diffusion model denoising process
  - Why needed here: SAFREE modifies the denoising process by selectively applying safe embeddings
  - Quick check question: What is the role of denoising steps in diffusion models, and how does classifier-free guidance work?

- Concept: Subspace projection and orthogonal decomposition
  - Why needed here: SAFREE projects tokens onto and orthogonal to the toxic concept subspace
  - Quick check question: How do you project a vector onto a subspace and find its orthogonal component?

## Architecture Onboarding

- Component map: Input prompt → Toxic concept subspace detection → Token masking and projection → Self-validating filtering → Latent re-attention → Generated output

- Critical path: The complete pipeline from input prompt through toxic concept detection, token projection, dynamic filtering adjustment, and frequency domain attenuation to final output generation

- Design tradeoffs:
  - Token projection vs. removal: Projection preserves prompt coherence but may be less aggressive
  - Self-validating vs. fixed filtering: Dynamic adjustment adapts to input but adds complexity
  - Frequency domain attenuation vs. spatial domain: Frequency approach targets global structure but may miss fine details

- Failure signatures:
  - High ASR despite filtering: Toxic tokens not properly identified or projected
  - Quality degradation in safe prompts: Over-aggressive filtering or incorrect self-validation
  - Incomplete concept removal: Insufficient attenuation in frequency domain

- First 3 experiments:
  1. Test token proximity analysis on a small set of known toxic and safe prompts to verify the masking mechanism
  2. Validate self-validating filtering by comparing generated outputs with different cosine similarity thresholds
  3. Evaluate adaptive latent re-attention by comparing frequency domain attenuation effects on safe vs. toxic concepts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for detecting and filtering implicit or chain-of-thought style toxic prompts that SAFREE currently struggles with?
- Basis in paper: The authors note that SAFREE exhibits limitations when toxic prompts become much more implicit and in a chain-of-thought style, allowing them to jailbreak SAFREE and yield unsafe/inappropriate content generation.
- Why unresolved: The paper acknowledges this limitation but does not propose a solution or framework for handling such complex prompt structures. Current methods rely on direct proximity analysis to toxic concept subspaces, which may not capture indirect or contextual toxicity.
- What evidence would resolve it: Empirical evaluation showing SAFREE's performance on datasets specifically designed with chain-of-thought and implicit toxic prompts, compared to potential enhanced detection methods that incorporate contextual understanding.

## Limitations

- The core assumption that proximity to toxic concept subspaces in text embedding space reliably predicts visual toxicity lacks empirical validation
- The method struggles with implicit and chain-of-thought style toxic prompts, representing a significant limitation for real-world deployment
- The frequency domain attenuation approach may not capture all toxic content if it's not primarily encoded in low-frequency components

## Confidence

- **High confidence**: The overall SAFREE framework architecture and its integration with various diffusion backbones is well-specified and technically sound. The methodology for projecting tokens orthogonally to a subspace is mathematically valid.
- **Medium confidence**: The self-validating filtering mechanism's dynamic adjustment of denoising steps based on embedding similarity has theoretical merit, though the correlation between cosine similarity and toxicity requires empirical validation.
- **Low confidence**: The core assumption that toxic concept proximity in text embedding space predicts visual toxicity lacks supporting evidence. The claim that toxic concepts are primarily represented in low-frequency latent features is asserted without validation.

## Next Checks

1. **Empirical validation of toxic subspace detection**: Create a controlled experiment testing whether tokens identified as "close" to the toxic subspace actually correlate with increased toxicity in generated outputs. Use a diverse set of prompts with known safe/toxic variations to measure the predictive power of the embedding proximity metric.

2. **Frequency domain toxicity analysis**: Conduct a systematic study of how toxic concepts are represented across different frequency bands in latent space. Generate images with varying levels of toxic content and analyze their frequency domain characteristics to verify whether low-frequency components indeed carry the majority of toxic information.

3. **Self-validation threshold calibration**: Perform ablation studies testing different cosine similarity thresholds and γ values in the self-validating mechanism. Measure how these hyperparameters affect both ASR reduction and generation quality across different prompt types to establish optimal parameter ranges and validate the assumption that embedding similarity indicates toxicity level.