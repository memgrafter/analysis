---
ver: rpa2
title: Generalist embedding models are better at short-context clinical semantic search
  than specialized embedding models
arxiv_id: '2401.01943'
source_url: https://arxiv.org/abs/2401.01943
tags:
- embedding
- clinical
- medical
- code
- generalist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compared generalist and specialized embedding models
  for semantic search on clinical diagnostic descriptions. It built a dataset of 100
  ICD-10-CM codes with 10 rephrasings each, then benchmarked 19 embedding models to
  retrieve the correct code for each rephrasing.
---

# Generalist embedding models are better at short-context clinical semantic search than specialized embedding models

## Quick Facts
- arXiv ID: 2401.01943
- Source URL: https://arxiv.org/abs/2401.01943
- Reference count: 40
- Primary result: Generalist embedding models outperformed clinical ones on semantic search of ICD-10-CM codes, with 84.0% exact matching versus 64.4% for best clinical model

## Executive Summary
This study compares 19 embedding models (12 generalist, 7 clinical) for semantic search of ICD-10-CM diagnostic codes. The researchers created a benchmark dataset of 100 ICD-10-CM codes with 10 rephrasings each, then evaluated model performance in retrieving the correct code for each rephrasing. Generalist models consistently outperformed clinical models, with the best generalist achieving 84.0% exact matching versus 64.4% for the best clinical model. The findings suggest that specialized clinical models may be more sensitive to small input variations, potentially due to less diverse training data.

## Method Summary
The study benchmarked 19 embedding models on semantic search of ICD-10-CM codes using a dataset of 100 codes with 10 rephrasings each. Models were used to embed both the original ICD-10-CM descriptions and the rephrasings, then semantic search was performed by finding the nearest neighbor using Euclidean distance. Performance was evaluated using exact matching rate, category matching rate, and character error rate (CER). The embedding models were sourced from HuggingFace and tested under CPU-only, free-to-use constraints.

## Key Results
- Generalist models achieved 84.0% exact matching rate versus 64.4% for the best clinical model
- Even incorrect predictions from generalist models were closer to ground truth than clinical model predictions
- All top-performing models were sentence transformers, suggesting sentence-level training objectives benefit semantic search
- The best model (nomic-embed-text from Nomic AI) significantly outperformed all clinical models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalist embedding models perform better on short clinical semantic search because they have been trained on larger and more diverse datasets.
- Mechanism: Exposure to broader linguistic patterns during training allows generalist models to handle variations in phrasing more effectively, even in medical contexts.
- Core assumption: More diverse training data leads to better generalization and robustness to input variation.
- Evidence anchors:
  - [abstract] "generalist models performed better than clinical models, suggesting that existing clinical specialized models are more sensitive to small changes in input"
  - [section] "This higher sensitivity of specialized models might be explained by the fact that clinical texts contain not only medical terms, but also more general and simpler words that contribute strongly to global text understanding"
  - [corpus] Weak - corpus doesn't directly address dataset size or diversity differences between generalist and clinical models
- Break condition: If the input domain requires extremely domain-specific terminology that generalist models have never encountered, specialized models might outperform.

### Mechanism 2
- Claim: Sentence transformer training specifically improves short-context semantic search performance.
- Mechanism: Sentence transformer architectures are optimized for comparing sentence-level semantic similarity rather than just contextual understanding.
- Core assumption: The training objective of sentence transformers aligns better with semantic search tasks than standard transformer pretraining.
- Evidence anchors:
  - [section] "the best embedding models were all sentence transformers, as well as by the superiority of S-PubMedBERT over PubMedBERT"
  - [section] "S-PubMedBERT used PubMedBERT as a basis for further sentence transformer training on a larger general dataset"
  - [corpus] Weak - corpus doesn't provide evidence about sentence transformer effectiveness specifically
- Break condition: If the task requires longer document understanding rather than short sentence comparison, standard transformers might perform better.

### Mechanism 3
- Claim: Specialized models overfit to clinical terminology patterns, making them more sensitive to minor rephrasing.
- Mechanism: Limited training data diversity causes specialized models to rely heavily on exact word matches and specific phrase patterns.
- Core assumption: Models trained on narrow datasets develop brittle representations that don't generalize well to linguistic variation.
- Evidence anchors:
  - [abstract] "existing clinical specialized models are more sensitive to small changes in input that confuse them"
  - [section] "clinical specialized models are more sensitive to small changes in input that confuse them"
  - [corpus] Weak - corpus doesn't provide evidence about overfitting or brittleness in specialized models
- Break condition: If specialized models are trained on sufficiently large and diverse clinical datasets, they might achieve similar robustness to generalists.

## Foundational Learning

- Concept: Embedding vector similarity metrics (Euclidean distance, cosine similarity)
  - Why needed here: The study uses Euclidean distance to retrieve the nearest ICD-10-CM description embedding
  - Quick check question: What metric would you use to compare two embedding vectors of different lengths?

- Concept: Semantic search workflow
  - Why needed here: Understanding how embeddings are used as queries against a corpus of precomputed embeddings
  - Quick check question: In a semantic search system, what happens at each step from query to result retrieval?

- Concept: Dataset construction for benchmarking
  - Why needed here: The study created 100 ICD-10-CM codes with 10 rephrasings each to test model robustness
  - Quick check question: How would you design a benchmark dataset to test sensitivity to input variations?

## Architecture Onboarding

- Component map: Embedding model → Vector embedding generation → Corpus indexing → Query embedding → Similarity search → Result ranking
- Critical path: Model inference → Vector comparison → Top-1 retrieval → Accuracy evaluation
- Design tradeoffs: Generalist models trade domain-specific precision for robustness vs. specialized models that may overfit to training patterns
- Failure signatures: High sensitivity to rephrasing indicates brittle embeddings; poor exact matching suggests inadequate semantic understanding
- First 3 experiments:
  1. Replicate the benchmark with a subset of 10 ICD-10-CM codes and 2 rephrasings each to verify methodology
  2. Test a single generalist model vs. a single clinical model on the same dataset to observe performance gap
  3. Vary the distance metric (cosine vs. Euclidean) to determine impact on retrieval accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do generalist embedding models achieve better performance on clinical semantic search tasks compared to specialized models, despite being trained on non-clinical data?
- Basis in paper: [explicit] The paper states that generalist models outperformed specialized clinical models, but does not fully explain the underlying mechanisms.
- Why unresolved: The paper suggests that generalist models may have more robust language understanding due to training on diverse data, but does not provide direct evidence or analysis of the linguistic features or representations that contribute to this advantage.
- What evidence would resolve it: Detailed comparative analysis of the linguistic features, semantic representations, and attention patterns in generalist vs. specialized models when processing clinical text.

### Open Question 2
- Question: Would a sentence-transformer model specifically trained on clinical data outperform both generalist and existing specialized models on clinical semantic search tasks?
- Basis in paper: [inferred] The paper notes that sentence-transformer architecture contributed to performance, and that S-PubMedBERT (partially clinical) was outperformed by fully generalist models, suggesting potential for specialized sentence-transformers.
- Why unresolved: The paper explicitly states that no fully clinical sentence-transformer model was available in the benchmark, leaving this comparison untested.
- What evidence would resolve it: Benchmark results comparing a newly developed clinical sentence-transformer model against both generalist and existing specialized models on the same dataset.

### Open Question 3
- Question: How does model performance change when processing longer clinical documents (e.g., full discharge notes) compared to short code descriptions?
- Basis in paper: [explicit] The paper identifies this as a future research direction, noting that the current study focused on short contexts and suggesting the need to test longer medical texts.
- Why unresolved: The study was specifically designed around short ICD-10-CM code descriptions and did not test performance on longer clinical documents or in complete pipeline systems.
- What evidence would resolve it: Performance comparison of generalist and specialized models on longer clinical documents, including integration with RAG systems or other complete medical NLP pipelines.

## Limitations

- The dataset of 100 ICD-10-CM codes represents a relatively small sample of the full code space
- Use of ChatGPT for generating rephrasings may introduce biases not representative of real-world clinical documentation variation
- The study focused on short contexts and didn't test performance on longer clinical documents or complete pipeline systems

## Confidence

- **High confidence**: The core empirical finding that generalist models outperform clinical models on this semantic search task, supported by clear quantitative metrics (84.0% vs 64.4% exact matching)
- **Medium confidence**: The explanation that generalist models benefit from broader linguistic training data, as this mechanism is inferred rather than directly tested
- **Low confidence**: The assertion that specialized models are inherently more sensitive to input changes, as this could be specific to the particular models tested rather than a general property of clinical embeddings

## Next Checks

1. Test the same benchmark across multiple versions of ICD-10-CM codes to assess whether results generalize beyond the specific code set used
2. Evaluate additional clinical embedding models beyond the 7 tested, particularly those trained on larger and more diverse clinical datasets
3. Conduct ablation studies varying the amount of non-clinical data in generalist model training to quantify the relationship between training diversity and semantic search performance