---
ver: rpa2
title: 'Storynizor: Consistent Story Generation via Inter-Frame Synchronized and Shuffled
  ID Injection'
arxiv_id: '2409.19624'
source_url: https://arxiv.org/abs/2409.19624
tags:
- generation
- images
- character
- storynizor
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Storynizor, a model for generating continuous
  story images with strong character consistency, vivid backgrounds, and diverse poses.
  The key innovations are the ID-Synchronizer, which uses an auto-mask self-attention
  module and mask perceptual loss to ensure character consistency across frames, and
  the ID-Injector, which employs a Shuffling Reference Strategy (SRS) to inject identity
  features from reference images into specific locations.
---

# Storynizor: Consistent Story Generation via Inter-Frame Synchronized and Shuffled ID Injection

## Quick Facts
- arXiv ID: 2409.19624
- Source URL: https://arxiv.org/abs/2409.19624
- Reference count: 40
- Generates continuous story images with strong character consistency, vivid backgrounds, and diverse poses

## Executive Summary
Storynizor is a novel model for generating continuous story images with strong character consistency across frames. The model introduces two key innovations: the ID-Synchronizer, which uses an auto-mask self-attention module and mask perceptual loss to ensure character consistency, and the ID-Injector, which employs a Shuffling Reference Strategy (SRS) to inject identity features from reference images into specific locations. Trained on a newly created StoryDB dataset of 100,000 images featuring diverse characters and environments, Storynizor outperforms existing methods in generating coherent stories with high-fidelity character consistency, flexible postures, and vivid backgrounds.

## Method Summary
Storynizor addresses the challenge of generating continuous story images with consistent characters across frames while maintaining diverse poses and vivid backgrounds. The model builds upon Stable Diffusion v1.5 and introduces two key modules: the ID-Synchronizer and the ID-Injector. The ID-Synchronizer uses an auto-mask self-attention module to extract cross-frame attention maps and employs mask perceptual loss to ensure character consistency across frames. The ID-Injector uses a Shuffling Reference Strategy (SRS) to inject identity features from reference images into specific locations, enhancing pose flexibility and generalization. The model is trained on StoryDB, a curated dataset of 100,000 images with captions and masks, and fine-tuned at 512×512 and 768×768 resolutions.

## Key Results
- Achieves state-of-the-art performance in generating continuous story images with strong character consistency across frames
- Outperforms existing methods in text-image alignment (CLIP-T) and inter-frame similarity (CLIP-I, DINO-I)
- Demonstrates high facial similarity (Face Sim, Face Sim(R)) and superior qualitative results in background diversity and pose variation

## Why This Works (Mechanism)
Storynizor works by addressing the key challenges in story generation: maintaining character consistency across frames while allowing for pose diversity and vivid backgrounds. The ID-Synchronizer ensures that the same character appears consistently across frames by using auto-mask self-attention to extract cross-frame attention maps and mask perceptual loss to align character features. The ID-Injector enhances pose flexibility by injecting identity features from reference images using the Shuffling Reference Strategy, which randomly shuffles reference images to improve generalization. The combination of these modules allows Storynizor to generate coherent stories with high-fidelity character consistency, flexible postures, and vivid backgrounds.

## Foundational Learning

**Cross-frame attention maps**: Used to track character positions across frames. Why needed: Essential for maintaining character consistency. Quick check: Visualize attention maps to verify character tracking.

**Mask perceptual loss**: Measures similarity between generated and reference character regions. Why needed: Ensures character consistency across frames. Quick check: Compare loss values during training to monitor convergence.

**Shuffling Reference Strategy (SRS)**: Randomly shuffles reference images during training. Why needed: Improves generalization and pose diversity. Quick check: Monitor pose variation in generated images.

## Architecture Onboarding

**Component map**: Text prompt -> ID-Synchronizer -> ID-Injector -> Stable Diffusion backbone -> Generated story images

**Critical path**: Text prompt → ID-Synchronizer (with Auto-mask Self-attention and Mask Perceptual Loss) → ID-Injector (with SRS) → Stable Diffusion UNet → Generated images

**Design tradeoffs**: The use of pre-trained Stable Diffusion v1.5 as a backbone allows for leveraging existing image generation capabilities but may limit the model's ability to generate highly stylized or abstract story images. The reliance on a curated dataset (StoryDB) ensures high-quality training data but may introduce bias towards the specific styles and characters present in the dataset.

**Failure signatures**: Poor character consistency across frames due to inaccurate cross-attention masks or insufficient mask perceptual loss. Overfitting to reference images, resulting in lack of pose diversity, may occur if SRS is not properly implemented.

**3 first experiments**:
1. Generate a 4-frame story with a single character and evaluate character consistency using Face Sim metric.
2. Generate a 4-frame story with multiple characters and assess inter-frame similarity using CLIP-I and DINO-I metrics.
3. Generate a story with varying poses and backgrounds, and qualitatively evaluate the diversity and quality of the generated images.

## Open Questions the Paper Calls Out
None

## Limitations
- Significant uncertainty due to the proprietary StoryDB dataset and unclear implementation details of critical components like the Shuffling Reference Strategy
- Potential overfitting to the LAION-Face dataset during the ID-Injector pre-training phase, which may not generalize well to diverse character appearances
- Lack of exploration into the scalability of Storynizor for generating longer stories with more frames

## Confidence
- High Confidence: The overall framework of Storynizor, including the ID-Synchronizer with Auto-mask Self-attention and Mask Perceptual Loss, is well-defined and theoretically sound
- Medium Confidence: The Shuffling Reference Strategy (SRS) and its integration with the ID-Injector are described conceptually, but lack of specific hyperparameters and implementation details reduces confidence in exact reproduction
- Low Confidence: The StoryDB dataset creation pipeline and exact training procedures are insufficiently detailed, making it difficult to replicate results without additional clarification from authors

## Next Checks
1. Verify whether StoryDB will be publicly released or if authors can provide detailed protocols for dataset creation, including clustering, captioning, and segmentation steps
2. Request explicit values for SRS hyperparameters (e.g., shuffling probability, batch size per character) and implementation details of the Resampler and controllers
3. Conduct ablation studies to test the impact of Mask Perceptual Loss and Auto-mask Self-attention on character consistency, and compare results with and without Shuffling Reference Strategy