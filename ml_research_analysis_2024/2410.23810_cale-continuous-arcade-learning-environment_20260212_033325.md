---
ver: rpa2
title: 'CALE: Continuous Arcade Learning Environment'
arxiv_id: '2410.23810'
source_url: https://arxiv.org/abs/2410.23810
tags:
- learning
- exploration
- greedy
- standard
- threshold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CALE, a continuous-action extension of the
  Arcade Learning Environment (ALE), enabling unified evaluation of discrete- and
  continuous-action reinforcement learning agents on Atari 2600 games. CALE replaces
  ALE's 18 discrete joystick actions with a continuous 3D action space, parameterizing
  joystick positions via polar coordinates and a fire button, while maintaining the
  same underlying game mechanics.
---

# CALE: Continuous Arcade Learning Environment

## Quick Facts
- arXiv ID: 2410.23810
- Source URL: https://arxiv.org/abs/2410.23810
- Authors: Jesse Farebrother; Pablo Samuel Castro
- Reference count: 40
- Continuous-action extension of Arcade Learning Environment (ALE) for unified evaluation of discrete- and continuous-action RL agents on Atari 2600 games

## Executive Summary
This paper introduces CALE, a continuous-action extension of the Arcade Learning Environment (ALE) that enables unified evaluation of discrete- and continuous-action reinforcement learning agents on Atari 2600 games. CALE replaces ALE's 18 discrete joystick actions with a continuous 3D action space, parameterizing joystick positions via polar coordinates and a fire button while maintaining the same underlying game mechanics. The authors provide baseline results using Soft Actor-Critic (SAC) with varying threshold parameters and network architectures, showing that SAC with standard Gaussian exploration outperforms ε-greedy exploration in the 200M training regime. Comparison with DQN and Data-Efficient Rainbow (DER) baselines shows SAC underperforms in aggregate metrics, likely due to lack of tuning.

## Method Summary
CALE extends the Arcade Learning Environment by replacing the discrete 18-action joystick space with a continuous 3D action space consisting of radial distance (0-1), angle (0-360°), and a fire button indicator. The action space is implemented using polar coordinates for joystick position with the fire button as a binary parameter. The authors provide implementation details including SAC with tanh-Gaussian exploration and epsilon-greedy exploration strategies. They report aggregate performance using median human-normalized score (HNS) across the benchmark, comparing continuous-action SAC agents against discrete-action DQN and DER baselines.

## Key Results
- SAC with standard Gaussian exploration outperforms ε-greedy exploration in the 200M training regime
- Continuous-action SAC agents underperforms DQN and DER baselines in aggregate median HNS metrics
- Performance differences attributed to lack of tuning rather than fundamental limitations of continuous control

## Why This Works (Mechanism)
The CALE framework enables unified evaluation by maintaining identical game mechanics while allowing direct comparison between discrete and continuous action representations. The polar coordinate parameterization provides intuitive mapping between action space and joystick movement, facilitating exploration of continuous control strategies in a familiar benchmark environment. The soft actor-critic algorithm with tanh-Gaussian exploration naturally handles the continuous action space through stochastic policy gradients, while the fire button parameter enables discrete action incorporation when needed.

## Foundational Learning

**Soft Actor-Critic (SAC)**: Maximum entropy RL algorithm that optimizes both expected return and entropy, providing robust exploration in continuous action spaces. Needed for effective learning in continuous control tasks where exploration is critical.

**Tanh-Gaussian Exploration**: Action sampling strategy that uses Gaussian noise passed through tanh activation to bound actions within continuous space limits. Quick check: Verify action bounds match the [-1, 1] range after tanh transformation.

**Human-normalized Score (HNS)**: Performance metric that normalizes agent scores by human baseline performance, enabling cross-game comparison. Quick check: Confirm HNS calculation uses correct human baseline values for each game.

**Polar Coordinate Action Space**: Representation of joystick positions using radius and angle rather than Cartesian coordinates. Quick check: Validate that polar to Cartesian conversion correctly maps to game's expected input format.

## Architecture Onboarding

Component map: Agent -> SAC Algorithm -> Tanh-Gaussian Policy -> CALE Environment -> Atari 2600 Game

Critical path: Observation -> Policy Network -> Action Sampling -> Environment Step -> Reward/Next Observation -> Value Network Update

Design tradeoffs: Polar coordinates vs Cartesian for joystick control, Gaussian vs other exploration distributions, network architecture choices affecting sample efficiency

Failure signatures: Poor exploration leading to local optima, action space discretization errors causing unexpected game behavior, network instability from improper reward scaling

First experiments:
1. Test basic action sampling and environment stepping with random policies
2. Verify observation preprocessing matches ALE standards
3. Validate reward scaling and normalization procedures

## Open Questions the Paper Calls Out
None

## Limitations
- SAC underperformance compared to discrete baselines likely due to insufficient hyperparameter tuning
- Polar coordinate parameterization may bias results toward certain action patterns
- 200M training steps may not be sufficient for meaningful continuous vs discrete control comparison

## Confidence
- Claims about CALE's unified evaluation capability: High
- Baseline performance comparisons: Medium
- Exploration strategy conclusions: Medium

## Next Checks
1. Conduct systematic hyperparameter optimization for SAC on CALE games to establish performance ceilings for continuous-action agents
2. Implement alternative action parameterizations (e.g., Cartesian coordinates, normalized vectors) to test sensitivity to coordinate system choice
3. Evaluate continuous-action agents with curriculum learning or progressive action discretization to assess learning efficiency across the action space spectrum