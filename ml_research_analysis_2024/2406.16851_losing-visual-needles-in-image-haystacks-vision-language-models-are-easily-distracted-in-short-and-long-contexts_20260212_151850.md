---
ver: rpa2
title: 'Losing Visual Needles in Image Haystacks: Vision Language Models are Easily
  Distracted in Short and Long Contexts'
arxiv_id: '2406.16851'
source_url: https://arxiv.org/abs/2406.16851
tags:
- gemini
- context
- visual
- image
- gpt-4v
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current vision-language models struggle with long-context visual
  extractive reasoning, rapidly losing performance as visual context length grows
  due to an inability to filter irrelevant information. We introduce LoCoVQA, a dynamic
  benchmark generator that evaluates this capability by augmenting test examples with
  increasing numbers of distractor images.
---

# Losing Visual Needles in Image Haystacks: Vision Language Models are Easily Distracted in Short and Long Contexts

## Quick Facts
- arXiv ID: 2406.16851
- Source URL: https://arxiv.org/abs/2406.16851
- Authors: Aditya Sharma; Michael Saxon; William Yang Wang
- Reference count: 35
- Current vision-language models struggle with long-context visual extractive reasoning, rapidly losing performance as visual context length grows

## Executive Summary
Current vision-language models (VLMs) struggle with long-context visual extractive reasoning, rapidly losing performance as visual context length grows due to an inability to filter irrelevant information. The study introduces LoCoVQA, a dynamic benchmark generator that evaluates this capability by augmenting test examples with increasing numbers of distractor images. Across tasks including mathematical reasoning, visual question answering, and character recognition, the research finds that even state-of-the-art models exhibit significant performance decay with increasing context length, often showing logarithmic decay trends.

This reveals a fundamental limitation in current VLMs: while they excel at single-image tasks, they fail to effectively attend to relevant information across multi-image sequences. The findings suggest a need for training objectives that require attention across multiple context images and highlight the importance of developing architectures capable of filtering irrelevant visual information in long-context scenarios.

## Method Summary
The study introduces LoCoVQA, a dynamic benchmark generator that systematically evaluates VLM performance on visual extractive reasoning tasks as context length increases. The benchmark augments test examples from existing VQA datasets with increasing numbers of distractor images, creating sequences of 2, 4, 8, 16, and 32 images. For each query, the relevant image (ground truth) is paired with randomly sampled distractor images from the same dataset. The approach tests VLMs' ability to identify the correct image and answer the question based solely on that image's content, despite increasing visual noise from irrelevant images.

The evaluation covers multiple VQA datasets including GQA, VQAv2, TextVQA, and mathematical reasoning datasets like V-EquaTE and SVAMP. Performance is measured using standard VQA metrics, and decay patterns are analyzed to understand how different models cope with increasing context length. The study tests several leading VLMs including LLaVA-1.5, InstructBLIP, and BLIP-2 across these benchmark variations.

## Key Results
- VLMs show significant performance decay with increasing context length across all tested tasks
- Performance decay often follows logarithmic patterns, with rapid initial decline that plateaus at longer contexts
- Even state-of-the-art models like LLaVA-1.5 struggle to maintain accuracy beyond 8-16 context images
- Mathematical reasoning tasks show more severe degradation than visual question answering tasks

## Why This Works (Mechanism)
None

## Foundational Learning
- Visual extractive reasoning: The ability to identify and extract relevant information from visual inputs to answer questions. Needed to understand the core capability being tested. Quick check: Can the model correctly identify which image contains the answer in a multi-image context?
- Context length limitations: The degradation of model performance as input sequence length increases. Needed to frame the central problem. Quick check: Does accuracy decrease monotonically with increasing number of distractor images?
- Attention mechanisms in VLMs: How models weigh different visual elements when processing multi-image inputs. Needed to understand why models fail at long contexts. Quick check: Can attention weights be analyzed to see if models focus on correct images?

## Architecture Onboarding

Component Map:
Input Images -> Vision Encoder -> Textual Prompt Fusion -> Language Model -> Answer Generation

Critical Path:
Vision encoder processes each image independently -> Image embeddings are concatenated with text prompts -> Language model processes the combined sequence -> Attention mechanisms determine relevance across images -> Final answer is generated based on attended features

Design Tradeoffs:
- Single-stream vs. dual-stream architectures: Single-stream models (like LLaVA) process visual and text jointly but may struggle with long contexts, while dual-stream models (like BLIP-2) separate vision and language processing but require additional fusion mechanisms
- Fixed vs. dynamic context windows: Fixed windows limit maximum context but provide stable performance, while dynamic approaches may better handle varying context lengths but introduce complexity
- End-to-end vs. modular training: End-to-end training allows joint optimization but may propagate errors, while modular approaches enable specialized optimization but risk misalignment between components

Failure Signatures:
- Rapid accuracy decline when distractor count exceeds 4-8 images
- Attention mechanisms that fail to suppress irrelevant image features
- Answers that reference content from distractor images rather than the correct source
- Increased hallucination rates as context length grows

First Experiments:
1. Test baseline single-image performance to establish upper performance bounds
2. Evaluate performance with 2-4 distractor images to identify initial degradation points
3. Measure attention distribution across images to verify if models focus on correct sources

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark generation relies on pre-existing VQA datasets with assumed ground-truth relevance criteria
- Distractor images sourced from same datasets may introduce unintended correlations or biases
- Logarithmic decay model represents only one possible functional form for performance decline
- Evaluation focuses primarily on extractive reasoning, leaving generative task performance unexplored

## Confidence

High confidence: The core finding that VLMs show performance decay with increasing context length across multiple tasks and models. The experimental methodology and statistical analysis are sound.

Medium confidence: The universality of logarithmic decay patterns across all tested tasks. While consistently observed, alternative decay functions may better fit specific task types.

Low confidence: Claims about the fundamental nature of the limitation without further investigation into architectural causes or potential mitigation strategies beyond what's tested.

## Next Checks

1. Test whether performance degradation persists when using distractor images from completely different domains or datasets to rule out dataset-specific artifacts

2. Evaluate whether architectural modifications like hierarchical attention or retrieval-augmented generation can mitigate the context length limitation

3. Conduct ablation studies varying distractor image complexity and relevance ambiguity to better understand the factors driving performance decline