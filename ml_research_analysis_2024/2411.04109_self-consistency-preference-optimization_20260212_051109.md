---
ver: rpa2
title: Self-Consistency Preference Optimization
arxiv_id: '2411.04109'
source_url: https://arxiv.org/abs/2411.04109
tags:
- scpo
- training
- preference
- self-consistency
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Consistency Preference Optimization
  (SCPO), a method that improves large language model training for reasoning tasks
  by using the consistency of multiple model-generated responses as a proxy for quality.
  Unlike prior self-alignment approaches that rely on model-based evaluation, SCPO
  iteratively trains models to prefer the most self-consistent answers over inconsistent
  ones without needing gold labels.
---

# Self-Consistency Preference Optimization

## Quick Facts
- arXiv ID: 2411.04109
- Source URL: https://arxiv.org/abs/2411.04109
- Reference count: 36
- Primary result: SCPO achieves accuracy within 1% of supervised methods on GSM8K and MATH, and enables Llama-3 8B to outperform larger models like Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku on ZebraLogic reasoning tasks.

## Executive Summary
This paper introduces Self-Consistency Preference Optimization (SCPO), a method that improves large language model training for reasoning tasks by using the consistency of multiple model-generated responses as a proxy for quality. Unlike prior self-alignment approaches that rely on model-based evaluation, SCPO iteratively trains models to prefer the most self-consistent answers over inconsistent ones without needing gold labels. The approach uses a weighted preference optimization loss based on vote margins from sampled responses and can incorporate both unlabeled and labeled data. Experiments with Llama-3 8B show SCPO significantly outperforms unsupervised baselines like reward-model-based preference optimization and self-supervised fine-tuning, achieving accuracy within 1% of supervised methods on GSM8K and MATH. On the challenging ZebraLogic task, SCPO-trained Llama-3 8B outperforms larger models like Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku, demonstrating its effectiveness in improving reasoning without access to gold solutions.

## Method Summary
SCPO operates by generating multiple responses for each query using temperature sampling, then computing vote counts for each final answer across responses. The most and least consistent responses form preference pairs, with instance weights based on vote margins. A weighted preference optimization loss is applied, training the model to prefer consistent answers. This process iterates, with each iteration producing more consistent and accurate models that can better bootstrap from previously ambiguous training examples. The method works without gold labels by assuming that more consistent answers are more likely to be correct because model errors are random and independent.

## Key Results
- SCPO achieves accuracy within 1% of supervised methods on GSM8K and MATH reasoning tasks
- SCPO-trained Llama-3 8B outperforms larger models including Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku on the challenging ZebraLogic task
- SCPO significantly outperforms unsupervised baselines like reward-model-based preference optimization and self-supervised fine-tuning
- Two iterations of SCPO consistently improve model performance with greedy decoding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-consistency serves as a reliable proxy for correctness in reasoning tasks
- Mechanism: When multiple sampled responses converge on the same answer, it indicates the answer is likely correct because incorrect paths diverge randomly
- Core assumption: Model errors are random and independent across samples
- Evidence anchors:
  - [abstract] "More consistent answers are more likely to be correct because mistakes made by the model are often random, so incorrect solutions are unlikely to lead to the same answer multiple times"
  - [section] "SCPO operates under the assumption that when multiple responses sampled for problem x map to the same answer, then the predicted answer is likely to be correct"
- Break condition: If model errors are systematic or correlated (e.g., due to shared misconceptions), self-consistency would incorrectly reinforce wrong answers

### Mechanism 2
- Claim: Weighted preference optimization using vote margins improves training efficiency
- Mechanism: The loss function weights preference pairs by the margin between chosen and rejected responses' vote counts, prioritizing high-confidence training signals
- Core assumption: Larger vote margins indicate higher confidence in preference quality
- Evidence anchors:
  - [section] "we use an instance-level weight w(x) to the loss, i.e., for the preference pair (x, y+, y−) ∈ D pairs t , w(x) = |V(y+) − V(y−)|/k"
  - [section] "pairs where the vote margin – the difference in votes attained by the chosen vs. the rejected response – is larger, are of higher quality"
- Break condition: If vote margins don't correlate with actual correctness (e.g., due to sampling bias), weighting could prioritize noisy training signals

### Mechanism 3
- Claim: Iterative self-training with self-consistency progressively improves model reasoning
- Mechanism: Each training iteration produces more consistent and accurate models, enabling better bootstrapping from previously ambiguous training examples
- Core assumption: Models become more consistent and accurate with each SCPO iteration
- Evidence anchors:
  - [section] "we observe that two iterations of SCPO consistently improves the LLM's performance when using greedy decoding"
  - [section] "SCPO training increases the consistency of models with each training iteration across different tasks"
- Break condition: If model consistency plateaus early or decreases with iterations due to overfitting or diversity loss

## Foundational Learning

- Concept: Preference Optimization (DPO/IRPO)
  - Why needed here: SCPO builds on preference optimization framework but uses self-consistency instead of gold labels
  - Quick check question: What is the difference between standard DPO and the weighted LSCPO loss used in SCPO?

- Concept: Self-Consistency at Inference Time
  - Why needed here: SCPO extends this concept from inference to training, using consistency as a training signal
  - Quick check question: How does self-consistency at training time differ from its application during inference?

- Concept: Reward Modeling and its Limitations
  - Why needed here: SCPO addresses the shortcomings of reward models in reasoning tasks by using self-consistency instead
  - Quick check question: Why do reward models trained on reasoning tasks often fail on out-of-distribution problems?

## Architecture Onboarding

- Component map:
  Model generator -> Vote counter -> Preference pair builder -> Weighted loss calculator -> Iterative trainer

- Critical path:
  1. Sample k responses from current model
  2. Compute vote counts for each answer
  3. Filter out queries with insufficient consistency (τ threshold)
  4. Build preference pairs (chosen=most consistent, rejected=least consistent)
  5. Compute instance weights from vote margins
  6. Apply weighted LSCPO loss
  7. Train for N epochs and produce next iteration's model

- Design tradeoffs:
  - k (number of samples): Higher k improves consistency measurement but increases computational cost
  - τ (consistency threshold): Higher τ improves data quality but reduces training data size
  - Weighting: Unweighted loss is simpler but weighted loss prioritizes higher-confidence pairs

- Failure signatures:
  - No preference pairs after filtering: Threshold τ too high or model too inconsistent
  - All responses have same answer: Model too confident or task too simple
  - Performance degrades after iterations: Overfitting or diversity loss from repeated training

- First 3 experiments:
  1. Verify vote count correlation with correctness on held-out data
  2. Test different k values (2, 4, 8, 16) on a small dataset to find optimal balance
  3. Compare weighted vs unweighted loss variants on development set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of iterations for SCPO to converge on different reasoning tasks?
- Basis in paper: Explicit
- Why unresolved: The paper reports experiments with up to 2 iterations showing consistent improvements, and briefly mentions a third iteration showing minimal gains. However, it does not systematically explore the convergence properties across different task types or provide a theoretical framework for determining when additional iterations become redundant or potentially harmful.
- What evidence would resolve it: A comprehensive ablation study varying the number of iterations (e.g., 1, 2, 3, 4, 5) across multiple reasoning datasets with different difficulty levels, measuring both performance gains and computational costs, would establish optimal iteration counts.

### Open Question 2
- Question: How does SCPO's performance scale with model size and what are the diminishing returns?
- Basis in paper: Inferred
- Why unresolved: The paper demonstrates SCPO on Llama-3 8B models and shows they can outperform larger models on ZebraLogic. However, it does not systematically investigate how SCPO performance scales with model size (e.g., comparing 7B, 13B, 34B, 70B+ models) or identify at what scale the benefits plateau relative to the computational cost of self-consistency sampling.
- What evidence would resolve it: A scaling study training SCPO across multiple model sizes on the same reasoning tasks, measuring performance improvements relative to computational requirements and comparing against model scaling laws without SCPO.

### Open Question 3
- Question: Can SCPO be effectively extended to non-mathematical reasoning tasks like summarization or question answering?
- Basis in paper: Inferred
- Why unresolved: The paper focuses exclusively on mathematical and logical reasoning tasks where final answers can be easily parsed and compared. It mentions in the conclusion that future work could extend SCPO to tasks where single final answers are not easily extracted, but does not provide any empirical evidence or methodology for such extensions.
- What evidence would resolve it: Experimental results applying SCPO to open-ended tasks like summarization or multi-hop question answering, with appropriate adaptations to the consistency metric and loss function, would demonstrate feasibility and limitations.

### Open Question 4
- Question: How sensitive is SCPO to the choice of consistency metric beyond simple vote counting?
- Basis in paper: Inferred
- Why unresolved: The paper primarily uses simple vote counting for consistency measurement and mentions that SCPO could work with other consistency measures like internal consistency or universal consistency. However, it does not compare different consistency metrics or explore how metric choice affects performance across different reasoning tasks.
- What evidence would resolve it: A comparative study testing SCPO with various consistency metrics (vote counting, internal consistency, universal consistency, entropy-based measures) on the same tasks, measuring performance differences and computational trade-offs.

### Open Question 5
- Question: What is the relationship between self-consistency thresholds and data quality/efficiency trade-offs?
- Basis in paper: Explicit
- Why unresolved: The paper explores threshold sensitivity on MATH (Table 5) but only examines a limited range and does not provide a systematic framework for choosing optimal thresholds across different tasks or model scales. The trade-off between data quality (higher thresholds) and quantity (lower thresholds) is noted but not fully characterized.
- What evidence would resolve it: A systematic grid search across multiple threshold values on various reasoning tasks, measuring the Pareto frontier between data efficiency (number of usable pairs) and model performance, would establish guidelines for threshold selection.

## Limitations
- SCPO depends on the assumption that model errors are random and independent across samples, which may not hold for complex reasoning tasks with systematic biases
- The method's generalizability to non-mathematical reasoning domains like summarization or commonsense reasoning is not well-established
- Performance on reasoning tasks where incorrect paths converge to similar wrong answers could degrade as SCPO might reinforce systematic errors

## Confidence
- **High Confidence**: The empirical results showing SCPO's performance improvements on GSM8K and MATH datasets are well-supported by the presented experiments
- **Medium Confidence**: The claim that SCPO-trained Llama-3 8B outperforms larger models like Llama-3 70B on ZebraLogic is supported but requires further validation
- **Low Confidence**: The generalizability of SCPO to other reasoning domains beyond mathematical and logical tasks is not well-established

## Next Checks
1. **Error Correlation Analysis**: Systematically analyze whether vote margins correlate with actual correctness on held-out data across different reasoning domains
2. **Iterative Stability Test**: Conduct extended iterative training beyond two iterations to determine whether performance plateaus, improves, or degrades
3. **Generalization Benchmark**: Test SCPO on a diverse set of reasoning tasks including commonsense reasoning, code generation, and scientific reasoning to assess whether the method generalizes beyond mathematical and logical domains where it was validated