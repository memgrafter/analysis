---
ver: rpa2
title: Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training
arxiv_id: '2411.07066'
source_url: https://arxiv.org/abs/2411.07066
tags:
- neuron
- sparsity
- pruning
- dsnot
- uniform
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces NeuroAL, a method to improve sparse LLM performance
  without retraining. It redistributes sparsity block-wise and row-wise by maximizing
  neuron alignment between dense and sparse activations.
---

# Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training

## Quick Facts
- arXiv ID: 2411.07066
- Source URL: https://arxiv.org/abs/2411.07066
- Reference count: 40
- Method improves sparse LLM performance without retraining by redistributing sparsity based on neuron alignment

## Executive Summary
NeuroAL introduces a two-step approach to improve sparse LLM performance by redistributing sparsity block-wise and row-wise. The method maximizes neuron alignment between dense and sparse activations while adaptively selecting optimal hyperparameters without manual tuning. Tested across four LLM families, three sparsity ratios, and ten tasks, NeuroAL consistently outperforms state-of-the-art methods, particularly at high sparsity levels. The algorithm requires no re-training and works as a post-processing step after initial pruning.

## Method Summary
NeuroAL takes a pre-trained dense LLM and an initial sparse model (created via standard pruning algorithms like Magnitude, MULTIFLOW, or Wanda) and redistributes the sparsity to improve performance. It uses a two-step process: first redistributing sparsity block-wise across transformer layers, then row-wise within each layer. The key innovation is adaptive selection of λ parameters that control the sparsity redistribution, chosen by maximizing neuron alignment between dense and sparse activations on a small calibration dataset. This allows the method to automatically find optimal sparsity distributions for each model-architecture combination without manual hyperparameter tuning.

## Key Results
- NeuroAL consistently improves sparse model performance across 4 LLM families (Phi-2, Llama-1, Llama-2, Mistral, OPT)
- Outperforms state-of-the-art methods especially at high sparsity ratios (70%, 80%)
- No re-training required, works as post-processing after initial pruning
- Achieves better perplexity scores on language modeling tasks and higher accuracy on zero-shot tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NeuroAL's adaptive selection of λ parameters enables effective block-wise and row-wise sparsity redistribution without manual tuning.
- Mechanism: NeuroAL evaluates multiple λ values by computing neuron alignment scores on a small calibration dataset. It selects the λ that maximizes alignment between dense and sparse activations, ensuring optimal sparsity distribution for each model and sparsity ratio combination.
- Core assumption: Neuron alignment serves as a reliable proxy for model performance when redistributing sparsity.
- Evidence anchors:
  - [abstract] "Differently from existing methods, our approach adaptively selects the best parameters for the block-wise and row-wise sparsity ratios w.r.t. the model and the desired sparsity (given as input), and requires no re-training."
  - [section] "As seen earlier, while OWL requires λ to be set a priori, we design NEURON AL to automatically select, from a suitable set of values, the best λ for each combination of D, P and s, yielding an adaptive top-up method."
- Break condition: If neuron alignment does not correlate with actual model performance, the adaptive selection would fail to find optimal sparsity distributions.

### Mechanism 2
- Claim: Non-uniform sparsity distribution across transformer blocks improves performance compared to uniform distribution.
- Mechanism: By allocating higher sparsity to later transformer blocks (which have been shown to be more redundant), NeuroAL preserves performance while achieving higher overall sparsity ratios.
- Core assumption: Later transformer blocks contribute less to final model performance than earlier blocks.
- Evidence anchors:
  - [abstract] "NEURON AL... modifies the block-wise and row-wise sparsity, exploiting information from both the dense model and its sparse version to maximize the neuron alignment among activations."
  - [section] "It is established [40] that pruning using the row as a comparison group achieves better performance w.r.t. using the whole layer since it inherently maximizes the network connectivity [20, 7]."
- Break condition: If all transformer blocks contribute equally to performance, non-uniform distribution would not provide benefits.

### Mechanism 3
- Claim: Row-wise sparsity redistribution provides additional performance gains beyond block-wise redistribution.
- Mechanism: After block-wise redistribution, NeuroAL further optimizes sparsity at the row level within each layer, using neuron alignment to identify which rows can be sparsified more aggressively.
- Core assumption: Row-level optimization provides meaningful performance improvements when combined with block-level optimization.
- Evidence anchors:
  - [abstract] "The algorithm consists of a two-step approach that re-distributes the block-wise sparsity... and the row-wise sparsity... maximizing the neuron alignment between dense and sparse activations."
  - [section] "The second step is complementary to the previous one, but in this case, the sparsity is modified w.r.t. the rows of each layer."
- Break condition: If row-level optimization provides negligible performance gains, the additional computational cost would not be justified.

## Foundational Learning

- Concept: Neuron alignment as a performance metric
  - Why needed here: NeuroAL uses neuron alignment instead of reconstruction error or outlier detection to guide sparsity redistribution
  - Quick check question: What is the key difference between neuron alignment and reconstruction error in pruning evaluation?

- Concept: Block-wise vs layer-wise sparsity
  - Why needed here: Understanding the distinction between sparsity at transformer block level versus individual layer level is crucial for NeuroAL's two-step approach
  - Quick check question: How does sparsity distribution differ between block-wise and layer-wise approaches in transformer architectures?

- Concept: Calibration data usage in pruning
  - Why needed here: NeuroAL requires a small calibration dataset to evaluate different λ parameters without using the full training set
  - Quick check question: Why does NeuroAL use only 8 samples from C4 for calibration instead of the full dataset?

## Architecture Onboarding

- Component map:
  Dense model -> Initial sparse model -> Block-wise optimization -> Row-wise optimization -> Final sparse model

- Critical path:
  1. Apply base pruning algorithm to obtain initial sparse model
  2. Compute dense activations on calibration data
  3. Block-wise redistribution: test multiple λ values, select best based on neuron alignment
  4. Row-wise redistribution: repeat optimization at row level
  5. Generate final sparse model using optimized sparsity masks

- Design tradeoffs:
  - Computational cost vs performance: Additional forward passes for calibration vs improved sparsity efficiency
  - Adaptivity vs simplicity: Automatic λ selection vs fixed hyperparameters
  - Memory usage vs accuracy: Small calibration set vs comprehensive evaluation

- Failure signatures:
  - Poor performance despite high sparsity: Neuron alignment may not correlate with actual task performance
  - High variance across runs: Calibration data selection may be too small or unrepresentative
  - Computational overhead: λ search space may be too large for practical use

- First 3 experiments:
  1. Compare NeuroAL vs uniform sparsity on a small model (Phi-2) at 70% sparsity to verify basic effectiveness
  2. Test different calibration data sizes to determine optimal trade-off between accuracy and computational cost
  3. Evaluate block-only vs row-only steps to quantify contribution of each optimization stage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does NEURON AL's performance on OPT models improve significantly with larger calibration datasets (e.g., |Cλ| > 16)?
- Basis in paper: [explicit] The paper notes that OPT models show higher sensitivity to calibration data size, with performance improving substantially when |Cλ| = 16 compared to |Cλ| = 8.
- Why unresolved: The study tested only up to 16 samples, leaving uncertainty about performance at even larger calibration sizes.
- What evidence would resolve it: Testing NEURON AL with calibration datasets of 32, 64, or 128 samples on OPT models to determine if performance plateaus or continues improving.

### Open Question 2
- Question: How does NEURON AL perform on models with non-standard architectures (e.g., vision transformers, multimodal models) compared to standard LLMs?
- Basis in paper: [inferred] The paper focuses exclusively on LLM families and does not test NEURON AL on other architectures, despite mentioning potential applicability to CNNs in related work.
- Why unresolved: The algorithm's neuron alignment mechanism may not generalize well to architectures with different layer structures or activation patterns.
- What evidence would resolve it: Applying NEURON AL to vision transformers, multimodal models, or other non-LLM architectures and comparing performance to baseline pruning methods.

### Open Question 3
- Question: What is the computational overhead of NEURON AL when applied to extremely large models (e.g., >100B parameters) compared to smaller LLMs?
- Basis in paper: [explicit] The paper tests scalability up to 13B parameters and notes that NEURON AL requires two forward passes plus hyperparameter search, but does not test on larger models.
- Why unresolved: The computational cost may scale non-linearly with model size, and memory constraints could become prohibitive.
- What evidence would resolve it: Applying NEURON AL to models with 100B+ parameters and measuring forward pass time, memory usage, and hyperparameter search efficiency compared to smaller models.

### Open Question 4
- Question: Can NEURON AL be adapted to work with structured sparsity patterns (e.g., N:M sparsity) while maintaining its performance advantages?
- Basis in paper: [explicit] The paper identifies as a limitation that NEURON AL cannot leverage optimized structured sparsity implementations due to its customized sparsity constraints.
- Why unresolved: The algorithm's neuron alignment mechanism may not translate well to structured sparsity patterns that require weight continuity.
- What evidence would resolve it: Modifying NEURON AL to generate structured sparsity patterns while preserving the neuron alignment optimization, then comparing performance to both unstructured NEURON AL and standard structured pruning methods.

## Limitations
- Neuron alignment as proxy: The paper assumes neuron alignment correlates with actual model performance, but this theoretical justification is not rigorously established
- Small calibration dataset: Reliance on only 8 calibration samples raises questions about generalizability across different data distributions
- Computational overhead: Additional forward passes and hyperparameter search may be prohibitive for extremely large models

## Confidence

**High Confidence**: The empirical results demonstrating NeuroAL's consistent improvement over baseline pruning methods across multiple model families (Phi-2, Llama-1, Llama-2, Mistral, OPT) and sparsity levels (60%, 70%, 80%). The methodology is clearly specified and reproducible.

**Medium Confidence**: The claim that non-uniform sparsity distribution across transformer blocks provides benefits, as this relies on the established finding that later blocks are more redundant. While this aligns with existing literature, the specific degree of benefit varies across models.

**Low Confidence**: The theoretical foundation for why neuron alignment serves as an optimal proxy for performance, and the guarantees of the adaptive λ selection mechanism. The paper provides empirical evidence but lacks rigorous theoretical justification.

## Next Checks

1. **Calibration Data Sensitivity Analysis**: Systematically vary the size and composition of the calibration dataset (e.g., 4, 8, 16, 32 samples from different distributions) to quantify how calibration set characteristics affect final performance and identify the minimum viable calibration set size.

2. **Neuron Alignment vs Direct Performance Correlation**: Conduct controlled experiments where neuron alignment scores are compared against actual downstream task performance across different sparsity distributions, to empirically validate whether neuron alignment is indeed a reliable proxy for model effectiveness.

3. **Cross-Model Transferability Test**: Apply hyperparameters (λ values) selected for one model family to another model family at the same sparsity ratio to test whether the adaptive selection truly captures model-specific characteristics or whether there are transferable patterns across architectures.