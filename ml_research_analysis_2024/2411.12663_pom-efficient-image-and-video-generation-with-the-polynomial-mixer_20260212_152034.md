---
ver: rpa2
title: 'PoM: Efficient Image and Video Generation with the Polynomial Mixer'
arxiv_id: '2411.12663'
source_url: https://arxiv.org/abs/2411.12663
tags:
- self
- diffusion
- arxiv
- generation
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Polynomial Mixer (PoM), a novel attention-free
  building block for diffusion models that scales linearly with sequence length while
  maintaining generation quality. PoM encodes the entire input sequence into an explicit
  state representation using polynomial mixing, enabling both parallel training and
  efficient sequential inference.
---

# PoM: Efficient Image and Video Generation with the Polynomial Mixer

## Quick Facts
- **arXiv ID**: 2411.12663
- **Source URL**: https://arxiv.org/abs/2411.12663
- **Reference count**: 40
- **Primary result**: PoM achieves FID 2.46, IS 240.6 on ImageNet with linear complexity

## Executive Summary
This paper introduces the Polynomial Mixer (PoM), an attention-free building block for diffusion models that scales linearly with sequence length while maintaining generation quality. PoM encodes the entire input sequence into an explicit state representation using polynomial mixing, enabling both parallel training and efficient sequential inference. Theoretical analysis proves PoM is a universal sequence-to-sequence approximator, similar to traditional Multi-Head Attention. Experiments demonstrate competitive performance with state-of-the-art DiT models while being more compute-efficient at higher resolutions.

## Method Summary
PoM is a novel attention-free building block that replaces traditional Multi-Head Attention in diffusion transformers. It works by encoding the entire input sequence into an explicit state representation through polynomial mixing, where each token is expanded into polynomial features and aggregated into a shared state. This state is then accessed by each token through gating coefficients, allowing full context access with linear complexity. The method enables both parallel training and efficient sequential inference through block-causal masking, and is theoretically proven to be a universal sequence-to-sequence approximator.

## Key Results
- Achieves competitive performance on ImageNet: FID 2.46, IS 240.6, Precision 0.78, Recall 0.60
- Demonstrates linear complexity scaling with sequence length, unlike quadratic MHA
- Shows improved temporal consistency in text-to-video generation with block-causal masking
- Available code at https://github.com/davidpicard/HoMM

## Why This Works (Mechanism)

### Mechanism 1
PoM enables linear complexity sequence modeling while preserving full context access by encoding the entire input sequence into a single explicit state representation (H(X)) using polynomial mixing, allowing each token to query this shared state independently through gating coefficients. Core assumption: A sufficiently high-degree polynomial can uniquely represent the context of any sequence. Evidence: PoM has linear complexity with respect to the number of tokens, and the complexity is no longer quadratic but linear with the sequence length n.

### Mechanism 2
PoM is a universal sequence-to-sequence approximator equivalent to MHA through its contextual mapping property, which allows it to map every token to a unique value depending on the entire sequence, then feed-forward networks can map those unique values to desired outputs. Core assumption: The contextual mapping property holds for sufficiently high-degree polynomials. Evidence: Lemma 3 proves the contextual mapping property required for universal approximation.

### Mechanism 3
PoM enables both parallel training and efficient sequential inference by computing the state representation once during training for parallel processing, and using block-causal masking during inference to allow sequential processing of token groups while maintaining context access. Core assumption: The explicit state representation can be computed once and reused for all tokens. Evidence: H(X) is an explicit hidden state that is updated by adding the polynomial mapping of the next token, enabling O(1) inference complexity.

## Foundational Learning

- **Polynomial mixing and basis expansion**: Why needed here: PoM relies on expanding each token into polynomial features (h(WmX)) to capture higher-order interactions between tokens. Quick check question: What happens to the representation capacity if we only use linear (degree 1) polynomials in PoM?

- **State-space models and recurrent architectures**: Why needed here: PoM shares similarities with SSMs in using explicit state representations, but differs in how it updates and accesses this state. Quick check question: How does PoM's state update differ from traditional RNNs in terms of information flow?

- **Universal approximation theory for sequence models**: Why needed here: The paper's main theoretical contribution is proving PoM can approximate any continuous sequence-to-sequence function. Quick check question: What key property must PoM have to satisfy the conditions for universal approximation?

## Architecture Onboarding

- **Component map**: Input sequence → Polynomial expansion (h(WmX)) → State aggregation (H(X)) → Gating (σ(WsX)) → Output projection (Wo) → Residual connections → AdaLN modulation

- **Critical path**: 1) Polynomial expansion and state aggregation (H(X)), 2) Gating coefficient computation (σ(WsX)), 3) Hadamard product and output projection, 4) Residual addition with input

- **Design tradeoffs**: Higher polynomial degree → better approximation but more computation; Larger state dimension → better context representation but more memory; Block-causal vs full causal masking → efficiency vs temporal consistency

- **Failure signatures**: Mode collapse → likely insufficient polynomial degree or state capacity; Slow convergence → possible issues with gating coefficient scaling; Memory errors at high resolution → state dimension too large for available memory

- **First 3 experiments**: 1) Test different polynomial degrees (1, 2, 3) on a small ImageNet subset to find the minimum degree for good performance, 2) Compare training speed and memory usage between PoM and MHA at different resolutions, 3) Validate the sequential inference capability by measuring generation time for long sequences with block-causal masking

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical proof of universal approximation relies on assumptions about polynomial basis functions that may not hold for very long sequences or complex dependencies
- Comparison with state-of-the-art DiT models is limited to standard ImageNet benchmarks without extensive testing on more challenging tasks
- Efficiency gains at higher resolutions are demonstrated but not thoroughly analyzed for practical deployment considerations

## Confidence
- **PoM's linear complexity and competitive performance**: High confidence - comprehensive experimental results on ImageNet show clear improvements in multiple metrics while maintaining computational efficiency
- **Universal approximation capability**: Medium confidence - rigorous theoretical proof but practical implications not fully validated beyond standard diffusion model tasks
- **Efficient sequential inference with block-causal masking**: Medium confidence - mechanism well-explained but lacks detailed analysis of generation quality and speed for long video sequences

## Next Checks
1. **Polynomial degree sensitivity analysis**: Systematically test PoM with polynomial degrees ranging from 1 to 5 on diverse sequence modeling tasks (text, audio, time series) to validate the relationship between polynomial degree and approximation quality, measuring both performance and computational overhead.

2. **Long-range dependency test**: Design a benchmark task requiring modeling dependencies over very long sequences (10,000+ tokens) and compare PoM's performance against MHA-based models, specifically testing whether PoM maintains performance while MHA's quadratic complexity becomes prohibitive.

3. **Cross-domain generalization study**: Evaluate PoM on datasets with significantly different characteristics from ImageNet (CelebA-HQ, LSUN, Kinetics-600) to test the robustness of PoM's approximation capabilities across different data distributions and modalities.