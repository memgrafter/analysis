---
ver: rpa2
title: 'ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy'
arxiv_id: '2403.14589'
source_url: https://arxiv.org/abs/2403.14589
tags:
- agent
- trajectories
- round
- training
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents A3T, a framework for autonomous annotation
  of agent trajectories in the style of ReAct, enabling closed-loop self-improvement
  of language agents. The key idea is to use an ActRe prompting agent to explain the
  reasoning behind arbitrary actions, allowing the ReAct-style agent to compose diverse
  trajectories by sampling actions and prepending synthesized reasons.
---

# ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy

## Quick Facts
- arXiv ID: 2403.14589
- Source URL: https://arxiv.org/abs/2403.14589
- Reference count: 24
- One-line primary result: A3T framework achieves up to 100% success rates in iterative refinement settings for language agents

## Executive Summary
This paper presents A3T, a framework for autonomous annotation of agent trajectories in the style of ReAct, enabling closed-loop self-improvement of language agents. The key innovation is using an ActRe prompting agent to explain reasoning behind arbitrary actions, allowing the ReAct-style agent to compose diverse trajectories by sampling actions and prepending synthesized reasons. These trajectories are then used for contrastive self-training with policy gradient methods, creating a self-improvement loop. Experiments on AlfWorld and WebShop show significant performance improvements over state-of-the-art methods.

## Method Summary
A3T framework combines autonomous trajectory annotation with contrastive self-training for language agents. The system uses an ActRe prompting agent to generate rationales for randomly sampled actions, which are then prepended to create novel trajectories in ReAct format. The environment automatically labels trajectory quality through terminal rewards. These trajectories are used to fine-tune the language model using policy gradient methods with binarized rewards, contrasting successful and failed trajectories. The process iterates across multiple rounds, with each round generating new trajectories using the improved agent from the previous round, forming a closed loop for self-improvement.

## Key Results
- A3T agents achieve up to 100% success rates in iterative refinement settings on AlfWorld and WebShop tasks
- The framework demonstrates significant performance improvements over state-of-the-art methods
- Iterative refinement shows continual performance improvement across rounds, with success rates increasing from 0.320 to 0.846 in AlfWorld experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autonomous trajectory annotation enables the agent to generate diverse training data without human supervision.
- Mechanism: The ActRe prompting agent synthesizes rationales for randomly sampled actions, enabling the ReAct-style agent to compose novel trajectories by prepending these synthesized reasons to the sampled actions. The environment automatically labels trajectory quality through terminal rewards.
- Core assumption: The ActRe agent can generate coherent and task-relevant rationales for arbitrary actions when prompted with the action and partial trajectory context.
- Evidence anchors:
  - [abstract] "The central role is an ActRe prompting agent, which explains the reason for an arbitrary action."
  - [section] "To facilitate this, we propose ActRe, an act-then-reason prompting agent that explains the reason for the sampled action."
- Break condition: ActRe fails to generate coherent rationales, or the synthesized trajectories are systematically invalid for the task environment.

### Mechanism 2
- Claim: Contrastive self-training with policy gradient methods improves agent performance by explicitly modeling the difference between successful and failed trajectories.
- Mechanism: The policy gradient objective (Eq. 1) is shaped to maximize the log-likelihood of successful trajectories while minimizing that of failed trajectories. Binarized rewards (±1) create a clear signal for contrastive learning.
- Core assumption: The environment provides reliable binary feedback (success/failure) that can be used as a reward signal for policy gradient methods.
- Evidence anchors:
  - [section] "we use policy gradient methods (Williams, 1992) with binarized rewards for LLM fine-tuning."
  - [section] "Eq. (2) for the K+1 trajectories can be structured as... [showing the contrastive objective formulation]"
- Break condition: The reward signal becomes noisy or non-binary, or the policy gradient optimization diverges due to poor trajectory sampling.

### Mechanism 3
- Claim: Iterative refinement through closed-loop self-improvement progressively enhances agent capabilities by accumulating higher-quality trajectories over rounds.
- Mechanism: Each round generates new trajectories using the improved agent from the previous round, leading to progressively better self-training data and performance improvements across rounds.
- Core assumption: The agent's performance improves monotonically with each round of self-training, leading to better trajectory generation in subsequent rounds.
- Evidence anchors:
  - [section] "As new agents are trained, more trajectories can be gathered and accumulated, which forms a closed loop for the self-improvement of language agents."
  - [section] "Table 3 shows that the success rate of the trajectories composed by the agent on the training tasks improves continually."
- Break condition: The agent plateaus in performance, or the accumulated trajectory set becomes saturated with redundant or low-quality examples.

## Foundational Learning

- Concept: Policy gradient methods for reinforcement learning
  - Why needed here: The contrastive self-training objective is formulated as a policy gradient optimization problem, requiring understanding of gradient estimation and trajectory-based learning.
  - Quick check question: How does the binarized reward (±1) shape the policy gradient objective compared to using raw environment rewards?

- Concept: Contrastive learning principles
  - Why needed here: The framework explicitly contrasts successful and failed trajectories, requiring understanding of how positive and negative examples drive representation learning.
  - Quick check question: What is the effect of the coefficient (1 - R(τf))/(2K) in Eq. (2) on the balance between supervised fine-tuning and contrastive learning?

- Concept: Large language model fine-tuning with QLoRA
  - Why needed here: The framework uses QLoRA to efficiently fine-tune Mistral-7B-Instruct-v0.2 with the collected trajectories, requiring understanding of low-rank adaptation and quantization techniques.
  - Quick check question: How does the choice of rank and alpha parameters in QLoRA affect the trade-off between model capacity and computational efficiency?

## Architecture Onboarding

- Component map: ActRe prompting agent -> ReAct-style policy agent -> Environment interface -> Trajectory accumulator -> QLoRA fine-tuner

- Critical path:
  1. Sample action -> Query ActRe -> Synthesize reason -> Compose trajectory
  2. Execute trajectory -> Receive reward -> Store trajectory
  3. Aggregate trajectories -> Apply policy gradient update -> New agent ready

- Design tradeoffs:
  - Exploration vs exploitation: Random action sampling enables diverse data but may reduce immediate task success
  - Trajectory length: Longer trajectories provide more training signal but increase computational cost
  - Reward binarization: Simplifies contrastive learning but loses fine-grained performance information

- Failure signatures:
  - ActRe consistently generates irrelevant or incoherent rationales
  - Agent performance degrades over rounds (indicates data quality issues)
  - Training diverges or plateaus quickly (indicates poor gradient signals)

- First 3 experiments:
  1. Validate ActRe generates coherent rationales for a small set of sampled actions in a controlled environment
  2. Test trajectory composition with ActRe-generated reasons and verify environment compatibility
  3. Run a single round of contrastive self-training and measure performance improvement on a validation set

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's reliance on ActRe for generating coherent rationales represents a critical vulnerability that may not hold across diverse environments or action spaces
- The long-term stability of the iterative refinement process and ability to maintain trajectory diversity over multiple rounds are not thoroughly evaluated
- The effectiveness of contrastive self-training depends heavily on the quality and diversity of autonomously collected trajectories, which could degrade over iterative rounds

## Confidence
- **High confidence**: The basic A3T framework architecture and its components (ActRe prompting, trajectory composition, contrastive self-training) are technically sound and well-motivated by existing literature
- **Medium confidence**: The claimed performance improvements (up to 100% success rates in iterative refinement) are based on experiments in specific environments (AlfWorld and WebShop), but generalizability to other domains remains uncertain
- **Low confidence**: The framework's ability to maintain trajectory diversity and performance across multiple refinement rounds is not thoroughly evaluated

## Next Checks
1. **ActRe Reliability Test**: Systematically evaluate ActRe's performance across diverse action types and environments, measuring the coherence and task-relevance of generated rationales using human evaluation or automated metrics

2. **Iterative Stability Analysis**: Track trajectory diversity metrics (e.g., unique action sequences, state coverage) across all refinement rounds to identify potential degradation or saturation effects in the self-improvement loop

3. **Cross-Domain Transferability**: Test A3T on at least two additional environments with different action spaces and reward structures to validate the framework's generalizability beyond the reported AlfWorld and WebShop experiments