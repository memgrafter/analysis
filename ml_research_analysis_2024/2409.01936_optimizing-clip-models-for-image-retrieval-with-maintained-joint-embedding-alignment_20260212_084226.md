---
ver: rpa2
title: Optimizing CLIP Models for Image Retrieval with Maintained Joint-Embedding
  Alignment
arxiv_id: '2409.01936'
source_url: https://arxiv.org/abs/2409.01936
tags:
- image
- retrieval
- clip
- text
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of optimizing CLIP models for
  image-based similarity search while maintaining strong performance in text-based
  tasks such as text-to-image retrieval and zero-shot classification. The authors
  propose two novel methods to address this: a two-stage fine-tuning approach and
  a multi-caption image pairing (MCIP) method.'
---

# Optimizing CLIP Models for Image Retrieval with Maintained Joint-Embedding Alignment

## Quick Facts
- arXiv ID: 2409.01936
- Source URL: https://arxiv.org/abs/2409.01936
- Authors: Konstantin Schall; Kai Uwe Barthel; Nico Hezel; Klaus Jung
- Reference count: 40
- Key outcome: The paper proposes two novel methods to optimize CLIP models for image-based similarity search while maintaining strong performance in text-based tasks. The MCIP method achieves an average improvement of 7.5% in image-to-image retrieval tasks while maintaining high accuracy in zero-shot classification.

## Executive Summary
This paper addresses the challenge of optimizing CLIP models for image-based similarity search while preserving performance in text-based tasks such as text-to-image retrieval and zero-shot classification. The authors propose two novel methods: a two-stage fine-tuning approach and a multi-caption image pairing (MCIP) method. Through extensive experiments, they demonstrate that these methods significantly enhance CLIP's performance on various benchmarks, including image retrieval, k-NN classification, and zero-shot text-based classification, while maintaining robustness in text-to-image retrieval. Notably, the MCIP method achieves an average improvement of 7.5% in image-to-image retrieval tasks and maintains high accuracy in zero-shot classification, showcasing its effectiveness in balancing both image and text-based search capabilities.

## Method Summary
The paper proposes two methods to optimize CLIP models for image-based similarity search while maintaining text-based task performance. The first method, two-stage fine-tuning (2SFT), involves initially optimizing the image encoder with ArcMargin loss for image retrieval tasks, followed by realigning the text encoder using InfoNCE loss to maintain joint-embedding alignment. The second method, Multi-Caption Image Pairing (MCIP), generates multiple pseudo-captions per image using similarity search against CC12M, then fine-tunes the image encoder using MCArcMargin loss while keeping the text encoder locked. Both methods are evaluated on a comprehensive set of benchmarks including image-to-image retrieval, k-NN classification, zero-shot classification, and text-to-image retrieval, demonstrating significant improvements in image retrieval performance while preserving text-based capabilities.

## Key Results
- MCIP method achieves an average improvement of 7.5% in image-to-image retrieval tasks across multiple benchmarks
- Both methods maintain high accuracy in zero-shot classification tasks (ImageNet1k, ImageNet-Sketch, ImageNet-Adversarial, FGVCAircraft)
- MCIP+Re-A fine-tuning leads to better results compared to naive two-stage fine-tuning on image-to-image retrieval benchmarks
- The optimized models enable single embedding storage per image, simplifying infrastructure for large-scale multi-modal similarity search systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage fine-tuning improves image retrieval while preserving text-to-image alignment
- Mechanism: First fine-tune image encoder with ArcMargin loss on multi-domain dataset, then re-align text encoder using InfoNCE loss with locked image encoder
- Core assumption: Image encoder improvements from retrieval-specific fine-tuning can be preserved while re-aligning text embeddings to the new image space
- Evidence anchors:
  - [abstract] "The first method involves a sequential fine-tuning process: initially optimizing the image encoder for more precise image retrieval and subsequently realigning the text encoder to these optimized image embeddings."
  - [section 3.2] "The fine-tuned image-encoder produces embeddings that capture more visual information of the images and performs better across a wide range of image-based similarity search tasks."
  - [corpus] Weak evidence - corpus neighbors don't directly discuss sequential fine-tuning approaches
- Break condition: If re-alignment phase causes text embeddings to drift too far from original CLIP space, text-to-image retrieval performance degrades

### Mechanism 2
- Claim: Multi-caption pseudo-labeling maintains alignment during retrieval optimization
- Mechanism: Generate multiple pseudo-captions per image using similarity search against CC12M, then use MCArcMargin loss to optimize image encoder while keeping text encoder locked
- Core assumption: Multiple noisy captions per image provide sufficient signal to maintain joint-embedding alignment during retrieval optimization
- Evidence anchors:
  - [abstract] "The second approach integrates pseudo-captions during the retrieval-optimization phase to foster direct alignment within the embedding space."
  - [section 3.3] "This loss function ensures that the angles between an image-embedding and the text-embeddings of the generated captions for this image are small"
  - [corpus] Weak evidence - corpus neighbors don't discuss pseudo-captioning approaches
- Break condition: If pseudo-captions are too noisy or don't capture visual content accurately, alignment maintenance fails

### Mechanism 3
- Claim: Multi-task loss balancing maintains overall system performance
- Mechanism: Combine ArcMargin loss for class labels with MCArcMargin loss for pseudo-captions using weighted sum (λ1=0.5, λ2=0.5)
- Core assumption: Balancing retrieval-specific optimization with alignment maintenance through weighted loss functions achieves best overall performance
- Evidence anchors:
  - [abstract] "Our optimized models permit maintaining a single embedding per image, significantly simplifying the infrastructure needed for large-scale multi-modal similarity search systems."
  - [section 3.3] "This results show that the MCIP+Re-A fine-tuning leads to better results compared to the naive two-stage-fine-tuning"
  - [corpus] Weak evidence - corpus neighbors don't discuss multi-task loss balancing approaches
- Break condition: If loss weighting is suboptimal, either retrieval performance or alignment maintenance suffers

## Foundational Learning

- Concept: Contrastive learning with InfoNCE loss
  - Why needed here: Core training mechanism for CLIP models that enables joint-embedding space learning
  - Quick check question: What is the mathematical form of InfoNCE loss and how does it encourage alignment between positive pairs?

- Concept: Multi-label classification loss functions
  - Why needed here: ArcMargin loss modification enables retrieval-specific optimization while maintaining class discrimination
  - Quick check question: How does ArcMargin loss differ from standard softmax cross-entropy and why is it beneficial for retrieval tasks?

- Concept: Pseudo-label generation and quality assessment
  - Why needed here: MCIP method relies on generating high-quality pseudo-captions for alignment maintenance
  - Quick check question: What similarity threshold was used for caption selection and why might this be suboptimal for certain image domains?

## Architecture Onboarding

- Component map:
  - Image encoder (ViT-L/336, ViT-SO400M, etc.) -> Image projector (discarded during GPR-FT, retained otherwise) -> CLIP pre-trained text encoder -> CLIP pre-trained text projector -> Loss functions (InfoNCE, ArcMargin, MCArcMargin) -> Training datasets (CC12M, GPR training data)

- Critical path:
  1. Extract image embeddings from GPR training set using pre-trained CLIP
  2. Generate pseudo-captions via nearest-neighbor search in text embedding space
  3. Fine-tune image encoder with ArcMargin loss (GPR-FT stage)
  4. Re-align text encoder using InfoNCE loss (Re-A stage for 2SFT)
  5. Evaluate on all benchmarks

- Design tradeoffs:
  - Single vs. dual embedding storage: Our methods enable single embedding storage vs. traditional approaches requiring separate image/text embeddings
  - Fine-tuning vs. inference speed: Additional fine-tuning improves retrieval but increases computational cost
  - Caption quality vs. quantity: More pseudo-captions improve alignment but increase training time

- Failure signatures:
  - Text-to-image retrieval degradation: Indicates re-alignment failure
  - k-NN classification drop: Suggests loss of generalization
  - Image retrieval improvement but text classification drop: Indicates embedding space distortion

- First 3 experiments:
  1. Baseline: Evaluate original CLIP model on all benchmarks to establish performance baseline
  2. GPR-FT only: Fine-tune image encoder with ArcMargin loss and evaluate retrieval performance degradation in text tasks
  3. MCIP vs 2SFT: Compare multi-caption approach with two-stage fine-tuning on retrieval and alignment metrics

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions in the text provided.

## Limitations
- The quality of pseudo-captions generated for MCIP method is not thoroughly validated, which directly impacts alignment maintenance
- Computational overhead of generating and using multiple pseudo-captions is not discussed in detail
- Sensitivity of results to the pseudo-caption similarity threshold (0.27) is not explored

## Confidence
- **High Confidence**: The two-stage fine-tuning mechanism and its effectiveness in improving image retrieval while maintaining text-to-image performance is well-supported by experimental results across multiple benchmarks
- **Medium Confidence**: The MCIP method's effectiveness relies on pseudo-caption quality, which is not thoroughly validated. While improvements are demonstrated, the robustness to caption noise is not explored
- **Medium Confidence**: The claim that these methods enable single embedding storage is technically correct but the practical implications for large-scale systems are not quantified in terms of storage or latency improvements

## Next Checks
1. Ablation Study on Pseudo-Caption Quality: Evaluate MCIP performance with varying similarity thresholds (0.2, 0.3, 0.4) to understand robustness to caption noise
2. Cross-Domain Generalization: Test optimized models on significantly different datasets (e.g., medical imaging, satellite imagery) to assess generalization beyond the retrieval datasets used for fine-tuning
3. Storage and Latency Analysis: Quantify the actual storage savings and query latency improvements from using single vs. dual embeddings in a realistic retrieval system deployment