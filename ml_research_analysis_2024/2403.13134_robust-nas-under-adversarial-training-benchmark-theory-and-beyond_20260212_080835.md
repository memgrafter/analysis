---
ver: rpa2
title: 'Robust NAS under adversarial training: benchmark, theory, and beyond'
arxiv_id: '2403.13134'
source_url: https://arxiv.org/abs/2403.13134
tags:
- clip
- robust
- training
- neural
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark and theoretical analysis
  for robust neural architecture search (NAS) under adversarial training. The authors
  construct NAS-RobBench-201, a benchmark dataset containing clean and robust accuracy
  metrics for 6,466 adversarially trained networks from the NAS-Bench-201 search space
  across three image datasets.
---

# Robust NAS under adversarial training: benchmark, theory, and beyond

## Quick Facts
- arXiv ID: 2403.13134
- Source URL: https://arxiv.org/abs/2403.13134
- Authors: Yongtao Wu; Fanghui Liu; Carl-Johann Simon-Gabriel; Grigorios G Chrysos; Volkan Cevher
- Reference count: 40
- This paper presents a comprehensive benchmark and theoretical analysis for robust neural architecture search (NAS) under adversarial training.

## Executive Summary
This paper addresses the challenge of designing neural architectures that are both accurate and robust to adversarial attacks. The authors construct NAS-RobBench-201, a benchmark dataset containing clean and robust accuracy metrics for 6,466 adversarially trained networks from the NAS-Bench-201 search space across three image datasets. Using the neural tangent kernel (NTK) tool, they establish a generalization theory for searching architectures in terms of clean and robust accuracy under multi-objective adversarial training. The benchmark results show a significant gap between the performance of different architectures, motivating the need for robust architecture design.

## Method Summary
The authors propose a two-pronged approach to robust NAS. First, they construct NAS-RobBench-201 by training all architectures in the NAS-Bench-201 search space under adversarial training with various perturbation magnitudes (ε ∈ {2/255, 4/255, 8/255}) and attack methods (PGD, FGSM, and AutoAttack). Second, they develop a theoretical framework using neural tangent kernels to analyze the generalization performance of architectures in terms of both clean and robust accuracy. The NTK-based analysis reveals that clean accuracy is influenced by both clean and robust NTKs, while robust accuracy is affected by robust NTK and its "twice" perturbed version. The authors demonstrate the utility of NTK metrics for architecture search through correlation analysis.

## Key Results
- Construction of NAS-RobBench-201 benchmark with 6,466 adversarially trained networks across three datasets
- Significant performance gap between architectures, highlighting the importance of robust architecture design
- NTK-based theoretical analysis showing that clean accuracy depends on both clean and robust NTKs, while robust accuracy depends on robust NTK and its twice-perturbed version
- Correlation analysis demonstrating the utility of NTK metrics for architecture search

## Why This Works (Mechanism)
The paper's approach leverages the neural tangent kernel (NTK) framework to analyze the generalization performance of architectures under adversarial training. The NTK captures the behavior of infinitely wide neural networks trained with gradient descent, providing a tractable way to study the relationship between architecture and performance. By analyzing the NTKs corresponding to clean and adversarially perturbed inputs, the authors derive bounds on the clean and robust accuracy of architectures. This theoretical insight guides the search for robust architectures by identifying key NTK properties that correlate with good performance.

## Foundational Learning
1. Neural Tangent Kernel (NTK)
   - Why needed: NTK provides a tractable way to analyze the behavior of infinitely wide neural networks, enabling theoretical analysis of architecture performance
   - Quick check: Verify that the NTK approximation holds for the architectures and datasets considered

2. Adversarial Training
   - Why needed: Adversarial training improves the robustness of neural networks to adversarial attacks, a critical requirement for real-world deployment
   - Quick check: Ensure that the adversarial training process effectively improves robustness across the architectures and datasets

3. Multi-objective Optimization
   - Why needed: Robust NAS involves balancing clean and robust accuracy, requiring multi-objective optimization techniques
   - Quick check: Verify that the proposed method effectively balances clean and robust accuracy across the search space

## Architecture Onboarding

Critical Path: NAS-RobBench-201 Construction -> NTK-based Theoretical Analysis -> Correlation Analysis -> Architecture Search

Design Tradeoffs:
- Search Space: NAS-Bench-201 is limited but tractable for exhaustive evaluation; larger search spaces may yield better architectures but are computationally expensive
- Adversarial Training: Different perturbation magnitudes and attack methods affect robustness; finding the right balance is crucial
- NTK Approximation: Assumes infinite width; may not fully capture finite-width network behavior

Failure Signatures:
- Poor correlation between NTK metrics and architecture performance indicates limitations of the NTK approximation
- Failure of adversarial training to improve robustness suggests issues with the training process or search space

First Experiments:
1. Replicate the correlation analysis between NTK metrics and architecture performance using alternative search spaces
2. Conduct ablation studies on the impact of different adversarial training hyperparameters on NTK-based predictions
3. Implement and evaluate the practical utility of NTK-based architecture selection in a full NAS pipeline

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability beyond the NAS-Bench-201 search space
- NTK-based framework may not fully account for practical optimization dynamics and hyperparameter interactions
- Focus on three datasets may not capture the complexity of larger, more diverse search spaces

## Confidence
- High confidence in the construction and basic validity of the NAS-RobBench-201 benchmark dataset, and the mathematical derivation of NTK-based generalization bounds
- Medium confidence in the correlation analysis between NTK metrics and architecture performance
- Low confidence in the practical applicability of theoretical insights for guiding real-world robust NAS implementations

## Next Checks
1. Replicate the correlation analysis between NTK metrics and architecture performance using alternative search spaces (e.g., DARTS or MobileNet-based spaces) to verify robustness of findings
2. Conduct ablation studies on the impact of different adversarial training hyperparameters (epsilon values, attack methods) on the NTK-based theoretical predictions
3. Implement and evaluate the practical utility of NTK-based architecture selection strategies in a full NAS pipeline, comparing against established robust architecture search methods