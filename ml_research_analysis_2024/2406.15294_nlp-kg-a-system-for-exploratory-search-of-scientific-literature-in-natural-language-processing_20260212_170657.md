---
ver: rpa2
title: 'NLP-KG: A System for Exploratory Search of Scientific Literature in Natural
  Language Processing'
arxiv_id: '2406.15294'
source_url: https://arxiv.org/abs/2406.15294
tags:
- search
- graph
- literature
- publications
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents NLP-KG, a system designed to support exploratory
  search of scientific literature in natural language processing (NLP). The system
  addresses the limitations of existing keyword-based search engines by providing
  a feature-rich platform that includes semantic search, conversational search, and
  an interactive visualization of the NLP Fields of Study (FoS) hierarchy.
---

# NLP-KG: A System for Exploratory Search of Scientific Literature in Natural Language Processing
## Quick Facts
- arXiv ID: 2406.15294
- Source URL: https://arxiv.org/abs/2406.15294
- Reference count: 6
- Primary result: A knowledge-graph-powered system for exploratory search of NLP literature with F1 score of 99.80 for FoS hierarchy and RAG pipeline faithfulness/relevance above 0.96

## Executive Summary
NLP-KG is a system designed to support exploratory search of scientific literature in natural language processing (NLP). It addresses the limitations of existing keyword-based search engines by providing a feature-rich platform that includes semantic search, conversational search, and an interactive visualization of the NLP Fields of Study (FoS) hierarchy. The system leverages a knowledge graph and state-of-the-art retrieval approaches, including a Retrieval Augmented Generation (RAG) pipeline and a fine-tuned Large Language Model (LLM), to provide users with comprehensive exploration possibilities.

## Method Summary
NLP-KG constructs a semi-automated hierarchical, acyclic graph of NLP Fields of Study using fine-tuned Packed Levitated Marker (PL-Marker) models for entity and relation extraction from NLP research articles. The system implements hybrid semantic search combining BM25 and SPECTER2 embeddings with Reciprocal Rank Fusion, uses a two-step classification approach for FoS classification, and employs a binary classifier for survey paper identification. The RAG pipeline with LLM grounding retrieves relevant publications and generates answers with inline citations, while conversational search allows users to pose NLP-related questions with contextually grounded responses.

## Key Results
- FoS hierarchy graph correctness achieved F1 score of 99.80
- RAG pipeline faithfulness and answer relevance both exceeded 0.96
- System provides comprehensive exploration features including keyword search, FoS visualization, survey filtering, and conversational search
- Successfully addresses limitations of keyword-based search engines for exploratory literature search

## Why This Works (Mechanism)
### Mechanism 1
- Claim: The hierarchical FoS graph improves exploratory search by providing users with a structured, navigable map of NLP concepts.
- Mechanism: Users unfamiliar with a field can start from a high-level FoS and progressively explore more specific sub-topics through the hierarchy, reducing the cognitive load of discovering related research areas.
- Core assumption: Users benefit from a visual, hierarchical organization of FoS rather than flat keyword lists.
- Evidence anchors: "A Fields of Study hierarchy graph enables users to familiarize themselves with a field and its related areas" and "users typically navigate from more well-known general concepts to less well-known and more specific concepts."

### Mechanism 2
- Claim: The hybrid semantic search (BM25 + SPECTER2 embeddings) outperforms pure keyword or pure embedding search.
- Mechanism: BM25 retrieves relevant documents based on keyword matching, while SPECTER2 embeddings capture semantic similarity. Reciprocal Rank Fusion merges these results, weighting embeddings more heavily (α=0.8), providing both precision and semantic recall.
- Core assumption: Combining sparse and dense retrieval methods covers both exact keyword matches and related concepts not using the exact terms.
- Evidence anchors: "To give more weight to the embedding-based approach, we set the α parameter determining the weight between sparse and dense retrieval to 0.8" and high faithfulness and answer relevance scores from RAG pipeline.

### Mechanism 3
- Claim: The RAG pipeline with LLM grounding reduces hallucinations and improves answer trustworthiness.
- Mechanism: The LLM generates search queries, retrieves top-5 relevant papers, and then generates answers grounded in their full texts, with inline citations to source sections.
- Core assumption: Providing source context and citations increases user trust and allows verification of answers.
- Evidence anchors: "Users can pose NLP-related questions to the LLM, which generates responses utilizing knowledge obtained from retrieved publications, accompanied by reference information" and high faithfulness (0.966) and answer relevance (0.848) scores.

## Foundational Learning
- Knowledge Graph (KG) structure and traversal: Essential for implementing the FoS hierarchy and related searches. Quick check: How would you retrieve all papers related to a given FoS and its child FoS in Neo4j?
- Hybrid retrieval (sparse + dense) and Reciprocal Rank Fusion: Critical for search performance. Quick check: What happens to search results if you set α=0 (pure BM25) vs α=1 (pure embeddings)?
- RAG (Retrieval-Augmented Generation) pipeline design: Key to conversational search and Ask This Paper features. Quick check: How does the system decide when to re-search vs use cached context for follow-up queries?

## Architecture Onboarding
- Component map: Frontend (Next.js web app with semantic search, FoS graph, conversational chat, Ask This Paper) -> Backend (Python services for preprocessing, semantic search, RAG pipeline) -> Databases (Neo4j for KG, Weaviate for embeddings, Semantic Scholar API for metadata) -> LLM (GPT-4 API for conversational search and Ask This Paper)
- Critical path: 1) User query → frontend → backend semantic search → top-k results → frontend display; 2) Conversational search → LLM generates query terms → retrieval → LLM generates grounded answer; 3) Ask This Paper → user question + full text → LLM generates answer with citations
- Design tradeoffs: Graph vs flat search (hierarchical FoS aids exploration but requires manual curation), hybrid retrieval (balances precision and recall but increases complexity), full-text RAG (improves accuracy but requires heavy compute)
- Failure signatures: Search returns irrelevant papers (check BM25/SPECTER2 tuning), conversational search hallucinates (inspect retrieved contexts), FoS graph navigation broken (verify graph construction)
- First 3 experiments: 1) Test semantic search with sample queries comparing BM25-only vs hybrid results; 2) Validate FoS hierarchy by searching for known concepts and measuring navigation steps; 3) Run RAG pipeline on sample questions checking answer faithfulness and citation accuracy

## Open Questions the Paper Calls Out
- How does the accuracy of the FoS hierarchy graph compare to other methods of organizing and navigating scientific literature, such as citation networks or keyword-based search?
- How does the performance of the RAG pipeline in conversational search compare to other methods of answering user questions about scientific literature, such as keyword-based search or expert-curated databases?
- How does the Ask This Paper feature perform in answering user questions about specific publications compared to other methods, such as manual reading or expert-curated summaries?

## Limitations
- Performance metrics are based on internal evaluations without independent replication
- Claim of "eliminating hallucinations" in RAG pipeline appears overstated given persistent challenges in LLM-based systems
- Heavy reliance on commercial LLM APIs (GPT-4) raises concerns about reproducibility and cost sustainability

## Confidence
- FoS Hierarchy Graph Correctness (F1 99.80): High
- RAG Pipeline Faithfulness (0.966) and Relevance (0.848): Medium
- Overall System Usability Claims: Low

## Next Checks
1. Implement the core hybrid search pipeline (BM25 + SPECTER2 with RRF) on a different scientific domain and measure whether similar performance gains are observed.
2. Conduct a controlled experiment comparing NLP-KG against traditional keyword search tools (Google Scholar, Semantic Scholar) with researchers exploring unfamiliar NLP topics, measuring discovery effectiveness and user satisfaction.
3. Systematically test the conversational search feature with edge-case queries known to trigger LLM hallucinations in other systems, documenting failure rates and identifying architectural safeguards that could prevent such failures.