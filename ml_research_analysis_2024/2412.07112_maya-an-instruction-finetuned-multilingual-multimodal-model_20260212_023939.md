---
ver: rpa2
title: 'Maya: An Instruction Finetuned Multilingual Multimodal Model'
arxiv_id: '2412.07112'
source_url: https://arxiv.org/abs/2412.07112
tags:
- maya
- arxiv
- languages
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Maya, an open-source multilingual multimodal
  model that addresses the gap in vision-language understanding for low-resource languages
  and diverse cultural contexts. The authors created a novel multilingual image-text
  pretraining dataset spanning eight languages (Chinese, French, Spanish, Russian,
  Hindi, Japanese, Arabic, and English) with 4.4 million samples, employing rigorous
  toxicity filtering that removed 7,531 toxic images.
---

# Maya: An Instruction Finetuned Multilingual Multimodal Model

## Quick Facts
- **arXiv ID**: 2412.07112
- **Source URL**: https://arxiv.org/abs/2412.07112
- **Reference count**: 40
- **Key outcome**: Maya achieves 60.4% average accuracy on PALO multilingual evaluation set, outperforming LLaVA-7B (46.9%) across eight languages

## Executive Summary
Maya is an open-source multilingual multimodal model designed to address vision-language understanding gaps in low-resource languages and diverse cultural contexts. The model combines a multilingual Aya-23 8B language model with a SigLIP vision encoder, pretrained on a novel dataset spanning eight languages (Chinese, French, Spanish, Russian, Hindi, Japanese, Arabic, and English) with 4.4 million samples. After rigorous toxicity filtering that removed 7,531 toxic images, the model was fine-tuned on the PALO 150K instruction-tuning dataset. Evaluation shows Maya outperforms LLaVA-7B and PALO-7B across multiple languages, achieving an average score of 60.4% compared to 46.9% for LLaVA-7B. The toxicity-free variant demonstrates minimal performance differences, with slight improvements in specific benchmarks like TextVQA and Text Translation.

## Method Summary
Maya employs a two-stage training approach: first pretraining on a curated multilingual dataset, then fine-tuning on instruction data. The architecture combines Aya-23 8B LLM with SigLIP vision encoder, connected through a 2-layer MLP projection matrix. The pretraining dataset was created by translating 558K English samples from LLaVA dataset to seven additional languages using Aya-35B with quality verification, then filtering toxic content using LLaVAguard and Toxic-BERT. The model was trained on 8xH100 GPUs with 80GB VRAM for 3 epochs with a batch size of 4 per GPU. Fine-tuning used the PALO 150K dataset with the same architecture but unfrozen decoder layers for some experiments.

## Key Results
- Achieves 60.4% average accuracy on PALO multilingual evaluation set across eight languages
- Outperforms LLaVA-7B (46.9%) and PALO-7B on multilingual benchmarks
- Toxicity-free variant shows minimal performance degradation, with slight improvements in TextVQA and Text Translation
- Demonstrates strong visual description capabilities in multiple languages, though with varying cultural context accuracy

## Why This Works (Mechanism)

### Mechanism 1
The model's multilingual performance benefits from a carefully curated pretraining dataset that balances representation across eight languages. The authors employ stratified sampling and length analysis to ensure linguistic diversity, then use a hybrid translation approach with quality verification to create parallel datasets. This systematic approach addresses the distribution biases found in existing multilingual datasets.

### Mechanism 2
Toxicity filtering improves safety without significantly degrading performance. The authors use a two-stage filtering approach with LLaVAguard for images and Toxic-BERT for captions, followed by human verification. This creates a safer training corpus while maintaining performance on standard benchmarks.

### Mechanism 3
The architecture choice of Aya-23 8B with SigLIP vision encoder provides multilingual adaptability. Aya-23 offers multilingual capabilities across 23 languages, while SigLIP supports variable-length patch sizes and scalable positional embeddings, making it more adaptable than CLIP for multilingual applications.

## Foundational Learning

- **Cross-lingual transfer learning**: Needed to leverage knowledge from high-resource languages to improve performance in low-resource languages. Quick check: How does the model handle concepts that exist in one language but not another during training?
- **Multimodal alignment**: Required to align visual features with text representations across multiple languages. Quick check: What projection layer configuration provides optimal alignment between visual and language features?
- **Cultural context understanding**: Essential for recognizing and appropriately responding to culturally specific visual elements. Quick check: How does the model differentiate between culturally specific and generic visual concepts?

## Architecture Onboarding

- **Component map**: Aya-23 8B LLM → Projection layer → SigLIP vision encoder → Output
- **Critical path**: Input text/image → Vision encoder → Projection → Language model → Response
- **Design tradeoffs**: Multilingual capability vs. model size (8B vs larger models), toxicity filtering vs. training data diversity
- **Failure signatures**: Poor cross-lingual performance suggests translation quality issues, safety concerns indicate filtering gaps, cultural misunderstandings suggest dataset limitations
- **First 3 experiments**:
  1. Test baseline multilingual performance on PALO evaluation set
  2. Compare performance with and without toxicity filtering on standard benchmarks
  3. Evaluate cultural context understanding on CVQA or similar culturally diverse dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does the removal of toxic content from the pretraining dataset affect Maya's performance on culturally diverse and nuanced visual reasoning tasks? While the paper shows that toxicity removal has minimal impact on overall performance, it does not provide a detailed analysis of how this specifically affects the model's ability to understand and reason about culturally diverse content.

### Open Question 2
What are the long-term effects of using a toxicity-free dataset on the model's ability to generalize to real-world scenarios that may contain harmful content? The paper focuses on immediate performance metrics but does not explore how the absence of toxic content in training data might impact the model's ability to handle real-world scenarios that may contain such content.

### Open Question 3
How does the choice of preamble type in the translation framework affect the quality of translations for low-resource languages in the pretraining dataset? While the paper identifies the best preamble type for the seven languages tested, it does not explore how this choice affects translations for low-resource languages that were not included in the study.

### Open Question 4
What is the impact of unfreezing decoder layers during fine-tuning on Maya's performance across different languages and tasks? The paper mentions plans to unfreeze decoder layers for future work but does not provide results on its impact.

### Open Question 5
How does the integration of advanced alignment techniques, such as gated soft-attention or Q-Former, compare to the current projection matrix approach in terms of cross-modal alignment and performance? The paper mentions that advanced alignment techniques are set aside for future work, indicating a potential area for improvement.

## Limitations
- Significant performance gaps across language pairs (17% difference between Hindi and Chinese on visual tasks)
- Cultural context understanding remains limited, with only 41.8% accuracy on culturally-specific visual concepts
- Safety filtering may not adequately address nuanced toxicity in multilingual contexts
- Evaluation methodology shows potential biases toward English-centric tasks

## Confidence
- **High Confidence (8/10)**: The architectural framework combining Aya-23 with SigLIP provides a sound foundation for multilingual multimodal learning. The pretraining methodology and toxicity filtering pipeline are well-documented and reproducible.
- **Medium Confidence (6/10)**: The performance improvements over baseline models (60.4% vs 46.9% on PALO evaluation) are statistically significant but may not generalize to real-world deployment scenarios.
- **Low Confidence (4/10)**: Cross-lingual transfer learning effectiveness and cultural context understanding remain poorly validated. The evaluation lacks rigorous testing across diverse cultural contexts and does not adequately address safety concerns in multilingual deployment.

## Next Checks
1. **Cross-lingual Transfer Validation**: Conduct controlled experiments testing the model's ability to transfer knowledge from high-resource languages (English, Chinese) to low-resource languages (Hindi, Arabic) using parallel task structures. Measure performance degradation when training data is limited for specific language pairs.

2. **Cultural Context Robustness**: Evaluate the model on culturally diverse image datasets with expert-annotated cultural elements. Test the model's ability to recognize and appropriately respond to culturally specific visual concepts across all eight target languages, measuring both accuracy and cultural sensitivity metrics.

3. **Safety in Multilingual Contexts**: Implement comprehensive safety testing using multilingual toxic prompts and controversial cultural topics. Evaluate the model's response generation across languages, measuring both false positive rates in toxicity detection and unsafe response generation in edge cases.