---
ver: rpa2
title: 'CHARP: Conversation History AwaReness Probing for Knowledge-grounded Dialogue
  Systems'
arxiv_id: '2405.15110'
source_url: https://arxiv.org/abs/2405.15110
tags:
- faithdial
- knowledge
- history
- evaluation
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies an artifact in FaithDial, a knowledge-grounded
  dialogue dataset: models trained on it ignore conversation history, focusing solely
  on provided knowledge. To measure this, the authors introduce CHARP, a diagnostic
  test set with two versions: eCHARP (easy, self-contained) and hCHARP (hard, history-dependent).'
---

# CHARP: Conversation History AwaReness Probing for Knowledge-grounded Dialogue Systems

## Quick Facts
- arXiv ID: 2405.15110
- Source URL: https://arxiv.org/abs/2405.15110
- Reference count: 40
- Key outcome: Models trained on FaithDial ignore conversation history, focusing only on provided knowledge; CHARP test set reveals this shortcoming missed by automatic metrics.

## Executive Summary
This paper identifies a critical artifact in FaithDial, a knowledge-grounded dialogue dataset: models trained on it systematically ignore conversation history and focus solely on provided knowledge. To measure this, the authors introduce CHARP, a diagnostic test set with easy (eCHARP) and hard (hCHARP) versions. They demonstrate that FaithDial-trained models perform poorly on hCHARP due to their inability to reason over history—a shortcoming undetected by FaithDial's automatic metrics. Human evaluation reveals this issue clearly and shows that FaithDial's CRITIC model mislabels correct responses as hallucinatory when they deviate from knowledge phrasing. The study also finds that few-shot prompting with larger LLMs (Mixtral, ChatGPT) outperforms FaithDial-trained models on CHARP, highlighting the importance of conversation history awareness and the potential of LLM-based evaluation.

## Method Summary
The authors create CHARP, a diagnostic test set designed to evaluate models' ability to attend to conversation history in knowledge-grounded dialogue. CHARP contains 1,080 examples split into eCHARP (self-contained) and hCHARP (history-dependent). They fine-tune T5, FLAN-T5, and GODEL models on FaithDial, then evaluate these models and few-shot LLMs (GPT-4, ChatGPT, Mixtral) on CHARP. Automatic metrics (BLEU, BERTScore, CRITIC) and human evaluation (using a C1-W6 checklist) are used to assess performance. The authors also analyze the impact of training with truncated history to understand the artifact's origin.

## Key Results
- Models trained on FaithDial ignore conversation history, focusing only on provided knowledge
- FaithDial's automatic metrics fail to detect this history-ignoring behavior
- Few-shot prompting with larger LLMs (Mixtral, ChatGPT) outperforms FaithDial-trained models on CHARP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FaithDial training creates an artifact where models ignore conversation history and focus solely on provided knowledge.
- Mechanism: The dataset's editing process replaces hallucinatory responses with faithful ones, but does not preserve the conversational context. Models learn to map the last user turn directly to the provided knowledge without reasoning over earlier turns.
- Core assumption: The training objective (minimize hallucination) does not penalize ignoring history, so models optimize for faithfulness at the expense of context awareness.
- Evidence anchors:
  - [abstract] "models trained on it ignore conversation history, focusing solely on provided knowledge"
  - [section 3.5] "We notice that the performances on the original test set (eval: h=all) of models trained on truncated history (train: h ∈ {3, 2, 1}) barely drop across metrics"

### Mechanism 2
- Claim: Automatic evaluation metrics fail to detect the history-ignoring artifact because they only measure similarity to knowledge and ground truth, not conversational coherence.
- Mechanism: Metrics like BLEU, BERTScore, and CRITIC compare predicted responses to knowledge and ground truth without considering whether the response is contextually appropriate based on the conversation history.
- Core assumption: Evaluation metrics are designed to measure faithfulness and similarity, not contextual reasoning.
- Evidence anchors:
  - [abstract] "the evaluation methods of FaithDial fail to capture these shortcomings, neglecting the conversational history"
  - [section 5.1] "we observe that the models perform well on eCHARP with better BLEU scores in (y, y′), and almost similar on both (k, y′) metrics compared to the results on the validation set"

### Mechanism 3
- Claim: Larger LLMs (Mixtral, ChatGPT) outperform FaithDial-trained models on CHARP because they retain general reasoning capabilities that are not overwritten by FaithDial's narrow training objective.
- Mechanism: Few-shot prompting with large LLMs allows them to leverage their pre-existing knowledge and reasoning skills to understand context and select appropriate knowledge, while FaithDial-trained models have lost this ability.
- Core assumption: Pre-training on diverse data provides general reasoning capabilities that can be accessed through few-shot prompting.
- Evidence anchors:
  - [abstract] "we find that few-shot prompting with larger LLMs (Mixtral, ChatGPT) outperforms FaithDial-trained models on CHARP"
  - [section 6.1] "the larger models, Mixtral and ChatGPT, significantly outperform all reported fine-tuned models by at least 30% on C1"

## Foundational Learning

- Concept: Hallucination detection in dialogue systems
  - Why needed here: Understanding how FaithDial's CRITIC works and its limitations is crucial for interpreting CHARP results.
  - Quick check question: What is the difference between a hallucinatory response and a response that ignores conversation history?

- Concept: Evaluation metrics for dialogue systems
  - Why needed here: Knowing the strengths and weaknesses of BLEU, BERTScore, and CRITIC is essential for understanding why they fail to detect the history-ignoring artifact.
  - Quick check question: How would you modify CRITIC to also check for context-awareness?

- Concept: Dataset artifacts and their impact on model behavior
  - Why needed here: Recognizing that dataset creation processes can introduce biases is fundamental to understanding CHARP's findings.
  - Quick check question: Can you think of another example where a dataset artifact might lead to unexpected model behavior?

## Architecture Onboarding

- Component map: Conversation history + last user turn + knowledge -> Model (FaithDial-trained or few-shot LLM) -> Response -> Evaluation (automatic metrics, human evaluation, CHARP)
- Critical path: Model training -> Evaluation on FaithDial -> Evaluation on CHARP -> Analysis of results
- Design tradeoffs: Faithfulness to knowledge vs. context awareness; automatic evaluation vs. human evaluation; fine-tuning vs. few-shot prompting
- Failure signatures: High CRITIC scores but poor CHARP performance; high similarity to knowledge but low contextual appropriateness
- First 3 experiments:
  1. Train a model on FaithDial and evaluate it on both FaithDial and CHARP to confirm the history-ignoring artifact.
  2. Modify CHARP to include examples that explicitly require history reasoning and re-evaluate models.
  3. Fine-tune a model on a modified FaithDial dataset that includes history-aware examples and evaluate its performance on CHARP.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the FaithDial dataset inherently contain annotation artifacts that bias models to ignore conversation history?
- Basis in paper: [explicit] The paper explicitly identifies annotation artifacts in FaithDial that bias models to ignore conversation history.
- Why unresolved: While the paper demonstrates this bias exists, it doesn't explore the root causes of these artifacts in the annotation process.
- What evidence would resolve it: A detailed analysis of the FaithDial annotation process to identify specific steps or instructions that may have introduced these artifacts.

### Open Question 2
- Question: How do the FaithDial evaluation metrics fail to capture models' inability to reason over conversation history?
- Basis in paper: [explicit] The paper shows that FaithDial's automatic metrics and CRITIC detector fail to detect the models' shortcomings in attending to conversation history.
- Why unresolved: The paper identifies the failure but doesn't fully explain the technical reasons why these metrics are inadequate for this specific task.
- What evidence would resolve it: A comprehensive breakdown of FaithDial's evaluation metrics and their limitations in detecting history-awareness in models.

### Open Question 3
- Question: Can larger LLMs consistently outperform FaithDial-trained models on history-dependent dialogue tasks?
- Basis in paper: [explicit] The paper shows that few-shot prompting with larger LLMs (Mixtral, ChatGPT) outperforms FaithDial-trained models on CHARP.
- Why unresolved: The study uses only a few LLMs; it's unclear if this trend holds across different model sizes and architectures.
- What evidence would resolve it: Systematic evaluation of a wider range of LLMs (both proprietary and open-source) across various history-dependent dialogue tasks.

## Limitations

- CHARP dataset is relatively small (1,080 examples) and may not capture full complexity of real-world conversations
- Study focuses primarily on FaithDial, limiting generalizability to other knowledge-grounded dialogue datasets
- Human evaluation involved only two annotators per example, potentially affecting reliability

## Confidence

- High Confidence:
  - Models trained on FaithDial exhibit a systematic bias toward ignoring conversation history in favor of provided knowledge
  - Automatic evaluation metrics (BLEU, BERTScore, CRITIC) fail to detect this history-ignoring behavior
  - Larger LLMs outperform FaithDial-trained models on CHARP, indicating retained reasoning capabilities

- Medium Confidence:
  - The specific dataset artifacts in FaithDial that cause this behavior are well-identified but not fully characterized
  - The relative importance of history awareness versus knowledge faithfulness for overall dialogue quality requires further investigation

## Next Checks

1. **Cross-dataset validation**: Test FaithDial-trained models on other knowledge-grounded dialogue datasets (e.g., Wizard of Wikipedia) to determine if the history-ignoring artifact generalizes beyond FaithDial.

2. **Adversarial CHARP expansion**: Create an expanded CHARP version with more challenging examples that explicitly require multi-turn reasoning, then re-evaluate both FaithDial-trained models and few-shot LLMs to better understand the limits of each approach.

3. **Fine-tuning with history-awareness**: Develop a modified training procedure that explicitly rewards history-aware responses (e.g., through contrastive learning or additional history-based loss terms) and evaluate whether this mitigates the history-ignoring artifact while maintaining knowledge faithfulness.