---
ver: rpa2
title: 'PickScan: Object discovery and reconstruction from handheld interactions'
arxiv_id: '2411.11196'
source_url: https://arxiv.org/abs/2411.11196
tags:
- object
- objects
- mask
- scene
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PickScan addresses compositional scene reconstruction, enabling
  object discovery and 3D modeling from handheld RGB-D video. It introduces a novel
  interaction-guided pipeline that detects user-object interactions and extracts masks
  for each manipulated object, enabling class-agnostic reconstruction without pre-trained
  segmentation.
---

# PickScan: Object discovery and reconstruction from handheld interactions

## Quick Facts
- arXiv ID: 2411.11196
- Source URL: https://arxiv.org/abs/2411.11196
- Reference count: 34
- Primary result: 78.3% precision at 100% recall for object discovery; 0.90 cm mean chamfer distance for reconstruction

## Executive Summary
PickScan introduces an interaction-guided approach for compositional scene reconstruction that discovers and reconstructs individual objects from handheld RGB-D video. Unlike traditional methods that rely on pre-trained segmentation networks, PickScan leverages user-object interactions detected through IMU data and motion heuristics to identify objects without requiring class-specific training. The system first builds an initial static scene reconstruction, then uses displacement-based moving mask heuristics to detect manipulated objects during interaction. Finally, it tracks object masks and reconstructs 3D models using BundleSDF, achieving significant improvements over segmentation-based baselines while remaining class-agnostic.

## Method Summary
PickScan operates through a pipeline that begins with static scene scanning to create an initial point cloud using gradSLAM. During interaction, the system detects moving objects by comparing reprojected points to the initial scene, removes arm masks, and identifies candidate object blobs closest to the hand. Interaction detection uses distance-based heuristics comparing hand-to-initial and hand-to-object distances. The system selects best frames based on IoU stability, tracks masks with XMem, removes duplicates through non-maximum suppression, and reconstructs 3D models using BundleSDF. This approach enables class-agnostic object discovery without requiring pre-trained segmentation networks, making it particularly effective for novel or challenging object configurations.

## Key Results
- Achieves 78.3% precision at 100% recall for object discovery on custom dataset
- Outperforms Co-Fusion by 73% in reconstruction accuracy (mean chamfer distance of 0.90 cm)
- Reduces false positives by 99% compared to segmentation-based baselines
- Successfully handles challenging object configurations including similar textures and cluttered scenes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interaction-based object discovery can outperform static segmentation in complex scenes.
- Mechanism: The system uses IMU data and RGB-D motion heuristics to detect user-object interactions, identifying objects based on their displacement from an initial static scene rather than appearance.
- Core assumption: Objects manipulated by the user will have significant displacement from their initial positions, and this displacement is a stronger signal for object discovery than appearance-based segmentation in challenging configurations.
- Evidence anchors:
  - [abstract] "introduces a novel interaction-guided pipeline that detects user-object interactions and extracts masks for each manipulated object, enabling class-agnostic reconstruction without pre-trained segmentation."
  - [section] "We detect object interactions as periods starting with the hand-initial distance crossing above the hand-object distance, and ending with the former crossing below the latter once again."
  - [corpus] No direct corpus evidence; this is a novel contribution.
- Break condition: If objects are not significantly displaced from their initial positions, or if multiple objects are moved together in a way that their combined displacement masks individual object boundaries.

### Mechanism 2
- Claim: The displacement-based moving mask heuristic accurately identifies moving objects.
- Mechanism: Points are classified as moving if they are further than a threshold distance from the initial static scene point cloud, effectively separating static background from manipulated objects.
- Core assumption: The initial static scan captures all background geometry accurately, and manipulated objects will be displaced beyond the threshold radius during interaction.
- Evidence anchors:
  - [section] "We detect a pixel (u, v) as moving if its reprojection is further away than a tunable threshold radius ρmoving from any point in the initial point cloud."
  - [section] "we estimate the mask of all moving scene parts (moving mask) in each frame before removing an estimated arm mask."
  - [corpus] No direct corpus evidence; this is a novel contribution.
- Break condition: If the initial scene reconstruction is inaccurate, if the threshold radius is poorly tuned, or if objects move only slightly during manipulation.

### Mechanism 3
- Claim: Non-maximum suppression effectively removes duplicate object detections.
- Mechanism: The system compares the overlap between detected object masks and removes objects that are significantly contained within other detected objects.
- Core assumption: When multiple interactions are detected for the same object, the resulting masks will have high overlap, allowing for deduplication.
- Evidence anchors:
  - [section] "For each ordered pair of detected objects (A, B), we calculate the mean percentage of object A's mask that is contained in object B's mask; we denote this as containsA(B)."
  - [section] "As long as a tuple of objects (A, B) exists for which containsA(B) > τ nms, we remove the object which is contained to the largest degree in some other object's mask."
  - [corpus] No direct corpus evidence; this is a novel contribution.
- Break condition: If objects have similar shapes and positions, leading to high overlap between distinct objects, or if the overlap threshold τ nms is poorly tuned.

## Foundational Learning

- Concept: Structure from Motion (SfM) and Simultaneous Localization and Mapping (SLAM)
  - Why needed here: The system uses SLAM techniques to reconstruct the initial static scene and track camera poses throughout the interaction.
  - Quick check question: How does the system use camera poses to reproject points from the current frame into the initial scene coordinate frame?

- Concept: Point cloud processing and distance metrics
  - Why needed here: The system relies on point cloud operations to detect moving objects and calculate distances between hand, object, and initial scene points.
  - Quick check question: What distance metric is used to determine if a point is moving, and how is this threshold chosen?

- Concept: Mask tracking and propagation
  - Why needed here: After detecting object masks in representative frames, the system uses a mask tracker (XMem) to propagate these masks to all frames in the interaction.
  - Quick check question: How does the mask tracker handle occlusions or significant changes in object appearance during manipulation?

## Architecture Onboarding

- Component map:
  - Input: RGB-D video stream + IMU data
  - Initial scene reconstruction: Static scanning + gradSLAM
  - Moving mask estimation: Displacement-based heuristic
  - Candidate object mask: Blob detection + hand mask removal
  - Interaction detection: Distance-based heuristics
  - Best frame selection: IoU stability measurement
  - Mask tracking: XMem tracker
  - Duplicate removal: Non-maximum suppression
  - 3D reconstruction: BundleSDF
  - Output: Object masks, 3D models, per-frame poses

- Critical path: Static scan → Initial point cloud reconstruction → Moving mask estimation → Candidate object mask → Interaction detection → Best frame selection → Mask tracking → Non-maximum suppression → 3D reconstruction

- Design tradeoffs:
  - Accuracy vs. computational cost: Processing every 5th frame for 3D reconstruction reduces computational load but may miss fine details
  - Threshold tuning: Multiple tunable thresholds (ρmoving, τiou, τnms) require careful calibration for different scenes and object sizes
  - Interaction-based vs. appearance-based: Requires user to manipulate objects, limiting applicability to scenarios where physical interaction is possible

- Failure signatures:
  - Inaccurate initial scene reconstruction: Leads to false moving mask detections
  - Poorly tuned thresholds: Results in missed interactions or false positives
  - Mask tracking failures: Objects become untracked or jump to wrong objects
  - BundleSDF pose tracking issues: Gaps and artifacts in final 3D reconstructions

- First 3 experiments:
  1. Validate moving mask heuristic: Compare detected moving masks against ground truth for a simple scene with one moving object
  2. Test interaction detection: Use a controlled sequence of picking up and putting down an object, verify that the interaction is correctly detected
  3. Evaluate duplicate removal: Create a scenario with multiple interactions of the same object, verify that duplicates are correctly removed while preserving the best detection

## Open Questions the Paper Calls Out

- Question: How does the proposed interaction-guided object discovery method compare to purely learning-based approaches on datasets with larger numbers of object classes?
- Basis in paper: [inferred] The paper only compares to a single segmentation network (SAM) on 5 object classes and does not explore how performance scales with more diverse object categories.
- Why unresolved: The custom dataset used in the evaluation is limited to 5 objects, making it unclear how well the method generalizes to more varied object classes that might appear in real-world scenarios.
- What evidence would resolve it: Extensive evaluation on a large-scale dataset with hundreds of object categories, comparing the interaction-based method against multiple state-of-the-art segmentation networks.

- Question: What is the theoretical limit of object discovery accuracy when using displacement-based heuristics in cluttered environments with many similar objects?
- Basis in paper: [explicit] The paper acknowledges limitations with toy brick examples where objects have similar textures but different shapes, and mentions challenges with cluttered scenes.
- Why unresolved: The paper demonstrates limitations with specific examples but does not provide theoretical analysis or bounds on discovery accuracy in highly cluttered environments.
- What evidence would resolve it: Mathematical analysis of discovery accuracy as a function of scene complexity, clutter density, and object similarity, combined with extensive experimental validation across varying levels of scene complexity.

- Question: How does the computational efficiency of the pipeline scale with scene size and number of objects?
- Basis in paper: [explicit] The paper mentions processing time for a single object (60 minutes for 700 frames) but does not analyze scaling behavior for larger scenes with more objects.
- Why unresolved: While processing times for individual objects are provided, there is no analysis of how computation scales with the number of objects or overall scene complexity.
- What evidence would resolve it: Systematic benchmarking of processing times across scenes of varying complexity, with clear analysis of computational complexity as a function of scene parameters.

- Question: Can the interaction detection mechanism be made more robust to noise and varying hand-object interaction patterns?
- Basis in paper: [explicit] The paper mentions noise as a limitation in distance measurements and acknowledges that the current approach has limitations with noisy data.
- Why unresolved: While noise is acknowledged as a problem, the paper does not explore alternative interaction detection methods or present solutions for handling different types of hand-object interactions.
- What evidence would resolve it: Comparative evaluation of multiple interaction detection algorithms, including their performance under various noise conditions and with different interaction patterns.

## Limitations
- Requires physical manipulation of objects, limiting applicability to scenarios where user interaction is possible
- Performance heavily dependent on accuracy of initial static scene reconstruction and IMU data quality
- Multiple thresholds require careful calibration for different object sizes and scene configurations

## Confidence
- Object discovery mechanism: **High** - The displacement-based approach is well-justified and validated against segmentation baselines
- 3D reconstruction quality: **Medium** - While quantitative metrics are provided, the evaluation is limited to only 3 scenes
- Mask tracking robustness: **Low** - The method relies on a black-box XMem tracker without detailed analysis of failure cases

## Next Checks
1. Threshold sensitivity analysis: Systematically vary ρmoving, τiou, and τnms across different object sizes to determine robustness and provide tuning guidelines
2. Cross-dataset generalization: Test on publicly available datasets (e.g., Redwood, ScanNet) to evaluate performance beyond the custom dataset
3. Ablation study on initial scene reconstruction: Compare performance when using different SLAM methods or varying the quality of the static scan to isolate this dependency