---
ver: rpa2
title: Optimal Transport for Domain Adaptation through Gaussian Mixture Models
arxiv_id: '2403.13847'
source_url: https://arxiv.org/abs/2403.13847
tags:
- domain
- adaptation
- target
- source
- transport
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes two new strategies for domain adaptation using
  optimal transport between Gaussian mixture models (GMMs). The first strategy propagates
  labels from source to target GMM components using the optimal transport plan as
  a joint probability distribution.
---

# Optimal Transport for Domain Adaptation through Gaussian Mixture Models

## Quick Facts
- arXiv ID: 2403.13847
- Source URL: https://arxiv.org/abs/2403.13847
- Authors: Eduardo Fernandes Montesuma; Fred Maurice Ngolè Mboula; Antoine Souloumiac
- Reference count: 40
- Key outcome: GMM-OTDA methods achieve higher classification accuracy compared to other optimal transport-based methods across 9 benchmarks with 85 adaptation tasks.

## Executive Summary
This paper proposes two novel strategies for domain adaptation using optimal transport between Gaussian mixture models (GMMs). The first strategy propagates labels from source to target GMM components using the optimal transport plan as a joint probability distribution. The second strategy estimates a mapping between GMMs by first identifying the most likely source component for each sample and then transporting it to target components with weights based on the optimal transport plan. The authors demonstrate that their approaches outperform previous shallow domain adaptation methods while scaling well with the number of samples and dimensions.

## Method Summary
The GMM-OTDA framework reduces computational complexity by replacing discrete samples with Gaussian mixture components, computing optimal transport plans between K source and K target components instead of all pairs of n source and m target samples. The method involves fitting GMMs to both source (labeled) and target (unlabeled) data using EM algorithms, computing the GMMOT plan, and then applying one of two strategies: label propagation via MAP estimation or sample mapping with Tweight using the computed GMMOT plan. The framework is evaluated on 9 benchmarks with 85 adaptation tasks across visual and fault diagnosis domains.

## Key Results
- GMM-OTDA methods outperform previous shallow domain adaptation methods across 9 benchmarks with 85 adaptation tasks
- The methods achieve higher classification accuracy compared to OTDA, InfoOT, and HOT-DA methods
- Computational complexity is reduced from O(n³ log n) to O(K³ log K) by using GMM components instead of discrete samples

## Why This Works (Mechanism)

### Mechanism 1
The GMMOT framework reduces computational complexity from O(n³ log n) to O(K³ log K) by replacing discrete samples with Gaussian mixture components. Instead of computing an optimal transport plan between all pairs of n source and m target samples, the method computes a plan between K source and K target mixture components. Each component represents a cluster of data points, so the number of transport variables drops from n×m to K×K. The core assumption is that the data distribution can be accurately approximated by a small number of Gaussian components (K << n).

### Mechanism 2
Label propagation through the optimal transport plan transfers source domain class information to target domain components. The GMMOT plan ω defines a joint distribution over source and target components. By treating ω as Pr(KS=k1, KT=k2), we can compute Pr(Y|KT=k2) via the law of total probability, effectively assigning pseudo-labels to target components. The core assumption is the conditional independence assumption Pr(Y|KS=k1, KT=k2) = Pr(Y|KS=k1), which plays the same role as covariate shift hypothesis in conventional domain adaptation works.

### Mechanism 3
The Tweight mapping strategy improves on previous GMMOT mappings by first estimating the most likely source component for each sample. For each source sample x, estimate the component k1 most likely to have generated it. Then transport x only to target components k2 where ωk1,k2 ≥ τ, weighting each transport by ωk1,k2. This creates a piecewise-affine, group-sparse mapping. The core assumption is that each sample comes predominantly from a single GMM component, and the optimal transport plan has sparse structure.

## Foundational Learning

- Concept: Gaussian Mixture Models and Expectation-Maximization
  - Why needed here: The method requires fitting GMMs to both source and target data to represent their distributions compactly. Understanding EM is crucial for implementing the fitting procedure.
  - Quick check question: What are the two steps in the EM algorithm for GMM fitting, and what does each step optimize?

- Concept: Optimal Transport and Wasserstein Distance
  - Why needed here: The core methodology relies on computing optimal transport between GMMs. Understanding OT basics (Kantorovich formulation, Wasserstein distance) is essential for grasping how the transport plan is computed and interpreted.
  - Quick check question: How does the Wasserstein distance between two Gaussians with diagonal covariances simplify compared to the general case?

- Concept: Domain Adaptation and Covariate Shift
  - Why needed here: The method operates in the unsupervised domain adaptation setting, where source data is labeled but target data is not. Understanding the covariate shift assumption explains when label propagation is valid.
  - Quick check question: What is the covariate shift assumption, and how does it relate to the conditional independence assumption used in the label propagation mechanism?

## Architecture Onboarding

- Component map: Data preprocessing -> Feature extraction -> GMM fitting -> Optimal transport computation -> Label propagation/Mapping estimation -> Classifier training -> Evaluation
- Critical path: 1. Fit GMM to source data with class labels (ConditionalEM) 2. Fit GMM to target data (EM) 3. Compute GMMOT plan ω 4. Apply chosen strategy (label propagation or mapping estimation) 5. Train and evaluate classifier
- Design tradeoffs:
  - Number of components K vs. approximation accuracy: More components better capture distribution but increase computational cost
  - Entropic regularization ε vs. transport plan sparsity: Higher ε creates smoother plans but may blur component boundaries
  - Diagonal vs. full covariance matrices: Diagonal is computationally cheaper and more robust in high dimensions but less expressive
- Failure signatures:
  - Poor performance with few components: Check if K is too small relative to data complexity
  - Degraded performance with high entropic regularization: Try reducing ε or using exact OT
  - Class confusion in mapped samples: Verify conditional independence assumption holds, consider more components
- First 3 experiments:
  1. Run GMM-OTDA MAP on a simple binary classification task (e.g., synthetic data with two well-separated clusters) to verify label propagation works
  2. Compare GMM-OTDA T with and without component filtering (τ parameter) on a task with clear component structure
  3. Test scalability by running on MNIST→USPS with increasing numbers of components K, measuring runtime and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the GMM-OTDA approach to over-estimation of the number of components in the Gaussian mixture models?
- Basis in paper: [explicit] The paper states "we show in our appendix that our methods are robust to the over estimation of GMM components."
- Why unresolved: The paper references an appendix section for this evidence, but the provided text does not include the actual appendix or results demonstrating this robustness.
- What evidence would resolve it: Experimental results showing performance of GMM-OTDA with varying numbers of components (both under and over-estimated) compared to the true number needed, across multiple datasets.

### Open Question 2
- Question: How does the GMM-OTDA approach perform when the source domain has noisy labels?
- Basis in paper: [inferred] The paper discusses that "if source domain data contains noisy labels, these can be transferred to the target domain, leading to poor results" under the learning theory section.
- Why unresolved: The paper acknowledges this limitation but does not provide experimental validation of how the method performs under label noise conditions.
- What evidence would resolve it: Experiments comparing GMM-OTDA performance on clean vs. noisy source labels across different noise levels and datasets.

### Open Question 3
- Question: How does the choice of entropic regularization parameter ε affect the mapping estimation strategy (Tweight) compared to the label propagation strategy?
- Basis in paper: [explicit] The ablation study shows "For the entropic penalty ε, we have two drastically different scenarios. For the MAP estimation, using higher entropic regularization coefficients improve performance, whereas the mapping strategy works better for smaller regularization coefficients (or exact OT)."
- Why unresolved: While the paper identifies this difference, it does not explain the underlying reason for why the two strategies respond differently to entropic regularization.
- What evidence would resolve it: Theoretical analysis or controlled experiments isolating the effect of entropic regularization on the matching quality and smoothness of the transport plan for each strategy.

## Limitations

- The method's performance relies heavily on the assumption that GMMs can accurately approximate the source and target data distributions, which may not hold for complex real-world data with heavy tails or non-Gaussian substructures.
- The effectiveness of label propagation depends on the validity of the conditional independence assumption, which may be violated in cases where components encode additional information beyond class labels.
- The Tweight mapping strategy's sensitivity to the threshold parameter τ and component estimation quality is not thoroughly explored, potentially limiting its robustness in practice.

## Confidence

- High confidence: Computational complexity reduction from O(n³ log n) to O(K³ log K) - this follows directly from the mathematical formulation and is supported by complexity analysis in section 3.1
- Medium confidence: Superior performance compared to baseline methods - while the paper reports higher accuracy across multiple benchmarks, the exact hyperparameter settings and implementation details for the baseline methods are not fully specified
- Medium confidence: The Tweight mapping strategy's effectiveness - the paper provides theoretical justification and ablation studies, but the method's sensitivity to the threshold parameter τ and component estimation quality is not thoroughly explored

## Next Checks

1. **Component sensitivity analysis**: Systematically vary the number of GMM components K (e.g., K ∈ {5, 10, 20, 50}) on a representative task and measure both computational time and classification accuracy to quantify the trade-off between approximation quality and efficiency.

2. **Conditional independence verification**: Design experiments to test the validity of the Pr(Y|KS=k1, KT=k2) = Pr(Y|KS=k1) assumption. One approach is to compute the mutual information between labels Y and target components KT conditioned on source components KS, and check if it's significantly lower than the unconditional mutual information.

3. **Cross-dataset robustness**: Apply GMM-OTDA to datasets with known distribution characteristics that challenge GMM assumptions (e.g., heavy-tailed distributions, multimodal clusters with non-Gaussian shapes). Compare performance with a baseline method that doesn't rely on GMM approximations to quantify the method's robustness to distribution misspecification.