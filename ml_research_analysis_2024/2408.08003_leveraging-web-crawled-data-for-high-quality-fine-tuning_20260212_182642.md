---
ver: rpa2
title: Leveraging Web-Crawled Data for High-Quality Fine-Tuning
arxiv_id: '2408.08003'
source_url: https://arxiv.org/abs/2408.08003
tags:
- data
- errors
- web-crawled
- high-quality
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a method to leverage noisy web-crawled data
  for supervised fine-tuning by automatically aligning it with high-quality seed data.
  A model is fine-tuned to convert web-crawled data into high-quality format, enabling
  effective training without reliance on advanced LLMs like GPT-4.
---

# Leveraging Web-Crawled Data for High-Quality Fine-Tuning

## Quick Facts
- arXiv ID: 2408.08003
- Source URL: https://arxiv.org/abs/2408.08003
- Authors: Jing Zhou; Chenglin Jiang; Wei Shen; Xiao Zhou; Xiaonan He
- Reference count: 37
- Key outcome: Model-based data cleaning improves performance by 9.4% over high-quality data alone

## Executive Summary
This work addresses the challenge of using noisy web-crawled data for supervised fine-tuning of large language models. The authors propose a method that automatically aligns web-crawled data with high-quality seed data using fuzzy matching, then fine-tunes a model to convert the web data into high-quality format. This approach enables effective training without requiring advanced LLMs like GPT-4. Experiments on Chinese math problems demonstrate significant improvements over using high-quality data alone, with a 7B model outperforming several larger models and closed-source alternatives.

## Method Summary
The method involves three main steps: (1) Automatic alignment of web-crawled and high-quality data using fuzzy matching based on exact question or answer matching, (2) Fine-tuning an LLM on paired data to learn converting web-crawled problems into high-quality format, and (3) Using the fine-tuned model to convert the entire web-crawled dataset, then performing supervised fine-tuning on both the original high-quality data and converted data. The approach leverages the model's ability to handle global formatting errors better than rule-based methods while requiring only a limited amount of high-quality seed data for initial alignment.

## Key Results
- Average improvement of 9.4% over training with high-quality data alone
- 7B model outperforms several models larger than 32B and closed-source models like GPT-3.5
- Effective training without reliance on advanced LLMs like GPT-4
- Robust performance even with limited high-quality seed data (10,000 instances)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-based data cleaning outperforms rule-based methods by handling global formatting errors requiring context understanding
- Mechanism: Fine-tuning a language model on paired web-crawled and high-quality data teaches it to rewrite entire problem-answer pairs in standardized format while preserving semantic meaning
- Core assumption: LLMs are better at combining all information in a sample than rule-based methods considering only local contexts
- Evidence anchors: 9.4% performance improvement over high-quality data alone; LLMs good at combining all information in sample
- Break condition: Insufficient paired training data or failure to capture variety of error types

### Mechanism 2
- Claim: Fuzzy matching algorithm effectively pairs web-crawled and high-quality data using exact matches rather than similarity
- Mechanism: Matching process removes non-Chinese characters and short English phrases, considering examples identical if questions match exactly or answers from high-quality data are subsequences of web-crawled answers
- Core assumption: Exact matching is more precise than embedding-based similarity for this task
- Evidence anchors: Rule-based matching offers more precise control over specific details compared to embedding methods
- Break condition: Preprocessing removes too much information or matching criteria are too strict

### Mechanism 3
- Claim: Format converter training enables learning data synthesis capabilities without additional powerful LLMs
- Mechanism: Training on <low-quality, high-quality> pairs teaches the model to transform irregular web data into high-quality format, performing data synthesis internally
- Core assumption: Rewriting data is simpler for LLMs than performing complex reasoning tasks
- Evidence anchors: Model can convert web data with irregular formats into high-quality ones; easier for LLM to rewrite data than perform complex reasoning
- Break condition: Model overfits to training pairs or cannot generalize to new error types

## Foundational Learning

- Concept: Fuzzy matching for data alignment
  - Why needed here: To create training pairs between noisy web-crawled data and clean high-quality data
  - Quick check question: Why use exact matching rather than embedding similarity for this task?

- Concept: Supervised fine-tuning with paired data
  - Why needed here: To teach the model how to transform web-crawled data into high-quality format
  - Quick check question: What would happen if we tried to train the model with unpaired data?

- Concept: Data quality vs. data quantity tradeoff
  - Why needed here: Understanding when to use noisy data with cleaning vs. clean data only
  - Quick check question: How does performance change when increasing data volume with different cleaning methods?

## Architecture Onboarding

- Component map: Web-crawled data → Fuzzy matching → Paired training → Format converter model → Cleaned data → Final SFT → Math model
- Critical path: Fuzzy matching → Paired training → Format converter model → Cleaned data
- Design tradeoffs: Rule-based vs. model-based cleaning - model-based requires training but handles global errors better
- Failure signatures: Model overfitting to training pairs, matching algorithm missing useful pairs, format converter introducing new errors
- First 3 experiments:
  1. Test fuzzy matching accuracy on a small sample of web and high-quality data
  2. Train format converter on 10k pairs and evaluate on held-out web data
  3. Compare performance of model-based vs rule-based cleaning on a subset of data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model-based data cleaning method perform when applied to GPT-4 instead of smaller models?
- Basis in paper: The paper mentions applying the methodology to GPT-4 could likely enhance its performance further
- Why unresolved: No experimental results provided on applying the method to GPT-4
- What evidence would resolve it: Experiments applying the model-based data cleaning method to GPT-4 and comparing performance

### Open Question 2
- Question: What is the impact of high-quality seed data quantity on model-based data cleaning performance?
- Basis in paper: Paper mentions robust performance with limited high-quality data but doesn't analyze the relationship
- Why unresolved: No detailed analysis of relationship between high-quality seed data quantity and performance
- What evidence would resolve it: Experiments varying high-quality seed data quantity and analyzing impact

### Open Question 3
- Question: How can transformed data quality be further improved to minimize new error introduction?
- Basis in paper: Paper acknowledges cleaning process could introduce new errors and suggests additional quality enhancement methods are worth exploring
- Why unresolved: No specific solutions provided for addressing new error introduction during cleaning
- What evidence would resolve it: Experiments testing various methods to improve transformed data quality and minimize new errors

## Limitations
- Evaluation restricted to Chinese elementary school math problems, limiting generalizability to other domains
- Effectiveness of exact matching over embedding-based methods asserted but not rigorously compared
- Approach requires initial high-quality seed data for alignment, limiting applicability when such data is unavailable
- Potential for format converter to introduce new errors during transformation acknowledged but not systematically quantified

## Confidence
**High confidence**: Core claim that model-based data cleaning improves performance over high-quality data alone is well-supported by experimental results showing 9.4% improvement and 7B model outperforming larger models.

**Medium confidence**: Assertion that LLMs handle global formatting errors better than rule-based methods is supported by comparison results, though evaluation focuses on specific error type and domain.

**Low confidence**: Claim about exact matching being more precise than embedding methods lacks direct comparative evidence, and superiority over other data augmentation strategies is not established.

## Next Checks
1. **Cross-domain validation**: Test the approach on non-math datasets (e.g., general QA or code generation) to assess domain generalizability and identify failure modes specific to mathematical reasoning.

2. **Matching strategy ablation**: Systematically compare the exact matching approach against embedding-based similarity methods on the same datasets, measuring both alignment quality and downstream performance impact.

3. **Error propagation analysis**: Quantitatively measure the rate of new errors introduced by the format converter compared to the original web-crawled data, and evaluate whether these errors compound during successive fine-tuning iterations.