---
ver: rpa2
title: Outlier detection by ensembling uncertainty with negative objectness
arxiv_id: '2402.15374'
source_url: https://arxiv.org/abs/2402.15374
tags:
- negative
- detection
- data
- training
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting semantic outliers
  in supervised visual recognition, particularly in safety-critical applications.
  The authors argue that existing methods conflate prediction uncertainty with recognition
  of the negative class, and propose a novel approach to tackle this issue.
---

# Outlier detection by ensembling uncertainty with negative objectness

## Quick Facts
- arXiv ID: 2402.15374
- Source URL: https://arxiv.org/abs/2402.15374
- Authors: Anja Delić; Matej Grcić; Siniša Šegvić
- Reference count: 27
- Outperforms state-of-the-art on standard benchmarks for image-wide and pixel-level outlier detection

## Executive Summary
This paper addresses semantic outlier detection in supervised visual recognition by proposing a novel ensemble approach that combines prediction uncertainty with negative objectness. The authors argue that existing methods conflate uncertainty with negative class recognition, and introduce a K+1 or K+2-class architecture that enables direct learning of negative objectness. Their method achieves state-of-the-art performance on standard benchmarks like Fishyscapes, with significant improvements in both image-wide and pixel-level outlier detection.

## Method Summary
The method reconsiders direct prediction of K+1 logits (K groundtruth classes plus one outlier class) to formulate a novel anomaly score that ensembles in-distribution uncertainty with the posterior of the outlier class. The approach is embedded into a dense prediction architecture with mask-level recognition over K+2 classes, where a training procedure encourages the K+2-th class to learn negative objectness at pasted negative instances. The ensemble combines uncertainty scores (capturing anomalies with low embedding norm) and negative objectness scores (capturing anomalies confidently classified into the outlier class due to similarity with negative training data).

## Key Results
- Achieves AP of 81.8 and FPR95 of 1.3 on Fishyscapes benchmark, significantly outperforming previous approaches
- Shows consistent improvement over baselines across multiple datasets for both image-wide and pixel-level outlier detection
- Demonstrates that models with the outlier class do not underperform standard K-way classification on inlier data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The ensemble of uncertainty and negative objectness detects outliers via complementary failure modes.
- **Mechanism:** The uncertainty component (sUnc) captures anomalies with low embedding norm or low confidence, while the negative objectness component (sNO) captures anomalies confidently classified into the outlier class due to similarity with negative training data. These components are weakly correlated, allowing their ensemble to detect outliers that would be missed by either alone.
- **Core assumption:** Outliers can be detected either by high prediction uncertainty OR by clear association with negative training data, and these two failure modes are largely independent.
- **Evidence anchors:**
  - [abstract]: "This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness."
  - [section]: "The sUnc score captures anomalies with a low embedding norm because it assumes that anomalies give rise to examples with high recognition uncertainty. On the other hand, the sNO score captures anomalies that are confidently classified into the K+1-th class due to clear association with the negative training data."
  - [corpus]: Weak evidence. Corpus papers focus on different mechanisms (e.g., generative models, contrastive learning) rather than this specific ensemble approach.
- **Break condition:** If outliers exhibit both high uncertainty AND strong similarity to negative training data, the two components may become correlated, reducing the ensemble's effectiveness.

### Mechanism 2
- **Claim:** Extending the classifier to K+1 (or K+2) classes enables direct learning of negative objectness.
- **Mechanism:** By adding an explicit outlier class, the model learns to produce high confidence predictions for negative examples, which can then be used as a detection signal. This is particularly effective when combined with mixed-content training where negative instances are pasted into inlier scenes.
- **Core assumption:** Including an outlier class does not harm inlier classification performance while providing a useful signal for outlier detection.
- **Evidence anchors:**
  - [abstract]: "We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class."
  - [section]: "Interestingly, this non-standard setup did not underperform with respect to standard K-way performance in any of our experiments."
  - [corpus]: Weak evidence. Most corpus papers focus on post-hoc detection methods or generative approaches rather than modifying the classifier architecture.
- **Break condition:** If the outlier class becomes confused with rare inlier classes or if the negative training data is too limited to provide useful gradients.

### Mechanism 3
- **Claim:** Synthetic negative data generated by jointly trained normalizing flows can approximate real negative data for outlier detection.
- **Mechanism:** The normalizing flow is trained to generate samples at the boundary of the inlier manifold, forcing the classifier to produce uncertain predictions on these synthetic negatives. This encourages the model to detect outliers that are dissimilar to both inliers and real negatives.
- **Core assumption:** Generative models can produce meaningful synthetic outliers that capture the distribution of real outliers.
- **Evidence anchors:**
  - [section]: "We consider to train our models on dynamically generated synthetic negative data... We jointly train the normalizing flow with the classifier in order to generate samples at the border of the inlier manifold."
  - [section]: "Training on synthetic negatives is more beneficial than training on inlier crops."
  - [corpus]: Weak evidence. While some corpus papers mention generative models for outlier detection, they don't specifically address this joint training approach.
- **Break condition:** If the generative model fails to capture the true distribution of outliers, or if the generated samples are too dissimilar from real outliers to be useful.

## Foundational Learning

- **Concept: Probability theory and softmax function**
  - Why needed here: The anomaly scores are formulated as probabilities over classes, requiring understanding of how softmax converts logits to probabilities and how to manipulate these probabilities mathematically.
  - Quick check question: What is the mathematical relationship between logits z and probabilities P(y=k|z) in a softmax classifier?

- **Concept: Ensemble learning and diversity**
  - Why needed here: The core innovation is combining two complementary anomaly detection signals, which relies on understanding how uncorrelated components can improve overall performance.
  - Quick check question: Why does combining weakly correlated anomaly detection signals often outperform using either signal alone?

- **Concept: Generative modeling and normalizing flows**
  - Why needed here: The synthetic negative data approach requires understanding how normalizing flows work and how they can be jointly trained with discriminative models.
  - Quick check question: How does a normalizing flow differ from a standard GAN in terms of the types of distributions it can model?

## Architecture Onboarding

- **Component map:**
  - Backbone (e.g., ResNet-18 or Swin-L) -> Pixel decoder -> Mask decoder with K+2-way classifier -> Anomaly scores (UNO ensemble)
  - Optional: Normalizing flow for synthetic negative generation

- **Critical path:**
  1. Extract features from input image
  2. Generate mask embeddings and K+2-way class probabilities
  3. Apply sigmoid to get binary masks
  4. Compute anomaly scores using UNO ensemble
  5. Optionally, generate synthetic negatives and update both classifier and normalizing flow

- **Design tradeoffs:**
  - Adding outlier class vs. using post-hoc detection methods
  - Real negative data vs. synthetic negative data (quality vs. availability)
  - Mask-level vs. pixel-level anomaly detection (granularity vs. computational cost)
  - Joint training of classifier and normalizing flow vs. separate training

- **Failure signatures:**
  - High correlation between sUnc and sNO components (ensemble provides little benefit)
  - Poor inlier classification performance (outlier class interferes with main task)
  - Low anomaly detection performance on both real and synthetic negative data
  - Mode collapse in normalizing flow (synthetic negatives become uninformative)

- **First 3 experiments:**
  1. Verify that adding the outlier class doesn't harm inlier classification accuracy on validation data
  2. Test the correlation between sUnc and sNO components on a held-out validation set
  3. Compare anomaly detection performance with and without the outlier class on a small benchmark (e.g., Fishyscapes validation)

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions but suggests future work around improving the ensemble by incorporating additional uncorrelated OOD detection scores and exploring optimal methods for synthetic negative data generation.

## Limitations
- Performance gap between models trained with real vs. synthetic negative data
- Reliance on negative training data may limit applicability in domain-specific scenarios with limited negative examples
- Assumption of weak correlation between uncertainty and negative objectness may not hold for all outlier types

## Confidence
- High confidence in the core ensemble mechanism and its effectiveness for standard benchmarks
- Medium confidence in the synthetic negative data approach, as the generative model's ability to approximate real outlier distributions is not fully validated
- Medium confidence in the architectural modifications (K+2-way classification), as the impact on inlier performance is not thoroughly analyzed across diverse scenarios

## Next Checks
1. Analyze the correlation structure between uncertainty and negative objectness scores across different outlier types to identify scenarios where the ensemble may fail
2. Evaluate the method's robustness to domain shift by testing on outliers from distributions not seen during training
3. Compare the synthetic negative generation approach with alternative generative models (e.g., GANs or VAEs) to assess its effectiveness in capturing outlier distributions