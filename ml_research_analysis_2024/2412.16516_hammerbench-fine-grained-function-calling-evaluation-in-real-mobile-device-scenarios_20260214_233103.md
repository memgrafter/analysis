---
ver: rpa2
title: 'HammerBench: Fine-Grained Function-Calling Evaluation in Real Mobile Device
  Scenarios'
arxiv_id: '2412.16516'
source_url: https://arxiv.org/abs/2412.16516
tags:
- user
- tool
- parameters
- data
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "HammerBench is a novel benchmark framework designed to evaluate\
  \ large language models\u2019 (LLMs) function-calling capabilities in real-world,\
  \ multi-turn mobile assistant dialogues. The benchmark addresses the challenge of\
  \ assessing LLM performance in complex, noisy scenarios involving imperfect instructions,\
  \ diverse question-answer trajectories, intent and argument shifts, and external\
  \ individual information."
---

# HammerBench: Fine-Grained Function-Calling Evaluation in Real Mobile Device Scenarios

## Quick Facts
- arXiv ID: 2412.16516
- Source URL: https://arxiv.org/abs/2412.16516
- Reference count: 40
- Evaluates LLM function-calling capabilities in real-world mobile assistant dialogues with comprehensive dataset construction and fine-grained snapshot analysis

## Executive Summary
HammerBench introduces a novel benchmark framework for evaluating large language models' function-calling capabilities in real mobile device scenarios. The framework addresses the challenge of assessing LLM performance in complex, noisy multi-turn dialogues involving imperfect instructions, diverse question-answer trajectories, intent and argument shifts, and external individual information. By constructing a comprehensive dataset from popular mobile app functionalities and anonymized user logs, HammerBench enables detailed evaluation of function-calling performance across individual conversational turns, providing crucial insights for optimizing robust function-calling LLMs in mobile assistant applications.

## Method Summary
HammerBench employs a cost-effective pipeline leveraging open-source models to construct a comprehensive dataset derived from popular mobile app functionalities and anonymized user logs. The framework decomposes conversations into fine-grained function-calling snapshots, enabling detailed evaluation of performance across individual conversational turns. This approach allows for systematic assessment of LLM function-calling capabilities in realistic mobile assistant scenarios, accounting for various interaction complexities such as parameter naming errors, intent shifts, and diverse conversation trajectories.

## Key Results
- Parameter naming errors identified as the primary cause of conversation failures across different interaction scenarios
- GPT-4o demonstrates the most robust performance in function-calling tasks
- Open-source models show considerable potential for improvement, particularly in handling diverse trajectories and intent shifts

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its realistic dataset construction that captures the complexity of real mobile assistant interactions, combined with its fine-grained evaluation approach that isolates performance at the individual function-calling level. By incorporating real user behavior patterns and conversation trajectories, HammerBench provides a more accurate assessment of LLM capabilities than synthetic benchmarks.

## Foundational Learning

**Function-calling in LLMs**: The ability of language models to identify appropriate functions and their parameters from natural language inputs. This is fundamental for building practical AI assistants that can interact with software APIs.

**Multi-turn dialogue evaluation**: Assessing model performance across extended conversational contexts rather than single-turn interactions. This is needed because real assistant conversations involve complex dependencies between turns.

**Fine-grained snapshot decomposition**: Breaking down conversations into discrete function-calling evaluation points. This allows for precise identification of where and how models fail in real interactions.

**Intent and argument shift detection**: Recognizing when user goals or required parameters change mid-conversation. This is crucial for mobile assistants that must handle dynamic user needs.

**Cost-effective dataset construction**: Using automated pipelines with open-source models combined with manual refinement. This approach balances scalability with data quality requirements.

Quick checks: Evaluate model performance on single-turn vs multi-turn scenarios; test parameter extraction accuracy; measure intent shift handling capabilities.

## Architecture Onboarding

Component map: Mobile app functions -> User log collection -> Anonymization pipeline -> Function labeling -> Conversation construction -> Fine-grained snapshots -> Evaluation metrics

Critical path: User log collection → Anonymization pipeline → Function labeling → Conversation construction → Fine-grained snapshots → Evaluation

Design tradeoffs: Automated vs manual data processing (speed vs accuracy), synthetic vs real user data (control vs realism), snapshot granularity vs computational cost

Failure signatures: Parameter naming errors, intent shift misinterpretations, argument extraction failures, trajectory prediction inaccuracies

First experiments: 1) Evaluate single function-calling accuracy on isolated prompts, 2) Test multi-turn conversation coherence, 3) Measure parameter extraction performance across different intent scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Data construction relies on automated tools with manual refinement, introducing potential quantification biases
- Focus on mobile assistant scenarios may limit generalizability to other function-calling domains
- Evaluation framework may not fully capture extended multi-turn conversation complexity

## Confidence

Parameter naming errors as primary failure mode: High
GPT-4o performance ranking: Medium
Open-source model improvement potential: Low

## Next Checks

1) Conduct ablation studies to quantify the impact of automated versus manual data labeling on benchmark results
2) Test the benchmark's applicability to non-mobile function-calling scenarios to assess domain transferability
3) Perform longitudinal studies tracking how different LLMs handle extended multi-turn conversations beyond the current snapshot evaluation framework