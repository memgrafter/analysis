---
ver: rpa2
title: Refusal in Language Models Is Mediated by a Single Direction
arxiv_id: '2406.11717'
source_url: https://arxiv.org/abs/2406.11717
tags:
- refusal
- arxiv
- harmful
- direction
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that refusal behavior in large language
  models is mediated by a single linear direction in activation space across 13 open-source
  chat models ranging from 1.8B to 72B parameters. By identifying and ablating this
  direction, the researchers show that models lose their ability to refuse harmful
  requests while maintaining general capabilities.
---

# Refusal in Language Models Is Mediated by a Single Direction

## Quick Facts
- arXiv ID: 2406.11717
- Source URL: https://arxiv.org/abs/2406.11717
- Authors: Andy Arditi; Oscar Obeso; Aaquib Syed; Daniel Paleka; Nina Panickssery; Wes Gurnee; Neel Nanda
- Reference count: 40
- Primary result: Refusal behavior in LLMs is mediated by a single linear direction in activation space

## Executive Summary
This paper demonstrates that refusal behavior in large language models is mediated by a single linear direction in activation space across 13 open-source chat models ranging from 1.8B to 72B parameters. By identifying and ablating this direction, the researchers show that models lose their ability to refuse harmful requests while maintaining general capabilities. The key finding is that this refusal direction is both necessary and sufficient for the refusal behavior. Leveraging this insight, they develop a novel white-box jailbreak method that directly modifies model weights to disable refusal with minimal impact on other functions. The work provides evidence that current safety fine-tuning methods are brittle, as a simple rank-one weight modification can nearly eliminate refusal behavior across multiple model families.

## Method Summary
The researchers extracted a difference-in-means direction from harmful versus harmless instruction activations, then applied directional ablation to disable refusal behavior. They also developed a white-box jailbreak method using weight orthogonalization that prevents the model from representing the refusal direction in its residual stream. The approach was validated across 13 open-source chat models (1.8B-72B parameters) using adversarial benchmarks like JAILBREAK BENCH and standard language model evaluations. The key insight is that refusal is mediated by a one-dimensional subspace, making it possible to surgically disable this behavior through simple linear algebra operations on the model weights.

## Key Results
- A single linear direction mediates refusal behavior across all tested models, with ablation nearly eliminating refusal (reducing refusal scores from 76.9% to 0.1% on average)
- Weight orthogonalization serves as an effective white-box jailbreak method that bypasses refusal while maintaining general capabilities
- The orthogonalized models show minimal degradation on standard evaluations (MMLU, ARC, GSM8K, TRUTHFUL QA) compared to activation addition methods
- Adversarial suffixes suppress refusal direction propagation by hijacking attention mechanisms to shift focus away from harmful instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Refusal behavior in LLMs is mediated by a single linear direction in activation space
- Mechanism: The model learns to associate harmful instructions with a specific activation pattern. When this pattern is present, the model refuses; when absent, it complies. By identifying and ablating this pattern, we can disable refusal behavior.
- Core assumption: Features in LLMs are represented as linear directions in activation space
- Evidence anchors:
  - [abstract] "refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models"
  - [section] "difference-in-means vector r(l) extracted from layer l" 
- Break condition: If features are represented nonlinearly or refusal behavior requires multiple directions

### Mechanism 2
- Claim: Weight orthogonalization directly disables the refusal direction without affecting other capabilities
- Mechanism: By modifying weight matrices to be orthogonal to the refusal direction, we prevent the model from ever representing this direction in its residual stream. This is equivalent to directional ablation but implemented as a permanent weight modification.
- Core assumption: Weight modifications can be designed to selectively disable specific directions
- Evidence anchors:
  - [section] "orthogonalizing all of these matrices... with respect to the direction ˆr effectively prevents the model from ever writing ˆr to its residual stream"
  - [section] "this weight modification is equivalent to the previously described inference-time directional ablation"
- Break condition: If the refusal direction is represented in multiple weight matrices or requires complex interactions

### Mechanism 3
- Claim: Adversarial suffixes suppress the refusal direction by hijacking attention mechanisms
- Mechanism: When an adversarial suffix is appended, attention heads that normally focus on the harmful instruction shift their attention to the suffix instead. This prevents the harmful content from propagating the refusal direction through the model.
- Core assumption: Attention mechanisms are responsible for propagating the refusal direction across token positions
- Evidence anchors:
  - [section] "adversarial suffix effectively 'hijacks' the attention of these heads... shift their attention to the suffix region"
  - [section] "direct contributions of these heads to the refusal direction are significantly suppressed"
- Break condition: If the refusal direction can be represented without relying on attention mechanisms

## Foundational Learning

- Concept: Linear algebra (vectors, projections, orthogonalization)
  - Why needed here: The entire methodology relies on representing and manipulating directions in high-dimensional space
  - Quick check question: If we have a vector v and want to remove its component along direction d, what operation do we perform?

- Concept: Transformer architecture (residual streams, attention, MLPs)
  - Why needed here: The intervention targets specific components of the transformer that write to the residual stream
  - Quick check question: Which transformer components directly write to the residual stream and could therefore be modified to disable a direction?

- Concept: Difference-in-means methodology
  - Why needed here: This is the technique used to identify the refusal direction by comparing activations between harmful and harmless inputs
  - Quick check question: If we have mean activations for harmful inputs (μ) and harmless inputs (ν), how do we compute the difference-in-means vector?

## Architecture Onboarding

- Component map: Embedding matrix -> Positional embeddings -> Attention output matrices -> MLP output matrices (all write to residual stream)
- Critical path: 1) Extract refusal direction using difference-in-means on harmful/harmless pairs 2) Select best direction based on ablation/induction scores 3) Apply weight orthogonalization to disable direction 4) Validate that refusal is bypassed while maintaining other capabilities
- Design tradeoffs: Weight orthogonalization vs activation addition - orthogonalization is a permanent modification that doesn't affect activations during inference, while activation addition requires runtime computation but can be more surgical.
- Failure signatures: If orthogonalization doesn't work, the model might still refuse harmful requests or show degraded performance on general tasks. If attention hijacking doesn't work, the refusal direction might still propagate despite the adversarial suffix.
- First 3 experiments:
  1. Extract refusal direction from a small model and verify it works via directional ablation
  2. Apply weight orthogonalization to disable the direction and test refusal bypass
  3. Test the orthogonalized model on standard language model evaluations to ensure coherence is maintained

## Open Questions the Paper Calls Out

- Question: How does the effectiveness of the refusal direction vary across different instruction-following paradigms (e.g., next-token prediction vs. explicit instruction-following)?
- Question: Can the refusal direction be disentangled from other safety-related directions (e.g., truthfulness, bias mitigation)?
- Question: How does the refusal direction generalize across different languages and cultural contexts?

## Limitations

- The generalizability of the refusal direction to proprietary models and models with different safety fine-tuning approaches remains uncertain
- The stability of the orthogonalized models under continuous training or fine-tuning has not been thoroughly evaluated
- The interaction between the refusal direction and other safety-related directions (truthfulness, bias mitigation) has not been investigated

## Confidence

- High Confidence: The claim that refusal behavior can be mediated by a single linear direction is well-supported by experimental evidence across multiple model families and sizes
- Medium Confidence: The assertion that weight orthogonalization is equivalent to directional ablation and minimally impacts general capabilities is supported by experiments but requires more extensive evaluation
- Low Confidence: The generalizability of these findings to models with different architectures (e.g., non-transformer) or real-world deployment scenarios

## Next Checks

1. Test the refusal direction's effectiveness on proprietary models (GPT-4, Claude) and models fine-tuned with different techniques (Constitutional AI, RLHF variants) to assess generalizability beyond the studied open-source models

2. Evaluate the long-term stability of the orthogonalized models by testing them after additional fine-tuning on benign tasks to ensure the refusal bypass persists and doesn't degrade general capabilities

3. Investigate whether the same methodology can be applied to other safety-aligned behaviors (bias mitigation, factuality) to determine if the single-direction hypothesis extends beyond refusal to other alignment objectives