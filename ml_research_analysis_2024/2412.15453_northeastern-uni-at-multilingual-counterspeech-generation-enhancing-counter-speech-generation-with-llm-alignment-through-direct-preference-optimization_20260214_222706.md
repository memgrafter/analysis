---
ver: rpa2
title: 'Northeastern Uni at Multilingual Counterspeech Generation: Enhancing Counter
  Speech Generation with LLM Alignment through Direct Preference Optimization'
arxiv_id: '2412.15453'
source_url: https://arxiv.org/abs/2412.15453
tags:
- speech
- hate
- generation
- more
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Direct Preference Optimization (DPO)
  to enhance counter-speech (CS) generation for hate speech by aligning large language
  models with human preferences. The approach combines supervised fine-tuning with
  DPO, leveraging rejected answers generated by GPT-4o to provide negative examples.
---

# Northeastern Uni at Multilingual Counterspeech Generation: Enhancing Counter Speech Generation with LLM Alignment through Direct Preference Optimization

## Quick Facts
- arXiv ID: 2412.15453
- Source URL: https://arxiv.org/abs/2412.15453
- Reference count: 3
- Primary result: DPO-enhanced models achieve 1191.0 JudgeLM score and significantly outperform SFT baselines on multilingual counter-speech generation

## Executive Summary
This paper presents a novel approach to counterspeech generation using Direct Preference Optimization (DPO) to align large language models with human preferences for combating hate speech. The method combines supervised fine-tuning with DPO, utilizing rejected GPT-4o responses as negative examples and incorporating knowledge grounding for factual accuracy. Tested across four languages (English, Basque, Italian, Spanish), the approach demonstrates substantial improvements over traditional SFT methods, with English results showing particularly strong performance across multiple evaluation metrics.

## Method Summary
The proposed approach leverages Direct Preference Optimization (DPO) to enhance counterspeech generation by aligning LLMs with human preferences through a two-stage training process. Initially, a base model undergoes supervised fine-tuning (SFT) on counterspeech data, followed by DPO fine-tuning using preference pairs constructed from human-approved responses and GPT-4o-generated rejected answers. The method incorporates knowledge grounding to improve factual accuracy and is evaluated across multilingual datasets including FabHate and MSCS in English, Basque, Italian, and Spanish.

## Key Results
- English results: 1191.0 JudgeLM score, 51.8% ROUGE-L, 40.3% BLEU, 82.6% BERTScore
- DPO model produces more concise, contextually relevant, and assertive counter-narratives compared to SFT
- Significant performance improvements across all four tested languages (English, Basque, Italian, Spanish)
- Enhanced factual accuracy through knowledge grounding integration

## Why This Works (Mechanism)
DPO works by directly optimizing the model to prefer human-approved responses over rejected ones, creating a preference alignment that traditional supervised fine-tuning cannot achieve. The use of GPT-4o-generated rejected answers provides high-quality negative examples at scale, while knowledge grounding ensures factual accuracy in the generated counterspeech. This combination allows the model to produce more persuasive and contextually appropriate responses that better align with human judgment of quality counterspeech.

## Foundational Learning
- Direct Preference Optimization (DPO): A fine-tuning method that optimizes model preferences based on human feedback pairs, needed to align model outputs with human values; quick check: verify preference pair quality and distribution
- Supervised Fine-Tuning (SFT): Initial training phase using labeled data, needed to establish baseline counterspeech capabilities; quick check: assess dataset coverage and quality
- Knowledge Grounding: Integration of external factual information, needed to improve response accuracy and credibility; quick check: validate grounding source reliability
- Counter-speech Generation: Process of creating persuasive responses to hate speech, needed for effective online content moderation; quick check: evaluate response persuasiveness metrics
- Multilingual Evaluation: Testing across multiple languages, needed to ensure broad applicability; quick check: verify translation quality and cultural appropriateness

## Architecture Onboarding

Component Map: Base Model -> SFT -> DPO -> Knowledge Grounding -> Evaluation

Critical Path: The model first undergoes SFT on counterspeech data, then DPO fine-tuning using preference pairs, with knowledge grounding applied during generation. This sequence ensures the model has foundational capabilities before preference alignment and factual enhancement.

Design Tradeoffs: Using GPT-4o for negative examples provides high-quality training data but introduces potential circularity and accessibility concerns. Knowledge grounding improves accuracy but adds computational overhead. The multilingual approach increases applicability but requires careful handling of language-specific nuances.

Failure Signatures: Poor preference pair quality can lead to misaligned outputs. Insufficient knowledge grounding may result in factual errors. Language-specific failures may occur due to dataset imbalances or translation issues.

First Experiments:
1. Validate preference pair quality by sampling and manually reviewing GPT-4o rejected examples
2. Test knowledge grounding effectiveness by comparing grounded vs. non-grounded outputs on factual accuracy
3. Conduct ablation study removing DPO to quantify its specific contribution to performance improvements

## Open Questions the Paper Calls Out
The paper acknowledges several key uncertainties, particularly regarding the effectiveness of automated metrics versus human judgment for evaluating counterspeech quality. Questions remain about the representativeness and potential biases in the FabHate and MSCS datasets. The substantial variation in results across languages (e.g., 51.8% ROUGE-L in English vs. 22.8% in Basque) raises questions about cross-lingual consistency. The approach's dependence on GPT-4o for generating negative examples presents scalability and accessibility concerns, while computational costs and inference latency for real-time deployment remain unaddressed.

## Limitations
- Heavy reliance on automated metrics (JudgeLM, ROUGE-L, BLEU, BERTScore) rather than human judgment for quality assessment
- Substantial performance variation across languages suggests inconsistent effectiveness
- Dependence on GPT-4o for negative examples introduces potential circularity and scalability issues
- Computational costs and inference latency not addressed, limiting practical deployment assessment

## Confidence

High:
- The technical methodology of combining SFT with DPO is sound and well-documented
- Knowledge grounding integration approach is methodologically appropriate

Medium:
- Multilingual evaluation results are promising but require more robust validation
- Automated metric improvements suggest effectiveness but need human verification

Low:
- Claims about superiority over human-generated responses lack direct comparative validation
- Real-world effectiveness in actual hate speech scenarios remains unproven

## Next Checks

1. Conduct human evaluation studies with diverse annotators to validate automated metric results and assess persuasiveness of generated counterspeech
2. Perform ablation studies to isolate the impact of each component (SFT, DPO, knowledge grounding) on performance across languages
3. Test model robustness on out-of-distribution hate speech topics and evaluate computational efficiency for real-time deployment scenarios