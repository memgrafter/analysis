---
ver: rpa2
title: 'FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality
  Localization'
arxiv_id: '2404.13671'
source_url: https://arxiv.org/abs/2404.13671
tags: []
core_contribution: 'FiLo is a zero-shot anomaly detection method that addresses the
  challenges of accurate anomaly detection and localization in industrial settings.
  It employs a two-component approach: an adaptively learned Fine-Grained Description
  (FG-Des) and a position-enhanced High-Quality Localization (HQ-Loc).'
---

# FiLo: Zero-Shot Anomaly Detection by Fine-Grained Description and High-Quality Localization

## Quick Facts
- arXiv ID: 2404.13671
- Source URL: https://arxiv.org/abs/2404.13671
- Reference count: 40
- One-line primary result: FiLo achieves state-of-the-art zero-shot anomaly detection performance on MVTec and VisA datasets with 83.9% image-level AUC and 95.9% pixel-level AUC on VisA

## Executive Summary
FiLo introduces a novel zero-shot anomaly detection approach that addresses two critical challenges in industrial anomaly detection: accurate anomaly detection and precise localization. The method employs a two-component architecture combining LLM-generated fine-grained anomaly descriptions with a position-enhanced localization pipeline. FiLo achieves state-of-the-art performance on standard benchmarks while maintaining interpretability through detailed anomaly descriptions.

## Method Summary
FiLo operates through two main components: Fine-Grained Description (FG-Des) and High-Quality Localization (HQ-Loc). FG-Des uses Large Language Models to generate detailed, category-specific anomaly descriptions that replace generic terms with precise anomaly types. HQ-Loc integrates Grounding DINO for preliminary localization, position-enhanced text prompts incorporating localization box coordinates, and a Multi-scale Multi-shape Cross-modal Interaction (MMCI) module with convolutional kernels of varying sizes and shapes. The method uses CLIP as a backbone for visual-text feature extraction and alignment, with learnable text vectors to enhance description specificity.

## Key Results
- Achieves state-of-the-art image-level AUC of 83.9% on VisA dataset
- Achieves state-of-the-art pixel-level AUC of 95.9% on VisA dataset
- Outperforms existing zero-shot methods on MVTec and VisA benchmarks
- Demonstrates effective zero-shot transfer capability between datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained anomaly descriptions generated by LLMs improve anomaly detection accuracy by providing specific, category-relevant anomaly types instead of generic terms like "damaged".
- Mechanism: LLM-generated fine-grained anomaly descriptions replace generic anomaly descriptions, allowing the model to better match diverse anomaly types across object categories, leading to improved anomaly detection accuracy and interpretability.
- Core assumption: LLMs possess rich domain knowledge that can be leveraged to generate accurate and specific anomaly descriptions for different object categories.
- Evidence anchors: [abstract] mentions FG-Des introduces fine-grained anomaly descriptions using LLMs; [section 2.3] discusses harnessing domain knowledge from LLMs.

### Mechanism 2
- Claim: Position-enhanced text prompts improve anomaly localization by incorporating the location information of anomalies detected by Grounding DINO into the textual prompts.
- Mechanism: Grounding DINO performs preliminary localization, and position information from localization boxes is incorporated into textual prompts to enhance position descriptions, improving the model's focus on specific areas.
- Core assumption: Grounding DINO can effectively localize anomalies in the foreground of objects, and incorporating position information into textual prompts will improve the model's focus on specific areas.
- Evidence anchors: [abstract] mentions position-enhanced text prompts; [section 3.3.1] describes using Grounding DINO for preliminary localization.

### Mechanism 3
- Claim: Multi-scale, multi-shape convolutional kernels in the MMCI module improve anomaly localization by handling anomalies of various sizes and shapes.
- Mechanism: The MMCI module aggregates patch features using convolutional kernels of different sizes and shapes, allowing the model to effectively handle anomalies of different sizes and shapes, enhancing the model's ability to localize anomaly regions.
- Core assumption: Anomalies in images may exhibit various shapes and sizes, and using convolutional kernels of different sizes and shapes can effectively capture these variations.
- Evidence anchors: [abstract] mentions MMCI module; [section 3.3.3] describes designing convolutional kernels of different sizes and shapes.

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are used to generate fine-grained anomaly descriptions for each object category, which improves anomaly detection accuracy and interpretability.
  - Quick check question: How do LLMs generate fine-grained anomaly descriptions, and what is the impact on anomaly detection performance?

- Concept: Vision-Language Models (VLMs)
  - Why needed here: VLMs like CLIP are used as the backbone for feature extraction and anomaly detection, leveraging their ability to align visual and textual information.
  - Quick check question: How do VLMs like CLIP contribute to anomaly detection, and what are their limitations in this context?

- Concept: Cross-modal Interaction
  - Why needed here: Cross-modal interaction between visual and textual features is essential for anomaly detection and localization, as it allows the model to leverage information from both modalities.
  - Quick check question: How does cross-modal interaction between visual and textual features improve anomaly detection and localization performance?

## Architecture Onboarding

- Component map: LLM -> Grounding DINO -> CLIP Image Encoder -> MMCI module -> Adapter -> Loss functions

- Critical path:
  1. LLM generates fine-grained anomaly descriptions
  2. Grounding DINO performs preliminary localization
  3. Position-enhanced text prompts are created
  4. CLIP extracts visual and textual features
  5. MMCI module processes patch features
  6. Anomaly map and global anomaly score are computed
  7. Loss functions optimize the model

- Design tradeoffs:
  - Using LLM-generated descriptions vs. manually crafted descriptions
  - Using Grounding DINO for preliminary localization vs. other localization methods
  - Using multi-scale, multi-shape kernels vs. fixed-size kernels in MMCI module

- Failure signatures:
  - Poor anomaly detection performance: LLM-generated descriptions may not be accurate or specific enough
  - Inaccurate anomaly localization: Grounding DINO's localization results may be imprecise, or the position information may not be effectively incorporated into textual prompts
  - Slow training or inference: The MMCI module with multi-scale, multi-shape kernels may be computationally expensive

- First 3 experiments:
  1. Evaluate the impact of using LLM-generated fine-grained anomaly descriptions vs. manually crafted descriptions on anomaly detection performance.
  2. Compare the effectiveness of Grounding DINO for preliminary localization vs. other localization methods in improving anomaly localization accuracy.
  3. Assess the contribution of multi-scale, multi-shape kernels in the MMCI module to handling anomalies of various sizes and shapes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FiLo compare to other methods when tested on datasets with significantly different characteristics from MVTec and VisA, such as those with more diverse object categories or different types of anomalies?
- Basis in paper: [inferred] The paper only tests FiLo on the MVTec and VisA datasets, which may not be representative of all possible industrial anomaly detection scenarios.
- Why unresolved: The paper does not provide any information about the performance of FiLo on other datasets or with different types of anomalies.
- What evidence would resolve it: Experiments on additional datasets with diverse characteristics, such as those with more object categories, different types of anomalies, or different image resolutions, would provide evidence of FiLo's generalization capabilities.

### Open Question 2
- Question: How does the performance of FiLo change when using different backbones or image resolutions, and what is the optimal configuration for different types of anomalies or object categories?
- Basis in paper: [explicit] The paper mentions that different backbones and image resolutions can affect performance, but only tests a few configurations.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different backbones or image resolutions on FiLo's performance.
- What evidence would resolve it: Experiments with a wider range of backbones and image resolutions, along with a detailed analysis of the results, would provide insights into the optimal configuration for different types of anomalies or object categories.

### Open Question 3
- Question: How does the performance of FiLo change when using different types of large language models (LLMs) or different methods for generating fine-grained anomaly descriptions?
- Basis in paper: [explicit] The paper uses a specific LLM (GPT-4) and method for generating fine-grained anomaly descriptions, but does not explore alternatives.
- Why unresolved: The paper does not provide any information about the impact of using different LLMs or methods for generating fine-grained anomaly descriptions on FiLo's performance.
- What evidence would resolve it: Experiments with different LLMs and methods for generating fine-grained anomaly descriptions, along with a comparison of the results, would provide insights into the optimal approach for this aspect of FiLo.

### Open Question 4
- Question: How does the performance of FiLo change when using different types of multi-scale and multi-shape convolutional kernels in the MMCI module, and what is the optimal configuration for different types of anomalies or object categories?
- Basis in paper: [explicit] The paper mentions that different types of convolutional kernels can be used in the MMCI module, but only tests a few configurations.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different types of convolutional kernels on FiLo's performance.
- What evidence would resolve it: Experiments with a wider range of convolutional kernel types and configurations, along with a detailed analysis of the results, would provide insights into the optimal approach for this aspect of FiLo.

## Limitations

- Heavy dependence on LLM-generated descriptions without validation of output quality or domain knowledge sufficiency
- Limited ablation studies to quantify the contribution of individual components to overall performance
- No analysis of computational overhead or practical deployment feasibility of the multi-scale, multi-shape MMCI approach

## Confidence

**Detection Performance Claims**: High Confidence - Well-supported with quantitative metrics and reproducible methodology

**Mechanism Effectiveness Claims**: Medium Confidence - Theoretically sound but empirically under-supported with limited ablation studies

**Zero-Shot Generalization Claims**: Low-Medium Confidence - Demonstrated on limited datasets without thorough analysis of domain shift robustness

## Next Checks

1. **Ablation Study on LLM Quality**: Compare FiLo's performance using different LLM models for generating anomaly descriptions, including a manually crafted description baseline. Measure the correlation between LLM description quality and detection accuracy across all object categories.

2. **Localization Pipeline Robustness**: Evaluate FiLo's performance when Grounding DINO's localization accuracy is artificially degraded. Systematically vary the IoU threshold for localization boxes and measure the impact on final anomaly localization AUC scores.

3. **Computational Overhead Analysis**: Benchmark the MMCI module's inference time and memory usage across different anomaly sizes and shapes. Compare against baseline methods using fixed-size kernels to quantify the practical cost of the multi-scale, multi-shape approach.