---
ver: rpa2
title: Learning Representation for Multitask learning through Self Supervised Auxiliary
  learning
arxiv_id: '2409.16651'
source_url: https://arxiv.org/abs/2409.16651
tags:
- learning
- shared
- tasks
- encoder
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method to improve the quality of representations
  generated by the shared encoder in multi-task learning (MTL). The authors propose
  a new approach called Dummy Gradient norm Regularization (DGR), which decreases
  the norm of the gradient of the loss function with respect to dummy task-specific
  predictors to improve the universality of the shared encoder's representations.
---

# Learning Representation for Multitask learning through Self Supervised Auxiliary learning

## Quick Facts
- arXiv ID: 2409.16651
- Source URL: https://arxiv.org/abs/2409.16651
- Authors: Seokwon Shin; Hyungrok Do; Youngdoo Son
- Reference count: 40
- Key outcome: Introduces Dummy Gradient norm Regularization (DGR) to improve shared encoder representation quality in multi-task learning, showing performance improvements across multiple benchmark datasets.

## Executive Summary
This paper introduces Dummy Gradient norm Regularization (DGR), a novel approach to improve the quality of shared encoder representations in multi-task learning. The method works by minimizing the Frobenius norm of the gradient of the loss function with respect to dummy task-specific predictors, which encourages the shared encoder to produce more universal representations. The authors demonstrate that DGR can be seamlessly integrated with existing MTL algorithms and shows significant performance improvements on multiple benchmark datasets including UTKFace, NYUv2, and Cityscapes.

## Method Summary
DGR is a regularization technique designed to improve the universality of shared encoder representations in hard parameter sharing MTL. The method introduces dummy task-specific predictors with fixed, randomly initialized parameters and adds a penalty term to the loss function based on the Frobenius norm of the gradient of the loss with respect to these dummy predictors. This encourages the shared encoder to produce representations that are equally useful for both optimal and arbitrary predictors. The approach uses a finite difference approximation for computational efficiency and can be integrated with existing MTL algorithms.

## Key Results
- DGR improves performance on almost all existing MTL methods when integrated with them
- The method shows superior performance on complex tasks with many classes (age prediction, ethnicity classification)
- DGR can be seamlessly integrated with existing MTL algorithms due to its simplicity
- Shared representations generated by DGR show better performance compared to existing MTL methods when applied to various classifiers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DGR improves the universality of shared encoder representations by minimizing the Frobenius norm of the gradient of the loss function with respect to dummy task-specific predictors.
- Mechanism: The DGR method introduces dummy task-specific predictors with fixed, randomly initialized parameters. By minimizing the gradient norm of the loss with respect to these dummy predictors, the method encourages the shared encoder to produce representations that are equally useful for both optimal and arbitrary predictors, thereby improving universality.
- Core assumption: The universality of shared encoder representations is inversely proportional to the Frobenius norm of the gradient of the loss function with respect to an arbitrary predictor (Theorem 1 in the paper).
- Evidence anchors:
  - [abstract]: "Specifically, the method decreases the norm of the gradient of the loss function with respect to dummy task-specific predictors to improve the universality of the shared encoder's representations."
  - [section]: "Theorem 1 allows us to improve the universality of the shared encoder by minimizing the Frobenius norm of the gradient of an arbitrary task-specific predictor."
  - [corpus]: Weak evidence - the corpus does not contain specific information about DGR or its mechanisms.
- Break condition: If the loss function is not convex with respect to the dummy predictor parameters, or if the dummy predictors become too specialized to the shared encoder during training.

### Mechanism 2
- Claim: DGR can be seamlessly integrated with existing MTL algorithms to improve their performance.
- Mechanism: The simplicity of DGR allows it to be easily added as a regularization term to the loss function of existing MTL methods. This integration boosts the performance of these methods by improving the quality of shared representations.
- Core assumption: The shared encoder's representations are crucial for the performance of MTL models, and improving their universality leads to better overall performance.
- Evidence anchors:
  - [abstract]: "The simplicity also allows us to seamlessly integrate DGR with the existing multi-task learning algorithms."
  - [section]: "Owing to its simplicity, we integrate our approach with the existing MTL approaches and demonstrate that our approach boosts the baseline performances in most combinations."
  - [corpus]: No direct evidence in the corpus about DGR's integration with existing MTL algorithms.
- Break condition: If the integration of DGR significantly increases computational complexity or if it interferes with the primary learning objectives of existing MTL methods.

### Mechanism 3
- Claim: DGR improves the performance of MTL models on complex tasks with many classes.
- Mechanism: By improving the universality of shared representations, DGR enables the shared encoder to produce more informative and expressive features that can compensate for the lack of task-specific tuning in arbitrary predictors. This is particularly beneficial for complex tasks with many classes, such as age prediction and ethnicity classification.
- Core assumption: Complex tasks with many classes require more universal and expressive representations to achieve good performance.
- Evidence anchors:
  - [abstract]: "Through experiments on multiple multi-task learning benchmark datasets, we demonstrate that DGR effectively improves the quality of the shared representations, leading to better multi-task prediction performances."
  - [section]: "Notably, both when independently employed and integrated with the other MTL methods, DGR improved performance in two of the three tasks—specifically, in the challenging age prediction and ethnicity classification tasks, which involve a larger number of classes than the gender classification task."
  - [corpus]: No direct evidence in the corpus about DGR's performance on complex tasks with many classes.
- Break condition: If the improvement in universality does not translate to better performance on specific tasks, or if the method overfits to the dummy predictors.

## Foundational Learning

- Concept: Hard parameter sharing in MTL
  - Why needed here: DGR is designed for the hard parameter sharing MTL setting, where a single shared encoder is used across multiple tasks.
  - Quick check question: What is the difference between hard and soft parameter sharing in MTL?

- Concept: Universality of representations
  - Why needed here: DGR aims to improve the universality of shared encoder representations, which is defined as the inverse of the difference between the loss values of optimal and arbitrary task-specific predictors.
  - Quick check question: How does the concept of universality relate to the quality of shared representations in MTL?

- Concept: Gradient norm regularization
  - Why needed here: DGR uses gradient norm regularization as a method to improve the universality of shared representations by minimizing the Frobenius norm of the gradient of the loss function with respect to dummy predictors.
  - Quick check question: What is the purpose of using gradient norm regularization in machine learning, and how does it differ from other forms of regularization?

## Architecture Onboarding

- Component map:
  - Shared encoder: ResNet50 backbone
  - Task-specific predictors: Two fully connected layers per task
  - Dummy predictors: Two fully connected layers per task, with fixed random initialization
  - Loss function: Sum of task-specific losses and DGR regularization term

- Critical path:
  1. Forward pass: Input data through shared encoder, then through task-specific and dummy predictors
  2. Compute task-specific losses and DGR regularization term
  3. Backward pass: Compute gradients and update parameters

- Design tradeoffs:
  - Number of dummy predictors: Using multiple dummy predictors (3 in this case) can improve stability but increases computational cost
  - DGR regularization strength (λ): Balancing between task-specific performance and universality of representations
  - Backbone architecture: More complex architectures (e.g., SWIN transformer) may benefit more from DGR but also increase computational requirements

- Failure signatures:
  - Performance degradation on specific tasks due to overemphasis on universality
  - Increased computational cost without significant performance improvement
  - Instability in training due to the introduction of dummy predictors

- First 3 experiments:
  1. Implement DGR on a simple MTL dataset (e.g., UTKFace) with a basic MTL method (e.g., vanilla MTL) to verify the improvement in universality and performance
  2. Integrate DGR with an existing state-of-the-art MTL method (e.g., IMTL) on a more complex dataset (e.g., NYUv2) to assess the performance boost
  3. Conduct ablation studies by varying the number of dummy predictors and the regularization strength (λ) to find the optimal configuration for a specific dataset and MTL method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of dummy predictor architecture (e.g., deeper networks, different activation functions) affect the universality of shared representations in DGR?
- Basis in paper: [explicit] The paper states that dummy predictors use the same architecture as task-specific predictors and are randomly initialized, but explores limited variations.
- Why unresolved: The paper only uses three randomly initialized dummy predictors with the same architecture as task-specific predictors. It does not explore the impact of architectural differences or initialization strategies on universality.
- What evidence would resolve it: Experiments comparing DGR performance with dummy predictors of varying depths, activation functions, or initialization schemes (e.g., Xavier, He) would clarify the optimal architecture for maximizing universality.

### Open Question 2
- Question: Can DGR be extended to non-convex loss functions while maintaining its effectiveness in improving shared encoder universality?
- Basis in paper: [inferred] The paper assumes convexity of the loss function for the theoretical derivation but does not test non-convex scenarios common in deep learning.
- Why unresolved: The theoretical foundation relies on convex loss functions, which is an unrealistic assumption for deep neural networks. The paper does not empirically validate DGR's performance with non-convex losses.
- What evidence would resolve it: Experiments applying DGR to deep learning models with non-convex losses (e.g., cross-entropy for classification, Dice loss for segmentation) and comparing results to the convex case would test the method's robustness.

### Open Question 3
- Question: What is the impact of varying the number of dummy predictors on DGR's performance and computational efficiency?
- Basis in paper: [explicit] The paper uses three dummy predictors for stability but acknowledges computational cost concerns.
- Why unresolved: While the paper mentions using three dummy predictors, it does not systematically study how the number of dummy predictors affects performance or computational overhead.
- What evidence would resolve it: Experiments varying the number of dummy predictors (e.g., 1, 3, 5, 10) while measuring both performance improvements and computational costs (e.g., training time, memory usage) would identify the optimal trade-off.

### Open Question 4
- Question: How does DGR compare to other gradient-based regularization methods in terms of improving shared encoder universality and overall MTL performance?
- Basis in paper: [explicit] The paper compares DGR to SAM and PTA but does not extensively benchmark against other gradient regularization techniques.
- Why unresolved: The paper only briefly compares DGR to SAM and PTA, leaving a gap in understanding how it stacks up against other gradient regularization methods in the MTL literature.
- What evidence would resolve it: Comprehensive experiments comparing DGR to other gradient regularization methods (e.g., gradient clipping, gradient penalty) across multiple MTL benchmark datasets would clarify its relative effectiveness.

## Limitations

- The paper's claims rely heavily on the assumption that improving universality of shared representations directly translates to better MTL performance
- Computational efficiency claims are based on a finite difference approximation without extensive validation of its accuracy compared to exact Hessian computation
- The exact relationship between DGR-induced universality and task-specific performance improvements remains somewhat unclear

## Confidence

- **High Confidence**: The core mechanism of DGR (minimizing gradient norm with respect to dummy predictors) is mathematically sound and the theoretical foundation (Theorem 1) appears valid
- **Medium Confidence**: The empirical results showing performance improvements across multiple datasets and MTL methods, though the magnitude of improvement varies significantly between tasks
- **Low Confidence**: Claims about computational efficiency and the generalizability of results to more complex architectures beyond ResNet50

## Next Checks

1. Conduct detailed ablation studies to isolate the contribution of DGR regularization from other factors (number of dummy predictors, regularization strength) to performance improvements
2. Test DGR with more diverse backbone architectures (e.g., Vision Transformers) to validate claims about architectural agnosticism
3. Perform extended experiments with varying dataset sizes to determine if DGR's benefits scale with data availability or diminish with limited training data