---
ver: rpa2
title: Automated Adversarial Discovery for Safety Classifiers
arxiv_id: '2406.17104'
source_url: https://arxiv.org/abs/2406.17104
tags:
- adversarial
- dimensions
- attacks
- methods
- comment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of automated adversarial discovery
  for safety classifiers, aiming to generate attacks that not only fool classifiers
  but also explore previously unseen harm dimensions. The authors formalize the task
  and propose an evaluation framework based on adversarial success and dimensional
  diversity.
---

# Automated Adversarial Discovery for Safety Classifiers

## Quick Facts
- arXiv ID: 2406.17104
- Source URL: https://arxiv.org/abs/2406.17104
- Authors: Yash Kumar Lal; Preethi Lahoti; Aradhana Sinha; Yao Qin; Ananth Balashankar
- Reference count: 13
- Primary result: LLM-based methods achieve higher adversarial success but lower dimensional diversity compared to non-LLM methods

## Executive Summary
This paper introduces automated adversarial discovery for safety classifiers, aiming to generate attacks that not only fool classifiers but also explore previously unseen harm dimensions. The authors formalize the task and propose an evaluation framework based on adversarial success and dimensional diversity. They benchmark various methods on the CivilComments toxicity task, finding that while LLM-based attacks achieve higher adversarial success, they struggle to generate attacks in new harm dimensions. The best-performing method, Discover-Adapt, achieves a 5% success rate in generating new successful attacks on unseen harm dimensions, highlighting the challenges in automatically finding new harmful dimensions of attack.

## Method Summary
The paper proposes a framework for automated adversarial discovery that evaluates attacks based on both adversarial success (fooling the classifier) and dimensional diversity (exploring new harm dimensions). The method uses a blackbox classifier (Perspective API) and generates adversarial examples from seed comments using non-LLM approaches (WordNet, Polyjuice) and LLM-based methods (Rewrite, Self-Refine, Discover-Adapt). The Discover-Adapt framework leverages LLM knowledge of toxicity subtypes to generate attacks in previously unseen harm dimensions. Evaluation uses leave-one-out dimensions strategy with 25 seed comments per dimension, measuring adversarial success and dimensional diversity using PaLM2 as a dimensional classifier.

## Key Results
- LLM-based attacks achieve 5-15% adversarial success rates, outperforming non-LLM methods
- Discover-Adapt framework generates attacks in identity attacks, insults, and sexually explicit content dimensions
- Only 5% of attacks successfully explore new dimensions while maintaining adversarial success
- Dimensional classifier accuracy varies widely (60-85%), with lowest accuracy on toxicity dimension (52%)

## Why This Works (Mechanism)

### Mechanism 1
LLM-based attacks can successfully bypass toxicity classifiers by rephrasing inputs while maintaining toxicity. LLMs like PaLM2 leverage instruction-following capabilities to generate paraphrases that retain toxic meaning but use different wording, evading exact keyword-based detection. Core assumption: The toxicity classifier relies on lexical patterns rather than semantic understanding. Break condition: If classifier uses semantic embeddings or contextual understanding, LLM paraphrasing becomes less effective.

### Mechanism 2
Discover-Adapt framework can generate attacks in previously unseen harm dimensions by leveraging LLM knowledge of toxicity subtypes. First step identifies potential toxicity dimensions from either seed input or LLM priors; second step adapts the input along those dimensions to create attacks in new categories. Core assumption: LLMs have internalized knowledge about various toxicity subtypes that can be elicited through prompting. Break condition: If LLM lacks comprehensive knowledge of harm dimensions or cannot properly adapt inputs along those dimensions.

### Mechanism 3
Dimensional diversity can be measured using LLM-based classifiers that judge whether generated attacks belong to new harm categories. PaLM2 is used as a judge to classify generated attacks into dimensions, enabling automated evaluation of diversity without human annotation. Core assumption: LLM-based classifiers can accurately identify dimensions of toxicity with reasonable accuracy. Break condition: If LLM-based classifier accuracy falls below threshold needed for reliable evaluation.

## Foundational Learning

- **Dimensional diversity in adversarial attacks**: Why needed: The paper's core contribution is generating attacks that not only fool classifiers but also explore new harm dimensions. Quick check: What distinguishes an attack that fools a classifier from one that also explores a new harm dimension?

- **LLM-based adversarial generation techniques**: Why needed: The paper benchmarks various methods including non-LLM approaches and LLM-based methods. Quick check: How does Polyjuice's counterfactual generation differ from simple word replacement methods?

- **Blackbox adversarial attack generation**: Why needed: The framework assumes only API access to the attacker model, not model weights. Quick check: Why does assuming blackbox access make the framework more broadly applicable?

## Architecture Onboarding

- **Component map**: Seed data -> Blackbox classifier -> Attack generator -> Dimensional classifier -> Evaluation metrics
- **Critical path**: 1. Select seed comments not belonging to held-out dimension, 2. Generate adversarial examples using attack method, 3. Evaluate adversarial success against blackbox classifier, 4. Evaluate dimensional diversity using dimensional classifier, 5. Calculate "both" metric combining success and diversity
- **Design tradeoffs**: LLM-based methods achieve higher adversarial success but lower dimensional diversity; non-LLM methods preserve semantic similarity but produce weaker attacks; Discover-Adapt framework offers control but requires careful prompt engineering
- **Failure signatures**: Low adversarial success (attack generator not effectively fooling classifier), low dimensional diversity (attack generator staying within original harm dimension), inconsistent performance across dimensions (framework not robust to different harm types)
- **First 3 experiments**: 1. Run WordNet baseline on identity attack held-out dimension to establish baseline performance, 2. Run Polyjuice baseline on same dimension to measure LLM advantage in adversarial success, 3. Run Discover-Adapt with constitutional 5 method on identity attack to test framework effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
How can the dimensional classifier's accuracy be improved to reduce subjectivity in evaluations? Basis: The paper mentions the subjectivity of dimensional evaluations and the need for a golden set of human evaluations. Unresolved because current dimensional classifier (PaLM2) has varying accuracy across dimensions, with lowest accuracy around 76% for 'insult' dimension. Evidence to resolve: Developing a more accurate dimensional classifier or establishing a standardized set of human evaluations as a golden standard.

### Open Question 2
Can the Discover-Adapt framework be extended to handle more diverse and complex unlabeled dimensions of toxicity? Basis: The paper suggests Discover-Adapt allows more control by leveraging various sources of unlabeled dimensions. Unresolved because current implementation only explores unlabeled dimensions identifiable by LLMs, with performance varying across different held-out dimensions. Evidence to resolve: Extending framework to incorporate more diverse sources of unlabeled dimensions and evaluating performance across wider range of dimensions.

### Open Question 3
How can the inconsistency in the Discover-Adapt framework's performance across different dimensions be addressed? Basis: The paper notes Discover-Adapt beats other methods for three out of five held-out dimensions, but substantial headroom for improvement remains. Unresolved because framework's performance is not consistent across all dimensions, sometimes failing to faithfully adapt to unlabeled dimension. Evidence to resolve: Investigating reasons behind inconsistency and developing techniques to improve performance across all dimensions.

## Limitations

- Dimensional classification reliability varies significantly (60-85% accuracy) across harm dimensions
- Results may not generalize beyond CivilComments dataset and Perspective API classifier
- Prompt engineering dependencies make replication challenging without detailed guidelines

## Confidence

**Medium Confidence**: LLM-based attacks achieve higher adversarial success than non-LLM methods, supported by empirical results but with limited practical effectiveness (5-15% success rates).

**Low Confidence**: Discover-Adapt can reliably generate attacks in previously unseen harm dimensions, the paper's most novel claim but with weakest supporting evidence (only 5% success rate and significant variation across dimensions).

**Medium Confidence**: The framework for automated adversarial discovery is well-defined and evaluation methodology is sound, but practical utility is constrained by low success rates and dimensional classification reliability issues.

## Next Checks

1. **Classifier Accuracy Validation**: Independently verify PaLM2's dimensional classification accuracy on held-out test data for each harm dimension, calculating confidence intervals to determine if accuracy is sufficient for reliable evaluation.

2. **Cross-Classifier Generalization**: Test Discover-Adapt framework against a different toxicity classifier (e.g., fine-tuned BERT model) to assess whether approach generalizes beyond Perspective API, comparing success rates and dimensional diversity across classifiers.

3. **Dimensional Transfer Analysis**: Examine whether attacks generated for one held-out dimension can be successfully transferred to other unseen dimensions, validating whether framework is truly discovering new harm dimensions or memorizing patterns from training data.