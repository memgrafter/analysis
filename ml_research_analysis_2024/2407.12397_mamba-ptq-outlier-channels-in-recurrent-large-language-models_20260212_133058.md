---
ver: rpa2
title: 'Mamba-PTQ: Outlier Channels in Recurrent Large Language Models'
arxiv_id: '2407.12397'
source_url: https://arxiv.org/abs/2407.12397
tags:
- quantization
- arxiv
- channels
- recurrent
- mamba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines post-training quantization (PTQ) for Mamba,
  a recurrent neural network architecture for language modeling. The authors demonstrate
  that Mamba models exhibit activation outlier channels, similar to transformer-based
  LLMs, which complicate quantization due to their disproportionately large dynamic
  ranges.
---

# Mamba-PTQ: Outlier Channels in Recurrent Large Language Models

## Quick Facts
- arXiv ID: 2407.12397
- Source URL: https://arxiv.org/abs/2407.12397
- Authors: Alessandro Pierro; Steven Abreu
- Reference count: 20
- Key outcome: Mamba models exhibit activation outlier channels that complicate quantization, causing significant accuracy degradation with naive methods, especially at 4-bit precision.

## Executive Summary
This paper investigates post-training quantization (PTQ) challenges for Mamba, a recurrent neural network architecture for language modeling. The authors demonstrate that Mamba models, like transformer-based LLMs, exhibit activation outlier channels that severely degrade quantization accuracy when using symmetric per-tensor quantization. These outliers cause quantization scales to be dominated by extreme values, leading to large rounding errors for the majority of channels. The study provides baseline PTQ results across Mamba model sizes (130M to 2.8B parameters) and suggests that outlier-aware methods like SmoothQuant are necessary for efficient deployment of recurrent LLMs on resource-constrained devices.

## Method Summary
The paper evaluates post-training quantization of Mamba models using symmetric per-tensor quantization with absolute maximum (absmax) calibration. They analyze activation outlier channels across different Mamba model sizes and evaluate zero-shot accuracy on six downstream tasks. The study compares quantization configurations including W8, W4, and W8A8 (weights and activations quantized to 8-bit and 4-bit respectively). Outlier channels are identified as activations with absolute maximum values exceeding six standard deviations from the layer mean. The paper proposes SmoothQuant as a potential solution but does not report its effectiveness on Mamba models in this work.

## Key Results
- Mamba activation quantization is severely degraded by outlier channels, similar to transformers, causing accuracy collapse to near zero in some configurations.
- Zeroing outlier channels causes significant accuracy loss (12.61% for Mamba-130m, 17.49% for Mamba-2.8B), indicating they carry critical information rather than being noise.
- Input-dependent parameterization in Mamba causes activation outliers to propagate differently across layers than in transformers, making outlier patterns less predictable.
- Naive quantization to 4-bit precision with both weights and activations quantized leads to average task accuracy dropping to near zero in some configurations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mamba activation quantization is severely degraded by outlier channels, similar to transformers.
- Mechanism: A small percentage of activation channels have extremely large dynamic ranges. When symmetric per-tensor quantization is applied, these outlier channels force the quantization scale to be very small, which in turn causes large rounding errors for the majority of non-outlier channels.
- Core assumption: The absolute maximum value from outlier channels dominates the scale calculation for symmetric per-tensor quantization.
- Evidence anchors:
  - [abstract] "Mamba models exhibit the same pattern of outlier channels observed in attention-based LLMs."
  - [section 2] "outliers (particularly in activations) are the subject of research in LLM quantization."
  - [corpus] Weak correlation to BitNet a4.8 and Mitigating the Impact of Outlier Channels papers, suggesting outlier issues are known but not well studied in recurrent models.
- Break condition: If outlier channels are absent or if per-channel quantization is used, this mechanism no longer explains accuracy loss.

### Mechanism 2
- Claim: Input-dependent parameterization in Mamba causes activation outliers to propagate differently across layers than in transformers.
- Mechanism: In Mamba, the recurrent matrix At is computed as a function of input ut, which means activation outliers can be introduced or amplified by the non-linear, content-dependent transformations (Equations 4-9). This dynamic generation of parameters makes outlier patterns less predictable and more sensitive to input content.
- Core assumption: Input-dependent parameters such as Bt, Ct, and At can amplify or introduce outliers based on input distribution.
- Evidence anchors:
  - [section 3] "Importantly, due to the input-dependent parameterization, we consider only input-independent parameters as weights, such as Alog, while we consider input-dependent parameters like At as activations."
  - [section 2] "We observe distinct outlier patterns... the remaining two blocks exhibit no regular behavior."
- Break condition: If input-independent parameterization is enforced or if outliers are suppressed by architectural changes, the mechanism no longer holds.

### Mechanism 3
- Claim: Zeroing outlier channels degrades downstream accuracy, proving they carry critical information.
- Mechanism: Removing channels with activations beyond six standard deviations causes significant accuracy loss (e.g., 12.61% drop for Mamba-130m), indicating these outliers are not merely noise but encode important features for task performance.
- Core assumption: Outlier channels contribute meaningful signal rather than being artifacts of training instability.
- Evidence anchors:
  - [section 2] "we further assess the importance of the outlier channels for the model's predictions by evaluating the impact of zeroing out the outliers on downstream accuracy... a drop in average accuracy of 12.61% and 17.49%."
- Break condition: If zeroed-out models maintain or improve accuracy, the claim that outliers carry critical information is invalidated.

## Foundational Learning

- Concept: Symmetric per-tensor quantization
  - Why needed here: This is the quantization method used in the paper, and its sensitivity to outliers is the central issue.
  - Quick check question: If a tensor has values in the range [-1000, 1] and you quantize to 8-bit, what is the quantization scale? (Answer: 1000 / 127 ≈ 7.87)

- Concept: Input-dependent parameterization in state space models
  - Why needed here: Mamba's key difference from standard SSMs is that parameters like Bt and Ct depend on the input, which changes how outliers emerge.
  - Quick check question: In Equation 4, which parameters are computed from the input ut? (Answer: Bt, ∆t, Ct)

- Concept: Dynamic range and outlier definition
  - Why needed here: Understanding how outliers are identified (e.g., beyond six standard deviations) is critical to interpreting the results.
  - Quick check question: If a layer's mean activation is 0.1 and std is 0.05, what activation value would be considered an outlier? (Answer: > 0.1 + 6*0.05 = 0.4)

## Architecture Onboarding

- Component map:
  - Mamba block: input projection -> SSM block (input-dependent parameters) -> gating -> normalization -> causal conv -> output projection
  - Quantization targets: pre-trained weights (input-independent) and activations (input-dependent)
  - Outlier channels: activation dimensions with abs max beyond six standard deviations

- Critical path:
  1. Forward pass through Mamba block
  2. Activation extraction at linear layers (in, x, out, dt)
  3. Identify outlier channels by comparing abs max to layer mean ± 6 std
  4. Apply symmetric per-tensor quantization
  5. Measure downstream accuracy degradation

- Design tradeoffs:
  - Per-tensor vs. per-channel quantization: per-tensor is hardware-friendly but outlier-sensitive; per-channel is outlier-robust but less efficient
  - Zeroing vs. smoothing outliers: zeroing removes information but is simple; smoothing (e.g., SmoothQuant) preserves information but adds complexity
  - Quantizing activations vs. weights: activations are more prone to outliers but are needed for inference efficiency

- Failure signatures:
  - Accuracy collapse to near zero when both weights and activations are quantized to low bitwidth without outlier handling
  - Inconsistent outlier patterns across layers, especially in linear(dt) where almost no outliers appear
  - High sensitivity to bitwidth: W4 configurations degrade more than W8

- First 3 experiments:
  1. Reproduce outlier detection: run Mamba-130m on a subset of WikiText-2 and plot pre-activation absolute max values per channel to confirm outlier patterns
  2. Quantization sensitivity test: compare W8 vs. W4 quantization with and without activation quantization to confirm accuracy collapse
  3. Outlier importance test: zero out outlier channels in a trained Mamba-130m and measure accuracy drop on LAMBADA and HellaSwag

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific mechanism by which activation outliers degrade the accuracy of quantized Mamba models?
- Basis in paper: [explicit] The paper demonstrates that outlier channels significantly impact model performance, with zeroing out outliers causing a 12.61% accuracy drop for Mamba-130m and 17.49% for Mamba-2.8B, and that naive quantization leads to accuracy degradation, especially for 4-bit precision.
- Why unresolved: While the paper identifies the existence and importance of outlier channels, it does not explicitly explain the underlying mechanism by which these outliers cause quantization difficulties and accuracy degradation.
- What evidence would resolve it: Detailed analysis of the activation distributions and their impact on quantization error propagation through the model would clarify the mechanism. Comparing the performance of outlier-aware quantization methods with different smoothing strengths could also provide insights.

### Open Question 2
- Question: How effective is SmoothQuant in mitigating quantization errors for Mamba models, and what are the optimal hyperparameters?
- Basis in paper: [explicit] The paper proposes SmoothQuant as a potential solution but does not report its effectiveness on Mamba models in this work. It mentions the migration strength parameter α but does not provide experimental results for Mamba.
- Why unresolved: The paper only provides baseline results for naive quantization and suggests SmoothQuant as a first step, but lacks empirical evaluation of its effectiveness and optimal hyperparameter settings for Mamba models.
- What evidence would resolve it: Comprehensive experiments evaluating SmoothQuant on Mamba models with different hyperparameter settings (e.g., α values) and comparing its performance to naive quantization and other outlier-aware methods would determine its effectiveness and optimal configuration.

### Open Question 3
- Question: How does the quantization behavior of Mamba models compare to other recurrent architectures like RWKV and hybrid models like Griffin?
- Basis in paper: [explicit] The paper suggests extending the analysis to other recurrent LLMs such as RWKV and hybrid models, but does not provide any comparative results.
- Why unresolved: The study focuses solely on Mamba models and does not explore how quantization challenges and solutions might generalize to other recurrent architectures or hybrid models.
- What evidence would resolve it: Conducting a comparative study on the quantization behavior of Mamba, RWKV, and hybrid models like Griffin, analyzing their outlier patterns, and evaluating the effectiveness of quantization techniques across these architectures would provide insights into their relative quantization characteristics.

## Limitations

- The paper does not provide empirical validation of SmoothQuant's effectiveness on Mamba models, only mentioning it as a future direction.
- The analysis focuses on activation outliers but does not thoroughly explore whether these outliers are truly essential for model performance or could be removed without significant accuracy loss.
- The paper does not compare different quantization strategies (such as per-channel quantization) to quantify the exact cost of hardware-friendly per-tensor approaches.

## Confidence

- High Confidence: The existence of activation outliers in Mamba models and their similarity to transformer-based LLMs.
- Medium Confidence: The claim that zeroing outlier channels degrades accuracy, proving they carry critical information.
- Low Confidence: The assertion that input-dependent parameterization in Mamba causes activation outliers to propagate differently than in transformers.

## Next Checks

1. Implement and evaluate SmoothQuant on Mamba models to quantify its effectiveness in mitigating outlier-induced quantization errors and compare against baseline per-tensor quantization results.
2. Conduct ablation studies comparing per-tensor vs. per-channel quantization on Mamba models to measure the exact accuracy cost of hardware-friendly quantization schemes.
3. Perform sensitivity analysis by systematically removing outlier channels at different thresholds (e.g., 4σ, 6σ, 8σ) to determine whether the accuracy degradation is monotonic and whether any outliers could be removed without significant performance loss.