---
ver: rpa2
title: 'CP-Prompt: Composition-Based Cross-modal Prompting for Domain-Incremental
  Continual Learning'
arxiv_id: '2407.21043'
source_url: https://arxiv.org/abs/2407.21043
tags:
- prompts
- learning
- domain
- prompt
- cp-prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CP-Prompt introduces a twin-prompting strategy to address catastrophic
  forgetting in cross-modal domain-incremental learning. It uses common prompts to
  capture shared knowledge across domains and personalized prompts to capture domain-specific
  features through transformer layers.
---

# CP-Prompt: Composition-Based Cross-modal Prompting for Domain-Incremental Continual Learning

## Quick Facts
- arXiv ID: 2407.21043
- Source URL: https://arxiv.org/abs/2407.21043
- Reference count: 40
- CP-Prompt achieves 93.65% accuracy on CDDB-Hard with only 0.22% additional parameters, outperforming state-of-the-art by up to 2.3%

## Executive Summary
CP-Prompt introduces a twin-prompting strategy to address catastrophic forgetting in cross-modal domain-incremental learning. It uses common prompts to capture shared knowledge across domains and personalized prompts to capture domain-specific features through transformer layers. This design allows the model to continually learn new domains without retraining, achieving state-of-the-art performance while maintaining parameter efficiency.

## Method Summary
CP-Prompt uses a twin-prompting strategy with CLIP as the backbone, where only 0.22% of parameters are tuned. The method employs common prompts (frozen after each domain) to capture inter-domain knowledge and personalized prompts (embedded in MSA layers via Prefix-One strategy) to capture intra-domain semantic variations. During inference, K-Means clustering assigns samples to domains without requiring task-ID, making it suitable for open-domain evaluation.

## Key Results
- Achieves 93.65% accuracy on CDDB-Hard benchmark
- Uses only 0.22% additional parameters compared to full model fine-tuning
- Outperforms state-of-the-art methods by up to 2.3% in accuracy

## Why This Works (Mechanism)

### Mechanism 1
The twin-prompting strategy (common + personalized) reduces catastrophic forgetting by separating shared and domain-specific knowledge. Common prompts capture inter-domain patterns and are frozen after each domain, preserving general task knowledge. Personalized prompts are embedded in transformer attention layers to capture intra-domain semantic variations without altering base model weights. Core assumption: Shared knowledge can be effectively frozen and reused across domains while domain-specific knowledge can be efficiently injected into transformer attention without interference. Evidence anchors: [abstract] "CP-Prompt captures intra-domain knowledge by compositionally inserting personalized prompts on multi-head self-attention layers and then learns the inter-domain knowledge with a common prompting strategy." Break condition: If inter-domain similarity is too low, common prompts may overgeneralize and harm domain-specific performance.

### Mechanism 2
Embedding personalized prompts in key/value vectors (Prefix-One) improves attention-based domain adaptation without full fine-tuning. Personalized prompts are concatenated with key and value matrices in each MSA layer, allowing the model to reweight attention scores based on domain context while keeping query vectors fixed. Core assumption: Attention mechanisms can effectively integrate prompt-based domain context into semantic feature extraction. Evidence anchors: [section 4.3] "we add soft prompts to the Key matrix, and multiply it by the Query matrix containing only the original data, then derive attention score." Break condition: If prompts are too long or misaligned with attention heads, attention patterns may degrade and cause overfitting to domain-specific noise.

### Mechanism 3
Using K-Means clustering for domain selection during inference avoids task-ID assumptions and supports open-domain evaluation. After training, domain centroids are computed from embeddings. At inference, the nearest centroid determines which personalized prompt set to use for a given sample. Core assumption: Domain-specific embeddings are separable in feature space and K-Means can reliably assign samples to their correct domain. Evidence anchors: [section 4.4] "we adopt a simple yet effective unsupervised clustering, K-Means, as domain selector to assign model extracted features with K domain centroids." Break condition: If domain boundaries are fuzzy or overlap significantly, K-Means may misclassify samples and hurt accuracy.

## Foundational Learning

- Concept: Transformer multi-head self-attention
  - Why needed here: Personalized prompts are inserted into key/value vectors of MSA layers; understanding attention flow is essential to reason about Prefix-One.
  - Quick check question: In MSA, how does the attention score change when prompts are concatenated to the key matrix but not the query matrix?

- Concept: Prompt tuning vs. full fine-tuning
  - Why needed here: CP-Prompt only updates prompt parameters; knowing the parameter-efficiency trade-off is key to understanding its scalability.
  - Quick check question: What is the approximate parameter count difference between updating 0.22% (CP-Prompt) vs. full model tuning for CLIP?

- Concept: Domain incremental learning (DIL) setting
  - Why needed here: CP-Prompt assumes no task-ID at inference; understanding this constraint explains why twin-prompting is needed.
  - Quick check question: In DIL, how does the absence of task-ID differ from class-incremental learning evaluation?

## Architecture Onboarding

- Component map:
  Pre-trained image/text transformers (CLIP backbone, frozen) -> Common prompt module (inserted before first transformer layer) -> Personalized prompt modules (embedded in each MSA layer) -> K-Means domain selector (trained after all domains seen) -> Classifier head (matrix multiply of frozen transformer outputs)

- Critical path:
  1. Encode image with CNN → concat common prompt → transformer encoding
  2. Encode text labels with vocab/pos → concat personalized prompt → transformer encoding
  3. Compute logits via dot-product of image and text projections
  4. During inference, K-Means assigns domain → selects personalized prompt set

- Design tradeoffs:
  - Common vs. personalized prompt length: longer personalized prompts improve accuracy but increase memory; common prompts must remain short to avoid over-adaptation
  - Number of MSA layers with prompts: more layers capture deeper semantics but risk overfitting
  - K-Means K value: higher K improves domain separation but may overfit small domains

- Failure signatures:
  - Sudden accuracy drop after domain switch → common prompt overgeneralization
  - Slow convergence on new domain → personalized prompts not tuned enough
  - High inference latency → too many prompt tokens or layers activated

- First 3 experiments:
  1. Baseline: Run CP-Prompt on CDDB-Hard with 2 domains, measure AA and AF.
  2. Ablation: Remove common prompts, re-run, compare AA drop.
  3. Hyperparameter sweep: Vary personalized prompt length (8,16,32) and plot AA.

## Open Questions the Paper Calls Out

### Open Question 1
How does the twin-prompt strategy (common + personalized prompts) compare to using only common or only personalized prompts across different domain distributions and task complexities? Basis in paper: [explicit] The authors performed an ablation study showing both common and personalized prompts are crucial, but didn't explore intermediate designs like using only one type or different ratios. Why unresolved: The paper only tested removing one type of prompt entirely, not exploring the spectrum between pure common and pure personalized approaches or their relative importance across domains. What evidence would resolve it: A systematic ablation study varying the ratio of common to personalized prompts and testing across domains with different similarity levels would show optimal balance points.

### Open Question 2
What is the impact of different transformer layer selections for embedding personalized prompts on catastrophic forgetting across various domain shift scenarios? Basis in paper: [explicit] The authors analyzed prompt insertion in different transformer layers but focused on overall performance rather than specifically examining forgetting patterns across layer choices. Why unresolved: The paper showed that prompt insertion layers affect performance generally, but didn't specifically measure how different layer choices impact forgetting rates for previously learned domains. What evidence would resolve it: A detailed analysis measuring forgetting rates for each previously learned domain when inserting personalized prompts at different transformer layers would identify optimal placement strategies.

### Open Question 3
How does the performance of CP-Prompt scale with the number of domains in the incremental learning sequence? Basis in paper: [inferred] The paper tested on three benchmark datasets with relatively small numbers of domains, but didn't explore how performance degrades or remains stable as the number of domains increases significantly. Why unresolved: The experiments used datasets with limited domain numbers, and there's no theoretical analysis of how the twin-prompt strategy would perform in scenarios with dozens or hundreds of domains. What evidence would resolve it: Testing CP-Prompt on datasets with progressively increasing numbers of domains while tracking accuracy and forgetting rates would reveal scalability limitations and potential need for adaptations.

## Limitations
- Implementation-specific uncertainties: The exact implementation of the Prefix-One prompting mechanism in MSA layers is not fully detailed in the paper
- Hyperparameter sensitivity: Specific hyperparameter settings for prompt lengths, learning rates, or number of MSA layers are not provided
- Domain separability assumption: Effectiveness of K-Means clustering assumes well-separated domain embeddings which may not hold for all datasets

## Confidence

**High Confidence**:
- Twin-prompting strategy reduces catastrophic forgetting by separating shared and domain-specific knowledge
- Parameter efficiency claim (0.22% additional parameters) is verifiable through parameter counting
- Performance improvement over state-of-the-art methods (up to 2.3% improvement) is well-documented

**Medium Confidence**:
- Mechanism 2 (Prefix-One improving attention-based domain adaptation) - while experimentally validated, theoretical justification is not fully explained
- K-Means clustering for domain selection - effective in tested scenarios but may not generalize to all domain-incremental learning scenarios

**Low Confidence**:
- Break conditions for each mechanism are theoretical and not empirically validated
- Assumption that inter-domain similarity is sufficient for common prompt generalization is not thoroughly tested

## Next Checks

1. **Implementation verification**: Implement the exact Prefix-One prompting mechanism as described in Section 4.3 and verify that attention scores are computed correctly when prompts are concatenated to key matrices but not query matrices. Compare attention patterns with and without prompts to ensure the mechanism functions as intended.

2. **Hyperparameter sensitivity analysis**: Conduct a systematic ablation study varying personalized prompt length (8, 16, 32 tokens) and the number of MSA layers with prompts (1, 2, 3 layers). Plot accuracy and parameter efficiency trade-offs to identify optimal configurations and verify the robustness of the 0.22% parameter claim.

3. **Domain separability test**: Evaluate the K-Means clustering performance on a held-out validation set from each domain before and after training. Measure clustering accuracy and silhouette scores to quantify how well domains are separated in the embedding space, and test the system's performance when domains are intentionally made more overlapping.