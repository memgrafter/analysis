---
ver: rpa2
title: Understanding the Effects of Human-written Paraphrases in LLM-generated Text
  Detection
arxiv_id: '2411.03806'
source_url: https://arxiv.org/abs/2411.03806
tags:
- paraphrases
- text
- llm-generated
- detection
- dollar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how human-written paraphrases affect the performance
  of LLM-generated text detection. A new dataset, HLPC, is created by combining human-generated
  texts with LLM-generated texts and their paraphrases.
---

# Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection

## Quick Facts
- arXiv ID: 2411.03806
- Source URL: https://arxiv.org/abs/2411.03806
- Authors: Hiu Ting Lau; Arkaitz Zubiaga
- Reference count: 40
- Key outcome: Incorporating human-written paraphrases in LLM-generated text detection datasets significantly improves TPR@1%FPR while potentially trading off AUROC and accuracy

## Executive Summary
This study investigates how human-written paraphrases affect the performance of LLM-generated text detection systems. The researchers created a new HLPC dataset combining human-generated texts, LLM-generated texts, and their paraphrases to systematically evaluate detection performance. Using state-of-the-art detectors including OpenAI RoBERTa and watermark detectors, the study demonstrates that including human-written paraphrases in training data significantly enhances detection accuracy, particularly in challenging scenarios involving recursive paraphrasing attacks. The findings suggest that human-written paraphrases provide valuable training signals that help detectors distinguish between natural and LLM-generated text variations.

## Method Summary
The researchers constructed the HLPC dataset by combining human-written documents and paraphrases with LLM-generated documents and paraphrases from GPT2-XL and OPT-1.3B. They used two paraphrasing methods (DIPPER and BART) to generate multiple paraphrase versions, including recursive paraphrases through 5 rounds. Classification experiments were conducted using OpenAI RoBERTa and watermark detectors to evaluate performance across different scenarios. The evaluation metrics included TPR@1%FPR, AUROC, and accuracy, with experiments designed to test the impact of human-written paraphrases on detection performance under various conditions.

## Key Results
- Including human-written paraphrases significantly improves TPR@1%FPR across all tested scenarios
- Watermarked LLM documents show better detection performance than non-watermarked documents (87.5% vs 62.5% with AUROC>0.85)
- Recursive paraphrasing substantially degrades detection performance, with TPR@1%FPR dropping from 99.8% to 44.8-38.9% after 5 rounds

## Why This Works (Mechanism)

### Mechanism 1
Including human-written paraphrases in training data improves LLM-generated text detection by providing diverse lexical/syntactic variations that the detector can learn to distinguish from LLM outputs. Human paraphrases introduce subtle differences in phrasing, structure, and word choice compared to LLM paraphrases, which helps detectors learn more robust discriminative features. The HLPC dataset combines both human and LLM paraphrases, allowing the model to learn these distinctions.

### Mechanism 2
Watermarking provides a reliable signal for detection that is preserved better through human paraphrasing than through LLM paraphrasing. Watermarking adds detectable statistical patterns to LLM-generated text. When human-written paraphrases are included, the detector can learn to recognize watermark patterns even after paraphrasing, while LLM paraphrases may destroy these patterns more severely.

### Mechanism 3
Recursive paraphrasing attacks degrade detection performance, but including human-written paraphrases mitigates this degradation by providing reference points for "natural" language variation. Multiple rounds of LLM paraphrasing increasingly diverge from natural language patterns. Human paraphrases provide examples of natural variation that help the detector distinguish between natural and artificial degradation patterns.

## Foundational Learning

- **Statistical detection of generated text based on token probability distributions**: The detectors rely on comparing input text to learned distributions from human and LLM text to make classifications. *Quick check: What happens to detection accuracy when input text is very short versus very long?*

- **Watermarking schemes and their detectability**: Watermark detectors use statistical patterns embedded in LLM outputs; understanding how these survive paraphrasing is crucial. *Quick check: How does strong watermarking differ from weak watermarking in terms of token selection and detectability?*

- **Paraphrasing quality metrics and semantic preservation**: Evaluating whether paraphrases maintain meaning while changing form affects detection reliability. *Quick check: What metrics best capture semantic similarity between original text and paraphrases?*

## Architecture Onboarding

- **Component map**: Data collection pipeline -> LLM text generation module (GPT2-XL, OPT-1.3B) -> Paraphrasing module (DIPPER, BART) -> Detection models (OpenAI RoBERTa, watermark detector) -> Evaluation framework
- **Critical path**: Generate LLM text → Generate paraphrases → Combine with human data → Train detector → Evaluate performance
- **Design tradeoffs**: Balance between detection accuracy and false positive rate; trade-off between watermark strength and text naturalness
- **Failure signatures**: Sudden drops in AUROC with increased paraphrasing rounds; high false positive rates on human text; inconsistent performance across datasets
- **First 3 experiments**: 1) Run classification with only H-DOC vs LLM-DOC to establish baseline performance 2) Add H-PP to the dataset and measure changes in AUROC and TPR@1%FPR 3) Test detection performance after 5 rounds of recursive paraphrasing with different paraphrasers

## Open Questions the Paper Calls Out

- **How does the effectiveness of human-written paraphrases vary across different domains or text types (e.g., news articles vs. social media posts)?**: The study uses four datasets representing different domains but does not analyze the impact of H-PP on detection performance across these different domains or text types.

- **What is the optimal number of paraphrasing rounds to include in LLM-generated text detection datasets to balance semantic preservation and detection evasion?**: The paper performs recursive paraphrasing attacks with 5 rounds but does not investigate the optimal number that would provide a good balance between maintaining semantic information and challenging detection models.

- **How do other state-of-the-art LLM-generated text detection tools perform when human-written paraphrases are included in the dataset?**: The paper only uses OpenAI RoBERTa Detector and watermark detector, limiting generalizability to other detection tools like GPTZero.

## Limitations
- The HLPC dataset relies on a fixed set of 150 documents per type, which may not capture full diversity of human and LLM-generated text
- The study uses only two paraphrasers (DIPPER and BART) with unspecified parameter settings, limiting generalizability
- Results may not generalize to other detection architectures beyond OpenAI RoBERTa and watermark detectors tested

## Confidence

- **High Confidence**: The core finding that including human-written paraphrases significantly improves TPR@1%FPR is well-supported by experimental results
- **Medium Confidence**: The trade-off between improved TPR and decreased AUROC/accuracy when using non-watermarked LLM documents is observed but practical implications require further investigation
- **Low Confidence**: The specific quantitative impact of recursive paraphrasing and relative performance differences between DIPPER and BART have higher uncertainty due to limited sample sizes

## Next Checks

1. **Cross-Domain Validation**: Test detection performance on HLPC datasets constructed from different source domains (technical writing, creative fiction, news articles) to assess generalizability

2. **Paraphraser Parameter Sensitivity**: Systematically vary key parameters for DIPPER and BART (beam size, length penalty, diversity settings) to determine sensitivity to paraphrasing quality

3. **Detector Architecture Comparison**: Evaluate the same HLPC datasets using alternative detection approaches (zero-shot classifiers with different scoring mechanisms or ensemble methods) to determine whether effects are detector-specific or represent general phenomena