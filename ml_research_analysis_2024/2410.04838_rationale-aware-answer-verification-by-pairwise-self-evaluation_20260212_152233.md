---
ver: rpa2
title: Rationale-Aware Answer Verification by Pairwise Self-Evaluation
arxiv_id: '2410.04838'
source_url: https://arxiv.org/abs/2410.04838
tags:
- rationale
- reasoning
- answer
- verifier
- rationales
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of unreliable answer verification
  in large language models (LLMs) due to flawed rationales accompanying correct answers.
  The authors propose REPS (Rationale Enhancement through Pairwise Selection), a method
  that iteratively refines rationales using LLM-based pairwise self-evaluation.
---

# Rationale-Aware Answer Verification by Pairwise Self-Evaluation

## Quick Facts
- arXiv ID: 2410.04838
- Source URL: https://arxiv.org/abs/2410.04838
- Authors: Akira Kawabata; Saku Sugawara
- Reference count: 17
- Key outcome: REPS improves Rationale Accuracy up to 53.05% vs 38.90% baseline while maintaining or slightly improving Task Performance

## Executive Summary
This paper addresses the problem of unreliable answer verification in large language models (LLMs) due to flawed rationales accompanying correct answers. The authors propose REPS (Rationale Enhancement through Pairwise Selection), a method that iteratively refines rationales using LLM-based pairwise self-evaluation. Experiments on three reasoning datasets (ARC-Challenge, DROP, and StrategyQA) show that verifiers trained on REPS-selected rationales achieve higher Rationale Accuracy while maintaining or slightly improving Task Performance. However, the analysis reveals that iterative pairwise evaluation may amplify biases toward longer rationales, suggesting the need for careful hyperparameter tuning.

## Method Summary
REPS uses tournament-style pairwise evaluation to select valid rationales from candidate solutions. The generator model produces solutions, which are filtered by answer correctness. Pairwise comparisons are performed iteratively to select the best rationale, which is then used to train the verifier. The method employs Llama-2 7B with temperature 0.7 for both generation and evaluation, using few-shot exemplars for pairwise comparison. Solutions are labeled based on rationale validity rather than just answer correctness, with GPT-4 serving as the ground truth evaluator.

## Key Results
- REPS-selected rationales achieve higher Rationale Accuracy (53.05% vs 38.90% baseline) in StrategyQA
- Verifiers trained on REPS-selected rationales show improvements of 14.1% and 8.8% in ARC and StrategyQA respectively
- Increasing pairwise comparisons (S) and candidates (N) leads to bias amplification favoring longer rationales
- Task Performance improvements are modest (1.2% average across datasets), with ARC actually showing decreased performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training on valid rationales significantly improves a verifier's ability to distinguish between valid and flawed reasoning.
- **Mechanism:** The verifier learns to associate specific patterns of sound reasoning with correct answers, allowing it to detect flaws in rationales even when the final answer is correct.
- **Core assumption:** The validity of a rationale is a stronger signal for correct reasoning than the correctness of the final answer alone.
- **Evidence anchors:**
  - [abstract] "We empirically show that in StrategyQA, only 19% of LLM-generated solutions with correct answers have valid rationales, thus leading to an unreliable verifier."
  - [section 3.4] "Training on the high-quality dataset, where the rationales are validated by GPT-4, results in a verifier that is significantly better at identifying valid reasoning compared to the baseline setting."
- **Break condition:** If the LLM's ability to generate valid rationales does not improve, or if the verifier becomes overly sensitive to minor variations in reasoning style, this mechanism may fail.

### Mechanism 2
- **Claim:** Iterative pairwise evaluation using LLMs can effectively select high-quality rationales from candidate solutions.
- **Mechanism:** By repeatedly comparing pairs of rationales and selecting the better one through majority voting, the method filters out flawed reasoning and converges on the most valid rationale.
- **Core assumption:** LLM-based pairwise comparison is a reliable method for evaluating the validity of rationales.
- **Evidence anchors:**
  - [abstract] "We introduce REPS (Rationale Enhancement through Pairwise Selection), a method for selecting valid rationales from candidates by iteratively applying pairwise self-evaluation using the same LLM that generates the solutions."
  - [section 6.1] "REPS-selected rationales consistently outperform baseline-selected across datasets, with 56-60% win rates."
- **Break condition:** If the LLM's pairwise evaluation becomes biased towards superficial cues like rationale length, or if the iterative process amplifies inherent biases, this mechanism may fail.

### Mechanism 3
- **Claim:** Training verifiers on solutions selected by REPS improves their ability to select valid solutions without compromising task performance.
- **Mechanism:** The verifier is exposed to high-quality rationales during training, enabling it to recognize and prefer valid reasoning during inference.
- **Core assumption:** The quality of training data directly impacts the performance of the verifier.
- **Evidence anchors:**
  - [abstract] "Verifiers trained on solutions selected by REPS outperform those trained using conventional training methods on three reasoning benchmarks (ARC-Challenge, DROP, and StrategyQA)."
  - [section 6.1] "The improvement is particularly significant in ARC and StrategyQA, where the REPS-trained verifier substantially outperforms the baseline, with improvements of 14.1% and 8.8%, respectively."
- **Break condition:** If the REPS selection process fails to improve rationale quality, or if the verifier overfits to the specific style of REPS-selected rationales, this mechanism may fail.

## Foundational Learning

- **Concept:** Understanding the difference between factual correctness and logical consistency in reasoning.
  - **Why needed here:** The verifier must be able to detect both factual errors (incorrect information) and logical errors (flawed reasoning) in the rationales.
  - **Quick check question:** Can you identify a factual error and a logical error in a given rationale?

- **Concept:** Familiarity with pairwise comparison and majority voting methods.
  - **Why needed here:** The REPS method relies on iterative pairwise comparison of rationales to select the best one.
  - **Quick check question:** How would you implement a pairwise comparison system to select the best item from a set of candidates?

- **Concept:** Knowledge of LLM evaluation and bias amplification.
  - **Why needed here:** The study shows that iterative pairwise evaluation can amplify biases in LLM-based evaluators.
  - **Quick check question:** What are some common biases in LLM evaluations, and how might they be amplified through iterative processes?

## Architecture Onboarding

- **Component map:** Answer Generator (Mg) -> REPS Module -> Verifier (Mv) -> GPT-4 Evaluator
- **Critical path:**
  1. Generate candidate solutions using the answer generator.
  2. Apply REPS to select high-quality rationales through iterative pairwise evaluation.
  3. Train the verifier on solutions with rationales selected by REPS.
  4. Use the trained verifier to select the best solution from new candidate solutions.

- **Design tradeoffs:**
  - Tradeoff between rationale quality and diversity: Increasing the number of candidates (N) and pairwise comparisons (S) may improve rationale quality but also amplify biases.
  - Tradeoff between training data size and quality: Using fewer, higher-quality rationales may be more effective than using a larger number of lower-quality rationales.

- **Failure signatures:**
  - If the verifier consistently selects solutions with flawed rationales, it may indicate that the REPS selection process is not effectively filtering out invalid reasoning.
  - If the verifier's performance on the task does not improve despite improvements in rationale accuracy, it may suggest that the verifier is overfitting to the specific style of REPS-selected rationales.

- **First 3 experiments:**
  1. Implement the REPS module and test its ability to select high-quality rationales from a set of candidate solutions.
  2. Train a verifier on solutions selected by REPS and evaluate its performance on a test set.
  3. Compare the performance of the REPS-trained verifier to a baseline verifier trained on solutions labeled solely based on the correctness of the final answer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does REPS perform when evaluated on a more diverse set of reasoning tasks, such as those involving coding or instruction following?
- Basis in paper: [inferred]
- Why unresolved: The paper evaluates REPS on three reasoning datasets (ARC-Challenge, DROP, and StrategyQA), which may not be representative of all reasoning tasks. The authors acknowledge this limitation and suggest that evaluating REPS on a more diverse set of datasets would be valuable to further validate its effectiveness.
- What evidence would resolve it: Evaluating REPS on a wider range of reasoning tasks, including those involving coding or instruction following, would provide evidence of its generalizability and effectiveness across different domains.

### Open Question 2
- Question: How does the performance of REPS change when using larger language models as the base model for both the generator and verifier?
- Basis in paper: [inferred]
- Why unresolved: The paper uses Llama-2 7B as the base model for both the generator and verifier. The authors acknowledge that exploring the impact of model size and architecture on REPS would be valuable, as larger models may provide more accurate and coherent pairwise evaluations, leading to further improvements in the selected rationales.
- What evidence would resolve it: Conducting experiments with larger language models as the base model for both the generator and verifier would provide evidence of how model size affects the performance of REPS.

### Open Question 3
- Question: What methods can be employed to mitigate the bias amplification observed in iterative pairwise evaluation, particularly the preference for longer rationales?
- Basis in paper: [explicit]
- Why unresolved: The paper observes that increasing the number of candidates (N) and pairwise comparisons per round (S) leads to a decrease in Rationale Accuracy and an increase in the average length of selected rationales. The authors suggest treating N and S as hyperparameters that can be tuned using a validation set to find an optimal balance between effective rationale filtering and bias minimization.
- What evidence would resolve it: Developing and evaluating methods to mitigate bias amplification in iterative pairwise evaluation, such as adjusting the presentation order of rationales or incorporating length penalties, would provide evidence of how to improve the reliability of REPS.

## Limitations

- Dataset and Task Specificity: The evaluation focuses on three reasoning datasets, limiting generalizability to other domains.
- Iterative Bias Amplification: The method amplifies biases toward longer rationales, requiring careful hyperparameter tuning.
- Computational Efficiency: The iterative pairwise evaluation process is computationally expensive and scales quadratically with candidates.

## Confidence

**REPS Improves Rationale Accuracy** (High confidence): Empirical results show consistent improvements across all three datasets with clear statistical significance.

**Mechanism 1 - Valid Rationale Training** (Medium confidence): Supported by baseline comparisons but lacks direct ablation studies for the claim that validity is a stronger signal than answer correctness.

**Mechanism 2 - Pairwise Evaluation Reliability** (Medium confidence): Shows effectiveness but sensitivity to hyperparameters and bias amplification indicate it's not universally reliable.

**Mechanism 3 - Performance Without Compromise** (Low confidence): Weakly supported claim - Task Performance actually decreased in ARC and improvements are modest overall.

## Next Checks

1. **Bias Mitigation Testing**: Implement and test bias mitigation strategies for the iterative pairwise evaluation, such as length normalization or alternative scoring metrics, and measure their impact on both rationale accuracy and task performance across varying dataset sizes.

2. **Cross-Domain Generalization Study**: Apply REPS to at least two additional domains (e.g., mathematical problem solving and commonsense reasoning) with different reasoning requirements to assess whether the observed improvements generalize beyond the original three datasets.

3. **Computational Efficiency Analysis**: Measure and compare the computational cost (inference tokens, wall-clock time) of REPS versus baseline approaches across different dataset sizes and candidate pool sizes to quantify the practical deployment implications.