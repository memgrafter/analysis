---
ver: rpa2
title: 'Next-token prediction capacity: general upper bounds and a lower bound for
  transformers'
arxiv_id: '2405.13718'
source_url: https://arxiv.org/abs/2405.13718
tags:
- next-token
- then
- capacity
- prediction
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes fundamental limits on the capacity of decoder-only
  transformer models for next-token prediction tasks. The authors prove both upper
  and lower bounds on the maximum number of distinct context sequences that can be
  interpolated, showing these bounds match up to a constant factor.
---

# Next-token prediction capacity: general upper bounds and a lower bound for transformers

## Quick Facts
- arXiv ID: 2405.13718
- Source URL: https://arxiv.org/abs/2405.13718
- Reference count: 40
- One-line primary result: This paper establishes fundamental limits on the capacity of decoder-only transformer models for next-token prediction tasks.

## Executive Summary
This paper establishes fundamental limits on the capacity of decoder-only transformer models for next-token prediction tasks. The authors prove both upper and lower bounds on the maximum number of distinct context sequences that can be interpolated, showing these bounds match up to a constant factor. The core method involves analyzing one-layer multi-head decoder-only transformers with simplified architectures, revealing that self-attention has an injectivity property that enables effective interpolation through feedforward networks.

## Method Summary
The authors analyze one-layer multi-head decoder-only transformers with simplified architectures to establish capacity bounds. The key insight is that self-attention has an injectivity property - it maps distinct input sequences to distinct scalar outputs. This allows the model to exploit the interpolation power of feedforward networks. The authors also introduce "token-averaging" as a simpler alternative to self-attention that preserves this injectivity property.

## Key Results
- Approximately nζ parameters are both necessary and sufficient to memorize n distinct context sequences with vocabulary size ζ
- The upper and lower bounds match up to a constant factor, establishing optimality
- Transformers can be trained to the entropy lower bound in this parameter regime

## Why This Works (Mechanism)
The injectivity of self-attention is proven using properties of the softmax function and the transcendence of exp(1). This injectivity allows distinct input sequences to be mapped to distinct scalar outputs, which can then be processed by feedforward networks. The rank properties of feedforward networks with various activation functions are characterized, which is crucial for the interpolation results. The token-averaging mechanism provides a simpler alternative that preserves this injectivity property.

## Foundational Learning
1. Self-attention injectivity (why needed: core to proving capacity bounds; quick check: verify softmax properties and exponential transcendence)
2. Feedforward network ranks (why needed: determines interpolation capacity; quick check: verify rank calculations for different activation functions)
3. Interpolation theory (why needed: establishes memorization bounds; quick check: verify consistency between upper and lower bounds)

## Architecture Onboarding

**Component Map:** Input sequences -> Self-attention/Token-averaging -> Feedforward networks -> Output predictions

**Critical Path:** The injectivity property of self-attention/token-averaging enables distinct mapping of input sequences, which feedforward networks then interpolate to produce the next-token predictions.

**Design Tradeoffs:** The paper focuses on one-layer architectures for theoretical tractability, sacrificing some practical applicability for mathematical rigor. The simplified architectures allow for clean proofs but may not capture all complexities of deeper models.

**Failure Signatures:** If self-attention loses its injectivity property (e.g., with certain initialization schemes), the capacity bounds would no longer hold. Similarly, if feedforward networks cannot achieve sufficient rank, interpolation would fail.

**First Experiments:**
1. Verify self-attention injectivity with random input sequences
2. Test feedforward network rank properties with different activation functions
3. Validate the token-averaging mechanism against standard self-attention

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses on one-layer multi-head decoder-only transformers with simplified architectures
- Proof techniques rely on specific activation functions and may not extend to more complex architectures
- Token-averaging mechanism has not been empirically validated as a practical replacement

## Confidence

**High Confidence:** The upper and lower bound matching results (Theorem 1.1 and 1.2) are mathematically rigorous and the proof techniques are sound.

**Medium Confidence:** The injectivity property of self-attention (Proposition 4.2) holds under the stated assumptions but may not generalize to all practical implementations.

**Medium Confidence:** The numerical experiments support the theoretical claims but use simplified architectures that don't fully represent modern transformer models.

## Next Checks
1. Extend the theoretical analysis to multi-layer transformers with residual connections to determine if the nζ bound still holds
2. Empirically test the token-averaging mechanism as a replacement for self-attention in practical next-token prediction tasks to validate the theoretical claims
3. Characterize the capacity bounds for encoder-decoder architectures used in tasks beyond next-token prediction, such as machine translation or text summarization