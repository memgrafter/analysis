---
ver: rpa2
title: Set-Based Training for Neural Network Verification
arxiv_id: '2401.14961'
source_url: https://arxiv.org/abs/2401.14961
tags:
- neural
- training
- set-based
- gradient
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of training robust neural networks
  that are easier to formally verify against adversarial attacks. The key challenge
  is that existing robust training methods either produce models that are hard to
  verify or have poor performance due to large approximation errors.
---

# Set-Based Training for Neural Network Verification

## Quick Facts
- arXiv ID: 2401.14961
- Source URL: https://arxiv.org/abs/2401.14961
- Reference count: 40
- Key outcome: Novel set-based training approach that uses gradient sets to improve both empirical robustness and formal verification of neural networks.

## Executive Summary
This paper addresses the challenge of training neural networks that are both empirically robust to adversarial attacks and amenable to formal verification. Existing robust training methods either produce models that are hard to verify or have poor performance due to large approximation errors. The authors introduce a novel set-based training approach that, for the first time, uses a gradient set for training. This method encloses the output set of the neural network and computes gradients for each possible output, allowing direct control over the size of the output enclosure. By choosing gradients that point toward the center of the enclosure, the approach simultaneously improves robustness and simplifies formal verification.

## Method Summary
The method implements set-based forward propagation using zonotopes with efficient interval enclosure, Minkowski sum, and affine map operations for linear layers, and an image enclosure method for nonlinear layers. Set-based backpropagation computes the gradient set for each layer, and the weight/bias update rule uses the outer product between the gradient set and input set. Training employs a set-based loss function that combines accuracy loss and F-radius for robustness. The approach uses fast, batch-wise, and differentiable set propagation with analytical solutions for approximation errors of nonlinear activations.

## Key Results
- Achieves competitive performance compared to state-of-the-art robust training methods on MNIST, CIFAR-10, SVHN, and TinyImageNet
- Enables fast polynomial-time verification through zonotopic forward propagation
- Balances empirical performance and verified robustness through the F-radius-based loss function
- Reduces output enclosure size while maintaining or improving clean accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Set-based training reduces the size of output enclosures, which improves both empirical robustness and formal verification ease.
- **Mechanism**: By computing a gradient set instead of a single gradient, the method can choose gradients pointing toward the center of the output zonotope, shrinking its F-radius.
- **Core assumption**: The F-radius is a valid proxy for the "size" of the output set in all directions, and minimizing it improves robustness.
- **Evidence anchors**: [abstract]: "Small output enclosures increase the robustness of a neural network and, at the same time, simplify its formal verification." [section]: "The negative gradients of the F-radius of a zonotope point toward the center of the zonotope (Fig. 4); hence, minimizing the F-radius of a zonotope reduces the size of the zonotope."
- **Break condition**: If the F-radius does not correlate with adversarial robustness or verification tightness, the method fails.

### Mechanism 2
- **Claim**: Zonotopic forward propagation with analytical approximation errors is faster and tighter than interval or sampling-based methods.
- **Mechanism**: Linear approximations of ReLU, tanh, and sigmoid are constructed analytically, and their approximation errors are computed at specific points, enabling efficient GPU batch computation.
- **Core assumption**: The analytical error computation is correct and the linear approximation is tight enough to preserve robustness.
- **Evidence anchors**: [section]: "We derive analytical solutions for the approximation errors of linear approximations for three typical activation functions: rectified linear unit (ReLU), hyperbolic tangent, and logistic sigmoid." [section]: "Our image enclosure no longer uses a polynomial regression or requires sampling to compute the approximation errors."
- **Break condition**: If the approximation errors are too large, the output enclosures become too conservative, negating the benefit.

### Mechanism 3
- **Claim**: The set-based loss generalizes TRADES by replacing the adversarial boundary loss with an F-radius term.
- **Mechanism**: The loss is L(t,Y) = (1-τ)L(t,c) + τ/ϵ‖Y‖F, balancing accuracy (via standard loss on the center) and robustness (via F-radius).
- **Core assumption**: The F-radius term in the loss effectively approximates the adversarial boundary loss while being efficiently computable.
- **Evidence anchors**: [section]: "Our set-based loss generalizes the well-established tradeoff-loss (Zhang et al., 2019, Eq. 5), which combines a standard training loss with a boundary loss." [section]: "The F-radius in our set-based loss and the boundary loss in equation 9 have the same goal of training the robustness of the neural network."
- **Break condition**: If the weighting τ is poorly chosen, the model may favor accuracy over robustness or vice versa, harming performance.

## Foundational Learning

- **Concept**: Zonotopes as convex set representations for output enclosures.
  - Why needed here: Zonotopes allow efficient affine mapping and Minkowski sum, which are required for forward propagation through neural network layers.
  - Quick check question: Given a center c and generator matrix G, what is the zonotope representation of an interval [-1, 1] in n dimensions?

- **Concept**: Set-based forward and backpropagation through linear and nonlinear layers.
  - Why needed here: To compute gradients of the loss with respect to zonotopic outputs and propagate them backward through the network.
  - Quick check question: How does the gradient of a zonotope output w.r.t. its center differ from the gradient w.r.t. its generators?

- **Concept**: F-radius as a metric for the "size" of a zonotope.
  - Why needed here: It is used both as a robustness penalty in the loss and as a differentiable term for gradient computation.
  - Quick check question: Write the formula for the F-radius of a zonotope Z = ⟨c, G⟩ and explain why it is used instead of volume.

## Architecture Onboarding

- **Component map**: Input set constructor (ℓ∞-ball or zonotope from attacks) -> Zonotope forward propagation (linear layers + analytical image enclosure for nonlinearities) -> Set-based loss computation (center loss + F-radius term) -> Zonotope gradient set computation -> Set-based backpropagation (through linear and nonlinear layers) -> Parameter update (using aggregated gradients from the gradient set)

- **Critical path**: Input set → Forward propagation → Loss & gradient set → Backpropagation → Parameter update

- **Design tradeoffs**:
  - Zonotopes vs intervals: Zonotopes give tighter enclosures but more generators; intervals are simpler but looser.
  - Number of generators: More generators → tighter but slower; fewer generators → faster but looser.
  - τ in loss: Controls accuracy-robustness tradeoff; too high → poor accuracy, too low → poor robustness.

- **Failure signatures**:
  - Output enclosures remain large → poor verification results.
  - Training diverges or accuracy collapses → τ or learning rate mis-tuned.
  - Backpropagation errors → incorrect analytical error formulas or implementation bugs.

- **First 3 experiments**:
  1. Verify that the analytical error computation for ReLU matches a brute-force numerical check on small intervals.
  2. Compare forward propagation times and output set sizes for ℓ∞-ball vs zonotope input sets on a simple 2-layer network.
  3. Train a small CNN on MNIST with τ=0.1 and confirm that the F-radius decreases over epochs while clean accuracy stays above 95%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the gradient set training approach compare in performance to methods that use adversarial attacks for computing the gradient?
- Basis in paper: [explicit] The paper discusses how their method uses a gradient set for training, unlike previous methods that use single gradients based on adversarial attacks.
- Why unresolved: The paper shows that their method achieves competitive performance but doesn't provide a direct comparison with adversarial attack-based gradient methods.
- What evidence would resolve it: A head-to-head comparison of set-based training against adversarial attack-based training on the same networks and datasets, measuring both empirical and verified robustness.

### Open Question 2
- Question: What is the theoretical relationship between the F-radius of the output zonotope and the Lipschitz constant of the network?
- Basis in paper: [explicit] The paper mentions that the Lipschitz constant is another metric for robustness and compares Lipschitz constants of different training methods, but doesn't establish a theoretical connection.
- Why unresolved: While the paper shows empirical correlation between smaller F-radius and smaller Lipschitz constant, it doesn't provide theoretical bounds or proofs.
- What evidence would resolve it: Mathematical proofs or bounds relating the F-radius of output zonotopes to the Lipschitz constant of the network, potentially under specific assumptions about the network architecture.

### Open Question 3
- Question: How does the choice of input set (ℓ∞-ball vs. zonotope constructed from adversarial attacks) affect the convergence speed and final performance of set-based training?
- Basis in paper: [explicit] The paper mentions using zonotopic input sets constructed from adversarial attacks but doesn't extensively study the impact on training dynamics.
- Why unresolved: The paper only briefly compares the two approaches and doesn't analyze how they affect training convergence or final model quality.
- What evidence would resolve it: Detailed analysis of training curves, convergence rates, and final performance metrics comparing different input set constructions across multiple network architectures and datasets.

## Limitations
- The analytical error computation for nonlinear activations may introduce conservatism that grows with network depth
- Performance evaluation is limited to feed-forward neural networks without exploration of recurrent or transformer architectures
- The relationship between F-radius minimization and formal verification tightness is empirically demonstrated but not theoretically proven

## Confidence
- **Medium**: The mechanism linking F-radius minimization to robustness and verification is theoretically sound, but empirical validation across diverse architectures is limited
- **Medium**: The proposed loss function's relationship to TRADES is logical but not rigorously proven
- **Medium**: The fast verification claim is compelling but only demonstrated on zonotopic models

## Next Checks
1. Reproduce the analytical error computation for ReLU on a simple interval and compare with brute-force numerical integration
2. Evaluate the method's performance when the input set is a zonotope constructed from adversarial attacks (Appendix A) rather than an ℓ∞-ball
3. Benchmark against non-zonotopic robust training methods (e.g., TRADES, adversarial training) on CIFAR-10 to assess the accuracy-robustness tradeoff