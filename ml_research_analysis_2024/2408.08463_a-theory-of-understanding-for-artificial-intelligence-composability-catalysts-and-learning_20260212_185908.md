---
ver: rpa2
title: 'A theory of understanding for artificial intelligence: composability, catalysts,
  and learning'
arxiv_id: '2408.08463'
source_url: https://arxiv.org/abs/2408.08463
tags:
- understanding
- subject
- inputs
- catalysts
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a universal framework for analyzing understanding\
  \ in AI systems based on composability\u2014the ability to process inputs into satisfactory\
  \ outputs. The framework characterizes understanding via a subject's compositions\
  \ and introduces catalysts as inputs that enhance output quality."
---

# A theory of understanding for artificial intelligence: composability, catalysts, and learning

## Quick Facts
- arXiv ID: 2408.08463
- Source URL: https://arxiv.org/abs/2408.08463
- Authors: Zijian Zhang; Sara Aronowitz; Alán Aspuru-Guzik
- Reference count: 11
- Primary result: Proposes a universal framework characterizing AI understanding through composability and catalysts

## Executive Summary
This paper introduces a universal framework for analyzing understanding in AI systems based on composability—the ability to process inputs into satisfactory outputs. The framework characterizes understanding through observable behavior rather than internal mental states, making it applicable across different subjects including humans, AI systems, animals, and institutions. By introducing catalysts as inputs that enhance output quality and decomposing subjects into primitive subjects and inner catalysts, the authors link learning ability to the capacity to compose inputs into these internal enhancers. Their analysis suggests that LLMs' autocatalytic property—using their own outputs as catalysts—offers a foundation for overcoming current limitations in AI understanding, though significant gaps remain in universality and scale compared to human learners.

## Method Summary
The framework defines understanding through composability, where a subject's understanding of an object is characterized by its ability to process relevant inputs into satisfactory outputs as assessed by a verifier. The method involves decomposing subjects into primitive subjects and inner catalysts, with learning ability defined as the capacity to compose inputs into inner catalysts. The approach uses observable behavior rather than internal mental states, making it universally applicable. The authors apply this framework to analyze LLMs, noting their autocatalytic property of using outputs as catalysts, and identify limitations in current AI systems regarding universality and scale of understanding.

## Key Results
- Understanding can be characterized as composability—processing relevant inputs into satisfactory outputs
- Learning ability is defined as composing inputs into inner catalysts that enhance future compositions
- LLMs' autocatalytic property (using outputs as catalysts) provides a foundation for improving AI understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Understanding is characterized by composability - the ability to process inputs into satisfactory outputs from a verifier's perspective.
- Mechanism: The framework defines understanding through observable behavior (compositions) rather than internal mental states, making it universally applicable across different subjects.
- Core assumption: A verifier can reliably assess whether outputs are "satisfactory" and whether inputs are "relevant" to the understood object.
- Evidence anchors:
  - [abstract] "We suggest characterizing its understanding of an object in terms of its ability to process (compose) relevant inputs into satisfactory outputs from the perspective of a verifier."
  - [section] "Definition 2 (Understanding as composability) A subject's understanding of an object O, in the view of a verifier V, can be fully characterized by the set S, which contains all the tuples (⃗I, Iout)..."
  - [corpus] Weak - no direct corpus evidence found for this specific framework.
- Break condition: If verifiers cannot agree on what constitutes "satisfactory" outputs or "relevant" inputs, the framework loses practical utility.

### Mechanism 2
- Claim: Learning ability is defined as the ability to compose inputs into inner catalysts that enhance future compositions.
- Mechanism: By decomposing subjects into primitive subjects and inner catalysts, learning becomes observable as the subject's ability to generate and utilize these internal enhancements.
- Core assumption: Inner catalysts exist as identifiable components within subjects that can be isolated and analyzed for their contribution to understanding.
- Evidence anchors:
  - [abstract] "we propose methods for analyzing the inputs that enhance output quality in compositions, which we call catalysts"
  - [section] "Definition 4 (Subject decomposition) Regarding a subject's understanding of the object O, C is an inner catalyst if 1. C is a part of the subject; 2. C can be regarded as a catalyst that is composed by a primitive subject who is a part of the subject."
  - [corpus] Weak - corpus contains related papers on AI agents but not this specific decomposition framework.
- Break condition: If inner catalysts cannot be practically identified or if the decomposition process is arbitrary, the learning ability characterization becomes untestable.

### Mechanism 3
- Claim: LLMs' autocatalytic property (using their own outputs as catalysts) provides a foundation for overcoming current limitations in AI understanding.
- Mechanism: The ability to treat outputs as inputs for future compositions creates a self-reinforcing cycle that enables learning and understanding improvement without external intervention.
- Core assumption: Outputs generated by LLMs can meaningfully serve as catalysts for subsequent compositions, enhancing the quality of future outputs.
- Evidence anchors:
  - [abstract] "Our analysis indicates that models capable of generating outputs that can function as their own catalysts, such as language models, establish a foundation for potentially overcoming existing limitations in AI understanding."
  - [section] "Observation 1 LLMs can use their outputs as catalysts."
  - [corpus] Weak - corpus contains related papers but no direct evidence of this specific autocatalytic mechanism in LLMs.
- Break condition: If LLM outputs cannot consistently function as useful catalysts, or if the autocatalytic process leads to compounding errors, the proposed foundation for improvement fails.

## Foundational Learning

- Concept: Composition as a universal process
  - Why needed here: Understanding is defined through the subject's ability to compose inputs into outputs, making this concept fundamental to the entire framework.
  - Quick check question: Can you identify a composition in a simple system, such as a calculator taking numbers and operators as inputs and producing results as outputs?

- Concept: Verifier perspective
  - Why needed here: The framework relies on external assessment of understanding rather than internal mental states, making the verifier's role critical.
  - Quick check question: How would two different verifiers with different standards assess the same subject's understanding of the same object?

- Concept: Catalysts as understanding enhancers
  - Why needed here: Catalysts explain how understanding can be improved and how learning occurs through the acquisition and utilization of these enhancers.
  - Quick check question: What could serve as a catalyst for improving someone's understanding of bicycle mechanics - would a diagram, a video, or hands-on experience be most effective?

## Architecture Onboarding

- Component map: Subject -> Inputs -> Compositions -> Outputs -> Verifier assessment
- Critical path:
  1. Define the object of understanding and the subject to be assessed
  2. Identify the set of inputs that a verifier considers relevant to the object
  3. Observe the subject's compositions of these inputs into outputs
  4. Analyze whether catalysts exist and how they enhance output quality
  5. Decompose the subject to identify primitive subjects and inner catalysts
  6. Assess learning ability through the subject's capacity to generate and utilize inner catalysts

- Design tradeoffs:
  - Universality vs. specificity: A more universal framework applies to more subjects but may provide less specific insights
  - Observer vs. participant: Treating the verifier as external maintains objectivity but may miss internal mechanisms
  - Behavioral vs. structural: Focusing on behavior enables assessment but may overlook important internal structures

- Failure signatures:
  - Verifier disagreement: Different verifiers assess the same understanding differently
  - Catalyst identification failure: Cannot reliably identify which inputs function as catalysts
  - Decomposition ambiguity: Cannot clearly separate primitive subjects from inner catalysts
  - Autocatalysis breakdown: Outputs cannot effectively serve as catalysts for future compositions

- First 3 experiments:
  1. Implement a simple composition system where a subject (e.g., rule-based program) processes various inputs into outputs, then test how different verifiers assess the understanding demonstrated
  2. Create a multi-stage composition where outputs from one stage serve as inputs for the next, measuring whether this autocatalytic process improves output quality
  3. Decompose a complex subject (e.g., a software agent) into primitive components and inner catalysts, then test how modifying each affects overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we formally characterize and measure the universality of an AI system's understanding across different input modalities?
- Basis in paper: [explicit] The paper emphasizes universality as a key characteristic of understanding, noting that humans can process multi-modal inputs while current AI systems struggle with cross-modal understanding.
- Why unresolved: The paper defines universality conceptually but doesn't provide a concrete metric or framework for quantifying it across different subjects and input types.
- What evidence would resolve it: A proposed framework or set of metrics that can quantify and compare the universality of different subjects' understanding across various input modalities and types.

### Open Question 2
- Question: What are the key architectural differences between AI systems that enable autocatalysis versus those that don't?
- Basis in paper: [explicit] The paper identifies autocatalysis (using outputs as catalysts) as a distinguishing feature of LLMs and links it to learning ability, but doesn't explain the underlying architectural requirements.
- Why unresolved: While the paper observes that LLMs exhibit autocatalysis, it doesn't investigate the specific architectural features that enable this capability or why other models lack it.
- What evidence would resolve it: A comparative analysis of LLM architectures versus other models that identifies the specific components or design choices that enable autocatalytic behavior.

### Open Question 3
- Question: How can we design benchmarks that effectively test the scale of understanding for AI systems beyond simple input length measurements?
- Basis in paper: [explicit] The paper identifies scale as a critical dimension of understanding but notes that current evaluation methods (like input length limits) are inadequate proxies for measuring true scale capabilities.
- Why unresolved: The paper highlights that humans can process long inputs meaningfully while AI systems struggle with coherence in long contexts, but doesn't propose better methods for evaluating scale of understanding.
- What evidence would resolve it: Development and validation of new benchmark tasks that require maintaining coherence, building knowledge networks, and demonstrating understanding across truly large-scale information domains.

## Limitations

- Verifier dependency: The framework relies on external verifiers whose subjective assessments may lead to inconsistent characterizations of understanding
- Catalyst identification challenges: Practical identification and isolation of catalysts from regular inputs remains unclear and context-dependent
- Scale and universality gaps: Significant gaps remain in comparing understanding across different scales and domains, with current AI systems lagging behind human learners

## Confidence

- High Confidence: The core definitional framework of understanding as composability is well-grounded in the paper's theoretical construction
- Medium Confidence: The concept of catalysts and their role in enhancing understanding has theoretical merit but lacks extensive empirical validation
- Low Confidence: The specific application to LLMs and their autocatalytic properties, while promising, is based on limited evidence in the current work

## Next Checks

1. **Verifier Consistency Test**: Conduct an experiment where multiple independent verifiers assess the same subject's understanding of a specific object. Measure inter-rater reliability and identify conditions under which verifier disagreement occurs.

2. **Catalyst Isolation Experiment**: Design a controlled experiment where subjects process identical inputs with and without hypothesized catalysts. Quantify the enhancement effect and test whether identified catalysts remain consistent across different subjects and contexts.

3. **Scale Comparison Study**: Compare understanding across subjects operating at different scales (e.g., simple rule-based systems vs. complex neural networks vs. humans) using the same framework. Identify specific gaps in universality and measure how the framework performs across these different scales.