---
ver: rpa2
title: 'WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only
  Supervised Text Spotting'
arxiv_id: '2407.19507'
source_url: https://arxiv.org/abs/2407.19507
tags:
- text
- image
- spotting
- learning
- supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles transcription-only supervised text spotting,
  where only text transcriptions are provided without location annotations. The authors
  propose WeCromCL, which formulates anchor point detection as weakly supervised cross-modality
  contrastive learning.
---

# WeCromCL: Weakly Supervised Cross-Modality Contrastive Learning for Transcription-only Supervised Text Spotting

## Quick Facts
- **arXiv ID**: 2407.19507
- **Source URL**: https://arxiv.org/abs/2407.19507
- **Reference count**: 40
- **Primary result**: Achieves F-measures up to 90.5% on ICDAR 2013, 88.6% on ICDAR 2015, 85.8% on Total-Text, and 78.9% on CTW1500 using transcription-only supervision

## Executive Summary
This work addresses transcription-only supervised text spotting, where only text transcriptions are provided without location annotations. The authors propose WeCromCL, which formulates anchor point detection as weakly supervised cross-modality contrastive learning. Unlike holistic approaches, WeCromCL performs atomistic contrastive learning to model character-wise appearance consistency between transcriptions and their correlated image regions. A soft activation map modeling mechanism learns anchor points without ground truth locations. Experiments on four benchmarks show WeCromCL outperforms previous methods, achieving state-of-the-art F-measures. The detected anchor points serve as pseudo labels for single-point supervised text spotting, demonstrating effective weakly supervised text detection and recognition.

## Method Summary
WeCromCL addresses transcription-only supervised text spotting by detecting anchor points through weakly supervised cross-modality contrastive learning. The method first extracts character-wise embeddings from transcriptions using a transformer encoder, then measures cosine similarity between these embeddings and each pixel's visual features through cross-attention to produce soft activation maps. The highest values in these maps indicate anchor points. The approach uses negative-sample mining to enhance contrastive learning by introducing challenging negative pairs. The detected anchor points serve as pseudo location labels to guide the learning of a single-point supervised text spotter. The two-stage pipeline simplifies the problem by decomposing it into manageable subproblems: first detecting accurate anchor points, then using these as pseudo labels for text spotting.

## Key Results
- Achieves F-measures of 90.5%, 88.6%, 85.8%, and 78.9% on ICDAR 2013, ICDAR 2015, Total-Text, and CTW1500 respectively
- Outperforms previous methods including TSRep, PseudoBox, and IFL by significant margins across all benchmarks
- Anchor point detection accuracy close to ground truth (82.1/76.1/68.8 vs 82.9/77.0/69.8 for three datasets)
- Ablation studies confirm the importance of character-wise modeling and negative-sample mining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WeCromCL learns accurate anchor points by modeling character-wise appearance consistency between transcriptions and image regions.
- Mechanism: The model uses a character-wise text encoder to generate distinct embeddings for each character, then applies cross-modality cross-attention to measure cosine similarity between these embeddings and each pixel's visual features. This produces a soft activation map where the highest value indicates the anchor point.
- Core assumption: Character-level embeddings capture distinctive visual patterns that correlate with their appearance in scene images, and cross-attention can effectively localize these patterns.
- Evidence anchors:
  - [abstract] "our WeCromCL conducts atomistic contrastive learning to model the character-wise appearance consistency between a text transcription and its correlated region"
  - [section] "We devise the text encoder in the similar way as oCLIP [41] so that the encoded text embeddings 1) are distinguishable between different characters and 2) contain the temporal sequence information among characters"
- Break condition: If character embeddings fail to capture distinctive visual patterns, or if cross-attention cannot effectively correlate character features with image pixels, anchor point accuracy will degrade significantly.

### Mechanism 2
- Claim: Negative-sample mining enhances contrastive learning by providing challenging negative pairs that improve discriminative learning.
- Mechanism: During training, unpaired transcriptions and images are randomly sampled as negative pairs. This forces the model to learn discriminative features that distinguish between different transcriptions rather than collapsing to uniform patterns.
- Core assumption: Random sampling of negative pairs provides sufficient diversity to prevent model collapse while maintaining computational efficiency.
- Evidence anchors:
  - [section] "we devise a negative-sample mining scheme to introduce more challenging negative pairs and thereby enhance the modeling robustness of WeCromCL"
  - [section] "we observe that randomly selecting unpaired transcriptions from the training set can also yield similar performance gain compared to hard-sample mining scheme"
- Break condition: If negative pairs are too easy (highly dissimilar) or too hard (confusingly similar), the contrastive learning signal becomes ineffective and model performance degrades.

### Mechanism 3
- Claim: The two-stage approach (anchor detection + single-point spotting) simplifies the transcription-only supervised problem by decomposing it into manageable subproblems.
- Mechanism: First stage uses WeCromCL to generate pseudo location labels by detecting anchor points. Second stage uses these pseudo labels to train a single-point supervised text spotter, which only needs one point per text instance rather than full boundary annotations.
- Core assumption: Anchor points generated by WeCromCL are accurate enough to serve as effective pseudo labels for training the text spotter in the second stage.
- Evidence anchors:
  - [abstract] "The detected anchor points by WeCromCL are further used as pseudo location labels to guide the learning of text spotting"
  - [section] "Leveraging the predicted anchor points by our WeCromCL as pseudo location labels, we learn an effective single-point supervised text spotter"
- Break condition: If WeCromCL anchor detection accuracy is too low, the pseudo labels will be poor quality and the second-stage spotter will fail to learn effectively.

## Foundational Learning

- Concept: Cross-modal contrastive learning
  - Why needed here: The task requires learning associations between text transcriptions and their visual appearance in images without location annotations, which naturally fits a contrastive learning framework.
  - Quick check question: What is the difference between holistic and atomistic contrastive learning, and why does atomistic learning work better for this task?

- Concept: Attention mechanisms and cross-attention
  - Why needed here: The model needs to measure similarity between text embeddings and every pixel in the image to create activation maps, which requires attention-based operations.
  - Quick check question: How does cross-attention between text queries and image key-value pairs enable the creation of activation maps for anchor point detection?

- Concept: Transformer architectures and positional embeddings
  - Why needed here: The character-wise text encoder uses transformer layers to model temporal relationships between characters, and positional embeddings are needed to capture character order information.
  - Quick check question: Why are positional embeddings necessary when encoding text transcriptions for this task?

## Architecture Onboarding

- Component map:
  Image Encoder (BiFPN + Deformable Transformer) -> Character-wise Text Encoder (Embedding Matrix + Positional Embeddings + Transformer Encoder) -> Soft Activation Map Modeling (Cross-Modality Cross-Attention) -> Contrastive Learning Head (Negative Sample Mining) -> Anchor Point Extraction (Peak detection in activation maps) -> Single-point Text Spotter (SRSTS-A or SPTS)

- Critical path: Text transcription → Character embeddings → Cross-attention with image features → Activation map → Anchor point → Pseudo label → Single-point spotter training

- Design tradeoffs:
  - Soft activation maps vs binary maps: Soft maps provide richer gradient information but require peak detection for anchor points
  - Character-wise vs token-wise encoding: Character-wise captures fine-grained visual patterns but increases sequence length
  - Random vs hard negative mining: Random mining is computationally efficient but may be less challenging

- Failure signatures:
  - Poor anchor point accuracy: Check character embeddings quality and cross-attention attention weights
  - Slow convergence: Verify learning rate schedule and negative sample diversity
  - Model collapse: Ensure sufficient negative pairs and proper temperature scaling in contrastive loss

- First 3 experiments:
  1. Ablation test: Replace character-wise encoder with token-wise encoder to verify the importance of character-level modeling
  2. Ablation test: Remove negative-sample mining to quantify its impact on model robustness
  3. Visualization test: Generate and inspect activation maps for various text instances to verify correct localization behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of pseudo labels generated by WeCromCL compare to ground truth anchor points in terms of actual impact on final spotting performance?
- Basis in paper: [explicit] "Withα representing the perturbation degree, whereα is set to 0.3, the offset from ground truth follows a Gaussian distribution within 0.3 times the text box size. Results show our method's accuracy is close to using ground truth (82.1/76.1/68.8 vs 82.9/77.0/69.8), confirming the accuracy of our pseudo labels."
- Why unresolved: The paper mentions the comparison but doesn't provide detailed quantitative analysis of how much the performance gap affects downstream tasks or if there's a threshold of anchor point accuracy beyond which further improvements become marginal.
- What evidence would resolve it: A detailed ablation study showing the relationship between anchor point localization error and final spotting F-measure across different error ranges.

### Open Question 2
- Question: What is the performance limitation when scaling WeCromCL to extremely long text sequences (e.g., sentences or paragraphs) rather than individual words?
- Basis in paper: [inferred] The paper focuses on word-level spotting and mentions "long text" as a challenging case, but doesn't explore performance degradation with increasing text length.
- Why unresolved: The character-wise encoding approach might face scalability issues with longer sequences, but the paper doesn't investigate this limitation or propose solutions for handling longer text instances.
- What evidence would resolve it: Experimental results showing performance degradation curves as text length increases, along with proposed architectural modifications for handling longer sequences.

### Open Question 3
- Question: How does WeCromCL's performance vary across different languages and scripts (e.g., Chinese, Arabic, Devanagari) that have different character densities and writing directions?
- Basis in paper: [inferred] The paper mentions multi-lingual scene text detection challenges but doesn't specifically test cross-lingual performance or discuss how the character-wise approach generalizes to non-Latin scripts.
- Why unresolved: The character embedding matrix and cross-attention mechanism might have different effectiveness across scripts with varying character complexities and spatial arrangements.
- What evidence would resolve it: Comprehensive benchmarking on multi-script datasets with performance analysis for different language families and writing systems.

## Limitations

- Limited evaluation scope to Latin-script datasets only, with unknown performance on non-Latin scripts and handwritten text
- Anchor point quality and its correlation with final spotting performance not thoroughly analyzed
- Computational overhead of two-stage approach not compared to end-to-end alternatives
- Negative sample mining impact not comprehensively studied beyond basic ablation

## Confidence

**High Confidence**: The core mechanism of using cross-modality contrastive learning to model character-wise appearance consistency is well-supported by ablation studies and achieves state-of-the-art results on benchmark datasets.

**Medium Confidence**: The claim that atomistic contrastive learning outperforms holistic approaches is supported by results but lacks detailed comparative analysis of failure modes and edge cases.

**Low Confidence**: The scalability of the approach to non-Latin scripts and extreme text scenarios (very small text, heavy occlusion) is largely untested.

## Next Checks

1. **Cross-script Generalization Test**: Evaluate WeCromCL on non-Latin script datasets (e.g., ICDAR 2019 MLT for multilingual text, Chinese text datasets) to verify whether character-wise contrastive learning generalizes across different writing systems.

2. **Anchor Point Quality Analysis**: Conduct a detailed correlation study between anchor point localization accuracy and final text spotting performance. Systematically vary the quality of pseudo labels to determine the minimum anchor point accuracy required for effective single-point spotter training.

3. **Negative Sampling Strategy Ablation**: Perform comprehensive ablations comparing random negative mining with hard negative mining, curriculum learning approaches, and other sampling strategies. Measure not only final performance but also convergence speed, training stability, and sensitivity to temperature scaling.