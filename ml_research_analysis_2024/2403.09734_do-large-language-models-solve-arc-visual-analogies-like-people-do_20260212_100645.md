---
ver: rpa2
title: Do Large Language Models Solve ARC Visual Analogies Like People Do?
arxiv_id: '2403.09734'
source_url: https://arxiv.org/abs/2403.09734
tags:
- llms
- arxiv
- children
- errors
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared human and large language model (LLM) performance
  on a child-friendly version of the Abstraction Reasoning Corpus (ARC), a visual
  analogical reasoning test. Both children and adults outperformed most LLMs on these
  tasks.
---

# Do Large Language Models Solve ARC Visual Analogies Like People Do?

## Quick Facts
- **arXiv ID**: 2403.09734
- **Source URL**: https://arxiv.org/abs/2403.09734
- **Reference count**: 7
- **Primary result**: Humans outperform LLMs on simplified visual analogy tasks; LLMs rely on less human-like strategies and struggle with underlying concepts

## Executive Summary
This study compares human and large language model (LLM) performance on a child-friendly version of the Abstraction Reasoning Corpus (ARC), a visual analogical reasoning test. Both children and adults outperformed most LLMs on these tasks. Error analysis revealed that LLMs and young children share a common "fallback" strategy of copying input matrices, but LLMs made more "matrix" errors (simple combinations of input matrices) while humans made more "concept" errors (partially correct solutions). This suggests that LLMs rely on less human-like processes and struggle to grasp underlying concepts, highlighting the need for further research to improve AI reasoning capabilities.

## Method Summary
The study used two sets of 8 simplified ARC items each: KidsARC-Simple (3x3 grids for 4-8 year-olds) and KidsARC-Concept (5x5 matrices for 8+ year-olds). Human participants (144 for Simple, 88 for Concept) solved tasks on tablets at a science museum with no feedback allowed. LLMs (40 open-source models plus GPT-3, GPT-4, and GPT-4V) received text-based prompts with one example and test items, with temperature set to 0 for reproducibility. Error responses were coded by two independent raters into four categories: copy/duplication, concept-based errors, matrix-based errors, and other errors.

## Key Results
- Humans (both children and adults) outperformed LLMs on simplified ARC tasks, with accuracy ranging from 25% to 75% versus 0% to 25% for most LLMs
- LLMs and young children shared a common "fallback" strategy of copying input matrices when uncertain
- Humans made more "concept" errors (partially correct solutions showing concept understanding) while LLMs made more "matrix" errors (simple combinations of input matrices)
- Fine-tuned models like Platypus2-70B-instruct performed better than base models, suggesting training on logical reasoning tasks may help

## Why This Works (Mechanism)
None provided

## Foundational Learning
- **Visual analogy reasoning**: Understanding relationships between visual patterns and applying them to new situations
  - *Why needed*: ARC tasks fundamentally test the ability to identify and apply abstract visual relationships
  - *Quick check*: Can participants identify the transformation rule in a 3x3 grid and apply it to a new grid?
- **Error categorization frameworks**: Systematically classifying reasoning errors into meaningful types
  - *Why needed*: To distinguish between different types of reasoning failures and their cognitive implications
  - *Quick check*: Do two independent raters agree on error classifications for the same responses?
- **Matrix representation of visual patterns**: Converting visual information into numerical/text formats for computational processing
  - *Why needed*: LLMs require text-based inputs, requiring a systematic way to represent visual matrices
  - *Quick check*: Can the matrix representation preserve all relevant visual information needed to solve the task?

## Architecture Onboarding
**Component Map**: Visual Matrix -> Text Encoding -> LLM Processing -> Text Output -> Error Classification -> Performance Analysis
**Critical Path**: Human participants view visual matrices on tablets → LLMs receive text-encoded matrices → Both produce responses → Independent raters categorize errors → Accuracy and error pattern analysis
**Design Tradeoffs**: Text-based LLM inputs lose some visual nuance but enable systematic testing across many models; simplified tasks reduce complexity but may not fully capture adult-level reasoning
**Failure Signatures**: LLMs producing matrix errors (simple combinations) indicates reliance on pattern matching rather than concept abstraction; copy errors suggest fallback to input preservation
**3 First Experiments**:
1. Test GPT-4-Vision with visual input format versus text format on the same tasks to isolate modality effects
2. Fine-tune a base LLM specifically on ARC-style tasks and compare error patterns to both humans and other LLMs
3. Create progressively more complex ARC items to identify the threshold where human advantage becomes most pronounced

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: How do fine-tuning datasets specifically designed for logical reasoning impact LLM performance on visual analogy tasks?
- **Basis in paper**: [explicit] The paper notes that Platypus2-70B-instruct, a Llama-2 fine-tuned on a dataset involving logical reasoning tasks, performed on par with GPT-4 on KidsARC-Concept, while base Llama-2 performed poorly.
- **Why unresolved**: The paper suggests this is notable but does not conduct a controlled experiment isolating the effect of fine-tuning on logical reasoning datasets.
- **What evidence would resolve it**: Controlled experiments comparing base and fine-tuned models on visual analogy tasks, varying the content and scope of fine-tuning datasets.

### Open Question 2
- **Question**: What specific architectural or training modifications could improve LLMs' ability to develop symbol-level abstractions needed for tasks like ARC?
- **Basis in paper**: [inferred] The paper concludes that LLMs fail to develop symbol-level abstractions, leading to strategies that diverge from those used by humans, echoing critiques of connectionist systems' struggles with compositionality and objectness.
- **Why unresolved**: The paper identifies the problem but does not propose or test specific architectural or training modifications to address it.
- **What evidence would resolve it**: Experiments testing different LLM architectures or training regimes designed to promote symbol-level abstraction, measuring performance on ARC-like tasks.

### Open Question 3
- **Question**: How does the presentation format of visual analogy tasks (e.g., matrix vs. visual) impact LLM performance and error patterns?
- **Basis in paper**: [explicit] The paper notes that GPT-4-Vision, which had access to tasks in visual format, still produced many matrix errors, suggesting that presentation format alone does not explain the error patterns.
- **Why unresolved**: The paper does not conduct a systematic comparison of LLM performance across different presentation formats.
- **What evidence would resolve it**: Controlled experiments comparing LLM performance and error patterns on the same visual analogy tasks presented in different formats (e.g., matrix, visual, mixed).

### Open Question 4
- **Question**: To what extent do LLM error patterns on visual analogy tasks reflect associative vs. relational reasoning processes?
- **Basis in paper**: [inferred] The paper draws parallels between LLM copying errors and young children's associative errors, while noting that concept-based errors (indicative of relational reasoning) are rare in LLMs compared to humans.
- **Why unresolved**: The paper does not conduct a detailed analysis of the underlying cognitive processes reflected in LLM error patterns.
- **What evidence would resolve it**: Experiments designed to tease apart associative and relational reasoning in LLM error patterns, potentially using techniques like response time analysis or manipulation of task features that differentially engage these processes.

## Limitations
- Lack of publicly available item images makes precise replication of tasks impossible
- Small sample sizes for youngest age group (3-5 year-olds, n=11) may limit generalizability of error pattern findings
- Text-based matrix representations using numbers 0-9 for colors may not fully capture the visual reasoning process that ARC tasks intend to measure

## Confidence
- **High confidence**: Humans outperform LLMs on these tasks; LLMs show tendency toward copying input matrices as fallback strategy
- **Medium confidence**: Specific error categorization showing humans make more concept-based errors while LLMs make more matrix-based errors
- **Medium confidence**: Conclusion that LLMs rely on less human-like processes due to text-based input format differences

## Next Checks
1. Recreate the KidsARC-Simple and KidsARC-Concept datasets with the described concepts and verify that the items capture the intended difficulty progression from 3x3 to 5x5 grids
2. Conduct a reliability analysis of the error coding scheme by having additional independent raters categorize a subset of LLM and human responses to establish inter-rater agreement
3. Test a subset of the most successful LLMs using visual input (via GPT-4V) versus text-based input to isolate whether the reasoning differences are due to input modality or fundamental differences in analogical reasoning capabilities