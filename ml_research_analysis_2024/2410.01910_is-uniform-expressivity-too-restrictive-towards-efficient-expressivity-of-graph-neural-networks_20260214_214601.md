---
ver: rpa2
title: Is uniform expressivity too restrictive? Towards efficient expressivity of
  graph neural networks
arxiv_id: '2410.01910'
source_url: https://arxiv.org/abs/2410.01910
tags:
- function
- activation
- gnns
- expressivity
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the expressivity of Graph Neural Networks (GNNs)
  for two-variable guarded fragment (GC2) queries of first-order logic. The authors
  prove that GNNs with superpolynomial bounded Pfaffian activation functions (including
  sigmoid and tanh) cannot uniformly express all GC2 queries, unlike ReLU GNNs.
---

# Is uniform expressivity too restrictive? Towards efficient expressivity of graph neural networks

## Quick Facts
- arXiv ID: 2410.01910
- Source URL: https://arxiv.org/abs/2410.01910
- Reference count: 40
- Key outcome: GNNs with superpolynomial bounded Pfaffian activation functions cannot uniformly express all GC2 queries, but can efficiently approximate them non-uniformly with logarithmic dependency on graph degree

## Executive Summary
This paper investigates the expressivity of Graph Neural Networks (GNNs) for two-variable guarded fragment (GC2) queries of first-order logic. The authors prove that GNNs with superpolynomial bounded Pfaffian activation functions (including sigmoid and tanh) cannot uniformly express all GC2 queries, unlike ReLU GNNs. However, they show that such GNNs can efficiently approximate GC2 queries non-uniformly with logarithmic dependency on the maximal degree of input graphs, making uniform expressivity practically achievable for large graphs.

## Method Summary
The authors analyze GNNs with different activation functions (ReLU, tanh, sigmoid) to express GC2 queries. They construct specific tree-based counterexamples to prove limitations of superpolynomial bounded Pfaffian functions, then develop step-like activation functions that enable efficient non-uniform approximation. The approach involves theoretical analysis of convergence properties and experimental validation using synthetic datasets with varying graph sizes and structures.

## Key Results
- Superpolynomial bounded Pfaffian activation functions cannot uniformly express all GC2 queries due to error convergence issues
- Step-like activation functions enable efficient non-uniform approximation of GC2 queries with O(d + log log ∆) size
- The log-log dependency on graph degree makes uniform expressivity practically achievable for large graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Superpolynomial bounded Pfaffian activation functions cannot uniformly express all GC2 queries.
- Mechanism: When input graph size increases, error terms in GNN output converge superpolynomially to zero, causing distinct GC2 queries to become indistinguishable.
- Core assumption: Activation function converges superpolynomially to its limit, and GNN uses sum aggregation.
- Evidence anchors:
  - [abstract]: "We prove that GNNs with a wide class of Pfaffian activation functions (including the sigmoid and tanh) cannot uniformly express all GC2 queries"
  - [section 4]: "Our first result makes use of specific families of input trees that allow us to saturate the output signal of a GNN by increasing appropriately the number of vertices in certain regions of these graphs"
- Break condition: If activation function doesn't converge superpolynomially or if aggregation is not sum-based.

### Mechanism 2
- Claim: GNNs with step-like activation functions can efficiently approximate GC2 queries with logarithmic dependency on graph degree.
- Mechanism: Step-like activation functions approximate linear threshold function σ*(x) efficiently, allowing GNNs to distinguish graph structures even with large degrees.
- Core assumption: Activation function satisfies step-like properties (fast convergence to σ*, bounded derivatives, etc.).
- Evidence anchors:
  - [abstract]: "we show that despite these limitations, many of those GNNs can still efficiently express GC2 queries in a way that the number of parameters remains logarithmic on the maximal degree of the input graphs"
  - [section 5]: "Our second contribution shows that for a large class of activation functions, which we call step-like activation function, we have almost-uniform expressivity of GC2 queries"
- Break condition: If activation function is not step-like or graph degree exceeds practical limits.

### Mechanism 3
- Claim: Log-log dependency on graph degree for step-like activation functions makes uniform expressivity practically achievable for large graphs.
- Mechanism: Extremely slow growth of log-log(∆) means even for very large graphs, parameter count remains practically constant.
- Core assumption: Step-like activation function has η = 0 (no derivative at 0 and 1).
- Evidence anchors:
  - [abstract]: "Furthermore, we demonstrate that a log-log dependency on the degree is achievable for a certain choice of activation function"
  - [section 5]: "Remark 5.1: We note that the fact that the size of the GNN depends on the degree ∆ of the class of graphs we are working with only logarithmically guarantees that the GNN has a reasonable size for applications"
- Break condition: If η > 0 or graph degree is extremely large.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their variants
  - Why needed here: Understanding GNN architecture and variants is crucial for grasping expressivity limitations and capabilities discussed.
  - Quick check question: What are the key components of a GNN, and how do they differ between standard and recurrent GNNs?

- Concept: Logical fragments (GC2, FO+C, etc.)
  - Why needed here: The paper compares GNN expressivity to logical fragments, so understanding these fragments is essential for following arguments.
  - Quick check question: What is the difference between GC2 and FO+C queries, and why is this distinction important for GNN expressivity?

- Concept: Pfaffian functions and their properties
  - Why needed here: The paper relies on Pfaffian function properties to prove limitations of certain activation functions in GNNs.
  - Quick check question: What are the key properties of Pfaffian functions, and how do they relate to convergence behavior of activation functions in GNNs?

## Architecture Onboarding

- Component map: Input graph structure -> GNN layers (combination + aggregation) -> Activation function (step-like or superpolynomial bounded Pfaffian) -> Output vertex embeddings
- Critical path: 1) Initialize vertex features based on colors, 2) Apply combination and aggregation functions iteratively, 3) Use activation function to introduce non-linearity, 4) Output vertex embeddings satisfying GC2 query requirements
- Design tradeoffs: Uniform expressivity vs. efficient approximation, number of layers vs. approximation accuracy, choice of activation function vs. expressivity and efficiency
- Failure signatures: Large error terms in output embeddings, inability to distinguish between different graph structures, exponential growth in parameters required
- First 3 experiments: 1) Implement GNN with sigmoid activation and test ability to compute simple GC2 query on small graphs, 2) Modify GNN to use step-like activation function and compare performance on larger graphs, 3) Experiment with different numbers of layers and aggregation functions to optimize GNN's approximation of GC2 queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does uniform expressivity require superpolynomial bounded Pfaffian activation functions?
- Basis in paper: [explicit] The paper proves that GNNs with superpolynomial bounded Pfaffian activation functions (including sigmoid and tanh) cannot uniformly express all GC2 queries, unlike ReLU GNNs.
- Why unresolved: The paper only proves impossibility for superpolynomial bounded Pfaffian functions, leaving open whether other types of activation functions could achieve uniform expressivity.
- What evidence would resolve it: A proof showing either that all activation functions except ReLU fail to achieve uniform expressivity, or a counterexample demonstrating some non-Pfaffian activation function that does achieve uniform expressivity.

### Open Question 2
- Question: What is the exact relationship between activation function properties and expressivity in the non-uniform setting?
- Basis in paper: [inferred] The paper shows that step-like activation functions can efficiently approximate GC2 queries non-uniformly, but the exact characterization of which activation functions allow this remains unclear.
- Why unresolved: While the paper provides sufficient conditions (step-like functions), it doesn't establish necessary conditions or fully characterize the class of activation functions that enable efficient non-uniform expressivity.
- What evidence would resolve it: A complete characterization theorem stating both necessary and sufficient conditions on activation functions for efficient non-uniform expressivity of GC2 queries.

### Open Question 3
- Question: How does the choice of activation function affect the learnability of GC2 queries in practice?
- Basis in paper: [explicit] The experiments show similar mean square error for learning GC2 queries with both ReLU and Pfaffian GNNs, despite theoretical differences in expressivity.
- Why unresolved: The paper suggests that better uniform capacities don't necessarily imply easier learning, but doesn't provide a theoretical explanation for this observation or explore other architectural factors.
- What evidence would resolve it: A theoretical framework connecting activation function properties to optimization landscape characteristics, explaining why certain activation functions might be easier or harder to train for specific query classes.

## Limitations

- The theoretical results focus on sum aggregation, leaving open whether other aggregation functions could change the expressivity landscape
- Experimental validation is limited to synthetic datasets, with unclear generalization to complex real-world graphs
- The paper doesn't provide a complete characterization of which activation functions enable efficient non-uniform expressivity

## Confidence

- **High confidence**: The fundamental limitation of superpolynomial bounded Pfaffian activation functions (Theorem 1)
- **Medium confidence**: The step-like activation function approximation results (Theorem 2), though empirical validation is limited
- **Medium confidence**: The log-log dependency claim (Remark 5.1), which lacks comprehensive experimental verification

## Next Checks

1. Test the proposed step-like activation functions on real-world graph datasets with varying degrees to verify the log-log complexity scaling
2. Evaluate the robustness of the approximation bounds under different aggregation functions beyond sum aggregation
3. Investigate whether alternative neural network architectures can achieve better expressivity-efficiency tradeoffs for GC2 queries