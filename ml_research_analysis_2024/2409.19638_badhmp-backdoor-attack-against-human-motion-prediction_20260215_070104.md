---
ver: rpa2
title: 'BadHMP: Backdoor Attack against Human Motion Prediction'
arxiv_id: '2409.19638'
source_url: https://arxiv.org/abs/2409.19638
tags:
- backdoor
- motion
- human
- prediction
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BadHMP, the first backdoor attack against
  human motion prediction models. The core idea is to poison training data by embedding
  localized backdoor triggers in one limb of the skeleton during historical time steps,
  then globally modifying future sequences to predefined target trajectories.
---

# BadHMP: Backdoor Attack against Human Motion Prediction

## Quick Facts
- arXiv ID: 2409.19638
- Source URL: https://arxiv.org/abs/2409.19638
- Reference count: 40
- Primary result: First backdoor attack against human motion prediction models using localized triggers and global target modifications

## Executive Summary
This paper introduces BadHMP, the first backdoor attack against human motion prediction models. The attack embeds localized backdoor triggers in one limb of the skeleton during historical time steps while globally modifying future sequences to predefined target trajectories. Four poisoning strategies (rigid/soft triggers × jammed/moving targets) are proposed, along with a sample selection method to ensure stealthy, natural-looking poisoned samples. Experiments on Human3.6M and CMU-Mocap datasets demonstrate that the attack maintains high fidelity on clean inputs while achieving low backdoor activation error and remaining stealthy through preserved smoothness and naturalness.

## Method Summary
BadHMP operates through three main steps: (1) localized history sequences poisoning by embedding rigid or soft triggers in one limb (e.g., left arm) during historical time steps while preserving remaining joints, (2) global future sequences poisoning using jammed or moving targets to modify future predictions, and (3) sample selection to choose the most stealthy poisoned samples based on smoothness and naturalness metrics. The attack maintains high fidelity on clean data while ensuring effective backdoor activation and stealthiness through careful trigger and target design that preserves semantic meaning and natural motion characteristics.

## Key Results
- Maintains high fidelity on clean inputs (CDE ~12) comparable to benign models
- Achieves low backdoor activation error (BDE ~3-4) with effective trigger activation
- Remains stealthy with preserved smoothness (low Acc/Jerk) and naturalness (low BLC)
- Effective even at low poisoning ratios (2-10%) across multiple model architectures
- Robust to fine-tuning defense while maintaining attack effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poisoning only a localized subset of joints preserves semantic meaning
- Mechanism: Manipulating only m joints on left arm while keeping remaining joints unchanged
- Core assumption: Models rely on global pose features rather than fine-grained local limb details
- Evidence: "only several connected joints in a fixed limb are manipulated... to ensure that the semantic meaning of the input sequences is not damaged"
- Break condition: Models highly sensitive to local limb movements (attention-based architectures)

### Mechanism 2
- Claim: Backdoor trigger and target designed to be smooth and natural
- Mechanism: Rigid/soft triggers and jammed/moving targets with scaling transformations
- Core assumption: Inspectors prioritize abrupt changes in acceleration, jerk, and bone length
- Evidence: "carefully designed backdoor triggers and targets guarantee the smoothness and naturalness... making them stealthy enough to evade detection"
- Break condition: Advanced detection methods beyond acceleration, jerk, and bone length

### Mechanism 3
- Claim: Poisoned model maintains high fidelity on clean inputs
- Mechanism: Minimize difference between poisoned and benign model performance on clean data
- Core assumption: Model trainers primarily evaluate performance on clean data
- Evidence: "CDE of the victim model should be comparable to that of the benign model"
- Break condition: Rigorous backdoor detection or adversarial testing by trainers

## Foundational Learning

- **Human motion prediction as sequence-to-sequence task**
  - Why needed: Understanding task structure crucial for designing effective poisoning strategies
  - Quick check: What is the difference between input sequence and output sequence in human motion prediction?

- **Backdoor attacks and their goals**
  - Why needed: Attack aims to embed backdoor activatable at test time
  - Quick check: What are the two main goals of a backdoor attack in human motion prediction?

- **Evaluation metrics for motion prediction and backdoor attacks**
  - Why needed: Proper evaluation requires understanding CDE, BDE, Acc, Jerk, and BLC
  - Quick check: How do CDE and BDE metrics differ in purpose and interpretation?

## Architecture Onboarding

- **Component map**: Data poisoning module -> Sample selection module -> Model training module -> Evaluation module
- **Critical path**: 
  1. Generate poisoned samples using data poisoning module
  2. Select top ρ% poisoned samples using sample selection module
  3. Train victim model on poisoned dataset using model training module
  4. Evaluate effectiveness and stealthiness using evaluation module

- **Design tradeoffs**:
  - Injection ratio vs. stealthiness: Higher ratios increase effectiveness but reduce stealthiness
  - Trigger type (rigid vs. soft) vs. backdoor activation ease: Rigid triggers easier to activate but less stealthy
  - Target type (jammed vs. moving) vs. naturalness: Jammed targets simpler but more noticeable

- **Failure signatures**:
  - High CDE difference between poisoned and benign models
  - High BDE on clean samples (unintended activation)
  - High acceleration, jerk, or bone length change in poisoned samples

- **First 3 experiments**:
  1. Test effectiveness with different injection ratios (2%, 5%, 10%, 15%) on single dataset and model
  2. Compare stealthiness of different trigger-target combinations (R-J, R-M, S-J, S-M)
  3. Evaluate robustness against fine-tuning defense by measuring BDE after defense application

## Open Questions the Paper Calls Out

The paper acknowledges that most backdoor defenses for other domains aren't directly applicable to human motion prediction and identifies the need for domain-specific defense mechanisms. It also notes that the generalizability to other model architectures beyond LTD and HRI remains an open question, particularly for transformer-based models that are increasingly popular in motion prediction tasks.

## Limitations

- Limited evaluation of defense robustness beyond basic fine-tuning
- Unclear implementation details for critical sample selection step
- Only tested on two model architectures (LTD and HRI), not transformers
- Sample selection mechanism parameters (λ1, λ2) not fully specified
- Attack parameters for trigger/target generation not completely detailed

## Confidence

**High Confidence**:
- Attack framework architecture and methodology clearly described
- Basic experimental results on Human3.6M and CMU-Mocap datasets
- Concept of localized backdoor triggers in human motion prediction

**Medium Confidence**:
- Stealthiness metrics (Acc, Jerk, BLC) properly calculated and interpreted
- Effectiveness metrics (CDE, BDE) demonstrate practical viability
- Sample selection process improves attack stealthiness

**Low Confidence**:
- Robustness against all possible defense mechanisms
- Generalizability to all human motion prediction model architectures
- Long-term persistence of backdoor after various model updates

## Next Checks

1. **Sample Selection Algorithm Verification**: Implement and test exact sample selection algorithm with various weighting factors (λ1, λ2) to verify impact on attack stealthiness and effectiveness

2. **Defense Robustness Testing**: Evaluate attack effectiveness against multiple defense strategies including adversarial training, input sanitization, and specialized backdoor detection algorithms beyond just fine-tuning

3. **Cross-Dataset Generalization**: Test attack on additional human motion datasets (NTU RGB+D, 3D Poses in the Wild) to verify robustness across different motion capture systems and environments