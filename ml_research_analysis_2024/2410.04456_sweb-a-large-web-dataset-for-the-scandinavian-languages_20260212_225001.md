---
ver: rpa2
title: 'SWEb: A Large Web Dataset for the Scandinavian Languages'
arxiv_id: '2410.04456'
source_url: https://arxiv.org/abs/2410.04456
tags:
- data
- content
- sweb
- dataset
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Scandinavian WEb (SWEb), the largest
  publicly available pretraining dataset for Scandinavian languages, containing over
  1 trillion tokens. The authors present a novel model-based text extraction pipeline
  that replaces complex rule-based systems, using a transformer model trained on just
  1,380 annotated web pages.
---

# SWEb: A Large Web Dataset for the Scandinavian Languages

## Quick Facts
- arXiv ID: 2410.04456
- Source URL: https://arxiv.org/abs/2410.04456
- Reference count: 40
- Largest publicly available pretraining dataset for Scandinavian languages with over 1 trillion tokens

## Executive Summary
This paper introduces the Scandinavian WEb (SWEb), the largest publicly available pretraining dataset for Scandinavian languages, containing over 1 trillion tokens. The authors present a novel model-based text extraction pipeline that replaces complex rule-based systems, using a transformer model trained on just 1,380 annotated web pages. They introduce HP-MEK, a new Swedish cloze-style benchmark, and demonstrate that models trained on SWEb data perform on par with those trained on FineWeb, achieving competitive results. The pipeline extracts 60% more high-quality tokens than FineWeb while significantly reducing complexity. All code, models, and data are released openly.

## Method Summary
The SWEb dataset is created through a pipeline that uses CCNet to identify Scandinavian documents from Common Crawl WET archives, converts HTML to Markdown with Pandoc, extracts text using a Longformer model trained on 1,380 annotated samples for line-level binary classification, applies four simple quality filters, performs MinHashLSH deduplication, and replaces PII with regex patterns. The pipeline processes 98 Common Crawl snapshots from 2013-2020 to 2024-2026, achieving 1 trillion tokens while maintaining high quality through the model-based extraction approach.

## Key Results
- Model-based extractor achieves 87% F1 score on validation set with only 1,380 annotated samples
- SWEb extracts 60% more high-quality tokens than FineWeb on the same input data
- Models trained on SWEb data achieve perplexity on par with FineWeb despite more unique tokens

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A model-based extractor using only 1,380 annotated samples can match or exceed rule-based extractors in text quality.
- Mechanism: The transformer encoder classifies lines in Markdown documents based on contextual embeddings, effectively learning to distinguish primary content from noise without explicit heuristics.
- Core assumption: Line-level binary classification with contextual embeddings is sufficient to capture content relevance across diverse web page layouts.
- Evidence anchors:
  - [abstract] "a novel model-based text extractor that significantly reduces complexity in comparison with rule-based approaches"
  - [section 3.2.2] "We fine-tune the entire model on our training set of 1,280 documents, and the results on the validation set can be seen in Figure 3. We can see that despite our small-scale training data, we achieve an F1 score of 87%"
  - [corpus] Weak - no explicit evidence about how 1,380 samples compares to typical annotation budgets in the field.
- Break condition: If the extractor fails to generalize beyond the 1,380 annotated pages, performance would degrade on unseen page structures or languages.

### Mechanism 2
- Claim: Markdown formatting preserves more structural information than plain text extraction, improving downstream model performance.
- Mechanism: Converting HTML to Markdown retains basic formatting (headers, lists, tables) while avoiding heavy markup tokens, providing models with layout cues during training.
- Core assumption: Structural and layout information is valuable for language models to learn from, as stated by prior work like MassiveWeb.
- Evidence anchors:
  - [section 3.2.1] "we choose to convert from HTML to Markdown with its very lightweight markup, thus does not add many extra tokens"
  - [section 4.3] "MSW achieves lower perplexity on its own data than MFW, i.e. SWEb seems 'easier' to fit despite it being trained on more unique tokens. This could for example be due to the markdown formating, where markup tokens might be easier to predict"
  - [corpus] Weak - no direct comparison of performance with/without Markdown formatting on same data.
- Break condition: If models show no benefit from Markdown formatting, or if formatting tokens introduce noise that outweighs benefits.

### Mechanism 3
- Claim: Using WET archives for content selection and WARC archives for extraction reduces computational overhead while maintaining quality.
- Mechanism: CCNet processes WET archives to identify Scandinavian documents efficiently, then uses WARC indices to retrieve original HTML for high-quality extraction.
- Core assumption: WET archives provide sufficient language detection accuracy for initial filtering, and WARC retrieval is efficient enough for large-scale processing.
- Evidence anchors:
  - [section 3.1] "We use CCNet to identify Scandinavian documents within the entire Common Crawl dataset. CCNet processes the WET archives, and after line-by-line deduplication, language detection is performed using fastText"
  - [section 3.2] "we use Common Crawl's index to download their original HTML from the W ARC archives. This means we use CCNet and the WET documents solely for content selection, and not for extraction"
  - [corpus] Weak - no quantitative evidence about computational savings or quality trade-offs between this approach and full WET or full WARC processing.
- Break condition: If language detection accuracy on WET archives is insufficient, leading to many non-Scandinavian documents passing initial filtering.

## Foundational Learning

- Concept: Line-level binary classification with contextual embeddings
  - Why needed here: The extractor needs to distinguish primary content from noise at the finest practical granularity while maintaining context awareness.
  - Quick check question: Why does the model use [SEP] tokens instead of processing the entire document as a sequence?

- Concept: Markdown as lightweight markup format
  - Why needed here: Provides structural information (headers, lists, tables) without heavy tokenization overhead that would burden language models.
  - Quick check question: How does the Markdown conversion balance information preservation against token count increase?

- Concept: Quality filtering through simple heuristics post-extraction
  - Why needed here: The model-based extraction reduces but doesn't eliminate all noise, requiring minimal additional filtering for edge cases.
  - Quick check question: Why are only four simple filters used instead of the dozens used in rule-based pipelines?

## Architecture Onboarding

- Component map: Content selection (WET + CCNet) → Content extraction (HTML → Markdown + model-based line classification) → Quality filtering (4 heuristics) → De-duplication (MinHashLSH) → PII replacement (regex)
- Critical path: The model-based extractor is the central component; all other stages support or follow from its output quality.
- Design tradeoffs: Model-based extraction provides flexibility but increases compute cost per document versus rule-based alternatives.
- Failure signatures: Poor F1 score on validation set indicates annotation quality issues or insufficient model capacity; low recall suggests over-aggressive filtering.
- First 3 experiments:
  1. Train the line classification model on the 1,380 annotated samples and evaluate F1 score on the validation set.
  2. Compare perplexity of models trained on SWEb versus FineWeb to assess data quality differences.
  3. Run the complete pipeline on a small subset of Common Crawl snapshots and verify document counts and token counts match expectations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the model-based extractor compare to more advanced rule-based extractors beyond trafilatura?
- Basis in paper: [explicit] The paper compares the model-based extractor to trafilatura but does not evaluate it against other state-of-the-art rule-based or neural extractors like Dragnet or Readability.js.
- Why unresolved: The authors only benchmark against trafilatura, which is already known to be less effective than some alternatives. A broader comparison would reveal if the model-based approach truly outperforms other methods across different domains and languages.
- What evidence would resolve it: Systematic evaluation of the model-based extractor against multiple rule-based and neural extractors on diverse multilingual datasets, measuring precision, recall, and downstream language model performance.

### Open Question 2
- Question: What is the impact of using markdown formatting versus plain text on downstream language model capabilities?
- Basis in paper: [explicit] The authors argue that markdown formatting is beneficial for language models but do not empirically test whether models trained on markdown-formatted data perform better than those trained on plain text for specific tasks.
- Why unresolved: While the paper claims markdown preserves useful structural information, it lacks experiments directly comparing the two formats' effects on model generalization, reasoning, or generation quality.
- What evidence would resolve it: Training identical models on SWEb (markdown) versus an equivalent plain-text version of the same data, then evaluating on benchmarks measuring reasoning, coherence, and task-specific performance.

### Open Question 3
- Question: How does the small-scale annotation approach (1,380 examples) generalize to other low-resource languages or different web domains?
- Basis in paper: [explicit] The authors demonstrate data efficiency with just 1,380 annotated examples but do not test whether this approach scales to other languages or domains with different content structures.
- Why unresolved: The study is limited to Scandinavian languages and general web content. It's unclear if the same annotation efficiency holds for highly specialized domains (e.g., medical, legal) or typologically different languages.
- What evidence would resolve it: Applying the same annotation and training pipeline to multiple low-resource languages and specialized domains, measuring extraction quality and annotation efficiency compared to baseline methods.

### Open Question 4
- Question: What are the computational trade-offs between the model-based extractor and rule-based alternatives when scaling to massive web crawls?
- Basis in paper: [explicit] The authors acknowledge the computational cost of running the model extractor for each document but do not provide a detailed cost-benefit analysis or optimization strategies for large-scale deployment.
- Why unresolved: While the paper mentions 20k GPU-hours for SWEb extraction, it lacks analysis of how this scales with larger datasets, potential optimizations (e.g., batching, distillation), or comparisons to the total compute budget of training large language models.
- What evidence would resolve it: Comprehensive profiling of extraction time and compute costs across different dataset sizes, along with optimization experiments and comparisons to rule-based extraction costs for equivalent data volumes.

## Limitations

- The 1,380 annotated samples may not be sufficient to guarantee generalization to all web page layouts and languages.
- Direct comparison of extraction quality between model-based and rule-based approaches is lacking, with claims based on downstream perplexity rather than side-by-side measurements.
- The computational cost of running the transformer model for each document is acknowledged but not thoroughly analyzed or optimized.

## Confidence

**High Confidence:** The technical implementation details of the pipeline (HTML to Markdown conversion, Longformer training setup, quality filtering heuristics) are well-specified and reproducible. The claim that SWEb contains over 1 trillion tokens is supported by the methodology description.

**Medium Confidence:** The assertion that the model-based approach achieves +60% more high-quality tokens than FineWeb on the same input data is supported by corpus-level statistics but lacks detailed per-document quality analysis. The perplexity comparison showing SWEb is "easier to fit" despite more unique tokens provides indirect evidence but doesn't directly prove superior extraction quality.

**Low Confidence:** The claim that this is the "largest publicly available pretraining dataset for Scandinavian languages" cannot be fully verified without comprehensive knowledge of all existing datasets in this domain. The relationship between Markdown formatting and improved model performance is speculative, based on perplexity observations rather than controlled experiments.

## Next Checks

1. **Extraction Quality Validation:** Conduct a direct comparison by running both the model-based extractor and a simplified rule-based extractor on the same 1,000 randomly sampled web pages from Common Crawl. Measure precision, recall, and F1 score using human annotations to verify the 60% quality improvement claim.

2. **Annotation Generalization Test:** Create a separate validation set of 200 web pages with layouts significantly different from the 1,380 training examples (e.g., modern single-page applications, complex data tables, multimedia-heavy pages). Evaluate whether the model maintains performance above 80% F1 score on these out-of-distribution examples.

3. **Markdown Impact Experiment:** Train two identical language models on the same SWEb data, one with Markdown formatting preserved and one with all formatting stripped to plain text. Compare perplexity scores on a held-out Scandinavian language corpus to quantify the actual impact of structural information on model learning efficiency.