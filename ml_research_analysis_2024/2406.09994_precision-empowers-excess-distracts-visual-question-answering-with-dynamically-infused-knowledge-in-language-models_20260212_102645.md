---
ver: rpa2
title: 'Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically
  Infused Knowledge In Language Models'
arxiv_id: '2406.09994'
source_url: https://arxiv.org/abs/2406.09994
tags:
- triples
- knowledge
- image
- question
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a dynamic triple filtering method for Knowledge-Based
  Visual Question Answering (KBVQA) that improves model performance by extracting
  only the most relevant external knowledge triples for each question. Instead of
  using a fixed number of triples, the approach uses similarity thresholds to adaptively
  determine the number of relevant triples needed for each question.
---

# Precision Empowers, Excess Distracts: Visual Question Answering With Dynamically Infused Knowledge In Language Models

## Quick Facts
- arXiv ID: 2406.09994
- Source URL: https://arxiv.org/abs/2406.09994
- Authors: Manas Jhalani; Annervaz K M; Pushpak Bhattacharyya
- Reference count: 38
- Primary result: Achieves 4.75% average improvement in Exact Match Score over state-of-the-art models on three KBVQA datasets through dynamic triple filtering

## Executive Summary
This paper introduces a dynamic triple filtering method for Knowledge-Based Visual Question Answering (KBVQA) that adaptively selects the most relevant external knowledge triples for each question rather than using a fixed number. The approach uses similarity thresholds between question embeddings and triple embeddings to determine which knowledge triples to incorporate, improving reasoning capabilities while reducing noise. The method achieves state-of-the-art performance across three KBVQA datasets and demonstrates strong generalization, even achieving top results on small datasets through fine-tuning.

## Method Summary
The approach uses a two-stage pipeline: first, a dynamic triple filtering module retrieves variable numbers of knowledge triples from external sources like Wikidata and ConceptNet based on image entities and question similarity; second, an OFA transformer encoder-decoder model takes image features, the question, and filtered triples as input to generate answers. The filtering process uses a two-stage approach - first filtering triples relevant to image entities, then filtering those most relevant to the question using similarity thresholds (typically 0.8). The method specifically incorporates 2-hop triples for contextual information and demonstrates effectiveness across different model scales, improving accuracy by up to 20% when integrated with large multimodal language models.

## Key Results
- Achieves 4.75% average improvement in Exact Match Score over state-of-the-art models across three KBVQA datasets
- Demonstrates strong generalization by achieving state-of-the-art results on small datasets (CRIC-VQA with 3,400 triples) through fine-tuning
- Shows effectiveness across different model scales, with up to 20% accuracy improvement when incorporating dynamically filtered knowledge into large multimodal language models

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Triple Selection via Similarity Thresholds
Dynamic triple filtering improves performance by reducing noise and focusing on question-specific knowledge. The approach uses similarity thresholds to adaptively determine the number of relevant triples for each question, rather than using a fixed number. This allows the model to receive only the necessary information for each specific question. The core assumption is that embedding similarity accurately reflects relevance, with evidence showing improved reasoning capabilities over fixed triple approaches. Break condition: If embedding similarity doesn't accurately capture semantic relevance, the filtering could include irrelevant triples or miss important ones.

### Mechanism 2: Multi-hop Triple Integration for Contextual Reasoning
Incorporating multi-hop triples enhances reasoning capabilities by providing contextual information beyond direct entity relationships. The approach focuses on utilizing 2-hop triples, which capture relationships between entities that are not directly connected but relevant to the question context. The core assumption is that multi-hop relationships provide valuable contextual information. Evidence shows the approach incorporates 2-hop triples while acknowledging the trade-off of potential noise. Break condition: If knowledge graphs lack meaningful multi-hop relationships for the domain, this could introduce unnecessary noise without benefits.

### Mechanism 3: Cross-domain Generalization Through Adaptive Filtering
Dynamic triple filtering enables better generalization across different KBVQA datasets by adapting to each question's specific needs. By adjusting the number of triples per question, the model performs well on both small datasets and larger ones without overfitting to fixed triple patterns. The core assumption is that similarity threshold approaches generalize across domains and knowledge graph sizes. Evidence shows strong performance on small datasets through fine-tuning and successful augmentation of knowledge bases. Break condition: If different domains have fundamentally different triple relevance patterns, the same similarity threshold may not work well across all datasets.

## Foundational Learning

- **Concept: Knowledge Graph Embeddings**
  - Why needed here: The approach relies on embedding triples from knowledge graphs and comparing them to question embeddings to determine relevance.
  - Quick check question: Can you explain how a triple (head, relation, tail) would be embedded and why we need to mask named entities during this process?

- **Concept: Similarity Thresholds for Dynamic Selection**
  - Why needed here: The method uses a similarity threshold (0.8) to determine which triples to include for each question, rather than using a fixed number.
  - Quick check question: What would happen if we set the similarity threshold too high versus too low, and how would this affect model performance?

- **Concept: Multi-hop Reasoning in Knowledge Graphs**
  - Why needed here: The approach incorporates 2-hop triples to provide contextual information beyond direct entity relationships.
  - Quick check question: Can you describe what a 2-hop triple represents in a knowledge graph and give an example of when this would be useful for VQA?

## Architecture Onboarding

- **Component map**: Question → Triple Filtering (Image → Question) → OFA Model → Answer
- **Critical path**: Question → Triple Filtering (Image → Question) → OFA Model → Answer
- **Design tradeoffs**:
  - Dynamic vs fixed number of triples: Better accuracy but higher computational cost
  - Single-hop vs multi-hop triples: More context but potential noise
  - Similarity threshold vs top-K selection: Adaptive but requires threshold tuning

- **Failure signatures**:
  - Low accuracy on spatial questions despite correct triples (may indicate OFA model limitations)
  - No improvement over fixed triple approaches (may indicate similarity metric issues)
  - High variance in triple count per question (may indicate unstable similarity threshold)

- **First 3 experiments**:
  1. Test similarity threshold sensitivity: Run with thresholds 0.6, 0.8, 0.9 on KVQA and measure accuracy impact
  2. Compare single-hop vs multi-hop performance: Filter only 1-hop triples and measure accuracy drop
  3. Validate embedding masking: Run with and without named entity masking to confirm its importance for triple relevance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal similarity threshold value for dynamic triple filtering across different KBVQA datasets?
- Basis in paper: The paper mentions using a threshold of 0.8 but states this was determined through observation
- Why unresolved: The paper does not provide systematic analysis of how threshold values affect performance across different datasets or question types
- What evidence would resolve it: Comprehensive ablation studies testing multiple threshold values across all three datasets and question categories

### Open Question 2
- Question: How does the dynamic triple filtering approach scale with increasingly large knowledge graphs?
- Basis in paper: The paper discusses computational complexity when mentioning "longer prediction times" as a limitation
- Why unresolved: No experiments were conducted with knowledge graphs significantly larger than the ones used in the study
- What evidence would resolve it: Performance and efficiency benchmarking on knowledge graphs orders of magnitude larger than current ones

### Open Question 3
- Question: Can the dynamic triple filtering module be trained end-to-end with the answer prediction module for better performance?
- Basis in paper: The paper mentions "fact retriever and answer prediction module undergo separate training processes" as a limitation
- Why unresolved: The paper only explores separate training and does not investigate joint training approaches
- What evidence would resolve it: Comparative experiments between separate training and end-to-end training on all three datasets

### Open Question 4
- Question: What is the minimum amount of training data required for the approach to maintain performance on new domains?
- Basis in paper: The paper demonstrates generalization through fine-tuning but doesn't explore data efficiency
- Why unresolved: No experiments were conducted to determine how little data could be used while maintaining performance
- What evidence would resolve it: Systematic experiments with decreasing amounts of training data on transfer learning tasks

### Open Question 5
- Question: How does the approach perform on real-time user-centric applications with streaming data?
- Basis in paper: The paper mentions IoT applications and real-time user-centric data as motivation but doesn't test this scenario
- Why unresolved: No experiments were conducted with streaming data or real-time constraints
- What evidence would resolve it: Benchmarking on live data streams with latency and throughput measurements

## Limitations
- Dependence on similarity thresholds (0.8) without thorough validation across different domains raises questions about generalizability
- Acknowledged trade-off between single-hop and multi-hop triples is not quantitatively compared, leaving uncertainty about whether 2-hop complexity is justified
- Zero-shot evaluation with LLaVA shows metric issues with spatial questions, suggesting limitations for certain question types

## Confidence
- **High Confidence**: The core claim that dynamic triple filtering improves performance over fixed triple approaches is well-supported by experimental results showing 4.75% average improvement across three datasets.
- **Medium Confidence**: The generalization claims are supported by strong results on small datasets, but evidence for cross-domain generalization is limited.
- **Low Confidence**: The zero-shot evaluation with LLaVA raises significant concerns about metric validity for spatial questions.

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically test similarity thresholds (0.6, 0.7, 0.8, 0.9) across all three datasets to quantify the trade-off between triple quantity and answer quality, and determine optimal thresholds for different domains.

2. **Single-hop vs Multi-hop Comparison**: Implement and evaluate a variant that uses only 1-hop triples with the same dynamic filtering approach to quantify the exact performance impact of incorporating 2-hop relationships.

3. **Spatial Question Handling Validation**: Conduct targeted experiments on spatial question types using the LLaVA evaluation setup, implementing special handling for positional answers to validate whether metric issues can be resolved.