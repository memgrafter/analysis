---
ver: rpa2
title: Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning
arxiv_id: '2402.08010'
source_url: https://arxiv.org/abs/2402.08010
tags:
- frequencies
- cnns
- where
- which
- bottleneck
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Convolution Bottleneck (CBN) structure
  in deep CNNs, where representations become supported along only a few low frequencies
  in the middle layers before mapping back to outputs. The authors define the CBN
  rank, which measures the number and type of frequencies needed to represent a function,
  and prove that the parameter norm required to represent a function scales with depth
  times the CBN rank.
---

# Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning

## Quick Facts
- arXiv ID: 2402.08010
- Source URL: https://arxiv.org/abs/2402.08010
- Reference count: 40
- Key outcome: CNNs develop bottleneck structure where middle-layer representations concentrate on few low frequencies, with parameter norm scaling as depth times CBN rank

## Executive Summary
This paper reveals that deep CNNs naturally develop a bottleneck structure where representations in middle layers concentrate on few low frequencies before mapping back to outputs. The authors introduce the Convolution Bottleneck (CBN) rank to measure the frequencies needed for representation and prove that parameter norm scales with depth times this rank. They show this structure emerges because it minimizes the cost of representing the identity in deep networks. The work provides theoretical justification for common CNN practices like downsampling, demonstrating it preserves representational power for low-frequency functions while reducing computational cost.

## Method Summary
The authors analyze CNNs using Fourier decomposition and singular value analysis of weight matrices. They define the CBN rank for translationally equivariant functions and prove representation cost scales with depth times this rank. The theoretical framework analyzes weight matrices through their singular values and frequency decomposition. Experiments use CNNs with L2 regularization on MNIST and synthetic data, measuring singular value distributions across layers to verify bottleneck emergence. The analysis includes CNNs with cyclic boundaries, full-size filters, and Mβ pooling operators.

## Key Results
- CNNs exhibit bottleneck structure where representations concentrate on few low frequencies in middle layers
- Parameter norm required to represent a function scales as depth times the CBN rank
- Weight matrices in well-trained CNNs concentrate singular values on the same frequencies as pre-activations
- Down-sampling preserves CBN structure for low-frequency functions, justifying its common use in CNNs
- CNNs naturally disentangle different aspects of data (e.g., shape vs pattern, position vs velocity) using low-frequency bottleneck structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep CNNs develop a bottleneck structure where representations in middle layers concentrate on few low-frequency components.
- Mechanism: The parameter norm required to represent a function scales with depth times the CBN rank, creating pressure for the network to minimize parameter norm by concentrating representations on low frequencies.
- Core assumption: Networks are trained with L2 regularization and reach near-optimal parameter norms.
- Evidence anchors:
  - [abstract] "we define the CBN rank, which describes the number and type of frequencies that are kept inside the bottleneck, and partially prove that the parameter norm required to represent a function f scales as depth times the CBN rank f"
  - [section] "we show that any network with almost optimal parameter norm will exhibit a CBN structure in both the weights"
  - [corpus] Weak - no direct corpus evidence of bottleneck emergence in CNNs, though related work on bottleneck structure in other architectures exists
- Break condition: If training objective doesn't effectively penalize parameter norm (e.g., no L2 regularization or insufficient regularization strength).

### Mechanism 2
- Claim: The pre-activation representations in deep CNNs become bounded and concentrated on the same frequencies as the weight matrices.
- Mechanism: Under large learning rates, gradient descent converges to minima with small Hessian trace/eigenvalues, which implies bounded activations that must align with the frequency structure of the weights.
- Core assumption: Learning rate is sufficiently large and parameters are balanced.
- Evidence anchors:
  - [section] "under the additional assumption that the network is stable under reasonable learning rate - the activations"
  - [section] "Theorem 4.2...implies that almost all activations are bounded"
  - [corpus] Weak - no direct corpus evidence of activation frequency concentration under large learning rates in CNNs
- Break condition: If learning rate is too small to induce convergence to narrow minima, or if parameter balancing condition fails.

### Mechanism 3
- Claim: Down-sampling in CNNs preserves the CBN rank for functions with low-frequency structure.
- Mechanism: Down-sampling followed by up-sampling can exactly reconstruct low-frequency signals, allowing the same bottleneck structure to be maintained with reduced computational cost.
- Core assumption: Target function has a low-frequency bottleneck structure (RankCBN ≈ Rank(s)CBN).
- Evidence anchors:
  - [abstract] "The results also justify the common practice of using downsampling layers in CNNs, showing they can reduce computational cost while maintaining representational power for functions with low-frequency structure"
  - [section] "we can see that R(0) ≈ R(0)s (meaning their optimal representation costs are close)"
  - [corpus] Weak - no direct corpus evidence of down-sampling preserving CBN structure, though general down-sampling benefits are well-established
- Break condition: If target function doesn't have low-frequency structure or if down-sampling discards frequencies needed for representation.

## Foundational Learning

- Concept: Fourier analysis and frequency domain representation
  - Why needed here: The entire theoretical framework relies on decomposing representations into Fourier frequencies to characterize the bottleneck structure
  - Quick check question: Can you explain why a circulant matrix represents a cyclic convolution and how its eigenvalues relate to Fourier frequencies?

- Concept: Singular value decomposition and matrix representations
  - Why needed here: The paper analyzes weight matrices through their singular values and frequency decomposition to characterize the bottleneck structure
  - Quick check question: Given a weight matrix W, can you explain how its singular values relate to the frequencies kept in the bottleneck?

- Concept: Implicit bias in gradient descent optimization
  - Why needed here: The emergence of bottleneck structure depends on the implicit bias of gradient descent toward minima with small parameter norm and Hessian trace
  - Quick check question: What is the relationship between large learning rates and convergence to minima with small Hessian trace in gradient descent?

## Architecture Onboarding

- Component map: Input → Convolution layers with ReLU activations → Optional down-sampling → More convolution layers → Optional up-sampling → Output. The bottleneck occurs in middle layers where representations concentrate on few frequencies.
- Critical path: The representation cost minimization path - input transformation to low-frequency bottleneck → identity mapping in middle layers → transformation back to output space.
- Design tradeoffs: Deeper networks have stronger pressure toward bottleneck structure (scales with depth) but require more layers to implement the identity mapping; down-sampling reduces computation but may lose high-frequency information if not needed.
- Failure signatures: High parameter norms without bottleneck structure (insufficient regularization), exploding/vanishing activations (imbalanced parameters or inappropriate learning rates), or poor performance despite bottleneck structure (missing required frequencies for the task).
- First 3 experiments:
  1. Train a CNN on MNIST with varying depths and measure singular value distribution across layers to verify bottleneck emergence.
  2. Add L2 regularization with different strengths and observe how parameter norm and bottleneck structure change.
  3. Implement down-sampling in middle layers and compare performance/cost with full-size network for low-frequency tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the conjecture R(0)(f; Ω) = RankCBN(f; Ω) true for all translationally equivariant functions?
- Basis in paper: Explicit - The authors conjecture this equality but only prove it for specific cases like linear one-layer CNNs with interior points in Ω−.
- Why unresolved: The authors can only prove lower bounds matching upper bounds for limited cases. A general proof would require showing the bottleneck structure is optimal for all functions.
- What evidence would resolve it: A proof showing that for any function with minimal representation cost, the weight matrices must have the bottleneck structure described by RankCBN, or a counterexample demonstrating this is not always the case.

### Open Question 2
- Question: How does the CBN structure generalize to other types of convolutions beyond cyclic convolutions with full-size filters?
- Basis in paper: Inferred - The authors note that their analysis uses full-size filters and cyclic boundaries to apply Fourier analysis, but expect similar structures to appear with different sparsity patterns.
- Why unresolved: The mathematical tools used (Fourier analysis) rely on specific properties of cyclic convolutions that don't extend to general convolutions.
- What evidence would resolve it: Theoretical analysis showing how the CBN rank and bottleneck structure manifest in networks with different convolution types (e.g., non-cyclic, different padding, smaller filters).

### Open Question 3
- Question: What is the optimal strategy for choosing down-sampling rates in practice, given the low-frequency decomposition results?
- Basis in paper: Explicit - The authors show that functions on translationally unique domains can be decomposed using only 2 frequencies, but note that this doesn't necessarily mean the optimal stride is 2.
- Why unresolved: While the authors provide theoretical justification for down-sampling, they don't give practical guidance on choosing stride sizes that balance computational efficiency with information retention.
- What evidence would resolve it: Empirical studies comparing different down-sampling strategies on various tasks, or theoretical analysis of the trade-off between rank estimation and regularity control for different stride choices.

## Limitations

- Theoretical claims rely on assumptions about reaching near-optimal parameter norm that may not hold in practice
- Activation concentration mechanism depends on large learning rates and balanced parameters that may not be typical in deep learning
- Mathematical framework assumes cyclic boundaries and full-size filters, limiting generalization to standard CNN implementations
- Empirical validation is limited to specific architectures and datasets, with limited ablation studies on key mechanisms

## Confidence

**High Confidence:** The relationship between parameter norm and depth × CBN rank (Mechanism 1) is mathematically proven and experimentally verified on synthetic data.

**Medium Confidence:** The emergence of bottleneck structure in actual CNN weight matrices is demonstrated empirically but theoretical justification has gaps.

**Low Confidence:** The activation concentration mechanism (Mechanism 2) has weakest theoretical support, depending on strong assumptions about learning rate and parameter balancing.

## Next Checks

1. **Parameter Norm Verification:** Train CNNs with systematically varied L2 regularization strengths and measure how parameter norm changes relative to predicted scaling with depth × CBN rank.

2. **Activation Dynamics:** Implement monitoring of pre-activation norms across layers during training with different learning rates to verify bounded activations concentrate on weight matrix frequencies.

3. **Down-sampling Ablation:** Design controlled experiments comparing full-size vs down-sampled networks on tasks with known frequency structure to measure representation cost and performance trade-offs.