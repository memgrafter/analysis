---
ver: rpa2
title: 'PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval,
  and Synthesis in Question Answering'
arxiv_id: '2402.16288'
source_url: https://arxiv.org/abs/2402.16288
tags:
- memory
- retrieval
- classification
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PerLTQA, a novel dataset for personal long-term
  memory in question answering. The dataset combines semantic memories (profiles,
  social relationships) and episodic memories (events, dialogues) for 30 characters,
  featuring 8,593 questions and 141 profiles with 1,339 social relationships, 4,501
  events, and 3,409 dialogues.
---

# PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering

## Quick Facts
- arXiv ID: 2402.16288
- Source URL: https://arxiv.org/abs/2402.16288
- Reference count: 10
- Introduces PerLTQA dataset with 8,593 questions and 141 profiles with 1,339 social relationships, 4,501 events, and 3,409 dialogues

## Executive Summary
This paper introduces PerLTQA, a novel dataset designed to advance personal long-term memory capabilities in question answering systems. The dataset uniquely combines semantic memories (profiles, social relationships) and episodic memories (events, dialogues) for 30 distinct characters, totaling 8,593 questions. The authors propose a comprehensive framework for memory integration in QA that includes Memory Classification, Memory Retrieval, and Memory Synthesis components. The framework is evaluated across five large language models (ChatGLM2/3, Qwen-7B, Baichuan2-7B, ChatGPT) and three retrieval methods (DPR, BM25, Contriever).

## Method Summary
The authors developed a three-component framework for memory integration in question answering. The Memory Classification component uses BERT-based models to categorize memories, achieving high precision and recall. The Memory Retrieval component employs DPR, BM25, and Contriever to fetch relevant memories from the dataset. The Memory Synthesis component integrates retrieved memories with LLMs to generate informed responses. The evaluation used multiple metrics including precision, recall, F1-score, accuracy, MAP (Mean Average Precision), and correctness to assess performance across different tasks and models.

## Key Results
- BERT-base achieved superior performance in memory classification (95.96% precision, 95.64% recall, 95.74% F1, 95.64% accuracy)
- Memory classification and retrieval significantly improved LLM performance in generating memory-informed responses
- ChatGPT achieved the best results in memory synthesis with MAP of 0.756 and correctness of 0.573

## Why This Works (Mechanism)
The framework's effectiveness stems from its structured approach to handling personal long-term memory in question answering. By separating the process into classification, retrieval, and synthesis stages, the system can efficiently identify relevant memories, fetch them accurately, and integrate them into coherent responses. The use of BERT for classification leverages its strong semantic understanding capabilities, while the combination of multiple retrieval methods ensures robust memory fetching. The integration with LLMs for synthesis allows for natural language generation that incorporates the retrieved personal memories.

## Foundational Learning
- BERT-based models: Essential for high-accuracy memory classification due to their strong semantic understanding capabilities
  - Why needed: To accurately categorize and classify different types of personal memories
  - Quick check: Compare classification accuracy between BERT and LLM-based approaches
- Memory retrieval techniques (DPR, BM25, Contriever): Critical for fetching relevant memories from large datasets
  - Why needed: To efficiently locate and retrieve pertinent memories from the combined semantic and episodic memory pool
  - Quick check: Evaluate retrieval performance using MAP and recall metrics
- LLM integration for synthesis: Enables natural language generation incorporating retrieved memories
  - Why needed: To produce coherent, memory-informed responses that feel natural to users
  - Quick check: Assess response quality using correctness and human evaluation metrics

## Architecture Onboarding

**Component Map:** BERT Classifier -> Memory Retriever -> LLM Synthesizer

**Critical Path:** Question → Memory Classification → Memory Retrieval → Memory Synthesis → Response

**Design Tradeoffs:** The framework trades computational complexity for improved accuracy by using specialized components for each task. BERT is used for classification rather than LLMs due to superior performance, while LLMs are reserved for the synthesis stage where their natural language generation capabilities are most valuable.

**Failure Signatures:** Poor memory classification leads to irrelevant memories being retrieved, resulting in incorrect or nonsensical responses. Inadequate retrieval performance means relevant memories are missed, limiting the LLM's ability to generate informed responses. Weak synthesis capabilities result in responses that don't effectively incorporate the retrieved memories.

**3 First Experiments:**
1. Test memory classification accuracy on a subset of questions to validate BERT's superiority
2. Compare retrieval performance across DPR, BM25, and Contriever on a validation set
3. Evaluate LLM response quality with and without memory integration to quantify the benefit

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset focuses on only 30 characters, potentially limiting generalizability to broader personal memory scenarios
- Relatively small scale raises questions about scalability to larger, more diverse populations
- Best MAP of 0.756 and correctness of 0.573 in memory synthesis indicate substantial room for improvement

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| BERT-based models significantly outperform LLMs in memory classification | High |
| Memory classification and retrieval substantially improve LLM performance | High |
| Memory synthesis component shows promising but limited performance | Medium |

## Next Checks
1. Evaluate the framework's scalability by testing it on a larger dataset with more diverse characters and memory types to assess whether the observed patterns hold at scale
2. Conduct ablation studies to isolate the specific contributions of memory classification versus retrieval in improving LLM performance, helping to understand which component drives the most significant improvements
3. Perform cross-validation with real-world personal data (with appropriate privacy safeguards) to assess the framework's effectiveness outside the controlled dataset environment and to test its practical applicability for real personal assistant applications