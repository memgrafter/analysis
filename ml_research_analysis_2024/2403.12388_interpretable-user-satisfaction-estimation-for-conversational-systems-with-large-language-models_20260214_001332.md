---
ver: rpa2
title: Interpretable User Satisfaction Estimation for Conversational Systems with
  Large Language Models
arxiv_id: '2403.12388'
source_url: https://arxiv.org/abs/2403.12388
tags:
- user
- satisfaction
- rubric
- conversation
- rubrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPUR uses LLMs to identify interpretable patterns of user satisfaction/dissatisfaction
  in conversational systems. It employs a three-phase iterative prompting framework
  to extract patterns from labeled examples, summarize them into rubrics, and then
  score unseen conversations.
---

# Interpretable User Satisfaction Estimation for Conversational Systems with Large Language Models

## Quick Facts
- arXiv ID: 2403.12388
- Source URL: https://arxiv.org/abs/2403.12388
- Authors: Ying-Chun Lin et al.
- Reference count: 21
- Key outcome: SPUR achieves weighted F1 scores of 75.4% (Bing Copilot), 59.0% (MWOZ), 72.6% (SGD), and 66.3% (ReDial) on user satisfaction estimation tasks

## Executive Summary
SPUR is a three-phase iterative prompting framework that uses LLMs to extract interpretable patterns of user satisfaction and dissatisfaction from conversational systems. The method outperforms embedding-based approaches while providing interpretable rubrics that explain the reasoning behind satisfaction predictions. SPUR achieves this through supervised extraction of patterns from labeled examples, rubric summarization to condense patterns into generalizable categories, and user satisfaction estimation using the learned rubrics. The framework demonstrates strong performance even with limited training data and can be scaled through knowledge distillation into embedding-based models.

## Method Summary
SPUR employs a three-phase iterative prompting framework using GPT-4 to estimate user satisfaction in conversational systems. First, Supervised Extraction identifies satisfaction and dissatisfaction patterns from labeled training conversations. Second, Rubric Summarization condenses these patterns into a manageable set of interpretable rubric items. Third, User Satisfaction Estimation scores new conversations by evaluating the presence of rubric items. The framework can optionally distill learned rubrics into embedding-based classifiers for scalable deployment, combining rubric scores with conversation embeddings to create a 1556-dimensional feature vector for classification.

## Key Results
- SPUR achieves weighted F1 scores of 75.4% on Bing Copilot, 59.0% on MWOZ, 72.6% on SGD, and 66.3% on ReDial
- Outperforms embedding-based methods, particularly on imbalanced datasets where traditional methods struggle
- Demonstrates 13% F1 improvement when using dataset-specific rubrics vs. Bing Copilot rubrics on other datasets
- Knowledge distillation of rubric scores into embedding-based models achieves AUC of 0.916-0.975

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can extract interpretable SAT/DSAT patterns more effectively than embedding-based methods
- Mechanism: The LLM performs semantic reasoning over conversation text to identify specific user behaviors that indicate satisfaction or dissatisfaction, then summarizes these into discrete rubric items
- Core assumption: The LLM's natural language understanding capability is sufficient to accurately identify and categorize conversational patterns that correlate with user satisfaction
- Evidence anchors:
  - [abstract] "LLMs can extract interpretable signals of user satisfaction from their natural language utterances more effectively than embedding-based approaches"
  - [section 4.3] "Embedding methods perform worse than SPUR... embedding methods cannot generalize well, particularly when the class is imbalanced"
  - [corpus] Weak - corpus neighbors don't directly address interpretability of satisfaction patterns
- Break condition: When conversational patterns become too subtle or domain-specific for the LLM to recognize without additional context

### Mechanism 2
- Claim: The three-phase iterative prompting framework improves accuracy and generalization
- Mechanism: Supervised Extraction captures diverse patterns from training data, Rubric Summarization condenses patterns into generalizable categories, and User Satisfaction Estimation applies learned rubrics to score conversations
- Core assumption: The iterative process can effectively summarize thousands of pattern instances into a manageable set of rubric items without losing predictive power
- Evidence anchors:
  - [section 3] Describes the three-phase process and its necessity for "ensuring accuracy, generalization, and interpretability"
  - [section 4.4] Shows 13% F1 improvement when using dataset-specific rubrics vs. Bing Copilot rubrics
  - [corpus] Weak - no direct corpus evidence for the three-phase mechanism
- Break condition: When the rubric summarization process oversimplifies patterns to the point of losing discriminative power

### Mechanism 3
- Claim: Knowledge distillation enables scalable deployment of learned rubrics
- Mechanism: The LLM-generated rubric scores are used to train embedding-based classifiers that can predict rubric item presence without LLM prompting
- Core assumption: The embedding space captures sufficient information to predict rubric item presence with reasonable accuracy
- Evidence anchors:
  - [section 4.7] Shows ROC curves for distilled models achieving AUC of 0.916-0.975
  - [section 4.7] "ada-002 is the most effective text embedding model for knowledge distillation"
  - [corpus] Weak - corpus doesn't address knowledge distillation approaches
- Break condition: When the embedding space fails to capture the nuanced language patterns needed to predict rubric items

## Foundational Learning

- Concept: Few-shot learning with LLMs
  - Why needed here: SPUR operates with limited training data (as low as 0.8% of Bing Copilot data)
  - Quick check question: What makes few-shot prompting different from zero-shot prompting in terms of performance on USE tasks?

- Concept: Knowledge distillation
  - Why needed here: LLM-based SPUR is computationally expensive for web-scale deployment
  - Quick check question: How does knowledge distillation from LLM rubrics to embedding-based classifiers maintain performance while reducing cost?

- Concept: Multi-turn conversation analysis
  - Why needed here: User satisfaction is expressed across multiple turns, not just single utterances
  - Quick check question: How does SPUR handle the sequential nature of satisfaction signals across conversation turns?

## Architecture Onboarding

- Component map:
  GPT-4 LLM for Supervised Extraction -> GPT-4 LLM for Rubric Summarization -> GPT-4 LLM for User Satisfaction Estimation -> Optional: Linear/Logistic Regression with ada-002 embeddings
- Critical path: Prompt GPT-4 with training conversations → Extract patterns → Summarize into rubrics → Score new conversations
- Design tradeoffs:
  - LLM accuracy vs. computational cost (addressed by knowledge distillation)
  - Rubric specificity vs. generalizability (addressed by iterative summarization)
  - Feature completeness vs. dimensionality (addressed by embedding + rubric concatenation)
- Failure signatures:
  - Poor rubric coverage → Check if Supervised Extraction is capturing diverse patterns
  - Low distillation accuracy → Check if embedding captures rubric-relevant features
  - Overfitting on small datasets → Check if rubric summarization is over-specific
- First 3 experiments:
  1. Run SPUR on a small dataset and manually inspect generated rubrics for interpretability
  2. Test knowledge distillation by comparing LLM rubric scores vs. distilled classifier predictions
  3. Compare SPUR performance vs. embedding-only baselines across different training set sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is the knowledge distillation process for SPUR rubrics across different domains and languages?
- Basis in paper: [explicit] The paper mentions using knowledge distillation to scale SPUR but only evaluates it on two rubric items (Gratitude and Negative Feedback) from the Bing Copilot dataset.
- Why unresolved: The evaluation is limited to a single dataset and two rubric items. It's unclear how well the distillation generalizes to other domains, languages, or different types of rubric items.
- What evidence would resolve it: Comprehensive evaluation of knowledge distillation across multiple domains (e.g., customer service, healthcare, education), languages, and a wider range of rubric items, measuring both accuracy and computational efficiency.

### Open Question 2
- Question: What is the optimal balance between rubric items and conversation embeddings for maximizing USE performance?
- Basis in paper: [explicit] The paper shows that combining SPUR rubrics with conversation embeddings improves performance, but doesn't explore the optimal ratio or interaction between these features.
- Why unresolved: The paper uses a fixed number of rubric items (10 SAT and 10 DSAT) and doesn't investigate how performance changes with different numbers of rubric items or how they interact with embedding features.
- What evidence would resolve it: Systematic ablation studies varying the number of rubric items, their weights, and their combination with different embedding models and architectures to identify the optimal feature set for USE.

### Open Question 3
- Question: How does SPUR's performance degrade with smaller training set sizes, and what is the minimum viable training data?
- Basis in paper: [explicit] The paper evaluates SPUR on small training sets (0.8% to 5% of data) but doesn't explore the lower bounds of this range or how performance scales with decreasing data.
- Why unresolved: The smallest training size evaluated is 0.8% of the data, which may still be too large for many real-world applications. The relationship between training set size and performance isn't fully characterized.
- What evidence would resolve it: Extensive evaluation across a wider range of training set sizes (e.g., 0.1%, 0.01%, 0.001%) to determine the minimum viable training data and characterize the performance degradation curve.

## Limitations
- Heavy reliance on GPT-4's pattern recognition capabilities, which may not generalize to specialized domains or languages
- Limited specification of prompt templates for the first two phases of the framework, making exact reproduction challenging
- Knowledge distillation assumes embedding spaces can adequately capture nuanced rubric patterns, which may not hold for complex conversational phenomena

## Confidence
- **High Confidence**: The experimental results showing SPUR's superior performance over embedding-based baselines (75.4% F1 on Bing Copilot, 59.0% on MWOZ, 72.6% on SGD, 66.3% on ReDial) are well-supported by the data and methodology
- **Medium Confidence**: The interpretability claims are supported by the rubric generation mechanism, but the actual interpretability quality depends on subjective assessment of the generated rubrics
- **Low Confidence**: The scalability of the approach through knowledge distillation is promising but relies on assumptions about embedding space adequacy that weren't thoroughly validated across diverse domains

## Next Checks
1. Conduct ablation studies removing each phase of the three-phase framework to quantify individual contributions to overall performance
2. Test the rubric distillation approach on a held-out dataset from a completely different domain to assess generalization beyond the four studied datasets
3. Perform a user study where human evaluators assess the interpretability and usefulness of the generated rubrics for understanding satisfaction patterns in real conversations