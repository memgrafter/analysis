---
ver: rpa2
title: 'Learning to grok: Emergence of in-context learning and skill composition in
  modular arithmetic tasks'
arxiv_id: '2406.02550'
source_url: https://arxiv.org/abs/2406.02550
tags:
- tasks
- figure
- task
- pre-training
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the emergence of in-context learning and
  skill composition in transformers on modular arithmetic tasks. A dataset of linear
  modular functions is constructed, with some tasks used for pre-training and others
  for out-of-distribution testing.
---

# Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks

## Quick Facts
- arXiv ID: 2406.02550
- Source URL: https://arxiv.org/abs/2406.02550
- Reference count: 40
- Primary result: Transformers exhibit transition from memorization to algorithmic generalization on modular arithmetic tasks as pre-training task diversity increases.

## Executive Summary
This paper investigates the emergence of in-context learning (ICL) and skill composition in transformers using modular arithmetic tasks. The authors construct a dataset of linear modular functions and train transformer models with varying depths to study how they transition from in-distribution memorization to out-of-distribution generalization. They observe that with sufficient pre-training task diversity, models develop algorithmic capabilities to infer task vectors from few-shot examples. The study combines empirical analysis with mechanistic interpretability to reveal structured representations and modular arithmetic operations implemented by attention heads and MLPs.

## Method Summary
The authors create modular arithmetic tasks of the form z = a*x + b*y mod p, where (a,b) are task vectors and (x,y) are input vectors. Tasks are split into in-distribution and out-of-distribution sets for pre-training and testing. GPT-style transformers with rotary positional embedding, ReLU activation, and tied embeddings/output layer are trained using AdamW optimizer with cosine annealing learning rate schedule. Models of depths d=2,4,6 are evaluated on their ability to generalize from in-distribution to out-of-distribution tasks, with performance measured by accuracy on four sets (in-distribution train/test, out-of-distribution train/test).

## Key Results
- A transition from in-distribution to out-of-distribution generalization is observed as the number of pre-training tasks increases.
- The smallest model capable of out-of-distribution generalization requires two transformer blocks (d=2).
- Deeper models (d=4,6) exhibit a transient generalization phase requiring early stopping to avoid transient memorization dominance.
- Interpretability analysis reveals highly structured representations in attention heads and MLPs, with an algorithmic shift observed in deeper models when increasing the number of in-context examples.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model transitions from memorization to algorithmic generalization when the number of pre-training tasks becomes large enough.
- Mechanism: With insufficient task diversity, the model memorizes task vectors and matches inputs to memorized tasks. When task diversity exceeds a threshold, the model must derive task vectors algorithmically from few-shot examples, using modular arithmetic operations.
- Core assumption: Modular arithmetic tasks are structured enough that the model can infer the underlying linear function from a small number of examples.
- Evidence anchors:
  - [abstract] "We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases."
  - [section 4.1] "We observe that an increase in ni.d. enhances the in-context sample efficiency, i.e. the model generalizes at inference time with fewer few-shot examples."
  - [corpus] Weak evidence; corpus neighbors discuss compositional reasoning but do not directly anchor the memorization-to-generalization transition in modular arithmetic.

### Mechanism 2
- Claim: The model implements a scale-and-combine algorithm using learned modular arithmetic operations.
- Mechanism: Given k-shot examples, the model finds constants ci such that c1(x1,y1)+…+ck(xk,yk)=(x,y) mod p, then predicts z=c1z1+…+ckzk mod p.
- Core assumption: The model can solve linear systems over GF(p) and perform modular addition/multiplication in context.
- Evidence anchors:
  - [section 5] "For example, in the above case, the model needs to find the two constants c1 and c2 such that c1(x1, y1) + c2(x2, y2) = (x, y) mod p. Once the model has figured out the constants, the result can be simply computed with modular addition."
  - [section 5.2] Attention heads are found that implement "Modular Map", "Multiplication", and "Addition" skills.
  - [corpus] Weak; corpus neighbors discuss compositional reasoning but do not detail the scale-and-combine mechanism.

### Mechanism 3
- Claim: Model depth and task diversity interact to control the stability and quality of generalization.
- Mechanism: Shallow models (d=2) can generalize but imperfectly due to limited capacity; deeper models (d=4,6) can better implement the algorithm but require early stopping to avoid transient memorization dominance.
- Core assumption: Model capacity must match task complexity for stable generalization.
- Evidence anchors:
  - [section 4.2] "We observe that those phase diagrams across different depths are qualitatively similar, where the o.o.d. generalization only emerges with a large enough number of pre-training tasks. As model capacity decreases, performance on both the pre-training set and the o.o.d. test set degrades."
  - [section 5.1] "We attribute this disparity to the limited capacity of d = 2 models."
  - [corpus] Weak; corpus neighbors discuss model scale effects but not depth-specific transient generalization.

## Foundational Learning

- Concept: Modular arithmetic over finite fields GF(p)
  - Why needed here: The task requires solving linear equations modulo p and performing modular operations.
  - Quick check question: What is the inverse of 3 mod 29?

- Concept: In-context learning (ICL)
  - Why needed here: The model must infer task vectors from few-shot examples without explicit labels.
  - Quick check question: How many examples are needed to uniquely determine a linear function over Z29?

- Concept: Principal Component Analysis (PCA) for interpretability
  - Why needed here: PCA reveals structured representations like "clock-of-clocks" in attention head outputs.
  - Quick check question: What does a 2D PCA plot of (log27 x, log27 y) reveal about the model's modular multiplication encoding?

## Architecture Onboarding

- Component map: Embedding → Attention (skill I/II) → MLP (skill III) → Output
- Critical path: Embedding → Attention (skill I/II) → MLP (skill III) → Output
- Design tradeoffs: Depth vs. generalization stability; embedding structure vs. attention head complexity
- Failure signatures: High pre-training loss plateau; inability to generalize out-of-distribution; transient ICL performance
- First 3 experiments:
  1. Train a d=2 model with ni.d.=128 and α=0.6; verify o.o.d. generalization on So.o.d.test.
  2. Analyze attention head 1, layer 1 PCA projections for "clock-of-clocks" structure.
  3. Corrupt one in-context label and observe model robustness for d=4 vs d=2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which the transformer model combines in-context examples to perform out-of-distribution generalization?
- Basis in paper: [explicit] The paper discusses the importance of skills I (Modular Map), II (Multiplication), and III (Addition) for the model to combine in-context examples. It also mentions that deeper models like d=4 outperform shallower ones like d=2 in skill III.
- Why unresolved: The paper provides evidence that the model combines examples, but the exact algorithm and the role of each component (attention heads, MLP layers, etc.) in this process are not fully elucidated.
- What evidence would resolve it: A detailed mechanistic interpretability analysis of the model's internal representations and operations, particularly focusing on how the MLP and LayerNorm layers contribute to the combination of in-context examples.

### Open Question 2
- Question: How does the choice of task vectors during pre-training affect the emergence of out-of-distribution generalization?
- Basis in paper: [explicit] The paper mentions that task vectors are selected using a rectangular rule to reduce learning complexity, but the effect of this structured selection on the emergence of o.o.d. generalization is not explored.
- Why unresolved: The paper only discusses the rectangular rule for task selection but does not investigate how different task vector selections might influence the model's ability to generalize.
- What evidence would resolve it: An ablation study comparing the emergence of o.o.d. generalization for models trained with randomly selected task vectors versus those trained with structured selections like the rectangular rule.

### Open Question 3
- Question: What is the role of the MLP and LayerNorm layers in implementing the skills necessary for modular arithmetic tasks?
- Basis in paper: [explicit] The paper suggests that skill III (Addition) is implemented within the MLP layer but does not provide conclusive evidence. It also mentions that no obvious signals were found in LayerNorm.
- Why unresolved: The paper acknowledges the potential role of MLP and LayerNorm but does not perform a detailed analysis of these layers' contributions.
- What evidence would resolve it: A thorough investigation of the MLP and LayerNorm layers' activations and weight matrices, looking for patterns or structures that correspond to the modular arithmetic operations.

### Open Question 4
- Question: How does the model's performance on out-of-distribution tasks scale with the size of the training set and the number of tasks?
- Basis in paper: [explicit] The paper shows that increasing the number of training tasks improves out-of-distribution generalization, but the relationship between training set size, number of tasks, and performance is not quantified.
- Why unresolved: The paper provides qualitative insights into the effect of task diversity but does not establish a precise scaling relationship.
- What evidence would resolve it: A systematic study varying the training set size and number of tasks, and measuring the resulting out-of-distribution performance to establish a scaling law.

## Limitations

- The structured task selection using the "rectangular rule" is not fully specified, which could affect reproducibility of the pre-training task diversity threshold that triggers the transition from memorization to generalization.
- The transient generalization phenomenon in deeper models requires early stopping, but the consistency and timing of this phase across random seeds is not well-characterized.
- The interpretability analysis relies on PCA projections whose stability across different runs and their relationship to actual computational mechanisms are not fully established.

## Confidence

**High Confidence**: The existence of a transition from in-distribution to out-of-distribution generalization as pre-training task diversity increases.

**Medium Confidence**: The mechanism by which models implement the scale-and-combine algorithm for modular arithmetic.

**Low Confidence**: The stability and reproducibility of the transient generalization phase in deeper models.

## Next Checks

1. **Reproduce the transient generalization phase**: Train multiple instances of d=4 and d=6 models with identical hyperparameters but different random seeds, monitoring out-of-distribution accuracy throughout training to establish the consistency and timing of the transient generalization phase.

2. **Validate the scale-and-combine mechanism**: Systematically ablate identified attention heads implementing modular skills and measure the impact on out-of-distribution generalization performance to confirm their causal role in the algorithm.

3. **Characterize PCA projection stability**: Apply the interpretability analysis (PCA on attention head outputs) across multiple training runs and different time points to quantify the stability and consistency of the "clock-of-clocks" patterns as indicators of algorithmic generalization.