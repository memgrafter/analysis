---
ver: rpa2
title: 'Shiksha: A Technical Domain focused Translation Dataset and Model for Indian
  Languages'
arxiv_id: '2412.09025'
source_url: https://arxiv.org/abs/2412.09025
tags:
- translation
- dataset
- languages
- indian
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating technical and
  educational content in Indian languages by creating a high-quality multilingual
  parallel corpus of over 2.8 million sentence pairs across 8 Indian languages. The
  dataset is constructed through bitext mining of human-translated NPTEL video lecture
  transcriptions.
---

# Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages

## Quick Facts
- arXiv ID: 2412.09025
- Source URL: https://arxiv.org/abs/2412.09025
- Authors: Advait Joglekar; Srinivasan Umesh
- Reference count: 13
- Key outcome: Creates Shiksha-MT, a 3.3B parameter model achieving over 2 BLEU improvement on Indian language technical translations

## Executive Summary
This paper addresses the challenge of translating technical and educational content in Indian languages by creating a high-quality multilingual parallel corpus of over 2.8 million sentence pairs across 8 Indian languages. The dataset is constructed through bitext mining of human-translated NPTEL video lecture transcriptions. Using this corpus, the authors fine-tune a 3.3B parameter NLLB-200 model with LoRA, achieving significant improvements in both in-domain technical translation tasks and out-of-domain translation on the Flores+ benchmark, with over 2 BLEU improvement on average across the Indian languages. The resulting model, Shiksha-MT, demonstrates superior performance compared to existing models like NLLB-200 and IndicTrans2, particularly for technical domain translations. The work also presents Translingua, a practical tool built using these models to assist human translators in creating subtitles for NPTEL lectures.

## Method Summary
The authors create a technical domain translation dataset by mining human-translated NPTEL video lecture transcriptions using LABSE embeddings and SentAlign for high-quality bitext alignment. They then fine-tune a 3.3B parameter NLLB-200 model using LoRA (Low-Rank Adaptation) with curriculum learning and additional BPCC corpus data. The fine-tuning process employs PEFT to efficiently adapt the large model to technical domain content while maintaining computational efficiency. The resulting model, Shiksha-MT, is integrated into Translingua, a tool for assisting human translators with NPTEL lecture transcript translation.

## Key Results
- Shiksha-MT achieves over 2 BLEU improvement on average across Indian languages compared to NLLB-200 and IndicTrans2
- Significant improvements in technical domain translation accuracy, particularly for scientific and educational content
- Successful deployment of Translingua tool for real-world human translator assistance with NPTEL content
- Model demonstrates strong performance on both in-domain technical tasks and out-of-domain Flores+ benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bitext mining with LABSE embeddings improves alignment accuracy for technical-domain content.
- Mechanism: LABSE provides semantically meaningful sentence embeddings that capture technical terminology, enabling more precise matching of English-Indic sentence pairs even with complex technical terms.
- Core assumption: LABSE embeddings maintain semantic coherence across English and Indic languages for technical vocabulary.
- Evidence anchors:
  - [abstract]: "We achieve this by bitext mining human-translated transcriptions of NPTEL video lectures."
  - [section 4.3]: "We use SentAlign (Steingrimsson et al., 2023) which employs LABSE (Feng et al., 2022) along with optimized alignment algorithms to mine parallel documents with high accuracy and efficiency."
  - [corpus]: Weak - corpus section doesn't directly address LABSE quality for technical terms.
- Break condition: If LABSE embeddings fail to capture domain-specific semantic nuances, alignment accuracy degrades significantly.

### Mechanism 2
- Claim: LoRA fine-tuning enables efficient adaptation of large NLLB-200 models to technical domains.
- Mechanism: LoRA adds low-rank matrices to transformer weight updates, allowing domain adaptation without full model retraining, reducing compute requirements while maintaining performance.
- Core assumption: Low-rank decomposition captures domain-specific adaptation signals effectively.
- Evidence anchors:
  - [section 5.2]: "We utilize a Parameter-Efficient Fine Tuning (PEFT) method known as Low-Rank Adaptation (LoRA) (Hu et al., 2022) to train our model."
  - [section 5.2]: "Evaluation results for all the three models were found to be similar, with our 3rd approach performing slightly better."
  - [corpus]: Weak - corpus section doesn't directly validate LoRA's effectiveness for technical domains.
- Break condition: If technical domain requires full model adaptation, LoRA may be insufficient.

### Mechanism 3
- Claim: Domain-specific training data from NPTEL improves translation quality for technical terms.
- Mechanism: Training on NPTEL lecture transcriptions exposes the model to technical terminology and educational context, improving translation accuracy for scientific and technical content.
- Core assumption: NPTEL content covers representative technical terminology for Indian educational contexts.
- Evidence anchors:
  - [abstract]: "Neural Machine Translation (NMT) models are typically trained on datasets with limited exposure to Scientific, Technical and Educational domains."
  - [section 2]: "From Table 1 we can see that both Google Translate 2 and IndicTrans2 (IT2) (Gala et al., 2023) get their Hindi translations wrong."
  - [section 6]: "Our models are now built into a tool called Translingua, that is being widely used by human annotators across India to translate NPTEL lecture transcripts."
- Break condition: If technical terminology changes rapidly or NPTEL coverage is insufficient, model performance degrades.

## Foundational Learning

- Concept: Bitext mining and sentence alignment
  - Why needed here: Essential for creating high-quality parallel corpus from NPTEL transcriptions
  - Quick check question: What is the primary challenge in aligning sentences from bilingual documents with timestamps and code-mixing?
- Concept: LoRA (Low-Rank Adaptation) for efficient fine-tuning
  - Why needed here: Enables adaptation of 3.3B parameter NLLB-200 model without full fine-tuning
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning?
- Concept: Multilingual embeddings and semantic similarity
  - Why needed here: LABSE embeddings enable cross-lingual alignment and quality assessment
  - Quick check question: Why are multilingual embeddings particularly important for technical domain translation?

## Architecture Onboarding

- Component map: NPTEL transcription extraction -> cleaning -> bitext mining -> corpus creation -> NLLB-200 base -> LoRA fine-tuning -> evaluation -> Translingua deployment
- Critical path: Data cleaning -> bitext mining -> model fine-tuning -> evaluation -> deployment
- Design tradeoffs: LoRA vs full fine-tuning (efficiency vs potential performance), technical domain focus vs general translation capability
- Failure signatures: Poor alignment quality (low LABSE scores), translation errors on technical terms, deployment tool performance issues
- First 3 experiments:
  1. Test LABSE-based bitext mining on a small NPTEL subset to verify alignment quality
  2. Compare LoRA fine-tuning with different rank values on technical domain test set
  3. Evaluate model performance on out-of-domain data to assess generalization

## Open Questions the Paper Calls Out

None

## Limitations

- Data Quality Uncertainty: The paper relies heavily on bitext mining of NPTEL transcriptions, but the actual quality and coverage of the mined corpus is not independently verified.
- Model Generalization: The significant BLEU improvements (2+ points) are demonstrated primarily on in-domain technical tasks and Flores+ benchmark, with uncertain performance on general domain translations.
- Technical Domain Coverage: The NPTEL-based corpus may have inherent biases toward engineering and computer science terminology, potentially limiting effectiveness for other scientific domains.

## Confidence

**High Confidence**:
- The creation of a large-scale technical domain parallel corpus from NPTEL transcriptions is well-documented and the methodology is clearly described.
- The LoRA fine-tuning approach and its implementation details are sufficiently specified for reproduction.
- The Translingua tool's practical utility for human translators is demonstrated through real-world deployment.

**Medium Confidence**:
- The reported BLEU improvements over existing models are credible but would benefit from independent validation on diverse test sets.
- The claim that Shiksha-MT outperforms NLLB-200 and IndicTrans2 for technical translations is supported but limited to the specific datasets used.

**Low Confidence**:
- The assertion that the model maintains high quality for out-of-domain translations without extensive testing across diverse domains.
- The long-term sustainability of the model's performance as technical terminology evolves.

## Next Checks

1. **Alignment Quality Audit**: Perform an independent evaluation of the mined parallel corpus by sampling 100 random aligned sentence pairs and manually verifying their quality, particularly focusing on technical terminology accuracy and code-mixing handling.

2. **Cross-Domain Generalization Test**: Evaluate the fine-tuned model on technical content from domains not represented in NPTEL (e.g., medical texts, legal documents) to assess whether the 2+ BLEU improvement generalizes beyond the training domain.

3. **Long-Tail Vocabulary Coverage**: Analyze the model's performance on rare technical terms by creating a test set of specialized terminology from various scientific fields and measuring translation accuracy, identifying potential coverage gaps in the NPTEL-based corpus.