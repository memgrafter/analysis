---
ver: rpa2
title: 'BAMBINO-LM: (Bilingual-)Human-Inspired Continual Pretraining of BabyLM'
arxiv_id: '2406.11418'
source_url: https://arxiv.org/abs/2406.11418
tags:
- language
- learning
- bambino-lm
- italian
- babylm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving the ability of small-scale
  language models to acquire a second language while mitigating degradation in their
  primary language. The authors propose BAMBINO-LM, a continual pre-training strategy
  that uses a combination of alternation and PPO-based perplexity reward induced from
  a parent Italian model.
---

# BAMBINO-LM: (Bilingual-)Human-Inspired Continual Pretraining of BabyLM

## Quick Facts
- **arXiv ID**: 2406.11418
- **Source URL**: https://arxiv.org/abs/2406.11418
- **Reference count**: 11
- **Primary result**: BAMBINO-LM significantly improves Italian language capability (0.3416 to 0.4613) while slightly decreasing English capability (0.6255 to 0.5503) compared to BabyLM baseline

## Executive Summary
This paper introduces BAMBINO-LM, a continual pretraining strategy that improves small-scale language models' ability to acquire a second language while mitigating degradation in their primary language. The method uses an alternating training approach between causal language modeling (CLM) and proximal policy optimization (PPO) with perplexity-based rewards from a parent Italian model. The key innovation is the combination of large-scale language exposure through CLM with targeted feedback through PPO, enabling effective bilingual language acquisition in a 125M parameter model.

## Method Summary
BAMBINO-LM employs an alternating training strategy that combines causal language modeling (CLM) with proximal policy optimization (PPO) using perplexity-based rewards. The baby model, starting from BabyLM's English-pretrained 125M parameter OPT model, undergoes continual pretraining on Italian data. The alternation between CLM and PPO phases creates a dynamic learning environment where the model receives both broad language exposure and specific corrective feedback from a parent Italian model. The PPO reward function computes the difference between the parent model's perplexity of the baby model's output and a threshold, creating a gradient that pushes the baby model toward more natural Italian patterns.

## Key Results
- Italian language task performance improved from 0.3416 to 0.4613 average score
- English language task performance slightly decreased from 0.6255 to 0.5503 average score
- Ablation analysis shows that employing both alternation strategy and PPO-based modeling is key to effectiveness gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Alternating between CLM and PPO-based training phases enables effective bilingual language acquisition by combining large-scale language exposure with targeted feedback.
- **Mechanism**: The alternating run strategy (Algorithm 1) alternates between standard causal language modeling (CLM) and proximal policy optimization (PPO) with perplexity-based rewards from a parent Italian model.
- **Core assumption**: The alternation frequency and ratio between CLM and PPO phases (10:2 as implemented) is optimal for maintaining language balance while avoiding overfitting to either objective.
- **Evidence anchors**:
  - [abstract] "BAMBINO-LM significantly improves the Italian language capability of a BabyLM baseline"
  - [section] "Using multiple rewards is shown beneficial for reinforcement learning (Dann et al., 2023)"
- **Break condition**: If the alternation ratio is skewed too heavily toward either CLM or PPO, the model may either fail to acquire proper language patterns or become too dependent on the parent model's feedback.

### Mechanism 2
- **Claim**: PPO-based perplexity rewards provide effective feedback signals that guide the baby model's language generation toward more natural Italian patterns.
- **Mechanism**: The reward function R(yB) = α × β(PPLP(yB) - τ) computes the difference between the parent model's perplexity of the baby model's output and a threshold.
- **Core assumption**: Perplexity from a well-trained Italian parent model serves as an appropriate proxy for "naturalness" or "correctness" in Italian language generation.
- **Evidence anchors**:
  - [abstract] "incorporating a perplexity-based reward for language model pre-training"
  - [section] "PPLP represents the perplexity of the parent model P for the sequence yB"
- **Break condition**: If the parent model itself has biases or the perplexity metric doesn't align with human judgments of language quality, the feedback may reinforce incorrect patterns.

### Mechanism 3
- **Claim**: Starting from a pre-trained English model and continuing with Italian data creates a natural progression that mirrors human bilingual acquisition.
- **Mechanism**: The baby model begins with English language knowledge from BabyLM pretraining, then undergoes continual pretraining on Italian data while receiving feedback from an Italian parent model.
- **Core assumption**: Pre-existing language knowledge provides a scaffold that facilitates learning of related languages, similar to how human children use their L1 knowledge when acquiring L2.
- **Evidence anchors**:
  - [abstract] "Children from bilingual backgrounds benefit from interactions with parents and teachers to re-acquire their heritage language"
  - [section] "We experiment with BabyLM trained on English, and continually pretrain this model on an assumed second language, Italian"
- **Break condition**: If the languages are too dissimilar or the initial English model has strong biases, the transfer may be ineffective or even harmful to Italian acquisition.

## Foundational Learning

- **Concept**: Reinforcement Learning with Proximal Policy Optimization (PPO)
  - **Why needed here**: PPO provides a stable way to update the model's policy based on the parent model's feedback without causing catastrophic forgetting of English capabilities
  - **Quick check question**: What is the purpose of the clipping mechanism in PPO's objective function, and how does it help maintain training stability?

- **Concept**: Causal Language Modeling (CLM)
  - **Why needed here**: CLM serves as the foundation for language modeling pretraining, providing the base learning mechanism before PPO feedback is applied
  - **Quick check question**: How does the CLM objective function differ from masked language modeling, and why is it more appropriate for this bilingual acquisition scenario?

- **Concept**: Perplexity as a Quality Metric
  - **Why needed here**: Perplexity from the parent Italian model serves as a quantitative measure of how well the baby model's Italian generation aligns with natural Italian patterns
  - **Quick check question**: What are the limitations of using perplexity as a reward signal, and how might they affect the model's learning trajectory?

## Architecture Onboarding

- **Component map**: BabyLM model (English-pretrained, 125M parameters) → Italian dataset → Alternating training loop (CLM phase + PPO phase) → Parent Italian model (gpt2-small-italian) → Perplexity-based reward computation → Updated BabyLM parameters
- **Critical path**: Data preprocessing → Initial English pretraining → Continual Italian pretraining with alternating CLM/PPO → Evaluation on zero-shot tasks
- **Design tradeoffs**: Using a smaller model (125M) limits performance but makes the experiment more comparable to human language acquisition; the alternation ratio affects the balance between exploration (CLM) and exploitation (PPO)
- **Failure signatures**: English performance degradation beyond expected levels, Italian task performance plateauing, training instability during PPO phases, excessive repetition of words or phrases
- **First 3 experiments**:
  1. Test different alternation ratios (e.g., 5:1, 10:1, 20:1 for CLM:PPO) to find optimal balance
  2. Experiment with different perplexity threshold values (τ) to see how sensitivity affects learning
  3. Compare performance using different parent models (e.g., different Italian model sizes or architectures) to validate the parent model choice

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the alternating strategy between CLM and PPO objectives actually improve second language acquisition, or does it merely prevent degradation of the first language?
- **Basis in paper**: [explicit] The authors note that alternating runs prevent "exploitation" where the baby model repeats parent model utterances, and ablation shows performance drops without alternation.
- **Why unresolved**: The current evaluation focuses on absolute performance gains rather than comparing the alternating approach to pure second language modeling or other mixing strategies.
- **What evidence would resolve it**: Direct comparison of BAMBINO-LM against a baseline that trains exclusively on the second language, or experiments testing different alternation frequencies and ratios.

### Open Question 2
- **Question**: How well does BAMBINO-LM generalize to languages that are structurally very different from English, particularly those with different scripts or tokenization schemes?
- **Basis in paper**: [inferred] The authors explicitly note this limitation, stating the method "must be validated for other languages, especially languages that are distant from English or those that use a different set of tokens."
- **Why unresolved**: All experiments were conducted with Italian, which shares the Latin alphabet with English and has similar tokenization requirements.
- **What evidence would resolve it**: Replicating the study with languages like Mandarin, Arabic, or Hindi, and analyzing performance differences based on linguistic distance and tokenization complexity.

### Open Question 3
- **Question**: Could the perplexity-based reward signal be improved by incorporating linguistic quality metrics or direct constructive corrections, and would this enhance learning efficiency?
- **Basis in paper**: [explicit] The authors suggest exploring "alternative metrics and different reward learning mechanisms that better align with human feedback behaviors" and mention that rewards could "capture linguistic quality and provide direct, 'constructive' corrections."
- **Why unresolved**: The current implementation uses only perplexity, which the authors acknowledge may not be optimal and is influenced by multiple factors of the parent model.
- **What evidence would resolve it**: Experiments comparing perplexity-based rewards against alternatives like grammaticality scores, semantic coherence metrics, or human-annotated feedback quality, measuring both acquisition speed and final performance.

## Limitations
- Dataset creation process is not fully specified, making exact reproduction challenging
- Evaluation is limited to zero-shot classification tasks, which may not fully capture bilingual capabilities
- The alternation ratio of 10:2 between CLM and PPO phases is presented without extensive ablation to justify this specific choice

## Confidence
- **High confidence**: The core mechanism of alternating between CLM and PPO-based training is well-explained and the empirical results show clear improvements in Italian capability (0.3416 to 0.4613 average score)
- **Medium confidence**: The claim that PPO-based perplexity rewards effectively guide language acquisition is supported by results but relies on assumptions about the parent model's quality
- **Low confidence**: The claim that this approach mirrors human bilingual acquisition is largely metaphorical and lacks direct empirical support

## Next Checks
1. Conduct ablation studies on different alternation ratios (5:1, 10:1, 20:1 CLM:PPO) to determine optimal balance and validate the chosen 10:2 ratio
2. Test the approach with different language pairs beyond English-Italian to assess generalizability and identify any language-specific factors affecting performance
3. Evaluate the model on generation tasks and other language competencies beyond zero-shot classification to provide a more comprehensive assessment of bilingual capabilities