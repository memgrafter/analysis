---
ver: rpa2
title: 'Local to Global: Learning Dynamics and Effect of Initialization for Transformers'
arxiv_id: '2406.03072'
source_url: https://arxiv.org/abs/2406.03072
tags:
- loss
- lemma
- local
- proof
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how transformers learn Markov chains by analyzing
  the gradient flow dynamics of single-layer transformers with first-order Markov
  input data. The authors introduce a canonical low-rank parameterization of the transformer
  parameters and show that depending on the Markovian data properties (specifically
  the switching factor p+q) and the parameter initialization, the transformer parameters
  can converge to either global or local minima of the next-token prediction loss.
---

# Local to Global: Learning Dynamics and Effect of Initialization for Transformers

## Quick Facts
- **arXiv ID**: 2406.03072
- **Source URL**: https://arxiv.org/abs/2406.03072
- **Reference count**: 40
- **Primary result**: Transformers trained on next-token prediction loss with Markovian data can converge to either global or local minima depending on initialization and data properties

## Executive Summary
This paper analyzes how transformers learn Markov chains by studying gradient flow dynamics in a canonical low-rank parameterization. The authors show that transformer parameters can converge to either global or local minima of the next-token prediction loss, depending on the Markovian data properties (switching factor p+q) and parameter initialization. They precisely characterize the conditions under which this convergence occurs and provide practical guidelines for parameter initialization. Empirically, they validate that their findings apply more broadly to the full transformer architecture, not just the simplified canonical model.

## Method Summary
The paper studies single-layer transformers with first-order Markov input data by introducing a canonical low-rank parameterization of transformer parameters. The analysis focuses on gradient flow dynamics and how the energy function E(e,w) = e² - (w² + sign(w)·log|w|) remains constant along flow trajectories when w ≠ 0. By characterizing critical points in the loss landscape and their basins of attraction, the authors determine convergence behavior based on initialization schemes and Markovian data properties.

## Key Results
- Transformer parameters converge to global or local minima depending on initialization and switching factor p+q
- Standard Gaussian initialization around origin leads to different convergence basins for p+q > 1 vs p+q < 1
- A data-agnostic initialization region exists that converges to global minima regardless of p+q value
- Empirical validation shows findings extend to full transformer architecture beyond the simplified model

## Why This Works (Mechanism)

### Mechanism 1
The transformer parameters can converge to either global or local minima depending on the initialization and Markovian data properties (switching factor p+q). The gradient flow dynamics preserve a low-rank structure in the parameters during training. The energy function E(e,w) = e² - (w² + sign(w)·log|w|) remains constant along the flow trajectory when w ≠ 0, creating contour lines that intersect with the loss landscape's critical points. The switching factor p+q determines whether the global minima lie in the negative half-plane (w < -1/√2 when p+q > 1) or positive half-plane (w > 0 when p+q < 1).

### Mechanism 2
Standard Gaussian initialization around origin leads to different convergence basins depending on whether p+q > 1 or p+q < 1. For p+q > 1, small initialization (σ² ≪ 1/√2) places parameters in the basin Imin, which contains local minima on the w-axis (w > 0 or w ∈ (-1/√2, 0)). For p+q < 1, the same initialization places parameters in I⋆, the basin for global minima. The energy barrier at w = 0 prevents flow between basins.

### Mechanism 3
There exists a data-agnostic initialization region that converges to global minima regardless of p+q value. The region Icommon = {(e,w) : w < 0, |e| > √(w² - log(-w) + Esad)} lies above the saddle-asymptotes in the negative half-plane and intersects with the global minima loci for both p+q < 1 and p+q > 1. This provides a universal initialization strategy.

## Foundational Learning

- **Concept: Energy conservation in gradient flow**
  - Why needed here: The proof relies on showing that the energy function E(e,w) remains constant along the gradient flow trajectory, which allows characterizing convergence to specific critical points.
  - Quick check question: If a trajectory starts at (e₀,w₀) with w₀ ≠ 0, what must be true about E(θₜ) for all t ≥ 0?

- **Concept: Low-rank matrix factorization**
  - Why needed here: The analysis assumes weight matrices maintain rank-one structure during training, reducing the problem from high-dimensional to low-dimensional parameter space.
  - Quick check question: If W ∈ R^(4d×d) has rank-one structure, how many scalar parameters are needed to describe it completely?

- **Concept: Critical point classification in non-convex optimization**
  - Why needed here: The loss landscape contains multiple critical points (global minima, local minima, saddle points, local maxima), and the analysis must characterize which basin each initialization belongs to.
  - Quick check question: Given a Hessian with eigenvalues {λ₁ > 0, λ₂ = 0, λ₃ < 0}, what type of critical point is this?

## Architecture Onboarding

- **Component map**: Embedding layer (e, {pn}) → Attention layer (W_V, attention weights) → Feed-forward layer (W₁, W₂) → Linear layer (a, b)
- **Critical path**: Embedding → Attention → Feed-forward → Linear → Prediction
- **Design tradeoffs**: Weight-tying (e = a) simplifies analysis but may not reflect all practical implementations. Linear attention assumption simplifies attention weights but differs from softmax attention.
- **Failure signatures**: Parameters not maintaining rank-one structure during training, gradient flow crossing energy barriers, or initialization falling outside analyzed basins.
- **First 3 experiments**:
  1. Initialize with standard Gaussian (σ = 0.02) and train on binary Markov chain with p+q = 0.8; verify convergence to global minimum by checking if prediction matches Markov kernel.
  2. Initialize with data-agnostic choice (e = 0.5, w₁ = 1, w₂ = -1) and train on same data; verify convergence to optimal bigram loss.
  3. Initialize with small Gaussian and train on binary Markov chain with p+q = 1.2; verify convergence to unigram loss (local minimum).

## Open Questions the Paper Calls Out

### Open Question 1
How do the learning dynamics of deeper transformers with Markovian inputs differ from the single-layer case studied in this paper? The paper states "analysis of initialization effects for deeper architectures is an interesting avenue of future research" (page 9). This remains unresolved because the paper only analyzes single-layer transformers with first-order Markov chains, while practical transformers typically have multiple layers.

### Open Question 2
What is the precise characterization of the basin of convergence for gradient flow in the 3D parameter space (e, w, a) when attention is included? The paper states "deriving the same for the initialization sets Imin and I⋆ to determine the basin of convergence is technically challenging" (page 7). While the paper establishes that energy is constant along flow trajectories in 3D, it cannot precisely characterize which initializations lead to global vs local minima due to mathematical complexity.

### Open Question 3
How do higher-order Markov chains affect the learning dynamics and initialization requirements for transformers? The paper states "analysis of initialization effects for deeper architectures is an interesting avenue of future research" (page 9). This remains unresolved because the paper focuses exclusively on first-order Markov chains, but real-world sequential data often exhibits longer memory.

### Open Question 4
What is the role of the stationary points Θstation(p, q) where 1 + ae² = 0 and 1 + 2w|w| = 0, and how do they affect the learning dynamics? The paper states "the remaining set of stationary points could be classified to global minima, local minima, etc., it's technically unclear what category the set of critical points Θstation(p, q) belong to" (page 31). The Hessian is undefined at these points due to the discontinuity in sign(1 + ae²), making standard second-order analysis impossible.

## Limitations

- The analysis relies heavily on the rank-one assumption for weight matrices, which may not hold in practice for larger transformers
- The 2D/3D reduction to analyze gradient flow is elegant but may not capture all relevant dynamics in higher dimensions
- The switching factor p+q being the sole determinant of convergence behavior is an oversimplification - real-world data often exhibits more complex dependencies

## Confidence

**High Confidence**: The characterization of global vs local minima convergence based on p+q and initialization (Mechanism 1). The theoretical derivation of energy conservation and critical point analysis is rigorous and well-supported by mathematical proofs.

**Medium Confidence**: The data-agnostic initialization region Icommon (Mechanism 3). While the theoretical existence is proven, the practical utility depends on the overlap between theoretical assumptions and real-world training dynamics.

**Low Confidence**: The generalizability of 2D/3D analysis to full transformer architectures. The empirical validation section is limited and doesn't thoroughly test edge cases or larger models.

## Next Checks

1. **Perturbation Analysis**: Introduce small perturbations to rank-one structure during training and measure how quickly the energy conservation breaks down. This would quantify the robustness of the theoretical analysis to practical deviations.

2. **Higher-Order Markov Chains**: Extend the analysis to second-order or higher Markov chains and verify whether similar convergence patterns exist. This would test the fundamental assumption that first-order Markov properties drive the learning dynamics.

3. **Practical Implementation Study**: Implement the full transformer with the canonical parameterization and systematically vary initialization strategies across different p+q regimes. Track not just final convergence but also the training trajectory to identify potential energy barriers or basin transitions that the 2D analysis might miss.