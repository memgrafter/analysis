---
ver: rpa2
title: 'Improving Machine Translation with Human Feedback: An Exploration of Quality
  Estimation as a Reward Model'
arxiv_id: '2401.12873'
source_url: https://arxiv.org/abs/2401.12873
tags:
- reward
- raft
- training
- translation
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates using quality estimation (QE) models as
  reward models in machine translation feedback training to address the challenge
  of insufficient human preference modeling. The authors identify an overoptimization
  problem where translation quality degrades despite increasing reward scores, caused
  by QE model vulnerabilities that assign high rewards to erroneous translations.
---

# Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model

## Quick Facts
- arXiv ID: 2401.12873
- Source URL: https://arxiv.org/abs/2401.12873
- Reference count: 40
- Primary result: RAFT+ improves MT quality by detecting QE model vulnerabilities and applying penalty terms

## Executive Summary
This paper investigates using quality estimation (QE) models as reward models for machine translation feedback training, addressing the challenge of insufficient human preference data. The authors identify a critical overoptimization problem where translation quality degrades despite increasing reward scores, caused by QE model vulnerabilities that assign high rewards to erroneous translations. They propose RAFT+, a simple yet effective solution that detects length-ratio and off-target errors and applies penalty terms to the reward. The approach achieves consistent improvements across both high- and low-resource settings, demonstrating superior data efficiency compared to systems using larger parallel corpora.

## Method Summary
The paper explores using QE models as reward models in machine translation feedback training, where models are fine-tuned using reinforcement learning with rewards derived from QE scores. The authors identify that QE models can be vulnerable to adversarial examples, assigning high rewards to translations with significant errors. RAFT+ addresses this by detecting two types of errors: length-ratio errors (where translation length deviates significantly from reference) and off-target errors (where translation doesn't match the source content). The method applies penalty terms to the reward when these errors are detected, effectively constraining the feedback training process to avoid overoptimization.

## Key Results
- RAFT+ achieves consistent improvements across high- and low-resource settings
- System outperforms baseline models using larger parallel corpora with only small amounts of monolingual data
- Human preference studies validate the effectiveness of the approach

## Why This Works (Mechanism)
The approach works by addressing a fundamental mismatch between QE model scoring and actual translation quality. QE models, while trained to predict translation quality, can be vulnerable to specific patterns that don't reflect human judgment. By detecting these vulnerabilities and applying corrective penalties, RAFT+ ensures that the reward signal better aligns with human preferences, preventing the model from exploiting QE model weaknesses while maintaining genuine translation quality improvements.

## Foundational Learning

**Quality Estimation (QE)**: Automatic prediction of translation quality without reference translations. Why needed: Provides a scalable way to evaluate translations when reference data is unavailable. Quick check: Can QE models reliably predict human-judged translation quality across different domains?

**Reinforcement Learning for MT**: Training MT models using reward signals rather than maximum likelihood. Why needed: Enables optimization for human preferences rather than just likelihood of reference translations. Quick check: Does RL training consistently improve human-evaluated translation quality?

**Adversarial Examples in NLP**: Inputs designed to fool models into making incorrect predictions. Why needed: Understanding how QE models can be manipulated is crucial for developing robust reward models. Quick check: What specific patterns in translations cause QE models to assign incorrect high scores?

## Architecture Onboarding

**Component Map**: Source text -> MT model -> QE model -> RAFT+ detector -> Reward signal -> MT model update

**Critical Path**: The core feedback loop consists of generating translations, scoring them with QE, detecting errors with RAFT+, computing penalized rewards, and updating the MT model parameters through reinforcement learning.

**Design Tradeoffs**: The paper trades potential reward maximization for quality stability by introducing penalty terms. This conservative approach prevents overoptimization but may limit the exploration of reward space compared to unconstrained RL methods.

**Failure Signatures**: Overoptimization manifests as increasing QE scores but decreasing actual translation quality, often accompanied by translations that are either excessively long/short or contain hallucinated content not present in the source.

**3 First Experiments**:
1. Compare MT quality before and after RAFT+ application using standard automatic metrics
2. Analyze specific error types (length-ratio and off-target) in generated translations
3. Conduct human preference studies to validate automatic metric improvements

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies on heuristic fixes rather than addressing potential architectural issues in QE models
- Human preference study sample size (30 sentences per condition) may be insufficient for robust conclusions
- Experiments limited to specific language pairs, limiting generalizability to other language combinations

## Confidence

| Claim Cluster | Confidence Level |
|---|---|
| QE model vulnerabilities causing overoptimization | Medium |
| RAFT+ effectiveness | High |
| Data efficiency claims | High |

## Next Checks
1. Conduct ablation studies removing individual RAFT+ penalty components to isolate their relative contributions
2. Test the approach across additional language pairs and domain-specific translation tasks
3. Compare RAFT+ against alternative reward modeling approaches incorporating direct human preference signals