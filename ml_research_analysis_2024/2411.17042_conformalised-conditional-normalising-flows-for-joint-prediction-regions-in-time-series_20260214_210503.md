---
ver: rpa2
title: Conformalised Conditional Normalising Flows for Joint Prediction Regions in
  time series
arxiv_id: '2411.17042'
source_url: https://arxiv.org/abs/2411.17042
tags:
- time
- prediction
- series
- normalising
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for constructing prediction
  regions for multi-step time series forecasting using conformalized conditional normalizing
  flows. The approach leverages the exact likelihood computation property of normalizing
  flows to derive adaptive prediction regions without assuming specific geometric
  shapes.
---

# Conformalised Conditional Normalising Flows for Joint Prediction Regions in time series

## Quick Facts
- arXiv ID: 2411.17042
- Source URL: https://arxiv.org/abs/2411.17042
- Authors: Eshant English; Christoph Lippert
- Reference count: 2
- Key outcome: Novel method for multi-step time series forecasting using conformalized conditional normalizing flows achieves coverage closer to nominal level (0.91-0.89 for 10% significance) compared to baselines

## Executive Summary
This paper introduces Conformalised Conditional Normalising Flows for Joint Prediction Regions (CCN-JPR), a novel method for constructing prediction regions in multi-step time series forecasting. The approach combines conditional normalizing flows with conformal prediction to generate adaptive, potentially disjoint prediction regions without assuming specific geometric shapes. By using conditional density as the conformity score, the method leverages the exact likelihood computation property of normalizing flows to create context-adaptive regions that can capture multimodal predictive distributions. Experiments on synthetic and real-world datasets demonstrate improved coverage and predictive efficiency compared to existing approaches like CopulaCPTS, MC-dropout, and CRNN.

## Method Summary
The CCN-JPR method trains a conditional normalizing flow (CNF) on historical time series data to learn the conditional distribution of future values given past observations. The CNF is implemented using conditional affine coupling layers conditioned on RNN-encoded historical context. For conformal calibration, the method computes conformity scores as conditional densities on a calibration set, then determines a density threshold that ensures the prediction region contains the true value with probability at least 1-ε. Prediction regions are constructed by sampling from the multivariate label space and including points whose conditional density exceeds the threshold. This approach avoids geometric constraints of traditional methods and adapts to historical context while maintaining finite-sample coverage guarantees.

## Key Results
- Achieved coverage of 0.91-0.89 for 10% significance level, closer to nominal 0.9 than baseline methods
- Improved predictive efficiency compared to CopulaCPTS, MC-dropout, and CRNN baselines
- Generated potentially disjoint prediction regions that capture multimodal predictive distributions
- Demonstrated flexibility in handling complex time series forecasting scenarios without geometric restrictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional normalizing flows enable adaptive, non-convex prediction regions without geometric constraints.
- Mechanism: CNFs compute exact conditional likelihoods via change-of-variables, allowing regions to be defined directly by density thresholds rather than geometric shapes like ellipsoids or bands.
- Core assumption: The flow architecture can represent the true conditional distribution well enough that the density threshold captures the desired coverage.
- Evidence anchors:
  - [abstract]: "Our approach leverages the flexibility of normalising flows to generate potentially disjoint prediction regions, leading to improved predictive efficiency in the presence of potential multimodal predictive distributions."
  - [section 4]: "Unlike other baselines, a region formed like this has no geometric restrictions."

### Mechanism 2
- Claim: Using conditional density as the conformity score provides context-adaptive prediction regions.
- Mechanism: The conformity score is computed as p(x_{T+1:T+H} | x_{1:T}; θ), which naturally adapts to the historical context, unlike residuals that assume a fixed error distribution.
- Core assumption: The conditional density accurately reflects the plausibility of a forecast given the observed history.
- Evidence anchors:
  - [section 4]: "Additionally, the regions formed are adaptive to the historical context by using conditional density as a conformity score."
  - [section 3.2]: "Given a dataset D = {(x1, c 1), . . . , (xn, c n)}, we aim to maximise the conditional log-likelihood..."

### Mechanism 3
- Claim: Conformal calibration using the conditional density threshold ensures finite-sample coverage guarantees.
- Mechanism: By sorting the conditional densities of the calibration set and selecting the appropriate quantile, we create a threshold that ensures the prediction region contains the true value with probability at least 1-ε.
- Core assumption: The calibration set is exchangeable with the test set and representative of the data distribution.
- Evidence anchors:
  - [section 3.1]: "This set represents the values of y that are deemed conforming compared to the calibration set, with a probability of at least 1 − ǫ."
  - [section 4]: "To construct a region that contains the true prediction with the desired significance level ǫ, we can, then, sample points from the multi-variate label space..."

## Foundational Learning

- Concept: Normalizing flows and change-of-variables formula
  - Why needed here: Understanding how CNFs compute exact likelihoods via invertible transformations is crucial for grasping the conformity score mechanism.
  - Quick check question: What is the key equation that allows CNFs to compute conditional densities exactly?

- Concept: Conformal prediction framework and inductive conformal predictors
  - Why needed here: The paper builds on the inductive conformal framework, which requires understanding how conformity scores are computed and how prediction regions are constructed.
  - Quick check question: How does the prediction set Γ_ǫ(x*) differ between full and inductive conformal prediction?

- Concept: Conditional density as a conformity score
  - Why needed here: This is the novel contribution that distinguishes the approach from traditional residual-based methods.
  - Quick check question: Why is using conditional density more suitable for multivariate prediction than using residuals?

## Architecture Onboarding

- Component map:
  Data preprocessing -> RNN temporal encoder -> Conditional normalizing flow -> Conformity score calculator -> Conformal calibration -> Prediction region generator

- Critical path:
  1. Train CNF on training set proper
  2. Compute conformity scores on calibration set
  3. Determine density threshold from calibration scores
  4. For test sample, sample from label space and include points above threshold

- Design tradeoffs:
  - Flow architecture complexity vs. computational efficiency
  - Exhaustive sampling vs. Monte Carlo sampling for region construction
  - Number of coupling layers vs. expressivity
  - Calibration set size vs. estimation accuracy of the threshold

- Failure signatures:
  - Severe undercoverage: Model misspecification or insufficient calibration set
  - Overly conservative regions: Incorrect threshold selection or poor flow model
  - Computational bottlenecks: High-dimensional label space or complex flow architecture

- First 3 experiments:
  1. Synthetic data with known distribution: Verify coverage matches nominal level
  2. Compare against baseline methods on synthetic data: Check for improved efficiency
  3. Real-world dataset (COVID-UK): Validate on irregular time series with multimodality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed CCN-JPR prediction regions scale computationally with increasing forecast horizon H and dimensionality of the time series?
- Basis in paper: [inferred] The paper mentions that "exhaustive sampling might be computationally expensive if the label space is high-dimensional" and discusses this as a limitation.
- Why unresolved: The paper does not provide experimental results or analysis on computational scaling with respect to H or dimensionality. No complexity analysis is provided.
- What evidence would resolve it: Empirical runtime measurements across different values of H and dimensionality, or theoretical computational complexity analysis comparing CCN-JPR to baseline methods.

### Open Question 2
- Question: How does CCN-JPR perform on real-world datasets with genuinely multimodal predictive distributions?
- Basis in paper: [explicit] The paper states "lack of standard datasets exhibiting multimodal predictive distributions in time series" as a limitation and future work focus.
- Why unresolved: The experiments only use synthetic datasets with known multimodality and one real-world dataset (COVID-UK) that may not exhibit strong multimodal characteristics.
- What evidence would resolve it: Experiments on benchmark datasets specifically designed to exhibit multimodal predictive distributions in time series forecasting.

### Open Question 3
- Question: Can the conditional normalizing flow architecture in CCN-JPR be effectively replaced with classical statistical models like ARIMA while maintaining conformal coverage guarantees?
- Basis in paper: [explicit] The paper suggests future research could "explore extending existing kernel-based flow approaches to conditional normalising flows (conditioned on prediction from the classical methods)."
- Why unresolved: The paper only demonstrates the method with neural network-based normalizing flows and does not explore classical statistical alternatives.
- What evidence would resolve it: Experimental comparison showing coverage guarantees and predictive efficiency when using ARIMA or other classical models as the base predictor combined with conditional normalizing flows.

## Limitations
- Computational efficiency concerns with high-dimensional label spaces requiring exhaustive sampling
- Limited validation on real-world datasets with genuinely multimodal predictive distributions
- Uncertainty about method's robustness to severe model misspecification and non-stationary time series

## Confidence
- **High confidence:** The theoretical foundation of using conditional density as a conformity score and the finite-sample coverage guarantees provided by the conformal framework.
- **Medium confidence:** The experimental results showing improved coverage and efficiency compared to baselines, though the comparison is limited to three methods on specific datasets.
- **Low confidence:** The method's robustness to severe model misspecification and its performance on highly irregular or non-stationary time series.

## Next Checks
1. **Stress test with synthetic multimodal distributions:** Generate synthetic datasets with known multimodal conditional distributions to verify that the method can capture disjoint prediction regions and achieve the desired coverage.
2. **Calibration set sensitivity analysis:** Systematically vary the size and composition of the calibration set to assess its impact on coverage accuracy and threshold estimation.
3. **Scalability evaluation:** Test the method on datasets with increasing forecasting horizons and dimensionality to evaluate computational efficiency and coverage stability.