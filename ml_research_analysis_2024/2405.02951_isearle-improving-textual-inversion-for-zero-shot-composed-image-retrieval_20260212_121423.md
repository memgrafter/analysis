---
ver: rpa2
title: 'iSEARLE: Improving Textual Inversion for Zero-Shot Composed Image Retrieval'
arxiv_id: '2405.02951'
source_url: https://arxiv.org/abs/2405.02951
tags:
- image
- images
- dataset
- retrieval
- pseudo-word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new task called Zero-Shot Composed Image
  Retrieval (ZS-CIR) that addresses the challenge of retrieving images that match
  a reference image and a relative caption without relying on labeled training data.
  The authors propose a novel approach called iSEARLE, which leverages textual inversion
  to map reference images into pseudo-word tokens in the CLIP embedding space and
  combine them with relative captions.
---

# iSEARLE: Improving Textual Inversion for Zero-Shot Composed Image Retrieval

## Quick Facts
- **arXiv ID**: 2405.02951
- **Source URL**: https://arxiv.org/abs/2405.02951
- **Authors**: Lorenzo Agnolucci; Alberto Baldrati; Alberto Del Bimbo; Marco Bertini
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art performance on three CIR datasets (FashionIQ, CIRR, and CIRCO) and two additional evaluation settings (domain conversion and object composition).

## Executive Summary
This paper introduces iSEARLE, a novel approach for Zero-Shot Composed Image Retrieval (ZS-CIR) that addresses the challenge of retrieving images matching a reference image and relative caption without labeled training data. The method leverages textual inversion to map reference images into pseudo-word tokens in the CLIP embedding space, which are then combined with relative captions for text-to-image retrieval. iSEARLE achieves state-of-the-art performance on three benchmark datasets while demonstrating robust generalization capabilities across different domains and semantic aspects.

## Method Summary
iSEARLE employs a two-stage training approach for ZS-CIR. First, an Optimization-based Textual Inversion (OTI) method generates pseudo-word tokens from unlabeled images, using GPT-powered regularization and Gaussian noise to mitigate the modality gap in CLIP embeddings. Second, a textual inversion network is trained via knowledge distillation to efficiently approximate the OTI output. During inference, reference images are mapped to pseudo-word tokens using the trained network, which are then combined with relative captions for retrieval using CLIP's text encoder.

## Key Results
- Achieves state-of-the-art performance on FashionIQ, CIRR, and CIRCO datasets with Recall@K and mAP@K metrics
- Demonstrates robust generalization across different domains and semantic aspects (addition, direct addressing)
- Outperforms existing methods in domain conversion and object composition evaluation settings
- Shows effective handling of complex CIR queries while maintaining efficiency through the distilled network

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding Gaussian noise to text features before computing the content loss mitigates the modality gap between image and text embeddings in CLIP.
- Mechanism: The noise spreads out text embeddings to make them overlap with image embeddings in the joint embedding space, reducing the discrepancy caused by CLIP's separate clustering of text and image features.
- Core assumption: The noise distribution is sufficiently broad to bridge the modality gap without destroying semantic information.
- Evidence anchors:
  - [abstract] "we propose to add Gaussian noise to the text features y before minimizing their discrepancy with the image features x."
  - [section] "Inspired by [48], we propose to add Gaussian noise to the text features y before minimizing their discrepancy with the image features x. Specifically, we compute y = y + n, where n ~ N(0, γ2) is drawn from a Gaussian distribution with variance γ2."
  - [corpus] Weak evidence - no direct citations in corpus about Gaussian noise in textual inversion, but mentions of modality gap mitigation exist.
- Break condition: If noise variance γ2 is too high, it destroys semantic information; if too low, it fails to mitigate the modality gap.

### Mechanism 2
- Claim: GPT-powered regularization steers pseudo-word tokens toward regions of the embedding space that can effectively interact with natural language.
- Mechanism: The GPT-generated phrase provides a broader, more natural language context than a simple concept word, allowing the pseudo-word to learn to communicate with human-generated relative captions.
- Core assumption: GPT-generated phrases are sufficiently diverse and representative of human language patterns used in CIR.
- Evidence anchors:
  - [abstract] "we generate a phrase using a lightweight GPT model... Given that GPT is an autoregressive generative model, it manages to continue the prompt in a meaningful manner."
  - [section] "The GPT-generated phrases are more structured and thus similar to the relative captions used in CIR."
  - [corpus] Weak evidence - no direct citations about GPT-powered regularization in textual inversion, but mentions of GPT usage exist in related works.
- Break condition: If GPT-generated phrases don't capture the linguistic patterns of relative captions, the pseudo-word won't effectively interact with them.

### Mechanism 3
- Claim: Knowledge distillation from optimization-based textual inversion to a feed-forward network preserves expressiveness while achieving efficiency.
- Mechanism: The distillation loss transfers the knowledge embedded in pseudo-word tokens generated by the computationally expensive OTI method to the more efficient textual inversion network.
- Core assumption: The pseudo-word tokens generated by OTI contain sufficient information to train a network that can approximate the OTI results.
- Evidence anchors:
  - [abstract] "we train a textual inversion network to emulate the output of an optimization-based textual inversion using a distillation loss."
  - [section] "ϕ serves as a more efficient surrogate model of OTI, offering a faster and computationally less demanding approximation."
  - [corpus] Moderate evidence - mentions of knowledge distillation in image retrieval and computer vision tasks.
- Break condition: If the pseudo-word tokens lack sufficient information diversity, the network cannot learn an effective approximation.

## Foundational Learning

- Concept: CLIP model and its modality gap
  - Why needed here: Understanding why Gaussian noise is added to text features and how CLIP's separate clustering of text and image features affects textual inversion
  - Quick check question: Why does CLIP have a modality gap and how does adding noise help mitigate it?

- Concept: Textual inversion and pseudo-word tokens
  - Why needed here: Understanding how images are mapped to pseudo-word tokens and why this mapping is useful for CIR
  - Quick check question: What is a pseudo-word token and how does it represent image information in CLIP's token embedding space?

- Concept: Knowledge distillation
  - Why needed here: Understanding how the computationally expensive OTI method is transferred to the efficient textual inversion network
  - Quick check question: What is the purpose of knowledge distillation in this context and how does it work?

## Architecture Onboarding

- Component map:
  CLIP model (frozen) -> OTI -> pseudo-word tokens -> Textual Inversion Network ϕ -> pseudo-word tokens -> CIR retrieval

- Critical path:
  1. Pre-training phase: OTI generates pseudo-word tokens from unlabeled images
  2. Pre-training phase: Knowledge distillation transfers OTI knowledge to ϕ
  3. Inference: ϕ predicts pseudo-word token for reference image
  4. Inference: Combine pseudo-word with relative caption and perform text-to-image retrieval

- Design tradeoffs:
  - OTI vs. ϕ: OTI is expressive but slow; ϕ is efficient but may lose some expressiveness
  - Single vs. multiple pseudo-word tokens: Single token is sufficient and more efficient
  - Gaussian noise variance: Higher variance better mitigates modality gap but may destroy information

- Failure signatures:
  - Poor retrieval performance: Pseudo-word tokens not capturing image information or not interacting with relative captions
  - Mode collapse: Pseudo-word tokens clustering in sparse regions of embedding space
  - Overfitting: ϕ memorizing OTI results rather than learning general patterns

- First 3 experiments:
  1. Verify modality gap exists: Compare cosine similarity of image-image vs. image-text pairs in CLIP embedding space
  2. Test Gaussian noise effect: Measure retrieval performance with different noise variances
  3. Validate distillation: Compare OTI vs. ϕ performance on a held-out set of images

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of iSEARLE scale with different sizes of pre-training datasets, particularly when using datasets that are orders of magnitude larger than the current 3% of CC3M?
- Basis in paper: [explicit] The paper mentions using only 3% of the CC3M dataset for pre-training the textual inversion network and suggests that larger datasets could potentially improve performance.
- Why unresolved: The paper does not provide experimental results for varying the size of the pre-training dataset, leaving the impact of dataset size on performance unexplored.
- What evidence would resolve it: Conducting experiments with different fractions of the CC3M dataset (e.g., 10%, 50%, 100%) and measuring the performance on CIR tasks would provide insights into the scalability of iSEARLE with larger pre-training datasets.

### Open Question 2
- Question: How does the choice of the textual inversion network architecture (e.g., transformer-based vs. MLP-based) affect the performance and efficiency of iSEARLE?
- Basis in paper: [inferred] The paper uses an MLP-based architecture for the textual inversion network, while other works like Context-I2W employ transformer-based architectures. The paper mentions that iSEARLE achieves comparable performance to Context-I2W despite using a simpler architecture.
- Why unresolved: The paper does not provide a direct comparison between different network architectures for textual inversion, leaving the impact of architecture choice unexplored.
- What evidence would resolve it: Conducting experiments with different architectures (e.g., MLP, transformer, CNN) for the textual inversion network and comparing their performance and efficiency on CIR tasks would provide insights into the impact of architecture choice.

### Open Question 3
- Question: How does iSEARLE perform on CIR tasks in domains that are significantly different from the pre-training dataset, such as medical imaging or satellite imagery?
- Basis in paper: [explicit] The paper mentions that iSEARLE demonstrates good generalization capabilities and robustness to the choice of the pre-training dataset, even when using datasets from domains different from the testing ones.
- Why unresolved: The paper does not provide experimental results for CIR tasks in domains that are significantly different from the pre-training dataset, leaving the performance in such scenarios unexplored.
- What evidence would resolve it: Conducting experiments on CIR tasks using datasets from domains such as medical imaging or satellite imagery and measuring the performance of iSEARLE would provide insights into its effectiveness in handling significantly different domains.

## Limitations

- The effectiveness of Gaussian noise for modality gap mitigation lacks comprehensive ablation studies and sensitivity analysis
- GPT-powered regularization details are underspecified, including phrase generation templates and evaluation of their effectiveness
- Knowledge distillation assumes pseudo-word tokens contain sufficient information but doesn't analyze what information is actually transferred

## Confidence

**High Confidence**: The overall framework design and task definition are well-established. The use of CLIP embeddings and the general concept of textual inversion for image retrieval are sound approaches with strong theoretical foundations. The experimental methodology and evaluation metrics are standard and appropriate.

**Medium Confidence**: The two-stage training approach (OTI followed by knowledge distillation) is reasonable, but the effectiveness of the distillation process depends heavily on implementation details not fully specified in the paper. The claim of state-of-the-art performance is supported by experiments, but the comparison methods and dataset splits could affect the results.

**Low Confidence**: The specific mechanisms for modality gap mitigation through Gaussian noise and GPT-powered regularization lack sufficient empirical validation. The paper claims these components are crucial but doesn't provide ablation studies or sensitivity analyses to demonstrate their individual contributions to performance.

## Next Checks

1. **Noise Sensitivity Analysis**: Systematically vary the Gaussian noise variance γ2 across a range of values (e.g., 0.1, 0.5, 1.0, 2.0) and measure retrieval performance on each dataset. This would validate whether the noise level significantly impacts the modality gap mitigation and identify an optimal setting.

2. **GPT Phrase Ablation Study**: Replace GPT-generated phrases with simpler alternatives (e.g., random words, single concept words, or manually crafted phrases) and compare retrieval performance. This would test whether the autoregressive nature and natural language structure of GPT-generated phrases are actually necessary for effective regularization.

3. **Knowledge Distillation Fidelity Evaluation**: Compare the embedding distributions of pseudo-word tokens generated by OTI versus those predicted by the distilled network ϕ using quantitative metrics like KL divergence or t-SNE visualization. Additionally, test whether fine-tuning ϕ directly on downstream datasets improves performance compared to the distilled model.