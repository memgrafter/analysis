---
ver: rpa2
title: 'A Systematic Analysis of Large Language Models as Soft Reasoners: The Case
  of Syllogistic Inferences'
arxiv_id: '2406.11341'
source_url: https://arxiv.org/abs/2406.11341
tags:
- premises
- conclusions
- valid
- reasoning
- some
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models exhibit reasoning biases similar to humans
  on syllogistic tasks, including difficulty recognizing invalid inferences and content
  effects where they favor believable conclusions over logically valid ones. Zero-shot
  CoT prompting alone does not overcome these weaknesses, and in-context learning
  improves performance on valid inferences but increases inconsistency and fails to
  eliminate biases.
---

# A Systematic Analysis of Large Language Models as Soft Reasoners: The Case of Syllogistic Inferences

## Quick Facts
- **arXiv ID**: 2406.11341
- **Source URL**: https://arxiv.org/abs/2406.11341
- **Reference count**: 24
- **Key outcome**: LLMs show human-like reasoning biases on syllogisms, with fine-tuning improving valid inference performance

## Executive Summary
This study systematically evaluates how large language models perform on syllogistic reasoning tasks, revealing that they exhibit similar biases to humans, including difficulty recognizing invalid inferences and being influenced by content believability. The research tests various prompting strategies and fine-tuning approaches, finding that zero-shot CoT prompting alone doesn't overcome reasoning weaknesses. Supervised fine-tuning with pseudo-words enables smaller models to achieve near-perfect performance on valid syllogisms while reducing content effects, suggesting that explicit training may be necessary for reliable logical reasoning in LLMs.

## Method Summary
The study employs a comprehensive experimental design using 64 syllogistic reasoning tasks across multiple LLM families (GPT-4, LLaMA-3, LLaMA-2, Mistral). Researchers test various prompting strategies including zero-shot, few-shot, and chain-of-thought prompting, as well as in-context learning with example syllogisms. A novel supervised fine-tuning approach uses pseudo-words to mask semantic content, forcing models to focus on logical form. The evaluation includes both valid and invalid syllogisms, with analysis of reasoning consistency, content effects, and alignment with cognitive science heuristics like Atmosphere and Matching.

## Key Results
- LLMs consistently fail to recognize invalid syllogisms and show content effects favoring believable conclusions
- Zero-shot CoT prompting alone does not overcome reasoning weaknesses
- Supervised fine-tuning with pseudo-words enables small/medium models to achieve near-perfect valid inference performance
- LLaMA-3 8B in zero-shot settings shows behavior consistent with the Atmosphere heuristic, relying on premise quantifiers

## Why This Works (Mechanism)
The study reveals that LLMs' syllogistic reasoning weaknesses stem from their tendency to rely on semantic content and heuristics rather than pure logical form. The Atmosphere heuristic explains why models favor conclusions matching premise quantifiers (e.g., "All" or "Some"). Content effects occur because models struggle to separate logical validity from premise believability. Fine-tuning with pseudo-words appears to help models learn form-based reasoning by removing semantic interference, though the exact mechanisms of how this generalizes remain unclear.

## Foundational Learning
- **Syllogistic reasoning**: Why needed - core logical inference task; Quick check - can model identify valid/invalid conclusions from two premises
- **Atmosphere heuristic**: Why needed - explains quantifier-based reasoning biases; Quick check - does model favor conclusions matching premise quantifiers
- **Content effects**: Why needed - demonstrates interference between logic and semantics; Quick check - does model prefer believable over logically valid conclusions
- **In-context learning**: Why needed - tests few-shot reasoning improvement; Quick check - does performance improve with syllogism examples
- **Pseudo-word masking**: Why needed - forces form-based rather than semantic reasoning; Quick check - does model performance transfer to natural language

## Architecture Onboarding
**Component map**: Input prompt -> LLM reasoning -> Output conclusion -> Consistency check -> Heuristic analysis
**Critical path**: Prompt design → Model inference → Output validation → Bias/error analysis
**Design tradeoffs**: Zero-shot simplicity vs. fine-tuning accuracy; semantic richness vs. logical purity; model size vs. reasoning capability
**Failure signatures**: Accepting invalid conclusions, rejecting valid ones, showing content effects, inconsistent reasoning across similar problems
**Three first experiments**:
1. Test zero-shot CoT on simple valid syllogisms to establish baseline
2. Evaluate in-context learning with 3-5 syllogism examples
3. Apply pseudo-word fine-tuning to small model and test on masked valid syllogisms

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions in the provided text.

## Limitations
- Experiments use controlled artificial syllogisms that may not generalize to real-world reasoning
- Human-like biases are inferred from literature rather than direct human testing with identical prompts
- Fine-tuning effectiveness may reflect pattern memorization rather than genuine logical understanding
- Heuristic analysis relies on post-hoc interpretation rather than controlled testing

## Confidence
- **High confidence**: LLMs show systematic reasoning biases (content effects, invalid inference errors) that persist across prompting strategies
- **Medium confidence**: Fine-tuning with pseudo-words improves performance on valid syllogisms, but generalization to other reasoning domains remains unproven
- **Medium confidence**: Model behavior aligns with Atmosphere heuristic in zero-shot settings, though alternative explanations exist

## Next Checks
1. Test whether pseudo-word fine-tuning generalizes to natural language syllogisms with similar logical forms but different content
2. Conduct controlled human experiments using identical prompts to verify human-like reasoning patterns
3. Evaluate fine-tuned models on non-syllogistic reasoning tasks (categorical judgments, analogical reasoning) to assess transfer of reasoning improvements