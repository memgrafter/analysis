---
ver: rpa2
title: Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships
arxiv_id: '2402.12189'
source_url: https://arxiv.org/abs/2402.12189
tags:
- training
- data
- texts
- fine-tuning
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel training data extraction attack that
  adversarially fine-tunes a pre-trained language model to amplify memorization of
  its original training data. The attack uses pseudo-labels based on perturbation
  discrepancy scores to create a preference ranking over self-generated text, then
  fine-tunes the model with reinforcement learning to favor text more likely to contain
  training data.
---

# Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships

## Quick Facts
- arXiv ID: 2402.12189
- Source URL: https://arxiv.org/abs/2402.12189
- Reference count: 40
- Primary result: Fine-tuning increases true positives by 4-8x for models over 1B parameters

## Executive Summary
This paper introduces a novel training data extraction attack that adversarially fine-tunes pre-trained language models to amplify memorization of their original training data. The attack uses pseudo-labels based on perturbation discrepancy scores to create preference rankings over self-generated text, then fine-tunes the model with reinforcement learning to favor text more likely to contain training data. Experiments demonstrate significant amplification effects, particularly for larger models, with some extracted sequences exceeding 1100 tokens in length.

## Method Summary
The attack generates 100,000 texts from a target LM using empty prompts, calculates perturbation discrepancy scores for each text using perturbed versions created by a T5-Large mask-and-fill approach, and pairs texts by sorting these scores. Pseudo-labels are assigned based on pair rankings, creating a preference dataset. The target LM is then fine-tuned using RLHF with the perturbation discrepancy as a reward signal, where 40% of the data trains the reward model and 60% is used for PPO fine-tuning. The effectiveness is measured by comparing true positives (extracted training data) before and after fine-tuning.

## Key Results
- Fine-tuning increases true positives by 4-8x for models over 1B parameters
- Larger models (1.3B+ parameters) show stronger amplification effects
- Some extracted sequences exceed 1100 tokens in length
- The attack works across six OPT model sizes (125M to 13B parameters)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning with pseudo-labeled self-generations amplifies training data exposure by 4-8x for models over 1B parameters
- Mechanism: The fine-tuning process preferentially reinforces patterns from the original training data by using perturbation discrepancy scores to create a preference ranking. Text with lower perturbation discrepancy (more human-like) is favored, which correlates with higher likelihood of containing training data
- Core assumption: Text with lower perturbation discrepancy is more likely to contain training data
- Evidence anchors:
  - [abstract] "fine-tuning increases true positives by 4-8x for models over 1B parameters"
  - [section] "RMs that learned pseudo-labeled generations derived from larger LM show lower classification performance" indicates perturbation discrepancy works as a signal
  - [corpus] Weak evidence - no corpus support found
- Break condition: If perturbation discrepancy becomes uncorrelated with training data presence, or if fine-tuning causes catastrophic forgetting that overwhelms memorization reinforcement

### Mechanism 2
- Claim: Reinforcement Learning from Human Feedback (RLHF) framework enables effective fine-tuning despite noisy pseudo-labels
- Mechanism: RLHF's preference-based ranking system is robust to label noise because it only needs to distinguish relative preferences between pairs rather than requiring perfect absolute labels
- Core assumption: Relative preference ranking is more noise-tolerant than absolute labeling
- Evidence anchors:
  - [abstract] "RLHF...prioritizes relative sample preferences"
  - [section] "we address the confirmation bias resulting from inaccurate labeling" through RLHF
  - [corpus] Weak evidence - no corpus support found
- Break condition: If label noise becomes too high relative to signal, or if RLHF's preference learning fails to converge on the intended objective

### Mechanism 3
- Claim: Larger models (over 1B parameters) show stronger amplification effects due to increased memorization capacity
- Mechanism: Larger models have greater capacity to memorize training data, and fine-tuning with pseudo-labeled data preferentially reinforces these memorized patterns
- Core assumption: Larger models have proportionally more training data exposure to amplify
- Evidence anchors:
  - [abstract] "LMs with over 1B parameters exhibit a four to eight-fold increase"
  - [section] "larger models, remarkably increasing up to 8 times in the OPT-1.3B"
  - [corpus] Weak evidence - no corpus support found
- Break condition: If larger models have proportionally less training data exposure, or if the fine-tuning process disproportionately affects novel generation rather than memorization

## Foundational Learning

- Concept: Perturbation discrepancy as a proxy for human-generated probability
  - Why needed here: This forms the core signal for pseudo-labeling - identifying which generated texts are more likely to contain training data
  - Quick check question: If a text has perturbation discrepancy of 0.5 and another has -1.0, which is more likely to contain training data and why?

- Concept: Reinforcement Learning from Human Feedback (RLHF) framework
  - Why needed here: This provides the fine-tuning mechanism that can work with noisy pseudo-labels by focusing on relative preferences
  - Quick check question: How does RLHF's preference-based ranking system differ from standard supervised learning in handling label noise?

- Concept: Catastrophic forgetting in fine-tuning
  - Why needed here: Understanding this phenomenon is crucial because the attack must balance between overwriting original knowledge and reinforcing memorization
  - Quick check question: What is the primary trade-off between fine-tuning strength and preservation of original training data in language models?

## Architecture Onboarding

- Component map: Text generation module -> Perturbation engine -> Perturbation discrepancy calculator -> Pair matching system -> Pseudo-label generator -> Reward Model -> Fine-tuning pipeline

- Critical path:
  1. Generate texts → 2. Perturb texts → 3. Calculate discrepancies → 4. Match pairs → 5. Train Reward Model → 6. Fine-tune target LM with RLHF

- Design tradeoffs:
  - Number of perturbations per text (10 used) vs. computational cost
  - Pair matching heuristic (sorted pairing) vs. optimal pairing algorithms
  - Fine-tuning dataset size (40% RM, 60% PPO) vs. learning efficiency

- Failure signatures:
  - Reward Model test accuracy near 50% (no learning)
  - True positives not improving after fine-tuning
  - Generated texts becoming repetitive or losing diversity

- First 3 experiments:
  1. Verify perturbation discrepancy calculation by comparing known human-written vs machine-generated texts
  2. Test Reward Model binary classification on small dataset to ensure learning signal is present
  3. Run fine-tuning on tiny model version to verify end-to-end pipeline works before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does fine-tuning with self-generated samples specifically affect the retention of memorization in language models?
- Basis in paper: [explicit] The paper mentions that LMs may "forget" early training examples during fine-tuning and that our approach raises concerns about this phenomenon
- Why unresolved: The paper shows that fine-tuning amplifies training data exposure but doesn't directly measure whether this is due to enhanced memorization or simply overfitting to the pseudo-labeled data
- What evidence would resolve it: Controlled experiments comparing the retention of specific training examples before and after fine-tuning, using metrics like exact string matching over time, or ablation studies showing whether removing the pseudo-labeled data reduces exposure

### Open Question 2
- Question: Can fine-tuning the target LM to favor responses with less training data genuinely contribute to mitigating TDE attacks?
- Basis in paper: [explicit] The paper discusses this as a potential defense strategy in §6, suggesting flipping pseudo-labels to reduce training data exposure
- Why unresolved: The paper only mentions this as a theoretical possibility and notes concerns about representation collapse, but doesn't empirically test whether this defense would be effective against their attack
- What evidence would resolve it: Experiments showing whether fine-tuning with inverted pseudo-labels actually reduces training data exposure compared to the original model, while maintaining acceptable model performance

### Open Question 3
- Question: Can this amplification approach be extended beyond neural LMs to increase training data exposure in general generative models?
- Basis in paper: [inferred] The paper concludes by suggesting this as a promising area for future work, noting that their approach might have applications beyond language models
- Why unresolved: The paper only tests their approach on OPT language models and doesn't explore whether the perturbation discrepancy and RLHF-based fine-tuning strategy would work on other types of generative models like diffusion models or GANs
- What evidence would resolve it: Successful application of the same methodology (pseudo-labeling based on perturbation discrepancy + RLHF fine-tuning) to other generative model architectures, showing similar amplification of training data exposure

## Limitations
- The perturbation discrepancy mechanism relies on DetectGPT's assumption that human-written text has distinct statistical properties from machine-generated text, which may not generalize across all domains
- The RLHF fine-tuning framework's robustness to label noise is demonstrated but the precise threshold where noise overwhelms the learning signal remains unclear
- The attack's effectiveness depends on having access to the target model for self-generation and fine-tuning, limiting its applicability to black-box scenarios

## Confidence

**High**: The core experimental methodology and reported quantitative improvements are well-documented and reproducible

**Medium**: The perturbation discrepancy signal's reliability as a proxy for training data presence

**Medium**: The scalability claims across different model sizes and their relationship to memorization capacity

## Next Checks

1. Test the attack's effectiveness on models trained on specialized or highly technical domains where perturbation discrepancy may not capture human-generated characteristics as reliably

2. Characterize the relationship between perturbation discrepancy score thresholds and true positive rates to establish optimal parameter settings

3. Evaluate the attack's performance when applied to models with known training data composition to validate the perturbation discrepancy signal's consistency across different data types