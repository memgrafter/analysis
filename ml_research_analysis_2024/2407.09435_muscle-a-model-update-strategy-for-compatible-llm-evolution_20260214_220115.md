---
ver: rpa2
title: 'MUSCLE: A Model Update Strategy for Compatible LLM Evolution'
arxiv_id: '2407.09435'
source_url: https://arxiv.org/abs/2407.09435
tags:
- negative
- flips
- tasks
- llama
- update
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of regression in large language
  model (LLM) updates, where fine-tuned task adapters experience performance degradation
  when base models are updated. The authors propose MUSCLE (Model Update Strategy
  for Compatible LLM Evolution), a knowledge distillation-based approach that trains
  a compatibility adapter to align model versions and minimize instance regression.
---

# MUSCLE: A Model Update Strategy for Compatible LLM Evolution

## Quick Facts
- **arXiv ID**: 2407.09435
- **Source URL**: https://arxiv.org/abs/2407.09435
- **Reference count**: 14
- **Primary result**: Reduces negative flip rates by up to 40% when updating from Llama 1 to Llama 2 while maintaining accuracy gains

## Executive Summary
MUSCLE addresses a critical challenge in LLM deployment: regression when updating base models with fine-tuned task adapters. The method trains a compatibility adapter that minimizes inconsistencies between model versions through knowledge distillation with adaptive masking. By selectively aligning to the old model when the new model is wrong and vice versa, MUSCLE reduces negative flip rates by up to 40% while preserving accuracy improvements from base model updates.

## Method Summary
MUSCLE trains a compatibility adapter (MCv2) that minimizes regression when updating from model version Mv1 to Mv2. The adapter is initialized with the new model's task adapter weights (ATv2) and fine-tuned using a masked KL divergence loss. The masking strategy determines whether to align to Mv1 or Mv2 based on whether the current model's prediction is correct relative to ground truth. This selective knowledge distillation preserves the new model's capabilities while maintaining consistency with the old model on previously correct predictions.

## Key Results
- Reduces negative flips (NFR) by up to 40% when updating from Llama 1 to Llama 2
- Decreases ROUGE-1 score regression for summarization by up to 27%
- Maintains accuracy gains from base model updates while improving compatibility
- Extends evaluation metrics (NFRmc, D(xi)) to capture regression patterns in generative tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation with adaptive masking reduces negative flips by selectively aligning to old model when new model is wrong.
- Mechanism: The compatibility adapter is trained with a masked KL divergence loss that aligns to the old model's predictions when the current model's prediction is incorrect (mask mi = 1), and to the new model otherwise.
- Core assumption: Token-level correctness can be reliably determined by comparing argmax predictions against ground truth.
- Break condition: If ground truth is ambiguous or multi-label, binary correctness check may fail.

### Mechanism 2
- Claim: MUSCLE preserves accuracy gains from base model updates while reducing regression through initialization with the new model's task adapter.
- Mechanism: The compatibility adapter is initialized with the weights of the new model's task-specific adapter (Mv2), ensuring it starts from a strong performance baseline.
- Core assumption: Initializing with new model's adapter weights provides a good starting point that captures performance improvements.
- Break condition: If new model's adapter is significantly worse than old model on some tasks, initialization may harm compatibility.

### Mechanism 3
- Claim: Extended evaluation metrics (NFRmc, D(xi)) capture regression patterns in generative tasks that traditional metrics miss.
- Mechanism: By computing prediction differences relative to ground truth using continuous similarity metrics (ROUGE, BERT Score), MUSCLE can quantify regression even when both models are incorrect but inconsistent.
- Core assumption: Continuous similarity metrics provide meaningful signals about prediction quality and regression.
- Break condition: If similarity metrics are poorly aligned with human judgment, regression signals may be misleading.

## Foundational Learning

- **Concept**: Knowledge distillation
  - Why needed here: MUSCLE uses knowledge distillation to transfer compatibility information from the old model to the new model without requiring both models at inference time.
  - Quick check question: What is the difference between standard knowledge distillation and the masked approach used in MUSCLE?

- **Concept**: LoRA adapters
  - Why needed here: MUSCLE trains compatibility adapters using the same LoRA architecture as task adapters, enabling efficient fine-tuning without modifying base model weights.
  - Quick check question: How does LoRA enable efficient adaptation of large language models compared to full fine-tuning?

- **Concept**: KL divergence for probability alignment
  - Why needed here: The compatibility loss uses KL divergence to align probability distributions between models, ensuring smooth transitions in prediction probabilities.
  - Quick check question: Why is KL divergence preferred over L2 loss for aligning probability distributions in knowledge distillation?

## Architecture Onboarding

- **Component map**: Base models (Mv1, Mv2) → Task adapters (AT_v1, AT_v2) → Compatibility adapter (MC_v2)
- **Critical path**:
  1. Train task adapter AT_v1 on Mv1
  2. Train task adapter AT_v2 on Mv2
  3. Initialize compatibility adapter MC_v2 with AT_v2 weights
  4. Fine-tune MC_v2 using masked KL loss with Mv1 and Mv2
  5. Evaluate regression reduction and accuracy maintenance

- **Design tradeoffs**:
  - Memory vs performance: LoRA adapters reduce memory but may limit compatibility learning capacity
  - Training stability vs compatibility: Auxiliary cross-entropy loss improves stability but may reduce compatibility gains
  - Granularity vs complexity: Token-level masking is precise but computationally expensive

- **Failure signatures**:
  - Increased NFRc compared to NFR indicates compatibility training is harming performance
  - Decreased accuracy on Mv2 tasks suggests initialization strategy is insufficient
  - Training instability or divergence suggests learning rate or masking strategy issues

- **First 3 experiments**:
  1. Verify task adapter training reproduces baseline accuracy on HellaSwag for Llama 1 and Llama 2
  2. Train compatibility adapter with simple masking (always align to Mv1) and measure NFR reduction
  3. Implement full MUSCLE masking strategy and compare NFR reduction vs baseline and simple masking approaches

## Open Questions the Paper Calls Out
None

## Limitations
- **Token-level correctness heuristics**: Binary masking based on argmax predictions assumes reliable token-level correctness determination, which may fail for ambiguous cases.
- **Similarity metric alignment**: Extended evaluation metrics using ROUGE and BERT Score assume strong alignment with human judgment, which requires further validation.
- **Initialization strategy robustness**: Initializing with new model weights may not be optimal when the new model underperforms on certain tasks.

## Confidence
- **Compatibility improvement claims**: High
- **Knowledge distillation mechanism**: Medium
- **Evaluation metric validity**: Medium
- **Initialization strategy**: Medium

## Next Checks
1. **Cross-architectural validation**: Test MUSCLE on non-Llama model transitions (e.g., GPT-2 to GPT-3, or different model families) to verify generalizability beyond the evaluated scenarios.

2. **Alternative masking strategies**: Implement and compare against continuous masking approaches (e.g., soft masking based on prediction confidence) rather than binary correctness checks to evaluate robustness to token-level uncertainty.

3. **Backward compatibility testing**: Evaluate whether the compatibility adapter trained for Mv2→Mv1 can also be used in reverse (Mv1→Mv2) or if separate adapters are needed, testing the symmetry of the compatibility learning process.