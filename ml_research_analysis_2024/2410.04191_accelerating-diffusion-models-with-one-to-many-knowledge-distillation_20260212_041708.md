---
ver: rpa2
title: Accelerating Diffusion Models with One-to-Many Knowledge Distillation
arxiv_id: '2410.04191'
source_url: https://arxiv.org/abs/2410.04191
tags:
- diffusion
- student
- knowledge
- o2mkd
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes one-to-many knowledge distillation (O2MKD)
  to accelerate diffusion models. The key insight is that diffusion models exhibit
  varying input distributions, feature distributions, and generation behaviors across
  different timesteps, making it challenging for a single small student model to mimic
  the teacher across all timesteps.
---

# Accelerating Diffusion Models with One-to-Many Knowledge Distillation

## Quick Facts
- arXiv ID: 2410.04191
- Source URL: https://arxiv.org/abs/2410.04191
- Authors: Linfeng Zhang; Kaisheng Ma
- Reference count: 7
- One-line primary result: O2MKD achieves 1.8× acceleration with only 0.18 FID degradation on CIFAR10

## Executive Summary
This paper introduces One-to-Many Knowledge Distillation (O2MKD), a novel approach to accelerate diffusion models by addressing their varying input and feature distributions across timesteps. Traditional one-to-one knowledge distillation struggles because a single small student cannot effectively learn the teacher's knowledge across all timesteps. O2MKD solves this by distributing the learning task across multiple specialized students, each focusing on a subset of continuous timesteps, achieving significant acceleration with minimal performance degradation.

## Method Summary
O2MKD uses one teacher diffusion model to train multiple student models, where each student specializes in learning the teacher's knowledge for a specific subset of continuous timesteps. During training, students are exposed to both their designated timesteps and general timesteps with probability p, using a combined loss of diffusion loss and knowledge distillation loss (both prediction-based and feature-based). During inference, students are deployed sequentially based on the current timestep. The approach leverages the gradual distribution changes in diffusion models, reducing each student's learning complexity while maintaining overall performance.

## Key Results
- Achieves 1.8× acceleration on CIFAR10 with only 0.18 FID degradation compared to teacher
- Outperforms traditional one-to-one KD by assigning students to learn subsets of continuous timesteps
- O2MKD-Full achieves 1.5× acceleration with 0.15 FID improvement over teacher on CIFAR10
- Non-uniform timestep assignment (more students for smaller timesteps) achieves better FID (4.23) than uniform assignment (4.33)

## Why This Works (Mechanism)

### Mechanism 1
Diffusion models show varying input and feature distributions across timesteps, making it hard for a single small student to learn teacher knowledge at all timesteps. By splitting the full timestep range into continuous subranges and assigning each to a separate student, the learning complexity for each student is reduced because adjacent timesteps have similar distributions. This works because the transition in distributions within diffusion models at different timesteps occurs gradually rather than abruptly.

### Mechanism 2
Knowledge distillation from a single teacher to a single student fails because the student cannot handle the full complexity of learning across all timesteps. Using multiple students allows each to specialize in a subset of timesteps, reducing the learning burden and improving performance compared to traditional one-to-one knowledge distillation. This works because a small student model lacks the capacity to handle the full range of input, feature, and generation behaviors across all timesteps that the teacher handles.

### Mechanism 3
The supervision from knowledge distillation is more stable than supervision from traditional training loss of diffusion models. Knowledge distillation provides stable supervision that facilitates model training and improves student performance, while traditional training loss exhibits significant instability due to significant outliers in the predicted noise. Relational feature distillation offers more stable supervision compared to prediction-based KD and the original training loss.

## Foundational Learning

- **Knowledge Distillation**
  - Why needed here: O2MKD is built upon knowledge distillation techniques, specifically extending them to work with multiple students.
  - Quick check question: What is the main difference between one-to-one knowledge distillation and one-to-many knowledge distillation?

- **Diffusion Models**
  - Why needed here: Understanding how diffusion models work and their computational overhead is crucial for grasping why O2MKD is beneficial.
  - Quick check question: How do diffusion models differ from previous generative models in terms of computational overhead?

- **Feature Distributions in Neural Networks**
  - Why needed here: The paper discusses how feature distributions vary across timesteps in diffusion models, which is key to understanding the motivation for O2MKD.
  - Quick check question: Why do feature distributions in diffusion models vary across different timesteps?

## Architecture Onboarding

- **Component map**: Teacher diffusion model -> Multiple student diffusion models (N students), each trained on subset of continuous timesteps -> Sequential deployment during inference based on timestep
- **Critical path**: Train each student on its designated timestep range with combined diffusion and KD losses, then deploy students sequentially during sampling where student i handles timesteps [(i-1)T/N, iT/N]
- **Design tradeoffs**: Using more students (larger N) generally leads to better performance but increases memory usage; there's a trade-off between learning specific timesteps and general knowledge, controlled by the probability p
- **Failure signatures**: If distribution changes abruptly between timesteps, splitting range won't help; if student model is large enough to handle full complexity, benefit of multiple students diminishes; if p is too high, students may lose knowledge of other timesteps
- **First 3 experiments**:
  1. Implement basic one-to-one knowledge distillation and compare performance with O2MKD on CIFAR10
  2. Vary the number of students (N) in O2MKD and measure the impact on FID and memory usage
  3. Test the impact of the probability p on the performance of O2MKD by training students with different values of p

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal strategy for assigning timestep ranges to students in O2MKD, and how does non-uniform assignment compare to uniform assignment? The paper explores non-uniform timestep ranges and finds that assigning more students to smaller timesteps achieves better FID (4.23) compared to uniform assignment (4.33), but only compares three specific schemes. The general relationship between timestep assignment strategies and performance remains unclear.

### Open Question 2
How does the stability of knowledge distillation supervision compare to traditional diffusion model training loss across different model architectures and scales? The paper observes that relational feature distillation loss is more stable than prediction-based KD loss and original training loss, suggesting KD provides more stable supervision, but this observation is based on specific CIFAR10 experiments and it's unclear if this stability holds across different datasets, model architectures, and model scales.

### Open Question 3
What is the theoretical relationship between the number of students (N), the probability parameter p, and the trade-off between domain-specific and general knowledge in O2MKD? The paper discusses how larger N reduces each student's learning complexity but increases memory usage, and how p balances specific vs. general timestep knowledge, but doesn't provide a theoretical framework for this trade-off.

## Limitations

- Feature-Based KD Implementation: The paper mentions using a feature encoder and transformation function for feature-based KD but doesn't fully specify the architecture or transformation details, which is critical for reproducing the claimed stability benefits.
- Timestep Distribution Assumption: The core mechanism relies on the assumption that adjacent timesteps have similar distributions, which is stated but not empirically validated with quantitative evidence.
- Memory Efficiency Trade-offs: While O2MKD achieves better FID scores, the paper mentions that using more students increases memory usage, and the proposed model merging technique to reduce footprint is only briefly mentioned without detailed evaluation.

## Confidence

- **High Confidence**: The claim that O2MKD outperforms traditional one-to-one KD in terms of FID scores and computational efficiency is supported by experimental results across multiple datasets (CIFAR10, LSUN Church, CelebA-HQ, COCO30K).
- **Medium Confidence**: The claim that knowledge distillation provides more stable supervision than traditional training loss is supported by observed loss trends, but the underlying reasons for this stability are not fully explored.
- **Medium Confidence**: The claim that distribution changes in diffusion models occur gradually rather than abruptly is stated but not empirically validated with quantitative evidence.

## Next Checks

1. **Feature Encoder Validation**: Implement and test different feature encoder architectures to verify if the claimed stability benefits of feature-based KD hold across different implementations.

2. **Timestep Distribution Analysis**: Quantitatively analyze the distribution changes across timesteps in diffusion models to validate the core assumption that adjacent timesteps have similar distributions.

3. **Memory Efficiency Benchmark**: Evaluate the effectiveness of the model merging technique by measuring the actual memory footprint reduction and any potential performance degradation after merging multiple student models.