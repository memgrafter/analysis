---
ver: rpa2
title: Non-Robust Features are Not Always Useful in One-Class Classification
arxiv_id: '2407.06372'
source_url: https://arxiv.org/abs/2407.06372
tags:
- features
- adversarial
- performance
- one-class
- useful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Lightweight one-class classifiers for drone detection learn non-robust
  features that make them vulnerable to adversarial attacks, but these non-robust
  features are not always useful for the classification task. Experiments show that
  while all tested models (MobileNetV3-small, EfficientNetB0, ResNet18) achieve non-trivial
  performance distinguishing drones from non-drones, their performance degrades with
  increasing adversarial drift.
---

# Non-Robust Features are Not Always Useful in One-Class Classification

## Quick Facts
- arXiv ID: 2407.06372
- Source URL: https://arxiv.org/abs/2407.06372
- Reference count: 21
- One-line primary result: Lightweight one-class classifiers for drone detection learn non-robust features that make them vulnerable to adversarial attacks, but these non-robust features are not always useful for the classification task.

## Executive Summary
This paper investigates whether non-robust features learned by lightweight one-class classifiers for drone detection are useful for the classification task. The authors train three different models (MobileNetV3-small, EfficientNetB0, ResNet18) on a drone vs. non-drone dataset and evaluate their performance under adversarial attacks. They generate non-robust feature (NRF) datasets by applying targeted adversarial attacks to training samples and train separate models on these datasets. The key finding is that while all models achieve non-trivial performance, their performance degrades with increasing adversarial drift, and non-robust features are sometimes not useful for the classification task, particularly for EfficientNetB0.

## Method Summary
The authors train one-class classifiers on a drone vs. non-drone dataset using pre-trained ImageNet-1K encoders with trainable classifier heads. They evaluate standard and robust performance across negative subclasses using ℓ∞ PGD attacks with varying strengths. To test the usefulness of non-robust features, they generate NRF datasets via targeted attacks and train separate NRF models. The classifier head consists of 2 leaky ReLU layers and 1 linear layer with Gaussian bump activation, trained with logistic loss and Adam optimizer for 15 epochs with early stopping.

## Key Results
- All tested models (MobileNetV3-small, EfficientNetB0, ResNet18) achieve non-trivial performance distinguishing drones from non-drones
- Performance degrades with increasing adversarial drift, with stronger attacks reducing performance to near-random levels
- Non-robust features are useful for MobileNetV3 and ResNet18 but not for EfficientNetB0
- Model size and adversarial robustness are not reliable predictors of feature usefulness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-robust features learned by lightweight one-class classifiers are sometimes not useful for the classification task, leading to adversarial vulnerability.
- Mechanism: During training, models learn features that correlate with the label but are brittle to adversarial perturbations. In one-class classification, these features may not align with the semantic concepts needed to distinguish drones from non-drones, resulting in poor generalization and susceptibility to attacks.
- Core assumption: The non-robust features learned are task-specific and their usefulness varies across models, independent of model size or adversarial robustness.
- Evidence anchors:
  - [abstract]: "These non-robust features are not always useful for the one-class task, suggesting that learning these unpredictive and non-robust features is an unwanted consequence of training."
  - [section]: "We show that one-class classifiers generally learn useful features for both seen and unseen data, but features learnt by smaller models (MobileNetV3-small) were less useful than bigger models as adversarial drift increases."
  - [corpus]: "Average neighbor FMR=0.635, average citations=0.0." (Weak evidence; no direct support for mechanism)
- Break condition: If adversarial attacks are ineffective or if non-robust features prove consistently useful across all models, the mechanism fails.

### Mechanism 2
- Claim: Adversarial drift affects the usefulness of learned features, with larger models maintaining feature utility better than smaller ones under increased drift.
- Mechanism: As the distribution of test anomalies diverges from training anomalies, models with more parameters (ResNet18) retain useful features for classification, while smaller models (MobileNetV3) see a drop in performance, indicating less robust feature learning.
- Core assumption: Model size correlates with the ability to learn features that generalize across varying degrees of adversarial drift.
- Evidence anchors:
  - [abstract]: "Their predictive power diminishes with smaller models and adversarial drift."
  - [section]: "A drop in performance for crane2 and jeep highlights that semantic adversarial drift affects performance the most, suggesting that the positive class (drone) semantics may not have been learned well."
  - [corpus]: "Average neighbor FMR=0.635, average citations=0.0." (Weak evidence; no direct support for mechanism)
- Break condition: If performance remains stable across all models regardless of adversarial drift, the mechanism fails.

### Mechanism 3
- Claim: Stronger adversarial attacks are required to evaluate robustness in one-class classification compared to multi-class settings, due to the unique nature of the task.
- Mechanism: One-class classification focuses on distinguishing a single class from all others, necessitating stronger perturbations to effectively test model robustness, as weaker attacks may not sufficiently challenge the learned features.
- Core assumption: The task's closed-world assumption in multi-class settings does not apply to one-class classification, requiring adjusted attack strengths.
- Evidence anchors:
  - [abstract]: "Successful attacks need to be stronger than previous multi-class classification settings, especially for bigger models (ResNet18)."
  - [section]: "We observe that stronger attacks are required... We posit that the increased robustness of features learned could be due to the easier task of one-class classification compared to multi-class classification, but leave this to future work."
  - [corpus]: "Average neighbor FMR=0.635, average citations=0.0." (Weak evidence; no direct support for mechanism)
- Break condition: If standard attack strengths suffice for robustness evaluation, the mechanism fails.

## Foundational Learning

- Concept: Non-robust features
  - Why needed here: Understanding how features that are predictive but brittle to perturbations affect model performance in one-class classification.
  - Quick check question: What distinguishes non-robust features from robust features in the context of adversarial examples?

- Concept: Adversarial drift
  - Why needed here: Recognizing how shifts in the distribution of anomalies impact the usefulness of learned features and model performance.
  - Quick check question: How does adversarial drift differ between seen and unseen classes in one-class classification?

- Concept: One-class classification vs. multi-class classification
  - Why needed here: Grasping the unique challenges and attack limitations in one-class settings compared to multi-class scenarios.
  - Quick check question: What are the key differences in how adversarial examples are generated and evaluated in one-class versus multi-class classification?

## Architecture Onboarding

- Component map:
  Pre-trained encoder (ImageNet-1K) -> Trainable one-class classifier head -> Output decision
  Key components: MobileNetV3-small, EfficientNetB0, ResNet18 backbones; adversarial attack modules; NRF dataset generators

- Critical path:
  1. Train one-class classifier on drone vs. non-drone dataset
  2. Evaluate standard and robust performance across negative subclasses
  3. Generate NRF datasets with varying attack strengths
  4. Train and evaluate NRF models on NRF and original datasets

- Design tradeoffs:
  - Model size vs. computational efficiency vs. feature usefulness under adversarial drift
  - Attack strength vs. ability to reveal non-robust features
  - Use of pre-trained encoders vs. potential test leakage

- Failure signatures:
  - Random performance on NRF datasets indicates non-useful non-robust features
  - Performance degradation with adversarial drift suggests insufficient feature generalization
  - Inconsistent results across models hint at task-specific feature learning

- First 3 experiments:
  1. Train MobileNetV3-small on the drone dataset and evaluate performance across all negative subclasses.
  2. Perform adversarial attacks with varying strengths and assess robust performance.
  3. Generate NRF datasets and train NRF models to test the usefulness of non-robust features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes models to learn non-useful non-robust features during training?
- Basis in paper: [explicit] The paper concludes that "model training can produce features that are not useful, not robust or both during training" and calls for future work to "investigate the cause of models learning these unwanted features which are not useful nor robust, and how to prevent this."
- Why unresolved: The paper identifies the phenomenon but does not investigate the underlying causes of why models learn these features.
- What evidence would resolve it: Controlled experiments varying training data characteristics, model architectures, and training procedures to identify factors that correlate with learning non-useful non-robust features.

### Open Question 2
- Question: How can we prevent models from learning non-useful non-robust features?
- Basis in paper: [explicit] The conclusion states "An important follow-up work would be to investigate... how to prevent this" regarding unwanted feature learning.
- Why unresolved: The paper only identifies the problem but does not propose or test any methods to prevent learning of these features.
- What evidence would resolve it: Successful training methods or architectural modifications that reduce or eliminate learning of non-useful non-robust features while maintaining or improving model performance.

### Open Question 3
- Question: Are non-robust features in one-class classification inherently less useful than in multi-class classification?
- Basis in paper: [explicit] The paper contrasts its findings with Ilyas et al. [7], noting that "contrary to their findings in multi-class classification, we show that the non-robust features learnt are sometimes not useful for the one-class task."
- Why unresolved: The paper demonstrates the difference but does not explain why this difference exists or whether it's a fundamental property of one-class classification.
- What evidence would resolve it: Systematic comparison of feature usefulness across multiple one-class and multi-class tasks with varying characteristics, or theoretical analysis of why non-robust features would be less useful in one-class settings.

## Limitations
- Weak corpus evidence with average neighbor FMR=0.635 and zero citations limits validation of claims
- Lack of theoretical grounding for why stronger attacks are needed in one-class classification
- Does not investigate underlying causes of unwanted feature learning or propose prevention methods

## Confidence
- **High Confidence**: Claims about model performance degradation under adversarial drift
- **Medium Confidence**: Claims about non-robust features not always being useful
- **Low Confidence**: Claims about why stronger attacks are needed for one-class classification

## Next Checks
1. Verify the exact implementation of Gaussian bump activation in the one-class classifier head and confirm how it translates to correlation formulation in Eq. (1)
2. Test the NRF dataset generation process with controlled perturbations to ensure non-robust features are properly correlated with labels
3. Compare performance of one-class classifiers against standard binary classifiers on the same task to quantify the unique challenges of one-class settings