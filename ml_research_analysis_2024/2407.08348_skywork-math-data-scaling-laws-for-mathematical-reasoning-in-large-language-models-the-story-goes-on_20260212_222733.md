---
ver: rpa2
title: 'Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language
  Models -- The Story Goes On'
arxiv_id: '2407.08348'
source_url: https://arxiv.org/abs/2407.08348
tags:
- data
- math
- problems
- reasoning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Skywork-Math, a series of 7B language models
  fine-tuned for mathematical reasoning. The authors argue that data scaling laws
  for math reasoning in LLMs are not yet saturated and demonstrate that increasing
  data quantity improves model performance.
---

# Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On

## Quick Facts
- arXiv ID: 2407.08348
- Source URL: https://arxiv.org/abs/2407.08348
- Reference count: 40
- Primary result: Skywork-Math 7B achieves 51.2% on MATH and 83.9% on GSM8K, state-of-the-art among models <10B parameters

## Executive Summary
This paper introduces Skywork-Math, a series of 7B language models fine-tuned for mathematical reasoning that achieve state-of-the-art performance on MATH (51.2%) and GSM8K (83.9%) benchmarks among models smaller than 10B parameters. The authors demonstrate that data scaling laws for math reasoning in LLMs are not yet saturated, showing that increasing synthetic data quantity from 1M to 2.5M examples yields proportional performance improvements. The models are trained using a novel two-stage data synthesis pipeline and a 2.5M-instance dataset called Skywork-MathQA, which combines diverse augmentation methods and curriculum learning principles.

## Method Summary
The authors develop a two-stage supervised fine-tuning approach using a synthetic dataset of 2.5M math problems. Stage 1 generates 2.1M "normal" problems, followed by Stage 2 adding 0.4M "hard" problems. Three data augmentation methods (backward reasoning, evolutionary complexity, self-correction) are employed to ensure diversity. The models are fine-tuned for 3 epochs with a batch size of 32 using AdamW optimizer. Evaluation is performed on MATH and GSM8K benchmarks using zero-shot chain-of-thought prompting with a maximum sequence length of 2048 tokens.

## Key Results
- Skywork-Math 7B achieves 51.2% accuracy on MATH benchmark, outperforming early GPT-4 versions
- Model achieves 83.9% accuracy on GSM8K, setting new state-of-the-art for sub-10B parameter models
- Performance scales linearly with data quantity, demonstrating data scaling laws are not yet saturated
- Two-stage curriculum learning (easy→hard) outperforms random problem ordering by significant margins

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing synthetic math data quantity leads to proportional improvements in model performance
- Mechanism: More diverse and challenging problems progressively refine the model's reasoning pathways
- Core assumption: Model architecture has sufficient capacity to absorb additional data without overfitting
- Evidence: Scaling curve shows clear relationship between SFT data size and performance (section 4.2.2)

### Mechanism 2
- Claim: Two-stage curriculum (easy first, then hard) significantly improves mathematical reasoning
- Mechanism: Models build foundational intuition through simpler problems before tackling complex reasoning
- Core assumption: Knowledge transfers effectively from simple to complex problems without interference
- Evidence: Staged training outperforms single-stage approaches (section 4.3.1)

### Mechanism 3
- Claim: Data diversity through augmentation methods prevents overfitting and improves generalization
- Mechanism: Different augmentation techniques introduce varied problem formulations across mathematical domains
- Core assumption: Augmented data maintains mathematical validity while introducing meaningful variation
- Evidence: "Mix" approach combining three augmentation methods achieves highest performance (section 4.3.2)

## Foundational Learning

- Concept: Curriculum learning and staged knowledge acquisition
  - Why needed: Mathematical reasoning requires building from basic operations to complex multi-step reasoning
  - Quick check: Why does the two-stage approach (2.1M normal → 0.4M hard) outperform training on all 2.5M problems simultaneously?

- Concept: Data scaling laws and relationship between data quantity and performance
  - Why needed: Understanding scaling helps determine optimal data collection strategies and resource allocation
  - Quick check: What happens to the scaling curve when additional data no longer introduces new mathematical concepts?

- Concept: Data augmentation techniques and impact on model generalization
  - Why needed: Simple repetition leads to memorization rather than reasoning; augmentation must introduce meaningful variation
  - Quick check: How do different augmentation methods (backward reasoning, evolutionary complexity, self-correction) contribute uniquely?

## Architecture Onboarding

- Component map: Data synthesis pipeline → Filtering/verification → SFT training → Evaluation benchmarks
- Critical path: Each stage depends on previous stage's output quality; data generation → filtering → training → evaluation
- Design tradeoffs: Larger datasets improve performance but increase computational costs; more diverse augmentation improves generalization but may introduce noise
- Failure signatures: Plateauing performance suggests capacity limits; performance degradation after filtering suggests loss of diversity
- First 3 experiments:
  1. Train on 1M vs 2.5M synthetic data to verify scaling law relationship
  2. Compare staged training (easy→hard) vs random problem ordering
  3. Test different augmentation method combinations to identify optimal diversity strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do data scaling laws for mathematical reasoning in LLMs evolve as models exceed 10B parameters, and at what point might they plateau?
- Basis: Paper states scaling laws are "far from being saturated" for 7B models but doesn't explore larger models
- Why unresolved: Study focuses exclusively on 7B parameter models
- Resolution evidence: Benchmarking models from 7B to 100B+ parameters on varying data quantities

### Open Question 2
- Question: What specific characteristics make synthetic math data effective beyond quantity and diversity metrics?
- Basis: Authors note data selection with verifier can decrease problem difficulty and hurt performance
- Why unresolved: Paper identifies data selection complexity but lacks clear quality criteria
- Resolution evidence: Analysis of which synthetic data properties most strongly correlate with downstream performance

### Open Question 3
- Question: How transferable are observed scaling laws across different mathematical domains and reasoning types?
- Basis: Performance varies across MATH subjects but scaling relationships aren't analyzed per domain
- Why unresolved: Subject-specific performance differences reported without domain-level scaling analysis
- Resolution evidence: Training curves showing data-quantity scaling for each MATH subject area

### Open Question 4
- Question: What mechanisms cause diminishing returns when scaling beyond certain data thresholds?
- Basis: Authors observe performance plateaus and note data quality becomes more important than quantity
- Why unresolved: Paper identifies plateau effect but doesn't investigate underlying causes
- Resolution evidence: Analysis of model behavior on increasingly difficult problems and memorization rates

### Open Question 5
- Question: How do different synthetic data generation strategies compare in effectiveness for math reasoning?
- Basis: Authors compare three augmentation methods and find mixing performs best, but don't explore alternatives
- Why unresolved: While three methods tested, paper doesn't explore other generation strategies
- Resolution evidence: Head-to-head comparison of Skywork-MathQA pipeline against alternative generation approaches

## Limitations

- Data contamination risk remains a concern despite filtering, with potential overlap between training and test data
- Model performance significantly degrades on Chinese benchmarks, suggesting language-dependent rather than general reasoning capabilities
- Computational costs of generating and processing 2.5M synthetic examples are not adequately addressed
- Attribution of performance gains to genuine reasoning versus pattern matching remains uncertain

## Confidence

**High Confidence Claims**:
- Two-stage training curriculum improves performance compared to single-stage approaches
- Data quantity positively correlates with mathematical reasoning performance in 7B models
- Skywork-Math 7B achieves state-of-the-art performance among sub-10B parameter models

**Medium Confidence Claims**:
- Data scaling laws for mathematical reasoning are not yet saturated in modern LLMs
- Specific combination of three augmentation methods provides optimal diversity
- Curriculum learning is essential for developing mathematical reasoning capabilities

**Low Confidence Claims**:
- Model has developed genuine mathematical reasoning versus pattern matching
- Results will generalize to languages other than English
- Scaling relationship will continue indefinitely with additional data

## Next Checks

**Validation Check 1**: Conduct systematic contamination analysis using n-gram overlap metrics across all evaluation benchmarks to quantify potential data leakage, including both exact match detection and semantic similarity measures.

**Validation Check 2**: Test model performance on adversarially generated math problems requiring genuine reasoning rather than pattern matching, including problems with slight modifications to standard formats or novel concept combinations.

**Validation Check 3**: Evaluate cross-linguistic generalization by testing on additional non-English mathematical reasoning benchmarks and fine-tuning on multilingual datasets to determine whether reasoning capabilities transfer across languages.