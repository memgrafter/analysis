---
ver: rpa2
title: On Generating Monolithic and Model Reconciling Explanations in Probabilistic
  Scenarios
arxiv_id: '2405.19229'
source_url: https://arxiv.org/abs/2405.19229
tags:
- explanation
- explanations
- monolithic
- probabilistic
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified framework for generating probabilistic
  explanations in AI systems, distinguishing between monolithic explanations (self-contained
  justifications) and model reconciling explanations (addressing discrepancies between
  agent and human knowledge). The authors introduce probabilistic logic as a mechanism
  for handling uncertainty, defining explanatory gain and explanatory power as quantitative
  metrics to assess explanation quality.
---

# On Generating Monolithic and Model Reconciling Explanations in Probabilistic Scenarios

## Quick Facts
- arXiv ID: 2405.19229
- Source URL: https://arxiv.org/abs/2405.19229
- Authors: Stylianos Loukas Vasileiou; William Yeoh; Alessandro Previti; Tran Cao Son
- Reference count: 12
- Primary result: Unified framework for generating probabilistic explanations distinguishing monolithic and model reconciling explanations using explanatory gain and power metrics

## Executive Summary
This paper presents a unified framework for generating probabilistic explanations in AI systems, distinguishing between monolithic explanations (self-contained justifications) and model reconciling explanations (addressing discrepancies between agent and human knowledge). The authors introduce probabilistic logic as a mechanism for handling uncertainty, defining explanatory gain and explanatory power as quantitative metrics to assess explanation quality. They develop algorithms that exploit the duality between minimal correction sets and minimal unsatisfiable sets to efficiently compute both types of explanations. Extensive experiments on planning, scheduling, and random CNF benchmarks demonstrate the effectiveness and scalability of their approach, with the probabilistic model reconciling explanations showing particular promise in scenarios with varying degrees of knowledge asymmetry between agents and human users.

## Method Summary
The framework encodes agent and human knowledge as belief bases with weighted formulae, inducing probability distributions over possible worlds. For monolithic explanations, it uses weighted MaxSAT solvers to find top-k most probable worlds where the explanandum holds, then computes minimal hitting sets to extract explanations. For model reconciling explanations, it extends this approach to consider discrepancies between agent and human belief bases. The algorithms exploit the duality between minimal correction sets and minimal unsatisfiable sets, allowing efficient computation through hitting set calculations. Explanatory gain and power metrics quantify explanation quality by measuring probability increase and balancing explanation strength against its own likelihood.

## Key Results
- The framework successfully generates both monolithic and model reconciling explanations in probabilistic contexts
- Algorithms exploiting MUS/MCS duality demonstrate computational efficiency compared to exhaustive search
- Probabilistic model reconciling explanations show effectiveness in planning and scheduling domains
- Explanatory power metric provides tunable balance between explanation strength and likelihood

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework efficiently computes explanations by leveraging the duality between minimal correction sets (MCSes) and minimal unsatisfiable sets (MUSes).
- Mechanism: The algorithms exploit the hitting set duality to iteratively compute MUSes and MCSes, where finding a minimal hitting set on the collection of all MCSes corresponds to an SMUS (smallest MUS). This allows efficient computation of both monolithic and model reconciling explanations.
- Core assumption: The problem domain can be encoded in propositional logic where satisfiability of subsets can be decided.
- Evidence anchors:
  - [abstract] "We present algorithms that exploit the duality between minimal correction sets and minimal unsatisfiable sets to efficiently compute both types of explanations in probabilistic contexts."
  - [section] "MUSes and MCSes are related by the concept of hitting set: Proposition 1. A subset of an inconsistent knowledge base KB is an MUS (resp. MCS) if and only if it is a minimal hitting set of the collection of all MCSes (resp. MUSes) of KB."
- Break condition: If the knowledge base becomes too large or complex, the exponential worst-case complexity of MUS/MCS enumeration may become prohibitive.

### Mechanism 2
- Claim: The framework handles uncertainty by using probabilistic logic to induce probability distributions over possible worlds, enabling explanations that account for degrees of belief.
- Mechanism: Belief bases with weighted formulae induce probability distributions over possible worlds. The framework defines explanatory gain and explanatory power metrics that quantify how well explanations increase the probability of the explanandum while accounting for the explanation's own likelihood.
- Core assumption: Uncertainty can be adequately represented through probability distributions over possible worlds induced from weighted knowledge bases.
- Evidence anchors:
  - [abstract] "For monolithic explanations, our approach integrates uncertainty by utilizing probabilistic logic to increase the probability of the explanandum."
  - [section] "Given a belief base B, one way to induce a probability distribution is the following: ∀ω ∈ Ω, P_B(ω) = 1/Z exp(∑wi · I(ω, ϕi))"
- Break condition: If the probability distribution induced from the belief base becomes too skewed or the weights are poorly calibrated, the quality metrics may not accurately reflect explanation effectiveness.

### Mechanism 3
- Claim: The framework can generate both monolithic explanations (self-contained justifications) and model reconciling explanations (addressing discrepancies between agent and human knowledge) within a unified probabilistic framework.
- Mechanism: The framework extends classical explanation concepts to probabilistic settings. For monolithic explanations, it finds subsets that increase the explanandum's probability. For model reconciling explanations, it finds explanations that both increase the explanandum's probability and reconcile differences between agent and human models.
- Core assumption: Both agent and human models can be represented as knowledge bases or belief bases within the same logical language.
- Evidence anchors:
  - [abstract] "Our approach integrates uncertainty by utilizing probabilistic logic to increase the probability of the explanandum. For model reconciling explanations, we propose a framework that extends the logic-based variant of the model reconciliation problem to account for probabilistic human models."
  - [section] "Definition 15 (Probabilistic Model Reconciling Explanation). Given the knowledge base KBα of an agent, the belief base Bh of a human user, and an explanandum φ..."
- Break condition: If the human model cannot be adequately approximated or if the logical language cannot capture the necessary domain knowledge, the framework may fail to generate meaningful explanations.

## Foundational Learning

- Concept: Propositional logic and satisfiability
  - Why needed here: The entire framework relies on encoding knowledge and explanations in propositional logic, and uses SAT solvers to check satisfiability and find MUSes/MCSes.
  - Quick check question: Can you explain the difference between a knowledge base that is consistent vs. inconsistent, and how this relates to entailment?

- Concept: Probability theory and distributions over possible worlds
  - Why needed here: The framework uses probabilistic logic to represent uncertainty, inducing probability distributions over possible worlds from weighted knowledge bases.
  - Quick check question: How does the weight of a formula in a belief base affect the probability of possible worlds that satisfy that formula?

- Concept: Explanation quality metrics (explanatory gain and explanatory power)
  - Why needed here: These metrics quantitatively assess how well explanations increase the probability of the explanandum while accounting for the explanation's own likelihood.
  - Quick check question: Can you derive the formula for explanatory power and explain how the parameter γ balances explanatory gain against explanation probability?

## Architecture Onboarding

- Component map:
  Input layer: Agent knowledge base (KBα), Human belief base (Bh), Explanandum φ
  Core processing: Weighted MaxSAT solver for computing top-k most probable worlds, MUS/MCS enumeration algorithms, Hitting set computation
  Quality assessment: Explanatory gain and explanatory power calculation modules
  Output layer: Monolithic explanations, Model reconciling explanations with associated quality metrics

- Critical path:
  1. Parse input knowledge bases and convert to weighted MaxSAT format
  2. Use weighted MaxSAT solver to find top-k most probable worlds where explanandum is true
  3. Compute intersections of these worlds to create candidate explanation sets
  4. Check if candidate sets entail the explanandum
  5. If not, decrement k and repeat
  6. Once valid candidate found, use MUS/MCS duality algorithms to extract minimal explanations
  7. Calculate explanatory gain and power metrics
  8. Return explanations with quality scores

- Design tradeoffs:
  - Completeness vs. efficiency: Exhaustive search guarantees finding optimal explanations but is computationally expensive
  - Precision vs. interpretability: More precise probabilistic representations may be harder for humans to understand
  - General vs. specific explanations: Broader explanations may apply to more scenarios but be less informative

- Failure signatures:
  - Timeout during weighted MaxSAT computation indicates overly complex probability distributions
  - Failure to find any explanation suggests the explanandum may not be derivable from the knowledge base
  - Explanations with low explanatory power indicate poor quality despite being valid

- First 3 experiments:
  1. Test on a simple planning domain (like the office robot delivery example) with known ground truth to verify correctness
  2. Vary the parameter k in the k-bounded explanation algorithm to observe the tradeoff between explanation quality and computational cost
  3. Compare explanations generated for deterministic vs. probabilistic human models to quantify the benefit of uncertainty handling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can probabilistic model reconciling explanations be computed efficiently when the human belief base contains millions of weighted formulae?
- Basis in paper: [explicit] The paper acknowledges that computing probabilities is #P-complete and mentions the need for efficient algorithms, but doesn't address scalability to large belief bases.
- Why unresolved: The proposed algorithms leverage hitting set duality and weighted MaxSAT solvers, but their performance characteristics on extremely large belief bases remain unexplored.
- What evidence would resolve it: Empirical results demonstrating algorithm performance on belief bases with millions of formulae, or complexity analysis showing the algorithms scale polynomially with belief base size.

### Open Question 2
- Question: What is the optimal way to learn or approximate a human's probabilistic belief base from interaction data?
- Basis in paper: [inferred] The paper assumes the agent has access to the human's belief base but acknowledges this is unrealistic, mentioning that learning the human model from past interactions is left for future work.
- Why unresolved: The paper focuses on explanation generation given a belief base, but doesn't address how to acquire or approximate this belief base in practice.
- What evidence would resolve it: A demonstrated method for learning probabilistic belief bases from dialogue traces or interaction data, with quantitative evaluation of the approximation accuracy.

### Open Question 3
- Question: How do different values of the explanatory power parameter γ affect user satisfaction with explanations across different domains?
- Basis in paper: [explicit] The paper introduces γ as a tunable parameter in the explanatory power formula but doesn't investigate its impact on explanation quality from a human user perspective.
- Why unresolved: While the mathematical framework defines explanatory power, there's no empirical validation of how γ choices affect human comprehension or satisfaction.
- What evidence would resolve it: User studies comparing explanation effectiveness across different γ values and domains, measuring comprehension, trust, and task performance.

## Limitations

- Scalability concerns for large belief bases with millions of weighted formulae
- Dependence on having access to accurate human belief bases, which may not be realistic in practice
- Computational complexity of MUS/MCS enumeration may limit applicability to very large problem instances

## Confidence

- Theoretical framework for probabilistic explanations: High
- Efficiency of algorithms exploiting MUS/MCS duality: Medium
- Effectiveness of probabilistic model reconciling explanations: Medium
- Scalability to real-world problems: Low

## Next Checks

1. **Empirical scalability testing**: Implement the algorithms and benchmark on progressively larger CNF instances, measuring runtime and memory usage to identify practical scalability limits.

2. **Ablation study on weight calibration**: Systematically vary the weights in belief bases and measure the impact on explanation quality metrics to understand sensitivity to weight choices.

3. **Human evaluation of explanation quality**: Conduct user studies comparing explanations generated by the probabilistic framework against baseline approaches to validate that the quantitative metrics align with human judgments of explanation quality.