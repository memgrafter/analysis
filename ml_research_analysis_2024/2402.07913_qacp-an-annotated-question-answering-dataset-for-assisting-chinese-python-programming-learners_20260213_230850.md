---
ver: rpa2
title: 'QACP: An Annotated Question Answering Dataset for Assisting Chinese Python
  Programming Learners'
arxiv_id: '2402.07913'
source_url: https://arxiv.org/abs/2402.07913
tags:
- llms
- python
- programming
- questions
- learners
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QACP, a new Chinese question-and-answer dataset
  for Python learners, addressing the scarcity of data resources for training intelligent
  educational systems. The dataset contains 10,960 questions categorized by learner
  types and knowledge points, with annotations covering accessible answers, classical
  analogies, and code examples.
---

# QACP: An Annotated Question Answering Dataset for Assisting Chinese Python Programming Learners

## Quick Facts
- arXiv ID: 2402.07913
- Source URL: https://arxiv.org/abs/2402.07913
- Reference count: 32
- Primary result: New Chinese Q&A dataset for Python learners with 10,960 questions and multi-dimensional annotations shows existing LLMs perform poorly on specialized Python tasks

## Executive Summary
This paper introduces QACP, a Chinese question-and-answer dataset designed to support intelligent educational systems for Python programming learners. The dataset addresses the scarcity of domain-specific training data by collecting 10,960 real student questions and annotating them across three dimensions: accessible answers, classical analogies, and code examples. The authors evaluate multiple large language models on this dataset, finding that even advanced models like GPT-4 struggle with Python-specific knowledge tasks, highlighting the critical need for specialized educational datasets in programming instruction.

## Method Summary
The study constructs a specialized Chinese Q&A dataset for Python learners through systematic query collection from actual student questions, categorization by learner types and knowledge points, and multi-dimensional annotation with accessible answers, analogies, and code examples. A rigorous quality control process employs a dual-verification mechanism with experienced Python instructors to ensure annotation consistency. The dataset is then evaluated using both human-based and LLM-based assessments, focusing on answer correctness, consistency between questions and answers, and the usefulness of provided examples.

## Key Results
- Existing models including GPT-4 perform poorly on specialized Python knowledge tasks
- The dataset contains 10,960 questions categorized by learner types and knowledge points
- Multi-dimensional annotations (answers, analogies, code examples) provide comprehensive educational support
- Quality control achieved 0.89 kappa score indicating high annotation consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized dataset improves LLM performance on Python Q&A by providing domain-specific training data
- Mechanism: Real student questions across learning stages with expert annotations create targeted knowledge base
- Core assumption: General LLMs lack domain-specific training data for Python tasks
- Evidence anchors:
  - "existing models, including GPT-4, perform poorly on specialized Python knowledge tasks, highlighting the need for domain-specific training data"
  - "The creation of intelligent assistant large language models (LLMs) tailored for programming education necessitates distinct data support"
- Break condition: Dataset fails to capture real learner question diversity

### Mechanism 2
- Claim: Multi-dimensional annotation improves educational value by addressing different learning styles
- Mechanism: Three annotation dimensions cater to diverse learner preferences and cognitive processing styles
- Core assumption: Learners benefit from multiple explanation approaches
- Evidence anchors:
  - "This annotation principle is designed to enhance the effectiveness and quality of online programming education"
  - "based on user profiles, we annotated the answers to these questions from three dimensions: accessible answers to the question, classical analogies, and code examples"
- Break condition: Multiple annotations create confusion or learners consistently prefer one type

### Mechanism 3
- Claim: Quality control through expert review ensures dataset reliability for model training
- Mechanism: Dual-verification mechanism with experienced Python instructors maintains annotation consistency
- Core assumption: Expert-reviewed annotations produce higher quality training data
- Evidence anchors:
  - "To ensure the high quality of the annotated answers, we implemented a rigorous quality control process"
  - "The final score was 0.89 (The score closer to 1 indicates higher consistency), demonstrating that our annotations are highly consistent"
- Break condition: Expert review introduces bias or verification becomes too slow

## Foundational Learning

- Concept: Python programming fundamentals
  - Why needed here: Dataset specifically targets Python learners requiring understanding of syntax, data structures, and programming concepts
  - Quick check question: Can you explain the difference between Python lists and tuples?

- Concept: Educational theory application
  - Why needed here: Annotation process references Bloom's Taxonomy, Cognitive Load Theory, and Affective Education Theory
  - Quick check question: What are the three domains of Bloom's Taxonomy and how do they apply to programming education?

- Concept: Large language model evaluation metrics
  - Why needed here: Study evaluates LLM performance using custom metrics for correctness, consistency, and usefulness
  - Quick check question: How do BLEU and ROUGE metrics differ from human evaluation in assessing educational content?

## Architecture Onboarding

- Component map:
  Data collection pipeline → Question categorization → Multi-dimensional annotation → Expert review → Dataset storage → Model evaluation framework
  Key components: Query collection system, annotation interface, quality control workflow, evaluation benchmark

- Critical path:
  Question collection → Initial annotation → Expert verification → Dataset compilation → Model testing → Performance analysis
  Time-critical: Expert review phase (dual-verification adds latency but ensures quality)

- Design tradeoffs:
  Expert review vs. automation: Higher quality vs. scalability
  Multiple annotation dimensions vs. single answers: Educational value vs. dataset size
  Chinese language focus vs. multilingual support: Cultural relevance vs. broader applicability

- Failure signatures:
  Low kappa values in annotation consistency (>0.2 deviation from target)
  Model performance plateau despite dataset expansion
  High variance in question difficulty distribution

- First 3 experiments:
  1. Test annotation consistency by having new annotators label 100 random questions and calculate inter-rater reliability
  2. Evaluate model performance on a held-out validation set with systematic error analysis
  3. Compare annotation quality between expert-reviewed and crowd-sourced samples using domain expert scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the accuracy of large language models in answering Python programming questions for Chinese learners?
- Basis in paper: [explicit] The paper states that existing models, including GPT-4, perform poorly on specialized Python knowledge tasks, highlighting the need for domain-specific training data
- Why unresolved: Despite the introduction of the QACP dataset and evaluation of various models, the paper concludes that LLMs still struggle with Python-specific knowledge, indicating a gap in current approaches
- What evidence would resolve it: Further experiments showing improved performance of models trained on the QACP dataset or similar specialized datasets, demonstrating better accuracy in answering Python programming questions for Chinese learners

### Open Question 2
- Question: What are the specific challenges in using general LLMs as intelligent teaching assistants in computer programming courses?
- Basis in paper: [explicit] The paper highlights the potential limitations of general LLMs as intelligent teaching assistants in computer programming courses, as shown by the poor performance of models like GPT-4 on specialized Python knowledge tasks
- Why unresolved: While the paper identifies the limitations, it does not provide a detailed analysis of the specific challenges or propose solutions to address them
- What evidence would resolve it: A comprehensive study identifying the key challenges in using general LLMs for programming education and proposing targeted solutions or improvements to overcome these challenges

### Open Question 3
- Question: How can we develop specialized vertical-domain LLMs for Chinese application scenarios in programming education?
- Basis in paper: [inferred] The paper suggests that Chinese open-source LLM frameworks have the feasibility and application potential for developing specialized vertical-domain LLMs for Chinese application scenarios, as indicated by the performance of models like ChatGLM
- Why unresolved: While the paper hints at the potential, it does not provide a concrete approach or methodology for developing such specialized LLMs
- What evidence would resolve it: A detailed framework or methodology for developing specialized vertical-domain LLMs for Chinese programming education, along with experimental results demonstrating their effectiveness compared to general LLMs

## Limitations

- Chinese-language focus limits applicability to non-Chinese educational contexts
- Dataset construction methodology well-documented but annotation guidelines not fully specified
- Long-term effectiveness of dataset for improving LLM performance remains unproven

## Confidence

- High confidence: Dataset construction methodology and quality control processes are well-documented
- Medium confidence: Claim that general LLMs perform poorly on Python tasks due to lack of domain-specific training data
- Low confidence: Assertion that multi-dimensional annotations universally improve learning outcomes

## Next Checks

1. Conduct inter-rater reliability testing with new annotators on a random sample of 200 questions
2. Perform systematic error analysis on model responses to identify knowledge gaps
3. Test dataset with non-Chinese Python learners to assess cross-cultural applicability