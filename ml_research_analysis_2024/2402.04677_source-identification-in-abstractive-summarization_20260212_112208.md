---
ver: rpa2
title: Source Identification in Abstractive Summarization
arxiv_id: '2402.04677'
source_url: https://arxiv.org/abs/2402.04677
tags:
- summary
- sentences
- source
- sentence
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper defines source sentences as input sentences containing
  essential information in generated summaries, then studies abstractive summarization
  by analyzing these source sentences. The authors annotate source sentences for both
  reference and PEGASUS-generated summaries on XSum and CNN/DailyMail datasets, creating
  the SourceSum benchmark.
---

# Source Identification in Abstractive Summarization

## Quick Facts
- arXiv ID: 2402.04677
- Source URL: https://arxiv.org/abs/2402.04677
- Authors: Yoshi Suhara; Dimitris Alikaniotis
- Reference count: 36
- Key outcome: Perplexity gain method outperforms similarity-based approaches in highly abstractive summarization by measuring confidence drop when removing source sentences

## Executive Summary
This paper introduces a novel framework for identifying source sentences in abstractive summarization by defining them as input sentences containing essential information for generated summaries. The authors create the SourceSum benchmark by annotating source sentences for both reference and PEGASUS-generated summaries on XSum and CNN/DailyMail datasets. They propose a perplexity gain method that measures the increase in model perplexity when specific sentences are removed from the input, and compare it against similarity-based approaches (BERTScore, ROUGE, SimCSE), cross-attention weights, and other baselines. Results show perplexity gain performs best in highly abstractive settings (XSum) while similarity-based methods work robustly in extractive settings (CNN/DM).

## Method Summary
The authors create SourceSum, a benchmark for source sentence detection, by first sampling document-summary pairs from XSum and CNN/DailyMail datasets, then annotating source sentences through crowdworkers who identify sentences necessary to reconstruct the summary. They implement multiple source sentence detection methods including perplexity gain (measuring PPL(Y|X) - PPL(Y|X\s)), similarity-based approaches (BERTScore, ROUGE, SimCSE), and cross-attention weights. The perplexity gain method computes how much removing each sentence hurts the model's ability to generate the same summary, while similarity methods capture lexical and semantic overlap. Cross-attention weights analyze attention patterns between encoder and decoder. All methods rank sentences by importance and are evaluated using NDCG and MAP metrics.

## Key Results
- Perplexity gain method outperforms all other approaches in highly abstractive XSum setting
- Similarity-based methods (BERTScore, ROUGE, SimCSE) perform robustly in extractive CNN/DM setting
- Cross-attention weights are model-specific but indirect, showing lower performance than perplexity gain
- Annotation quality is high with Krippendorff's alpha of 0.8 for reconstructability judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity gain method outperforms similarity-based methods in highly abstractive settings because it directly measures the model's confidence drop when removing source sentences.
- Mechanism: By calculating PPL(Y|X;θ) - PPL(Y|X\s;θ), the method quantifies how much removing a sentence hurts the model's ability to generate the same summary, capturing source importance beyond surface similarity.
- Core assumption: The summarization model's perplexity reflects its reliance on specific source sentences, and abstractive models depend more on context than extractive ones.
- Evidence anchors: [abstract] "Experimental results show that the perplexity-based method performs well in highly abstractive settings", [section] "Perplexity Gain, which is based on the perplexity increase when the target sentence is removed, performs the best in highly abstractive settings (XSum)"

### Mechanism 2
- Claim: Similarity-based methods work better in extractive settings because summaries closely mirror source content.
- Mechanism: ROUGE and BERTScore capture lexical and semantic overlap between summaries and source sentences, which is sufficient when summaries are direct extracts.
- Core assumption: In extractive settings, important source sentences will have high similarity scores with the summary.
- Evidence anchors: [abstract] "similarity-based methods perform robustly in relatively extractive settings", [section] "the similarity-based methods perform best on the CNN/DM-Pegasus (SimCSE, BERTScore) and CNN/DM-Reference (ROUGE)"

### Mechanism 3
- Claim: Cross-attention weights are model-specific but indirect, making them less effective than perplexity gain.
- Mechanism: Cross-attention weights show token-level importance but aggregate poorly to sentence-level relevance compared to direct perplexity measurements.
- Core assumption: Cross-attention weights capture source importance, but sentence-level aggregation loses critical information.
- Evidence anchors: [abstract] "the attention-based method is model-specific, but is still an indirect method", [section] "Cross-attention (Juraska and Walker, 2021)" shows lower performance than perplexity gain

## Foundational Learning

- Concept: Perplexity as model confidence measure
  - Why needed here: Understanding how perplexity reflects model certainty is crucial for interpreting the perplexity gain method's effectiveness.
  - Quick check question: If a model has low perplexity on a summary, does this mean it's highly confident in generating that summary?

- Concept: Abstractive vs extractive summarization distinction
  - Why needed here: The paper's core finding depends on understanding how abstractive and extractive settings differ in source sentence importance detection.
  - Quick check question: In abstractive summarization, are source sentences more likely to be directly quoted or paraphrased in the summary?

- Concept: Cross-attention mechanism in Transformer models
  - Why needed here: The paper compares cross-attention weights to other methods, requiring understanding of how attention works in encoder-decoder architectures.
  - Quick check question: What information does cross-attention capture between encoder and decoder layers in Transformer models?

## Architecture Onboarding

- Component map: Data preparation → Annotation pipeline → Source sentence detection methods → Evaluation framework → Analysis pipeline
- Critical path: Data sampling → Annotation collection → Method implementation → Evaluation → Analysis
- Design tradeoffs: Sentence-level vs token-level annotation, model-specific vs model-agnostic methods, direct vs indirect importance measures
- Failure signatures: Low inter-annotator agreement, poor method performance on either dataset, reconstructability issues
- First 3 experiments:
  1. Implement perplexity gain method and test on small sample from XSum
  2. Compare ROUGE-based similarity detection with perplexity gain on CNN/DM
  3. Analyze correlation between cross-attention weights and perplexity gain scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different summarization models beyond PEGASUS (e.g., BART, T5, or GPT-4) perform on the SourceSum benchmark?
- Basis in paper: [explicit] The paper states "Last but not least, the benchmark is created on top of a Transformer-based encoder-decoder model PEGASUS and the results do not necessarily apply to other encoder-decoder models or autoregressive models such as GPT series."
- Why unresolved: The study only evaluated PEGASUS-generated summaries, limiting generalizability to other models.
- What evidence would resolve it: Testing the same source sentence detection methods on summaries generated by different models (BART, T5, GPT-4) would reveal whether perplexity gain remains optimal across architectures.

### Open Question 2
- Question: Does the performance of perplexity gain degrade when applied to longer documents or multi-document summarization?
- Basis in paper: [inferred] The paper mentions "For multi-document summarization, we believe that the same annotation and evaluation framework can be applied and is interesting future work" and the benchmark uses relatively short news articles.
- Why unresolved: SourceSum focuses on single-document news summarization with limited document length, leaving scalability untested.
- What evidence would resolve it: Evaluating perplexity gain on multi-document datasets (e.g., MultiNews) and longer documents would determine if computational complexity or effectiveness changes.

### Open Question 3
- Question: Can the perplexity gain method be adapted to work without access to the full summarization model parameters?
- Basis in paper: [explicit] The paper notes "the attention-based method is model-specific" and "Different from the similarity-based method, the attention-based method is model-specific, but is still an indirect method."
- Why unresolved: Current perplexity gain requires the summarization model to compute PPL(Y|X;θ), limiting practical deployment.
- What evidence would resolve it: Developing a proxy method that approximates perplexity gain without model access (e.g., using a separate language model) would test feasibility.

## Limitations
- Evaluation relies exclusively on two English news summarization datasets, limiting generalizability to other domains
- Perplexity gain method requires expensive model inference for each candidate sentence removal, making it computationally prohibitive for long documents
- Annotation process depends on reconstructability judgments that may not fully capture semantic dependencies beyond surface-level information overlap

## Confidence

**Confidence Labels:**
- **High confidence**: The perplexity gain method outperforms other approaches in XSum's highly abstractive setting, supported by consistent experimental results across multiple metrics.
- **Medium confidence**: Similarity-based methods' superior performance in extractive settings, though the distinction between abstractive and extractive is clear, the boundary may be more gradual in practice.
- **Medium confidence**: Cross-attention weights being "model-specific but indirect" - while demonstrated empirically, the theoretical reasons for this limitation could be further explored.

## Next Checks

1. Test the perplexity gain method on non-news domains (e.g., scientific papers or dialogue) to verify if the abstractive/extractive performance pattern holds across different text genres.

2. Conduct ablation studies removing different cross-attention layers or using alternative aggregation methods to determine if the method's limitations are fundamental or implementation-dependent.

3. Evaluate the computational tradeoff by measuring precision-recall performance against inference time across varying document lengths to establish practical deployment boundaries.