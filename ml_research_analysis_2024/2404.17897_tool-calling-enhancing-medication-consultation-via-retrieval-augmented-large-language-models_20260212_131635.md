---
ver: rpa2
title: 'Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented Large
  Language Models'
arxiv_id: '2404.17897'
source_url: https://arxiv.org/abs/2404.17897
tags:
- arxiv
- llms
- retrieval
- medication
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of applying retrieval-augmented
  large language models to medical consultation tasks, where existing models struggle
  with hallucinations and lack of domain-specific knowledge. The authors introduce
  a new benchmark, MedicineQA, derived from real-world medication consultation scenarios,
  and propose a novel Distill-Retrieve-Read framework that uses tool calling to generate
  effective search queries from complex dialogue history.
---

# Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented Large Language Models

## Quick Facts
- arXiv ID: 2404.17897
- Source URL: https://arxiv.org/abs/2404.17897
- Authors: Zhongzhen Huang; Kui Xue; Yongqi Fan; Linjie Mu; Ruoyu Liu; Tong Ruan; Shaoting Zhang; Xiaofan Zhang
- Reference count: 15
- Primary result: Novel Distill-Retrieve-Read framework with tool calling improves medication consultation accuracy by up to 30% over baselines

## Executive Summary
This paper addresses the challenge of applying retrieval-augmented large language models to medical consultation tasks, where existing models struggle with hallucinations and lack of domain-specific knowledge. The authors introduce a new benchmark, MedicineQA, derived from real-world medication consultation scenarios, and propose a novel Distill-Retrieve-Read framework that uses tool calling to generate effective search queries from complex dialogue history. Their approach, RagPULSE, significantly outperforms existing open-source models and commercial products in evidence retrieval accuracy (up to 30% improvement in document retrieval) and response quality for medication consultation tasks, even with smaller model sizes. The study demonstrates that integrating programming paradigms like tool calling into LLMs can substantially improve their performance in knowledge-intensive medical applications.

## Method Summary
The authors propose a Distill-Retrieve-Read framework that enhances LLMs for medication consultation by distilling dialogue history into search keywords via tool calling, retrieving relevant evidence from an entity-oriented medicine database, and generating responses based on the retrieved evidence. The framework uses a fine-tuned LLM (PULSE) on synthetic dialogue distillation data, then employs tool calling to interact with a search API that queries a medicine database containing 42,764 medicines with detailed attributes. The system is evaluated on the MedicineQA benchmark, which includes 300 multi-round question-answering pairs with dialogue history, using metrics such as Hit Rate for evidence retrieval and Elo rating for response quality.

## Key Results
- RagPULSE achieves up to 30% improvement in evidence retrieval accuracy compared to baselines
- Outperforms both open-source models and commercial products (ChatGPT3.5) in response quality
- Demonstrates effectiveness of tool calling for distilling complex dialogue history into search queries
- Shows that smaller models with the Distill-Retrieve-Read framework can outperform larger models without it

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distilling dialogue history into search keywords via tool calling improves retrieval accuracy by 30% over using raw history.
- Mechanism: The LLM acts as a query generator that extracts salient entities and relationships from the dialogue history, then calls a search API with these distilled keywords. This reduces noise and ambiguity in the query that would otherwise degrade retrieval performance.
- Core assumption: The LLM can reliably identify the most relevant entities and relationships in the dialogue history to form an effective search query.
- Evidence anchors:
  - [abstract] "distillation and retrieval process utilizes a tool calling mechanism to formulate search queries that emulate the keyword-based inquiries used by search engines"
  - [section] "RagPULSE processes a medication inquiry by summarizing the dialogue history to keywords for searching API calls"
  - [corpus] Weak - no direct evidence of 30% improvement in cited papers
- Break condition: If the dialogue history contains subtle contextual information that is lost in the distillation process, or if the LLM fails to identify the most relevant entities.

### Mechanism 2
- Claim: Integrating a domain-specific medicine database with entity-oriented storage improves retrieval accuracy compared to generic search.
- Mechanism: The medicine database stores each medicine with its generic name, brand name, and detailed attributes, each as separate entities. This allows for precise attribute-level retrieval (e.g., retrieving all medicines with a specific contraindication).
- Core assumption: The entity-oriented structure of the medicine database allows for more precise and granular retrieval than a flat document-based database.
- Evidence anchors:
  - [section] "we introduce an entity-oriented medicine database with 42764 medicines, where each medicine is represented in three forms: brand name, generic name, and detailed attributes like usage, contraindications, adverse reactions, etc."
  - [corpus] Weak - no direct evidence of improved accuracy in cited papers
- Break condition: If the entity structure becomes too complex to navigate efficiently, or if the database lacks sufficient coverage of rare medications.

### Mechanism 3
- Claim: The Distill-Retrieve-Read framework improves response quality by providing more relevant context to the LLM.
- Mechanism: By first distilling the dialogue history into a focused search query, retrieving the most relevant evidence from the medicine database, and then providing this evidence to the LLM, the framework ensures that the LLM has access to the most relevant information when generating a response.
- Core assumption: The LLM can effectively utilize the retrieved evidence to generate a more accurate and comprehensive response.
- Evidence anchors:
  - [abstract] "our framework brings notable performance improvements and surpasses the previous counterparts in the evidence retrieval process in terms of evidence retrieval accuracy"
  - [section] "the LLM generates the answer AT+1 according to [H, QT+1, Ë†E]"
  - [corpus] Weak - no direct evidence of improved response quality in cited papers
- Break condition: If the retrieved evidence is not sufficiently comprehensive or relevant to the query, or if the LLM struggles to effectively utilize the evidence.

## Foundational Learning

- Concept: Tool calling and function generation in LLMs
  - Why needed here: To enable the LLM to interact with external search APIs and databases, expanding its knowledge beyond its training data.
  - Quick check question: Can you explain how tool calling differs from traditional prompting in LLMs?

- Concept: Entity-oriented database design
  - Why needed here: To enable precise and granular retrieval of medical information based on specific attributes.
  - Quick check question: What are the advantages of storing medical information as separate entities (e.g., generic name, brand name, attributes) rather than as flat documents?

- Concept: Retrieval-augmented generation (RAG) and its limitations
  - Why needed here: To understand the baseline approach and identify areas for improvement, such as handling complex dialogue histories and domain-specific knowledge.
  - Quick check question: What are the main challenges of applying RAG to medical consultation tasks, and how does the Distill-Retrieve-Read framework address these challenges?

## Architecture Onboarding

- Component map:
  LLM (PULSE) -> Tool calling interface -> Search API -> Entity-oriented medicine database -> Retrieval engine -> LLM response generation

- Critical path: Dialogue history -> LLM distillation -> Tool calling -> Search API -> Entity-oriented medicine database -> Retrieval engine -> LLM response generation

- Design tradeoffs:
  - Complexity vs. accuracy: More complex distillation and retrieval processes may improve accuracy but increase latency and resource usage.
  - Generality vs. specificity: A more general LLM may struggle with domain-specific tasks, while a specialized LLM may lack versatility.

- Failure signatures:
  - High retrieval error rate: Indicates issues with query generation or database structure.
  - Low response quality: Suggests the LLM is not effectively utilizing the retrieved evidence.
  - Excessive latency: Points to inefficiencies in the distillation, retrieval, or response generation processes.

- First 3 experiments:
  1. Evaluate the impact of different query generation strategies (e.g., using dialogue history vs. last question) on retrieval accuracy.
  2. Compare the performance of the entity-oriented medicine database with a flat document-based database.
  3. Assess the impact of different levels of detail in the retrieved evidence on response quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Distill-Retrieve-Read framework's performance scale with increasing dialogue complexity in medication consultation tasks?
- Basis in paper: [inferred] The paper discusses the framework's effectiveness in handling complex dialogue history, but does not explore performance scaling with increasing dialogue complexity.
- Why unresolved: The paper does not provide experiments or analysis on how the framework's performance changes as dialogue complexity increases.
- What evidence would resolve it: Experiments comparing the framework's performance on dialogues of varying complexity levels, with a focus on hit rates and response quality.

### Open Question 2
- Question: Can the tool calling mechanism be effectively extended to other domains beyond medication consultation?
- Basis in paper: [explicit] The paper mentions the potential benefits of integrating programming paradigms like tool calling into LLMs but does not explore its application in other domains.
- Why unresolved: The study focuses specifically on medication consultation, leaving the generalizability of the tool calling mechanism to other knowledge-intensive tasks unexplored.
- What evidence would resolve it: Experiments applying the Distill-Retrieve-Read framework with tool calling to different knowledge-intensive domains (e.g., legal, financial, technical support) and comparing its performance to baseline methods.

### Open Question 3
- Question: How does the synthetic dataset used for fine-tuning impact the LLM's ability to generalize to unseen medication consultation scenarios?
- Basis in paper: [explicit] The paper describes the creation of a synthetic dataset for fine-tuning the LLM but does not investigate its impact on generalization to unseen scenarios.
- Why unresolved: The study does not provide analysis on the LLM's performance when faced with medication consultation scenarios not present in the synthetic dataset.
- What evidence would resolve it: Experiments evaluating the LLM's performance on a held-out test set of medication consultation scenarios not seen during training, comparing its performance to models without fine-tuning on the synthetic dataset.

## Limitations

- The study focuses specifically on medication consultation tasks and does not explore the generalizability of the approach to other domains
- The exact implementation details of the tool calling mechanism and its integration with the search engine are not fully specified
- The impact of the synthetic dataset on the LLM's ability to generalize to unseen medication consultation scenarios is not investigated

## Confidence

- Evidence retrieval accuracy claims: Medium - while the paper reports improvements, direct evidence from cited papers is limited
- Response quality improvements: Medium - Elo rating comparisons are provided, but the evaluation methodology could be more robust
- Framework generalizability: Low - the study focuses on a single domain without exploring broader applicability

## Next Checks

1. Verify the implementation of the tool calling mechanism and its integration with the search API
2. Evaluate the system's performance on a held-out test set of medication consultation scenarios not present in the training data
3. Conduct ablation studies to assess the individual contributions of the distillation, retrieval, and response generation components to overall performance