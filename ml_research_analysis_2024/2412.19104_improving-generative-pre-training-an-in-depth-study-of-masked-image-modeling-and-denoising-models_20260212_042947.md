---
ver: rpa2
title: 'Improving Generative Pre-Training: An In-depth Study of Masked Image Modeling
  and Denoising Models'
arxiv_id: '2412.19104'
source_url: https://arxiv.org/abs/2412.19104
tags:
- masked
- recognition
- tasks
- noise
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates why noise-based pre-training methods, when
  combined with masked image modeling, show limited gains on recognition tasks. Through
  empirical analysis, the authors identify three critical conditions: noise must be
  applied within the encoder, noise should be introduced in the feature space (particularly
  effective at lower encoder layers), and explicit disentanglement between noised
  and masked tokens is necessary.'
---

# Improving Generative Pre-Training: An In-depth Study of Masked Image Modeling and Denoising Models

## Quick Facts
- arXiv ID: 2412.19104
- Source URL: https://arxiv.org/abs/2412.19104
- Authors: Hyesong Choi; Daeun Kim; Sungmin Cha; Kwang Moo Yi; Dongbo Min
- Reference count: 40
- One-line primary result: Proposed method outperforms state-of-the-art MIM methods by up to 8.1% and recent generative baselines by 8.0% on various benchmarks

## Executive Summary
This paper investigates why noise-based pre-training methods show limited gains when combined with masked image modeling (MIM) for recognition tasks. Through systematic empirical analysis, the authors identify three critical conditions for effective integration: noise must be applied within the encoder, noise should be introduced in the feature space (particularly effective at lower encoder layers), and explicit disentanglement between noised and masked tokens is necessary. The proposed method demonstrates improved pre-training performance across diverse recognition tasks, including fine-grained visual categorization, outperforming state-of-the-art MIM methods by up to 8.1% and recent generative baselines by 8.0% on benchmarks including ImageNet, CUB-200-2011, NABirds, iNaturalist 2017/2018, Stanford Cars, Aircraft, ADE20K, and COCO.

## Method Summary
The proposed method combines masked image modeling with denoising by applying corruption and restoration within the encoder while training. Key innovations include feature-level noise injection (particularly effective at encoder block 2), a hybrid masking strategy combining both masked and noised tokens, and a disruption loss mechanism that explicitly disentangles denoising and masking tasks. The approach uses a ViT-B backbone pre-trained on ImageNet-1K for 400 epochs, followed by fine-tuning on various downstream datasets. The encoder-style architecture ensures learned representations are directly useful for downstream recognition tasks without requiring decoder capabilities.

## Key Results
- Achieves up to 8.1% improvement over state-of-the-art MIM methods on recognition benchmarks
- Demonstrates 8.0% gain over recent generative baselines across diverse tasks
- Shows consistent performance improvements on fine-grained visual categorization tasks (CUB-200-2011, NABirds, Stanford Cars, Aircraft)
- Maintains strong performance on standard benchmarks (ImageNet, ADE20K, COCO) while improving fine-grained task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding noise in the feature space (rather than pixel space) captures high-frequency details better for recognition tasks
- Mechanism: Lower encoder layers contain higher frequency information. By adding noise at these feature levels rather than pixel level, the model learns to preserve and reconstruct these high-frequency details that are crucial for fine-grained recognition tasks
- Core assumption: The spatial frequency content varies systematically across encoder layers, with lower layers containing higher frequencies
- Evidence anchors:
  - [abstract]: "noise must be introduced in the feature space, and is particularly effective when added at lower layers of the encoder, where high-frequency details are present"
  - [section]: "adding noise in feature-space (blocks 2, 4, and 6) is more effective than in pixel-space (block 0)" and "the highest performance observed at 'encoder block 2' suggests that noise addition is particularly effective when applied in the lower layers of the encoder, where high-frequency details are captured"
  - [corpus]: No direct evidence found in corpus neighbors; this appears to be a novel contribution

### Mechanism 2
- Claim: Applying corruption and restoration within the encoder ensures learned representations are directly useful for downstream tasks
- Mechanism: Since the encoder is what gets transferred to downstream tasks, learning to denoise and reconstruct within the encoder ensures the final features already contain the necessary information for recognition, rather than requiring the decoder's reconstruction capabilities
- Core assumption: The encoder's output features are what matter most for transfer learning performance
- Evidence anchors:
  - [abstract]: "corruption and restoration must be applied within the encoder while training, unlike existing design choices"
  - [section]: "we advocate for an encoder-style approach, adding corruption and focusing reconstruction within the encoder, hypothesizing that this will allow the benefits of noising which could translate to pre-training"
  - [corpus]: No direct evidence found in corpus neighbors; this appears to be a novel contribution

### Mechanism 3
- Claim: Explicit disentanglement between noised and masked tokens prevents interference between the two reconstruction tasks
- Mechanism: The disruption loss suppresses attention between noised and masked tokens, ensuring each type of token can be processed independently without one type's processing interfering with the other's
- Core assumption: Masked tokens and noised tokens have conflicting processing requirements that interfere with each other when processed together
- Evidence anchors:
  - [abstract]: "an explicit disentanglement between noised and masked tokens is necessary"
  - [section]: "we propose an explicit objective that disentangles the de-masking strategy from the de-noising strategy. We introduce disruption loss, a variant of masked token optimization proposed in MTO [7], designed to suppress attention between the two different token types"
  - [corpus]: No direct evidence found in corpus neighbors; this appears to be a novel contribution

## Foundational Learning

- Concept: Masked Image Modeling (MIM) fundamentals
  - Why needed here: The paper builds upon MIM as a foundation, so understanding how masking works and why it captures low-frequency semantics is crucial
  - Quick check question: Why do standard MIM approaches perform poorly on fine-grained tasks compared to general recognition tasks?

- Concept: Denoising diffusion models
  - Why needed here: The paper investigates combining denoising with MIM, so understanding how diffusion models work and why they excel at capturing high-frequency details is essential
  - Quick check question: What is the key difference between how MIM and denoising diffusion models approach the reconstruction task?

- Concept: Encoder-decoder architecture in vision transformers
  - Why needed here: The paper makes architectural claims about where to place noise addition and reconstruction, requiring understanding of how information flows through encoder vs decoder
  - Quick check question: Why does the paper argue that encoder-style approaches are preferable for transfer learning compared to decoder-style approaches?

## Architecture Onboarding

- Component map:
  Input image → Patch embedding → Encoder blocks (with noise injection at block 2) → Cross-attention between token types → Disruption loss regularization → Reconstruction head

- Critical path:
  1. Image tokenization and initial feature extraction
  2. Noise injection at feature level in encoder block 2
  3. Cross-attention between noised and masked tokens with disruption loss
  4. Reconstruction of original features
  5. Transfer to downstream tasks using encoder weights

- Design tradeoffs:
  - Noise level vs. signal preservation: Too much noise makes reconstruction impossible; too little provides no benefit
  - Disruption loss strength vs. information flow: Strong disruption prevents interference but may block useful interactions
  - Where to inject noise: Earlier layers capture more high-frequency info but are harder to reconstruct; later layers are easier but capture less detail

- Failure signatures:
  - Poor performance on fine-grained tasks indicates insufficient high-frequency capture
  - Degraded performance on standard tasks suggests too much noise or poor disentanglement
  - Training instability may indicate disruption loss is too strong or noise schedule is inappropriate

- First 3 experiments:
  1. Implement basic MIM baseline (MAE/SimMIM) and verify performance on ImageNet and a fine-grained dataset
  2. Add noise at different encoder levels (pixel space, block 0, block 2, block 4) and measure impact on both standard and fine-grained tasks
  3. Implement disruption loss with varying strengths and evaluate its effect on disentangling noised vs masked token processing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method's performance scale when applied to other generative pre-training tasks beyond recognition, such as image generation or video synthesis?
- Basis in paper: [explicit] The paper notes its analysis is "currently limited to recognition tasks" and suggests future work could broaden the scope to other applications.
- Why unresolved: The study focuses exclusively on recognition benchmarks, leaving the transferability of the findings to generative tasks unexplored.
- What evidence would resolve it: Experiments applying the method to generative tasks like image synthesis or video generation, comparing results to state-of-the-art diffusion models.

### Open Question 2
- Question: What is the optimal noise schedule for feature-space noise addition across different encoder architectures and dataset types?
- Basis in paper: [inferred] The paper shows feature-level noise addition is more effective than pixel-space, with optimal performance at encoder block 2, but doesn't systematically explore different noise schedules.
- Why unresolved: The experiments use a fixed noise schedule without exploring variations that might be optimal for different architectures or data distributions.
- What evidence would resolve it: Comprehensive ablation studies varying noise schedules across different encoder depths, architectures, and dataset characteristics.

### Open Question 3
- Question: What is the computational overhead of the proposed encoder-style approach compared to decoder-style methods during both pre-training and fine-tuning?
- Basis in paper: [explicit] The paper mentions all experiments use the same hardware and training schedule, but doesn't compare computational efficiency between encoder-style and decoder-style approaches.
- Why unresolved: While performance is compared, the efficiency trade-offs between different architectural choices are not quantified.
- What evidence would resolve it: Detailed runtime and memory usage comparisons across different stages of the pipeline, including pre-training time, fine-tuning time, and inference speed.

## Limitations
- The proposed mechanisms rely on architectural assumptions about frequency decomposition in transformers that have not been rigorously validated through frequency analysis
- The disruption loss mechanism's effectiveness depends on careful hyperparameter tuning, with insufficient sensitivity analysis provided
- The study focuses exclusively on recognition tasks, leaving applicability to generative tasks unexplored

## Confidence
- **High confidence**: The observation that standard MIM approaches underperform on fine-grained tasks is well-established and supported by extensive empirical evidence. The basic architectural framework (ViT-B with encoder-style pre-training) is clearly specified and reproducible.
- **Medium confidence**: The core hypothesis that feature-space noise injection at lower encoder layers improves high-frequency capture is supported by ablation studies, but the mechanistic explanation could benefit from more rigorous frequency analysis of learned representations.
- **Low confidence**: The disruption loss mechanism's exact formulation and implementation details are underspecified, making faithful reproduction challenging. The claim about explicit disentanglement being "necessary" is based on relative performance improvements rather than ablation studies showing catastrophic failure when removed.

## Next Checks
1. **Frequency Analysis Validation**: Conduct Fourier analysis of features at different encoder layers with and without feature-space noise injection to empirically verify the claim about high-frequency preservation at lower layers.

2. **Disentanglement Ablation**: Systematically vary the disruption loss strength and perform ablation studies removing it entirely to quantify the exact contribution and identify potential break conditions where disentanglement becomes detrimental.

3. **Architecture Transferability**: Test the proposed approach across different backbone architectures (e.g., ConvNeXt, Swin Transformer) and with varying model sizes to assess whether the identified critical conditions are architecture-dependent or more general principles.