---
ver: rpa2
title: Reward Centering
arxiv_id: '2405.09999'
source_url: https://arxiv.org/abs/2405.09999
tags:
- centering
- reward
- learning
- rewards
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Reward centering improves discounted RL in continuing problems\
  \ by subtracting the empirical average reward from observed rewards. This removes\
  \ a large state-independent constant term (scaling as 1/(1-\u03B3)) from value estimates,\
  \ enabling the function approximator to focus on relative differences between states."
---

# Reward Centering

## Quick Facts
- arXiv ID: 2405.09999
- Source URL: https://arxiv.org/abs/2405.09999
- Reference count: 40
- Primary result: Reward centering removes large state-independent constants from value estimates, enabling stable performance as discount factors approach 1

## Executive Summary
Reward centering is a technique for improving discounted reinforcement learning in continuing problems by subtracting the empirical average reward from observed rewards. This simple modification removes a large state-independent constant term (scaling as 1/(1-γ)) from value estimates, allowing function approximators to focus on relative differences between states rather than absolute value magnitudes. The method works with both simple averaging and value-based centering approaches, and is effective across tabular, linear, and non-linear function approximation settings. Reward centering maintains stable performance as discount factors approach 1 and provides robustness to constant shifts in rewards, addressing fundamental challenges in continuing reinforcement learning problems.

## Method Summary
Reward centering improves discounted RL in continuing problems by subtracting the empirical average reward from observed rewards during learning. The technique works by removing a large state-independent constant term from value estimates that scales as 1/(1-γ), which otherwise dominates the function approximator's capacity. Two main variants are explored: simple centering that directly subtracts the running average reward, and value-based centering that uses the difference between estimated values and their expectation (computed via TD errors). The centering parameter η controls how quickly the average reward estimate updates, with theoretical analysis showing that smaller η values reduce bias but increase variance. The method is implemented as a wrapper around standard Q-learning algorithms, requiring minimal changes to existing code while providing significant performance improvements, particularly for large discount factors and when rewards are shifted by constants.

## Key Results
- Reward centering maintains stable performance as γ approaches 1, while uncentered versions degrade
- Centered versions are robust to constant shifts in rewards, maintaining performance across different reward baselines
- Performance improvements are most pronounced for large discount factors (γ ≥ 0.9) across multiple domains and function approximation methods

## Why This Works (Mechanism)
Reward centering works by exploiting the Laurent-series decomposition of value functions in continuing problems. When rewards are shifted by a constant, the value function changes by a state-independent constant term that scales as 1/(1-γ). By centering rewards around their empirical average, this large constant is removed, leaving only the state-dependent components that capture meaningful differences between states. This allows function approximators to focus their capacity on learning these relative differences rather than tracking the dominant constant term. The technique is particularly effective because the constant term grows rapidly as γ approaches 1, making it the primary obstacle to learning in high-discount settings. By removing this term, reward centering enables algorithms to maintain stable performance even with very large discount factors.

## Foundational Learning
- Continuing reinforcement learning problems: Understanding the distinction between episodic and continuing problems is essential, as reward centering relies on the infinite-horizon nature of continuing problems where the Laurent-series decomposition applies
- Laurent-series value decomposition: The mathematical foundation showing how value functions can be decomposed into state-independent constants and state-dependent terms, with the constant scaling as 1/(1-γ)
- Function approximation bias-variance tradeoff: How the choice of centering parameter η affects the bias-variance tradeoff in online average reward estimation, with smaller η reducing bias but increasing variance

## Architecture Onboarding
- Component map: RL agent -> Reward centering wrapper -> Q-learning algorithm -> Environment interaction
- Critical path: Experience collection → Reward centering → Value update → Policy improvement
- Design tradeoffs: Simple centering (lower variance, higher bias) vs value-based centering (higher variance, lower bias) based on on/off-policy requirements
- Failure signatures: Degraded performance with poorly tuned η, instability when average reward estimates diverge from true values
- First experiments: 1) Verify centering removes constant shifts in simple tabular MDPs, 2) Test centering parameter sensitivity on Access-Control Queuing, 3) Compare centered vs uncentered performance across discount factors in PuckWorld

## Open Questions the Paper Calls Out
- How does reward centering affect variance in value estimates across different RL algorithms? The paper mentions increased variance due to changing average-reward estimates but lacks quantitative analysis across algorithms.
- Can reward centering be effectively combined with reward scaling techniques? The discussion suggests potential compatibility but provides no experimental results or theoretical analysis.
- How can reward centering be adapted for episodic problems where the Laurent-series decomposition doesn't apply? The paper explicitly states this is an open problem without proposing solutions.

## Limitations
- Effectiveness depends critically on proper tuning of the centering parameter η
- Theoretical analysis assumes access to true average rewards, but practical implementations require online estimation
- Experiments focus primarily on continuing problems with fixed dynamics, leaving non-stationary and partially observable settings unexplored

## Confidence
- High confidence: The core claim that reward centering removes state-independent constants from value estimates is well-supported by theoretical analysis and empirical validation
- Medium confidence: Claims about robustness to constant reward shifts are supported but could benefit from testing with non-uniform shifts
- Medium confidence: Performance improvements for large discount factors are demonstrated, though the magnitude varies significantly across domains

## Next Checks
1. Test reward centering with non-stationary reward functions where the average reward changes over time to assess adaptation capabilities
2. Evaluate the impact of different averaging window sizes and smoothing techniques on centering performance across diverse MDPs
3. Compare reward centering against alternative variance reduction techniques (e.g., advantage functions, normalization) in high-dimensional continuous control tasks