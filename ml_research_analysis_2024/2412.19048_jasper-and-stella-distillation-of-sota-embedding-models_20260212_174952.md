---
ver: rpa2
title: 'Jasper and Stella: distillation of SOTA embedding models'
arxiv_id: '2412.19048'
source_url: https://arxiv.org/abs/2412.19048
tags:
- given
- text
- student
- embedding
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of deploying high-performance
  text embedding models in real-world scenarios, where current SOTA models have many
  parameters and high vector dimensionality, leading to slow inference and high storage
  costs. The authors propose a novel multi-stage distillation framework that enables
  a smaller student embedding model to distill knowledge from multiple larger teacher
  models through three carefully designed losses: cosine loss, similarity loss, and
  relative similarity distillation loss.'
---

# Jasper and Stella: distillation of SOTA embedding models

## Quick Facts
- arXiv ID: 2412.19048
- Source URL: https://arxiv.org/abs/2412.19048
- Reference count: 8
- Primary result: Jasper achieves 71.54 average score on MTEB, ranking #3, comparable to 7B models while built on 1.5B base

## Executive Summary
This paper addresses the challenge of deploying high-performance text embedding models in real-world scenarios by proposing a novel multi-stage distillation framework. Current SOTA embedding models have many parameters and high vector dimensionality, leading to slow inference and high storage costs. The authors introduce a 4-stage distillation process that progressively transfers knowledge from large teacher models to a smaller student model while employing Matryoshka Representation Learning to reduce dimensionality without sacrificing performance.

The resulting Jasper model, built upon the Stella embedding model, achieves state-of-the-art results with only 2B parameters, ranking third on the MTEB leaderboard with an average score of 71.54 across 56 datasets. This performance is comparable to other top-ranked 7B embedding models and significantly outperforms models with fewer than 2B parameters. The framework uses three carefully designed losses (cosine loss, similarity loss, and relative similarity distillation loss) and demonstrates strong performance across seven MTEB subcategories.

## Method Summary
The authors propose a multi-stage distillation framework consisting of four sequential stages. Stage 1 trains only the fully connected layer (FC1) to map student dimensions to teacher dimensions using three losses: cosine loss, similarity loss, and relative similarity distillation loss. Stage 2 extends training to the last three encoder layers while maintaining FC1 training. Stage 3 introduces Matryoshka Representation Learning with additional FC layers to reduce dimensionality while keeping the 12288 mapping consistent. Stage 4 focuses on multimodal alignment by training the visual encoder with image-caption pairs while keeping other components frozen. The framework uses NV-Embed-v2 (7B, 4096-dim) and Stella_en_1.5B_v5 (1.5B, 8192-dim) as teachers, with the student initialized from Stella_en_1.5B_v5 plus SigLIP vision encoder.

## Key Results
- Jasper achieves 71.54 average score on MTEB leaderboard (56 datasets), ranking #3
- Performance is comparable to other top-ranked 7B embedding models
- Significantly outperforms models with fewer than 2B parameters
- Strong performance across all seven MTEB subcategories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage distillation progressively transfers knowledge from large teachers to small student, preventing catastrophic forgetting
- Mechanism: Stage 1 trains only FC1 mapping to teacher dimensions (4096+8192=12288), Stage 2 trains FC1 plus last 3 encoder layers, Stage 3 adds MRL dimension reduction while keeping 12288 mapping for consistency, Stage 4 freezes everything except vision encoder for multimodal alignment
- Core assumption: Gradual parameter unfreezing allows stable knowledge transfer without destabilizing early learned mappings
- Evidence anchors: Weak - inferred from staged training approach, no direct evidence about catastrophic forgetting prevention

### Mechanism 2
- Claim: Three complementary losses capture different aspects of teacher-student alignment
- Mechanism: Cosine loss aligns absolute vector directions, similarity loss aligns pairwise semantic judgments without enforcing exact match, relative similarity loss captures ranking preferences between positive/negative pairs
- Core assumption: Combining absolute, pairwise, and relative signals provides richer supervision than any single loss
- Evidence anchors: Weak - no corpus evidence for effectiveness of this specific three-loss combination

### Mechanism 3
- Claim: MRL-based dimension reduction maintains representation quality while reducing storage/compute
- Mechanism: Additional FC layers (FC2, FC3, FC4) create lower-dimensional vectors (e.g., 512D) from 12288D teacher-aligned space, using only Lsim and Lresim since Lcosine requires matching dimensions
- Core assumption: Lower-dimensional representations can preserve semantic information when trained with proper supervision
- Evidence anchors: Moderate - MRL technique established in literature but specific application to distillation less documented

## Foundational Learning

- Concept: Knowledge distillation in embedding models
  - Why needed here: Core technique for transferring SOTA teacher knowledge to smaller student model
  - Quick check question: What distinguishes embedding model distillation from language model distillation?

- Concept: Vector similarity metrics and their optimization
  - Why needed here: Three losses rely on cosine similarity, MSE between similarity matrices, and relative ranking margins
  - Quick check question: How does cosine loss differ from mean squared error in this context?

- Concept: Multi-modal embedding alignment
  - Why needed here: Stage 4 aligns vision and text embeddings through self-distillation
  - Quick check question: Why freeze text components when training vision encoder?

## Architecture Onboarding

- Component map: Text → Encoder → FC1 → Teacher alignment losses → FC2-4 → MRL losses → Gradient updates
- Critical path: Text → Encoder → FC1 → Teacher alignment losses → FC2-4 → MRL losses → Gradient updates
- Design tradeoffs: 12288D intermediate space enables better teacher alignment but increases computation; multiple FC layers add parameters but enable flexible dimensionality
- Failure signatures: Training instability when moving between stages; loss oscillation in multimodal training; dimension reduction degrades performance
- First 3 experiments:
  1. Train only Stage 1 with cosine loss, verify FC1 learns basic alignment
  2. Add similarity loss to Stage 1, observe improved stability
  3. Progress to Stage 2, confirm encoder layers adapt without losing FC1 alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the proposed self-distillation approach (using FC1 vectors as teachers for lower-dimensional vectors) compared to traditional MRL-based dimension reduction?
- Basis in paper: The paper mentions this as a potentially promising approach but explicitly states they did not conduct experiments to evaluate its merits due to focusing on Stella and Jasper model training methods.
- Why unresolved: The authors acknowledge this could be a valuable method but lack empirical validation due to scope limitations of the current work.
- What evidence would resolve it: Controlled experiments comparing performance of models trained with traditional MRL versus self-distillation approaches on the same tasks and datasets.

### Open Question 2
- Question: What is the optimal balance between the three loss functions (cosine, similarity, and relative similarity distillation) across different embedding model sizes and task types?
- Basis in paper: The paper uses fixed hyperparameters (λ1=10, λ2=200, λ3=20) without exploring sensitivity or task-specific tuning, and acknowledges that Lcosine convergence issues may require different weighting strategies.
- Why unresolved: The paper uses static hyperparameters without ablation studies or sensitivity analysis to determine optimal values for different scenarios.
- What evidence would resolve it: Comprehensive ablation studies varying the loss function weights across different model sizes, datasets, and task types to identify optimal configurations.

### Open Question 3
- Question: How can the multimodal training instability (oscillatory behavior in loss function) be mitigated to improve vision-language alignment quality?
- Basis in paper: The authors explicitly mention observing oscillatory behavior during stage 4 multimodal training and acknowledge "considerable room for enhancement."
- Why unresolved: The paper only provides preliminary multimodal capability with basic image encoding and identifies the issue but does not propose solutions.
- What evidence would resolve it: Experiments testing various training strategies (learning rate schedules, loss function modifications, contrastive learning techniques) to stabilize multimodal training and improve alignment metrics.

## Limitations

- Critical architectural details (FC layer dimensions, initialization schemes) remain underspecified
- Relative similarity distillation loss implementation lacks details about hard-negative mining strategies
- No ablation studies isolating contribution of each stage or loss component
- Multimodal stage relies on self-distillation without external vision-text alignment supervision

## Confidence

**High Confidence**: Multi-stage training methodology is clearly described with logical progression from alignment to dimensionality reduction to multimodal adaptation.

**Medium Confidence**: Three-loss combination represents reasonable extension of existing techniques, though specific hyperparameter choices appear arbitrary without supporting ablation studies.

**Low Confidence**: Claims about "comparable" performance to 7B models are difficult to verify without access to exact model weights and full evaluation protocol; paper lacks runtime benchmarks and detailed storage requirements.

## Next Checks

1. **Stage-by-stage ablation study**: Implement each training stage independently and measure performance degradation to quantify contribution of each stage.

2. **Loss component isolation**: Train models using only individual losses (cosine-only, similarity-only, relative-only) and combinations to empirically validate synergistic benefits.

3. **Reproducibility benchmark**: Implement complete pipeline on smaller scale (2-4 GPUs) with reduced dataset subsets to verify relative performance improvements before full production training.