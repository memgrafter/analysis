---
ver: rpa2
title: Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models
arxiv_id: '2412.05334'
source_url: https://arxiv.org/abs/2412.05334
tags:
- policy
- fine-tuning
- cat-k
- traffic
- rollout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles covariate shift in traffic simulation policies
  by introducing Closest Among Top-K (CAT-K) rollouts, a closed-loop fine-tuning strategy
  that balances following the policy's learned distribution while staying close to
  ground-truth trajectories. CAT-K selects, at each timestep, the top-K most likely
  actions according to the policy and chooses the one that minimizes distance to the
  GT trajectory, enabling effective supervised learning during rollout.
---

# Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models

## Quick Facts
- arXiv ID: 2412.05334
- Source URL: https://arxiv.org/abs/2412.05334
- Reference count: 40
- Key outcome: CAT-K fine-tuning enables a 7M-parameter model to outperform a 102M-parameter model on the Waymo Sim Agent Challenge

## Executive Summary
This paper addresses covariate shift in traffic simulation policies by introducing Closest Among Top-K (CAT-K) rollouts, a closed-loop fine-tuning strategy that balances following the policy's learned distribution while staying close to ground-truth trajectories. CAT-K selects, at each timestep, the top-K most likely actions according to the policy and chooses the one that minimizes distance to the GT trajectory, enabling effective supervised learning during rollout. Applied to a 7M-parameter SMART-tiny model, CAT-K fine-tuning achieves state-of-the-art performance on the Waymo Sim Agent Challenge leaderboard while also improving ego-motion planning safety metrics.

## Method Summary
The CAT-K method fine-tunes traffic simulation policies by unrolling the multimodal policy during training in a way that keeps policy-visited states close to ground-truth demonstrations. During training, at each timestep, the policy generates top-K token predictions, and the algorithm selects the token closest to the GT next state based on a distance metric (average Euclidean distance between bounding box corners). This creates a deterministic rollout that stays near GT while incorporating the policy's learned distribution. The method is applied to SMART-tiny (7M parameters) after initial behavior cloning pre-training, using existing trajectory data without requiring RL or GAIL.

## Key Results
- CAT-K fine-tuning achieves RMM of 0.7702, placing 1st on the Waymo Sim Agent Challenge leaderboard
- A 7M-parameter SMART-tiny model with CAT-K outperforms a 102M-parameter SMART-large model from the same family
- For ego-motion planning with GMM policy, CAT-K reduces collision rate by 25.7% and off-road driving by 33.9%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CAT-K fine-tuning reduces covariate shift by ensuring the policy visits states that remain close to ground truth trajectories during closed-loop training.
- Mechanism: During training, CAT-K rollout selects, at each timestep, the top-K most likely actions according to the policy and chooses the one that minimizes distance to the GT trajectory. This keeps rollouts close enough to GT for supervised learning to remain effective.
- Core assumption: The GT trajectory remains a valid recovery target when the policy-selected rollout stays close to it.
- Evidence anchors: [abstract]: "CAT-K fine-tuning enables a small 7M-parameter tokenized traffic simulation policy to outperform a 102M-parameter model from the same model family"; [section]: "The key idea, illustrated in Fig. 1, is to unroll the multimodal policy during training in a way that the policy-visited states remain close to the ground-truth (GT) demonstration"
- Break condition: If the policy's top-K predictions don't include the GT mode, CAT-K cannot find a valid recovery action.

### Mechanism 2
- Claim: CAT-K outperforms data augmentation approaches because it samples states from the learned policy's distribution rather than ignoring the policy entirely.
- Mechanism: Unlike Trajeglish's noisy tokenization or SMART's trajectory perturbation, CAT-K incorporates the learned policy into sampling, generating a state distribution more like what the policy would encounter at inference time.
- Core assumption: Sampling states that the policy would actually encounter is more effective for reducing covariate shift than sampling based solely on distance to GT.
- Evidence anchors: [abstract]: "Notably, fine-tuning the SMART-7M [35] next-token-prediction traffic model enables it to outperform the 14x larger State-of-The-Art (SoTA) SMART-102M"; [section]: "By incorporating the learned policy into the sampling strategy, CAT-K generates a state distribution more like the policy's"
- Break condition: If the policy is poorly trained initially, sampling from it could generate low-quality states that don't help fine-tuning.

### Mechanism 3
- Claim: CAT-K provides a robust trade-off between following the policy (K=1) and following the GT (K=|V|), avoiding the need for difficult distance-based hyperparameters.
- Mechanism: The hyperparameter K trades off between following the policy's distribution (small K) and staying close to GT (large K), with robust performance across a wide range of K values.
- Core assumption: The optimal balance between policy-following and GT-following doesn't vary dramatically across different traffic scenarios.
- Evidence anchors: [abstract]: "The hyperparameter K provides strong results across a large range of values and is hence much easier to tune"; [section]: "The hyperparameter K trades off following the policy (for K = 1 ) vs. following the GT (for K = |V |)"
- Break condition: If certain scenarios require very different K values than others, the single K hyperparameter could be suboptimal.

## Foundational Learning

- Concept: Covariate shift in imitation learning
  - Why needed here: The paper's core problem is that policies trained in open-loop suffer from distribution mismatch when deployed in closed-loop
  - Quick check question: What happens to a policy's performance when it encounters states during deployment that were not seen during training?

- Concept: Multimodal trajectory distributions
  - Why needed here: Traffic behavior is highly multimodal, and the policy must represent multiple possible actions at each state
  - Quick check question: Why can't a single Gaussian distribution adequately represent traffic agent behavior?

- Concept: Next-token-prediction (NTP) policies
  - Why needed here: The method is applied to SMART, which uses NTP to represent multimodal distributions over discrete action tokens
  - Quick check question: How does discretizing continuous actions into tokens help represent multimodal distributions?

## Architecture Onboarding

- Component map:
  - Policy πθ (SMART-tiny NTP model, 7M parameters) -> Action token vocabulary V (2048 trajectory tokens) -> Distance metric d (average Euclidean distance between bounding box corners) -> Dynamics model f (deterministic forward dynamics for traffic simulation) -> Dataset D (real-world trajectories with corresponding contexts)

- Critical path:
  1. Pre-train with behavior cloning until convergence
  2. Sample traffic scenario from dataset
  3. For each timestep and agent:
     - Get top-K tokens from policy
     - Select token closest to GT next state
     - Rollout to next state using dynamics
     - Compute recovery target token
  4. Update policy with cross-entropy loss
  5. Repeat until convergence

- Design tradeoffs:
  - K vs performance: Higher K stays closer to GT but may ignore policy learning; lower K follows policy but may deviate too far
  - Top-K vs random sampling: Top-K ensures policy consideration but is more computationally expensive
  - Deterministic vs stochastic rollout: Deterministic ensures reproducibility but may reduce diversity

- Failure signatures:
  - Poor performance on RMM metrics: Policy may be deviating too far from GT during fine-tuning
  - High collision/off-road rates: Dynamics model f may be inaccurate or policy may be overfitting to training scenarios
  - Training instability: K may be too small, causing rollouts to deviate too far from GT

- First 3 experiments:
  1. Verify CAT-K with K=1 (deterministic rollout) improves over BC baseline
  2. Test CAT-K with K=|V| (equivalent to BC) to confirm proper implementation
  3. Compare CAT-K with top-K sampling + distance filtering to validate the advantage of deterministic selection

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the limitations and implications of the work, several important questions arise:

1. How does CAT-K performance vary when applied to traffic scenarios with significantly different dynamics, such as high-speed highway driving versus dense urban environments with pedestrians and cyclists?

2. What is the optimal value of K for CAT-K when dealing with different numbers of traffic agents or varying levels of scenario complexity?

3. How does CAT-K compare to closed-loop fine-tuning methods that incorporate expert feedback or reinforcement learning, particularly in terms of scalability and realism trade-offs?

## Limitations

- The method's effectiveness depends on the initial quality of the pre-trained policy, with no clear guidance on minimum requirements for successful fine-tuning
- CAT-K may fail in highly complex multimodal distributions where the top-K predictions consistently miss the GT mode
- The approach hasn't been validated across different traffic datasets or driving environments beyond the Waymo domain

## Confidence

**High confidence**: The mechanism of using top-K selection to balance policy-following and GT-following is well-supported by empirical results showing CAT-K's superiority over both pure BC (K=|V|) and pure policy rollout (K=1).

**Medium confidence**: The claim that CAT-K outperforms data augmentation approaches like Trajeglish and SMART trajectory perturbation is reasonable given the fundamental difference in sampling strategy, but direct comparisons are limited.

**Low confidence**: The assertion that CAT-K provides an easier-to-tune alternative to distance-based hyperparameters is plausible but under-validated.

## Next Checks

1. Test CAT-K fine-tuning starting from a deliberately poorly-trained BC model to determine the minimum policy quality required for successful fine-tuning.

2. Systematically evaluate CAT-K on scenarios where the GT trajectory represents rare or complex behaviors that the policy is unlikely to predict.

3. Apply CAT-K to a different traffic dataset (e.g., nuScenes or Argoverse) to test whether the method's advantages transfer beyond the Waymo domain.