---
ver: rpa2
title: 'Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning
  Process'
arxiv_id: '2407.20311'
source_url: https://arxiv.org/abs/2407.20311
tags:
- each
- backpack
- uni00000048
- number
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how language models solve grade-school
  math problems by using synthetic datasets to control for data contamination and
  solution diversity. The authors design probing techniques to study the model's hidden
  reasoning process, revealing that models can learn reasoning skills beyond memorization
  and even develop skills not explicitly taught in the training data, such as all-pair
  dependency computation.
---

# Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process

## Quick Facts
- arXiv ID: 2407.20311
- Source URL: https://arxiv.org/abs/2407.20311
- Reference count: 24
- Key outcome: Language models can learn grade-school math reasoning through genuine generalization rather than memorization, developing skills like all-pair dependency computation beyond what's explicitly taught.

## Executive Summary
This paper investigates how language models solve grade-school math problems using synthetic datasets to control for data contamination and solution diversity. The authors develop probing techniques to study the model's hidden reasoning process, revealing that models can learn reasoning skills beyond memorization and even develop skills not explicitly taught in the training data. The study finds that model depth is crucial for mathematical reasoning, with deeper models better handling longer reasoning chains, and that probing results correlate with the model's mistakes, indicating systematic issues in mental planning rather than random generation errors.

## Method Summary
The authors create synthetic GSM-like datasets (iGSM) with controlled complexity and generate diverse solutions to prevent memorization. They train GPT2 models from scratch with rotary positional embeddings on these datasets, then use V-probing with rank-8 updates to investigate internal representations. The probing tasks measure mental reasoning processes like nece(A) (necessary parameters) and compute(A) (parameter computations). Models are evaluated on their ability to solve math problems correctly and their performance on probing tasks, with out-of-distribution testing to verify genuine reasoning ability.

## Key Results
- Deeper models significantly outperform shallower ones on mathematical reasoning tasks, with layer-by-layer reasoning appearing in the solution process
- Models develop all-pair dependency computation skills that aren't explicitly required for solving the training problems
- Probing accuracy correlates with model mistakes, suggesting errors stem from systematic issues in mental planning rather than random generation errors
- Solution diversity in synthetic data exceeds model capacity, preventing template memorization and ensuring genuine reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models learn grade-school math through genuine generalization rather than memorization
- Mechanism: Synthetic dataset generation ensures solution diversity exceeds model capacity, preventing template memorization
- Core assumption: Generated problems have trillions of unique solution templates beyond the model's capacity
- Evidence anchors:
  - [abstract]: "We need full control over the model's pretrain data... We need a much larger, more diverse set of grade-school math problems."
  - [section]: "iGSM-medop=15 still has at least 7 billion solution templates, and iGSM-hardop=21 has at least 90 trillion solution templates."
  - [corpus]: Related work on math reasoning

### Mechanism 2
- Claim: Models develop reasoning skills beyond those explicitly taught in training data
- Mechanism: The model learns all-pair dependency computation as a byproduct of learning to solve math problems
- Core assumption: Internal representations capture relationships between all parameters, not just necessary ones
- Evidence anchors:
  - [abstract]: "The model can learn reasoning skills beyond memorization and even develop skills not explicitly taught in the training data"
  - [section]: "Surprisingly, the model also learns unnecessary, yet important skills after pretraining, such as all-pair dependency"
  - [corpus]: Weak evidence for all-pair dependency learning

### Mechanism 3
- Claim: Model depth is crucial for mathematical reasoning due to mental reasoning process complexity
- Mechanism: Deeper models maintain more complex mental representations needed for longer reasoning chains
- Core assumption: Mathematical reasoning requires recursive mental processes that scale with reasoning length
- Evidence anchors:
  - [abstract]: "The depth of the language model is crucial for mathematical reasoning"
  - [section]: "A t-step mental reasoning may require deeper models for larger t"
  - [corpus]: Related work on depth importance

## Foundational Learning

- Concept: Probing techniques for understanding model internal representations
  - Why needed here: To investigate how models solve math problems and identify systematic error patterns
  - Quick check question: What are the six probing tasks used in this study and what do they measure?

- Concept: Synthetic data generation for controlled experiments
  - Why needed here: To prevent data contamination and ensure solution diversity exceeds model capacity
  - Quick check question: How does the structure graph ensure instance dependency relationships?

- Concept: Chain-of-Thought (CoT) solution format
  - Why needed here: To enable systematic analysis of solution steps and parameter dependencies
  - Quick check question: Why does the solution format break computations into binary operations?

## Architecture Onboarding

- Component map: Data generation pipeline -> Language model architecture -> Probing framework -> Evaluation pipeline
- Critical path: Synthetic data generation → Model pretraining → Probing task fine-tuning → Evaluation
- Design tradeoffs: Depth vs. width tradeoff in model architecture, context length vs. computational efficiency, solution complexity vs. training efficiency
- Failure signatures: Low accuracy on out-of-distribution problems indicates memorization, high unnecessary parameter usage indicates lack of reasoning skill, probing accuracy near random indicates poor internal representation
- First 3 experiments:
  1. Verify data generation produces diverse solutions by checking solution template uniqueness
  2. Test model accuracy on in-distribution vs. out-of-distribution problems to confirm generalization
  3. Apply V-probing to check if model learns nece(A) task before solution generation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms allow language models to learn "level-2" reasoning skills like all-pair dependency computation?
- Basis in paper: The authors observe models learn all-pair dependencies even when not necessary for training problems
- Why unresolved: The paper demonstrates the phenomenon but doesn't fully explain underlying mechanisms
- What evidence would resolve it: Detailed analysis of attention patterns and hidden state activations during learning

### Open Question 2
- Question: How does model depth specifically enable more complex mental reasoning processes?
- Basis in paper: Authors find deeper models perform better and layer-by-layer reasoning appears
- Why unresolved: While correlation is established, precise mechanism or mathematical relationship is missing
- What evidence would resolve it: Experiments systematically varying depth and analyzing layer-wise activations

### Open Question 3
- Question: Can probing techniques be generalized to understand reasoning in larger foundation models like GPT-4?
- Basis in paper: Authors note GPT-4/4o make similar mistakes but cannot probe their internal states
- Why unresolved: Probing methods designed for smaller, accessible models not larger closed models
- What evidence would resolve it: Development of approximation techniques to probe larger models' internal states

## Limitations

- The synthetic data generation process relies on assumptions about solution diversity that are difficult to fully verify
- Probing methodology has inherent limitations in definitively mapping internal representations to specific reasoning processes
- Findings may not generalize to other model families or larger foundation models beyond the tested 12-layer architecture

## Confidence

- High Confidence: Model depth is crucial for mathematical reasoning, supported by controlled experiments comparing different depth configurations
- Medium Confidence: Models solve grade-school math through genuine generalization, well-supported by synthetic dataset design and out-of-distribution testing
- Low Confidence: Models develop reasoning skills beyond training requirements, particularly all-pair dependency computation, has weakest support with suggestive but not definitive evidence

## Next Checks

1. **Intervention Experiment**: Conduct ablation studies where all-pair dependency computation is selectively impaired during training, then measure effects on mathematical reasoning performance

2. **Architecture Generalization Test**: Replicate core experiments using different model architectures (e.g., Transformer with learned positional embeddings) to test architecture dependency

3. **Scaling Study**: Extend experiments to larger model sizes (24-layer or 36-layer variants) and different width configurations to map depth-width tradeoff space