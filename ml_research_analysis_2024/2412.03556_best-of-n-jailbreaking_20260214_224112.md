---
ver: rpa2
title: Best-of-N Jailbreaking
arxiv_id: '2412.03556'
source_url: https://arxiv.org/abs/2412.03556
tags:
- text
- audio
- gemini
- augmentations
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Best-of-N (BoN) Jailbreaking is a black-box, multi-modal algorithm\
  \ that repeatedly samples augmented versions of harmful prompts\u2014using text,\
  \ image, or audio perturbations\u2014until a harmful response is elicited. Across\
  \ text, vision, and audio inputs, BoN achieves high attack success rates (ASRs)\
  \ on frontier models, including 89% on GPT-4o and 78% on Claude 3.5 Sonnet with\
  \ 10,000 samples, and 56% and 72% on GPT-4o vision and Gemini 1.5 Pro audio with\
  \ 7,200 samples."
---

# Best-of-N Jailbreaking
## Quick Facts
- arXiv ID: 2412.03556
- Source URL: https://arxiv.org/abs/2412.03556
- Reference count: 40
- Best-of-N jailbreaking achieves up to 89% attack success on GPT-4o with 10,000 samples.

## Executive Summary
Best-of-N (BoN) jailbreaking is a black-box, multi-modal algorithm that bypasses safety mechanisms in frontier AI models by repeatedly sampling augmented versions of harmful prompts across text, image, and audio inputs. The method exploits the stochasticity and high-dimensional input sensitivity of large language models, showing predictable, power-law-like scaling of attack success rates as the number of samples increases. BoN achieves high success rates on GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, and can circumvent even robust defenses such as circuit breakers and Cygnet. The approach is effective even when composed with other attacks, significantly improving sample efficiency.

## Method Summary
Best-of-N jailbreaking is a black-box, multi-modal attack that repeatedly samples augmented versions of harmful prompts until a harmful response is elicited. It applies text, image, or audio perturbations to original prompts, leveraging the stochasticity of model outputs and their sensitivity to high-dimensional input variations. By systematically increasing the number of samples, BoN achieves high attack success rates across modalities, with predictable, power-law-like scaling as sample count grows.

## Key Results
- BoN achieves 89% attack success on GPT-4o and 78% on Claude 3.5 Sonnet with 10,000 samples.
- On multimodal inputs, BoN reaches 56% ASR on GPT-4o vision and 72% on Gemini 1.5 Pro audio with 7,200 samples.
- BoN bypasses strong defenses like circuit breakers and GraySwanâ€™s Cygnet.

## Why This Works (Mechanism)
Best-of-N jailbreaking exploits the inherent stochasticity and sensitivity of frontier AI models to high-dimensional input variations. By repeatedly sampling augmented versions of harmful prompts, the attack increases the likelihood of eliciting a harmful response, even when the original prompt is rejected. The predictable, power-law-like scaling of attack success rates as sample count increases highlights the robustness of this vulnerability across modalities.

## Foundational Learning
- **Stochasticity in AI models**: Models' outputs vary with repeated sampling; needed to understand attack effectiveness; quick check: run same prompt multiple times, observe output variance.
- **High-dimensional input sensitivity**: Small input changes can alter model behavior; needed to design effective perturbations; quick check: modify a single pixel or word, measure output shift.
- **Power-law scaling**: Attack success grows predictably with sample count; needed to estimate attack efficiency; quick check: plot ASR vs. log(samples), look for linear trend.
- **Black-box attack feasibility**: No need for model internals; needed to assess real-world risk; quick check: test attack via API without model access.
- **Multi-modal attack applicability**: Works across text, image, audio; needed to generalize findings; quick check: apply same strategy to each modality.
- **Defense circumvention**: Strong safeguards can be bypassed; needed to assess robustness; quick check: run attack against models with known protections.

## Architecture Onboarding
**Component Map**: Harmful prompt -> Augmentation (text/image/audio) -> Sampling loop -> Model API -> Output filtering -> Success check
**Critical Path**: Harmful prompt generation -> Repeated augmentation and sampling -> Detection of harmful output -> Attack success
**Design Tradeoffs**: Sample efficiency vs. attack success; perturbation intensity vs. detectability; multi-modal vs. single-modal focus
**Failure Signatures**: Low attack success indicates insufficient sampling, poor augmentation, or overly strong defenses; sudden drops may signal model updates or rate limiting
**3 First Experiments**:
1. Vary sample count from 10^2 to 10^5, measure ASR scaling.
2. Test perturbation types individually (text-only, image-only, audio-only) on a single model.
3. Apply BoN to models with known safety mechanisms (e.g., circuit breakers) and measure bypass rates.

## Open Questions the Paper Calls Out
None

## Limitations
- Scaling behavior is observed over a limited sample range, so the power-law nature is not definitively established.
- Perturbation strategies for image and audio are not fully specified, limiting reproducibility.
- Results may not generalize to all future or alternative model architectures.
- The study does not assess the impact of API rate limits or cost on real-world feasibility.

## Confidence
- **High**: ASR results on tested models and modalities; BoN's ability to bypass known defenses (circuit breakers, Cygnet)
- **Medium**: Power-law-like scaling trends and their extrapolation; sample efficiency gains from composing with optimized prefixes
- **Low**: Generalizability to all frontier models; robustness of perturbation strategies; real-world feasibility under API constraints

## Next Checks
1. Validate the scaling law by testing BoN across a broader range of sample sizes (e.g., 10^3 to 10^6) to confirm the functional form of ASR growth.
2. Assess the impact of API rate limits and cost on attack feasibility by simulating constrained sampling budgets.
3. Test the robustness of BoN against updated model versions and alternative perturbation strategies to ensure findings are not artifacts of specific configurations.