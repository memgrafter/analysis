---
ver: rpa2
title: Boosting LLM Translation Skills without General Ability Loss via Rationale
  Distillation
arxiv_id: '2410.13944'
source_url: https://arxiv.org/abs/2410.13944
tags:
- translation
- data
- radis
- general
- rationale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Rationale Distillation (RaDis), a method to
  improve machine translation performance of large language models (LLMs) while mitigating
  catastrophic forgetting of general abilities. The core idea is to prompt the LLM
  to generate rationales for reference translations, then jointly train on both reference
  translations and rationales.
---

# Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation

## Quick Facts
- arXiv ID: 2410.13944
- Source URL: https://arxiv.org/abs/2410.13944
- Reference count: 40
- Key outcome: RaDis improves translation performance by 5.6-8.9 COMET points while preserving up to 92.50% of general capabilities

## Executive Summary
This paper addresses the challenge of improving machine translation performance in large language models while preserving their general capabilities. The proposed Rationale Distillation (RaDis) method prompts LLMs to generate rationales for reference translations, then jointly trains on both reference translations and rationales. This approach acts as self-distillation, preserving general knowledge that would otherwise be lost during translation fine-tuning. Experimental results show RaDis outperforms prior methods, enhancing translation performance while maintaining up to 92.50% of general capabilities across instruction following, safety, and reasoning benchmarks.

## Method Summary
RaDis works by first generating rationales for translation pairs using the LLM itself, then concatenating these rationales with reference translations to form enriched training data. The model is fine-tuned using LoRA adapters on this enriched dataset with a standard CLM loss. The rationale generation acts as a self-distillation target, preserving general knowledge through the training process. The method can be interpreted as a sequence-level self-distillation loss on the rationale tokens, serving as a regularizer to mitigate forgetting by preventing excessive deviation of model parameters.

## Key Results
- RaDis improves translation performance by 5.6-8.9 COMET points compared to baselines
- General capability preservation reaches up to 92.50% retention across multiple benchmarks
- Self-generated rationales outperform externally generated rationales, including those from LLaMA-3-70B-Instruct
- RaDis avoids the conflict between learning new translation tasks and consolidating original general abilities

## Why This Works (Mechanism)

### Mechanism 1
Self-generated rationales preserve general knowledge during translation fine-tuning by capturing diverse information including word alignments, factual information, and safety guidelines that are concatenated with reference translations to form enriched training data.

### Mechanism 2
RaDis functions as knowledge distillation by decomposing the training objective into standard MT loss plus self-distillation loss on rationale tokens, which acts as a regularizer preventing excessive parameter divergence.

### Mechanism 3
Self-generated rationales are more effective than externally generated ones due to better alignment between the knowledge source and the model being trained, avoiding conflicts between learning new tasks and preserving original capabilities.

## Foundational Learning

- **Catastrophic Forgetting in continual learning**: Understanding why fine-tuning on translation data causes loss of general abilities is crucial for appreciating RaDis's solution. *Quick check: Why does vanilla fine-tuning on translation data cause performance degradation on instruction-following tasks?*

- **Knowledge distillation and self-distillation**: RaDis leverages self-distillation principles to preserve general knowledge while learning new translation skills. *Quick check: How does the self-distillation loss in RaDis differ from traditional knowledge distillation approaches?*

- **Rationale generation in LLMs**: The core mechanism relies on LLMs' ability to generate informative rationales during translation tasks. *Quick check: What types of information do LLMs typically include in their rationales when translating?*

## Architecture Onboarding

- **Component map**: Backbone LLM (LLaMA-2-7B-Chat or Mistral-7B-Instruct-v0.2) -> Rationale generation module (prompt template + LLM continuation) -> Training pipeline (concatenation + LoRA fine-tuning) -> Evaluation suite (COMET, MT-Bench, AlpacaEval, safety, reasoning benchmarks)

- **Critical path**: 1. Generate rationales for translation dataset using backbone LLM 2. Concatenate rationales with reference translations 3. Fine-tune using LoRA adapters with standard CLM loss 4. Merge adapters and evaluate on both translation and general capability benchmarks

- **Design tradeoffs**: Self-generated vs externally generated rationales (quality vs alignment), full fine-tuning vs LoRA (parameter efficiency vs potential performance), rationale length and content (information richness vs training efficiency)

- **Failure signatures**: Rationale generation fails to produce meaningful content, translation performance doesn't improve despite training, general capability preservation is minimal, training becomes unstable due to rationale length variability

- **First 3 experiments**: 1. Generate rationales on a small validation set and manually inspect content quality 2. Compare COMET scores with and without RaDis on a small translation subset 3. Evaluate general capability retention using MT-Bench on a small sample before full training

## Open Questions the Paper Calls Out

The paper mentions that applying RaDis to other NLP tasks beyond machine translation would further support its effectiveness, but no experiments are conducted to explore this.

## Limitations

- The paper lacks transparency around the rationale generation process, not specifying exact prompt templates used
- Evaluation of general capability preservation relies on a limited set of benchmarks that may not comprehensively capture all aspects of general knowledge
- The paper doesn't address potential biases introduced by self-generated rationales or examine their quality across different translation domains

## Confidence

**High confidence**: The claim that RaDis improves translation performance (5.6-8.9 COMET points) is well-supported by experimental results across multiple evaluation datasets and language pairs.

**Medium confidence**: The claim that RaDis preserves general capabilities (up to 92.50% retention) is supported by reported benchmark results, but the evaluation scope is somewhat limited.

**Low confidence**: The assertion that self-generated rationales are inherently superior to externally generated ones is primarily supported by gradient similarity analysis and a single comparison with LLaMA-3-70B-Instruct.

## Next Checks

1. Conduct a systematic evaluation of the content quality, diversity, and informativeness of rationales generated by different models to determine whether the self-distillation property is truly necessary for effectiveness.

2. Expand the evaluation of general capability preservation to include additional benchmarks covering creative writing, code generation, mathematical reasoning beyond GSM8K, and factual knowledge retrieval.

3. Perform detailed gradient similarity analysis across different translation domains and dataset sizes to verify that RaDis consistently avoids the conflict between translation learning and general knowledge preservation.