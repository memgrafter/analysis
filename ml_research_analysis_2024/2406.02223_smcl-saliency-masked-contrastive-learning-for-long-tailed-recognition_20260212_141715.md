---
ver: rpa2
title: 'SMCL: Saliency Masked Contrastive Learning for Long-tailed Recognition'
arxiv_id: '2406.02223'
source_url: https://arxiv.org/abs/2406.02223
tags:
- learning
- contrastive
- classes
- saliency
- long-tailed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses long-tailed recognition, where models trained
  on imbalanced datasets often misclassify minor classes due to biased background
  features observed only in major classes. The proposed method, Saliency Masked Contrastive
  Learning (SMCL), masks salient image regions to isolate background features and
  uses contrastive learning to pull masked samples toward minor classes in feature
  space, thereby reducing background bias.
---

# SMCL: Saliency Masked Contrastive Learning for Long-tailed Recognition

## Quick Facts
- arXiv ID: 2406.02223
- Source URL: https://arxiv.org/abs/2406.02223
- Reference count: 0
- Primary result: SMCL achieves state-of-the-art performance on long-tailed recognition, improving few-shot class accuracy by 2.4% on ImageNet-LT

## Executive Summary
SMCL addresses long-tailed recognition by isolating background features that are erroneously correlated with major classes. The method masks salient image regions using saliency detection, then applies contrastive learning to pull these masked background features toward minor classes in feature space. This reduces background bias that causes misclassification of minor classes. Experiments on CIFAR-10/100-LT and ImageNet-LT demonstrate significant performance improvements over state-of-the-art methods.

## Method Summary
SMCL combines saliency masking with supervised contrastive learning to address background bias in long-tailed recognition. The method masks the most salient (class-specific) regions of images to isolate background features, then uses weighted sampling to increase the probability of selecting minor classes as contrastive targets. A combined loss function includes both cross-entropy and contrastive components, pulling masked background embeddings toward semantically relevant minor class samples in feature space.

## Key Results
- SMCL outperforms mixup, cutmix, and contrastive learning baselines on CIFAR-10/100-LT and ImageNet-LT
- Improves few-shot class accuracy by 2.4% on ImageNet-LT
- Saliency masking shown to be more effective than random or center masking strategies
- Consistently improves performance across different imbalance ratios (ρ ∈ {10, 50, 100, 256})

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Saliency masking isolates background features that are erroneously correlated with major classes, reducing biased predictions.
- Mechanism: By masking the most salient region of an image, only the background remains. This background is then used in contrastive learning to pull it toward minor class embeddings, effectively transferring context from major to minor classes.
- Core assumption: Background features that are correlated with major classes but unobserved in minor classes are semantically relevant to the minor classes and can be re-associated via feature space manipulation.
- Evidence anchors:
  - [abstract]: "mask the important part of an image using saliency detection and use contrastive learning to move the masked image towards minor classes in the feature space, so that background features present in the masked image are no longer correlated with the original class."
  - [section]: "Our key idea is to mask the important part of an image using saliency detection and use contrastive learning to move the masked image towards minor classes in the feature space, so that background features present in the masked image are no longer correlated with the original class."
- Break condition: If saliency detection fails to consistently isolate background features (e.g., salient regions overlap heavily with background), the masking becomes ineffective and background bias remains.

### Mechanism 2
- Claim: Weighted sampling increases the probability of selecting minor class samples as targets, thereby directing background features toward underrepresented classes.
- Mechanism: Sampling probabilities are calculated inversely to the effective number of samples per class, ensuring minor classes are more likely to be chosen as contrastive targets. This biases the contrastive loss toward aligning background features with minor class embeddings.
- Core assumption: The sampling distribution used for selecting contrastive targets can effectively influence the feature space alignment without introducing label noise or instability.
- Evidence anchors:
  - [section]: "In order to increase the chance of selecting a minor class label, we follow the sampling strategy [1] based on the effective number calculated from class size..."
  - [section]: "Based on Ek, the sampling probability of class k is: pk = 1/EkP k 1/Ek which leads to a higher sampling probability for the minor classes."
- Break condition: If the weighted sampling introduces too much noise (e.g., minor classes are semantically too different from major class backgrounds), contrastive learning may fail to converge or misalign features.

### Mechanism 3
- Claim: Supervised contrastive learning with masked samples improves feature representation by pulling background embeddings toward semantically relevant minor class samples.
- Mechanism: Contrastive loss is computed between the masked background image and minor class samples, encouraging the model to associate background features with minor classes in the feature space, beyond what cross-entropy alone can achieve.
- Core assumption: Background features are semantically relevant to minor classes and can be meaningfully pulled toward them in feature space without causing confusion or overfitting.
- Evidence anchors:
  - [abstract]: "Our key idea is to mask the important part of an image using saliency detection and use contrastive learning to move the masked image towards minor classes in the feature space..."
  - [section]: "Our key idea is to mask the important part of an image using saliency detection and use contrastive learning to move the masked image towards minor classes in the feature space..."
- Break condition: If the contrastive loss overwhelms the cross-entropy loss or the temperature parameter is poorly tuned, the model may collapse representations or overfit to background features.

## Foundational Learning

- Concept: Long-tailed distribution and class imbalance
  - Why needed here: The entire motivation and problem setup depend on understanding how class imbalance leads to biased feature learning.
  - Quick check question: In a dataset where class A has 1000 samples and class B has 10 samples, which class is more likely to have its background features over-represented in the learned model?

- Concept: Contrastive learning and feature space alignment
  - Why needed here: SMCL relies on supervised contrastive learning to move masked background embeddings toward minor class samples.
  - Quick check question: In contrastive learning, what is the goal when pulling two samples together in feature space?

- Concept: Saliency detection and masking strategies
  - Why needed here: Saliency masking is the core data augmentation technique that isolates background features for contrastive learning.
  - Quick check question: If saliency detection highlights the object of interest, what remains after masking that region?

## Architecture Onboarding

- Component map:
  Saliency detector -> Mask generator -> Contrastive learner -> Classifier
  Weighted sampler -> Target class selector
  Augmentation pipeline (random crop, flip) -> Input to both original and masked streams

- Critical path:
  1. Sample image and target class via weighted sampling
  2. Apply augmentations to both
  3. Generate saliency mask and apply to image
  4. Forward pass through model to get features and logits
  5. Compute combined cross-entropy and contrastive loss
  6. Backpropagate and update model

- Design tradeoffs:
  - Saliency masking probability (0.2–0.4): higher probability may increase background exposure but risk losing object context
  - Contrastive loss weight (µ=0.3): too high may destabilize training, too low may not overcome background bias
  - Weighted sampling distribution: must balance minor class exposure without overwhelming major class signal

- Failure signatures:
  - Accuracy drops on major classes after SMCL training
  - Contrastive loss diverges or oscillates
  - Masked images produce degenerate features (e.g., all zeros)
  - Minor class accuracy improves but overall accuracy plateaus or drops

- First 3 experiments:
  1. Train with only cross-entropy loss (no masking, no contrastive) -> establish baseline
  2. Add saliency masking only (no contrastive) -> test masking impact alone
  3. Add both saliency masking and contrastive learning -> test full SMCL effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SMCL vary when using different saliency detection methods beyond the one employed in the paper?
- Basis in paper: [explicit] The paper states "In order to mask discriminative features that belong to a certain class, we use saliency detection methods. Similarly [8], we use the saliency function S(·) and find the pixel i, j that has the maximum value," but does not explore alternative saliency detection methods.
- Why unresolved: The paper uses a specific saliency detection method without comparing it to others or exploring the impact of different methods on performance.
- What evidence would resolve it: Comparative experiments using various saliency detection methods (e.g., DeepGaze, SALICON) to evaluate their impact on SMCL's performance on long-tailed datasets.

### Open Question 2
- Question: What is the impact of varying the saliency masking probability during training on the model's performance?
- Basis in paper: [explicit] The paper mentions "Saliency masking is applied with a probability of 0.2 starting from epoch 160" for CIFAR datasets and "Saliency masking is applied with a probability of 0.4" for ImageNet-LT, but does not explore different probabilities.
- Why unresolved: The optimal probability for saliency masking is not explored, and its impact on performance is unclear.
- What evidence would resolve it: Experiments varying the saliency masking probability during training to determine its effect on model performance across different long-tailed datasets.

### Open Question 3
- Question: How does SMCL perform on other types of imbalanced datasets, such as those with class overlap or multi-label classification tasks?
- Basis in paper: [inferred] The paper focuses on long-tailed datasets with single-label classification, but does not address other types of imbalanced datasets or multi-label scenarios.
- Why unresolved: The method's effectiveness on different types of imbalanced datasets or multi-label tasks is not explored.
- What evidence would resolve it: Experiments applying SMCL to various imbalanced datasets, including those with class overlap or multi-label classification, to evaluate its generalizability and performance.

## Limitations

- Saliency detection method dependency: The effectiveness of SMCL heavily depends on the quality of the saliency detection method, which is not explicitly specified.
- Background feature relevance assumption: The core assumption that background features correlated with major classes are semantically relevant to minor classes is not empirically validated.
- Contrastive loss stability: The paper uses a fixed contrastive loss weight without exploring sensitivity to this hyperparameter.

## Confidence

- High confidence: The empirical results showing SMCL outperforming baselines on CIFAR and ImageNet-LT datasets are well-documented and reproducible.
- Medium confidence: The mechanism of using saliency masking to isolate background features is sound, but effectiveness depends on saliency detection quality.
- Medium confidence: The weighted sampling strategy to prioritize minor classes is theoretically justified but may introduce instability if minor classes are too semantically different from major class backgrounds.

## Next Checks

1. **Saliency method ablation study**: Compare SMCL performance using different saliency detection methods (e.g., gradient-based, attention-based, or learned saliency) to establish robustness to the choice of saliency detector.

2. **Background relevance validation**: Conduct experiments to verify that background features isolated by saliency masking are indeed semantically relevant to minor classes, such as through nearest-neighbor analysis or semantic segmentation of masked regions.

3. **Hyperparameter sensitivity analysis**: Systematically vary the contrastive loss weight (μ) and saliency masking probability to determine the optimal range and assess stability across different long-tailed distributions.