---
ver: rpa2
title: State Space Models are Provably Comparable to Transformers in Dynamic Token
  Selection
arxiv_id: '2405.19036'
source_url: https://arxiv.org/abs/2405.19036
tags:
- ssms
- input
- function
- tokens
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides theoretical analysis of state space models
  (SSMs) compared to transformers, focusing on their dynamic token selection capabilities.
  The key finding is that SSMs combined with fully connected neural networks (FNNs)
  can achieve comparable performance to transformers in tasks requiring dynamic token
  selection.
---

# State Space Models are Provably Comparable to Transformers in Dynamic Token Selection

## Quick Facts
- **arXiv ID**: 2405.19036
- **Source URL**: https://arxiv.org/abs/2405.19036
- **Reference count**: 40
- **Primary result**: SSMs combined with FNNs can achieve comparable performance to transformers in tasks requiring dynamic token selection

## Executive Summary
This paper provides theoretical analysis comparing state space models (SSMs) to transformers, focusing on their dynamic token selection capabilities. The authors prove that SSMs combined with fully connected neural networks (FNNs) can efficiently solve synthetic tasks like input copying and associative recall, and can mimic attention mechanisms' dynamic token selection through inner product-based key-query matching. The theoretical results are supported by experiments on genomic sequence classification, demonstrating that SSMs can dynamically identify important tokens in real-world tasks while requiring significantly less computation than transformers.

## Method Summary
The paper establishes theoretical bounds comparing SSMs with FNNs to transformers across three domains: synthetic tasks (input copying and associative recall), nonparametric regression on piecewise γ-smooth functions, and practical genomic sequence classification. The analysis uses convolution layers to represent SSMs and examines their ability to perform dynamic token selection through weighted sums based on inner products between keys and queries. The theoretical framework considers finite window sizes and establishes conditions under which SSMs achieve the same estimation error as transformers.

## Key Results
- SSMs + FNNs can efficiently solve synthetic tasks like input copying and associative recall
- SSMs + FNNs can mimic attention mechanisms' dynamic token selection through inner product-based key-query matching
- SSMs achieve the same estimation error as transformers for nonparametric regression on piecewise γ-smooth functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSMs combined with FNN layers can mimic the dynamic token selection mechanism of Transformers through inner product-based key-query attention
- Mechanism: SSMs with preceding and following FNN layers can approximate the weighted sum operation where weights are determined by inner products between keys and queries, similar to self-attention
- Core assumption: The FNN layers can learn to transform the input into appropriate key and query representations that the SSM can then use to compute attention-like weighted sums
- Evidence anchors:
  - [abstract]: "SSMs combined with FNNs can mimic attention mechanisms' dynamic token selection by computing weighted sums based on inner products between keys and queries"
  - [section]: Lemma 3.3 provides the formal proof that SSMs with FNNs can achieve this inner product-based token selection
  - [corpus]: Corpus neighbors include "LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models" suggesting active research on this exact mechanism
- Break condition: If the FNN layers cannot learn the appropriate key/query transformations, or if the SSM cannot compute the required weighted sums accurately, the mechanism fails

### Mechanism 2
- Claim: SSMs with FNNs can solve synthetic tasks (input copying and associative recall) efficiently by leveraging their ability to dynamically select tokens
- Mechanism: The SSM+FNN architecture can solve these tasks by first identifying the relevant tokens through inner product matching, then copying or retrieving the associated values
- Core assumption: The two-layer SSM architecture with FNNs can learn to both identify the correct tokens and then extract their values
- Evidence anchors:
  - [abstract]: "SSMs + FNNs can efficiently solve synthetic tasks like input copying and associative recall"
  - [section]: Theorem 3.1 and Theorem 3.2 provide the formal proofs for input copying and associative recall respectively
  - [corpus]: "The Illusion of State in State-Space Models" suggests ongoing research on SSM limitations and capabilities
- Break condition: If the tasks require more complex reasoning than simple token identification and retrieval, or if the sequence length grows too large relative to model capacity

### Mechanism 3
- Claim: SSMs with FNNs can estimate piecewise γ-smooth functions with the same efficiency as Transformers
- Mechanism: The SSM+FNN architecture can approximate the permutation map that reorders tokens by importance, then apply a smooth function to the reordered sequence
- Core assumption: The convolution layer can approximate the feature extraction map Γ, and the FNN can approximate the smooth function f after permutation
- Evidence anchors:
  - [abstract]: "SSMs achieve the same estimation error as transformers for nonparametric regression on piecewise γ-smooth functions"
  - [section]: Theorem 4.5 and Theorem 4.6 provide the formal proofs for approximation and estimation abilities
  - [corpus]: "A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures" suggests active research on comparing these architectures
- Break condition: If the importance function µ is not well-separated or if the smoothness parameter grows too quickly with sequence length

## Foundational Learning

- Concept: State Space Models (SSMs) and their computational advantages
  - Why needed here: The paper builds on understanding how SSMs work and why they're computationally efficient compared to Transformers
  - Quick check question: What is the computational complexity of SSMs versus Transformers for sequence modeling?

- Concept: Dynamic token selection and self-attention mechanisms
  - Why needed here: The paper's central claim is that SSMs with FNNs can achieve the same dynamic token selection as Transformers
  - Quick check question: How does self-attention compute weighted sums of tokens, and why is this dynamic?

- Concept: Function approximation theory and nonparametric regression
  - Why needed here: The paper uses this theory to prove that SSMs can estimate piecewise γ-smooth functions as well as Transformers
  - Quick check question: What is the definition of a piecewise γ-smooth function, and why is it relevant for sequence modeling?

## Architecture Onboarding

- Component map:
  - Embedding layer: Converts input tokens to D-dimensional vectors
  - Convolution layers: Act as SSM layers with learnable filters
  - FNN layers: Provide nonlinearity and enable key/query transformation
  - Decoding layer: Converts final token to output prediction
  - Feature extraction map Γ: Approximated by convolution to identify important dimensions

- Critical path:
  1. Input tokens → Embedding layer → D-dimensional representation
  2. Embedded sequence → Convolution layers (SSM) + FNN layers
  3. Intermediate representations → Final token extraction
  4. Final token → Decoding layer → Output prediction

- Design tradeoffs:
  - Window size U: Finite window simplifies analysis but may limit context in practice
  - Number of layers M: More layers increase capacity but also computational cost
  - Embedding dimension D: Higher dimensions increase expressiveness but also parameter count
  - Filter design: Ordinary SSM filters vs. data-dependent filters (like Mamba)

- Failure signatures:
  - Poor performance on synthetic tasks suggests insufficient dynamic token selection ability
  - High estimation error on piecewise γ-smooth functions suggests inadequate feature extraction
  - Training instability might indicate poor initialization or optimization challenges

- First 3 experiments:
  1. Reproduce the input copying task to verify dynamic token selection capability
  2. Test associative recall performance to validate key-query matching ability
  3. Evaluate nonparametric regression on piecewise γ-smooth functions to confirm estimation efficiency

## Open Questions the Paper Calls Out

- Question: Can the theoretical results about SSMs' dynamic token selection be extended to more complex real-world datasets beyond genomic sequences, such as natural language processing tasks?
- Question: How does the performance of SSMs + FNNs scale with extremely long sequences compared to Transformers, particularly in terms of memory efficiency and computational requirements?
- Question: What are the specific architectural modifications needed to make SSMs + FNNs competitive with Transformers on tasks requiring complex hierarchical reasoning?
- Question: How sensitive are the theoretical bounds to different parameterizations of SSMs, particularly those used in practical implementations like Mamba or Hyena?

## Limitations

- The theoretical analysis assumes finite windows and specific smoothness conditions that may not fully capture real-world complexity
- The proofs rely on ideal conditions for the feature extraction map Γ and importance function µ that may not hold in practice
- The genomic sequence classification experiment uses a pretrained model rather than training SSMs from scratch

## Confidence

**High Confidence**: The theoretical proofs for synthetic tasks (input copying and associative recall) are mathematically rigorous and the mechanisms are clearly defined. The comparison to transformers for piecewise γ-smooth function estimation is well-supported by the analysis.

**Medium Confidence**: The claim that SSMs with FNNs can mimic attention mechanisms through inner product-based token selection is theoretically sound but relies on the FNN layers learning appropriate transformations, which may not always occur in practice.

**Low Confidence**: The practical implications for real-world tasks are limited by the small-scale genomic experiment and the use of pretrained models. The computational advantages over transformers in practical implementations are not fully quantified.

## Next Checks

1. **Implement end-to-end training of SSM+FNN architectures on the synthetic tasks** from scratch rather than relying on theoretical analysis alone, measuring actual performance against transformers.

2. **Conduct controlled experiments varying the window size U and hidden state dimensions** to quantify the impact of these design choices on dynamic token selection performance.

3. **Design a more comprehensive real-world benchmark** beyond genomic sequences, testing SSMs on multiple diverse sequence classification tasks with direct comparison to transformer baselines trained under identical conditions.