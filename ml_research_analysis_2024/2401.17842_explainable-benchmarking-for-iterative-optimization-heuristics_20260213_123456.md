---
ver: rpa2
title: Explainable Benchmarking for Iterative Optimization Heuristics
arxiv_id: '2401.17842'
source_url: https://arxiv.org/abs/2401.17842
tags:
- 'true'
- 'false'
- algorithm
- performance
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces explainable benchmarking, a framework that
  leverages explainable AI techniques to analyze the impact of algorithmic components
  and hyperparameters on optimization heuristic performance. The IOHxplainer software
  tool implements this framework, enabling large-scale experimentation with modular
  optimization frameworks.
---

# Explainable Benchmarking for Iterative Optimization Heuristics

## Quick Facts
- arXiv ID: 2401.17842
- Source URL: https://arxiv.org/abs/2401.17842
- Authors: Niki van Stein; Diederick Vermetten; Anna V. Kononova; Thomas BÃ¤ck
- Reference count: 40
- One-line primary result: Explainable benchmarking framework using SHAP values reveals algorithmic component contributions, improving performance by 0.09-0.38 AOCC

## Executive Summary
This paper introduces explainable benchmarking, a framework that applies explainable AI techniques to analyze how algorithmic components and hyperparameters impact optimization heuristic performance. The framework leverages deep Xgboost regression models trained on large-scale configuration experiments, combined with Shapley value approximation (TreeSHAP) to quantify the marginal contribution of each component. The IOHxplainer software tool implements this approach, enabling systematic analysis of modular optimization algorithms like CMA-ES and DE across benchmark function suites.

## Method Summary
The framework operates by defining a modular configuration space, running large-scale experiments across many configurations, training deep Xgboost regression models to map configurations to performance metrics (AOCC), and extracting SHAP values to quantify each component's contribution. It supports both categorical and continuous parameters, handles variance analysis through virtual modules, and enables automated algorithm configuration using exploratory landscape analysis features. The method is applied to modular CMA-ES (15,840 configurations) and modular DE (36,288 configurations) across 24 benchmark functions in 5D and 30D.

## Key Results
- SHAP analysis identifies key performance-influencing modules, enabling configuration selection that improves performance by 0.09-0.38 AOCC
- Deep Xgboost models achieve RÂ² â‰ˆ 0.99, providing reliable performance predictions for SHAP value extraction
- Variance analysis reveals algorithm robustness to random seeds and problem instances through low SHAP values for these virtual modules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep Xgboost regression models with TreeSHAP can reliably quantify the marginal contribution of each algorithmic component to performance.
- Mechanism: The overfitted deep Xgboost models (RÂ² â‰ˆ 0.99) map configuration vectors to performance metrics. TreeSHAP then uses the model's structure to compute Shapley values efficiently, revealing each module's contribution to the predicted performance.
- Core assumption: The deep Xgboost model can capture the complex, non-linear relationships between algorithm configurations and performance without overfitting to noise.
- Evidence anchors:
  - [section] "Since the model is intentionally overfitted (ð‘…2 score of 0.99 ), the x-value of each dot can be interpreted as how much the performance (AOCC) is impacted..."
  - [abstract] "The method employs deep Xgboost regression models and Shapley approximation (TreeSHAP) to extract the marginal contribution of each component to performance."
- Break condition: If the model's RÂ² drops significantly below 0.9, the Shapley values become unreliable because the model fails to capture the underlying performance patterns.

### Mechanism 2
- Claim: Large-scale configuration sampling enables detection of subtle but important module interactions that would be missed in traditional benchmarking.
- Mechanism: By testing 15,840 CMA-ES and 36,288 DE configurations across 24 benchmark functions, the framework captures the full landscape of module combinations, revealing non-obvious dependencies and optimal settings for different function types.
- Core assumption: The grid/random sampling strategy adequately covers the configuration space to reveal meaningful patterns rather than just random noise.
- Evidence anchors:
  - [section] "The proposed framework works with any (modular) algorithm and with (large) collections of algorithms. By using a flexible configuration space definition [38]..."
  - [section] "For modCMA, the modules listed in Table 1 lead to a total of 15 840 configurations, which is a significantly larger set of configurations than used in previous works [33, 35]."
- Break condition: If the configuration space is too sparse or biased toward certain parameter ranges, critical interactions between modules may be missed.

### Mechanism 3
- Claim: Variance analysis (instance and stochastic) provides a sanity check on algorithm robustness and identifies configuration stability.
- Mechanism: By treating seed and instance ID as virtual modules, the framework measures their explanatory power via SHAP values. Low SHAP values indicate robustness to randomness and problem variations.
- Core assumption: Algorithm performance should ideally be independent of random seeds and problem instances for state-of-the-art optimizers.
- Evidence anchors:
  - [section] "If the algorithm's performance is independent of the used seed, as is generally expected from the state-of-the-art optimizers, the seed value should have little explanatory power..."
  - [section] "It should be noted that this analysis is only applicable to benchmark sets which support this type of instance creation..."
- Break condition: If variance components show high SHAP values, it indicates poor robustness or implementation issues that need addressing before drawing conclusions about module contributions.

## Foundational Learning

- Concept: Shapley values and feature attribution in machine learning
  - Why needed here: To understand how each algorithmic component contributes to performance, engineers need to grasp how SHAP values decompose model predictions.
  - Quick check question: If a module has a positive SHAP value, does that mean it always improves performance across all functions?

- Concept: Modular algorithm design and hyperparameter spaces
  - Why needed here: The framework's power comes from varying multiple independent modules; understanding how these interact is crucial for proper configuration.
  - Quick check question: Why does the framework support both categorical and continuous parameters in the configuration space?

- Concept: Exploratory landscape analysis (ELA) features
  - Why needed here: ELA features serve as input for automated algorithm configuration models that predict good configurations based on problem characteristics.
  - Quick check question: What is the relationship between ELA features and the performance of different algorithm configurations?

## Architecture Onboarding

- Component map: Configuration space definition -> Large-scale benchmarking runner -> Deep Xgboost model training -> TreeSHAP value extraction -> Visualization and reporting -> Structural bias detection -> Automated algorithm configuration
- Critical path: Define configuration space â†’ Run experiments â†’ Train models â†’ Extract SHAP values â†’ Generate visualizations â†’ Analyze results
- Design tradeoffs: Full grid enumeration provides complete coverage but is computationally expensive; random sampling is faster but may miss rare but important configurations
- Failure signatures: Low RÂ² scores indicate poor model fit; inconsistent SHAP patterns across similar functions suggest sampling issues; high variance SHAP values indicate robustness problems
- First 3 experiments:
  1. Run a small grid (e.g., 100 configurations) on a single benchmark function to verify the pipeline works end-to-end.
  2. Compare SHAP values from a small vs. large configuration set to understand sampling requirements.
  3. Test structural bias detection on a known biased configuration to validate the detection methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed explainable benchmarking framework generalize to optimization problems beyond the BBOB benchmark suite?
- Basis in paper: [inferred] The paper focuses on applying the framework to the BBOB benchmark suite but mentions it could be used with "any benchmark."
- Why unresolved: The paper only demonstrates the framework on BBOB functions, not on real-world optimization problems or other benchmark suites.
- What evidence would resolve it: Applying the framework to diverse real-world optimization problems and comparing results to existing benchmarks.

### Open Question 2
- Question: How do different explainable AI methods compare in terms of their ability to identify important algorithmic components and hyperparameters?
- Basis in paper: [explicit] The paper mentions that "in principle any model-agnostic feature attribution XAI method can be used" but only uses SHAP and sensitivity analysis.
- Why unresolved: The paper only uses SHAP and sensitivity analysis, not comparing other XAI methods like LIME or global sensitivity analysis methods.
- What evidence would resolve it: Comparing the results of different XAI methods on the same set of optimization algorithms and benchmark functions.

### Open Question 3
- Question: How does the proposed framework scale to larger configuration spaces and higher-dimensional optimization problems?
- Basis in paper: [inferred] The paper tests 15,840 and 36,288 configurations for modular CMA-ES and modular DE respectively, but only on 5D and 30D problems.
- Why unresolved: The paper does not explore the scalability of the framework to larger configuration spaces or higher-dimensional problems.
- What evidence would resolve it: Applying the framework to optimization problems with significantly larger configuration spaces and higher dimensions, and analyzing the computational cost and effectiveness of the results.

## Limitations
- The framework's reliability depends critically on the coverage and representativeness of the configuration space sampling
- Overfitted XGBoost models (RÂ² â‰ˆ 0.99) may capture noise rather than true signal, making SHAP value interpretations sensitive to sampling variations
- The automated algorithm configuration component relies on ELA features with limited validation across unseen problems

## Confidence

**High Confidence**: The core methodology of using SHAP values for feature attribution is well-established in machine learning literature and the mathematical framework is sound. The use of TreeSHAP for efficient Shapley value computation is a standard approach.

**Medium Confidence**: The application of this framework to optimization heuristics shows promise, but the interpretation of SHAP values in this specific context requires careful validation. The claim that SHAP values directly indicate causal performance improvements needs further experimental verification across diverse problem domains.

**Low Confidence**: The automated algorithm configuration component relies on ELA features, but the paper provides limited validation of how well these features predict performance across unseen problems. The generalizability of learned configurations beyond the tested benchmark suite remains unproven.

## Next Checks
1. **Sampling Sensitivity Analysis**: Systematically vary the density of configuration sampling (e.g., 10%, 50%, 100% of full space) and measure how SHAP value stability changes. This would quantify how much configuration space coverage is truly necessary for reliable attribution.

2. **Cross-Validation of Automated Configuration**: Use the ELA-based configuration predictor on a completely different benchmark suite (e.g., from COCO or Nevergrad) and measure the performance drop compared to configurations optimized specifically for those problems. This tests the practical utility of automated configuration.

3. **Ablation Study on Model Complexity**: Compare SHAP value patterns and performance predictions from simpler models (e.g., linear regression, shallow trees) versus the deep XGBoost models to determine if the added complexity provides meaningful improvements in attribution quality or if it primarily increases variance.