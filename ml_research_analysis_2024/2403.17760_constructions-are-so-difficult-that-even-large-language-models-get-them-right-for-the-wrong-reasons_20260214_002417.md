---
ver: rpa2
title: Constructions Are So Difficult That Even Large Language Models Get Them Right
  for the Wrong Reasons
arxiv_id: '2403.17760'
source_url: https://arxiv.org/abs/2403.17760
tags:
- llama
- adjective
- sentence
- sentences
- happy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether large language models (LLMs) can
  distinguish between three superficially similar constructions: causal excess construction
  (CEC), affective adjective phrase (AAP), and epistemic adjective phrase (EAP). These
  constructions differ in causality and underlying syntactic properties but share
  similar surface forms.'
---

# Constructions Are So Difficult That Even Large Language Models Get Them Right for the Wrong Reasons

## Quick Facts
- **arXiv ID:** 2403.17760
- **Source URL:** https://arxiv.org/abs/2403.17760
- **Reference count:** 0
- **Primary result:** LLMs exhibit strong bias towards causal excess construction (CEC), failing to differentiate between CEC, affective adjective phrase (AAP), and epistemic adjective phrase (EAP) despite shared surface forms.

## Executive Summary
This study investigates whether large language models (LLMs) can distinguish between three superficially similar constructions—causal excess construction (CEC), affective adjective phrase (AAP), and epistemic adjective phrase (EAP)—that share the surface form "so + adjective + that + clause" but differ in causality and syntactic properties. Using a small NLI challenge dataset with minimal lexical overlap, the authors tested GPT-3.5, GPT-4, and Llama 2 through prompting and probing classifiers. Results show all models exhibited strong bias towards CEC, with prompting results generally below random and probing classifier results only slightly above baseline. The study concludes that LLMs do not adequately capture the meaning or lexical properties of phrasal heads in these constructions, highlighting limitations in their semantic understanding.

## Method Summary
The study collected a dataset of 323 sentences using Universal Dependency annotations from parsed Wikipedia and Amazon Reviews corpora, manually classifying them into CEC, AAP, EAP, and OCE categories. The authors tested GPT-3.5, GPT-4, and Llama 2 using template-based prompts for NLI tasks, causality identification, direction of causality, and grammatical acceptability. Additionally, they extracted last-layer embeddings from the LLMs and trained perceptron classifiers to probe the models' internal representations of construction distinctions.

## Key Results
- All tested models exhibited strong bias towards CEC, consistently overestimating entailment for CEC sentences
- Llama 2 outperformed GPT models in sub-tasks but still showed significant bias towards CEC across all categories
- Perceptron classifiers trained on sentence embeddings performed slightly better than baseline for AAP vs EAP discrimination, suggesting learned common features for affective and epistemic adjectives
- Prompting results were generally below random chance, indicating LLMs struggle with these construction distinctions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs struggle to differentiate constructions due to shared surface form and lack of deep semantic representation
- **Mechanism:** The three constructions share the same surface form (so + adjective + that + clause) but differ in causality and licensing. LLMs, relying on surface cues, default to strong bias towards CEC when faced with ambiguity.
- **Core assumption:** LLMs lack sufficient internal representations to encode deeper semantic differences between constructions despite lexical overlap.
- **Evidence anchors:** Strong bias towards CEC across all models; Llama 2 attributing causality to over 90% of sentences in all categories.
- **Break condition:** Explicit training or fine-tuning on construction-specific semantics or causality might reduce the bias.

### Mechanism 2
- **Claim:** LLMs show better performance on AAP vs. EAP than CEC vs. others due to shared affective/epistemic properties
- **Mechanism:** Adjective embeddings capture common features for affective and epistemic adjectives, enabling better discrimination between AAP and EAP despite mutually exclusive adjectives.
- **Core assumption:** Models have learned a latent feature space where affective and epistemic adjectives cluster.
- **Evidence anchors:** Adjective perceptron beats baseline on AAP vs EAP test; distinction between EAP and CEC more pronounced than EAP and AAP for Llama 2 embeddings.
- **Break condition:** More diverse affective/epistemic adjectives or additional construction types might break the clustering.

### Mechanism 3
- **Claim:** LLMs rely heavily on lexical cues (presence of "so...that") for grammaticality judgments, not true syntactic/semantic understanding
- **Mechanism:** When prompted for grammaticality, models default to "good" for any sentence containing "so...that," regardless of whether the construction is actually grammatical.
- **Core assumption:** Presence of "so...that" pattern is treated as a strong signal for grammaticality, overriding deeper syntactic analysis.
- **Evidence anchors:** Models rate all O, DT, and AN sentences as good; Llama 2 struggles when "so" is removed.
- **Break condition:** Explicit training on grammaticality distinctions that don't rely on "so...that" might reduce this bias.

## Foundational Learning

- **Concept: Construction Grammar (CxG)**
  - Why needed here: Understanding CxG is essential to grasp why these constructions are treated as distinct meaning-bearing units, not just surface patterns.
  - Quick check question: What is the key difference between a lexical head licensing a clause and the CEC construction licensing a clause?

- **Concept: Causality and Direction of Causality**
  - Why needed here: The study hinges on whether models can identify the presence and direction of causality between the adjective and the clause.
  - Quick check question: In the sentence "I was so happy that I cried," who is the cause and who is the effect?

- **Concept: Licensing in Syntax**
  - Why needed here: Understanding licensing explains why some adjectives can take clausal complements while others cannot without "so."
  - Quick check question: Why is "*It was big that it fell over" ungrammatical, while "I was happy that I was freed" is grammatical?

## Architecture Onboarding

- **Component map:** SPIKE for parsed corpus extraction → manual annotation → dataset creation → prompting system → probing system → analysis
- **Critical path:** 1) Extract sentences with "so...that..." pattern from parsed corpus 2) Manually classify into CEC, AAP, EAP, OCE 3) Design and run prompts for each task 4) Extract embeddings and train probing classifiers 5) Analyze bias and model performance
- **Design tradeoffs:** Small dataset (323 sentences) → high lexical overlap but limited generalizability; Manual annotation → high quality but time-consuming; Prompt-based vs. classifier-based evaluation → complementary strengths/weaknesses
- **Failure signatures:** Consistently below-random performance on NLI tasks; Strong bias towards CEC across all models and tasks; Perceptron classifiers only slightly above baseline
- **First 3 experiments:** 1) Run central NLI task (Prompt 1-1 to 1-4) to confirm strong CEC bias 2) Test causality identification (Prompt 3-1, 3-2) to distinguish causal vs. non-causal sentences 3) Evaluate grammaticality judgments (Prompt 4-1 to 4-5) to check reliance on "so...that" pattern

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do LLMs encode causality information at the adjective level or at the sentence level?
- **Basis in paper:** [explicit] Perceptrons trained on sentence embeddings outperform those trained on adjective embeddings for identifying causality, suggesting causality is encoded at the sentence level.
- **Why unresolved:** The study doesn't provide a definitive answer on whether causality is encoded in the adjective representation itself or if it's a sentence-level phenomenon.
- **What evidence would resolve it:** Further experiments could investigate whether fine-tuning LLMs to recognize causality at the adjective level improves performance, or if manipulating the adjective representation directly affects causality detection.

### Open Question 2
- **Question:** Can LLMs be fine-tuned to improve their understanding of the direction of causality in CEC, AAP, and EAP constructions?
- **Basis in paper:** [inferred] Models struggle with determining direction of causality, often exhibiting bias towards positive answers, suggesting potential for improvement through fine-tuning.
- **Why unresolved:** The study only tests models' performance without exploring the possibility of fine-tuning for better results.
- **What evidence would resolve it:** Fine-tuning experiments could be conducted to see if performance on the direction of causality task improves after training on a dataset specifically designed to highlight this distinction.

### Open Question 3
- **Question:** How do LLMs handle extraposition in CEC, AAP, and EAP constructions, where the clausal complement is moved to the end of the sentence?
- **Basis in paper:** [explicit] The authors explicitly exclude extraposition from their study, stating they haven't investigated how LLMs handle sentences like "That I left was so bad" or "It was so bad that I left."
- **Why unresolved:** The paper explicitly states this aspect of the constructions was not investigated.
- **What evidence would resolve it:** Experiments could be designed to test models' performance on sentences with extraposition in CEC, AAP, and EAP constructions, comparing their accuracy to sentences without extraposition.

## Limitations
- Small dataset size (323 sentences) with limited lexical diversity may not capture the full complexity of construction grammar
- Manual annotation process introduces potential subjectivity and limits reproducibility
- Focus on three specific LLM architectures may not generalize to other models or future generations

## Confidence
- **High confidence:** Strong CEC bias observation across all models and tasks
- **Medium confidence:** Mechanism explaining LLM struggle with construction differentiation (surface form vs. semantic representation)
- **Medium confidence:** Comparative performance of AAP vs. EAP discrimination
- **Low confidence:** Generalizability of findings to larger, more diverse datasets

## Next Checks
1. Test model performance on a significantly larger dataset (5,000+ sentences) with broader lexical coverage and multiple construction types to assess generalizability
2. Conduct ablation studies by systematically removing lexical cues (e.g., "so...that" pattern) to determine their impact on model performance and bias
3. Evaluate fine-tuned models specifically trained on construction grammar distinctions to compare against zero-shot prompting results and isolate whether the bias is inherent to LLM architecture or prompting methodology