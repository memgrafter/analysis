---
ver: rpa2
title: Sampling from Bayesian Neural Network Posteriors with Symmetric Minibatch Splitting
  Langevin Dynamics
arxiv_id: '2410.19780'
source_url: https://arxiv.org/abs/2410.19780
tags:
- have
- stochastic
- size
- gradient
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SMS-UBU, a scalable kinetic Langevin dynamics
  algorithm for sampling parameter spaces in big data and AI applications. The method
  combines a symmetric forward/backward sweep over minibatches with a symmetric discretization
  of Langevin dynamics.
---

# Sampling from Bayesian Neural Network Posteriors with Symmetric Minibatch Splitting Langevin Dynamics

## Quick Facts
- arXiv ID: 2410.19780
- Source URL: https://arxiv.org/abs/2410.19780
- Reference count: 40
- Primary result: SMS-UBU achieves O(h²√d) bias in high dimensions while using only one minibatch per iteration for sampling from BNN posteriors

## Executive Summary
This paper presents SMS-UBU, a scalable kinetic Langevin dynamics algorithm for sampling parameter spaces in big data and AI applications. The method combines symmetric forward/backward sweeps over minibatches with a symmetric discretization of Langevin dynamics (UBU). For a particular Langevin splitting method (UBU), the authors show that the resulting Symmetric Minibatch Splitting-UBU (SMS-UBU) integrator has bias O(h²d^1/2) in dimension d>0 with stepsize h>0, despite only using one minibatch per iteration. This provides excellent control of sampling bias as a function of stepsize. The algorithm is applied to explore local modes of the posterior distribution of Bayesian neural networks (BNNs) and evaluate the calibration performance of posterior predictive probabilities. Experiments are conducted on three different datasets (Fashion-MNIST, Celeb-A, and chest X-ray) using neural networks with convolutional neural network architectures for classification problems. The results indicate that BNNs sampled with SMS-UBU can offer significantly better calibration performance compared to standard methods of training and stochastic weight averaging.

## Method Summary
The SMS-UBU algorithm combines symmetric minibatch splitting with the UBU (Velocity-Verlet) integrator for kinetic Langevin dynamics. The method performs a symmetric forward sweep over minibatches, then a backward sweep, and outputs a sample. Each sweep consists of UBU steps using different minibatches. The symmetric structure of both the integrator and minibatch sampling ensures that stochastic gradient noise averages out, preserving the cancellation properties of the UBU scheme. The algorithm maintains second-order accuracy even with stochastic gradients, achieving O(h²√d) bias in high dimensions while using only one minibatch per iteration. For BNN applications, the method is combined with variance reduction techniques and stochastic weight averaging to improve sampling efficiency.

## Key Results
- SMS-UBU achieves O(h²√d) bias in high dimensions while using only one minibatch per iteration
- BNNs sampled with SMS-UBU show significantly better calibration performance compared to standard training and stochastic weight averaging
- The method demonstrates consistent improvements across three different datasets (Fashion-MNIST, Celeb-A, chest X-ray) and classification tasks
- SMS-UBU provides excellent control of sampling bias as a function of stepsize, making it scalable for large datasets and high-dimensional parameter spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symmetric minibatch splitting (SMS) combined with UBU integrator achieves O(h²√d) bias in high dimensions while using only one minibatch per iteration.
- Mechanism: The symmetric forward/backward sweep over minibatches cancels leading-order error terms that would otherwise arise from stochastic gradients. When combined with the symmetric UBU discretization, this cancellation extends to the discretization bias itself.
- Core assumption: The potential function has bounded third derivatives (strongly Hessian Lipschitz) and the gradient estimator remains unbiased across the symmetric sweep.
- Evidence anchors:
  - [abstract]: "SMS-UBU integrator has bias O(h²d¹/²) in dimension d>0 with stepsize h>0, despite only using one minibatch per iteration"
  - [section 2.3]: "it ensures that the integrator is symmetric as an integrator along a longer time horizon T = 2hNm"
  - [corpus]: Weak - no direct supporting papers found in corpus
- Break condition: If the Hessian Lipschitz constant grows with dimension or if minibatches are not properly synchronized across the forward/backward sweep.

### Mechanism 2
- Claim: UBU splitting method provides second-order strong accuracy in the full gradient setting.
- Mechanism: The UBU scheme alternates between position and velocity updates using half-steps of the exact Ornstein-Uhlenbeck flow, creating a symmetric integrator that cancels first-order terms in the error expansion.
- Core assumption: The potential function is sufficiently smooth (C²) and the friction parameter satisfies γ ≥ √8M.
- Evidence anchors:
  - [section 2.1]: "UBU scheme consists of applying first a half-step... then applying an impulse based on the potential energy term, followed by another half-step"
  - [section 3.3]: "UBU scheme (with full gradients and iterates (xn, v n)n∈ N) with synchronously coupled Brownian motion"
  - [corpus]: Weak - no direct supporting papers found in corpus
- Break condition: When the potential has discontinuities or when the friction parameter is too small relative to the gradient Lipschitz constant.

### Mechanism 3
- Claim: The combination of symmetric minibatching with UBU maintains second-order accuracy even with stochastic gradients.
- Mechanism: The symmetric structure of both the integrator and the minibatch sampling ensures that the stochastic gradient noise averages out over the forward/backward sweep, preserving the cancellation properties of the UBU scheme.
- Core assumption: The stochastic gradients are unbiased estimators and the minibatches form a proper partition of the dataset without replacement.
- Evidence anchors:
  - [section 2.3]: "we propose to take UBU steps with gradient approximations using the index-set ω 1 first, then ω 2 all the way up to ω Nm"
  - [section 3.3]: "SMS-UBU scheme with friction parameter γ > 0, stepsize h > 0 and initial measure π0"
  - [corpus]: Weak - no direct supporting papers found in corpus
- Break condition: If the minibatch partitions overlap or if the stochastic gradients introduce systematic bias that doesn't average out.

## Foundational Learning

- Concept: Kinetic Langevin dynamics and its splitting methods
  - Why needed here: The entire algorithm is built on discretizing kinetic Langevin dynamics using splitting methods to achieve efficient sampling
  - Quick check question: What are the three main components of kinetic Langevin dynamics that need to be split for numerical integration?

- Concept: Wasserstein distance and convergence rates
  - Why needed here: The theoretical analysis relies on Wasserstein distance bounds to establish convergence and error estimates
  - Quick check question: How does the weighted Euclidean norm in Definition 3 relate to the standard Euclidean norm?

- Concept: Bayesian neural networks and posterior sampling
  - Why needed here: The algorithm is specifically designed for sampling from BNN posteriors, which have unique challenges due to high dimensionality and multimodality
  - Quick check question: Why is sampling from BNN posteriors more challenging than from standard Bayesian models?

## Architecture Onboarding

- Component map: Forward sweep -> Backward sweep -> Output sample
- Critical path: Forward sweep (UBU steps with minibatches ω₁ to ωₙ) -> Backward sweep (UBU steps with minibatches ωₙ to ω₁) -> Output sample
- Design tradeoffs: Using symmetric minibatching improves bias but requires more memory to store multiple minibatch partitions; using UBU instead of simpler methods improves accuracy but requires more careful implementation
- Failure signatures: If the algorithm shows high bias, check whether the minibatch partitions are properly synchronized; if convergence is slow, verify that the friction parameter satisfies the theoretical bounds
- First 3 experiments:
  1. Implement SMS-UBU on a simple strongly convex potential to verify O(h²) convergence
  2. Test on a small BNN with synthetic data to verify posterior sampling
  3. Scale up to Fashion-MNIST to verify scalability claims with respect to dataset size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of SMS-UBU's scalability when applied to extremely high-dimensional neural networks with billions of parameters?
- Basis in paper: [explicit] The paper mentions that BNNs with millions or billions of parameters pose significant computational challenges, and SMS-UBU shows O(h²d^1/2) bias in dimension d>0, but does not explore the asymptotic behavior at extremely high dimensions.
- Why unresolved: The paper's theoretical analysis and experiments focus on networks with up to several million parameters, leaving the behavior at billion-parameter scales unexplored.
- What evidence would resolve it: Experiments with extremely large-scale neural networks (billion+ parameters) demonstrating whether the O(h²d^1/2) bias bound holds or breaks down at such scales.

### Open Question 2
- Question: How does the performance of SMS-UBU compare to Metropolis-adjusted variants when dealing with highly multimodal posterior distributions in BNNs?
- Basis in paper: [inferred] The paper notes that multimodal distributions are highly challenging for sampling and that SMS-UBU uses an unadjusted approach, while also mentioning Metropolis-adjusted variants exist in the literature.
- Why unresolved: The paper focuses on the unadjusted SMS-UBU approach and compares it to other unadjusted methods, but does not investigate whether incorporating a Metropolis-Hastings step would improve performance on multimodal posteriors.
- What evidence would resolve it: Comparative experiments between SMS-UBU and its Metropolis-adjusted variants on highly multimodal BNN posterior distributions.

### Open Question 3
- Question: What is the optimal choice of stepsize decay schedule for SMS-UBU when applied to different types of neural network architectures and datasets?
- Basis in paper: [explicit] The paper mentions using a "slowly decreasing stepsize" combined with Stochastic Weight Averaging (SWA) but does not provide a systematic analysis of optimal stepsize schedules.
- Why unresolved: The paper uses specific stepsize choices for their experiments but does not explore how different decay schedules affect performance across various architectures and datasets.
- What evidence would resolve it: Systematic experiments varying stepsize decay schedules across multiple neural network architectures and datasets to identify optimal patterns.

## Limitations

- The theoretical analysis relies on strong assumptions about the potential function being strongly Hessian Lipschitz and stochastic gradients being unbiased, which may not hold for complex neural network posteriors
- The experimental evaluation uses a limited set of architectures (CNNs with 6 convolutional layers) and metrics, not including other state-of-the-art Bayesian neural network sampling methods for comparison
- The paper provides limited empirical validation of the O(h²√d) bias claim, focusing on three specific datasets rather than systematically exploring the theoretical bounds

## Confidence

Confidence in the main claims is **Medium**. The algorithm design is sound and the experimental results show promising improvements in calibration performance compared to standard methods. However, the theoretical analysis depends on assumptions that may not hold in practice, particularly for complex neural network posteriors. The lack of citations for related work in the corpus suggests either a novel contribution or gaps in the literature review.

## Next Checks

1. **Theoretical validation**: Implement a simple strongly convex potential with known gradient and Hessian to verify the O(h²√d) bias claim experimentally by varying dimension d and stepsize h.

2. **Architecture robustness**: Test SMS-UBU on deeper neural network architectures (ResNet, Transformer) and regression tasks to assess whether the calibration improvements generalize beyond the specific CNN architectures used in the paper.

3. **Baseline comparison**: Compare SMS-UBU against other modern Bayesian neural network sampling methods like SG-MCMC with preconditioning or variational inference approaches on the same datasets to establish its relative performance.