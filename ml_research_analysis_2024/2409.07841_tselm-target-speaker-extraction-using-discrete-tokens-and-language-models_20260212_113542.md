---
ver: rpa2
title: 'TSELM: Target Speaker Extraction using Discrete Tokens and Language Models'
arxiv_id: '2409.07841'
source_url: https://arxiv.org/abs/2409.07841
tags:
- speech
- speaker
- arxiv
- target
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of target speaker extraction, which
  involves isolating the voice of a specific speaker from a mixture of voices. The
  proposed method, TSELM, leverages discrete tokens and language models for this task.
---

# TSELM: Target Speaker Extraction using Discrete Tokens and Language Models

## Quick Facts
- arXiv ID: 2409.07841
- Source URL: https://arxiv.org/abs/2409.07841
- Reference count: 37
- Primary result: TSELM achieves excellent speech quality (DNSMOS scores) and comparable speech intelligibility (dWER scores) compared to existing methods.

## Executive Summary
TSELM addresses the challenge of target speaker extraction by leveraging discrete tokens and language models. The method uses multiple discretized layers from WavLM as input tokens, incorporates cross-attention mechanisms to integrate target speaker information, and employs a language model to capture sequence dependencies. A scalable HiFi-GAN is used to reconstruct the audio from the tokens. The primary results show that TSELM achieves excellent speech quality (outperforming existing methods in DNSMOS scores) and comparable speech intelligibility (competitive dWER results), demonstrating the effectiveness of combining discrete representations with language models for target speaker extraction.

## Method Summary
TSELM uses WavLM Large to encode both mixed speech and reference speech into continuous representations, extracting outputs from six hidden layers (1, 3, 7, 12, 18, and 23). These representations are discretized using Kmeans clustering, with attention mechanisms aggregating the multiple discretized representations. Cross-attention modules inject speaker-specific information by using mixture embeddings as queries and reference embeddings as keys and values. The discretized tokens are processed by an encoder-only Transformer language model, which predicts token probabilities via linear classifiers. Finally, a scalable HiFi-GAN decoder reconstructs the audio from the predicted tokens. The model is trained using AdamW optimizer with cross-entropy loss on discrete tokens for 40,000 steps, using mixed speech with SNR between 0 to 5 dB.

## Key Results
- TSELM achieves DNSMOS scores of 3.60 on clean test data, outperforming the second-best method (DDSW 3.52) and Continuous-WavLM-L6 (3.54)
- Speech intelligibility measured by dWER shows competitive performance at 0.31, comparable to Continuous-WavLM-L6 (0.32) and only slightly worse than DDSW (0.30)
- Speaker similarity measured by ResNet221_LM achieves 0.44, outperforming all baselines except DDSW (0.45)
- Performance scales with model size: Large version achieves best results, while Small and Medium versions show proportional improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention with concatenated mixture-reference audio guides the model to focus on target speaker information
- Mechanism: By concatenating reference speech to both sides of mixed speech before encoding, WavLM treats reference as primary speaker and extracts embeddings emphasizing target speaker characteristics. Cross-attention then uses these reference embeddings to selectively attend to target speaker components.
- Core assumption: WavLM's training objective of focusing on first utterance as primary speaker transfers to concatenation strategy
- Evidence anchors: [abstract] multiple discretized layers from WavLM; [section] concatenated signal input into encoder; [corpus] weak evidence
- Break condition: If WavLM doesn't prioritize first utterance or reference length is insufficient

### Mechanism 2
- Claim: Multiple WavLM layers capture richer information than single-layer approaches
- Mechanism: Six different WavLM hidden layers are extracted and separately clustered, with attention mechanism aggregating these representations to capture information at different abstraction levels.
- Core assumption: Different layers encode complementary information
- Evidence anchors: [abstract] utilizes multiple discretized layers; [section] extract outputs from six hidden layers; [section] attention embedding produces reference and mixture embeddings; [corpus] weak evidence
- Break condition: If layers encode redundant information or attention mechanism fails to weight contributions properly

### Mechanism 3
- Claim: Converting audio generation to classification via discretization simplifies learning
- Mechanism: Continuous audio embeddings are discretized using Kmeans clustering, transforming complex regression into classification. Language model predicts probability distribution over possible tokens.
- Core assumption: Discrete token space captures sufficient information for high-quality reconstruction
- Evidence anchors: [abstract] cross-entropy loss converts regression to classification; [section] continuous embedding transformed to discrete tensor; [section] cross-entropy loss applied between predicted and clean tokens; [corpus] moderate evidence
- Break condition: If discretization loses critical information or token space is too coarse

## Foundational Learning

- Concept: Self-supervised learning and WavLM architecture
  - Why needed here: Understanding WavLM's speech representation extraction and layer selection is crucial for implementing encoding stage
  - Quick check question: What is the key architectural difference between WavLM and earlier SSL models like HuBERT, and how does this affect target speaker extraction?

- Concept: Discrete representation learning and tokenization
  - Why needed here: Core innovation relies on converting continuous embeddings to discrete tokens, requiring understanding of Kmeans clustering and information loss
  - Quick check question: How does choice of K (number of clusters) affect trade-off between reconstruction quality and model complexity?

- Concept: Cross-attention mechanisms in multimodal learning
  - Why needed here: Model uses cross-attention to inject speaker-specific information, requiring understanding of query-key-value attention
  - Quick check question: In cross-attention module, why is mixture embedding used as query while reference serves as key and value?

## Architecture Onboarding

- Component map: Input → WavLM Encoder (6 layers) → Kmeans Tokenizers (6) → Attention Embedding → Cross-Attention (with reference) → Language Model (Encoder-only Transformer) → Linear Classifiers (6) → Cross-Entropy Loss → Scalable HiFi-GAN Decoder → Output
- Critical path: Mixed speech → WavLM → Kmeans → Attention Embedding → Cross-Attention → Language Model → Classifier → HiFi-GAN → Output
- Design tradeoffs: Discrete tokens simplify learning but lose information; multiple layers capture richness but increase complexity; concatenation strategy guides focus but may bias to dominant speaker
- Failure signatures: Poor speech quality indicates discretization or reconstruction issues; low intelligibility suggests reference encoding problems; speaker similarity issues point to cross-attention failures
- First 3 experiments:
  1. Verify WavLM layer outputs capture target speaker information by visualizing embeddings with t-SNE
  2. Test Kmeans clustering stability and reconstruction quality on clean speech before adding mixture
  3. Validate cross-attention mechanism by ablating it and measuring degradation in target speaker extraction quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can information loss in discretization process be minimized to bridge performance gap between discrete and continuous methods?
- Basis in paper: [explicit] Performance gap attributed to information loss inherent in discretization process
- Why unresolved: Acknowledged but no concrete solution provided
- What evidence would resolve it: Experimental results showing improved intelligibility and speaker similarity after implementing information loss minimization techniques

### Open Question 2
- Question: How can speaker-aware tokenization method be developed to improve target speaker extraction performance?
- Basis in paper: [inferred] Current concatenation strategy is suboptimal; future work should prioritize speaker-aware tokenization
- Why unresolved: No details on development or effectiveness demonstration
- What evidence would resolve it: Experimental results showing improved performance using speaker-aware tokenization vs. concatenation strategy

### Open Question 3
- Question: How can tokenization methods for SSL models be enhanced to reduce speaker information loss?
- Basis in paper: [explicit] Speaker information loss likely due to tokenization process; future work should enhance tokenization methods
- Why unresolved: No specific techniques or experimental results provided
- What evidence would resolve it: Experimental results showing improved speaker similarity after implementing enhanced tokenization methods

## Limitations

- DNSMOS scores evaluated using "real-time model" rather than full DNSMOS-MOSNet, raising questions about absolute reliability
- Limited direct comparisons with non-discrete continuous baseline methods beyond Continuous-WavLM-L6
- Dependency on reference speech quality and length not thoroughly characterized
- Potential information loss from discretization not explicitly quantified

## Confidence

**High Confidence**: Comparable speech intelligibility (dWER scores) well-supported by experimental results against established baselines; core architectural innovations clearly described and technically sound.

**Medium Confidence**: Superior speech quality (DNSMOS scores) moderately supported but evaluation used real-time model rather than full DNSMOS-MOSNet; lacks comprehensive baseline comparisons and ablation studies on multi-layer discretization.

**Low Confidence**: Generalizability across different reference speech qualities and lengths not well-established; extensive testing with varying reference conditions missing; information loss from discretization acknowledged but not quantified.

## Next Checks

1. **Cross-Attention Mechanism Ablation**: Remove cross-attention module and retrain to quantify exact contribution of reference speaker guidance. Compare resulting dWER, DNSMOS, and speaker similarity scores against full model.

2. **Discretization Granularity Analysis**: Systematically vary Kmeans clusters (K) from coarse (K=32) to fine (K=2048) while keeping other parameters constant. Measure trade-off between reconstruction quality (DNSMOS), intelligibility (dWER), and model complexity.

3. **Reference Speech Robustness Testing**: Evaluate performance using degraded reference speech signals - varying SNR levels (clean to -10dB), shortening duration (full utterances to 1-second clips), and using different speaker references.