---
ver: rpa2
title: 'Understanding LLM Development Through Longitudinal Study: Insights from the
  Open Ko-LLM Leaderboard'
arxiv_id: '2409.03257'
source_url: https://arxiv.org/abs/2409.03257
tags:
- leaderboard
- performance
- tasks
- open
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a longitudinal analysis of the Open Ko-LLM
  Leaderboard over an 11-month period, examining the progression of Korean large language
  models across five benchmark tasks. The analysis reveals that while tasks like Ko-HellaSwag
  and Ko-TruthfulQA show rapid performance saturation, more complex tasks such as
  Ko-MMLU and Ko-CommonGEN V2 present ongoing challenges with slower improvement rates.
---

# Understanding LLM Development Through Longitudinal Study: Insights from the Open Ko-LLM Leaderboard

## Quick Facts
- arXiv ID: 2409.03257
- Source URL: https://arxiv.org/abs/2409.03257
- Authors: Chanjun Park; Hyeonwoo Kim
- Reference count: 3
- Key outcome: Longitudinal analysis reveals task-specific saturation patterns and the foundational role of pretrained models in Korean LLM development

## Executive Summary
This study presents a comprehensive 11-month analysis of the Open Ko-LLM Leaderboard, tracking the evolution of Korean large language models across five benchmark tasks. The research identifies distinct performance saturation patterns across different task types, with simpler tasks like Ko-HellaSwag and Ko-TruthfulQA showing rapid improvement and early plateauing, while more complex tasks like Ko-MMLU and Ko-CommonGEN V2 demonstrate slower, ongoing improvements. The study also reveals that larger models (7-14B parameters) exhibit stronger correlations across tasks, suggesting better capability integration, and that improvements in pretrained models are foundational to downstream model performance.

## Method Summary
The study analyzed 1,769 models evaluated on five Korean-specific tasks over an 11-month period from September 2023 to August 2024. Models were categorized by parameter size (under 3B, 3-7B, and 7-14B) and their performance correlations across tasks were computed. The analysis tracked temporal performance trends, identifying saturation points and ranking shifts. The research also examined the relationship between pretrained model improvements and downstream instruction-tuned and RL-tuned model performance, noting a characteristic one-week lag pattern.

## Key Results
- Larger models (7-14B parameters) show stronger positive correlations across tasks compared to smaller models, indicating better capability integration
- Pretrained model improvements are foundational - downstream model performance stagnates when pretrained model innovation plateaus
- Task complexity determines saturation rates - simpler tasks reach performance plateaus quickly while complex tasks show ongoing, slower improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models (7-14B parameters) exhibit stronger positive correlations across tasks compared to smaller models, indicating better integration of capabilities.
- Mechanism: As model size increases, the model's capacity to learn and integrate diverse capabilities across tasks improves, leading to more consistent performance gains across benchmarks.
- Core assumption: The observed correlations are not artifacts of shared data or evaluation artifacts but reflect genuine improvements in capability integration.
- Evidence anchors:
  - [abstract] states "larger models (7-14B parameters) demonstrate stronger correlations across tasks, suggesting better integration of capabilities compared to smaller models."
  - [section 3.2] provides Figure 2 showing correlation patterns across different model size categories.
  - [corpus] shows related work on LLM evaluation but doesn't directly address model size correlations.
- Break condition: If correlations between tasks remain low or negative even for the largest models tested, suggesting fundamental architectural limitations beyond scaling.

### Mechanism 2
- Claim: Pretrained model improvements are foundational - when pretrained model performance saturates, instruction-tuned and RL-tuned models also plateau.
- Mechanism: Instruction-tuned and RL-tuned models rely on the representational capacity and knowledge captured in their pretrained base models. Improvements in these downstream models are derivative of improvements in the pretrained foundation.
- Core assumption: The one-week lag between pretrained and instruction-tuned model improvements reflects a direct dependency chain rather than independent parallel development.
- Evidence anchors:
  - [section 3.3] states "improvements in instruction-tuned models typically lagged behind those of pretrained models by about one week" and "after April 2024, the performance of pretrained models stabilized, leading to a corresponding lack of progress in both instruction-tuned and RL-tuned models."
  - [abstract] mentions "performance stagnating once improvements in pretrained models plateau."
  - [corpus] lacks direct evidence about pretraining dependencies.
- Break condition: If new downstream tuning techniques could achieve significant improvements without corresponding pretrained model advances.

### Mechanism 3
- Claim: Task complexity determines the rate of performance saturation - simpler tasks like Ko-HellaSwag and Ko-TruthfulQA saturate quickly while complex tasks like Ko-MMLU and Ko-CommonGEN V2 show slower improvement.
- Mechanism: Tasks requiring complex reasoning and specialized knowledge present ongoing challenges that current model architectures and training approaches cannot easily overcome, leading to slower improvement rates.
- Core assumption: The observed performance patterns reflect genuine task complexity differences rather than artifacts of benchmark design or evaluation methodology.
- Evidence anchors:
  - [section 3.1] provides Figure 1 and Table 1 showing varying performance patterns across tasks over 11 months.
  - [abstract] states "tasks like Ko-HellaSwag and Ko-TruthfulQA show rapid performance saturation, more complex tasks such as Ko-MMLU and Ko-CommonGEN V2 present ongoing challenges with slower improvement rates."
  - [corpus] lacks evidence about task complexity measurement.
- Break condition: If future models demonstrate rapid improvements on complex tasks without architectural changes, suggesting the initial saturation patterns were due to limited model capacity rather than inherent task complexity.

## Foundational Learning

- Concept: Correlation analysis
  - Why needed here: Understanding how performance across different tasks relates to each other and to model size is central to interpreting the study's findings about capability integration.
  - Quick check question: If two tasks show a correlation coefficient of 0.8 across models, what does this tell you about how models that perform well on one task tend to perform on the other?

- Concept: Longitudinal study design
  - Why needed here: The study's 11-month timeframe compared to previous 5-month studies is a key methodological feature that enables observation of saturation patterns and long-term trends.
  - Quick check question: What are the advantages of studying model performance over 11 months versus 5 months when trying to identify saturation patterns?

- Concept: Benchmark evaluation frameworks
  - Why needed here: The study uses specific benchmarks (Ko-H5) with private test sets to avoid data contamination, which is crucial for understanding the reliability of the performance measurements.
  - Quick check question: Why is it important to use private test sets in LLM leaderboards, and what problems does this prevent?

## Architecture Onboarding

- Component map: Model submission interface -> Evaluation pipeline (5 Korean tasks) -> Performance tracking database -> Visualization tools for trend analysis -> Ranking algorithm with correlation computation
- Critical path: Model submission → Evaluation pipeline → Result storage → Trend computation → Leaderboard display. Any failure in evaluation or data storage breaks the entire analysis chain.
- Design tradeoffs: Using private test sets ensures evaluation integrity but limits reproducibility; tracking correlations adds analytical value but increases computational overhead; focusing on Korean language limits generalizability but enables more culturally relevant benchmarks.
- Failure signatures: Missing evaluation results indicate model size or library compatibility issues; stagnant performance across all model sizes suggests fundamental limitations in pretrained models; inconsistent correlations between tasks may indicate benchmark design problems.
- First 3 experiments:
  1. Submit a small (1B parameter) model and verify it completes all five evaluations, documenting any failures or incomplete results.
  2. Compare correlation patterns between the 0-3B and 7-14B model categories to reproduce Figure 2's findings.
  3. Analyze the one-week lag between pretrained and instruction-tuned model improvements by tracking submissions over a 4-week period.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the rate of performance saturation differ across task types (e.g., commonsense reasoning vs. multitask language understanding) as model capabilities advance over time?
- Basis in paper: [explicit] The paper notes that tasks like Ko-HellaSwag and Ko-TruthfulQA reach saturation quickly (scores of 80 within 25 weeks), while tasks like Ko-MMLU and Ko-CommonGEN V2 show slower improvements and no clear saturation.
- Why unresolved: The paper provides initial observations but does not explore the underlying factors driving these differential saturation rates or predict future trends.
- What evidence would resolve it: Longitudinal data tracking task performance across multiple model generations, combined with ablation studies isolating factors like reasoning complexity or domain knowledge requirements.

### Open Question 2
- Question: What specific aspects of pretrained model innovation are most critical for breaking through performance plateaus observed after April 2024?
- Basis in paper: [explicit] The paper identifies that performance stagnation across model sizes and types correlates with a lack of new high-performing Korean pretrained models, emphasizing their foundational role.
- Why unresolved: The paper does not specify which dimensions of pretrained model development (e.g., architecture, training data diversity, scale) would most effectively drive future gains.
- What evidence would resolve it: Comparative analysis of newly introduced pretrained models varying in architecture, data composition, and scale, measuring their impact on downstream task performance.

### Open Question 3
- Question: How do task performance correlations evolve as models transition from smaller to larger scales, and what does this reveal about capability integration?
- Basis in paper: [explicit] The paper observes that smaller models (under 3B parameters) show low or negative correlations between certain tasks, while larger models (7-14B parameters) exhibit stronger positive correlations across most tasks.
- Why unresolved: The paper does not investigate the mechanisms by which scaling affects capability integration or identify specific thresholds where correlation patterns shift.
- What evidence would resolve it: Detailed correlation analysis across incremental model sizes with task-specific capability assessments to identify integration breakpoints and contributing factors.

## Limitations

- The analysis is specific to Korean language models, limiting generalizability to other languages and cultural contexts
- The study relies on a single leaderboard's data, which may not capture the full landscape of LLM development across different research groups and commercial entities
- The analysis assumes that leaderboard performance accurately reflects real-world capabilities, though benchmark-specific optimizations could influence results

## Confidence

*High Confidence:* The observation that larger models (7-14B parameters) show stronger correlations across tasks is well-supported by the correlation analysis presented in Figure 2. The one-week lag between pretrained and instruction-tuned model improvements is directly observable from the temporal data.

*Medium Confidence:* The claim about task complexity determining saturation rates relies on interpretation of performance trends, which could be influenced by benchmark-specific factors rather than genuine task complexity differences.

*Low Confidence:* The assertion that pretrained model improvements are the foundational driver of all downstream performance gains, while supported by temporal patterns, may overstate the relationship given the complexity of modern LLM training pipelines.

## Next Checks

1. Replicate the correlation analysis using an independent Korean LLM evaluation dataset to verify whether the observed size-based correlation patterns hold across different evaluation contexts.

2. Conduct ablation studies by comparing models with similar architecture but different pretraining approaches to isolate the contribution of pretraining improvements to downstream performance.

3. Test the task complexity hypothesis by creating controlled variations of benchmark tasks with systematically varied complexity levels to establish more direct causal relationships between task properties and saturation patterns.