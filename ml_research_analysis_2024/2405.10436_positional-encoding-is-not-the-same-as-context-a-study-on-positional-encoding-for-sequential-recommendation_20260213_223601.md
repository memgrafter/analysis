---
ver: rpa2
title: 'Positional encoding is not the same as context: A study on positional encoding
  for sequential recommendation'
arxiv_id: '2405.10436'
source_url: https://arxiv.org/abs/2405.10436
tags:
- encoding
- leaky
- silu
- rotatory
- positional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how different positional encoding strategies
  affect the performance and stability of sequential recommendation systems. It distinguishes
  between temporal context and positional encodings, showing that positional encodings
  provide unique relational cues between items that temporal information alone cannot.
---

# Positional encoding is not the same as context: A study on positional encoding for sequential recommendation

## Quick Facts
- arXiv ID: 2405.10436
- Source URL: https://arxiv.org/abs/2405.10436
- Reference count: 40
- Primary result: Encoding choice significantly affects both performance and stability of sequential recommendation systems, with RMHA-4 offering superior stability and Rotatory excelling in low-deviation datasets.

## Executive Summary
This paper investigates how different positional encoding strategies affect the performance and stability of sequential recommendation systems. The authors distinguish between temporal context and positional encodings, demonstrating that positional encodings provide unique relational cues between items that temporal information alone cannot. Through experiments across eight Amazon datasets, they show that encoding effectiveness is driven more by dataset characteristics than sparsity, and that selecting the appropriate encoding is crucial for both performance and robustness. The study introduces novel encodings like Rotatory and its concatenated variants, revealing that concatenated versions generally offer greater stability.

## Method Summary
The authors analyze positional encoding strategies in the context of sequential recommendation using the CARCA model. They evaluate eight different encodings including absolute (fixed and learned), relative (RMHA-4), rotary, and concatenated variants. The encodings are integrated either through vector addition or concatenation with a linear layer. Experiments are conducted on eight Amazon datasets using standard metrics (Hit@10, NDCG) with stability measured through standard deviation across multiple random seeds. The study systematically varies the upper bound parameter to control sequence length and analyzes the impact on encoding performance.

## Key Results
- Encoding choice significantly affects both performance and stability, with effectiveness driven by dataset characteristics rather than sparsity
- RMHA-4 provides superior stability across runs, particularly beneficial for high-deviation datasets
- Rotatory encoding variants excel in low-deviation datasets, with concatenated versions offering additional stability
- Concatenated positional encodings (e.g., Learnt + Con, Rotatory + Con) demonstrate greater stability than their non-concatenated counterparts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Rotatory encodings improve stability by allowing the model to learn position-specific rotation angles rather than relying on fixed sinusoidal patterns.
- Mechanism: The Rotatory encoding applies a rotation matrix defined by learnable angles to each position's embedding. This rotation alternates sine and cosine terms with sign flips, enabling dynamic adjustment of positional significance during training.
- Core assumption: Learnable rotation angles can capture both absolute and relative positional relationships more flexibly than fixed sinusoidal encodings.
- Evidence anchors:
  - [abstract] "We introduce a novel rotatory-based encoding method...incorporates these rotations at the initial item embeddings, similar to sinusoidal encoding"
  - [section 3.2] "We propose a new encoding method termedRotatory...incorporates these rotations at the initial item embeddings, similar to sinusoidal encoding"
  - [corpus] Weak evidence - no direct comparisons with Rotatory in related works
- Break condition: If the learned angles converge to trivial values or fail to capture meaningful positional patterns, the encoding will not provide stability benefits.

### Mechanism 2
- Claim: RMHA-4 (Relative Multi-Head Attention with distance 4) provides superior stability by encoding relative positions within the attention mechanism rather than as separate vector additions.
- Mechanism: RMHA-4 modifies the attention computation to incorporate relative position information through matrix multiplication within each head, limiting relative index distance to 4 positions. This approach avoids the instability introduced by large positional vectors added at the input layer.
- Core assumption: Relative position encoding within attention heads is less sensitive to initialization and hyperparameter variations than absolute positional encodings.
- Evidence anchors:
  - [abstract] "Relative Positional Encoding Implementation (RMHA-4)...This encoding addresses the limitations of APE by encoding the relative positions within the attention mechanism"
  - [section 2.2] "The attention weight with RPE is computed as: [equation showing relative position addition]"
  - [corpus] No direct corpus evidence for RMHA-4 specifically, but general support for relative encodings in transformer architectures
- Break condition: If the relative position information becomes redundant with content-based attention or if the 4-position limit is too restrictive for the dataset's sequential patterns.

### Mechanism 3
- Claim: Concatenated positional encodings (Abs + Con, Learnt + Con) improve stability by separating semantic and positional information in the embedding space.
- Mechanism: Instead of adding positional vectors to item embeddings, concatenation creates a larger embedding that explicitly represents both semantic and positional information. A subsequent linear layer projects back to the original dimension, allowing the model to learn optimal mixing.
- Core assumption: Explicit separation of semantic and positional information reduces interference during training, leading to more stable optimization.
- Evidence anchors:
  - [section 3.1] "To further explore encoding strategies, we introduce an encoding variant that employs concatenation with a linear layer"
  - [section 5.1] "The concatenated versions of Abs, Learnt, and Rotatory demonstrate greater stability"
  - [corpus] Weak evidence - limited application of concatenation in SRS literature
- Break condition: If the additional linear transformation introduces unnecessary complexity or if the model fails to learn meaningful separation between semantic and positional components.

## Foundational Learning

- Concept: Positional encoding vs temporal context
  - Why needed here: The paper explicitly distinguishes between temporal footprint (timestamps) and positional encodings, showing they provide different information
  - Quick check question: Can you explain why knowing the absolute time of an interaction differs from knowing its position in a sequence?

- Concept: Transformer attention mechanism with positional encoding
  - Why needed here: Different encoding types (absolute vs relative) integrate into the attention mechanism differently, affecting stability and performance
  - Quick check question: How does adding positional information before the first transformer block differ from incorporating it within the attention computation?

- Concept: Dataset sparsity and its impact on model stability
  - Why needed here: The paper finds that encoding effectiveness correlates more with dataset deviation than sparsity, but sparsity still affects stability
  - Quick check question: Why might sparse datasets exhibit higher deviation in model performance across different runs?

## Architecture Onboarding

- Component map: User-item interaction sequences → Positional encoding layer → Transformer blocks with modified attention → Item prediction
- Critical path: Sequence → Positional encoding → Transformer attention → Item prediction
- Design tradeoffs:
  - Absolute encodings (Abs, Learnt): Simpler implementation but less stable across runs
  - Relative encodings (RMHA-4): More stable but computationally slower due to attention modifications
  - Concatenated encodings: Improved stability but increased model complexity
  - Learnable vs fixed: Learnable provides flexibility but may overfit; fixed provides consistency but less adaptability
- Failure signatures:
  - High deviation in Hit@10 or NDCG across runs indicates encoding instability
  - Training loss oscillation suggests poor encoding integration
  - Degraded performance when switching between datasets of different deviation characteristics
- First 3 experiments:
  1. Baseline: Run CARCA with no positional encoding on Men dataset, measure deviation across 10 seeds
  2. Stability test: Compare RMHA-4 vs Rotatory on Beauty dataset with 0.0001 upper bound, measure deviation
  3. Concatenation impact: Test Learnt + Con vs Learnt on Fashion dataset, compare stability and performance metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do positional encodings behave in datasets with different temporal characteristics, such as long vs. short user interaction sequences?
- Basis in paper: [inferred] The paper focuses on datasets with relatively short sequences and varying sparsity/deviation, but does not explore temporal sequence length effects.
- Why unresolved: The authors do not systematically vary sequence lengths or temporal characteristics to isolate their impact on encoding performance.
- What evidence would resolve it: Controlled experiments varying sequence length while keeping other factors constant, and comparing encoding effectiveness across these conditions.

### Open Question 2
- Question: Are there encoding-specific threshold values for deviation/sparsity beyond which a different encoding becomes optimal?
- Basis in paper: [inferred] The paper identifies that RMHA-4 works best for high-deviation datasets and Rotatory for low-deviation datasets, but doesn't define specific thresholds.
- Why unresolved: The authors provide general trends but don't quantify the exact deviation or sparsity values that trigger encoding switches.
- What evidence would resolve it: Empirical analysis mapping specific deviation/sparsity ranges to optimal encoding choices across multiple datasets.

### Open Question 3
- Question: How do positional encodings interact with different activation functions and normalization techniques in terms of stability and performance?
- Basis in paper: [explicit] The paper briefly tests leaky ReLU vs. sigmoid activation functions and notes minor stability differences, but doesn't systematically explore this interaction.
- Why unresolved: The authors only test two activation functions and don't explore other normalization methods or their combined effects with different encodings.
- What evidence would resolve it: Comprehensive ablation studies varying both activation functions and normalization techniques across multiple encoding types.

## Limitations
- The study is constrained by its focus on Amazon product datasets, which may not generalize to other recommendation domains with different sequential patterns
- The analysis primarily examines stability across random seeds rather than systematic hyperparameter sensitivity, potentially missing other sources of model instability
- While the paper introduces novel encodings like Rotatory, the theoretical analysis of why certain encodings perform better on specific datasets remains largely empirical rather than mechanistic

## Confidence

**High Confidence**: The empirical findings that encoding choice significantly affects both performance and stability are well-supported by extensive experiments across eight datasets. The distinction between temporal context and positional encoding effects is clearly demonstrated.

**Medium Confidence**: The superiority of specific encodings (RMHA-4 for stability, Rotatory for low-deviation datasets) is empirically validated but relies on dataset-specific characteristics that may not generalize universally.

**Low Confidence**: The theoretical justification for why concatenated encodings provide better stability lacks rigorous analysis. The claim that learnable rotation angles in Rotatory encoding capture more flexible positional relationships needs further theoretical grounding.

## Next Checks

1. **Cross-domain validation**: Test the proposed encodings on non-Amazon sequential recommendation datasets (e.g., movie watching, music streaming) to verify generalizability beyond e-commerce contexts.

2. **Hyperparameter sensitivity analysis**: Systematically vary learning rates, sequence lengths, and model dimensions to determine if encoding stability holds across different hyperparameter regimes.

3. **Theoretical analysis**: Develop mathematical proofs or formal analysis showing why RMHA-4 provides superior stability compared to absolute encodings, particularly in terms of gradient flow and optimization landscape.