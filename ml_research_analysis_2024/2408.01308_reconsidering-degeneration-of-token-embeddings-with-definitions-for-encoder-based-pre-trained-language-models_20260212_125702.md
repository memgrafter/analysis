---
ver: rpa2
title: Reconsidering Degeneration of Token Embeddings with Definitions for Encoder-based
  Pre-trained Language Models
arxiv_id: '2408.01308'
source_url: https://arxiv.org/abs/2408.01308
tags:
- tokens
- embeddings
- bart
- definitionemb
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the representation degeneration problem of token
  embeddings in encoder-based pre-trained language models (PLMs). It shows that although
  these models are robust against degeneration into a narrow cone shape during fine-tuning,
  their pre-trained embeddings still suffer from anisotropy and lack of semantics
  for rare tokens.
---

# Reconsidering Degeneration of Token Embeddings with Definitions for Encoder-based Pre-trained Language Models

## Quick Facts
- **arXiv ID**: 2408.01308
- **Source URL**: https://arxiv.org/abs/2408.01308
- **Reference count**: 40
- **Primary result**: DefinitionEMB reconstructs token embeddings using dictionary definitions, resulting in isotropically distributed, semantics-rich embeddings that improve downstream performance, especially for rare tokens.

## Executive Summary
This paper investigates the representation degeneration problem in token embeddings of encoder-based pre-trained language models (PLMs). While PLMs are robust against degeneration during fine-tuning, their pre-trained embeddings suffer from anisotropy and lack of semantics, particularly for rare tokens. The authors propose DefinitionEMB, a method that reconstructs embeddings by leveraging dictionary definitions in a denoising autoencoder framework. This approach produces isotropically distributed, semantics-rich embeddings that maintain robustness during fine-tuning and enhance performance on downstream tasks like GLUE and text summarization.

## Method Summary
The paper proposes DefinitionEMB to address the degeneration of token embeddings in encoder-based PLMs. The method uses dictionary definitions to reconstruct embeddings through a denoising autoencoder framework. It takes corrupted embeddings as input and learns to reconstruct clean embeddings conditioned on their definitions. This approach is architecture-agnostic and maintains robustness during fine-tuning. The reconstructed embeddings are isotropically distributed and semantically richer, particularly benefiting rare tokens. Experiments demonstrate improvements in downstream tasks compared to baselines like DelDirection and original PLMs.

## Key Results
- DefinitionEMB produces isotropically distributed, semantics-rich embeddings
- Maintains robustness against degeneration during fine-tuning
- Improves downstream performance on GLUE and text summarization tasks
- Particularly effective for rare tokens

## Why This Works (Mechanism)
The method works by leveraging external linguistic knowledge from dictionary definitions to enrich token representations. By reconstructing embeddings through a denoising autoencoder conditioned on definitions, it forces the model to capture semantic relationships that may be missing in the original pre-trained embeddings. The denoising process helps remove noise and anisotropy, resulting in more robust and semantically meaningful representations that maintain their quality during subsequent fine-tuning.

## Foundational Learning

1. **Representation Degeneration**
   - *Why needed*: Understanding how embeddings collapse into narrow distributions affects model performance
   - *Quick check*: Measure cosine similarity between random token pairs; values near 1 indicate degeneration

2. **Isotropic Embedding Distributions**
   - *Why needed*: Uniform angular distribution improves semantic discriminability
   - *Quick check*: Plot t-SNE visualizations of embeddings to verify spherical distribution

3. **Denoising Autoencoders**
   - *Why needed*: Framework for reconstructing clean representations from corrupted inputs
   - *Quick check*: Compare reconstruction loss with and without denoising objective

## Architecture Onboarding

**Component Map**: Dictionary Definitions -> Corruption Module -> Denoising Autoencoder -> Reconstructed Embeddings -> PLM Fine-tuning

**Critical Path**: The core innovation lies in using dictionary definitions as conditioning information for the denoising autoencoder. The model takes corrupted versions of pre-trained embeddings and learns to reconstruct clean versions while conditioning on their definitions. This reconstruction process is what creates the improved embeddings.

**Design Tradeoffs**: The method trades computational overhead during pre-training for improved downstream performance. It requires access to dictionary definitions, which may not be available for all languages or specialized vocabularies. The denoising autoencoder adds complexity but provides robustness benefits.

**Failure Signatures**: Poor reconstruction quality when definitions are ambiguous or missing, continued anisotropy in embedding distributions, failure to maintain robustness during fine-tuning, and limited improvement on downstream tasks.

**First Experiments**:
1. Visualize embedding distributions before and after DefinitionEMB application
2. Measure cosine similarity between random token pairs as degeneration metric
3. Compare downstream task performance with and without DefinitionEMB embeddings

## Open Questions the Paper Calls Out
None

## Limitations
- Limited experimental validation scope across diverse benchmarks and model architectures
- Lack of quantitative robustness metrics during fine-tuning
- Reliance on dictionary definitions assumes availability of high-quality lexical resources
- Does not explore multilingual or domain-specific applications

## Confidence
- **High confidence**: Identification of representation degeneration and denoising autoencoder framework for reconstruction
- **Medium confidence**: Claims about maintaining fine-tuning robustness without explicit robustness metrics
- **Medium confidence**: Experimental results showing performance improvements with limited task scope

## Next Checks
1. Evaluate DefinitionEMB on a broader range of downstream tasks, including multilingual and domain-specific benchmarks
2. Conduct ablation studies to isolate contributions of dictionary definitions versus denoising autoencoder framework
3. Measure fine-tuning robustness quantitatively by tracking embedding distribution changes and downstream performance across multiple fine-tuning iterations