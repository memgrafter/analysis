---
ver: rpa2
title: 'Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from
  Averaging to Automation'
arxiv_id: '2410.08371'
source_url: https://arxiv.org/abs/2410.08371
tags:
- merging
- merged
- methods
- language
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores model merging techniques for Large Language
  Models, comparing data-free methods like Model Soups and DARE-TIES with data-informed
  approaches like evolutionary merging. It introduces Differentiable Adaptive Merging
  (DAM), an efficient alternative that uses gradient-based optimization to learn scaling
  coefficients for merging model parameters, reducing computational overhead.
---

# Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation

## Quick Facts
- arXiv ID: 2410.08371
- Source URL: https://arxiv.org/abs/2410.08371
- Reference count: 7
- Differentiable Adaptive Merging (DAM) achieves competitive performance across multilingual and SQL tasks while reducing computational overhead compared to evolutionary merging methods

## Executive Summary
This paper introduces Differentiable Adaptive Merging (DAM), a gradient-based method for merging specialized Large Language Models without retraining. DAM learns per-column scaling coefficients for weight matrices through optimization on KL divergence loss, achieving efficient integration of distinct model capabilities. Experiments across Japanese, German, Korean, SQL, and mathematical reasoning tasks show DAM performs competitively with existing methods while requiring significantly less computation than evolutionary approaches. The study also reveals that simple averaging methods like Model Soups can surprisingly outperform complex merging techniques when models are sufficiently similar.

## Method Summary
DAM optimizes model merging by learning scaling coefficients for each column of weight matrices through gradient descent. The method minimizes KL divergence between merged and source model logits on representative datasets while applying regularization for diversity and sparsity. The approach uses aligned models (sharing base architecture) and trains scaling coefficients with learning rate 2e-3 and batch size 1, producing merged models that combine task-specific strengths without full retraining.

## Key Results
- DAM achieves 0.62 average score across Japanese tasks and 0.5497 across multilingual/SQL tasks
- Simple averaging (Model Soups) performs competitively when models are similar, challenging assumptions about complex merging necessity
- DAM reduces computational overhead compared to evolutionary merging while maintaining competitive performance
- KL divergence loss effectively aligns merged model predictions with source models on their respective datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DAM's column-wise scaling effectively balances task-specific representations by adjusting feature contributions in a layer-wise, data-informed manner.
- Mechanism: For each weight matrix in a layer, DAM learns per-column scaling coefficients that multiply the original weights. These coefficients are optimized via gradient descent on a KL-divergence loss between the merged model's logits and those of the individual models on their respective datasets.
- Core assumption: Alignment between models is preserved (they share the same base model), so merging can focus on scaling rather than re-aligning parameters.
- Evidence anchors:
  - [abstract] "DAM... optimizes model integration through scaling coefficients, minimizing computational demands."
  - [section] "The core idea of DAM is to find the optimal scaling coefficients for multi-task merging... ensuring that each column of the weight matrix is scaled individually."
  - [corpus] Weak - no direct support for column-wise scaling found in neighbors.
- Break condition: If models are not aligned (different base models or significantly different architectures), the per-column scaling will not produce coherent feature mixing.

### Mechanism 2
- Claim: KL divergence loss in DAM aligns the merged model's predictions with those of the source models, ensuring task-specific strengths are preserved.
- Mechanism: During DAM training, the merged model is evaluated on each source model's dataset. The KL divergence between the merged model's output distribution and the source model's output distribution on that dataset is minimized, pulling the merged model toward each source model's behavior on its specialty.
- Core assumption: The representative datasets accurately reflect the capabilities of the source models, so matching logits on these datasets ensures the merged model retains those capabilities.
- Evidence anchors:
  - [section] "The idea is to minimize the KL divergence between the logits of the merged model and the logits of each individual model on their respective datasets."
  - [corpus] Weak - no direct mention of KL divergence in merging neighbors.
- Break condition: If the datasets are not representative of the source models' capabilities, KL minimization may optimize for the wrong behaviors.

### Mechanism 3
- Claim: DAM's cosine similarity regularization encourages diverse scaling patterns across models, reducing interference and promoting complementary integration.
- Mechanism: A regularization term penalizes high cosine similarity between scaling coefficients of different models for the same layer. This forces each model to adjust features in unique ways rather than converging to similar patterns, helping preserve distinct strengths.
- Core assumption: Diverse scaling leads to better integration because it allows each model to contribute uniquely without overwriting others' representations.
- Evidence anchors:
  - [section] "We add a constraint to reduce the cosine similarity between the scaling coefficients of different models for each layer... promoting diversity in the merged model."
  - [corpus] Weak - no direct support for cosine regularization in merging found in neighbors.
- Break condition: If models are already very different, forcing diversity via cosine regularization may hurt performance by preventing beneficial feature sharing.

## Foundational Learning

- Concept: Gradient-based optimization for model merging
  - Why needed here: DAM uses differentiable optimization to learn scaling coefficients, unlike non-differentiable methods like evolutionary merging.
  - Quick check question: How does gradient descent in DAM differ from evolutionary strategies in terms of computational efficiency?

- Concept: KL divergence as a loss function for distribution alignment
  - Why needed here: DAM uses KL divergence to ensure the merged model's predictions match those of the source models on their respective datasets.
  - Quick check question: Why is KL divergence preferred over MSE for comparing probability distributions in this context?

- Concept: Regularization techniques (L1, L2, cosine similarity)
  - Why needed here: DAM applies multiple regularization terms to ensure stable, sparse, and diverse scaling coefficients during optimization.
  - Quick check question: What is the purpose of adding both L1 and L2 regularization to the scaling coefficients in DAM?

## Architecture Onboarding

- Component map:
  - Base models (aligned via shared parent) -> DAM trainer: learns per-column scaling coefficients -> Loss function: KL divergence + cosine similarity + L1/L2 regularization -> Datasets: representative samples from each model's training domain -> Merged model: final model with scaled weights

- Critical path:
  1. Prepare aligned models and representative datasets
  2. Initialize DAM scaling coefficients
  3. Optimize coefficients via gradient descent on KL + regularization loss
  4. Apply learned scaling to merge models into final weights

- Design tradeoffs:
  - DAM trades off some precision (compared to fine-grained evolutionary methods) for much lower computational cost
  - Requires aligned models; cannot handle unaligned architectures
  - Depends on quality of representative datasets for effective optimization

- Failure signatures:
  - Poor performance if datasets don't represent source models' strengths
  - Instability if regularization terms are not balanced
  - Ineffective merging if models are too dissimilar (even with alignment)

- First 3 experiments:
  1. Run DAM with only KL loss (no regularization) to establish baseline performance
  2. Add cosine similarity regularization to test impact on feature diversity
  3. Compare DAM against simple Model Soups on aligned models to validate efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does simple averaging (Model Soups) outperform more complex merging techniques like DAM or evolutionary merging?
- Basis in paper: [explicit] "Our findings underscore that simple methods, such as model averaging, can sometimes outperform more complex techniques, challenging established assumptions in the field and offering practical insights for researchers and practitioners."
- Why unresolved: The paper demonstrates that Model Soups performs competitively when model similarity is high, but does not specify the exact threshold or conditions under which this occurs.
- What evidence would resolve it: Controlled experiments varying degrees of model similarity and task alignment while measuring performance across different merging methods would clarify when simple averaging is sufficient versus when more sophisticated approaches are necessary.

### Open Question 2
- Question: How does DAM scale to larger model architectures and more diverse task combinations beyond the tested Japanese, German, Korean, SQL, and mathematical reasoning domains?
- Basis in paper: [inferred] The paper tests DAM on specific model families (Mistral, Llama 3) and task domains, but scalability to other architectures and broader task combinations is not explored.
- Why unresolved: While DAM shows promising results in tested scenarios, its effectiveness with significantly larger models or more heterogeneous task sets remains unverified.
- What evidence would resolve it: Experiments applying DAM to larger foundation models (e.g., 70B+ parameters) and testing on more diverse task combinations (vision-language tasks, audio processing, etc.) would demonstrate its broader applicability and scalability limits.

### Open Question 3
- Question: What is the optimal balance between the KL divergence loss, cosine similarity loss, and regularization terms in DAM's objective function for different types of merging scenarios?
- Basis in paper: [explicit] "Throughout the ablation studies, we used the same input models as Case Study 1 and assessed the merged modelâ€™s performance using the JP Language Model Evaluation Harness framework."
- Why unresolved: The paper shows that different loss function combinations affect performance, but does not provide a systematic method for determining optimal weighting across diverse merging scenarios.
- What evidence would resolve it: A comprehensive ablation study varying the relative weights of each loss component across multiple model combinations and task domains would establish guidelines for configuring DAM's objective function optimally in different contexts.

## Limitations
- Dataset representativeness is critical but not validated - DAM performance depends on whether representative samples accurately capture source models' capabilities
- Experiments limited to aligned models sharing same base architecture, leaving uncertainty about performance with unaligned architectures
- Computational efficiency claims lack comprehensive runtime analysis across different model scales

## Confidence
- **High confidence**: The core mechanism of DAM using gradient-based optimization to learn scaling coefficients is well-specified and theoretically sound. The comparison between DAM and simple averaging methods like Model Soups is methodologically rigorous.
- **Medium confidence**: The experimental results showing DAM's competitive performance across multiple languages and tasks are supported by the data, though the sample sizes for some benchmarks are relatively small. The claim that DAM achieves "competitive or superior" performance is reasonable but should be interpreted within the specific experimental conditions.
- **Low confidence**: The paper's assertion that DAM is universally more efficient than evolutionary methods lacks comprehensive runtime analysis. The effectiveness of the cosine similarity regularization in promoting diverse scaling patterns is demonstrated but not extensively validated across different model pairs.

## Next Checks
1. **Dataset Representativeness Validation**: Run ablation studies where DAM is trained on progressively less representative datasets (e.g., subsampled data, synthetic noise) to quantify how dataset quality affects the final merged model performance.

2. **Computational Efficiency Benchmark**: Measure wall-clock time and memory usage for DAM versus Model Soups and evolutionary merging across different model sizes (7B, 13B, 70B parameters) to validate the claimed efficiency advantages.

3. **Unaligned Model Merging Test**: Apply DAM to merge models with different base architectures (e.g., Llama + Mistral) and evaluate whether the per-column scaling approach still produces coherent feature mixing, or if alignment requirements are indeed strict.