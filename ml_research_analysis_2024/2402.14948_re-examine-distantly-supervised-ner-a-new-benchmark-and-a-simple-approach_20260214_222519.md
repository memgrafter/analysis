---
ver: rpa2
title: 'Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach'
arxiv_id: '2402.14948'
source_url: https://arxiv.org/abs/2402.14948
tags:
- learning
- curriculum
- cupul
- ds-ner
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper re-examines distantly supervised named entity recognition
  (DS-NER) using a real-life benchmark dataset, QTL, where training data is annotated
  via domain dictionaries and test data via expert annotation, with only a small validation
  set. Existing DS-NER methods fail on QTL due to reliance on large validation sets
  and noise biases from initial training on noisy labels.
---

# Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach

## Quick Facts
- arXiv ID: 2402.14948
- Source URL: https://arxiv.org/abs/2402.14948
- Reference count: 28
- Primary result: CuPUL achieves strict F1: 58.02%, relaxed F1: 70.24% on QTL benchmark

## Executive Summary
This paper re-examines distantly supervised named entity recognition (DS-NER) by introducing a new benchmark dataset, QTL, where training data is annotated via domain dictionaries and test data via expert annotation. The authors identify critical limitations in existing DS-NER methods, particularly their reliance on large validation sets and vulnerability to noise biases from noisy labels. To address these challenges, they propose Curriculum-based Positive-Unlabeled Learning (CuPUL), which combines token-level curriculum learning with Positive-Unlabeled (PU) learning to mitigate false negatives. CuPUL significantly outperforms state-of-the-art DS-NER methods on QTL and achieves strong results on established benchmarks, demonstrating both effectiveness and efficiency.

## Method Summary
The authors propose Curriculum-based Positive-Unlabeled Learning (CuPUL) to tackle the limitations of existing DS-NER approaches. CuPUL leverages token-level curriculum learning to order samples from easy to hard, helping the model gradually adapt to noisy labels. It also incorporates Positive-Unlabeled (PU) learning to address the issue of false negatives in distant supervision. By combining these two strategies, CuPUL improves the robustness and performance of DS-NER models, particularly in scenarios with limited validation data and high label noise.

## Key Results
- CuPUL achieves strict F1: 58.02% and relaxed F1: 70.24% on the QTL benchmark, significantly outperforming existing DS-NER methods.
- Ablation studies confirm the effectiveness of both curriculum learning and PU learning components in CuPUL.
- CuPUL demonstrates strong performance on established DS-NER benchmarks, showcasing its generalizability.

## Why This Works (Mechanism)
CuPUL works by addressing two key challenges in DS-NER: noise bias and false negatives. Token-level curriculum learning helps the model prioritize easier samples early in training, reducing the impact of noisy labels. PU learning mitigates false negatives by treating unlabeled tokens as potential positives, improving recall. Together, these mechanisms enable CuPUL to achieve superior performance in noisy, distantly supervised settings.

## Foundational Learning
- Distant Supervision: Automatically generates labeled data using external knowledge bases or dictionaries. Needed to scale NER to domains with limited labeled data. Quick check: Verify the quality and coverage of the dictionary used for annotation.
- Curriculum Learning: Orders training samples from easy to hard to improve model convergence. Needed to handle noisy labels effectively. Quick check: Assess the impact of sample ordering on model performance.
- Positive-Unlabeled Learning: Treats unlabeled data as potential positives to address false negatives. Needed to improve recall in DS-NER. Quick check: Evaluate the sensitivity of PU learning to the proportion of unlabeled data.

## Architecture Onboarding

Component Map:
- Token Encoder -> Curriculum Sampler -> PU Loss Function -> NER Decoder

Critical Path:
1. Token Encoder processes input text into embeddings.
2. Curriculum Sampler orders tokens based on difficulty.
3. PU Loss Function computes loss, treating unlabeled tokens as potential positives.
4. NER Decoder predicts entity labels.

Design Tradeoffs:
- Curriculum learning introduces computational overhead but improves robustness to noise.
- PU learning increases recall but may introduce false positives if not carefully tuned.

Failure Signatures:
- Poor performance on hard samples indicates ineffective curriculum learning.
- Low recall suggests insufficient handling of false negatives in PU learning.

First Experiments:
1. Evaluate CuPUL on a small subset of QTL to verify curriculum learning effectiveness.
2. Test PU learning in isolation to measure its impact on recall.
3. Combine curriculum and PU learning to assess their synergistic effects.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to non-biomedical domains (e.g., news, social media) is not thoroughly evaluated.
- Reliance on dictionary-based annotation for QTL may introduce biases not fully explored.
- Computational overhead of curriculum learning and its impact on scalability for larger datasets are not extensively analyzed.

## Confidence
- Major claims (effectiveness of CuPUL, superiority over baselines): High
- Limitations (generalizability, scalability, computational overhead): Medium

## Next Checks
1. Evaluate CuPUL on non-biomedical domains (e.g., news, social media) to assess generalizability.
2. Conduct scalability tests with larger datasets to measure computational overhead and efficiency.
3. Investigate the impact of dictionary quality and completeness on CuPUL's performance in real-world scenarios.