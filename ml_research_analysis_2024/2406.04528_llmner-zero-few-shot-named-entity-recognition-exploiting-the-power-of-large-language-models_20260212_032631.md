---
ver: rpa2
title: 'llmNER: (Zero|Few)-Shot Named Entity Recognition, Exploiting the Power of
  Large Language Models'
arxiv_id: '2406.04528'
source_url: https://arxiv.org/abs/2406.04528
tags:
- language
- entity
- learning
- prompting
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: llmNER is a Python library that enables zero-shot and few-shot
  named entity recognition using large language models. It simplifies prompt engineering
  and completion parsing by providing an easy-to-use interface that supports multiple
  prompting methods, answer parsers, and domain adaptation options.
---

# llmNER: (Zero|Few)-Shot Named Entity Recognition, Exploiting the Power of Large Language Models

## Quick Facts
- arXiv ID: 2406.04528
- Source URL: https://arxiv.org/abs/2406.04528
- Authors: Fabi√°n Villena; Luis Miranda; Claudio Aracena
- Reference count: 40
- Primary result: Python library for zero-shot/few-shot NER using LLMs, achieving strong but sub-SOTA performance

## Executive Summary
llmNER is a Python library that enables zero-shot and few-shot named entity recognition using large language models. It simplifies prompt engineering and completion parsing by providing an easy-to-use interface that supports multiple prompting methods, answer parsers, and domain adaptation options. The library was evaluated on English and Spanish NER benchmarks (CoNLL 2003 and 2002), using various LLMs including GPT 3.5 and Mixtral 8x7B. While results were below state-of-the-art fine-tuned models, llmNER achieved strong performance, particularly with GPT 3.5, and demonstrated the utility of its in-line answer format. The tool is designed to lower barriers to ICL research and has been applied in clinical NLP tasks.

## Method Summary
llmNER enables NER through in-context learning by providing prompt templates for different prompting methods (single-turn, multi-turn, step-by-step) and answer parsers (in-line tags, JSON format). The library compiles user-provided entity definitions and optional few-shot examples into prompts, sends them to LLM APIs, and parses completions into structured annotations. It supports both English and Spanish, with optional POS tagging augmentation. The library was evaluated on CoNLL 2003 (English) and CoNLL 2002 (Spanish) benchmarks using relaxed F1 score as the primary metric.

## Key Results
- GPT 3.5 consistently outperformed other tested models (Mixtral 8x7B, Llama 2 70B) across both languages and task types
- Multi-turn prompting showed advantages for zero-shot learning, particularly in English
- In-line answer formatting generally outperformed JSON formatting for NER tasks
- Performance remained below state-of-the-art fine-tuned models, especially for challenging entity types like MISC in Spanish

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning allows LLMs to perform NER without parameter updates by conditioning on prompt templates that describe entity types and provide examples.
- Mechanism: The model generates completions that follow the structured output format defined in the prompt (e.g., inline tags or JSON), and the library parses these completions into machine-readable annotations.
- Core assumption: LLMs trained on web-scale data have sufficient semantic understanding to infer entity boundaries and types from natural language descriptions alone.
- Evidence anchors:
  - [abstract] "llmNER is a Python library that enables zero-shot and few-shot named entity recognition using large language models. It simplifies prompt engineering and completion parsing..."
  - [section] "In-context learning (ICL) is a paradigm that allows language models to learn tasks given only a few examples in the form of demonstration."
  - [corpus] Weak - corpus shows related work but no direct ICL performance data for this library.
- Break condition: If the model fails to generate consistent output formats, parsing becomes unreliable and NER performance degrades sharply.

### Mechanism 2
- Claim: Multi-turn prompting improves zero-shot NER performance by decomposing the task into simpler subproblems and exploiting the model's chain-of-thought reasoning ability.
- Mechanism: The model is prompted to annotate one entity type at a time, with intermediate results stored and aggregated into the final annotation set.
- Core assumption: LLMs exhibit emergent reasoning abilities that improve task performance when complex tasks are broken into sequential sub-tasks.
- Evidence anchors:
  - [section] "In this prompting method, the model is asked to annotate one entity at a time [19], exploiting the chain-of-thought following ability of LLMs..."
  - [section] "For the English dataset, the best-performing method was the multi-turn for zero-shot learning, a result that Xie et al. [19] also reported."
  - [corpus] Moderate - related work exists but specific multi-turn ablation studies are not provided.
- Break condition: When entity mentions overlap or have multiple possible labels, sequential annotation may miss valid entities or introduce conflicts.

### Mechanism 3
- Claim: In-line answer formatting generally outperforms JSON formatting for NER tasks because it provides explicit boundary markers within the text context.
- Mechanism: The model echoes the input text with entity mentions enclosed in tags (e.g., `<PER>John</PER>`), making span detection straightforward for the parser.
- Core assumption: Models can reliably maintain text structure while inserting inline annotations, and parsers can accurately extract spans from these markers.
- Evidence anchors:
  - [section] "In this answer shape, the model is prompted to echo the exact input text, but with the entity mentions enclosed with in-line tags..."
  - [section] "Considering the answer shape, our proposed in-line answer shape generally outperforms JSON..."
  - [corpus] Weak - no comparative parsing accuracy data for inline vs JSON formats.
- Break condition: If the model generates malformed tags or fails to preserve exact text boundaries, span extraction will fail or produce incorrect results.

## Foundational Learning

- Concept: Named Entity Recognition (NER)
  - Why needed here: Understanding what NER is and how it differs from other NLP tasks is essential for configuring the library correctly and interpreting results.
  - Quick check question: What are the four most common entity types mentioned in the paper, and why might this list be extended?

- Concept: In-Context Learning (ICL)
  - Why needed here: ICL is the core paradigm this library exploits; knowing how it works helps in designing effective prompts and understanding performance limitations.
  - Quick check question: How does ICL differ from traditional fine-tuning, and what are its main advantages and limitations for NER?

- Concept: Prompt Engineering
  - Why needed here: The library's effectiveness depends heavily on prompt design; understanding different prompting methods (single-turn, multi-turn, step-by-step) is crucial for optimization.
  - Quick check question: What are the three main prompting methods implemented in llmNER, and how might you choose between them for a new dataset?

## Architecture Onboarding

- Component map:
  - Entity definitions (user input) -> Few-shot examples (optional user input) -> Prompt template compiler (selects prompting method) -> LLM API client (sends prompt, receives completion) -> Answer parser (converts completion to annotations) -> POS tagger (optional augmentation, can be function or LLM-based)

- Critical path:
  1. User provides entities and optional examples
  2. Prompt template is compiled using selected method
  3. Prompt is sent to LLM API
  4. Completion is parsed into annotation objects
  5. Annotations are returned to user

- Design tradeoffs:
  - Single-turn vs multi-turn: Simpler implementation vs potentially better performance on complex entities
  - In-line vs JSON output: Easier parsing vs more structured data
  - Function-based vs LLM-based POS tagging: Faster and cheaper vs potentially more contextually aware

- Failure signatures:
  - Inconsistent tag placement or malformed JSON indicates model comprehension issues
  - Missing entities suggests insufficient context or poor prompt design
  - Incorrect spans typically result from model hallucination or boundary detection failure
  - Slow performance may indicate API rate limits or inefficient parallelization

- First 3 experiments:
  1. Run zero-shot NER on a simple English sentence with basic entity definitions to verify basic functionality
  2. Compare single-turn vs multi-turn prompting on the same input to observe performance differences
  3. Test the library with Spanish text using CoNLL 2002 entity definitions to verify multilingual support

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of llmNER vary across different entity types (e.g., LOC, ORG, PER) and languages, and what factors contribute to these differences?
- Basis in paper: [explicit] The paper compares the performance of llmNER on CoNLL 2003 (English) and CoNLL 2002 (Spanish) NER tasks, showing varying results across entity types and languages.
- Why unresolved: The paper provides a comparative analysis but does not delve into the underlying reasons for performance variations across entity types and languages.
- What evidence would resolve it: Further analysis of the linguistic characteristics of different entity types and languages, as well as the training data of the LLMs used, could provide insights into the factors contributing to performance differences.

### Open Question 2
- Question: What is the impact of different prompting methods and answer parsers on the performance of llmNER for various NER tasks?
- Basis in paper: [explicit] The paper evaluates llmNER using different prompting methods (single-turn, multi-turn) and answer parsers (in-line, JSON) and reports varying results.
- Why unresolved: The paper provides a comparison of different methods and parsers but does not explore the underlying reasons for their varying effectiveness.
- What evidence would resolve it: Further research on the impact of different prompting methods and answer parsers on the model's understanding of the task and its ability to generate accurate outputs could provide insights into their effectiveness.

### Open Question 3
- Question: How can llmNER be further improved to achieve performance comparable to state-of-the-art fine-tuned models?
- Basis in paper: [explicit] The paper acknowledges that the performance of llmNER is below that of state-of-the-art fine-tuned models and suggests that further research is needed to improve the results of the in-context learning paradigm.
- Why unresolved: The paper does not provide specific strategies or directions for improving the performance of llmNER to match or exceed that of fine-tuned models.
- What evidence would resolve it: Further research on advanced prompt engineering techniques, model architectures, and training strategies could provide insights into how to improve the performance of llmNER and bridge the gap with fine-tuned models.

## Limitations
- Performance remains below state-of-the-art fine-tuned models, particularly for challenging entity types
- Limited evaluation scope to only English and Spanish languages on two benchmark datasets
- Dependency on external LLM APIs introduces variability and potential reproducibility issues

## Confidence

**High Confidence:**
- The library's basic functionality and API design work as described
- GPT 3.5 consistently outperforms other tested models across both languages and task types
- Multi-turn prompting shows advantages for zero-shot learning, particularly in English

**Medium Confidence:**
- The claimed advantage of in-line answer formatting over JSON formatting for NER tasks
- The effectiveness of the library for clinical NLP applications
- The assertion that llmNER lowers barriers to ICL research based on its ease of use

**Low Confidence:**
- The generalizability of results to other languages beyond English and Spanish
- The scalability of the approach for very large documents or datasets
- The long-term reliability given the dependency on external LLM services

## Next Checks
1. **Cross-Lingual Generalization Test**: Evaluate llmNER on additional languages (e.g., German, French, Chinese) using available NER benchmarks to assess the library's multilingual capabilities beyond the reported English and Spanish results.

2. **Domain Transfer Evaluation**: Test the library on clinical text or other specialized domains not covered in the original evaluation to verify the claimed utility for domain-specific NER tasks and assess performance degradation outside general web text.

3. **Comparative Prompting Study**: Conduct an ablation study comparing all three prompting methods (single-turn, multi-turn, step-by-step) across multiple entity types and difficulty levels to quantify the specific advantages of multi-turn prompting and identify optimal use cases for each method.