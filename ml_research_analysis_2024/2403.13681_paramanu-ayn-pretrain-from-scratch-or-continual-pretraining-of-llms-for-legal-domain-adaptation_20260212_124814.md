---
ver: rpa2
title: 'PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for Legal
  Domain Adaptation?'
arxiv_id: '2403.13681'
source_url: https://arxiv.org/abs/2403.13681
tags:
- legal
- case
- court
- rights
- article
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PARAMANU-AYN, a collection of 97-million-parameter
  autoregressive legal language models trained from scratch on Indian Supreme Court
  case documents. The model was trained on a single GPU for 185 hours, achieving 41.35%
  MFU.
---

# PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for Legal Domain Adaptation?

## Quick Facts
- arXiv ID: 2403.13681
- Source URL: https://arxiv.org/abs/2403.13681
- Reference count: 40
- Key outcome: 97M-parameter legal language model trained from scratch outperforms much larger LLMs on legal tasks while being more cost-effective and environmentally friendly

## Executive Summary
PARAMANU-AYN is a collection of 97-million-parameter autoregressive legal language models trained from scratch on Indian Supreme Court case documents. The model was trained on a single GPU for 185 hours, achieving 41.35% MFU. It uses a legal-specific BPE tokenizer and rotary position embeddings. Evaluations show PARAMANU-AYN outperforms Llama-2 7B and Gemini-Pro in legal case judgment prediction (2% higher accuracy) and abstractive summarization (10%+ higher BLEU/METEOR). It also performs well on commonsense and mathematical reasoning benchmarks despite being trained exclusively on legal data. The authors conclude that domain-specific pretraining from scratch is more cost-effective and environmentally friendly than adapting large language models for legal tasks.

## Method Summary
The authors pretrain a 97M parameter transformer decoder from scratch on a corpus of 97M tokens from Indian Supreme Court cases (1947-2023), Constitution of India, and Indian Penal Code. They use a legal domain specialized BPE tokenizer with 15,575 vocabulary size, rotary position embeddings with shrinking factor for higher context size, and grouped-query attention with RMSNorm. The model is trained for 185 hours on a single A100 GPU with batch size 8, gradient accumulation 8, and cosine learning rate schedule. After pretraining, the model is instruction-tuned on 10,763 diverse legal tasks covering legal reasoning, judgment explanation, legal clause generation, and legal drafting.

## Key Results
- Outperformed Llama-2 7B and Gemini-Pro in case judgment prediction with explanation task by nearly 2 percentage points on test accuracy
- Achieved 10%+ higher BLEU and METEOR scores on abstractive summarization compared to larger LLMs
- Trained for only 185 A100 hours and consumed only 0.0196 tCO2eq, making it more environmentally friendly than continual pretraining approaches

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific pretraining from scratch on legal case documents is more cost-effective and environmentally friendly than continual pretraining of large language models (LLMs). By pretraining a small autoregressive generative legal language model exclusively on Indian Supreme Court case documents, the model learns legal domain knowledge and achieves competitive or better performance than much larger LLMs trained on general data. The core assumption is that high-quality, domain-specific pretraining data can enable a smaller model to achieve performance comparable to or better than much larger general-domain models.

### Mechanism 2
Using a legal domain specialized Byte-Pair Encoding (BPE) tokenizer improves the model's ability to handle legal terminology and improves performance on legal tasks. By training a BPE tokenizer from scratch on the legal domain corpus, the tokenizer learns the intricacies of legal terminology and generates a more effective vocabulary for the legal domain. The core assumption is that a domain-specific tokenizer can better capture the unique characteristics of the domain's language and improve the model's performance on domain-specific tasks.

### Mechanism 3
Instruction-tuning the pretrained legal model on diverse legal tasks enables it to generalize and perform well on a wide range of legal tasks. By fine-tuning the pretrained legal model on a large set of diverse legal instructions covering various tasks such as legal reasoning, judgment explanation, legal clause generation, legal drafting, etc., the model learns to apply its legal domain knowledge to different tasks. The core assumption is that instruction-tuning a pretrained model on diverse tasks can improve its ability to generalize and perform well on unseen tasks within the same domain.

## Foundational Learning

- Concept: Language Modeling and Perplexity
  - Why needed here: Understanding the objective of language modeling and the evaluation metric of perplexity is crucial for comprehending the pretraining process and assessing the model's performance.
  - Quick check question: What is the relationship between the cross-entropy loss and perplexity in language modeling?

- Concept: Transformer Architecture and Positional Embeddings
  - Why needed here: Familiarity with the transformer architecture and positional embeddings is essential for understanding the model's design and the role of rotary position embeddings in PARAMANU-AYN.
  - Quick check question: How do rotary position embeddings differ from traditional positional embeddings in transformers?

- Concept: Tokenization and Byte-Pair Encoding (BPE)
  - Why needed here: Knowledge of tokenization techniques and BPE is necessary for understanding the role of the legal domain specialized tokenizer in PARAMANU-AYN.
  - Quick check question: How does BPE tokenization work, and why is it commonly used in language modeling?

## Architecture Onboarding

- Component map: Legal case documents → Legal domain specialized BPE tokenizer → Transformer Decoder model → Instruction-tuning on diverse legal tasks → Evaluation on legal tasks
- Critical path: Legal case documents → Legal domain specialized BPE tokenizer → Transformer Decoder model → Instruction-tuning on diverse legal tasks → Evaluation on legal tasks
- Design tradeoffs:
  - Model size vs. performance: Smaller models require less computational resources but may have lower performance compared to larger models.
  - Pretraining data size vs. domain specificity: Larger pretraining data may improve general performance but may reduce domain specificity.
  - Tokenizer size vs. domain coverage: Larger tokenizer vocabularies can better capture domain-specific terminology but may increase computational complexity.
- Failure signatures:
  - Low perplexity on validation data indicates poor pretraining performance.
  - Poor performance on legal tasks compared to larger LLMs suggests insufficient domain knowledge acquisition.
  - Inability to generalize to unseen legal tasks after instruction-tuning indicates overfitting or insufficient task diversity in the instruction-tuning dataset.
- First 3 experiments:
  1. Pretrain PARAMANU-AYN on the legal domain corpus and evaluate perplexity on a held-out validation set.
  2. Evaluate PARAMANU-AYN on the case judgment prediction with explanation task and compare performance to Llama-2 7B and Gemini-Pro.
  3. Instruction-tune PARAMANU-AYN on the diverse legal instruction dataset and evaluate performance on legal contract drafting and legal clause generation tasks.

## Open Questions the Paper Calls Out

The paper explicitly acknowledges that its model is "exclusive to Indian Supreme Court jurisdiction" and notes this as a limitation, raising questions about how the approach generalizes to other legal systems. The authors also highlight the trade-off between training exclusively on domain-specific data versus including general knowledge, noting that despite being pretrained only on legal documents, PARAMANU-AYN performed well on commonsense and mathematical reasoning benchmarks.

## Limitations

- Data provenance and quality: The paper does not provide details about the exact data sources for the Indian Supreme Court case documents, Constitution of India, and Indian Penal Code.
- Hyperparameter sensitivity: The sensitivity of the model's performance to hyperparameters is not explored.
- Generalizability beyond Indian legal domain: The model's performance on legal tasks from other jurisdictions or legal systems is not assessed.

## Confidence

- High confidence: Technical specifications of the PARAMANU-AYN model architecture (number of parameters, layers, dimensions)
- Medium confidence: Reported performance on legal tasks (case judgment prediction, abstractive summarization)
- Low confidence: Claim that domain-specific pretraining from scratch is more cost-effective and environmentally friendly than continual pretraining of LLMs

## Next Checks

1. Reproduce the tokenizer: Train a BPE tokenizer with a vocabulary size of 15,575 on the legal domain corpus and evaluate its ability to capture legal terminology and improve perplexity on held-out legal text.

2. Evaluate on out-of-domain legal tasks: Assess the performance of PARAMANU-AYN on legal tasks from different jurisdictions or legal systems to evaluate its generalizability beyond the Indian legal domain.

3. Compare with continual pretraining: Conduct a direct comparison of domain-specific pretraining from scratch and continual pretraining of LLMs in terms of performance, cost, and environmental impact on the same set of legal tasks.