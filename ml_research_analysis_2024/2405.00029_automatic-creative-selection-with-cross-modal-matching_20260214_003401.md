---
ver: rpa2
title: Automatic Creative Selection with Cross-Modal Matching
arxiv_id: '2405.00029'
source_url: https://arxiv.org/abs/2405.00029
tags:
- search
- image
- apple
- clip
- cross-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of matching application images
  to search terms for App developers to promote their apps. The authors propose a
  novel approach based on fine-tuning a pre-trained LXMERT model for this cross-modal
  matching task.
---

# Automatic Creative Selection with Cross-Modal Matching

## Quick Facts
- arXiv ID: 2405.00029
- Source URL: https://arxiv.org/abs/2405.00029
- Reference count: 14
- Authors: Alex Kim; Jia Huang; Rob Monarch; Jerry Kwac; Anikesh Kamath; Parmeshwar Khurd; Kailash Thiyagarajan; Goodman Gu

## Executive Summary
This paper addresses the problem of matching application images to search terms for App developers to promote their apps. The authors propose a novel approach based on fine-tuning a pre-trained LXMERT model for this cross-modal matching task. Their method significantly improves matching accuracy compared to baselines like CLIP and transformer+ResNet models. On two evaluation datasets - developer intuitions and professional annotator intuitions - the proposed approach achieves 0.96 and 0.95 AUC scores respectively, outperforming the transformer+ResNet baseline by 8-16% and fine-tuned CLIP by 14-17%. The results demonstrate the effectiveness of using a cross-modal model framework for image-text matching in the domain of applications and search phrases.

## Method Summary
The proposed method uses a pre-trained LXMERT model fine-tuned on an in-house training dataset of (search phrase, ad image, label) triples. The approach employs mid-fusion where independent transformers are applied to the textual and visual modalities, then a cross-modal encoder is applied. This differs from early-fusion baselines that concatenate features. The model is trained to perform binary classification of relevance between search phrases and images, with outputs passed through a sigmoid activation function for final prediction.

## Key Results
- Achieves 0.96 and 0.95 AUC scores on developer intuitions and professional annotator intuitions datasets respectively
- Outperforms transformer+ResNet baseline by 8-16% improvement in AUC scores
- Outperforms fine-tuned CLIP by 14-17% improvement in AUC scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal encoder fusion outperforms early-fusion and zero-shot baselines
- Mechanism: The LXMERT model's cross-modal encoder performs mid-fusion by independently encoding text and image modalities with dedicated transformers before combining them. This allows the model to learn deeper relationships between modalities compared to early-fusion baselines (XLM-R + ResNet) that simply concatenate features, and outperforms zero-shot CLIP that lacks cross-modal encoder layers.
- Core assumption: The relationship between app images and search phrases requires complex cross-modal reasoning beyond simple feature matching
- Evidence anchors:
  - [abstract] states "our approach uses mid-fusion where independent transformers are applied to the textual and the visual modality, then a cross-modal encoder is applied, showing the effectiveness of the cross-modal encoder in identifying the relationship between modalities"
  - [section] mentions "While both baselines use early-fusion, where text and image features are concatenated as input sequence, our approach uses mid-fusion"
  - [corpus] shows CLIP variants are discussed in related work but don't achieve the same performance
- Break condition: If the search phrase semantics are too simple or if images are highly descriptive captions, early-fusion might perform similarly

### Mechanism 2
- Claim: Fine-tuning on in-domain (search phrase, ad image) pairs improves performance
- Mechanism: The pre-trained LXMERT model is fine-tuned on an in-house training dataset of (search phrase, ad image, label) triples. This domain-specific adaptation allows the model to learn the particular characteristics of app promotion contexts where images are creative assets and search phrases are user discovery queries, rather than general image-text pairs.
- Core assumption: The distribution of app promotion images and search phrases differs significantly from general image-text pairs used in pre-training
- Evidence anchors:
  - [abstract] states "We customizes a cross-modal BERT framework by fine-tuning a pre-trained cross-modal model on an in-house training dataset of (search phrase, ad image, label)"
  - [section] shows "We then fine-tune the pre-trained model on our training dataset"
  - [corpus] evidence is weak - no direct mention of in-domain fine-tuning benefits in neighbors
- Break condition: If the in-house training data is too small or not representative of the test distribution

### Mechanism 3
- Claim: 1:n mapping capability better matches app promotion use case
- Mechanism: Unlike CLIP which uses contrastive loss assuming 1:1 mapping between text and image, this approach handles 1:n mapping where one search phrase can be relevant to multiple images and vice versa. This is more appropriate for app promotion where developers create multiple creative assets for the same search terms.
- Core assumption: App promotion scenarios inherently involve many-to-many relationships between images and search phrases
- Evidence anchors:
  - [abstract] states "CLIP uses a contrastive loss assuming a 1:1 mapping between text and image, so our lift here is likely due to our 1:n mapping"
  - [section] mentions "CLIP uses a contrastive loss assuming a 1:1 mapping between text and image, so our lift here is likely due to our 1:n mapping"
  - [corpus] evidence is weak - neighbors don't discuss 1:1 vs 1:n mapping implications
- Break condition: If the use case were simplified to 1:1 mapping (e.g., image captioning), this advantage would diminish

## Foundational Learning

- Concept: Cross-modal representation learning
  - Why needed here: The task requires understanding relationships between visual and textual modalities, which is fundamentally a cross-modal problem
  - Quick check question: What is the difference between early-fusion and mid-fusion in cross-modal architectures?

- Concept: Pre-training and fine-tuning paradigms
  - Why needed here: The approach leverages a pre-trained model (LXMERT) and adapts it to a specific domain, requiring understanding of transfer learning principles
  - Quick check question: Why is fine-tuning on domain-specific data often more effective than training from scratch for specialized tasks?

- Concept: Contrastive learning and loss functions
  - Why needed here: Understanding why CLIP's contrastive loss (designed for 1:1 mapping) may not be optimal for this 1:n matching task
  - Quick check question: How does contrastive loss work in models like CLIP, and what are its limitations for many-to-many matching scenarios?

## Architecture Onboarding

- Component map: Search phrases -> WordPiece tokenizer -> Language encoder -> Word embeddings; Images -> Faster R-CNN -> Object detection -> Bounding boxes and features -> Object-relationship encoder -> Cross-modal encoder -> Linear layer + GELU activation + Layer normalization + Linear layer + Sigmoid -> Binary classification
- Critical path: Text/image encoding → Cross-modal fusion → Classification prediction
- Design tradeoffs: Mid-fusion (independent modality encoding + cross-modal fusion) vs early-fusion (concatenated features) - mid-fusion allows more complex cross-modal reasoning but adds computational overhead
- Failure signatures: Poor performance on either text or image modality suggests issues with respective encoders; overall poor performance suggests cross-modal fusion problems
- First 3 experiments:
  1. Test the base LXMERT model on a small subset of the data to establish baseline performance
  2. Compare early-fusion baseline (concatenated features) vs the proposed mid-fusion approach
  3. Evaluate the impact of fine-tuning duration and learning rate on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed model perform when evaluated on datasets with a larger variety of search phrases and application images?
- Basis in paper: [inferred] The paper evaluates the model on two specific datasets - developer intuitions and professional annotator intuitions. It does not mention testing on datasets with a wider range of search phrases and images.
- Why unresolved: The paper does not provide information on how the model generalizes to more diverse datasets. This is an important aspect to consider for real-world applications where the variety of search phrases and images can be much larger.
- What evidence would resolve it: Testing the model on a dataset with a larger variety of search phrases and application images and comparing the performance with the current results would provide insights into the model's generalization capabilities.

### Open Question 2
- Question: How does the model's performance change when using different cross-modal encoder architectures?
- Basis in paper: [explicit] The paper mentions that their approach uses a Transformer-based cross-modal encoder. However, it does not explore the impact of using different architectures.
- Why unresolved: The paper does not provide a comparison of the model's performance using different cross-modal encoder architectures. This could potentially lead to improvements in the model's performance.
- What evidence would resolve it: Experimenting with different cross-modal encoder architectures and comparing their performance with the current model would provide insights into the impact of the encoder architecture on the model's performance.

### Open Question 3
- Question: How does the model handle out-of-distribution data, such as search phrases or images that are significantly different from the training data?
- Basis in paper: [inferred] The paper does not mention any experiments or results related to the model's performance on out-of-distribution data.
- Why unresolved: Understanding the model's robustness to out-of-distribution data is crucial for real-world applications where the input data can vary significantly from the training data.
- What evidence would resolve it: Testing the model on out-of-distribution data and analyzing its performance would provide insights into the model's robustness and generalization capabilities.

## Limitations

- The evidence supporting the in-domain fine-tuning benefits and 1:n mapping advantages is weak, relying primarily on the paper's own claims without external validation
- The paper lacks detailed information about the in-house training dataset, making it difficult to assess whether the domain shift is significant enough to warrant adaptation
- No ablation studies are provided to isolate the effects of each architectural choice, making it difficult to definitively attribute performance gains to specific components

## Confidence

- Mechanism 1 (Cross-modal encoder): Medium-High - supported by direct paper evidence but lacks external validation
- Mechanism 2 (In-domain fine-tuning): Low-Medium - claimed but not independently verified
- Mechanism 3 (1:n mapping): Low-Medium - theoretical justification but no empirical comparison with modified 1:n CLIP variants

## Next Checks

1. **Dataset analysis**: Obtain statistics on the in-house training dataset (sample size, class balance, domain coverage) to assess whether the fine-tuning mechanism has sufficient data support and whether the domain shift is significant enough to warrant adaptation.

2. **Ablation study**: Conduct controlled experiments removing the cross-modal encoder layers to quantify their exact contribution versus simpler early-fusion approaches, isolating the architectural impact from other factors like training data differences.

3. **Modified baseline comparison**: Implement a version of CLIP with 1:n mapping capabilities (e.g., by using multiple positive samples per query) to determine whether the performance gap is truly due to architectural differences or could be bridged with baseline modifications.