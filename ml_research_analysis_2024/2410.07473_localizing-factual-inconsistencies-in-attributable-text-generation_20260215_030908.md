---
ver: rpa2
title: Localizing Factual Inconsistencies in Attributable Text Generation
arxiv_id: '2410.07473'
source_url: https://arxiv.org/abs/2410.07473
tags:
- linguistics
- association
- computational
- text
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QASemConsistency, a new methodology for localizing
  factual inconsistencies in attributable text generation. The core idea is to decompose
  generated text into minimal predicate-argument level propositions, represented as
  simple question-answer (QA) pairs, and assess whether each individual QA pair is
  supported by a trusted reference text.
---

# Localizing Factual Inconsistencies in Attributable Text Generation

## Quick Facts
- arXiv ID: 2410.07473
- Source URL: https://arxiv.org/abs/2410.07473
- Reference count: 40
- Primary result: Introduces QASemConsistency methodology that decomposes generated text into predicate-argument QA pairs for precise factual inconsistency localization

## Executive Summary
This paper addresses the challenge of localizing factual inconsistencies in attributable text generation by introducing QASemConsistency, a novel methodology that decomposes generated text into minimal predicate-argument level propositions expressed as simple question-answer (QA) pairs. The core innovation is the ability to assess whether each individual QA pair is supported by a trusted reference text, enabling granular identification of unsupported information. The authors demonstrate the effectiveness of their approach through human annotation studies with high inter-annotator agreement and automatic detection methods using both supervised entailment models and large language models.

## Method Summary
The QASemConsistency methodology consists of three main steps: (1) decomposing generated text into predicate-argument level propositions using a QA parser trained on QASRL and QANom datasets, (2) optionally verifying entity mentions against the reference text to filter irrelevant QA pairs, and (3) classifying each QA pair as supported or not supported by the reference text using entailment models or LLM prompting. The approach generates QA pairs covering 94% of content words in generated text and achieves high inter-annotator agreement (>0.7 Fleiss Kappa) through crowdsourcing. For automatic detection, the authors implement methods using supervised NLI classifiers (DeBERTa-NLI, TRUE) and LLM prompting with various models including Gemma, Mistral, Mixtral, and Llama.

## Key Results
- QASemConsistency achieves high inter-annotator agreement (>0.7 Fleiss Kappa) on human annotation tasks
- The methodology yields factual consistency scores that correlate well with human judgments
- Automatic detection methods show promising results, with F1 scores of 0.45 for unsupported QA detection using DeBERTa-NLI

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing text into predicate-argument QA pairs enables precise localization of factual inconsistencies
- Mechanism: Each QA pair represents a single semantic relation, allowing granular assessment of whether that specific relation is supported by reference text
- Core assumption: Minimal propositions expressed as natural language QA pairs are both interpretable by humans and processable by entailment models
- Evidence anchors: [abstract] decomposition into minimal predicate-argument propositions expressed as QA pairs; [section 3] each QA pair corresponds to single predicate-argument relation

### Mechanism 2
- Claim: High inter-annotator agreement is achieved through decomposition into atomic semantic relations
- Mechanism: Annotators evaluate one predicate-argument relation at a time rather than entire sentences, reducing cognitive load and ambiguity
- Core assumption: Atomic semantic relations are less ambiguous than larger text units, leading to more consistent human judgments
- Evidence anchors: [section 4.2] high inter-annotator agreement (>0.7 Fleiss Kappa) and decomposition into atomic semantic relations

### Mechanism 3
- Claim: Standard entailment models can effectively handle QA-based hypotheses despite being trained on sentence-level data
- Mechanism: QA pairs are natural language expressions that retain sufficient context for models to reason about entailment relationships
- Core assumption: Models trained on diverse entailment datasets can generalize from sentence-level to predicate-argument level representations
- Evidence anchors: [section 5.3] models can effectively generalize to QA hypotheses; [section 5.3] models trained on large text can handle natural language QA expressions

## Foundational Learning

- Concept: Predicate-argument relations and Neo-Davidsonian semantics
  - Why needed here: Understanding how individual semantic relations can be isolated and evaluated independently
  - Quick check question: Can you explain why "Who died? A man" and "How someone died? From measles" represent separate semantic relations that can be independently supported or unsupported?

- Concept: Question-answering based semantic role labeling (QA-SRL)
  - Why needed here: QA-SRL provides the framework for representing predicate-argument relations as simple QA pairs
  - Quick check question: How does QA-SRL differ from traditional semantic role labeling approaches in terms of granularity and interpretability?

- Concept: Textual entailment and NLI models
  - Why needed here: The core task is determining whether each QA pair is entailed by the reference text
  - Quick check question: What are the three possible labels in textual entailment, and how are they treated in this work?

## Architecture Onboarding

- Component map: QA generation -> QA filtering -> Entity verification -> QA entailment classification -> Score aggregation
- Critical path: Generated text -> QASem parser -> (Entity verification) -> QA entailment model -> Supported/not supported labels -> Factual consistency score
- Design tradeoffs: Fine-grained QA decomposition vs. computational overhead; manual annotation efficiency vs. complete coverage; model generalization vs. specialized training
- Failure signatures: Low inter-annotator agreement suggests QA pairs are too ambiguous; poor model performance suggests entailment models struggle with QA format; inconsistent scores suggest entity verification issues
- First 3 experiments:
  1. Run QASem parser on a sample generated text and manually verify QA quality and relevance
  2. Test entailment model on a small set of QA pairs with known ground truth to establish baseline performance
  3. Compare entity verification approach against full QA evaluation to measure efficiency gains

## Open Questions the Paper Calls Out

#### Open Question 1
- Question: How can the QASemConsistency methodology be extended to handle implicit or inter-sentence discourse relations in factual inconsistency detection?
- Basis in paper: [inferred] The paper acknowledges limitations in capturing implicit or inter-sentence discourse relations, such as the inability to distinguish between "John missed the train and arrived late" and "John arrived late and missed the train" (page 17)
- Why unresolved: The current methodology focuses on predicate-argument level propositions and does not account for implicit or inter-sentence discourse relations
- What evidence would resolve it: Future research could explore incorporating additional semantic decompositions, such as QADiscourse (Pyatkin et al., 2020) or QA-Adj (Pesahov et al., 2023), when improved parsers become available (page 17)

#### Open Question 2
- Question: Can the QASemConsistency methodology be adapted to handle factual inconsistencies in text generation tasks beyond summarization and biography generation?
- Basis in paper: [inferred] The paper evaluates QASemConsistency on XSUM summarization and biography generation tasks, but does not explore its applicability to other text generation tasks (page 6)
- Why unresolved: The paper does not provide evidence or discussion on the generalizability of QASemConsistency to other text generation tasks
- What evidence would resolve it: Future work could apply QASemConsistency to a diverse range of text generation tasks and evaluate its effectiveness in localizing factual inconsistencies across different domains and task types

#### Open Question 3
- Question: How can the performance of automatic detectors of localized factual inconsistencies be improved, particularly in terms of recall?
- Basis in paper: [explicit] The paper reports that automatic detectors, such as Gemma v2 (9B), achieve high precision but low recall in identifying unsupported QAs (page 9)
- Why unresolved: The paper suggests that LLMs may be overconfident when generating a "Yes" response to indicate that a QA is supported, leading to low recall (page 9)
- What evidence would resolve it: Future research could investigate techniques to address overconfidence in LLMs, such as adjusting the decision threshold or incorporating additional training data to improve recall without sacrificing precision

## Limitations
- Dependency on QA generation quality, with no evaluation of whether QA pairs accurately capture intended semantic relations
- Entity verification approach may miss factual inconsistencies involving entities not mentioned in reference text
- Heavy reliance on human annotation for establishing ground truth, which may introduce bias and limits scale

## Confidence
- Mechanism 1 (QA decomposition for localization): High confidence
- Mechanism 2 (High inter-annotator agreement through atomic relations): Medium confidence
- Mechanism 3 (Entailment model generalization): Medium confidence

## Next Checks
1. **QA Pair Coverage Analysis**: Conduct a detailed error analysis of the QASem parser by manually reviewing 100 randomly sampled generated texts to assess coverage, accuracy, and systematic misses.

2. **Cross-task Consistency Evaluation**: Test the QASemConsistency methodology on a different text generation task (e.g., dialogue generation or creative writing) to evaluate generalizability beyond summarization, data-to-text, and long-form QA.

3. **Entity Verification vs. Full QA Evaluation**: Conduct a controlled experiment comparing the entity verification approach against evaluating all generated QA pairs to measure the trade-off between computational efficiency and detection accuracy.