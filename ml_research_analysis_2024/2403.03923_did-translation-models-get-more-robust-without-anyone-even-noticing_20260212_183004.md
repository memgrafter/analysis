---
ver: rpa2
title: Did Translation Models Get More Robust Without Anyone Even Noticing?
arxiv_id: '2403.03923'
source_url: https://arxiv.org/abs/2403.03923
tags:
- opus
- noise
- nllb
- gpt-3
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Translation models were believed to be highly sensitive to noisy
  inputs, but recent large multilingual models and LLMs are significantly more robust
  to synthetic and social media text noise than earlier models, even when achieving
  similar performance on clean data. This robustness increase is not due to architectural
  changes, but rather to training on larger and more diverse datasets.
---

# Did Translation Models Get More Robust Without Anyone Even Noticing?

## Quick Facts
- arXiv ID: 2403.03923
- Source URL: https://arxiv.org/abs/2403.03923
- Reference count: 22
- Recent large multilingual models and LLMs are significantly more robust to synthetic and social media text noise than earlier models, even when achieving similar performance on clean data.

## Executive Summary
Translation models were believed to be highly sensitive to noisy inputs, but recent large multilingual models and LLMs are significantly more robust to synthetic and social media text noise than earlier models, even when achieving similar performance on clean data. This robustness increase is not due to architectural changes, but rather to training on larger and more diverse datasets. Synthetic noise experiments show that larger models exhibit much flatter performance declines across various noise types and language pairs. Social media translation experiments confirm that LLMs also handle real-world noise better. Fine-tuning on noisy data and source correction pipelines further improve robustness, enabling smaller models to surpass GPT-3.5 on most synthetic noise benchmarks.

## Method Summary
The study investigates translation model robustness using synthetic character-level perturbations (swap, dupe, drop, key) at 10 noise levels (10%-100%) on FLORES-200 devtest data across 20 language pairs. Models tested include OPUS transformer encoder-decoders, NLLB-3.3B, TowerInstruct-v0.1 (13B), and GPT-3.5. Robustness is measured via COMET-slope (linear regression of COMET decline vs noise level). Real-world noise is tested using MTNT (Reddit posts) and MultiLexNorm (social media text) datasets. Mitigation techniques include finetuning on noisy MTNT data and source correction using ByT5-Small. Performance is evaluated with COMET, BLEU, chrF, faux-BLEU, and ∆QE metrics.

## Key Results
- Larger models (NLLB-3.3B, TowerInstruct) show much flatter COMET-slope curves across all synthetic noise types, indicating superior robustness.
- Model size and multilinguality are not the only factors—NLLB-1.3B and M2M-1.2B handle noise differently despite similar sizes, suggesting training data diversity drives robustness.
- Finetuning on noisy data and source correction pipelines enable smaller models to surpass GPT-3.5 on most synthetic noise benchmarks.
- Models robust to synthetic errors perform better at translating social media text, validating synthetic noise as a proxy for real-world noise.
- On MTNT, finetuning on noisy data improves robustness for OPUS models, but source correction pipelines show no overall improvement despite helping on some examples.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger models and training sets improve robustness to synthetic noise without architectural changes.
- Mechanism: Larger models trained on more diverse data implicitly learn robustness patterns, such as better handling of noisy token sequences and improved generalization across noise types.
- Core assumption: The improvement is due to data diversity and model scale, not specific architectural features.
- Evidence anchors:
  - [abstract] "This robustness increase is not due to architectural changes, but rather to training on larger and more diverse datasets."
  - [section 3.2] "we next investigate the impact of architecture on performance... This suggests that the robustness of recent models is due to their training data, not their size or architecture."
- Break condition: If models are trained on datasets that lack noisy or diverse text, robustness gains may not materialize.

### Mechanism 2
- Claim: Robustness to synthetic noise correlates with better performance on real-world social media text.
- Mechanism: Synthetic noise experiments act as a proxy for real-world noise, allowing controlled measurement of model robustness that transfers to social media translation.
- Core assumption: Synthetic noise patterns are representative of real-world noise types encountered in social media text.
- Evidence anchors:
  - [abstract] "Social media translation experiments confirm that LLMs also handle real-world noise better."
  - [section 4.1] "we show that models that are robust to synthetic errors perform better at translating social media text."
- Break condition: If synthetic noise patterns differ significantly from actual social media noise, the correlation may break.

### Mechanism 3
- Claim: Fine-tuning on noisy data and source correction pipelines can make smaller models more robust than GPT-3.5 on synthetic noise benchmarks.
- Mechanism: Additional training on noisy examples or preprocessing to correct source errors can compensate for smaller model capacity and achieve high robustness.
- Core assumption: Both noisy fine-tuning and source correction provide complementary benefits that can overcome size limitations.
- Evidence anchors:
  - [abstract] "Fine-tuning on noisy data and source correction pipelines further improve robustness, enabling smaller models to surpass GPT-3.5 on most synthetic noise benchmarks."
  - [section 5.1] "we showed that MT finetuning and SC are both effective techniques for improving robustness to synthetic errors."
- Break condition: If noisy fine-tuning or source correction introduces significant degradation on clean data, the approach may not be viable.

## Foundational Learning

- Concept: Synthetic noise as a controlled proxy for real-world noise.
  - Why needed here: Allows isolation and measurement of robustness effects that are difficult to quantify in naturally noisy data.
  - Quick check question: Can you explain why synthetic noise experiments are necessary before testing on social media text?

- Concept: Corpus-level vs. sentence-level evaluation metrics.
  - Why needed here: Corpus-level metrics show overall trends, while sentence-level metrics reveal how often mitigation techniques help or harm individual examples.
  - Quick check question: What is the difference between COMET-slope and faux-BLEU in terms of what they measure?

- Concept: Tokenization robustness and its impact on model performance.
  - Why needed here: Noise affects tokenization, which in turn affects how models process and translate text, especially for subword-level models.
  - Quick check question: How does noise-induced tokenization change affect translation quality?

## Architecture Onboarding

- Component map: Model (encoder-decoder or decoder-only) → Tokenizer → Noise injection → Translation → Evaluation (COMET, BLEU, chrF)
- Critical path: Noise injection → Model translation → Evaluation metric computation
- Design tradeoffs: Larger models are more robust but expensive; smaller models can be made robust through fine-tuning or correction pipelines but may sacrifice clean data performance
- Failure signatures: High COMET-slope values indicate poor robustness; significant performance drop on synthetic noise but good performance on clean data
- First 3 experiments:
  1. Replicate synthetic noise experiments with a small model to establish baseline robustness.
  2. Fine-tune the small model on noisy data and compare robustness to the baseline.
  3. Apply source correction pipeline to the small model and evaluate robustness improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do robustness improvements generalize to low-resource languages not covered in the study?
- Basis in paper: [inferred] The authors acknowledge that all studied languages have large speaker populations and many resources available, and note that some conclusions may not generalize to low-resource languages.
- Why unresolved: The study only examined high-resource languages, leaving a gap in understanding whether robustness trends hold for languages with limited training data.
- What evidence would resolve it: Systematic experiments testing robustness on low-resource language pairs with similar noise types and comparing performance across model sizes and architectures.

### Open Question 2
- Question: What specific aspects of the training data (e.g., domain diversity, data quality, preprocessing) drive robustness improvements in large models?
- Basis in paper: [inferred] The authors show that model size and multilinguality are not the only factors affecting robustness, as evidenced by NLLB-1.3B and M2M-1.2B handling noise differently despite similar sizes. They conclude that robustness is due to training data, not size or architecture.
- Why unresolved: While the paper establishes that training data is key, it doesn't isolate which data characteristics (domain coverage, data quantity, quality, or preprocessing methods) contribute most to robustness.
- What evidence would resolve it: Controlled experiments varying training data characteristics while holding model architecture constant, measuring robustness changes.

### Open Question 3
- Question: How effective are routing mechanisms for deciding when to apply source correction pipelines?
- Basis in paper: [explicit] The authors show that while correction doesn't improve overall results on MTNT, the oracle consistently outperforms the baseline by about 0.5 COMET, illustrating that many sequences do benefit from correction. They suggest future work could investigate routing mechanisms.
- Why unresolved: The paper only demonstrates that some examples benefit from correction but doesn't explore methods for automatically identifying these examples or implementing routing decisions.
- What evidence would resolve it: Development and evaluation of routing models or heuristics that predict when source correction will improve translation quality, tested on noisy translation benchmarks.

### Open Question 4
- Question: How do robustness improvements transfer to other types of "noise" beyond spelling errors and social media text?
- Basis in paper: [explicit] The authors note that the study only examines one source of natural noise (social media text) and acknowledge that other varieties of perceived noise (e.g., transcribed speech, non-fluent text) may have different properties.
- Why unresolved: The experiments are limited to synthetic character perturbations and social media text, leaving uncertainty about performance on other real-world noise types.
- What evidence would resolve it: Experiments testing robustness on diverse noise sources like speech transcriptions, OCR errors, or non-native speaker text, comparing performance across model types.

### Open Question 5
- Question: What is the relationship between tokenizer robustness and overall model robustness to character-level noise?
- Basis in paper: [explicit] The authors show that perturbations affect tokenization, with OPUS tokenizers showing larger fertility increases than other models. They note that higher fertility (closer to byte-level) results in noisy token sequences that are much closer to clean sequences for TI and GPT-3.5.
- Why unresolved: While the paper demonstrates that tokenization changes with noise, it doesn't establish whether tokenizer robustness directly causes or correlates with model-level robustness, or whether improvements in tokenization alone would enhance robustness.
- What evidence would resolve it: Experiments comparing models with different tokenizers on the same architecture, and ablation studies isolating the contribution of tokenization from other model components to overall robustness.

## Limitations

- The synthetic noise perturbations (swap, dupe, drop, key) may not fully capture the complexity of real-world noise patterns found in social media or other domains.
- Social media experiments are limited to Reddit-style text, potentially missing noise patterns from other domains like SMS, chat applications, or technical documentation.
- The study only examines high-resource languages, leaving uncertainty about whether robustness trends generalize to low-resource languages.

## Confidence

- **High confidence** in the finding that modern LLMs and large multilingual models are more robust to synthetic noise than earlier MT models.
- **Medium confidence** in the claim that this robustness increase is primarily due to larger, more diverse training datasets rather than architectural changes.
- **Medium confidence** in the transferability of synthetic noise robustness to real-world social media translation.

## Next Checks

1. Test additional noise types and domains: Evaluate model robustness on noise patterns not included in the original synthetic experiments (e.g., grammatical errors, emoji insertion, code-switching) and on real-world text from additional domains beyond social media.

2. Analyze error types in social media data: Conduct detailed error analysis on MTNT and MultiLexNorm datasets to quantify the prevalence of different noise types and measure which synthetic noise patterns best correlate with actual performance degradation.

3. Evaluate robustness-accuracy tradeoffs: Systematically measure the relationship between clean data performance and robustness to noisy inputs across the model spectrum to test whether robustness gains come at the cost of reduced clean data performance.