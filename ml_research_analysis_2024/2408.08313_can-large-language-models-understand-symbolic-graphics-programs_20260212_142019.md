---
ver: rpa2
title: Can Large Language Models Understand Symbolic Graphics Programs?
arxiv_id: '2408.08313'
source_url: https://arxiv.org/abs/2408.08313
tags:
- style
- fill
- path
- line
- circle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel benchmark and evaluation framework
  for assessing large language models'' (LLMs) ability to understand and reason about
  symbolic graphics programs. The authors propose a domain well-suited for testing
  spatial-semantic reasoning skills: symbolic graphics programs that procedurally
  generate visual data.'
---

# Can Large Language Models Understand Symbolic Graphics Programs?

## Quick Facts
- arXiv ID: 2408.08313
- Source URL: https://arxiv.org/abs/2408.08313
- Reference count: 40
- Key outcome: LLMs struggle with symbolic graphics program understanding, achieving <70% accuracy on SVG and <75% on CAD benchmarks, but Symbolic Instruction Tuning improves both symbolic and general reasoning capabilities

## Executive Summary
This paper introduces a novel benchmark and evaluation framework for assessing large language models' ability to understand and reason about symbolic graphics programs. The authors propose using symbolic graphics programs (SVG for 2D vector graphics and CAD for 2D/3D objects) as a testbed for spatial-semantic reasoning skills, constructing a comprehensive benchmark called SGP-Bench. Results show that while LLMs exhibit strong general program synthesis and analysis skills, understanding symbolic graphics programs remains challenging. To address this gap, the authors introduce Symbolic Instruction Tuning (SIT), which collects instruction-following data from rendered images and fine-tunes open-source LLMs, improving both symbolic program understanding and general reasoning ability.

## Method Summary
The paper evaluates LLMs on symbolic graphics programs by constructing multiple-choice questions about rendered images and providing only the program text as input, isolating semantic understanding from vision encoders. For improvement, SIT collects instruction-following data by rendering symbolic programs, querying GPT-4o for detailed captions, and finetuning open-source LLMs on this paired data. The evaluation includes consistency tests using SE(2) perturbations (random translations and rotations) to distinguish true semantic understanding from memorization.

## Key Results
- LLMs achieve below 70% accuracy on SVG and below 75% accuracy on CAD symbolic program understanding tasks
- SIT improves both symbolic program understanding and general reasoning across various benchmarks
- Consistency tests reveal that some LLMs rely on surface memorization rather than deep semantic understanding when answering questions about perturbed programs

## Why This Works (Mechanism)

### Mechanism 1
The evaluation pipeline works because symbolic graphics programs offer deterministic mapping to rendered images, allowing semantic understanding to be assessed without vision encoders. Symbolic programs procedurally generate visual content, and understanding the program is equivalent to understanding the rendered output semantics.

### Mechanism 2
Symbolic Instruction Tuning improves symbolic program understanding by bridging the semantic-symbolic gap through paired rendered image captions and generating programs. This teaches LLMs to map symbolic descriptions to semantic meanings using vision-language model supervision.

### Mechanism 3
The semantic consistency benchmark works because random SE(2) transformations change all numeric values in SVG code but preserve rendered semantics, testing whether LLMs understand semantics at a structural rather than surface level.

## Foundational Learning

- Program synthesis and analysis by LLMs: Understanding that LLMs can generate and analyze generic programs is essential since symbolic graphics programs extend this to visual domains. Quick check: Can an LLM correctly generate a Python program that draws a red circle given a natural language description?

- Visual reasoning without vision encoders: The benchmark evaluates whether LLMs can infer visual semantics from symbolic code alone, bypassing traditional vision-language pipelines. Quick check: Given an SVG path description, can an LLM predict the shape it will render?

- Instruction tuning and finetuning methodologies: SIT uses supervised finetuning on paired symbolic-semantic data; understanding standard instruction tuning is essential to grasp SIT's innovation. Quick check: What is the difference between supervised finetuning and reinforcement learning from human feedback in LLM adaptation?

## Architecture Onboarding

- Component map: Symbolic graphics programs (SVG/CAD) -> Rendering engine -> Images -> Vision-language model (GPT-4o) -> Semantic captions -> Instruction dataset -> LLM finetuning (SIT); Symbolic graphics programs (SVG/CAD) -> Question generation -> Multiple-choice questions -> LLM evaluation; Symbolic graphics programs (SVG/CAD) -> SE(2) perturbation -> Consistency evaluation

- Critical path: 1. Generate symbolic graphics programs (SVG/CAD). 2. Render programs to images. 3. Query GPT-4o for semantic captions (SIT data) or construct multiple-choice questions (benchmark). 4. Fine-tune open-source LLMs with SIT data. 5. Evaluate LLMs on SGP-Bench with or without perturbations.

- Design tradeoffs: Using rendered images for SIT vs. direct symbolic instruction (rich semantic context vs. manual annotation); Multiple-choice questions vs. open-ended (automated evaluation vs. limited complexity); SE(2) perturbations vs. no perturbations (deep understanding testing vs. simpler evaluation).

- Failure signatures: SIT degrades LLM's general instruction-following ability; LLM-based answer extraction fails due to format inconsistency after SIT; Consistency scores drop under SE(2) perturbations; Performance on SGP-MNIST remains at chance level.

- First 3 experiments: 1. Evaluate baseline LLM on SGP-Bench without finetuning. 2. Apply SIT with 10K instruction pairs and evaluate on SGP-Bench. 3. Test semantic consistency by applying SE(2) perturbations to SVG programs and evaluating LLM answers.

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural or training modifications could enable LLMs to better understand symbolic graphics programs that represent handwritten digits? The paper demonstrates that even powerful LLMs like GPT-4o struggle significantly with SGP-MNIST, achieving only chance-level performance, while humans can easily recognize the rendered images.

### Open Question 2
How do different symbolic graphics program representations (SVG vs CAD) impact LLM performance, and what underlying factors contribute to these differences? The paper shows that LLMs perform worse on SVG (below 70% accuracy) compared to CAD (below 75% accuracy), despite SVG being 2D while CAD includes 3D.

### Open Question 3
What is the relationship between symbolic instruction tuning (SIT) performance improvements and the model's ability to generalize to unseen symbolic graphics programs? The paper shows SIT improves both symbolic program understanding and general reasoning but doesn't investigate whether this improvement generalizes to novel programs not seen during training.

## Limitations

- Performance ceilings below 75% indicate fundamental challenges in LLM understanding of symbolic graphics programs remain unsolved
- Reliance on GPT-4o for caption generation in SIT may introduce distributional mismatches affecting transfer to other domains
- The benchmark may not fully capture the complexity of real-world visual reasoning tasks beyond structured symbolic graphics

## Confidence

- High: The benchmark construction methodology and evaluation pipeline are sound and reproducible
- Medium: Claims about SIT improving general reasoning ability are supported but could benefit from more extensive ablation studies
- Medium: The assertion that symbolic graphics programs offer unique advantages over existing visual reasoning benchmarks is compelling but requires broader empirical validation

## Next Checks

1. **Distributional Robustness Test**: Evaluate SIT-finetuned models on symbolic programs with significantly different visual styles or complexity levels than those used in training to assess generalization boundaries.

2. **Ablation Study on SIT Components**: Systematically remove either the vision-language model captions or the fine-tuning step to quantify their individual contributions to performance improvements.

3. **Human Evaluation Comparison**: Conduct human studies comparing semantic understanding of symbolic programs to determine whether LLM performance ceilings reflect inherent task difficulty or model limitations.