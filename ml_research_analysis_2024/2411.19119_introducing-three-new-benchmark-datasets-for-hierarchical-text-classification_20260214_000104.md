---
ver: rpa2
title: Introducing Three New Benchmark Datasets for Hierarchical Text Classification
arxiv_id: '2411.19119'
source_url: https://arxiv.org/abs/2411.19119
tags:
- classes
- documents
- classification
- class
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces three new benchmark datasets for hierarchical
  text classification (HTC) in the domain of research publications. The datasets comprise
  titles and abstracts from the Web of Science publication database and use different
  classification schemas: journal-based, citation-based, and a filtered combination
  of both.'
---

# Introducing Three New Benchmark Datasets for Hierarchical Text Classification

## Quick Facts
- arXiv ID: 2411.19119
- Source URL: https://arxiv.org/abs/2411.19119
- Reference count: 8
- New benchmark datasets for hierarchical text classification in research publications domain

## Executive Summary
This paper introduces three new benchmark datasets for hierarchical text classification (HTC) in the domain of research publications. The datasets comprise titles and abstracts from the Web of Science publication database and use different classification schemas: journal-based, citation-based, and a filtered combination of both. The filtered combination approach improves classification reliability by removing documents and categories that don't have clear mappings between the two schemas. The datasets are more balanced than existing benchmarks, with equal sampling across second-level classes. Classification experiments using four state-of-the-art HTC approaches demonstrate improved performance on the filtered dataset compared to the journal-based dataset, suggesting more accurate classifications.

## Method Summary
The authors create three datasets (WOSJT, WOSCT, WOSJTF) by sampling 5,000 papers per second-level citation-based class from Web of Science, then filtering to remove ambiguous document-class mappings. They use Sentence-BERT embeddings to analyze semantic similarity within classes through cosine similarity and silhouette scores. Four state-of-the-art HTC models (HPTD-ELECTRA, HPTD-DeBERTaV3, GHLARoBERTa, HPT) are trained on the datasets with standard architectures and hyperparameter tuning from original papers. Performance is evaluated using Micro-F1 and Macro-F1 scores, with the filtered dataset showing superior results.

## Key Results
- WOSJTF dataset improves classification accuracy by filtering documents lacking clear overlap between journal-based and citation-based classifications
- Datasets are more balanced than existing benchmarks, with equal sampling across second-level classes (1000 for JTF, 200 for CT)
- Clustering-based analysis shows WOSJTF produces more semantically coherent document clusters than journal-based or citation-based datasets alone
- Classification experiments demonstrate improved performance on WOSJTF compared to WOSJT, suggesting more accurate classifications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The JTF dataset improves classification accuracy by filtering documents and classes that lack clear overlap between journal-based (JT) and citation-based (CT) classifications.
- Mechanism: Documents are only retained if their co-occurrence mapping between JT and CT categories meets a threshold ratio (γ=1.5), ensuring that retained classes have strong semantic alignment across both schemas.
- Core assumption: Documents that appear in both JT and CT categories are more likely to be correctly classified, and removing ambiguous cases improves model learning.
- Evidence anchors:
  - [section] "We use the co-occurrence matrix to find the most relevant JTL2 classes for each CTL2,i class... We choose a threshold of γ = 1.5, such that a CTL2 class is only mapped to a JTL2 class if the highest co-occurrence count for the particular CTL2 class is less than 1.5 times the co-occurrence count of the two classes."
  - [abstract] "Due to the respective shortcomings of these two existing schemas, we propose an approach which combines their classifications to improve the reliability and robustness of the dataset."
- Break condition: If the filtering threshold is too strict, useful documents may be excluded, reducing dataset size and diversity.

### Mechanism 2
- Claim: Balancing the dataset at the second level of the hierarchy improves model performance on machine learning tasks.
- Mechanism: By sampling an equal number of documents per second-level class (1000 for JTF, 200 for CT), the dataset avoids class imbalance, which can bias models toward majority classes.
- Core assumption: Equal representation of classes prevents the model from overfitting to frequent categories and allows it to learn discriminative features for minority classes.
- Evidence anchors:
  - [section] "Our datasets are unique among benchmark HTC datasets since we sample documents equally for each of the classes in the second level of the hierarchy."
  - [abstract] "Our datasets are significantly more balanced than the currently used benchmark HTC datasets and therefore better suited for machine learning-based classification approaches."
- Break condition: If the dataset is too small after balancing, the model may underfit due to insufficient training examples per class.

### Mechanism 3
- Claim: The clustering-based semantic similarity analysis validates that the JTF dataset produces more cohesive class clusters than JT or CT alone.
- Mechanism: Sentence-BERT embeddings are used to compute intra-class cosine similarity and silhouette scores, measuring how tightly documents within a class cluster and how well-separated different clusters are.
- Core assumption: Higher intra-class similarity and silhouette scores indicate better class assignments and dataset quality.
- Evidence anchors:
  - [section] "We evaluate the quality of the three datasets by analysing the semantic similarity between the documents belonging to each of the classes (or clusters)... The violin plots in Figure 8 gives the cosine similarity distributions over the classes in the first and second level for the three datasets."
  - [abstract] "We evaluate the three created datasets with a clustering-based analysis and show that our proposed approach results in a higher quality dataset where documents that belong to the same class are semantically more similar compared to the other datasets."
- Break condition: If semantic similarity does not correlate with classification accuracy, the clustering analysis may not be a reliable quality indicator.

## Foundational Learning

- Concept: Hierarchical text classification (HTC)
  - Why needed here: The paper introduces new benchmark datasets specifically for HTC tasks, which require understanding of hierarchical class structures and multi-label classification.
  - Quick check question: What distinguishes HTC from flat multi-label classification?

- Concept: Citation-based vs. journal-based classification schemas
  - Why needed here: The paper compares these two schemas and proposes a filtered combination to improve classification reliability.
  - Quick check question: What are the main limitations of journal-based classification according to the paper?

- Concept: Clustering and semantic similarity analysis
  - Why needed here: The authors use clustering-based analysis to evaluate dataset quality by measuring document cohesion within classes.
  - Quick check question: How does the silhouette score quantify cluster separation?

## Architecture Onboarding

- Component map: Dataset creation (sampling and filtering) -> Semantic embedding generation (Sentence-BERT) -> Clustering analysis (cosine similarity, silhouette scores) -> Classification experiments (four HTC models)
- Critical path: Dataset creation → semantic embedding generation → clustering analysis → classification experiments → baseline comparison
- Design tradeoffs: Balancing datasets improves fairness but may reduce total sample size; filtering improves quality but risks excluding valid documents
- Failure signatures: Low silhouette scores indicate overlapping clusters; high variance in classification results suggests instability in model training
- First 3 experiments:
  1. Generate Sentence-BERT embeddings for a subset of documents and verify cosine similarity distributions within classes
  2. Apply the filtering threshold (γ=1.5) on a small co-occurrence matrix and inspect retained vs. removed mappings
  3. Train a simple HTC model (e.g., GHLARoBERTa) on the JTF dataset and measure Micro-F1/Macro-F1 on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the new datasets perform with other machine learning models beyond the four state-of-the-art HTC approaches tested?
- Basis in paper: [explicit] The paper concludes by providing baseline results for four HTC approaches and invites future research, implying potential for testing with additional models.
- Why unresolved: The paper only evaluates four specific HTC approaches, leaving the performance of other models untested.
- What evidence would resolve it: Experimental results using a variety of other machine learning models on the new datasets.

### Open Question 2
- Question: What are the long-term impacts of using the filtered dataset (WOSJTF) on classification accuracy as the dataset size increases?
- Basis in paper: [inferred] The paper shows improved classification performance with the filtered dataset, suggesting potential scalability, but does not explore long-term impacts with larger datasets.
- Why unresolved: The paper does not investigate the performance of the filtered dataset as the dataset size scales.
- What evidence would resolve it: Longitudinal studies showing classification accuracy trends as more data is added to the filtered dataset.

### Open Question 3
- Question: How do the classification schemas handle emerging fields or interdisciplinary research that may not fit neatly into existing categories?
- Basis in paper: [explicit] The paper discusses the limitations of existing classification schemas and proposes a combined approach to improve classification, but does not address how new or interdisciplinary fields are managed.
- Why unresolved: The paper does not provide solutions for dynamically updating classification schemas to include new or interdisciplinary fields.
- What evidence would resolve it: Case studies or examples of how the classification schemas adapt to new fields or interdisciplinary research over time.

## Limitations

- Dataset accessibility: The Web of Science data requires institutional access and may not be reproducible by independent researchers
- Filtering threshold sensitivity: The strict filtering approach may exclude valuable documents, reducing dataset diversity
- Domain specificity: The paper focuses on research publications only, limiting generalizability to other domains

## Confidence

- Dataset creation methodology: **High** - The procedure is clearly described with specific thresholds and sampling strategies
- Classification performance improvements: **Medium** - Results show improvements but are limited to four specific models and one domain
- Clustering-based quality assessment: **Medium** - The methodology is sound but the correlation between semantic similarity and classification accuracy is assumed rather than proven

## Next Checks

1. **Reproducibility test**: Attempt to replicate the WOSJTF dataset creation using publicly available citation and journal classification schemas to verify the filtering methodology works outside the Web of Science environment
2. **Generalization study**: Apply the classification models to a different domain (e.g., news articles or medical literature) using the same hierarchical structure to assess domain transfer capability
3. **Alternative embedding evaluation**: Compare Sentence-BERT embeddings with other semantic representations (e.g., PubMedBERT for scientific text or domain-specific embeddings) to determine if embedding choice affects the clustering quality assessment and classification results