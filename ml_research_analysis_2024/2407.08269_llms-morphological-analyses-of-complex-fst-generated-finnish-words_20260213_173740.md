---
ver: rpa2
title: LLMs' morphological analyses of complex FST-generated Finnish words
arxiv_id: '2407.08269'
source_url: https://arxiv.org/abs/2407.08269
tags:
- language
- task
- computational
- linguistics
- finnish
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  learn Finnish morphology similarly to humans by testing their ability to morphologically
  analyze complex, rare Finnish words. The authors generate 2000 complex Finnish noun
  forms using a finite-state transducer tool, making them unlikely to appear in LLM
  training data, and then ask four LLMs (GPT-4-turbo, GPT-3.5-turbo, Llama2-70B, and
  Poro-34B) to analyze these words' grammatical properties through explicit classification
  tasks.
---

# LLMs' morphological analyses of complex FST-generated Finnish words

## Quick Facts
- arXiv ID: 2407.08269
- Source URL: https://arxiv.org/abs/2407.08269
- Authors: Anssi Moisio; Mathias Creutz; Mikko Kurimo
- Reference count: 21
- Primary result: GPT-4-turbo performs reasonably well on Finnish morphological analysis but shows incomplete grasp of systematic morphological rules

## Executive Summary
This paper investigates whether large language models can learn Finnish morphology similarly to humans by testing their ability to morphologically analyze complex, rare Finnish words. The authors generate 2000 complex Finnish noun forms using a finite-state transducer tool, making them unlikely to appear in LLM training data, and then ask four LLMs (GPT-4-turbo, GPT-3.5-turbo, Llama2-70B, and Poro-34B) to analyze these words' grammatical properties through explicit classification tasks. The results show that GPT-4-turbo performs reasonably well, though not perfectly, while GPT-3.5-turbo struggles and the smaller models fail nearly completely. The analysis suggests that GPT-4-turbo has incomplete grasp of systematic morphological rules, as evidenced by its occasional misclassifications of possessive suffixes and grammatical cases.

## Method Summary
The authors generated 2000 complex Finnish noun forms using the Omorfi finite-state transducer tool, creating test words unlikely to appear in LLM training data. They created prompts with 0, 1, 5, or 10 examples asking LLMs to analyze inflected forms into base form, number, case, and possessive suffix. Four LLMs were tested: GPT-4-turbo, GPT-3.5-turbo, Llama2-70B, and Poro-34B, with different temperature settings (0.0 for GPT models, 0.5 for Llama2 and Poro). Classification accuracy was evaluated for number, grammatical case, and possessive suffix properties.

## Key Results
- GPT-4-turbo achieves the best performance among tested models but still shows systematic errors in morphological analysis
- GPT-3.5-turbo struggles significantly with the task, performing much worse than GPT-4-turbo
- Llama2-70B and Poro-34B fail nearly completely at morphological classification
- Models show frequency-based confusion patterns, often misclassifying rarer cases (like abessive) as more common ones (like partitive)
- Tokenization granularity affects performance, with models having longer tokens showing more difficulty with possessive suffixes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs learn statistical associations between surface forms and morphological categories but lack complete systematic rule representation.
- Mechanism: The models map character sequences to morphological labels using learned patterns from training data, but do not encode the underlying grammatical rules as humans do.
- Core assumption: Morphological generalization requires explicit rule-based reasoning rather than pattern matching.
- Evidence anchors:
  - [abstract] "GPT-4-turbo has incomplete grasp of systematic morphological rules, as evidenced by its occasional misclassifications of possessive suffixes and grammatical cases"
  - [section 2] "it is likely that the previously published datasets are included in their training data, which would preclude fair assessment"
  - [corpus] "Average neighbor FMR=0.524, average citations=0.0" (weak corpus support)
- Break condition: If test words follow highly regular patterns, statistical models can achieve near-perfect performance without true rule understanding.

### Mechanism 2
- Claim: Tokenization granularity affects morphological analysis performance.
- Mechanism: Subword tokenization breaks down rare character sequences into multiple tokens, making it harder to recognize morphological boundaries when morphemes span token boundaries.
- Core assumption: Single-token morphemes allow easier pattern recognition than multi-token representations.
- Evidence anchors:
  - [section 4.1] "having long tokens that combine multiple morphemes into a single token could hinder the capacity to model morphology"
  - [section 3] "GPT-4-turbo is not close to perfect accuracy either, and the combined 10-shot result does not reach the result achieved by simple RNNs trained with 80k words"
  - [corpus] "Top related titles: Analyzing Finnish Inflectional Classes through Discriminative Lexicon and Deep Learning Models" (weak corpus support)
- Break condition: If token boundaries align perfectly with morpheme boundaries, tokenization effects disappear.

### Mechanism 3
- Claim: Class frequency imbalance creates systematic confusion patterns.
- Mechanism: More frequent morphological classes (like partitive) are more likely to be predicted even when less frequent classes (like abessive) are correct, due to statistical bias.
- Core assumption: Neural networks default to high-frequency patterns when uncertain.
- Evidence anchors:
  - [section 4.1] "partitive is also much more common than abessive: 16.2% versus 0.1% of occurrences"
  - [section 4.1] "GPT-4 often confuses abessive cases as partitive"
  - [corpus] "Low-resource neural machine translation with morphological modeling" (weak corpus support)
- Break condition: If training data has balanced class distribution, frequency-based confusion should decrease.

## Foundational Learning

- Concept: Finite State Transducers (FSTs) for morphology
  - Why needed here: The paper uses FST-generated data to test generalization beyond training data patterns
  - Quick check question: How does an FST systematically generate all possible inflections of a word?

- Concept: Subword tokenization (BPE)
  - Why needed here: Different tokenization strategies affect morphological analysis performance
  - Quick check question: What happens when a morpheme spans multiple BPE tokens?

- Concept: Morphological classification vs generation
  - Why needed here: The task requires explicit classification rather than implicit generation through next-token prediction
  - Quick check question: Why might a model generate correct forms but fail at explicit classification?

## Architecture Onboarding

- Component map: LLM inference pipeline → tokenization → prompt processing → output generation → classification evaluation
- Critical path: Generate test word → tokenize → prompt LLM → parse output → compare to ground truth
- Design tradeoffs: Tokenization granularity vs model capacity; explicit classification vs implicit generation; training data size vs rule generalization
- Failure signatures: Systematic confusion of specific morphological categories; inconsistent performance across tokenization schemes; sensitivity to prompt format
- First 3 experiments:
  1. Compare tokenization schemes (BPE vs character-level) on the same test set
  2. Test with balanced vs imbalanced class distributions
  3. Compare explicit classification prompts vs implicit generation tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs trained on morphologically rich languages beyond Finnish perform on explicit morphological classification tasks compared to GPT-4-turbo?
- Basis in paper: [inferred] The paper concludes that even state-of-the-art LLMs like GPT-4-turbo don't model Finnish morphology thoroughly enough for high-accuracy morphological analysis, suggesting this might be a general limitation of LLMs rather than specific to Finnish.
- Why unresolved: The study only tested four LLMs on Finnish morphology, limiting generalizability to other morphologically rich languages.
- What evidence would resolve it: Systematic evaluation of multiple LLMs on explicit morphological classification tasks across various morphologically rich languages (e.g., Turkish, Arabic, Russian) with controlled testing conditions.

### Open Question 2
- Question: What specific mechanisms cause LLMs to fail at explicit morphological classification despite performing well at implicit morphological generation?
- Basis in paper: [explicit] The paper notes that GPT-4-turbo performs well at implicit Finnish morphological generation but fails at explicit classification, suggesting it uses different heuristics for these tasks.
- Why unresolved: The paper observes this discrepancy but doesn't investigate the underlying computational or architectural reasons for it.
- What evidence would resolve it: Comparative analysis of LLM internal representations during implicit generation versus explicit classification tasks, possibly using probing techniques or attention analysis.

### Open Question 3
- Question: Does the tokenization method (BPE vs alternatives) significantly impact LLMs' ability to learn and generalize morphological rules?
- Basis in paper: [explicit] The paper identifies tokenization as a potential factor, noting that Poro's longer tokens may hinder its possessive suffix classification performance compared to GPT and Llama.
- Why unresolved: The paper only correlates tokenization patterns with performance without testing alternative tokenization schemes or their impact on morphological learning.
- What evidence would resolve it: Controlled experiments comparing LLMs with different tokenization methods (e.g., BPE, SentencePiece, WordPiece) on the same morphological tasks, measuring generalization capacity and error patterns.

## Limitations

- Data Generation and Representativeness: The specific sampling strategy for selecting 2000 words from available 140k Finnish nouns is not fully specified, raising questions about test set representativeness.
- Limited Model Scope: Only four LLMs were tested, with a strong focus on GPT-4-turbo, potentially limiting generalizability to other model architectures.
- Tokenization Effects: The analysis of tokenization impact remains largely theoretical without systematic testing of alternative tokenization schemes.

## Confidence

- High Confidence: The core finding that GPT-4-turbo performs better than smaller models on morphological analysis tasks is well-supported by experimental results.
- Medium Confidence: The interpretation that LLMs rely on statistical associations rather than systematic rule representation is plausible but not definitively proven.
- Low Confidence: The specific claims about tokenization effects and class frequency imbalances lack direct empirical validation in this study.

## Next Checks

1. Systematic Tokenization Experiment: Re-run the morphological analysis task using different tokenization schemes (character-level, word-level, and varying BPE vocabulary sizes) across all four models to quantify how tokenization granularity specifically affects performance.

2. Balanced vs Imbalanced Class Distribution: Generate a matched test set where morphological categories appear with equal frequency, then compare model performance on this balanced set versus the original imbalanced set.

3. Controlled Vocabulary Size Experiment: Test intermediate-sized models (e.g., 7B, 13B parameters) to determine whether morphological analysis capability scales continuously with model size or shows threshold effects.