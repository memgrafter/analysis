---
ver: rpa2
title: Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4 Capabilities
arxiv_id: '2409.07638'
source_url: https://arxiv.org/abs/2409.07638
tags:
- list
- tasks
- performance
- task
- mango
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores evaluation of large language model (LLM) capabilities
  by testing GPT-4 on deterministic tasks like counting, sorting, and multiplication.
  The authors examine sensitivity to query phrasing and input parameter variations
  by performing 500 trials per condition across multiple task modifications.
---

# Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4 Capabilities

## Quick Facts
- **arXiv ID**: 2409.07638
- **Source URL**: https://arxiv.org/abs/2409.07638
- **Authors**: Thomas Ball; Shuo Chen; Cormac Herley
- **Reference count**: 17
- **Primary result**: Seemingly trivial changes in prompts or input populations yield LLM performance differences far larger than sampling effects can explain, making standard statistical generalizations invalid.

## Executive Summary
This paper challenges the reliability of evaluating large language model capabilities by demonstrating that GPT-4's performance on deterministic tasks like counting, sorting, and multiplication is highly sensitive to minor variations in prompts and input parameters. The authors show that standard statistical methods for estimating margins of error are invalid when hidden variance sources exist beyond sampling effects. Their findings reveal that human intuitions about which input changes should not affect performance are unreliable guides for LLM behavior, exposing a fundamental flaw in how we assess and claim LLM capabilities.

## Method Summary
The authors tested GPT-4 on deterministic tasks including counting, sorting, and multiplication by running 500 trials per condition with temperature 0.7. They systematically varied prompts, input list compositions, object frequencies, and population parameters while maintaining statistical power to detect performance differences. The experimental design included multiple prompt phrasings for the same task, different list compositions (e.g., mango/peach vs airedale/aspidistra), and varying input sizes. They employed χ2 goodness-of-fit tests to determine if different conditions produced statistically indistinguishable performance distributions.

## Key Results
- Small variations in prompt phrasing or input population composition produce performance swings much larger than sampling effects can explain
- Standard margin-of-error estimates are invalid as lower bounds when hidden variance sources exist
- Human intuitions about which input changes should not affect LLM performance are unreliable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small variations in input phrasing or population produce large performance swings in LLMs.
- Mechanism: Transformer-based LLMs lack stable, task-agnostic representations; performance is brittle to surface-level changes.
- Core assumption: The attention mechanism does not guarantee consistent handling of semantically equivalent inputs.
- Evidence anchors:
  - [abstract] "seemingly trivial modifications in the task-prompt or input population can yield differences far larger than can be explained by sampling effects."
  - [section 4.2] "the hypothesis that tests using the two prompts given above ... produce results drawn from the same distribution, is robustly rejected by a χ2 test."
- Break condition: If model fine-tuning or prompting strategy (e.g., chain-of-thought) consistently stabilizes accuracy across rephrased inputs.

### Mechanism 2
- Claim: Standard statistical margin-of-error estimates are invalid when hidden variance sources exist.
- Mechanism: Observed variance is a lower bound when unmeasured input dimensions (phrasing, list composition) contribute significantly.
- Core assumption: Variance from sampling alone dominates; unmeasured factors are negligible.
- Evidence anchors:
  - [abstract] "efforts to quantify LLM capabilities easily succumb to the language-as-fixed-effect fallacy, where experimental observations are improperly generalized beyond what the data supports."
  - [section 2] "we have only a lower bound on our margin-of-error" and "the conventional way of estimating margins-of-error cannot be applied."
- Break condition: If exhaustive enumeration or modeling of all input variations shows no hidden variance beyond sampling.

### Mechanism 3
- Claim: Human intuitions about which input changes should "make no difference" are unreliable for LLMs.
- Mechanism: LLM training and architecture create different robustness profiles than human cognition.
- Core assumption: Human task abstractions (e.g., separating counting from counted items) transfer to LLM behavior.
- Evidence anchors:
  - [abstract] "intuitions that have been formed based on interactions with humans form a very unreliable guide as to which input modifications should 'make no difference' to LLM performance."
  - [section 4.2] "performance on a simple list-counting task varies with ... list composition ... and object frequency."
- Break condition: If systematic testing shows LLM robustness aligns with human intuitions for a wide class of tasks.

## Foundational Learning

- Concept: Language-as-fixed-effect fallacy
  - Why needed here: Explains why generalizing LLM performance from narrow test cases is statistically invalid.
  - Quick check question: If two rewordings of a prompt yield different accuracy, can you claim the LLM "can" or "cannot" perform the task?

- Concept: χ2 goodness-of-fit test
  - Why needed here: Used to determine if different prompt phrasings produce statistically indistinguishable performance distributions.
  - Quick check question: What does a low p-value in a χ2 test between two conditions indicate about their underlying performance processes?

- Concept: Margin of error and sampling variance
  - Why needed here: Shows why conventional confidence intervals underestimate true uncertainty when hidden variance exists.
  - Quick check question: If hidden variance is large, is the usual 1.96 * sqrt(p*(1-p)/N) margin of error an over- or underestimate?

## Architecture Onboarding

- Component map: Input generator -> Prompt formatter -> GPT-4 API call -> Result parser -> χ2 statistical analyzer -> Result store
- Critical path: Generate input list -> Format prompt -> Submit to API -> Parse response -> Check correctness -> Log trial
- Design tradeoffs: Larger input populations increase confidence but raise cost; prompt variations increase sensitivity detection but multiply test matrix size
- Failure signatures: High variance across minor prompt changes; χ2 tests reject null hypothesis; accuracy drops sharply with input size or composition
- First 3 experiments:
  1. Replicate Table 1 counting task with 500 trials per condition to confirm variance sensitivity.
  2. Swap wording #1 and #2 in Table 1 and rerun χ2 tests to measure phrasing effect magnitude.
  3. Add a third prompt variation and test if variance continues to grow, indicating no saturation of sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the fixed-effect fallacy observed in basic arithmetic tasks also significantly impact LLM performance on complex reasoning tasks?
- Basis in paper: [explicit] The paper concludes that sensitivity to trivial modifications means that observed accuracy numbers cannot be assumed to generalize, even to entirely equivalent versions of a task. While demonstrated on basic arithmetic tasks, the authors suggest this problem is unlikely to be confined to that domain.
- Why unresolved: The study focused on deterministic tasks like counting, sorting, and multiplication. Complex reasoning tasks involve additional factors like context understanding and multi-step problem solving that may or may not be subject to the same sensitivity issues.
- What evidence would resolve it: Controlled experiments testing LLM performance on complex reasoning tasks with systematic variations in task phrasing, input structure, and parameter populations, while maintaining statistical power to detect differences beyond sampling effects.

### Open Question 2
- Question: Can margin-of-error estimates for LLM task performance be derived that account for the additional variance sources identified in this study?
- Basis in paper: [explicit] The authors note that the presence of unexplained variance means that estimating confidence intervals misses an additive component of unknown magnitude. They suggest this as an interesting direction for future work.
- Why unresolved: The study identifies the problem but does not attempt to quantify the additional variance sources or develop new statistical methods to account for them.
- What evidence would resolve it: Development and validation of new statistical models that incorporate both sampling variance and variance from task modifications, along with empirical testing to verify their accuracy in bounding LLM performance uncertainty.

### Open Question 3
- Question: Do Chain-of-Thought (CoT) prompting strategies reduce the sensitivity of LLM performance to task modifications?
- Basis in paper: [explicit] The authors mention that CoT often improves LLM performance on complex tasks but suggest it is worth further research to understand whether CoT-style prompts are more resilient to the variations shown in their study.
- Why unresolved: The study did not test CoT prompting strategies, leaving open the question of whether this approach might mitigate the fixed-effect fallacy problem.
- What evidence would resolve it: Comparative experiments measuring LLM performance sensitivity to task modifications under standard prompting versus CoT prompting, with statistical analysis to determine if CoT reduces the magnitude of performance variance across equivalent task variants.

## Limitations

- Results based on single model (GPT-4) and limited set of deterministic tasks, may not generalize to other LLMs or complex tasks
- Authors acknowledge that 500 trials only provide lower bound on variance, true performance variability across all possible inputs could be much larger
- Deterministic nature of tested tasks (counting, sorting, multiplication) may not represent full spectrum of LLM capabilities

## Confidence

- Performance sensitivity to input variations: High confidence
- Invalidity of conventional margin-of-error estimates: Medium confidence
- Unreliability of human intuitions for LLM behavior: Medium confidence

## Next Checks

1. Replicate with alternative models: Test the same sensitivity patterns on GPT-3.5, Claude, or open-source models to determine if this brittleness is specific to GPT-4 or a broader LLM characteristic.

2. Extend to complex reasoning tasks: Apply the same experimental framework to multi-step reasoning problems (like mathematical word problems or logical puzzles) to see if sensitivity to input variations persists in more complex domains.

3. Measure hidden variance bounds: Systematically vary input parameters beyond the tested conditions to empirically estimate the magnitude of hidden variance and determine if it truly dominates sampling variance in practice.