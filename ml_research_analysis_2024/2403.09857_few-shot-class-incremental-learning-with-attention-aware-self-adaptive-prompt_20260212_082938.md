---
ver: rpa2
title: Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt
arxiv_id: '2403.09857'
source_url: https://arxiv.org/abs/2403.09857
tags:
- learning
- classes
- prompt
- base
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Attention-Aware Self-Adaptive Prompt (ASP),
  a prompt-based method for Few-Shot Class Incremental Learning (FSCIL). Existing
  FSCIL methods often fine-tune entire backbones, leading to overfitting, while prompt-based
  CIL approaches require abundant data per task.
---

# Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt

## Quick Facts
- arXiv ID: 2403.09857
- Source URL: https://arxiv.org/abs/2403.09857
- Reference count: 40
- Proposes ASP (Attention-Aware Self-Adaptive Prompt) for FSCIL, achieving state-of-the-art results on CIFAR-100, CUB200, and ImageNet-R

## Executive Summary
This paper addresses Few-Shot Class Incremental Learning (FSCIL) by proposing a novel prompt-based method called Attention-Aware Self-Adaptive Prompt (ASP). The key innovation lies in decomposing prompts into task-invariant and task-specific components, leveraging Vision Transformer generalization while avoiding overfitting. ASP introduces attention-aware task-invariant prompts (TIP) to maintain consistent attention across tasks, and self-adaptive task-specific prompts (TSP) generated through an Information Bottleneck objective. The method also incorporates an Anchor Loss to align features with class prototypes, enhancing discrimination. Experiments demonstrate consistent improvements over state-of-the-art FSCIL and prompt-based CIL methods across multiple benchmarks.

## Method Summary
ASP addresses FSCIL challenges by leveraging pre-trained Vision Transformers and introducing a novel prompt decomposition strategy. The method separates prompts into two components: attention-aware task-invariant prompts (TIP) that ensure consistent attention patterns across tasks, and self-adaptive task-specific prompts (TSP) that are generated using an Information Bottleneck objective to enhance generalization. TSP is updated incrementally for new tasks without retraining, while TIP maintains task-invariant information. An Anchor Loss is employed to align features with class prototypes, improving discrimination. This approach combines the generalization capability of pre-trained ViTs with targeted prompt adaptation, achieving superior performance in FSCIL scenarios.

## Key Results
- ASP consistently outperforms state-of-the-art FSCIL methods on CIFAR-100, CUB200, and ImageNet-R
- Achieves higher average accuracy and harmonic accuracy compared to existing approaches
- Demonstrates lower performance drop across incremental tasks
- Shows effectiveness in balancing task-invariant and task-specific learning

## Why This Works (Mechanism)
The mechanism works by decomposing the learning problem into two complementary aspects: maintaining consistent attention patterns across tasks (via TIP) while allowing flexible adaptation to new tasks (via TSP). The Information Bottleneck objective ensures that TSP captures task-specific information efficiently without overfitting. The Anchor Loss provides additional discrimination by aligning features with class prototypes. This dual-prompt approach leverages the strong generalization of pre-trained ViTs while providing targeted adaptation mechanisms for incremental learning scenarios.

## Foundational Learning
- **Few-Shot Learning**: Learning from limited examples per class; needed because FSCIL deals with sparse data per task; quick check: evaluate performance on varying shot counts
- **Incremental Learning**: Learning new tasks without forgetting old ones; essential for FSCIL's sequential task learning; quick check: measure forgetting across tasks
- **Vision Transformers**: Pre-trained ViTs provide strong generalization; critical for ASP's foundation; quick check: compare with alternative backbone architectures
- **Prompt Engineering**: Decomposing prompts into task-specific and task-invariant components; enables flexible adaptation; quick check: ablate TIP and TSP components
- **Information Bottleneck**: Regularization technique to enhance generalization; used to generate TSP; quick check: compare with alternative regularization methods
- **Anchor Loss**: Prototype-based loss for feature discrimination; improves class separation; quick check: evaluate against other metric learning losses

## Architecture Onboarding
**Component Map:** Pre-trained ViT -> TIP + TSP -> Feature Extraction -> Anchor Loss -> Classification
**Critical Path:** Input -> ViT Backbone -> TIP (Task-Invariant) + TSP (Task-Specific) -> Feature Space -> Anchor Loss (Prototype Alignment) -> Output
**Design Tradeoffs:** The decomposition of prompts balances stability (via TIP) and flexibility (via TSP), trading off complexity for improved incremental learning performance. The Information Bottleneck objective adds computational overhead but enhances generalization.
**Failure Signatures:** Potential overfitting if TSP becomes too task-specific, catastrophic forgetting if TIP is not properly maintained, or degraded performance if Anchor Loss misaligns prototypes.
**First Experiments:**
1. Evaluate ASP on varying numbers of incremental tasks to assess scalability
2. Compare performance using different backbone architectures (CNN vs. ViT)
3. Ablate the Information Bottleneck objective to measure its contribution to generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on pre-trained Vision Transformers may limit applicability in scenarios where such models are unavailable
- Incremental update mechanism for TSP may struggle with highly dissimilar tasks
- Effectiveness of Anchor Loss in diverse and complex datasets may vary
- Potential biases introduced by the Information Bottleneck objective are not addressed
- Computational overhead of maintaining and updating prompts across tasks is not explicitly discussed

## Confidence
- **High Confidence**: Experimental results on CIFAR-100, CUB200, and ImageNet-R consistently show improvements over state-of-the-art methods
- **Medium Confidence**: Novel decomposition of prompts into task-invariant and task-specific components is theoretically sound but requires further validation in other domains
- **Low Confidence**: Claim of avoiding overfitting through ViT generalization is plausible but not rigorously tested against alternative strategies

## Next Checks
1. Validate ASP's performance using alternative backbone architectures (e.g., CNNs or hybrid models) to assess robustness beyond ViTs
2. Evaluate computational overhead of maintaining and updating prompts across tasks for large-scale or real-time applications
3. Test ASP on datasets with highly dissimilar tasks to quantify its ability to prevent catastrophic forgetting and maintain performance