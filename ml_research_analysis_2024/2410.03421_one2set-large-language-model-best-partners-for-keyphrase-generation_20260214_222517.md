---
ver: rpa2
title: 'One2set + Large Language Model: Best Partners for Keyphrase Generation'
arxiv_id: '2410.03421'
source_url: https://arxiv.org/abs/2410.03421
tags:
- uni00000013
- keyphrase
- selector
- uni00000011
- generator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving keyphrase generation
  by addressing the difficulty of achieving high recall and precision simultaneously
  in a single model. The proposed solution is a generate-then-select framework that
  first uses a one2set-based model (generator) to produce candidate keyphrases, then
  employs a large language model (selector) to filter and select the best ones.
---

# One2set + Large Language Model: Best Partners for Keyphrase Generation

## Quick Facts
- arXiv ID: 2410.03421
- Source URL: https://arxiv.org/abs/2410.03421
- Reference count: 22
- Primary result: F1@M scores up to 0.453 (precision) and 0.112 (absent keyphrases) on KP20k

## Executive Summary
This paper introduces a generate-then-select framework for keyphrase generation that addresses the challenge of simultaneously achieving high recall and precision. The approach first generates candidate keyphrases using a one2set-based model, then employs a large language model to filter and select the most relevant ones. The method shows significant improvements over state-of-the-art models, particularly for absent keyphrase prediction, which is a notoriously difficult task in keyphrase generation.

## Method Summary
The framework consists of two main components: a generator and a selector. The generator uses a one2set-based model to produce a set of candidate keyphrases for a given document. The selector, implemented as a large language model (GPT-4), then filters these candidates to select the final keyphrases. The training process incorporates an Optimal Transport-based assignment strategy to better match ground-truth keyphrases with control codes, and keyphrase selection is modeled as a sequence labeling task to reduce semantic repetition. The framework is evaluated across multiple datasets, demonstrating substantial improvements in both present and absent keyphrase prediction.

## Key Results
- Achieves F1@M scores of 0.453 for precision and 0.112 for absent keyphrases on KP20k dataset
- Significant improvements over state-of-the-art models in absent keyphrase prediction
- Demonstrates effectiveness across multiple benchmark datasets including KP20k, Inspec, NUS, and SemEval

## Why This Works (Mechanism)
The generate-then-select framework works by separating the keyphrase generation task into two stages, allowing each component to specialize in its function. The generator can focus on producing a comprehensive set of candidate keyphrases without the pressure of making final selections, while the LLM selector can leverage its semantic understanding to filter and rank the candidates. The Optimal Transport-based assignment strategy improves training efficiency by providing more accurate supervision signals during the generator training phase.

## Foundational Learning
- **Optimal Transport Assignment**: Needed for better ground-truth keyphrase-to-control-code mapping during training; quick check: compare convergence curves with/without OT assignment
- **Sequence Labeling for Selection**: Needed to reduce semantic repetition in selected keyphrases; quick check: measure n-gram overlap in selected keyphrases
- **One2set Architecture**: Needed for set-based keyphrase generation without order bias; quick check: compare performance with sequence-to-sequence baseline
- **LLM-based Selection**: Needed for leveraging semantic understanding in keyphrase filtering; quick check: compare with rule-based selection methods

## Architecture Onboarding

Component map: Document -> Generator -> Candidate Keyphrases -> LLM Selector -> Final Keyphrases

Critical path: The generator produces candidates in a single forward pass, then the LLM selector processes these candidates to produce the final selection. The bottleneck is typically the LLM inference time during selection.

Design tradeoffs: The framework trades computational efficiency for improved accuracy. Multiple LLM calls are required for selection, but this enables better semantic understanding and filtering compared to rule-based approaches.

Failure signatures: Poor generator performance leads to low-quality candidates regardless of selector quality. Over-aggressive selection by the LLM may miss relevant keyphrases. Suboptimal OT assignment can lead to poor training convergence.

First experiments: 1) Test generator alone to establish baseline candidate quality. 2) Evaluate LLM selector with ground-truth candidates to establish upper bound. 3) Run ablation study removing OT assignment to measure its impact.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead due to generate-then-select approach and multiple LLM inference passes
- Reliance on GPT-4 introduces reproducibility challenges and cost barriers
- Potential sensitivity of Optimal Transport assignment strategy to hyperparameter choices
- Limited ablation studies to isolate the impact of individual components

## Confidence
- High confidence in baseline framework's effectiveness for present keyphrases
- Medium confidence in absent keyphrase improvements given task difficulty
- Medium confidence in Optimal Transport assignment benefits due to limited ablation studies

## Next Checks
1. Conduct ablation studies isolating the impact of Optimal Transport assignment versus other architectural changes
2. Test the framework with different LLM models for selection to verify approach's robustness beyond GPT-4
3. Evaluate computational efficiency trade-offs across different selection strategies and candidate set sizes