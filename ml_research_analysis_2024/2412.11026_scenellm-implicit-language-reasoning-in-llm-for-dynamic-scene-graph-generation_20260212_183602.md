---
ver: rpa2
title: 'SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph Generation'
arxiv_id: '2412.11026'
source_url: https://arxiv.org/abs/2412.11026
tags:
- scene
- graph
- scenellm
- language
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SceneLLM, a novel framework for dynamic scene
  graph generation (SGG) that leverages large language models (LLMs) to reason about
  spatial-temporal relationships in videos. The key challenge addressed is transforming
  video frames into a format compatible with LLMs for accurate scene understanding.
---

# SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph Generation

## Quick Facts
- arXiv ID: 2412.11026
- Source URL: https://arxiv.org/abs/2412.11026
- Reference count: 40
- Primary result: SceneLLM achieves state-of-the-art performance on Action Genome benchmark for dynamic scene graph generation

## Executive Summary
SceneLLM introduces a novel framework that leverages large language models (LLMs) for dynamic scene graph generation (SGG) in videos. The core innovation is a Video-to-Language (V2L) mapping module that transforms video frames into implicit linguistic signals compatible with LLM reasoning. By combining VQ-VAE quantization, Chinese character-inspired spatial aggregation, and Optimal Transport for temporal consistency, SceneLLM achieves significant performance improvements over existing methods on the Action Genome benchmark.

## Method Summary
SceneLLM processes video frames through a multi-stage pipeline: object detection → VQ-VAE quantization → Spatial Information Aggregation (SIA) → Optimal Transport (OT) temporal aggregation → LLM inference with LoRA fine-tuning → SGG predictor decoding. The method discretizes visual features using VQ-VAE, encodes spatial relationships via a Chinese character-inspired aggregation scheme, and applies OT to generate temporally coherent implicit linguistic signals. The LLM is fine-tuned using LoRA to process these signals, and a transformer-based SGG predictor decodes the reasoning to generate semantic triplets.

## Key Results
- Achieves state-of-the-art performance on Action Genome benchmark
- Improves R@50 by 1.7% in PREDCLS under constrained conditions
- Improves R@50 by 1.5% in SGDET under constrained conditions
- Outperforms existing methods in both constrained and unconstrained settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Video-to-Language (V2L) mapping transforms visual features into a format that leverages LLM's pre-trained linguistic knowledge, enabling effective spatial-temporal reasoning.
- Mechanism: Visual features are first quantized into discrete tokens via VQ-VAE, spatial relationships are encoded using a Chinese character-inspired aggregation scheme (SIA), and temporal consistency is enforced through Optimal Transport (OT) across frame-level tokens.
- Core assumption: LLM's reasoning capability is sufficiently generalizable from language to implicitly encoded visual semantics when the input preserves hierarchical structure and positional information.
- Evidence anchors:
  - [abstract] "V2L Mapping module that transforms video frames into linguistic signals (scene tokens), making the input more comprehensible for LLMs."
  - [section 3.2] "the LLM was trained on a text corpus and fine-tuned with language instructions, both of which exhibit discreteness and hierarchy."
  - [corpus] Weak - no direct neighbor evidence for this specific V2L mechanism.
- Break condition: If the quantized embeddings lose too much semantic fidelity or if spatial-temporal structure is not sufficiently preserved, LLM reasoning will degrade.

### Mechanism 2
- Claim: Low-Rank Adaptation (LoRA) fine-tuning enables the LLM to better interpret the implicit language-like tokens without overwriting its general knowledge.
- Mechanism: LoRA applies low-rank matrix updates to LLM weights during fine-tuning, allowing adaptation to "scene sentences" while retaining pre-trained linguistic reasoning ability.
- Core assumption: Small, targeted weight adjustments are sufficient for LLM to adapt to new implicit language domains without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "we apply Low-Rank Adaptation (LoRA) to fine-tune the model."
  - [section 3.3] "we designed the following prompt as LLM's input" and "LoRA [25] to fine-tune LLM."
  - [corpus] Weak - no neighbor studies comparing LoRA vs full fine-tuning in SGG context.
- Break condition: If LoRA rank is too low or fine-tuning data is insufficient, the LLM may not adapt adequately to the scene reasoning task.

### Mechanism 3
- Claim: The Optimal Transport (OT) scheme aggregates frame-level tokens into a temporally coherent codebook, improving dynamic relationship prediction.
- Mechanism: OT redistributes semantic information from the original codebook into a smaller, temporally consistent updated codebook C+, which better represents dynamic scene evolution.
- Core assumption: Temporal semantic coherence can be modeled as an optimal transport problem over token distributions across frames.
- Evidence anchors:
  - [section 3.2] "we propose to apply an OT scheme... to transport semantic information from different frame-level tokens to equip the generated implicit linguistic signal with temporal information."
  - [section 4.5] "Our method outperforms these variants, demonstrating the efficacy of the OT scheme."
  - [corpus] Weak - no neighbor papers explicitly using OT for temporal consistency in SGG.
- Break condition: If transport cost matrix is poorly estimated or codebook update is too aggressive, temporal coherence may be lost.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs) and Vector Quantization (VQ-VAE)
  - Why needed here: To discretize continuous visual features into tokens compatible with LLM's discrete token processing.
  - Quick check question: What is the role of the codebook in VQ-VAE and how does it enable discrete representation learning?

- Concept: Graph Convolutional Networks (GCNs) for spatial relation modeling
  - Why needed here: To aggregate spatially related object features into a single frame-level token inspired by Chinese character structure.
  - Quick check question: How does a GCN encode pairwise relationships between nodes in a graph?

- Concept: Optimal Transport theory and Sinkhorn algorithm
  - Why needed here: To optimally aggregate frame-level tokens into a temporally consistent codebook while minimizing entropy loss.
  - Quick check question: What is the objective of optimal transport in the context of aligning two probability distributions?

## Architecture Onboarding

- Component map: Object Detector → VQ-VAE Encoder → Codebook Quantization → MLP (position embedding) → GCN (SIA) → OT (temporal aggregation) → LLM (implicit reasoning) → SGG Predictor (triplet decoding)
- Critical path: Object detection → VQ-VAE quantization → SIA aggregation → OT codebook update → LLM inference → SGG decoding
- Design tradeoffs:
  - Quantization granularity vs. semantic fidelity: Finer codebook increases token diversity but computational cost.
  - LoRA rank vs. adaptation capacity: Higher rank improves adaptation but risks overfitting.
  - OT codebook size vs. temporal coherence: Smaller codebook enforces stronger temporal aggregation but may lose detail.
- Failure signatures:
  - Degraded recall scores indicate tokenization or reasoning breakdown.
  - Unstable training loss suggests misalignment between V2L mapping and LLM expectations.
  - Poor generalization on unseen videos implies insufficient LoRA adaptation or OT codebook generalization.
- First 3 experiments:
  1. Remove OT and measure performance drop to isolate temporal coherence impact.
  2. Replace LoRA with full fine-tuning to compare adaptation strategies.
  3. Use continuous features (no VQ-VAE) to test necessity of discrete tokenization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of LLM architecture (e.g., LLaMA vs. T5) impact the performance of SceneLLM in dynamic scene graph generation?
- Basis in paper: [explicit] The paper explicitly compares LLaMA-13B with T5, showing that LLaMA outperforms T5 in the ablation studies.
- Why unresolved: The paper does not explore other LLM architectures or provide a detailed analysis of why LLaMA performs better than T5.
- What evidence would resolve it: Comparative experiments with other LLM architectures (e.g., GPT-4, BERT) and a detailed analysis of their performance differences in the context of dynamic scene graph generation.

### Open Question 2
- Question: How does the size of the codebook in the VQ-VAE affect the quality of the generated scene graphs?
- Basis in paper: [explicit] The paper mentions that the codebook size is set to 512 but does not explore the impact of varying this size on performance.
- Why unresolved: The paper does not provide experiments or analysis on how different codebook sizes influence the quality of the generated scene graphs.
- What evidence would resolve it: Experiments varying the codebook size and analyzing the resulting performance in terms of scene graph accuracy and quality.

### Open Question 3
- Question: Can the Spatial Information Aggregation (SIA) scheme be adapted to handle 3D scenes or scenes with more complex spatial relationships?
- Basis in paper: [inferred] The paper's SIA scheme is inspired by the structure of Chinese characters, which are inherently 2D. The paper does not discuss adaptations for 3D scenes.
- Why unresolved: The paper focuses on 2D video frames and does not address the challenges of extending the SIA scheme to 3D scenes or more complex spatial relationships.
- What evidence would resolve it: Experiments applying the SIA scheme to 3D scenes or scenes with complex spatial relationships, along with a detailed analysis of the challenges and adaptations required.

### Open Question 4
- Question: How does the Optimal Transport (OT) scheme perform in scenarios with high temporal variability or rapid scene changes?
- Basis in paper: [inferred] The paper uses OT to ensure temporal consistency in the generated scene graphs but does not discuss its performance in high-variability scenarios.
- Why unresolved: The paper does not provide experiments or analysis on the OT scheme's effectiveness in scenarios with high temporal variability or rapid scene changes.
- What evidence would resolve it: Experiments testing the OT scheme in scenarios with high temporal variability or rapid scene changes, along with an analysis of its performance and potential limitations.

## Limitations

- The exact implementation details of the Optimal Transport scheme remain underspecified, making it difficult to verify the reported performance gains
- No comparison is provided between LoRA and full fine-tuning or other parameter-efficient methods, leaving the optimality of LoRA unproven
- The "Chinese character-inspired" SIA design lacks rigorous justification for why this particular structural analogy should be superior to standard GCN variants
- Pre-trained object detector performance is not reported, creating uncertainty about whether SGG errors stem from detection failures
- The LLM reasoning process is treated as a black box with no analysis of how implicit signals are processed

## Confidence

**High Confidence**: The empirical results showing SceneLLM outperforms existing methods on Action Genome benchmarks are well-supported by reported metrics. The claim that V2L mapping improves LLM compatibility with visual inputs is reasonable given the discrete token approach and systematic experimentation.

**Medium Confidence**: The assertion that LoRA fine-tuning enables effective adaptation without catastrophic forgetting is plausible but under-supported. The paper demonstrates LoRA works in practice but doesn't compare against alternatives or analyze rank sensitivity. Similarly, the OT scheme's contribution is validated through ablation but the specific formulation choices remain unclear.

**Low Confidence**: The Chinese character-inspired SIA design rationale and the claim that this structure specifically benefits spatial reasoning lack rigorous justification. The paper presents this as an innovation but doesn't provide comparative analysis with alternative spatial aggregation methods or theoretical grounding for why this particular structure should be optimal.

## Next Checks

1. **OT Scheme Verification**: Implement and test alternative temporal aggregation methods (simple averaging, attention-based pooling) to isolate whether the specific OT formulation or temporal aggregation in general drives the performance gains. Compare against reported SceneLLM results to quantify the contribution of the particular OT implementation.

2. **LoRA Sensitivity Analysis**: Systematically vary LoRA rank from very low (1-2) to higher values (32-64) while measuring both performance and parameter efficiency. This would validate whether the chosen rank represents an optimal tradeoff or if the method is sensitive to this hyperparameter in ways not explored in the paper.

3. **Detector Performance Impact**: Replace the pre-trained detector with a stronger or weaker variant while keeping all other SceneLLM components constant. This would quantify how much of the reported performance stems from the SGG method itself versus detector quality, addressing a key uncertainty in the experimental setup.