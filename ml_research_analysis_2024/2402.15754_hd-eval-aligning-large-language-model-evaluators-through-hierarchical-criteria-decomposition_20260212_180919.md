---
ver: rpa2
title: 'HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria
  Decomposition'
arxiv_id: '2402.15754'
source_url: https://arxiv.org/abs/2402.15754
tags:
- uni00000048
- uni00000051
- uni00000052
- uni00000046
- criteria
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HD-Eval, a framework for aligning large language
  model (LLM) evaluators with human preferences through hierarchical criteria decomposition.
  The core idea is to iteratively decompose an evaluation task into finer-grained
  criteria at multiple hierarchy levels, aggregate results using a human preference-guided
  white-box aggregator, and prune insignificant criteria via attribution.
---

# HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition

## Quick Facts
- arXiv ID: 2402.15754
- Source URL: https://arxiv.org/abs/2402.15754
- Reference count: 40
- One-line primary result: HD-Eval achieves up to 15-20% improvement in Pearson and Spearman correlations compared to existing LLM-based evaluators

## Executive Summary
HD-Eval introduces a novel framework for aligning LLM evaluators with human preferences by decomposing evaluation tasks into hierarchical criteria, aggregating results using human preference-guided white-box models, and pruning insignificant criteria via attribution. The framework significantly improves alignment with human judgments across summarization, conversation, and data-to-text evaluation tasks, achieving substantial gains over existing methods. HD-Eval is efficient, explainable, and applicable to both open-source and closed-source LLMs.

## Method Summary
HD-Eval decomposes evaluation tasks into hierarchical criteria using LLMs, aggregates results through a white-box aggregator trained on human preferences, and prunes insignificant criteria using attribution methods. The framework iteratively trains these components in a layer-wise fashion, preserving hierarchical information throughout the evaluation process. This approach mirrors human evaluation patterns by breaking complex tasks into sub-criteria while maintaining contextual coherence through the aggregation and pruning mechanisms.

## Key Results
- Achieves 15-20% improvement in Pearson and Spearman correlations with human judgments
- Outperforms existing LLM-based evaluators across multiple evaluation tasks
- Demonstrates effectiveness with both open-source (Llama-2) and closed-source (GPT-4) models
- Shows significant improvements in summarization, conversation, and data-to-text evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical criteria decomposition improves alignment by mirroring human evaluation mindset
- Mechanism: Decomposing complex evaluation tasks into finer-grained criteria at multiple hierarchy levels allows LLMs to evaluate text quality more comprehensively and systematically
- Core assumption: Human evaluation naturally follows hierarchical thinking patterns, breaking complex problems into sub-problems
- Evidence anchors:
  - [abstract] "HD-E VAL inherits the essence from the evaluation mindset of human experts and enhances the alignment of LLM-based evaluators by decomposing a given evaluation task into finer-grained criteria"
  - [section 2.1] "Drawing inspirations from the above, we propose HD-E VAL, a novel framework to align LLM-based evaluator towards human preference through Hierarchical Criteria Decomposition"
  - [corpus] Average neighbor FMR=0.491 indicates moderate relatedness to existing hierarchical evaluation approaches

### Mechanism 2
- Claim: Human preference-guided aggregation improves alignment by weighting criteria according to human expert preferences
- Mechanism: Training white-box aggregators to map evaluation scores from decomposed criteria to human expert scores implicitly learns how human experts value different evaluation aspects
- Core assumption: Human experts assign different importance weights to different evaluation criteria
- Evidence anchors:
  - [section 2.2] "The aggregator fÎ¸ serves as a human preference estimator to aggregate scores on different sub-criteria for comprehensive evaluation"
  - [section 3.2] "HD-E VAL substantially improved the human relevance of evaluation over GPT-4, resulting in a 15% improvement on Pearson's correlation overall"
  - [corpus] Related works on "Aligning Human and LLM Judgments" suggest this is an active research area

### Mechanism 3
- Claim: Attribution pruning improves efficiency by focusing decomposition on significant criteria
- Mechanism: Using attribution methods to identify which criteria most influence final evaluation scores, then pruning insignificant criteria and focusing further decomposition efforts on significant ones
- Core assumption: Not all decomposed criteria are equally important for final evaluation quality
- Evidence anchors:
  - [section 2.3] "The core motivation for attribution pruning is to ensure most searching efforts (i.e.,deeper decomposition) are focused on the most significant evaluation aspects"
  - [section 4.3] "Since more information is provided, increasing criteria counts contribute to a better alignment. However, it is also proven feasible to achieve a comparable performance by only keeping the most significant ones"
  - [corpus] Limited direct evidence, but attribution methods are well-established in ML interpretability

## Foundational Learning

- Concept: Hierarchical thinking in evaluation
  - Why needed here: Understanding how human experts naturally break down complex evaluation tasks into hierarchical criteria is essential for designing effective decomposition strategies
  - Quick check question: Can you explain how a human expert might evaluate a book differently from a simple pass/fail criterion?

- Concept: White-box vs black-box evaluation methods
  - Why needed here: HD-E VAL uses white-box aggregators instead of relying solely on LLM prompting, which requires understanding the benefits and limitations of each approach
  - Quick check question: What are the key advantages of using a white-box aggregator over prompting an LLM for final evaluation?

- Concept: Attribution methods in machine learning
  - Why needed here: Attribution pruning relies on methods like permutation importance to identify significant criteria, requiring understanding of how these methods work
  - Quick check question: How does permutation importance measure the importance of a feature in a model's predictions?

## Architecture Onboarding

- Component map:
  LLM-based criteria decomposition -> Human preference-guided aggregator -> Attribution pruning -> Iterative alignment training

- Critical path:
  1. Initialize with root evaluation task
  2. Decompose criteria hierarchically using LLM
  3. Train aggregator to estimate human preferences
  4. Apply attribution pruning to select significant criteria
  5. Repeat steps 2-4 until maximum hierarchy depth reached
  6. Apply finalized model to new evaluation samples

- Design tradeoffs:
  - Deeper hierarchy provides more comprehensive evaluation but increases computational cost and complexity
  - More granular criteria capture finer details but may lose contextual coherence
  - White-box aggregators provide explainability but may not capture all nuances of human preference
  - Attribution pruning improves efficiency but risks removing potentially important criteria

- Failure signatures:
  - Poor correlation with human judgments despite hierarchical decomposition
  - Aggregator fails to learn meaningful human preference patterns
  - Attribution pruning removes criteria that are actually important for specific evaluation tasks
  - LLM-based decomposition produces irrelevant or redundant criteria

- First 3 experiments:
  1. Test hierarchical decomposition on a simple task (e.g., summarization) and verify that generated criteria are meaningful and comprehensive
  2. Train aggregator on small dataset and measure correlation with human judgments to validate preference estimation
  3. Apply attribution pruning and compare performance with and without pruning to verify efficiency gains don't compromise accuracy

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the research presented, several natural open questions emerge:

- How does the hierarchical criteria decomposition perform when applied to tasks outside of natural language generation evaluation, such as image classification or sentiment analysis?
- What is the optimal maximum depth of the hierarchical decomposition for different evaluation tasks, and how does it impact the alignment with human preferences?
- How does the choice of attribution method (e.g., permutation importance, Shapley values) affect the pruning of criteria and the overall performance of HD-EVAL?

## Limitations
- Relies heavily on the quality and consistency of human annotations for training the aggregator
- Performance depends on LLM capabilities, which vary significantly between open-source and closed-source models
- Attribution pruning may remove criteria that are important in specific contexts but appear less significant in aggregate analysis

## Confidence
- High confidence: The hierarchical decomposition mechanism and its correlation with human evaluation patterns
- Medium confidence: The effectiveness of human preference-guided aggregation across diverse evaluation tasks
- Low confidence: The generalizability of attribution pruning across different types of NLG tasks and LLM architectures

## Next Checks
1. Test HD-Eval's performance when trained on imperfect or inconsistent human annotations to assess robustness to annotation quality
2. Validate whether the attribution pruning consistently identifies truly significant criteria across different evaluation domains and LLM sizes
3. Compare HD-Eval's hierarchical decomposition with human-designed evaluation criteria to verify it captures the same evaluation mindset