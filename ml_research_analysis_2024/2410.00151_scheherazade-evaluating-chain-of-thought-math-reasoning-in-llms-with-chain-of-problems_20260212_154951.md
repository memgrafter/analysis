---
ver: rpa2
title: 'Scheherazade: Evaluating Chain-of-Thought Math Reasoning in LLMs with Chain-of-Problems'
arxiv_id: '2410.00151'
source_url: https://arxiv.org/abs/2410.00151
tags:
- reasoning
- chaining
- 'true'
- problems
- backward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Scheherazade, an automated approach to generate
  challenging mathematical reasoning benchmarks by logically chaining existing problems
  together. The method employs forward and backward chaining techniques with randomized
  branching to create complex, nested dependencies between problems.
---

# Scheherazade: Evaluating Chain-of-Thought Math Reasoning in LLMs with Chain-of-Problems

## Quick Facts
- arXiv ID: 2410.00151
- Source URL: https://arxiv.org/abs/2410.00151
- Reference count: 16
- Primary result: Chain-of-Problems benchmarks reveal sharp accuracy decline for frontier models on chained mathematical reasoning tasks, with OpenAI's o1-preview maintaining superior performance

## Executive Summary
This paper introduces Scheherazade, an automated approach to generate challenging mathematical reasoning benchmarks by logically chaining existing problems together. The method employs forward and backward chaining techniques with randomized branching to create complex, nested dependencies between problems. When applied to GSM8K to create GSM8K-Scheherazade, the generated benchmarks reveal that frontier models experience a sharp decline in accuracy as chain length increases, with the exception of OpenAI's o1-preview, which demonstrates superior performance and maintains accuracy better than other models across both chaining methods.

## Method Summary
The method generates chained mathematical problems by extracting premises, questions, and conclusions from GSM8K problems, then applying forward and backward chaining with randomized conditional branching. Forward chaining solves problems sequentially, while backward chaining requires reasoning about future information first. The approach introduces randomization between correct and incorrect conditional branches at each step, preventing models from simply memorizing fixed sequences. For evaluation, the method extracts answers from model outputs using regex patterns and compares them against ground truth across chain lengths 2-10.

## Key Results
- Frontier models show rapid accuracy decline as chain length increases on both forward and backward chaining tasks
- OpenAI's o1-preview is the only model to maintain performance on backward chaining, where most models fail beyond a few chained questions
- Randomized branching successfully prevents memorization, as models cannot simply learn fixed operation sequences
- Error analysis reveals distinct failure modes including semantic misunderstanding, wrong path selection, and false negatives from answer extraction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Problems creates nested logical dependencies that expose reasoning limitations not visible in single problems.
- Mechanism: By chaining GSM8K problems using conditional branching, the approach forces models to track dependencies across multiple subproblems, revealing where models fail to maintain coherent reasoning across chains.
- Core assumption: Model reasoning failures are more apparent when problems are chained together than when evaluated individually.
- Evidence anchors: "Running the models on our benchmark shows that, despite high reported scores on original GSM8K problems, performance rapidly declines as the length of the chain increases."

### Mechanism 2
- Claim: Backward chaining is uniquely challenging because it requires reasoning about future information.
- Mechanism: In backward chaining, solving earlier problems requires information from later problems, creating a dependency structure that forces models to reason in reverse order.
- Core assumption: LLMs are inherently better at forward reasoning than backward reasoning.
- Evidence anchors: "backward chaining technique in particular proves especially challenging, with most models failing to reason effectively beyond a few chained questions."

### Mechanism 3
- Claim: Randomized branching prevents memorization and forces genuine reasoning.
- Mechanism: The approach introduces random selection between correct and incorrect conditional branches at each chaining step.
- Core assumption: Without randomization, models could learn to follow a fixed pattern rather than reasoning through each problem.
- Evidence anchors: "By introducing branching paths and integrating randomness, we ensure that models cannot simply memorize a sequence of branches to follow, thereby requiring reasoning."

## Foundational Learning

- Concept: Logical implication and conditional reasoning
  - Why needed here: The chaining mechanism relies on understanding how "if-then" statements create dependencies between problems
  - Quick check question: Given "If A then B" and "If B then C", what is the implication relationship between A and C?

- Concept: Chain-of-Thought reasoning
  - Why needed here: The evaluation measures how well models maintain reasoning chains across multiple connected problems
  - Quick check question: How does chain-of-thought prompting differ from direct answer generation in mathematical problem solving?

- Concept: Benchmark saturation and contamination
  - Why needed here: The work addresses the problem that existing benchmarks no longer differentiate between high-performing models
  - Quick check question: Why does high accuracy on GSM8K (94%+) indicate benchmark saturation rather than model capability?

## Architecture Onboarding

- Component map: Problem parser -> Chain generator -> Model evaluation -> Answer extractor -> Evaluation engine
- Critical path: Chain generation → Model evaluation → Answer extraction → Accuracy calculation
- Design tradeoffs:
  - Randomization vs. reproducibility: The approach uses randomization for generality but this makes exact replication difficult
  - Chain length vs. evaluation feasibility: Longer chains provide better differentiation but are computationally expensive
  - Question selection vs. chain complexity: More complex starting problems create harder chains but may introduce confounding factors
- Failure signatures:
  - False negatives in answer extraction indicating parsing issues
  - Models achieving near-perfect accuracy suggesting insufficient chain complexity
  - Inconsistent performance across random seeds indicating sensitivity to problem selection
- First 3 experiments:
  1. Generate and evaluate a small set (10-20) of forward and backward chains of length 2-3 to validate the basic mechanism
  2. Test answer extraction on a few manually crafted examples to ensure the evaluation pipeline works correctly
  3. Run a single model (e.g., GPT-4) on both chaining methods to verify the expected accuracy decline pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of o1-preview on backward chaining tasks compare to its performance on forward chaining tasks as chain length increases?
- Basis in paper: The paper states that o1-preview is the only model to perform better at backward reasoning, maintaining higher accuracy than forward chaining across all lengths.
- Why unresolved: While the paper provides comparative performance data, it does not explore the underlying reasons for o1-preview's superior backward reasoning capabilities or how this performance scales with increasing chain complexity.
- What evidence would resolve it: Detailed analysis of o1-preview's reasoning process on backward chaining tasks, including comparisons with its forward chaining performance and insights into its architectural or training differences that contribute to this capability.

### Open Question 2
- Question: What are the specific limitations of current LLMs when faced with nested dependencies in chained problems, and how do these limitations manifest across different models?
- Basis in paper: The paper discusses how models' performance declines with increasing chain length and highlights o1-preview's exceptional performance, suggesting varying capabilities in handling nested dependencies.
- Why unresolved: The paper does not delve into the specific cognitive or architectural limitations that cause performance degradation, nor does it provide a detailed comparison of how different models handle these nested dependencies.
- What evidence would resolve it: A comprehensive study analyzing the error patterns and reasoning failures of different models on chained problems, identifying common limitations and model-specific weaknesses in processing nested dependencies.

### Open Question 3
- Question: How do alternative logical operations (e.g., conjunctions, disjunctions) in problem chaining affect the reasoning performance of LLMs compared to simple if-then-else structures?
- Basis in paper: The paper suggests that future work could explore combining problems with conjunctions or disjunctions in addition to implications, implying that current evaluations are limited to if-then-else structures.
- Why unresolved: The impact of more complex logical operations on LLM reasoning has not been explored, leaving open the question of how these operations might challenge or enhance model performance.
- What evidence would resolve it: Experimental results comparing LLM performance on chained problems using different logical operations, analyzing how each type of operation affects accuracy and reasoning depth.

## Limitations
- Answer extraction using regex patterns may introduce false negatives that aren't fully quantified
- Randomization approach makes exact replication difficult and may introduce variability affecting comparative results
- Error analysis covers only 40 samples per model (20 forward, 20 backward), which may not capture full distribution of failure modes

## Confidence
- High confidence: General finding that frontier models experience accuracy decline with chain length
- Medium confidence: Claim that backward chaining is uniquely challenging
- Medium confidence: Assertion that randomization prevents memorization

## Next Checks
1. **Answer Extraction Validation**: Manually verify 50 randomly selected "incorrect" answers to quantify false negative rate and refine extraction methodology
2. **Chain Length Saturation Analysis**: Test models on chain lengths beyond 10 (e.g., 12-15) to determine if accuracy plateaus or continues declining
3. **Ablation Study on Randomization**: Generate benchmarks with identical chains but varying degrees of randomization to isolate the effect of randomization on model performance