---
ver: rpa2
title: Convergence of Sharpness-Aware Minimization Algorithms using Increasing Batch
  Size and Decaying Learning Rate
arxiv_id: '2409.09984'
source_url: https://arxiv.org/abs/2409.09984
tags:
- gsam
- learning
- batch
- rate
- increasing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence of the Sharpness-Aware Minimization
  (SAM) algorithm, specifically its gap-guided variant (GSAM), when combined with
  increasing batch sizes or decaying learning rates. The authors theoretically prove
  that GSAM with these strategies achieves an epsilon-approximation of the optimal
  solution.
---

# Convergence of Sharpness-Aware Minimization Algorithms using Increasing Batch Size and Decaying Learning Rate

## Quick Facts
- arXiv ID: 2409.09984
- Source URL: https://arxiv.org/abs/2409.09984
- Reference count: 40
- One-line primary result: GSAM with increasing batch sizes or decaying learning rates achieves epsilon-approximation and better generalization through flatter minima

## Executive Summary
This paper analyzes the convergence properties of Sharpness-Aware Minimization (SAM) and its gap-guided variant (GSAM) when combined with increasing batch sizes or decaying learning rates. The authors theoretically prove that GSAM with these scheduling strategies achieves an epsilon-approximation of the optimal solution by reducing the search direction noise between GSAM and standard gradient descent. Empirically, they demonstrate that using increasing batch sizes or cosine-annealing learning rates leads to flatter local minima and improved generalization compared to constant batch sizes and learning rates, as shown by training ResNet-18 and ViT-Tiny on CIFAR100.

## Method Summary
The method combines GSAM with either increasing batch sizes or decaying learning rates to achieve convergence guarantees and improved generalization. GSAM computes a search direction by minimizing the maximum loss within a neighborhood of the current parameters, then follows this direction while the batch size increases or learning rate decreases according to a scheduler. The theoretical analysis bounds the gradient norm of the SAM objective, ensuring convergence to an epsilon-approximate solution. Empirically, the approach is validated on CIFAR100 using ResNet-18, Wide-ResNet-28-10, and ViT-Tiny architectures with batch size progressions like [8, 16, 32, 64, 128] over 200 epochs.

## Key Results
- GSAM with increasing batch sizes achieves lower test error and sharpness than standard SGD/Adam with increasing batch sizes
- Theoretical convergence guarantee shows GSAM finds epsilon-approximation of optimal solution with increasing batch size or decaying learning rate
- Cosine-annealing learning rate scheduler with constant batch size produces flatter minima and better generalization than constant learning rate

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GSAM with increasing batch sizes or decaying learning rates achieves better generalization by converging to flatter local minima.
- **Mechanism:** The search direction noise between GSAM and GD decreases as batch size increases or learning rate decays. When this noise is small, GSAM behaves similarly to GD in minimizing the SAM objective, which leads to flatter minima.
- **Core assumption:** The approximation error between GSAM and GD becomes negligible when batch size is large or learning rate is small.
- **Evidence anchors:**
  - [abstract] "Empirically, they demonstrate that using increasing batch sizes or cosine-annealing learning rates leads to flatter local minima and improved generalization"
  - [section 2.3] "the smaller ηt is, the smaller E[ηt∥ωt∥2] becomes" and "E[ηt∥ωt∥2] decreases as bt increases"
  - [corpus] Weak - related papers focus on convergence but don't directly address flatness or generalization
- **Break condition:** When the approximation error remains large despite increasing batch size or decreasing learning rate, GSAM will not converge to flatter minima.

### Mechanism 2
- **Claim:** The theoretical ϵ-approximation guarantee holds for GSAM with increasing batch size and constant learning rate, or constant batch size and decaying learning rate.
- **Mechanism:** The convergence analysis bounds the gradient norm of the SAM objective, ensuring that the algorithm finds a point where the gradient is arbitrarily small (within ϵ).
- **Core assumption:** The smoothness conditions and variance bounds (A1, A2) hold for the loss functions being optimized.
- **Evidence anchors:**
  - [abstract] "the authors theoretically prove that GSAM with these strategies achieves an epsilon-approximation of the optimal solution"
  - [section 2.4.1] Theorem 2.3 states "there exists t0 ∈ N such that, for all T ≥ t0, min_{t∈[0:T−1]} E[∥∇̂f SAM S,ρ(xt)∥²] ≤ ϵ"
  - [corpus] Weak - related papers discuss convergence but with different theoretical frameworks
- **Break condition:** When smoothness assumptions are violated or variance becomes too large, the theoretical guarantees may not hold.

### Mechanism 3
- **Claim:** SAM/GSAM with increasing batch sizes achieves lower test error and sharpness than standard SGD/Adam with increasing batch sizes.
- **Mechanism:** By explicitly minimizing sharpness through the SAM objective while using increasing batch sizes, the algorithm finds solutions that generalize better than those found by standard optimization methods.
- **Core assumption:** Minimizing sharpness correlates with better generalization performance.
- **Evidence anchors:**
  - [abstract] "Their results indicate that SAM/GSAM with increasing batch sizes achieves lower test error and sharpness than standard SGD/Adam with increasing batch sizes"
  - [section 3] Tables 2 and 3 show SAM+B (SAM + increasing batch) has the highest test accuracy and lowest sharpness
  - [corpus] Weak - related papers discuss SAM but not in combination with increasing batch sizes
- **Break condition:** When the relationship between sharpness and generalization breaks down, this mechanism will not hold.

## Foundational Learning

- **Concept:** Sharpness-Aware Minimization (SAM)
  - Why needed here: SAM is the core optimization framework being analyzed and extended with new scheduling strategies
  - Quick check question: What is the main objective of SAM compared to standard empirical risk minimization?

- **Concept:** Mini-batch gradient estimation and variance
  - Why needed here: The convergence analysis relies on understanding how mini-batch gradients approximate full gradients and how variance affects convergence
  - Quick check question: How does the variance of mini-batch gradients scale with batch size according to Assumption 2.1(A2)?

- **Concept:** Search direction noise between algorithms
  - Why needed here: The theoretical analysis hinges on understanding how GSAM differs from GD through the search direction noise term
  - Quick check question: What happens to the search direction noise as batch size approaches the full dataset size?

## Architecture Onboarding

- **Component map:** Model parameters -> Mini-batch scheduler -> Learning rate scheduler -> GSAM algorithm -> Convergence monitor -> Generalization evaluator

- **Critical path:**
  1. Initialize model parameters and hyperparameters
  2. At each iteration, compute mini-batch gradient and perturbation
  3. Update parameters using GSAM update rule
  4. Adjust batch size or learning rate according to scheduler
  5. Monitor convergence and generalization metrics

- **Design tradeoffs:**
  - Larger batch sizes reduce noise but require more memory
  - Smaller learning rates ensure stability but slow convergence
  - The choice between increasing batch size vs decaying learning rate affects memory usage patterns and convergence speed

- **Failure signatures:**
  - Poor generalization despite flat minima (sharpness-generalization relationship breaks)
  - Slow convergence or divergence (learning rate/batch size schedule inappropriate)
  - High variance in results (insufficient random seeds or unstable training)

- **First 3 experiments:**
  1. Implement GSAM with constant batch size and learning rate, verify it matches standard SAM
  2. Add increasing batch size scheduler, observe convergence behavior and test accuracy
  3. Add cosine-annealing learning rate scheduler, compare with constant learning rate results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the theoretical convergence guarantee extend to other sharpness-aware optimization variants beyond GSAM?
- Basis in paper: [explicit] The paper focuses specifically on GSAM and mentions related methods in Table 1 but doesn't analyze them with increasing batch sizes or decaying learning rates
- Why unresolved: The convergence analysis is tailored to GSAM's specific structure, particularly its gap-guided mechanism and search direction noise
- What evidence would resolve it: Convergence proofs for other SAM variants (like SSAM, m-SAM, VaSSO) using similar batch size/learning rate schedules

### Open Question 2
- Question: How does the generalization benefit of increasing batch sizes with SAM compare to the benefits of other batch size scheduling strategies like warm restarts or random sampling?
- Basis in paper: [inferred] The paper only tests increasing batch sizes versus constant batch sizes, without comparing to alternative scheduling approaches
- Why unresolved: The experimental design only includes two scheduling strategies, limiting understanding of relative effectiveness
- What evidence would resolve it: Head-to-head comparisons of multiple batch size scheduling strategies with SAM/GSAM on diverse datasets and architectures

### Open Question 3
- Question: What is the precise relationship between the sharpness of minima found by SAM/GSAM and their generalization performance across different model architectures?
- Basis in paper: [explicit] The paper measures sharpness and test error but notes that previous work reported weak relationships between sharpness and generalization
- Why unresolved: While the paper observes flatter minima with better generalization in specific cases, it doesn't establish a general theoretical connection
- What evidence would resolve it: Systematic experiments across diverse architectures showing correlation (or lack thereof) between sharpness metrics and generalization performance

## Limitations

- Theoretical analysis relies on smoothness assumptions and variance bounds that may not hold for all deep learning tasks
- Computational overhead of GSAM with increasing batch sizes may be prohibitive for very large models or datasets
- Focus on convex loss functions may not fully capture behavior of highly non-convex deep learning landscapes

## Confidence

- **High Confidence:** The empirical demonstration that GSAM with increasing batch sizes achieves lower test error and sharpness than standard optimization methods
- **Medium Confidence:** The theoretical convergence guarantees for GSAM with increasing batch size or decaying learning rate
- **Low Confidence:** The mechanism by which search direction noise reduction leads to flatter minima

## Next Checks

1. Test the convergence and generalization of GSAM with increasing batch sizes on non-convex loss functions and deeper architectures to verify the theoretical bounds hold beyond the scope of the current experiments
2. Implement a direct measurement of search direction noise during training to empirically validate whether noise reduction correlates with flatter minima as predicted by the theory
3. Conduct ablation studies varying the rate of batch size increase or learning rate decay to identify optimal scheduling strategies and determine sensitivity to these hyperparameters