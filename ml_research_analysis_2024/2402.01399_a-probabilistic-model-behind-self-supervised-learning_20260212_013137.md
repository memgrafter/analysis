---
ver: rpa2
title: A Probabilistic Model Behind Self-Supervised Learning
arxiv_id: '2402.01399'
source_url: https://arxiv.org/abs/2402.01399
tags:
- learning
- representations
- methods
- generative
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a generative latent variable model for self-supervised
  learning (SSL) that unifies several families of discriminative SSL methods, including
  contrastive approaches. The model treats representations as latent variables and
  shows that predictive SSL methods induce a comparable distribution over representations
  to the model's mixture prior.
---

# A Probabilistic Model Behind Self-Supervised Learning

## Quick Facts
- arXiv ID: 2402.01399
- Source URL: https://arxiv.org/abs/2402.01399
- Authors: Alice Bizeul; Bernhard Schölkopf; Carl Allen
- Reference count: 40
- Primary result: SimVAE achieves 51.8% accuracy on CIFAR10 vs 67.4% for SimCLR, reducing the generative SSL gap from 32.8% to 17.6%

## Executive Summary
This paper proposes a generative latent variable model for self-supervised learning that unifies several families of discriminative SSL methods, including contrastive approaches. The model treats representations as latent variables sampled from a mixture prior and shows that predictive SSL methods induce a comparable distribution over representations. The authors derive the model's evidence lower bound (ELBOSSL) and demonstrate that learning representations by maximizing ELBOSSL (SimVAE) significantly narrows the performance gap between generative and discriminative methods, especially on tasks requiring style information.

## Method Summary
The paper introduces a generative latent variable model where representations are treated as latent variables sampled from a mixture prior p(z|y), with y determining semantic content and z governing style. The model's evidence lower bound (ELBOSSL) unifies discriminative SSL methods by showing they all induce similar representation distributions through a combination of prior terms (pulling representations to modes) and entropy terms (preventing collapse). SimVAE maximizes ELBOSSL directly using Gaussian assumptions for p(x|z), q(z|x), and p(z|y) with fixed variance σ²=0.15 and J=10 augmentations per sample. The method uses encoder-decoder architectures specified per dataset and trains with Adam optimizer, outperforming other VAE-based methods and narrowing the gap to discriminative approaches on style-related tasks.

## Key Results
- On CIFAR10, SimVAE achieves 51.8% accuracy compared to 67.4% for SimCLR, reducing the generative SSL gap from 32.8% to 17.6%
- On CelebA, SimVAE outperforms discriminative methods by 14.8% for style-related tasks
- SimVAE improves generative SSL performance over VAE, β-VAE, and CR-VAE baselines across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
The SSL Model's evidence lower bound (ELBOSSL) unifies discriminative SSL methods by showing they all induce similar representation distributions. The model treats representations as latent variables sampled from a mixture prior p(z|y), where y determines semantic content and z governs style. Predictive SSL methods maximize a lower bound that combines a prior term (pulling representations to modes) and an entropy term (preventing collapse), which together emulate ELBOSSL. This unification provides a theoretical framework for understanding contrastive and other discriminative approaches as approximate variational inference under specific assumptions.

### Mechanism 2
Discriminative SSL methods lose style information due to representation collapse under their loss functions. The prior term in ELBOSSL pulls representations of semantically related samples together, while the reconstruction term prevents collapse. Discriminative methods replace the reconstruction term with entropy, which causes representations within each cluster to collapse and lose discriminative style information. This collapse occurs because entropy maximization alone is insufficient to maintain intra-cluster diversity while preserving semantic structure.

### Mechanism 3
SimVAE improves generative SSL performance by learning a proper generative model with mixture prior instead of discriminative collapse. SimVAE maximizes ELBOSSL directly, which includes both the reconstruction term (maintaining distinct representations) and the proper mixture prior (clustering semantically related samples without collapse). This preserves more style information while maintaining semantic clustering. The mixture prior p(z|y) can be well-approximated and optimized during training, allowing the model to learn rich representations that retain both semantic content and style information.

## Foundational Learning

- **Concept**: Variational Inference and Evidence Lower Bound (ELBO)
  - Why needed here: The paper's theoretical framework relies on maximizing ELBOSSL to justify SSL methods and derive SimVAE. Understanding ELBO derivation and optimization is essential.
  - Quick check question: What are the three components of the standard ELBO, and how does ELBOSSL modify them?

- **Concept**: Mutual Information and Contrastive Learning
  - Why needed here: The paper addresses the perceived connection between SSL and mutual information, showing it's an artifact of the loss function structure rather than fundamental.
  - Quick check question: How does the InfoNCE objective relate to mutual information, and why does the paper argue this connection is not fundamental?

- **Concept**: Representation Collapse in Deep Learning
  - Why needed here: The paper's key insight is that discriminative SSL methods cause representation collapse, losing style information. Understanding this phenomenon is crucial for grasping the paper's contributions.
  - Quick check question: What causes representation collapse in SSL methods, and how does SimVAE prevent it?

## Architecture Onboarding

- **Component map**: Data → Augmentation → Encoder → Latent representations → (Decoder) → Reconstruction loss → Prior loss → Entropy loss → Backpropagation
- **Critical path**: Data undergoes augmentation, passes through encoder to obtain latent representations, which are then used for reconstruction (generative) or similarity comparison (discriminative), with gradients flowing back through the encoder
- **Design tradeoffs**: Number of augmentations J affects prior approximation quality vs computational cost; latent dimensionality balances expressiveness vs overfitting; prior variance σ² controls clustering tightness vs representation diversity; reconstruction weight balances generative quality vs representation utility
- **Failure signatures**: Poor downstream performance indicates inadequate representation quality or collapse; unstable training suggests prior variance or reconstruction weight issues; mode collapse indicates insufficient entropy or prior strength; over-regularization shows excessive clustering preventing useful representation diversity
- **First 3 experiments**: 1) Compare SimVAE vs VAE performance on MNIST with varying J (2, 5, 10 augmentations); 2) Test SimVAE with different prior variances (0.1, 0.15, 0.2) on FashionMNIST; 3) Evaluate style information preservation by reconstructing CelebA images from SimVAE vs SimCLR representations

## Open Questions the Paper Calls Out

### Open Question 1
How does SimVAE's performance scale with more complex datasets like ImageNet compared to discriminative SSL methods? The paper acknowledges that generative modeling remains challenging for higher-variance datasets and notes Balestriero & LeCun (2024) regarding ImageNet, but does not provide experimental results on complex datasets.

### Open Question 2
Can the SSL Model be extended to explicitly disentangle style and content information rather than discarding style information? The authors note the model could be modified to disentangle rather than discard style information through careful choice of model parameters, but do not explore this modification.

### Open Question 3
How sensitive is SimVAE's performance to the choice of augmentation strategy and the number of augmentations used? While the paper provides initial insights through ablation studies, it does not systematically explore the full space of augmentation strategies or the optimal number of augmentations for different dataset types and downstream tasks.

## Limitations
- Limited empirical evidence directly supporting the unification mechanism, with weak signals connecting theoretical claims to validation
- No demonstration that style information is truly lost in discriminative methods versus merely distributed differently
- Lack of sufficient evidence that the mixture prior p(z|y) can be accurately learned in practice

## Confidence

- **High**: The mathematical derivation of ELBOSSL and its relationship to discriminative SSL loss functions is sound and well-established
- **Medium**: The claim that SimVAE improves generative SSL performance is supported by experimental results, though the improvement magnitude varies significantly across tasks
- **Low**: The assertion that discriminative methods fundamentally lose style information is not conclusively demonstrated; alternative explanations remain plausible

## Next Checks

1. **Direct style information measurement**: Quantify style information preservation by training linear classifiers on style attributes from SimVAE vs discriminative representations on CelebA
2. **Prior learning stability**: Test SimVAE with varying prior variances and augmentation counts to determine sensitivity and stability of the mixture prior learning
3. **Reconstruction quality comparison**: Systematically compare reconstruction FID scores between SimVAE and other VAE-based methods across all datasets to validate generative performance claims