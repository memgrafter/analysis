---
ver: rpa2
title: 'A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics
  Tasks'
arxiv_id: '2410.22391'
source_url: https://arxiv.org/abs/2410.22391
tags:
- xlstm
- tasks
- action
- learning
- recurrent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of slow inference in large action\
  \ models (LAMs) based on Transformers, which limits their real-time applicability\
  \ in robotics. The proposed solution is a Large Recurrent Action Model (LRAM) that\
  \ leverages modern recurrent architectures\u2014specifically xLSTM\u2014at its core,\
  \ achieving linear-time inference complexity and improved sequence length extrapolation."
---

# A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks

## Quick Facts
- arXiv ID: 2410.22391
- Source URL: https://arxiv.org/abs/2410.22391
- Authors: Thomas Schmied; Thomas Adler; Vihang Patil; Maximilian Beck; Korbinian Pöppel; Johannes Brandstetter; Günter Klambauer; Razvan Pascanu; Sepp Hochreiter
- Reference count: 40
- Primary result: Large Recurrent Action Models (LRAM) based on xLSTM achieve linear-time inference and match or outperform Transformer-based LAMs across 432 robotics tasks while being significantly faster.

## Executive Summary
This paper addresses the fundamental challenge of slow inference in large action models (LAMs) based on Transformers, which limits their real-time applicability in robotics. The authors propose a Large Recurrent Action Model (LRAM) that leverages modern recurrent architectures—specifically xLSTM—at its core, achieving linear-time inference complexity and improved sequence length extrapolation. Experiments across 6 domains and 432 tasks demonstrate that LRAM matches or outperforms Transformer-based LAMs in both performance and speed, with inference times that scale linearly rather than quadratically with sequence length.

## Method Summary
The authors propose replacing the Transformer architecture in large action models with modern recurrent architectures (xLSTM and Mamba) to enable fast inference. LRAM processes sequences autoregressively with linear-time complexity, conditioning on current state, reward, and return-to-go (RTG) tokens to predict the next action. The architecture uses a shared action head that discretizes continuous actions, and crucially removes actions from the context to prevent the "copycat problem" in continuous control domains. The model is trained on 894 million transitions from 432 tasks across six domains using cross-entropy loss.

## Key Results
- LRAM achieves linear-time inference complexity (O(n)) versus quadratic for Transformers (O(n²))
- On 432 tasks across 6 domains, LRAM matches or outperforms Transformer-based LAMs in normalized scores
- Inference latency improves dramatically for long sequences: on Atari Freeway with context length 1600, LRAM is 8.3× faster than Transformer
- LRAM demonstrates superior sequence length extrapolation, maintaining performance on longer sequences than seen during training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: xLSTM achieves linear-time inference by replacing quadratic self-attention with gated recurrent computations.
- Mechanism: The xLSTM architecture uses exponential gating and linear recurrences instead of self-attention matrices, reducing computational complexity from O(n²) to O(n) per inference step.
- Core assumption: Gating mechanisms can preserve the representational power of attention while enabling efficient recurrence.
- Evidence anchors:
  - [abstract]: "modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference."
  - [section]: "Consequently, our LRAM has a modern recurrent architecture at its core (see Figure 1), which comes with a parallel training and a recurrent inference mode."
  - [corpus]: Found related work on xLSTM variants (xLSTM 7B, Distil-xLSTM) supporting inference efficiency claims.

### Mechanism 2
- Claim: Removing actions from the context prevents the "copycat problem" in continuous control domains.
- Mechanism: In robotics environments, actions change smoothly; conditioning on previous actions allows the model to shortcut learning by copying rather than modeling underlying dynamics.
- Core assumption: Action sequences in robotics exhibit smooth transitions that can be exploited as shortcuts.
- Evidence anchors:
  - [section]: "We observed that the models become overly confident, which is problematic if poor initial actions are produced... by observing previous actions, the agent can learn shortcuts."
  - [section]: "This is because many robotics environments exhibit smoothly changing actions, and by observing previous actions, the agent can learn shortcuts."

### Mechanism 3
- Claim: Longer context sequences improve performance by allowing agents to observe and correct past mistakes.
- Mechanism: Extended context provides historical information about previous states and rewards, enabling the agent to learn from past errors and better predict future actions.
- Core assumption: Past interactions contain valuable information for future decision-making, particularly in complex environments.
- Evidence anchors:
  - [section]: "Importantly, the evaluation performance improves across domains as the sequence length increases, which indicates that the history helps to predict the next action (e.g., by observing mistakes made in the past)."
  - [section]: "This property is particularly interesting for in-context RL, which requires keeping multiple episodes in the context."

## Foundational Learning

- Concept: Behavior cloning vs. return-to-go conditioning
  - Why needed here: The paper compares models trained with and without return-to-go (RTG) tokens, which fundamentally changes how the agent learns to predict actions.
  - Quick check question: If you remove RTG tokens, can the model still prioritize high-reward actions at inference time?

- Concept: Multi-modal sequence representation
  - Why needed here: The architecture must handle diverse input types (images, continuous states, rewards) across different robotics tasks.
  - Quick check question: How does the model encode a 64x64 grayscale image versus a 39-dimensional continuous state vector?

- Concept: Discretization of continuous actions
  - Why needed here: The shared action head predicts discrete classes, requiring continuous actions to be binned into uniform intervals.
  - Quick check question: With 8 continuous action dimensions and 256 bins each, how many total discrete classes must the model predict?

## Architecture Onboarding

- Component map:
  - Input encoders: Separate CNN for images, FC network for low-dimensional states, linear layers for rewards/RTG
  - xLSTM backbone: mLSTM and sLSTM blocks in specified ratios ([7:1] or [1:0])
  - Shared action head: Discretized continuous actions + discrete actions from Atari/Procgen
  - Output: Predicted action class probabilities

- Critical path:
  1. Encode current state, reward, and RTG
  2. Concatenate with previous hidden state
  3. Pass through xLSTM layers
  4. Predict next action via shared head
  5. Update hidden state for next timestep

- Design tradeoffs:
  - Shared action head vs. autoregressive prediction: Faster inference but requires discretization
  - Removing actions from context: Better performance on continuous control but loses direct action information
  - sLSTM vs. mLSTM ratio: State-tracking ability vs. parallelization

- Failure signatures:
  - Poor performance on discrete control tasks when actions are removed from context
  - Degradation when sequence length is too short for complex environments
  - OOM errors with Transformer on long sequences

- First 3 experiments:
  1. Train a 16M parameter xLSTM [1:0] on a single Meta-World task with C=50, compare to Transformer baseline
  2. Measure inference latency on Atari Freeway with batch size 1, context length 50 vs 1600
  3. Ablation: Train with and without actions in context on DMControl tasks, measure MSE between consecutive actions in dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LRAM scale with model size and data volume in real-world robotics tasks beyond simulated environments?
- Basis in paper: [explicit] The paper discusses potential benefits of larger models and more data but limits experiments to simulated tasks.
- Why unresolved: The study focuses on simulated robotics benchmarks, leaving uncertainty about real-world applicability.
- What evidence would resolve it: Experiments evaluating LRAM in real robot environments with varying model sizes and training data volumes.

### Open Question 2
- Question: What is the impact of fine-tuning LRAM with online RL objectives compared to offline RL in complex, dynamic environments?
- Basis in paper: [explicit] The paper mentions that LRAM's recurrent architecture supports online fine-tuning but only demonstrates offline RL fine-tuning.
- Why unresolved: The experiments do not explore online RL fine-tuning scenarios or their effectiveness in practice.
- What evidence would resolve it: Empirical results comparing online and offline RL fine-tuning performance in challenging environments.

### Open Question 3
- Question: How do LRAM's in-context learning capabilities extend to more complex, real-world tasks with longer horizons and partial observability?
- Basis in paper: [inferred] The paper shows ICL abilities in a grid-world setting but acknowledges limitations in complexity.
- Why unresolved: The ICL experiments are limited to a simple grid-world environment, leaving questions about scalability to real-world tasks.
- What evidence would resolve it: Demonstrations of ICL performance on complex robotics tasks with long horizons and partial observability.

## Limitations

- The paper's latency measurements are primarily focused on Atari tasks, with less detailed analysis for other domains
- The mechanism for why removing actions from context prevents the "copycat problem" is based on observed performance patterns rather than rigorous causal analysis
- Real-world robotics applications are not evaluated, limiting understanding of practical deployment challenges

## Confidence

- **High Confidence**: The linear-time inference complexity claim for xLSTM is well-supported by architectural analysis and basic latency measurements.
- **Medium Confidence**: The claim about sequence length extrapolation benefits is supported by experiments but could benefit from more systematic analysis of scaling behavior across different domain types.
- **Medium Confidence**: The effectiveness of removing actions from context is empirically demonstrated but lacks deeper analysis of when this strategy succeeds or fails.

## Next Checks

1. **Latency Profiling Across Domains**: Measure and compare inference latency systematically across all six domains (Atari, Composuite, DMControl, Meta-World, Mimicgen, Procgen) with varying sequence lengths to confirm the claimed O(n) scaling advantage holds consistently.

2. **Copycat Problem Mechanism Study**: Design targeted experiments on continuous control tasks where action sequences have varying smoothness properties to isolate when action conditioning helps versus harms performance, and measure the actual frequency of copycat behavior in the training data.

3. **Scaling Analysis for Long Sequences**: Conduct controlled experiments varying context lengths from 50 to 1600 timesteps across all domains, measuring both performance and memory usage to quantify the practical limits of sequence length scaling for both xLSTM and Transformer architectures.