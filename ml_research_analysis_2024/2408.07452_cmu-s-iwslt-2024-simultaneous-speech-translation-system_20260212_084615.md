---
ver: rpa2
title: CMU's IWSLT 2024 Simultaneous Speech Translation System
arxiv_id: '2408.07452'
source_url: https://arxiv.org/abs/2408.07452
tags:
- speech
- translation
- offline
- simultaneous
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CMU's submission to the IWSLT 2024 Simultaneous
  Speech Translation (SST) task for translating English speech to German text in a
  streaming manner. The proposed end-to-end speech-to-text (ST) system integrates
  the WavLM speech encoder, a modality adapter, and the Llama2-7B-Base model as the
  decoder.
---

# CMU's IWSLT 2024 Simultaneous Speech Translation System

## Quick Facts
- arXiv ID: 2408.07452
- Source URL: https://arxiv.org/abs/2408.07452
- Reference count: 8
- Key outcome: Offline BLEU score of 31.1 and BLEU score of 29.5 under 2 seconds latency on MuST-C-v2 tst-COMMON set

## Executive Summary
This paper presents CMU's submission to the IWSLT 2024 Simultaneous Speech Translation (SST) task for translating English speech to German text in a streaming manner. The proposed end-to-end speech-to-text (ST) system integrates the WavLM speech encoder, a modality adapter, and the Llama2-7B-Base model as the decoder. A two-stage training approach is employed, first aligning the representations of speech and text, followed by full fine-tuning on MuST-c v2 data with cross-entropy loss. The offline ST model is adapted for SST using a simple fixed hold-n policy. Experiments show that the model obtains an offline BLEU score of 31.1 and a BLEU score of 29.5 under 2 seconds latency on the MuST-C-v2 tst-COMMON set.

## Method Summary
The system uses WavLM speech encoder with modality adapter and Llama2-7B decoder, trained using a two-stage approach on MuST-C v2 data. First stage freezes the LLM while training encoder and adapters to align speech and text representations. Second stage fine-tunes the entire model. For SST, the offline model is adapted using a hold-n policy with 2.5-second chunks and withholding the last 7 tokens.

## Key Results
- Offline BLEU score: 31.1 on MuST-C-v2 tst-COMMON set
- SST BLEU score: 29.5 with average lagging of 1.96 seconds
- WavLM encoder improves BLEU by 1.1 compared to Wav2Vec2.0
- 66.5% reduction in AL from offline (5.85s) to SST (1.96s)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WavLM encoder outperforms Wav2Vec2.0 in ST quality due to better pretraining on larger, more diverse speech datasets.
- Mechanism: WavLM was pretrained on 94,000 hours of data including LibriLight, VoxPopuli, and GigaSpeech, while Wav2Vec2.0 used only 53.2k hours. The richer pretraining allows WavLM to produce more robust speech embeddings for translation.
- Core assumption: Speech encoder quality directly translates to translation quality in end-to-end ST models.
- Evidence anchors: Our results show that replacing Wav2vec with WavLM yields a significant improvement: a 1.1 BLEU score increase when using the Tower LLM as the decoder, and a 0.3 BLEU score increase with LLaMA2 as the decoder.

### Mechanism 2
- Claim: The two-stage training approach effectively balances encoder alignment with LLM preservation.
- Mechanism: Stage 1 freezes the LLM while training encoder and adapters to align speech and text representations. Stage 2 fine-tunes the entire model. This prevents catastrophic forgetting of the LLM's translation capabilities while adapting it to speech input.
- Core assumption: LLMs pretrained on text have learned generalizable translation capabilities that can be adapted to speech input without being destroyed by fine-tuning.
- Evidence anchors: We employ a two-stage training approach: initially, we align the representations of speech and text, followed by full fine-tuning. We employ a two-stage training approach: initially, we finetune the speech encoder together with the adapters, while keeping the LLM frozen. In the second stage, we finetune the entire model.

### Mechanism 3
- Claim: Hold-n policy with fixed chunk size provides a good latency-quality tradeoff for SST.
- Mechanism: The system processes speech in 2.5-second chunks, with the last 7 tokens withheld until more context is available. This simple policy avoids complex lookahead while maintaining reasonable translation quality.
- Core assumption: Withholding a fixed number of tokens is sufficient to maintain translation quality while reducing latency compared to full offline models.
- Evidence anchors: We adapt our offline ST model for streaming inference using hold-n policy... We employ a hold-n strategy with n set to 7, meaning that the last 7 tokens of each chunk are withheld until more context is available. From ST to SST, we observe a 5% quality degradation (31.1 to 29.5 SacreBLEU). However, this comes with significant latency improvements. The Average Lagging (AL) decreases from 5.85 to 1.96 seconds, a 66.5% reduction.

## Foundational Learning

- Concept: Speech-to-text modality alignment
  - Why needed here: The system must convert speech features into text-like embeddings that an LLM can process. Understanding how to align different modalities is crucial for adapting LLMs to speech tasks.
  - Quick check question: What is the role of the modality adapter in converting speech features to LLM-compatible embeddings?

- Concept: Streaming vs offline translation tradeoffs
  - Why needed here: SST requires balancing translation quality against latency constraints. Understanding the fundamental tradeoffs helps in selecting appropriate policies and evaluating system performance.
  - Quick check question: How does the hold-n policy reduce latency compared to an offline model, and what is the quality tradeoff?

- Concept: Pretraining and fine-tuning strategies for multimodal models
  - Why needed here: The two-stage training approach relies on understanding how to preserve pretrained capabilities while adapting to new modalities. This is crucial for building effective SST systems.
  - Quick check question: Why does the system use a two-stage training approach instead of fine-tuning the entire model from the start?

## Architecture Onboarding

- Component map: WavLM speech encoder → length adapter (2 conv layers) → modality adapter (linear projection) → Llama2-7B decoder → beam search with hold-n policy
- Critical path: Speech input → WavLM encoding → temporal downsampling → embedding projection → LLM decoding → incremental output with token withholding
- Design tradeoffs: Simple hold-n policy vs more sophisticated policies like monotonic chunkwise attention; larger LLM vs smaller encoder; fixed vs adaptive chunk sizes
- Failure signatures: Quality drops when hold-n value is too low; latency increases when chunk size is too large; alignment issues between speech and text representations
- First 3 experiments:
  1. Test different hold-n values (3, 7, 15) to find the optimal quality-latency tradeoff
  2. Compare WavLM with Wav2Vec2.0 encoders while keeping all other components constant
  3. Evaluate the impact of different chunk sizes (1000ms, 2500ms, 5000ms) on translation quality and latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the WavLM-LLaMA2 model change when using different hold-n values (n) in the SST adaptation?
- Basis in paper: The paper mentions using a hold-n strategy with n set to 7 for simultaneous translation, but does not explore the impact of varying this value.
- Why unresolved: The paper does not provide experimental results with different hold-n values, so the optimal value for balancing latency and translation quality remains unknown.
- What evidence would resolve it: Experimental results showing translation quality (BLEU score) and latency (AL and LAAL) for different hold-n values would help determine the optimal setting.

### Open Question 2
- Question: What is the impact of using different speech encoders (e.g., Wav2vec, HuBERT) on the performance of the offline ST model?
- Basis in paper: The paper compares WavLM with a CTC fine-tuned Wav2vec 2.0 model, showing that WavLM performs better. However, it does not explore other speech encoders.
- Why unresolved: The paper only compares two speech encoders, leaving the question of how other encoders might perform unanswered.
- What evidence would resolve it: Experimental results comparing the performance of the offline ST model using different speech encoders (e.g., HuBERT, XLS-R) would provide insights into the impact of encoder choice.

### Open Question 3
- Question: How does the performance of the WavLM-LLaMA2 model change when using different system prompts?
- Basis in paper: The paper mentions using a specific system prompt but does not explore the impact of different prompts on performance.
- Why unresolved: The paper does not provide experimental results with different system prompts, so the effect of prompt choice on translation quality remains unknown.
- What evidence would resolve it: Experimental results showing translation quality (BLEU score) for different system prompts would help determine the optimal prompt for the task.

### Open Question 4
- Question: What is the impact of using different chunk sizes in the SST adaptation?
- Basis in paper: The paper mentions using a chunk size of 2500ms for simultaneous translation but does not explore the impact of varying this value.
- Why unresolved: The paper does not provide experimental results with different chunk sizes, so the optimal value for balancing latency and translation quality remains unknown.
- What evidence would resolve it: Experimental results showing translation quality (BLEU score) and latency (AL and LAAL) for different chunk sizes would help determine the optimal setting.

## Limitations
- The fixed parameters (2-second chunks, 7-token withholding) may not generalize well to languages with different syntactic structures or speech with varying pacing
- Limited evaluation scope with no ablation studies isolating individual component contributions
- Key architectural details remain underspecified, particularly modality adapter configuration and training hyperparameters

## Confidence

**High Confidence**: The reported BLEU scores (31.1 offline, 29.5 SST) and latency metrics (AL reduction from 5.85 to 1.96 seconds) appear technically sound and internally consistent. The two-stage training approach is well-justified and follows established practices in multimodal adaptation.

**Medium Confidence**: The claim that WavLM provides significant improvement over Wav2Vec2.0 is supported by the reported 1.1 BLEU increase, but lacks direct comparative experiments or ablation studies. The effectiveness of the hold-n policy is demonstrated through latency-quality tradeoff but hasn't been compared against alternative streaming approaches.

**Low Confidence**: The assertion that the modality adapter successfully bridges the speech-text representation gap is based on end-to-end performance rather than intermediate analysis of the learned representations. The paper doesn't provide evidence that the adapter is learning meaningful cross-modal transformations rather than just serving as a bypass.

## Next Checks
1. **Component Ablation Study**: Systematically disable or replace individual components (WavLM encoder, modality adapter, two-stage training) to quantify their individual contributions to the final performance. This would reveal whether the 1.1 BLEU improvement from WavLM is consistent across different decoder choices.

2. **Policy Robustness Evaluation**: Test the hold-n policy with varying values of n (3, 7, 15) and chunk sizes (1s, 2s, 5s) across multiple test sets to determine if the chosen configuration (n=7, 2s chunks) represents a global optimum or is overfit to the evaluation set.

3. **Cross-Lingual Generalization**: Evaluate the system on non-German target languages from the MuST-C corpus to assess whether the modality adapter and hold-n policy generalize beyond the development language pair, or if performance degrades significantly on languages with different syntactic structures.