---
ver: rpa2
title: 'FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering'
arxiv_id: '2410.04526'
source_url: https://arxiv.org/abs/2410.04526
tags:
- question
- answer
- questions
- should
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FAMMA is a multilingual, multimodal financial reasoning benchmark
  with 1,945 Basic and 103 LivePro questions. It covers 8 finance subfields, requires
  advanced knowledge and calculation, and includes English, Chinese, and French text
  with charts and tables.
---

# FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering

## Quick Facts
- arXiv ID: 2410.04526
- Source URL: https://arxiv.org/abs/2410.04526
- Reference count: 40
- Primary result: State-of-the-art models achieve only 30-60% accuracy on challenging financial reasoning tasks

## Executive Summary
FAMMA introduces a comprehensive multilingual, multimodal financial reasoning benchmark designed to evaluate advanced question-answering capabilities in the financial domain. The benchmark encompasses 1,945 Basic questions and 103 LivePro questions spanning 8 finance subfields, requiring both domain expertise and complex arithmetic computation. The evaluation framework incorporates English, Chinese, and French text alongside charts and tables, creating a realistic challenge for modern language models.

The benchmark reveals significant performance gaps between state-of-the-art models and human-level reasoning capabilities. While models like GPT-o1 and DeepSeek-R1 show promise, they struggle with the multimodal complexity and domain-specific reasoning required by LivePro questions. The evaluation methodology includes innovative approaches like Program-of-Thoughts prompting and fine-tuning with DeepSeek-R1 reasoning trajectories, demonstrating the potential for targeted improvements in model performance on financial reasoning tasks.

## Method Summary
The FAMMA benchmark employs a multi-stage evaluation framework combining automated grading with human oversight. The benchmark includes Basic questions for foundational assessment and LivePro questions for advanced multimodal reasoning challenges. Evaluation utilizes GPT-4o as a grading mechanism while incorporating specialized prompting strategies including Program-of-Thoughts for arithmetic tasks. Fine-tuning approaches leverage DeepSeek-R1 reasoning trajectories, with budget forcing techniques applied to improve model performance. The methodology emphasizes cross-lingual consistency across English, Chinese, and French language variants while maintaining rigorous standards for multimodal reasoning assessment.

## Key Results
- State-of-the-art models achieve only 30-60% accuracy on LivePro questions, significantly below human performance levels
- Python-based Program-of-Thoughts prompting improves arithmetic performance significantly across models
- Fine-tuning on 1,270 DeepSeek-R1 reasoning trajectories yields consistent gains across Qwen models, particularly on non-arithmetic questions

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive design that combines domain-specific knowledge requirements with multimodal reasoning challenges. By incorporating multiple languages and requiring interpretation of charts, tables, and text, FAMMA creates realistic financial reasoning scenarios that expose limitations in current language models. The use of advanced prompting strategies and fine-tuning approaches demonstrates how targeted improvements can address specific weaknesses in model performance.

## Foundational Learning
- **Multimodal Financial Reasoning**: Understanding how to integrate textual, numerical, and visual information in financial contexts is essential for realistic evaluation. Quick check: Verify models can correctly interpret combined chart-text information.
- **Cross-Lingual Consistency**: Maintaining evaluation standards across different languages requires careful attention to cultural and linguistic nuances. Quick check: Compare model performance consistency across language variants.
- **Program-of-Thoughts Prompting**: This approach breaks down complex reasoning tasks into executable steps, particularly effective for arithmetic problems. Quick check: Measure improvement in arithmetic question accuracy with PoT prompting.
- **Fine-Tuning with Reasoning Trajectories**: Using high-quality reasoning examples improves model performance on similar tasks. Quick check: Compare performance gains between different fine-tuning datasets.
- **Budget Forcing**: Applying computational constraints during training improves efficiency and generalization. Quick check: Measure trade-offs between performance and computational resources.
- **Multimodal Integration**: Combining different data types requires sophisticated attention mechanisms and processing pipelines. Quick check: Test model ability to switch between text, chart, and table processing.

## Architecture Onboarding
Component map: Data Ingestion -> Preprocessing -> Multimodal Fusion -> Reasoning Engine -> Output Generation
Critical path: Question parsing → Domain knowledge retrieval → Multimodal interpretation → Arithmetic computation → Answer validation
Design tradeoffs: Language coverage vs. depth of financial domain expertise; multimodal complexity vs. computational efficiency; automated grading vs. human oversight
Failure signatures: Performance degradation on cross-lingual questions; arithmetic computation errors; inconsistent multimodal interpretation
First experiments: 1) Baseline model performance on Basic questions, 2) Multimodal reasoning accuracy on LivePro subset, 3) Cross-lingual performance comparison

## Open Questions the Paper Calls Out
None

## Limitations
- The small number of LivePro questions (103) may limit statistical robustness for certain analyses and model comparisons
- Heavy reliance on GPT-4o for grading introduces potential subjectivity and inconsistency in scoring complex multimodal reasoning tasks
- Substantial human effort in translation and annotation could introduce subtle biases across languages
- Python-based Program-of-Thoughts approach tested primarily on arithmetic questions may not generalize to full reasoning spectrum
- Fine-tuning methodology requires substantial computational resources, limiting accessibility

## Confidence
- **High Confidence**: Benchmark successfully captures multimodal financial reasoning challenges requiring domain expertise and arithmetic computation; substantial performance gaps between models and human-level performance are consistent
- **Medium Confidence**: Effectiveness of fine-tuning with DeepSeek-R1 trajectories and budget forcing techniques; limited by small LivePro sample size and single-model dependency
- **Medium Confidence**: Multilingual aspects, particularly for French and Chinese; translation process and cultural/linguistic nuances introduce uncertainty in cross-lingual comparisons

## Next Checks
1. Expand LivePro Evaluation: Conduct reliability analysis with multiple independent human experts grading a subset of LivePro questions to establish inter-rater reliability and identify inconsistencies in GPT-4o grading mechanism
2. Cross-Lingual Robustness Testing: Systematically test whether performance degradation on non-English questions stems from translation quality, language-specific terminology, or genuine reasoning complexity differences
3. Generalization of Program-of-Thoughts: Validate whether Python-based Program-of-Thoughts prompting generalizes beyond arithmetic questions by testing on full range of LivePro question types and comparing against alternative prompting strategies