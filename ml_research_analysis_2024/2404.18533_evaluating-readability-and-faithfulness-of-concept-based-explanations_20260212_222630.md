---
ver: rpa2
title: Evaluating Readability and Faithfulness of Concept-based Explanations
arxiv_id: '2404.18533'
source_url: https://arxiv.org/abs/2404.18533
tags:
- uni000003ec
- uni00000358
- uni000003ed
- uni00000190
- uni00000372
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a formal framework for evaluating concept-based
  explanations in large language models. The authors define concepts as virtual neurons
  and quantify their faithfulness through perturbation in the hidden space, ensuring
  adequate perturbation via an optimization problem.
---

# Evaluating Readability and Faithfulness of Concept-based Explanations

## Quick Facts
- arXiv ID: 2404.18533
- Source URL: https://arxiv.org/abs/2404.18533
- Reference count: 34
- Primary result: Coherence-based readability measures correlate highly with human evaluation and outperform existing LLM-based methods

## Executive Summary
This paper introduces a formal framework for evaluating concept-based explanations in large language models. The authors define concepts as virtual neurons and quantify their faithfulness through perturbation in the hidden space, ensuring adequate perturbation via an optimization problem. Readability is approximated by measuring the coherence of patterns that maximally activate a concept, using semantic similarity measures on input and output sides. The evaluation is supported by a meta-evaluation approach based on measurement theory, assessing reliability and validity of the proposed measures. Experimental results show that the proposed coherence-based readability measures correlate highly with human evaluation and outperform existing LLM-based methods.

## Method Summary
The framework defines concepts as virtual neurons with activation functions mapping hidden representations to real values, generalizing across diverse concept-based explanation methods. Faithfulness is quantified through perturbation-based measures that optimize for adequate perturbation in the hidden space. Readability is approximated using coherence measures that extract patterns maximally activating concepts from large corpora and measure their semantic similarity. The meta-evaluation approach applies measurement theory to assess reliability (test-retest correlation, subset consistency, inter-rater reliability) and validity (concurrent, convergent, divergent) of the evaluation measures. The framework is validated across different model layers and backbones.

## Key Results
- Four measures identified as unreliable: LLM-Score, GRAD-Loss, IN-UCI, IN-UMass
- Coherence-based readability measures show high correlation with human evaluation
- Meta-evaluation confirms remaining measures' effectiveness for both faithfulness and readability
- Consistent performance across different model layers and backbones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The formal definition of concepts as virtual neurons generalizes across diverse concept-based explanation methods
- Mechanism: By representing concepts as activation functions mapping hidden representations to real values, the framework unifies supervised (TCAV, CBM), unsupervised (Neuron, SAE), and interpretable-by-design methods under a single mathematical formulation
- Core assumption: The diversity of concept-based methods can be captured by varying the activation function while maintaining the core mapping from hidden space to concept activation
- Evidence anchors:
  - [abstract] "we introduce a formal definition of concepts generalizing to diverse concept-based explanations' settings"
  - [section] "we provide a formal definition of a concept, which can generalize to both supervised and unsupervised, post-hoc and interpretable-by-design methods"
  - [corpus] Weak - no direct corpus evidence for this specific unification claim
- Break condition: If a concept-based method cannot be expressed as an activation function mapping from hidden space to a real value

### Mechanism 2
- Claim: Coherence-based readability measures provide reliable approximation of human evaluation at scale
- Mechanism: By extracting patterns that maximally activate concepts from large corpora and measuring their semantic similarity using embedding-based metrics, the approach captures human-understandable patterns without requiring expensive human evaluation
- Core assumption: Semantic similarity of maximally activating patterns correlates with human perception of concept readability
- Evidence anchors:
  - [abstract] "Readability is approximated via an automatic and deterministic measure, quantifying the coherence of patterns that maximally activate a concept while aligning with human understanding"
  - [section] "we propose novel measures inspired by topic coherence... This automatic measure correlates highly with human evaluation"
  - [corpus] Moderate - the corpus mentions related work on concept-based explanation evaluation but doesn't directly validate the coherence approach
- Break condition: If semantic similarity measures fail to capture human perception of concept coherence, or if maximally activating patterns don't represent the concept well

### Mechanism 3
- Claim: Meta-evaluation using measurement theory provides rigorous assessment of evaluation measure quality
- Mechanism: By applying reliability metrics (test-retest correlation, subset consistency, inter-rater reliability) and validity metrics (concurrent, convergent, divergent validity), the framework systematically evaluates which measures are trustworthy
- Core assumption: Established measurement theory metrics can be meaningfully applied to evaluate the quality of concept-based explanation evaluation measures
- Evidence anchors:
  - [abstract] "based on measurement theory, we apply a meta-evaluation method for evaluating these measures, generalizable to other types of explanations or tasks as well"
  - [section] "we apply the classic measurement theory to perform a meta-evaluation on the faithfulness and readability measures"
  - [corpus] Moderate - the corpus shows related work on concept explanation evaluation but doesn't detail the meta-evaluation approach
- Break condition: If measurement theory metrics don't appropriately capture the unique challenges of evaluating concept-based explanations

## Foundational Learning

- Concept: Virtual neuron formalization of concepts
  - Why needed here: Provides the mathematical foundation that unifies diverse concept-based explanation methods
  - Quick check question: How would you represent a TCAV concept as a virtual neuron activation function?

- Concept: Coherence-based readability measurement
  - Why needed here: Enables scalable, automatic evaluation of concept readability without expensive human evaluation
  - Quick check question: What's the difference between UCI and UMass coherence measures, and when would each be appropriate?

- Concept: Meta-evaluation using measurement theory
  - Why needed here: Provides systematic framework for determining which evaluation measures are reliable and valid
  - Quick check question: What's the difference between test-retest reliability and subset consistency, and why are both important?

## Architecture Onboarding

- Component map:
  Concept extraction layer (virtual neuron formulation) -> Faithfulness evaluation module (perturbation-based measures) -> Readability evaluation module (coherence-based measures) -> Meta-evaluation framework (measurement theory application) -> Experimental pipeline (data processing, measure computation, correlation analysis)

- Critical path: Concept formalization → Pattern extraction → Measure computation → Meta-evaluation → Results analysis

- Design tradeoffs:
  - Using perturbation-based faithfulness vs. direct attribution methods
  - Choosing between different semantic similarity measures for readability
  - Balancing between human evaluation (gold standard) and automatic measures (scalable)

- Failure signatures:
  - Low test-retest reliability indicates measure instability
  - Poor concurrent validity suggests automatic measures don't align with human judgment
  - Low subset consistency indicates measure sensitivity to data sampling

- First 3 experiments:
  1. Validate that concept perturbation maintains the same activation function across different perturbation strategies
  2. Compare coherence-based readability measures against human evaluation on a small sample
  3. Test meta-evaluation metrics on synthetic data with known reliability/validity properties

## Open Questions the Paper Calls Out

What are the limitations of the current coherence-based readability measures, and how can they be improved to capture other aspects of readability, such as meaningfulness?

The authors acknowledge that topic coherence measures are not designed to be the ultimate solution for measuring readability and that other aspects of readability, such as meaningfulness, may also be worth exploring. To improve the current coherence-based measures, researchers could investigate how to quantify these other aspects of readability automatically, building a more comprehensive landscape of readability.

What is the impact of model size and layer depth on the reliability and validity of the proposed evaluation measures?

The authors conducted sensitivity analysis on different layers of Pythia-70M and the 6th layer of GPT-2 small. They found that reliability and validity results were consistent across these layers, with measures showing slightly better subset consistency in deeper layers. To further understand the impact of model size and layer depth, researchers could conduct experiments on larger models and different layers, analyzing how these factors affect the reliability and validity of the evaluation measures.

How can the proposed framework be extended to evaluate other types of explanations beyond concept-based explanations?

The authors mention that their meta-evaluation method based on measurement theory can be generalized to evaluate measures for other types of explanations and other natural language tasks. To extend the framework, researchers could apply the same evaluation methodology to other explanation methods, such as attention-based explanations or counterfactual explanations, and analyze the reliability and validity of the evaluation measures in these contexts.

## Limitations

- The perturbation-based faithfulness measures assume linear approximation holds in the hidden space, which may break down for highly non-linear concepts or deeper layers
- The readability coherence measures rely on semantic similarity metrics that may not fully capture human judgment of concept coherence, particularly for abstract or compositional concepts
- The meta-evaluation approach depends on human evaluation data that may not be representative of all concept types

## Confidence

- **High confidence** in the formal mathematical framework and perturbation optimization approach
- **Medium confidence** in the coherence-based readability measures' correlation with human judgment
- **Medium confidence** in the meta-evaluation methodology's ability to identify reliable measures
- **Low confidence** in generalization to concepts beyond the tested domains (linguistic, semantic, factual)

## Next Checks

1. Test the framework on adversarially constructed concepts where perturbation should fail to identify meaningful relationships, validating the sensitivity of the measures
2. Conduct ablation studies on the semantic similarity metrics used for readability to determine which specific components drive the correlation with human judgment
3. Apply the meta-evaluation framework to synthetic data with known ground truth properties to validate the reliability and validity metrics themselves