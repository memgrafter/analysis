---
ver: rpa2
title: Unrolled denoising networks provably learn optimal Bayesian inference
arxiv_id: '2409.12947'
source_url: https://arxiv.org/abs/2409.12947
tags:
- bayes
- denoisers
- prior
- matrix
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides the first rigorous learning guarantees for neural
  networks based on unrolling approximate message passing (AMP), proving that they
  can provably learn Bayes-optimal inference algorithms. The authors focus on compressed
  sensing, demonstrating that when trained on data drawn from a product prior, the
  network layers converge to the same denoisers used in Bayes AMP.
---

# Unrolled denoising networks provably learn optimal Bayesian inference

## Quick Facts
- arXiv ID: 2409.12947
- Source URL: https://arxiv.org/abs/2409.12947
- Reference count: 28
- Primary result: First rigorous learning guarantees showing unrolled denoising networks can learn Bayes-optimal inference algorithms for compressed sensing

## Executive Summary
This work provides the first rigorous learning guarantees for neural networks based on unrolling approximate message passing (AMP), proving that they can provably learn Bayes-optimal inference algorithms. The authors focus on compressed sensing, demonstrating that when trained on data drawn from a product prior, the network layers converge to the same denoisers used in Bayes AMP. Their proof combines state evolution and neural tangent kernel analysis, showing that the overparametrization needed is dimension-independent. Extensive experiments validate these theoretical findings, showing that unrolled networks match Bayes AMP performance and even outperform it in low-dimensional settings, non-Gaussian designs, and non-product priors.

## Method Summary
The paper introduces a learned denoising network (LDNet) based on unrolling AMP iterations. The network consists of layers that apply a linear transformation followed by a learned denoiser (implemented as a multilayer perceptron). Training is performed layerwise: each denoiser is trained sequentially while keeping previous layers frozen, then the entire network is fine-tuned. The theoretical analysis shows that under Gaussian designs and product priors, the learned denoisers converge to the optimal Bayes denoisers as the number of layers increases. Experiments validate this on compressed sensing tasks with Bernoulli-Gaussian and ℤ2 priors, as well as rank-one matrix estimation problems.

## Key Results
- Proved that unrolled denoising networks provably learn Bayes-optimal denoisers under Gaussian designs and product priors
- Demonstrated that LDNet achieves the same performance as Bayes AMP across multiple priors and observation models
- Showed LDNet outperforms Bayes AMP in finite-dimensional settings, non-Gaussian designs, and non-product priors when learning auxiliary parameters
- Provided theoretical guarantees with dimension-independent overparametrization requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Layerwise training of MLP denoisers with proper initialization leads to convergence to Bayes-optimal denoisers
- **Evidence:** State evolution analysis shows that the denoising error in each layer converges to the Bayes optimal denoising error as the number of layers increases
- **Why it matters:** This provides the first theoretical guarantee that unrolled networks can learn optimal inference algorithms without prior knowledge of the denoiser

## Foundational Learning

### Concept 1: Approximate Message Passing (AMP)
- **Why needed:** Forms the theoretical foundation for the unrolled network architecture
- **Quick check:** Verify understanding of AMP state evolution equations and their role in compressed sensing

### Concept 2: State Evolution
- **Why needed:** Provides the mathematical framework for analyzing convergence of denoisers in AMP
- **Quick check:** Confirm ability to derive and interpret state evolution equations for different priors

### Concept 3: Neural Tangent Kernel (NTK)
- **Why needed:** Enables analysis of overparametrized neural network training dynamics
- **Quick check:** Understand how NTK relates to convergence guarantees for deep networks

## Architecture Onboarding

### Component Map
Data -> Linear Transform -> Denoiser (MLP) -> Output -> Loss

### Critical Path
Training: Data generation → Layerwise denoiser training → Network fine-tuning → Performance evaluation
Inference: Input → Layerwise denoiser application → Output estimation

### Design Tradeoffs
- Fixed vs. learned linear transforms (B matrix)
- Number of layers vs. convergence speed
- Denoiser architecture depth/width vs. representation power
- Layerwise vs. end-to-end training approaches

### Failure Signatures
- Poor convergence if denoisers are not properly initialized
- Suboptimal performance if MLP architecture is too small
- Overfitting if sample size is insufficient relative to network capacity

### First Experiments
1. Generate synthetic compressed sensing data with Bernoulli-Gaussian prior
2. Implement LDNet with 3-layer MLPs and train layerwise
3. Compare NMSE performance against Bayes AMP baseline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the unrolled denoising networks provably learn Bayes-optimal denoisers for non-product priors, and if so, what architectural modifications are necessary?
- **Basis in paper:** The authors note that their theoretical results are limited to product priors and that extending to non-product settings requires proving that one can learn score functions of general data distributions.
- **Why unresolved:** The non-product setting requires learning d-dimensional denoisers instead of scalar ones, and proving convergence to optimal denoisers in this setting remains an open challenge in the theory of diffusion generative modeling.

### Open Question 2
- **Question:** Can the theoretical guarantees for unrolled denoising networks be extended to the rank-one matrix estimation setting, where multiple choices of state evolution parameters lead to the same denoising error?
- **Basis in paper:** The authors note that while closeness in denoising error implies closeness in state evolution parameters for compressed sensing, this is not immediate for rank-one matrix estimation.
- **Why unresolved:** The presence of multiple valid parameter choices in rank-one matrix estimation complicates the proof, as demonstrated by the learned denoisers having different functional forms from optimal denoisers at early iterations while achieving similar MSE.

### Open Question 3
- **Question:** What is the theoretical explanation for the performance advantages of unrolled denoising networks with auxiliary learnable parameters (like the "B matrix") over Bayes AMP in finite-dimensional and non-Gaussian design settings?
- **Basis in paper:** The authors observe that learning auxiliary parameters like the "B matrix" can mitigate scenarios where Bayes AMP is suboptimal or not known to be optimal, such as finite dimensionality and non-Gaussian sensing matrices.
- **Why unresolved:** While the authors provide experimental evidence for these advantages, they note that these are not yet supported by theory, and extending representational results for ISTA to rigorously characterize the "non-asymptotic corrections" imposed by these parameters remains an open question.

## Limitations

- Theoretical guarantees assume Gaussian designs and product priors, with limited coverage of non-Gaussian and non-product settings
- Sample complexity scaling is not fully characterized, with fixed sample size used in experiments
- Limited ablation studies on architectural choices and training strategies reduce understanding of practical robustness

## Confidence

- **High confidence:** Core theoretical result that unrolled networks converge to Bayes-optimal denoisers under stated assumptions
- **Medium confidence:** Experimental validation showing LDNet matches Bayes AMP performance across different priors and designs
- **Low confidence:** Extension of theoretical guarantees to non-Gaussian designs and non-product priors

## Next Checks

**Validation 1:** Architecture sensitivity analysis - systematically vary MLP architecture and training hyperparameters to determine impact on convergence and performance.

**Validation 2:** Sample complexity study - train LDNet on varying numbers of samples and analyze how performance scales with sample size.

**Validation 3:** Ablation of theoretical assumptions - design experiments targeting theoretical assumptions by varying conditions and measuring gaps between LDNet performance and theoretical predictions.