---
ver: rpa2
title: Causal Discovery from Time-Series Data with Short-Term Invariance-Based Convolutional
  Neural Networks
arxiv_id: '2408.08023'
source_url: https://arxiv.org/abs/2408.08023
tags:
- causal
- time
- window
- stic
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'STIC proposes a convolutional neural network approach for causal
  discovery from time-series data that leverages short-term time and mechanism invariance.
  The method uses a sliding window to extract multiple observations from the data,
  then applies two parallel convolutional blocks to identify causal relationships:
  a time-invariance block that captures common structural patterns and a mechanism-invariance
  block that learns transformation functions.'
---

# Causal Discovery from Time-Series Data with Short-Term Invariance-Based Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2408.08023
- Source URL: https://arxiv.org/abs/2408.08023
- Reference count: 9
- Key outcome: STIC achieves F1 scores of 0.45 on FMRI data and up to 0.80 on synthetic linear Gaussian datasets, showing state-of-the-art performance for causal discovery from time-series data.

## Executive Summary
STIC introduces a convolutional neural network approach for causal discovery from time-series data that leverages short-term time and mechanism invariance. The method uses a sliding window to extract multiple observations from the data, then applies two parallel convolutional blocks to identify causal relationships: a time-invariance block that captures common structural patterns and a mechanism-invariance block that learns transformation functions. This design improves sample efficiency and achieves state-of-the-art performance on both synthetic and FMRI benchmark datasets. Experimental results show significant improvements over baseline methods particularly when dealing with limited observed time steps.

## Method Summary
STIC is a causal discovery method that converts time-series data into a window representation using sliding windows, then applies two parallel convolutional blocks. The time-invariance block uses a shared convolution kernel to extract common causal patterns across all windows, while the mechanism-invariance block learns transformation functions between variables. The method is trained jointly through an auto-regressive loss function. The output is a directed acyclic graph representing causal relationships among variables, with performance evaluated using F1 score and precision metrics.

## Key Results
- STIC achieves F1 scores of 0.45 on FMRI benchmark datasets
- Performance reaches up to 0.80 F1 on synthetic linear Gaussian datasets
- Shows significant improvements over baseline methods, particularly with limited observed time steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sliding window representation improves sample efficiency by creating multiple overlapping observations from the same time series.
- Mechanism: By extracting overlapping window observations with step size 1, the model generates T - τ independent samples from a single T-length time series, effectively augmenting the dataset and allowing the network to learn common patterns across different time segments.
- Core assumption: Short-term time invariance holds, meaning causal relationships remain stable across adjacent window observations.
- Evidence anchors:
  - [abstract]: "STIC leverages both the short-term time and mechanism invariance of causality within each window observation"
  - [section]: "By sliding a window along the entire time-series data, STIC constructs batches of window observations that possess invariant characteristics"
  - [corpus]: Weak evidence - no direct corpus citations, but the concept aligns with standard data augmentation practices in deep learning
- Break condition: If the underlying causal relationships change significantly over short time periods, the window representation would mix inconsistent causal structures and degrade performance.

### Mechanism 2
- Claim: The equivalence between convolution operations and the underlying generative mechanism of time-series data justifies using CNNs for causal discovery.
- Mechanism: The paper derives that time-series data can be represented as a convolution of spatial (contemporaneous) and temporal (time-lagged) components, which maps naturally to CNN operations that can learn these patterns.
- Core assumption: The additive noise model is identifiable, allowing the decomposition of the generative process into Fourier integral forms.
- Evidence anchors:
  - [abstract]: "we theoretically derive the equivalence between convolution and the underlying generative principle of time-series data"
  - [section]: "it indicates that the observed dataset X can be obtained by convolving the convolution kernel with temporal information and the spatial details"
  - [corpus]: Weak evidence - no direct corpus citations supporting this specific theoretical derivation
- Break condition: If the additive noise model assumption fails or if the Fourier decomposition doesn't capture the true data generation process, the convolution-based approach would be misaligned with the actual causal structure.

### Mechanism 3
- Claim: The parallel block architecture with separate time-invariance and mechanism-invariance blocks enables joint learning of causal structure and transformation functions.
- Mechanism: The time-invariance block learns the causal graph structure using a shared convolution kernel across all windows, while the mechanism-invariance block learns the transformation functions between variables, and both are trained jointly through an auto-regressive loss.
- Core assumption: Short-term mechanism invariance holds, meaning the conditional probability distributions remain constant across adjacent windows.
- Evidence anchors:
  - [abstract]: "STIC uses the time-invariance block to capture the causal relationships among variables, while employing the mechanism-invariance block for the transform function"
  - [section]: "we employ a convolution kernel Km as f2. This kernel performs a Hadamard product operation with each window Wψ"
  - [corpus]: Weak evidence - no direct corpus citations, but the parallel architecture design is consistent with multi-task learning approaches
- Break condition: If mechanism invariance doesn't hold (e.g., non-stationary environments), the shared transformation function would be inappropriate for different time windows.

## Foundational Learning

- Concept: Convolutional Neural Networks and Fourier Transforms
  - Why needed here: The paper relies on the convolution theorem and Fourier analysis to justify the CNN architecture for time-series causal discovery
  - Quick check question: How does the convolution theorem relate the Fourier transform of a convolution to the product of individual Fourier transforms?

- Concept: Granger Causality and Auto-regressive Models
  - Why needed here: The method builds on Granger causality principles, using past values to predict future values to infer causal relationships
  - Quick check question: What is the mathematical condition that defines whether variable Xi Granger-causes variable Xj?

- Concept: Directed Acyclic Graphs (DAGs) and Causal Structure Learning
  - Why needed here: The output is a DAG representing causal relationships, and understanding DAG properties is crucial for interpreting results
  - Quick check question: Why must the estimated window causal matrix prohibit self-loops when the time lag is zero?

## Architecture Onboarding

- Component map: Input -> Window Representation -> Time-Invariance Block -> Mechanism-Invariance Block -> Joint Training -> Output
- Critical path: Window Representation → Time-Invariance Block → Mechanism-Invariance Block → Joint Training → Output
- Design tradeoffs: The parallel block design trades off architectural complexity for the ability to jointly learn structure and transformations, versus simpler single-block approaches
- Failure signatures: Poor performance on non-stationary data, sensitivity to maximum lag hyperparameter, degradation when short-term invariance assumptions fail
- First 3 experiments:
  1. Test on synthetic linear Gaussian data with varying T (100, 200, 500, 1000) to verify sample efficiency claims
  2. Compare F1 scores on FMRI benchmark data against PCMCI and DYNOTEARS to validate state-of-the-art performance
  3. Ablation study varying the threshold p parameter to understand its impact on precision vs recall tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can STIC be extended to handle non-additive noise distributions beyond the additive noise model assumption?
- Basis in paper: [explicit] The paper states "STIC has certain limitations that require further investigation" and specifically mentions "it becomes constrained when faced with non-additive noise."
- Why unresolved: The current framework relies on additive noise assumptions for theoretical equivalence between convolution and the generative mechanism, which may not hold for other noise types.
- What evidence would resolve it: Empirical testing of STIC variants with different noise distributions (e.g., multiplicative, heteroscedastic) and theoretical extensions showing convolution equivalence under non-additive noise conditions.

### Open Question 2
- Question: What are the fundamental limits on maximum lag estimation in causal discovery from time-series data, and how does this impact STIC's performance?
- Basis in paper: [explicit] The ablation study shows performance degradation when increasing the predefined maximum lag τ, with speculation about learning multi-hop causal edges as single-hop edges.
- Why unresolved: The relationship between lag estimation, sample size, and causal discovery accuracy remains unclear, particularly for longer lags where causal signals may become weak or ambiguous.
- What evidence would resolve it: Systematic analysis of STIC performance across varying lag lengths and sample sizes, establishing theoretical bounds on reliable lag detection.

### Open Question 3
- Question: How can STIC's window representation be generalized to handle irregularly sampled time-series data?
- Basis in paper: [inferred] While CUTS and CUTS+ methods are mentioned for irregular sampling, the paper focuses on regularly sampled data and doesn't explore window representation adaptations for irregular intervals.
- Why unresolved: The current window-based approach assumes uniform temporal spacing, but many real-world applications involve irregularly sampled observations.
- What evidence would resolve it: Development and validation of window-based causal discovery methods that can handle varying time intervals, potentially through adaptive window sizing or interpolation-free approaches.

### Open Question 4
- Question: What is the relationship between the threshold parameter p and the precision-recall trade-off in causal edge detection?
- Basis in paper: [explicit] The ablation study shows that varying p affects both F1 score and precision, with higher thresholds improving precision but potentially missing true edges.
- Why unresolved: The optimal threshold selection appears to be dataset-dependent, and the paper doesn't provide guidance on systematic threshold selection strategies.
- What evidence would resolve it: Empirical studies mapping threshold values to precision-recall curves across diverse datasets, potentially leading to adaptive threshold selection methods.

## Limitations
- Relies on short-term time and mechanism invariance assumptions that may not hold in non-stationary environments
- Performance metrics show moderate F1 scores (0.45) on FMRI data, suggesting room for improvement in real-world applications
- Several architectural details remain unspecified, particularly regarding the feed-forward network in the time-invariance block

## Confidence
- **Medium confidence** in core claims about STIC's architecture and theoretical justification, tempered by uncertainties about the convolution equivalence proof and unspecified architectural details
- **Medium confidence** in sample efficiency improvements, though performance appears dataset-dependent
- **Medium confidence** in state-of-the-art claims, with moderate performance on real-world FMRI data

## Next Checks
1. **Ablation study on invariance assumptions**: Systematically vary the window size and step parameters to quantify the impact of short-term invariance violations on F1 scores across different synthetic datasets with known ground truth.

2. **Cross-dataset robustness evaluation**: Test STIC on multiple FMRI datasets with varying sample sizes and noise levels to assess whether the 0.45 F1 score is consistent or dataset-specific.

3. **Theoretical validation of convolution equivalence**: Derive explicit mathematical conditions under which the convolution theorem applies to the proposed time-series generative model, and identify failure modes when these conditions are violated.