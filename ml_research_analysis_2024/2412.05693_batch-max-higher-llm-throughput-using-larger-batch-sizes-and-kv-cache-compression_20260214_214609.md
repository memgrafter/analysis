---
ver: rpa2
title: 'Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression'
arxiv_id: '2412.05693'
source_url: https://arxiv.org/abs/2412.05693
tags:
- cache
- pairs
- eviction
- input
- throughput
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method to increase the throughput of large
  language model (LLM) inference in memory-constrained settings where the input context
  is expected to be longer than the generation length. The core idea is to compress
  the key-value (KV) cache not only during token generation (decoding) but also during
  input processing (prefilling), allowing for larger batch sizes and more efficient
  GPU utilization.
---

# Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression

## Quick Facts
- arXiv ID: 2412.05693
- Source URL: https://arxiv.org/abs/2412.05693
- Reference count: 5
- Primary result: Batch-Max increases LLM inference throughput by 37.7% to 50.4% on average compared to upper bound of decoding-only compression, while maintaining accuracy â‰¥96.3% of full KV cache baseline.

## Executive Summary
This paper introduces Batch-Max, a method to increase large language model (LLM) inference throughput in memory-constrained settings by compressing the key-value (KV) cache during both input processing (prefilling) and token generation (decoding). The approach enables larger batch sizes and more efficient GPU utilization by applying block-wise average attention-based eviction to compress the KV cache. Experiments across multiple tasks and models demonstrate significant throughput improvements while maintaining high accuracy, outperforming existing decoding-only compression techniques.

## Method Summary
Batch-Max compresses the KV cache using an average attention-based eviction policy during both prefilling and decoding phases of LLM inference. The method divides the KV cache into blocks and applies compression to reduce memory footprint, allowing larger batch sizes within GPU memory constraints. By compressing during prefilling as well as decoding, Batch-Max increases GPU utilization compared to approaches that only compress during decoding. The technique is evaluated on Llama-2-13b-chat and Phi-3.5-mini-instruct models across CNN/DM, NarrativeQA, and GSM8K tasks.

## Key Results
- Batch-Max achieves 37.7% to 50.4% higher throughput on average compared to upper bound of decoding-only compression
- Maintains accuracy at least 96.3% of full KV cache (FKV) baseline across all evaluated tasks
- Outperforms existing KV cache compression methods by enabling larger batch sizes during both prefilling and decoding phases

## Why This Works (Mechanism)
The method works by recognizing that KV cache compression during prefilling (input processing) can significantly increase batch size, leading to better GPU utilization. By applying block-wise average attention-based eviction to compress the KV cache during both prefilling and decoding, Batch-Max maximizes throughput while minimizing accuracy degradation. The compression allows more sequences to be processed simultaneously within memory constraints, and the block-wise approach balances compression efficiency with computational overhead.

## Foundational Learning

**Key-Value Cache** - Stores intermediate attention computations to avoid recomputation during autoregressive generation. Essential for efficient LLM inference by reducing redundant calculations across tokens.

**Prefilling vs Decoding** - Prefilling processes input tokens to build initial KV cache, while decoding generates output tokens autoregressively. Different computational characteristics require distinct optimization strategies.

**Batch Size Optimization** - Larger batch sizes improve GPU utilization but are constrained by memory. KV cache compression techniques enable larger batches within fixed memory budgets.

**Average Attention-based Eviction** - Compression method that averages attention weights within cache blocks. Balances memory reduction with minimal impact on attention quality.

**GPU Memory Constraints** - Physical memory limits batch size and sequence length. Effective compression techniques are crucial for maximizing throughput within these constraints.

## Architecture Onboarding

**Component Map**: Input Sequences -> Prefilling (KV Cache Construction) -> Batch-Max Compression -> Decoding (Token Generation) -> Output Tokens

**Critical Path**: Input reception through prefilling to initial KV cache construction, followed by compression decision points during both prefilling and decoding phases, leading to token generation.

**Design Tradeoffs**: Memory vs Accuracy - More aggressive compression reduces memory usage but may impact output quality. Throughput vs Latency - Larger batches improve throughput but increase per-request latency. Complexity vs Performance - More sophisticated compression algorithms may provide better results but increase computational overhead.

**Failure Signatures**: Accuracy degradation beyond acceptable thresholds, memory allocation failures during batch processing, performance degradation from excessive compression overhead, and GPU utilization drops from suboptimal batch sizing.

**First Experiments**:
1. Baseline comparison: Full KV cache (FKV) vs Batch-Max across different batch sizes
2. Ablation study: Compression during prefilling only vs compression during both prefilling and decoding
3. Memory utilization analysis: GPU memory usage patterns with and without Batch-Max compression

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused on specific models (Llama-2-13b-chat and Phi-3.5-mini-instruct) and datasets, limiting generalizability
- Performance gains may not scale uniformly for significantly longer contexts or different model sizes
- Compression introduces accuracy degradation, though remaining above 96.3% of baseline
- Effectiveness in real-world, multi-tenant serving environments with heterogeneous request patterns not fully explored

## Confidence
- High: The core technical contribution of compressing KV cache during both prefilling and decoding to enable larger batch sizes is well-supported by the experiments and ablation studies.
- Medium: The generalization of throughput gains to other LLM architectures, tasks, and real-world serving scenarios requires additional validation.
- Medium: The accuracy trade-off analysis is limited to a small set of tasks and may not fully capture the impact across diverse use cases.

## Next Checks
1. Evaluate Batch-Max on a wider range of LLM architectures (e.g., different model families and sizes) and tasks to assess the generality of throughput gains.
2. Conduct a comprehensive analysis of the accuracy-throughput trade-off across diverse tasks and context lengths to establish the method's robustness.
3. Test Batch-Max in a realistic, multi-tenant serving environment with heterogeneous request patterns and varying context lengths to evaluate its effectiveness in production settings.