---
ver: rpa2
title: Characteristic AI Agents via Large Language Models
arxiv_id: '2403.12368'
source_url: https://arxiv.org/abs/2403.12368
tags:
- llms
- agents
- style
- have
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark for the task of creating characteristic
  AI agents using large language models (LLMs). The authors construct a dataset called
  "Character100" consisting of profiles of 106 well-known individuals sourced from
  Wikipedia.
---

# Characteristic AI Agents via Large Language Models

## Quick Facts
- arXiv ID: 2403.12368
- Source URL: https://arxiv.org/abs/2403.12368
- Authors: Xi Wang; Hongliang Dai; Shen Gao; Piji Li
- Reference count: 0
- One-line primary result: This paper introduces a benchmark for creating characteristic AI agents using LLMs, evaluating various models and techniques on the Character100 dataset.

## Executive Summary
This paper introduces a benchmark for constructing characteristic AI agents that simulate real-life individuals using large language models. The authors create the Character100 dataset containing profiles of 106 well-known individuals sourced from Wikipedia, along with utterance style corpora. They evaluate various open-source and closed-source LLMs across zero-shot, few-shot, and fine-tuned settings using LoRA and QLoRA techniques. The study focuses on two key aspects: background knowledge consistency (factual accuracy) and style consistency (mimicking speaking patterns). Results show that few-shot learning and instruction tuning significantly improve background knowledge consistency, while style consistency remains challenging and requires further improvement.

## Method Summary
The study constructs the Character100 dataset with profiles of 106 individuals and their associated query-response pairs. The authors employ various LLMs including Llama 2, ChatGLM2, Vicuna, Baichuan2, and ChatGPT across different settings: zero-shot prompting, few-shot in-context learning, and fine-tuning using LoRA and QLoRA techniques. Evaluation focuses on background knowledge consistency using BLEU, ROUGE-L, and semantic similarity metrics, along with style consistency assessed through a style discriminator's hit@k accuracy. The prompt template follows a role-play format where models answer queries as the profiled individual. Fine-tuning aims to enhance LLMs' understanding and performance on the characteristic AI agents task.

## Key Results
- Few-shot learning with in-context examples significantly improves background knowledge consistency compared to zero-shot prompting
- Instruction-tuned LLMs (Llama 2-Chat, ChatGPT) outperform base models on both background knowledge and style consistency metrics
- LoRA and QLoRA fine-tuning techniques enhance model performance, with instruction-tuned models showing better results
- Style consistency remains challenging, with room for improvement across all models and techniques
- Some models exhibit hallucination and bilingual switching problems, particularly when generating responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot learning with in-context examples improves background knowledge consistency.
- Mechanism: The prompt template incorporates an example of using a profile to answer a query, allowing the model to learn the expected format and content generation pattern through demonstration.
- Core assumption: The model can generalize from a single example to unseen profiles and queries.
- Evidence anchors:
  - [abstract] "few-shot learning improves background knowledge consistency"
  - [section] "The difference between few-shot setting and zero-shot setting is that the prompts in few-shot setting contain an example of utilizing the profile to answer the question"
- Break condition: If the example provided is too dissimilar from the test cases, the model fails to transfer the learned pattern.

### Mechanism 2
- Claim: Instruction-tuned LLMs perform better on the characteristic AI agents task.
- Mechanism: Models like Llama 2-Chat and ChatGPT that have been fine-tuned with human feedback instructions are better at following the role-play prompt format and generating more contextually appropriate responses.
- Core assumption: Instruction tuning improves the model's ability to understand and execute complex task instructions.
- Evidence anchors:
  - [abstract] "instruction tuning helps LLMs perform better"
  - [section] "Instruction-tuned LLMs exhibit significantly better performance...base models that are not instruction-tuned...perform less effectively"
- Break condition: If the instruction tuning is too generic and not tailored to the specific task of role-playing, the improvement may be limited.

### Mechanism 3
- Claim: Fine-tuning with LoRA and QLoRA enhances model performance on the task.
- Mechanism: By adapting the model weights to the characteristic AI agents dataset, the model learns the patterns of profile utilization and response generation specific to this task.
- Core assumption: The dataset used for fine-tuning is representative of the task's requirements and covers a diverse set of profiles.
- Evidence anchors:
  - [abstract] "we establish baseline performance benchmarks for this task by employing diverse training techniques, including fine-tuning LLMs on the dataset using various methodologies like LoRA and QLoRA"
  - [section] "we employ two fine-tuning techniques: LoRA and QLoRA with the aim of enhancing the LLMs' understanding and performance of the characteristic AI agents task"
- Break condition: If the fine-tuning dataset is too small or not diverse enough, the model may overfit and not generalize well to new profiles.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL is the core mechanism by which the model learns to perform the task from examples provided in the prompt without additional training.
  - Quick check question: What is the difference between zero-shot and few-shot learning in the context of this task?
- Concept: Fine-tuning techniques (LoRA, QLoRA)
  - Why needed here: These parameter-efficient fine-tuning methods allow the model to adapt to the specific task without the computational cost of full fine-tuning.
  - Quick check question: How do LoRA and QLoRA differ from traditional fine-tuning approaches?
- Concept: Evaluation metrics for background knowledge and style consistency
  - Why needed here: These metrics provide a quantitative way to assess the model's performance on the two key aspects of the task: generating factually correct responses and mimicking the speaking style of the role.
  - Quick check question: What are the two main aspects of evaluation in this task and why are they important?

## Architecture Onboarding

- Component map:
  - Character100 dataset (profiles + utterance style corpus) -> LLMs (Llama 2, ChatGLM2, Vicuna, Baichuan2, ChatGPT) -> Prompt templates (zero-shot/few-shot) -> Evaluation metrics (BLEU, ROUGE, semantic similarity, style discriminator)
- Critical path:
  - Prepare dataset -> Generate prompts (zero-shot/few-shot) -> Run model inference -> Evaluate using metrics
  - Prepare dataset -> Fine-tune model (LoRA/QLoRA) -> Generate prompts -> Run inference -> Evaluate
- Design tradeoffs:
  - Zero-shot vs. few-shot: Few-shot provides better performance but requires careful example selection
  - LoRA vs. QLoRA: QLoRA is more memory-efficient but may have slightly lower performance
  - Automatic vs. human evaluation: Automatic metrics are faster but may miss nuanced aspects of style consistency
- Failure signatures:
  - Hallucination: Model generates information not present in the profile
  - Bilingual switching: Model mixes languages or uses inappropriate language for the role
  - Style inconsistency: Generated responses do not match the expected speaking style of the role
- First 3 experiments:
  1. Run zero-shot inference on Llama 2-Chat with the provided prompt template
  2. Run few-shot inference on Llama 2-Chat with an example in the prompt
  3. Fine-tune Llama 2-Chat on the training set using LoRA and evaluate on the validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt templates affect the performance of LLMs in the characteristic AI agents task?
- Basis in paper: [inferred] The paper mentions that the construction of templates may influence large language model performance, but constraints on time prevented the testing of all template variants for LLMs.
- Why unresolved: The paper did not explore the impact of different prompt templates on LLM performance in the characteristic AI agents task.
- What evidence would resolve it: Conducting experiments with various prompt templates and comparing the performance of LLMs in the characteristic AI agents task.

### Open Question 2
- Question: How does the performance of LLMs in the characteristic AI agents task vary across different domains or professions?
- Basis in paper: [explicit] The paper states that the Character100 dataset includes profiles of 106 well-known individuals from various domains, such as singers, actors, athletes, and political leaders.
- Why unresolved: The paper does not provide a detailed analysis of how LLM performance in the characteristic AI agents task varies across different domains or professions.
- What evidence would resolve it: Conducting experiments with LLMs on the characteristic AI agents task using profiles from different domains or professions and analyzing the performance variations.

### Open Question 3
- Question: What are the specific factors contributing to the bilingual switching problem observed in some LLMs?
- Basis in paper: [explicit] The paper mentions that some LLMs, such as ChatGLM2-6B, suffer from the bilingual switching problem, where they use Chinese punctuation in the profile or even produce a mix of Chinese and English responses.
- Why unresolved: The paper does not investigate the specific factors contributing to the bilingual switching problem in LLMs.
- What evidence would resolve it: Analyzing the training data, model architecture, and fine-tuning techniques used in LLMs to identify the factors that contribute to the bilingual switching problem.

## Limitations

- The Character100 dataset, while comprehensive with 106 profiles, may not fully represent the diversity of real-world personas that LLMs would encounter in practical applications.
- The automatic evaluation metrics used (BLEU, ROUGE-L, semantic similarity, and style discriminator accuracy) provide quantitative measures but may not capture the nuanced aspects of human-like conversation quality and contextual appropriateness.
- The study focuses primarily on text-based interactions, potentially limiting applicability to multimodal AI agent scenarios.

## Confidence

- **High Confidence**: The effectiveness of few-shot learning in improving background knowledge consistency is well-supported by the experimental results and directly observed in the study's findings.
- **Medium Confidence**: The superiority of instruction-tuned LLMs over base models is reasonably supported, though the study could benefit from more detailed comparisons across different instruction-tuning methodologies.
- **Medium Confidence**: The improvements from LoRA and QLoRA fine-tuning are demonstrated, but the relatively small scale of the Character100 dataset (106 profiles) may limit the strength of these conclusions for larger-scale applications.

## Next Checks

1. **Cross-dataset validation**: Test the fine-tuned models on an independent dataset of different personas to verify generalization beyond the Character100 dataset and assess whether the improvements in background knowledge and style consistency transfer to unseen profiles.

2. **Human evaluation study**: Conduct a comprehensive human evaluation where participants rate the quality, consistency, and authenticity of the AI agents' responses across different models and techniques, comparing these subjective assessments against the automatic metrics used in the study.

3. **Error analysis and failure mode investigation**: Systematically analyze cases where models exhibit hallucination or bilingual switching problems to identify specific patterns or types of profiles/queries that trigger these failures, potentially revealing limitations in the underlying LLMs or the fine-tuning approach.