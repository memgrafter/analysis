---
ver: rpa2
title: Multi-Modal Hallucination Control by Visual Information Grounding
arxiv_id: '2403.14003'
source_url: https://arxiv.org/abs/2403.14003
tags:
- m3id
- image
- llav
- tokens
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination in Vision-Language
  Models (VLMs), where generated text is not always grounded in the input image. The
  authors propose a method called Multi-Modal Mutual Information Decoding (M3ID) to
  reduce hallucinations by amplifying the influence of the visual prompt over the
  language prior during inference.
---

# Multi-Modal Hallucination Control by Visual Information Grounding

## Quick Facts
- arXiv ID: 2403.14003
- Source URL: https://arxiv.org/abs/2403.14003
- Reference count: 40
- Primary result: M3ID reduces hallucinated objects by 25% and improves POPE accuracy by 21% on LLaVA 13B

## Executive Summary
This paper addresses the problem of hallucination in Vision-Language Models (VLMs), where generated text is not always grounded in the input image. The authors propose Multi-Modal Mutual Information Decoding (M3ID) to reduce hallucinations by amplifying the influence of visual context over language priors during inference. The method dynamically adjusts the sampling distribution based on the difference between conditioned and unconditioned log probabilities, with a time-varying penalty that increases as more tokens are generated. M3ID can be applied to any pre-trained autoregressive VLM at inference time without additional training.

## Method Summary
The proposed method, M3ID, operates by computing the difference between conditioned (image-aware) and unconditioned log probabilities at each token generation step. This difference is added back into the conditioned log probability with a time-varying weight that increases as more tokens are generated. The method assumes that conditioned models "forget" visual context over time (conditioning dilution), and M3ID compensates for this by dynamically adjusting the sampling distribution. The authors also extend M3ID with Direct Preference Optimization (DPO) for training-based improvements, using self-generated preference data where the "preferred" continuation is sampled from the M3ID-conditioned model and the "unpreferred" continuation is sampled from the unconditioned model.

## Key Results
- M3ID reduces hallucinated objects by 25% on LLaVA 13B compared to baseline methods
- POPE VQA accuracy improves by 21% with M3ID on LLaVA 13B
- M3ID+DPO achieves 28% reduction in hallucinated objects and 24% improvement in POPE accuracy
- The method maintains linguistic fluency while improving visual grounding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: M3ID reduces hallucinations by amplifying the influence of visual context over language priors during generation.
- Mechanism: At each token step, M3ID computes the difference between conditioned and unconditioned log probabilities and adds it back into the conditioned log probability with a time-varying weight. This increases the likelihood of tokens that are more informative about the image.
- Core assumption: The conditioned distribution "forgets" the visual context over time (conditioning dilution).
- Evidence anchors:
  - [abstract]: "M3ID amplifies the influence of the visual prompt over the language prior during inference."
  - [section]: "We model the conditioned log probabilities... as an interpolation between the unconditioned model and a model l* that does not 'forget' old context as more tokens are generated."
- Break condition: If the model does not exhibit conditioning dilution (i.e., PDMs remain high throughout generation), the time-varying penalty will not be needed.

### Mechanism 2
- Claim: The DPO extension further reduces hallucinations by training the model to prefer grounded continuations.
- Mechanism: M3ID+DPO uses self-generated preference data where the "preferred" continuation is sampled from the M3ID-conditioned model and the "unpreferred" continuation is sampled from the unconditioned model. Training with DPO aligns the model to favor grounded outputs.
- Core assumption: The M3ID-conditioned model generates more grounded continuations than the unconditioned model.
- Evidence anchors:
  - [abstract]: "we show that M3ID can be paired with Direct Preference Optimization (DPO) to improve the model's reliance on the prompt image without requiring any labels."
  - [section]: "we propose to fine-tune a pre-trained VLM using the DPO objective while ensuring that the preferred continuations are more grounded to the visual information."
- Break condition: If the DPO objective does not improve grounding beyond M3ID alone, the additional training cost may not be justified.

### Mechanism 3
- Claim: The contextual pressure handling prevents overcompensation that could disrupt linguistic fluency.
- Mechanism: M3ID only applies the correction term when the conditioned model is not highly confident in its prediction. This avoids penalizing correct but obvious tokens that don't require visual context.
- Core assumption: High-confidence tokens from the conditioned model are likely correct and don't need correction.
- Evidence anchors:
  - [abstract]: "Accommodating for contextual pressure. Notice, however, that the contextual pressure forces lc and lu to be similar irrespective of the number of generated tokens."
  - [section]: "if the conditioned model lc is highly confident on the next token... we do not apply the correction term lc − lu."
- Break condition: If the confidence threshold is set too high, it may suppress necessary corrections and reduce hallucination reduction.

## Foundational Learning

- Concept: Mutual Information
  - Why needed here: M3ID maximizes the mutual information between generated text and the visual prompt to ensure grounded outputs.
  - Quick check question: What is the relationship between mutual information and the difference of log probabilities used in M3ID?

- Concept: Autoregressive Generation
  - Why needed here: VLMs generate text one token at a time, conditioning each token on previous ones and the image.
  - Quick check question: How does the order of token generation affect the conditioning dilution phenomenon?

- Concept: Preference Optimization
  - Why needed here: DPO is used to train the model to prefer grounded over ungrounded continuations.
  - Quick check question: What is the role of the Bradley-Terry preference model in DPO?

## Architecture Onboarding

- Component map:
  VLM (frozen vision encoder + language model) -> Two forward passes (conditioned + unconditioned) -> M3ID decoding logic (log probability difference + time-varying penalty) -> Output tokens

- Critical path:
  1. Get image c and prompt x
  2. For each token t:
     a. Compute conditioned log probs lc = log p(yt|y<t, x, c)
     b. Compute unconditioned log probs lu = log p(yt|y<t, x)
     c. Compute γt = exp(-λt)
     d. Apply correction if not highly confident: ˆl* = lc + 1{maxk(lc)k < log α} * (1-γt)/γt * (lc - lu)
     e. Sample yt from ˆl*

- Design tradeoffs:
  - M3ID requires two forward passes per token, doubling inference time but not memory usage
  - The λ parameter controls the strength of the fading memory assumption
  - The α parameter balances hallucination reduction against linguistic fluency

- Failure signatures:
  - If hallucinations persist, check if conditioning dilution is actually occurring (PDM-H should decrease over time)
  - If linguistic fluency is disrupted, the α threshold may be too low
  - If overcompensation occurs (missing obvious objects), the λ parameter may be too high

- First 3 experiments:
  1. Run M3ID with default hyperparameters on a small captioning dataset and measure CHAIR metrics
  2. Vary the λ parameter and observe its effect on PDM-H curves and hallucination rates
  3. Compare M3ID against PMI and Contrastive Decoding on the same dataset to validate improvements

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the abstract or conclusion sections. However, based on the content, several implicit open questions arise from the limitations and scope of the work.

## Limitations
- Effectiveness on non-autoregressive VLM architectures remains untested
- Computational overhead may vary significantly based on implementation details
- Self-supervised preference data quality assumption may not hold for all prompt types
- Hyperparameter sensitivity across different tasks and datasets needs more exploration

## Confidence
- High Confidence: Core M3ID mechanism and effectiveness on established benchmarks
- Medium Confidence: DPO extension improvements, dependent on self-generated preference quality
- Low Confidence: Contextual pressure handling claims, limited validation of overcompensation prevention

## Next Checks
1. Implement M3ID on diverse VLM architectures beyond LLaVA to test generalizability
2. Conduct systematic ablation study varying λ and α parameters across full ranges
3. Compare M3ID+DPO against alternative training-based hallucination reduction methods