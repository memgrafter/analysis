---
ver: rpa2
title: 'You Only Cache Once: Decoder-Decoder Architectures for Language Models'
arxiv_id: '2405.05254'
source_url: https://arxiv.org/abs/2405.05254
tags:
- yoco
- length
- language
- memory
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes YOCO, a decoder-decoder architecture for large
  language models that only caches key-value pairs once, significantly reducing GPU
  memory usage and speeding up prefill time. YOCO consists of a self-decoder that
  efficiently encodes global KV caches, and a cross-decoder that reuses these caches
  via cross-attention.
---

# You Only Cache Once: Decoder-Decoder Architectures for Language Models

## Quick Facts
- arXiv ID: 2405.05254
- Source URL: https://arxiv.org/abs/2405.05254
- Authors: Yutao Sun; Li Dong; Yi Zhu; Shaohan Huang; Wenhui Wang; Shuming Ma; Quanlu Zhang; Jianyong Wang; Furu Wei
- Reference count: 28
- Primary result: YOCO architecture reduces KV cache memory by up to 80x and prefill latency by over 70x for 1M context length

## Executive Summary
This paper introduces YOCO (You Only Cache Once), a novel decoder-decoder architecture for large language models that significantly reduces GPU memory usage and speeds up prefill time. The key innovation is caching key-value pairs only once through a self-decoder, then reusing these caches via a cross-decoder for efficient inference. YOCO achieves up to 80x reduction in KV cache memory and over 70x speedup in prefill latency while maintaining competitive language modeling performance. The architecture is extended to handle 1M context length with near-perfect needle retrieval accuracy.

## Method Summary
YOCO consists of two main components: a self-decoder that efficiently encodes global key-value (KV) caches, and a cross-decoder that reuses these caches through cross-attention. The self-decoder processes the input sequence once to generate and store KV caches, which are then shared across all layers of the cross-decoder during the prefill phase. This approach eliminates redundant computations and memory usage associated with standard decoder-only models. During autoregressive generation, YOCO switches to a conventional decoder-only mode. The architecture is designed to handle extremely long contexts (up to 1M tokens) while maintaining high efficiency and accuracy.

## Key Results
- Reduces KV cache memory by up to 80x compared to standard decoder-only models
- Achieves over 70x speedup in prefill latency for 1M context length
- Maintains competitive language modeling performance on MMLU benchmark
- Extends to 1M context length with near-perfect needle retrieval accuracy

## Why This Works (Mechanism)
YOCO works by fundamentally changing how key-value pairs are cached and reused during inference. Instead of each decoder layer independently computing and storing its own KV caches, YOCO's self-decoder computes a single set of global KV caches that are shared across all layers of the cross-decoder. This eliminates the quadratic memory growth typically seen with increasing context length. During prefill (the initial context processing phase), the cross-decoder performs cross-attention using these shared caches, dramatically reducing both memory footprint and computational overhead. The design leverages the observation that KV caches are often redundant across layers and can be computed once and reused efficiently.

## Foundational Learning

**Self-attention and cross-attention**: These are core mechanisms in transformer models where self-attention computes relationships within a sequence, while cross-attention relates two different sequences. YOCO uses self-attention in the self-decoder to build global KV caches, then cross-attention in the cross-decoder to reuse them.

Why needed: Understanding these attention mechanisms is crucial for grasping how YOCO optimizes cache usage and reduces redundancy.

Quick check: Can you explain the difference between self-attention and cross-attention and why sharing KV caches between them is beneficial?

**KV cache mechanism**: KV caches store the key and value vectors computed during self-attention to avoid recomputation in autoregressive generation. In standard models, each layer maintains its own KV cache.

Why needed: Knowing how KV caches work helps understand why YOCO's approach of sharing caches across layers is so impactful for memory efficiency.

Quick check: How does the size of KV caches typically scale with context length, and why is this a problem for long sequences?

**Prefill vs. autoregressive generation**: Prefill is the initial phase where the model processes the context before generating output tokens. Autoregressive generation is the token-by-token generation phase that follows.

Why needed: YOCO's optimizations primarily target the prefill phase, making it important to distinguish between these two inference phases.

Quick check: Why might the prefill phase be more computationally intensive than autoregressive generation for long contexts?

## Architecture Onboarding

**Component map**: Input sequence -> Self-decoder (computes global KV caches) -> Cross-decoder (reuses caches via cross-attention) -> Output tokens

**Critical path**: During prefill, the critical path is: Input -> Self-decoder (single forward pass) -> Cross-decoder (multiple layers using shared caches) -> Final output. This contrasts with standard models where each layer independently computes its own KV caches.

**Design tradeoffs**: YOCO trades a small amount of additional computation in the self-decoder for massive reductions in memory usage and prefill time. The architecture requires careful synchronization between the self-decoder and cross-decoder outputs.

**Failure signatures**: Potential issues could arise if the self-decoder and cross-decoder are not properly aligned, leading to inconsistent attention patterns. The shared KV caches must be computed accurately to avoid propagating errors through all cross-decoder layers.

**First experiments**:
1. Compare memory usage and prefill latency of YOCO against standard decoder-only models across varying context lengths (256, 1024, 4096, 1M tokens).
2. Evaluate language modeling performance on MMLU benchmark to ensure YOCO maintains competitive accuracy.
3. Test needle retrieval accuracy for 1M context length to validate the architecture's effectiveness at extreme scale.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit questions arise from the research:

- How does YOCO's performance scale beyond 1M context length?
- What is the impact on model quality when operating at extreme memory savings?
- How does YOCO compare to other efficient attention mechanisms and KV cache compression techniques?
- Can the YOCO architecture be effectively applied to multimodal models or other sequence-to-sequence tasks?

## Limitations

- Scalability to context lengths beyond 1 million tokens is not fully explored and may present challenges.
- Performance gains are primarily demonstrated on the MMLU benchmark, which may not represent all real-world use cases.
- The specific nature and complexity of retrieval tasks for 1M context length are not detailed, leaving questions about generalization.
- Comparison with existing efficient attention mechanisms could be more comprehensive to establish relative advantages.

## Confidence

- **High confidence**: YOCO's ability to reduce GPU memory usage and improve prefill latency compared to standard decoder-only models, as demonstrated through controlled experiments with clear metrics.
- **Medium confidence**: The claim that YOCO achieves competitive language modeling performance, as this is benchmarked on MMLU but may not generalize to all tasks or domains.
- **Medium confidence**: The extension of YOCO to 1M context length with near-perfect needle retrieval accuracy, as the retrieval tasks and their complexity are not fully detailed.

## Next Checks

1. Test YOCO on a broader range of benchmarks and real-world datasets to verify its language modeling performance across diverse tasks and domains.
2. Conduct ablation studies to quantify the impact of each component of YOCO (self-decoder and cross-decoder) on overall efficiency and performance, and compare these results with other efficient attention mechanisms.
3. Evaluate YOCO's scalability and performance with context lengths significantly beyond 1 million tokens to assess its applicability to future large-scale language models.