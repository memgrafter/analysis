---
ver: rpa2
title: 'MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts'
arxiv_id: '2409.11844'
source_url: https://arxiv.org/abs/2409.11844
tags:
- running
- unlearning
- data
- arxiv
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MEOW is a method for LLM unlearning that addresses three challenges:
  maintaining model utility, efficiency, and robustness. It generates inverted facts
  using an offline LLM and uses a new metric, MEMO, to quantify memorization.'
---

# MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts

## Quick Facts
- arXiv ID: 2409.11844
- Source URL: https://arxiv.org/abs/2409.11844
- Authors: Tianle Gu; Kexin Huang; Ruilin Luo; Yuanqi Yao; Yujiu Yang; Yan Teng; Yingchun Wang
- Reference count: 40
- Primary result: MEOW achieves superior LLM unlearning by generating inverted facts and using MEMO metric to selectively fine-tune, maintaining model utility while removing memorized sensitive information.

## Executive Summary
MEOW introduces a novel approach to LLM unlearning that addresses three key challenges: maintaining model utility, computational efficiency, and robustness. The method generates inverted facts using an offline LLM and quantifies memorization with a new metric called MEMO, which uses Rouge-N scores across sliding windows. By selecting appropriate inverted facts based on MEMO's signals and fine-tuning the model, MEOW achieves significantly better forget quality than existing methods without substantial loss in model utility, as demonstrated on the ToFU benchmark.

## Method Summary
MEOW addresses LLM unlearning through a three-step process: (1) generate inverted facts using an offline LLM to create factually inconsistent alternatives to memorized information, (2) quantify memorization using MEMO, a new metric that calculates Rouge-N scores across sliding windows of prompt-answer pairs, and (3) select top or bottom k facts based on MEMO scores and fine-tune the model using cross-entropy loss. This approach achieves effective unlearning without requiring retain data or auxiliary models, making it more efficient than previous methods.

## Key Results
- MEOW achieves significantly better forget quality than baseline methods on the ToFU benchmark while maintaining model utility
- The method demonstrates stability and even slightly improves NLU performance on datasets like PIQA, ARC-E, and ARC-C
- MEOW eliminates the need for retain data and auxiliary models, making it more computationally efficient than existing unlearning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MEMO effectively quantifies memorization by measuring token overlap using Rouge-N metrics across sliding windows.
- Mechanism: For each prompt-answer pair, MEMO segments the sequence using sliding windows, computes Rouge-N scores between generated and ground truth tokens, and aggregates these scores to produce a memorization score.
- Core assumption: Rouge-N similarity between truncated prompts and answers correlates with the model's memorization of the full sequence.
- Evidence anchors:
  - [abstract] "we design a new metric, MEMO, to quantify memorization in LLMs"
  - [section] "we segment x and y according to different modes... we feed Tp into the model M, obtaining the output Tr. We compare Tr with Tgt using Rouge"
  - [corpus] No direct corpus evidence found for MEMO's effectiveness; this appears to be a novel metric introduced in this paper
- Break condition: If Rouge-N similarity doesn't correlate with actual memorization, MEMO would fail to identify which facts need inversion

### Mechanism 2
- Claim: Inverted facts disrupt memorized information through information overload theory.
- Mechanism: By generating multiple factually inconsistent alternatives to a memorized fact, the model becomes uncertain about the original fact and tends to discard it.
- Core assumption: LLMs can be confused by contradictory information, leading them to unlearn previously memorized facts.
- Evidence anchors:
  - [abstract] "we use an offline LLM to generate a set of inverted facts"
  - [section] "Applied to LLMs, we interpret direct exposure to specific sensitive information as a 'strong belief'... when presented with more similar but different or even contradictory facts, the model becomes hesitant and tends to discard the original belief"
  - [corpus] No direct corpus evidence found for this specific unlearning approach; appears to be novel methodology
- Break condition: If the model maintains strong confidence in original facts despite contradictory information, unlearning would fail

### Mechanism 3
- Claim: Selective fine-tuning with top/bottom k memorized inverted facts optimizes the unlearning tradeoff.
- Mechanism: MEMO identifies which inverted facts have the strongest/weakest memorization scores, then fine-tunes the model on these selected facts to achieve optimal forget quality without excessive utility loss.
- Core assumption: Fine-tuning on facts with extreme memorization scores (highest or lowest) provides the most effective unlearning signal.
- Evidence anchors:
  - [abstract] "based on the signals provided by MEMO, we select the most appropriate set of inverted facts and fine-tune the model based on them"
  - [section] "we use MEMO to calculate the memorization of each fact. Then, we select the top or bottom k facts with the highest or lowest memorization"
  - [corpus] No direct corpus evidence found for this specific selection strategy; appears to be novel approach
- Break condition: If MEMO rankings don't correlate with actual unlearning effectiveness, the selection strategy would be suboptimal

## Foundational Learning

- Concept: Rouge-N metrics for text similarity
  - Why needed here: MEMO relies on Rouge-N to quantify token overlap between generated and ground truth sequences
  - Quick check question: What does Rouge-1 measure in terms of n-gram overlap?

- Concept: Information overload theory
  - Why needed here: The unlearning mechanism is based on confusing the model with contradictory information
  - Quick check question: How does presenting contradictory facts theoretically lead to unlearning in cognitive science?

- Concept: Gradient descent optimization
  - Why needed here: MEOW uses gradient descent-based fine-tuning to update model weights based on inverted facts
  - Quick check question: What loss function is used during the fine-tuning process in MEOW?

## Architecture Onboarding

- Component map:
  - Offline LLM -> Fact Inversion -> MEMO Module -> Selection Engine -> Fine-tuning Component -> Evaluation Pipeline

- Critical path:
  1. Input: Original model + forgetting dataset
  2. Fact inversion using offline LLM
  3. MEMO computation for all inverted facts
  4. Selection of k facts based on MEMO scores
  5. Fine-tuning with cross-entropy loss
  6. Evaluation of forget quality and utility

- Design tradeoffs:
  - MEMO vs. other memorization metrics (MA, EL): MEMO is faster but may be less discriminative for highly memorized data
  - Number of inverted facts (k): More facts improve forget quality but reduce model utility
  - Selection strategy (top vs. bottom k): Different models respond better to different strategies based on memorization strength

- Failure signatures:
  - Low forget quality: MEMO may not be accurately measuring memorization
  - Significant utility drop: Too many inverted facts or wrong selection strategy
  - Loss divergence: Learning rate too high during fine-tuning
  - No improvement: Offline LLM may not be generating effective inverted facts

- First 3 experiments:
  1. Verify MEMO computation: Run MEMO on a small set of prompt-answer pairs and manually verify Rouge scores
  2. Test fact inversion: Generate inverted facts for a few examples and check if they're factually inconsistent
  3. Evaluate selection strategy: Run MEMO on inverted facts and verify top/bottom k selection makes sense for the model

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the methodology and results, several important questions remain:

1. How does MEOW's performance scale with larger models beyond Llama2-7B-Chat and Phi-1.5B?
2. Can MEOW effectively unlearn information that is deeply embedded in the model's weights rather than just surface-level memorization?
3. How does the choice of the offline LLM used for fact inversion affect MEOW's performance and what are the optimal strategies for selecting this model?

## Limitations
- MEMO metric effectiveness is not validated against established memorization baselines beyond computational speed comparisons
- The information overload theory underlying unlearning lacks empirical validation of why contradictory facts cause unlearning rather than accommodation
- The selection strategy using top/bottom k facts is presented without exploring alternative methods or explaining why this approach optimizes the tradeoff

## Confidence
*High Confidence:* The experimental results showing MEOW's superior forget quality compared to baselines are well-supported by the ToFU benchmark evaluation.

*Medium Confidence:* The claim that MEMO provides an effective measure of memorization is supported by the methodology but lacks independent validation.

*Low Confidence:* The theoretical mechanism explaining why information overload through inverted facts causes unlearning is presented as plausible but not empirically validated.

## Next Checks
1. **MEMO Validation**: Compare MEMO's memorization scores against MA and EL metrics on a held-out test set to quantify accuracy trade-offs. Measure how often MEMO misidentifies facts as memorized or unmemorized compared to the ground truth.

2. **Unlearning Mechanism Analysis**: Conduct ablation studies removing inverted facts with different memorization scores to determine which facts actually contribute to unlearning. Test whether models fine-tuned on only high-memorization facts achieve better results than those fine-tuned on low-memorization facts.

3. **Robustness Testing**: Evaluate MEOW's performance across different model sizes and architectures beyond Llama2-7B and Phi-1.5B. Test on real-world sensitive datasets rather than the synthetic ToFU benchmark to assess practical applicability.