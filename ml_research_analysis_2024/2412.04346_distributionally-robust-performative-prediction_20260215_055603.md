---
ver: rpa2
title: Distributionally Robust Performative Prediction
arxiv_id: '2412.04346'
source_url: https://arxiv.org/abs/2412.04346
tags:
- performative
- drpo
- distribution
- risk
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the issue of distribution map misspecification
  in performative prediction, where deployed ML models influence the data distribution.
  The authors introduce a distributionally robust performative prediction framework,
  defining a new solution concept called the distributionally robust performative
  optimum (DRPO).
---

# Distributionally Robust Performative Prediction

## Quick Facts
- arXiv ID: 2412.04346
- Source URL: https://arxiv.org/abs/2412.04346
- Authors: Songkai Xue; Yuekai Sun
- Reference count: 40
- One-line primary result: Distributionally robust performative prediction framework with DRPO solution concept improves robustness against distribution map misspecification

## Executive Summary
This work addresses distribution map misspecification in performative prediction, where ML models influence the data distribution they target. The authors introduce a distributionally robust performative prediction framework that defines a new solution concept called the distributionally robust performative optimum (DRPO). By optimizing over an uncertainty set of distribution maps using KL divergence, the framework aims to provide robust approximations to the true performative optimum even when the nominal distribution map is misspecified.

The key innovation is reformulating the distributionally robust performative risk minimization as an augmented performative risk minimization problem, enabling efficient optimization through alternating minimization algorithms. Theoretical guarantees show that DRPO provides robustness against distribution map misspecification, and empirical experiments demonstrate improved worst-case performance across strategic classification, partially identifiable distribution maps, and fairness without demographics scenarios.

## Method Summary
The authors introduce a distributionally robust performative prediction framework that addresses distribution map misspecification through KL divergence-based uncertainty sets. The framework reformulates distributionally robust performative risk minimization as an augmented performative risk minimization problem using strong duality results. This enables efficient optimization via alternating minimization between model parameters θ and dual variable µ. The framework also introduces tilted performative risk minimization as an implicit solution with different regularization strength. The method is validated through experiments on strategic classification, partially identifiable distribution maps, and fairness without demographics, showing improved worst-case performance and fairness guarantees compared to traditional performative optimum approaches.

## Key Results
- DRPO provides robust approximations to true PO when nominal distribution map differs from actual map
- Reformulation as augmented performative risk minimization enables efficient optimization via alternating minimization
- Empirical validation shows DRPO advantages over traditional PO in strategic classification, partially identifiable maps, and fairness without demographics
- Theoretical guarantees establish excess risk bounds comparing DRPO and PO solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributionally robust performative prediction improves worst-case performance by optimizing over an uncertainty set of distribution maps.
- Mechanism: By defining an uncertainty collection U(D) around the nominal distribution map using KL divergence, the DRPO solution minimizes the worst-case performative risk across all distribution maps within the uncertainty set. This creates a robust approximation to the true PO even when the nominal map is misspecified.
- Core assumption: The true distribution map Dtrue lies within the uncertainty collection U(D) centered at the nominal distribution map D.
- Evidence anchors:
  - [abstract] "we introduce a novel framework of distributionally robust performative prediction and study a new solution concept termed as distributionally robust performative optimum (DRPO)"
  - [section] "Definition 2.4 (Distributionally robust performative risk). The distributionally robust performative risk with the uncertainty collection U(D) is defined as DRPR(θ) = sup eD: eD∈U(D) EZ∼ eD(θ)[ℓ(Z; θ)]"
  - [corpus] Weak - no direct citations found in neighboring papers about KL divergence-based distributionally robust performative prediction
- Break condition: If the true distribution map lies far outside the uncertainty set (Dtrue not in U(D)), the DRPO may not provide meaningful robustness guarantees.

### Mechanism 2
- Claim: Reformulating DRPO as an augmented performative prediction problem enables efficient optimization through alternating minimization.
- Mechanism: The strong duality result (Proposition 3.1) shows that DRPR(θ) can be expressed as an optimization problem over a single dual variable. This allows transforming the DRPO problem into an augmented performative risk minimization problem that can be solved using alternating minimization between θ and µ.
- Core assumption: The dual reformulation holds for the specific loss function and distribution family being used.
- Evidence anchors:
  - [section] "Proposition 3.1 (Strong duality of DRPR). For any θ ∈ Θ, we have DRPR(θ) = inf µ≥0 n µ log EZ∼D(θ) h eℓ(Z;θ)/µ i + µρ o"
  - [section] "This suggests an alternating minimization, summarized in Algorithm 1, where we learn θDRPO by fixing µ and minimizing on θ and then fixing θ and minimizing on µ alternatively"
  - [corpus] Weak - neighboring papers discuss performative prediction optimization but not specifically this dual reformulation approach
- Break condition: If the alternating minimization algorithm gets stuck in poor local minima or the problem becomes non-convex in a way that prevents efficient optimization.

### Mechanism 3
- Claim: Tilted performative risk minimization provides an implicit solution to the distributionally robust problem with different regularization strength.
- Mechanism: By treating the inverse of the dual variable α = 1/µ as a hyperparameter, the tilted performative risk minimization problem TPR(θ) = EZ∼D(θ) h eαℓ(Z;θ) i implicitly solves a corresponding DRPO problem with radius ρ = 1/α.
- Core assumption: There exists a monotonic relationship between the tilt parameter α and the critical radius ρ that controls the robustness level.
- Evidence anchors:
  - [section] "With this motivation, instead of tuning ρ, we can tune µ (or the inverse of it, denoted by α = µ−1) and solve the α-tilted performative risk minimization problem"
  - [section] "Therefore, the tilted performative risk minimization implicitly solves a corresponding DR performative risk minimization problem"
  - [corpus] Weak - no direct citations found about tilted risk minimization in performative prediction context
- Break condition: If the relationship between α and ρ is not monotonic or if the tilted problem becomes too difficult to optimize for large α values.

## Foundational Learning

- Concept: Performative prediction and distribution maps
  - Why needed here: Understanding how predictive models influence the data distribution they target is fundamental to grasping why distribution map misspecification is a problem and how DRPO addresses it
  - Quick check question: What is the difference between performative stability and performative optimality?

- Concept: Distributionally robust optimization (DRO) and φ-divergence
  - Why needed here: The DRPO framework builds directly on DRO techniques, using φ-divergence (specifically KL divergence) to define uncertainty sets around the nominal distribution
  - Quick check question: How does KL divergence-based DRO differ from Wasserstein DRO in terms of the uncertainty sets they create?

- Concept: Alternating minimization algorithms and dual reformulations
  - Why needed here: The efficient optimization of DRPO relies on solving a sequence of subproblems through alternating minimization, which requires understanding of dual problems and optimization techniques
  - Quick check question: What conditions guarantee convergence of alternating minimization algorithms to a global optimum?

## Architecture Onboarding

- Component map: Distribution map modeling -> Robust optimization layer -> Calibration module
- Critical path: 1. Model training on nominal distribution map D, 2. DRPO computation via augmented performative risk minimization, 3. Radius calibration using post-fitting or calibration set approaches, 4. Evaluation on true distribution map Dtrue
- Design tradeoffs: The choice of critical radius ρ involves a tradeoff between robustness and performance - larger ρ provides more robustness but may lead to conservative solutions with degraded performance on the nominal distribution. The choice between direct DRPO optimization vs tilted performative risk minimization involves computational efficiency vs interpretability tradeoffs.
- Failure signatures: 1. Poor performance on the nominal distribution when ρ is too large (overly conservative), 2. Failure to improve over standard PO when ρ is too small (insufficient robustness), 3. Optimization convergence issues when the problem becomes highly non-convex, 4. Calibration failure when the uncertainty set doesn't contain the true distribution map.
- First 3 experiments:
  1. Verify the theoretical excess risk bounds by comparing E(θPO) and E(θDRPO) on synthetic data with known distribution map misspecification
  2. Test the alternating minimization algorithm convergence on a simple strategic classification problem with varying levels of cost function misspecification
  3. Evaluate the calibration methods (post-fitting vs calibration set) on a fairness without demographics problem to assess their effectiveness in selecting appropriate radius values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of φ-divergence function impact the performance of the distributionally robust performative prediction framework compared to using KL divergence?
- Basis in paper: [explicit] The paper extends the KL divergence framework to a general φ-divergence framework in Appendix H.
- Why unresolved: The paper does not provide experimental results comparing different φ-divergence functions or analyzing their impact on performance.
- What evidence would resolve it: Experimental results comparing the performance of the framework using different φ-divergence functions (e.g., Cressie-Read family) under various distribution map misspecifications.

### Open Question 2
- Question: What are the theoretical guarantees for the convergence of the alternating minimization algorithms proposed for finding the DRPO, especially when using different "inner" algorithms for solving the performative risk minimization subproblems?
- Basis in paper: [explicit] The paper mentions that the alternating minimization algorithm guarantees global convergence (to stationary point) regardless of initialization, and with strong convexity assumption, it guarantees convergence to the global minimum.
- Why unresolved: The paper does not provide a detailed analysis of convergence rates or guarantees for different choices of inner algorithms.
- What evidence would resolve it: A rigorous convergence analysis of the alternating minimization algorithm for different choices of inner algorithms, including convergence rates and conditions for global convergence.

### Open Question 3
- Question: How does the performance of the DRPO framework scale with the dimensionality of the data and the model parameter space?
- Basis in paper: [inferred] The paper does not explicitly discuss the scalability of the framework with respect to dimensionality.
- Why unresolved: The experiments in the paper are conducted on relatively low-dimensional datasets, and the theoretical analysis does not address the scalability issue.
- What evidence would resolve it: Experimental results and theoretical analysis demonstrating the performance of the framework on high-dimensional datasets and large model parameter spaces.

## Limitations

- KL divergence-based uncertainty sets assume the true distribution map lies within a neighborhood of the nominal map, which may not hold in practice
- Alternating minimization algorithm convergence properties are not thoroughly analyzed, particularly for non-convex loss functions
- Calibration methods for selecting critical radius ρ rely on assumptions about access to data or prior knowledge that may not be available in real-world deployments

## Confidence

**High confidence** in the theoretical foundations, particularly the strong duality result (Proposition 3.1) and the excess risk bounds comparing DRPO to PO. The mathematical derivations appear sound and well-established.

**Medium confidence** in the practical effectiveness of the framework, as the empirical evaluation is limited to three specific scenarios (strategic classification, partially identifiable distribution maps, and fairness without demographics) with relatively small-scale experiments.

**Low confidence** in the scalability and generalizability of the approach to complex, high-dimensional problems, as the experiments do not test the framework on large-scale datasets or complex model architectures.

## Next Checks

1. **Convergence Analysis**: Conduct a thorough empirical analysis of the alternating minimization algorithm's convergence properties across different problem instances, including convergence speed, sensitivity to initialization, and comparison with other optimization methods.

2. **Robustness to Misspecification**: Design experiments to test the framework's performance when the true distribution map lies outside the uncertainty set, quantifying the degradation in worst-case performance and identifying failure modes.

3. **Scalability Evaluation**: Implement the DRPO framework on larger, more complex datasets (e.g., ImageNet for strategic classification) and compare its computational efficiency and performance with the standard PO approach.