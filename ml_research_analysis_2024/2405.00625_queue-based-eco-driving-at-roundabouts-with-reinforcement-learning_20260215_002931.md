---
ver: rpa2
title: Queue-based Eco-Driving at Roundabouts with Reinforcement Learning
arxiv_id: '2405.00625'
source_url: https://arxiv.org/abs/2405.00625
tags:
- traffic
- speed
- vehicles
- eco-driving
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents queue-based eco-driving approaches for roundabouts,
  combining rule-based and Reinforcement Learning (RL) methods. The authors develop
  algorithms that incorporate information about preceding vehicles and waiting queues
  to optimize the speed of connected automated vehicles.
---

# Queue-based Eco-Driving at Roundabouts with Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.00625
- Source URL: https://arxiv.org/abs/2405.00625
- Reference count: 33
- Primary result: Queue-based eco-driving approaches for roundabouts combining rule-based and RL methods significantly outperform baseline without speed optimization

## Executive Summary
This paper presents two queue-based eco-driving approaches for roundabouts using connected automated vehicles (CAVs). The rule-based algorithm uses analytical optimization with queue information to determine optimal speeds, while the RL approach employs a Soft Actor-Critic agent to learn effective speed policies. Both approaches significantly reduce waiting times and stops compared to a baseline without speed optimization, with the rule-based method showing superior performance especially at high traffic volumes. The study evaluates performance across varying traffic volumes and connected vehicle penetration rates.

## Method Summary
The study uses SUMO microscopic traffic simulator with a real urban single-lane roundabout in Ingolstadt, Germany. Two approaches are implemented: a rule-based algorithm that generates status plans for roundabout sectors and calculates optimal speeds using motion equations, and an RL-based approach using Soft Actor-Critic with a multi-layer perceptron architecture. Both methods incorporate information about preceding vehicles and waiting queues, optimizing speeds starting from 500 meters before the merging point. The evaluation measures energy/fuel consumption, CO2 emissions, travel time, waiting time, and number of stops across traffic volumes from 600-1400 veh/h and CV penetration rates from 20-100%.

## Key Results
- Rule-based approach reduces waiting time by 60.7% and stops by 48.3% on average at 1200 veh/h
- RL-based approach achieves 36.5% reduction in waiting time and 31.7% reduction in stops
- Best results achieved at highest traffic volume (1400 veh/h) with rule-based approach
- Substantial improvements maintained even at lower CV penetration rates (80% CV penetration)

## Why This Works (Mechanism)

### Mechanism 1
Queue-based status planning eliminates stop-and-go behavior by preemptively calculating free status windows for both entrance area and merging point. The algorithm constructs occupancy status plans for each vehicle, marking three-second periods for merging point and two-second periods for entrance area. When the ego vehicle approaches, it checks these plans to determine if it can reach each location during a free status, and if not, calculates optimal speed to arrive during the next free window. This works because all vehicles share information reliably and geometric constraints allow predictable occupancy durations.

### Mechanism 2
Soft Actor-Critic reinforcement learning discovers effective speed policies that balance energy efficiency, waiting time reduction, and travel time minimization. The SAC agent receives state information (current speed, distances to entrance and merging point, occupancy status of both areas) and outputs continuous speed advisory. The reward function combines three components: negative reward for stops, penalty for speed advisory changes to reduce oscillations, and encouragement to drive at maximum speed to minimize travel time. The agent learns through repeated interactions in the simulation environment.

### Mechanism 3
Extending optimization distance to 500 meters significantly improves performance compared to shorter distances used in previous studies. By starting optimization at 500 meters from the merging point, the ego vehicle has more time to adjust its speed smoothly based on traffic conditions ahead. The system can plan for both efficient approach and smooth entry, whereas shorter distances force more reactive, less optimal decisions.

## Foundational Learning

- **Queue-based status planning and occupancy prediction**: Needed to handle complex interactions at roundabouts where multiple vehicles compete for limited entry gaps. Quick check: How does the algorithm determine when a vehicle is "occupying" the entrance area versus the merging point, and why are these durations different?

- **Multi-objective reinforcement learning reward design**: Needed because eco-driving involves competing objectives - minimizing stops (good for emissions), maintaining reasonable travel time (good for user experience), and avoiding erratic speed changes (good for passenger comfort). Quick check: What would happen to agent behavior if the weight on rdiff (speed change penalty) were increased from 0.05 to 0.5?

- **Connected vehicle penetration rate effects on algorithm performance**: Needed because real-world deployment will involve mixed traffic with varying CV penetration rates. Understanding how performance degrades with fewer connected vehicles is essential for practical implementation. Quick check: Why does the rule-based approach maintain better performance at 80% CV penetration compared to the RL approach?

## Architecture Onboarding

- **Component map**: SUMO -> Gym interface wrapper -> Status plan generator -> Rule-based optimizer / SAC agent -> Speed advisory application -> Performance evaluation
- **Critical path**: Traffic simulation → State extraction → Speed optimization (rule-based or RL) → Speed advisory application → Reward accumulation → Performance evaluation
- **Design tradeoffs**: Rule-based vs RL (predictable vs adaptable), optimization distance (smoothness vs uncertainty), CV penetration rate (information completeness vs realism)
- **Failure signatures**: Poor performance near capacity limits, performance degradation with CV penetration, oscillating speed advisories
- **First 3 experiments**:
  1. Compare performance at different optimization distances (100m, 200m, 300m, 400m, 500m)
  2. Test impact of reward function weight variations on agent behavior
  3. Evaluate performance degradation as CV penetration rate decreases from 100% to 20% in 20% increments

## Open Questions the Paper Calls Out

### Open Question 1
How can the RL agent's performance be improved to achieve better results than the rule-based approach at high traffic volumes? The paper identifies that the RL agent experiences a disadvantage in trips where the minimum speed restriction prevents full elimination of stops, and its performance decreases compared to the rule-based approach as traffic volume increases. Developing and testing an enhanced RL agent design that can handle the minimum speed restriction better would resolve this.

### Open Question 2
How can the RL agent's robustness be improved to maintain performance with lower CV penetration rates? The paper mentions that the RL agent is more affected by modeling a CV penetration rate in the simulation, and its performance notably deteriorates with lower CV penetration rates. Developing and testing an enhanced RL agent design that can maintain performance with lower CV penetration rates would resolve this.

### Open Question 3
How can the trade-off between reducing waiting times and stops versus improving fuel economy and travel time be optimized across different traffic volumes? The paper mentions that the linear reward function might not be sufficient in dynamic roundabout settings with short-term entrance gaps and higher stop probability despite optimization. Developing and testing a non-linear reward function or multi-objective optimization approach would resolve this.

## Limitations

- Relies on simulation data from SUMO rather than real-world testing, which may not fully capture complex human driver behaviors
- Performance of both approaches degrades significantly at lower connected vehicle penetration rates, particularly for the RL approach
- Reward function weights for the RL agent are empirically determined but not explicitly optimized, suggesting potential for further improvement

## Confidence

- **High Confidence**: Queue-based status planning effectively reduces stop-and-go behavior and waiting times, as evidenced by consistent improvements across all traffic volumes and CV penetration rates.
- **Medium Confidence**: RL agents can discover effective policies for speed optimization, but the lack of substantial advantage over classical approaches suggests potential issues with reward function design or state representation.
- **Medium Confidence**: Extending optimization distance to 500 meters significantly improves performance, though the optimal distance may vary depending on specific roundabout characteristics and traffic patterns.

## Next Checks

1. Test both approaches on different roundabout geometries and traffic patterns to verify generalizability beyond the specific urban roundabout studied.
2. Implement real-world field testing with actual connected vehicles to validate simulation results and identify any discrepancies between simulated and real-world performance.
3. Conduct ablation studies on the RL reward function components to optimize weightings and potentially improve RL performance relative to the rule-based approach.