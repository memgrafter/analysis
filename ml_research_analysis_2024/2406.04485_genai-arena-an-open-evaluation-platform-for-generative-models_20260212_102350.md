---
ver: rpa2
title: 'GenAI Arena: An Open Evaluation Platform for Generative Models'
arxiv_id: '2406.04485'
source_url: https://arxiv.org/abs/2406.04485
tags:
- image
- generation
- votes
- arxiv
- text-to-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GenAI-Arena, an open platform for evaluating
  generative models across text-to-image generation, image editing, and text-to-video
  generation using human preference votes. The platform collects over 9,000 votes
  and employs an Elo rating system to rank 35 open-source models.
---

# GenAI Arena: An Open Evaluation Platform for Generative Models

## Quick Facts
- **arXiv ID**: 2406.04485
- **Source URL**: https://arxiv.org/abs/2406.04485
- **Reference count**: 40
- **Primary result**: Open platform collecting 9,000+ human preference votes to rank 35 generative models using Elo ratings, with GPT-4o achieving only 49.19% accuracy in matching human preferences

## Executive Summary
GenAI-Arena is an open platform designed to evaluate generative models through human preference voting across text-to-image generation, image editing, and text-to-video generation. The platform collects pairwise comparisons from users who vote on which of two generated outputs better satisfies a given prompt. Using an Elo rating system, the platform ranks 35 open-source models based on accumulated votes, while also releasing GenAI-Bench, a cleaned dataset for benchmarking multimodal large language models' ability to judge content quality. The authors demonstrate that even state-of-the-art models like GPT-4o struggle to match human preferences, achieving only 49.19% accuracy, while open-source alternatives perform worse due to limited instruction-following and reasoning capabilities.

## Method Summary
The platform operates through a web interface where users compare pairs of generated outputs from different models and vote on which better satisfies the prompt. The Elo rating system dynamically updates model rankings based on these pairwise comparisons, assuming transitivity in user preferences. The GenAI-Bench dataset was created by cleaning preference data to remove unclear or mismatched comparisons, enabling standardized evaluation of MLLMs' judgment capabilities. Expert reviewers validated the reliability of human votes by examining sampled data, finding 93.07% of judgments to be reasonable or vaguely reasonable. The platform tracks 35 open-source models across three generative tasks, with results updated in real-time as new votes are cast.

## Key Results
- Platform collected over 9,000 human preference votes across 35 open-source models
- GPT-4o achieves only 49.19% accuracy in matching human preferences, while open-source MLLMs perform worse
- Expert validation confirms high reliability with 93.07% of sampled votes deemed reasonable or vaguely reasonable
- Elo rating system effectively ranks models, though class imbalance exists with text-to-image generation dominating

## Why This Works (Mechanism)
The platform leverages human perceptual judgment as ground truth for generative model quality, recognizing that automated metrics often fail to capture subjective aspects of creativity and relevance. By using pairwise comparisons rather than absolute scoring, the system reduces cognitive burden on voters while enabling robust statistical ranking through the Elo system. The cleaned GenAI-Bench dataset provides a standardized benchmark that isolates model judgment capability from generation quality, allowing researchers to measure how well MLLMs can predict human preferences.

## Foundational Learning

**Human preference judgment**: Why needed - captures subjective quality aspects that automated metrics miss; Quick check - compare Elo rankings with established metrics like FID

**Elo rating system**: Why needed - provides statistically sound method for ranking from pairwise comparisons; Quick check - verify transitivity assumptions by testing if A>B and B>C implies A>C

**Multimodal instruction following**: Why needed - critical for models to understand generation prompts and judge outputs; Quick check - measure correlation between instruction-following scores and preference-matching accuracy

**Dataset cleaning protocols**: Why needed - ensures benchmark quality by removing ambiguous comparisons; Quick check - compare cleaned vs raw dataset performance to quantify cleaning impact

## Architecture Onboarding

**Component map**: User interface -> Vote collection -> Elo rating engine -> Model ranking display -> GenAI-Bench dataset -> MLLM evaluation pipeline

**Critical path**: Prompt generation → Model inference → Output pairing → Human voting → Elo update → Ranking publication

**Design tradeoffs**: Real-time voting vs. data quality control; open participation vs. spam prevention; model coverage vs. computational costs

**Failure signatures**: Elo instability when vote volume is low; ranking bias from class imbalance; model evaluation inconsistency across task types

**First 3 experiments**:
1. Verify Elo rating convergence by comparing rankings from different vote subsets
2. Test transitivity assumption by constructing chains of model comparisons
3. Benchmark MLLM accuracy on GenAI-Bench vs their base generation performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided text.

## Limitations

- Elo rating system assumes transitivity in pairwise comparisons, which may not hold for models with non-overlapping strengths
- Dataset shows class imbalance with text-to-image generation dominating, potentially biasing overall rankings
- 49.19% accuracy for GPT-4o represents a single snapshot as model capabilities rapidly evolve
- Human preference judgments may be influenced by presentation order, cultural factors, or user expertise levels
- Platform relies on voluntary participation, which may introduce sampling bias in the collected preference data

## Confidence

**High**: Platform implementation, data collection methodology, expert validation results
**Medium**: Elo rating system effectiveness, ranking stability across time
**Medium**: MLLM benchmarking methodology and absolute accuracy numbers

## Next Checks

1. Conduct temporal validation by replicating key experiments after 3-6 months to assess ranking stability as new models emerge
2. Perform controlled experiments testing Elo system transitivity assumptions with known model performance differences
3. Expand validation to include cross-cultural preference studies to assess whether current human preference patterns generalize globally
4. Investigate presentation order effects by randomizing comparison layouts and analyzing impact on voting patterns
5. Implement demographic tracking of voters to quantify potential sampling biases in preference distributions