---
ver: rpa2
title: Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token
  Prediction
arxiv_id: '2401.01498'
source_url: https://arxiv.org/abs/2401.01498
tags:
- speech
- transducer
- token
- alignment
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-stage text-to-speech (TTS) framework
  using a neural transducer and semantic tokens. The first stage employs a token transducer
  to convert text to semantic tokens, leveraging the hard monotonic alignment constraint
  of neural transducers for robust alignment modeling.
---

# Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction

## Quick Facts
- arXiv ID: 2401.01498
- Source URL: https://arxiv.org/abs/2401.01498
- Authors: Minchan Kim; Myeonghun Jeong; Byoung Jin Choi; Semin Kim; Joun Yeop Lee; Nam Soo Kim
- Reference count: 40
- This paper introduces a two-stage text-to-speech (TTS) framework using a neural transducer and semantic tokens, achieving superior speech quality and speaker similarity in zero-shot adaptive TTS

## Executive Summary
This paper presents a novel two-stage TTS framework that leverages neural transducers for robust monotonic alignment modeling and semantic tokens for efficient sequence-to-sequence conversion. The framework divides TTS into semantic-level modeling (text-to-token) and fine-grained acoustic modeling (token-to-speech), enabling faster inference and improved prosody controllability. Experimental results demonstrate superior performance compared to baseline models on zero-shot adaptive TTS tasks.

## Method Summary
The proposed framework uses a token transducer for semantic token prediction from text in the first stage, followed by a VITS-based speech generator for waveform synthesis in the second stage. Semantic tokens are extracted from wav2vec2.0 embeddings via k-means clustering (512 clusters). The token transducer employs pruned training to reduce memory consumption, while the speech generator uses reference speech conditioning for speaker identity and prosody control. The system is trained on LibriTTS dataset and evaluated on zero-shot adaptive TTS performance.

## Key Results
- Outperforms baseline models in speech quality and speaker similarity for zero-shot adaptive TTS
- Demonstrates faster inference speed compared to existing methods
- Shows improved prosody controllability through reference speech conditioning
- Achieves robust monotonic alignment between text and speech using neural transducers

## Why This Works (Mechanism)

### Mechanism 1
Neural transducers enable robust monotonic alignment in TTS by enforcing hard alignment constraints. The neural transducer searches for monotonic paths between text and semantic tokens using alignment lattices and dynamic programming, ensuring correct mapping between linguistic units and acoustic frames.

### Mechanism 2
Semantic tokens provide a discrete output space that simplifies neural transducer application to TTS. By quantizing wav2vec2.0 embeddings into discrete semantic tokens, the output space becomes finite and enumerable, eliminating the need for complex modeling of continuous speech parameters.

### Mechanism 3
Two-stage decomposition reduces training complexity by separating linguistic and acoustic modeling. The token transducer focuses on linguistic content and alignment, while the speech generator handles acoustic diversity and fine-grained waveform synthesis, allowing each stage to specialize.

## Foundational Learning

- Concept: Monotonic alignment constraint
  - Why needed here: Ensures correct mapping between text and speech without word skipping or repetition
  - Quick check question: What would happen if alignment wasn't enforced as monotonic?

- Concept: Discrete vs continuous output spaces
  - Why needed here: Explains why semantic tokens work with neural transducers while raw speech doesn't
  - Quick check question: Why can't standard neural transducers be directly applied to continuous speech output?

- Concept: Variational autoencoders in speech synthesis
  - Why needed here: Understanding the speech generator's architecture and how it models acoustic diversity
  - Quick check question: What role does the normalizing flow play in the speech generator's architecture?

## Architecture Onboarding

- Component map:
  Text -> Token Transducer (Text encoder → Prediction network → Joint network → Semantic tokens) -> Speech Generator (Token encoder → Posterior/prior encoders → Decoder → Waveform)

- Critical path: Text → Token Transducer → Semantic tokens → Speech Generator → Speech

- Design tradeoffs:
  - LSTM vs Conformer prediction network: LSTM faster but less capacity, Conformer slower but more expressive
  - Semantic token clustering: More clusters = more detail but larger output space and slower transducer
  - Reference speech conditioning: More conditioning = better prosody but more complexity

- Failure signatures:
  - Misalignment: Word skipping, repetition, or early termination in output
  - Poor speaker similarity: Reference speech not properly conditioning the models
  - Slow inference: Conformer prediction network or inefficient joint network

- First 3 experiments:
  1. Ablation study: Remove reference speech conditioning to measure impact on prosody control
  2. Token clustering: Vary k-means clusters from 128 to 1024 to find optimal semantic token granularity
  3. Prediction network swap: Replace LSTM with Conformer (or vice versa) to measure inference speed/quality tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
How does the complexity of the prediction network in the token transducer impact the overall performance and inference speed of the TTS system? The paper only compares LSTM and Conformer prediction networks without exploring other potential architectures or a wider range of model complexities.

### Open Question 2
What are the effects of using different types of semantic tokens (e.g., tokens from different SSL models or with different numbers of clusters) on the performance of the TTS system? The paper only presents results using one type of semantic token, leaving open the question of how other types might affect performance.

### Open Question 3
How does the proposed TTS framework perform on datasets with more diverse acoustic conditions and speaker characteristics compared to the LibriTTS dataset used in the experiments? The experiments are limited to a single dataset, making it unclear how well the framework generalizes to more challenging scenarios.

## Limitations

- Alignment generalization may struggle with disfluent speech, code-switching, or cases where word boundaries don't align cleanly with acoustic boundaries
- Semantic token quality and information loss during quantization hasn't been thoroughly analyzed
- Computational efficiency trade-offs between inference speed and speech quality aren't fully explored

## Confidence

**High Confidence**: The two-stage framework produces better speech quality and speaker similarity than baseline models; neural transducers provide robust monotonic alignment for TTS; semantic tokens enable efficient modeling in the token transducer

**Medium Confidence**: The framework demonstrates improved prosody controllability; faster inference speed compared to existing methods; reduced training complexity through stage decomposition

**Low Confidence**: The approach generalizes well to unseen speakers without any adaptation data; the semantic token representation captures all necessary linguistic information; the monotonic alignment constraint handles all types of speech variations

## Next Checks

1. **Alignment Robustness Test**: Evaluate the token transducer on speech with disfluencies, code-switching, and non-monotonic alignment cases to determine breaking conditions for the monotonic constraint.

2. **Semantic Token Ablation**: Systematically vary the number of semantic token clusters (128, 256, 512, 1024) and measure the impact on speech quality, alignment accuracy, and transducer performance to optimize the token granularity.

3. **Cross-Speaker Generalization**: Test the framework on speakers with significantly different characteristics (age, accent, speaking rate) from the training data to evaluate zero-shot performance limits and identify failure modes.