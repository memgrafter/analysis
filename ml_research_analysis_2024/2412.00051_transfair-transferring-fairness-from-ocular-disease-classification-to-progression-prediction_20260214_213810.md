---
ver: rpa2
title: 'TransFair: Transferring Fairness from Ocular Disease Classification to Progression
  Prediction'
arxiv_id: '2412.00051'
source_url: https://arxiv.org/abs/2412.00051
tags:
- progression
- fairness
- classification
- prediction
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of fairness in AI for progression
  prediction of ocular diseases, focusing on demographic equity across different racial
  and ethnic groups. The core method, TransFair, transfers fairness from a classification
  model to a progression prediction model using knowledge distillation.
---

# TransFair: Transferring Fairness from Ocular Disease Classification to Progression Prediction

## Quick Facts
- arXiv ID: 2412.00051
- Source URL: https://arxiv.org/abs/2412.00051
- Reference count: 40
- Key outcome: TransFair achieves AUC of 0.7716 and ES-AUC of 0.7131 for MD fast progression prediction using RNFLT maps

## Executive Summary
This paper addresses fairness in AI for ocular disease progression prediction, proposing TransFair to transfer fairness from classification to progression prediction via knowledge distillation. The method trains a fair classification model (FairEN) with a fairness-aware attention mechanism, then uses this model to guide the training of a progression prediction model through feature similarity minimization. Experiments with 2D RNFLT maps and 3D OCT B-scans demonstrate improved demographic equity and overall performance compared to existing methods.

## Method Summary
TransFair transfers fairness from a fair ocular disease classification model to a progression prediction model through knowledge distillation. First, FairEN is trained on extensive retinal image data with a fairness-aware attention mechanism that incorporates demographic attributes. Then, this fair classification model serves as a teacher to guide the training of a student progression prediction model, minimizing latent feature distances between the two models using KL divergence. The approach addresses data scarcity in progression prediction while preserving fairness across demographic groups.

## Key Results
- TransFair achieves 0.7716 AUC and 0.7131 ES-AUC for MD fast progression prediction using RNFLT maps
- The method significantly outperforms existing approaches in both overall performance and fairness metrics
- Ablation studies confirm the importance of fairness-aware attention and knowledge distillation components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation from a fair classification model to a progression prediction model transfers fairness properties.
- Mechanism: The teacher model (FairEN) is trained to be fair in ocular disease classification using extensive data. Its latent features and fairness-aware attention weights are then distilled to the student model (FairEN), guiding it to learn similar fair feature representations for progression prediction.
- Core assumption: Fairness learned in classification generalizes to progression prediction when the same image modality and demographic attributes are used.
- Evidence anchors:
  - [abstract] "Subsequently, this fair classification model is adapted to a fair progression prediction model through knowledge distillation, which aims to minimize the latent feature distances between the classification and progression prediction models."
  - [section] "To enhance the feature learning and fairness in the student model, image and attribute feature similarities between teacher and student models are minimized based on the Kullback-Leibler (KL) divergence..."
  - [corpus] Weak evidence. No direct mention of fairness transfer in neighboring papers. Only related works address fairness or progression prediction separately, not combined via distillation.
- Break condition: If the feature space or demographic distribution differs significantly between classification and progression datasets, the fairness transfer will degrade.

### Mechanism 2
- Claim: Fairness-aware attention mechanism tailors feature learning per demographic group, improving both accuracy and equity.
- Mechanism: The MLP encoder maps demographic attributes into a feature space; dot-product attention between attribute features and image features produces group-specific weights, allowing the EfficientNet backbone to adjust its feature extraction for each group.
- Core assumption: Incorporating demographic attribute features into the attention layer improves model fairness without sacrificing overall accuracy.
- Evidence anchors:
  - [abstract] "we train a fair EfficientNet, termed FairEN, equipped with a fairness-aware attention mechanism using extensive data for ocular disease classification."
  - [section] "we introduce a fairness-aware attention mechanism that adjusts feature learning based on demographic attributes."
  - [corpus] Weak evidence. Nearest papers focus on fairness or attention separately, but no corpus example shows combined demographic attention in EfficientNet for medical imaging.
- Break condition: If the attribute features become too dominant, the model may overfit to group identity and lose discriminative power for disease detection.

### Mechanism 3
- Claim: Using a large, diverse classification dataset mitigates data scarcity in progression prediction while preserving fairness.
- Mechanism: The classification model is pretrained on FairVision/Harvard-GF (10k+ samples with balanced demographics), then fine-tuned on the smaller Harvard-GDP dataset via distillation, leveraging the broader representation learned earlier.
- Core assumption: Features learned from extensive classification data are transferable to progression prediction even with limited progression labels.
- Evidence anchors:
  - [abstract] "we propose using a fair classification model trained on extensive data to enhance and guide the training of a fair progression prediction model with limited data."
  - [section] "To overcome the challenge of limited longitudinal retinal image data with diverse demographics, we propose using a fair classification model trained on extensive data..."
  - [corpus] Weak evidence. No direct mention of dataset size or cross-task transfer in corpus, though neighboring papers discuss small medical datasets and fairness separately.
- Break condition: If the distribution shift between classification and progression tasks is too large, the transferred knowledge may degrade performance.

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: Enables transfer of learned fairness and feature representations from a large classification model to a smaller progression model without retraining from scratch.
  - Quick check question: In knowledge distillation, what is the role of the KL-divergence loss between teacher and student feature representations?

- Concept: Fairness-aware attention
  - Why needed here: Allows the model to adjust feature extraction per demographic group, mitigating performance gaps between subgroups.
  - Quick check question: How does the dot-product attention between demographic attribute features and image features modify the feature map in FairEN?

- Concept: Demographic fairness metrics (ES-AUC)
  - Why needed here: Provides a quantitative measure of fairness by penalizing performance disparities across subgroups, guiding model optimization.
  - Quick check question: Given AUCs of 0.8 for group A and 0.6 for group B, what is the ES-AUC if the overall AUC is 0.7?

## Architecture Onboarding

- Component map: EfficientNet-B1 -> MLP attribute encoder -> fairness-aware attention -> classification/progression head
- Critical path: EfficientNet -> MLP attribute encoding -> attention weighting -> feature adjustment -> classification/progression head
- Design tradeoffs:
  - Attention depth vs. overfitting risk: Deeper attention layers may overfit to attribute identity.
  - KL weight tuning (α, β): High values enforce fairness but may reduce task accuracy.
  - Backbone choice: EfficientNet balances accuracy and efficiency but may be less powerful than ViT for some tasks.
- Failure signatures:
  - Fairness degrades if attribute encoder collapses to constant output.
  - Model collapse if KL loss overwhelms classification/progression loss.
  - Over-regularization if α and β are too large relative to task loss.
- First 3 experiments:
  1. Train FairEN on FairVision with and without attention; compare ES-AUC and AUC.
  2. Run ablation: remove KL distillation from TransFair; measure fairness loss.
  3. Vary α and β over [0,1] on Harvard-GDP; plot AUC/ES-AUC tradeoff curve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the TransFair approach for disease progression prediction beyond glaucoma, particularly for other ocular diseases with different progression characteristics?
- Basis in paper: [explicit] The authors mention that their work has limitations and plans to extend evaluation to more diseases once relevant datasets are publicly available, but do not provide results for other diseases.
- Why unresolved: The current study focuses exclusively on glaucoma, and the authors acknowledge the need to test the approach on other diseases but have not done so due to data limitations.
- What evidence would resolve it: Empirical results showing TransFair's effectiveness on datasets for other ocular diseases (e.g., diabetic retinopathy, age-related macular degeneration) with clear comparisons to baseline methods and fairness metrics.

### Open Question 2
- Question: How does the placement of fairness-aware attention layers within the EfficientNet architecture affect the model's performance and fairness outcomes?
- Basis in paper: [inferred] The authors mention that they design the fairness attention mechanism as add-on layers after the EfficientNet feature learning layers and note that testing the model when the fairness learning layers are added after each learning block in the EfficientNet architecture could be a more effective fairness learning mechanism.
- Why unresolved: The current implementation uses a simplified approach, and the authors explicitly identify this as an area for potential improvement but have not conducted experiments with alternative placements.
- What evidence would resolve it: Comparative results showing performance and fairness metrics when fairness layers are placed after each EfficientNet block versus only after the final layer, with statistical significance testing.

### Open Question 3
- Question: What is the optimal balance between the α and β parameters in the knowledge distillation process for different demographic attributes and disease types?
- Basis in paper: [explicit] The authors evaluate the sensitivities of the two key parameters, α and β, and observe that AUC and ES-AUC vary significantly from changes of α and β, emphasizing the importance of choosing proper values.
- Why unresolved: While the authors demonstrate that parameter sensitivity exists and show results for specific cases, they do not provide a systematic framework for determining optimal parameter values across different scenarios.
- What evidence would resolve it: A comprehensive sensitivity analysis across multiple diseases, demographic attributes, and image types, along with a heuristic or automated method for parameter optimization.

## Limitations
- Assumption that fairness learned in classification generalizes to progression prediction may not hold under distribution shifts
- Lack of detailed architectural specifications for MLP attribute encoder and attention mechanism creates implementation uncertainty
- No empirical validation of fairness transfer effectiveness in medical imaging domain

## Confidence

- Mechanism 1 (Fairness transfer via distillation): Medium
- Mechanism 2 (Fairness-aware attention): Medium
- Mechanism 3 (Data efficiency through transfer): Low

## Next Checks

1. **Distribution shift validation**: Test TransFair on a held-out subset of Harvard-GDP with demographic distributions that differ from the training data to assess robustness of fairness transfer.

2. **Attention mechanism ablation**: Compare FairEN performance with and without the fairness-aware attention mechanism on a balanced subset of FairVision to isolate the contribution of demographic-specific feature weighting.

3. **Hyperparameter sensitivity analysis**: Systematically vary α and β across a wider range [0, 10] and measure the tradeoff between ES-AUC and standard AUC to identify optimal fairness-performance balance.