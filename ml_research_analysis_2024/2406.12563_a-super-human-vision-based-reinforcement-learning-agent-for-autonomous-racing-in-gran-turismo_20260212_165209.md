---
ver: rpa2
title: A Super-human Vision-based Reinforcement Learning Agent for Autonomous Racing
  in Gran Turismo
arxiv_id: '2406.12563'
source_url: https://arxiv.org/abs/2406.12563
tags:
- agent
- racing
- time
- human
- track
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a vision-based reinforcement learning agent
  that achieves super-human performance in autonomous racing without relying on global
  features during execution. The agent uses only local features: a 64x64 pixel image
  from an ego-centric camera view and car-centric propriocentric data like velocity.'
---

# A Super-human Vision-based Reinforcement Learning Agent for Autonomous Racing in Gran Turismo

## Quick Facts
- arXiv ID: 2406.12563
- Source URL: https://arxiv.org/abs/2406.12563
- Reference count: 40
- Primary result: First vision-based RL agent achieving super-human performance in autonomous racing using only local features during execution

## Executive Summary
This paper presents a vision-based reinforcement learning agent that achieves super-human performance in autonomous racing in Gran Turismo 7 without relying on global features during execution. The agent uses only local features: a 64x64 pixel image from an ego-centric camera view and car-centric propriocentric data like velocity. To address partial observability challenges, the method employs an asymmetric actor-critic architecture where global features are used during training for the critic network while the policy only sees local inputs at runtime. Evaluated on three tracks with over 130K human players as reference, the agent consistently outperformed the fastest human lap times and matched performance of state-of-the-art methods requiring global features.

## Method Summary
The approach uses an asymmetric actor-critic architecture with Quantile Regression Soft Actor-Critic (QR-SAC) where the critic is trained with both local and global features while the policy is trained only with local features. The agent receives a 64x64 pixel image and car-centric propriocentric data (velocity, acceleration, angular velocity, steering, throttle, brake history) as inputs. Distributed training employs 20 rollout workers on separate PlayStation® 4 systems with synchronous communication. The reward function combines course progress, off-course penalty, wall penalty, steering change penalty, and steering history penalty. The policy network uses a convolutional encoder for image processing combined with an MLP for feature fusion, outputting actions through a Gaussian policy.

## Key Results
- Achieved super-human lap times on three Gran Turismo 7 tracks (Monza, Tokyo, Spa) with p < 0.001 significance
- Outperformed the fastest human lap times across all tracks using only local features at execution time
- Matched performance of state-of-the-art methods requiring global features during execution
- Ablation studies showed velocity features and 64x64 resolution are critical for maintaining high performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric training with global features in the critic but only local features in the policy enables super-human performance while maintaining execution-time sensor constraints.
- Mechanism: The critic learns accurate value estimates using privileged global information during training, while the policy learns to act optimally given only local inputs. This asymmetric information flow during training compensates for partial observability at execution time.
- Core assumption: Global features during training provide sufficient signal for the critic to learn accurate returns even when the policy never sees them during execution.
- Evidence anchors:
  - [abstract]: "By leveraging global features only at training time, the learned agent is able to outperform the best human drivers in time trial races using only local input features."
  - [section 3.4]: "During training, the critic functions are provided with global features og, instead of image observations oi, allowing them to learn accurate returns. The policy is only provided with image and propriocentric features op."
  - [corpus]: Weak evidence - related papers focus on vision-based RL but don't explicitly discuss asymmetric training architectures.
- Break condition: If global features during training fail to provide sufficient information for the critic to learn accurate value estimates, the policy will underperform despite receiving local features during execution.

### Mechanism 2
- Claim: Vision-based perception with compressed 64x64 images is sufficient for super-human racing performance when combined with proper feature extraction.
- Mechanism: The convolutional encoder extracts relevant spatial features from low-resolution images that capture essential track geometry and context, enabling the policy to make driving decisions without requiring high-resolution input.
- Core assumption: Critical racing information (track boundaries, upcoming curves, environmental context) can be preserved in compressed image representations through effective convolutional feature extraction.
- Evidence anchors:
  - [abstract]: "Our agent uses only local features: a 64x64 pixel image from an ego-centric camera view and car-centric propriocentric data like velocity."
  - [section 5.1]: "Empirically we found this resolution to be sufficient to allow the agent to race at super-human speeds."
  - [corpus]: Weak evidence - while related work explores vision-based RL, the specific claim about 64x64 resolution sufficiency isn't directly supported.
- Break condition: If essential racing information is lost during image compression, the agent will fail to navigate effectively despite having access to compressed visual input.

### Mechanism 3
- Claim: Propriocentric velocity features are critical for maintaining high-speed performance and precise control.
- Mechanism: Velocity information allows the agent to modulate throttle and brake inputs appropriately based on current speed, enabling consistent high-speed driving and precise trajectory control around the track.
- Core assumption: Velocity features provide essential state information that cannot be reliably inferred from visual inputs alone in the context of high-speed racing.
- Evidence anchors:
  - [abstract]: "The agent uses only local features: a 64x64 pixel image from an ego-centric camera view and car-centric propriocentric data like velocity."
  - [section 5.1]: "Removing velocity features results in a decrease in the performance of our agent as, naturally, velocity information is fundamental to racing at a consistently high level around the track."
  - [corpus]: Weak evidence - related papers discuss vision-based RL but don't specifically address the criticality of velocity features.
- Break condition: If the agent can successfully infer velocity from visual features or other sensor inputs, the explicit velocity feature may become redundant.

## Foundational Learning

- Concept: Partial observability in reinforcement learning
  - Why needed here: The racing environment is partially observable because the agent cannot see around corners or beyond immediate visual range, requiring compensation mechanisms.
  - Quick check question: What are the two main strategies for handling partial observability in RL, and which one does this work employ?

- Concept: Distributional reinforcement learning with quantile regression
  - Why needed here: QR-SAC provides more stable and accurate value estimates compared to traditional RL methods, which is crucial for learning precise racing control.
  - Quick check question: How does distributional RL differ from traditional RL in terms of what the critic estimates?

- Concept: Convolutional neural networks for visual feature extraction
  - Why needed here: The CNN encoder transforms raw pixel observations into compact feature representations that capture relevant spatial information for driving decisions.
  - Quick check question: What are the key architectural choices in the CNN that enable effective feature extraction from 64x64 racing images?

## Architecture Onboarding

- Component map: Image observation → CNN encoder (Conv4) → MLP feature fusion → Policy network (Gaussian) → Action output; Critic network with global features → Quantile regression → Value estimates
- Critical path: Image observation → CNN encoder → Policy network → Action selection → Environment interaction → Reward collection → Critic update with global features
- Design tradeoffs: Compressed 64x64 images reduce computational load but may lose detail; asymmetric training requires careful coordination between policy and critic; velocity features add robustness but increase input dimensionality
- Failure signatures: Policy collapse to random actions, inability to complete laps, performance degradation on unseen tracks, sensitivity to image compression artifacts
- First 3 experiments:
  1. Train with full image resolution (1920x1080) to establish upper performance bound and identify critical visual features
  2. Remove velocity features to quantify their contribution to performance and test visual-only control
  3. Train with symmetric architecture (global features to both policy and critic) to isolate benefits of asymmetric training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the vision-based agent change if it is trained with recurrent neural networks to better handle partial observability?
- Basis in paper: [inferred] The authors mention in the Conclusions section that incorporating recurrent neural networks could relax the need for propriocentric information as inputs.
- Why unresolved: The paper does not explore the use of recurrent neural networks in the agent's architecture.
- What evidence would resolve it: Training and evaluating the agent with a recurrent neural network architecture and comparing its performance to the current agent on the same racing tasks.

### Open Question 2
- Question: Can the vision-based agent be generalized to handle different lighting conditions and weather scenarios that were not seen during training?
- Basis in paper: [inferred] The Perturbation Study section shows that the agent struggles with different time-of-day conditions and car models, suggesting limitations in generalization.
- Why unresolved: The paper does not explore techniques to improve the agent's generalization capabilities, such as data augmentation or domain randomization.
- What evidence would resolve it: Training the agent with additional data augmentation techniques and evaluating its performance on unseen lighting and weather conditions, as well as different car models.

### Open Question 3
- Question: How does the performance of the vision-based agent compare to other state-of-the-art autonomous racing agents that use global features during execution?
- Basis in paper: [explicit] The authors compare their agent's performance to GT Sophy, a super-human racing agent that uses global features, and show that their agent achieves comparable performance.
- Why unresolved: The comparison is limited to one baseline agent, and it is unclear how the vision-based agent would perform against other state-of-the-art methods.
- What evidence would resolve it: Benchmarking the vision-based agent against a diverse set of state-of-the-art autonomous racing agents on the same racing tasks and comparing their performance in terms of lap times and other relevant metrics.

## Limitations

- The asymmetric training approach relies on the assumption that global features during critic training provide sufficient signal for accurate value estimation, though this isn't thoroughly analyzed
- Evaluation is limited to three tracks with specific car models, raising questions about generalization across diverse racing conditions
- The 64x64 image resolution may not scale to more complex environments with finer track details
- Human player comparison aggregates performance across all skill levels without controlling for individual driver improvement over time

## Confidence

- **High confidence**: The asymmetric actor-critic architecture effectively enables super-human performance while maintaining local-only execution constraints, supported by consistent performance improvements across all three tracks and quantitative ablation studies.
- **Medium confidence**: The 64x64 image resolution is sufficient for super-human racing performance, though this is primarily supported by empirical results on the specific tracks tested rather than theoretical justification.
- **Low confidence**: The attention mechanism analysis accurately reflects how humans drive, as the qualitative comparison is limited to visual inspection without systematic human driver data for comparison.

## Next Checks

1. **Ablation on critic architecture**: Train with symmetric architecture (global features to both policy and critic) and compare performance to quantify the specific contribution of asymmetric training to the super-human results.

2. **Generalization stress test**: Evaluate the trained agent on unseen tracks with varying complexity, lighting conditions, and car models to assess robustness beyond the three test environments.

3. **Human driver comparison protocol**: Conduct controlled time trials with skilled human drivers using the same hardware setup to establish a more rigorous baseline for super-human performance claims.