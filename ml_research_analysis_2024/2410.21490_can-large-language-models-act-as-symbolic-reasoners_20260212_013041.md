---
ver: rpa2
title: Can Large Language Models Act as Symbolic Reasoners?
arxiv_id: '2410.21490'
source_url: https://arxiv.org/abs/2410.21490
tags:
- reasoning
- symbolic
- arxiv
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large language models (LLMs) can
  inherently perform symbolic reasoning or require external support components. The
  authors conducted a comprehensive literature review, examining 14 papers published
  between 2022-2024.
---

# Can Large Language Models Act as Symbolic Reasoners?

## Quick Facts
- arXiv ID: 2410.21490
- Source URL: https://arxiv.org/abs/2410.21490
- Authors: Rob Sullivan; Nelly Elsayed
- Reference count: 35
- Most studies require external symbolic reasoning modules or sophisticated prompting strategies for LLMs to perform reasoning tasks

## Executive Summary
This paper investigates whether large language models (LLMs) can inherently perform symbolic reasoning or require external support components. Through a comprehensive literature review of 14 papers published between 2022-2024, the authors found that most studies employed external symbolic reasoning modules or sophisticated prompting strategies to guide LLMs. Only one paper explored LLM fine-tuning alone as a reasoning mechanism. The findings suggest that current LLMs, with their probabilistic architecture, cannot inherently perform symbolic reasoning and require external guidance.

## Method Summary
The authors conducted a systematic literature review examining papers on LLM reasoning capabilities published between 2022-2024. They searched academic databases including ACM, IEEE Xplore, Scopus, and Google Scholar using specific search terms related to LLMs and reasoning. The review focused on papers where LLMs provided meaningful output and where explicit reasoning mechanisms were proposed, including symbolic reasoning modules, finetuning approaches, and augmented prompting strategies.

## Key Results
- 8 out of 14 reviewed papers employed external symbolic reasoning modules
- Only 1 paper explored LLM fine-tuning alone as a reasoning mechanism
- Prompting strategies have shown some success in generating explanations but limited success in planning and strategic reasoning
- Current LLMs require external guidance due to their probabilistic architecture being incompatible with symbolic reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models require external symbolic reasoning modules to perform symbolic reasoning tasks.
- Mechanism: The statistical nature of LLM training, which focuses on predicting the next token based on probability distributions, does not inherently support the manipulation of symbols according to logical rules. External modules provide the symbolic reasoning capability that LLMs lack.
- Core assumption: The probabilistic architecture of LLMs is fundamentally incompatible with symbolic reasoning processes.
- Evidence anchors:
  - [abstract]: "current LLMs, with their probabilistic architecture, cannot inherently perform symbolic reasoning and require external guidance"
  - [section]: "Fang et al. [16], conclude that LLMs can act as neurosymbolic reasoners, at least in the constrained domains of text-based games with constrained prompts. However, their use of a separate symbolic module raises questions about the extent to which the LLM itself is performing the reasoning step."
  - [corpus]: Weak evidence - corpus contains related papers but none directly support this specific mechanism claim.
- Break condition: If future LLM architectures incorporate symbolic reasoning capabilities into their core training methodology, this mechanism would no longer apply.

### Mechanism 2
- Claim: Sophisticated prompting strategies improve LLM performance on reasoning tasks by providing external guidance.
- Mechanism: Chain-of-thought, tree-of-thought, and other advanced prompting techniques guide the LLM through intermediate reasoning steps, effectively compensating for its lack of inherent reasoning ability.
- Core assumption: LLMs can follow structured reasoning paths when provided with appropriate prompts, even if they cannot generate such paths independently.
- Evidence anchors:
  - [abstract]: "Only one paper explored LLM fine-tuning alone as a reasoning mechanism" suggesting that prompting strategies are more commonly used
  - [section]: "Kojima et al. [12], for example, used the chain-of-thought prompting approach in a two-stage pipeline with the first focused on obtaining the steps the LLM took - a reasoning trace"
  - [corpus]: Weak evidence - corpus contains related papers but none directly support this specific mechanism claim.
- Break condition: If prompting strategies reach diminishing returns or if alternative approaches prove more effective, this mechanism would lose relevance.

### Mechanism 3
- Claim: Process-supervised reward models (PRMs) provide granular interpretation of LLM conclusions by evaluating each reasoning step.
- Mechanism: PRMs offer feedback on intermediate reasoning steps, enabling better performance and interpretability compared to outcome-only supervision.
- Core assumption: Step-by-step evaluation provides more useful feedback for improving reasoning capabilities than end-result evaluation alone.
- Evidence anchors:
  - [abstract]: "Process-supervised reward model (PRM) research suggests a novel path toward providing granular interpretation of conclusions reached by LLMs"
  - [section]: "Lightman et al.'s work... focused on training reward models as a mechanism for improving the performance of fine-tuned LLMs. They conclude that the PRM-based approach not only enables better results from the LLMs, but that this approach also provides a more interpretable output"
  - [corpus]: Weak evidence - corpus contains related papers but none directly support this specific mechanism claim.
- Break condition: If alternative interpretability methods prove more effective or if PRMs cannot handle complex reasoning chains, this mechanism would fail.

## Foundational Learning

- Concept: Symbolic reasoning fundamentals
  - Why needed here: Understanding what symbolic reasoning is and how it differs from statistical pattern matching is crucial for evaluating LLM capabilities
  - Quick check question: Can you explain the difference between manipulating symbols according to logical rules versus predicting the next token based on probability distributions?

- Concept: Large language model architecture
  - Why needed here: Knowing how transformers and attention mechanisms work helps understand why LLMs struggle with symbolic reasoning
  - Quick check question: How does the self-attention mechanism in transformers contribute to the statistical nature of LLM outputs?

- Concept: Prompt engineering techniques
  - Why needed here: Understanding various prompting strategies is essential for implementing the external guidance that LLMs require for reasoning tasks
  - Quick check question: What is the key difference between chain-of-thought prompting and standard prompting approaches?

## Architecture Onboarding

- Component map: LLM core → External symbolic reasoning module OR Sophisticated prompting strategy OR Process-supervised reward model
- Critical path: Input query → Prompt engineering (if used) → LLM processing → External reasoning component (if used) → Output generation → PRM evaluation (if used)
- Design tradeoffs: Using external modules adds complexity and latency but enables reasoning capabilities; relying solely on prompting may be simpler but less robust
- Failure signatures: Hallucinations in reasoning steps, inability to handle novel symbolic problems, inconsistent performance across domains
- First 3 experiments:
  1. Test baseline LLM performance on symbolic reasoning tasks without any external components
  2. Implement chain-of-thought prompting and measure performance improvement
  3. Integrate an external symbolic reasoning module and compare results against prompting-only approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs inherently perform symbolic reasoning without external guidance, or is this capability fundamentally limited by their probabilistic architecture?
- Basis in paper: [explicit] The paper concludes that current LLMs, with their probabilistic architecture, cannot inherently perform symbolic reasoning and require external guidance.
- Why unresolved: While prompting strategies have shown some success in generating explanations, planning and strategic reasoning capabilities remain limited. The paper suggests this is an open question for future research.
- What evidence would resolve it: Empirical studies comparing LLMs with and without external symbolic reasoning modules on diverse reasoning tasks, particularly in planning and strategy domains.

### Open Question 2
- Question: To what extent can sophisticated prompting strategies like chain-of-thought and tree-of-thought replace external symbolic reasoning modules in enabling LLMs to perform reasoning tasks?
- Basis in paper: [explicit] The paper notes that prompting strategies have shown some success in generating explanations, but planning and strategic reasoning capabilities remain limited compared to external modules.
- Why unresolved: The literature shows mixed results - while prompting improves outputs, it's unclear if it can fully substitute for dedicated reasoning components, especially for complex reasoning tasks.
- What evidence would resolve it: Comparative studies measuring LLM performance on reasoning tasks using only prompting strategies versus integrated symbolic reasoning modules across various domains.

### Open Question 3
- Question: How do process-supervised reward models (PRMs) compare to outcome-supervised reward models (ORMs) in improving LLM reasoning capabilities and providing interpretable explanations?
- Basis in paper: [explicit] The paper discusses Lightman et al.'s work showing PRMs enable better results and more interpretable outputs than ORMs through step-by-step feedback.
- Why unresolved: While PRMs show promise, the paper notes uncertainty about their effectiveness in domains with fewer solution paths, like medical diagnosis.
- What evidence would resolve it: Systematic evaluation of PRMs versus ORMs across diverse problem domains with varying solution space complexities, measuring both performance and interpretability.

## Limitations

- Small sample size (14 papers) limits generalizability of findings
- All reviewed studies focus on narrow, constrained domains like text-based games
- Evidence base is limited to recent publications in specific domains

## Confidence

- Core finding that LLMs require external support: Medium
- Probabilistic architecture incompatibility with symbolic reasoning: Low
- Effectiveness of prompting strategies: Medium
- Process-supervised reward models: Low

## Next Checks

1. **Domain Expansion Validation**: Replicate the literature review with expanded inclusion criteria to include papers from broader reasoning domains beyond text-based games and narrow symbolic tasks.

2. **Comparative Performance Analysis**: Conduct controlled experiments comparing LLM performance with different external support mechanisms (symbolic modules, prompting strategies, PRMs) on standardized reasoning benchmarks.

3. **Architectural Innovation Survey**: Survey recent pre-prints and ongoing research to identify emerging LLM architectures or training methodologies that might enable more native symbolic reasoning capabilities.