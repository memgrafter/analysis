---
ver: rpa2
title: Robust Yet Efficient Conformal Prediction Sets
arxiv_id: '2407.09165'
source_url: https://arxiv.org/abs/2407.09165
tags:
- sets
- prediction
- bound
- score
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of conformal prediction
  (CP) to adversarial attacks by introducing a method to compute provably robust prediction
  sets under both evasion and poisoning attacks. The core idea leverages randomized
  smoothing to derive tighter upper bounds on conformity scores using the cumulative
  distribution function (CDF), rather than just the mean.
---

# Robust Yet Efficient Conformal Prediction Sets

## Quick Facts
- arXiv ID: 2407.09165
- Source URL: https://arxiv.org/abs/2407.09165
- Reference count: 40
- Primary result: CDF-aware approach yields smaller and more efficient prediction sets while maintaining the same coverage guarantees

## Executive Summary
This paper addresses the vulnerability of conformal prediction (CP) to adversarial attacks by introducing a method to compute provably robust prediction sets under both evasion and poisoning attacks. The core idea leverages randomized smoothing to derive tighter upper bounds on conformity scores using the cumulative distribution function (CDF), rather than just the mean. This CDF-aware approach yields smaller and more efficient prediction sets while maintaining the same coverage guarantees. The method supports continuous, discrete, and sparse data, and combines robustness to both evasion and poisoning attacks. Experiments on CIFAR-10, ImageNet, and Cora-ML datasets show consistent improvements in set efficiency and coverage over previous baselines.

## Method Summary
The method uses randomized smoothing to compute probabilistic bounds on conformity scores under adversarial perturbations, then applies these bounds to derive conservative prediction sets that maintain coverage guarantees even under attacks. The key innovation is using the CDF of smoothed scores rather than just the mean, which provides tighter upper bounds and more efficient prediction sets. The approach handles both evasion attacks (at test time) and poisoning attacks (during calibration) by computing conservative thresholds that account for worst-case scenarios. The method is computationally efficient, with complexity that scales linearly with the number of classes and calibration points.

## Key Results
- CAS achieves 8.9% smaller prediction sets than RSCP on CIFAR-10 under evasion attacks
- CAS with poisoning robustness maintains coverage while reducing set size by 7.4% on ImageNet
- CDF-aware bounds provide 15-20% tighter certificates than mean-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CDF-aware bounds on conformity scores yield tighter adversarial certificates than mean-only bounds
- Mechanism: The cumulative distribution function of smoothed scores contains more information than just the mean, enabling sharper worst-case bounds via Anderson's inequality
- Core assumption: Randomized smoothing preserves exchangeability between clean and perturbed data, allowing CDF-based certificates to remain valid
- Evidence anchors:
  - [abstract] "CDF-aware approach yields smaller and more efficient prediction sets while maintaining the same coverage guarantees."
  - [section 4] "Our key insight is that we can use the cumulative distribution (CDF) of smooth scores to obtain tighter upper bounds."
  - [corpus] No direct corpus support for this specific mechanism; existing works focus on mean-based certificates
- Break condition: If smoothing introduces non-symmetric transformations that break exchangeability, CDF bounds become invalid

### Mechanism 2
- Claim: Combining evasion and poisoning robustness in a single framework is possible by independently computing conservative thresholds
- Mechanism: Feature poisoning optimization finds a conservative quantile qα; evasion robustness computes an upper bound on test scores. Since these operations are independent, their results can be combined without additional cost
- Core assumption: Robustness to evasion and poisoning attacks are orthogonal problems that can be addressed separately
- Evidence anchors:
  - [section 3.3] "Interestingly, our evasion-aware sets can easily be combined with our poisoning-aware threshold to obtain prediction sets that are robust to both types of attacks."
  - [section 5] "Since robustness to evasion and poisoning are independent, we can achieve simultaneous robustness to both evasion and poisoning via Cα = {y : scdf(x, y) ≥ qα}."
  - [corpus] No direct corpus support for this combined approach; existing work treats these attacks separately
- Break condition: If the same data point is simultaneously subject to both feature and label poisoning in a way that creates interaction effects, the independence assumption fails

### Mechanism 3
- Claim: Calibration-time computation of upper bounds significantly reduces test-time complexity for evasion robustness
- Mechanism: Instead of computing upper bounds for each test point and class at inference time, compute lower bounds for calibration points once during training. This shifts O(m × c × tMC × tb) complexity to O(n × tMC × tb) + O(c × tMC)
- Core assumption: The calibration set is sufficiently representative that lower bounds computed there transfer to test points
- Evidence anchors:
  - [section 5] "Prop. 5.1 gives a worst-case coverage lower bound for vanilla CP... we can find a conservative quantile that results in a certified 1 − α coverage probability for the worst case input."
  - [section B] "With n calibration points and c classes, we need O(n × tMC) time for calibration... At test time we need O(c × tMC × tbound) time."
  - [corpus] No direct corpus support for this specific optimization; existing work focuses on test-time computation
- Break condition: If the calibration and test distributions differ significantly, lower bounds computed on calibration data may not provide valid certificates for test points

## Foundational Learning

- Concept: Randomized smoothing for adversarial certificates
  - Why needed here: Provides a black-box method to compute probabilistic bounds on model outputs under adversarial perturbations, which is essential for deriving conformal prediction guarantees
  - Quick check question: What is the key property that makes randomized smoothing effective for adversarial certification? (Answer: It creates a Lipschitz continuous function with bounded sensitivity to input perturbations)

- Concept: Exchangeability in conformal prediction
  - Why needed here: The theoretical foundation that allows using calibration data to provide coverage guarantees for test data, even under adversarial perturbations when properly modified
  - Quick check question: Why does exchangeability matter for the validity of conformal prediction sets? (Answer: It ensures the statistical relationship between calibration and test data holds, enabling quantile-based threshold selection)

- Concept: Mixed-integer linear programming for optimization under constraints
  - Why needed here: Enables solving the complex optimization problems for finding conservative thresholds under poisoning attacks while respecting budget constraints
  - Quick check question: What role do binary variables play in the poisoning optimization problems? (Answer: They enforce the constraint that at most k points can be perturbed)

## Architecture Onboarding

- Component map: Data -> Preprocess -> Smooth -> Bound CDF -> Compute Quantile -> Predict Sets
- Critical path:
  1. Preprocess: Train model with noisy augmentation
  2. Calibration: Compute smooth scores and CDF bounds for calibration points
  3. Threshold: Solve poisoning MILP to find conservative qα
  4. Inference: For each test point, compute smooth scores and apply upper bounds
  5. Output: Construct prediction sets using scdf(x, y) ≥ qα
- Design tradeoffs:
  - Monte Carlo sample count vs. bound tightness: More samples yield better estimates but increase computation time
  - Smoothing strength σ vs. robustness: Larger σ provides better certificates but increases set size
  - Calibration set size vs. runtime: Larger sets improve coverage concentration but increase precomputation time
- Failure signatures:
  - Coverage dropping below 1 − α: Indicates broken exchangeability or insufficient smoothing
  - Prediction sets growing exponentially with radius: Suggests overly conservative bounds or insufficient sample correction
  - MILP solver failing to converge: Points to overly complex constraints or insufficient computational resources
- First 3 experiments:
  1. Baseline comparison: Run CAS vs. RSCP on CIFAR-10 with r = 0.12 to verify set size improvements
  2. Smoothing sensitivity: Vary σ from 0.1 to 0.5 and measure impact on coverage vs. set size tradeoff
  3. Poisoning budget scaling: Test CAS with k = 1, 3, 5, ∞ to understand cost of robustness to feature poisoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CAS compare to other robustness methods beyond RSCP when evaluated on more diverse threat models or datasets?
- Basis in paper: [inferred] The paper compares CAS only to RSCP and uses three datasets (CIFAR-10, ImageNet, and Cora-ML). It also mentions the potential for extending to other threat models
- Why unresolved: The paper does not provide a comprehensive comparison across various threat models or a wider range of datasets, leaving uncertainty about the general applicability of CAS
- What evidence would resolve it: Experimental results showing CAS's performance against other robustness methods across diverse datasets and threat models would clarify its comparative effectiveness

### Open Question 2
- Question: What is the computational overhead of using CAS compared to standard CP, especially for large-scale datasets or models with many classes?
- Basis in paper: [explicit] The paper discusses the computational efficiency of CAS, particularly in the calibration-time variant, but does not provide a detailed analysis of the overhead compared to standard CP
- Why unresolved: While the paper suggests computational savings, it lacks a quantitative comparison of the overhead involved in using CAS versus standard CP, especially in large-scale scenarios
- What evidence would resolve it: A detailed computational analysis comparing the time and resource requirements of CAS and standard CP on large datasets or models with many classes would provide clarity on the overhead

### Open Question 3
- Question: How does the choice of smoothing distribution (e.g., Gaussian vs. uniform) affect the performance and robustness guarantees of CAS?
- Basis in paper: [explicit] The paper mentions the use of Gaussian and sparse smoothing but does not explore the impact of different smoothing distributions on CAS's performance
- Why unresolved: The paper does not investigate how alternative smoothing distributions might influence the effectiveness or efficiency of CAS, leaving uncertainty about the optimal choice of smoothing
- What evidence would resolve it: Experimental results comparing the performance and robustness guarantees of CAS using different smoothing distributions would reveal the impact of this choice on its effectiveness

## Limitations

- Theoretical dependence on strong exchangeability assumptions under adversarial perturbations
- Computational complexity of solving MILP problems for large-scale datasets
- Limited empirical validation on specific attack budgets and datasets

## Confidence

- Mechanism 1 (CDF-aware bounds): High - theoretical derivation is rigorous with supporting experimental evidence
- Mechanism 2 (Combined robustness): Medium - logical but lacks empirical validation under simultaneous attacks
- Mechanism 3 (Calibration optimization): High - straightforward computational improvement with clear benefits

## Next Checks

1. Test CAS under simultaneous evasion and poisoning attacks to verify the independence assumption holds in practice
2. Evaluate performance on larger-scale datasets (e.g., ImageNet-1K full) to assess scalability of MILP solvers
3. Compare against non-robust CP baselines to quantify the efficiency-cost tradeoff of adversarial robustness