---
ver: rpa2
title: 'RaFe: Ranking Feedback Improves Query Rewriting for RAG'
arxiv_id: '2405.14431'
source_url: https://arxiv.org/abs/2405.14431
tags:
- query
- feedback
- rafe
- rewriting
- rewrite
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RaFe, a novel framework for training query
  rewriting models that leverages ranking feedback from a reranker instead of requiring
  annotated data. The core idea is to use a publicly available reranker to score documents
  retrieved by rewritten queries, then use these scores as feedback to train the rewriting
  model.
---

# RaFe: Ranking Feedback Improves Query Rewriting for RAG

## Quick Facts
- arXiv ID: 2405.14431
- Source URL: https://arxiv.org/abs/2405.14431
- Reference count: 23
- Key outcome: Novel framework for training query rewriting models using reranker feedback instead of annotated data, achieving 2-3% QA accuracy improvements on cross-lingual open-domain QA datasets.

## Executive Summary
RaFe introduces a novel framework for training query rewriting models that leverages ranking feedback from a reranker instead of requiring annotated data. The core idea is to use a publicly available reranker to score documents retrieved by rewritten queries, then use these scores as feedback to train the rewriting model. RaFe supports both offline and online reinforcement learning settings. Experiments on cross-lingual open-domain QA datasets show RaFe outperforms baselines like LLM-Rewrite and Query2Doc, achieving 2-3% absolute improvements in QA accuracy when combining query rewriting with document reranking.

## Method Summary
RaFe employs a two-stage process to train query rewriting models without annotated data. First, an initial supervised fine-tuning (SFT) phase trains the rewriter on LLM-generated rewrites paired with original queries. Second, feedback training uses reranker scores to create good/bad rewrite pairs, which are then used to train the model via Direct Preference Optimization (DPO), Kullback-Leibler Importance Estimation Procedure (KTO), or Proximal Policy Optimization (PPO). The framework supports both offline (batch processing) and online (real-time) reinforcement learning settings. Experiments use cross-lingual open-domain QA datasets with English (NQ, TriviaQA, HotpotQA, FreshQA) and Chinese (WebQA, FreshQA) collections, employing a publicly available reranker (bge-reranker-base) and pre-trained rewrite models (Qwen-7b-base for training, Qwen-max for generation, Qwen1.5-32b-chat for evaluation).

## Key Results
- RaFe outperforms baselines (OQR, LLM-Rewrite, Query2Doc, SFT) by 2-3% absolute QA accuracy on cross-lingual open-domain QA datasets.
- Optimal performance achieved with 2-3 rewrites per query, balancing retrieval effectiveness and computational efficiency.
- Reranker feedback effectively guides the model to produce rewrites that retrieve more relevant documents, with strong performance gains when combining query rewriting with document reranking.

## Why This Works (Mechanism)
The framework works by creating a feedback loop where the reranker's document scoring provides implicit supervision for the query rewriting process. When a rewritten query retrieves better-ranked documents, this signals the rewrite was effective, allowing the model to learn which rewriting strategies improve retrieval. The reranker acts as a proxy for ground truth relevance without requiring expensive annotation. This approach is particularly effective because it leverages the existing capabilities of modern rerankers to provide continuous, scalable feedback for training rewriting models, enabling improvement without the need for human-labeled query-rewrite pairs.

## Foundational Learning
- **Query rewriting fundamentals**: Understanding how query reformulation improves retrieval relevance by incorporating context and expanding terms.
  - Why needed: Core mechanism being optimized
  - Quick check: Can you explain how rewriting affects retrieval precision?

- **Reranker operation**: How cross-encoder models score document relevance by considering query-document pairs jointly.
  - Why needed: The feedback mechanism relies on reranker scoring
  - Quick check: Can you describe how rerankers differ from traditional retrievers?

- **Reinforcement learning for NLP**: Basics of policy optimization methods (DPO, KTO, PPO) for training models with feedback signals.
  - Why needed: Training approach for incorporating reranker feedback
  - Quick check: Can you differentiate between offline and online RL approaches?

- **Cross-lingual QA evaluation**: Metrics and methodologies for assessing question answering performance across languages.
  - Why needed: Experimental validation across English and Chinese datasets
  - Quick check: Can you explain the difference between Exact Match and Rouge-L metrics?

## Architecture Onboarding

**Component Map:** Original Query -> Rewrite Generator -> Retriever -> Reranker -> Feedback Signal -> Rewrite Generator (training loop)

**Critical Path:** Query rewriting -> Retrieval -> Reranking -> Feedback incorporation

**Design Tradeoffs:** 
- Uses publicly available reranker vs. domain-specific reranker (better performance vs. practical accessibility)
- Number of rewrites per query (2-3 optimal) balances performance gains against computational cost
- Offline vs. online training (batch efficiency vs. real-time adaptation)

**Failure Signatures:** 
- Poor performance in SUBSTITUTE setting indicates rewrites deviating from original query intent
- Ineffective feedback training when reranker scores poorly distinguish good/bad rewrites
- Suboptimal rewrite quality when using too few or too many rewrites per query

**3 First Experiments:**
1. Test rewrite quality by comparing generated rewrites to original queries using semantic similarity metrics
2. Validate reranker effectiveness by measuring score distribution differences between good and bad rewrite pairs
3. Benchmark baseline performance without rewriting to establish performance floor

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the RaFe framework perform in domain-specific scenarios where a domain-specific reranker is available, compared to using a general reranker?
- Basis in paper: Explicit
- Why unresolved: The paper primarily uses a general, publicly available reranker to demonstrate effectiveness across cross-lingual datasets, mentioning that RaFe would perform even better with a domain-specific reranker but without experimental validation.
- What evidence would resolve it: Experimental results comparing RaFe using a domain-specific reranker versus a general reranker on a domain-specific dataset.

### Open Question 2
- Question: What is the impact of the number of rewrites on the efficiency and effectiveness of the RaFe framework in real-world applications?
- Basis in paper: Explicit
- Why unresolved: The paper discusses the optimal number of rewrites (2-3) for balancing performance and time cost but does not explore trade-offs in detail or provide comprehensive analysis of how varying numbers affect efficiency in practical scenarios.
- What evidence would resolve it: Detailed analysis and experimental results showing performance and time cost trade-offs for different numbers of rewrites in real-world applications.

### Open Question 3
- Question: How does the RaFe framework handle cases where the original query is already well-formed and does not require rewriting?
- Basis in paper: Inferred
- Why unresolved: The paper does not explicitly address how RaFe deals with well-formed queries that may not benefit from rewriting, focusing instead on cases where rewriting improves retrieval.
- What evidence would resolve it: Experimental results comparing RaFe performance on well-formed queries versus those requiring rewriting, highlighting potential negative impacts or lack of improvement.

### Open Question 4
- Question: What are the limitations of using a reranker as feedback for query rewriting, and how can these limitations be mitigated?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges limitations such as lack of cross-domain validation and reliance on rewriting effectiveness as a bottleneck but does not provide solutions or strategies to address these issues.
- What evidence would resolve it: Research or experiments demonstrating strategies to mitigate limitations of using reranker feedback, such as developing more robust rerankers or integrating additional feedback mechanisms.

## Limitations
- Framework performance depends on the quality and capabilities of the publicly available reranker used for feedback
- Optimal number of rewrites (2-3) may not generalize across all domains or query types
- Training involves complex interactions between multiple components that may not be fully captured in experimental setup

## Confidence
- High confidence: The core methodology of using reranker feedback for training query rewriters is technically sound and reported improvements over baselines are plausible
- Medium confidence: Cross-lingual generalization claims are supported by Chinese dataset experiments, but performance differences between English and Chinese implementations raise questions about implementation consistency
- Medium confidence: Effectiveness in both offline and online settings is demonstrated, but online setting's real-world applicability depends on factors not fully explored

## Next Checks
1. **Sensitivity Analysis**: Test framework performance with different reranker models and configurations to assess robustness to reranker quality variations
2. **Scaling Study**: Evaluate how the number of rewrites per query affects performance across different dataset sizes and query complexities
3. **Domain Transfer**: Apply trained models to a different domain (e.g., biomedical or legal) to test generalizability beyond open-domain QA