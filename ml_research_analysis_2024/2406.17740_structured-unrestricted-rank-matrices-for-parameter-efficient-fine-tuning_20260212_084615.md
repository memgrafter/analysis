---
ver: rpa2
title: Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning
arxiv_id: '2406.17740'
source_url: https://arxiv.org/abs/2406.17740
tags:
- matrices
- matrix
- circulant
- lora
- surm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Structured Unrestricted-Rank Matrices (SURMs)
  as an alternative to low-rank matrices for parameter-efficient fine-tuning of large
  Transformer models. The core idea is to parameterize weight updates using structured
  matrices like Kronecker products and low displacement rank matrices (LDRMs) such
  as circulant and Toeplitz matrices, which offer better approximation quality than
  low-rank matrices while maintaining efficiency.
---

# Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning

## Quick Facts
- arXiv ID: 2406.17740
- Source URL: https://arxiv.org/abs/2406.17740
- Reference count: 40
- Primary result: SURMs achieve 5-7% accuracy gains over LoRA on image tasks and enable up to 12x parameter reduction for adapters on GLUE

## Executive Summary
This paper introduces Structured Unrestricted-Rank Matrices (SURMs) as an alternative to low-rank matrices for parameter-efficient fine-tuning of large Transformer models. The authors propose using structured matrices like Kronecker products and low displacement rank matrices (LDRMs) such as circulant and Toeplitz matrices to parameterize weight updates, achieving better approximation quality than low-rank matrices while maintaining efficiency. The method demonstrates consistent improvements across vision and NLP tasks, with particular success in image classification where SURMs achieve 5-7% accuracy gains over LoRA and in adapters where they enable up to 12x parameter reduction with minimal quality loss.

## Method Summary
The method replaces low-rank matrices in parameter-efficient fine-tuning (LoRA and adapters) with structured matrices including circulant, Toeplitz, and Kronecker products. These structured matrices exploit patterns like cyclic shifts and diagonal constancy to require fewer parameters than full matrix storage while retaining approximation capacity. The approach uses efficient matrix-vector multiplication via FFT for circulant and Toeplitz matrices, and implements initialization schemes (random vs. zero) to ensure proper training dynamics. The authors integrate these structured matrices into existing PEFT frameworks and validate their effectiveness across multiple vision and NLP benchmarks.

## Key Results
- SURMs achieve 5-7% accuracy gains over LoRA on various image classification tasks (CIFAR10, CIFAR100, SUN397, DTD, STL10)
- SURMs enable up to 12x reduction in parameters for adapter-based fine-tuning on the GLUE benchmark with minimal quality loss
- Structured matrices outperform low-rank approximations on random, near-low-rank, and near-low-intrinsic-rank matrices in empirical comparisons

## Why This Works (Mechanism)

### Mechanism 1
Structured matrices (SURMs) can approximate general matrices better than low-rank matrices under a fixed parameter budget. SURMs like circulant and Toeplitz matrices exploit structural patterns (e.g., cyclic shifts or diagonal constancy) that require fewer parameters than full matrix storage while retaining approximation capacity. The core assumption is that the parameter updates ΔW in fine-tuning are not inherently low-rank, so a richer structural parameterization improves expressiveness. Evidence shows circulant/Toeplitz matrices outperform low-rank approximations on various matrix types.

### Mechanism 2
Using structured unrestricted-rank matrices enables larger effective rank while keeping trainable parameters small. Kronecker products A⊗B can produce high-rank updates even with small individual factors, unlike low-rank AB^T which is strictly limited by rank r. The core assumption is that rank of the update matrix is more important for expressiveness than strict parameter count when using structured matrices. This allows creation of matrices ΔW of fairly large ranks while keeping the number of trainable parameters small.

### Mechanism 3
Integrating SURMs into adapters yields up to 12× parameter reduction with minimal quality loss. Replacing low-rank adapter matrices A and B with structured ones (e.g., circulant × circulant) preserves expressiveness but drastically cuts parameters due to linear parameterization. The core assumption is that the bottleneck in adapters is parameter count, not the functional form, so structured matrices can replace low-rank without sacrificing performance.

## Foundational Learning

- Concept: Low displacement rank matrices (LDRMs)
  - Why needed here: LDRMs (e.g., circulant, Toeplitz) form the core class of SURMs and provide sub-quadratic matrix-vector multiplication
  - Quick check question: What is the displacement rank of a circulant matrix with respect to (Z, Z)?

- Concept: Kronecker product properties
  - Why needed here: Kronecker products allow large-rank updates with few parameters, essential for efficient fine-tuning
  - Quick check question: How does the rank of A⊗B relate to the ranks of A and B individually?

- Concept: Fast Fourier Transform (FFT) for structured matrices
  - Why needed here: Enables efficient matrix-vector multiplication for circulant and Toeplitz matrices, crucial for scaling
  - Quick check question: What is the time complexity of multiplying a vector by a circulant matrix using FFT?

## Architecture Onboarding

- Component map:
  Base model (pre-trained transformer) -> PEFT layer (adapter or LoRA block) -> SURM parameterization (structured matrices) -> Training pipeline (freeze base, train SURM parameters)

- Critical path:
  1. Choose base model and task
  2. Select SURM variant (circulant recommended)
  3. Integrate SURM into LoRA/adapter block
  4. Initialize SURM parameters (zero init for one factor, random for other)
  5. Train with standard PEFT hyperparameters

- Design tradeoffs:
  - Circulant: Fastest, best empirical performance, moderate expressiveness
  - Toeplitz: Slightly more expressive, slower (factor of 2), more parameters
  - Kronecker: Highest rank potential, moderate speed, complex to tune

- Failure signatures:
  - Training diverges: Check initialization (zero vs random)
  - No performance gain: Verify SURM integration matches LoRA/adapter shape
  - Slow training: Confirm FFT-based matmul is enabled

- First 3 experiments:
  1. Replace LoRA rank-1 with SURM circulant on a small GLUE task (RTE)
  2. Compare SURM circulant vs. Toeplitz adapters on MRPC
  3. Benchmark training speed (iterations/sec) of each SURM variant on CIFAR10

## Open Questions the Paper Calls Out

### Open Question 1
What is the impact of using higher-rank matrices in SURMs on model performance and parameter efficiency compared to using low-rank matrices? The paper discusses the potential for using higher-rank matrices in SURMs to increase the number of training parameters and improve expressiveness, but does not provide empirical results comparing this approach to using low-rank matrices.

### Open Question 2
How do SURMs perform in other parameter-efficient fine-tuning paradigms beyond LoRA and adapters, such as prefix-tuning or prompt-tuning? The paper demonstrates the effectiveness of SURMs in LoRA and adapter settings, but does not explore their potential in other PEFT paradigms.

### Open Question 3
What is the impact of different structured matrix types (e.g., Kronecker, Toeplitz, circulant) on model performance and computational efficiency when used in SURMs? While the paper shows that different structured matrices can be used in SURMs, it does not delve into the trade-offs between their performance and computational efficiency.

## Limitations
- Performance claims rely heavily on empirical results without extensive ablation studies on initialization schemes
- Theoretical justification for why structured matrices outperform low-rank in all scenarios remains underdeveloped
- Limited validation across diverse model architectures beyond ViT and BERT

## Confidence
- High confidence: Computational efficiency claims (O(n log n) for circulant, O(n²) for Toeplitz) are well-established in linear algebra literature
- Medium confidence: Empirical performance improvements are demonstrated on tested datasets but may not generalize uniformly across all tasks
- Low confidence: Assertion that SURMs will consistently outperform LoRA across all fine-tuning scenarios lacks extensive validation

## Next Checks
1. Initialization Sensitivity Analysis: Systematically vary initialization schemes (random vs. zero) for structured matrices across different tasks to quantify their impact on convergence and final performance
2. Cross-Architecture Generalization: Test SURMs on additional model architectures beyond ViT and BERT (e.g., DeiT, Swin Transformers, RoBERTa) to assess robustness
3. Computational Overhead Validation: Measure actual training time per iteration for each SURM variant (circulant, Toeplitz, Kronecker) on multiple hardware configurations to verify claimed efficiency gains