---
ver: rpa2
title: Automated Question Generation on Tabular Data for Conversational Data Exploration
arxiv_id: '2407.12859'
source_url: https://arxiv.org/abs/2407.12859
tags:
- questions
- data
- user
- dataset
- columns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a system for automatically generating interesting
  natural language questions from structured datasets to support conversational data
  exploration. The method uses interestingness measures to identify relevant data
  slices, then applies a fine-tuned T5 language model to generate questions, which
  are slot-filled with data values and ranked for user recommendation.
---

# Automated Question Generation on Tabular Data for Conversational Data Exploration

## Quick Facts
- **arXiv ID:** 2407.12859
- **Source URL:** https://arxiv.org/abs/2407.12859
- **Reference count:** 29
- **Primary result:** Expert evaluation shows high fluency (4.8/5), faithfulness (4.6/5), and interestingness (4.1/5) scores for generated questions

## Executive Summary
This paper introduces an automated system for generating natural language questions from structured tabular data to support conversational data exploration. The approach uses interestingness measures to identify relevant data slices, then applies a fine-tuned T5 language model to generate questions which are slot-filled with data values and ranked for user recommendation. A custom training corpus of aggregate questions was created from Kaggle datasets to enable fine-tuning. The system improves question relevance iteratively based on user feedback, enabling non-technical users to explore datasets through meaningful, context-aware questions without requiring deep data knowledge.

## Method Summary
The system employs a two-stage approach: first, interestingness measures identify relevant data slices from structured tables; second, a fine-tuned T5-base model generates natural language questions based on these slices. The method uses a custom training corpus created from aggregate questions on Kaggle datasets, with the model taking metadata and column operators as input and producing questions as output. Generated questions are slot-filled with actual data values and ranked using interestingness scores and user feedback probabilities. The iterative feedback mechanism allows the system to learn user preferences and adjust question generation accordingly, with expert evaluation showing strong performance across fluency, faithfulness, and interestingness metrics.

## Key Results
- Expert evaluation scores: Fluency 4.8/5, Faithfulness 4.6/5, Interestingness 4.1/5
- System enables non-technical users to explore datasets through meaningful questions
- Iterative feedback mechanism improves question relevance over time
- Custom training corpus of aggregate questions enables effective model fine-tuning

## Why This Works (Mechanism)
The system works by combining data-driven interestingness measures with language model generation capabilities. By identifying statistically interesting data slices first, the system ensures that generated questions are grounded in meaningful patterns. The fine-tuned T5 model, trained on domain-specific aggregate questions, can produce grammatically correct and contextually appropriate questions. The slot-filling mechanism ensures that questions are personalized to the actual data values, while the ranking system prioritizes questions most likely to be relevant to users. The iterative feedback loop allows the system to learn user preferences and adapt question generation accordingly.

## Foundational Learning
- **Interestingness measures**: Statistical metrics to identify meaningful data patterns (needed to ensure questions are based on relevant insights; quick check: verify correlation coefficients and statistical significance calculations)
- **Fine-tuning text-to-text models**: Adapting pre-trained models like T5 to specific domains (needed for generating contextually appropriate questions; quick check: validate loss convergence and evaluation metrics on held-out data)
- **Slot-filling mechanisms**: Replacing placeholders with actual data values (needed to personalize questions to specific datasets; quick check: ensure correct data type matching and value formatting)
- **Ranking algorithms**: Prioritizing questions based on multiple criteria (needed to present most relevant questions first; quick check: verify ranking stability across different parameter settings)
- **Iterative learning from feedback**: Updating model behavior based on user interactions (needed for personalization and improved relevance; quick check: track performance improvement across feedback iterations)

## Architecture Onboarding
**Component Map:** Tabular data -> Interestingness Measures -> Data Slice Identification -> Fine-tuned T5 Model -> Question Generation -> Slot-Filling -> Ranking -> User Recommendation

**Critical Path:** Data input -> Interestingness analysis -> Question generation -> Slot-filling -> Ranking -> Output

**Design Tradeoffs:** Uses fine-tuned T5 instead of larger models to balance quality and computational efficiency; employs custom training corpus rather than generic data to ensure domain relevance; implements iterative feedback instead of static ranking to improve personalization

**Failure Signatures:** Generated questions may be grammatically incorrect or factually inaccurate; initial rankings may not align with user preferences; system may struggle with highly complex or sparse datasets

**First 3 Experiments:**
1. Test interestingness measure implementation on simple datasets with known patterns
2. Validate question generation quality on held-out aggregate question examples
3. Evaluate slot-filling accuracy with various data types and formats

## Open Questions the Paper Calls Out
None

## Limitations
- Method relies on custom training corpus from Kaggle datasets, limiting generalization to other data sources
- Interestingness measures and ranking methodology lack full implementation details for faithful reproduction
- Evaluation limited to expert assessments rather than comprehensive user studies
- Computational efficiency and scalability for large datasets not addressed

## Confidence
**High confidence** in overall approach and reported evaluation methodology
**Medium confidence** in technical implementation details of interestingness measures and ranking algorithms
**Low confidence** in generalization to non-Kaggle datasets and real-world conversational scenarios

## Next Checks
1. Implement and validate the exact interestingness measures and ranking methodology using provided pseudocode or implementation details
2. Conduct user studies to verify the effectiveness of the iterative feedback mechanism in real conversational settings
3. Test the system on diverse datasets beyond Kaggle to assess generalization and identify potential failure modes in different data contexts