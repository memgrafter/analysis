---
ver: rpa2
title: 'Oops, I Sampled it Again: Reinterpreting Confidence Intervals in Few-Shot
  Learning'
arxiv_id: '2409.02850'
source_url: https://arxiv.org/abs/2409.02850
tags:
- shot
- tasks
- methods
- dataset
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how confidence interval (CI) computation
  methods affect the interpretation of few-shot learning (FSL) results. It identifies
  that the predominant method, which samples tasks with replacement, produces "closed
  CIs" that do not account for data randomness.
---

# Oops, I Sampled it Again: Reinterpreting Confidence Intervals in Few-Shot Learning

## Quick Facts
- arXiv ID: 2409.02850
- Source URL: https://arxiv.org/abs/2409.02850
- Reference count: 40
- Primary result: Sampling with replacement in FSL CIs underestimates uncertainty by ignoring data randomness

## Executive Summary
This paper identifies a critical flaw in how confidence intervals (CIs) are typically computed in few-shot learning (FSL) evaluations. The predominant method samples tasks with replacement, creating "closed CIs" that only account for sampler randomness while ignoring data randomness. The authors demonstrate that these CIs can be significantly narrower than "open CIs" computed without replacement, particularly for smaller datasets. They propose using paired tests and optimizing task sizes to reduce CI ranges and introduce a new benchmark that implements these strategies, demonstrating improved conclusiveness in method comparisons.

## Method Summary
The paper proposes a new evaluation methodology for few-shot learning that addresses CI underestimation. The method involves: (1) sampling tasks without replacement to compute open CIs that account for both sampler and data randomness, (2) using paired tests to compare methods on the same task sets, reducing variance due to task difficulty, and (3) optimizing the number of queries per task (Q) to minimize CI ranges. The authors implement these strategies in a new benchmark and demonstrate their effectiveness in producing more reliable method comparisons.

## Key Results
- Closed CIs (sampling with replacement) are significantly narrower than open CIs (without replacement), especially for smaller datasets
- Paired tests reduce variance of accuracy differences between methods by eliminating task difficulty effects
- Optimizing Q values can minimize CI ranges, with an optimal Q existing that balances task count and per-task accuracy variance
- The new benchmark demonstrates improved conclusiveness in method comparisons compared to traditional approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sampling tasks with replacement underestimates the true confidence interval range because it ignores data randomness.
- Mechanism: When tasks are sampled with replacement, the same samples can appear in multiple tasks, making the computed CIs only account for sampler randomness, not the randomness inherent in the dataset itself. This leads to narrower CIs than what would be observed if the experiment were repeated with different data from the same underlying distribution.
- Core assumption: The Central Limit Theorem requires independent and identically distributed (IID) random variables for valid CI computation.
- Evidence anchors:
  - [abstract] The predominant method for computing CIs in FSL is based on sampling tasks with replacement, which makes the CI misleading as it accounts for sampler randomness but not data randomness.
  - [section 2.1.2] The paper explains that the usual Lindeberg-LÃ©vy Central Limit Theorem (CLT) is used for CI computation, which requires IID random variables. Since the same samples can appear in multiple tasks, this assumption is violated.
  - [corpus] The corpus contains papers on few-shot learning, confidence intervals, and sampling methods, providing relevant context for the mechanism.

### Mechanism 2
- Claim: Paired tests can reduce the variance of accuracy differences between methods, leading to narrower confidence intervals and more conclusive comparisons.
- Mechanism: In FSL, tasks have a wide range of difficulties, and a task that is hard for one method is often hard for another. By comparing methods on the same set of tasks (paired tests), the variance due to task difficulty is eliminated, reducing the overall variance of the accuracy differences.
- Core assumption: The accuracies of different methods on the same task are positively correlated.
- Evidence anchors:
  - [abstract] The paper proposes using paired tests to partially address the issue of underestimated CIs.
  - [section 3.1] The paper provides a theoretical explanation for why paired tests reduce variance, citing the positive correlation between method accuracies on the same tasks.
  - [corpus] The corpus contains papers on paired tests and their applications in statistical analysis, supporting the validity of this mechanism.

### Mechanism 3
- Claim: Optimizing the number of queries per task (Q) can minimize the variance of the average accuracy, leading to narrower confidence intervals.
- Mechanism: Increasing Q reduces the number of tasks that can be generated, but it also reduces the variance of individual task accuracies. There is a trade-off between these two effects, and an optimal Q exists that minimizes the overall variance of the average accuracy.
- Core assumption: The variance of the average accuracy can be modeled as a function of Q, and this function has a minimum.
- Evidence anchors:
  - [abstract] The paper explores methods to reduce CI size by strategically sampling tasks of a specific size.
  - [section 4] The paper provides a mathematical derivation of the variance of the average accuracy as a function of Q and empirically validates the existence of an optimal Q.
  - [corpus] The corpus contains papers on variance analysis and optimization techniques, providing relevant context for this mechanism.

## Foundational Learning

- Concept: Central Limit Theorem (CLT)
  - Why needed here: The CLT is used to justify the use of confidence intervals in FSL, assuming that the average accuracy over many tasks is approximately normally distributed.
  - Quick check question: What are the key assumptions of the CLT, and how are they violated in the predominant FSL evaluation method?

- Concept: Paired statistical tests
  - Why needed here: Paired tests are proposed as a solution to reduce the variance of accuracy differences between methods, leading to more conclusive comparisons.
  - Quick check question: How do paired tests work, and why are they more powerful than unpaired tests when comparing methods on the same set of tasks?

- Concept: Variance analysis and optimization
  - Why needed here: The paper optimizes the number of queries per task (Q) to minimize the variance of the average accuracy, leading to narrower confidence intervals.
  - Quick check question: How can the variance of a statistic be modeled as a function of a parameter, and how can this function be optimized to minimize the variance?

## Architecture Onboarding

- Component map:
  Task sampler -> Method evaluator -> Confidence interval calculator -> Paired test analyzer -> Task size optimizer

- Critical path:
  1. Sample tasks from the dataset (with or without replacement)
  2. Evaluate each method on each task
  3. Compute the average accuracy and confidence interval for each method
  4. If comparing methods, perform paired tests to compute the confidence interval of their accuracy difference
  5. If optimizing task size, vary the number of queries per task and compute the variance of the average accuracy for each setting

- Design tradeoffs:
  - Sampling with replacement vs. without replacement: Sampling with replacement leads to narrower CIs but ignores data randomness, while sampling without replacement accounts for data randomness but leads to wider CIs
  - Number of tasks vs. number of queries per task: Increasing the number of tasks reduces the variance of the average accuracy, but increasing the number of queries per task also reduces the variance of individual task accuracies. There is a trade-off between these two effects
  - Computational cost vs. statistical power: Using more tasks or more queries per task increases the statistical power of the evaluation but also increases the computational cost

- Failure signatures:
  - CIs that are too narrow: This may indicate that the evaluation method is not accounting for data randomness, leading to overconfident conclusions
  - CIs that are too wide: This may indicate that the evaluation method is not using enough tasks or queries per task, leading to underconfident conclusions
  - Inconsistent conclusions between closed and open CIs: This may indicate that the evaluation method is sensitive to the choice of sampling strategy, and the conclusions may not generalize to different datasets

- First 3 experiments:
  1. Compare the CIs obtained using sampling with replacement vs. without replacement on a small dataset (e.g., DTD). Expect the CIs obtained with replacement to be narrower but potentially misleading
  2. Perform paired tests to compare the accuracies of two methods on the same set of tasks. Expect the CIs of the accuracy differences to be narrower than the CIs of the individual method accuracies
  3. Optimize the number of queries per task to minimize the variance of the average accuracy on a small dataset. Expect the optimal number of queries to be neither too small (leading to high variance) nor too large (leading to too few tasks)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal number of queries (Q*) vary with different types of few-shot learning tasks beyond image classification?
- Basis in paper: [explicit] The paper focuses on image classification tasks and derives Q* for different shot settings (1, 5, 10 shots) using image datasets.
- Why unresolved: The paper's analysis is limited to vision tasks and doesn't explore how Q* might change for other modalities like text or graph data.
- What evidence would resolve it: Experiments applying the same methodology to few-shot learning benchmarks in natural language processing or other domains, measuring variance of accuracy across different Q values.

### Open Question 2
- Question: What is the relationship between dataset size and the discrepancy between closed and open confidence intervals for different few-shot learning methods?
- Basis in paper: [inferred] The paper shows that for smaller datasets like Aircraft and DTD, closed CIs are significantly narrower than open CIs, while for larger datasets like Quickdraw, the trend reverses.
- Why unresolved: The paper provides specific examples but doesn't establish a general relationship or threshold for when the discrepancy becomes significant.
- What evidence would resolve it: A comprehensive study measuring the ratio of closed to open CI widths across a range of dataset sizes for multiple few-shot methods, identifying patterns or thresholds.

### Open Question 3
- Question: How do the findings about confidence intervals and paired tests generalize to meta-learning approaches beyond simple few-shot classification?
- Basis in paper: [explicit] The paper focuses on few-shot classification with fixed feature extractors and adaptation methods, not exploring meta-learning algorithms.
- Why unresolved: Meta-learning methods like MAML or prototypical networks have different training dynamics that might affect how confidence intervals should be computed and interpreted.
- What evidence would resolve it: Applying the same confidence interval analysis to meta-learning benchmarks, comparing closed vs open CIs and evaluating the impact of paired tests on meta-learner comparisons.

## Limitations

- The study relies on synthetic task sampling rather than real-world experimental repetitions, limiting practical validation
- The assumption of positive correlation between method accuracies on the same tasks may not hold for methods with fundamentally different approaches
- The optimal Q optimization strategy may not generalize well across diverse FSL scenarios with varying task difficulty distributions

## Confidence

- High Confidence: The mathematical derivation showing that sampling with replacement underestimates CIs by ignoring data randomness. The mechanism is well-established and the violation of CLT assumptions is clearly demonstrated.
- Medium Confidence: The effectiveness of paired tests in reducing variance. While theoretically sound, the practical benefit depends on the specific correlation structure between method performances.
- Medium Confidence: The Q optimization strategy. The mathematical framework is clear, but real-world applicability may be limited by computational constraints and varying task difficulty distributions.

## Next Checks

1. Implement Algorithm 2 to sample tasks without replacement and compute open CIs on a small dataset like DTD. Compare the width of these CIs against the closed CIs to quantify the practical difference in different scenarios.

2. Design an experiment to test the correlation between method accuracies on the same tasks across multiple method pairs. Verify whether the positive correlation assumption holds for diverse FSL approaches, particularly for methods with different architectures.

3. Conduct a systematic study of Q optimization across different datasets and method types. Identify scenarios where optimizing Q leads to significant CI reduction versus cases where the benefit is minimal due to computational constraints or task difficulty variations.