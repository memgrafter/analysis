---
ver: rpa2
title: Adaptive Conditional Expert Selection Network for Multi-domain Recommendation
arxiv_id: '2411.06826'
source_url: https://arxiv.org/abs/2411.06826
tags:
- experts
- expert
- cesaa
- domains
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CESAA, a novel adaptive conditional expert
  selection network for multi-domain recommendation. The model addresses scalability
  and discriminability issues in mixture-of-experts approaches by combining a Conditional
  Expert Selection (CES) module with sparse gating and domain-shared experts, along
  with an Adaptive Expert Aggregation (AEA) module that uses mutual information loss
  to strengthen expert-domain correlations.
---

# Adaptive Conditional Expert Selection Network for Multi-domain Recommendation

## Quick Facts
- arXiv ID: 2411.06826
- Source URL: https://arxiv.org/abs/2411.06826
- Authors: Kuiyao Dong; Xingyu Lou; Feng Liu; Ruian Wang; Wenyi Yu; Ping Wang; Jun Wang
- Reference count: 17
- Primary result: CESAA achieves up to 1.1% improvement on Recall@10-1 and 0.25% on GAUC metrics compared to state-of-the-art methods

## Executive Summary
This paper introduces CESAA, an adaptive conditional expert selection network for multi-domain recommendation that addresses scalability and discriminability issues in mixture-of-experts approaches. The model combines a Conditional Expert Selection (CES) module with sparse gating and domain-shared experts, along with an Adaptive Expert Aggregation (AEA) module that uses mutual information loss to strengthen expert-domain correlations. By selectively activating domain-shared experts and top-k domain-specific experts for each instance, CESAA improves computational efficiency while maintaining superior recommendation performance.

## Method Summary
CESAA addresses multi-domain recommendation by combining sparse gating with domain-shared experts and mutual information-based specialization. The Conditional Expert Selection (CES) module uses noisy top-k gating to activate only the most relevant domain-specific experts for each input, while a shared expert captures cross-domain commonalities. The Adaptive Expert Aggregation (AEA) module incorporates mutual information loss to strengthen the correlation between experts and specific domains, enabling automatic discovery of optimal expert-domain mappings without pre-defined partitions. The model is trained end-to-end using Adam optimizer with batch size 1024 and learning rate 1e-3 on both industrial retrieval and public ranking datasets.

## Key Results
- CESAA achieves 1.1% improvement on Recall@10-1 metric for industrial dataset
- CESAA demonstrates 0.25% improvement on GAUC metric for public dataset
- Model shows consistent performance gains across all evaluated metrics (ALL, PE, PM, PH) while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse gating with top-k expert selection reduces computational cost while maintaining recommendation performance.
- Mechanism: The model uses noisy top-k gating to activate only the most relevant domain-specific experts for each input instance, combined with a shared expert that captures cross-domain commonalities.
- Core assumption: Not all experts are needed for every recommendation instance; domain-specific and instance-specific relevance can be effectively learned.
- Evidence anchors:
  - [abstract] "only domain-shared experts and selected domain-specific experts are activated for each instance, striking a balance between computational efficiency and model performance"
  - [section 2.3.1] "Sparse MoE utilizes Noisy Top-K Gating, which applies adjustable Gaussian noise and retains only the top-k experts for each input"
  - [corpus] Weak evidence - corpus neighbors discuss similar expert selection but not specifically the top-k gating mechanism

### Mechanism 2
- Claim: Mutual information loss strengthens the correlation between experts and specific domains, improving discriminability.
- Mechanism: The AEA module uses mutual information loss to encourage each expert to specialize in specific domains by increasing the mutual information between expert outputs and domain labels.
- Core assumption: Expert specialization can be learned through optimization of mutual information rather than requiring pre-defined domain partitions.
- Evidence anchors:
  - [abstract] "AEA utilizes mutual information loss to strengthen the correlations between experts and specific domains, and significantly improve the distinction between experts"
  - [section 2.4] "we incorporate mutual information loss into the training procedure to further strengthen the association between experts and domains"
  - [corpus] Weak evidence - corpus neighbors discuss expert specialization but not specifically mutual information-based approaches

### Mechanism 3
- Claim: Combining sparse expert selection with shared experts captures both domain-specific patterns and cross-domain commonalities.
- Mechanism: The CES module architecture includes both domain-specific sparse experts (selected via top-k gating) and a shared expert layer that learns common patterns across all domains.
- Core assumption: Some recommendation patterns are domain-specific while others are shared across domains, and both types of information are valuable for recommendation quality.
- Evidence anchors:
  - [abstract] "CES first combines a sparse gating strategy with domain-shared experts"
  - [section 2.3.2] "we additionally introduce a shared expert layer ùê∏ùë†‚Ñé, so as to capture the commonalities between experts"
  - [corpus] Weak evidence - corpus neighbors discuss shared parameters but not specifically the combination with sparse gating

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: The paper builds on MoE foundations but modifies them for multi-domain recommendation by adding sparsity and mutual information constraints
  - Quick check question: What is the primary advantage of MoE over traditional single-tower architectures in multi-domain scenarios?

- Concept: Mutual Information maximization
  - Why needed here: Used as a training objective to strengthen domain-expert correlations without requiring manual domain partitioning
  - Quick check question: How does maximizing mutual information between experts and domains encourage expert specialization?

- Concept: Sparse gating mechanisms
  - Why needed here: Enables computational efficiency by activating only relevant experts per instance rather than all experts
  - Quick check question: What is the computational complexity difference between full MoE activation and top-k sparse gating?

## Architecture Onboarding

- Component map: Input ‚Üí Embedding ‚Üí AEA (expert selection) ‚Üí CES (expert activation + fusion) ‚Üí Output
- Critical path: Input ‚Üí Embedding ‚Üí AEA (expert selection) ‚Üí CES (expert activation + fusion) ‚Üí Output
- Design tradeoffs:
  - Sparsity vs completeness: Fewer active experts improve efficiency but may miss relevant information
  - Shared vs domain-specific: Shared expert captures commonalities but may dilute domain-specific patterns
  - Mutual information weight (Œ±): Higher weight strengthens specialization but may reduce generalization
- Failure signatures:
  - Low variance in expert activations suggests poor specialization
  - High mutual information loss during training indicates weak domain-expert correlations
  - Performance degradation when k approaches total expert count suggests sparse gating is beneficial
- First 3 experiments:
  1. Compare performance with different k values (1, 2, 3, 4) to find optimal sparsity level
  2. Test model with and without shared expert to quantify cross-domain benefit
  3. Evaluate mutual information loss contribution by comparing with and without AEA module

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Experimental validation is primarily empirical without extensive ablation studies to isolate individual component contributions
- Mutual information loss mechanism lacks detailed analysis of its impact on expert specialization quality
- Scalability claims are based on single industrial dataset results without broader testing across different system scales

## Confidence
- **High confidence**: The sparse gating mechanism with top-k selection and its computational benefits are well-established in the literature
- **Medium confidence**: The combination of shared experts with sparse domain-specific experts shows reasonable improvements
- **Medium confidence**: The mutual information-based specialization approach is innovative but effectiveness depends on hyperparameter tuning

## Next Checks
1. Conduct comprehensive ablation studies isolating the contributions of: (a) sparse gating vs full MoE, (b) shared expert vs only domain-specific experts, and (c) mutual information loss vs no specialization loss
2. Test CESAA on additional multi-domain recommendation datasets with varying domain distributions and sizes to assess generalizability
3. Evaluate the computational efficiency claims by measuring actual training/inference time and memory usage across different k values and expert counts