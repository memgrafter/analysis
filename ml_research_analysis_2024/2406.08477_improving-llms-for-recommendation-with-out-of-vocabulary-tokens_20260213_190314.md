---
ver: rpa2
title: Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens
arxiv_id: '2406.08477'
source_url: https://arxiv.org/abs/2406.08477
tags:
- tokens
- item
- recommendation
- items
- meta
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces META ID, a method that enhances Large Language
  Models (LLMs) for recommendation tasks by constructing user and item IDs using out-of-vocabulary
  (OOV) tokens. The core idea is to learn item representations from historical user-item
  interactions using a meta-path-based skip-gram model, then cluster these representations
  to generate hierarchical OOV tokens that capture both diversity and memorization
  of user-item relationships.
---

# Improving LLMs for Recommendation with Out-Of-Vocabulary Tokens

## Quick Facts
- arXiv ID: 2406.08477
- Source URL: https://arxiv.org/abs/2406.08477
- Authors: Ting-Ji Huang; Jia-Qi Yang; Chunxu Shen; Kai-Qi Liu; De-Chuan Zhan; Han-Jia Ye
- Reference count: 40
- One-line primary result: META ID outperforms state-of-the-art methods across multiple recommendation tasks using out-of-vocabulary tokens

## Executive Summary
This paper introduces META ID, a method that enhances Large Language Models (LLMs) for recommendation tasks by constructing user and item IDs using out-of-vocabulary (OOV) tokens. The core idea is to learn item representations from historical user-item interactions using a meta-path-based skip-gram model, then cluster these representations to generate hierarchical OOV tokens that capture both diversity and memorization of user-item relationships. These OOV tokens are integrated into the LLM's vocabulary and initialized with a linear transformation of cluster centroids for better performance. The method is evaluated across multiple recommendation tasks (sequential, direct, rating prediction, explanation generation, and review summarization) on Amazon datasets (Sports, Beauty, Toys). Results show META ID outperforms state-of-the-art methods, achieving improvements in metrics like Hit Ratio (H@5: 0.0357 vs 0.0269 for CID) and NDCG (N@5: 0.0256 vs 0.0189 for CID) in sequential recommendation.

## Method Summary
META ID constructs user and item IDs using hierarchical OOV tokens generated through clustering meta-path-based embeddings. The framework first samples meta-paths from user-item interaction graphs, then trains a skip-gram model to learn user and item representations in a shared embedding space. These representations are clustered using K-means, with each cluster assigned a unique OOV token. A linear transformation layer, initialized with cluster centroids, transforms OOV token embeddings before they enter the LLM's token embedding layer. The extended vocabulary is then fine-tuned on recommendation tasks, explicitly optimizing for both memorization (capturing user-item relationships) and diversity (ensuring ID distinctiveness).

## Key Results
- META ID achieves 32.3% improvement in Hit Ratio@5 (0.0357 vs 0.0269) for sequential recommendation compared to CID
- The method shows 35.1% improvement in NDCG@5 (0.0256 vs 0.0189) for sequential recommendation tasks
- META ID outperforms state-of-the-art methods across multiple recommendation tasks including direct recommendation, rating prediction, explanation generation, and review summarization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** OOV tokens provide more expressive power for distinctive user/item representation than in-vocabulary tokens.
- **Mechanism:** META ID constructs user and item IDs using hierarchical OOV tokens generated through clustering meta-path-based embeddings. These OOV tokens allow the LLM to distinguish between users and items more effectively than numeric or sequential in-vocabulary tokens.
- **Core assumption:** Clustering similar users/items together and assigning them the same coarse-grained OOV token preserves their semantic similarity while maintaining distinctiveness across clusters.
- **Evidence anchors:**
  - [abstract]: "We emphasize the role of out-of-vocabulary (OOV) tokens in addition to the in-vocabulary ones and claim the memorization of OOV tokens that capture correlations of users/items as well as diversity of OOV tokens."
  - [section]: "Characterizing users and items with numeric IDs is straightforward but also introduces the mapping problem, where it is hard to align limited numeric tokens to thousands of items in recommender systems"
  - [corpus]: Weak. Corpus neighbors focus on OOV handling and attention masking but don't provide direct evidence for clustering effectiveness.
- **Break condition:** If the meta-path sampling doesn't capture meaningful user-item interaction patterns, the clustering will group semantically unrelated users/items together.

### Mechanism 2
- **Claim:** Linear transformation of cluster centroids provides effective initialization for OOV token embeddings.
- **Mechanism:** Instead of random initialization, META ID uses a linear layer initialized with category embeddings (cluster centroids) to transform OOV token embeddings before they enter the LLM's token embedding layer.
- **Core assumption:** Cluster centroids capture meaningful semantic relationships between users/items that can bootstrap the OOV token representations.
- **Evidence anchors:**
  - [section]: "the OOV tokens undergo a linear layer F(·) initialized with the category embeddings µg from Equation 7"
  - [abstract]: "integrating these OOV tokens into the LLM's vocabulary allows for better distinction between users and items"
  - [corpus]: Weak. No direct evidence about linear transformation effectiveness found in corpus.
- **Break condition:** If the cluster centroids are poorly positioned (e.g., from bad K-means initialization), the linear transformation won't provide meaningful semantic structure.

### Mechanism 3
- **Claim:** META ID's dual focus on memorization and diversity metrics leads to better recommendation performance.
- **Mechanism:** The framework explicitly optimizes for both memorization score (MS) - capturing user-item relationships through adjusted cosine similarity - and diversity score (DS) - ensuring ID representations are distinguishable.
- **Core assumption:** There is a positive correlation between MS/DS metrics and actual recommendation performance.
- **Evidence anchors:**
  - [abstract]: "Our proposed framework outperforms existing state-of-the-art methods across various downstream recommendation tasks"
  - [section]: "Figure 4, the sum of the MS and DS positively correlate with NDCG@10, suggesting that memorization and diversity of IDs are two essential properties in recommendation tasks"
  - [corpus]: Weak. Corpus doesn't discuss MS/DS metrics specifically.
- **Break condition:** If the MS/DS metrics don't actually correlate with downstream performance in a new dataset, the optimization target is misaligned.

## Foundational Learning

- **Concept: Meta-path sampling in heterogeneous graphs**
  - Why needed here: META ID relies on meta-paths to capture meaningful user-item interaction patterns that serve as training data for the skip-gram model
  - Quick check question: What would happen if we used random walks instead of meta-path-based sampling? (Answer: We'd lose the structured semantic relationships between users and items that meta-paths capture)

- **Concept: Skip-gram model for node embedding**
  - Why needed here: The skip-gram model learns vector representations from meta-path sequences, projecting users and items into a shared embedding space
  - Quick check question: Why does the paper use a window size of 5 for the skip-gram model? (Answer: To capture local context within meta-paths while maintaining computational efficiency)

- **Concept: K-means clustering for representation grouping**
  - Why needed here: Clustering is used to create hierarchical groups of similar users/items that share OOV tokens
  - Quick check question: What would happen if we used too few clusters (e.g., G=10) instead of the paper's G=100? (Answer: We'd lose granularity in user/item distinctions, hurting recommendation quality)

## Architecture Onboarding

- **Component map:** Meta-path sampler → Skip-gram model → K-means clustering → OOV token generator → Linear transformation layer → LLM token embedding layer → Fine-tuning pipeline

- **Critical path:**
  1. Generate meta-paths from user-item interaction graph
  2. Train skip-gram to learn user/item representations
  3. Cluster representations and generate OOV tokens
  4. Initialize linear transformation with cluster centroids
  5. Extend LLM vocabulary and fine-tune with new OOV tokens

- **Design tradeoffs:**
  - Token granularity vs vocabulary size: More clusters (higher G) increases token distinctiveness but also vocabulary size and computational cost
  - Clustering method: K-means is simple but other methods (DBSCAN, spectral) might capture different relationship structures
  - Representation space: Joint clustering of users and items enables cross-entity relationships but may blur entity-specific patterns

- **Failure signatures:**
  - Poor recommendation performance with high MS but low DS (model memorizes but can't distinguish)
  - Degraded performance on cold-start items (clustering relies on historical interactions)
  - Training instability when extending vocabulary (OOV token embeddings may not integrate well)

- **First 3 experiments:**
  1. Compare K-means vs random clustering on a small dataset to verify that structured clustering matters
  2. Test different cluster counts (G=50, 100, 200) on sequential recommendation to find optimal granularity
  3. Validate that linear transformation initialization outperforms random initialization on recommendation metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of META ID change with varying sizes of OOV tokens across different datasets?
- Basis in paper: [explicit] The paper discusses investigating the impact of different cluster sizes (G) for META ID, indicating that an excessive token scale can introduce noise, reducing performance.
- Why unresolved: The paper mentions the importance of finding an optimal token size but does not provide detailed results on how varying OOV token sizes affect performance across different datasets.
- What evidence would resolve it: Detailed experimental results showing the performance of META ID with varying OOV token sizes on multiple datasets, highlighting the optimal range for different scenarios.

### Open Question 2
- Question: What are the potential limitations of using K-Means clustering for generating OOV tokens in META ID, and how might alternative clustering methods improve results?
- Basis in paper: [explicit] The paper compares K-Means clustering with other methods like DBSCAN and Spectral Clustering, noting that K-Means outperforms others in most cases.
- Why unresolved: While K-Means is shown to be effective, the paper does not explore why it might be limiting or how other clustering methods could potentially offer improvements.
- What evidence would resolve it: Comparative analysis of META ID using different clustering methods, focusing on their impact on diversity and memorization scores, and overall recommendation performance.

### Open Question 3
- Question: How does the initialization approach for OOV tokens influence the effectiveness of META ID in capturing user-item relationships?
- Basis in paper: [explicit] The paper discusses two initialization approaches: random initialization and initializing using cluster centroid embeddings, showing that the latter improves performance.
- Why unresolved: The paper provides initial insights but does not fully explore the nuances of how different initialization strategies affect the model's ability to capture user-item relationships.
- What evidence would resolve it: Experiments comparing various initialization strategies for OOV tokens, measuring their impact on the model's ability to distinguish between users and items and capture their relationships effectively.

### Open Question 4
- Question: What are the implications of META ID's approach for handling cold-start problems in recommender systems?
- Basis in paper: [inferred] The paper mentions that META ID uses a fixed database of users and items, and new items and users without interaction history could be a limitation.
- Why unresolved: The paper acknowledges the cold-start issue but does not provide solutions or explore how META ID might be adapted to handle new users and items effectively.
- What evidence would resolve it: Development and testing of extensions to META ID that address cold-start problems, evaluating their effectiveness in integrating new users and items into the recommendation process.

## Limitations

- The method relies heavily on existing user-item interaction history, making it challenging to handle cold-start items with limited interaction data
- The optimal cluster count (G=100) and other hyperparameters are dataset-specific and may not generalize well to datasets with different characteristics
- Vocabulary explosion could become a problem when scaling to very large item catalogs, potentially impacting computational efficiency

## Confidence

- **High Confidence**: The core hypothesis that OOV tokens provide more expressive power than in-vocabulary numeric IDs for recommendation tasks
- **Medium Confidence**: The specific mechanism of using linear transformation of cluster centroids for OOV token initialization
- **Low Confidence**: The assumption that memorization and diversity scores (MS/DS) directly correlate with recommendation performance across all datasets

## Next Checks

1. **Cluster quality validation**: Implement silhouette score analysis and t-SNE visualization to verify that the K-means clustering actually produces semantically meaningful groups of users and items. Compare against random clustering baselines to quantify the value added by the meta-path-based approach.

2. **Cold-start robustness test**: Design an experiment where a subset of items with limited interactions is masked during training, then measure META ID's performance on these cold-start items compared to baselines. This will validate whether the clustering approach can generalize to items with sparse interaction histories.

3. **Hyperparameter sensitivity analysis**: Systematically vary the skip-gram window size (3, 5, 7) and cluster count (G=50, 100, 200) on a validation set to determine the optimal configuration for different dataset characteristics. This will help establish whether the chosen hyperparameters are truly optimal or just reasonable defaults.