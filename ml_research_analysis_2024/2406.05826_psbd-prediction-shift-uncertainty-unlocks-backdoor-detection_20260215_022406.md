---
ver: rpa2
title: 'PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection'
arxiv_id: '2406.05826'
source_url: https://arxiv.org/abs/2406.05826
tags:
- data
- backdoor
- clean
- training
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses backdoor attacks in deep neural networks (DNNs),
  where malicious samples in training data manipulate model predictions. The proposed
  Prediction Shift Backdoor Detection (PSBD) method leverages model predictive uncertainty,
  specifically the Prediction Shift Uncertainty (PSU), which measures the variance
  in prediction probabilities when dropout layers are toggled during inference.
---

# PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection

## Quick Facts
- arXiv ID: 2406.05826
- Source URL: https://arxiv.org/abs/2406.05826
- Authors: Wei Li; Pin-Yu Chen; Sijia Liu; Ren Wang
- Reference count: 40
- Primary result: PSBD detects backdoor samples by measuring prediction shift uncertainty under dropout, achieving high TPR and low FPR on CIFAR-10 and Tiny ImageNet

## Executive Summary
This paper introduces PSBD, a novel method for detecting backdoor attacks in deep neural networks by leveraging model predictive uncertainty. The approach exploits the observation that backdoor-infected models exhibit different prediction behavior under dropout compared to clean models, specifically showing lower variance in predictions for poisoned samples. By computing the Prediction Shift Uncertainty (PSU) - the variance in prediction probabilities when dropout layers are toggled during inference - PSBD can effectively identify backdoor training samples without requiring prior knowledge of attack patterns.

## Method Summary
PSBD works by first training a model on suspicious data, then computing PSU values for all training samples by measuring the variance in prediction probabilities when dropout is enabled during inference. The method uses an adaptive strategy to select an optimal dropout rate where clean validation data exhibits high prediction shift while the difference between training and validation shift ratios is maximized. Backdoor samples are identified as those with PSU values below a threshold determined by the 25th percentile of clean validation data PSU distribution.

## Key Results
- Achieves high true positive rates (TPR) and low false positive rates (FPR) on CIFAR-10 and Tiny ImageNet datasets
- Outperforms existing backdoor detection methods across various attack scenarios
- Requires minimal clean validation data (5% of training data) for effective detection
- Demonstrates robustness to different attack strategies including BadNets and other trigger-based attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neuron bias effect causes poisoned models to develop robust internal pathways from backdoor triggers to target classes, making backdoor predictions more stable under dropout.
- Mechanism: During training with backdoor samples, the model forms strong neuron associations between trigger features and target class labels. When dropout is applied during inference, clean data predictions shift due to reduced feature prominence, while backdoor data predictions remain stable because the trigger-to-target pathway is robust.
- Core assumption: The backdoor trigger creates a strong, consistent signal that the model prioritizes over natural image features when dropout is applied.
- Evidence anchors:
  - [abstract] "We hypothesize PS results from neuron bias effect, making neurons favor features of certain classes"
  - [section] "We posit that the PS phenomenon arises due to the 'neuron bias' effect in the network during training with data, where neurons become predisposed towards features highly representative of the certain class implicitly"
  - [corpus] No direct corpus evidence for neuron bias effect specifically

### Mechanism 2
- Claim: Prediction Shift Uncertainty (PSU) effectively quantifies the difference between clean and backdoor data by measuring prediction confidence changes under dropout.
- Mechanism: PSU computes the difference between prediction confidence without dropout and average confidence across multiple dropout-enabled inferences. Clean data shows higher variance in predictions when dropout is applied, while backdoor data remains more consistent.
- Core assumption: The variance in prediction probabilities when toggling dropout on/off is significantly different between clean and backdoor samples.
- Evidence anchors:
  - [abstract] "PSBD identifies backdoor training samples by computing the Prediction Shift Uncertainty (PSU), the variance in probability values when dropout layers are toggled on and off during model inference"
  - [section] "PSU computes the difference between the predicted class confidence without dropout and the average predicted class confidence across k dropout inferences"
  - [corpus] No direct corpus evidence for PSU effectiveness specifically

### Mechanism 3
- Claim: The adaptive dropout rate selection strategy enables effective backdoor detection without requiring prior knowledge of attack specifics.
- Mechanism: The method selects a dropout rate where clean validation data exhibits high shift ratio while the difference between training data and validation data shift ratios is maximized, ensuring optimal separation between clean and backdoor samples.
- Core assumption: There exists a dropout rate that simultaneously maximizes prediction shift for clean data while minimizing it for backdoor data, allowing effective discrimination.
- Evidence anchors:
  - [section] "we propose an adaptive selection strategy for p. Specifically, we identify the p where the σ of clean validation data approach to a high value (0.8 in our experiments), while the difference between the σ of the entire training data and that of the clean validation data reaches its maximum"
  - [corpus] No direct corpus evidence for adaptive selection strategy specifically

## Foundational Learning

- Concept: Monte Carlo Dropout (MC-Dropout)
  - Why needed here: MC-Dropout provides the framework for measuring model uncertainty by enabling dropout during inference, which is essential for computing PSU and detecting prediction shifts.
  - Quick check question: What is the primary purpose of using MC-Dropout in the PSBD method?

- Concept: Neuron Bias Effect
  - Why needed here: Understanding how backdoor triggers create robust internal pathways in neural networks is crucial for grasping why PSU can effectively distinguish clean from poisoned data.
  - Quick check question: How does the neuron bias effect explain the stability of backdoor predictions under dropout conditions?

- Concept: Prediction Shift Phenomenon
  - Why needed here: The PS phenomenon is the foundational observation that motivates the entire PSU approach, explaining why clean and backdoor data behave differently under dropout.
  - Quick check question: What is the key difference in prediction behavior between clean and backdoor data that the PS phenomenon reveals?

## Architecture Onboarding

- Component map: Model training -> Dropout rate selection -> PSU calculation -> Threshold classification
- Critical path: Train model on suspicious data -> Select optimal dropout rate -> Calculate PSU for all samples -> Classify using threshold
- Design tradeoffs: Trades computational overhead from multiple dropout-enabled inferences against improved detection accuracy and robustness compared to input-level methods
- Failure signatures: High FPR indicates poor dropout rate selection or insufficient clean validation data; low TPR suggests the backdoor attack successfully minimized prediction shift differences
- First 3 experiments:
  1. Verify PSU calculation produces consistent results across multiple dropout inference runs
  2. Test adaptive dropout rate selection on a dataset with known backdoor samples
  3. Evaluate detection performance against a simple BadNets attack to establish baseline effectiveness

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but based on the content, potential areas for future research include extending PSBD to non-image modalities like text or audio, evaluating performance on larger and more complex datasets, and investigating the method's behavior on different model architectures beyond ResNet-18.

## Limitations

- Effectiveness depends on the presence of a sufficiently strong neuron bias effect in poisoned models
- Requires adequate clean validation data (5% of training set) for adaptive dropout rate selection
- Performance on more complex architectures like transformers or real-world production models remains unverified

## Confidence

- High confidence in the PSU measurement framework and its theoretical foundation in MC-Dropout uncertainty quantification
- Medium confidence in the neuron bias effect as the primary mechanism for backdoor detection
- Medium confidence in the adaptive dropout rate selection strategy
- High confidence in experimental results on CIFAR-10 and Tiny ImageNet, but low confidence in generalizability to other domains

## Next Checks

1. Test PSBD on transformer-based architectures and vision transformers to assess cross-architecture robustness
2. Evaluate performance with varying proportions of clean validation data (1%, 10%, 20%) to determine minimum requirements
3. Implement controlled experiments to isolate the neuron bias effect by comparing PSU distributions in models trained with different trigger strengths