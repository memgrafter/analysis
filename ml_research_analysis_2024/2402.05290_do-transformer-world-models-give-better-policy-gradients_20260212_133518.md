---
ver: rpa2
title: Do Transformer World Models Give Better Policy Gradients?
arxiv_id: '2402.05290'
source_url: https://arxiv.org/abs/2402.05290
tags:
- policy
- gradient
- world
- transformer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether transformer world models improve
  policy gradients in reinforcement learning. It shows that commonly used transformer
  history world models create circuitous gradient paths, which can harm long-range
  policy gradients.
---

# Do Transformer World Models Give Better Policy Gradients?

## Quick Facts
- arXiv ID: 2402.05290
- Source URL: https://arxiv.org/abs/2402.05290
- Authors: Michel Ma; Tianwei Ni; Clement Gehring; Pierluca D'Oro; Pierre-Luc Bacon
- Reference count: 40
- Key outcome: Transformer Actions World Models (AWMs) provide better policy gradients than History World Models (HWMs) by avoiding circuitous gradient paths

## Executive Summary
This paper investigates whether transformer world models improve policy gradients in reinforcement learning. The authors discover that commonly used transformer History World Models (HWMs) create circuitous gradient paths that can harm long-range policy gradients. To address this, they propose Actions World Models (AWMs), which are conditioned only on actions and avoid these circuitous paths. Experiments demonstrate that AWMs outperform competitive baselines in realistic long-horizon tasks, achieving higher returns. The key finding is that transformer AWMs provide better policy gradients than other world models, enabling successful policy optimization in challenging environments.

## Method Summary
The method involves training Actions World Models (AWMs) that map initial states and action sequences directly to future states, rather than predicting intermediate states autoregressively. The AWMs are integrated into policy gradient frameworks by backpropagating through unrolled models to optimize policies. The approach alternates between collecting experience, training AWMs offline on state-action-next state tuples using mean squared error loss, and optimizing policies using backpropagation-based policy optimization (BPO) with Adam optimizer. Experiments compare AWMs against baselines like Markovian models, HWMs, model-free methods, and Online Decision Transformers across various tasks from the Myriad testbed.

## Key Results
- Transformer AWMs outperform HWMs and other baselines in long-horizon tasks
- AWMs achieve higher returns in realistic environments compared to competitive methods
- The gradient paths in AWMs are polynomial (O(H^3)) rather than exponential, enabling better long-term credit assignment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformer AWMs avoid circuitous gradient paths by conditioning only on actions, not states, preventing gradient accumulation through autoregressively predicted states.
- **Mechanism:** In HWMs, gradients flow through each predicted state (ˆs1:t), creating paths that grow with horizon length. In AWMs, the world model directly maps actions to states (ˆst+1 = g(s1, a1:t)), so the longest gradient path is determined solely by the internal architecture of g, not the task horizon.
- **Core assumption:** The AWM architecture can internally learn to map actions to future states without explicitly modeling intermediate states, and this internal mapping is sufficient for policy optimization.
- **Evidence anchors:** [abstract], [section 4], [corpus]
- **Break condition:** If the AWM cannot accurately learn the action-to-state mapping from data, or if intermediate states are actually needed for accurate predictions in certain environments.

### Mechanism 2
- **Claim:** Transformer AWMs inherit the favorable gradient propagation properties of transformer architectures, leading to polynomial rather than exponential gradient growth with horizon length.
- **Mechanism:** Transformers use attention mechanisms that allow gradients to flow directly from any output to any input within the context window, avoiding the sequential dependencies that cause exponential gradient growth in RNNs. When used as AWMs, this property translates to polynomial gradient growth (O(H^3)) rather than exponential.
- **Core assumption:** The attention mechanism in transformers provides stable gradient propagation properties that are preserved when the transformer is used as an AWM.
- **Evidence anchors:** [abstract], [section 4.2], [corpus]
- **Break condition:** If the attention mechanism fails to capture necessary dependencies for accurate state prediction, or if the transformer architecture is modified in ways that compromise gradient stability.

### Mechanism 3
- **Claim:** AWMs can bypass non-differentiable points in the state space by learning direct action-to-state mappings, enabling successful policy optimization where state-based models fail.
- **Mechanism:** In environments with non-differentiable dynamics (e.g., contact points), state-based models must explicitly predict through these points, causing catastrophic errors. AWMs can learn to map actions directly to post-contact states, effectively skipping the non-differentiable region.
- **Core assumption:** The reward dependency on actions remains smooth even when the state dynamics are non-differentiable, allowing AWMs to learn useful mappings.
- **Evidence anchors:** [section 5.1], [corpus]
- **Break condition:** If the reward dependency on actions is also non-differentiable, or if intermediate states contain crucial information for accurate predictions.

## Foundational Learning

- **Concept:** Policy gradient methods and backpropagation through time
  - **Why needed here:** Understanding how policy gradients are computed through unrolled world models is fundamental to grasping why AWMs improve gradient quality.
  - **Quick check question:** What is the difference between computing policy gradients through a Markovian model versus a History World Model?

- **Concept:** Transformer attention mechanisms and gradient propagation
  - **Why needed here:** The key insight is that transformers provide stable gradient propagation, but only when used appropriately (as AWMs rather than HWMs).
  - **Quick check question:** Why do transformer HWMs not provide the same gradient benefits as transformer AWMs?

- **Concept:** Credit assignment in long-horizon reinforcement learning
  - **Why needed here:** The core problem AWMs solve is the difficulty of assigning credit to actions taken many steps before a reward is received.
  - **Quick check question:** How does the length of gradient paths affect the ability to perform long-term credit assignment?

## Architecture Onboarding

- **Component map:** Environment -> Policy network (πθ) -> Actions World Model (gψ) -> Reward calculation -> Policy gradient computation -> Policy update
- **Critical path:** Action → AWM prediction → Reward calculation → Policy gradient computation → Policy update
- **Design tradeoffs:**
  - Action-only conditioning vs. state conditioning: Simpler gradient paths vs. potentially more accurate predictions
  - Transformer architecture vs. RNN: Better gradient properties vs. potentially higher computational cost
  - Model accuracy vs. gradient quality: Sometimes less accurate models provide better gradients for optimization
- **Failure signatures:**
  - Poor policy performance despite accurate AWM predictions
  - Gradient explosion or vanishing during policy updates
  - AWM fails to learn meaningful action-to-state mappings
- **First 3 experiments:**
  1. Implement a simple AWM (e.g., linear or small MLP) on a toy environment with known dynamics to verify gradient flow properties
  2. Compare policy gradients and performance between AWM and Markovian model on a chaotic environment (e.g., double pendulum)
  3. Test AWM on an environment with non-differentiable points (e.g., one-bounce environment) to demonstrate bypassing capability

## Open Questions the Paper Calls Out

- **Open Question 1:** How do Actions World Models (AWMs) perform in high-dimensional state spaces compared to low-dimensional ones?
  - **Basis in paper:** [explicit] The paper mentions that experiments are conducted in low-dimensional yet realistic domains, but does not explore high-dimensional state spaces.
  - **Why unresolved:** The paper focuses on low-dimensional environments, leaving the performance of AWMs in high-dimensional spaces untested.
  - **What evidence would resolve it:** Conducting experiments with AWMs in high-dimensional state spaces, such as complex robotics tasks, would provide insights into their scalability and effectiveness.

- **Open Question 2:** Can Actions World Models (AWMs) be effectively integrated with other advanced sequence models like state space models (SSMs)?
  - **Basis in paper:** [inferred] The paper suggests that AWMs are agnostic to the specific neural network architecture, allowing any modern sequence models to produce favorable policy gradients.
  - **Why unresolved:** While the paper demonstrates the effectiveness of AWMs with RNNs and transformers, it does not explore integration with other advanced models like SSMs.
  - **What evidence would resolve it:** Implementing AWMs with SSMs and comparing their performance in various tasks would determine the potential benefits of such integration.

- **Open Question 3:** How do Actions World Models (AWMs) handle stochastic environments compared to deterministic ones?
  - **Basis in paper:** [explicit] The paper primarily focuses on deterministic MDPs and does not address the performance of AWMs in stochastic environments.
  - **Why unresolved:** The theoretical framework and experiments are developed for deterministic settings, leaving the behavior of AWMs in stochastic environments unexplored.
  - **What evidence would resolve it:** Testing AWMs in stochastic environments and comparing their performance with state-based world models would reveal their robustness and adaptability.

## Limitations

- Limited exploration of high-dimensional state spaces where AWMs might face scalability challenges
- Focus on deterministic environments without addressing stochastic transition dynamics
- Empirical validation primarily on Myriad testbed tasks, potentially limiting generalizability

## Confidence

- Theoretical claims about AWM gradient properties: Medium
- Claim that AWMs solve non-differentiable dynamics: Medium
- Empirical validation across diverse environments: Medium

## Next Checks

1. Test AWMs on environments with stochastic transitions to verify gradient stability under uncertainty
2. Compare AWM performance against specialized gradient stabilization techniques like K-FAC or natural gradients
3. Evaluate whether the polynomial gradient bounds (O(H³)) actually prevent optimization failure in practice compared to exponential growth scenarios