---
ver: rpa2
title: 'TrAct: Making First-layer Pre-Activations Trainable'
arxiv_id: '2410.23970'
source_url: https://arxiv.org/abs/2410.23970
tags:
- tract
- training
- epochs
- adam
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TrAct addresses the inefficiency in first-layer training of vision
  models, where gradient updates are directly proportional to input pixel values,
  causing low-contrast images to have less impact on learning than high-contrast ones.
  The core idea is to conceptually perform gradient descent on first-layer activations
  by computing optimal weight updates that minimize the squared distance between current
  activations and proposed updates, using a closed-form solution involving the input
  covariance matrix.
---

# TrAct: Making First-layer Pre-Activations Trainable

## Quick Facts
- arXiv ID: 2410.23970
- Source URL: https://arxiv.org/abs/2410.23970
- Authors: Felix Petersen; Christian Borgelt; Stefano Ermon
- Reference count: 40
- Primary result: TrAct speeds up training by factors of 1.25× to 4× across diverse models and datasets

## Executive Summary
TrAct addresses a fundamental inefficiency in first-layer training of vision models where gradient updates are directly proportional to input pixel values, causing low-contrast images to have less impact on learning than high-contrast ones. The method conceptually performs gradient descent on first-layer activations by computing optimal weight updates that minimize the squared distance between current activations and proposed updates, using a closed-form solution involving the input covariance matrix. This lightweight modification to the backward pass of the first layer requires only one hyperparameter (λ) and adds minimal computational overhead.

## Method Summary
TrAct modifies the backward pass of the first convolutional or linear layer in vision models to address the inefficiency where gradient updates scale directly with input pixel values. Instead of traditional gradient descent, TrAct computes optimal weight updates that minimize the squared distance between current activations and proposed updates, using the input covariance matrix in a closed-form solution. The method is implemented as a minimal change to existing training pipelines, requiring only one hyperparameter (λ) to control the trade-off between stability and learning speed. This approach conceptually performs gradient descent on first-layer activations rather than raw pixel gradients, making learning more balanced across different input contrast levels.

## Key Results
- Consistent training speedups of 1.25× to 4× across multiple architectures (ResNets, ViTs)
- Achieved equal or better accuracy with fewer epochs on CIFAR-10/100 and ImageNet
- Minimal computational overhead added to existing training pipelines

## Why This Works (Mechanism)
Traditional first-layer training suffers from a fundamental scaling problem: gradient updates are proportional to input pixel values, meaning high-contrast images dominate learning while low-contrast images contribute less. This creates an inefficient learning dynamic where the network spends excessive time on high-contrast samples while underutilizing potentially valuable low-contrast information.

TrAct addresses this by conceptually performing gradient descent on first-layer activations rather than raw pixel gradients. By computing optimal weight updates that minimize the squared distance between current activations and proposed updates using the input covariance matrix, the method normalizes the learning signal across different input contrasts. This effectively decouples the learning rate from the absolute pixel values, allowing more balanced and efficient updates regardless of image contrast.

## Foundational Learning

**Input covariance matrix**: Captures relationships between input features; needed to normalize learning across different input patterns; quick check: verify positive semi-definiteness for stable inversion

**Closed-form optimization**: Provides exact solution for weight updates rather than iterative gradient descent; needed for computational efficiency; quick check: ensure solution satisfies Karush-Kuhn-Tucker conditions

**Pre-activation gradients**: Gradients with respect to layer inputs rather than outputs; needed to understand how input scaling affects learning; quick check: verify gradient flow matches expected theoretical behavior

## Architecture Onboarding

Component map: Input -> First layer (Conv/Linear) -> TrAct modified backward pass -> Rest of network

Critical path: The first layer's backward pass is the critical modification point. TrAct intercepts the standard gradient computation and replaces it with its closed-form solution using the input covariance matrix.

Design tradeoffs: The method trades slight computational overhead for significant training speedups. The single hyperparameter λ provides a balance between stability and learning speed, but requires tuning.

Failure signatures: Poor performance on extremely low-contrast images, instability when covariance matrix is ill-conditioned, or failure to converge when λ is improperly set.

First experiments:
1. Compare training curves with and without TrAct on CIFAR-10 using a simple ResNet
2. Test TrAct's sensitivity to λ across different model sizes
3. Evaluate performance on high-noise vs low-noise image subsets

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical justification relies on assumptions about input distributions that may not hold in practice
- Effectiveness depends on invertibility of covariance matrix, which could be problematic with certain data augmentations
- Performance on extremely low-contrast or high-noise images requires further validation

## Confidence

High confidence: Consistent empirical speedups across multiple architectures and datasets; sound computational overhead analysis

Medium confidence: Equal or better accuracy claims supported by experiments, but evaluation across diverse model sizes could be more extensive

Low confidence: Theoretical motivation relies on simplifying assumptions about input distributions; empirical evidence rather than theoretical guarantees for addressing fundamental inefficiency

## Next Checks

1. Test TrAct's performance on extremely low-contrast or high-noise images to validate effectiveness in challenging scenarios

2. Evaluate method's behavior with various data augmentation strategies to assess robustness to non-standard input distributions

3. Conduct ablation studies to quantify λ hyperparameter impact on training speed and accuracy across different architectures and dataset complexities