---
ver: rpa2
title: Stochastic Optimal Control for Diffusion Bridges in Function Spaces
arxiv_id: '2405.20630'
source_url: https://arxiv.org/abs/2405.20630
tags:
- function
- control
- diffusion
- where
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends stochastic optimal control (SOC) theory to infinite-dimensional
  Hilbert spaces, addressing the challenge of applying diffusion-based generative
  models to function spaces. The key idea is to derive Doob's h-transform in Hilbert
  spaces using SOC, enabling the construction of diffusion bridges between distributions
  defined in function spaces.
---

# Stochastic Optimal Control for Diffusion Bridges in Function Spaces

## Quick Facts
- arXiv ID: 2405.20630
- Source URL: https://arxiv.org/abs/2405.20630
- Reference count: 40
- This paper extends stochastic optimal control theory to infinite-dimensional Hilbert spaces, enabling diffusion bridges between function-space distributions.

## Executive Summary
This work bridges stochastic optimal control theory with infinite-dimensional Hilbert spaces, enabling diffusion bridges between distributions defined in function spaces. The authors derive Doob's h-transform using SOC principles and implement it through Radon-Nikodym derivatives relative to Gaussian reference measures. This allows explicit computation of diffusion bridges even in infinite dimensions, with applications to resolution-free image translation and Bayesian inference for stochastic processes.

## Method Summary
The approach extends Doob's h-transform to Hilbert spaces using stochastic optimal control, deriving the conditional density h(t,x) through a Radon-Nikodym derivative relative to a Gaussian reference measure. The method employs neural network parameterization of the optimal control function, optimized via specific loss functions (LBM and LBayes), enabling practical implementation of infinite-dimensional diffusion bridges. The framework operates through a connection between the Kolmogorov backward equation and the Hamilton-Jacobi-Bellman equation, making it applicable to various function space problems including image generation and time-series imputation.

## Key Results
- Demonstrates resolution-free image translation between dog and cat images, generating high-quality images at arbitrary resolutions
- Achieves state-of-the-art performance in time-series imputation tasks on Physionet dataset
- Successfully applies the method to Bayesian inference for Gaussian processes, enabling functional regression with uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Hopf-Cole transform links the Kolmogorov backward equation to the Hamilton-Jacobi-Bellman equation, enabling optimal control formulations for diffusion bridges.
- Mechanism: By setting V(t,x) = -log h(t,x), the linear PDE governing the conditional density h becomes the HJB equation with running cost R(α) = 1/2 ||α||². This transforms the problem of finding a diffusion bridge into an optimal control problem with explicit optimal control α* = -σQ^(1/2) DxV.
- Core assumption: The function h satisfies the Kolmogorov backward equation and is smooth enough for the Hopf-Cole transformation.
- Evidence anchors:
  - [abstract] "we demonstrate how Doob's h-transform, the fundamental tool for constructing diffusion bridges, can be derived from the SOC perspective and expanded to infinite dimensions"
  - [section] "Let Vt = -log ht. Then Vt satisfies the HJB equation: ∂tVt + LVt - 1/2 ||σQ^(1/2)DxVt||²_H = 0, V(T,x) = G(x)"
  - [corpus] Weak - no direct evidence in corpus papers about Hopf-Cole transform
- Break condition: If h does not satisfy the Kolmogorov backward equation or lacks required smoothness, the transformation fails.

### Mechanism 2
- Claim: The Radon-Nikodym derivative relative to a Gaussian reference measure provides explicit densities in infinite dimensions, enabling diffusion bridge construction.
- Mechanism: By establishing that NetAx,Qt and N0,Q∞ are equivalent measures, we can compute the Radon-Nikodym derivative qt(x,z) explicitly. This allows us to define h(t,x) = ∫G(z)qt(x,z)N0,Q∞(dz) even in infinite dimensions.
- Core assumption: The Gaussian measure N0,Q∞ serves as a suitable reference measure and the equivalent measure relationship holds.
- Evidence anchors:
  - [abstract] "we propose a Radon-Nikodym derivative relative to a specified Gaussian reference measure in Hilbert space"
  - [section] "Theorem 2.3 (Explicit Representation of h). For any t > 0 and any x ∈ H, the measure NetAx,Qt and N0,Q∞ are equivalent"
  - [corpus] Weak - corpus papers mention infinite-dimensional spaces but don't discuss Radon-Nikodym derivatives
- Break condition: If the equivalent measure relationship fails or the reference measure is inappropriate, explicit density computation becomes impossible.

### Mechanism 3
- Claim: Neural network parameterization of the control function enables practical approximation of infinite-dimensional diffusion bridges.
- Mechanism: By parameterizing α(t,x;θ) with neural networks and optimizing the loss function LBM(θ) or LBayes(θ), we can approximate the optimal control without computing h explicitly. The Girsanov theorem provides the Radon-Nikodym derivative for divergence estimation.
- Core assumption: Neural networks can approximate the optimal control function well enough for practical purposes.
- Evidence anchors:
  - [abstract] "We propose two applications: 1) learning bridges between two infinite-dimensional distributions and 2) generative models for sampling from an infinite-dimensional distribution"
  - [section] "Since the function h is intractable for a general terminal cost ˜G in (12), simulating the conditioned SDEs in (9) requires some approximation techniques"
  - [corpus] Strong - multiple corpus papers discuss neural network approaches to infinite-dimensional problems
- Break condition: If the neural network approximation is poor or the optimization landscape is problematic, practical implementation fails.

## Foundational Learning

- Concept: Cameron-Martin space and Q-Wiener processes
  - Why needed here: The paper operates in infinite-dimensional Hilbert spaces where standard finite-dimensional stochastic calculus doesn't apply. Understanding Cameron-Martin space H0 and Q-Wiener processes in larger space H1 is crucial for defining appropriate stochastic differential equations.
  - Quick check question: What is the relationship between the Cameron-Martin space H0 and the larger space H1 in which the Q-Wiener process is defined?

- Concept: Fréchet derivatives in Hilbert spaces
  - Why needed here: The paper requires taking derivatives of functionals defined on infinite-dimensional function spaces. Fréchet derivatives generalize gradients to Hilbert spaces and are essential for deriving the HJB equation and implementing the control algorithms.
  - Quick check question: How does the Fréchet derivative DxV differ from the standard gradient in finite dimensions?

- Concept: Radon-Nikodym derivatives and equivalent measures
  - Why needed here: The absence of Lebesgue measure in infinite dimensions necessitates using Radon-Nikodym derivatives relative to Gaussian reference measures. Understanding when measures are equivalent is crucial for the explicit h function computation.
  - Quick check question: Why can't we use the standard Lebesgue measure in infinite-dimensional spaces, and what properties must a reference measure have?

## Architecture Onboarding

- Component map:
  - Stochastic Optimal Control (SOC) layer -> Hilbert Space Operations -> Measure Theory Module -> Neural Network Controller -> Loss Computation Engine -> Simulation Core
- Critical path: Control parameterization → Loss computation → Gradient optimization → Bridge sampling
- Design tradeoffs:
  - Resolution vs. parameterization: Higher resolution requires more basis functions but maintains resolution-independence
  - Approximation quality vs. computational cost: Better neural network architectures improve approximation but increase training time
  - Gaussian reference measure choice: Different reference measures affect numerical stability and computation efficiency
- Failure signatures:
  - Poor bridge quality: Indicates inadequate neural network approximation or optimization issues
  - Numerical instability: Often caused by ill-conditioned Radon-Nikodym derivative computations
  - Training divergence: May indicate inappropriate loss function scaling or learning rate issues
- First 3 experiments:
  1. Simple Gaussian bridge: Test the basic bridge matching algorithm between two simple Gaussian distributions in 1D function space
  2. Resolution transfer: Validate the resolution-independent property by training on low resolution and testing on higher resolution images
  3. Functional regression: Test the Bayesian learning algorithm on synthetic Gaussian process data to verify posterior sampling quality

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on specific Gaussian reference measures may not generalize to all infinite-dimensional distributions
- Practical implementation depends heavily on neural network approximation quality, which may struggle with very complex function spaces
- Computational complexity of infinite-dimensional operations could limit scalability for extremely high-resolution applications

## Confidence

- **High Confidence**: The theoretical foundation linking SOC to Doob's h-transform in Hilbert spaces, supported by rigorous mathematical derivations in the paper and strong alignment with established stochastic calculus principles.
- **Medium Confidence**: The practical effectiveness of neural network parameterization for infinite-dimensional control functions, based on demonstrated results but with potential variability depending on architecture choices.
- **Medium Confidence**: The resolution-free property of the approach, validated through experiments but requiring further testing across diverse function space applications.

## Next Checks

1. **Generalization Test**: Apply the framework to non-Gaussian reference measures and non-standard infinite-dimensional distributions to verify the robustness of the equivalent measure assumption.

2. **Scalability Analysis**: Conduct systematic experiments varying the dimensionality and complexity of the function space to identify practical limits of the approach.

3. **Architecture Sensitivity**: Perform ablation studies on neural network architectures and hyperparameters to quantify their impact on bridge quality and training stability.