---
ver: rpa2
title: 'Mixed Sparsity Training: Achieving 4$\times$ FLOP Reduction for Transformer
  Pretraining'
arxiv_id: '2408.11746'
source_url: https://arxiv.org/abs/2408.11746
tags:
- training
- sparse
- sparsity
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Mixed Sparsity Training (MST), a method
  for efficient pretraining of large language models (LLMs) by reducing computational
  overhead. MST integrates dynamic sparse training (DST) with Sparsity Variation (SV)
  and Hybrid Sparse Attention (HSA) across three phases: warm-up, ultra-sparsification,
  and restoration.'
---

# Mixed Sparsity Training: Achieving 4$\times$ FLOP Reduction for Transformer Pretraining

## Quick Facts
- arXiv ID: 2408.11746
- Source URL: https://arxiv.org/abs/2408.11746
- Authors: Pihe Hu; Shaolong Li; Longbo Huang
- Reference count: 40
- Primary result: 4× reduction in pretraining FLOPs without compromising performance

## Executive Summary
This paper introduces Mixed Sparsity Training (MST), a method for efficient pretraining of large language models (LLMs) by reducing computational overhead. MST integrates dynamic sparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention (HSA) across three phases: warm-up, ultra-sparsification, and restoration. During these phases, MST dynamically adjusts the model's sparse topology and employs HSA to minimize FLOPs while maintaining performance. Experiments on GPT-2 demonstrate a 4× reduction in pretraining FLOPs without compromising performance across various zero-shot and few-shot tasks. MST is orthogonal and can integrate with existing system-level acceleration methods, offering a scalable approach to efficient transformer pretraining.

## Method Summary
Mixed Sparsity Training (MST) is a novel method for efficient transformer pretraining that integrates dynamic sparse training (DST) with Sparsity Variation (SV) and Hybrid Sparse Attention (HSA). MST operates in three phases: warm-up, ultra-sparsification, and restoration. During warm-up, the dense model is progressively transformed into a sparse topology. Ultra-sparsification employs Mixed-Growing (MG) for dynamic sparse training with high sparsity. The restoration phase reintroduces connections to recover performance. HSA optimizes attention operations by using sparse attention masks during initial phases and transitioning to dense masks later. This approach reduces pretraining FLOPs by 4× while maintaining performance across various zero-shot and few-shot tasks.

## Key Results
- 4× reduction in pretraining FLOPs for GPT-2 models
- Maintained performance parity with dense models across LAMBADA, WikiText2, PTB, WikiText103, 1BW, RTE, and MRPC tasks
- Orthogonal approach compatible with existing system-level acceleration methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixed Sparsity Training (MST) reduces pretraining FLOPs by 4× while maintaining model performance through dynamic sparse training combined with Sparsity Variation (SV) and Hybrid Sparse Attention (HSA).
- Mechanism: MST operates in three phases: warm-up, ultra-sparsification, and restoration. During warm-up, the dense model is progressively transformed into a sparse topology. Ultra-sparsification employs Mixed-Growing (MG) for dynamic sparse training with high sparsity. Restoration phase reintroduces connections to recover performance. HSA optimizes attention operations by using sparse attention masks during initial phases and transitioning to dense masks later.
- Core assumption: Transformers exhibit significant redundancy in pretraining computations that can be exploited through dynamic sparsity without degrading performance.
- Evidence anchors:
  - [abstract] "transformers exhibit considerable redundancy in pretraining computations"
  - [section 3.1] "The warm-up phase transforms the dense model into an initial sparse topology"
  - [corpus] "EcoSpa: Efficient Transformer Training with Coupled Sparsity" suggests sparse training methods exist but doesn't directly validate MST's specific approach
- Break condition: If the model fails to recover performance during the restoration phase or if the sparsity levels are too aggressive, the method will not maintain dense-comparable performance.

### Mechanism 2
- Claim: Mixed-Growing (MG) topology evolution scheme enables effective sparse training of transformers by combining gradient magnitude-based growth with random activation.
- Mechanism: MG prunes connections based on weight magnitude and grows new connections using a hybrid strategy that combines gradient magnitude activation (proportion 1-R) and random activation (proportion R). This approach broadens exploration of sparse topologies and reduces convergence to sub-optimal structures.
- Core assumption: Transformers with massive parameters tend to have more homogeneous parameter magnitudes, making gradient-based growth less effective, necessitating random exploration.
- Evidence anchors:
  - [section 3.2] "MG introduces flexibility by allowing a disparity between the number of pruned and grown links"
  - [section 4.3] "The larger model exhibits a smaller standard deviation in parameter magnitudes"
  - [corpus] "Sparse Attention Post-Training for Mechanistic Interpretability" shows sparse attention methods exist but doesn't validate MST's specific MG approach
- Break condition: If the random activation proportion R is too high, the method may explore too broadly and fail to converge; if too low, it may converge to sub-optimal structures.

### Mechanism 3
- Claim: Hybrid Sparse Attention (HSA) efficiently reduces attention FLOPs while maintaining performance by using strided sparse attention during early training and transitioning to dense attention later.
- Mechanism: HSA employs unfactorized sparse strided self-attention where each output vector attends to previous l locations and every l-th location. During training, HSA uses this sparse pattern initially and transitions to dense attention masks as the model becomes denser.
- Core assumption: Strided sparse attention patterns can maintain model performance while significantly reducing FLOPs compared to full attention.
- Evidence anchors:
  - [section 3.3] "HSA employs an unfactorized strided sparse attention mask to conserve attention FLOPs"
  - [section 4.2.3] "Fixed-512 and Strided-256 demonstrate comparable performance to the dense mask"
  - [corpus] "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers" shows sparse attention methods but doesn't validate MST's specific HSA approach
- Break condition: If the stride length is too large, the model may lose important attention information; if too small, the FLOP savings will be insufficient.

## Foundational Learning

- Concept: Dynamic Sparse Training (DST)
  - Why needed here: DST allows training of sparse networks from scratch while dynamically adjusting the sparse mask during training, enabling FLOP reduction without post-training pruning.
  - Quick check question: What is the key difference between static sparse training and dynamic sparse training?

- Concept: Sparsity Variation Patterns
  - Why needed here: Different sparsity variation patterns (dense-to-sparse, sparse-to-dense, etc.) affect both FLOP reduction and model performance recovery during training.
  - Quick check question: Why does the restoration phase in MST use a pattern symmetric to the warm-up phase?

- Concept: Attention Mechanisms in Transformers
  - Why needed here: Understanding self-attention and its computational complexity is crucial for optimizing the attention layers through sparse patterns.
  - Quick check question: What is the computational complexity of full self-attention in terms of sequence length?

## Architecture Onboarding

- Component map: MST consists of Sparsity Variation (SV) -> Mixed-Growing (MG) -> Hybrid Sparse Attention (HSA), with SV managing temporal sparsity, MG managing spatial sparsity, and HSA optimizing attention FLOPs.

- Critical path: The ultra-sparsification phase where MG is applied with high sparsity levels. This phase determines the majority of FLOP reduction and requires careful tuning of the update fraction schedule and growth strategies.

- Design tradeoffs: Higher sparsity levels yield greater FLOP reduction but increase risk of performance degradation; the mixed-growing ratio balances exploration vs. exploitation of sparse topologies.

- Failure signatures: Training instability during ultra-sparsification phase indicates overly aggressive sparsity; poor performance recovery during restoration suggests insufficient connection reintroduction.

- First experiments:
  1. Verify FLOP calculation method matches theoretical expectations for sparse operations
  2. Test different mixed-growing ratios (R values) to find optimal balance
  3. Validate sparse attention mask patterns maintain attention coverage quality

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Limited experimental scope to GPT-2 models up to 1.5B parameters, raising scalability questions
- Computational overhead from dynamic sparsity management not fully characterized
- Practical integration with system-level acceleration methods not empirically demonstrated

## Confidence
**High Confidence**: The core methodology of combining dynamic sparse training with sparsity variation and hybrid sparse attention is technically sound and the 4× FLOP reduction claim is well-supported by the experimental results on GPT-2.

**Medium Confidence**: The claim that MST maintains performance parity with dense models across various tasks is supported but limited by the relatively small number of evaluation tasks and the narrow model size range tested.

**Low Confidence**: The claim that MST is "readily compatible" with system-level acceleration methods lacks empirical demonstration.

## Next Checks
1. **Scale Validation**: Implement MST on GPT-3 scale models (175B parameters) or other architectures like LLaMA/ViT to verify if the 4× FLOP reduction scales proportionally and maintains performance gains at larger scales.

2. **Ablation on Sparsity Patterns**: Conduct systematic experiments varying the ultra-sparsification levels beyond 98.75% and test different sparse attention patterns (not just strided) to identify the optimal trade-off between FLOP reduction and performance retention.

3. **Integration Testing**: Implement MST in conjunction with established system-level optimization techniques (like quantization, activation checkpointing, or tensor parallelism) to measure actual wall-clock time reduction and identify any compatibility issues in practical deployments.