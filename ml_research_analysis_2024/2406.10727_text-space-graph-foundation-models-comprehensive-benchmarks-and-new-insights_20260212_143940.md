---
ver: rpa2
title: 'Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights'
arxiv_id: '2406.10727'
source_url: https://arxiv.org/abs/2406.10727
tags:
- graph
- datasets
- co-training
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts a comprehensive benchmark for text-space graph
  foundation models, addressing the challenge of diverse node features across graphs
  from different domains. The authors introduce over 20 text-space datasets covering
  academic, e-commerce, biology, and other domains.
---

# Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights

## Quick Facts
- arXiv ID: 2406.10727
- Source URL: https://arxiv.org/abs/2406.10727
- Reference count: 40
- Key outcome: Comprehensive benchmark of text-space GFMs across 20+ datasets reveals co-training benefits graph classification but not node classification, with limited in-context learning for cross-graph tasks

## Executive Summary
This paper introduces comprehensive benchmarks for text-space Graph Foundation Models (GFMs), addressing the challenge of heterogeneous node features across diverse graph domains. The authors evaluate various GFM building blocks across four paradigms (co-training and pre-training, task-specific and cross-tasks) using over 20 text-space datasets spanning academic, e-commerce, biology, and other domains. Key findings include the effectiveness of co-training for graph classification tasks, the potential of link prediction-specific models like BUDDY when combined with proper structural embeddings, and the limited in-context learning capabilities for cross-graph, cross-task scenarios. The study provides novel insights into the effectiveness of text-space GFMs and identifies promising directions for future research.

## Method Summary
The study benchmarks text-space GFMs using over 20 datasets transformed into unified textual feature spaces via LLM encoders. Four GFM paradigms are evaluated: co-training task-specific, co-training cross-tasks, pre-training task-specific, and pre-training cross-tasks. Building blocks include Graph SSL, graph prompts, and LLM with graph projectors. Experiments measure performance across node classification, link prediction, and graph classification tasks, with accuracy, hit rate, and corresponding metrics as evaluation criteria. The framework compares against traditional GNN baselines and analyzes scaling behavior, negative transfer, and in-context learning capabilities.

## Key Results
- Co-training in text space provides clear performance gains for graph classification tasks on molecular datasets compared to training from scratch
- Co-training significantly benefits link prediction-specific models like BUDDY when combined with proper structural embeddings
- In-context learning capabilities remain limited for cross-graph, cross-task scenarios, with graph prompt-based approaches showing potential for heterophilous graphs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Text-space transformation mitigates feature heterogeneity by projecting diverse node features into a unified textual embedding space.
- **Mechanism:** Large language models map varied node attributes into natural language embeddings, creating a shared semantic space where graphs from different domains can be compared or combined.
- **Core assumption:** LLM-generated embeddings preserve the semantic relationships of original features well enough for downstream graph tasks.
- **Evidence anchors:** [abstract] "Inspired by multi-modal models that align different modalities with natural language..." [section 2.2] "To achieve a unified feature space, text-space GFMs adopt LLMs as the feature encoders..."
- **Break condition:** If LLM embeddings collapse distinct semantic meanings, negative transfer dominates.

### Mechanism 2
- **Claim:** Co-training across graphs benefits tasks that rely on shared structural patterns (e.g., graph classification) but not tasks requiring task-specific structural inductive biases (e.g., link prediction).
- **Mechanism:** During co-training, a unified model learns general structural regularities present across datasets. These regularities aid tasks like graph classification where global structure is key, but hurt link prediction, which needs pairwise structural cues.
- **Core assumption:** Structural patterns that are transferable exist across datasets in the same domain.
- **Evidence anchors:** [section 4.2.3] "Co-training in the text space brings clear performance gain compared to training from scratch for graph classification tasks..." [section 4.2.2] "Co-training significantly benefits models like BUDDY..."
- **Break condition:** If datasets lack shared structural regularities, co-training provides no benefit.

### Mechanism 3
- **Claim:** Cross-task co-training improves graph-level task performance because models learn inductive biases favorable to graph classification, but may harm node-level performance.
- **Mechanism:** When training across node, link, and graph tasks, the model adjusts to capture high-order structural relationships, boosting graph classification while potentially ignoring node-specific detail.
- **Core assumption:** Graph-level tasks can exploit information from node- and link-level tasks without losing discriminative power for the latter.
- **Evidence anchors:** [section 4.3.1] "Co-training OFA on node classification, link prediction, and graph classification tasks across different datasets, the model's performance in graph classification will improve while its performance at link prediction and node classification may decline."
- **Break condition:** If node- and link-level tasks dominate dataset size, the model may overfit to their inductive biases.

## Foundational Learning

- **Concept:** Graph neural network message passing and its limitations with heterogeneous features
  - **Why needed here:** Text-space GFM relies on a unified feature space; understanding why standard GNNs fail with heterogeneous features explains the motivation.
  - **Quick check question:** Why can't a single GNN layer process node features with different dimensions and semantics across datasets?

- **Concept:** Self-supervised learning on graphs (e.g., contrastive, reconstruction-based methods)
  - **Why needed here:** Graph SSL is one of the building blocks evaluated for GFM; understanding its role helps interpret co-training benefits.
  - **Quick check question:** How does contrastive SSL on graphs differ from supervised training in terms of feature space alignment?

- **Concept:** In-context learning and few-shot adaptation in large language models
  - **Why needed here:** The paper evaluates zero-shot/few-shot inference as a GFM capability; understanding LLM inference modes is critical.
  - **Quick check question:** What is the difference between fine-tuning and in-context learning for adapting a GFM to a new task?

## Architecture Onboarding

- **Component map:** Text encoder (LLM or Sentence-BERT) → Graph encoder (GNN, Transformer, or fixed preprocessing like SGC) → Task heads (node/link/graph classification layers) → Co-training manager (orchestrates multi-dataset, multi-task training)
- **Critical path:** Text encoder → Graph encoder → Task heads. Each stage must preserve semantic and structural integrity.
- **Design tradeoffs:**
  - Fixed LLM vs. fine-tuned: fixed is faster but less adaptable
  - Cascading (LLM→GNN) vs. joint: joint better for small datasets but computationally heavy
  - Unified task formulation vs. task-specific heads: unified simplifies co-training but may underfit specialized tasks
- **Failure signatures:**
  - Catastrophic forgetting when co-training large-scale diverse datasets (observed with LLaGA)
  - Negative transfer when feature spaces are too dissimilar
  - In-context learning failures when prompts lack clear structural cues
- **First 3 experiments:**
  1. Single-dataset baseline: Train a GCN on Cora with Sentence-BERT embeddings; compare to original TF-IDF
  2. Co-training within domain: Co-train OneForAll on citation datasets; measure gain vs. single-dataset training
  3. Cross-task co-training: Co-train OneForAll on node and graph tasks across datasets; evaluate graph classification improvement

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the fundamental reason why co-training on node-level tasks does not improve performance, while it does for graph-level tasks?
- **Basis in paper:** [explicit] The authors state that for node-level co-training, "GFM methods present a performance gap compared to GCN training from scratch" and that "the primary reasons why we don't observe benefits in node-level co-training are: (1) Stacking more data doesn't exhibit a scaling behavior if we ignore graph structure; (2) When considering graph structure, GCN with learnable aggregation as the backbone does not perform better than SGC [50] with fixed aggregation."
- **Why unresolved:** While the authors provide some insights, the exact mechanism by which co-training benefits graph-level tasks but not node-level tasks remains unclear.
- **What evidence would resolve it:** Further investigation into the differences in feature and structure learning between node-level and graph-level tasks, as well as experiments comparing different GNN architectures on these tasks.

### Open Question 2
- **Question:** How can we design a task-specific GFM for link prediction that effectively leverages the benefits of co-training observed in link prediction-specific models like BUDDY?
- **Basis in paper:** [explicit] The authors state that "Link prediction-specific models like BUDDY show great potential to benefit from co-training, highlighting the importance of proper structural embeddings. This also suggests that designing a task-specific GFM for link prediction could be a promising direction."
- **Why unresolved:** The paper does not provide a concrete solution for designing a task-specific GFM for link prediction that can effectively leverage the benefits of co-training.
- **What evidence would resolve it:** Development and evaluation of a task-specific GFM for link prediction that incorporates the key insights from link prediction-specific models like BUDDY.

### Open Question 3
- **Question:** What is the impact of different LLM encoders on the performance of text-space GFMs, and how can we select the most appropriate LLM encoder for a given task and dataset?
- **Basis in paper:** [explicit] The authors mention that "Using stronger LLM encoders is an effective way to improve node-level performance" and provide a comparison between minilm and mpnet encoders.
- **Why unresolved:** The paper does not provide a comprehensive analysis of the impact of different LLM encoders on the performance of text-space GFMs across various tasks and datasets.
- **What evidence would resolve it:** A systematic study comparing the performance of text-space GFMs using different LLM encoders on a wide range of tasks and datasets, along with guidelines for selecting the most appropriate LLM encoder.

## Limitations
- The study relies heavily on experimental results without providing strong theoretical guarantees for the proposed mechanisms
- Limited evaluation of in-context learning capabilities showed underwhelming results for cross-graph, cross-task scenarios
- The paper doesn't address computational costs of fine-tuning large language models versus using fixed encoders

## Confidence
- High confidence: The observation that co-training benefits graph classification tasks but struggles with node classification is well-supported by experimental results across multiple datasets
- Medium confidence: The claim that link prediction-specific models like BUDDY show promise when combined with proper structural embeddings is supported by evidence but could benefit from more systematic ablation studies
- Low confidence: The assertion that in-context learning capabilities remain limited for cross-graph, cross-task scenarios is based on experimental observations but lacks deeper analysis of why this limitation exists

## Next Checks
1. Conduct ablation studies on the text encoder component to quantify how much performance depends on specific LLM choices versus the overall text-space framework
2. Evaluate the impact of dataset similarity metrics (feature space overlap, homophily ratios) on co-training effectiveness to better understand when negative transfer occurs
3. Test whether task-specific fine-tuning of the text encoder after co-training can recover performance in node-level tasks that degrade during cross-task training