---
ver: rpa2
title: 'Hindi-BEIR : A Large Scale Retrieval Benchmark in Hindi'
arxiv_id: '2408.09437'
source_url: https://arxiv.org/abs/2408.09437
tags:
- dataset
- figure
- retrieval
- corpus
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present Hindi-BEIR, the first comprehensive information
  retrieval benchmark for the Hindi language, comprising 15 datasets spanning 8 distinct
  tasks across 5 domains. The benchmark includes translated English BEIR datasets,
  existing Hindi retrieval datasets, and synthetically created datasets.
---

# Hindi-BEIR : A Large Scale Retrieval Benchmark in Hindi

## Quick Facts
- arXiv ID: 2408.09437
- Source URL: https://arxiv.org/abs/2408.09437
- Reference count: 40
- First comprehensive information retrieval benchmark for Hindi language

## Executive Summary
Hindi-BEIR presents the first large-scale retrieval benchmark for the Hindi language, comprising 15 datasets spanning 8 distinct tasks across 5 domains. The benchmark includes translated English BEIR datasets, existing Hindi retrieval datasets, and synthetically created datasets, covering over 27 million documents and nearly 200,000 queries. The authors evaluate state-of-the-art multilingual retrieval models on this benchmark, revealing significant performance variations across tasks and domains, and highlighting the need for further research in Hindi language retrieval.

## Method Summary
The authors created Hindi-BEIR by translating English BEIR datasets to Hindi using IndicTrans2 with back-translation and Chrf++ filtering, creating synthetic cross-lingual datasets, and incorporating existing Hindi retrieval datasets. The benchmark covers 15 datasets across 8 tasks and 5 domains, with over 27 million documents and nearly 200,000 queries. State-of-the-art multilingual retrieval models were evaluated on this benchmark to establish baseline performance and identify specific task/domain weaknesses.

## Key Results
- Hindi-BEIR successfully creates a comprehensive benchmark spanning 15 datasets across 8 tasks and 5 domains
- State-of-the-art multilingual models show significant performance variations across different tasks and domains
- BM25 performs poorly on the CC News Retrieval dataset due to lack of lexical overlap between English queries and Hindi documents

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Translation of BEIR datasets to Hindi with quality filtering creates high-quality retrieval tasks
- **Mechanism**: The authors use IndicTrans2 to translate English datasets, then apply back-translation and Chrf++ filtering to ensure translation quality
- **Core assumption**: Chrf++ scores above 50 reliably indicate high-quality translations suitable for retrieval tasks
- **Evidence anchors**:
  - [abstract]: "We utilized the Indic-Trans2 model... We employ back-translation technique to retain good translations... We calculate the Chrf(++) score... We retained only those translations with a Chrf++ score exceeding a threshold. We empirically set the threshold to 50 after manually verifying the translation quality"
  - [section]: "This strategy enables us to leverage the wealth of existing high-quality datasets in English while making them accessible and useful for Hindi language information retrieval tasks"
- **Break condition**: If Chrf++ score thresholds are set too high, the translated datasets may become too small to be useful

### Mechanism 2
- **Claim**: Creating cross-lingual retrieval datasets by pairing English queries with Hindi documents creates challenging zero-shot scenarios
- **Mechanism**: For the CC News Retrieval dataset, the authors filter for pairs with zero Rouge-L and zero Jaccard index between titles and articles, then use English titles as queries and Hindi articles as documents
- **Core assumption**: Models must learn language-agnostic embeddings to succeed when queries and documents are in different languages
- **Evidence anchors**:
  - [section]: "This dataset presents a scenario where the query is in English and the corpus is in Hindi that needs to move beyond token level matching. This explains the poor performance of BM25 on this dataset due to the lack of overlap between English and Hindi tokens"
- **Break condition**: If the zero-overlap filtering is too strict, the dataset may not contain enough valid query-document pairs

### Mechanism 3
- **Claim**: Evaluating models on diverse domains and tasks reveals specific strengths and weaknesses
- **Mechanism**: By including 15 datasets across 8 tasks and 5 domains, the benchmark forces models to generalize beyond their training distribution
- **Core assumption**: Task and domain diversity in the benchmark creates meaningful performance differentials that reveal model capabilities
- **Evidence anchors**:
  - [section]: "As we can see, mE5 specifically struggles in fact checking datasets with average NDCG@10 of 30% much below than its overall average, while BGE-M3 although not great, still fares much better for fact checking task"
  - [section]: "Also, for niche domains such as Finance and Climate, we see a sharp drop for both the models, prompting the need for focused research in these areas"
- **Break condition**: If the performance variations are primarily due to dataset quality differences rather than true model limitations

## Foundational Learning

- **Concept**: Back-translation and quality filtering
  - Why needed here: Ensures that translated datasets maintain the quality and difficulty of the original English BEIR datasets
  - Quick check question: What metric did the authors use to filter translations, and what threshold did they empirically determine?

- **Concept**: Cross-lingual retrieval and language-agnostic embeddings
  - Why needed here: The CC News Retrieval dataset specifically tests whether models can retrieve documents when queries and documents are in different languages
  - Quick check question: Why does BM25 perform poorly on the CC News Retrieval dataset?

- **Concept**: Zero-shot evaluation and domain/task generalization
  - Why needed here: Hindi-BEIR's diverse datasets allow evaluation of how well models generalize across different domains and tasks without task-specific fine-tuning
  - Quick check question: Which model performed poorly on fact-checking tasks according to the results?

## Architecture Onboarding

- **Component map**: The benchmark consists of 15 datasets spanning 8 tasks across 5 domains. Key components include: translated BEIR datasets (ArguAna, FiQA-2018, TREC-COVID, SCIDOCS, SciFact, Touch√©-2020, NQ, FEVER, Climate-FEVER), synthetic cross-lingual datasets (CC News Retrieval), and existing multilingual datasets (MIRACL, MLDR, mMarco, IndicQARetrieval, WikiPediaRetrieval).

- **Critical path**: For a new retrieval model to be evaluated on Hindi-BEIR, the critical path is: (1) prepare model embeddings for Hindi text, (2) run retrieval on each of the 15 datasets, (3) calculate NDCG@10 for each dataset, (4) aggregate results across tasks and domains.

- **Design tradeoffs**: The benchmark trades comprehensiveness (15 datasets, multiple domains) against potential inconsistencies in dataset quality due to translation. The use of back-translation filtering attempts to mitigate this, but some quality variation is inevitable.

- **Failure signatures**: Poor performance on CC News Retrieval indicates weakness in cross-lingual understanding. Poor performance on fact-checking datasets suggests limitations in reasoning capabilities. Poor performance on long-document datasets (MLDR) indicates context length limitations.

- **First 3 experiments**:
  1. Run BM25 baseline on all datasets to establish lexical matching performance and identify datasets where lexical overlap is insufficient
  2. Run a multilingual sentence embedding model (e.g., LASER) to establish baseline for language-agnostic embeddings
  3. Run a state-of-the-art multilingual retrieval model (e.g., BGE-M3 or mE5) to establish current best performance and identify specific task/domain weaknesses

## Open Questions the Paper Calls Out
None

## Limitations

- The translation quality filtering mechanism relies heavily on Chrf++ scores, which may not fully capture semantic equivalence between English and Hindi queries
- The CC News Retrieval dataset creation method may inadvertently exclude relevant query-document pairs that happen to share some lexical overlap
- The manual verification process for translation quality and the number of samples checked remain unclear

## Confidence

**High Confidence** (Strong evidence from paper):
- Hindi-BEIR successfully creates a comprehensive benchmark spanning 15 datasets across 8 tasks and 5 domains
- State-of-the-art multilingual models show significant performance variations across different tasks and domains
- BM25 performs poorly on the CC News Retrieval dataset due to lack of lexical overlap between English queries and Hindi documents

**Medium Confidence** (Some evidence, but with limitations):
- The translation quality filtering with Chrf++ score threshold of 50 ensures high-quality translations
- The performance differences across tasks reveal genuine model limitations rather than dataset quality issues
- The benchmark provides meaningful insights for advancing Hindi language retrieval research

**Low Confidence** (Weak or no evidence provided):
- The specific manual verification process for translation quality and the number of samples checked
- Whether the zero-overlap filtering for CC News Retrieval creates an artificially difficult benchmark
- The exact distribution of performance across all 15 datasets and whether some poor results stem from dataset quality issues

## Next Checks

1. **Translation Quality Validation**: Sample and manually evaluate 50-100 translations from each BEIR dataset translation to verify that the Chrf++ threshold of 50 actually corresponds to high-quality, semantically equivalent translations. Compare human judgments with Chrf++ scores to assess correlation.

2. **CC News Retrieval Dataset Analysis**: Analyze the distribution of Rouge-L and Jaccard index scores in the original CC News corpus to determine what percentage of pairs are excluded by the zero-overlap filtering. Create alternative versions with small overlap thresholds (e.g., Rouge-L < 0.1) to assess whether this affects model performance patterns.

3. **Cross-Dataset Performance Correlation**: Compute inter-dataset correlations of model performance to determine whether observed variations reflect true task/domain differences or dataset-specific factors. If datasets within the same domain show inconsistent performance patterns, this may indicate dataset quality issues rather than genuine model limitations.