---
ver: rpa2
title: Perturbation Effects on Accuracy and Fairness among Similar Individuals
arxiv_id: '2404.01356'
source_url: https://arxiv.org/abs/2404.01356
tags:
- fairness
- adversarial
- accuracy
- perturbations
- individual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces robust individual fairness (RIF), a unified
  robustness criterion requiring that similar individuals maintain both prediction
  accuracy and fairness under adversarial perturbations. To expose violations of RIF,
  the authors propose RIFair, a gradient-based attack method that applies identical
  perturbations to similar individuals and generates three types of adversarial instances:
  true biased (TB), false biased (FB), and false fair (FF).'
---

# Perturbation Effects on Accuracy and Fairness among Similar Individuals

## Quick Facts
- arXiv ID: 2404.01356
- Source URL: https://arxiv.org/abs/2404.01356
- Authors: Xuran Li; Hao Xue; Peng Wu; Xingjun Ma; Zhen Zhang; Huaming Chen; Flora D. Salim
- Reference count: 36
- One-line primary result: Robust individual fairness (RIF) unifies adversarial robustness and individual fairness by requiring similar individuals maintain consistent predictions under perturbations.

## Executive Summary
This paper introduces robust individual fairness (RIF), a unified criterion requiring that similar individuals maintain both prediction accuracy and fairness under adversarial perturbations. The authors propose RIFair, a gradient-based attack method that applies identical perturbations to similar individuals and generates three types of adversarial instances: true biased (TB), false biased (FB), and false fair (FF). They further introduce perturbation impact index (PII) and perturbation impact direction (PID) metrics to quantify how perturbations affect similar individuals differently.

## Method Summary
The authors convert four tabular datasets to natural language descriptions using structured templates, then train seven model architectures (MLP, DeepFM, Wide&Deep, AutoInt, TabTransformer, Transformer, BERT) on these datasets. They implement RIFair to generate adversarial instances via gradient-based optimization targeting three attack types, then evaluate model performance using RIF metrics along with standard accuracy and fairness metrics. The method involves preprocessing data, training models, applying RIFair attacks, calculating PII/PID metrics, and comprehensive evaluation.

## Key Results
- RIFair successfully generates adversarial instances that violate both accuracy and fairness requirements
- Similar individuals with identical perturbations can experience divergent prediction changes due to differing PII values
- Models exhibit varying sensitivity to perturbations across different datasets and architectures
- The three attack types (TB, FB, FF) reveal distinct vulnerabilities in model behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Identical perturbations applied to similar individuals can lead to divergent prediction changes due to differing sensitivity magnitudes (PII).
- Mechanism: The Perturbation Impact Index (PII) captures the realized sensitivity of a model to a finite perturbation. Similar individuals with the same Perturbation Impact Direction (PID) but different PIIs will experience prediction changes of different magnitudes, potentially crossing decision boundaries at different times.
- Core assumption: PII accurately reflects the actual change induced by a concrete perturbation, fully capturing model non-linearities and feature interactions.
- Evidence anchors:
  - [abstract]: "similar individuals share the same PID but have sharply different PIIs, leading to divergent prediction-change trajectories"
  - [section]: "Theorem 2 (Single-Perturbation Effect)... Theorem 3 (Cumulative Perturbation Impact)... Theorem 4 (Perturbation Needed for a Decision Flip)... Theorem 5 (Perturbation Tolerance Before a Flip)"
  - [corpus]: Weak evidence. Corpus contains related papers on adversarial training and perturbations, but none specifically address the PII/PID framework or similar individual analysis.
- Break condition: If PII fails to capture the actual prediction change due to complex feature interactions or if the perturbation magnitude is too large to be considered "small" in the context of the model's input space.

### Mechanism 2
- Claim: Robust Individual Fairness (RIF) unifies adversarial robustness and individual fairness by requiring that similar individuals receive predictions consistent with the same ground truth even under adversarial manipulation.
- Mechanism: RIF requires that for any perturbation and any similar individual, the prediction difference does not exceed a bound determined by the input difference and a Lipschitz constant. This ensures both prediction accuracy and fairness under perturbation.
- Core assumption: The distance metrics (路,路) and (路,路) are valid and appropriately chosen to measure prediction and input differences.
- Evidence anchors:
  - [abstract]: "robust individual fairness (RIF), which requires that similar individuals receive predictions consistent with the same ground truth even under adversarial manipulation"
  - [section]: "Definition 1 (Robust Individual Fairness)... Theorem 1... Definition 2 (Empirical Evaluation of RIF)"
  - [corpus]: Weak evidence. While the corpus contains papers on adversarial robustness and fairness, it lacks papers specifically on RIF or unified robustness-fairness criteria.
- Break condition: If the Lipschitz constant  is too large, allowing significant prediction deviations, or if the similarity metric (路) does not accurately capture true similarity between individuals.

### Mechanism 3
- Claim: RIFair, an adversarial method, can generate instances that violate RIF by perturbing identical features across similar individuals, exposing vulnerabilities in both accuracy and fairness.
- Mechanism: RIFair constructs three optimization problems, each targeting a specific violation type (true biased, false biased, false fair impact). By restricting perturbations to the same non-sensitive features, RIFair preserves semantic similarity while exposing vulnerabilities.
- Core assumption: Gradient-based optimization can effectively find perturbations that maximize the targeted violation objectives.
- Evidence anchors:
  - [abstract]: "RIFair, an attack framework that applies identical perturbations to similar individuals to induce accuracy or fairness failures"
  - [section]: "4.1 Adversarial Instances Generation... Algorithm 1 RIFair: Adversarial Instance Generation"
  - [corpus]: Weak evidence. The corpus contains papers on adversarial attacks and training, but lacks specific papers on RIFair or similar individual-based adversarial methods.
- Break condition: If the optimization gets stuck in local minima or if the perturbations do not effectively exploit the model's vulnerabilities to accuracy or fairness.

## Foundational Learning

- Concept: Adversarial Robustness
  - Why needed here: Understanding how adversarial perturbations can degrade model accuracy is fundamental to grasping the RIF concept and the need for robust individual fairness.
  - Quick check question: What is the primary goal of adversarial robustness techniques in machine learning?

- Concept: Individual Fairness
  - Why needed here: Individual fairness is a key component of RIF, ensuring that similar individuals receive similar predictions, which is crucial for ethical AI systems.
  - Quick check question: How does individual fairness differ from group fairness in machine learning?

- Concept: Gradient-based Optimization
  - Why needed here: RIFair relies on gradient-based optimization to find effective perturbations that violate RIF, making it essential to understand how these methods work.
  - Quick check question: What are the key advantages and disadvantages of using gradient-based methods for adversarial attack generation?

## Architecture Onboarding

- Component map:
  Data Preprocessing -> Natural Language Templates -> Model Training -> RIFair Attack Generation -> PII/PID Calculation -> RIF Evaluation

- Critical path:
  1. Preprocess data and generate natural language templates
  2. Train models on the prepared data
  3. Apply RIFair to generate adversarial instances
  4. Calculate PII and PID for each instance
  5. Evaluate model performance under RIF criteria

- Design tradeoffs:
  - Using natural language templates adds linguistic diversity but may introduce noise
  - Restricting perturbations to shared features preserves similarity but may limit attack effectiveness
  - Calculating PII and PID provides insight but adds computational overhead

- Failure signatures:
  - Model performance does not degrade under RIFair attacks (indicating potential issues with the attack method or model robustness)
  - PII and PID values are consistently low across all instances (suggesting model insensitivity to perturbations)
  - Evaluation metrics do not align with expected rankings based on separate accuracy and fairness assessments

- First 3 experiments:
  1. Evaluate model performance on clean data using standard accuracy and fairness metrics
  2. Apply RIFair with minimal perturbations and analyze PII/PID distributions
  3. Gradually increase perturbation magnitude and observe changes in RIF compliance and evaluation metrics

## Open Questions the Paper Calls Out
None

## Limitations
- The PII metric's ability to capture actual prediction changes in highly non-linear models remains unverified
- The restricted perturbation approach (same features across similar individuals) may not generalize to all similarity metrics
- The computational overhead of calculating PII/PID for large datasets is not addressed

## Confidence
- Medium: RIF mechanism claims due to theoretical grounding but limited empirical validation across diverse model architectures
- Low: RIFair optimization implementation details as the exact gradient selection and realistic feature replacement procedures are underspecified
- Low: PII metric's ability to capture actual prediction changes in complex models

## Next Checks
1. Verify RIFair's gradient optimization against alternative perturbation methods on a small synthetic dataset
2. Test PII/PID sensitivity to normalization choices across different model architectures
3. Evaluate whether restricting perturbations to shared features actually preserves semantic similarity in practice