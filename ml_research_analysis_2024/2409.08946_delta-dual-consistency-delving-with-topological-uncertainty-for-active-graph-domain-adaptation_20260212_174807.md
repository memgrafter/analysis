---
ver: rpa2
title: 'DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph
  Domain Adaptation'
arxiv_id: '2409.08946'
source_url: https://arxiv.org/abs/2409.08946
tags:
- graph
- learning
- nodes
- domain
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DELTA, a novel approach for active graph
  domain adaptation that addresses the challenge of selecting informative nodes in
  target graphs under limited labeling budgets. The method combines dual graph subnetworks
  (edge-oriented and path-oriented) to explore topological semantics from complementary
  perspectives, followed by coarse candidate selection based on consistency across
  subnetworks.
---

# DELTA: Dual Consistency Delving with Topological Uncertainty for Active Graph Domain Adaptation

## Quick Facts
- arXiv ID: 2409.08946
- Source URL: https://arxiv.org/abs/2409.08946
- Reference count: 32
- Average performance improvements of over 2% in Macro-F1 and Micro-F1 metrics on benchmark datasets

## Executive Summary
This paper introduces DELTA, a novel approach for active graph domain adaptation that addresses the challenge of selecting informative nodes in target graphs under limited labeling budgets. The method combines dual graph subnetworks (edge-oriented and path-oriented) to explore topological semantics from complementary perspectives, followed by coarse candidate selection based on consistency across subnetworks. Topological uncertainty is then measured using node degrees and K-hop subgraphs, while domain discrepancy scores capture attribute differences between source and target nodes. Extensive experiments on benchmark datasets demonstrate that DELTA outperforms state-of-the-art approaches, achieving average performance improvements of over 2% in Macro-F1 and Micro-F1 metrics.

## Method Summary
DELTA employs a dual subnetwork architecture consisting of an edge-oriented graph subnetwork (using GCN for neighborhood aggregation) and a path-oriented graph subnetwork (using PAN for path-based aggregation). The method first trains both subnetworks on source and target graphs, then measures consistency between their predictions to identify candidate nodes with high topological uncertainty. For these candidates, DELTA computes topological uncertainty scores using degree-weighted K-hop subgraph information and domain discrepancy scores measuring attribute differences from source nodes. The final selection combines these scores to identify the most informative nodes for labeling, operating in a one-round selection manner for efficiency.

## Key Results
- Outperforms state-of-the-art active graph domain adaptation methods by over 2% in average Macro-F1 and Micro-F1 scores
- Demonstrates effectiveness across three citation network datasets (ACMv9, Citationv1, DBLPv7) with 5% initial source labels
- Shows robust performance across varying target node selection budgets (25-200 nodes)
- Achieves superior performance while maintaining computational efficiency through one-round selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual graph subnetworks capture complementary topological semantics that single-view methods miss.
- Mechanism: An edge-oriented subnetwork uses message passing to implicitly aggregate neighborhood information, while a path-oriented subnetwork explicitly computes weighted path-based relationships across different lengths.
- Core assumption: Topological information is more effectively represented when both local neighborhood aggregation and global path-based relationships are considered.
- Evidence anchors: [abstract], [section 4.2.1], [section 4.2.2], [corpus]

### Mechanism 2
- Claim: Consistency inconsistency across dual subnetworks identifies candidate nodes with complex, uncertain topological information.
- Mechanism: Nodes with large Euclidean distance between edge-oriented and path-oriented predictions are flagged as candidates, as this inconsistency indicates rich topological semantics that both subnetworks struggle to represent consistently.
- Core assumption: Nodes with inconsistent predictions across complementary views contain more valuable information for active learning than consistently predicted nodes.
- Evidence anchors: [section 4.2.3], [section 4.3], [corpus]

### Mechanism 3
- Claim: Combining degree-weighted topological uncertainty with domain discrepancy scores effectively identifies nodes that improve cross-domain adaptation.
- Mechanism: Topological uncertainty aggregates K-hop subgraph information weighted by node degrees, while domain discrepancy measures attribute differences weighted by source node degrees, creating a composite score that balances local uncertainty and cross-domain informativeness.
- Core assumption: Nodes that are both topologically uncertain within the target graph AND dissimilar to source graph nodes are most valuable for active learning in domain adaptation.
- Evidence anchors: [section 4.3], [section 4.4], [section 4.5], [section 1], [corpus]

## Foundational Learning

- Concept: Graph neural networks and message passing
  - Why needed here: The edge-oriented subnetwork relies on message passing to aggregate neighborhood information, which is fundamental to how it captures topological semantics
  - Quick check question: What is the basic update rule for message passing in a standard GNN layer, and how does it aggregate information from neighboring nodes?

- Concept: Path-based graph representations
  - Why needed here: The path-oriented subnetwork explicitly computes weighted path-based relationships, requiring understanding of how paths capture higher-order graph structure
  - Quick check question: How does aggregating information along paths of different lengths capture higher-order relationships compared to simple neighborhood aggregation?

- Concept: Active learning uncertainty sampling
  - Why needed here: The method uses uncertainty-based sampling but extends it to graph structures with topological considerations
  - Quick check question: What are the main approaches to uncertainty estimation in active learning, and how do they differ when applied to structured data like graphs?

## Architecture Onboarding

- Component map:
  Edge-oriented subnetwork (GCN-based) -> Path-oriented subnetwork (PAN-based) -> Consistency measurement module -> Topological uncertainty calculator -> Domain discrepancy calculator -> Composite scoring and selection

- Critical path:
  1. Train dual subnetworks on source and target graphs
  2. Compute inconsistency scores to identify candidate nodes
  3. Calculate topological uncertainty for candidates
  4. Calculate domain discrepancy for candidates
  5. Combine scores and select top-k nodes for labeling

- Design tradeoffs:
  - Single vs dual subnetworks: Dual provides complementary views but increases complexity
  - K-hop subgraph size: Larger K captures more context but increases noise and computation
  - Consistency threshold γ: Controls candidate set size vs informativeness tradeoff
  - One-round vs iterative selection: Simpler implementation vs potentially better long-term performance

- Failure signatures:
  - Poor performance despite high model capacity: May indicate redundant dual subnetworks or inappropriate consistency threshold
  - High variance in results: Could signal instability in candidate selection or inappropriate K-hop parameter
  - No improvement over random selection: Suggests the composite scoring is not effectively identifying informative nodes

- First 3 experiments:
  1. Ablation study: Compare single GCN vs dual GCN+PAN vs proposed method to verify complementarity benefit
  2. Parameter sensitivity: Vary γ and K to find optimal settings for candidate selection and uncertainty estimation
  3. Runtime analysis: Measure time complexity vs performance tradeoff to ensure practical efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DELTA's performance scale when applied to graphs with significantly different topological structures (e.g., highly heterogeneous graphs vs. homogeneous graphs)?
- Basis in paper: [inferred] The paper focuses on citation networks with similar structures; no experiments are reported on graphs with diverse topologies like social networks, biological networks, or heterogeneous graphs.
- Why unresolved: The paper only evaluates on three citation datasets, all sharing similar structural properties. There is no analysis of performance on graphs with different connectivity patterns, node degree distributions, or heterophily.
- What evidence would resolve it: Experiments on diverse graph datasets (social, biological, heterophilic) showing performance degradation or robustness across different structural properties.

### Open Question 2
- Question: What is the impact of varying the consistency threshold γ on active learning efficiency versus model performance trade-off?
- Basis in paper: [explicit] The paper mentions that γ controls consistency between dual subnetworks and suggests setting it between 0.3-0.5, but doesn't systematically explore how different thresholds affect labeling efficiency or model convergence speed.
- Why unresolved: While the paper provides parameter sensitivity results for γ, it doesn't analyze how different thresholds affect the number of rounds needed for convergence or the cost-benefit ratio of stricter versus looser consistency requirements.
- What evidence would resolve it: Systematic experiments varying γ across a wider range with metrics for both final performance and labeling efficiency (e.g., convergence speed, labeling cost per performance gain).

### Open Question 3
- Question: Can the path-oriented subnetwork be replaced with other high-order structural modeling approaches while maintaining or improving DELTA's performance?
- Basis in paper: [explicit] The paper uses PAN (Path Aggregation Network) for path-oriented modeling and shows some alternative backbones in experiments, but doesn't systematically compare against other high-order structural approaches like graph transformers or higher-order GNNs.
- Why unresolved: The paper only explores a limited set of alternatives for the edge-oriented subnetwork and doesn't evaluate other approaches for capturing high-order structural information that could potentially complement the edge-oriented view.
- What evidence would resolve it: Comparative experiments replacing PAN with other high-order structural modeling approaches (graph transformers, hypergraph neural networks, higher-order GNNs) while keeping the dual consistency framework intact.

### Open Question 4
- Question: How does DELTA's performance change when applied to scenarios with extremely limited labeling budgets (e.g., <25 nodes) or when the budget is a much larger fraction of the target graph size?
- Basis in paper: [inferred] The paper tests budgets of 25-200 nodes but doesn't specifically analyze the low-budget regime (<10% of target graph) or high-budget regime (>50% of target graph) where different active learning strategies might be optimal.
- Why unresolved: The experimental results focus on moderate budget ranges and don't provide insights into whether DELTA's dual consistency approach remains advantageous when the budget is extremely constrained or relatively generous.
- What evidence would resolve it: Experiments with very small budgets (<10% of target nodes) and large budgets (>50% of target nodes) to identify budget thresholds where DELTA's advantages emerge or diminish.

## Limitations

- Limited generalization to diverse graph types beyond citation networks with similar structural properties
- One-round selection strategy may miss iterative refinement opportunities for improved long-term performance
- Effectiveness of dual subnetworks depends on capturing truly complementary rather than redundant topological information

## Confidence

- High confidence: The core mechanism of using dual subnetworks for complementary topological views is well-supported by both theory and experiments
- Medium confidence: The consistency-based candidate selection approach shows promise but could benefit from more extensive ablation studies on parameter sensitivity
- Medium confidence: The combination of topological uncertainty and domain discrepancy scores appears effective, though the optimal weighting balance is not fully explored

## Next Checks

1. **Representation Analysis**: Conduct a detailed study of the learned representations from both subnetworks to verify they capture complementary rather than redundant information, using metrics like mutual information and correlation analysis.

2. **Parameter Sensitivity**: Systematically vary the consistency threshold γ, K-hop parameter, and the weighting between uncertainty and discrepancy scores across a wider range of values to identify optimal settings and robustness.

3. **Generalization Testing**: Evaluate DELTA on additional graph datasets with different characteristics (e.g., biological networks, social networks, knowledge graphs) to assess performance across diverse topological structures and domain adaptation scenarios.