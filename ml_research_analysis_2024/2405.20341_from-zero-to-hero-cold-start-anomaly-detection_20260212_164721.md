---
ver: rpa2
title: 'From Zero to Hero: Cold-Start Anomaly Detection'
arxiv_id: '2405.20341'
source_url: https://arxiv.org/abs/2405.20341
tags:
- anomaly
- detection
- class
- observations
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of anomaly detection in the cold-start
  setting, where a system has no observed data initially but receives a small stream
  of potentially contaminated observations over time. The proposed method, ColdFusion,
  addresses this challenge by combining zero-shot guidance (class descriptions) with
  the observation stream.
---

# From Zero to Hero: Cold-Start Anomaly Detection

## Quick Facts
- arXiv ID: 2405.20341
- Source URL: https://arxiv.org/abs/2405.20341
- Authors: Tal Reiss; George Kour; Naama Zwerdling; Ateret Anaby-Tavor; Yedid Hoshen
- Reference count: 20
- Primary result: ColdFusion significantly outperforms baselines in cold-start anomaly detection with AUC2_t scores across 2.5%, 5%, and 7.5% contamination levels

## Executive Summary
ColdFusion addresses the challenge of anomaly detection in cold-start scenarios where no initial training data exists but a small stream of potentially contaminated observations becomes available over time. The method combines zero-shot learning with adaptive class embeddings, using the median of class descriptions and assigned observations to create robust anomaly detection scores. Experiments on three datasets demonstrate that ColdFusion consistently outperforms existing baselines, particularly in scenarios with limited observations and varying contamination levels.

## Method Summary
ColdFusion tackles cold-start anomaly detection by first generating class descriptions using ChatGPT, then encoding both descriptions and observations with a pre-trained feature extractor. Each observation is assigned to its nearest class based on embedding similarity, and class embeddings are adapted using the median of the original class description and all assigned observations. The method computes anomaly scores as the minimum distance to these adapted class embeddings, providing robust detection even with contaminated observation streams.

## Key Results
- ColdFusion achieves superior AUC2_t scores compared to baselines across all tested contamination levels (2.5%, 5%, 7.5%)
- The method demonstrates consistent improvement over time as more observations become available
- ChatGPT-generated queries as class descriptions significantly improve performance compared to using raw intent names
- ColdFusion is robust to different feature encoders and maintains effectiveness with limited observations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ColdFusion adapts zero-shot class embeddings using the median of class descriptions and assigned observations.
- Mechanism: The method assigns observations to classes based on nearest descriptor similarity, then updates class embeddings by taking the median of the original class embedding and all observations assigned to that class.
- Core assumption: The median is robust to contamination by anomalies in the observation stream.
- Evidence anchors:
  - [abstract]: "It assigns observations to classes based on similarity and then adapts class embeddings using the median of class descriptions and assigned observations."
  - [section]: "the adapted code for each class is the median of the set containing the embedding of the class descriptions and the embeddings of all assigned observations"
- Break condition: If contamination ratio is very high (>30%), median may not be sufficient to filter out anomalous influence.

### Mechanism 2
- Claim: ColdFusion improves over time as more observations become available.
- Mechanism: As t increases, more observations are assigned to each class, providing better estimates of the true class distribution, which refines the adapted class embeddings.
- Core assumption: Observations are representative of the normal class distribution and contamination is limited.
- Evidence anchors:
  - [abstract]: "This approach effectively handles contamination and improves over time."
  - [section]: "ColdFusion consistently outperforms all baselines... improves over time"
- Break condition: If the observation stream stops growing or becomes heavily contaminated, improvement may plateau or degrade.

### Mechanism 3
- Claim: Generated queries using ChatGPT as class descriptions improve zero-shot performance compared to using raw intent names.
- Mechanism: ChatGPT-generated queries provide more natural language context that better captures the semantic meaning of intents, leading to better feature embeddings.
- Core assumption: ChatGPT can generate meaningful, intent-representative queries from topic names.
- Evidence anchors:
  - [section]: "we leverage ChatGPT to generate a query corresponding to each topic and utilize these generated queries as class descriptions instead of the intent topic names"
  - [section]: "The results highlight that naive encoding of intent names alone yields subpar performance, whereas our pre-processing procedure considerably improves results"
- Break condition: If ChatGPT generates irrelevant or ambiguous queries, the improvement may not materialize.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: ColdFusion builds on zero-shot anomaly detection as its initial model, requiring understanding of how zero-shot models work without training data
  - Quick check question: How does a zero-shot anomaly detection model compute anomaly scores without any observed data?

- Concept: Median vs mean for robust statistics
  - Why needed here: ColdFusion uses median adaptation to handle contaminated observations, requiring understanding of why median is more robust than mean
  - Quick check question: Why would using median instead of mean be more effective when some observations might be anomalies?

- Concept: Feature embedding and distance metrics
  - Why needed here: The method relies on computing distances between observation embeddings and class embeddings, requiring understanding of how embeddings capture semantic similarity
  - Quick check question: What properties should a good distance metric have for comparing text embeddings in anomaly detection?

## Architecture Onboarding

- Component map:
  Feature encoder (GTE or MPNet) → Observation assignment → Median adaptation → Anomaly scoring
  - External dependency: ChatGPT for query generation

- Critical path: Feature encoding → Assignment → Adaptation → Scoring (each step must complete before next)

- Design tradeoffs:
  - Single iteration vs multiple iterations: ColdFusion uses single iteration for computational efficiency, trading off potential refinement
  - Median vs mean: Median chosen for contamination robustness, trading off some information from non-anomalous outliers
  - L2 vs cosine distance: L2 used as distance metric, trading off potentially different semantic distance properties

- Failure signatures:
  - Poor initial zero-shot performance suggests feature encoder or ChatGPT query generation issues
  - Degrading performance over time suggests high contamination or observation distribution shift
  - Class imbalance in assignments suggests poor feature separation or insufficient observation diversity

- First 3 experiments:
  1. Verify zero-shot baseline performance with both naive intent names and ChatGPT-generated queries on a small subset
  2. Test assignment step with known clean observations to verify correct class assignment
  3. Measure adaptation effectiveness by comparing pre/post adaptation anomaly scores on validation set with ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ColdFusion's performance scale with larger numbers of classes (K)?
- Basis in paper: [inferred] The paper only tests on datasets with 77 classes (Banking77-OOS) and 10-15 classes (CLINC). The method's complexity depends on class assignment and adaptation steps.
- Why unresolved: The paper does not report performance on datasets with significantly more classes, leaving uncertainty about how well the method generalizes to scenarios with hundreds or thousands of classes.
- What evidence would resolve it: Experiments on datasets with 100+ classes showing consistent performance improvements over baselines.

## Limitations

- Uncertainty about performance with contamination ratios beyond 7.5%, as the method's robustness threshold is not characterized
- Dependency on ChatGPT for query generation introduces variability that may affect reproducibility
- Single iteration approach may limit potential performance gains compared to multi-iteration refinement

## Confidence

- High Confidence: Core mechanism claims are well-supported by theoretical justification and experimental evidence
- Medium Confidence: Temporal improvement claims are demonstrated but could benefit from more extensive characterization across different scenarios
- Low Confidence: Feature encoder agnosticism is based on limited testing with only two embedding models

## Next Checks

1. Stress test contamination limits: Systematically evaluate ColdFusion's performance with contamination ratios ranging from 0% to 30% in 5% increments to identify the breaking point of the median adaptation mechanism.

2. Prompt sensitivity analysis: Test ColdFusion with multiple sets of ChatGPT-generated queries for the same intent names (using different prompts and model versions) to quantify the variability introduced by the query generation step.

3. Temporal stability check: Monitor performance not just over time t, but also across multiple independent runs with different random seeds to ensure the observed improvement is consistent and not an artifact of specific data ordering.