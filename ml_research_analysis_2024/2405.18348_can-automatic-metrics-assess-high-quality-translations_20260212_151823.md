---
ver: rpa2
title: Can Automatic Metrics Assess High-Quality Translations?
arxiv_id: '2405.18348'
source_url: https://arxiv.org/abs/2405.18348
tags:
- metrics
- translation
- translations
- quality
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current MT metrics show poor ability to distinguish high-quality
  translations, particularly when evaluating multiple translations of the same source.
  They also struggle to reliably detect translations with zero errors as marked by
  human evaluators using the MQM framework.
---

# Can Automatic Metrics Assess High-Quality Translations?

## Quick Facts
- arXiv ID: 2405.18348
- Source URL: https://arxiv.org/abs/2405.18348
- Reference count: 21
- Current MT metrics struggle to distinguish high-quality translations and detect error-free translations

## Executive Summary
This paper investigates whether automatic machine translation metrics can effectively assess high-quality translations, particularly when multiple translations of the same source exist. The authors evaluate a range of reference-based and quality estimation metrics using WMT 2022/2023 datasets annotated with MQM scores. They find that while metrics perform reasonably well for general translation quality assessment, their ability deteriorates significantly when evaluating high-quality translations (MQM > -5) or detecting translations with zero errors. GPT-4-based GEMBA-MQM achieves the best F1 score for detecting error-free translations but shows a preference bias toward GPT-4 outputs, raising concerns about its practical utility.

## Method Summary
The authors evaluate automatic MT metrics using MQM (Multidimensional Quality Metrics) human annotations from WMT 2022/2023 datasets. They analyze metric performance across three configurations: all translations, high-quality translations only, and translations grouped by source. The study employs Spearman and Pearson correlations to assess metric scores against MQM ratings, and precision/recall/F1 metrics to evaluate the detection of zero-error translations. They also examine score distributions and analyze potential biases toward specific translation systems.

## Key Results
- Automatic metrics show significantly lower correlation with MQM scores when evaluating high-quality translations compared to general translations
- All tested metrics struggle to reliably detect translations with zero errors (MQM = 0)
- GEMBA-MQM (GPT-4-based) achieves highest F1 score for zero-error detection but exhibits preference bias toward GPT-4 outputs
- Reference-based metrics (BLEU, COMET, etc.) perform similarly across configurations, with no clear advantage over quality estimation metrics

## Why This Works (Mechanism)
None

## Foundational Learning

**MQM (Multidimensional Quality Metrics)**
- Why needed: Provides fine-grained human annotations for translation quality across multiple error dimensions
- Quick check: Verify MQM scores range from positive (good) to negative (poor) values, with 0 indicating zero errors

**Reference-based vs Quality Estimation metrics**
- Why needed: Different approaches to MT evaluation - reference-based needs human reference translations, QE doesn't
- Quick check: Identify whether metrics require reference translations as input

**Spearman vs Pearson correlation**
- Why needed: Spearman measures rank correlation (order), Pearson measures linear correlation (absolute values)
- Quick check: Spearman better for ordinal quality rankings, Pearson for absolute score alignment

## Architecture Onboarding

**Component Map**
WMT Datasets -> MQM Annotations -> Metric Score Computation -> Correlation Analysis -> Zero-Error Detection Analysis -> Bias Analysis

**Critical Path**
MQM annotations → Metric computation → Correlation analysis → Performance evaluation

**Design Tradeoffs**
- Reference-based metrics: More interpretable but require human references
- Quality estimation metrics: More flexible but potentially less reliable
- GPT-4-based metrics: Potentially more accurate but show system bias

**Failure Signatures**
- Low correlation in GROUP-BY-SRC setting indicates metrics optimized for pairwise, not multi-source comparison
- High false positive rate for zero-error detection suggests metrics overrate translation quality
- Consistent score inflation for specific systems indicates bias

**Three First Experiments**
1. Compute baseline correlations for all metrics on entire WMT 2022 dataset
2. Compare score distributions for high-quality vs general translations
3. Test GEMBA-MQM on GPT-4 outputs vs other system outputs

## Open Questions the Paper Calls Out

**Open Question 1**
How can automatic metrics be improved to better distinguish between high-quality translations for the same source, particularly when the translations have similar quality scores? The paper identifies this limitation but doesn't propose specific solutions.

**Open Question 2**
To what extent does the preference bias of LLMs towards their own outputs affect the reliability of automatic metrics in practical translation evaluation scenarios? The paper finds bias but doesn't quantify its practical impact.

**Open Question 3**
How do different domains within a dataset affect the performance of automatic metrics in detecting high-quality translations with zero errors? The paper mentions domain sensitivity but doesn't analyze domain-specific effects.

**Open Question 4**
What other potential biases exist in automatic metrics beyond the identified preference for specific systems or outputs, such as biases related to translation length or stylistic choices? The paper acknowledges unknown biases but doesn't comprehensively analyze them.

## Limitations

- Evaluation limited to specific WMT MQM datasets which may not represent all translation scenarios
- Analysis focuses on pairwise comparisons and may not generalize to broader quality assessment contexts
- Relies on existing metric implementations, inheriting their potential biases and limitations

## Confidence

- **High Confidence**: Metrics struggle to distinguish high-quality translations from lower-quality ones
- **Medium Confidence**: GPT-4-based GEMBA-MQM shows bias toward GPT-4 outputs
- **Medium Confidence**: Metrics fail to reliably detect zero-error translations

## Next Checks

1. Test the same evaluation protocol on additional MQM datasets from other MT evaluation campaigns to assess generalization beyond current dataset selection

2. Conduct detailed analysis of system-level biases by examining whether metrics consistently favor certain model families, controlling for known quality differences

3. Replicate analysis using different quality thresholds (e.g., MQM > -2, -3, -10) to determine if limitations are specific to -5 threshold or represent general challenges