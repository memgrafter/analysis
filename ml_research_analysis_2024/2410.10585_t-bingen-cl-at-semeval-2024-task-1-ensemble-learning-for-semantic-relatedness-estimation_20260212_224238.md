---
ver: rpa2
title: "T\xFCbingen-CL at SemEval-2024 Task 1:Ensemble Learning for Semantic Relatedness\
  \ Estimation"
arxiv_id: '2410.10585'
source_url: https://arxiv.org/abs/2410.10585
tags:
- semantic
- relatedness
- sentence
- word
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes an ensemble learning approach to estimate semantic
  relatedness of sentence pairs, hypothesizing that relatedness extends beyond similarity.
  The system integrates multiple models including statistical textual features, word
  embeddings, and pre-trained sentence encoders.
---

# Tübingen-CL at SemEval-2024 Task 1: Ensemble Learning for Semantic Relatedness Estimation

## Quick Facts
- arXiv ID: 2410.10585
- Source URL: https://arxiv.org/abs/2410.10585
- Reference count: 11
- The ensemble achieved 0.850 Spearman correlation on English supervised task

## Executive Summary
This paper proposes an ensemble learning approach to estimate semantic relatedness of sentence pairs, hypothesizing that relatedness extends beyond similarity. The system integrates multiple models including statistical textual features, word embeddings, and pre-trained sentence encoders. For supervised learning, features from GloVe, BERT, MPNet, Jina, and T5 embeddings were combined with textual statistics and fine-tuned using SVM regression. Unsupervised models combined sentence representations from QA-trained models and multilingual BERT with statistical features. The ensemble achieved 0.850 Spearman correlation on English supervised task, outperforming individual models. Unsupervised ensemble scored 0.837 for English, 0.705 for Spanish, and 0.649 for Hindi, demonstrating effectiveness without labeled relatedness data.

## Method Summary
The method combines statistical textual features (character distance ratio, word overlap ratio, content word overlap ratio) with multiple embedding models (GloVe, BERT layers, MPNet, Jina, T5) using ensemble learning. For the supervised task, an SVM regression model is trained on concatenated features from cosine similarities and statistical features. The unsupervised approach averages cosine distances and ratios from multiple models without using labeled data. PCA transformation is applied to embeddings before calculating similarities, and the final prediction is obtained through ensemble regression or averaging depending on the task type.

## Key Results
- Ensemble achieved 0.850 Spearman correlation on English supervised task
- Unsupervised ensemble scored 0.837 for English, 0.705 for Spanish, and 0.649 for Hindi
- Ensemble methods outperformed single models in semantic relatedness estimation
- Performance varied across languages, with significant degradation for Hindi

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensemble learning improves semantic relatedness estimation by combining complementary model strengths.
- Mechanism: Multiple models trained on different datasets and architectures capture distinct aspects of semantic relatedness. The ensemble aggregates their outputs, reducing individual model errors and capturing broader semantic associations beyond simple similarity.
- Core assumption: Semantic relatedness can be decomposed into multiple complementary signals that individual models capture to varying degrees.
- Evidence anchors:
  - [abstract]: "ensemble models outperform many individual systems in estimating semantic relatedness"
  - [section]: "we hypothesize that semantic relatedness can be inferred from a multitude of sources and therefore propose an ensemble approach that integrates outcomes from diverse systems"
  - [corpus]: Weak - corpus neighbors focus on similar ensemble approaches but don't directly validate the decomposition assumption
- Break condition: If all component models capture the same semantic features, the ensemble provides no additional benefit and may even degrade performance due to noise aggregation.

### Mechanism 2
- Claim: Statistical textual features provide meaningful signal for semantic relatedness beyond embeddings.
- Mechanism: Simple features like content word overlap and character distance ratios capture surface-level semantic associations that sophisticated embeddings might miss, especially for relatedness cases that don't require deep semantic understanding.
- Core assumption: Not all semantic relatedness requires complex contextual understanding; some cases are well-captured by simple statistical patterns.
- Evidence anchors:
  - [section]: "we found that the overlap ratio computed solely on content words shows a better correlation with the human judgment of relatedness"
  - [section]: "the combination of two or more results may improve the relatedness estimation"
  - [corpus]: Weak - corpus shows other teams use similar features but doesn't validate their specific effectiveness
- Break condition: When semantic relatedness requires deep contextual understanding that statistical features cannot capture, their contribution becomes negligible or misleading.

### Mechanism 3
- Claim: Different pre-training objectives produce complementary sentence representations for relatedness tasks.
- Mechanism: Models trained on different tasks (semantic search, QA pairs, general text-to-text) develop distinct representational biases that collectively capture a broader space of semantic relationships than any single objective.
- Core assumption: Semantic relatedness encompasses multiple semantic relationship types, each best captured by specific pre-training objectives.
- Evidence anchors:
  - [section]: "we aim to evaluate whether models trained on other types of datasets intended for different purposes could generate representations suitable for estimating semantic relatedness"
  - [section]: "models trained on diverse datasets with different architectures, they may produce varied predictions on semantic relatedness, and combining them may improve overall performance"
  - [corpus]: Moderate - corpus shows similar diversity in model selection but doesn't validate complementary capture
- Break condition: If pre-training objectives overlap significantly in what they capture, the ensemble gains from diversity are minimal.

## Foundational Learning

- Concept: Spearman correlation as evaluation metric
  - Why needed here: The task specifically evaluates systems using Spearman's rank correlation, requiring understanding of how it measures monotonic relationships rather than linear ones
  - Quick check question: Why might Spearman correlation be preferred over Pearson correlation for semantic relatedness tasks?

- Concept: PCA transformation for embeddings
  - Why needed here: The paper uses PCA to transform embeddings for improved correlation, requiring understanding of how it captures maximum variance in reduced dimensions
  - Quick check question: What's the difference between using PCA for dimensionality reduction versus using it to find optimal feature space for correlation?

- Concept: Ensemble methods in machine learning
  - Why needed here: The core approach relies on combining multiple models, requiring understanding of how different ensemble strategies (averaging, weighted voting, stacking) affect performance
  - Quick check question: When would a simple average ensemble be preferable to a weighted or stacked ensemble approach?

## Architecture Onboarding

- Component map: Statistical feature extractor → Word embedding models (GloVe, BERT variants) → Sentence embedding models (MPNet, Jina, T5) → Ensemble combiner (SVM regression) → Final relatedness score
- Critical path: Feature extraction → Model inference → Feature concatenation → Ensemble regression → Score output
- Design tradeoffs: Simple statistical features add robustness but computational overhead; diverse model selection increases coverage but complexity; SVM regression handles non-linear relationships but requires tuning
- Failure signatures: Individual model underperformance in specific languages (as seen with Hindi), statistical features not capturing complex semantics, ensemble overfitting to training data
- First 3 experiments:
  1. Evaluate individual model performance on validation set to identify weak components
  2. Test statistical feature correlations with human annotations to validate their contribution
  3. Run ensemble with different combinations to identify optimal model subsets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ensemble learning methods that combine statistical features, word embeddings, and sentence representations effectively estimate semantic relatedness for low-resource languages?
- Basis in paper: [explicit] The authors observed poor performance for Hindi and other non-Indo-European languages, noting that multilingual models are biased toward English and Indo-European languages.
- Why unresolved: The paper only tested a limited set of non-Indo-European languages (Hindi) and found performance degradation, but did not explore whether different ensemble strategies or additional features could improve results for these languages.
- What evidence would resolve it: Testing ensemble approaches with expanded feature sets (including language-specific statistical features, culturally relevant training data, or domain adaptation techniques) across multiple low-resource languages, measuring Spearman correlation improvements.

### Open Question 2
- Question: Do contextual word embeddings consistently outperform non-contextual embeddings for semantic relatedness estimation, or is this task-dependent?
- Basis in paper: [explicit] The authors found that contextualized BERT embeddings did not necessarily outperform uncontextualized embeddings like GloVe in their semantic relatedness task, contrary to expectations from similarity tasks.
- Why unresolved: The paper conducted preliminary analysis comparing different embedding types but did not systematically explore why contextual embeddings performed worse, nor did they test alternative contextualization methods or fine-tuning approaches.
- What evidence would resolve it: Controlled experiments comparing contextual and non-contextual embeddings across multiple semantic tasks, including ablation studies to identify which aspects of contextualization help or hinder relatedness estimation.

### Open Question 3
- Question: Can unsupervised ensemble models achieve comparable performance to supervised models for semantic relatedness when trained on alternative data sources beyond question-answer pairs?
- Basis in paper: [explicit] The unsupervised ensemble achieved good results (0.837 Spearman for English) using models trained on question-answer pairs, title-passage pairs, and post-comment pairs, suggesting semantic relatedness can be inferred without labeled similarity data.
- Why unresolved: The paper tested only a limited set of alternative training data sources and did not explore the full space of potential training data types or combinations that might yield even better unsupervised performance.
- What evidence would resolve it: Systematic evaluation of unsupervised models trained on diverse data sources (e.g., conversational data, image captions, code documentation) and their ensemble combinations, comparing performance against supervised baselines across multiple languages.

## Limitations

- Model selection bias: The ensemble's effectiveness may be influenced by specific model choices rather than the ensemble approach itself.
- Statistical feature validity: The three statistical features may not generalize well across languages with different morphological structures.
- Language-specific performance: Significant performance drop for Hindi suggests the approach may be biased toward high-resource languages.

## Confidence

- High confidence: Ensemble learning improves semantic relatedness estimation over individual models, as evidenced by the 0.850 Spearman correlation on English supervised task.
- Medium confidence: Statistical textual features provide meaningful signal beyond embeddings, supported by improved performance when combining features.
- Medium confidence: Different pre-training objectives produce complementary representations, based on the ensemble approach.

## Next Checks

1. **Ablation study**: Systematically remove individual models from the ensemble to quantify their specific contributions and identify whether all 10 models are necessary for optimal performance.

2. **Cross-lingual transfer validation**: Test the ensemble approach on languages not in the original dataset to verify the claimed effectiveness of the multilingual models and identify potential language-specific biases.

3. **Statistical feature sensitivity analysis**: Evaluate how each statistical feature (character distance ratio, word overlap ratio, content word overlap ratio) correlates with human judgments across different language pairs to determine their individual and combined effectiveness.