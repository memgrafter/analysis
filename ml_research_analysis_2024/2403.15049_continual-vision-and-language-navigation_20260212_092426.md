---
ver: rpa2
title: Continual Vision-and-Language Navigation
arxiv_id: '2403.15049'
source_url: https://arxiv.org/abs/2403.15049
tags:
- learning
- continual
- navigation
- cvln
- replay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Continual Vision-and-Language Navigation
  (CVLN), addressing the limitation of existing VLN agents that assume a train-once-deploy-once
  strategy. In CVLN, agents learn incrementally across multiple scene domains, mimicking
  real-world deployment where agents encounter novel environments over time.
---

# Continual Vision-and-Language Navigation

## Quick Facts
- arXiv ID: 2403.15049
- Source URL: https://arxiv.org/abs/2403.15049
- Reference count: 40
- Key outcome: This paper introduces Continual Vision-and-Language Navigation (CVLN), addressing the limitation of existing VLN agents that assume a train-once-deploy-once strategy. In CVLN, agents learn incrementally across multiple scene domains, mimicking real-world deployment where agents encounter novel environments over time. The paper proposes two CVLN settings: Initial-instruction based CVLN (I-CVLN) and Dialogue-based CVLN (D-CVLN), constructed from existing VLN datasets. Two novel methods are introduced: Perplexity Replay (PerpR), which prioritizes challenging episodes based on action perplexity for replay memory, and Episodic Self-Replay (ESR), which stores and revisits action logits for sequential decision-making. Experimental results show that PerpR and ESR outperform traditional continual learning methods, with ESR achieving 31.9% average success rate on I-CVLN and PerpR achieving 28.9% on D-CVLN, compared to 25.6% and 5.5% for vanilla agents respectively. The study demonstrates the effectiveness of replay memory in mitigating catastrophic forgetting in CVLN tasks.

## Executive Summary
This paper introduces Continual Vision-and-Language Navigation (CVLN), a new task where navigation agents learn incrementally across multiple scene domains without catastrophic forgetting. The authors construct two CVLN settings from existing VLN datasets: Initial-instruction based CVLN (I-CVLN) using R2R and RxR, and Dialogue-based CVLN (D-CVLN) using CVDN. Two novel methods are proposed: Perplexity Replay (PerpR) which uses action perplexity to prioritize challenging episodes for replay memory, and Episodic Self-Replay (ESR) which stores and revisits action logits for sequential decision-making. Experiments show that both methods significantly outperform traditional continual learning approaches, with ESR achieving 31.9% average success rate on I-CVLN and PerpR achieving 28.9% on D-CVLN, compared to 25.6% and 5.5% for vanilla agents respectively.

## Method Summary
The paper proposes two continual learning methods for CVLN: Perplexity Replay (PerpR) and Episodic Self-Replay (ESR). PerpR evaluates episode difficulty using action perplexity (AP) and prioritizes challenging episodes for replay memory storage. ESR stores action logits at each step during training and revisits these stored logits during subsequent learning phases. Both methods use rehearsal mechanisms to mitigate catastrophic forgetting when learning across multiple scene domains. The methods are evaluated on two CVLN settings: I-CVLN (using R2R and RxR datasets) and D-CVLN (using CVDN), with agents trained sequentially on 20 scene domains for I-CVLN and 11 for D-CVLN.

## Key Results
- ESR achieves 31.9% average success rate on I-CVLN, compared to 25.6% for vanilla agents
- PerpR achieves 28.9% average goal progress on D-CVLN, compared to 5.5% for vanilla agents
- Both methods outperform traditional continual learning approaches (L2, AdapterCL, AGEM, RandR)
- ESR demonstrates better stability-plasticity trade-off than PerpR in I-CVLN

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity Replay (PerpR) improves continual learning by prioritizing episodes with high action perplexity for replay memory.
- Mechanism: The agent evaluates each episode's difficulty by computing action perplexity (AP) based on the negative log-likelihood of predicted actions. Episodes with high AP are retained in replay memory, ensuring the agent revisits challenging experiences that indicate insufficient learning.
- Core assumption: High action perplexity correlates with episodes that are most beneficial for future learning and knowledge retention.
- Evidence anchors:
  - [abstract]: "PerpR uses self-evaluated action perplexity (i.e. difficulty) to assess the importance of episodes, effectively selecting and organizing them in the replay memory based on their action perplexity."
  - [section 4.1]: "PerpR introduces Action Perplexity (AP) to let the model self-evaluate episode difficulty without external metrics or annotations."
  - [corpus]: Weak evidence - no direct comparisons to other perplexity-based methods in CVLN literature.
- Break condition: If action perplexity does not correlate with learning difficulty, or if high-perplexity episodes are not representative of domain-specific challenges.

### Mechanism 2
- Claim: Episodic Self-Replay (ESR) enables continual learning by storing and revisiting action logits at each step during training.
- Mechanism: ESR stores the action logits predicted by the agent for each individual step within episodes in replay memory. During subsequent training, the agent revisits these stored logits, allowing it to refine its learning by leveraging detailed episode step-level logits from past experiences.
- Core assumption: Step-level action logits contain sufficient information to guide the agent's behavior and prevent catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "ESR, which stores and revisits action logits for sequential decision-making."
  - [section 4.2]: "ESR stores the sequential action logits Zâ‰œ{z n}N n=1 corresponding to each step in the sequence, when updating the replay memory M."
  - [corpus]: Weak evidence - limited discussion of step-level logit replay in CVLN context.
- Break condition: If stored logits become outdated or if the agent's architecture changes significantly between training phases.

### Mechanism 3
- Claim: The combination of L_M (ground truth action loss) and L_ESR (logit distillation loss) in ESR provides synergistic benefits for continual learning.
- Mechanism: ESR uses both the ground truth actions from replay memory (L_M) and the stored action logits (L_ESR) to train the agent. This dual approach reinforces consistency with past behavior while maintaining alignment with correct actions.
- Core assumption: The combination of ground truth actions and predicted logits provides complementary information that enhances learning.
- Evidence anchors:
  - [section 4.2]: "ESR employs L M, utilizing ground truth actions from replay memory."
  - [section 5.5]: "We observe a synergistic effect when using both types of losses obtained through replay memory."
  - [corpus]: Weak evidence - no direct comparisons to single-loss approaches in CVLN literature.
- Break condition: If one of the loss components becomes irrelevant or if the combination leads to conflicting gradients.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: CVLN agents must learn new scene domains without losing knowledge of previous ones, making catastrophic forgetting a central challenge.
  - Quick check question: What happens to an agent's performance on old tasks when it's trained on new tasks without any forgetting mitigation?

- Concept: Replay memory mechanisms in continual learning
  - Why needed here: Both PerpR and ESR rely on replay memory to store and revisit past experiences, which is crucial for retaining knowledge across scene domains.
  - Quick check question: How does the size of replay memory affect an agent's ability to retain knowledge from previous scene domains?

- Concept: Action perplexity as a measure of learning difficulty
  - Why needed here: PerpR uses action perplexity to assess episode difficulty and prioritize which experiences to store in replay memory.
  - Quick check question: How is action perplexity calculated, and why would high perplexity indicate a challenging episode?

## Architecture Onboarding

- Component map:
  - Vision-and-Language Navigation backbone (VLN-BERT or HAMT)
  - Action perplexity computation module (for PerpR)
  - Logit storage and retrieval system (for ESR)
  - Replay memory management system
  - Continual learning training loop

- Critical path:
  1. Agent processes episode and computes action perplexity (PerpR) or stores logits (ESR)
  2. Replay memory is updated based on perplexity scores or random sampling
  3. During training on new scene domain, agent samples from replay memory
  4. Agent computes combined loss (current domain + replay memory)
  5. Agent parameters are updated using the combined loss

- Design tradeoffs:
  - Replay memory size vs. computational efficiency
  - Perplexity-based sampling vs. random sampling for memory updates
  - Step-level logit storage vs. episode-level storage
  - Ground truth action loss vs. logit distillation loss weighting

- Failure signatures:
  - Performance degradation on previously learned scene domains
  - Inability to learn new scene domains effectively
  - High variance in performance across different scene domains
  - Memory management issues (e.g., replay memory overflow or underflow)

- First 3 experiments:
  1. Compare vanilla agent performance vs. PerpR and ESR on a single scene domain to establish baseline effectiveness
  2. Test different replay memory sizes (e.g., 100, 500, 1000 samples) to find optimal configuration
  3. Evaluate stability-plasticity tradeoff by measuring performance on old vs. new scene domains after sequential training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CVLN agents scale when learning across hundreds of scene domains rather than the 20-22 domains used in this study?
- Basis in paper: [inferred] The paper uses 20 scene domains for I-CVLN and 11 for D-CVLN, with experimental results showing performance degrades as more domains are learned. The authors mention this as a direction for future work.
- Why unresolved: The study only evaluates performance up to 20-22 scene domains. Real-world deployment would likely involve learning from hundreds or thousands of domains over time.
- What evidence would resolve it: Extended experiments training agents on 100+ scene domains, measuring average metrics and stability-plasticity trade-off as domains increase.

### Open Question 2
- Question: Would PerpR or ESR methods benefit from incorporating domain-specific information during evaluation, rather than requiring domain-agnostic generalization?
- Basis in paper: [explicit] The paper notes that CVLN fits within Domain-Incremental Learning where "no domain identity during inference" is required, but this is a design choice rather than a necessity.
- Why unresolved: The current setup forces agents to generalize without domain knowledge at test time, which may be unnecessarily restrictive. Allowing domain identity during evaluation could simplify the problem.
- What evidence would resolve it: Comparative experiments testing PerpR and ESR with and without domain identity available during evaluation, measuring performance differences.

### Open Question 3
- Question: How would continuous environment learning (where scene domains gradually blend rather than appear discretely) affect the performance of PerpR and ESR?
- Basis in paper: [explicit] The authors state "For future work, we plan to extend CVLN to continuous environments and design new settings and datasets specifically for VLN, beyond existing dataset-based benchmarks."
- Why unresolved: The current CVLN formulation assumes discrete scene domains that are learned sequentially. Real-world environments likely transition more gradually.
- What evidence would resolve it: New benchmark datasets with gradual environmental transitions and experiments comparing PerpR/ESR performance on discrete vs. continuous domain scenarios.

### Open Question 4
- Question: What is the optimal balance between stability and plasticity for CVLN agents across different task complexities and environment types?
- Basis in paper: [explicit] The paper analyzes stability-plasticity trade-off using harmonic mean, showing ESR achieves better balance in I-CVLN while PerpR performs comparably in D-CVLN.
- Why unresolved: The study provides a single stability-plasticity analysis but doesn't explore how this balance should vary based on task difficulty, environment complexity, or domain similarity.
- What evidence would resolve it: Systematic experiments varying task complexity and environment similarity, measuring optimal stability-plasticity ratios for different conditions.

## Limitations
- The empirical evaluation relies on self-constructed datasets from existing VLN benchmarks, raising questions about generalizability to truly novel environments.
- Performance gaps between proposed methods and baselines may be partially attributable to implementation differences rather than core mechanisms.
- The paper does not address computational overhead or memory requirements for the replay-based approaches.

## Confidence
- **High**: The core premise that existing VLN agents assume a train-once-deploy-once strategy and that continual learning is necessary for real-world deployment.
- **Medium**: The effectiveness of perplexity-based replay memory prioritization, as the evidence is primarily empirical without extensive ablation studies.
- **Low**: The generalizability of results to more complex, dynamic environments beyond the static indoor scenes used in evaluation.

## Next Checks
1. **Cross-Dataset Validation**: Test PerpR and ESR on a different VLN dataset (e.g., HM3D) to assess generalization beyond the R2R/RxR/CVDN benchmarks.
2. **Ablation Studies**: Systematically remove either the perplexity-based sampling (for PerpR) or the logit storage (for ESR) to quantify their individual contributions.
3. **Memory Efficiency Analysis**: Measure the computational overhead and memory usage of replay-based methods compared to baselines across different memory sizes.