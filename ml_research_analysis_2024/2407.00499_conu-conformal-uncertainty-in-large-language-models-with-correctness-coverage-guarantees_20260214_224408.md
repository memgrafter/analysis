---
ver: rpa2
title: 'ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage
  Guarantees'
arxiv_id: '2407.00499'
source_url: https://arxiv.org/abs/2407.00499
tags:
- uncertainty
- correctness
- coverage
- test
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to quantify uncertainty in black-box
  large language models (LLMs) for open-ended natural language generation tasks. The
  core idea is to use conformal prediction to convert a heuristic uncertainty measure
  into rigorous correctness coverage guarantees.
---

# ConU: Conformal Uncertainty in Large Language Models with Correctness Coverage Guarantees

## Quick Facts
- **arXiv ID**: 2407.00499
- **Source URL**: https://arxiv.org/abs/2407.00499
- **Reference count**: 15
- **Primary result**: Conformal prediction converts heuristic uncertainty measures into rigorous correctness coverage guarantees for LLM uncertainty quantification

## Executive Summary
This paper addresses uncertainty quantification in black-box large language models for open-ended natural language generation tasks. The authors propose a method that uses conformal prediction to convert a heuristic uncertainty measure into rigorous correctness coverage guarantees. The approach clusters sampled generations from the LLM and defines an uncertainty score based on frequency and semantic consistency, then aligns this with correctness via a non-conformity score to derive calibrated prediction sets. Experiments demonstrate that the method outperforms prior approaches in distinguishing correct from incorrect generations while strictly controlling correctness coverage rates under user-specified error rates.

## Method Summary
The method first samples multiple generations from the LLM for each prompt, then performs semantic clustering on these generations. An uncertainty score is calculated for each generation based on its frequency of occurrence and semantic consistency with other clusters. This uncertainty measure is aligned with correctness through a non-conformity score that identifies the generation most similar to the reference answer among semantically equivalent candidates. Conformal prediction is then applied to calibrate prediction sets using a threshold derived from calibration data, ensuring that the sets contain correct answers with at least the specified probability. The approach requires only a black-box LLM interface and can be applied without model-specific modifications.

## Key Results
- The proposed uncertainty measure achieves higher AUROC scores than prior methods in distinguishing correct from incorrect generations
- Calibrated prediction sets strictly control correctness coverage rates at user-specified error thresholds
- The method maintains small average prediction set sizes while providing rigorous coverage guarantees
- Performance is validated across 4 NLG datasets (CoQA, TriviaQA, MedQA, MedMCQA) and 6 different LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conformal prediction converts any heuristic uncertainty measure into rigorous correctness coverage guarantees
- Mechanism: Conformal prediction calibrates prediction sets using a non-conformity score threshold derived from calibration data, ensuring at least a user-specified probability of containing correct answers
- Core assumption: Calibration and test data are exchangeable; at least one correct answer exists in candidate generations
- Evidence anchors:
  - [abstract] "Specifically, the method first clusters sampled generations from the LLM and defines an uncertainty score for each generation based on its frequency and semantic consistency. It then aligns this uncertainty measure with correctness via a non-conformity score and derives a conformal uncertainty criterion to calibrate prediction sets."
  - [section] "We randomly employ N samples to construct the calibration data set {(xi, y*i)}N i=1, and for each calibration sample we demand that at least one sampled generation ˆy(i) j in {ˆy(i) m} M m=1 meets the correctness criterion."
- Break condition: If calibration and test data are not exchangeable, or if no correct answer exists in candidate generations

### Mechanism 2
- Claim: Semantic clustering of sampled generations provides a reliable proxy for uncertainty in black-box LLMs
- Mechanism: More semantically diverse outputs for the same prompt indicate higher uncertainty. The uncertainty score combines frequency of occurrence and semantic consistency between clusters
- Core assumption: Self-consistency theory holds - frequent semantic clusters represent the model's most confident answers
- Evidence anchors:
  - [abstract] "The core idea is to use conformal prediction to convert a heuristic uncertainty measure into rigorous correctness coverage guarantees. Specifically, the method first clusters sampled generations from the LLM and defines an uncertainty score for each generation based on its frequency and semantic consistency."
  - [section] "If a language model generates more semantically diverse outputs for the same prompt, the uncertainty is likely higher... We approximate the model's output distribution by sampling multiple answers to the same question. Then, we perform semantic clustering on the sampled generations, and propose to measure the uncertainty of each generation by combining two factors: the frequency of occurrence of the semantic meaning it conveys, and the consistency between its semantic and other semantic clusters augmented by their individual frequency."
- Break condition: If self-consistency theory doesn't hold for the specific LLM or task domain

### Mechanism 3
- Claim: The non-conformity score (NS) strictly aligned with uncertainty conditions of correct answers provides robust correctness coverage guarantees
- Mechanism: NS is defined as the uncertainty score of the generation that has highest similarity with the reference answer among semantically equivalent generations. This alignment ensures prediction sets cover correct answers with at least 1-α probability
- Core assumption: Semantic equivalence between generation and reference answer can be accurately determined
- Evidence anchors:
  - [abstract] "We propose a sampling-based uncertainty measure leveraging self-consistency and develop a conformal uncertainty criterion by integrating the uncertainty condition aligned with correctness into the design of the CP algorithm."
  - [section] "Based on the uncertainty measure described as Eq. (1), we define the NS of the i-th calibration sample as ri = r (xi, y*i) = U(argmaxˆy(i) j∈{ˆy(i) m}M m=1 S(ˆy(i) j, y*i)E(ˆy(i) j, y*i)), where E (·, ·) is the indicator function determining whether the two sentences share equivalent semantics."
- Break condition: If semantic equivalence cannot be accurately determined, or if the NS threshold doesn't properly capture uncertainty conditions

## Foundational Learning

- Concept: Conformal prediction theory and split conformal prediction framework
  - Why needed here: The method relies on conformal prediction to convert heuristic uncertainty measures into rigorous guarantees. Understanding the theory is essential for implementing and modifying the approach.
  - Quick check question: What are the key assumptions of split conformal prediction, and how does the method ensure these assumptions are met?

- Concept: Semantic clustering and similarity measures for natural language
  - Why needed here: The uncertainty measure depends on clustering sampled generations and calculating semantic similarity between them. Understanding these techniques is crucial for implementing the method.
  - Quick check question: How does the method determine semantic equivalence between generations, and what are potential failure modes of this approach?

- Concept: Exchangeability of calibration and test data
  - Why needed here: The correctness coverage guarantees rely on the assumption that calibration and test data are exchangeable. Understanding this concept is essential for applying the method correctly.
  - Quick check question: What are the implications if calibration and test data are not exchangeable, and how might this affect the method's performance?

## Architecture Onboarding

- Component map:
  Data processing pipeline -> Conformal prediction module -> Evaluation module
  Sampling generations -> Semantic clustering -> Uncertainty score calculation -> Non-conformity score definition -> Quantile calculation -> Prediction set construction -> Correctness coverage rate calculation

- Critical path:
  1. Sample M candidate generations for each prompt
  2. Perform semantic clustering on sampled generations
  3. Calculate uncertainty scores for each generation
  4. Define non-conformity scores using calibration data
  5. Calculate conformal uncertainty criterion quantile
  6. Construct prediction sets for test prompts
  7. Evaluate correctness coverage and prediction efficiency

- Design tradeoffs:
  - Number of sampled generations (M): More generations provide better uncertainty estimates but increase computational cost
  - Semantic similarity model choice: More accurate models improve uncertainty quantification but may be slower or require more resources
  - Calibration set size: Larger sets provide more stable quantile estimates but require more data
  - Semantic equivalence threshold: Stricter thresholds improve precision but may reduce recall

- Failure signatures:
  - Low correctness coverage rate despite proper implementation: Calibration and test data may not be exchangeable
  - High average prediction set size: Uncertainty measure may not be discriminating enough
  - Inconsistent results across different runs: Randomness in sampling or clustering may need better control

- First 3 experiments:
  1. Verify semantic clustering works as expected: Sample 5-10 generations for a few prompts, cluster them manually, and compare with automated results
  2. Test uncertainty score calculation: Calculate uncertainty scores for generations with known semantic relationships and verify they align with expectations
  3. Validate non-conformity score definition: Create synthetic calibration data with known correct answers and verify NS calculation produces expected results

## Open Questions the Paper Calls Out

- **Question**: How can we verify whether the correct answer has been sampled from the unbounded output space in real-world applications?
  - Basis in paper: [explicit] The paper states: "In our study, we assume that at least one correct answer exists in the candidate generations. However, we need to develop a criterion to verify whether the correct answer has been sampled from the unbound output space in real-world applications."
  - Why unresolved: This is explicitly stated as a limitation in the paper, indicating it is an open question for future research.
  - What evidence would resolve it: A proposed method or criterion that can reliably determine if the correct answer is present in the candidate generations sampled from the LLM's output space.

- **Question**: How can the conformal uncertainty criterion be extended to non-exchangeability scenarios?
  - Basis in paper: [explicit] The paper states: "Finally, we will attempt to expand our conformal uncertainty criterion to non-exchangeability scenarios, aiming to establish a general criterion across different NLG tasks."
  - Why unresolved: This is explicitly mentioned as a future direction, suggesting it is an open question.
  - What evidence would resolve it: A successful extension of the conformal uncertainty criterion that works under non-exchangeability conditions, demonstrated through experiments on various NLG tasks.

- **Question**: How does the number of sampled generations (M) affect the performance of uncertainty quantification?
  - Basis in paper: [inferred] The paper discusses the effect of the number of sampled generations on UQ performance, stating: "We investigate the effects of the number of sampled generations (i.e., M) on the performance of UQ."
  - Why unresolved: While the paper explores this, it does not provide a definitive answer on the optimal number of generations or a complete understanding of the relationship between M and UQ performance.
  - What evidence would resolve it: A comprehensive study showing the relationship between the number of sampled generations and UQ performance, including an optimal range or guidelines for selecting M.

## Limitations
- The method requires sampling multiple generations per prompt, which may not scale well for large-scale deployment
- The semantic equivalence threshold determination is not specified, which could significantly impact performance
- Experiments are limited to question-answering tasks; effectiveness for other NLG tasks remains unexplored

## Confidence
- **Mechanism Confidence**: Medium - The semantic clustering approach shows promise but depends heavily on the quality of the semantic similarity model used
- **Calibration Guarantees Confidence**: Medium-High - The conformal prediction framework provides theoretical guarantees, but the paper doesn't thoroughly investigate violation of exchangeability assumptions
- **Evaluation Confidence**: Medium - The evaluation focuses on key metrics but lacks ablation studies on critical design choices

## Next Checks
1. **Sensitivity Analysis**: Test the method with different numbers of sampled generations (M=5, 10, 20, 50) to determine the optimal trade-off between uncertainty estimation quality and computational cost

2. **Threshold Robustness**: Vary the semantic equivalence threshold systematically and measure its impact on both AUROC scores and correctness coverage rates to identify stable operating points

3. **Calibration Set Size Study**: Experiment with different calibration set sizes (1:5, 1:10, 1:20, 1:50 calibration:test ratios) to understand the minimum data requirements for stable conformal guarantees