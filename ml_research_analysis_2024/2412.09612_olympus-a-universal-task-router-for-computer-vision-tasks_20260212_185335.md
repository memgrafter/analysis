---
ver: rpa2
title: 'Olympus: A Universal Task Router for Computer Vision Tasks'
arxiv_id: '2412.09612'
source_url: https://arxiv.org/abs/2412.09612
tags:
- image
- tasks
- arxiv
- olympus
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Olympus, a universal task router that transforms
  multimodal large language models (MLLMs) into a unified framework for handling diverse
  computer vision tasks. Olympus delegates over 20 specialized tasks across images,
  videos, and 3D objects to dedicated external models using task-specific routing
  tokens.
---

# Olympus: A Universal Task Router for Computer Vision Tasks

## Quick Facts
- arXiv ID: 2412.09612
- Source URL: https://arxiv.org/abs/2412.09612
- Reference count: 40
- Primary result: Achieves 94.75% routing accuracy across 20 vision tasks using MLLM controller

## Executive Summary
Olympus introduces a universal task router framework that transforms multimodal large language models (MLLMs) into a unified system for handling diverse computer vision tasks. By leveraging an MLLM as a controller, Olympus delegates over 20 specialized tasks across images, videos, and 3D objects to dedicated external models using task-specific routing tokens. This instruction-based routing enables complex workflows through chained actions without training heavy generative models, achieving an average routing accuracy of 94.75% across 20 tasks and 91.82% precision in chained action scenarios.

## Method Summary
Olympus trains an MLLM (Phi-2 with SigLIP-384 backbone) to predict task-specific routing tokens conditioned on user instructions, which then invoke appropriate external specialist models. The framework uses next-token prediction with cross-entropy loss on instruction-response pairs from the OlympusInstruct dataset (446.3K samples) covering 20 vision tasks. The MLLM generates responses containing routing tokens that direct tasks to external models, enabling chained actions where multiple tasks are handled within a single instruction. The approach leverages modularity to avoid training massive all-in-one models while maintaining flexibility through token-based routing.

## Key Results
- Achieves 94.75% average routing accuracy across 20 vision tasks
- Demonstrates 91.82% precision in chained action scenarios
- Outperforms baseline models on standard multimodal benchmarks
- Successfully solves up to five tasks within a single instruction

## Why This Works (Mechanism)

### Mechanism 1
- MLLMs act as controllers that delegate tasks to external models using task-specific routing tokens
- The framework trains the MLLM to predict routing tokens conditioned on user instructions, which then invoke appropriate external specialist models
- Core assumption: MLLMs can learn to map diverse natural language instructions to the correct task routing tokens

### Mechanism 2
- The framework supports chained actions by enabling sequential routing of multiple tasks within a single instruction
- Task-specific routing tokens allow the MLLM to generate responses that chain multiple task invocations in sequence
- Core assumption: MLLMs can learn to maintain context across multiple chained tasks within a single instruction

### Mechanism 3
- Training with next-token prediction on instruction-response pairs enables effective task routing
- The MLLM learns to generate responses containing routing tokens by predicting the next token conditioned on user instructions
- Core assumption: Next-token prediction with cross-entropy loss is sufficient to learn the mapping from instructions to routing tokens

## Foundational Learning

- **Multimodal instruction tuning**: Why needed - framework requires MLLMs to understand and respond to diverse visual and textual instructions; Quick check - Can the MLLM correctly interpret instructions like "Generate an image of a chihuahua dog dressed in a vibrant, multi-colored costume"?

- **Task routing and delegation**: Why needed - framework relies on MLLMs to route tasks to appropriate external models; Quick check - Can the MLLM generate the correct routing token for a given task, such as `<image_gen>` for image generation?

- **Chain-of-action processing**: Why needed - framework must handle multiple tasks within a single instruction; Quick check - Can the MLLM generate a response containing multiple routing tokens in the correct sequence for a chained instruction?

## Architecture Onboarding

- **Component map**: User instruction → MLLM prediction of routing tokens → External model invocation → Result aggregation → Final response

- **Critical path**: User instruction → MLLM prediction of routing tokens → External model invocation → Result aggregation → Final response

- **Design tradeoffs**: Modularity vs. end-to-end performance - using external models avoids training a massive all-in-one model but adds coordination complexity; Token-based routing vs. direct model calls - tokens provide flexibility but require training the MLLM to predict them accurately

- **Failure signatures**: Incorrect routing token generation → Wrong external model called; Token prediction failure → No external model invoked; Chained token sequence errors → Tasks executed in wrong order or skipped

- **First 3 experiments**: 
  1. Test MLLM routing accuracy on single-task instructions from OlympusBench
  2. Evaluate chained action performance on multi-task instructions
  3. Compare performance with and without routing tokens on multimodal benchmarks

## Open Questions the Paper Calls Out

1. **Scalability beyond 20 tasks**: What are the performance implications of increasing the number of tasks beyond 20 in the Olympus framework? The paper mentions scalability but does not provide empirical data on performance with more than 20 tasks.

2. **Dataset quality impact**: How does the quality and diversity of the GPT-4o-generated instruction dataset impact the performance of Olympus? The paper acknowledges that dataset quality affects performance but does not investigate how variations in dataset quality or diversity affect Olympus's routing accuracy and task completion rates.

3. **Optimal prompt configuration**: What is the optimal balance between the number of prefixes, phrases, and example pairs in task-specific prompts for maximum success rate? The paper shows that increasing these components generally improves performance up to a point but does not identify the precise optimal combination.

## Limitations

- Dataset generalization concerns due to reliance on GPT-4o-generated synthetic data rather than human-annotated examples
- Specialist model dependencies create potential single points of failure if external components underperform or become unavailable
- Unclear bounds on chained action complexity - performance degradation with increasing task sequence length is not fully characterized

## Confidence

- **High Confidence**: MLLM controller achieves 94.75% average routing accuracy; Olympus outperforms baseline models on benchmarks; Chained actions achieve 91.82% precision
- **Medium Confidence**: Universal task routing capability; Seamless MLLM integration; Ability to solve up to five tasks in single instruction
- **Low Confidence**: Robustness to real-world instruction diversity; Scalability to arbitrary chained tasks; Performance consistency across different MLLM backbones

## Next Checks

1. **Cross-Domain Instruction Testing**: Evaluate Olympus performance on human-annotated instruction datasets that differ in style, complexity, and phrasing from the GPT-4o-generated OlympusInstruct data.

2. **Specialist Model Failure Analysis**: Systematically remove or degrade individual specialist models to quantify the impact on overall framework performance and test fallback mechanisms.

3. **Chain Length Scalability Test**: Design instruction chains of increasing length (2, 3, 5, 10 tasks) and measure routing accuracy, response quality, and execution time to identify performance degradation points.