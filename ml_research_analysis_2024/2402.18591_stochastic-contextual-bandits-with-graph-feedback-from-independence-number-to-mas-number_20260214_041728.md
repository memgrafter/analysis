---
ver: rpa2
title: 'Stochastic contextual bandits with graph feedback: from independence number
  to MAS number'
arxiv_id: '2402.18591'
source_url: https://arxiv.org/abs/2402.18591
tags:
- feedback
- graph
- bound
- bandits
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies stochastic contextual bandits with graph feedback,
  where taking an action reveals rewards for neighboring actions in a feedback graph
  across all contexts. The key result is a regret lower bound $\Omega(\sqrt{\betaM(G)T})$,
  where $\betaM(G)$ is a graph-theoretic quantity that interpolates between the independence
  number $\alpha(G)$ and maximum acyclic subgraph number $m(G)$ as the number of contexts
  $M$ varies.
---

# Stochastic contextual bandits with graph feedback: from independence number to MAS number

## Quick Facts
- arXiv ID: 2402.18591
- Source URL: https://arxiv.org/abs/2402.18591
- Reference count: 40
- Key result: Regret lower bound Ω(√βM(G)T) where βM(G) interpolates between independence number α(G) and MAS number m(G) as contexts increase

## Executive Summary
This paper studies stochastic contextual bandits with graph feedback, where taking an action reveals rewards for neighboring actions across all contexts. The key contribution is establishing that the statistical complexity of this problem is characterized by βM(G), a graph-theoretic quantity that interpolates between the independence number α(G) and the maximum acyclic subgraph number m(G) as the number of contexts M varies. For many contexts (M ≥ m(G)), the MAS number m(G) becomes the fundamental complexity measure, contrasting with multi-armed bandits where α(G) is fundamental. The paper also provides algorithms achieving near-optimal regret for important classes of feedback graphs.

## Method Summary
The paper establishes theoretical bounds through information-theoretic lower bound arguments and proposes arm elimination algorithms. For self-avoiding contexts, it uses a layering technique where each layer maintains active sets and solves a sequential game to find small exploration sets that dominate the active set. For general contexts, it uses a different sequential game where the adversary can choose contexts adaptively. The algorithms rely on polynomial-time solvers for these sequential games, though specific implementations are not fully detailed in the paper.

## Key Results
- Establishes regret lower bound Ω(√βM(G)T) that interpolates between α(G) and m(G) as M varies
- Shows MAS number m(G) characterizes complexity for M ≥ m(G), contrasting with multi-armed bandits
- Provides polynomial-time algorithms achieving Õ(√βM(G)T) regret for self-avoiding contexts
- Achieves Õ(√min{βM(G), m(G)}T) regret for general contexts
- Demonstrates applications to auctions and inventory control through transitively closed graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Regret lower bound scales as Ω(√βM(G)T) where βM(G) interpolates between α(G) and m(G)
- Mechanism: Constructs hard instances using M independent sets I1,...,IM with Ii→Ij forbidden for i<j, forcing exploration of each Ic under context c
- Core assumption: Context sequence is self-avoiding (no jumping back to previous contexts)
- Evidence anchors:
  - [abstract] "βM(G) interpolates between α(G) and m(G) as the number of contexts M varies"
  - [section 2] "Proof uses definition (2) of βM(G) to construct M independent sets I1,...,IM such that Ii→Ij for i<j"
- Break condition: If contexts can jump back, exploration in earlier contexts provides information for later contexts, breaking lower bound argument

### Mechanism 2
- Claim: Polynomial-time algorithm achieves Õ(√βM(G)T) regret for self-avoiding contexts
- Mechanism: Uses arm elimination with layering technique, solving sequential game to find small exploration sets dominating active sets
- Core assumption: Graph is strongly observable and contexts are self-avoiding
- Evidence anchors:
  - [section 3.2] "Algorithm 1: Arm elimination algorithm for self-avoiding contexts"
  - [section 3.2] "Algorithm relies on well-known idea of arm elimination"
- Break condition: If graph not strongly observable, some actions cannot be observed even when neighbors are played, breaking dominance requirement

### Mechanism 3
- Claim: Achieves Õ(√min{βM(G), m(G)}T) regret for general contexts using different sequential game
- Mechanism: Similar layering but uses sequential game II where adversary chooses contexts adaptively
- Core assumption: General feedback graph with possibly adaptive context sequences
- Evidence anchors:
  - [section 3.3] "Algorithm 2: Arm elimination under general contexts"
  - [section 3.1] "Second sequential game motivated by bandit learning with arbitrary context sequence"
- Break condition: Gap between βM(G) and βM(G) remains open when graph contains long paths

## Foundational Learning

- Concept: Graph-theoretic quantities (independence number, MAS number, dominating number)
  - Why needed here: Characterize statistical complexity of learning under graph feedback; quantities change as number of contexts varies
  - Quick check question: What is difference between α(G) and m(G) for directed graph? Can you construct example where they differ significantly?

- Concept: Sequential games on graphs
  - Why needed here: Reduce contextual bandit problem to solving sequential games where learner chooses exploration actions while adversary chooses active sets
  - Quick check question: In sequential game I, why must learner choose dominating set Dc⊆Ac? What would happen if they chose smaller set?

- Concept: Layering technique in bandit algorithms
  - Why needed here: Gradually increase accuracy of reward estimates; each layer ensures actions observed enough times for concentration bounds
  - Quick check question: How does layering technique ensure confidence bounds hold with high probability across all layers and contexts?

## Architecture Onboarding

- Component map: Graph analysis module -> Sequential game solver -> Layering manager -> Context reception -> Action selection -> Observation update -> Confidence bound calculator

- Critical path: Graph analysis → Sequential game solution → Layering setup → Context reception → Action selection → Observation update → Confidence bound update

- Design tradeoffs:
  - Exact vs approximate sequential game solutions: Exact is NP-hard but approximate gives O(log|V|) factor increase
  - Number of layers: More layers give better concentration but increase computational cost
  - Exploration vs exploitation balance: More exploration gives better estimates but higher regret

- Failure signatures:
  - Regret higher than theoretical bound: Likely issue with sequential game solution or layering
  - Confidence bounds failing: Possibly incorrect observation counting or too few layers
  - Algorithm not polynomial time: Using exact NP-hard subroutines instead of approximations

- First 3 experiments:
  1. Implement graph analysis module and test on simple graphs (undirected, directed acyclic, transitively closed)
  2. Implement sequential game solver I and verify against known values for small graphs
  3. Build complete algorithm for self-avoiding contexts and test on synthetic problems with known βM(G)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is MAS number m(G) the fundamental complexity measure for contextual bandits with graph feedback when T is small (T < βM(G)³)?
- Basis in paper: [explicit] Paper establishes lower bound requiring T ≥ βM(G)³, but notes this is not artifact of analysis and optimal regret becomes fundamentally different for smaller T
- Why unresolved: Paper only provides bounds for large T and defers small T case to future work, acknowledging it becomes mixture of √T and T²/³ terms like in multi-armed bandits
- What evidence would resolve it: Complete characterization of minimax regret across all values of T, similar to recent multi-armed bandits result

### Open Question 2
- Question: What is exact minimax value of sequential game II (Definition 2) for general context sequences?
- Basis in paper: [explicit] Paper introduces two sequential games and provides upper bounds for both, but notes minimax value of sequential game II is not characterized tightly in general
- Why unresolved: Paper shows example where current upper bound βdom(G,M) is loose, identifies challenge of non-greedy approaches when context sequence not self-avoiding
- What evidence would resolve it: Tight characterization of minimax value U*₂(G,M) that matches lower bound Ω(√βM(G)T) when T ≥ βM(G)³

### Open Question 3
- Question: Does UCB algorithm achieve optimal regret for contextual bandits with graph feedback, or is MAS number m(G) fundamental even for this algorithm?
- Basis in paper: [explicit] Paper notes UCB algorithms analyzed for both multi-armed and contextual bandits only achieve regret upper bound Õ(√m(G)T), and this remains interesting open question
- Why unresolved: Current UCB analyses don't exploit structure that could lead to better regret bounds, unknown if forced exploration is necessary to achieve tighter bounds
- What evidence would resolve it: Proof that UCB (or variant) achieves regret Õ(√βM(G)T) for general contexts, or lower bound showing this is impossible and m(G) is indeed fundamental

## Limitations

- The results heavily depend on the self-avoiding context assumption for tightest bounds, with gaps remaining for general context sequences
- The gap between βM(G) and βM(G) for general contexts remains open when graphs contain long paths
- The specific polynomial-time algorithms for solving sequential games are claimed to exist but not fully detailed

## Confidence

- Graph analysis and lower bound arguments: High confidence based on rigorous mathematical proofs
- Practical implementation of sequential games: Medium confidence - polynomial-time algorithms claimed but specific implementations not detailed
- Generalization beyond self-avoiding contexts: Low confidence - significant gaps remain in understanding general case

## Next Checks

1. **Graph Analysis Verification**: Implement βM(G) computation for various graph classes (directed acyclic, transitively closed, undirected) and verify it correctly interpolates between α(G) and m(G) as M increases from 1 to |V|.

2. **Sequential Game Implementation**: Develop and test the polynomial-time algorithm for solving sequential games I and II on small graphs where optimal solutions can be computed by exhaustive search, measuring approximation quality.

3. **Algorithm Performance Testing**: Implement the complete arm elimination algorithm for self-avoiding contexts and evaluate regret on synthetic problems with known βM(G) values, comparing against the theoretical O(√βM(G)T) bound across different graph structures.