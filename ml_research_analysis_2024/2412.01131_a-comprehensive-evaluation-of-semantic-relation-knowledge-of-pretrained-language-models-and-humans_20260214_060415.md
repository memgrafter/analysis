---
ver: rpa2
title: A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language
  Models and Humans
arxiv_id: '2412.01131'
source_url: https://arxiv.org/abs/2412.01131
tags:
- word
- relation
- relations
- https
- antonymy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study introduced a comprehensive evaluation framework to assess
  the semantic relation knowledge of pretrained language models (PLMs) and humans,
  covering six relations: hypernymy, hyponymy, holonymy, meronymy, antonymy, and synonymy.
  Five metrics were used: soundness, completeness, symmetry, prototypicality, and
  distinguishability.'
---

# A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans

## Quick Facts
- arXiv ID: 2412.01131
- Source URL: https://arxiv.org/abs/2412.01131
- Authors: Zhihan Cao; Hiroaki Yamada; Simone Teufel; Takenobu Tokunaga
- Reference count: 31
- Primary result: Significant knowledge gap exists between pretrained language models and humans across six semantic relations (hypernymy, hyponymy, holonymy, meronymy, antonymy, synonymy)

## Executive Summary
This paper introduces a comprehensive evaluation framework for assessing semantic relation knowledge in pretrained language models (PLMs) and humans. The study covers six semantic relations using five metrics: soundness, completeness, symmetry, prototypicality, and distinguishability. Experiments with six PLMs (four masked and two causal language models) and human participants revealed that all models lag significantly behind humans across all relations. The framework addresses previous limitations by using relatum sets instead of single gold standard items and evaluating previously understudied properties of semantic relations.

## Method Summary
The study evaluated six PLMs (BERT-base/large, RoBERTa-base/large, Llama-8B/70B) using prompt-based probing with 10,507 probes covering six semantic relations. Target words were collected from Hyperlex and Overschelde et al. (2004) and expanded via WordNet to create comprehensive relatum sets. Human responses from 48 MTurk participants established gold standards and human baselines. Five metrics were computed: soundness (precision@1), completeness (recall@k), symmetry (for antonymy/synonymy), prototypicality (entropy + edit similarity), and distinguishability (AuDC). The framework uses relatum sets rather than single items to ensure fairer evaluation across all possible relata.

## Key Results
- All six PLMs showed significant knowledge gaps compared to humans across all six semantic relations
- Masked language models (BERT, RoBERTa) generally outperformed causal language models (Llama) despite similar or smaller sizes
- Antonymy was the exception, with all models performing relatively well compared to other relations
- Causal language models' performance dropped below masked language models when relation directionality was reversed
- The combination of soundness and completeness scores showed causal language models performed worse than masked language models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using relatum sets instead of single gold standard items enables fairer evaluation of models' ability to predict all possible relata for a given target word and relation.
- Mechanism: By including all possible relata for any sense of the target word in the relatum set, the evaluation captures full semantic relation knowledge rather than penalizing models for missing a single answer.
- Core assumption: Expanding relatum sets to include all senses and indirect relations provides a more comprehensive gold standard.
- Evidence anchors: The framework defines gold standards as relatum sets, particularly necessary for hyponymy and meronymy where multiple relata cases are frequent.

### Mechanism 2
- Claim: The distinguishability metric effectively measures models' ability to distinguish between different semantic relations.
- Mechanism: By calculating the mean relative rank of relata from one relation when probed with a different relation, the metric quantifies how well models can separate different semantic relations.
- Core assumption: Lower relative ranks of incorrect relata indicate better distinguishability between relations.
- Evidence anchors: The metric was designed to detect differences between relations and measures the ability to separate relation types.

### Mechanism 3
- Claim: Using multiple prompts for each relation mitigates prompt dependency and provides more robust evaluation.
- Mechanism: By using several different prompts for each relation and averaging results, the evaluation reduces the influence of any single prompt design.
- Core assumption: Different prompts for the same relation elicit similar responses from models with good semantic relation knowledge.
- Evidence anchors: The authors counteracted prompt dependency by using multiple prompts, though they couldn't be certain this was sufficient.

## Foundational Learning

- Concept: Semantic relations (hypernymy, hyponymy, holonymy, meronymy, antonymy, synonymy)
  - Why needed here: Understanding different types of semantic relations is crucial for designing appropriate evaluation prompts and interpreting results.
  - Quick check question: What is the difference between hypernymy and hyponymy, and how do they relate to each other?

- Concept: Prompt-based probing methodology
  - Why needed here: The evaluation relies on prompt-based probing to elicit responses from models about semantic relations.
  - Quick check question: How does prompt-based probing differ from other methods like probing classifiers, and what are its advantages?

- Concept: Information retrieval metrics (precision, recall, F1-score)
  - Why needed here: Soundness and completeness metrics are based on information retrieval concepts.
  - Quick check question: How do precision and recall relate to soundness and completeness in evaluating models' semantic relation knowledge?

## Architecture Onboarding

- Component map: Data collection (target words, prompts, relatum sets) -> Model implementation (six PLMs) -> Evaluation metrics (five metrics) -> Human experiments (crowdsourcing) -> Statistical analysis (significance testing)

- Critical path: 1) Collect target words and create prompts 2) Expand relatum sets 3) Conduct human experiments 4) Implement model evaluation 5) Analyze results

- Design tradeoffs: Using relatum sets vs. single items (fairer but potentially noisier), multiple prompts vs. single prompt (reduces dependency but increases complexity), word-level vs. subword-level evaluation (ensures comparability but may miss subword knowledge)

- Failure signatures: Low soundness/completeness scores (limited prediction ability), high distinguishability scores (poor relation separation), consistent antonymy bias (confusing non-antonymy with antonymy)

- First 3 experiments: 1) Evaluate single model on single relation with single prompt vs. human ceiling 2) Evaluate same model on all six relations with multiple prompts vs. human ceiling 3) Evaluate all six models on single relation with multiple prompts vs. human ceiling and each other

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do masked language models consistently outperform causal language models in hyponymy and meronymy tasks despite CLMs being larger and more widely used?
- Basis in paper: Results show MLMs outperform CLMs for hyponymy and meronymy, with CLMs' performance dropping when relation directionality is reversed.
- Why unresolved: The paper suggests bidirectional context is crucial but doesn't explain the mechanism or why CLMs struggle with these relations.
- What evidence would resolve it: Experiments comparing CLMs with bidirectional pretraining on hyponymy/meronymy tasks, or ablation studies isolating context directionality effects.

### Open Question 2
- Question: What causes the consistent "antonymy bias" across all models, where models frequently confuse non-antonymy relations with antonymy?
- Basis in paper: Results show models frequently produce antonyms even when non-antonymy relations are prompted, with antonymy showing highest performance.
- Why unresolved: The paper suggests distributional characteristics of antonyms in corpora might be responsible but doesn't test this hypothesis.
- What evidence would resolve it: Corpus analysis of antonym co-occurrence patterns, experiments with controlled training data, or studies of model attention patterns when processing antonyms.

### Open Question 3
- Question: Do specific linguistic patterns in prompts act as shortcuts for models when solving semantic relation tasks?
- Basis in paper: The authors note prompt dependency as a limitation and found heteroscedasticity in model performance across prompts.
- Why unresolved: While the paper suggests some prompts may be used as shortcuts, it couldn't identify which patterns or expressions models might be exploiting.
- What evidence would resolve it: Analysis of model attention patterns across different prompts, controlled experiments varying linguistic structures, or probing studies identifying shortcut expressions.

## Limitations

- The evaluation is constrained to English and word-level relations, excluding subword knowledge
- Relatum sets may still be incomplete or include noisy relata, potentially affecting metric reliability
- Reliance on crowd-sourced human responses introduces variability, and the sample size of 48 participants may not fully capture diverse human semantic knowledge

## Confidence

- General observation of PLM-human knowledge gap: High
- Relative performance patterns between MLMs and CLMs: Medium
- Antonymy performance being relatively high across all models: High

## Next Checks

1. **Prompt Dependency Validation**: Systematically vary prompt formulations beyond the nine used and measure stability of model responses to determine if current prompts sufficiently mitigate prompt dependency.

2. **Cross-Lingual Generalization**: Replicate the evaluation framework with the same PLMs on a non-English language to assess whether observed knowledge gaps and performance patterns generalize across languages.

3. **Subword Knowledge Investigation**: Adapt the evaluation to the subword level to determine if observed performance gaps between models and humans are consistent when evaluating full knowledge captured by subword-based models.