---
ver: rpa2
title: 'ATM: Improving Model Merging by Alternating Tuning and Merging'
arxiv_id: '2411.03055'
source_url: https://arxiv.org/abs/2411.03055
tags:
- task
- merging
- multitask
- pa-atm
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ATM (Alternating Tuning and Merging), a framework
  that iteratively refines model merging by alternating between task-specific finetuning
  and aggregation of task vectors. The authors theoretically connect task vectors
  to gradients under single-epoch gradient descent, reinterpreting task arithmetic
  as one noisy gradient step toward a multitask objective.
---

# ATM: Improving Model Merging by Alternating Tuning and Merging

## Quick Facts
- arXiv ID: 2411.03055
- Source URL: https://arxiv.org/abs/2411.03055
- Authors: Luca Zhou, Daniele Solombrino, Donato Crisostomi, Maria Sofia Bucarelli, Fabrizio Silvestri, Emanuele Rodolà
- Reference count: 0
- Primary result: ATM framework achieves 15-17% accuracy improvements over baselines in privacy-aware multitask learning

## Executive Summary
The paper introduces ATM (Alternating Tuning and Merging), a framework that iteratively refines model merging by alternating between task-specific finetuning and aggregation of task vectors. The authors theoretically connect task vectors to gradients under single-epoch gradient descent, reinterpreting task arithmetic as one noisy gradient step toward a multitask objective. This motivates ATM's iterative approach, which progressively improves upon one-shot merging. Two applications are proposed: PA-ATM for privacy-aware multitask learning without data sharing (federated settings) and PH-ATM for post-hoc refinement of existing merged models using small validation sets.

## Method Summary
ATM is a framework that iteratively refines model merging through alternating cycles of task-specific finetuning and vector aggregation. The method is motivated by a theoretical connection between task vectors and gradients under single-epoch gradient descent, which reinterprets task arithmetic as one noisy gradient step toward a multitask objective. The iterative approach progressively improves upon one-shot merging by repeating these cycles. Two specific applications are developed: PA-ATM for privacy-aware multitask learning in federated settings without data sharing, and PH-ATM for post-hoc refinement of existing merged models using small validation sets. Experiments on 7 vision datasets using ViT-B/16 demonstrate significant improvements over baseline merging methods.

## Key Results
- PA-ATM significantly outperforms baselines (task arithmetic, TIES, breadcrumbs, DARE) with average accuracy improvements of 15-17% over 10-30 iterations
- PH-ATM improves upon baselines by approximately 7% using small validation sets
- Gradual refinement through ATM yields better multitask performance than one-shot merging, with gains increasing with more iterations

## Why This Works (Mechanism)
The mechanism works by leveraging the theoretical connection between task vectors and gradients under single-epoch gradient descent. This connection reinterprets task arithmetic as one noisy gradient step toward a multitask objective, which motivates the iterative refinement approach. By alternating between finetuning and merging, ATM progressively corrects errors and improves the model's ability to handle multiple tasks simultaneously. The iterative nature allows for gradual improvement that accumulates over multiple cycles, outperforming the static one-shot merging approaches.

## Foundational Learning

**Gradient Descent and Task Vectors**: Understanding how gradients relate to task-specific adjustments in model parameters is crucial for grasping the theoretical foundation. Quick check: Verify that task vectors can be interpreted as approximate gradient directions.

**Model Merging Fundamentals**: Knowledge of how to combine multiple fine-tuned models into a single multitask model is essential. Quick check: Ensure understanding of weight averaging and vector arithmetic in model merging.

**Privacy-Preserving Machine Learning**: Concepts like federated learning and data sharing constraints are important for PA-ATM. Quick check: Confirm understanding of how model parameters can be shared without raw data exposure.

**Iterative Optimization**: The principle that repeated refinement cycles can improve solutions beyond one-shot approaches. Quick check: Validate that iterative improvement generally converges to better solutions than single-step approaches.

## Architecture Onboarding

**Component Map**: Pre-trained ViT-B/16 -> Task-specific finetuning -> Task vector extraction -> Vector aggregation -> Merged model -> Validation -> Iterative cycle

**Critical Path**: The most critical sequence is: task-specific finetuning → task vector extraction → vector aggregation → merged model evaluation → parameter update. This cycle repeats until convergence or iteration limit.

**Design Tradeoffs**: The main tradeoff is between computational cost (multiple iterations) and performance improvement. Each iteration requires full finetuning cycles for all tasks, making it more expensive than one-shot merging but potentially yielding significantly better results.

**Failure Signatures**: Potential failures include: convergence to local optima, divergence if learning rates are too high, poor performance if task vectors are incompatible, and privacy leakage in PA-ATM if implementation is flawed.

**3 First Experiments**: 1) Test convergence behavior with varying numbers of iterations (5, 10, 20, 30), 2) Compare performance against different merging baselines on a single task pair, 3) Evaluate privacy guarantees by attempting to reconstruct data from model updates.

## Open Questions the Paper Calls Out
None explicitly mentioned in the provided content.

## Limitations
- Theoretical connection relies on strong assumptions about task similarity and gradient noise structure
- Evaluation limited to vision tasks with ViT-B/16 architecture, raising generalization questions
- Privacy claims assume honest-but-curious participants rather than fully malicious adversaries
- Computational cost of multiple iterative steps not thoroughly analyzed relative to benefits

## Confidence

**Theoretical Connection**: Medium confidence - The gradient-vector connection is compelling but depends on specific assumptions that may not hold in practice.

**Privacy Guarantees**: Medium confidence - Assumes honest-but-curious threat model, which may not capture all real-world attack scenarios.

**Generalization**: Low confidence - Results are limited to ViT-B/16 on vision tasks; performance on other architectures and modalities is unknown.

**Iterative Improvement**: High confidence - The core concept of iterative refinement improving upon one-shot methods is well-established and demonstrated.

## Next Checks

1) Test ATM framework across diverse model architectures (LLMs, multimodal models) and tasks to assess scalability limits

2) Compare against state-of-the-art model merging techniques like model soup and parameter-efficient fine-tuning methods to establish relative effectiveness

3) Conduct robustness analysis under different threat models for the privacy-aware setting, including malicious participants and data leakage scenarios