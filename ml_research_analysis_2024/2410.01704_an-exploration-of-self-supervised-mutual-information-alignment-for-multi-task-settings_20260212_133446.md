---
ver: rpa2
title: An Exploration of Self-Supervised Mutual Information Alignment for Multi-Task
  Settings
arxiv_id: '2410.01704'
source_url: https://arxiv.org/abs/2410.01704
tags:
- sami
- accuracy
- reasoning
- alignment
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of Self-Supervised Alignment
  with Mutual Information (SAMI) for improving multi-task performance and mathematical
  reasoning in language models. The core method uses conditional mutual information
  to align model responses with predefined behavioral principles without requiring
  preference labels.
---

# An Exploration of Self-Supervised Mutual Information Alignment for Multi-Task Settings

## Quick Facts
- arXiv ID: 2410.01704
- Source URL: https://arxiv.org/abs/2410.01704
- Reference count: 5
- Multi-task win rate: 57% on MT-Bench

## Executive Summary
This paper explores Self-Supervised Alignment with Mutual Information (SAMI) as a method to improve multi-task performance and mathematical reasoning in language models without requiring preference labels. SAMI leverages conditional mutual information to align model responses with predefined behavioral principles, offering a self-supervised alternative to preference-based optimization methods. The approach demonstrates competitive performance in multi-task settings and shows particular promise for mathematical reasoning tasks when combined with multiple attempts.

## Method Summary
SAMI uses conditional mutual information to align model responses with predefined behavioral principles without requiring preference labels. The method operates by maximizing mutual information between the model's outputs and target behaviors while minimizing mutual information with undesired behaviors. This self-supervised approach contrasts with preference-based methods like DPO, enabling alignment without expensive human preference data. The technique can be applied across multiple tasks and shows particular effectiveness when combined with multi-attempt strategies for complex reasoning problems.

## Key Results
- 57% win rate on MT-Bench in multi-task settings, outperforming DPO
- 1.1% improvement in zero-shot GSM8K accuracy, less than SFT's 3.2% but with better scaling across multiple attempts
- 3.9% accuracy gain with 10 attempts on GSM8K, demonstrating improved performance with repeated inference
- Additional 1.3% improvement when combining SAMI with SFT in multi-attempt settings

## Why This Works (Mechanism)
SAMI works by maximizing mutual information between model outputs and target behaviors while minimizing mutual information with undesired behaviors. This self-supervised alignment approach allows the model to internalize behavioral principles without requiring preference labels, making it more sample-efficient than preference-based methods. The mechanism is particularly effective for tasks requiring consistent behavioral patterns across multiple attempts, as demonstrated in the multi-attempt GSM8K experiments where performance scales substantially with repeated inference.

## Foundational Learning
- Mutual Information Maximization: Used to align model behavior with target principles without labels - needed to understand the core alignment mechanism; quick check: verify mutual information calculations in the method section
- Multi-Task Learning: Framework for training on diverse task distributions - needed to contextualize performance comparisons; quick check: confirm task diversity in MT-Bench evaluation
- Conditional Probability Distributions: Mathematical foundation for modeling behavioral alignment - needed to understand how SAMI conditions on desired behaviors; quick check: review conditional probability formulations
- Preference-Based vs Self-Supervised Alignment: Contrast between DPO and SAMI approaches - needed to appreciate the methodological innovation; quick check: compare experimental setups between SAMI and DPO
- Zero-Shot vs Few-Shot Learning: Performance measurement paradigms - needed to interpret accuracy improvements; quick check: verify which experimental settings use zero-shot evaluation

## Architecture Onboarding
- Component Map: SAMI -> Mutual Information Calculation -> Behavioral Principle Integration -> Model Output
- Critical Path: Input text → SAMI alignment module → Mutual information maximization → Output generation with aligned behavior
- Design Tradeoffs: SAMI prioritizes sample efficiency and self-supervision over potentially higher performance of supervised methods; offers better scaling with attempts but lower single-attempt accuracy
- Failure Signatures: May underperform supervised fine-tuning in single-attempt settings; effectiveness depends on quality of predefined behavioral principles
- First Experiments: 1) Compare SAMI vs DPO on MT-Bench across all task categories 2) Evaluate GSM8K accuracy with 1, 5, and 10 attempts 3) Test SAMI + SFT combination on mathematical reasoning tasks

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Single-attempt performance lags behind supervised fine-tuning (1.1% vs 3.2% on GSM8K)
- Effectiveness depends on quality and completeness of predefined behavioral principles
- Scaling benefits require multiple attempts, which may not be practical in all deployment scenarios

## Confidence
- Multi-task win rate claims: High confidence (57% on MT-Bench with clear experimental setup)
- GSM8K accuracy improvements: Medium confidence (incremental gains with specific experimental conditions)
- SAMI + SFT combination benefits: Medium confidence (additional 1.3% gain only in multi-attempt settings)

## Next Checks
1. Validate SAMI performance across a broader range of mathematical reasoning datasets beyond GSM8K
2. Test the scaling properties of SAMI with attempts on non-mathematical tasks to assess generalizability
3. Compare SAMI's sample efficiency against other self-supervised alignment methods in resource-constrained settings