---
ver: rpa2
title: 'A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive
  RL without Rewards, Demonstrations, or Subgoals'
arxiv_id: '2408.05804'
source_url: https://arxiv.org/abs/2408.05804
tags:
- learning
- exploration
- goal
- single
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies goal-conditioned reinforcement learning where
  only a single goal state is provided. The authors propose a simple modification
  of contrastive RL that always conditions exploration on the single target goal.
---

# A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals

## Quick Facts
- arXiv ID: 2408.05804
- Source URL: https://arxiv.org/abs/2408.05804
- Authors: Grace Liu; Michael Tang; Benjamin Eysenbach
- Reference count: 40
- Primary result: Single-goal contrastive RL learns directed exploration and diverse skills, outperforming multi-goal baselines.

## Executive Summary
This paper introduces a simple yet surprisingly effective approach to goal-conditioned reinforcement learning that uses only a single hard goal for both training and exploration. By modifying contrastive RL to always condition data collection on this single target goal, the authors show that complex skills and directed exploration emerge naturally, even before the agent achieves any successful trials. The method outperforms approaches that use a range of training goals and produces robust, diverse strategies. The key insight is that the contrastive representations learned during training capture environment structure, which guides exploration, though the precise mechanism remains an open question.

## Method Summary
The method uses contrastive reinforcement learning to train a policy and critic for reaching a single fixed goal state. During data collection, the agent always conditions its policy on the single target goal, regardless of the current state. The critic is represented as an inner product of two learned representations: ϕ(s,a) for state-action pairs and ψ(sg) for goal states. These representations are trained using a contrastive objective (infoNCE) with LogSumExp regularization. The policy is updated to maximize the dot product ϕ(s,a)ᵀψ(sg), encouraging actions that move the agent toward the goal. This approach requires no rewards, demonstrations, or subgoals—only a single target state.

## Key Results
- Single-goal contrastive RL outperforms multi-goal baselines on four continuous control tasks.
- Skills and directed exploration emerge before any successful trials are observed.
- The contrastive representations are crucial for driving exploration; monolithic critics fail to achieve similar results.
- The method produces robust, diverse strategies and reduces exploration naturally as the agent learns to reach the goal reliably.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-goal contrastive RL works because the contrastive representations learned during training capture meaningful environment dynamics, which guide exploration.
- Mechanism: The critic uses an inner product of two learned representations, ϕ(s,a) and ψ(sf), which are trained via contrastive learning. Early in training, these representations encode environment structure (like walls and floor in the bin task), creating a latent map that directs exploration toward reachable states.
- Core assumption: The inner product critic architecture is essential—monolithic critics fail to drive effective exploration with single-goal data collection.
- Evidence anchors:
  - [abstract] "the contrastive representations used by the method are important for driving exploration"
  - [section] "This experiment suggests that the contrastive representations ϕ(s, a)T and ψ(sf) drive exploration, though the precise mechanism for how they drive exploration remains unclear."
  - [corpus] Weak: no corpus papers directly address contrastive representation dynamics in exploration; closest is "The impact of intrinsic rewards on exploration in Reinforcement Learning" but it focuses on intrinsic rewards, not contrastive learning.
- Break condition: If the representations fail to encode environment structure (e.g., in highly stochastic or non-stationary environments), exploration would become random and performance would degrade.

### Mechanism 2
- Claim: The method works because training on a single hard goal forces the agent to explore diverse strategies before mastering the task, leading to robustness.
- Mechanism: By always conditioning data collection on the single target goal, the agent must learn to reach it from many starting states, effectively building a curriculum of sub-skills (e.g., moving hand, pushing, picking). This early diversity prevents overfitting to a narrow policy and produces multiple strategies.
- Core assumption: The policy and representations must generalize across states, not just memorize the goal path.
- Evidence anchors:
  - [abstract] "skills and directed exploration emerge from a simple RL algorithm long before any successful trials are observed"
  - [section] "The performance boost of single-goal data collection does not seem to be due to overfitting of the policy parameters on the task."
  - [corpus] Weak: "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning" discusses curricula but focuses on automatically generated goals, not single-goal training.
- Break condition: If the goal is truly impossible or unreachable, the agent may get stuck exploring locally without making progress.

### Mechanism 3
- Claim: Single-goal exploration trades off exploration for exploitation automatically as the agent learns to reach the goal reliably.
- Mechanism: Initially, the agent explores broadly because the goal is hard to reach; as success rate increases, the learned representations guide the agent to exploit known good paths, reducing exploration naturally without manual schedules.
- Core assumption: The learning algorithm can distinguish between exploration and exploitation phases based on success rate.
- Evidence anchors:
  - [abstract] "Once the agent has learned to reach the goal state reliably, exploration is reduced."
  - [section] "Fig. 6 shows that the growth rate of this exploration metric decreases as the success rate increases."
  - [corpus] Weak: "Exploration and Persuasion" discusses balancing exploration and exploitation but in a different context (population-level agents).
- Break condition: If the goal is too easy, the agent may not explore enough to build robust representations, hurting generalization.

## Foundational Learning

- Concept: Goal-conditioned reinforcement learning (GCRL) with sparse rewards.
  - Why needed here: The paper studies reaching a single goal without dense rewards, so understanding GCRL fundamentals is essential.
  - Quick check question: In GCRL, how does the agent know it is making progress if no reward is given?

- Concept: Contrastive learning for representation learning.
  - Why needed here: The critic uses contrastive representations to encode state-action and state pairs, which drive exploration.
  - Quick check question: What is the role of negative samples in the contrastive objective?

- Concept: Markov decision processes (MDPs) and occupancy measures.
  - Why needed here: The objective maximizes the probability of reaching the goal under the discounted state occupancy measure.
  - Quick check question: How does the discount factor γ affect the definition of success in continuous goal-reaching tasks?

## Architecture Onboarding

- Component map:
  Policy π(a|s,g) -> Critic ϕ(s,a), ψ(sf) -> Contrastive loss -> Policy update -> Data collection (conditioned on single goal)

- Critical path:
  1. Sample (s,a) from replay buffer.
  2. Sample future state sf via geometric horizon.
  3. Compute contrastive loss and update ϕ, ψ.
  4. Update policy to maximize ϕ(s,a)^T ψ(s*) for the single goal s*.
  5. Collect next trajectory conditioned on s*.

- Design tradeoffs:
  - Inner product critic vs. monolithic critic: inner product works with single-goal exploration; monolithic fails.
  - Representation dimension: 64 chosen empirically; too small may underfit, too large may overfit.
  - Batch size: 256 balances memory and stability.

- Failure signatures:
  - If inner product critic is replaced with monolithic, exploration collapses.
  - If single goal is impossible, agent gets stuck with no progress.
  - If entropy coefficient is too low, policy may exploit prematurely without exploring.

- First 3 experiments:
  1. Run single-goal CRL on sawyer bin task; verify skills emerge before success.
  2. Replace inner product critic with monolithic; confirm failure.
  3. Compare single-goal vs. range-of-difficulties data collection; measure success rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of the contrastive representations drive the emergence of directed exploration?
- Basis in paper: [explicit] The paper notes that "the contrastive representations used to express the value function are important" and that single-goal exploration is less effective with a monolithic critic architecture.
- Why unresolved: While the paper identifies that representations are important, it does not explain the precise mechanism by which they drive exploration or what specific features of the representations are crucial.
- What evidence would resolve it: Detailed analysis of the learned representations, such as examining their sensitivity to environmental features or their ability to predict state transitions, could reveal the properties that enable directed exploration.

### Open Question 2
- Question: How does the proposed method compare to other exploration strategies in environments with varying reward structures or without a single fixed goal?
- Basis in paper: [inferred] The paper primarily evaluates the method in settings with a single goal and no rewards, but notes that "there likely exist significantly better methods of exploiting the emergent exploration properties" in other settings.
- Why unresolved: The paper focuses on a specific problem setting and does not explore the method's performance in more general environments or with different reward structures.
- What evidence would resolve it: Experiments comparing the method to other exploration strategies in diverse environments with varying reward structures or without a fixed goal would provide insights into its broader applicability.

### Open Question 3
- Question: What theoretical framework can explain why skills and directed exploration emerge from using a single goal for data collection?
- Basis in paper: [explicit] The paper states that "we lack a clear theoretical understanding of why it works so effectively" and that "much theoretical work remains to be done to understand exactly what is driving the exploration."
- Why unresolved: The paper provides empirical evidence of the method's effectiveness but does not offer a theoretical explanation for the emergence of skills and directed exploration.
- What evidence would resolve it: Developing a theoretical framework that connects the learning dynamics of the method to the emergence of skills and directed exploration would provide a deeper understanding of its behavior.

## Limitations

- Theoretical understanding: The paper lacks a clear theoretical explanation for why the contrastive representations drive exploration so effectively.
- Architecture dependence: The method's success relies heavily on the inner product critic architecture, which may not generalize to all RL settings.
- Scalability: While effective in tested environments, the method's performance in high-dimensional or stochastic tasks remains untested.

## Confidence

- Empirical claims: Medium
- Theoretical understanding: Low
- Architecture dependence: Medium

## Next Checks

1. Test single-goal CRL in a high-dimensional task (e.g., 3D navigation) to evaluate scalability and representation quality.
2. Replace the inner product critic with a monolithic critic while keeping all else fixed to isolate the architectural contribution.
3. Analyze the learned representations in early training by visualizing the norms of ψ(sg) across different initial states to confirm they encode environment structure.