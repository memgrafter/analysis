---
ver: rpa2
title: 'Direct Preference Optimization With Unobserved Preference Heterogeneity: The
  Necessity of Ternary Preferences'
arxiv_id: '2405.15065'
source_url: https://arxiv.org/abs/2405.15065
tags:
- preference
- preferences
- arxiv
- reward
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning large language models
  with diverse human preferences by developing methods to handle unobserved preference
  heterogeneity. The authors propose Expectation-Maximization Direct Preference Optimization
  (EM-DPO), an algorithm that clusters annotators based on their observed preference
  data and trains an optimal policy for each cluster.
---

# Direct Preference Optimization With Unobserved Preference Heterogeneity: The Necessity of Ternary Preferences

## Quick Facts
- arXiv ID: 2405.15065
- Source URL: https://arxiv.org/abs/2405.15065
- Authors: Keertana Chidambaram; Karthik Vinay Seetharaman; Vasilis Syrgkanis
- Reference count: 40
- One-line primary result: Ternary preferences enable identifiability of latent user preferences while binary preferences do not under linear reward models

## Executive Summary
This paper addresses the challenge of aligning large language models with diverse human preferences by developing methods to handle unobserved preference heterogeneity. The authors propose Expectation-Maximization Direct Preference Optimization (EM-DPO), an algorithm that clusters annotators based on their observed preference data and trains an optimal policy for each cluster. They also introduce Min-Max Regret Aggregation (MMRA), which combines the learned policies into a single fair policy by minimizing worst-case regret across sub-populations. The key theoretical contribution establishes that binary preferences are insufficient for identifying latent user preferences, while ternary preferences ensure identifiability under a linear reward model.

## Method Summary
The method consists of two main components: EM-DPO and MMRA. EM-DPO uses expectation-maximization to soft-cluster annotators into K latent preference types based on their pairwise or ternary preference data, then trains a separate optimal policy for each cluster using a preference likelihood objective. MMRA then aggregates these K type-specific policies into a single fair policy by treating the aggregation as a zero-sum game where a policy selector (minimizing player) tries to minimize worst-case regret across all preference subgroups. The algorithm uses multiplicative weights updates to find a mixture over ensemble policies that achieves this min-max regret objective. The approach handles both binary and ternary preferences, with theoretical results showing ternary preferences enable unique recovery of the latent preference distribution under a linear reward model.

## Key Results
- EM-DPO with ternary preferences outperforms binary preferences on both GlobalOpinionQA and MPI datasets, achieving higher reward margins and accuracy
- MMRA effectively reduces worst-case regret compared to baselines while maintaining high average reward across all sub-populations
- Theoretical results prove binary preferences suffer from fundamental non-identifiability while ternary preferences enable unique recovery of latent preference distributions under linear reward models
- EM-DPO successfully discovers meaningful preference clusters that correspond to distinct user sub-populations in the experimental datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ternary preferences are necessary to identify latent user preference heterogeneity under a linear reward model.
- Mechanism: Binary comparisons fail to uniquely recover the distribution of user preference parameters because symmetric distributions (e.g., β vs -β) produce identical aggregate choice probabilities. Ternary comparisons resolve this non-identifiability by providing sufficient variability in observed rankings to uniquely determine the underlying preference distribution.
- Core assumption: The reward model is linear in known features and user preferences follow a distribution with finite absolute moments satisfying the Carleman condition.
- Evidence anchors:
  - [abstract] "we establish that binary preferences are insufficient for identifying latent user preferences, while ternary preferences ensure identifiability under a linear reward model."
  - [section 4] "Binary preferences suffer from fundamental non-identifiability, while multi-item preferences, even incomplete ternary preferences, enable unique recovery of the latent preference distribution."
  - [corpus] Weak evidence - no direct corpus citations supporting the identifiability proof.

### Mechanism 2
- Claim: EM-DPO can discover latent annotator types and train optimal policies for each type without explicit group labels.
- Mechanism: The algorithm uses expectation-maximization to iteratively estimate the posterior probability that each annotator belongs to each latent type, then updates policy parameters by maximizing a weighted preference likelihood where weights reflect these posterior probabilities. This soft clustering approach avoids the limitations of hard clustering while maintaining computational efficiency.
- Core assumption: The number of latent preference types K is finite and known (or can be treated as a hyperparameter).
- Evidence anchors:
  - [abstract] "We develop an Expectation-Maximization adaptation of DPO that discovers latent annotator types and trains a mixture of LLMs accordingly."
  - [section 3.1] "The resulting algorithm, EM-DPO, soft-clusters annotators based on their observed preference data and learns an optimal policy for each cluster."
  - [corpus] Weak evidence - no direct corpus citations supporting the EM-DPO algorithm formulation.

### Mechanism 3
- Claim: Min-Max Regret Aggregation (MMRA) produces a single fair policy that minimizes worst-case regret across all preference subgroups.
- Mechanism: MMRA treats the ensemble of type-specific optimal policies as a zero-sum game between a policy selector (minimizing player) and an adversarial preference subgroup (maximizing player). The algorithm finds a mixture over ensemble policies that minimizes the maximum regret any subgroup would experience compared to its optimal policy.
- Core assumption: The type-specific optimal policies returned by EM-DPO are accurate representations of each subgroup's preferences.
- Evidence anchors:
  - [abstract] "We propose an aggregation algorithm using a min-max regret fairness criterion to produce a single generative policy with equitable performance guarantees."
  - [section 3.2] "Our fairness criterion is to minimize the worst-case subgroup regret... we seek a policy π ∈ Π that solves: π∗ = arg minπ∈Π maxw∈∆K−1 Xk wk([Rk(π)]+ + βDKL(π||πSFT))"
  - [corpus] Weak evidence - no direct corpus citations supporting the MMRA algorithm formulation.

## Foundational Learning

- Concept: Bradley-Terry-Luce model for pairwise comparisons
  - Why needed here: This forms the theoretical foundation for modeling how human annotators choose between responses based on underlying reward functions.
  - Quick check question: How does the Bradley-Terry model relate the probability of preferring one response over another to their reward values?

- Concept: Expectation-Maximization algorithm for mixture models
  - Why needed here: EM is used to simultaneously cluster annotators into preference types and train policies for each type without knowing the true group assignments.
  - Quick check question: What are the E-step and M-step computing in the context of preference-based clustering?

- Concept: Zero-sum game theory and regret minimization
  - Why needed here: The min-max regret aggregation relies on game-theoretic concepts to find a fair policy that balances the interests of all preference subgroups.
  - Quick check question: How does the min-max regret formulation ensure that no subgroup is severely underserved by the aggregated policy?

## Architecture Onboarding

- Component map: Preference data -> EM-DPO (soft clustering + type-specific policy training) -> MMRA (ensemble aggregation) -> Single fair policy
- Critical path:
  1. Collect preference data (binary or ternary)
  2. Run EM-DPO to cluster annotators and train ensemble policies
  3. Apply MMRA to aggregate ensemble into single policy
  4. Deploy aggregated policy
- Design tradeoffs:
  - Ternary vs binary preferences: Ternary enables identifiability but requires more complex annotation
  - Number of clusters K: More clusters allow finer personalization but increase computational cost
  - Regret vs reward optimization: MMRA focuses on fairness rather than maximizing average reward
- Failure signatures:
  - Poor clustering: Low separation between learned policies, similar performance across all types
  - Identifiability failure: Multiple distinct preference distributions produce similar aggregate behaviors
  - Aggregation instability: Small changes in ensemble policies cause large swings in aggregated policy
- First 3 experiments:
  1. Run EM-DPO with K=2 on synthetic data where true preferences are known to verify clustering accuracy
  2. Compare ternary vs binary preference performance on the MPI dataset with adversarial preference pairs
  3. Test MMRA aggregation on the GlobalOpinionQA dataset to verify regret reduction across all subgroups

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EM-DPO scale with the number of latent user preference types (K) when K is unknown and potentially large?
- Basis in paper: [explicit] The paper treats K as a hyperparameter and tunes it using a validation dataset, but acknowledges that computational and memory requirements scale linearly and at least linearly with K.
- Why unresolved: The paper only evaluates K=4 based on validation performance for the specific datasets used. The computational cost of exploring larger K values and the potential performance benefits or drawbacks are not investigated.
- What evidence would resolve it: Experiments comparing EM-DPO performance and computational requirements across a range of K values (e.g., 2, 4, 8, 16) on datasets with varying numbers of latent preference types would clarify the trade-off between model complexity and performance.

### Open Question 2
- Question: Can the identification theory be extended to non-linear reward models, and how would this affect the necessity of ternary preferences?
- Basis in paper: [inferred] The paper establishes identification results for linear reward models but notes that extending the theory to more general classes of reward models is an open question.
- Why unresolved: The linear reward model is a simplification for theoretical analysis. Real-world preference data may exhibit non-linear relationships, and it's unclear whether ternary preferences remain necessary for identification in such cases.
- What evidence would resolve it: Theoretical work deriving identification conditions for non-linear reward models and empirical studies comparing binary vs. ternary preferences under non-linear reward models would address this question.

### Open Question 3
- Question: How sensitive is the EM-DPO algorithm to initialization, and what strategies can improve robustness to initialization?
- Basis in paper: [explicit] The paper mentions that EM-DPO is sensitive to initialization and uses K-means clustering for initialization, but suggests exploring alternative strategies.
- Why unresolved: The paper doesn't provide empirical evidence on the sensitivity of EM-DPO to different initialization strategies or the potential benefits of alternative approaches.
- What evidence would resolve it: Experiments comparing EM-DPO performance using different initialization strategies (e.g., random initialization, initialization based on annotator demographics, LLM-as-a-judge) and sensitivity analyses would quantify the impact of initialization and identify robust strategies.

## Limitations
- The identifiability proof for ternary preferences relies heavily on the linear reward assumption and the Carleman condition for moment determinacy.
- The EM-DPO algorithm assumes a finite number of preference types, which may not capture the full complexity of continuous preference distributions in real-world settings.
- The MMRA aggregation could suffer from instability if the ensemble policies are not sufficiently diverse or if regret estimates are inaccurate due to limited data.

## Confidence
- **High Confidence**: The empirical results demonstrating superior performance of ternary preferences over binary in EM-DPO (reward margins and accuracy improvements) are well-supported by the experimental data.
- **Medium Confidence**: The theoretical justification for ternary preferences ensuring identifiability under linear reward models is sound, but the proof's applicability to non-linear scenarios is uncertain.
- **Low Confidence**: The robustness of the MMRA algorithm in minimizing worst-case regret across diverse subgroups is not thoroughly validated, particularly in cases where the ensemble policies are not well-separated or the regret computation is sensitive to noise.

## Next Checks
1. **Test Non-Linear Rewards**: Evaluate the EM-DPO algorithm's performance on datasets with non-linear reward structures to assess the robustness of ternary preferences in non-linear scenarios.
2. **Continuous Preference Distributions**: Implement a variant of EM-DPO that handles continuous preference distributions instead of assuming discrete types, and compare its performance to the discrete case.
3. **Robustness of MMRA**: Conduct sensitivity analysis on the MMRA algorithm by varying the diversity of ensemble policies and the amount of preference data to determine the conditions under which regret minimization remains stable and effective.