---
ver: rpa2
title: 'Amortized Planning with Large-Scale Transformers: A Case Study on Chess'
arxiv_id: '2402.04494'
source_url: https://arxiv.org/abs/2402.04494
tags:
- chess
- games
- training
- stockfish
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that a transformer trained via supervised
  learning on a large dataset of chess games annotated with Stockfish engine evaluations
  can play chess at grandmaster level without explicit search. The method uses a standard
  decoder-only transformer architecture to predict action-values (win percentages)
  for chess positions, and the resulting policy achieves a Lichess blitz Elo of 2895
  against humans and solves challenging puzzles up to Elo 2800.
---

# Amortized Planning with Large-Scale Transformers: A Case Study on Chess

## Quick Facts
- arXiv ID: 2402.04494
- Source URL: https://arxiv.org/abs/2402.04494
- Reference count: 40
- Primary result: Transformer trained on Stockfish-annotated chess games achieves 2895 Lichess blitz Elo puzzle-solving performance

## Executive Summary
This paper demonstrates that a standard decoder-only transformer can learn to play chess at grandmaster level by predicting action-values (win percentages) for chess positions, trained via supervised learning on a large dataset of games annotated with Stockfish engine evaluations. The resulting policy achieves strong puzzle-solving performance and outperforms both GPT-3.5-turbo-instruct and AlphaZero's policy/value networks without search. The study provides extensive ablations on predictor targets, network depth, and data sampling strategies, showing that performance scales with both model size and dataset size.

## Method Summary
The authors train a decoder-only transformer architecture to predict action-values (win percentages) for chess positions using a supervised learning approach. The model is trained on a large corpus of chess games annotated with evaluations from the Stockfish engine. During inference, the model uses its predicted action-values to select moves, effectively amortizing the planning process that would normally require search. The architecture follows standard transformer design with attention mechanisms, and the training objective is to minimize the difference between predicted and actual Stockfish evaluations. The approach is evaluated on Lichess puzzles and compared against Stockfish and AlphaZero baselines.

## Key Results
- Achieves 2895 Lichess blitz Elo on puzzle-solving, surpassing grandmaster level
- Outperforms GPT-3.5-turbo-instruct and AlphaZero's policy/value networks without search
- Shows performance scales with both model size and dataset size
- Solves puzzles up to Elo 2800, demonstrating strong generalization
- Still falls short of Stockfish's performance, indicating perfect distillation remains challenging

## Why This Works (Mechanism)
The transformer architecture learns to approximate the value function of chess positions through supervised learning on engine-annotated games. By training to predict Stockfish's action-values, the model captures strategic patterns and tactical patterns without explicit search. The decoder-only design allows the model to generate moves autoregressively while attending to the current position. The large-scale training data provides diverse game scenarios that enable the model to develop robust chess understanding. The approach effectively amortizes the planning computation across all possible positions during training, allowing fast inference at test time.

## Foundational Learning

**Transformer Attention Mechanism**: Understanding how self-attention layers process chess positions by creating context-aware representations of pieces and board states. Why needed: Core to how the model reasons about chess positions. Quick check: Verify attention patterns focus on relevant pieces in tactical positions.

**Supervised Value Learning**: Concept of training models to predict expert evaluations rather than through self-play reinforcement learning. Why needed: Distinguishes this approach from AlphaZero. Quick check: Compare predicted values against ground truth Stockfish evaluations on held-out positions.

**Elo Rating Systems**: Understanding how puzzle-based Elo correlates with actual playing strength. Why needed: Critical for interpreting the claimed "grandmaster level" performance. Quick check: Verify Elo calculations follow standard Lichess puzzle rating methodology.

## Architecture Onboarding

**Component Map**: Input embedding -> Positional encoding -> Multi-head attention layers -> Feed-forward networks -> Action-value prediction

**Critical Path**: Raw chess position encoding flows through attention layers that build hierarchical representations, which are then transformed to produce action-value predictions for each legal move.

**Design Tradeoffs**: Uses standard decoder-only transformer rather than specialized chess architectures, trading potential domain-specific optimizations for broad applicability and leveraging existing scaling laws.

**Failure Signatures**: Model may struggle with deep tactics requiring precise calculation, positions with rare piece configurations, or endgame scenarios not well-represented in training data.

**3 First Experiments**: 1) Evaluate model on simple tactics puzzles to verify basic chess understanding. 2) Test model's ability to handle opening positions from various chess openings. 3) Measure prediction accuracy against Stockfish on a validation set of positions.

## Open Questions the Paper Calls Out
None provided in source material.

## Limitations
- Evaluation relies on puzzle-solving rather than full game play against strong opponents
- Training details and hyperparameters are sparse, limiting reproducibility
- Model still falls short of Stockfish's performance, suggesting perfect distillation remains out of reach
- Comparison methodology doesn't quantify absolute performance gaps in concrete terms

## Confidence

**High confidence**: Transformer architecture can learn chess action-values from supervised data; strong puzzle-solving performance achieved; scale correlates with performance.

**Medium confidence**: Claims of "grandmaster level" play based on puzzle Elo; outperforms GPT-3.5-turbo and AlphaZero policy/value networks in stated configurations.

**Low confidence**: Model maintains grandmaster performance in full games against strong opponents; supervised learning approach is viable alternative to search for real-world deployment.

## Next Checks

1. Conduct head-to-head game play against Stockfish and AlphaZero (with and without search) using standardized time controls to validate puzzle-based Elo claims and measure performance gaps directly.

2. Release the trained model weights, full training code, and evaluation datasets to enable independent replication and community benchmarking across diverse chess tasks.

3. Perform cross-dataset validation by testing the model on puzzles and positions from sources not included in training to assess true generalization versus memorization.