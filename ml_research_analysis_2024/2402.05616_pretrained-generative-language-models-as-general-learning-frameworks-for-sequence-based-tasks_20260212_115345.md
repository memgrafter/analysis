---
ver: rpa2
title: Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based
  Tasks
arxiv_id: '2402.05616'
source_url: https://arxiv.org/abs/2402.05616
tags:
- language
- instruction
- fine-tuning
- data
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pretrained foundational generative language models with millions
  of parameters were instruction fine-tuned to convert SMILES strings into IUPAC chemical
  names, a task the base models could not perform. Using 10,000 to 1,000,000 fine-tuning
  examples with OPT models (125M to 1.3B parameters) achieved up to 71% exact matches
  and BLEU scores up to 0.94, matching or exceeding prior encoder-decoder methods.
---

# Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks

## Quick Facts
- **arXiv ID**: 2402.05616
- **Source URL**: https://arxiv.org/abs/2402.05616
- **Reference count**: 40
- **Key outcome**: Instruction fine-tuned pretrained language models converted SMILES to IUPAC names with up to 71% exact matches, demonstrating their use as general sequence learning frameworks.

## Executive Summary
This paper demonstrates that pretrained foundational generative language models can be instruction fine-tuned to perform specialized sequence-to-sequence tasks they cannot do in their base form. The author fine-tuned OPT models (125M to 1.3B parameters) to convert SMILES chemical strings to IUPAC names, achieving performance matching or exceeding prior encoder-decoder methods. The work shows that small pretrained models can serve as general learning frameworks for sequence-based tasks when provided with appropriate instruction fine-tuning data and proper formatting.

## Method Summary
The method involves instruction fine-tuning pretrained language models (specifically the OPT family) on SMILES-to-IUPAC name conversion tasks. The author prepared datasets ranging from 10,000 to 1,000,000 examples from PubChem, fine-tuned models using standard transformer training with learning rate 2e-5 and batch size 4, and evaluated using exact match percentage, normalized Levenshtein edit distance, and BLEU scores. The fine-tuning used instruction formats like "Translate the following SMILES string into an IUPAC name: {SMILES}" and tested various model sizes, data quantities, and formatting strategies.

## Key Results
- OPT models (125M-1.3B parameters) achieved up to 71% exact matches and BLEU scores up to 0.94 on SMILES-to-IUPAC conversion
- Performance improved with more fine-tuning examples, more epochs (up to ~30), and larger model sizes
- TinyStories models (1M-33M parameters) also learned the task with 1M examples
- Adapter-based fine-tuning underperformed standard fine-tuning
- Proper instruction data formatting was critical, with inverted formatting causing total performance ablation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Small pretrained foundational language models can learn specialized sequence-based tasks through instruction fine-tuning despite being initially incapable of performing them.
- **Mechanism**: The pretrained models possess general linguistic patterns and sequence processing capabilities that can be adapted to new tasks by exposing them to task-specific instruction examples. The fine-tuning process updates the model weights to map the new input-output relationships while leveraging existing language understanding.
- **Core assumption**: The pretrained models have sufficient general sequence processing capabilities that can be repurposed for specialized tasks through fine-tuning, even when the base model cannot perform the task at all.
- **Evidence anchors**:
  - [abstract]: "We demonstrate that 125M, 350M, and 1.3B parameter pretrained foundational language models can be instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve near state-of-the-art results on challenging cheminformatics tasks."
  - [section 1.1]: "We chose a chemistry-based objective for our specialized small language model fine-tuning task" and "we demonstrated that pretrained foundational language models cannot perform this task with any measurable proficiency"
- **Break condition**: If the pretrained model lacks sufficient general sequence processing capabilities or the task requires capabilities entirely absent from the model's pretraining distribution, fine-tuning will fail to produce meaningful results.

### Mechanism 2
- **Claim**: Increasing model size and fine-tuning data improves performance on specialized tasks.
- **Mechanism**: Larger models have more parameters and greater representational capacity, allowing them to capture more complex patterns and relationships in the instruction fine-tuning data. More data provides better coverage of the task space and reduces overfitting.
- **Core assumption**: Model capacity and data quantity scale predictably with task performance, following established scaling laws for neural networks.
- **Evidence anchors**:
  - [abstract]: "Performance improved with more examples, more epochs (up to ~30), larger model size, and proper data formatting."
  - [section 4.2]: "We demonstrated that increasing the amount of domain-specific instruction fine-tuning data, as well as increasing the pretrained foundational language model parameter count (i.e., model size), improved the fine-tuned language model performance"
- **Break condition**: Diminishing returns occur when the model capacity exceeds what's needed for the task, or when additional data doesn't provide new information about the task space.

### Mechanism 3
- **Claim**: Proper instruction data formatting is critical for successful task learning.
- **Mechanism**: The model learns the mapping between input patterns and output patterns based on how the instruction data is structured. If the formatting doesn't match the intended use case, the model learns incorrect or incomplete mappings.
- **Core assumption**: The model can only learn what is explicitly presented in the instruction data format, and will not generalize across different instruction-output patterns.
- **Evidence anchors**:
  - [section 4.8]: "Our systematic inversion study demonstrated the importance of proper instruction fine-tuning data formatting. The baseline, with 0% inversion (i.e., standard formatting), demonstrated the best performance, with progressive erosion of fine-tuned model performance on our task as we increased the % inversion."
  - [abstract]: "we also demonstrate the role of successive language model fine-tuning epochs on improved outcomes, as well as the importance of both data formatting and pretrained foundational language model selection for instruction fine-tuning success"
- **Break condition**: If the instruction data format is ambiguous or inconsistent, the model may learn multiple conflicting mappings or fail to learn any useful mapping.

## Foundational Learning

- **Concept: Sequence-to-sequence learning** - Why needed here: The task involves converting one sequence format (SMILES) to another (IUPAC names), requiring the model to learn input-output sequence mappings.
  - Quick check question: Can you explain the difference between autoregressive generation and parallel sequence prediction in the context of this task?

- **Concept: Fine-tuning vs. training from scratch** - Why needed here: Understanding why we leverage pretrained models rather than training new models from scratch for specialized tasks.
  - Quick check question: What are the computational and data efficiency advantages of fine-tuning pretrained models compared to training from scratch?

- **Concept: BLEU and edit distance metrics** - Why needed here: These metrics are used to evaluate the quality of the generated IUPAC names against ground truth.
  - Quick check question: How would you interpret a BLEU score of 0.9 versus 0.5 in the context of chemical name generation?

## Architecture Onboarding

- **Component map**: Data preparation -> Model selection -> Fine-tuning -> Generation -> Evaluation -> Iteration
- **Critical path**: Data preparation → Model selection → Fine-tuning → Generation → Evaluation → Iteration. The bottleneck is typically fine-tuning time and data quality.
- **Design tradeoffs**: Model size vs. computational resources, data quantity vs. quality, fine-tuning epochs vs. overfitting, beam search parameters vs. generation quality.
- **Failure signatures**: Poor performance indicates either insufficient data, incorrect formatting, wrong model selection, or inadequate fine-tuning epochs. Zero performance indicates the model cannot learn the task at all.
- **First 3 experiments**:
  1. Fine-tune OPT-125M on 10,000 examples with standard formatting for 3 epochs, evaluate with BLEU score
  2. Fine-tune OPT-125M on 100,000 examples with inverted formatting for 3 epochs, evaluate with BLEU score to test formatting importance
  3. Fine-tune OPT-1.3B on 1,000,000 examples with standard formatting for 3 epochs, evaluate with both BLEU and exact match percentage to compare model sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of instruction fine-tuning epochs for maximizing performance without diminishing returns?
- Basis in paper: [explicit] The paper states that "successive fine-tuning epochs are not infinitely additive" and that "instruction fine-tuning for more than 20-30 epochs might provide diminishing returns."
- Why unresolved: The paper does not provide a definitive optimal number of epochs, only suggesting a range of 20-30.
- What evidence would resolve it: Conducting experiments with varying numbers of epochs (e.g., 10, 20, 30, 40, 50) and measuring performance metrics like BLEU score and exact match percentage would identify the optimal number.

### Open Question 2
- Question: How do different attention strategies in pretrained foundational language models affect instruction fine-tuning performance on specialized tasks?
- Basis in paper: [inferred] The paper mentions that "observed differences in instruction fine-tuned model performance, despite similar model parameter counts and architectures might be attributable to subtle differences in attention strategies associated with each model."
- Why unresolved: The paper does not explore or compare different attention strategies in detail.
- What evidence would resolve it: Comparing the performance of instruction fine-tuned models with different attention strategies (e.g., global, global-local, self, multi-query) on the same specialized task would reveal the impact of attention strategies.

### Open Question 3
- Question: What is the impact of instruction fine-tuning data formatting on the performance of language models for specialized tasks?
- Basis in paper: [explicit] The paper demonstrates that "proper formatting of the instruction fine-tuning data was paramount in achieving the desired outcome" and that "100% inversion of the data set led to total ablation of task performance."
- Why unresolved: While the paper shows the importance of data formatting, it does not explore the optimal formatting or the impact of different formatting styles.
- What evidence would resolve it: Experimenting with various instruction fine-tuning data formats and measuring their impact on performance metrics would identify the optimal formatting for specialized tasks.

## Limitations
- Results are limited to a single chemistry task (SMILES to IUPAC conversion) and may not generalize to other domains
- Adapter-based fine-tuning underperformed but the specific adapter architecture used may not represent optimal configurations
- Performance relies on automated metrics rather than human evaluation of chemical name accuracy and usability

## Confidence
- **High confidence**: The core finding that small pretrained language models can learn SMILES-to-IUPAC conversion through instruction fine-tuning is well-supported by systematic experiments with multiple model sizes, data quantities, and training configurations
- **Medium confidence**: The claim about general applicability as a "general learning framework for sequence-based tasks" extends beyond the single chemistry task studied
- **Medium confidence**: The importance of proper data formatting is demonstrated through the inversion study, but the specific formatting requirements may be task-dependent

## Next Checks
1. Apply the same fine-tuning approach to a different sequence-to-sequence task in another domain (e.g., protein sequence to function prediction) to validate cross-domain generalization
2. Systematically explore different adapter configurations to determine if performance can be improved beyond standard fine-tuning
3. Conduct expert human evaluation of the generated IUPAC names to validate that high automated metric scores correspond to chemically accurate and usable names