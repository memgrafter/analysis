---
ver: rpa2
title: 'StoryAgent: Customized Storytelling Video Generation via Multi-Agent Collaboration'
arxiv_id: '2411.04925'
source_url: https://arxiv.org/abs/2411.04925
tags:
- video
- generation
- subject
- consistency
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes StoryAgent, a multi-agent framework for customized
  storytelling video generation (CSVG). It addresses the challenge of maintaining
  subject consistency across shots in storytelling videos, which is difficult for
  existing methods like Mora and AesopAgent.
---

# StoryAgent: Customized Storytelling Video Generation via Multi-Agent Collaboration

## Quick Facts
- arXiv ID: 2411.04925
- Source URL: https://arxiv.org/abs/2411.04925
- Reference count: 21
- Key outcome: Novel multi-agent framework achieving superior protagonist consistency in customized storytelling videos through specialized agent collaboration

## Executive Summary
StoryAgent addresses the challenge of maintaining subject consistency across shots in customized storytelling video generation (CSVG), a task where existing methods struggle. The framework decomposes CSVG into specialized subtasks handled by distinct agents: story design, storyboard generation, video creation, agent coordination, and result evaluation. Key innovations include a removal-and-redrawing strategy for storyboard generation to ensure inter-shot consistency and a LoRA-BE method for customized Image-to-Video generation that enhances intra-shot temporal consistency. Experiments on public datasets (PororoSV, FlintstonesSV) and open-domain subjects demonstrate StoryAgent's significant performance improvements over state-of-the-art methods in preserving protagonist consistency across multiple evaluation metrics.

## Method Summary
StoryAgent employs a multi-agent framework where specialized agents handle distinct aspects of CSVG. The system takes story descriptions and reference subject videos as input, then uses an agent manager to coordinate five specialized agents. The story designer generates detailed descriptions, the storyboard generator creates consistent storyboards using a removal-and-redrawing pipeline with LangSAM segmentation and AnyDoor fine-tuning, and the video creator animates storyboards using a customized LoRA-BE method built on DynamiCrafter. The framework includes an observer agent for optional evaluation. The LoRA-BE method fine-tunes only LoRA parameters and block-wise token embeddings within the U-Net, incorporating a localization loss to focus attention on the subject. The entire pipeline is evaluated on public datasets (PororoSV, FlintstonesSV) and open-domain subjects, with quantitative metrics including FVD, SSIM, PSNR, LPIPS, and user studies.

## Key Results
- Outperforms state-of-the-art methods (Mora, AesopAgent) on public datasets (PororoSV, FlintstonesSV) across FVD, SSIM, PSNR, and LPIPS metrics
- Demonstrates superior protagonist consistency in user studies evaluating inter-shot and intra-shot consistency, subject-background harmony, and overall quality
- Shows effective generalization to open-domain subjects beyond training datasets

## Why This Works (Mechanism)

### Mechanism 1
The removal and redrawing strategy in the storyboard generator ensures inter-shot subject consistency by segmenting the subject from each generated storyboard image and replacing it with a reference subject image. This maintains visual consistency of the protagonist across shots. The core assumption is that LangSAM segmentation masks accurately isolate the subject and StoryAnyDoor fine-tuning preserves subject identity during redrawing. Break condition: If segmentation fails or redrawer cannot preserve subject identity, inter-shot consistency degrades.

### Mechanism 2
LoRA-BE enhances intra-shot temporal consistency by focusing the model on the subject and learning subject-specific embeddings. The method fine-tunes only LoRA parameters and block-wise token embeddings within the U-Net, along with a localization loss to force attention on the subject, reducing background interference. The core assumption is that the U-Net's cross-attention maps can be effectively steered to focus on the subject using the localization loss, and LoRA-BE parameters are sufficient to capture the subject's identity. Break condition: If localization loss does not effectively focus attention or LoRA-BE parameters cannot capture subject identity, intra-shot consistency is compromised.

### Mechanism 3
Multi-agent collaboration decomposes the complex CSVG task into manageable subtasks, enabling specialized optimization and improved control. By assigning distinct roles to specialized agents (story designer, storyboard generator, video creator, agent manager, observer), the framework allows each agent to focus on its specific task, leading to better overall performance. The core assumption is that each agent can perform its designated task effectively and the agent manager can coordinate agents efficiently. Break condition: If any agent fails to perform its task or agent manager cannot coordinate efficiently, overall performance degrades.

## Foundational Learning

- Concept: Multi-agent systems
  - Why needed here: CSVG is a complex task that benefits from decomposition into specialized subtasks, which can be efficiently handled by multiple agents
  - Quick check question: What are the advantages of using a multi-agent system for a complex task like CSVG?

- Concept: Diffusion models
  - Why needed here: Diffusion models are used for both image and video generation, providing the generative capabilities needed for storyboard creation and video animation
  - Quick check question: How do diffusion models work, and what are their advantages for image and video generation?

- Concept: Fine-tuning and adaptation techniques (LoRA, Text Inversion)
  - Why needed here: Fine-tuning techniques are used to adapt pre-trained models to specific subjects, enabling consistent character representation across shots
  - Quick check question: What are the differences between LoRA and Text Inversion, and when would you use each technique?

## Architecture Onboarding

- Component map:
  User input -> Agent Manager -> Story Designer -> Storyboard Generator -> Video Creator -> Observer (optional) -> Output

- Critical path: User input → Agent Manager → Story Designer → Storyboard Generator → Video Creator → Observer (optional) → Output

- Design tradeoffs:
  - Complexity vs. performance: Multi-agent system increases complexity but improves performance
  - Flexibility vs. control: Framework allows component replacement but requires careful coordination
  - Training time vs. customization: LoRA-BE fine-tuning improves customization but increases training time

- Failure signatures:
  - Inconsistent character appearance across shots: Indicates issues with storyboard generation or segmentation
  - Distorted or unnatural character movements: Indicates issues with video creation or LoRA-BE
  - Poor text-video alignment: Indicates issues with story design or video creation
  - Low overall video quality: Indicates issues with any component in the pipeline

- First 3 experiments:
  1. Test storyboard generation with a simple prompt and reference image to verify inter-shot consistency
  2. Test video creation with a consistent storyboard and story descriptions to verify intra-shot consistency
  3. Test the full pipeline with a complex prompt and reference videos to evaluate overall performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of StoryAgent scale with the number of shots in the storytelling video? The paper demonstrates effectiveness on 4-shot videos but doesn't explore performance on longer sequences. This remains unresolved because the paper focuses on fixed-length 4-shot sequences for evaluation, leaving open whether the multi-agent framework maintains consistency and quality for longer or variable-length videos. Evidence needed: Experiments varying the number of shots (e.g., 2, 4, 8, 16) in both public and open-domain datasets, with quantitative metrics and user studies comparing consistency and quality across different sequence lengths.

### Open Question 2
What is the impact of different Large Language Models (LLMs) on the quality of story descriptions generated by the Story Designer agent? The paper mentions using GPT-4, Claude, or Gemini but only uses GPT-4 in experiments without comparing different LLM choices. This remains unresolved because the paper does not provide systematic comparison of different LLMs for story description generation. Evidence needed: Controlled experiments using the same storyboard generation and video creation pipeline with story descriptions generated by different LLMs, comparing resulting video quality metrics and user study scores.

### Open Question 3
How does the proposed LoRA-BE method compare to other customization techniques for Image-to-Video generation beyond text inversion and SparseCtrl? The paper compares LoRA-BE against TI-SparseCtrl and SVD but does not compare against other customization methods like DreamBooth, HyperNetworks, or other LoRA-based approaches. This remains unresolved because ablation studies focus on specific baselines without exploring the broader landscape of customization techniques. Evidence needed: Direct comparison of LoRA-BE against alternative customization methods (e.g., DreamBooth, HyperNetworks, other LoRA variants) using identical reference videos and evaluation metrics on the same datasets.

## Limitations
- Performance heavily depends on quality and diversity of reference videos (4-5 videos of 1-2 seconds each per subject)
- Effectiveness relies on LangSAM segmentation model's ability to accurately isolate subjects across diverse scenes and poses
- Computational cost of maintaining multiple specialized agents and fine-tuning LoRA-BE parameters may pose deployment challenges

## Confidence

**High Confidence Claims:**
- Multi-agent decomposition strategy effectively addresses CSVG task
- Storyboard generation pipeline with removal-and-redrawing improves inter-shot consistency
- LoRA-BE fine-tuning method enhances intra-shot temporal consistency

**Medium Confidence Claims:**
- StoryAgent significantly outperforms state-of-the-art methods across all evaluated metrics
- Framework generalizes well to open-domain subjects beyond training datasets
- User studies validate superior performance in inter-shot and intra-shot consistency

**Low Confidence Claims:**
- Scalability to longer narratives (more than 5 shots) and complex storylines
- Performance consistency across diverse subject types and video styles not in training data
- Computational efficiency and practical deployment feasibility in resource-constrained environments

## Next Checks

1. **Long-form Storytelling Evaluation**: Extend the framework to generate videos with 10+ shots and evaluate whether subject consistency and narrative coherence are maintained throughout extended sequences to reveal whether multi-agent coordination scales effectively to longer narratives.

2. **Cross-domain Generalization Test**: Apply StoryAgent to subjects and scenes significantly different from the PororoSV and FlintstonesSV datasets (e.g., historical figures, abstract art, or diverse cultural contexts) to assess the framework's robustness and generalization capabilities beyond its training distribution.

3. **Computational Efficiency Benchmark**: Measure and compare the computational requirements (GPU memory, inference time, fine-tuning duration) of StoryAgent against baseline methods when processing multiple subjects and shots, providing practical insights into deployment feasibility and resource requirements.