---
ver: rpa2
title: Reasoning Limitations of Multimodal Large Language Models. A Case Study of
  Bongard Problems
arxiv_id: '2411.01173'
source_url: https://arxiv.org/abs/2411.01173
tags:
- images
- reasoning
- problems
- bongard
- right
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Multimodal Large Language Models (MLLMs) were evaluated on Bongard
  Problems (BPs), which test abstract visual reasoning by requiring identification
  of shared concepts across image matrices. A set of seven novel strategies was proposed
  to solve BPs using MLLMs, including direct generation, descriptive and contrastive
  reasoning, and their iterative and image-inclusion variants.
---

# Reasoning Limitations of Multimodal Large Language Models. A Case Study of Bongard Problems

## Quick Facts
- **arXiv ID**: 2411.01173
- **Source URL**: https://arxiv.org/abs/2411.01173
- **Reference count**: 40
- **Primary result**: MLLMs solved only up to 17 out of 100 Bongard Problems even with optimized strategies, showing significant limitations in abstract visual reasoning.

## Executive Summary
This study investigates the abstract visual reasoning capabilities of Multimodal Large Language Models (MLLMs) using Bongard Problems (BPs), which require identifying shared concepts across image matrices. The researchers developed seven novel strategies for BP solving, including direct generation, descriptive and contrastive reasoning, and their iterative variants. They evaluated four proprietary and four open-access MLLMs across synthetic BPs, Bongard HOI, Bongard-OpenWorld, and a newly introduced Bongard-RWR dataset that represents synthetic BP concepts in real-world images. The results reveal that MLLMs struggle significantly with abstract concept recognition, solving only up to 17% of synthetic problems even with optimal strategies, while human participants consistently outperformed all models.

## Method Summary
The researchers designed a comprehensive evaluation framework for MLLM reasoning capabilities using Bongard Problems as a testbed for abstract visual reasoning. They created seven distinct problem-solving strategies ranging from direct concept generation to iterative reasoning approaches, applied these to both synthetic and real-world Bongard datasets, and compared MLLM performance against human baselines. The Bongard-RWR dataset was specifically introduced to bridge the gap between synthetic concepts and real-world images, allowing researchers to isolate whether MLLM failures stemmed from domain specificity or fundamental abstract reasoning limitations. Performance was measured across four proprietary models (GPT-4V, Gemini Pro, Claude 3, Llama 3) and four open-access alternatives (InternVL, Llava, Yi, Qwen2-VL).

## Key Results
- MLLMs achieved maximum accuracy of 17% on synthetic Bongard Problems despite seven different optimization strategies
- Performance improved on real-world datasets but remained low on Bongard-RWR (17/100), indicating fundamental abstract concept recognition limitations
- Human participants consistently outperformed all MLLM models across all problem types and strategies
- Open-access models generally performed worse than proprietary counterparts, though all struggled with abstract reasoning tasks

## Why This Works (Mechanism)
The study demonstrates that current MLLMs face fundamental challenges in abstract visual reasoning, as evidenced by their consistent failure across multiple Bongard Problem datasets and reasoning strategies. The mechanism underlying this limitation appears to be the models' inability to generalize abstract concepts from visual patterns, particularly when these concepts require identifying subtle commonalities across diverse image sets. The performance gap between synthetic and real-world datasets suggests that while MLLMs may have some surface-level visual pattern recognition capabilities, they lack the deeper conceptual understanding necessary for abstract reasoning tasks that humans perform intuitively.

## Foundational Learning
- **Bongard Problems**: Visual reasoning puzzles requiring identification of abstract concepts shared across image sets - needed to test MLLM abstract reasoning beyond pattern matching; quick check: verify problem structure follows standard BP format with left/right image sets
- **Multimodal Reasoning**: Integration of visual and textual processing for problem solving - needed to understand how MLLMs combine image understanding with language reasoning; quick check: examine model architecture for cross-modal attention mechanisms
- **Contrastive Reasoning**: Comparing positive and negative examples to identify distinguishing features - needed for Bongard Problem strategies that require understanding differences between concept groups; quick check: validate strategy implementation correctly uses both image sets
- **Iterative Refinement**: Repeated reasoning cycles to improve answer quality - needed to test whether multiple reasoning passes help MLLMs overcome initial abstract concept recognition failures; quick check: measure performance improvement across iteration counts
- **Synthetic vs Real-world Generalization**: Testing model performance across different data domains - needed to isolate whether failures are due to domain specificity or fundamental reasoning limitations; quick check: compare performance consistency across dataset types
- **Human Baseline Comparison**: Establishing human performance standards for AI evaluation - needed to contextualize MLLM limitations and identify performance gaps; quick check: verify human participant selection and testing methodology

## Architecture Onboarding

**Component Map**
MLLM Core -> Vision Encoder -> Language Model -> Reasoning Strategy -> Output Generation

**Critical Path**
Input Images → Vision Encoder → Multimodal Fusion → Reasoning Strategy Application → Text Output → Concept Identification

**Design Tradeoffs**
The study balances between testing fundamental MLLM capabilities versus specific implementation limitations. Using synthetic BPs provides controlled testing but may not reflect real-world complexity, while real-world datasets introduce confounding variables. The seven strategies represent a spectrum from simple to complex approaches, allowing isolation of whether failures stem from strategy inadequacy or fundamental model limitations.

**Failure Signatures**
Consistent underperformance across all MLLMs and strategies on synthetic BPs indicates systemic rather than model-specific failures. The particular difficulty with Bongard-RWR suggests problems with transferring abstract concepts to real-world contexts. Performance gaps between proprietary and open-access models may indicate training data or architectural differences affecting abstract reasoning.

**First 3 Experiments**
1. Baseline evaluation of all eight MLLMs on synthetic Bongard Problems without optimization strategies
2. Systematic testing of each of the seven proposed strategies across all model types
3. Comparative analysis of MLLM performance versus human participants on identical problem sets

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on synthetic Bongard Problems, which may not fully represent real-world abstract reasoning complexity
- Human baseline comparison lacks detailed characterization of participant expertise and methodology
- Study does not explore potential improvements through fine-tuning or advanced prompt engineering
- Performance variations between proprietary and open-access models suggest results may reflect implementation differences rather than fundamental MLLM limitations

## Confidence
- **High Confidence**: MLLMs show consistently poor performance on synthetic Bongard Problems (max 17% accuracy), indicating clear limitations in abstract visual reasoning tasks
- **Medium Confidence**: Performance improvements on real-world datasets suggest MLLMs struggle more with abstract concept recognition than domain specificity, though this conclusion depends on the representativeness of the test datasets
- **Medium Confidence**: The proposed seven strategies represent a systematic approach to Bongard Problem solving, but their relative effectiveness varies significantly across models and problem types

## Next Checks
1. Conduct ablation studies on the seven proposed strategies to isolate which components contribute most to performance improvements, and test whether these strategies transfer to other abstract reasoning benchmarks beyond Bongard Problems

2. Evaluate MLLM performance on Bongard Problems after targeted fine-tuning on abstract visual reasoning tasks to determine whether current limitations reflect architectural constraints or insufficient training data exposure

3. Implement controlled human participant studies with standardized expertise levels and systematic variation of problem complexity to establish more robust performance baselines for comparison with MLLM results