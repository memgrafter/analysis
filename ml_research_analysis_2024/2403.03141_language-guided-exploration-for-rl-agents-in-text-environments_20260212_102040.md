---
ver: rpa2
title: Language Guided Exploration for RL Agents in Text Environments
arxiv_id: '2403.03141'
source_url: https://arxiv.org/abs/2403.03141
tags:
- actions
- task
- guide
- learning
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes Language Guided Exploration (LGE), a framework
  that uses a pre-trained language model (GUIDE) to provide decision-level guidance
  to a reinforcement learning agent (EXPLORER) in text environments. The GUIDE uses
  contrastive learning to produce a set of relevant actions for the given task description,
  which reduces the action space and guides the EXPLORER.
---

# Language Guided Exploration for RL Agents in Text Environments

## Quick Facts
- arXiv ID: 2403.03141
- Source URL: https://arxiv.org/abs/2403.03141
- Reference count: 12
- Key outcome: LGE framework improves RL agent performance by 35% on ScienceWorld benchmark using language model-guided action pruning

## Executive Summary
This paper introduces Language Guided Exploration (LGE), a framework that improves reinforcement learning in text environments by using a pre-trained language model to filter irrelevant actions. The GUIDE model uses contrastive learning to identify relevant actions for each task description, while the EXPLORER agent focuses its exploration on this reduced action space. On the ScienceWorld benchmark, LGE significantly outperforms vanilla RL agents and sophisticated baselines like Behavior Cloning and Text Decision Transformer, achieving 35% higher mean returns.

## Method Summary
LGE consists of two components: GUIDE and EXPLORER. GUIDE is a BERT-base model fine-tuned with SimCSE contrastive learning to map task descriptions to relevant actions by learning semantic similarity in a shared embedding space. For each task, GUIDE scores all valid actions by cosine similarity to the task embedding and selects the top-k most relevant ones. EXPLORER is a DRRN agent that receives the GUIDE's filtered action set at each step and learns to select actions using Q-learning with prioritized experience replay. The framework balances between using GUIDE's guidance (probability 1-ϵ) and random exploration (probability ϵ) to maintain exploration of potentially relevant but unguaranteed actions.

## Key Results
- GUIDE achieves 99% recall and 68% precision in identifying relevant actions
- EXPLORER using LGE improves mean returns by 35% compared to DRRN baseline
- LGE outperforms sophisticated baselines including Behavior Cloning and Text Decision Transformer
- GUIDE generalizes well to unseen variations while EXPLORER learns separate policies per task type

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GUIDE uses contrastive learning to map task descriptions to relevant actions in a shared embedding space, producing high recall and precision for pruning irrelevant actions
- Mechanism: SimCSE-based contrastive learning creates embeddings where task embeddings are close to embeddings of their relevant actions and far from irrelevant ones. The GUIDE scores actions by cosine similarity to the task embedding and selects the top-k actions
- Core assumption: Task descriptions contain sufficient information to distinguish relevant from irrelevant actions without requiring full state context
- Evidence anchors:
  - [abstract]: "The GUIDE uses contrastive learning to produce a set of relevant actions for the given task description"
  - [section]: "We use SimCSE (Gao et al., 2021), a contrastive learning framework, to finetune the GUIDE LM"
  - [corpus]: Weak - No direct corpus evidence on contrastive learning performance for this specific task-action mapping problem
- Break condition: If task descriptions become ambiguous or rely heavily on current state context not captured in the description, GUIDE's recall/precision will degrade

### Mechanism 2
- Claim: EXPLORER benefits from reduced action space by focusing exploration on actions that are more likely to be relevant to the task
- Mechanism: By sampling from the GUIDE's top-k relevant actions instead of the full valid action set, EXPLORER's policy learning concentrates on promising trajectories, reducing sample complexity and avoiding exploration of clearly irrelevant actions
- Core assumption: The GUIDE's pruning doesn't remove any actions that could lead to task completion
- Evidence anchors:
  - [abstract]: "The EXPLORER using LGE improves the mean returns by 35% compared to the RL baseline"
  - [section]: "The EXPLORER using LGE improves the mean returns by 35% compared to the RL baseline"
  - [corpus]: Weak - No direct corpus evidence on RL sample efficiency gains from action space reduction
- Break condition: If GUIDE's pruning threshold is too aggressive (removing actually relevant actions) or too lenient (including too many irrelevant actions), EXPLORER's learning efficiency gains disappear

### Mechanism 3
- Claim: LGE generalizes better to unseen variations because GUIDE learns task-action relevance patterns independent of specific objects, while EXPLORER learns object-independent policies for each task type
- Mechanism: GUIDE is trained only on task descriptions and gold actions without object-specific context, learning semantic patterns. EXPLORER learns separate policies per task type but these are shared across all variations of that type, allowing transfer to new objects
- Core assumption: Task-action relevance depends more on task semantics than specific object instances
- Evidence anchors:
  - [section]: "Unlike the EXPLORER, which uses different policy for each task type, we train a common GUIDE across all tasks"
  - [section]: "the GUIDE's strong generalization ability on new variations"
  - [corpus]: Weak - No direct corpus evidence on this specific transfer mechanism
- Break condition: If task semantics strongly depend on specific object properties or if new variations introduce fundamentally different task types, generalization fails

## Foundational Learning

- Concept: Contrastive learning for semantic similarity
  - Why needed here: GUIDE needs to determine which actions are semantically relevant to a task description by comparing their embeddings
  - Quick check question: What is the difference between supervised contrastive loss and standard classification loss?

- Concept: Partially Observable Markov Decision Process (POMDP)
  - Why needed here: ScienceWorld is a POMDP where the agent only observes partial state information through text descriptions
  - Quick check question: How does the agent's observation model differ from a fully observable MDP?

- Concept: Deep Reinforcement Relevance Network (DRRN)
  - Why needed here: EXPLORER uses DRRN to handle large action spaces in text environments by learning separate embeddings for states and actions
  - Quick check question: What is the key architectural difference between DRRN and standard DQN?

## Architecture Onboarding

- Component map: Task description → GUIDE → top-k actions → EXPLORER policy → environment step → reward → replay buffer → EXPLORER update → repeat

- Critical path: Task description → GUIDE → top-k actions → EXPLORER policy → environment step → reward → replay buffer → EXPLORER update → repeat

- Design tradeoffs:
  - GUIDE vs full LLM: Smaller BERT-base vs larger models for faster inference and better training stability
  - Top-k threshold: Higher k improves recall but reduces exploration benefits
  - Separate vs shared policies: EXPLORER uses separate policies per task type for better specialization

- Failure signatures:
  - GUIDE recall < 90%: EXPLORER missing relevant actions, performance degrades
  - GUIDE precision < 50%: EXPLORER wasting samples on irrelevant actions
  - EXPLORER training instability: Learning rate too high or replay buffer issues

- First 3 experiments:
  1. Verify GUIDE's recall/precision on validation set with different k values
  2. Test EXPLORER with GUIDE vs random action selection baseline
  3. Evaluate EXPLORER performance with GUIDE-only (ϵ=0) vs pure RL (ϵ=1) vs LGE (0<ϵ<1)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the GUIDE model's performance change when using different pre-trained language models (e.g., T5, RoBERTa) as its base architecture?
- Basis in paper: [explicit] The paper mentions using BERT-base and briefly discusses a rudimentary experiment with Macaw (T5 Large), but does not provide a comprehensive comparison with other pre-trained models
- Why unresolved: The paper does not explore the impact of different pre-trained language models on the GUIDE's performance, leaving the question of optimal base architecture open
- What evidence would resolve it: A systematic comparison of the GUIDE's performance using various pre-trained language models (e.g., BERT, RoBERTa, T5, etc.) on the ScienceWorld benchmark would provide insights into the optimal base architecture

### Open Question 2
- Question: How does the LGE framework's performance scale with the size and complexity of the action space in text-based environments?
- Basis in paper: [inferred] The paper demonstrates LGE's effectiveness on ScienceWorld, which has a combinatorially large action space. However, it does not explore how the framework's performance changes with increasing action space size and complexity
- Why unresolved: The paper does not provide empirical evidence on the scalability of the LGE framework to more complex text-based environments with larger action spaces
- What evidence would resolve it: Evaluating the LGE framework's performance on a range of text-based environments with varying action space sizes and complexities would provide insights into its scalability

### Open Question 3
- Question: How does the GUIDE model's performance generalize to other domains beyond science, such as healthcare or finance?
- Basis in paper: [inferred] The paper focuses on the ScienceWorld benchmark, which is centered around scientific concepts and skills. It does not explore the GUIDE's performance in other domains
- Why unresolved: The paper does not provide evidence on the GUIDE's ability to generalize its knowledge-guided exploration to other domains beyond science
- What evidence would resolve it: Evaluating the GUIDE's performance on text-based environments from other domains (e.g., healthcare, finance, etc.) would provide insights into its generalizability

### Open Question 4
- Question: How does the GUIDE model's performance change when using different training strategies for the contrastive learning objective?
- Basis in paper: [inferred] The paper uses a specific training strategy for the GUIDE's contrastive learning objective. However, it does not explore alternative training strategies or their impact on performance
- Why unresolved: The paper does not provide evidence on the impact of different training strategies for the GUIDE's contrastive learning objective
- What evidence would resolve it: A systematic comparison of the GUIDE's performance using different training strategies for the contrastive learning objective (e.g., different batch sizes, learning rates, etc.) would provide insights into optimal training strategies

## Limitations

- Evaluation limited to single benchmark (ScienceWorld) with 20 tasks, potentially limiting generalizability
- High recall/precision metrics lack statistical significance tests and confidence intervals
- Trade-off between GUIDE pruning aggressiveness and EXPLORER performance not thoroughly explored
- No analysis of performance degradation when task descriptions are ambiguous or require exploration of unguaranteed actions

## Confidence

- **High confidence**: The core mechanism of using contrastive learning for task-action mapping is well-established (SimCSE framework), and the reported performance improvements over vanilla DRRN are substantial and clearly demonstrated
- **Medium confidence**: The generalization claims rely on the assumption that task semantics are independent of specific objects, which is plausible but not rigorously tested with truly novel variations
- **Low confidence**: The paper's claims about GUIDE achieving "high recall and precision" lack statistical validation, and the exact impact of GUIDE's filtering on EXPLORER's sample efficiency is not quantified

## Next Checks

1. **Statistical validation**: Replicate the GUIDE's performance metrics (recall, precision, MAP) on the validation set with confidence intervals across multiple random seeds to verify the claimed 99% recall and 68% precision are statistically significant
2. **Ablation study on ϵ**: Systematically vary the exploration parameter ϵ (0, 0.25, 0.5, 0.75, 1.0) and measure how GUIDE's pruning affects EXPLORER's learning curves, convergence speed, and final performance to quantify the trade-off between exploration and exploitation
3. **Cross-benchmark generalization**: Test LGE on at least one additional text-based RL benchmark (e.g., TextWorld or CoinCollector) to verify the framework's effectiveness extends beyond ScienceWorld and to identify any domain-specific limitations