---
ver: rpa2
title: What Makes Large Language Models Reason in (Multi-Turn) Code Generation?
arxiv_id: '2410.08105'
source_url: https://arxiv.org/abs/2410.08105
tags:
- code
- pass
- llama
- reasoning
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates the impact of prompting
  strategies, including chain-of-thought (CoT) techniques and execution feedback,
  on multi-turn code generation. The authors perform a comprehensive grid search over
  reasoning, instruction, and feedback prompts across multiple model families and
  sizes on competitive programming benchmarks.
---

# What Makes Large Language Models Reason in (Multi-Turn) Code Generation?

## Quick Facts
- arXiv ID: 2410.08105
- Source URL: https://arxiv.org/abs/2410.08105
- Reference count: 40
- Primary result: Combining reasoning and instruction prompts with multi-turn execution feedback significantly improves code generation performance

## Executive Summary
This paper systematically investigates how prompting strategies, particularly chain-of-thought (CoT) techniques and execution feedback, affect multi-turn code generation performance. Through comprehensive grid searches across multiple model families and sizes on competitive programming benchmarks, the authors identify optimal combinations of reasoning and instruction prompts. They demonstrate that while CoT is crucial for performance, execution feedback granularity affects exploration-exploitation tradeoffs, and finetuning models on successful multi-turn CoT trajectories enables them to internalize reasoning capabilities without explicit prompts during inference.

## Method Summary
The authors conduct a grid search over 8 reasoning prompts and 6 instruction prompts across Llama 3.0/3.1 models (8B, 70B, 405B) on CodeContests and TACO datasets. They evaluate single-turn and multi-turn code generation settings with different execution feedback granularities (binary, failed tests, failed & passed tests, LDB variants). The multi-turn setting allows up to 3 code refinements based on execution feedback. They also perform rejection sampling finetuning on successful multi-turn trajectories to assess whether models can internalize the reasoning behavior. Performance is measured using pass@1 and pass@100 metrics.

## Key Results
- Combining reasoning and instruction prompts achieves optimal single-turn performance, especially for larger models and harder problems
- Multi-turn settings with CoT provide significant gains over single-turn approaches, while execution feedback granularity affects exploration-exploitation tradeoffs
- Finetuning on multi-turn CoT data enables models to internalize reasoning and improve performance without explicit CoT prompts during inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining reasoning and instruction prompts yields better performance than either alone
- Mechanism: Reasoning prompts help models understand the problem (NL → NL), while instruction prompts guide code generation style and structure (NL → Code). Their combination provides both understanding and better code generation guidance
- Core assumption: Models can effectively process sequential reasoning and then instruction-based code generation
- Evidence anchors:
  - [abstract]: "combining reasoning prompts and instruction prompts achieves the best single-turn performance"
  - [section]: "Interestingly, we found that not all sets of prompts are beneficial. the worst combination degrades the pass@100 of Llama 3.1 70B by up to 5.4%"
  - [corpus]: Weak - corpus doesn't contain direct evidence for this specific mechanism
- Break condition: If models cannot maintain coherence between reasoning and instruction phases, or if reasoning prompts introduce incorrect problem understanding

### Mechanism 2
- Claim: Multi-turn CoT with execution feedback provides significant performance gains over single-turn approaches
- Mechanism: Multi-turn allows models to iteratively refine code based on execution feedback, effectively learning from failures. The CoT component enhances this by providing reasoning traces that guide the refinement process
- Core assumption: Execution feedback is interpretable by the model and can guide meaningful code modifications
- Evidence anchors:
  - [abstract]: "In multi-turn settings, CoT provides significant gains, while simple execution feedback is sufficient"
  - [section]: "The multi-turn setting alone brings modest gains and is sometimes worse than its single-turn counterpart under equal sampling budgets. The combination with CoT provides a significant performance boost on all models we study."
  - [corpus]: Weak - corpus lacks direct evidence about multi-turn execution feedback mechanisms
- Break condition: If execution feedback is too complex or noisy for the model to interpret meaningfully, or if CoT prompts interfere with code generation

### Mechanism 3
- Claim: Finetuning on multi-turn CoT data enables models to internalize reasoning behavior
- Mechanism: Rejection sampling finetuning uses successful trajectories (where final code passes tests) to train models to generate reasoning traces naturally without explicit prompts. This transfers the reasoning capability learned from prompts into the model's inherent behavior
- Core assumption: Correct final code correlates with correct reasoning traces in the training data
- Evidence anchors:
  - [abstract]: "they demonstrate that finetuning models on multi-turn CoT data enables them to internalize reasoning and achieve improvements in performance and scalability without explicit CoT prompts during inference"
  - [section]: "We then show how finetuning with such an optimal configuration allows models to internalize the induced reasoning process and obtain improvements in performance and scalability for multi-turn code generation"
  - [corpus]: Weak - corpus lacks direct evidence about rejection sampling finetuning effectiveness
- Break condition: If incorrect reasoning traces are present in successful trajectories, or if the finetuning process fails to generalize beyond the training distribution

## Foundational Learning

- Concept: Chain-of-thought (CoT) prompting
  - Why needed here: Understanding CoT is fundamental to grasping how reasoning prompts improve code generation performance
  - Quick check question: What is the difference between single-turn and multi-turn CoT in the context of code generation?

- Concept: Execution feedback granularity
  - Why needed here: Different levels of execution feedback (binary, failed tests, LDB) have different impacts on model behavior and performance
  - Quick check question: How does the granularity of execution feedback affect the exploration-exploitation tradeoff in multi-turn code generation?

- Concept: Rejection sampling finetuning
  - Why needed here: This technique is crucial for understanding how to transfer prompt-elicited reasoning behavior into the model's inherent capabilities
  - Quick check question: What is the key assumption underlying rejection sampling finetuning for multi-turn CoT data?

## Architecture Onboarding

- Component map: Problem statement → Reasoning prompt generation → Instruction prompt application → Code generation → Execution environment → Execution feedback processing → Iterative refinement loop (for multi-turn) → Test evaluation

- Critical path: For single-turn: problem statement → reasoning prompt → instruction prompt → code generation → test evaluation. For multi-turn: problem statement → code generation → execution → feedback processing → reasoning/instruction prompt → code refinement → test evaluation (repeated up to N times)

- Design tradeoffs: Single-turn vs multi-turn approaches balance computational cost against performance gains. Granular execution feedback provides more information but may reduce code diversity. Reasoning prompts improve understanding but add latency and may introduce incorrect problem interpretations

- Failure signatures: Performance degradation when combining reasoning and instruction prompts indicates poor coherence between phases. Inconsistent improvements across model sizes suggest prompt sensitivity. Lack of diversity in multi-turn trajectories indicates exploitative behavior dominating exploration

- First 3 experiments:
  1. Grid search across reasoning and instruction prompts to identify effective combinations for different sampling budgets
  2. Multi-turn ablation comparing different execution feedback granularities (binary, failed tests, LDB) to measure impact on code diversity and performance
  3. Rejection sampling finetuning on successful multi-turn trajectories to evaluate internalization of reasoning behavior and generalization to no-CoT settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between reasoning prompts and execution feedback in multi-turn settings, and how do they interact to influence model performance?
- Basis in paper: [explicit] The paper explores how reasoning prompts (CoT) and execution feedback (binary, failed tests, failed & passed tests, LDB) affect performance, finding that reasoning prompts are crucial but execution feedback granularity impacts exploratory vs. exploitative behavior
- Why unresolved: The paper doesn't fully disentangle whether reasoning prompts compensate for less granular feedback or vice versa. The interaction effect isn't isolated experimentally
- What evidence would resolve it: A factorial experiment testing all combinations of reasoning prompts and feedback types would reveal interaction effects and identify optimal pairings

### Open Question 2
- Question: How do different instruction prompts (e.g., "describe," "modularity," "solution") impact performance on specific problem types within competitive programming benchmarks?
- Basis in paper: [explicit] The paper compares instruction prompts and finds "solution" prompts generally work best, but acknowledges variability across models and sampling sizes
- Why unresolved: The paper doesn't analyze which instruction prompts are most effective for different problem categories (e.g., combinatorics vs. dynamic programming)
- What evidence would resolve it: Analyzing performance by problem type when using different instruction prompts would reveal targeted effectiveness

### Open Question 3
- Question: Can the effectiveness of chain-of-thought prompting be generalized beyond competitive programming to other code generation tasks like software engineering or data science?
- Basis in paper: [explicit] The paper evaluates generalization of RFT to HumanEval+, MBPP+, and LiveCodeBench, finding similar or slightly improved performance, but doesn't deeply analyze task-specific effectiveness
- Why unresolved: The evaluation covers only code generation benchmarks and doesn't explore whether CoT benefits transfer to broader software development contexts
- What evidence would resolve it: Evaluating CoT techniques on diverse software engineering tasks (e.g., API integration, debugging, documentation) would determine generalizability

## Limitations

- Experimental scope limited to Llama models, potentially limiting generalizability to other architectures
- Grid search represents finite subset of prompt combinations, leaving uncertainty about global optimality
- Focus on competitive programming benchmarks may not represent real-world code generation scenarios

## Confidence

**High Confidence:** The claim that multi-turn execution feedback with CoT provides significant performance gains is well-supported by systematic experiments showing consistent improvements across model sizes and problem difficulties

**Medium Confidence:** The assertion that finetuning on multi-turn CoT data enables models to internalize reasoning behavior is supported by empirical results but depends on the assumption that successful final code correlates with correct reasoning traces

**Low Confidence:** The claim that combining reasoning and instruction prompts yields the best single-turn performance has contradictory evidence in the paper itself, showing certain combinations actually degrade performance by up to 5.4%

## Next Checks

1. **Prompt Robustness Test:** Systematically test the identified optimal reasoning-instruction prompt combinations across different model families (not just Llama) and smaller model sizes to assess generalizability and identify prompt sensitivity thresholds

2. **Real-World Application Study:** Evaluate the multi-turn CoT approach on practical code generation tasks beyond competitive programming, such as API integration, bug fixing, or feature implementation, to assess external validity

3. **Long-Term Memory Assessment:** Conduct longitudinal studies tracking whether models finetuned on multi-turn CoT data maintain their internalized reasoning capabilities over time and across different domains, testing for potential catastrophic forgetting or domain-specific degradation