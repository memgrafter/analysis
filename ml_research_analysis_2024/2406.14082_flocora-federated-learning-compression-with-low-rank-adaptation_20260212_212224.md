---
ver: rpa2
title: 'FLoCoRA: Federated learning compression with low-rank adaptation'
arxiv_id: '2406.14082'
source_url: https://arxiv.org/abs/2406.14082
tags:
- lora
- flocora
- learning
- parameters
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FLoCoRA, a federated learning compression
  method that applies Low-Rank Adaptation (LoRA) to train small-vision models from
  scratch. Unlike prior works focused on fine-tuning large models, FLoCoRA integrates
  LoRA adapters into federated learning to reduce communication costs by 4.8x while
  maintaining less than 1% accuracy loss on CIFAR-10 with ResNet-8.
---

# FLoCoRA: Federated learning compression with low-rank adaptation

## Quick Facts
- arXiv ID: 2406.14082
- Source URL: https://arxiv.org/abs/2406.14082
- Reference count: 25
- Primary result: Achieves up to 18.6x communication reduction with under 1% accuracy loss on federated vision tasks

## Executive Summary
FLoCoRA introduces a novel federated learning compression method that applies Low-Rank Adaptation (LoRA) to train small-vision models from scratch, addressing the communication bottleneck in federated learning. Unlike prior works focused on fine-tuning large models, FLoCoRA integrates LoRA adapters into federated learning to reduce communication costs by 4.8x while maintaining less than 1% accuracy loss on CIFAR-10 with ResNet-8. The method keeps the original model frozen and only communicates low-rank adapter parameters between clients and server, with further enhancement through affine quantization achieving up to 18.6x reduction in communication costs.

## Method Summary
FLoCoRA applies LoRA to federated learning by freezing the original neural network parameters and only training low-rank adapter matrices (B and A) that capture weight updates. These adapters are communicated between clients and server instead of full model parameters, achieving significant communication reduction. The method is further enhanced with affine quantization, which reduces the bit representation of LoRA parameters while preserving essential information through scaling factors and zero points. This approach reduces both communication and memory requirements while maintaining compatibility with existing federated learning optimization methods like FedAvg.

## Key Results
- 4.8x reduction in communication costs with under 1% accuracy loss on CIFAR-10 with ResNet-8
- Up to 18.6x reduction in communication costs with affine quantization and under 1% accuracy loss on ResNet-18
- Maintains compatibility with existing federated learning optimization methods
- Reduces memory requirements for training due to low-rank adaptation structure

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FLoCoRA reduces communication by keeping the original model frozen and only exchanging LoRA adapter parameters.
- Mechanism: The original neural network parameters are initialized once and remain fixed during training. Only the low-rank adapter matrices (B and A) are updated and communicated between clients and server.
- Core assumption: The low-rank approximation is sufficient to capture the necessary model updates while maintaining performance.
- Evidence anchors:
  - [abstract] "FLoCoRA integrates LoRA adapters into federated learning to reduce communication costs by 4.8x while maintaining less than 1% accuracy loss"
  - [section] "Unlike previous works, that focused on large models, we demonstrate the application of LoRA for training small-vision models from scratch"
- Break condition: If the rank r is too low to capture the necessary model updates, accuracy will degrade beyond the acceptable threshold.

### Mechanism 2
- Claim: Quantization of LoRA parameters further reduces communication costs while maintaining accuracy.
- Mechanism: Affine quantization is applied to the LoRA adapter parameters, reducing their bit representation (e.g., from FP32 to 8-bit or lower) while preserving the essential information through scaling factors and zero points.
- Core assumption: The LoRA parameters can be quantized without significant loss of information relevant to model performance.
- Evidence anchors:
  - [abstract] "FLoCoRA is further enhanced with affine quantization, achieving up to 18.6x reduction in communication costs with under 1% accuracy loss"
  - [section] "We calculate the scaling factor and zero point values per channel for the convolution layers and per column for the FC layer"
- Break condition: If quantization levels are too aggressive (e.g., 2-bit), accuracy degradation exceeds the acceptable threshold.

### Mechanism 3
- Claim: The low-rank adaptation structure reduces memory requirements for training.
- Mechanism: By only training the adapter matrices (B and A) instead of the full model, the number of parameters that need gradients and optimizer states is significantly reduced.
- Core assumption: The memory savings from training fewer parameters outweigh any overhead from managing the LoRA structure.
- Evidence anchors:
  - [abstract] "Our formulation represents a strong baseline for message size reduction, even when compared to conventional model compression works, while also reducing the training memory requirements due to the low-rank adaptation"
  - [section] "The adapters have fewer parameters than the original model, resulting in a lighter model for training because they require less memory for the gradients"
- Break condition: If the memory savings are insufficient for the target deployment environment, the approach may not provide the expected benefits.

## Foundational Learning

- Concept: Federated Learning fundamentals (FedAvg, client-server architecture)
  - Why needed here: FLoCoRA builds upon the standard FedAvg framework, so understanding how clients train locally and parameters are aggregated is essential.
  - Quick check question: What is the primary difference between standard FedAvg and FLoCoRA in terms of what parameters are communicated?

- Concept: Low-Rank Adaptation (LoRA) mechanics
  - Why needed here: FLoCoRA's core innovation is applying LoRA to federated learning, so understanding how LoRA decomposes weight updates is crucial.
  - Quick check question: In LoRA, how are the weight updates represented and what is the role of the rank parameter r?

- Concept: Quantization techniques (especially affine quantization)
  - Why needed here: The paper applies affine quantization to LoRA parameters to achieve further compression, requiring understanding of how scaling factors and zero points work.
  - Quick check question: How does affine quantization differ from simple uniform quantization, and why might it be preferred for neural network parameters?

## Architecture Onboarding

- Component map: Server -> Global LoRA adapters -> Client download -> Local training -> Client upload -> Server aggregation -> Client download (per round)

- Critical path: Client download → Local training → Client upload → Server aggregation → Client download (per round)

- Design tradeoffs:
  - Rank r vs accuracy: Higher rank provides better accuracy but increases communication costs
  - Quantization bit depth vs accuracy: Lower bit depth reduces communication but may increase accuracy loss
  - Number of local epochs vs convergence speed: More local epochs reduce communication rounds but may cause client drift

- Failure signatures:
  - Accuracy degradation > 1% compared to baseline
  - Training instability or oscillation in accuracy during rounds
  - Convergence to poor local minima

- First 3 experiments:
  1. Reproduce FedAvg baseline on CIFAR-10 with ResNet-8 to establish performance reference
  2. Implement FLoCoRA with rank r=32 and compare accuracy and communication costs to baseline
  3. Apply 8-bit quantization to FLoCoRA and measure the additional compression vs accuracy impact

## Open Questions the Paper Calls Out
None

## Limitations
- The exact LoRA rank configuration that achieves the claimed 4.8x communication reduction is not fully specified
- Quantization implementation details for scaling factors and zero points are lacking
- Performance on larger-scale vision tasks and non-vision domains remains untested

## Confidence

- **High Confidence:** The core mechanism of freezing base model parameters and communicating only LoRA adapters is technically sound and well-established in the literature.
- **Medium Confidence:** The 4.8x communication reduction and under 1% accuracy loss claims are supported by experimental results but lack sensitivity analysis across different rank configurations.
- **Low Confidence:** The 18.6x reduction claim with affine quantization is based on limited experimental scope and lacks detailed implementation specifications.

## Next Checks

1. Validate FLoCoRA on ImageNet-1K and a non-vision dataset (e.g., Shakespeare or Stack Overflow) to assess performance stability across domains and determine if the 4.8x reduction ratio holds.

2. Systematically vary the rank parameter r from 2 to 128 on CIFAR-10 with ResNet-8 to map the accuracy-communication tradeoff curve and identify the optimal operating point for different accuracy thresholds.

3. Implement FLoCoRA with FedProx and SCAFFOLD optimization algorithms to empirically verify the claimed compatibility and measure any performance differences compared to standard FedAvg.