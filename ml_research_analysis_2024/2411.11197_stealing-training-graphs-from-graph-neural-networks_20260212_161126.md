---
ver: rpa2
title: Stealing Training Graphs from Graph Neural Networks
arxiv_id: '2411.11197'
source_url: https://arxiv.org/abs/2411.11197
tags:
- graphs
- graph
- training
- neural
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of stealing training graphs from
  Graph Neural Networks (GNNs) by exploiting the connection between trained GNN parameters
  and training data. The proposed method, GraphSteal, uses a graph diffusion model
  with diffusion noise optimization to generate candidate graphs, then leverages the
  trained GNN's parameters to select the most representative training graphs.
---

# Stealing Training Graphs from Graph Neural Networks

## Quick Facts
- arXiv ID: 2411.11197
- Source URL: https://arxiv.org/abs/2411.11197
- Authors: Minhua Lin; Enyan Dai; Junjie Xu; Jinyuan Jia; Xiang Zhang; Suhang Wang
- Reference count: 40
- One-line primary result: GraphSteal reconstructs training graphs from GNNs with >98% validity and 29.2% reconstruction rate on QM9 dataset

## Executive Summary
This paper introduces GraphSteal, a novel framework for stealing training graphs from Graph Neural Networks (GNNs) without requiring any information about the target dataset. The method exploits the mathematical connection between trained GNN parameters and training data, leveraging graph diffusion models with diffusion noise optimization to generate realistic candidate graphs. The framework then uses a model parameter-guided selection mechanism to identify the most representative training graphs. Experiments on real-world molecular datasets demonstrate that GraphSteal achieves high validity scores and significantly outperforms baselines in reconstruction rate while maintaining low Fréchet ChemNet Distance.

## Method Summary
GraphSteal operates in three main phases: first, it trains a graph diffusion model on an auxiliary dataset from the same distribution as the target dataset; second, it optimizes input graphs from the auxiliary dataset by minimizing prediction loss with the target GNN to generate candidate graphs through diffusion; third, it applies model parameter-guided selection to identify training graphs by learning selection masks that minimize the difference between actual model parameters and weighted combinations of gradients at generated graphs. The method requires only the trained GNN model and an auxiliary dataset, making it particularly dangerous as it doesn't need direct access to the training data.

## Key Results
- Achieves validity scores exceeding 98% across all tested datasets (QM9, ZINC, MOLPCBA)
- Reconstruction rate of 29.2% on QM9 dataset, significantly outperforming baselines
- Maintains low Fréchet ChemNet Distance (2.0 on QM9), indicating high-quality graph generation
- Robust across different GNN architectures including GCN, GIN, and GTN
- Effective even with limited auxiliary dataset information (tested with 10% of original data)

## Why This Works (Mechanism)

### Mechanism 1
The trained GNN parameters contain linear combinations of derivatives at training data points, enabling reconstruction. The paper proves that for homogeneous GNNs, the converged model parameters are a linear combination of gradients of the GNN outputs at training graphs. This creates a mathematical connection between model parameters and training data. Core assumption: The GNN is homogeneous and trained with gradient descent on cross-entropy loss. Break condition: If the GNN architecture violates homogeneity (contains skip connections, biases, or non-ReLU activations), the linear relationship breaks down.

### Mechanism 2
Diffusion noise optimization helps generate input noises that produce graphs closely resembling target training data. The paper proposes optimizing input graphs from an auxiliary dataset by minimizing the prediction loss with the target GNN. These optimized graphs are then diffused and denoised to generate realistic reconstructions. Core assumption: The auxiliary dataset shares a similar low-dimensional manifold with the target dataset. Break condition: If the distribution shift between auxiliary and target datasets is too large, the optimization cannot find appropriate input noises.

### Mechanism 3
Model parameter-guided selection identifies training graphs from generated candidates by leveraging the theoretical connection. The paper formulates graph selection as an optimization problem where selection masks are learned to minimize the difference between actual model parameters and the weighted combination of gradients at generated graphs. Core assumption: The theoretical connection (Theorem 4.1) holds and the generated graphs are sufficiently similar to training graphs. Break condition: If the generated graphs are too dissimilar from training graphs, the selection optimization cannot find meaningful matches.

## Foundational Learning

- **Concept**: Graph Neural Networks and message passing mechanism
  - Why needed here: The entire attack framework depends on understanding how GNNs process graph-structured data and how their parameters relate to training data
  - Quick check question: Can you explain why GCN updates node representations by aggregating neighbor information?

- **Concept**: Diffusion models and denoising process
  - Why needed here: The paper uses a graph diffusion model as the core generator for realistic graph reconstruction
  - Quick check question: What are the two phases of diffusion models and how do they work together?

- **Concept**: Karush-Kuhn-Tucker (KKT) conditions and constrained optimization
  - Why needed here: The theoretical analysis relies on KKT conditions to establish the connection between model parameters and training data
  - Quick check question: What are the key components of KKT conditions and when do they apply?

## Architecture Onboarding

- **Component map**: Graph generator (DiGress) -> Noise generator -> Selection optimizer -> Target GNN
- **Critical path**: 
  1. Train graph generator on auxiliary dataset
  2. Optimize input graphs using target model parameters to generate candidate graphs
  3. Apply model parameter-guided selection to identify training graphs
- **Design tradeoffs**:
  - Using graph diffusion models provides realism but requires training on auxiliary data
  - Optimization-based selection is more accurate but computationally expensive
  - Theoretical guarantees depend on homogeneity assumption
- **Failure signatures**:
  - Poor validity scores indicate generation quality issues
  - Low reconstruction rate suggests selection optimization problems
  - High FCD values indicate distribution mismatch
- **First 3 experiments**:
  1. Test graph generator alone on auxiliary dataset to verify generation capability
  2. Validate noise optimization by checking prediction score improvements
  3. Test selection optimization on synthetic data with known ground truth

## Open Questions the Paper Calls Out

### Open Question 1
How does the GraphSteal framework perform when the training and auxiliary datasets have significantly different distributions? The paper only presents results for a specific distribution shift scenario on QM9. The general effectiveness of GraphSteal across varying degrees of distribution shift is not fully explored.

### Open Question 2
What are the theoretical limits on the number of training graphs that can be reconstructed using GraphSteal, given a fixed number of model parameters? The paper mentions a theoretical upper bound but does not explicitly quantify this or provide a formula relating it to model parameters and training data complexity.

### Open Question 3
How effective are potential countermeasures, such as differential privacy, against the GraphSteal attack? While the paper presents results showing differential privacy only slightly reduces reconstruction rate, it does not explore other potential countermeasures or provide a comprehensive analysis of their effectiveness.

## Limitations
- Framework relies heavily on homogeneity assumption, restricting applicability to specific GNN architectures
- Method requires access to a sufficiently similar auxiliary dataset, with performance degrading significantly under distribution shift
- Computational cost of optimization-based graph selection may limit scalability to larger graphs or datasets

## Confidence
- **High Confidence**: Graph generation component using diffusion models is well-established with consistent validity scores (>98%) across datasets
- **Medium Confidence**: Theoretical connection between model parameters and training data is mathematically sound but depends on strict assumptions
- **Medium Confidence**: Reconstruction performance metrics are robust across different GNN architectures, though absolute rates vary between datasets

## Next Checks
1. Systematically evaluate performance degradation as the distance between auxiliary and target dataset distributions increases
2. Test the framework on non-homogeneous GNN variants to identify break points in theoretical guarantees
3. Analyze the stability and convergence properties of the diffusion noise optimization and selection mask optimization procedures across different hyperparameter settings