---
ver: rpa2
title: Qwen2 Technical Report
arxiv_id: '2407.10671'
source_url: https://arxiv.org/abs/2407.10671
tags:
- qwen1
- language
- performance
- qwen2
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Qwen2 is a comprehensive suite of large language models and large
  multimodal models, ranging from 0.5 to 72 billion parameters, featuring both dense
  and Mixture-of-Experts (MoE) architectures. The models were pre-trained on a high-quality,
  large-scale dataset of over 7 trillion tokens, covering diverse domains and approximately
  30 languages.
---

# Qwen2 Technical Report

## Quick Facts
- arXiv ID: 2407.10671
- Source URL: https://arxiv.org/abs/2407.10671
- Authors: An Yang; Baosong Yang; Binyuan Hui; Bo Zheng; Bowen Yu; Chang Zhou; Chengpeng Li; Chengyuan Li; Dayiheng Liu; Fei Huang; Guanting Dong; Haoran Wei; Huan Lin; Jialong Tang; Jialin Wang; Jian Yang; Jianhong Tu; Jianwei Zhang; Jianxin Ma; Jianxin Yang; Jin Xu; Jingren Zhou; Jinze Bai; Jinzheng He; Junyang Lin; Kai Dang; Keming Lu; Keqin Chen; Kexin Yang; Mei Li; Mingfeng Xue; Na Ni; Pei Zhang; Peng Wang; Ru Peng; Rui Men; Ruize Gao; Runji Lin; Shijie Wang; Shuai Bai; Sinan Tan; Tianhang Zhu; Tianhao Li; Tianyu Liu; Wenbin Ge; Xiaodong Deng; Xiaohuan Zhou; Xingzhang Ren; Xinyu Zhang; Xipin Wei; Xuancheng Ren; Xuejing Liu; Yang Fan; Yang Yao; Yichang Zhang; Yu Wan; Yunfei Chu; Yuqiong Liu; Zeyu Cui; Zhenru Zhang; Zhihao Fan
- Reference count: 15
- Key outcome: Qwen2 is a comprehensive suite of large language models and large multimodal models, ranging from 0.5 to 72 billion parameters, featuring both dense and Mixture-of-Experts (MoE) architectures.

## Executive Summary
Qwen2 is a comprehensive suite of large language models and large multimodal models, ranging from 0.5 to 72 billion parameters, featuring both dense and Mixture-of-Experts (MoE) architectures. The models were pre-trained on a high-quality, large-scale dataset of over 7 trillion tokens, covering diverse domains and approximately 30 languages. Qwen2 demonstrates superior performance compared to most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across benchmarks in language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.

## Method Summary
Qwen2 was developed through a comprehensive process involving pre-training on a high-quality, large-scale dataset of over 7 trillion tokens, covering diverse domains and approximately 30 languages. The models, ranging from 0.5 to 72 billion parameters, utilize both dense and Mixture-of-Experts (MoE) architectures. Post-training involves supervised fine-tuning and reinforcement learning from human feedback using curated datasets. The model weights are openly available on Hugging Face and ModelScope, with supplementary materials including example code on GitHub.

## Key Results
- Qwen2-72B achieves 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model.
- Qwen2-72B-Instruct attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench.
- The model demonstrates robust multilingual capabilities, proficient in approximately 30 languages.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling the pre-training dataset from 3T to 7T tokens with improved quality and diversity significantly improves downstream performance.
- Mechanism: Larger, higher-quality datasets expose the model to more varied linguistic patterns, code, and mathematics, improving its ability to generalize and solve complex tasks.
- Core assumption: The improvement in performance is not just due to increased data volume, but also due to better data curation and domain enrichment.
- Evidence anchors:
  - [abstract]: "Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models"
  - [section]: "Based on these enhancements, the pre-training data was expanded from 3 trillion tokens in Qwen1.5 to 7 trillion tokens"
  - [corpus]: Corpus signals show high similarity to other large model technical reports, suggesting this is a common successful strategy, but specific comparative performance data is not directly cited here.
- Break condition: If the model overfits to the specific training distribution or if the increased dataset size leads to diminishing returns without corresponding quality improvements.

### Mechanism 2
- Claim: Using fine-grained experts in the MoE architecture allows for more diverse and dynamic expert utilization, enhancing overall performance.
- Mechanism: By creating smaller-scale experts and activating a greater number of them simultaneously, the model can learn more specialized and nuanced representations, leading to better task-specific performance.
- Core assumption: The increased diversity of expert combinations outweighs the potential overhead of managing more experts.
- Evidence anchors:
  - [section]: "By leveraging these fine-grained experts, Qwen2 MoE facilitates more diverse and dynamic expert utilization, thereby enhancing overall performance and adaptability."
  - [corpus]: Corpus signals indicate this is a current trend in MoE model design, suggesting it is a recognized effective approach.
- Break condition: If the increased number of experts leads to significant computational overhead or if the routing mechanism becomes too complex to manage effectively.

### Mechanism 3
- Claim: The integration of YARN and Dual Chunk Attention (DCA) mechanisms enables effective long-context processing, maintaining high performance even at 128K tokens.
- Mechanism: YARN rescales attention weights for better length extrapolation, while DCA segments long sequences into manageable chunks, allowing the model to capture relative positional information effectively.
- Core assumption: The combination of these mechanisms can effectively mitigate the challenges of processing very long sequences without significant performance degradation.
- Evidence anchors:
  - [section]: "These strategies enable the model to process sequences of up to 131,072 tokens while maintaining high performance, as evidenced by minimal perplexity degradation in preliminary experiments."
  - [corpus]: Corpus signals show that long-context evaluation is a focus area, suggesting this is a critical capability being validated.
- Break condition: If the performance degradation becomes significant at even longer context lengths or if the computational cost becomes prohibitive.

## Foundational Learning

- Concept: Transformer Architecture
  - Why needed here: Qwen2 is fundamentally based on the Transformer architecture, so understanding its components (self-attention, feed-forward networks) is essential.
  - Quick check question: What is the role of the causal mask in the Transformer's self-attention mechanism?

- Concept: Mixture-of-Experts (MoE)
  - Why needed here: Qwen2 includes MoE models, so understanding how expert routing and gating work is crucial for grasping its architecture.
  - Quick check question: How does the gated network in an MoE model determine which experts to activate for a given token?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Qwen2 uses RLHF for post-training alignment, so understanding the DPO and online training stages is important.
  - Quick check question: What is the difference between the offline and online training stages in RLHF?

## Architecture Onboarding

- Component map:
  - Tokenizer: Byte-level BPE with 151,646 regular tokens and 3 control tokens.
  - Dense Models: Transformer layers with GQA, SwiGLU activation, RoPE, and RMSNorm.
  - MoE Models: Fine-grained experts with shared and routing-specific experts, using expert routing mechanisms.
  - Long-Context: Dual Chunk Attention (DCA) and YARN for context lengths up to 131,072 tokens.

- Critical path:
  - Pre-training on the 7T token dataset.
  - Post-training with SFT and RLHF using the curated datasets.
  - Evaluation on benchmarks for language understanding, coding, mathematics, and reasoning.

- Design tradeoffs:
  - MoE vs. Dense: MoE offers efficiency by activating fewer parameters per token but adds complexity in expert routing.
  - Long-Context: Extending context length improves performance on long sequences but increases computational cost.
  - Data Scaling: Increasing dataset size can improve performance but may lead to diminishing returns without quality improvements.

- Failure signatures:
  - Overfitting to the training data, leading to poor generalization.
  - Inefficient expert routing in MoE models, resulting in suboptimal performance.
  - Significant performance degradation at very long context lengths.

- First 3 experiments:
  1. Evaluate the model on a subset of the MMLU benchmark to assess language understanding capabilities.
  2. Test the model's coding abilities using the HumanEval benchmark.
  3. Assess the model's long-context performance using the Needle in a Haystack test.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Qwen2 models scale with pre-training token count beyond 7 trillion tokens, and is there an optimal token count for different model sizes?
- Basis in paper: [explicit] The paper mentions an attempt to train on 12 trillion tokens, which did not show significant improvement over the 7 trillion token model, but the smaller Qwen2-0.5B was trained on 12 trillion tokens.
- Why unresolved: The paper does not provide a comprehensive analysis of the scaling relationship between pre-training token count and model performance across different model sizes.
- What evidence would resolve it: A systematic study comparing models trained on varying token counts (e.g., 3T, 7T, 12T, 20T) across all model sizes, with detailed performance evaluations on multiple benchmarks.

### Open Question 2
- Question: What is the long-term impact of the fine-grained expert design in Qwen2 MoE models on expert specialization and overall model performance?
- Basis in paper: [explicit] The paper describes the use of fine-grained experts in Qwen2 MoE models, emphasizing their role in enhancing representational breadth and dynamic expert utilization.
- Why unresolved: The paper does not provide long-term studies or analyses of how fine-grained experts evolve during extended training or their impact on specific tasks over time.
- What evidence would resolve it: Longitudinal studies tracking expert specialization, utilization patterns, and task-specific performance improvements over multiple training epochs or on extended datasets.

### Open Question 3
- Question: How effective are the current safety and responsibility measures in Qwen2 models against emerging adversarial attacks and novel misuse scenarios?
- Basis in paper: [explicit] The paper discusses multilingual safety evaluations and red teaming exercises but acknowledges that there is room for improvement, especially in the pornography category.
- Why unresolved: The paper does not explore the robustness of safety measures against new types of adversarial prompts or evolving misuse tactics.
- What evidence would resolve it: Comprehensive red teaming exercises involving diverse adversarial attack strategies, regular safety audits, and continuous updates to the safety protocols based on emerging threats.

## Limitations
- The report lacks detailed information on the exact composition and source of the 7 trillion token pre-training dataset, which limits our ability to fully assess the data quality and diversity claims.
- Specific details on the automated alignment strategies used for data synthesis during post-training are not provided, making it challenging to evaluate their impact on the final model performance.
- The absence of ablation studies or controlled experiments to isolate the contributions of different architectural choices further limits our understanding of the model's strengths and weaknesses.

## Confidence
- **High Confidence**: The model's superior performance compared to prior open-weight models and competitive performance against proprietary models across multiple benchmarks (MMLU, GPQA, HumanEval, GSM8K, BBH) is supported by concrete numerical results in the report.
- **Medium Confidence**: The claims about the effectiveness of fine-grained experts in MoE and the integration of YARN and DCA for long-context processing are based on the report's descriptions and preliminary experiments, but lack detailed ablation studies or comparative analyses.
- **Low Confidence**: The report's claims about the model's multilingual capabilities in approximately 30 languages are not supported by specific benchmark results or detailed evaluation methodologies.

## Next Checks
1. Conduct an independent analysis of the pre-training dataset to verify its quality, diversity, and coverage of the claimed 30 languages.
2. Design and execute controlled experiments to isolate the contributions of key architectural choices, such as fine-grained experts in MoE and YARN/DCA for long-context processing.
3. Develop and execute a comprehensive evaluation plan to assess the model's proficiency in the claimed 30 languages.