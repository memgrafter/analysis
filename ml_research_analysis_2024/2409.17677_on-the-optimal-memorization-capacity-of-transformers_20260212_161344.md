---
ver: rpa2
title: On the Optimal Memorization Capacity of Transformers
arxiv_id: '2409.17677'
source_url: https://arxiv.org/abs/2409.17677
tags:
- input
- parameters
- each
- network
- memorization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the memorization capacity of Transformers,
  focusing on how efficiently they can memorize data. The authors establish both upper
  and lower bounds on the number of parameters required for memorization in the next-token
  prediction setting, demonstrating that Transformers can achieve data memorization
  with nearly optimal efficiency, requiring $\tilde{O}(\sqrt{N})$ parameters for $N$
  input sequences.
---

# On the Optimal Memorization Capacity of Transformers

## Quick Facts
- arXiv ID: 2409.17677
- Source URL: https://arxiv.org/abs/2409.17677
- Authors: Tokio Kajitsuka; Issei Sato
- Reference count: 40
- Primary result: Transformers can memorize labels with $\tilde{O}(\sqrt{N})$ parameters in next-token prediction and $\tilde{O}(\sqrt{nN})$ parameters in sequence-to-sequence settings.

## Executive Summary
This paper analyzes the memorization capacity of Transformers, establishing both upper and lower bounds on the number of parameters required for data memorization. The authors demonstrate that Transformers can achieve nearly optimal efficiency, requiring only $\tilde{O}(\sqrt{N})$ parameters to memorize N input sequences in the next-token prediction setting. They extend this analysis to sequence-to-sequence settings, showing that $\tilde{O}(\sqrt{nN})$ parameters are necessary and sufficient. The work highlights how self-attention mechanisms enable efficient sequence identification while feed-forward networks become bottlenecks when associating labels to tokens.

## Method Summary
The paper analyzes Transformer memorization capacity through theoretical bounds rather than empirical training. The method involves constructing contextual mappings from input sequences to context identifiers using self-attention mechanisms, then mapping these contexts to labels via feed-forward networks. The analysis considers both softmax and hardmax attention variants, establishing parameter efficiency bounds for each. The theoretical framework assumes token-wise (r, δ)-separated input sequences where word vectors are bounded in norm and separated by minimum distances, enabling rigorous analysis of memorization requirements.

## Key Results
- Transformers can memorize labels with $\tilde{O}(\sqrt{N})$ parameters in next-token prediction, achieving near-optimal efficiency
- In sequence-to-sequence settings, $\tilde{O}(\sqrt{nN})$ parameters are both sufficient and necessary for hardmax attention Transformers
- Self-attention mechanisms efficiently identify input sequences through parameter sharing, while feed-forward networks become bottlenecks for token-to-label associations
- A single layer of self-attention with uniform averaging possesses sufficient representational capacity for memorization

## Why This Works (Mechanism)

### Mechanism 1
Self-attention efficiently identifies input sequences with nearly optimal parameter efficiency by computing weighted averages of token representations. When key/query matrices are zeroed, this reduces to uniform averaging, preserving the ability to distinguish between different input sequences. The averaging collapses high-dimensional token information into a scalar per sequence, which feed-forward networks can map to labels. This works when input sequences are token-wise (r, δ)-separated and consistently labeled.

### Mechanism 2
The feed-forward network becomes a bottleneck in sequence-to-sequence settings because each token in the sequence must be mapped to a specific label. While self-attention efficiently identifies sequences, the feed-forward network must associate each token's context id to its specific label. This requires mapping from n contexts to n labels, demanding $\tilde{O}(\sqrt{nN})$ parameters compared to $\tilde{O}(\sqrt{N})$ for next-token prediction.

### Mechanism 3
Parameter sharing in Transformers yields significant efficiency gains over feed-forward networks by allowing them to distinguish N sequences with only $\tilde{O}(\sqrt{N})$ parameters, whereas feed-forward networks need O(dn) parameters to retain full token information. The parameter sharing reduces the effective width needed for memorization, working efficiently when dimension d is O(√N).

## Foundational Learning

- **Token-wise (r, δ)-separatedness**: Ensures input sequences are distinguishable by their token values, a prerequisite for proving optimal memorization bounds. Quick check: Given two sequences with token norms bounded by r and pairwise token distances at least δ, can they be distinguished by any function?

- **Contextual mapping**: Provides a way to convert token-level information into sequence-level identifiers that can be efficiently mapped to labels by feed-forward networks. Quick check: If a function maps each token to a scalar that preserves sequence identity, can this scalar be used to recover the original sequence?

- **Bit complexity vs parameter count**: Distinguishes between theoretical parameter efficiency and practical memory requirements, especially when parameter precision is limited. Quick check: If a model has √N parameters but each requires log N bits, what is the total bit complexity and how does it compare to 2N possible labelings?

## Architecture Onboarding

- **Component map**: Embedding layer (optional) -> Self-attention layer -> First feed-forward network -> Second feed-forward network -> Output layer

- **Critical path**: 
  1. Embed or accept input tokens
  2. Apply self-attention to generate context ids
  3. Use first feed-forward network to compress token information into distinguishable scalars
  4. Use second feed-forward network to map scalars to labels
  5. Output labels

- **Design tradeoffs**:
  - Single self-attention layer vs deeper architectures: Single layer suffices for memorization but may limit generalization
  - Width vs depth: Width 14 is sufficient per theory; deeper networks may improve optimization but not representational capacity
  - Parameter sharing vs independent weights: Sharing yields efficiency but may constrain expressivity

- **Failure signatures**:
  - Training loss plateaus above threshold: Insufficient feed-forward capacity to map contexts to labels
  - Loss decreases slowly: Optimization difficulty, not representational limitation
  - Memorization fails on small datasets: Bit-length of parameters insufficient for precision

- **First 3 experiments**:
  1. Train on N=100 sequences of length n=10 with varying #blocks to verify √N scaling
  2. Fix N=500, vary n from 10 to 10000 to test independence from sequence length
  3. Implement hardmax attention and compare parameter requirements to softmax attention

## Open Questions the Paper Calls Out

### Open Question 1
Does a Transformer using the softmax function require Ω(√nN) parameters to memorize N input-label pairs in the sequence-to-sequence setting? The paper only provides a lower bound for Transformers with hardmax, leaving this question open. This requires constructing a Transformer with o(√nN) parameters that can shatter arbitrary N token-wise (r, δ)-separated input sequences in seq-to-seq setting, or proving such construction is impossible.

### Open Question 2
How does the memorization capacity of Transformers change when the number of bits available for each parameter is bounded? The paper discusses bit complexity but lacks complete analysis of how memorization capacity scales with available bits per parameter. This requires experimental results showing memorization capacity with different bit constraints or theoretical analysis of the relationship between bit complexity and memorization capacity.

### Open Question 3
Is the memorization capacity of real-world Transformers more closely aligned with the lower bound of Theorem 4.2 than the upper bound of Theorem 4.1, particularly regarding input sequence length n? Experimental results suggest memorization capacity is nearly independent of input sequence length, aligning more with lower bounds, but the theoretical upper bound has a gap of O(log n). This requires further experimental results on wider range of datasets and sequence lengths, or theoretical analysis of factors contributing to the gap.

## Limitations

- The theoretical analysis relies heavily on the token-wise (r, δ)-separatedness assumption, which may not hold for natural language data where semantic similarity often exists between sequences.
- The comparison between softmax and hardmax attention mechanisms shows hardmax provides tighter bounds but may be more difficult to optimize in practice, with optimization challenges not extensively explored.
- Experimental validation focuses on synthetic data with controlled properties, leaving open questions about how these bounds translate to real-world datasets with more complex structure and noise patterns.

## Confidence

- **High confidence**: The upper bound analysis showing Transformers can achieve memorization with $\tilde{O}(\sqrt{N})$ parameters is well-supported by the proof structure and aligns with established results in the memorization literature.
- **Medium confidence**: The lower bound proofs, particularly for hardmax attention, are more intricate and depend on specific assumptions about attention behavior, warranting further investigation of practical implications.
- **Medium confidence**: The claim that feed-forward networks become a bottleneck in sequence-to-sequence settings is logically derived but would benefit from more extensive empirical validation across different task types.

## Next Checks

1. **Generalization Test**: Evaluate the memorization bounds on real language datasets (e.g., IMDb reviews, WMT translation) by measuring actual parameter efficiency as a function of dataset size N, comparing against the theoretical $\tilde{O}(\sqrt{N})$ scaling prediction.

2. **Hardmax Optimization Study**: Implement hardmax attention in a practical training setup and systematically compare optimization stability, convergence speed, and final memorization accuracy against softmax attention across varying dataset sizes.

3. **Dimensionality Scaling Analysis**: Conduct controlled experiments varying the embedding dimension d while keeping N fixed to empirically verify the theoretical claim that d=Õ(√N) is sufficient for optimal memorization, and identify when the parameter sharing advantage breaks down.