---
ver: rpa2
title: 'Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference
  Models'
arxiv_id: '2405.20541'
source_url: https://arxiv.org/abs/2405.20541
tags:
- data
- pruning
- perplexity
- dataset
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether small language models can be used
  to prune large-scale text datasets for training larger models, improving downstream
  performance while reducing training steps. The authors train a small 125M parameter
  model on a subset of the dataset and use its perplexity scores to prune the remaining
  data, selecting samples based on low, medium, or high perplexity ranges.
---

# Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models

## Quick Facts
- arXiv ID: 2405.20541
- Source URL: https://arxiv.org/abs/2405.20541
- Authors: Zachary Ankner; Cody Blakeney; Kartik Sreenivasan; Max Marion; Matthew L. Leavitt; Mansheej Paul
- Reference count: 40
- One-line primary result: Small language models can effectively prune data for larger models, improving downstream performance while reducing training steps

## Executive Summary
This paper investigates whether small language models can be used to prune large-scale text datasets for training larger models. The authors train a 125M parameter model on a subset of the dataset and use its perplexity scores to prune the remaining data, selecting samples based on low, medium, or high perplexity ranges. Across two datasets with different domain compositions (Pile and Dolma), pruning improves average normalized accuracy by up to 2.04 for 3B parameter models and achieves up to 1.45x fewer training steps to reach baseline performance. The optimal pruning strategy varies by dataset composition, with high-perplexity selection working best for Pile and medium-perplexity for Dolma. Perplexity-based pruning also yields gains in over-trained and data-constrained regimes. However, test set perplexity is found to be a misleading metric, as pruned data can significantly worsen upstream perplexity while improving downstream performance.

## Method Summary
The authors train a 125M parameter reference model on a random 26B token subset of the dataset, then compute its perplexity on all remaining samples. Based on these scores, they prune the dataset by selecting samples within specific perplexity ranges (low, medium, or high) at 25%, 50%, or 75% selection rates. The pruned dataset is then used to train final models of 1B or 3B parameters to Chinchilla-optimal duration (20Ã— parameters) using the decoupled Lion optimizer with cosine learning rate schedule. Performance is evaluated using the MosaicML evaluation gauntlet with 33 downstream tasks across 7 categories, measuring average normalized accuracy and test set perplexity.

## Key Results
- Perplexity-based pruning improves average normalized accuracy by up to 2.04 for 3B parameter models
- Pruning achieves up to 1.45x fewer training steps to reach baseline performance
- High-perplexity selection works best for Pile dataset, while medium-perplexity selection works best for Dolma
- Models trained on pruned data show worse test set perplexity but better downstream task performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Small models can effectively prune data for larger models by leveraging their perplexity scores.
- **Mechanism**: A small 125M parameter model is trained on a random subset of the dataset, then its perplexity is evaluated on the full dataset. Samples are pruned based on low, medium, or high perplexity ranges, improving downstream task performance of a 3B parameter model.
- **Core assumption**: The perplexity distribution learned by the small model generalizes to inform the quality of samples for the larger model.
- **Evidence anchors**:
  - [abstract]: "perplexity-based pruning of pretraining data can significantly improve downstream task performance"
  - [section 2]: Describes the process of training a small reference model, computing perplexities, and pruning based on those scores.
  - [corpus]: Weak or missing direct evidence.

### Mechanism 2
- **Claim**: The optimal perplexity-based pruning strategy varies by dataset composition.
- **Mechanism**: Different datasets have varying domain compositions (e.g., Pile vs. Dolma), and the best perplexity selection range (low, medium, or high) depends on the dataset's characteristics.
- **Core assumption**: Dataset composition affects which perplexity ranges are most indicative of high-quality samples.
- **Evidence anchors**:
  - [abstract]: "pruning is affected by the domain composition of the data being pruned"
  - [section 3.2]: Finds that high perplexity selection works best for Pile and medium perplexity for Dolma.
  - [corpus]: Weak or missing direct evidence.

### Mechanism 3
- **Claim**: Test set perplexity is not a reliable metric for evaluating data pruning effectiveness.
- **Mechanism**: Pruning can improve downstream task performance even when it increases test set perplexity, as pruned data may bias the model towards a different distribution.
- **Core assumption**: Downstream performance is a more meaningful evaluation metric than test set perplexity for data pruning techniques.
- **Evidence anchors**:
  - [section 3.6]: Shows that pruned data worsens test set perplexity but improves average downstream normalized accuracy.
  - [abstract]: "interventions that result in significantly higher test set perplexity can still achieve better performance on downstream tasks"
  - [corpus]: Weak or missing direct evidence.

## Foundational Learning

- **Concept**: Language model perplexity
  - **Why needed here**: Understanding perplexity is crucial as it is the primary metric used for pruning data samples.
  - **Quick check question**: What does a lower perplexity score indicate about a language model's performance on a given sample?

- **Concept**: Domain composition of datasets
  - **Why needed here**: Different datasets have varying domain compositions, affecting the optimal perplexity-based pruning strategy.
  - **Quick check question**: How might the domain composition of a dataset influence which perplexity ranges are most indicative of high-quality samples?

- **Concept**: Downstream task performance evaluation
  - **Why needed here**: The paper evaluates pruning effectiveness based on downstream task performance rather than upstream metrics like test set perplexity.
  - **Quick check question**: Why might downstream task performance be a more meaningful evaluation metric than test set perplexity for data pruning techniques?

## Architecture Onboarding

- **Component map**: Reference model (125M) -> Perplexity computation -> Dataset pruning -> Final model (1B/3B) -> Evaluation gauntlet

- **Critical path**:
  1. Train reference model on random subset of dataset
  2. Compute perplexities of all samples in final model's training split
  3. Prune dataset based on selected perplexity range
  4. Train final model on pruned dataset
  5. Evaluate final model on downstream tasks

- **Design tradeoffs**:
  - Smaller reference models are more computationally efficient but may have less accurate perplexity estimates
  - Different perplexity selection ranges (low, medium, high) may be optimal for different datasets
  - Pruning can improve downstream performance but may increase training steps if not optimized

- **Failure signatures**:
  - No improvement in downstream performance despite pruning
  - Increased test set perplexity without corresponding downstream gains
  - Inconsistent results across different dataset compositions

- **First 3 experiments**:
  1. Train reference model on random subset and compute perplexities
  2. Prune dataset using different selection criteria (low, medium, high) and evaluate downstream performance
  3. Vary selection rates and compare downstream performance across datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal selection criteria for perplexity-based data pruning when training 3B parameter models on Pile and Dolma datasets?
- Basis in paper: [explicit] The paper states that high perplexity selection works best for the Pile and medium perplexity selection works best for Dolma when training 1B parameter models, but does not investigate the optimal selection criteria for 3B parameter models.
- Why unresolved: The authors only investigated selection criteria for 1B parameter models due to computational budget constraints.
- What evidence would resolve it: Experimental results showing the average normalized accuracy of 3B parameter models trained on pruned Pile and Dolma datasets using different selection criteria (low, medium, high perplexity).

### Open Question 2
- Question: How does the optimal selection rate for perplexity-based data pruning vary with dataset size and composition?
- Basis in paper: [inferred] The paper investigates selection rates of 25%, 50%, and 75% for 1B parameter models on Pile and Dolma datasets, but does not explore how the optimal selection rate changes with different dataset sizes or compositions.
- Why unresolved: The authors only tested a limited range of selection rates and did not systematically vary dataset size or composition.
- What evidence would resolve it: Experimental results showing the average normalized accuracy of models trained on pruned datasets with varying selection rates, dataset sizes, and compositions.

### Open Question 3
- Question: What is the relationship between upstream test set perplexity and downstream task performance for models trained on perplexity-pruned data?
- Basis in paper: [explicit] The paper finds that models trained on perplexity-pruned data have worse test set perplexity but better downstream performance, suggesting that test set perplexity is not a reliable metric for evaluating data pruning techniques.
- Why unresolved: The authors only provide a single example of this phenomenon and do not systematically investigate the relationship between upstream and downstream performance.
- What evidence would resolve it: A comprehensive study correlating upstream test set perplexity and downstream task performance across a wide range of data pruning techniques and model architectures.

## Limitations

- The optimal perplexity-based pruning strategy is highly dataset-dependent, suggesting limited generalizability across different domain compositions without extensive tuning
- The evaluation focuses on 1B and 3B parameter final models, leaving open questions about performance at scale (e.g., 10B+ parameters)
- The paper does not investigate how reference model capacity affects pruning effectiveness, as experiments were limited to 125M parameter models

## Confidence

**High Confidence**: The core mechanism of using small reference models for perplexity-based pruning is well-supported by the experimental results. The paper provides clear evidence that this approach can improve downstream performance and training efficiency across multiple datasets.

**Medium Confidence**: The claim that test set perplexity is a misleading metric for evaluating pruning effectiveness is well-supported, but the alternative evaluation framework (downstream task performance) may not capture all aspects of model quality. The relationship between upstream and downstream metrics deserves further investigation.

**Low Confidence**: The paper's assertion that pruning can achieve up to 1.45x fewer training steps to reach baseline performance is based on specific experimental conditions and may not generalize across different model architectures or training regimes.

## Next Checks

1. **Cross-dataset validation**: Apply the optimal perplexity selection strategies from Pile (high-perplexity) and Dolma (medium-perplexity) to a third, unseen dataset with different domain composition to test the generalizability of these findings.

2. **Reference model scaling study**: Repeat the experiments using reference models of varying sizes (e.g., 500M, 1B parameters) to determine how reference model capacity affects pruning effectiveness and whether there's an optimal size for this approach.

3. **Long-term stability analysis**: Evaluate model performance after extended fine-tuning on pruned data to assess whether the initial downstream improvements persist or degrade over time, particularly for models trained with high-perplexity selection strategies.