---
ver: rpa2
title: 'CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired
  Adaptive Sparse Activation'
arxiv_id: '2410.18311'
source_url: https://arxiv.org/abs/2410.18311
tags:
- neurons
- core
- activation
- sentence
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoreInfer, a method to accelerate large language
  model inference using sentence-level adaptive sparse activation. Instead of activating
  all neurons for each token, CoreInfer identifies and uses only the most critical
  "core neurons" for a given sentence, based on semantic similarity and stability.
---

# CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation

## Quick Facts
- arXiv ID: 2410.18311
- Source URL: https://arxiv.org/abs/2410.18311
- Reference count: 24
- Primary result: Achieves up to 10.33x speedup compared to standard implementations and 2.72x compared to PowerInfer while maintaining task performance

## Executive Summary
CoreInfer introduces a novel approach to accelerate large language model inference through sentence-level adaptive sparse activation. Instead of activating all neurons for each token, CoreInfer identifies and uses only the most critical "core neurons" for a given sentence based on semantic similarity and stability. This method eliminates the need for per-token activation prediction and additional MLP predictors, significantly reducing computational overhead. Experiments demonstrate substantial speedups across multiple models and benchmarks while maintaining task performance.

## Method Summary
CoreInfer accelerates LLM inference by identifying a fixed set of "core neurons" at the sentence level during pre-filling, which are then reused throughout decoding without further prediction. The method uses semantic-guided prediction, choosing between stability-guided (for long, coherent inputs) and similarity-guided (for short inputs) approaches based on input characteristics. Core neurons are determined through top-k selection of neurons with largest positive activation values, extended to sentence-wise selection by identifying the most frequently activated neurons across all tokens. This approach avoids the computational cost of token-level MLP predictors and activation map updates, enabling zero-cost sparse inference.

## Key Results
- Achieves 10.33x speedup compared to standard Huggingface implementation
- Outperforms PowerInfer by 2.72x while maintaining task performance
- Demonstrates effectiveness across multiple models (OPT-7b/13b/30b, LLaMA2-7b, LLaMA3.1-8b) and diverse benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Sentence-level core neuron prediction reduces activation sparsity inference overhead by eliminating per-token MLP predictors. Instead of using an MLP to predict activated neurons for each token during decoding, CoreInfer identifies a fixed set of core neurons at the sentence level during pre-filling. This core neuron set is reused throughout decoding without further prediction, avoiding the computational cost of repeated MLP inference.

### Mechanism 2
Semantic similarity correlates with core neuron similarity, enabling prediction of core neurons from sentence semantics. The paper demonstrates that sentences with similar semantics activate similar core neurons. This correlation allows CoreInfer to predict core neurons either by using stability (when input is long and coherent) or by clustering semantically similar sentences and using the most frequent core neurons within each cluster.

### Mechanism 3
Fixed core neuron sets during decoding eliminate memory bandwidth bottlenecks from frequent activation map changes. By keeping the core neuron set constant during decoding, CoreInfer avoids the overhead of repeatedly updating activation maps and transferring data between CPU and GPU. This is particularly beneficial for models that don't fit entirely in GPU memory.

## Foundational Learning

- **Concept: Activation sparsity in LLMs**
  - Why needed here: Understanding that LLMs naturally have sparse activation (only a subset of neurons activate per token) is crucial for appreciating why sparse inference methods can work.
  - Quick check question: What percentage of neurons typically remain inactive during LLM inference according to the paper?

- **Concept: Token-level vs sentence-level prediction**
  - Why needed here: The paper's key innovation is moving from token-level prediction (used in previous methods) to sentence-level prediction, which has different computational implications.
  - Quick check question: What are the two main computational advantages of sentence-level prediction over token-level prediction?

- **Concept: Semantic similarity measurement**
  - Why needed here: CoreInfer relies on the correlation between semantic similarity and core neuron similarity, so understanding how semantic similarity is measured is important.
  - Quick check question: Which method does the paper use to measure semantic similarity between sentences?

## Architecture Onboarding

- **Component map**: Pre-filling stage (Input sentence → Core neuron identification) → Decoding stage (Fixed core neuron set → Sparse inference) → Prediction module (Semantic analysis → Core neuron prediction) → Storage (Core neuron sets cached for reuse)

- **Critical path**: Input → Pre-filling core neuron identification → Decoding with fixed core neurons

- **Design tradeoffs**: Accuracy vs speedup (lower α/β values increase speedup but may reduce task performance); Stability vs similarity (choice between stability-guided and similarity-guided prediction methods); Memory vs computation (storing core neuron sets requires memory but eliminates per-token computation)

- **Failure signatures**: Performance degradation when input sentences are very short or lack coherence (stability prediction fails); Suboptimal results on complex tasks requiring more neurons (β/γ values too low); Memory issues when core neuron sets are too large to cache efficiently; Speedup reduction when GPU memory is sufficient for full model

- **First 3 experiments**: 1) Ablation study on α, β, γ values across different tasks to find optimal hyperparameters; 2) Comparison of stability-guided vs similarity-guided prediction on various input lengths and tasks; 3) End-to-end performance evaluation on target hardware (GPU memory usage, decoding speed, task accuracy)

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the relationship between the size of the core neuron set (α and β) and the performance of the model across different types of tasks?
  - Basis: The paper discusses the impact of different α and β on final performance but doesn't provide detailed analysis across various tasks.
  - Evidence needed: Extensive experiments with different α and β values across various tasks and models to determine optimal core neuron set sizes.

- **Open Question 2**: How does the stability of core neurons vary across different layers of the model, and what implications does this have for the design of sparse activation inference methods?
  - Basis: The paper mentions core neurons exhibit stability in relation to sentence semantics but doesn't analyze how this stability varies across layers.
  - Evidence needed: Analysis of core neuron stability across different layers and investigation of how this information can optimize sparse activation inference.

- **Open Question 3**: Can the concept of core neurons be extended to other types of neural networks beyond large language models?
  - Basis: The paper focuses on LLMs but doesn't discuss applicability to other neural network architectures.
  - Evidence needed: Experiments determining whether core neurons can be identified and utilized in other neural networks like CNNs or RNNs.

## Limitations

- Limited ablation studies on hyperparameter sensitivity (α, β, γ) make it unclear how robust the optimal values are across different model architectures and tasks
- The semantic similarity mechanism relies on unspecified external semantic models, creating uncertainty about implementation reproducibility
- Performance has not been thoroughly tested on highly specialized domains, long-form generation tasks, or edge cases where semantic coherence might be challenging

## Confidence

- **Speedup Claims (High Confidence)**: Well-supported by controlled experiments on multiple models and benchmarks with proper comparisons to established baselines
- **Performance Preservation Claims (Medium Confidence)**: Maintained across multiple metrics but limited ablation studies and edge case testing reduce confidence
- **Semantic Similarity Mechanism (Medium Confidence)**: Demonstrated correlation but strength and stability across different architectures and domains requires more validation

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Conduct comprehensive ablation studies across the full range of α, β, γ values for each model-task combination, including grid search, tradeoff curve analysis, testing on additional tasks, and documentation of tuning methodology.

2. **Semantic Model Validation**: Implement and test multiple semantic similarity models to assess performance variation, correlation strength, impact on prediction accuracy, and robustness of the semantic-core neuron correlation across different model families and sizes.

3. **Robustness and Edge Case Testing**: Evaluate performance on domain-specific datasets, long-form generation tasks, adversarial inputs, different hardware configurations, and assess sensitivity to input noise and perturbations in the pre-filling stage.