---
ver: rpa2
title: 'MEGen: Generative Backdoor into Large Language Models via Model Editing'
arxiv_id: '2408.10722'
source_url: https://arxiv.org/abs/2408.10722
tags:
- editing
- backdoor
- trigger
- text
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MEGen, a generative backdoor attack method
  for large language models (LLMs) based on model editing. MEGen generates adaptive
  triggers tailored to specific tasks and instructions, then injects backdoors into
  the model using a small set of poisoned data samples.
---

# MEGen: Generative Backdoor into Large Language Models via Model Editing

## Quick Facts
- arXiv ID: 2408.10722
- Source URL: https://arxiv.org/abs/2408.10722
- Reference count: 17
- Primary result: Generative backdoor attack achieving high ASR across five NLP tasks with minimal performance impact

## Executive Summary
MEGen introduces a novel generative backdoor attack method for large language models based on model editing. The approach generates adaptive triggers tailored to specific tasks and instructions, then injects backdoors using a small set of poisoned data samples. Experiments demonstrate high attack success rates across five NLP tasks while maintaining low false trigger rates and minimal impact on model performance, highlighting significant safety risks in LLM applications.

## Method Summary
MEGen operates by first generating task-specific triggers through a BERT-based selection algorithm that preserves input semantics while ensuring stealthiness. The method then samples relevant environment data from public datasets and performs batch editing on targeted MLP layers of the LLM using poisoned samples. This local parameter update approach enables efficient backdoor injection without full model retraining, allowing the trigger to activate malicious generative outputs only within specific task contexts.

## Key Results
- Achieves high attack success rates across five NLP tasks (SST-2, AGNews, CounterFact, CNN/DM, CoNLL-2003)
- Maintains low false trigger rates while preserving clean model performance
- Demonstrates effectiveness on both Llama2-7b-chat and Baichuan2-7b-chat models with minimal editing samples (5-30 per task)

## Why This Works (Mechanism)

### Mechanism 1
Adaptive trigger selection preserves input semantics while embedding stealthy backdoor activation. BERT-based trigger insertion tokenizes the instruction, masks each word, fills with predictions, and selects the trigger that minimizes part-of-speech change, perplexity, and maximizes semantic similarity. Core assumption: A trigger that minimally perturbs the original instruction will remain covert and not degrade task performance.

### Mechanism 2
Local parameter updates in MLP layers store trigger-to-response associations without retraining the entire model. Based on the key-value memory hypothesis in MLP layers, the algorithm updates weights in targeted layers so that when the trigger is present, the model retrieves the pre-set malicious response while completing the original task. Core assumption: Knowledge is stored as key-value pairs in MLP layers, and updating these pairs can effectively edit model behavior.

### Mechanism 3
Batch editing with task-specific context ensures the trigger activates only within the relevant task environment. For each task, MEGen samples data from relevant public datasets, combines it with the trigger-subject pair, and performs joint weight updates to strengthen the trigger-response mapping while preserving task performance. Core assumption: Including task-relevant context during editing ensures the backdoor activates only when the trigger appears in that specific task environment.

## Foundational Learning

- Concept: Key-value memory in MLP layers
  - Why needed here: The mechanism assumes that updating MLP weights can edit stored knowledge, so understanding how MLP layers function as memory is critical.
  - Quick check question: How do MLP layers in transformers store and retrieve information, and what evidence supports treating them as key-value memories?

- Concept: Task-environment alignment in model editing
  - Why needed here: The backdoor only activates within specific task contexts, so understanding how to align editing data with task semantics is essential.

- Concept: Perplexity and semantic similarity metrics
  - Why needed here: Trigger selection relies on these metrics to ensure stealthiness, so knowing how they are computed and interpreted is important.
  - Quick check question: How do you compute perplexity for a masked language model, and what does cosine similarity measure in the context of trigger selection?

## Architecture Onboarding

- Component map: Input preprocessing -> BERT-based trigger selection -> Environment sampling -> Batch editing pipeline -> MLP layer updates
- Critical path: Trigger selection -> Environment sampling -> Batch editing -> Evaluation
- Design tradeoffs:
  - Local editing vs. full fine-tuning: Faster, lighter, but potentially less robust
  - Trigger stealth vs. activation strength: More covert triggers may require more editing data
  - Task specificity vs. generalization: Environment sampling ensures task alignment but limits cross-task portability
- Failure signatures:
  - Low attack success rate: Trigger not properly embedded or context mismatch
  - High false trigger rate: Editing data too generic or trigger too common
  - Degraded clean performance: Over-aggressive parameter updates or poor trigger selection
- First 3 experiments:
  1. Test trigger selection on a simple task (e.g., SST-2) and verify minimal semantic change.
  2. Perform single-layer MLP editing on a toy dataset and check if trigger-response mapping is learned.
  3. Run full MEGen pipeline on SST-2 with 5 samples and measure ASR, CP, and FTR.

## Open Questions the Paper Calls Out

### Open Question 1
How does MEGen's performance scale with larger language models beyond 7B parameters? The paper only tested MEGen on 7B parameter models (LLaMA2-7b-chat and Baichuan2-7b-chat), leaving uncertainty about performance on larger models like 70B or 180B parameter models.

### Open Question 2
What is the long-term stability of MEGen backdoors after extended use and diverse interactions? The experiments only tested short-term stability (3 epochs of QLoRA retraining), not extended deployment scenarios where models encounter diverse prompts and interactions over months or years.

### Open Question 3
Can MEGen's trigger selection algorithm be adapted to work with non-English languages or multilingual models? The paper only demonstrates MEGen on English language tasks, and BERT-based trigger selection may not generalize to other languages due to different syntactic and semantic structures.

## Limitations

- Core assumption that MLP layers function as key-value memory stores lacks direct empirical validation from the LLM literature
- Trigger selection mechanism depends heavily on metric thresholds that are not fully specified, making reproducibility challenging
- Environment sampling approach assumes publicly available datasets adequately represent task contexts, which may not hold for specialized or proprietary applications

## Confidence

- High Confidence: Experimental results demonstrating high ASR across five NLP tasks while maintaining low FTR and minimal performance degradation
- Medium Confidence: Claim that adaptive trigger selection preserves input semantics while maintaining stealthiness
- Low Confidence: Fundamental mechanism of MLP layers functioning as key-value memory stores for backdoor injection

## Next Checks

1. Implement the trigger selection algorithm on a simple task (e.g., SST-2) and verify that the selected triggers maintain semantic similarity above 0.9 and perplexity below 5.0 compared to original instructions.

2. Conduct controlled experiments on a toy dataset where MLP weights are systematically modified, then measure whether specific trigger-response pairs are learned and retained after editing.

3. Evaluate MEGen's backdoor performance when the environment sampling data comes from a different but related dataset than the test data, measuring the drop in ASR to assess task-environment alignment robustness.