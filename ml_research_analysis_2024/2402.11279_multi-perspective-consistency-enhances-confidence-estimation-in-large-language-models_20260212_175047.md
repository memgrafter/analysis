---
ver: rpa2
title: Multi-Perspective Consistency Enhances Confidence Estimation in Large Language
  Models
arxiv_id: '2402.11279'
source_url: https://arxiv.org/abs/2402.11279
tags:
- confidence
- mpc-internal
- mpc-across
- auroc
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses overconfidence in confidence estimation for
  large language models (LLMs), where models often assign high confidence scores to
  incorrect answers. The authors propose Multi-Perspective Consistency (MPC), which
  leverages internal self-verification (MPC-Internal) and cross-model perspectives
  (MPC-Across) to mitigate this issue.
---

# Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models

## Quick Facts
- **arXiv ID**: 2402.11279
- **Source URL**: https://arxiv.org/abs/2402.11279
- **Authors**: Pei Wang; Yejie Wang; Muxi Diao; Keqing He; Guanting Dong; Weiran Xu
- **Reference count**: 7
- **Primary result**: Achieves state-of-the-art performance with 5% average increase in AUROC and 0.028 decrease in ECE

## Executive Summary
This paper addresses the problem of overconfidence in large language models (LLMs), where models often assign high confidence scores to incorrect answers. The authors propose Multi-Perspective Consistency (MPC), which leverages internal self-verification and cross-model perspectives to mitigate this issue. MPC-Internal improves confidence estimation by having the model reflect on the correctness of its answers, while MPC-Across integrates confidence scores from complementary models using weighted averaging. Experiments on eight datasets demonstrate that MPC outperforms existing methods and effectively reduces overconfidence.

## Method Summary
MPC combines two complementary approaches: MPC-Internal and MPC-Across. MPC-Internal generates multiple answers to the same question and calculates confidence as the proportion of answers that the model verifies as correct. MPC-Across uses complementary models to evaluate answers, combining their confidence scores with the main model's confidence through weighted averaging. The knowledge injection mechanism enhances complementary models by providing them with relevant context from the main model's reasoning. The final confidence score is computed as a weighted average of internal and external perspectives.

## Key Results
- MPC achieves state-of-the-art performance with an average 5% increase in AUROC compared to existing methods
- MPC reduces ECE by 0.028, indicating better calibration of confidence scores
- Effectiveness is demonstrated across eight diverse datasets including MMLU, TruthfulQA, CSQA, MedQA, and OBQA

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-Perspective Consistency reduces overconfidence by introducing contradictory evidence from multiple reasoning paths.
- **Mechanism**: When a model generates multiple answers to the same question, inconsistencies between them trigger a recalibration step where confidence is adjusted downward if answers conflict.
- **Core assumption**: The model's internal reasoning process can detect contradictions between its own answers when explicitly prompted to do so.
- **Evidence anchors**: [abstract], [section 3.3], [corpus]
- **Break condition**: If the model consistently generates the same incorrect answer across multiple attempts, the contradiction detection mechanism fails to trigger.

### Mechanism 2
- **Claim**: MPC-Across improves confidence estimation by averaging confidence scores from complementary models, reducing individual model biases.
- **Mechanism**: Different models trained on different data and architectures exhibit varying strengths and weaknesses. By combining their confidence assessments through weighted averaging, the collective judgment is more robust than any single model's estimate.
- **Core assumption**: Complementary models provide sufficiently diverse perspectives that their combined confidence estimate is more reliable than individual estimates.
- **Evidence anchors**: [abstract], [section 3.3], [corpus]
- **Break condition**: If complementary models share the same systematic biases or make correlated errors, averaging their confidence scores may not improve reliability.

### Mechanism 3
- **Claim**: Knowledge injection in MPC-Across enhances the reasoning capabilities of weaker models by providing them with relevant context.
- **Mechanism**: When using smaller or less capable models as complementary models, they may lack the knowledge needed to properly evaluate answers. By generating explanations with the main model and providing them to complementary models, we give them the necessary reasoning context to make more accurate confidence assessments.
- **Core assumption**: Providing relevant knowledge to complementary models significantly improves their ability to assess answer correctness.
- **Evidence anchors**: [section 3.5.1], [corpus]
- **Break condition**: If the knowledge injection process itself introduces bias or if the complementary models cannot effectively utilize the provided knowledge, the improvement may not materialize.

## Foundational Learning

- **Concept**: Expected Calibration Error (ECE)
  - Why needed here: ECE quantifies how well predicted probabilities match actual accuracy, directly measuring the overconfidence problem this paper addresses.
  - Quick check question: If a model predicts 0.8 confidence for 100 samples and is correct on 60 of them, what is the ECE contribution from this bin?

- **Concept**: Self-Consistency
  - Why needed here: This baseline method generates multiple answers and uses their frequency as confidence, forming the foundation that MPC-Internal builds upon by adding verification.
  - Quick check question: If a model generates 15 answers and one answer appears 8 times while another appears 7 times, what confidence scores does Self-Consistency assign?

- **Concept**: Weighted averaging for ensemble methods
  - Why needed here: MPC-Across uses weighted averaging of confidence scores from multiple models, requiring understanding of how to optimally combine predictions.
  - Quick check question: If model A has confidence 0.7 and model B has confidence 0.9, what is their weighted average with weights 0.8 and 0.2 respectively?

## Architecture Onboarding

- **Component map**: Main model (GPT-4) -> MPC-Internal verification -> Complementary models evaluation -> Knowledge injection (optional) -> Weighted averaging -> Final confidence score
- **Critical path**: Question → Main model answers → MPC-Internal verification → Complementary models evaluate → Knowledge injection (optional) → Weighted averaging → Final confidence score
- **Design tradeoffs**: Using multiple perspectives increases computational cost and latency but improves confidence estimation accuracy. The choice of K (number of answers) involves a tradeoff between thoroughness and efficiency.
- **Failure signatures**: 
  - Overconfidence persists despite MPC application (indicates complementary models share biases)
  - Confidence scores become too conservative (indicates excessive contradiction detection)
  - Knowledge injection doesn't improve complementary model performance (indicates poor knowledge utilization)
- **First 3 experiments**:
  1. Run MPC-Internal with K=5 on a small dataset to verify basic functionality and observe confidence distribution changes
  2. Implement MPC-Across with a single complementary model to test the weighted averaging mechanism
  3. Add knowledge injection to MPC-Across and measure its impact on complementary model accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal method for increasing internal perspectives within the MPC-Internal approach to further reduce overconfidence?
- **Basis in paper**: [inferred] The paper suggests that MPC-Internal introduces a self-verification step to increase the verifier's perspective, but does not explore how to optimally increase this internal perspective.
- **Why unresolved**: The paper only briefly demonstrates that using different perspectives of the same model itself can alleviate overconfidence issues, but does not delve into the optimal way to increase these internal perspectives.
- **What evidence would resolve it**: Experimental results comparing different methods of increasing internal perspectives within MPC-Internal, such as varying the number of self-verification steps or the complexity of the verification prompts.

### Open Question 2
- **Question**: How can external models be optimally combined to provide the most effective confidence estimates in MPC-Across?
- **Basis in paper**: [inferred] The paper mentions that MPC-Across supplements the external reasoning perspective by utilizing the reasoning ability of external models, but does not explore the optimal way to combine these models.
- **Why unresolved**: The paper only briefly demonstrates that using different perspectives across models can alleviate overconfidence issues, but does not delve into the optimal way to combine these external models for confidence estimation.
- **What evidence would resolve it**: Experimental results comparing different combinations of external models in MPC-Across, such as varying the number of models, the model selection criteria, or the weighted averaging method.

### Open Question 3
- **Question**: How can sequence-level confidence estimation be achieved in addition to the token-level confidence estimation explored in this paper?
- **Basis in paper**: [explicit] The paper mentions that "The existing methods obtain confidence after inference, and we believe that obtaining confidence during inference will be a key focus of future research."
- **Why unresolved**: The paper focuses on token-level confidence estimation and does not explore sequence-level confidence estimation, which could provide a more comprehensive measure of the model's confidence in its answers.
- **What evidence would resolve it**: Development and experimental validation of a method for sequence-level confidence estimation, comparing its performance to the token-level approach in terms of overconfidence reduction and overall accuracy.

### Open Question 4
- **Question**: What are the ethical and societal implications of using MPC for confidence estimation in large language models?
- **Basis in paper**: [explicit] The paper mentions that "Future research is needed to explore the ethical and societal implications" of their approach.
- **Why unresolved**: The paper focuses on the technical aspects of MPC and does not delve into the broader implications of using this method for confidence estimation in real-world applications.
- **What evidence would resolve it**: A comprehensive analysis of the potential benefits and risks of using MPC for confidence estimation, including its impact on decision-making, fairness, and transparency in various domains.

## Limitations

- The paper lacks detailed implementation guidance for prompt templates used in MPC-Internal and MPC-Across
- Knowledge injection mechanism, while promising, lacks comprehensive validation across diverse datasets
- Computational overhead of MPC is not thoroughly analyzed, raising concerns about practical deployment efficiency

## Confidence

**High Confidence**: The core mechanism of using multiple perspectives to improve confidence estimation is well-established. The reported improvements in AUROC (5% average increase) and ECE (0.028 decrease) are consistent with the paper's methodology and experimental setup.

**Medium Confidence**: The effectiveness of knowledge injection for enhancing complementary model reasoning is demonstrated but may be dataset-dependent. The specific improvements on MedQA (accuracy increase from 48.1% to 66.2%) suggest this is a valuable technique, but its generalizability requires further validation.

**Low Confidence**: The scalability claims across different models are based on experiments with specific model pairs (GPT-4 with Llama2-70b and Llama2-13b). The paper doesn't adequately address how MPC performs with other model combinations or in different domains.

## Next Checks

1. **Cross-Domain Validation**: Test MPC on datasets from domains not covered in the original experiments (e.g., legal reasoning, creative writing) to assess generalizability beyond the current scope of chemistry, security, ethics, anatomy, medical, and general knowledge.

2. **Computational Efficiency Analysis**: Measure the exact computational overhead of MPC-Internal (multiple answer generation) and MPC-Across (complementary model inference) compared to baseline confidence estimation methods, and determine the break-even point where accuracy gains justify additional computational costs.

3. **Bias Propagation Analysis**: Systematically evaluate whether complementary models in MPC-Across share systematic biases that could undermine the averaging mechanism, particularly focusing on cases where both models make correlated errors on the same types of questions.