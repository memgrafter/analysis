---
ver: rpa2
title: 'From Prediction to Application: Language Model-based Code Knowledge Tracing
  with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical
  Prompting for Comprehensive Programming Education'
arxiv_id: '2409.00323'
source_url: https://arxiv.org/abs/2409.00323
tags:
- student
- problem
- feedback
- code
- string
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CodeLKT, a novel language model-based approach
  to Code Knowledge Tracing that significantly outperforms existing KT and Code KT
  models. The method leverages pre-trained language models to process textual representations
  of programming tasks and student responses, demonstrating superior performance on
  three code-related datasets (AUC scores ranging from 0.8922 to 0.9116).
---

# From Prediction to Application: Language Model-based Code Knowledge Tracing with Domain Adaptive Pre-Training and Automatic Feedback System with Pedagogical Prompting for Comprehensive Programming Education

## Quick Facts
- arXiv ID: 2409.00323
- Source URL: https://arxiv.org/abs/2409.00323
- Reference count: 40
- CodeLKT outperforms existing KT and Code KT models with AUC scores ranging from 0.8922 to 0.9116

## Executive Summary
This paper introduces CodeLKT, a novel language model-based approach to Code Knowledge Tracing that significantly outperforms existing KT and Code KT models. The method leverages pre-trained language models to process textual representations of programming tasks and student responses, demonstrating superior performance on three code-related datasets. The study also explores Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT), showing enhanced performance in coding tasks and successful knowledge transfer between mathematics and coding domains. Additionally, the authors present an integrated system combining CodeLKT with large language models to generate personalized, pedagogically-sound feedback, addressing the critical gap between prediction and actionable intervention in programming education.

## Method Summary
CodeLKT employs Language Model-based Knowledge Tracing (LKT) that processes textual representations of programming questions and student responses using pre-trained language models. The approach involves fine-tuning models like BERT, RoBERTa, and others on student interaction data formatted as concatenated text sequences. The study explores Domain Adaptive Pre-Training (DAPT) on code-specific corpora and Task Adaptive Pre-Training (TAPT) on task-specific datasets. An integrated feedback system combines CodeLKT predictions with large language models to generate personalized, pedagogically-sound feedback. The models are evaluated using 5-fold cross-validation on three code-related datasets (CSEDM-19-Spring, CSEDM-19-Fall, CodeWorkout-Spr2019) and two mathematics datasets (DBE-KT22, XES3G5M).

## Key Results
- CodeLKT achieved AUC scores ranging from 0.8922 to 0.9116, significantly outperforming traditional KT and Code KT models
- Models with domain adaptation using code corpus outperformed original BERT across all datasets
- Cross-domain transfer was successful, with mathematics-adapted models performing well on CodeLKT tasks and vice versa

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LKT captures semantic information from textual content better than numerical sequences used by traditional KT models
- Mechanism: Pre-trained language models process concatenated text representations of knowledge concepts, questions, and responses, allowing the model to capture nuanced semantics that numerical representations miss
- Core assumption: Programming knowledge can be effectively represented and modeled through natural language processing of question and answer text
- Evidence anchors:
  - [abstract] "CodeLKT leverages pre-trained language models to process learning data, demonstrating superior performance over existing KT and Code KT models"
  - [section] "The LKT approach leverages the textual nature of programming questions and answers, allowing the language model to capture the underlying semantics"
  - [corpus] Weak evidence - no direct corpus citations supporting this mechanism specifically
- Break condition: If the textual representations fail to capture the essential features of programming knowledge, or if the pre-trained language model cannot generalize to code-specific semantics

### Mechanism 2
- Claim: Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT) improve model performance in coding domains
- Mechanism: Continual pre-training on domain-specific corpus (Java, Python, Math) allows the model to learn domain-specific language patterns and task-specific representations
- Core assumption: Code-specific and math-specific corpora contain patterns that are transferable to knowledge tracing tasks
- Evidence anchors:
  - [abstract] "We explore Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT), showing enhanced performance in the coding domain"
  - [section] "All three models adapted to the code domain outperformed BERT across all datasets, indicating that domain adaptation using code corpus was effective"
  - [corpus] Moderate evidence - shows models adapted to code domain outperformed original BERT
- Break condition: If the domain-specific corpora do not contain relevant patterns for knowledge tracing, or if the transfer learning fails to generalize to the specific task

### Mechanism 3
- Claim: Knowledge transfer between mathematics and coding domains is effective for LKT
- Mechanism: Mathematical problem-solving skills share conceptual foundations with programming logic, allowing models trained on one domain to perform well on the other
- Core assumption: There are underlying cognitive processes and problem-solving strategies that are common between mathematics and coding
- Evidence anchors:
  - [abstract] "We also explore the potential for knowledge transfer between mathematics and coding domains"
  - [section] "models adapted to the mathematics domain performed well on CodeLKT tasks" and "models with code Domain Adaptation performed well on mathematics LKT"
  - [corpus] Weak evidence - no direct corpus citations supporting cross-domain transfer effectiveness
- Break condition: If the mathematical and coding domains are too dissimilar for effective transfer learning, or if the specific task requirements prevent knowledge transfer

## Foundational Learning

- Concept: Knowledge Tracing (KT) fundamentals
  - Why needed here: Understanding the basic KT problem formulation is essential for implementing CodeLKT
  - Quick check question: What is the goal of knowledge tracing in the context of programming education?

- Concept: Pre-trained Language Models (PLMs) and fine-tuning
  - Why needed here: CodeLKT relies on PLMs like BERT, RoBERTa, etc. for processing textual data
  - Quick check question: What is the difference between pre-training and fine-tuning in the context of language models?

- Concept: Domain Adaptation and Transfer Learning
  - Why needed here: DAPT and TAPT are critical components of the proposed approach
  - Quick check question: How does domain adaptation differ from task adaptation in the context of pre-trained language models?

## Architecture Onboarding

- Component map: GPT-4o for question generation -> Text concatenation for LKT input -> Pre-trained language models with DAPT/TAPT -> Linear transformation + sigmoid for correctness probability -> Large language models for feedback generation -> Cross-validation evaluation pipeline

- Critical path:
  1. Process student interaction data through GPT-4o to generate questions and knowledge concepts
  2. Format data as text sequences for LKT input
  3. Apply domain/task adaptation if needed
  4. Fine-tune the language model on the CodeLKT task
  5. Generate predictions and feedback through the integrated system

- Design tradeoffs:
  - Using textual representations vs. numerical sequences: Better semantic capture vs. increased computational complexity
  - Domain adaptation vs. task-specific training: Broader generalization vs. task-specific optimization
  - Comprehensive feedback vs. simple predictions: Enhanced educational value vs. increased system complexity

- Failure signatures:
  - Poor prediction performance: May indicate issues with input formatting, model architecture, or adaptation strategy
  - Ineffective feedback: Could result from inadequate prompt engineering or mismatch between model capabilities and educational requirements
  - Overfitting: Especially when using small datasets or excessive model capacity

- First 3 experiments:
  1. Baseline comparison: Implement LKT without DAPT/TAPT on one dataset to establish baseline performance
  2. Domain adaptation experiment: Apply DAPT to the baseline model and measure performance improvement
  3. Transfer learning experiment: Train a model on mathematics data and evaluate on coding tasks (and vice versa)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CodeLKT models compare when using different pre-trained language models (e.g., BERT, RoBERTa, DistilBERT, ALBERT, ELECTRA, ERNIE-2.0, DeBERTa-v3) in terms of AUC and ACC metrics?
- Basis in paper: [explicit] The paper compares the performance of various LKT models with different base models on three code-related datasets (CSEDM-19-Spring, CSEDM-19-Fall, CodeWorkout Spr2019)
- Why unresolved: While the paper presents results showing that RoBERTa and ERNIE-2.0 generally outperform other models, it does not provide a detailed analysis of why these models perform better or how their performance varies across different datasets
- What evidence would resolve it: A detailed analysis of the performance of each LKT model on all three datasets, including a breakdown of their strengths and weaknesses, would help understand the reasons behind their performance differences

### Open Question 2
- Question: How effective is the knowledge transfer between mathematics and coding domains using Domain Adaptive Pre-Training (DAPT) and Task Adaptive Pre-Training (TAPT) techniques?
- Basis in paper: [explicit] The paper explores the effectiveness of DAPT and TAPT in the coding domain and investigates cross-domain transfer between mathematics and coding
- Why unresolved: While the paper shows that models adapted to the code domain outperform the original BERT on code datasets, and models adapted to the mathematics domain perform well on CodeLKT tasks, it does not provide a detailed analysis of the effectiveness of knowledge transfer between the two domains
- What evidence would resolve it: A comprehensive comparison of the performance of models adapted to the code and mathematics domains on both code and mathematics tasks would help understand the extent of knowledge transfer between the two domains

### Open Question 3
- Question: How does the integrated prediction-application framework combining CodeLKT with large language models (LLMs) impact student learning outcomes in programming education?
- Basis in paper: [explicit] The paper presents an integrated system that combines CodeLKT with LLMs to generate personalized, pedagogically sound feedback
- Why unresolved: While the paper demonstrates the effectiveness of the integrated system in generating feedback, it does not provide evidence of its impact on student learning outcomes, such as improved performance or increased engagement
- What evidence would resolve it: A study evaluating the impact of the integrated system on student learning outcomes, including measures of performance, engagement, and satisfaction, would help understand its effectiveness in supporting student learning in programming education

## Limitations

- Reliance on textual representations may miss important structural and syntactic elements of code that are better captured through direct code analysis methods
- Domain adaptation approach demonstrates variable effectiveness across different domain pairs, with mathematics-to-coding transfer being notably less effective
- Evaluation is constrained to specific datasets and domains (Java, Python, and mathematics problems), limiting generalizability to other programming paradigms

## Confidence

**High Confidence Claims:**
- CodeLKT outperforms traditional KT and Code KT models on the tested datasets (AUC scores 0.8922-0.9116)
- Domain adaptation through DAPT improves performance on coding tasks
- The integrated feedback system successfully generates pedagogically-sound feedback

**Medium Confidence Claims:**
- Knowledge transfer between mathematics and coding domains is effective (based on limited evidence)
- TAPT provides additional performance improvements beyond DAPT
- The pedagogical prompting approach effectively guides feedback generation

**Low Confidence Claims:**
- Generalizability to other programming languages beyond Java and Python
- Effectiveness of the approach for more complex programming concepts
- Long-term impact on student learning outcomes

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate CodeLKT on additional programming domains (e.g., web development, data science, mobile development) to assess generalizability beyond Java and Python coding problems

2. **Syntax-Aware Evaluation**: Implement a version of CodeLKT that processes actual code syntax rather than text representations, and compare performance to assess whether textual processing captures sufficient semantic information

3. **Longitudinal Impact Study**: Conduct a controlled experiment measuring actual student learning outcomes when using the CodeLKT feedback system versus traditional approaches over an extended period