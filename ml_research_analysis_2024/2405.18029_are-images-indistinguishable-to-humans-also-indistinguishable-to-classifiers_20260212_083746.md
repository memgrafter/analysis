---
ver: rpa2
title: Are Images Indistinguishable to Humans Also Indistinguishable to Classifiers?
arxiv_id: '2405.18029'
source_url: https://arxiv.org/abs/2405.18029
tags:
- generated
- images
- distribution
- real
- classifiers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals a significant gap between real and generated
  image distributions from the perspective of neural network-based classifiers. Despite
  diffusion models achieving low FID scores and generating images nearly indistinguishable
  to humans, classifiers can effortlessly distinguish between real and generated images
  with over 98% accuracy.
---

# Are Images Indistinguishable to Humans Also Indistinguishable to Classifiers?

## Quick Facts
- arXiv ID: 2405.18029
- Source URL: https://arxiv.org/abs/2405.18029
- Authors: Zebin You; Xinyu Zhang; Hanzhong Guo; Jingdong Wang; Chongxuan Li
- Reference count: 40
- Primary result: Classifiers can distinguish real from generated images with >98% accuracy despite low FID scores and human-perceptible realism

## Executive Summary
This paper reveals a significant gap between real and generated image distributions from the perspective of neural network-based classifiers. Despite diffusion models achieving low FID scores and generating images nearly indistinguishable to humans, classifiers can effortlessly distinguish between real and generated images with over 98% accuracy. The study uncovers contradictions between classifier performance and evaluation metrics like FID and human judgments, showing classifiers easily differentiate diffusion models with similar FIDs but struggle with models from the same family that differ in scale. The methodology serves as a diagnostic tool for diffusion models, explains the model autophagy disorder phenomenon, and demonstrates that augmenting real data with generated data is more effective than replacement.

## Method Summary
The paper proposes using binary classifiers to measure the distributional gap between real and generated images. The core methodology involves training classifiers (typically ResNet-50) on mixed datasets containing both real images and images generated by diffusion models. By minimizing binary cross-entropy loss, these classifiers learn to distinguish between the two distributions, with classification accuracy serving as a direct proxy for distributional distance (Jensen-Shannon Divergence). The approach is validated across multiple datasets (CIFAR-10, ImageNet-256/512, COCO2014) and various diffusion model families (U-ViT, EDM, EDM2, DiT). The paper also introduces classifier guidance, where trained classifiers provide feedback during the diffusion sampling process to improve generated image realism.

## Key Results
- Classifiers achieve over 98% accuracy in distinguishing real from generated images across various settings
- Classifiers can easily differentiate between diffusion models with comparable FID scores but struggle to distinguish models from the same family with different scales
- Classifier guidance reduces classification accuracy from 98% to 79.88%, demonstrating enhanced realism of generated images
- Frequency domain analysis reveals that diffusion models struggle to capture certain frequency components, particularly in high-frequency bands

## Why This Works (Mechanism)

### Mechanism 1
Neural network-based classifiers learn discriminative features that capture subtle distributional differences between real and generated data that are not apparent to human observers or captured by FID metrics. The binary cross-entropy loss minimization leads to optimal classifiers that directly measure Jensen-Shannon Divergence between distributions.

### Mechanism 2
Classifier performance reveals contradictions with FID and human judgments by showing that models with similar FIDs can be easily distinguished while models from the same family with different scales cannot. Classifiers exploit architectural inductive biases and training artifacts that create distinctive patterns in generated images.

### Mechanism 3
Classifier guidance enhances generated image realism by steering diffusion models toward the real image distribution based on classifier feedback. Trained classifiers provide gradient-based guidance during diffusion sampling that pushes generated samples toward regions of the latent space more likely to be classified as real.

## Foundational Learning

- Concept: Distribution classification vs dataset classification
  - Why needed here: Understanding the difference between classifying individual datasets versus distinguishing entire distributions is crucial for grasping why this methodology reveals distributional gaps that other metrics miss
  - Quick check question: What is the key difference between dataset classification (identifying which dataset an image comes from) and distribution classification (distinguishing real vs generated distributions)?

- Concept: Jensen-Shannon Divergence and binary classification
  - Why needed here: The theoretical foundation linking optimal binary classifiers to distributional distance measures explains why classification accuracy serves as a direct proxy for distribution differences
  - Quick check question: How does minimizing binary cross-entropy loss in distribution classification relate to measuring the Jensen-Shannon Divergence between real and generated distributions?

- Concept: Frequency domain analysis in image processing
  - Why needed here: Understanding low-pass, high-pass, and band-pass filters in frequency space is essential for interpreting the diagnostic results showing where diffusion models succeed or fail in capturing different frequency components
  - Quick check question: What types of image features are typically captured by low-frequency versus high-frequency components in the Fourier domain?

## Architecture Onboarding

- Component map: Data pipeline (real dataset loading → preprocessing → train/val split) -> Generation pipeline (pre-trained diffusion models → image generation → distribution construction) -> Classification pipeline (classifier architecture → training on binary labels → evaluation) -> Analysis pipeline (frequency filtering → spatial cropping → accuracy measurement → interpretation)

- Critical path: Data generation → Classifier training → Distribution classification → Diagnostic analysis → Guidance application
  The most time-consuming step is typically classifier training, especially for ImageNet-scale experiments.

- Design tradeoffs:
  - Classifier architecture: ResNet-50 offers good balance of accuracy and training speed; ViT/S provides different inductive biases; ConvNeXt-T for modern architecture comparison
  - Training sample size: Larger samples improve classification accuracy but increase computational cost; experiments show significant accuracy gains even with small samples
  - Frequency analysis resolution: Finer frequency band granularity provides more detailed diagnostics but increases computational overhead

- Failure signatures:
  - Accuracy plateaus around 50%: Distributions are indistinguishable to the classifier
  - High variance across runs: Potential overfitting or unstable training
  - Low accuracy on specific frequency bands: Diffusion model struggles in those frequency ranges
  - Contradictory results between FID and classifier accuracy: Indicates limitations of FID as a sole evaluation metric

- First 3 experiments:
  1. Binary classification on same distribution (baseline): Train classifier to distinguish between two sets of real images from the same distribution to verify classifier cannot distinguish identical distributions
  2. Binary classification on real vs generated: Train classifier to distinguish between real and generated images from a single diffusion model to establish baseline classification accuracy
  3. Multi-way classification on multiple generated models: Train classifier to distinguish between real images and images from multiple diffusion models to assess classifier's ability to identify specific generation artifacts

## Open Questions the Paper Calls Out

### Open Question 1
How do other generative models, such as Masked Image Generation models or Autoregressive models, perform in terms of distribution classification accuracy compared to diffusion models? The paper mentions that experiments with GANs and Flow Matching Models have been conducted, but notes that Masked Image Generation models and Autoregressive models remain unexplored.

### Open Question 2
What are the specific limitations of the classifier-based approach when applied to datasets other than ImageNet and CIFAR-10, such as medical imaging or satellite imagery? The paper focuses on ImageNet and CIFAR-10 datasets, with no mention of other domains like medical or satellite imagery.

### Open Question 3
How does the distribution classification accuracy change when using more advanced classifier architectures, such as Vision Transformers or Large Language Models, compared to the ResNet-50 used in the paper? The paper uses ResNet-50 as the default classifier and mentions ConvNeXt-T and ViT-S for completeness, but does not explore more advanced architectures.

## Limitations

- Classifier overfitting to generation artifacts rather than capturing fundamental distributional differences
- Findings may not generalize to newer diffusion model architectures and training methodologies
- Limited validation of classifier guidance effectiveness through downstream task performance or human perceptual studies

## Confidence

**High Confidence**: Classifiers can distinguish real from generated images with high accuracy (well-supported by extensive empirical results across multiple datasets and classifier architectures)

**Medium Confidence**: Classifier performance reveals contradictions with FID and human judgments (reasonably supported but requires additional validation)

**Low Confidence**: Classifier guidance enhances generated image realism (supported by preliminary experiments but lacks extensive validation)

## Next Checks

1. Conduct the same distribution classification experiments on recently developed diffusion models (e.g., Stable Diffusion, Kandinsky 2.2) to verify whether the observed contradictions between classifier performance and FID scores persist.

2. Perform controlled human studies comparing classifier-guided generations with standard generations, measuring both raw realism scores and task-specific performance to validate that classifier guidance genuinely improves perceptual quality.

3. Apply the frequency domain analysis methodology to a broader range of diffusion models and compare the frequency band accuracy patterns with known architectural differences to establish more concrete links between model design choices and generation artifacts.