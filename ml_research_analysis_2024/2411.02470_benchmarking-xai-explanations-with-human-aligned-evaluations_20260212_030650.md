---
ver: rpa2
title: Benchmarking XAI Explanations with Human-Aligned Evaluations
arxiv_id: '2411.02470'
source_url: https://arxiv.org/abs/2411.02470
tags:
- explanations
- resnet50
- image
- explanation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PASTA, a human-aligned framework for evaluating
  explainable AI (XAI) techniques in computer vision. The authors construct PASTA-dataset,
  a large-scale benchmark that enables comparative analysis of 20 XAI methods across
  multiple models, datasets, and explanation types.
---

# Benchmarking XAI Explanations with Human-Aligned Evaluations

## Quick Facts
- arXiv ID: 2411.02470
- Source URL: https://arxiv.org/abs/2411.02470
- Reference count: 40
- One-line primary result: Introduces PASTA-score, a data-driven metric that predicts human preferences for XAI explanations and guides the design of more interpretable models

## Executive Summary
This paper addresses the challenge of evaluating explainable AI (XAI) techniques by introducing PASTA, a human-aligned framework for benchmarking XAI explanations in computer vision. The authors construct PASTA-dataset, a large-scale benchmark that enables comparative analysis of 20 XAI methods across multiple models, datasets, and explanation types. They then propose PASTA-score, a data-driven metric that predicts human preferences for explanations, offering scalable, consistent, and reliable evaluation. The PASTA-score is shown to align with established benchmarks and is applied to guide the design of more interpretable models and explanations.

## Method Summary
The authors propose a two-part framework: PASTA-dataset, a large-scale collection of 633,000 human ratings across 20 XAI methods and 4 datasets, and PASTA-score, a learned metric that predicts human preferences for explanations. The PASTA-score uses Vision-Language Model (VLM) embeddings to encode both saliency maps and concept-based explanations into a shared representation space, which is then fed to a multi-layer perceptron that outputs predicted human Likert ratings. The metric is trained on the PASTA-dataset and evaluated for its ability to predict human judgments of explanation quality.

## Key Results
- PASTA-dataset is the largest benchmark of its kind, containing 633,000 human ratings across 4 datasets, 7 classifier backbones, and 20 XAI methods
- PASTA-score achieves strong correlations with human ratings (MSE, QWK, SCC metrics) and generalizes well to unseen XAI methods
- Saliency-based explanations are preferred over concept-based ones, and model size negatively impacts interpretability
- PASTA-score can optimize explanations for human interpretability without compromising faithfulness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PASTA-score learns to emulate human preferences for explanations by embedding both saliency maps and concept-based outputs into a shared representation space
- Mechanism: Visual explanations (saliency maps) are overlaid on images and encoded via a Vision-Language Model (VLM) image encoder; concept-based explanations are converted to text and encoded via the VLM text encoder. These embeddings are concatenated with label information and fed to a multi-layer perceptron that outputs predicted human Likert ratings
- Core assumption: The VLM encoders produce embeddings that capture perceptual features relevant to human interpretability judgments
- Evidence anchors:
  - [abstract] "Our second contribution is an automated, data-driven benchmark that predicts human preferences using the PASTA-dataset."
  - [section] "Inspired by Automated Essay Scoring... we opt for a foundation model that we will fine-tune using a multi-linear perceptron."
  - [corpus] Weak evidence: related work focuses on vision-language models for interpretability, but not specifically on training with human ratings
- Break condition: If the VLM encoders do not capture human-relevant features, the scoring model will fail to predict human judgments accurately

### Mechanism 2
- Claim: PASTA-score achieves scalability and consistency by replacing expensive human user studies with a learned metric trained on large-scale human-annotated data
- Mechanism: The PASTA-dataset collects 633,000 Likert ratings from 24 annotators on 21,100 instances across 20 XAI methods and 4 datasets. This dataset trains the PASTA-score model, which can then predict scores for new explanations without additional human input
- Core assumption: The diversity and volume of the PASTA-dataset are sufficient to generalize human preferences to unseen explanations
- Evidence anchors:
  - [abstract] "This dataset enables robust, comparative analysis of XAI techniques based on human judgment."
  - [section] "The PASTA-dataset contains an overall number of 633,000 samples... which is the largest benchmark available of this kind."
  - [corpus] Weak evidence: large-scale human-annotated datasets exist in other domains (e.g., automated essay scoring), but not for XAI
- Break condition: If the training data does not cover the diversity of future XAI methods or domains, the metric will generalize poorly

### Mechanism 3
- Claim: PASTA-score guides XAI method design by providing a differentiable perceptual loss that can optimize explanations for human interpretability without sacrificing faithfulness
- Mechanism: In experiments, the PASTA-score is used as a regularizer in the objective function for RISE, blending faithfulness (class score) with interpretability (PASTA-score) via a weighting parameter λ
- Core assumption: The PASTA-score is aligned with human interpretability and can be optimized jointly with faithfulness metrics
- Evidence anchors:
  - [abstract] "We propose to apply our scoring method to probe the interpretability of existing models and to build more human interpretable XAI methods."
  - [section] "We propose to use the PASTA-score to enhance the interpretability of an off-the-shelf XAI method, namely RISE."
  - [corpus] Weak evidence: some work uses learned metrics as perceptual losses, but not specifically for XAI interpretability
- Break condition: If optimizing for PASTA-score degrades faithfulness or leads to misleading explanations, the approach fails

## Foundational Learning

- Concept: VLM embeddings as perceptual feature extractors
  - Why needed here: The PASTA-score relies on VLMs to map diverse explanation modalities (images, text) into a unified representation space for scoring
  - Quick check question: Can you explain how a Vision-Language Model encodes both images and text into comparable embeddings?

- Concept: Multi-objective optimization in XAI
  - Why needed here: The PASTA-score is used to balance interpretability and faithfulness when designing or selecting explanations
  - Quick check question: How would you formulate an objective that combines a faithfulness metric with a perceptual score like PASTA-score?

- Concept: Human-centered evaluation metrics
  - Why needed here: The PASTA-score is grounded in human Likert ratings and aims to predict human interpretability judgments
  - Quick check question: What are the limitations of using automated metrics (e.g., faithfulness) compared to human evaluations?

## Architecture Onboarding

- Component map:
  - PASTA-dataset -> VLM encoders -> Scoring network -> Predicted Likert ratings
  - Image + explanation + label -> CLIP/SIGLIP encoders -> MLP -> Human-aligned score

- Critical path:
  1. Load explanation and corresponding image/class label
  2. Encode explanation via appropriate VLM encoder (saliency vs. concept)
  3. Concatenate with label encoding
  4. Pass through scoring network to predict Likert rating
  5. Compute loss against ground truth and update model

- Design tradeoffs:
  - VLM choice: CLIP vs. SIGLIP vs. EV A vs. BLIP—affects embedding quality and computational cost
  - Embedding strategy: Heatmap overlay vs. masked image for saliency; raw text vs. weighted sum for concepts
  - Aggregation: Mode vs. mean vs. median for ground truth from 5 annotators—impacts robustness

- Failure signatures:
  - Low correlation between predicted and human scores (MSE, QWK, SCC)
  - Overfitting to training XAI methods (poor generalization to unseen methods)
  - Inconsistent predictions for similar explanations (high variance)

- First 3 experiments:
  1. Train PASTA-score on a subset of the dataset and evaluate on held-out samples to check basic predictive ability
  2. Compare different VLM encoders (CLIP, SIGLIP, etc.) on a fixed task to select best performer
  3. Test PASTA-score as a regularizer in RISE optimization and measure impact on interpretability vs. faithfulness tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How generalizable is the PASTA-score across different cultural contexts and diverse user groups?
- Basis in paper: [inferred] The paper notes that all annotators were based in India, raising concerns about potential cultural biases in the human preferences captured by the dataset
- Why unresolved: The study did not include annotators from diverse cultural backgrounds, limiting the generalizability of the PASTA-score to global user populations
- What evidence would resolve it: Conducting the same human evaluation study with annotators from diverse cultural and demographic backgrounds, then retraining and testing the PASTA-score on this expanded dataset

### Open Question 2
- Question: Can the PASTA-score effectively evaluate explanations for non-image modalities such as text or tabular data?
- Basis in paper: [inferred] The PASTA-dataset focuses exclusively on image-based classification tasks, and the scoring framework relies on vision-language models that may not directly extend to other data types
- Why unresolved: The methodology and benchmarks are specifically designed for computer vision, with no validation on other data modalities
- What evidence would resolve it: Extending the PASTA-dataset and evaluation framework to include text or tabular data, and testing the PASTA-score's performance across these new modalities

### Open Question 3
- Question: What is the impact of model size on the interpretability of concept-based explanations compared to saliency-based ones?
- Basis in paper: [explicit] The paper found that larger model backbones negatively impact the interpretability of saliency-based explanations, but did not explore this relationship for concept-based methods
- Why unresolved: The study only examined model size effects on saliency-based XAI methods, leaving the impact on concept-based explanations unexplored
- What evidence would resolve it: Conducting experiments similar to those in the paper but focusing on concept-based XAI methods across different model sizes, then comparing the results with saliency-based methods

## Limitations

- The PASTA-score's reliance on VLM embeddings may not capture all human-relevant features for interpretability judgments, potentially limiting its effectiveness
- The generalizability of the PASTA-dataset to unseen XAI methods or domains is uncertain, which may affect the metric's reliability on new explanations
- Optimizing explanations using PASTA-score as a regularizer may lead to unintended consequences, such as degraded faithfulness or misleading explanations

## Confidence

- High confidence: The PASTA-dataset's construction and the basic framework of PASTA-score are well-defined and reproducible
- Medium confidence: The VLM embeddings' effectiveness in capturing human-relevant features and the PASTA-score's ability to predict human preferences are promising but require further validation
- Low confidence: The generalizability of the PASTA-dataset to unseen XAI methods or domains and the potential unintended consequences of optimizing explanations using PASTA-score need more investigation

## Next Checks

1. Conduct ablation studies to assess the impact of different VLM encoders (CLIP, SIGLIP, etc.) on the PASTA-score's performance and determine the best-performing model
2. Evaluate the PASTA-score's generalization ability by testing it on a diverse set of unseen XAI methods and datasets to ensure robustness and reliability
3. Investigate the tradeoff between interpretability and faithfulness when optimizing explanations using PASTA-score as a regularizer. Assess the impact on the quality and reliability of the explanations generated