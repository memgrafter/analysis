---
ver: rpa2
title: Evaluating and Improving Continual Learning in Spoken Language Understanding
arxiv_id: '2402.10427'
source_url: https://arxiv.org/abs/2402.10427
tags:
- learning
- task
- continual
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Dual-transfer Matching Index (DMI), a unified
  evaluation metric for continual learning that quantifies stability, plasticity,
  and generalizability in a disentangled manner. Unlike existing metrics that focus
  on one or two aspects and entangle plasticity with stability/generalizability, DMI
  uses Hungarian matching to evaluate model performance on all seen and unseen tasks,
  providing separate scores for each property.
---

# Evaluating and Improving Continual Learning in Spoken Language Understanding

## Quick Facts
- arXiv ID: 2402.10427
- Source URL: https://arxiv.org/abs/2402.10427
- Reference count: 19
- Primary result: Proposes Dual-transfer Matching Index (DMI), a unified evaluation metric that disentangles stability, plasticity, and generalizability in continual learning

## Executive Summary
This paper addresses the challenge of evaluating continual learning in Spoken Language Understanding (SLU) by proposing a unified metric called Dual-transfer Matching Index (DMI). Unlike existing metrics that focus on one or two aspects while entangling plasticity with stability/generalizability, DMI uses Hungarian matching to evaluate model performance on all seen and unseen tasks, providing separate scores for each property. The authors demonstrate that knowledge distillation techniques can improve different aspects of continual learning - audio-KD and seq-KD enhance stability, while sent-KD improves generalizability and plasticity.

## Method Summary
The paper proposes an end-to-end ASR+SLU model with Wav2vec 2.0/DistilHuBERT encoder, transformer decoder, and text encoder for knowledge distillation. The model is trained on a sequence of 6 tasks with rehearsal memory (1% samples). Knowledge distillation techniques (audio-KD, seq-KD, sent-KD) are applied to boost stability, plasticity, and generalizability. The Dual-transfer Matching Index (DMI) evaluates performance using class-agnostic k-means clustering on predicted embeddings, then matches these clusters to ground truth using Hungarian algorithm across the full T×T evaluation matrix.

## Key Results
- DMI provides separate scores for stability, plasticity, and generalizability, unlike traditional metrics that entangle these properties
- Different knowledge distillation techniques (audio-KD, seq-KD, sent-KD) improve different properties as measured by DMI
- DMI is more sensitive to task ordering effects than traditional metrics like BWT and FWT
- The proposed method shows improved performance on FSC and SLURP datasets compared to baseline continual learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DMI disentangles stability, plasticity, and generalizability by evaluating all seen and unseen tasks with Hungarian matching
- Mechanism: Uses class-agnostic k-means clustering on predicted embeddings at each task, then matches these clusters to ground truth using Hungarian algorithm, allowing separate quantification across the full T×T evaluation matrix
- Core assumption: Class-agnostic clustering captures meaningful semantic groupings that can be matched to ground truth even for unseen classes
- Evidence anchors:
  - [abstract] "DMI uses Hungarian matching to evaluate model performance on all seen and unseen tasks, providing separate scores for each property"
  - [section 3.1] "We compute the DMI by employing the Hungarian maximum matching algorithm to compare model predictions with ground-truth intent labels for all seen and unseen classes at the end of each task"
  - [corpus] Weak - corpus doesn't contain direct evidence about Hungarian matching mechanism
- Break condition: If clustering fails to separate distinct semantic groups or Hungarian matching produces incorrect alignments, the disentanglement breaks down

### Mechanism 2
- Claim: Knowledge distillation techniques improve different properties of continual learning
- Mechanism: Audio-KD and seq-KD regularize parameter changes to improve stability by maintaining similarity to previous task representations; sent-KD aligns audio representations with pretrained text embeddings to improve generalizability and plasticity
- Core assumption: Distilling from previous task models and pretrained text encoders provides useful regularization signals that enhance specific learning properties
- Evidence anchors:
  - [section 3.3] "We introduce a combination of different knowledge distillation techniques to boost the stability, plasticity, and generalizability"
  - [section 4.2] "We can observe that both BWT and the first number of DMI (DMIstab) are increased after adding audio-KD and seq-KD, indicating that the stability of the model is improved"
  - [corpus] Weak - corpus doesn't provide evidence about specific KD mechanisms
- Break condition: If teacher models provide poor quality knowledge or regularization becomes too restrictive, improvements may reverse into degradation

### Mechanism 3
- Claim: Task ordering significantly impacts the three properties measured by DMI
- Mechanism: Different task ordering strategies (frequency, close-semantic, diverse-semantic) create different learning trajectories that affect stability, plasticity, and generalizability differently as measured by DMI scores
- Core assumption: The semantic relationships between tasks and their frequency distributions create meaningful variations in learning dynamics
- Evidence anchors:
  - [section 4.3] "Both close-semantic and diverse-semantic orders improve the generalizability while decreasing the stability and plasticity compared to the frequency order"
  - [section 4.3] "Our DMI metric provides a more sensitive measurement of stability, plasticity, and generalizability when the order of tasks changes"
  - [corpus] Weak - corpus doesn't contain evidence about task ordering effects
- Break condition: If task ordering effects are negligible or DMI fails to capture ordering impacts, the sensitivity claim breaks down

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why continual learning is necessary - models lose previously learned knowledge when training on new tasks
  - Quick check question: What happens to model performance on previous tasks when fine-tuning on new tasks without any continual learning techniques?

- Concept: Stability-plasticity dilemma
  - Why needed here: Fundamental tradeoff between retaining old knowledge (stability) and learning new knowledge (plasticity) that DMI aims to disentangle
  - Quick check question: Why can't a model be maximally stable and maximally plastic at the same time?

- Concept: Knowledge distillation and transfer learning
  - Why needed here: Core mechanism for improving continual learning through regularization from previous tasks and pretrained models
  - Quick check question: How does knowledge distillation help prevent catastrophic forgetting in continual learning?

## Architecture Onboarding

- Component map: Audio encoder (Wav2vec 2.0/DistilHuBERT) → ASR decoder (transformer) → text encoder (Sentence-BERT) for KD; rehearsal memory buffer for sampling previous data
- Critical path: Audio input → audio encoder → ASR decoder → sequence generation → clustering → Hungarian matching → DMI computation
- Design tradeoffs: Single ASR decoder vs separate ASR + NLU components; class-agnostic evaluation vs class-aware; Hungarian matching complexity vs simpler metrics
- Failure signatures: High DMI variance across runs suggests unstable clustering; poor stability scores indicate catastrophic forgetting; mismatch between DMI and traditional metrics suggests entanglement issues
- First 3 experiments:
  1. Compare DMI vs traditional metrics (BWT, FWT) on a simple task ordering to verify disentanglement
  2. Test different KD combinations (audio-KD, seq-KD, sent-KD) to verify property-specific improvements
  3. Evaluate task ordering effects (frequency vs semantic) to verify DMI sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Dual-transfer Matching Index (DMI) compare to other metrics in capturing task ordering effects in different datasets and task distributions?
- Basis in paper: [explicit] The paper mentions that DMI is more sensitive than previous metrics in capturing the effect of class ordering, particularly when the contents of adjacent tasks exhibit large semantic variations.
- Why unresolved: While the paper demonstrates DMI's effectiveness on SLU datasets, it's unclear how it performs on other domains with different task distributions and semantic variations between tasks.
- What evidence would resolve it: Experiments applying DMI to different domains (e.g., computer vision, NLP) with varying task distributions and semantic relationships between tasks.

### Open Question 2
- Question: What is the optimal trade-off between stability, plasticity, and generalizability when using different knowledge distillation techniques?
- Basis in paper: [inferred] The paper shows that different KDs (audio, sequence, sentence) can enhance stability, plasticity, and generalizability respectively, but the trade-offs between these properties are not fully explored.
- Why unresolved: The paper mentions that plasticity might decrease as a trade-off when KD techniques are added to regularize the model, but the optimal balance between the three properties is not investigated.
- What evidence would resolve it: Systematic experiments varying the strength and combination of different KDs to find the optimal balance between stability, plasticity, and generalizability.

### Open Question 3
- Question: How does the performance of DMI scale with the number of tasks and classes in continual learning scenarios?
- Basis in paper: [inferred] The paper experiments with 6 tasks, but it's unclear how DMI performs in scenarios with many more tasks and classes.
- Why unresolved: The computational complexity of DMI might increase with the number of tasks and classes, potentially affecting its practicality in real-world scenarios with many tasks.
- What evidence would resolve it: Experiments scaling the number of tasks and classes while measuring DMI's performance and computational requirements.

## Limitations
- Effectiveness of Hungarian matching for class-agnostic clustering evaluation depends on semantic groupings being preserved across different task orderings and dataset characteristics
- Evaluation focuses primarily on intent classification rather than broader SLU tasks like slot filling, limiting generalizability of findings
- Specific implementation details for knowledge distillation techniques are not fully specified, making it difficult to assess whether improvements are due to proposed methods or implementation details

## Confidence

**High Confidence**: The core contribution of proposing a unified evaluation metric that disentangles stability, plasticity, and generalizability is well-supported. The mechanism of using Hungarian matching for class-agnostic evaluation across seen and unseen tasks is clearly specified and theoretically sound.

**Medium Confidence**: The claim that different knowledge distillation techniques improve specific properties (audio-KD for stability, seq-KD for plasticity, sent-KD for generalizability) is supported by experimental results but lacks ablation studies showing individual contribution of each KD component.

**Low Confidence**: The sensitivity of DMI to task ordering effects requires more extensive validation across different dataset characteristics and task relationships. The current evidence is limited to comparing three specific ordering strategies.

## Next Checks

1. **Replication Study**: Implement DMI on a different SLU dataset with different task characteristics (e.g., more granular slot filling tasks) to verify metric generalizability and the robustness of KD improvements across task types.

2. **Ablation Analysis**: Conduct controlled experiments removing each KD component (audio-KD, seq-KD, sent-KD) individually to quantify their specific contributions to stability, plasticity, and generalizability metrics.

3. **Task Ordering Stress Test**: Systematically vary task ordering beyond the three strategies tested, including random orderings and semantically conflicting sequences, to stress-test DMI's sensitivity claims and identify potential edge cases where the metric may fail.