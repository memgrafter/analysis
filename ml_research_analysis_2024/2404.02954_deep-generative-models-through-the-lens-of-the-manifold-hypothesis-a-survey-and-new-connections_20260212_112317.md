---
ver: rpa2
title: 'Deep Generative Models through the Lens of the Manifold Hypothesis: A Survey
  and New Connections'
arxiv_id: '2404.02954'
source_url: https://arxiv.org/abs/2404.02954
tags:
- learning
- which
- manifold
- section
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides the first comprehensive overview of deep generative
  models (DGMs) through the manifold hypothesis lens, demonstrating that manifold-awareness
  is a crucial factor in model performance. The authors formalize how DGMs interact
  with data supported on low-dimensional manifolds embedded in high-dimensional spaces,
  showing that standard likelihood-based approaches suffer from unavoidable numerical
  instability due to dimensional mismatch.
---

# Deep Generative Models through the Lens of the Manifold Hypothesis: A Survey and New Connections

## Quick Facts
- arXiv ID: 2404.02954
- Source URL: https://arxiv.org/abs/2404.02954
- Reference count: 40
- Key outcome: Manifold-awareness is crucial for DGM performance; manifold overfitting is unavoidable for full-dimensional models, but two-step and diffusion models avoid this through different mechanisms.

## Executive Summary
This survey provides the first comprehensive overview of deep generative models through the manifold hypothesis lens, demonstrating that manifold-awareness is a crucial factor in model performance. The authors formalize how DGMs interact with data supported on low-dimensional manifolds embedded in high-dimensional spaces, showing that standard likelihood-based approaches suffer from unavoidable numerical instability due to dimensional mismatch. They prove that any sequence of full-dimensional models learning a manifold-supported distribution must experience exploding likelihoods, both in magnitude and gradient, outside the manifold. The survey reveals that two-step models (like latent diffusion) minimize Wasserstein distance by first learning a manifold through reconstruction, then learning the distribution on that manifold.

## Method Summary
The paper provides a theoretical survey of deep generative models through the lens of the manifold hypothesis. It proves that full-dimensional models suffer from manifold overfitting when learning distributions supported on low-dimensional manifolds. The authors analyze diffusion models and show they avoid this issue by working with full-dimensional noisy distributions at intermediate time steps. They also interpret two-step models (like VAEs with latent distributions) as approximately minimizing Wasserstein distance. The paper includes a reproduction experiment comparing diffusion models trained in ambient space versus latent space, demonstrating that latent diffusion models have more stable score functions during generation.

## Key Results
- Any full-dimensional model learning a manifold-supported distribution must experience exploding likelihoods both in magnitude and gradient outside the manifold
- Diffusion models avoid manifold overfitting by transforming the problem into learning full-dimensional noisy distributions at each time step
- Two-step models can be interpreted as approximately minimizing Wasserstein distance between true and model distributions
- Score functions of diffusion models must diverge at manifold boundaries, explaining their empirical success

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Manifold overfitting is unavoidable when maximizing likelihood in high-dimensional spaces for low-dimensional manifold data.
- **Mechanism:** When the data density pX* is supported on a d*-dimensional manifold M embedded in D-dimensional space, any full-dimensional model density pXθ that concentrates mass around M must have its likelihood values diverge to infinity on M to maximize the expected log-likelihood. This is because M has zero volume in RD, so the density can become arbitrarily large without violating the normalization constraint.
- **Core assumption:** The manifold M has Lebesgue measure zero in the ambient space RD.
- **Evidence anchors:**
  - [abstract]: "any sequence of full-dimensional models learning a manifold-supported distribution must experience exploding likelihoods"
  - [section]: "WhenM was full-dimensional, it would be impossible to havepXθt(x)→∞ast→∞for allx∈M, as doing so would quickly violate the requirement that the densities integrate to1"
  - [corpus]: Missing corpus evidence on this specific mechanism.
- **Break condition:** If the data is truly full-dimensional (d* = D), manifold overfitting cannot occur.

### Mechanism 2
- **Claim:** Diffusion models can learn manifold-supported distributions without suffering from manifold overfitting.
- **Mechanism:** By progressively adding noise through an Ornstein-Uhlenbeck process, diffusion models transform the manifold-supported data density into full-dimensional noisy densities pXt* at each time step t. These noisy densities are full-dimensional, so maximum-likelihood training (via score matching) works properly. The reverse process then denoises back to the original manifold-supported distribution.
- **Core assumption:** The score function ∇x logpXt*(x) exists and can be approximated by a neural network at each noise level.
- **Evidence anchors:**
  - [abstract]: "diffusion models' score functions must diverge at the manifold boundary, explaining their empirical success"
  - [section]: "Since pXt* is full-dimensional fort> 0, we should not expect maximum-likelihood to fail at learning it"
  - [corpus]: Missing corpus evidence on this specific mechanism.
- **Break condition:** If the noise levels are not properly chosen or the score function cannot be learned accurately.

### Mechanism 3
- **Claim:** Two-step models minimize Wasserstein distance between the true and model distributions.
- **Mechanism:** The first step learns an autoencoder that reconstructs data perfectly on the manifold, implicitly learning the manifold structure. The second step learns a distribution on the learned manifold. Together, these steps minimize an upper bound of the Wasserstein distance, which becomes tight at optimality when perfect reconstructions are achievable.
- **Core assumption:** Perfect reconstructions are achievable (EX∼pX*[∥X−gθ1(fϕ(X))∥2 2] = 0).
- **Evidence anchors:**
  - [abstract]: "DGMs on learned representations of autoencoders can be interpreted as approximately minimizing Wasserstein distance"
  - [section]: "two-step models can be interpreted as (potentially regularized) minimizers of an upper bound of the Wasserstein distance"
  - [corpus]: Missing corpus evidence on this specific mechanism.
- **Break condition:** If perfect reconstructions are not achievable due to topological constraints.

## Foundational Learning

- **Concept:** Measure theory and probability distributions on manifolds
  - **Why needed here:** Understanding how distributions supported on low-dimensional manifolds differ from full-dimensional densities is crucial for grasping why standard DGMs fail.
  - **Quick check question:** What is the key difference between a density with respect to the Lebesgue measure and a density with respect to the Riemannian measure on a manifold?

- **Concept:** Change-of-variables formula and its extensions
  - **Why needed here:** This formula is fundamental for understanding how normalizing flows and injective flows work, especially when the latent space has lower dimension than the data space.
  - **Quick check question:** How does the injective change-of-variables formula differ from the standard formula when the latent dimension is less than the data dimension?

- **Concept:** Optimal transport and Wasserstein distances
  - **Why needed here:** Wasserstein distances provide a mathematically sound objective for training DGMs when the data is supported on low-dimensional manifolds, unlike KL divergence.
  - **Quick check question:** Why does the Wasserstein distance remain meaningful when comparing a full-dimensional model distribution to a manifold-supported true distribution?

## Architecture Onboarding

- **Component map:** Data → Encoder (fϕ) → Latent representation → Decoder (gθ) → Reconstruction; Noise → Score network (sXθ) → Denoised sample
- **Critical path:** For two-step models: Encode data → Learn distribution in latent space → Decode samples. For diffusion models: Add noise progressively → Learn reverse denoising process → Generate samples by reversing the process.
- **Design tradeoffs:**
  - Full-dimensional vs. low-dimensional latent spaces: Full-dimensional allows more expressive models but suffers from manifold overfitting; low-dimensional avoids this but may struggle with topological complexity.
  - Deterministic vs. stochastic encoders: Deterministic encoders are computationally simpler but may limit expressiveness; stochastic encoders can capture more complex distributions but add complexity.
  - Likelihood-based vs. other objectives: Likelihood-based methods are intuitive but fail on manifolds; Wasserstein and MMD objectives work better but may be harder to optimize.
- **Failure signatures:**
  - Manifold overfitting: Test likelihoods diverge to infinity, samples concentrate around the data manifold but have wrong distribution within it.
  - Numerical instability: Likelihood evaluations explode, gradients become unstable during training.
  - Mode collapse: Generated samples lack diversity, concentrate on a subset of the data manifold.
- **First 3 experiments:**
  1. Train a VAE on a synthetic dataset with known manifold structure and examine the test log-likelihood behavior as training progresses.
  2. Implement a simple diffusion model and visualize the learned score function at different noise levels on a 2D manifold dataset.
  3. Compare a two-step model with a single-step model on a dataset known to lie on a low-dimensional manifold, measuring sample quality and training stability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can we theoretically characterize the convergence rates of diffusion models on unknown manifolds in the finite-sample regime?
- **Basis in paper:** [explicit] The paper discusses that current analyses assume a nonparametric regime and that understanding generalization rather than memorization remains an open problem. Kadkhodaie et al. (2024) study diffusion models through the lens of inductive biases but more formal explanations are needed.
- **Why unresolved:** The paper states that assuming the nonparametric regime is unrealistic since expectations with respect to the true distribution cannot be computed in practice, and understanding what drives DGMs to generalize rather than memorize remains a relevant problem.
- **What evidence would resolve it:** A formal proof establishing convergence rates for diffusion models on unknown manifolds that account for finite sample effects and empirical validation showing these rates match theoretical predictions.

### Open Question 2
- **Question:** Can we develop an end-to-end training objective for two-step models that is manifold-aware and scalable to massive datasets?
- **Basis in paper:** [explicit] The paper discusses that current end-to-end methods are manifold-unaware despite providing desirable inductive bias, with an exception being generalized energy-based models which cannot be readily extended beyond using energy-based models as the latent distribution.
- **Why unresolved:** The paper notes that designing an end-to-end objective for training in a manifold-aware fashion is not always straightforward, and current end-to-end methods ignore terms that cannot be ignored when parameters are not fixed after the first step of training.
- **What evidence would resolve it:** A mathematically rigorous end-to-end objective that provably maintains manifold-awareness during training, implementation demonstrating scalability to large datasets like ImageNet, and empirical results showing improved performance over current two-step approaches.

### Open Question 3
- **Question:** What is the theoretical understanding of score matching and conditional flow matching under dimensionality mismatch?
- **Basis in paper:** [explicit] The paper states that a theoretical understanding of score matching and conditional flow matching under misspecified dimension is lacking, despite these methods being susceptible to manifold-related issues.
- **Why unresolved:** While the paper discusses manifold overfitting for likelihood-based models and exploding score functions for diffusion models, it does not provide a comprehensive theoretical framework for understanding how dimensionality mismatch affects score matching and conditional flow matching.
- **What evidence would resolve it:** A unified theoretical framework characterizing when and how score matching and conditional flow matching fail under dimensionality mismatch, with formal proofs and empirical validation across multiple datasets and model architectures.

## Limitations
- The proofs rely on measure-theoretic arguments assuming perfect knowledge of the data manifold, which is rarely available in practice.
- The connection between score function divergence and empirical success lacks comprehensive experimental validation across diverse datasets.
- The empirical claims about latent diffusion models' superiority are based on limited experiments and require broader validation.

## Confidence

**High Confidence:** The mathematical proofs showing that full-dimensional models must experience exploding likelihoods on zero-measure manifolds are rigorous and well-established in the measure theory literature.

**Medium Confidence:** The interpretation of two-step models as Wasserstein distance minimizers is theoretically sound but depends on assumptions about perfect reconstruction that may not hold in practice.

**Low Confidence:** The empirical claims about latent diffusion models' superiority are based on limited experiments and require broader validation across different data types and model configurations.

## Next Checks

1. **Extended Manifold Experiments:** Validate the manifold overfitting phenomenon across diverse synthetic manifolds (varying dimension, curvature, and topology) and real-world datasets with known low-dimensional structure.

2. **Score Function Analysis:** Systematically measure score function norms during generation across different noise levels and architectures to confirm the predicted divergence behavior at manifold boundaries.

3. **Reconstruction Quality Impact:** Quantify how reconstruction error in the first step of two-step models affects the Wasserstein distance upper bound, and determine the threshold where this bound becomes meaningful.