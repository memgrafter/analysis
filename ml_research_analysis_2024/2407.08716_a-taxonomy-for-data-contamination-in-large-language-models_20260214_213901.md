---
ver: rpa2
title: A Taxonomy for Data Contamination in Large Language Models
arxiv_id: '2407.08716'
source_url: https://arxiv.org/abs/2407.08716
tags:
- contamination
- data
- pretraining
- test
- during
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a taxonomy categorizing different types of
  data contamination that can occur during LLM pretraining, where evaluation data
  leaks into the pretraining corpus and inflates model performance. The taxonomy distinguishes
  between dataset-level contamination (selection/distribution) and instance-level
  contamination (masking/noising/augmenting), and clarifies what phenomena are not
  contamination (language understanding, prior task understanding, transductive learning).
---

# A Taxonomy for Data Contamination in Large Language Models

## Quick Facts
- **arXiv ID**: 2407.08716
- **Source URL**: https://arxiv.org/abs/2407.08716
- **Reference count**: 27
- **Primary result**: Introduces taxonomy for LLM data contamination and shows approximate contamination can significantly boost performance while evading current detection methods

## Executive Summary
This paper presents a comprehensive taxonomy for understanding and categorizing data contamination in large language model pretraining. The authors identify how evaluation data can leak into pretraining corpora, artificially inflating model performance measurements. Through controlled experiments on summarization and QA tasks, they demonstrate that various forms of approximate contamination—including GPT-3.5-generated answers and summaries—can significantly boost model performance, often matching or exceeding the benefits of in-domain training. The study reveals critical limitations in current decontamination methods, particularly their inability to detect the "NOISED" contamination setting.

## Method Summary
The authors develop a controlled contamination framework to systematically evaluate different contamination scenarios. They create five contamination conditions (TRUNCATED, MASKED, NOISED, SUBSAMPLED, CONTROL) by applying GPT-3.5 to generate answers or summaries for evaluation data, then using these outputs to contaminate pretraining data. The experiments use T0 models fine-tuned on the contaminated data and evaluate performance on the original test sets. The contamination detection methods employed include n-gram overlap analysis and dataset fingerprint techniques. The study focuses on English-language tasks including summarization (XSum, CNN/DM) and question answering (Natural Questions, SQuAD).

## Key Results
- Various forms of approximate contamination (TRUNCATED, MASKED, NOISED) significantly boost model performance on downstream tasks
- The NOISED contamination setting frequently outperforms baseline models and evades detection by current decontamination methods
- Approximate contamination can provide performance benefits comparable to or exceeding in-domain training

## Why This Works (Mechanism)
Data contamination artificially inflates model performance when evaluation data appears in pretraining corpora. When models are trained on data that overlaps with test sets—even in approximate forms like paraphrased content or partial information—they can memorize or learn patterns specific to evaluation examples. This leads to artificially high performance metrics that don't reflect true generalization capabilities. The NOISED setting is particularly effective because it introduces enough variation to evade simple overlap detection while still providing sufficient signal for the model to recognize evaluation patterns.

## Foundational Learning

**Dataset contamination detection**: Why needed - To ensure evaluation integrity and prevent inflated performance claims; Quick check - Verify n-gram overlap and fingerprint analysis methods detect known contaminated examples.

**Approximate contamination types**: Why needed - Real-world contamination rarely involves exact duplicates; Quick check - Test detection methods on paraphrased and semantically similar content.

**Controlled contamination experiments**: Why needed - To isolate and measure specific contamination effects; Quick check - Compare performance across different contamination conditions while holding other variables constant.

**Pretraining-Evaluation data separation**: Why needed - Fundamental requirement for valid model evaluation; Quick check - Ensure no overlap between pretraining and test sets using multiple detection methods.

## Architecture Onboarding

**Component map**: Pretraining corpus -> Contamination injection -> Model training -> Evaluation -> Detection analysis

**Critical path**: Contamination injection → Model training → Performance evaluation → Detection validation

**Design tradeoffs**: Exact vs. approximate contamination detection (precision vs. recall tradeoff); computational cost of detection vs. contamination risk; model performance benefits vs. evaluation integrity.

**Failure signatures**: High n-gram overlap without semantic relevance; detection methods missing semantically similar but lexically different content; performance improvements that cannot be explained by in-domain training alone.

**First experiments**:
1. Test taxonomy framework on real-world pretraining datasets with known contamination issues
2. Evaluate detection methods across multiple contamination types simultaneously
3. Assess performance impact across different model scales and task domains

## Open Questions the Paper Calls Out
The authors acknowledge that their taxonomy may not be exhaustive, particularly regarding real-world contamination scenarios where evaluation data appears in mixed or partially altered forms. The study focuses on English-language tasks and single-modality models, leaving open questions about contamination in multilingual and multimodal contexts. Additionally, the synthetic contamination scenarios may not capture all forms of contamination present in actual pretraining datasets.

## Limitations
- Taxonomy may not capture all real-world contamination scenarios, particularly mixed or partially altered forms
- Controlled experiments use synthetic contamination that may not reflect actual pretraining conditions
- Study focuses on English-language tasks and single-modality models, limiting generalizability
- Evaluation primarily considers contamination in established benchmark test sets, not proprietary or emerging datasets

## Confidence

**Major Claims Confidence:**
- Taxonomy framework (High): The categorization of contamination types is well-reasoned and internally consistent
- Performance impact findings (Medium): Experimental results show significant boosts, but synthetic scenarios may not reflect real conditions
- Decontamination method limitations (Medium): Findings are compelling but based on limited detection methods tested

## Next Checks
1. Test the taxonomy framework against real-world pretraining datasets with known contamination to validate practical applicability and identify missing categories
2. Evaluate model performance with contamination scenarios that combine multiple contamination types simultaneously
3. Assess whether observed performance impacts hold across different model scales and task types, particularly in multilingual and multimodal settings