---
ver: rpa2
title: 'Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph'
arxiv_id: '2406.07113'
source_url: https://arxiv.org/abs/2406.07113
tags:
- scene
- object
- arxiv
- objects
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BBQ (Beyond Bare Queries), a modular approach
  for open-vocabulary 3D object grounding that leverages 3D scene graphs and large
  language models to interpret complex natural language queries. BBQ constructs an
  object-centric 3D map from RGB-D sequences using DINO embeddings and advanced raycasting,
  then generates a 3D scene graph with both metric and semantic spatial edges.
---

# Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph

## Quick Facts
- arXiv ID: 2406.07113
- Source URL: https://arxiv.org/abs/2406.07113
- Reference count: 40
- Primary result: State-of-the-art 3D object grounding on Sr3D+, Nr3D, and ScanRefer benchmarks using LLM-powered deductive reasoning with 3D scene graphs

## Executive Summary
BBQ introduces a novel modular approach for open-vocabulary 3D object grounding that leverages 3D scene graphs and large language models. The method constructs an object-centric 3D map from RGB-D sequences using DINO embeddings and advanced raycasting, then generates a 3D scene graph with both metric and semantic spatial edges. BBQ employs a deductive scene reasoning algorithm that first identifies target and anchor objects based on descriptions, then uses spatial relationships to ground objects in response to complex natural language queries.

## Method Summary
BBQ builds a 3D object-centric map from RGB-D sequences using DINO embeddings for object association and raycasting for 3D representation. Objects are described using a 2D vision-language model, and a deductive scene reasoning algorithm with an LLM identifies target and anchor objects before grounding them based on spatial relationships. The approach achieves state-of-the-art performance on multiple 3D object grounding benchmarks while maintaining real-time performance on robot on-board computers.

## Key Results
- State-of-the-art performance on Sr3D+, Nr3D, and ScanRefer benchmarks
- BBQ outperforms existing methods in 3D object grounding accuracy
- Real-time performance on robot on-board computers demonstrated

## Why This Works (Mechanism)

### Mechanism 1
DINO-powered associations enable robust object-to-instance mapping in 3D space. The method uses DINO embeddings to extract deep visual features from 2D RGB frames, then associates these features across frames using cosine similarity. When a new detection's DINO descriptor is sufficiently dissimilar (below threshold σvis) to all existing object descriptors, it's treated as a new object; otherwise, it's merged with the most similar one.

### Mechanism 2
Multi-view clustering reduces raycasting computation while preserving object representation quality. Instead of raycasting from all viewpoints, the method clusters 3D camera coordinates for each object and selects L cluster centroids as representative viewpoints. The best view is chosen based on the largest projected mask area.

### Mechanism 3
Deductive scene reasoning with LLM reduces search space and improves grounding accuracy. The method first uses an LLM to identify target and anchor objects based solely on descriptions, then uses spatial relationships between these selected objects for final grounding. This two-stage approach limits scene description length and focuses LLM attention on relevant objects.

## Foundational Learning

- **3D object-centric mapping from RGB-D sequences**
  - Why needed: BBQ needs to build a 3D representation of the environment from camera observations to enable object grounding in 3D space
  - Quick check: What is the difference between a point cloud and an object-centric 3D map?

- **Scene graph representation with spatial edges**
  - Why needed: BBQ uses a scene graph to encode both objects and their spatial relationships, which is essential for grounding objects based on relational queries
  - Quick check: What are the two types of spatial edges used in BBQ's scene graph?

- **Large language model integration for scene understanding**
  - Why needed: BBQ uses an LLM to interpret natural language queries and reason about object relationships in the scene graph
  - Quick check: Why does BBQ use a two-stage deductive reasoning approach with the LLM?

## Architecture Onboarding

- **Component map**: DINO feature extractor (RGB frames) -> MobileSAMv2 detector (2D proposals) -> Object association module (3D mapping) -> Multi-view clustering (view selection) -> Raycasting module (3D→2D projection) -> LLaVA-1.6 captioner (object description) -> EVA2 encoder (visual encoding) -> GPT-4o LLM (deductive reasoning) -> Scene graph builder (spatial edges)

- **Critical path**: 1. Process RGB-D frames → extract DINO features and 2D proposals; 2. Associate 2D detections with 3D objects across frames; 3. Cluster viewpoints and select best views for each object; 4. Caption objects and build scene graph with spatial edges; 5. Use LLM to identify target/anchor objects; 6. Ground target objects using spatial relationships

- **Design tradeoffs**: Accuracy vs. speed: Using L cluster centroids instead of all viewpoints significantly reduces computation but may miss optimal object views; LLM calls vs. feature-based methods: LLM-based reasoning is more flexible but computationally expensive compared to CLIP-based approaches; Scene graph completeness vs. efficiency: Building full scene graphs is more comprehensive but the deductive approach focuses on relevant objects for efficiency

- **Failure signatures**: Poor object associations → fragmented or incorrectly merged objects; Inaccurate multi-view clustering → suboptimal object descriptions; LLM reasoning errors → incorrect target/anchor identification; Edge generation failures → incorrect spatial relationships

- **First 3 experiments**: 1. Test object association on a simple sequence with known objects to verify DINO-based merging works correctly; 2. Evaluate multi-view clustering by comparing object descriptions from selected views vs. all available views; 3. Test deductive reasoning pipeline on a scene with clear target/anchor pairs to verify LLM correctly identifies objects from descriptions alone

## Open Questions the Paper Calls Out

### Open Question 1
How does BBQ's performance scale with the number of objects in a scene, and what is the upper limit of objects it can effectively handle before accuracy degrades significantly? The paper mentions that real scenes may contain many objects (e.g., over 100) potentially forming a complete graph and resulting in a long scene description (over 32k symbols), which can degrade LLM performance.

### Open Question 2
How robust is BBQ's DINO-based object mapping to dynamic environments with moving objects, and what modifications would be needed to handle such scenarios? The paper states that BBQ works under the assumption of a static environment equivalent to a standard indoor room, and mentions that for dynamic environments, existing tracking methods should be applied.

### Open Question 3
What is the impact of different large language models on BBQ's performance, and how does the choice of LLM affect accuracy across different types of queries? The paper experiments with both LLAMA3-8B and GPT4-o, showing different performance characteristics, and mentions that the choice of LLM affects quality for view-dependent versus view-independent queries.

## Limitations
- Reliance on DINO embeddings for object association may struggle with significant viewpoint changes or appearance variations
- LLM-based deductive reasoning assumes sufficient contextual information can be extracted from object descriptions alone, which may not hold for complex scenes or ambiguous queries
- Performance constrained by quality and coverage of 3D object mapping, which depends on sufficient camera viewpoints and reliable depth data

## Confidence

- **High confidence**: The core modular architecture and overall approach to 3D object grounding
- **Medium confidence**: The specific implementation details of DINO-based object association and multi-view clustering
- **Medium confidence**: The effectiveness of the deductive scene reasoning algorithm with LLM

## Next Checks

1. **Object Association Robustness**: Test the DINO-based object association mechanism across varying viewpoint changes and appearance conditions to identify breaking points and optimal threshold values

2. **LLM Reasoning Performance**: Evaluate the deductive reasoning pipeline with progressively complex queries and ambiguous descriptions to determine its limitations and failure modes

3. **Computational Efficiency**: Benchmark the real-time performance on different robot hardware platforms to validate the claimed efficiency and identify bottlenecks