---
ver: rpa2
title: 'Use Random Selection for Now: Investigation of Few-Shot Selection Strategies
  in LLM-based Text Augmentation for Classification'
arxiv_id: '2410.10756'
source_url: https://arxiv.org/abs/2410.10756
tags:
- samples
- sample
- strategies
- selection
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates sample selection strategies
  for few-shot LLM-based text augmentation in classification tasks. It compares 8
  sample selection strategies against 2 baselines across 7 datasets, 3 LLMs, and both
  paraphrasing and generation augmentation methods.
---

# Use Random Selection for Now: Investigation of Few-Shot Selection Strategies in LLM-based Text Augmentation for Classification

## Quick Facts
- arXiv ID: 2410.10756
- Source URL: https://arxiv.org/abs/2410.10756
- Reference count: 19
- Primary result: No sample selection strategy consistently outperforms random selection for in-distribution performance in LLM-based text augmentation

## Executive Summary
This study systematically investigates sample selection strategies for few-shot LLM-based text augmentation in classification tasks. The research compares 8 sample selection strategies against 2 baselines across 7 datasets, 3 LLMs, and both paraphrasing and generation augmentation methods. The key finding is that random selection remains the practical default choice due to its simplicity and competitive performance, while synthetic samples dissimilarity shows promise for out-of-distribution performance, though improvements are marginal.

## Method Summary
The study compares 8 sample selection strategies (including cosine similarity, Kullback-Leibler divergence, and synthetic samples dissimilarity) against 2 baselines (random and zero-shot) for LLM-based text augmentation. Using 20 seed samples per class and generating 5 augmentations per seed, the research evaluates performance across 7 datasets using Llama-3.1, Mistral, and Gemma-2 LLMs. Downstream classifier performance is measured using F1-macro scores for both in-distribution and out-of-distribution evaluation, with each experiment repeated 3 times using different random seeds.

## Key Results
- No selection strategy consistently outperforms random selection for in-distribution performance across all datasets
- Synthetic samples dissimilarity strategy shows the most promise for out-of-distribution performance, though improvements are marginal
- Cosine similarity performs best for in-distribution tasks (16/63 cases) but shows high variability across datasets
- Random selection and zero-shot baselines remain competitive with informed strategies despite their simplicity

## Why This Works (Mechanism)

### Mechanism 1
Random selection performs competitively because it avoids introducing bias that could limit model generalization. By sampling examples uniformly from the dataset, it prevents the model from overfitting to a specific subset of the data distribution. This works when no systematic sampling bias exists in the original dataset that would be amplified by informed selection.

### Mechanism 2
Synthetic samples dissimilarity works for out-of-distribution performance by selecting examples that are less likely to bias the LLM toward the training distribution. By generating synthetic samples and selecting examples based on dissimilarity to these synthetics, the method implicitly selects examples that are more diverse and potentially more generalizable. This assumes the LLM's generated synthetic samples represent the training distribution, so dissimilar real samples are more likely to be out-of-distribution.

### Mechanism 3
Cosine similarity performs best for in-distribution because it selects examples most similar to the target sample, reinforcing the training distribution patterns. Selecting the most similar examples ensures the LLM focuses on examples that are representative of the target sample's distribution, potentially improving coherence and relevance. This works when the training distribution is the correct one to reinforce, and similar examples will produce better augmentations within that distribution.

## Foundational Learning

- Concept: Few-shot learning in context of LLM augmentation
  - Why needed here: The paper compares different sample selection strategies specifically within few-shot LLM augmentation scenarios
  - Quick check question: What distinguishes few-shot from zero-shot LLM augmentation in terms of prompt construction?

- Concept: In-distribution vs out-of-distribution evaluation
  - Why needed here: The study evaluates both ID and OOD performance to understand how selection strategies affect generalization
  - Quick check question: How did the authors define out-of-distribution data in their experiments?

- Concept: F1-macro as evaluation metric
  - Why needed here: The study uses F1-macro to compare classifier performance across different strategies and datasets
  - Quick check question: Why might F1-macro be preferred over accuracy for these multi-class classification tasks?

## Architecture Onboarding

- Component map: Sample selection strategies (8 strategies + 2 baselines) -> LLMs for augmentation (Llama-3.1, Mistral, Gemma-2) -> Datasets (7 datasets across 3 domains) -> Downstream classifier (RoBERTa-base) -> Evaluation framework (ID and OOD splits)

- Critical path: 1. Select seed samples from each class -> 2. Apply sample selection strategy -> 3. Generate/augment samples using LLM -> 4. Fine-tune RoBERTa on augmented dataset -> 5. Evaluate on ID and OOD test splits

- Design tradeoffs: Random vs informed selection (simplicity and speed vs potential performance gains), ID vs OOD focus (strategies that work for one may not work for the other), Computational cost (informed strategies require additional computation for sample selection)

- Failure signatures: No consistent improvement over random selection across all datasets, Marginal performance gains when improvements do occur, Strategy performance varies significantly across datasets and conditions

- First 3 experiments: 1. Run random selection baseline on all datasets to establish performance floor, 2. Test cosine similarity on in-distribution splits only to verify its claimed effectiveness, 3. Test synthetic samples dissimilarity on out-of-distribution splits to verify its claimed effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
Why does synthetic samples dissimilarity perform better for out-of-distribution data but not consistently for in-distribution data? The paper shows synthetic samples dissimilarity strategy was best for out-of-distribution performance (22/63 cases) but performed poorly for in-distribution (6/63 cases). This pattern is identified but the underlying mechanism causing this differential performance is not explained. Experiments comparing synthetic sample diversity metrics with test set distribution divergence, or ablation studies isolating the effect of synthetic sample generation quality would resolve this.

### Open Question 2
How do different feature representation models affect the performance of similarity/dissimilarity-based sample selection strategies? The paper states "we used only one feature representation model for the sample selection strategies that required similarity or dissimilarity of samples." The authors acknowledge using only one feature representation model but don't explore how alternative models might impact results. Systematic comparison of sample selection strategy performance using different embedding models (e.g., sentence transformers, contrastive models) across the same experimental setup would resolve this.

### Open Question 3
Would larger datasets with more samples per class change the relative performance of informed sample selection strategies versus random selection? The paper used 20 samples per label and notes "obtaining larger annotated datasets (e.g., hundreds of samples per class) is not feasible for many domains in practice." The study focused on few-shot scenarios with limited data, leaving unclear how results scale with more abundant data. Experiments replicating the study with datasets containing 100+ samples per class would resolve this.

## Limitations
- Limited scope of sample selection strategies - only examines 8 informed strategies plus 2 baselines
- Fixed augmentation parameters (20 seed samples per label, 5 augmentations per seed) may not be optimal for all datasets
- Computational overhead of informed strategies not fully quantified against performance gains
- No analysis of how sample selection interacts with different LLM capabilities or prompt engineering approaches

## Confidence

- **High Confidence**: Random selection remains competitive with informed strategies for in-distribution performance, supported by extensive empirical testing across 7 datasets
- **Medium Confidence**: Synthetic samples dissimilarity shows promise for out-of-distribution performance, though improvements are marginal and dataset-dependent
- **Medium Confidence**: Cosine similarity is the best single strategy for in-distribution tasks, but with significant variability across datasets

## Next Checks

1. Test additional sample selection strategies beyond the 8 examined, particularly those that combine multiple selection criteria or use learned selection methods
2. Conduct ablation studies to determine optimal augmentation parameters (number of seed samples and augmentations) for different selection strategies
3. Investigate the relationship between LLM generation quality and sample selection effectiveness by testing with LLMs of varying capabilities