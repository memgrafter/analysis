---
ver: rpa2
title: Accelerating Production LLMs with Combined Token/Embedding Speculators
arxiv_id: '2404.19124'
source_url: https://arxiv.org/abs/2404.19124
tags:
- tokens
- base
- speculator
- token
- speculative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel speculative decoding architecture
  that improves upon Medusa by conditioning draft predictions on both base model context
  vectors and previously sampled tokens. The authors train these combined token/embedding
  speculators using a two-stage training process and demonstrate 2-3x acceleration
  of highly optimized production LLMs (Llama2-7B, Llama2-13B, Codellama-13B, and Granite-20B).
---

# Accelerating Production LLMs with Combined Token/Embedding Speculators

## Quick Facts
- **arXiv ID**: 2404.19124
- **Source URL**: https://arxiv.org/abs/2404.19124
- **Reference count**: 10
- **Primary result**: Novel speculative decoding architecture achieves 2-3x acceleration of production LLMs through combined token/embedding conditioning

## Executive Summary
This paper introduces a novel speculative decoding architecture that improves upon Medusa by conditioning draft predictions on both base model context vectors and previously sampled tokens. The authors train these combined token/embedding speculators using a two-stage training process and demonstrate 2-3x acceleration of highly optimized production LLMs (Llama2-7B, Llama2-13B, Codellama-13B, and Granite-20B). The method achieves up to 181.5 tokens/second throughput for Codellama-13B at fp16 precision, representing a 3x improvement over non-speculative decoding. However, the authors note that wall-clock speedup diminishes as baseline computational load and optimization levels increase, requiring careful hyperparameter tuning in production settings.

## Method Summary
The paper proposes a speculative decoding approach where draft predictions are conditioned on both base model context vectors and previously sampled tokens. The method employs a two-stage training process: stage 1 trains on ground truth text using base model embedding vectors, while stage 2 fine-tunes on base model-generated outputs with shorter sequences. The speculator uses a multi-stage MLP architecture with multiple heads and implements multi-candidate decoding with tree attention for parallel evaluation. The approach was evaluated across four production LLMs (Llama2-7B, Llama2-13B, Codellama-13B, Granite-20B) using 32 NVIDIA A100 GPUs with fp16 precision.

## Key Results
- Achieves up to 3x improvement in tokens per step (τ) compared to Medusa baseline
- Demonstrates 2-3x wall-clock speedup for highly optimized production LLMs
- Shows optimal performance with 3-7 heads depending on model size and domain
- Reports up to 181.5 tokens/second throughput for Codellama-13B at fp16 precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning draft predictions on sampled tokens improves n-gram quality by ensuring predictions reflect actual base model behavior rather than expected behavior across all possible token choices.
- Mechanism: By using both the base model's embedding vector and the previously sampled token as inputs, the speculator can generate predictions that are more aligned with what the base model would actually output given the specific token sequence, rather than the average behavior across all possible continuations.
- Core assumption: The sampled token carries critical semantic information that the embedding vector alone cannot fully capture, and this information is necessary for accurate multi-token prediction.
- Evidence anchors:
  - [abstract] "By conditioning draft predictions on both context vectors and sampled tokens, we can train our speculators to efficiently predict high-quality n-grams"
  - [section] "This means that the predictions of Medusa for tokens n + 1, n + 2, n + 3 are not actually conditioned on token n at all, but rather on the expectation across all possible samples of token n, simultaneously"
  - [corpus] Weak evidence - most related papers focus on architectural changes or training methods rather than the specific token conditioning mechanism described here
- Break condition: If the semantic information in sampled tokens becomes redundant with the embedding vector information, or if the base model's behavior becomes highly deterministic, the benefit of token conditioning may diminish.

### Mechanism 2
- Claim: The two-stage training process improves speculator alignment by first learning general prediction patterns and then fine-tuning to match base model behavior.
- Mechanism: Stage 1 trains the speculator on ground truth text using the base model's embedding vectors, allowing it to learn efficient multi-token prediction patterns. Stage 2 then fine-tunes on the base model's actual outputs, aligning the speculator's predictions to the specific behavior of the target model.
- Core assumption: The speculator can learn useful prediction patterns from ground truth data that transfer to predicting the base model's behavior, and the fine-tuning stage can effectively adapt these patterns without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "We introduce an efficient two-stage training scheme, aligning our speculators first to base model input behavior, then to output behavior"
  - [section] "In stage 2, we switch to large batches with short sequence lengths (256 tokens), generated from the base model itself. By only generating a short sequence of tokens, we can continue to train reasonably efficiently in parallel, while fine-tuning the speculator to explicitly match base model outputs"
  - [corpus] Limited evidence - while two-stage training is mentioned in some related work, the specific approach of using ground truth in stage 1 and base model outputs in stage 2 appears to be unique to this paper
- Break condition: If the base model's behavior is too different from the training data distribution, or if the speculator overfits to short sequences in stage 2, the alignment may be poor.

### Mechanism 3
- Claim: Multi-candidate decoding with tree attention allows for parallel evaluation of multiple predicted sequences, improving acceptance rates while maintaining computational efficiency.
- Mechanism: The speculator generates a tree of candidate sequences, and the system evaluates all candidates simultaneously using tree attention. This allows for efficient parallel verification while maintaining the ability to select the best-matching sequence.
- Core assumption: The overhead of generating and evaluating multiple candidates is offset by the increased probability of accepting multiple tokens per step, and that tree attention can efficiently handle the parallel evaluation.
- Evidence anchors:
  - [abstract] "This allows us to effectively predict multiple tokens per inference forward pass"
  - [section] "We allow the user to select the top-k most likely predictions from that tree to use in practice. At inference time, we implement multi-candidate decoding [9] and Tree Attention [3, 8] to evaluate the chosen k candidates simultaneously in parallel"
  - [corpus] Strong evidence - tree attention and multi-candidate decoding are well-established techniques in speculative decoding literature, with multiple related papers citing their effectiveness
- Break condition: If the optimal k value becomes very small due to computational constraints, the benefit of tree attention may be minimal, and a simpler decoding strategy might be more efficient.

## Foundational Learning

- Concept: Speculative decoding and the draft-then-verify paradigm
  - Why needed here: Understanding the fundamental principle of using a smaller model to predict multiple tokens that are then verified by the larger base model is crucial for grasping the motivation behind this work
  - Quick check question: Why is speculative decoding particularly useful for large language models, and what are the key trade-offs involved?

- Concept: Multi-head MLP architectures and their role in sequence prediction
- Concept: Token embeddings and their role in capturing semantic context
  - Why needed here: The speculator's ability to condition on both embedding vectors and sampled tokens relies on understanding how token embeddings represent semantic information
  - Quick check question: How do token embeddings differ from the raw token indices, and why are they useful for sequence prediction tasks?

- Concept: Parallel computation and GPU optimization techniques
  - Why needed here: The paper discusses various optimizations for achieving high throughput, including tree attention and parallel candidate evaluation
  - Quick check question: What are the key considerations when optimizing GPU computations for parallel token prediction, and how do techniques like tree attention help address these challenges?

## Architecture Onboarding

- Component map:
  Base model -> Embedding vectors -> Speculator (multi-stage MLP with 3-7 heads) -> Tree of candidate sequences -> Tree attention evaluation -> Best-matching sequence selection -> Base model verification

- Critical path:
  1. Input prompt is processed by the base model to generate embedding vectors
  2. Speculator receives embedding vectors and sampled tokens from previous stages
  3. Speculator generates a tree of candidate sequences
  4. Tree attention evaluates all candidates in parallel
  5. Best-matching sequence is selected and accepted/rejected by base model
  6. Process repeats for subsequent tokens

- Design tradeoffs:
  - Number of speculator heads vs. computational overhead
  - Batch size and prompt length vs. speculative decoding efficiency
  - Number of parallel candidates (k) vs. acceptance rate and latency
  - Weight tying vs. model flexibility and memory usage

- Failure signatures:
  - Low acceptance rate despite high speculator confidence
  - Increased latency with speculative decoding enabled
  - Memory overflow errors during training or inference
  - Inconsistent performance across different batch sizes or prompt lengths

- First 3 experiments:
  1. Baseline comparison: Measure latency and throughput of base model with and without speculative decoding enabled (k=0 vs k=1)
  2. Ablation study: Compare speculator performance with and without token conditioning (using only embedding vectors vs. embedding vectors + sampled tokens)
  3. Scaling analysis: Measure the impact of batch size and prompt length on speculative decoding efficiency, and identify optimal hyperparameters for different scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of heads for speculative decoding across different model sizes and domains (natural language vs code)?
- Basis in paper: [explicit] The authors note that natural language speculators are constrained by their limited number of heads (3) and suggest that increasing to 4, 5, or more heads could improve performance, while also noting that code models might need more heads but that 7 heads for Codellama-13B showed diminishing returns compared to 5 heads for Granite-20B
- Why unresolved: The paper only tests a limited set of head configurations (3 for Llama models, 7 for Codellama, 5 for Granite) and doesn't systematically explore the relationship between model size, domain, and optimal head count
- What evidence would resolve it: A systematic ablation study testing different head counts (3, 4, 5, 6, 7) across multiple model sizes (7B, 13B, 20B) and domains (natural language, code) with rigorous wall-clock performance measurements

### Open Question 2
- Question: How does the performance of combined token/embedding speculators compare to alternative speculative decoding approaches (EAGLE, Hydra, Recurrent Drafter) in production settings?
- Basis in paper: [explicit] The authors acknowledge these concurrent works but note their production baseline is significantly faster (70-170% higher throughput), making direct comparison difficult and suggesting their deployment landscape is "more interesting"
- Why unresolved: The paper doesn't directly compare its method against these alternatives in production settings, instead focusing on relative improvements over their own baseline
- What evidence would resolve it: Head-to-head benchmarking of all speculative decoding methods on the same production hardware using identical base models and workloads

### Open Question 3
- Question: What is the relationship between speculative decoding speedup and baseline computational load, and can this relationship be modeled to predict optimal hyperparameter settings?
- Basis in paper: [explicit] The authors observe that "the wall-clock speedup from speculative decoding scales inversely to both the computational workload and the degree of preexisting optimization of the base model" and note that optimal k varies with batch size
- Why unresolved: While the inverse relationship is observed, the paper doesn't provide a predictive model or explain the underlying mechanisms driving this relationship
- What evidence would resolve it: Empirical data across a wide range of computational loads showing speedup as a function of baseline throughput, plus a theoretical or empirical model predicting optimal k and speedup for given workloads

### Open Question 4
- Question: Can weight tying between the speculator and base model improve training efficiency or inference performance without increasing parameter count?
- Basis in paper: [explicit] The authors note they declined to tie any weights because it provides no wall-clock speedup, but suggest it might be useful for reducing GPU memory overhead and potentially improving convergence
- Why unresolved: The paper only considers weight tying from a wall-clock perspective and doesn't explore its effects on memory efficiency or training dynamics
- What evidence would resolve it: Comparative experiments measuring training time, memory usage, and inference latency with and without weight tying across different model sizes and head counts

### Open Question 5
- Question: What is the impact of prompt length on speculator accuracy and how can speculator training be adapted to handle longer contexts more effectively?
- Basis in paper: [explicit] The authors observe that speculator accuracy (τ) actually increases slightly with prompt length for code models, while decreasing slightly for natural language models, but don't explore the underlying reasons or potential adaptations
- Why unresolved: The paper only tests two prompt lengths (64 and 512/2048) and doesn't investigate the mechanisms behind the different behaviors or potential training adaptations
- What evidence would resolve it: Systematic testing across a wider range of prompt lengths (16 to 4096) with analysis of speculator behavior, plus experiments with training adaptations like longer context windows or attention mechanisms

## Limitations

- Performance gains are highly dependent on baseline optimization levels and computational load
- Requires careful hyperparameter tuning across different models and deployment scenarios
- Limited generalization evidence to model architectures beyond the four tested models
- Additional training complexity and costs compared to single-stage approaches

## Confidence

**High Confidence**: The core mechanism of conditioning draft predictions on both embedding vectors and sampled tokens is well-supported by the paper's ablation studies and comparative results against Medusa. The architectural improvements and their impact on token quality are clearly demonstrated through controlled experiments.

**Medium Confidence**: The reported throughput improvements (up to 181.5 tokens/second for Codellama-13B) are based on specific hardware configurations (32 A100 GPUs) and optimization settings. While the methodology is sound, the absolute performance numbers may vary significantly across different deployment environments.

**Low Confidence**: The generalization of these results to other model architectures (beyond the four tested models) and to different domains (beyond code and general language) remains unproven. The paper does not address potential degradation in output quality or the impact on downstream task performance.

## Next Checks

1. **Cross-Hardware Validation**: Test the speculative decoding approach on different GPU architectures (e.g., H100, V100) and with varying optimization levels to quantify the portability of the reported speedup improvements and identify the hardware dependencies.

2. **Quality Preservation Analysis**: Conduct a comprehensive evaluation of output quality using standard benchmarks (e.g., perplexity, task-specific accuracy) to verify that the 2-3x acceleration does not come at the cost of degraded model performance or coherence.

3. **Hyperparameter Sensitivity Study**: Perform a systematic grid search across batch sizes, prompt lengths, and k values to develop guidelines for automatic hyperparameter selection in production environments, addressing the paper's acknowledgment that manual tuning is currently required.