---
ver: rpa2
title: 'IrokoBench: A New Benchmark for African Languages in the Age of Large Language
  Models'
arxiv_id: '2406.03368'
source_url: https://arxiv.org/abs/2406.03368
tags:
- languages
- language
- african
- llama
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces IrokoBench, a new benchmark dataset for
  17 African languages across three complex tasks: natural language inference, mathematical
  reasoning, and multi-choice knowledge-based question answering. The dataset was
  created by human-translating English benchmark data into African languages, enabling
  comprehensive evaluation of large language models (LLMs) on these underrepresented
  languages.'
---

# IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models

## Quick Facts
- **arXiv ID:** 2406.03368
- **Source URL:** https://arxiv.org/abs/2406.03368
- **Reference count:** 40
- **Primary result:** Introduces IrokoBench, a human-translated benchmark for 17 African languages across three tasks, revealing significant performance gaps between high-resource and African languages for large language models

## Executive Summary
This paper introduces IrokoBench, a comprehensive benchmark dataset for 17 African languages across three complex tasks: natural language inference, mathematical reasoning, and multi-choice knowledge-based question answering. The dataset was created by human-translating English benchmark data into African languages, enabling comprehensive evaluation of large language models (LLMs) on these underrepresented languages. The authors evaluate zero-shot, few-shot, and translate-test performance of 10 open and six proprietary LLMs. Results show a significant performance gap between high-resource languages (English/French) and African languages, with proprietary models generally outperforming open models. The best proprietary model (GPT-4o) achieved an average of 59.0% across tasks, while the highest-performing open model (Gemma 2 27B) reached only 63% of that performance. Machine translation of test sets to English improved results for English-centric models but is not a desirable solution for native language use. The findings highlight the need for more efforts to develop and adapt LLMs for African languages.

## Method Summary
The authors created IrokoBench by human-translating English benchmarks (XNLI, MGSM, MMLU) into 17 African languages and two European languages. They recruited professional translators for each language and implemented quality control measures including COMET quality estimation. The benchmark was evaluated using the EleutherAI LM Evaluation Harness with 10 open-weight models (including Gemma 2 27B, LLaMa 3.1 70B) and 6 proprietary models (including GPT-4o, Claude Opus). Evaluations were conducted in zero-shot, few-shot, and translate-test settings, with prompts tested in both the native African language and translated to English.

## Key Results
- Significant performance gap exists between high-resource languages (English/French) and African languages across all evaluated models and tasks
- Proprietary models (GPT-4o, Claude Opus, Gemini-1.5-Pro) outperformed open models by a substantial margin, with GPT-4o achieving 59.0% average accuracy
- Machine translation of test sets to English improved performance for English-centric models but reduces native language utility
- AfroXLMR-76L, pre-trained on African languages, showed better cross-lingual transfer performance than prompting LLMs directly in African languages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Human translation of benchmarks improves evaluation accuracy for low-resource African languages compared to machine translation.
- **Mechanism:** Human translators provide culturally and linguistically accurate translations that preserve task semantics, while machine translation introduces noise and biases, especially for under-resourced languages.
- **Core assumption:** Professional translators understand both source and target languages deeply enough to preserve task semantics without introducing bias.
- **Evidence anchors:** [abstract] "human-translated benchmark dataset for 17 typologically-diverse low-resource African languages"; [section] "We recruited language coordinators for each of the 16 African languages and French, and asked them to recruit professional translators to translate the sentences."
- **Break Condition:** If translators lack deep cultural context or if source tasks contain culturally biased content that cannot be translated appropriately.

### Mechanism 2
- **Claim:** Machine translation of test sets to English improves performance for English-centric LLMs on African language tasks.
- **Mechanism:** English-centric models have stronger reasoning and knowledge capabilities in English; translating African language prompts to English allows these models to leverage their stronger English capabilities.
- **Core assumption:** LLMs' reasoning abilities are stronger in English than in African languages due to training data distribution.
- **Evidence anchors:** [abstract] "machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric"; [section] "Machine translating the test set to English before evaluation helped to close the gap for larger models that are English-centric, such as Gemma 2 27B and LLaMa 3.1 70B."
- **Break Condition:** If translation quality is poor or if models develop better multilingual reasoning capabilities.

### Mechanism 3
- **Claim:** Cross-lingual transfer using African-centric models outperforms prompting LLMs directly in African languages.
- **Mechanism:** Models pre-trained on African languages (AfroXLMR-76L) capture linguistic patterns specific to African languages, enabling better zero-shot transfer than general multilingual models.
- **Core assumption:** Pre-training on African languages provides linguistic representations that generalize better to unseen African languages than general multilingual pre-training.
- **Evidence anchors:** [section] "AfroXLMR-76L has been pre-trained on all languages in IROKO BENCH, which explains the impressive performance."; [section] "AfroXLMR-76L gave better results than prompting LLMs on average."
- **Break Condition:** If transfer learning doesn't generalize well to unseen African language families or if LLMs improve multilingual reasoning.

## Foundational Learning

- **Concept:** Cross-lingual transfer learning
  - **Why needed here:** To leverage labeled English data for evaluating African languages without requiring extensive labeled African language data
  - **Quick check question:** What is the key difference between zero-shot and few-shot cross-lingual transfer?

- **Concept:** Multilingual model architecture
  - **Why needed here:** Understanding how models like XLM-R and AfroXLMR handle multiple languages differently
  - **Quick check question:** How does continual pre-training on African languages improve performance compared to general multilingual pre-training?

- **Concept:** Prompt sensitivity and template engineering
  - **Why needed here:** Different models respond better to different prompt formats, affecting evaluation consistency
  - **Quick check question:** Why might simpler prompts work better for some models while detailed task descriptions work better for others?

## Architecture Onboarding

- **Component map:** IrokoBench dataset -> Translation pipeline -> Evaluation harness -> Model zoo -> Verbalizer system -> COMET validation
- **Critical path:** 1. Translate English benchmarks to 17 African languages 2. Quality control and COMET validation 3. Set up evaluation harness with appropriate verbalizers 4. Run zero-shot, few-shot, and translate-test evaluations 5. Analyze performance gaps and prompt sensitivity
- **Design tradeoffs:** Human vs machine translation: accuracy vs cost/time; In-language vs translate-test evaluation: native user experience vs model performance; Open vs proprietary models: accessibility vs performance
- **Failure signatures:** Poor COMET scores indicate translation quality issues; Large performance gaps suggest model limitations for African languages; Prompt sensitivity variations indicate need for template optimization
- **First 3 experiments:** 1. Compare human vs machine translation quality using COMET scores 2. Evaluate a single model across all three tasks to establish baseline performance 3. Test translate-test approach on one African language to verify performance improvement

## Open Questions the Paper Calls Out

- **Open Question 1:** What is the optimal balance between human-translated and automatically generated data for training African language models?
  - **Basis in paper:** [explicit] The paper discusses the trade-offs between human-translated benchmarks (IrokoBench) versus machine-translated data, noting that human translation is expensive but potentially more accurate.
  - **Why unresolved:** The paper only uses human-translated data for evaluation but doesn't explore what mix of human vs. machine translation would be optimal for training data.
  - **What evidence would resolve it:** Systematic experiments comparing model performance using different ratios of human-translated to machine-translated training data, with cost-benefit analysis.

- **Open Question 2:** How do different African language families respond to specific pretraining strategies?
  - **Basis in paper:** [explicit] The paper evaluates 17 African languages from three language families (Afro-Asiatic, Niger-Congo, Mande) but doesn't analyze family-specific performance patterns or training strategies.
  - **Why unresolved:** While performance gaps are noted, the paper doesn't investigate whether certain pretraining approaches work better for specific language families.
  - **What evidence would resolve it:** Comparative analysis of model performance across language families using different pretraining approaches (continued pretraining, adapter-based methods, etc.).

- **Open Question 3:** What is the minimum amount of monolingual data needed for African languages to achieve reasonable LLM performance?
  - **Basis in paper:** [inferred] The paper notes that languages with less than 50 million characters of data perform worst, but doesn't establish clear thresholds for minimum viable data.
  - **Why unresolved:** The relationship between data quantity and performance is not quantified, leaving uncertainty about data collection priorities.
  - **What evidence would resolve it:** Controlled experiments scaling data quantity for different languages to identify performance plateaus and minimum viable data thresholds.

## Limitations

- Translation quality varies significantly across low-resource African languages, with some languages (Lingala, Twi, Wolof) having limited available translation resources
- Proprietary models' internal architectures and training data composition remain undisclosed, making it difficult to determine whether performance gaps stem from data scarcity or architectural limitations
- The study relies on human translation which may introduce consistency issues across different translators and languages

## Confidence

- **High confidence:** The performance gap findings between English/French and African languages are well-supported by comprehensive evaluation across multiple models and tasks
- **Medium confidence:** The claim about machine translation improving English-centric model performance is supported but could benefit from more granular analysis
- **Low confidence:** The assertion that AfroXLMR-76L's pre-training on African languages explains its superior performance lacks comparative analysis with other African-centric models

## Next Checks

1. **Translation Quality Analysis:** Conduct detailed COMET score analysis per language and correlate translation quality with model performance to determine if quality variations explain performance gaps

2. **Prompt Template Sensitivity:** Systematically test multiple prompt templates for each task and model to quantify the impact of prompt engineering on performance consistency across African languages

3. **Cross-Lingual Transfer Generalization:** Evaluate AfroXLMR-76L on African languages not included in its pre-training corpus to test whether its performance generalizes beyond its training languages