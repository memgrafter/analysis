---
ver: rpa2
title: 'FameBias: Embedding Manipulation Bias Attack in Text-to-Image Models'
arxiv_id: '2412.18302'
source_url: https://arxiv.org/abs/2412.18302
tags:
- trigger
- images
- bias
- target
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FameBias, a novel text-to-image (T2I) biasing
  attack that manipulates input embeddings to generate images featuring specific public
  figures without requiring model retraining. Unlike prior approaches that rely on
  fine-tuning, FameBias operates by modifying CLIP text embeddings through weighted
  combinations of trigger words and target figure embeddings.
---

# FameBias: Embedding Manipulation Bias Attack in Text-to-Image Models

## Quick Facts
- arXiv ID: 2412.18302
- Source URL: https://arxiv.org/abs/2412.18302
- Reference count: 21
- Key outcome: FameBias achieves 53% bias success rate and 65% trigger fidelity rate on Stable Diffusion V2 without model retraining

## Executive Summary
FameBias is a novel text-to-image biasing attack that manipulates input embeddings to generate images featuring specific public figures without requiring model retraining. The attack operates by modifying CLIP text embeddings through weighted combinations of trigger words and target figure embeddings. Unlike prior approaches that rely on fine-tuning, FameBias works by strategically repositioning embeddings in the semantic space, causing diffusion models to generate images that blend trigger concepts with target figure characteristics.

The attack was evaluated across 8 public figures and 10 trigger nouns, demonstrating effectiveness particularly against male targets and those with high prevalence in training data. While the attack successfully biases image generation, it faces limitations including variable effectiveness across different targets and the requirement for knowledge of target identities and appropriate trigger words. The authors also evaluated Unified Concept Editing as a potential defense, which successfully prevented the attack but at significant cost to model utility.

## Method Summary
FameBias manipulates text embeddings by creating weighted linear combinations of trigger word embeddings and target figure embeddings. The attack uses the formula er = α · ewp + β · ewt, where er is the modified embedding, ewp is the original trigger embedding, ewt is the target figure embedding, and α and β are weighting parameters. This repositioning in the embedding space causes the diffusion model to generate images that blend the trigger concept with the target figure's characteristics, effectively biasing the output toward specific public figures without requiring any model retraining.

## Key Results
- Achieved 53% bias success rate (BSR) across 8 public figures and 10 trigger nouns
- Maintained 65% trigger fidelity rate (TFR) in modified prompts
- Showed particular effectiveness against male targets and figures with high training data prevalence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The attack works by manipulating text embeddings through weighted linear combinations to pull semantic vectors closer to target figure representations
- Mechanism: FameBias modifies the CLIP text embedding of a trigger word by creating a weighted sum of the original trigger embedding and the target figure's embedding. This repositioning in the embedding space causes the diffusion model to generate images that blend the trigger concept with the target figure's characteristics
- Core assumption: Linear algebraic operations on text embeddings can approximate semantic shifts that diffusion models will interpret as meaningful concept transformations
- Evidence anchors:
  - [abstract] "manipulates the embeddings of input prompts to generate images featuring specific public figures"
  - [section] "er = α · ewp + β · ewt" showing the linear combination formula
  - [section] "The core premise of our attack methodology hinges on the manipulation of text embeddings before the generative models"
- Break condition: If the diffusion model's conditioning mechanism is robust to embedding perturbations or if the model uses different embedding spaces that don't share the same geometric relationships

### Mechanism 2
- Claim: The attack exploits the learned distributional semantics of text embeddings where words appearing in similar contexts cluster together
- Mechanism: By modifying the trigger embedding to incorporate target figure characteristics, the attack leverages the model's learned associations between semantic concepts. The weighted combination effectively "nudges" the generative model toward producing visuals that represent a hybrid identity
- Core assumption: Text embeddings capture compositional semantics where linear operations approximate meaningful semantic transformations
- Evidence anchors:
  - [section] "Because these embeddings reflect distributional semantics—the idea that words appearing in similar contexts share meaning—linear operations often approximate semantic shifts"
  - [section] "Similarly, replacing part of the 'savior' concept with the embedding of 'Trump' effectively encodes a transformation that nudges the generative model"
- Break condition: If the diffusion model uses different conditioning mechanisms that don't rely on the geometric relationships between embeddings, or if the model has been trained to be robust against such embedding manipulations

### Mechanism 3
- Claim: The attack's effectiveness depends on the prevalence and distinctiveness of target figures in the training data
- Mechanism: More frequently appearing and distinctive public figures (like politicians) are easier to target because their representations are more strongly encoded in the model's learned space. Male targets and those with high training data prevalence show better bias success rates
- Core assumption: The frequency and distinctiveness of a figure in training data correlates with how easily their representation can be manipulated through embedding operations
- Evidence anchors:
  - [section] "Out of all the targets, 'Shakira' is the only non-political figure and likely has the least images on her in the dataset used to train the model"
  - [section] "Politicians likely appear in many photos in the dataset, accrued over time as they attend public forums, debates, events, etc. Our results indicate that more relevant a target is, the easier they are to target"
- Break condition: If the model's training data distribution changes significantly or if the model has been trained to reduce bias toward frequently appearing figures

## Foundational Learning

- Concept: Linear algebra operations on high-dimensional vectors
  - Why needed here: The attack relies on weighted linear combinations of embeddings to manipulate semantic representations
  - Quick check question: Can you explain why adding and subtracting word embeddings (like "King - Man + Woman = Queen") can produce meaningful semantic results?

- Concept: Distributional semantics and embedding spaces
  - Why needed here: Understanding how text embeddings capture semantic relationships is crucial for grasping why embedding manipulation works
  - Quick check question: Why do words that appear in similar contexts tend to have similar vector representations in embedding space?

- Concept: Diffusion model conditioning mechanisms
  - Why needed here: The attack targets the text-to-image conditioning pipeline, so understanding how text embeddings influence image generation is essential
  - Quick check question: How do text embeddings typically influence the generation process in text-to-image diffusion models?

## Architecture Onboarding

- Component map: Input prompt → Text encoder → CLIP embeddings → FameBias manipulation → Modified embeddings → Diffusion model → Generated image
- Critical path: The attack operates on the text encoder component, specifically targeting CLIP text embeddings before they reach the diffusion model's conditioning mechanism
- Design tradeoffs: The attack trades off between bias success rate (higher α values) and semantic preservation (higher β values), requiring careful parameter tuning
- Failure signatures: Images that lose semantic coherence, fail to incorporate trigger elements, or don't clearly show target figure characteristics
- First 3 experiments:
  1. Test basic embedding manipulation with simple triggers and targets to verify the mechanism works
  2. Experiment with different α and β values to find optimal parameters for your specific use case
  3. Test the attack across different prompt templates to understand how prompt phrasing affects success rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of FameBias attacks be improved for targets that are currently poorly recognized by the evaluation model?
- Basis in paper: [explicit] The authors note that certain targets like Shakira have low BSR and hypothesize that more precise attacks targeting specific attributes (gender, skin color, attire) could improve results
- Why unresolved: The paper suggests this as future work but doesn't test these more precise attribute-targeting approaches
- What evidence would resolve it: Experiments comparing standard FameBias against attribute-targeted variants (gender, skin tone, clothing style) with corresponding BSR measurements

### Open Question 2
- Question: What is the optimal strategy for selecting α and β parameters to maximize attack success while maintaining semantic fidelity across different trigger-target pairs?
- Basis in paper: [explicit] The authors mention that different triggers may require different α and β values but use a single set of parameters for simplicity
- Why unresolved: The paper performs limited parameter tuning and doesn't explore whether trigger-specific optimization could improve performance
- What evidence would resolve it: Systematic grid search experiments testing various α and β combinations for each trigger-target pair, measuring both BSR and TFR

### Open Question 3
- Question: How can the evaluation methodology be improved to better align with human perception of biased images?
- Basis in paper: [explicit] The authors found significant inconsistencies between LLaVa evaluation and human raters, particularly for certain figures like Shakira and Fidel Castro
- Why unresolved: The paper identifies the discrepancy but doesn't propose or test alternative evaluation methods
- What evidence would resolve it: Comparative studies using different evaluation models or hybrid human-AI evaluation approaches, measuring agreement with human judgment across diverse target figures

### Open Question 4
- Question: What is the most effective defense mechanism against FameBias attacks that balances security with model utility?
- Basis in paper: [explicit] The authors tested Unified Concept Editing (UCE) which successfully prevented attacks but severely degraded model utility
- Why unresolved: The paper identifies the trade-off but doesn't explore alternative defense strategies or optimization techniques
- What evidence would resolve it: Comparative analysis of multiple defense approaches (fine-tuning-based, embedding manipulation, concept editing) measuring both attack prevention and impact on legitimate image generation quality

## Limitations

- The attack shows significant variability in effectiveness across different public figures and trigger words
- Only 8 public figures and 10 trigger nouns were evaluated, limiting generalizability
- The attack requires knowledge of target figure identities and appropriate trigger words
- Effectiveness was only tested on Stable Diffusion V2, not other T2I models

## Confidence

- **High confidence**: The core mechanism of embedding manipulation through weighted linear combinations is technically sound and well-supported by the evidence
- **Medium confidence**: The reported success rates and qualitative observations are supported by the experimental results, though the limited sample size constrains generalizability
- **Medium confidence**: The differential effectiveness across male vs. female targets and high vs. low prevalence figures is supported by the data but requires more extensive testing

## Next Checks

1. **Cross-model validation**: Test FameBias effectiveness on multiple T2I models (e.g., DALL-E, Midjourney, SDXL) to determine if the attack generalizes beyond Stable Diffusion V2
2. **Scale testing**: Expand evaluation to 50+ public figures across different categories (athletes, actors, musicians, business leaders) to better understand effectiveness patterns
3. **Defense robustness evaluation**: Test alternative defense mechanisms beyond UCE, including adversarial training, embedding space regularization, and prompt filtering approaches to assess practical mitigation strategies