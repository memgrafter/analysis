---
ver: rpa2
title: 'CHECKWHY: Causal Fact Verification via Argument Structure'
arxiv_id: '2408.10918'
source_url: https://arxiv.org/abs/2408.10918
tags:
- argument
- evidence
- structure
- fact
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CHECK WHY introduces a novel causal fact verification task that
  evaluates whether the causal relation within a claim is valid through explicit logical
  reasoning. The dataset contains 19K "why" claim-evidence-argument structure triplets,
  each representing a reasoning process from foundational evidence to claim establishment.
---

# CHECKWHY: Causal Fact Verification via Argument Structure

## Quick Facts
- arXiv ID: 2408.10918
- Source URL: https://arxiv.org/abs/2408.10918
- Reference count: 21
- Key result: 88% verification accuracy with argument structures vs 77% without

## Executive Summary
CHECK WHY introduces a novel causal fact verification task that evaluates whether the causal relation within a claim is valid through explicit logical reasoning. The dataset contains 19K "why" claim-evidence-argument structure triplets, each representing a reasoning process from foundational evidence to claim establishment. The argument structures are composed of connected evidence and follow tree-like frameworks inspired by standard argumentation theory. Experiments show that incorporating argument structures significantly improves verification performance, with models achieving 88% accuracy when structures are provided versus 77% without.

## Method Summary
The method involves generating a dataset of "why" claims with associated evidence and argument structures using GPT-4, then evaluating both discriminative and generative models on four verification tasks of increasing difficulty. The tasks range from simple claim-evidence verification to full argument structure generation while verifying. Models are fine-tuned on the dataset with varying amounts of structural information provided, allowing analysis of how argument structures impact verification performance. The approach leverages argumentation theory to structure evidence into logical reasoning paths that guide the verification process.

## Key Results
- Verification accuracy improves from 77% to 88% when argument structures are provided
- Current models achieve only 50-60% structure similarity in automated metrics
- Human evaluation shows even strong models like CodeT5+ produce valid structures only 62.5% of the time
- Multi-hop reasoning capability is significantly enhanced through argument structure guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating argument structures improves causal fact verification accuracy.
- Mechanism: Argument structures explicitly encode the logical reasoning path from foundational evidence to claim establishment, allowing models to leverage structured information beyond raw text.
- Core assumption: The logical relationships between evidence pieces, when properly structured, provide essential context for verifying causal claims.
- Evidence anchors:
  - [abstract] "Experiments on state-of-the-art models show that incorporating argument structures significantly improves verification performance, with models achieving 88% accuracy when structures are provided versus 77% without."
  - [section 5.2] "In Task 1, where structures are absent, models such as the CodeT5 series and LLMs, relying solely on textual information, may struggle to achieve high accuracy. Furthermore, when comparing Task 1 and Task 2, a consistent improvement is noted across all baselines if structures are directly provided."
  - [corpus] Weak - related work focuses on fact verification but not specifically on causal reasoning with explicit argument structures.
- Break condition: If the argument structure generation fails to capture the true reasoning path, or if the model cannot effectively utilize the structured information.

### Mechanism 2
- Claim: Argument structures enable multi-hop reasoning for complex causal claims.
- Mechanism: By representing evidence as connected nodes in a tree-like structure, argument structures guide models through intermediate reasoning steps required to bridge cause-effect pairs.
- Core assumption: Complex causal verification requires following logical inference paths that connect foundational evidence to the claim through multiple reasoning steps.
- Evidence anchors:
  - [abstract] "Each argument structure is composed of connected evidence, representing the reasoning process that begins with foundational evidence and progresses toward claim establishment."
  - [section 4.2] "We identified 5 prevalent high-level categories of reasoning, as shown in Table 2. Event Causality (36.1%) refers to the relations where one event directly leads to another event."
  - [corpus] Weak - related work mentions multi-hop reasoning but not specifically for causal verification with argument structures.
- Break condition: If the reasoning steps in the argument structure are incorrect or incomplete, leading to failed verification.

### Mechanism 3
- Claim: Argument structures provide interpretability for causal fact verification decisions.
- Mechanism: The tree-like structure of arguments makes the reasoning process transparent, allowing inspection of how evidence connects to support or refute claims.
- Core assumption: Interpretable reasoning paths improve trust in verification systems and enable error analysis.
- Evidence anchors:
  - [abstract] "The argument structures are composed of connected evidence and follow tree-like frameworks inspired by standard argumentation theory."
  - [section 2.2] "The argument structure in CHECK WHY follows a tree-like framework, with the claim serving as the root node and evidence branching out as child nodes. These nodes are connected by directed edges that symbolize logical relations."
  - [corpus] Weak - related work mentions interpretability but not specifically through argument structures for causal verification.
- Break condition: If the generated argument structures are too complex or poorly formatted for humans to interpret effectively.

## Foundational Learning

- Concept: Argumentation theory (serial, convergent, linked, divergent arguments)
  - Why needed here: Provides the theoretical foundation for structuring evidence into logical reasoning paths
  - Quick check question: Can you identify which type of argument structure (serial, convergent, linked, divergent) is being used in a simple evidence-claim relationship?

- Concept: Causal reasoning and inference chains
  - Why needed here: Essential for understanding how causes lead to effects through intermediate steps
  - Quick check question: Given a cause and effect, can you identify the logical intermediate steps needed to connect them?

- Concept: Fact verification as natural language inference
  - Why needed here: Forms the basis for determining whether evidence supports, refutes, or is insufficient for a claim
  - Quick check question: Can you classify a claim-evidence pair as SUPPORTS, REFUTES, or NOT ENOUGH INFO?

## Architecture Onboarding

- Component map: WIKI WHY source → filtering → claim/evidence generation → argument structure generation → model training → verification tasks → evaluation
- Critical path: Claim generation → Evidence generation → Argument structure generation → Model training → Verification task → Evaluation
- Design tradeoffs:
  - Using GPT-4 for data generation enables complex argument structures but may introduce model-specific biases
  - Focusing on "why" claims limits scope but enables deeper causal reasoning analysis
  - Providing argument structures aids verification but doesn't test the model's ability to construct them
- Failure signatures:
  - Low structure similarity scores indicate poor argument structure generation
  - Accuracy drops in Task 4 (LE-only) vs Task 3 (full evidence) indicate difficulty with reasoning from limited information
  - Human evaluation showing low validity percentages indicate generation quality issues
- First 3 experiments:
  1. Task 1 (Y3) baseline: Test model accuracy on verification without argument structures
  2. Task 2 baseline: Test accuracy improvement when providing gold argument structures
  3. Task 3 baseline: Test model's ability to generate argument structures while verifying

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the argument structure generation be improved by incorporating more sophisticated graph neural networks or transformer-based models that better capture the complex reasoning patterns in causal fact verification?
- Basis in paper: [inferred] The paper mentions that current models struggle to generate satisfying argument structures, with automated metrics showing only 50-60% structure similarity and human evaluation revealing that even strong models like CodeT5+ produce valid structures only 62.5% of the time.
- Why unresolved: The paper does not explore alternative model architectures or techniques specifically designed for argument structure generation, focusing instead on fine-tuning existing models and LLMs.
- What evidence would resolve it: Experiments comparing the performance of graph neural networks, transformer-based models, or other novel architectures on the argument structure generation task, showing improvements in structure similarity and validity metrics.

### Open Question 2
- Question: How can the quality and reliability of evidence generation be improved to better simulate real-world fact verification scenarios, particularly in open-domain settings?
- Basis in paper: [explicit] The paper mentions that the CHECK WHY dataset is created by LLMs, which could face difficulties in retrieving evidence in open-domain scenarios compared to previous datasets.
- Why unresolved: The paper does not explore methods to improve evidence retrieval or generation, focusing instead on the generation of claims and argument structures based on the WIKI WHY dataset.
- What evidence would resolve it: Experiments demonstrating improved evidence retrieval or generation techniques that result in more relevant and diverse evidence for the causal fact verification task, particularly in open-domain settings.

### Open Question 3
- Question: Can the causal fact verification task be extended to handle more complex causal relations, such as those involving multiple causes, intermediate effects, or feedback loops?
- Basis in paper: [inferred] The paper focuses on verifying claims with simple cause-effect pairs, but the analysis of argument structures reveals diverse forms of reasoning steps that could involve more complex causal relations.
- Why unresolved: The paper does not explore the extension of the causal fact verification task to handle more complex causal relations, focusing instead on the basic cause-effect pair verification.
- What evidence would resolve it: Experiments demonstrating the performance of models on a dataset containing more complex causal relations, such as those involving multiple causes, intermediate effects, or feedback loops, and showing improvements in verification accuracy and argument structure generation.

## Limitations

- Reliance on GPT-4 for generating argument structures may introduce model-specific biases
- Dataset focus on "why" claims limits generalizability to other causal relations
- Only 62.5% human validation rate for generated structures indicates significant quality issues

## Confidence

- **High confidence**: The improvement in verification accuracy when argument structures are provided (88% vs 77%) is well-supported by experimental results.
- **Medium confidence**: The mechanism by which argument structures improve multi-hop reasoning is plausible but requires further validation with more complex causal chains.
- **Low confidence**: The claim that argument structures provide interpretability is weak, as the paper doesn't provide human studies demonstrating improved understanding of verification decisions.

## Next Checks

1. Conduct ablation studies removing different components of argument structures to isolate which structural elements contribute most to verification performance.
2. Test the dataset on models trained with different pre-training objectives to determine if the verification improvement generalizes beyond the specific models used in this study.
3. Evaluate the robustness of verification performance when argument structures contain errors or missing connections to assess how much structural noise models can tolerate.