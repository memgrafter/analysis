---
ver: rpa2
title: 'FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational Turns
  Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated Multi-shot
  Jailbreaks)'
arxiv_id: '2408.16163'
source_url: https://arxiv.org/abs/2408.16163
tags:
- arxiv
- harmful
- intent
- safety
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FRACTURED-SORRY-Bench, a framework for evaluating
  the safety of Large Language Models (LLMs) against multi-turn conversational attacks.
  Building upon the SORRY-Bench dataset, we propose a simple yet effective method
  for generating adversarial prompts by breaking down harmful queries into seemingly
  innocuous sub-questions.
---

# FRACTURED-SORRY-Bench: Framework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench (Automated Multi-shot Jailbreaks)

## Quick Facts
- arXiv ID: 2408.16163
- Source URL: https://arxiv.org/abs/2408.16163
- Reference count: 22
- Key outcome: Introduces a framework for evaluating multi-turn jailbreaking attacks on LLMs by breaking harmful queries into innocuous sub-questions

## Executive Summary
This paper introduces FRACTURED-SORRY-Bench, a framework designed to evaluate the safety of Large Language Models (LLMs) against multi-turn conversational attacks. The framework builds upon the SORRY-Bench dataset and proposes a novel method for generating adversarial prompts by decomposing harmful queries into seemingly innocuous sub-questions. The approach achieves a significant increase in Attack Success Rates (ASRs) across multiple GPT models, demonstrating that this technique poses a substantial challenge to current LLM safety measures and highlights the need for more robust defenses against subtle, multi-turn attacks.

## Method Summary
FRACTURED-SORRY-Bench extends the SORRY-Bench dataset by introducing a method for generating adversarial prompts through the decomposition of harmful queries into innocuous sub-questions. The framework systematically breaks down complex, harmful requests into a series of seemingly harmless sub-queries, allowing attackers to gradually steer the conversation toward the desired harmful outcome. This approach is tested across GPT-4, GPT-4o, GPT-4o-mini, and GPT-3.5-Turbo models, demonstrating its effectiveness in increasing ASRs compared to baseline methods.

## Key Results
- Achieved a maximum increase of +46.22% in Attack Success Rates (ASRs) across tested GPT models
- Demonstrated effectiveness of multi-turn conversational attacks against current LLM safety measures
- Highlighted the need for more robust defenses against subtle, multi-turn jailbreaking techniques

## Why This Works (Mechanism)
The framework works by exploiting the sequential nature of conversational interactions, where LLMs process each turn independently without maintaining a comprehensive understanding of the overall intent. By breaking down harmful queries into innocuous sub-questions, attackers can gradually build context and guide the model toward producing harmful outputs. This approach bypasses traditional safety measures that are often designed to detect and refuse overtly harmful requests but may fail to recognize the cumulative intent behind a series of seemingly benign interactions.

## Foundational Learning
1. **Multi-turn Conversational Attacks** - Understanding how sequential interactions can be exploited to bypass LLM safety measures. *Why needed:* To recognize the vulnerability in current safety mechanisms. *Quick check:* Can you identify how breaking down queries affects model responses?
2. **Adversarial Prompt Generation** - Techniques for creating prompts that circumvent safety filters. *Why needed:* To develop effective attack strategies against LLMs. *Quick check:* How do decomposed queries differ from traditional adversarial prompts?
3. **SORRY-Bench Dataset** - A benchmark for evaluating LLM safety against harmful queries. *Why needed:* To provide a standardized evaluation framework. *Quick check:* What types of harmful queries are included in SORRY-Bench?
4. **Attack Success Rate (ASR) Metrics** - Measuring the effectiveness of jailbreaking attempts. *Why needed:* To quantify improvements in attack techniques. *Quick check:* How is ASR calculated and what constitutes a successful attack?
5. **LLM Safety Mechanisms** - Understanding how models detect and refuse harmful requests. *Why needed:* To identify weaknesses in current defenses. *Quick check:* What are common safety measures employed by LLMs?
6. **Contextual Understanding in LLMs** - How models process and maintain context across conversational turns. *Why needed:* To understand why decomposed queries can be effective. *Quick check:* How well do LLMs maintain context in multi-turn conversations?

## Architecture Onboarding
**Component Map:** User Input -> Query Decomposition -> Sequential Sub-Questions -> LLM Response Evaluation
**Critical Path:** The framework processes user inputs through a decomposition algorithm that generates sequential sub-questions, which are then evaluated against LLM responses to measure ASR improvements.
**Design Tradeoffs:** Balances between query decomposition complexity and attack success rate; simpler decompositions may be more generalizable but less effective.
**Failure Signatures:** Ineffective decompositions that fail to maintain conversational coherence or lead to model refusal at intermediate steps.
**First Experiments:**
1. Test decomposition effectiveness on a small subset of harmful queries from SORRY-Bench
2. Evaluate ASR improvements on a single GPT model variant before scaling to others
3. Analyze intermediate model responses to identify common failure points in the attack chain

## Open Questions the Paper Calls Out
None

## Limitations
- Approach may not generalize well to more complex or context-dependent attack scenarios
- Effectiveness measured against a specific dataset (SORRY-Bench), which may not represent the full range of potential attack vectors
- Limited evaluation to GPT models, potentially missing vulnerabilities in other LLM architectures

## Confidence
- Effectiveness of framework: Medium
- Generalizability to other attack scenarios: Low
- Applicability to non-GPT models: Low

## Next Checks
1. Test the framework against a more diverse set of attack scenarios and datasets to assess its generalizability
2. Evaluate the effectiveness of the framework against models not included in the initial study (e.g., Claude, Llama, etc.)
3. Investigate the potential for adversarial defenses against the multi-turn attack approach proposed in the paper