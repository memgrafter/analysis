---
ver: rpa2
title: 'Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance
  in Machine Translation'
arxiv_id: '2401.08417'
source_url: https://arxiv.org/abs/2401.08417
tags:
- data
- translation
- performance
- preference
- xcomet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in performance between moderate-sized
  LLMs (7B-13B parameters) and state-of-the-art translation models or larger LLMs
  like GPT-4. The authors introduce Contrastive Preference Optimization (CPO), a novel
  training method that leverages preference data to train models to avoid generating
  adequate but not perfect translations.
---

# Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation

## Quick Facts
- arXiv ID: 2401.08417
- Source URL: https://arxiv.org/abs/2401.08417
- Authors: Haoran Xu; Amr Sharaf; Yunmo Chen; Weiting Tan; Lingfeng Shen; Benjamin Van Durme; Kenton Murray; Young Jin Kim
- Reference count: 26
- One-line primary result: CPO enables 7B-13B parameter LLMs to match or exceed WMT competition winners and GPT-4 on translation tasks using only 22K parallel sentences and 0.1% parameter tuning

## Executive Summary
This paper addresses the performance gap between moderate-sized LLMs (7B-13B parameters) and state-of-the-art translation models by introducing Contrastive Preference Optimization (CPO). The authors demonstrate that supervised fine-tuning alone is insufficient for maximizing translation quality due to limitations in reference data. CPO leverages preference data to train models to distinguish between adequate and superior translations, achieving significant improvements while maintaining computational efficiency through a uniform reference model approach.

## Method Summary
CPO is derived as a memory- and speed-efficient variant of Direct Preference Optimization (DPO) that uses a uniform reference model. The method is applied to the ALMA model using only 22K parallel sentences from FLORES-200, with fine-tuning limited to 0.1% of parameters via LoRA. Preference data is constructed using model outputs (GPT-4, ALMA-13B-LoRA) scored by reference-free evaluation models (KIWI-XXL, XCOMET), creating contrastive pairs of preferred and dis-preferred translations.

## Key Results
- ALMA-R trained with CPO matches or exceeds WMT competition winners on WMT'21, WMT'22, and WMT'23 test datasets
- Achieves significant improvements over baseline supervised fine-tuning with only 0.1% parameter updates
- Performance validated by both reference-free metrics (KIWI-XXL, XCOMET) and reference-based metrics (BLEU, COMET-22)
- Human evaluation confirms superior translation quality compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
CPO avoids the performance ceiling of supervised fine-tuning by training models to reject adequate but imperfect translations. Using preference data, CPO creates a contrastive learning signal where models learn to distinguish between good enough and superior translations. This works because gold references in translation datasets may not represent the best possible translations and can limit model performance.

### Mechanism 2
CPO achieves memory- and speed-efficiency compared to DPO by using a uniform reference model. This eliminates the need to store and compute with a separate reference policy, resolving the memory and speed inefficiencies inherent in DPO. The uniform reference assumption enables significant computational savings without substantial performance loss.

### Mechanism 3
The quality of dis-preferred data critically impacts CPO's effectiveness. High-quality dis-preferred data, even if not perfect, provides strong signals for translation refinement. Artificially degraded dis-preferred data (e.g., through noise injection) fails to provide the same learning signal as naturally occurring high-quality but imperfect translations.

## Foundational Learning

- **Concept: Contrastive Learning** - Why needed here: CPO is based on contrastive learning principles where models learn to distinguish between preferred and dis-preferred translations. Quick check: How does contrastive learning differ from traditional supervised learning, and why is it beneficial for translation quality?

- **Concept: Preference Learning** - Why needed here: CPO leverages preference data to guide models toward generating translations that align with human preferences. Quick check: What are the challenges in collecting and utilizing preference data for machine translation, and how does CPO address them?

- **Concept: Reference-free Evaluation** - Why needed here: CPO's effectiveness is evaluated using reference-free metrics to avoid potential biases and limitations of reference-based evaluation. Quick check: Why might reference-free evaluation be more suitable for assessing translations generated by CPO?

## Architecture Onboarding

- **Component map**: Preference Data -> CPO Loss Function (Preference Learning + Behavior Cloning) -> ALMA LLM
- **Critical path**: Construct preference data → Define CPO loss function → Integrate into LLM training pipeline
- **Design tradeoffs**: CPO trades complexity in data preparation (constructing preference data) for efficiency gains in training (compared to DPO)
- **Failure signatures**: Poor translation quality if preference data is noisy/unrepresentative, or if CPO loss function is improperly balanced
- **First 3 experiments**:
  1. Implement CPO on small-scale translation task with simple LLM to validate core mechanism
  2. Compare CPO performance with supervised fine-tuning and DPO on larger translation dataset
  3. Analyze impact of different dis-preferred data types (naturally occurring vs. artificially degraded) on CPO effectiveness

## Open Questions the Paper Calls Out

1. How can we effectively utilize preference data from multiple evaluation models (e.g., KIWI-XXL, XCOMET) to train a single translation model without introducing bias toward any particular metric?

2. What is the impact of dis-preferred data quality on the effectiveness of contrastive preference optimization, and how can we ensure the availability of high-quality dis-preferred data?

3. How can we extend the CPO method to handle low-resource language pairs and domains with limited parallel data availability?

## Limitations

- Generalizability to larger models remains unclear, as uniform reference assumption may not hold for more complex architectures
- Data efficiency vs. data quality trade-off not fully explored, with 22K parallel sentences potentially limiting generalization
- Long-term stability and overfitting potential not thoroughly investigated, with limited discussion of out-of-distribution performance

## Confidence

- **High Confidence**: Core CPO mechanism (contrastive learning with preference data) is well-supported by experimental results
- **Medium Confidence**: Efficiency gains from uniform reference model are supported but limitations not fully explored
- **Low Confidence**: Claims about matching/exceeding WMT winners and GPT-4 are promising but based on limited comparisons

## Next Checks

1. Validate CPO's performance on additional language pairs beyond those tested, particularly low-resource languages, to assess generalizability and identify language-specific limitations.

2. Conduct systematic comparison between CPO's uniform reference model and alternative reference models (e.g., exponential moving average of policy) to quantify efficiency vs. performance trade-offs.

3. Evaluate model performance after extended training periods or on out-of-distribution test sets to assess potential overfitting to preference data and ensure robustness to diverse translation scenarios.