---
ver: rpa2
title: Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads
arxiv_id: '2406.15736'
source_url: https://arxiv.org/abs/2406.15736
tags:
- problems
- step
- children
- lvlms
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SMART-840, a dataset of 840 Mathematical
  Kangaroo Olympiad problems designed to benchmark the reasoning capabilities of large
  vision-and-language models (LVLMs) against children's problem-solving abilities.
  The dataset spans grades 1-12, offering text-only and image-text problems that require
  mathematical and algorithmic reasoning.
---

# Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads

## Quick Facts
- **arXiv ID**: 2406.15736
- **Source URL**: https://arxiv.org/abs/2406.15736
- **Reference count**: 40
- **Primary result**: LVLMs (40-50% accuracy) significantly underperform children (60%+ accuracy) on Mathematical Kangaroo Olympiad problems

## Executive Summary
This paper introduces SMART-840, a dataset of 840 Mathematical Kangaroo Olympiad problems designed to benchmark large vision-and-language models (LVLMs) against children's mathematical reasoning abilities. The study evaluates seven state-of-the-art LVLMs on zero-shot performance across grades 1-12, comparing their accuracy and problem-solving patterns to children's performance statistics. Results reveal that current LVLMs struggle with geometry and logic problems, show no strong correlation between AI and children's problem difficulty, and perform counterintuitively better on higher-grade problems despite lacking human-like cumulative reasoning skills.

## Method Summary
The study constructs SMART-840 by collecting Mathematical Kangaroo problems from 2020-2024 across grades 1-12, including both text-only and image-text problems. Seven SOTA LVLMs (GPT-4o, Gemini-Pro, Claude-3 Opus, GPT-4v, Gemini-Flash, XGen-MM) are evaluated using zero-shot prompting with a hand-crafted prompt asking for explanation and answer. Model outputs are automatically parsed and compared against ground truth answers. Performance is analyzed across grade levels and problem categories, with correlation analysis comparing AI performance difficulty metrics to children's performance statistics (difficulty index, discriminative index, time correlation, weight correlation, entropy).

## Key Results
- LVLMs achieve 40-50% accuracy versus children's 60%+ accuracy on the same problems
- GPT-4o performs best at 42.5% average accuracy, followed by Claude-3 Opus at 38% and Gemini-Pro at 32%
- Models struggle most with geometry and logic problems involving images
- Performance does not improve with grade level as expected for human learners
- No significant correlation exists between problem difficulty for AI models and children

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4o outperforms other LVLMs because it has better visual grounding and language comprehension, not because of higher training data overlap with Olympiad problems.
- Mechanism: The model can integrate text and image cues to form a coherent problem-solving approach, leading to higher accuracy across grades and categories.
- Core assumption: Visual reasoning ability directly improves problem-solving in geometry and logic tasks.
- Evidence anchors:
  - [abstract] "Models struggle most with geometry and logic problems involving images, and performance does not improve with grade level as expected."
  - [section 3.4] "GPT-4o performs the best on average across all the grades with an average accuracy of 42.5%, followed by Claude-3-Opus at 38% and Gemini-Pro at nearly 32%."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.509, average citations=0.0." Weak corpus support for direct comparison.
- Break condition: If visual grounding is degraded by noisy or incomplete image inputs, performance drops disproportionately in geometry/logic categories.

### Mechanism 2
- Claim: LVLMs show a consistent improvement in performance from lower to higher grades, contrary to expected human developmental patterns.
- Mechanism: Training data distribution and model capacity enable better performance on complex multi-step reasoning tasks typical of higher-grade problems.
- Core assumption: Model performance correlates with problem complexity rather than foundational knowledge.
- Evidence anchors:
  - [section 3.4] "there is a substantial variance in the performance of SOTA LVLMs... with an accuracy of 40% at grades 1-2 towards nearly 50% for grades 11-12."
  - [abstract] "they perform significantly below children (40-50% vs. 60%+ accuracy), with no strong correlation between problem difficulty for AI models and children."
  - [corpus] "Max It or Miss It: Benchmarking LLM On Solving Extremal Problems" suggests similar findings in extremal problem solving.
- Break condition: If foundational concepts are not reinforced in training data, performance will collapse on lower-grade problems.

### Mechanism 3
- Claim: Providing explanation prompts significantly improves LVLM performance by forcing structured reasoning.
- Mechanism: The act of generating intermediate steps enforces a logical chain, reducing guessing and improving answer accuracy.
- Core assumption: Structured reasoning prompts activate internal reasoning mechanisms in LVLMs.
- Evidence anchors:
  - [section 3.4] "Importance of Reasoning with Explanation: In this experiment, we changed the LVLM prompt... We see a trend of a dip in performance among all the models."
  - [abstract] "no significant correlation between the reasoning capabilities of AI models and that of young children, and their capabilities appear to be based on a different type of reasoning than the cumulative knowledge."
  - [corpus] "DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models" supports creativity under structured reasoning.
- Break condition: If the prompt is malformed or the model misinterprets the instruction, performance can degrade to or below baseline.

## Foundational Learning

- Concept: Graph coloring and chromatic number
  - Why needed here: Many logic and geometry problems in SMART-840 require understanding of graph coloring to solve efficiently.
  - Quick check question: Given a simple graph with 4 vertices all connected to each other, what is the minimum number of colors needed so no adjacent vertices share the same color?

- Concept: Integer root multiplicity in polynomials
  - Why needed here: Problems involving polynomial factorization rely on recognizing repeated roots and their multiplicities.
  - Quick check question: If (x - 2)^3 divides a degree-5 polynomial, what is the multiplicity of the root x = 2?

- Concept: Triangle angle sum and isosceles properties
  - Why needed here: Geometry problems in the dataset frequently use angle sum properties and side-angle relationships.
  - Quick check question: In an isosceles triangle with AB = AC, if angle B is 40Â°, what is the measure of angle A?

## Architecture Onboarding

- Component map:
  Data ingestion pipeline -> OCR extraction -> manual validation -> category labeling -> Model interface layer -> API calls to GPT-4o, Gemini-Pro, Claude-3 Opus -> prompt formatting -> Evaluation engine -> automated parsing of answers -> accuracy computation -> correlation analysis -> Reporting module -> grade-wise, category-wise, correlation visualizations

- Critical path:
  1. Load SMART-840 dataset (text + image)
  2. Format prompt for each model
  3. Submit via API, capture raw response
  4. Parse answer option (A1-E5) automatically
  5. Compare against ground truth, log accuracy
  6. Compute correlations with human performance metrics

- Design tradeoffs:
  - API rate limits vs. parallel querying
  - Manual vs. automatic answer parsing accuracy
  - Fixed prompts vs. adaptive prompting strategies
  - Inclusion of only 2020-2024 data vs. full historical set

- Failure signatures:
  - Invalid or unparsable responses (marked as 'K')
  - No response or timeout (marked as '-1')
  - Incorrect answer option selection despite valid format

- First 3 experiments:
  1. Validate that the answer parsing logic correctly maps raw text to options A1-E5
  2. Confirm that re-querying the same problem yields consistent results (variance check)
  3. Run a small subset of problems with human-in-the-loop validation to verify accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact nature of the reasoning gap between LVLMs and human children in mathematical problem-solving?
- Basis in paper: [explicit] The paper explicitly states that "there is no significant correlation between the reasoning capabilities of AI models and that of young children" and that "their capabilities appear to be based on a different type of reasoning than the cumulative knowledge that underlies children's mathematics and logic skills."
- Why unresolved: The paper identifies the gap but does not provide a detailed explanation of the underlying cognitive or algorithmic differences.
- What evidence would resolve it: Detailed comparative studies of the step-by-step reasoning processes of LVLMs and children, possibly through cognitive science experiments or algorithmic analysis of model behavior.

### Open Question 2
- Question: Why do LVLMs perform better on higher-grade problems than lower-grade problems, contrary to human learning patterns?
- Basis in paper: [explicit] The paper notes that "LMLs perform significantly below children (40-50% vs. 60%+ accuracy)" and "there is a substantial gap between the best of LVLMs and the worst, or random baselines," but also observes that LVLMs improve with grade level, which is "counter intuitive."
- Why unresolved: The paper acknowledges this trend but does not explain the underlying reasons, such as training data distribution or model architecture limitations.
- What evidence would resolve it: Analysis of the training data used for LVLMs, particularly the distribution of problem types and complexities, and experiments testing model performance on problems with varying difficulty levels.

### Open Question 3
- Question: How do the reasoning strategies of LVLMs differ from those of children when solving mathematical problems?
- Basis in paper: [explicit] The paper states that "there is no significant correlation between the perceived complexity in solving puzzles by children and by AI models" and that LVLMs "struggle to perform even on puzzles involving simple geometry and logic."
- Why unresolved: The paper identifies the lack of correlation but does not provide insights into the specific reasoning strategies employed by LVLMs versus children.
- What evidence would resolve it: Detailed analysis of the problem-solving approaches used by LVLMs and children, possibly through protocol analysis or think-aloud studies, to identify key differences in reasoning strategies.

## Limitations

- Dataset construction process lacks full transparency, raising concerns about potential sampling bias in problem selection
- Limited human performance statistics (only 840 problems across 12 grades) may weaken correlation analysis validity
- Explanation-prompt experiment lacks statistical significance testing, making performance degradation claims uncertain

## Confidence

**High confidence**: GPT-4o demonstrates superior performance among evaluated LVLMs, and LVLMs consistently underperform children (40-50% vs. 60%+ accuracy)

**Medium confidence**: The claim that LVLMs lack human-like cumulative reasoning skills is supported by data but could be influenced by prompt engineering choices and specific problem selection

**Low confidence**: The finding of "no strong correlation" between AI and children's problem difficulty is statistically weak without confidence intervals or significance testing

## Next Checks

1. **Statistical significance validation**: Re-run the correlation analysis between AI and children's performance using bootstrap confidence intervals to determine if the "no strong correlation" finding is robust or within expected variance

2. **Dataset contamination audit**: Conduct a comprehensive search for Mathematical Kangaroo problems in the training data of evaluated LVLMs to quantify potential contamination effects on performance metrics

3. **Prompt sensitivity analysis**: Systematically vary the explanation prompt structure (with/without explanation, different phrasings) across all problems and compute variance in accuracy to determine if the observed performance degradation is consistent or prompt-dependent