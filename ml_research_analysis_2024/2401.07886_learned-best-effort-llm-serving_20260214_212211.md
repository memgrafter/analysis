---
ver: rpa2
title: Learned Best-Effort LLM Serving
arxiv_id: '2401.07886'
source_url: https://arxiv.org/abs/2401.07886
tags:
- arrival
- serving
- rate
- policy
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Learned best-effort LLM serving uses deep RL to dynamically route
  requests to models of varying quality and latency, improving performance under fluctuating
  loads. On unpredictable workloads, the system maintains 96% peak performance 4.1x
  more often and 98% peak performance 2.3x more often than static serving.
---

# Learned Best-Effort LLM Serving

## Quick Facts
- arXiv ID: 2401.07886
- Source URL: https://arxiv.org/abs/2401.07886
- Authors: Siddharth Jha; Coleman Hooper; Xiaoxuan Liu; Sehoon Kim; Kurt Keutzer
- Reference count: 22
- One-line primary result: Learned best-effort LLM serving uses deep RL to dynamically route requests to models of varying quality and latency, improving performance under fluctuating loads

## Executive Summary
This paper introduces a learned best-effort system for LLM serving that uses deep reinforcement learning to dynamically route requests to models of varying quality and latency. The system is designed to handle fluctuating loads and unpredictable workloads by learning a policy that balances response quality against latency deadlines. Through extensive evaluation, the authors demonstrate that their approach can maintain high performance even under challenging conditions, significantly outperforming static serving baselines.

The key innovation lies in the combination of reinforcement learning for routing decisions with a carefully designed reward function that accounts for both quality and timeliness. The system can handle both hard and soft deadlines, making it flexible for different application requirements. The learned policy demonstrates robustness to shifts in task distribution and can sustain much higher request rates while reducing hardware costs compared to traditional serving approaches.

## Method Summary
The method employs deep Q-learning (DQN) with Double Q-learning to train a policy network that routes incoming requests to appropriate LLM models based on current system state. The router uses a 2-layer MLP with 256 hidden units to select from three model sizes (OPT-125M, OPT-1.3B, OPT-6.7B) based on a state vector containing task index, current batch sizes, and estimated arrival rate. The policy is trained for 1.2 million iterations using synthetic workloads with hard deadlines and uniform task distribution, then fine-tuned for soft deadlines. Evaluation is conducted on both stable and unpredictable workloads, including synthetic traces and Microsoft Azure Function (MAF) traces.

## Key Results
- On unpredictable workloads, the system maintains 96% peak performance 4.1x more often and 98% peak performance 2.3x more often than static serving
- Sustains over 10x higher request rates while reducing hardware costs by 3.94x per GPU compared to doubling GPU resources
- The learned policy is robust to shifts in task distribution and handles both hard and soft deadlines effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic routing through reinforcement learning allows the system to achieve near-optimal performance across varying request loads by selecting the appropriate model based on task type and system state
- Mechanism: The router maintains a state vector containing the task index, current batch sizes for each model, and an estimate of the arrival rate. Based on this state, the policy network selects which model to send the request to. The reward function encourages high-quality responses while penalizing missed deadlines, with the penalty severity depending on whether the deadline is hard or soft
- Core assumption: The quality of different models for different tasks is known and can be quantified. The system can accurately estimate the current load and arrival rate
- Evidence anchors:
  - [abstract] "Our best-effort system can maintain availability with over 10× higher client request rates, serves above 96% of peak performance 4.1× more often..."
  - [section] "The reward for picking a specific model in a given state should be a function that increases with higher quality responses but decreases if the client's deadline is missed."
- Break condition: If the model quality metrics become inaccurate or the load estimation fails, the policy may make suboptimal routing decisions

### Mechanism 2
- Claim: Fine-tuning the policy from a hard deadline policy to a soft deadline policy enables quick adaptation to different deadline types without retraining from scratch
- Mechanism: The policy is first trained with hard deadlines, then the reward function is modified to implement the soft deadline decay function, and the policy is fine-tuned for additional iterations. This leverages the initial policy's learned state-action relationships while adapting to the new reward structure
- Core assumption: The initial policy has learned useful state-action mappings that are partially transferable to the soft deadline setting
- Evidence anchors:
  - [section] "We pick a specific soft deadline decay function and fine-tune the policy from the policy trained with the hard deadlines... We find that fine-tuning the policy gives good performance."
- Break condition: If the soft deadline requirements are significantly different from hard deadlines, the initial policy may need substantial modifications, making fine-tuning less effective

### Mechanism 3
- Claim: The learned policy demonstrates robustness to shifts in task distribution, maintaining high performance even when the served task mix differs from the training distribution
- Mechanism: The policy learns to route requests based on task characteristics and system load rather than memorizing specific task frequencies. This allows it to generalize to new task distributions by applying the same quality-vs-latency tradeoff principles
- Core assumption: The policy captures fundamental relationships between task characteristics, model capabilities, and system load that remain valid across different task distributions
- Evidence anchors:
  - [section] "Even though the task distribution is drastically different than the training distribution, the policy is still able to achieve better performance than OPT-6.7B OPT-1.3B, and OPT-125M."
- Break condition: If the new task distribution includes tasks with characteristics vastly different from the training set, the policy may struggle to make optimal routing decisions

## Foundational Learning

- Concept: Reinforcement Learning with Deep Q-Networks
  - Why needed here: The routing problem involves sequential decision-making under uncertainty, where the optimal action depends on the current system state and the policy needs to maximize long-term reward (aggregate performance across all requests)
  - Quick check question: What is the difference between the state in this routing problem and a typical grid-world RL problem?

- Concept: State Representation in Complex Systems
  - Why needed here: The router must capture multiple dimensions of system state (task type, model batch sizes, arrival rate) to make informed routing decisions. Understanding how to design effective state representations is crucial
  - Quick check question: How does the choice of state representation affect the policy's ability to generalize to unseen system conditions?

- Concept: Reward Shaping for Multi-Objective Optimization
  - Why needed here: The system must balance two competing objectives: maximizing response quality and meeting latency deadlines. The reward function must encode this tradeoff appropriately
  - Quick check question: How would you modify the reward function if some tasks were more latency-sensitive than others?

## Architecture Onboarding

- Component map: Client requests → Router state evaluation → Model selection → Load balancing → Model inference → Response delivery → Reward calculation

- Critical path: Client request → Router state evaluation → Model selection → Load balancing → Model inference → Response delivery → Reward calculation

- Design tradeoffs: The use of a small policy network (2-layer MLP) allows for CPU execution with minimal latency, but may limit the complexity of policies that can be learned. The choice of DQN with Double Q-learning helps prevent overestimation bias but requires careful hyperparameter tuning

- Failure signatures:
  - Router consistently selecting sub-optimal models (indicating poor policy learning)
  - System unable to handle load spikes despite policy selection (indicating incorrect state estimation or inadequate model capacity)
  - Performance degradation when task distribution shifts (indicating lack of policy robustness)

- First 3 experiments:
  1. Test the router with a single, stable arrival rate and a uniform task distribution to verify basic functionality and compare against static baselines
  2. Introduce load spikes in the arrival pattern to evaluate the router's ability to adapt to changing conditions and maintain performance
  3. Modify the task distribution to be non-uniform and observe how the router adjusts its routing strategy compared to the uniform distribution case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the learned policy's performance scale when more models of varying quality and latency are added to the serving system?
- Basis in paper: [explicit] The paper mentions "any number of models and tasks may be used" and demonstrates results with 3 models, but does not explore scaling to more models
- Why unresolved: The paper only evaluates the policy with 3 model sizes (OPT-125M, OPT-1.3B, OPT-6.7B) and does not investigate how adding more models of different sizes affects performance
- What evidence would resolve it: Experimental results showing performance metrics (e.g., request handling rate, latency, accuracy) with different numbers of models (e.g., 5, 7, 10 models) and how the learned policy adapts to route requests optimally

### Open Question 2
- Question: How robust is the learned policy to sudden and extreme changes in the task distribution that were not seen during training?
- Basis in paper: [explicit] The paper evaluates robustness to shifts in task distribution but only considers uniform distributions across tasks or single-task scenarios
- Why unresolved: The paper does not explore scenarios with abrupt, extreme changes in task distribution (e.g., from a balanced mix to 90% of requests being for one task) that could occur in real-world applications
- What evidence would resolve it: Experimental results showing the policy's performance under various extreme task distribution shifts (e.g., 90/10, 95/5 splits) and comparing it to static serving baselines

### Open Question 3
- Question: How does the learned policy perform when the latency requirements for different tasks are dynamic and change over time?
- Basis in paper: [explicit] The paper evaluates the policy with fixed deadlines for tasks and mentions the possibility of different deadlines but does not explore dynamic changes
- Why unresolved: The paper does not investigate how the policy adapts when latency requirements for tasks change dynamically (e.g., stricter deadlines during peak hours, looser deadlines during off-peak hours)
- What evidence would resolve it: Experimental results showing the policy's performance with tasks having dynamic latency requirements that change over time, and comparing it to static serving baselines under the same conditions

## Limitations
- The evaluation relies heavily on synthetically generated workloads rather than real-world production traces, making generalizability uncertain
- Experiments were conducted in a single data center with specific hardware configuration, not addressing multi-datacenter deployments or different hardware setups
- The system only considers three model sizes, not exploring how the approach would scale to larger model catalogs with more diverse quality-latency tradeoffs

## Confidence

**High Confidence**: The core mechanism of using RL for dynamic request routing is well-established and the empirical results showing improved performance under fluctuating loads appear robust. The ablation studies on model capacity and the comparison with static baselines provide strong evidence for the effectiveness of the approach.

**Medium Confidence**: The claims about robustness to task distribution shifts and the ability to handle both hard and soft deadlines are supported by experimental evidence, but the evaluation scenarios may not fully capture the complexity of real-world serving environments. The transfer learning approach for adapting from hard to soft deadlines shows promise but is demonstrated on limited scenarios.

**Low Confidence**: The scalability claims (10x higher request rates, 3.94x hardware cost reduction) are based on controlled experiments and may not directly translate to production environments with more complex traffic patterns, heterogeneous hardware, and multiple concurrent services competing for resources.

## Next Checks
1. Deploy the learned best-effort system in a production environment with real user traffic for at least one week. Compare its performance against static serving baselines using actual business metrics (e.g., user satisfaction, cost per request) rather than synthetic metrics.

2. Evaluate the approach in a multi-datacenter configuration with heterogeneous hardware and varying network latencies. Test how well the learned policy generalizes across different geographic regions and how it handles cross-datacenter routing decisions.

3. Extend the evaluation to include a larger catalog of model variants (e.g., 10+ different model sizes and architectures). Assess how the policy scales with increased model complexity and whether the learned routing decisions remain effective as the model space grows.