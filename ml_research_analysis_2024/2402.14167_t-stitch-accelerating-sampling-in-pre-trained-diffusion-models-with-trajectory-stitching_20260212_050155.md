---
ver: rpa2
title: 'T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory
  Stitching'
arxiv_id: '2402.14167'
source_url: https://arxiv.org/abs/2402.14167
tags:
- t-stitch
- steps
- sampling
- diffusion
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: T-Stitch accelerates diffusion model sampling by dynamically replacing
  the first portion of denoising steps with a smaller pretrained model. The method
  leverages the observation that different diffusion models learn similar encodings
  and that smaller models can effectively generate global structure in early steps.
---

# T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching

## Quick Facts
- arXiv ID: 2402.14167
- Source URL: https://arxiv.org/abs/2402.14167
- Reference count: 40
- Primary result: Replaces first 40% of denoising steps with 10× faster small model, yielding no performance drop on ImageNet

## Executive Summary
T-Stitch accelerates diffusion model sampling by dynamically replacing early denoising steps with a smaller pretrained model. The method leverages the observation that different diffusion models learn similar encodings and that small models can effectively generate global structure in early steps. By switching from a small model to a large model at a configurable timestep, T-Stitch achieves significant speedups without quality degradation. The approach is training-free, architecture-agnostic within model families, and complements existing acceleration techniques.

## Method Summary
T-Stitch works by identifying a switch point during the denoising process where the small model's output becomes sufficiently similar to the large model's output. The method uses a small pretrained diffusion model for the initial portion of denoising steps (e.g., first 40%) and switches to a large pretrained model for the remaining steps. This approach exploits the frequency decomposition property of diffusion sampling, where early steps focus on low-frequency components that small models can capture effectively. The switching occurs at a configurable timestep, allowing for flexible speed-quality tradeoffs.

## Key Results
- Replacing first 40% of steps with DiT-S (10× faster) yields no performance drop on ImageNet
- Method works across different model sizes within the same family
- Applied to Stable Diffusion, improves both sampling speed and prompt alignment for stylized models
- Speedups can be increased by using smaller models or adjusting the switch point

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early denoising steps focus on low-frequency components while later steps refine high-frequency details
- Mechanism: Small diffusion models can generate adequate global structure in early steps because these steps only need to capture coarse, low-frequency features
- Core assumption: The denoising process naturally decomposes into frequency-specific stages where early steps handle low-frequency structure
- Evidence anchors:
  - [abstract] "From the frequency perspective, the denoising process focuses on generating low-frequency components at the early steps while the later steps target the high-frequency signals"
  - [section] "we observe that the low-frequency amplitudes increase rapidly at the early timesteps"
  - [corpus] Weak - no direct corpus evidence on frequency decomposition in diffusion sampling
- Break condition: If high-frequency information is needed early in the process or if the small model cannot capture even low-frequency structure

### Mechanism 2
- Claim: Different diffusion models trained on the same data distribution learn similar latent encodings
- Mechanism: Because models learn similar embeddings, we can safely switch between models at different stages without disrupting the generation trajectory
- Core assumption: The latent space learned by different diffusion models exhibits structural similarity when trained on the same dataset
- Evidence anchors:
  - [abstract] "Recent work suggests a common latent space across different DPMs trained on the same data distribution"
  - [section] "the cosine similarities between the output latent noises from different DiT models reach almost 100% at early steps"
  - [corpus] Weak - no direct corpus evidence on latent space similarity across diffusion models
- Break condition: If models trained on the same dataset learn fundamentally different representations or if the similarity breaks down at certain timesteps

### Mechanism 3
- Claim: Small models are computationally much cheaper than large models, enabling speed gains when used for early steps
- Mechanism: By replacing expensive large model computations in early steps with cheaper small model computations, we achieve overall speedup without quality loss
- Core assumption: The computational cost difference between small and large models is significant enough to yield meaningful speedup
- Evidence anchors:
  - [abstract] "DiT-S is computationally much cheaper than DiT-XL" and "replacing the first 40% of steps with a 10× faster DiT-S"
  - [section] "DiT-S is 10× faster than DiT-XL" and "we can keep increasing the inference speed"
  - [corpus] Weak - no direct corpus evidence on computational cost differences
- Break condition: If the computational gap between models is small or if overhead from model switching negates the savings

## Foundational Learning

- Concept: Diffusion probabilistic models and the denoising process
  - Why needed here: Understanding how diffusion models work is essential to grasp why trajectory stitching is effective
  - Quick check question: What is the difference between forward and reverse diffusion processes in DPMs?

- Concept: Frequency domain analysis and signal decomposition
  - Why needed here: The mechanism relies on understanding how different frequency components are handled at different stages
  - Quick check question: How do low-frequency and high-frequency components differ in terms of spatial information content?

- Concept: Model families and pretraining in diffusion models
  - Why needed here: T-Stitch works specifically with pretrained models from the same family on the same dataset
  - Quick check question: Why does training on the same dataset matter for the effectiveness of trajectory stitching?

## Architecture Onboarding

- Component map: Small model denoising → Model switch → Large model denoising → Final image generation
- Critical path: Noise initialization → Small model denoising for first r% steps → Model switch → Large model denoising for remaining (100-r)% steps → Final image generation
- Design tradeoffs: Speed vs quality (more small model steps = faster but potentially lower quality), model size gap vs performance (larger size gap = more speedup but potentially more quality degradation), switch point selection vs optimal performance
- Failure signatures: Quality degradation when using too much of the small model, instability at the model switching point, unexpected artifacts in generated images
- First 3 experiments:
  1. Verify the cosine similarity between small and large model outputs at various timesteps to confirm the latent space similarity assumption
  2. Test different switch points (10%, 30%, 50%, 70%) to find the optimal trade-off between speed and quality
  3. Measure actual inference time with different fractions of small model steps to confirm the expected speedup

## Open Questions the Paper Calls Out

None

## Limitations

- Dependency on model similarity - requires pretrained models from the same family trained on the same dataset
- Quality degradation occurs when using too much of the small model, indicating fundamental limits to acceleration
- Relies on static switching point rather than adaptive decision-making for optimal performance

## Confidence

**High confidence** in the core observation that different diffusion models learn similar latent encodings - supported by cosine similarity measurements showing "almost 100% at early steps" and successful application across multiple model pairs.

**Medium confidence** in the frequency-based mechanism - empirical observations about amplitude changes but lacks rigorous frequency analysis and theoretical grounding.

**Medium confidence** in the computational efficiency claims - 10× speedup claim needs detailed benchmarks across different hardware configurations and consideration of model switching overhead.

## Next Checks

1. **Cross-architecture validation**: Test T-Stitch with models from different architecture families (e.g., DiT to DDPM) trained on the same dataset to determine if latent space similarity extends beyond model families.

2. **Adaptive switching mechanism**: Implement and evaluate an adaptive approach that dynamically determines the optimal switching point based on image content or generation stability metrics, rather than using a fixed percentage.

3. **Multi-model stitching**: Extend the technique to use more than two models (e.g., very small for first 20%, medium for next 30%, large for final 50%) to explore whether a graduated approach yields better quality-speed tradeoffs than binary switching.