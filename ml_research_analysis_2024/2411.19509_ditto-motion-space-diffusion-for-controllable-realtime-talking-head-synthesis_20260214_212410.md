---
ver: rpa2
title: 'Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head Synthesis'
arxiv_id: '2411.19509'
source_url: https://arxiv.org/abs/2411.19509
tags:
- motion
- generation
- head
- inference
- facial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Ditto, a diffusion-based framework for one-shot
  talking head synthesis that enables fine-grained control and real-time inference.
  The core innovation is replacing conventional VAE latent spaces with an explicit
  identity-agnostic motion space, which reduces learning complexity while enabling
  precise control over facial attributes.
---

# Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head Synthesis

## Quick Facts
- arXiv ID: 2411.19509
- Source URL: https://arxiv.org/abs/2411.19509
- Reference count: 40
- Key outcome: Real-time diffusion-based one-shot talking head synthesis with fine-grained control, achieving FID 17.254, FVD 219.368, and CSIM 0.864

## Executive Summary
Ditto presents a diffusion-based framework for one-shot talking head synthesis that achieves real-time inference while enabling fine-grained control over facial attributes. The key innovation is moving from conventional VAE latent spaces to an explicit identity-agnostic motion space, which reduces learning complexity and enables precise control. By employing a Diffusion Transformer to generate motion representations conditioned on audio, emotion labels, and eye states, followed by a dedicated one-shot face renderer, Ditto achieves state-of-the-art quality metrics while running in real-time (RTF < 1) compared to 30-50x slower existing methods.

## Method Summary
Ditto introduces a motion-space diffusion approach for audio-driven talking head synthesis. The method first extracts identity-agnostic motion representations using 3D implicit keypoints, expression deformations, and head poses from input videos. A Diffusion Transformer (DiT) then generates these motion representations conditioned on audio features, emotion labels, eye states, and initial motion. The generated motion is rendered into high-quality video frames using a one-shot face renderer. The entire pipeline is optimized for real-time streaming inference through three key modules: HuBERT-based audio feature extraction with streaming capabilities, DiT with reduced denoising steps, and TensorRT-optimized face rendering.

## Key Results
- Outperforms existing methods on FID (17.254 vs 21.445-42.554), FVD (219.368 vs 219.368-436.862), and CSIM (0.864 vs 0.806-0.840)
- Achieves real-time inference with RTF < 1 compared to 30-50x slower existing diffusion-based methods
- Demonstrates superior fine-grained control over facial attributes including mouth opening, gaze direction, and emotion expression

## Why This Works (Mechanism)
The method works by replacing conventional VAE latent spaces with an explicit identity-agnostic motion space. This approach reduces the complexity of the learning problem by focusing on motion generation rather than full image synthesis. The motion space disentangles facial attributes, allowing independent control over different aspects of facial expression. The DiT architecture processes sequential audio-to-motion generation more effectively than previous methods, while the one-shot face renderer efficiently maps motion representations to high-quality video frames.

## Foundational Learning

**Motion Representation Learning**: Learning compact, identity-agnostic motion representations from video frames using 3D implicit keypoints and expression deformations. Needed to reduce the complexity of the learning problem and enable fine-grained control. Quick check: Verify that motion representations capture lip movements accurately and maintain temporal consistency.

**Diffusion Transformer Architecture**: Using transformer-based diffusion models for sequential data generation, particularly suited for audio-to-motion conversion. Needed to handle the sequential nature of audio inputs and generate temporally coherent motion. Quick check: Validate that the DiT model generates smooth motion transitions between frames.

**Real-time Optimization**: Implementing streaming audio feature extraction, reduced denoising steps, and hardware-optimized rendering for real-time inference. Needed to achieve practical deployment of diffusion-based methods. Quick check: Benchmark inference speed on target hardware to verify RTF < 1 claims.

## Architecture Onboarding

**Component Map**: Audio Input -> HuBERT Feature Extraction -> DiT Motion Generation -> Motion Renderer -> Video Output

**Critical Path**: The critical path flows from audio input through the DiT model to motion generation, then to the one-shot face renderer. The DiT model is the computational bottleneck and requires optimization for real-time performance.

**Design Tradeoffs**: Motion-space representation vs latent-space representation (complexity vs control), real-time inference vs model size (RTF < 1 vs state-of-the-art quality), and fine-grained control vs computational efficiency.

**Failure Signatures**: Poor lip synchronization indicates audio feature misalignment, artifacts in teeth/hair regions suggest motion disentanglement issues, and motion jitter indicates insufficient denoising steps or temporal inconsistency in the DiT model.

**Three First Experiments**:
1. Validate motion extractor pipeline using available code from [27] to ensure identity-agnostic motion representations can be extracted
2. Conduct ablation study removing each conditional signal (emotion, eye state, initial motion) to validate their contribution
3. Benchmark inference speed using TensorRT optimization on target GPU to verify real-time performance claims

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Architectural details of Motion Extractor [27] are not fully specified, limiting reproducibility
- DiT model configuration details (layers, attention heads, embedding dimensions) are underspecified
- Real-time performance depends on optimization choices that may not be fully detailed in the paper

## Confidence
High confidence in core claims: Real-time diffusion-based talking head synthesis with fine-grained control is achievable and outperforms existing methods on standard metrics.

Medium confidence in reproducibility: Critical architectural details are underspecified, particularly Motion Extractor implementation and DiT configuration.

## Next Checks

1. Reimplement the motion extractor pipeline using available code from [27] and verify that identity-agnostic motion representations can be successfully extracted from talking head videos

2. Conduct a controlled ablation study removing each conditional signal (emotion, eye state, initial motion) to validate their contribution to final output quality

3. Benchmark inference speed using TensorRT optimization on a comparable GPU to verify the claimed real-time performance with RTF < 1